{"text":"Human Computer Interaction \u2013 Lecture Notes\nCambridge Computer Science Tripos, Part II\nAlan Blackwell\nOverview of content:\nLecture 1: The scope and challenges of HCI and Interaction Design.\nLecture 2: Visual representation. Segmentation and variables of the display plane. Modes\nof correspondence\nLecture 3: Text and gesture interaction. Evolution of interaction hardware. Measurement\nand assessment of novel methods.\nLecture 4: Inference-based approaches. Bayesian strategies for data entry, and\nprogramming by example.\nLecture 5: Augmented reality and tangible user interfaces. Machine vision, fiducial\nmarkers, paper interfaces, mixed reality.\nLecture 6: Usability of programming languages. End-user programming, programming for\nchildren, cognitive dimensions of notations.\nLecture 7: User-centred design research. Contextual observation, prototyping, think-aloud\nprotocols, qualitative data in the design cycle.\nLecture 8: Usability evaluation methods. Formative and summative methods. Empirical\nmeasures. Evaluation of part II projects.\n1 Lecture 1: What is HCI \/ Interaction Design?\nWith the exception of some embedded software and operating system code, the success of a\nsoftware product is determined by the humans who use the product. These notes present\ntheoretical and practical approaches to making successful and usable software. A user-\ncentred design process, as taught in earlier years of the tripos and experienced in many\ngroup design projects, provides a professional resource to creating software with\nfunctionality that users need. However, the availability of technical functionality does not\nguarantee that software will be practically usable. Software that is usable for its purpose is\nsometimes described by programmers as \u201cintuitive\u201d (easy to learn, easy to remember, easy\nto apply to new problems) or \u201cpowerful\u201d (efficient, effective). These terms are vague and\nunscientific, but they point in the right direction. This course presents scientific approaches\nto making software that is \u201cintuitive\u201d and \u201cpowerful\u201d.\nHCI helps us to understand why some software products are good and other software is\nbad. But sadly it is not a guaranteed formula for creating a successful product. In this sense\nit is like architecture or product design. Architects and product designers need a thorough\ntechnical grasp of the materials they work with, but the success of their work depends on\nthe creative application of this technical knowledge. This creativity is a craft skill that is\nnormally learned by working with a master designer in a studio, or from case studies of\nsuccessful designs. A computer science course does not provide sufficient time for this kind\nof training in creative design, but it can provide the essential elements: an understanding of\nthe user\u2019s needs, and an understanding of potential solutions.\nThere are many different approaches to the study and design of user interfaces. This course\nattempts, so far as possible within 8 lectures, to discuss the important aspects of fields\nincluding: Interaction Design, User Experience Design (UX), Interactive Systems Design,\nCognitive Ergonomics, Man-Machine Interface (MMI), User Interface Design (UI), Human\nFactors, Cognitive Task Design, Information Architecture (IA), Software Product Design,\nUsability Engineering, User-Centred Design (UCD) and Computer Supported Collaborative\nWork (CSCW).\nThese investigations require a wide range of academic styles, extending across all the parts\nof the University. Lack of familiarity with other kinds of investigation and analysis can\nmake it hard to absorb or collaborate with other perspectives. The advantages of different\ndisciplines can range from those that are interpretive to those that are scientific (both are\nnecessary), the first criticized as soft and the second as reductionist, and each relying on\ndifferent kinds of knowledge, with suspicion of those seen as relativist at one extreme or\npositivist at the other. In professional work, the most important attributes for HCI experts\nare to be both creative and practical, placing design at the centre of the field.\n2 Notes on recommended reading\nThe recommended reading for this course is as follows:\nInteraction Design: Beyond human-computer interaction by Helen Sharp, Yvonne Rogers\n& Jenny Preece (2nd Edition 2007) describes both theoretical approaches and practical\nprofessional design methods, at forefront of current practice.\nHCI Models, Theories and Frameworks: Toward a multidisciplinary science edited by John\nCarroll (2003) provides an in-depth introduction to the most influential theoretical\napproaches across the HCI field. Unfortunately the publisher has let this book go out of\nprint, but there are still many copies around Cambridge.\nResearch methods for human-computer interaction. Is a new text edited by Paul Cairns and\nAnna Cox (Cambridge University Press 2008) that explains the nature of HCI research, and\nthe range of methods used, within the context of academic HCI from a UK perspective.\nThese notes refer to specific chapters in those books for more detail on specific topics.\n3 Lecture 2: Visual representation.\nHow can you design computer displays that are as meaningful as possible to human\nviewers? Answering this question requires understanding of visual representation \u2013 the\nprinciples by which markings on a surface are made and interpreted.\nNote: many illustrations referred to in this section are easily available online, though with a\nvariety of copyright restrictions. I will show as many as possible in the lecture itself \u2013 if\nyou want to investigate further, Google should find most of those mentioned.\nTypography and text\nFor many years, computer displays resembled paper documents. This does not mean that\nthey were simplistic or unreasonably constrained. On the contrary, most aspects of modern\nindustrial society have been successfully achieved using the representational conventions of\npaper, so those conventions seem to be powerful ones. Information on paper can be\nstructured using tabulated columns, alignment, indentation and emphasis, borders and\nshading. All of those were incorporated into computer text displays. Interaction\nconventions, however, were restricted to operations of the typewriter rather than the pencil.\nEach character typed would appear at a specific location. Locations could be constrained,\nlike filling boxes on a paper form. And shortcut command keys could be defined using\nonscreen labels or paper overlays. It is not text itself, but keyboard interaction with text that\nis limited and frustrating compared to what we can do with paper (Sellen & Harper 2002).\nBut despite the constraints on keyboard interaction, most information on computer screens\nis still represented as text. Conventions of typography and graphic design help us to\ninterpret that text as if it were on a page, and human readers benefit from many centuries of\nrefinement in text document design. Text itself, including many writing systems as well as\nspecialised notations such as algebra, is a visual representation that has its own research\nand educational literature. Documents that contain a mix of bordered or coloured regions\ncontaining pictures, text and diagrammatic elements can be interpreted according to the\nconventions of magazine design, poster advertising, form design, textbooks and\nencyclopaedias. Designers of screen representations should take care to properly apply the\nspecialist knowledge of those graphic and typographic professions. Position on the page,\nuse of typographic grids, and genre-specific illustrative conventions should all be taken into\naccount.\nSummary: most screen-based information is interpreted according to textual and\ntypographic conventions, in which graphical elements are arranged within a visual grid,\noccasionally divided or contained with ruled and coloured borders.\n4 Maps and graphs\nThe computer has, however, also acquired a specialised visual vocabulary and conventions.\nBefore the text-based \u2018glass teletype\u2019 became ubiquitous, cathode ray tube displays were\nalready used to display oscilloscope waves and radar echoes. Both could be easily\ninterpreted because of their correspondence to existing paper conventions. An oscilloscope\nuses a horizontal time axis to trace variation of a quantity over time, as pioneered by\nWilliam Playfair in his 1786 charts of the British economy. A radar screen shows direction\nand distance of objects from a central reference point, just as the Hereford Mappa Mundi of\n1300 organised places according to their approximate direction and distance from\nJerusalem. Many visual displays on computers continue to use these ancient but powerful\ninventions \u2013 the map and the graph. In particular, the first truly large software project, the\nSAGE air defense system, set out to present data in the form of an augmented radar screen\n\u2013 an abstract map, on which symbols and text could be overlaid. The first graphics\ncomputer, the Lincoln Laboratory Whirlwind, was created to show maps, not text.\nSummary: basic diagrammatic conventions rely on quantitative correspondence between a\ndirection on the surface and a continuous quantity such as time or distance. These should\nfollow established conventions of maps and graphs.\nSchematic drawings\nIvan Sutherland\u2019s groundbreaking PhD research with Whirlwind\u2019s successor TX-2\nintroduced several more sophisticated alternatives (1963). The use of a light pen allowed\nusers to draw arbitrary lines, rather than relying on control keys to select predefined\noptions. An obvious application, in the engineering context of MIT, was to make\nengineering drawings such as a girder bridge (see figure). Lines on the screen are scaled\nversions of the actual girders, and text information can be overlaid to give details of force\ncalculations. Plans of this kind, as a visual representation, are closely related to maps.\nHowever, where the plane of a map corresponds to a continuous surface, engineering\ndrawings need not be continuous. Each set of connected components must share the same\nscale, but white space indicates an interpretive break, so that independent representations\ncan potentially share the same divided surface \u2013 a convention introduced in Diderot\u2019s\nencyclopedia of 1772, which showed pictures of multiple objects on a page, but cut them\nloose from any shared pictorial context.\nSummary: engineering drawing conventions allow schematic views of connected\ncomponents to be shown in relative scale, and with text annotations labelling the parts.\nWhite space in the representation plane can be used to help the reader distinguish elements\nfrom each other rather than directly representing physical space.\n5 Pictures\nSutherland also suggested the potential value that computer screens might offer as artistic\ntools. His Sketchpad system was used to create a simple animated cartoon of a winking\ngirl. This is the first computer visual representation that might suffer from the \u2018resemblance\nfallacy\u2019, i.e. that drawings are able to depict real object or scenes because the visual\nperception of the flat image simulates the visual perception of the real scene. Sutherland\u2019s\ncartoon could only be called an approximate simulation, but many flat images\n(photographs, photorealistic ray-traced renderings, \u2018old master\u2019 oil paintings) have been\ndescribed as though perceiving the representation is equivalent to perceiving a real object.\nIn reality, new perspective rendering conventions are invented and esteemed for their\naccuracy by critical consensus, and only more slowly adopted by untrained readers. The\nconsensus on preferred perspective shifts across cultures and historical periods, as is\nobvious from comparison of prehistoric, classical, medieval and renaissance artworks. It\nwould be na\u00efve to assume that the conventions of today are the final and perfect product of\ntechnical evolution. As with text, we become so accustomed to interpreting these\nrepresentations that we are blind to the artifice. When even psychological object-\nrecognition experiments employ line drawings as though they were objects, it can be hard\nto insist on the true nature of the representation. But professional artists are fully aware of\nthe conventions they use \u2013 the way that a photograph is framed changes its meaning, and a\nskilled pencil drawing is completely unlike visual edge-detection thresholds. A good\npictorial representation need not simulate visual experience any more than a good painting\nof a unicorn need resemble an actual unicorn.\nSummary: pictorial representations, including line drawings, paintings, perspective\nrenderings and photographs rely on shared interpretive conventions for their meaning. It is\nna\u00efve to treat screen representations as though they were simulations of experience in the\nphysical world.\nNode-and-link diagrams\nThe first impulse of a computer scientist, when given a pencil, seems to be to draw boxes\nand connect them with lines. These node and link diagrams can be analysed in terms of the\ngraph structures that are fundamental to the study of algorithms (but unrelated to the visual\nrepresentations known as graphs or charts). A predecessor of these connectivity diagrams\ncan be found in electrical circuit schematics, where the exact location of components, and\nthe lengths of the wires, can be arranged anywhere, because they are irrelevant to the circuit\nfunction. Another early program created for the TX-2, this time by Ivan Sutherland\u2019s\nbrother Bert, allowed users to create circuit diagrams of this kind. The distinctive feature of\na node-and-link connectivity diagram is that, since the position of each node is irrelevant to\nthe operation of the circuit, it can be used to carry other information. Marian Petre\u2019s\nresearch into the work of electronics engineers (1995) catalogued the ways in which they\n6 positioned components in ways that were meaningful to human readers, but not to the\ncomputer \u2013 like the blank space between Diderot\u2019s objects a form of \u2018secondary notation\u2019 \u2013\nuse of the plane to assist the reader in ways not related to the technical content.\nCircuit connectivity diagrams have been most widely popularised through the London\nUnderground diagram, an invention of electrical engineer Henry Beck. The diagram has\nbeen clarified by exploiting the fact that most underground travellers are only interested in\norder and connectivity, not location, of the stations on the line. However, popular resistance\nto reading \u2018diagrams\u2019 means that this one is more often described as the London\nUndergound \u2018map\u2019, despite Beck\u2019s complaints.\nSummary: node and link diagrams are still widely perceived as being too technical for\nbroad acceptance. Nevertheless, they can present information about ordering and\nrelationships clearly, especially if consideration is given to the value of allowing human\nusers to specify positions.\nIcons and symbols\nMaps frequently use symbols to indicate specific kinds of landmark. Sometimes these are\nrecognisably pictorial (the standard symbols for tree and church), but others are fairly\narbitrary conventions (the symbol for a railway station). As the resolution of computer\ndisplays increased in the 1970s, a greater variety of symbols could be differentiated, by\nmaking them more detailed, as in the MIT SDMS system that mapped a naval battle\nscenario with symbols for different kinds of ship. However, the dividing line between\npictures and symbols is ambiguous. Children\u2019s drawings of houses often use conventional\nsymbols (door, four windows, triangle roof and chimney) whether or not their own house\nhas two storeys, or a fireplace. Letters of the Latin alphabet are shapes with completely\narbitrary relationship to their phonetic meaning, but the Korean phonetic alphabet is easier\nto learn because the forms mimic the shape of the mouth when pronouncing those sounds.\nThe field of semiotics offers sophisticated ways of analysing the basis on which marks\ncorrespond to meanings. In most cases, the best approach for an interaction designer is\nsimply to adopt familiar conventions. When these do not exist, the design task is more\nchallenging.\nIt is unclear which of the designers working on the Xerox Star coined the term \u2018icon\u2019 for\nthe small pictures symbolising different kinds of system object. David Canfield Smith\nwinningly described them as being like religious icons, which he said were pictures\nstanding for (abstract) spiritual concepts. But \u2018icon\u2019 is also used as a technical term in\nsemiotics. Unfortunately, few of the Xerox team had a sophisticated understanding of\nsemiotics. It was fine art PhD Susan Kare\u2019s design work on the Apple Macintosh that\nestablished a visual vocabulary which has informed the genre ever since. Some general\nadvice principles are offered by authors such as Horton (1994), but the successful design of\nicons is still sporadic. Many software publishers simply opt for a memorable brand logo,\n7 while others seriously misjudge the kinds of correspondence that are appropriate (my\nfavourite blooper was a software engineering tool in which a pile of coins was used to\naccess the \u2018change\u2019 command).\nIt has been suggested that icons, being pictorial, are easier to understand than text, and that\npre-literate children, or speakers of different languages, might thereby be able to use\ncomputers without being able to read. In practice, most icons simply add decoration to text\nlabels, and those that are intended to be self-explanatory must be supported with textual\ntooltips. The early Macintosh icons, despite their elegance, were surprisingly open to\nmisinterpretation. One PhD graduate of my acquaintance believed that the Macintosh folder\nsymbol was a briefcase (the folder tag looked like a handle), which allowed her to carry her\nfiles from place to place when placed inside it. Although mistaken, this belief never caused\nher any trouble \u2013 any correspondence can work, so long as it is applied consistently.\nSummary: the design of simple and memorable visual symbols is a sophisticated graphic\ndesign skill. Following established conventions is the easiest option, but new symbols must\nbe designed with an awareness of what sort of correspondence is intended - pictorial,\nsymbolic, metonymic (e.g. a key to represent locking), bizarrely mnemonic, but probably\nnot monolingual puns.\nVisual metaphor\nThe ambitious graphic designs of the Xerox Star\/Alto and Apple Lisa\/Macintosh were the\nfirst mass-market visual interfaces. They were marketed to office professionals, making the\n\u2018cover story\u2019 that they resembled an office desktop a convenient explanatory device. Of\ncourse, as was frequently noted at the time, these interfaces behaved nothing like a real\ndesktop. The mnemonic symbol for file deletion (a wastebasket) was ridiculous if\ninterpreted as an object placed on a desk. And nobody could explain why the desk had\nwindows in it (the name was derived from the \u2018clipping window\u2019 of the graphics\narchitecture used to implement them \u2013 it was at some later point that they began to be\nexplained as resembling sheets of paper on a desk). There were immediate complaints from\nluminaries such as Alan Kay and Ted Nelson that strict analogical correspondence to\nphysical objects would become obstructive rather than instructive. Nevertheless, for many\nyears the marketing story behind the desktop metaphor was taken seriously, despite the fact\nthat all attempts to improve the Macintosh design with more elaborate visual analogies, as\nin General Magic and Microsoft Bob, subsequently failed.\nThe \u2018desktop\u2019 can be far more profitably analysed (and extended) by understanding the\nrepresentational conventions that it uses. The size and position of icons and windows on the\ndesktop has no meaning, they are not connected, and there is no visual perspective, so it is\nneither a map, graph nor picture. The real value is the extent to which it allows secondary\nnotation, with the user creating her own meaning by arranging items as she wishes.\nWindow borders separate areas of the screen into different pictorial, text or symbolic\n8 contexts as in the typographic page design of a textbook or magazine. Icons use a large\nvariety of conventions to indicate symbolic correspondence to software operations and\/or\ncompany brands, but they are only occasionally or incidentally organised into more\ncomplex semiotic structures.\nSummary: theories of visual representation, rather than theories of visual metaphor, are the\nbest approach to explaining the conventional Macintosh\/Windows \u2018desktop\u2019. There is huge\nroom for improvement.\nUnified theories of visual representation\nThe analysis in this lecture has addressed the most important principles of visual\nrepresentation for screen design, introduced with examples from the early history of\ngraphical user interfaces. In most cases, these principles have been developed and\nelaborated within whole fields of study and professional skill \u2013 typography, cartography,\nengineering and architectural draughting, art criticism and semiotics. Improving on the\ncurrent conventions requires serious skill and understanding. Nevertheless, interaction\ndesigners should be able, when necessary, to invent new visual representations.\nOne approach is to take a holistic perspective on visual language, information design,\nnotations, or diagrams. Specialist research communities in these fields address many\nrelevant factors from low-level visual perception to critique of visual culture. Across all of\nthem, it can be necessary to ignore (or not be distracted by) technical and marketing claims,\nand to remember that all visual representations simply comprise marks on a surface that are\nintended to correspond to things understood by the reader. The two dimensions of the\nsurface can be made to correspond to physical space (in a map), to dimensions of an object,\nto a pictorial perspective, or to continuous abstract scales (time or quantity). The surface\ncan also be partitioned into regions that should be interpreted differently. Within any\nregion, elements can be aligned, grouped, connected or contained in order to express their\nrelationships. In each case, the correspondence between that arrangement, and the intended\ninterpretation, must be understood by convention or explained. Finally, any individual\nelement might be assigned meaning according to many different semiotic principles of\ncorrespondence.\nThe following table summarises holistic views, as introduced above, drawing principally on\nthe work of Bertin, Richards, MacEachren, Blackwell & Engelhardt and Engelhardt.\n9 Graphic Resources Correspondence Design Uses\nMarks Shape Literal (visual imitation of physical Mark position, identify\nOrientation features) category (shape, texture\nSize Mapping (quantity, relative scale) colour)\nTexture Conventional (arbitrary) Indicate direction\nSaturation (orientation, line)\nColour Express magnitude\nLine (saturation, size, length)\nSimple symbols and\ncolour codes\nSymbols Geometric elements Topological (linking) Texts and symbolic calculi\nLetter forms Depictive (pictorial conventions) Diagram elements\nLogos and icons Figurative (metonym, visual puns) Branding\nPicture elements Connotative (professional and Visual rhetoric\nConnective elements cultural association) Definition of regions\nAcquired (specialist literacies)\nRegions Alignment grids Containment Identifying shared\nBorders and frames Separation membership\nArea fills Framing (composition, Segregating or nesting\nWhite space photography) multiple surface\nGestalt integration Layering conventions in panels\nAccommodating labels,\ncaptions or legends\nSurfaces The plane Literal (map) Typographic layouts\nMaterial object on Euclidean (scale and angle) Graphs and charts\nwhich marks are Metrical (quantitative axes) Relational diagrams\nimposed (paper, stone) Juxtaposed or ordered (regions, Visual interfaces\nMounting, orientation catalogues) Secondary notations\nand display context Image-schematic Signs and displays\nDisplay medium Embodied\/situated\nAs an example of how one might analyse (or working backwards, design) a complex visual\nrepresentation, consider the case of musical scores. These consist of marks on a paper\nsurface, bound into a multi-page book, that is placed on a stand at arms length in front of a\nperformer. Each page is vertically divided into a number of regions, visually separated by\nwhite space and grid alignment cues. The regions are ordered, with that at the top of the\npage coming first. Each region contains two quantitative axes, with the horizontal axis\nrepresenting time duration, and the vertical axis pitch. The vertical axis is segmented by\nlines to categorise pitch class. Symbols placed at a given x-y location indicate a specific\npitched sound to be initiated at a specific time. A conventional symbol set indicates the\nduration of the sound. None of the elements use any variation in colour, saturation or\ntexture. A wide variety of text labels and annotation symbols are used to elaborate these\nbasic elements. Music can be, and is, also expressed using many other visual\nrepresentations (see e.g. Duignan 2010 for a survey of representations used in digital music\nprocessing).\nSources and Further reading\nThe historical examples of early computer representations used in this lecture are mainly\ndrawn from Sutherland (Ed. Blackwell & Rodden 2003), Garland (1994), and Blackwell\n(2006). Historical reviews of visual representation in other fields include Ferguson (1992),\n10 P\u00e9rez-G\u00f3mez & Pelletier (1997), McCloud (1993), Tufte (1983). Reviews of human\nperceptual principles can be found in Gregory (1970), Ittelson (1996), Ware (2004),\nBlackwell (2002). Advice on principles of interaction with visual representation is\ndistributed throughout the HCI literature, but classics include Norman (1988), Horton\n(1994), Shneiderman (Shneiderman & Plaisant 2010, Card et al 1999, Bederson &\nShneiderman 2003) and Spence (2001). Green\u2019s Cognitive Dimensions of Notations\nframework has for many years provided a systematic classification of the design parameters\nin interactive visual representations. A brief introduction is provided in Blackwell & Green\n(2003).\nReferences related to visual representation\nBederson, B.B. and Shneiderman, B. (2003). The Craft of Information Visualization: Readings and Reflections,\nMorgan Kaufmann\nBertin, J. (1967). Semiologie graphique. Paris: Editions Gauthier-Villars. English translation by WJ. Berg (1983)\nas Semiology of graphics, Madison, WI: University of Wisconsin Press\nBlackwell, A.F. and Engelhardt, Y. (2002). A meta-taxonomy for diagram research. In M. Anderson & B. Meyer\n& P. Olivier (Eds.), Diagrammatic Representation and Reasoning, London: Springer-Verlag, pp. 47-64.\nBlackwell, A.F. (2002). Psychological perspectives on diagrams and their users. In M. Anderson & B. Meyer &\nP. Olivier (Eds.), Diagrammatic Representation and Reasoning. London: Springer-Verlag, pp. 109-123.\nBlackwell, A.F. and Green, T.R.G. (2003). Notational systems - the Cognitive Dimensions of Notations\nframework. In J.M. Carroll (Ed.) HCI Models, Theories and Frameworks: Toward a multidisciplinary\nscience. San Francisco: Morgan Kaufmann, 103-134.\nBlackwell, A.F. (2006). The reification of metaphor as a design tool. ACM Transactions on Computer-Human\nInteraction (TOCHI), 13(4), 490-530.\nDuignan, M., Noble, J. & Biddle, R. (2010). Abstraction and activity in computer mediated music production,\nComputer Music Journal, vol. 34\nEngelhardt, Y. (2002). The Language of Graphics. A framework for the analysis of syntax and meaning in maps,\ncharts and diagrams. PhD Thesis, University of Amsterdam.\nFerguson, E.S. (1992). Engineering and the mind's eye. MIT Press.\nGarland, K. (1994). Mr. Beck's Underground Map. Capital Transport Publishing.\nGoodman, N. (1976). Languages of art. Indianapolis: Hackett.\nP\u00e9rez-G\u00f3mez, A. and Pelletier, L. (1997). Architectural Representation and the Perspective Hinge. MIT Press\nGregory, R. (1970). The Intelligent Eye. Weidenfeld and Nicolson.\nHorton, W.K. (1994). The icon book: Visual symbols for computer systems and documentation. Wiley\nIttelson, W.H. (1996). Visual perception of markings. Psychonomic Bulletin & Review, 3(2), 171-187.\nMacEachren, A.M. (1995). How maps work: Representation, visualization, and design. Guilford.\nMcCloud, S. (1993). Understanding comics: The invisible art. Northhampton, MA:: Kitchen Sink Press.\nNorman, D.A. (1988\/2002). The Design of Everyday Things (originally published under the title \u2018The\nPsychology of Everyday Things\u2019). Newprint.\nPetre, M. (1995) Why looking isn\u2019t always seeing: readership skills and graphical programming.\nCommunications of the ACM, 38 (6), 33 - 44.\n11 Richards, C.J. (1984). Diagrammatics: an investigation aimed at providing a theoretical framework for studying\ndiagrams and for establishing a taxonomy of their fundamental modes of graphic organization.\nUnpublished Phd Thesis. London: Royal College of Art.\nSellen, A. J. & Harper, R. H. R. (2002). The Myth of the Paperless Office. MIT Press.\nShneiderman, B. & Plaisant, C. (2010). Designing the User Interface: Strategies for Effective Human-Computer\nInteraction, 5th edition. Addison-Wesley.\nCard, S.K., Mackinlay, J.D. & Shneiderman, B. (1999). Readings in Information Visualization: Using Vision to\nThink. Morgan Kaufmann\nSpence, R. (2001). Information visualization. Addison-Wesley.\nSutherland, I.E. (1963\/2003). Sketchpad, A Man-Machine Graphical Communication System. PhD Thesis at\nMassachusetts Institute of Technology, online version and editors' introduction by A.F. Blackwell & K.\nRodden. Technical Report 574. Cambridge University Computer Laboratory\nTufte, E.R. (1983). The visual display of quantitative information. Cheshire, CT: Graphics Press\nWare, C. (2004). Information Visualization - Perception for Design. Morgan Kaufmann.\n12 Visual representation design exercise\n13 Lecture 3: Text and gesture interaction\nGuest lecturer: Per Ola Kristensson will present these ideas, using a case study based on his\nown recent research, leading to a successful product, recent buyout and extensive press\ncoverage.\nWhen technical people are commenting on, or even creating, user interfaces, they often get\ndistracted or hung up on the hardware used for input and output. This is a sign that they\nhaven\u2019t thought very hard about what is going on underneath, and also that they will never\nkeep up with new technical advances. There have always been good and bad examples of\ninterface designs using control panels, punch cards, teletypes, text terminals, bitmap\ndisplays, light pens, tablets, mice, touch screens, and so on. With every generation, you can\nhear people debating whether, for example, \u2018the mouse is better than a touch screen\u2019 or\n\u2018voice input is better than a keyboard\u2019. Debates like this demonstrate only that those\ninvolved haven\u2019t been able to see past the surface appearance (and the marketing spiel of\nthe device manufacturers). And opinions or expertise on these matters quickly gets out of\ndate. Within the past few weeks, I\u2019ve heard a leading researcher tell his sponsors that \u2018we\nhave added a GUI to our prototype\u2019, as if that was an important thing. 20 years ago, it was\nsomething of an achievement to get some output on a bitmap display rather than a\ncommand line text application. Nowadays, it is more challenging to work with projection\nsurfaces or augmented reality (more of that in a later lecture). But sensing and display\ntechnologies change fast, and it\u2019s more important to understand the principles of interaction\nthan the details of a specific interaction device.\nThe lecture on visual representation was based on display principles that are independent of\nany particular display hardware. If we consider the interaction principles that are\nindependent of any particular hardware, these are:\n\uf0b7 How does the user get content (both data and structure) into digital form?\n\uf0b7 How does the user navigate around the content?\n\uf0b7 How does the user manipulate the content (restructuring, revising, replacing)?\nThese are often inter-dependent. The Dasher system for text entry presents an interface in\nwhich the user \u2018navigates\u2019 through a space of possible texts as predicted by a probabilistic\nlanguage model, so it can be considered both as content creation and navigation. It is\nrelatively hard to structure and revise text using Dasher, because the language model only\nuses a 5-character context, and many text documents have structure on a larger-scale than\nthat. However, Dasher provides an excellent example of an interaction \u2018paradigm\u2019 that is\nindependent of any particular hardware \u2013 it can be controlled using mouse, keys, voice,\nbreath, eyetracking, and many other devices.\n14 General principles: direct manipulation, and interface modes\nAt the point where the GUI was about to become popular, HCI researcher Ben\nShneiderman summarized the important opportunities it provided, under the name Direct\nManipulation. In fact some of these things were already possible with text interfaces (for\nexample after the advent of full-screen text editors), and they remain relevant in more\nrecent generations of hardware. It is also possible to use GUI libraries to create bad user\ninterfaces that do not support these principles \u2013 just being graphical doesn\u2019t make it good!\nThe principles of Direct Manipulation as described by Shneiderman are:\n\uf0b7 An object that is of interest to the user should be continuously visible in the form of\na graphical representation on the screen\n\uf0b7 Operations on objects should involve physical actions (using a pointing device to\nmanipulate the graphical representation) instead of commands with complex syntax\n\uf0b7 The actions that the user makes should be rapid, should offer incremental changes\nover the previous situation, and should be reversible\n\uf0b7 The effect of actions should immediately be visible, so that the user knows what has\nhappened\n\uf0b7 There should be a modest set of commands doing everything that a novice might\nneed, but it should be possible to expand these, gaining access to more functions as\nthe user develops expertise.\nWe should also note an additional principle, defined around the same time by Larry Tesler\nat Apple, that the same action should always have the same effect. It\u2019s hard to believe that\nthis wouldn\u2019t be done, but he was campaigning against editors like vi, which many people\nfound unusable because hitting a key on the keyboard could have different consequences at\ndifferent times. Tesler campaigned against \u2018modes\u2019 in the user interface, based on his\nstudies of non-technical users (search for \u2018nomodes\u2019 to learn more). The largest\nachievement of the \u2018windows\u2019 style interface is that the frames around each application\ngive the user a clue about different modes \u2013 but as Tesler said, removing modes altogether\nis a great ambition.\nContent creation\nText content: guest lecturer will discuss this, including how we can assess and measure the\nefficiency of alternative text entry mechanisms.\nNon-text content: This course won\u2019t say very much about non-text content creation.\n\u2018Content\u2019 can refer to music, visual arts, film, games, novels and many other genres. To\nunderstand any of them well, you would have to take a degree in the relevant discipline\n(some available in Cambridge). All of those fields develop their own professional tools, and\nthere is a constant stream of \u2018amateur\u2019 tools modeled on the professional ones. Cultural\ntastes don\u2019t change that fast (the rate of change is generational, not annual), so digital\ncontent creation tools are usually derived from and imitate the artistic tools of previous\n15 generations (cameras, microphones, mixing desks, typewriters etc). Innovative content\ncreation tools appear first in the \u2018avant garde\u2019 contemporary arts, and take a generation to\nreach popular audiences, get taken up by mainstream professional artists, and become\nsubject to consumer demand for amateur tools \u2013 for example, sampling and mashups were\nfirst explored in the mid-20th century by \u2018musique concrete\u2019 composers using tape\nrecorders. The Computer Lab Rainbow Group has always had an active programme of\nresearch engagement with contemporary artists, developing new digital media tools. That\nresearch continues actively at present, but is outside the scope of an introduction to HCI.\nContent manipulation and navigation via deixis\nIn order to manipulate content, the user has to be able to refer to specific parts of the\nproduct (whether text, diagram, video, audio etc) that he or she is working on. In early text\ninterfaces, references were made by numbering the lines of a text file (e.g. substitute \u2018fred\u2019\nfor \u2018frrd\u2019 on line 27 \u2013 \u201827;s\/frrd\/fred\/\u2019). As in programming languages, line number\ncould be replaced by labels, but it is irritating to give everything names. Imagine a shop\nwhere everything for sale was given its own unique name, or had to be referred to by index\nposition of aisle, shelf, and item. It\u2019s much easier just to point and say \u2018I want that one\u2019. In\nlanguage, this is called deixis \u2013 sentences in which the object is identified by pointing at it,\nrather than by naming it. For the same reason, deixis has become universal in computer\nlanguages, and this is why devices for pointing are so important in user interfaces.\nIn early GUIs, the combination of mouse and pointer to achieve deixis was a significant\ninvention (hence the WIMP interface \u2013 Windows, Icons, Mouse, Pointer). Other inventions\naround the same time were the placement of a text cursor between characters, rather than\nidentifying a single character (Larry Tesler had a hand in this invention too). But new\nhardware suggests new approaches to deixis \u2013 touch screens, augmented reality etc will all\nrequire new inventions. It\u2019s reasonable to assume that deixis in different media can be\nachieved in different ways too \u2013 audio interfaces, cameras, and other devices don\u2019t\nnecessarily need to have a cursor. In many cases, what is required is a deictic method that\nrelates user \u2018gestures\u2019 (detected via any kind of sensing device) with a media \u2018location\u2019.\nNavigation is then a matter of supporting user strategies to vary that location, including\ntechniques to show local detail within a larger context (via scroll bars, zooming,\nthumbnails, fisheye views, overview maps, structure navigation and so on)\nSimple content manipulations include simply adding more content (perhaps inserted within\na particular context), or removing content that isn\u2019t required. Anything more complex\ninvolves modifying the structure of the content. This is an area in which user interface\ndesign can build on insights from the usability of programming languages (in a later\nlecture).\n16 Evaluation of pointing devices and WIMP interfaces\nAs with text entry, modern user interfaces involve so much pointing, that it is worth\noptimizing the efficiency of the interaction. Early HCI models based their optimization on\nFitts' law \u2013 an experimental observation that the time it takes to point at a given location is\nrelated to the size of the target and also the distance from the current hand position to the\ntarget.\nFitts original experiment involved two targets of variable size, and separated by a variable\ndistance. Experimental subjects were required to touch first one target, then the other, as\nquickly as they could. The time that it takes to do this increased with the Amplitude of the\nmovement (i.e. the distance between the targets) and decreased with the Width of the target\nthat they were pointing to:\nT = K log (A \/ W + 1) where A = amplitude, W = width\n2\nWhen evaluating new pointing devices, it can be useful experimentally to measure\nperformance over a range of target sizes and motion distances, in order to establish the\nvalue of the constant in this equation (sometimes called ID: the Index of Difficulty).\nIn user interfaces that require a user to make many sequences of repetitive actions (for\nexample, people working in telephone call centres or in data entry), it can be useful to\ncompare alternative designs by counting the individual actions needed to carry out a\nparticular task, including the number and extent of mouse motions, as well as all the keys\npressed on the keyboard. This Keystroke Level Model can be used to provide a quantitative\nestimate of user performance, and to optimize the design and layout of the interaction\nsequence. It is more difficult to make numerical comparisons of user interfaces in cases\nwhere the user actions are less predictable \u2013 the GOMS model (Goals Operators Methods\nSelection) combines keystroke-level estimates of user actions with an AI planning model\nderived from the 1969 work of Ernst and Newell on a Generalised Problem Solver. The\nGPS operated in a search space characterised by possible intermediate states between some\ninitial state and a goal state. Problem solving consisted of finding a series of operations that\nwould eventually reach the goal state. This involved recursive application of two heuristics:\nA) select an intermediate goal that will reduce the difference between the current state and\nthe desired state, and B) if there is no operation to achieve that goal directly, decompose it\ninto sub-goals until the leaves of the sub-goal hierarchy can be achieved by individual\nkeystrokes or mouse movements.\nFor further reading on KLM and GOMS, see chapter 4 in Carroll, by Bonnie John.\nOnce we can measure interaction efficiency, whether text entry or time to point at a target,\nit is possible to compare alternative designs through controlled experiments with human\nparticipants. These are described in a later lecture.\n17 Lecture 4: Inference\nMental models \u2013 what the user infers about the system\nDon Norman, one of the first generation of cognitive scientists investigating HCI, also\nwrote the first popular book on the topic \u2013 The Design of Everyday Things1. What most\npeople remember about this book is the example of door handles that are so badly designed\nthey need labels telling you to pull them. But his key message was to draw attention to the\ngulf of evaluation and the gulf of execution\u2013 how does the user know what the system is\ndoing, and how do they know what they need to do, in order to achieve their goals?\nFor a review of Norman\u2019s model, see section 3.3.2 in Sharp, Rogers & Preece.\nComputer systems are so complex, that nobody really knows what is happening inside\n(except, possibly, the designer). In the face of incomplete information, the gulf of\nevaluation is unavoidable. The user has to make inferences (or guess) what is happening\ninside. The user\u2019s conclusions form a mental model of the system. One way of thinking\nabout the design problem is that the designer must give sufficient clues to the user to\nsupport that inference process, and help the user form an accurate (or at least adequate)\nmental model. The idea of a visual metaphor is that the screen display simulates some\nmore familiar real world object, and that the user\u2019s mental model will then be understood\nby analogy to the real world.\nThe metaphor\/analogy approach can potentially help with the gulf of execution too. If the\nsystem behaved exactly like the real world objects depicted, then users would know exactly\nwhat to do with them. In practice, computer systems never behave exactly like real world\nobjects, and the differences can make the system even more confusing. (Why do you have\nwindows in your desktop? Why do I have to put my USB drive in the rubbish before\nunplugging it?) Furthermore, designers inadvertently create metaphors that correspond very\nwell to their own understanding of the internal behaviour of the system, but users should\nnot be expected to know as much as designers. User studies can help to identify what users\nactually know, what they need to know, and how they interpret prototype displays.\nMental models research\nMental models research attempts to describe the structure of the mental representations that\npeople use for everyday reasoning and problem solving. Common mental models of\neveryday situations are often quite different from scientific descriptions of the same\nphenomena. They may be adequate for basic problem solving, but break down in unusual\nsituations. For example, many people imagine electricity as being like a fluid flowing\n1 originally called the Psychology of Everyday Things \u2013 he wrote much of it while on sabbatical\nleave at the Applied Psychology Unit in Cambridge, and among other examples, described the\nidiosyncratic voicemail system at the APU\n18 through the circuit. When electrical wiring was first installed in houses, it appeared very\nsimilar to gas or water reticulation, including valves to turn the flow on and off, and hoses\nto direct the flow into an appliance. Many people extended this analogy and believed that\nthe electricity would leak out of the light sockets if they were left without a lightbulb. This\nmental model did not cause any serious problems - people simply made sure that there were\nlightbulbs in the sockets, and they had no trouble operating electrical devices on the basis\nof their model.\nThe psychological nature of unofficial but useful mental models was described in the\n1970s, and these ideas have been widely applied to computer systems. Young's study of\ncalculator users in 1981 found that users generally had some cover story which explained to\ntheir satisfaction what happened inside the device. Payne carried out a more recent study of\nATM users, demonstrating that even though they have never been given explicit instruction\nabout the operation of the ATM network, they do have a definite mental model of data flow\nthrough the network, as well as clear beliefs about information such as the location of their\naccount details.\nThe basic claim of mental models theory is that if you know the users' beliefs about the\nsystem they are using, you can predict their behaviour. The users' mental models allow\nthem to make inferences about the results of their actions by a process of mental\nsimulation. The user imagines the effect of his or her actions before committing to a\nphysical action on the device. This mental simulation process is used to predict the effect of\nan action in accordance with a mental model, and it supports planning of future actions\nthrough inference on the mental model. Where the model is incomplete, and the user\nencounters a situation that cannot be explained by the mental model, this inference will\nusually rely on analogy to other devices that the user already knows.\nThink aloud studies\nA great deal of cognitive psychology research, including some basic research on mental\nmodels, has been based on think-aloud studies, in which subjects are asked to carry out\nsome task while talking as continuously as possible. The data are collected in the form of a\nverbal protocol, normally transcribed from a tape recording so that subtle points are not\nmissed. Use of this technique requires some care. It can be difficult to get subjects to think\naloud, and some methods of doing so can bias the experimental data. A detailed discussion\nof this kind of study is provided by Ericsson & Simon (1985).\nFor a description of think-aloud techniques, see section 7.6.2 in Sharp, Rogers & Preece.\nPerformance models of users\nEarly HCI research was largely concerned with the performance of the user, measured in\nengineering terms as a system component (\u2018cognitive psychology\u2019 is closely associated\nwith \u2018artificial intelligence\u2019, investigating human performance by simulating it with\n19 machines). One of the most famous findings in cognitive psychology research, and the one\nmost often known to user interface developers, is an observation by George Miller in 1956.\nMiller generalised from a number of studies finding that people can recall somewhere\nbetween 5 and 9 things at one time - usually referred to as \u201cseven plus or minus two\u201d.\nSurprisingly, this number always seems to be about the same, regardless of what the\n\u201cthings\u201d are. It applies to individual digits and letters, meaning that it would be very\ndifficult to remember 25 letters. However if the letters are arranged into five 5-letter words\n(apple, grape \u2026), we have no trouble remembering them. We can even remember 5 simple\nsentences reasonably easily. Miller called these units of short-term memory chunks. It is\nrather more difficult to define a chunk than to make the observation - but it clearly has\nsomething to do with how we can interpret the information. This is often relevant in user\ninterfaces - a user may be able to remember a sequence of seven meaningful operations, but\nwill be unable to remember them if they seem to be arbitrary combinations of smaller\nelements.\nShort term memory is also very different from long term memory - everything we know.\nLearning is the process of encoding information from short term memory into long term\nmemory, where it appears to be stored by association with the other things we already\nknow. Current models of long-term memory are largely based on connectionist theories -\nwe recall things as a result of activation from related nodes in a network. According to this\nmodel, we can improve learning and retrieval by providing rich associations - many related\nconnections. This is exploited in user interfaces that mimic either real world situations or\nother familiar applications.\nA further subtlety of human memory is that the information stored is not always verbal.\nShort term memory experiments involving recall of lists failed to investigate the way that\nwe remember visual scenes. Visual working memory is in fact independent of verbal short\nterm memory, and this can be exploited in mnemonic techniques which associate images\nwith items to be remembered \u2013 display icons combined with associated labels provide this\nkind of dual coding.\nIntelligent interfaces \u2013 what the system infers about the user\nA further inference problem is that, in addition to the user not knowing what is happening\ninside the system, the system doesn\u2019t \u2018know\u2019 what is happening inside the user. Advanced\nsystems can be designed to record and observe user interactions, and on the basis of that\ndata, make inferences about what the user intends to do next, and present short-cuts,\nusability cues or other aids. These kinds of \u2018intelligent user interface\u2019 are becoming more\ncommon, but they can also introduce severe usability problems. A notorious early example\nwas the Microsoft Word \u2018Clippy\u2019, which analysed features of the document, and offered to\nhelp with automatic formatting (\u201cYou appear to be writing a letter \u2026\u201d). Although some\nusers found it useful, a far larger number found the tone patronizing and the automated\n20 actions inaccurate. Google \u2018Death to Clippy\u2019 to see the extent to which smart user interface\ntechnology can get it wrong.\nMany intelligent user interfaces emerge from the machine learning community, and\nespecially Bayesian inference techniques. Bayesian techniques are more appropriate to\nuser interfaces than other techniques for a range of reasons:\n\uf0b7 They don\u2019t rely on large training sets (as is the case with neural net approaches), so they\ncan adapt more quickly to individual users\n\uf0b7 Bayesian consideration of prior probabilities corresponds better to commonsense\nhuman reasoning under uncertainty.\n\uf0b7 Bayes formula provides a consistent way to combine data from user interactions\nwith historical data and heuristic rules.\nThe lecture will provide further practical examples (others will have been included in the\nlectures on advanced interaction techniques, where Bayesian inference is often used for\ngesture interaction, or vision-based augmented reality systems).\nAn inference framework provides a valuable analytic perspective on many current trends in\nuser interaction. For example, the behaviour of Google, or of recommender systems such as\nAmazon or Facebook friend finder, use inference techniques to apply statistical data and\nguess what the user really wants. It remains the case that when the system makes inaccurate\ninferences, the results will be annoying, confusing, or even damaging. This means that\nsome advanced research areas, such as Programming by Example (where automated\nscripts or macros are created by inference, after observing repeated actions) provide a\nmajor challenge for HCI. These are active areas of research in Cambridge at present, and a\nfew advanced prototypes are available for experimental use, such as the Koala project at\nIBM's Almaden Research Center (Allen Cypher, one of the Koala team, has worked in this\narea for many years \u2013 his \u2018Eager\u2019 prototype at Apple Research was an early success).\n21 Lecture 5: Advanced user interface techniques\nThis lecture summarises recent and current HCI research into advanced interaction\ntechnologies, using a variety of projects (especially current research in Cambridge) to\nreview the principles introduced elsewhere in the course.\nVirtual reality (VR)\nThe term virtual reality originally applied only to full immersion VR, in which simulated\nworld is projected onto all walls of a room (CAVE \u2013 a recursive acronym for Cave\nAutomatic Virtual Environment), or via a head-mounted display (HMD) which uses\nmotion-tracking to change the view as you turn your head. Interaction was always a\nchallenge \u2013 data gloves could supposedly be used to pick up and interact with objects in the\nvirtual scene. However, actual systems tended to use the glove only for gesture recognition,\nwith all the problems of training, inference and accuracy that this implies. \u2018Natural\u2019\nnavigation in the real world is achieved by walking, but CAVEs were never large enough to\nwalk far, and HMDs with motion tracking were normally tethered by cables. In practice,\nthe illusion was always fairly limited, unlike the Matrix-style science fiction ideal that\nmotivated it. Marketing creep has meant that any interactive 3D environment (including\nFPS games, Second Life etc) might get called VR, even if presented on a standard monitor,\nand controlled by a mouse. As games players know very well, control of view and camera\nangle, unless constrained by a script, can make arbitrary action in 3D scenes complex.\nAugmented reality\nAugmented reality (AR) systems overlay digital information onto the real world, either\nusing partially-transparent head mounted displays, or by taking a video feed of an actual\nscene, and compositing it with computer generated elements. (This is now becoming\navailable on a few mobile phone applications, using the phone camera to provide the video\nfeed). A key technical problem is registration \u2013 relatively recently, this had to be done by\nintegrating GPS, compass orientation, accelerometer for gravity orientation, and often\ngyroscopes, into the HMD. Now that all these peripherals are available on high end mobile\nphones, Mobile AR is liable to become a major marketing buzzword, possibly with the\nsame loss of actual functionality that occurred when VR shifted from research ambition to\nmarketing buzzword.\nTangible user interfaces\nTangible user interfaces (TUIs) use physical objects to control the computer, most often a\ncollection of objects arranged on a tabletop to act as \u2018physical icons\u2019. An immediate\nproblem is that physical objects don\u2019t change their visible state very easily. You can\ninclude motors and displays in each object (expensive), or project overlaid AR information\n22 onto them, or just use them as multiple specialized mice\/pucks that control elements of the\ndisplay on a separate screen. In this case, it is necessary to track their positions, perhaps by\nusing a large tablet device. If they are just being used as tokens to select a particular\nfunction or piece of data, an embedded RFID chip can be used to sense when they are\nplaced within a certain distance of a reader.\nMachine vision\nMachine vision is a key technology for both AR and TUIs, as a way of recognizing real\nworld objects such as buildings (in the case of outdoor AR) or objects on a desk (used for\nTUIs). Many current AR prototypes recognize distinctive objects from a large number of\nlow-level visual features, as in the SIFT algorithm. Key problems are to maintain a\nsufficiently large database of object features, track them fast enough to give user feedback\nthat responds to camera, gesture or object motion in realtime, and do both of these in\nvarying lighting conditions. An alternative is fiducial markers \u2013 simple visual markers\nsuch as barcodes, that can be used to more reliably identify and track objects from camera\ninput. They are more robust to changes in camera angle and lighting than object recognition\nalgorithms.\nPaper interfaces\nInspired by the research conducted by Abigail Sellen and Richard Harper (originally at\nXerox EuroPARC in Cambridge, now at Microsoft Research Cambridge), whose book \u2018The\nMyth of the Paperless Office\u2019 analyses the ways in which the properties of paper are\npreferable to computers for many kinds of activity. The book remains a useful resource for\ndesigners of mobile devices substituting for paper (phones and tablets), but has also\ninspired research in which paper is integrated with digital systems, for example with\nfiducial markers on the page that can be traced by cameras (the Anoto pen can perhaps be\nconsidered an extreme example of gesture recognition implemented with fiducial markers).\nMixed reality\nMixed reality combines physical objects with information displays, for example by\nprojecting digital data onto objects on a table, or onto paper. Fiducial markers can be used\nto determine the identity and location of individual sheets of paper, and project additional\ninformation onto them. The ISMAR conference series presents new results in Mixed and\nAugmented Reality, often based on machine vision techniques.\nEye tracking and gaze control\nOriginally developed for psychological research into visual attention processes, eye-\ntrackers are now used fairly routinely in HCI research to study what position on the screen\n23 users are looking at. A high resolution close-up camera is used to capture video of one of\nthe user\u2019s eyes, and the precise position where they are looking is deduced from the\nposition of the pupil, often combined with reflections from a pair of small infrared (LED)\nspotlights. One company sells a device with the camera and spotlights integrated into the\nsurround of a monitor, to be unobtrusive. However, almost all systems like this require the\nuser to sit fairly still, and to undergo a calibration procedure in which they look at points on\nthe screen in sequence. Performance can be poor when there is strong ambient lighting,\nwhen the user wears spectacles, has watery eyes or shiny skin. Often practice is required to\nget good results.\nChapter 3 of the Cairns and Cox book (by Natalie Webb and Tony Renshaw) is devoted to\neyetracking in HCI\nEye trackers are occasionally used to make gaze-controlled interfaces. At first sight, it\nseems that these might be especially natural and intuitive to use. In practice, natural eye\nmovement of fixations and saccades can confuse the eye-tracker inference algorithms, it is\nhard work to keep your eyes fixated on control locations for substantial periods of time, and\nthe natural temptation to glance elsewhere (check work in progress, look at the time, look\ndown at your hands etc) or to blink excessively must be constantly fought.\nSurface and tabletop interaction\nSurface\/tabletop interaction uses large display areas, usually projected, on a flat surface\nsuch as a wall or table. User interaction takes place by touching, gesturing, or pointing at\nthe display. Many of these systems use camera input, with more accurate recognition of the\nusers hands possible by using infrared, rather than visible light. A low-powered infrared\nspotlight is often used to illuminate the scene, rather than relying on body heat (which can\nbe confused by other hot objects in the environment \u2013 such as computers!). A popular\ntechnique at present is frustrated total internal reflection (FTIR), where infrared light is\nshone inside a flat transparent medium such as a glass panel, and anything touching the\nsurface causes infrared to be scattered. This technique can be used to recognize fingertip\ntouches, or gestures involving more skin contact, such as multiple fingers or even a flat\npalm. A motivating scenario for many of these systems has been the gesture-controlled\nprojection interface in the movie Minority Report.\nEmbodied interaction\nA user sitting at a desk, in front of a screen, with a keyboard and mouse on the surface, was\nthe default assumption in most classical user interface designs. Most of the new\ntechnologies described above are used in other positions, making it necessary to take\naccount of how users stand or move around. Machine vision, tracking of infrared markers,\nuse of accelerometer data, \u2018smart\u2019 fabrics and clothing, and many other techniques can be\n24 used to analyse and track body positions. The theoretical perspectives necessary to account\nfor embodied interaction rather than the \u2018disembodied\u2019 view of the mind preferred in AI\nand cognitive science (along with some fairly heavy philosophy) are explained in Where\nThe Action Is: The Foundations of Embodied Interaction by Paul Dourish (MIT Press\n2001).\n25 Lecture 6: Usability of programming languages\nFor many years, it seemed that conventional text programming would eventually be\nreplaced by visual programming languages, where program behaviour is defined by\ndrawing diagrams (many proposals resembled software engineering diagrams, such as those\nin UML \u2013 flow charts, object interaction, state charts etc). At a time when software\ndevelopment methods involved creating a complete specification in diagram form, then\nemploying programmers to convert those into code, it seemed as though programming\ncould be completely automated. However the fallacy of this reasoning was the same error\nmade when FORTRAN (Formula Translation) was considered to be \u2018automatic\nprogramming\u2019 \u2013 any representation that defines the program behaviour in sufficiently\nprecise detail to be compiled will be more like programming than like design. Drawing\nhighly detailed diagrams is often more laborious than writing highly detailed text, so it isn\u2019t\nthe case that diagrams will always have superior usability relative to text.\nMany elements of the modern WIMP interface originated in programming language\nresearch \u2013 the ancestor of the Windows and Macintosh GUIs was originally created at\nXerox PARC as a user interface to the Smalltalk language, and Shneiderman\u2019s principles of\ndirect manipulation were originally described as an alternative to programming languages.\nIt is likely that research into advanced programming techniques will continue to influence\nfuture user interfaces. There are also some good examples of programming languages that\nhave been designed for use by special groups \u2013 end user programmers who are not\nprofessionally trained in programming, or educational programming languages that\nillustrate programming language principles using graphical display elements. Examples\ninclude the LabView language for programming laboratory instrumentation and control,\nMax\/MSP used for music performance and digital art installations, and Scratch, used as a\nfirst programming language for children around the ages of 8-10.\nFor many years, programmers often argued that their favourite language was the best in the\nworld, almost like children arguing whether a tractor is better than a Ferrari. It should be\nclear that different languages are good for different purposes, and for use by different\npeople. These often include a broad mix of visual and textual (or even physical and\ntangible) elements, selected to meet specific needs.\nCognitive Dimensions of Notations\nThe usability principles by which we describe what kind of activities a language in being\nused for, and what kinds of visual representation can be useful or not useful for those\nactivities, have been collected into guidance for language designers, under the name\nCognitive Dimensions of Notations (CDs), a programme of work initiated at the MRC\nApplied Psychology Unit in Cambridge under the leadership of Thomas Green. Just as\nmany innovations in programming language user interfaces have led to radically different\n26 approaches to user interfaces, CDs are one of the most appropriate theoretical frameworks\nfor analysis of completely new content manipulation styles.\nThe CDs are presented as a vocabulary for design discussion. Many of the dimensions\nreflect common usability factors that experienced designers might have noticed, but did not\nhave a name for. Giving them a name allows designers to discuss these factors easily.\nFurthermore, CDs are based on the observation that there is no perfect user interface any\nmore than a perfect programming language. Any user interface design reflects a set of\ndesign trade-offs that the designers have had to make. Giving designers a discussion\nvocabulary means that they can discuss the trade-offs that result from their design\ndecisions. The nature of the trade-offs is reflected in the structure of the dimensions. It is\nnot possible to create a design that has perfect characteristics in every dimensions - making\nimprovements along one dimension often results in degradation along another.\nAn example dimension is called viscosity, meaning resistance to change. In some notations,\nsmall conceptual changes can be very expensive to make. Imagine changing a variable\nfrom int to long in a large Java program. The programmer has to find every function to\nwhich that variable is passed, check the parameter declarations, check any temporary local\nvariables where it is stored, check any calculations using the value, and so on. The idea of\nwhat the programmer needs to do is simple, but achieving it is hard. This is viscosity. There\nare programming languages that do not suffer from this problem, but they have other\nproblems instead \u2013 trade-offs. This means that language designers must be able to\nrecognise and discuss such problems when planning a new language. The word \u201cviscosity\u201d\nhelps that discussion to happen.\nCDs are relevant to a wide range of content manipulation systems \u2013 audio and video\neditors, social networking tools, calendar and project management systems, and many\nothers. These systems all provide a notation of some kind, and an environment for viewing\nand manipulating the notation. Usability is a function of both the notation and the\nenvironment.\nRepresentative cognitive dimensions\nThe following list gives brief definitions of the main dimensions, and examples of the\nquestions that can be considered in order to determine the effects that these dimensions will\nhave on different user activities.\nPremature commitment: constraints on the order of doing things.\nWhen you are working with the notation, can you go about the job in any order you like, or\ndoes the system force you to think ahead and make certain decisions first? If so, what\ndecisions do you need to make in advance? What sort of problems can this cause in your\nwork?\n27 Hidden dependencies: important links between entities are not visible.\nIf the structure of the product means some parts are closely related to other parts, and changes\nto one may affect the other, are those dependencies visible? What kind of dependencies are\nhidden? In what ways can it get worse when you are creating a particularly large description?\nDo these dependencies stay the same, or are there some actions that cause them to get frozen?\nIf so, what are they?\nSecondary notation: extra information in means other than formal syntax.\nIs it possible to make notes to yourself, or express information that is not really recognised as\npart of the notation? If it was printed on a piece of paper that you could annotate or scribble\non, what would you write or draw? Do you ever add extra marks (or colours or format choices)\nto clarify, emphasise or repeat what is there already? If so, this may constitute a helper device\nwith its own notation.\nViscosity: resistance to change.\nWhen you need to make changes to previous work, how easy is it to make the change? Why?\nAre there particular changes that are especially difficult to make? Which ones?\nVisibility: ability to view components easily.\nHow easy is it to see or find the various parts of the notation while it is being created or\nchanged? Why? What kind of things are difficult to see or find? If you need to compare or\ncombine different parts, can you see them at the same time? If not, why not?\nCloseness of mapping: closeness of representation to domain.\nHow closely related is the notation to the result that you are describing? Why? (Note that if\nthis is a sub-device, the result may be part of another notation, not the end product). Which\nparts seem to be a particularly strange way of doing or describing something?\nConsistency: similar semantics are expressed in similar syntactic forms.\nWhere there are different parts of the notation that mean similar things, is the similarity clear\nfrom the way they appear? Are there places where some things ought to be similar, but the\nnotation makes them different? What are they?\nDiffuseness: verbosity of language.\nDoes the notation a) let you say what you want reasonably briefly, or b) is it long-winded?\nWhy? What sorts of things take more space to describe?\nError-proneness: the notation invites mistakes.\nDo some kinds of mistake seem particularly common or easy to make? Which ones? Do you\noften find yourself making small slips that irritate you or make you feel stupid? What are some\nexamples?\nHard mental operations: high demand on cognitive resources.\nWhat kind of things require the most mental effort with this notation? Do some things seem\nespecially complex or difficult to work out in your head (e.g. when combining several things)?\nWhat are they?\nProgressive evaluation: work-to-date can be checked at any time.\nHow easy is it to stop in the middle of creating some notation, and check your work so far?\nCan you do this any time you like? If not, why not? Can you find out how much progress you\nhave made, or check what stage in your work you are up to? If not, why not? Can you try out\npartially-completed versions of the product? If not, why not?\n28 Provisionality: degree of commitment to actions or marks.\nIs it possible to sketch things out when you are playing around with ideas, or when you aren't\nsure which way to proceed? What features of the notation help you to do this? What sort of\nthings can you do when you don't want to be too precise about the exact result you are trying\nto get?\nRole-expressiveness: the purpose of a component is readily inferred.\nWhen reading the notation, is it easy to tell what each part is for? Why? Are there some parts\nthat are particularly difficult to interpret? Which ones? Are there parts that you really don't\nknow what they mean, but you put them in just because it's always been that way? What are\nthey?\nAbstraction: types and availability of abstraction mechanisms.\nDoes the system give you any way of defining new facilities or terms within the notation, so\nthat you can extend it to describe new things or to express your ideas more clearly or\nsuccinctly? What are they? Does the system insist that you start by defining new terms before\nyou can do anything else? What sort of things? These facilities are provided by an abstraction\nmanager - a redefinition device. It will have its own notation and set of dimensions.\nNotational activities\nWhen users interact with content, there are a limited number of activities that they can\nengage in, when considered with respect to the way the structure of the content might\nchange. A CDs evaluation must consider which classes of activity will be the primary type\nof interaction for all representative system users. If the needs of different users have\ndifferent relative priorities, those activities can be emphasised when design trade-offs are\nselected. The basic list of activities includes:\nSearch\nFinding information by navigating through the content structure, using the facilities provided\nby the environment (e.g. finding a specific value in a spreadsheet). The notation is not\nchanging at all, though the parts of it that the users sees will vary. Visibility and hidden\ndependencies can be important factors in search.\nIncrementation\nAdding further content without altering the structure in any way (e.g. adding a new formula to\na spreadsheet). If the structure will not change, then viscosity is not going to be very\nimportant.\nModification\nChanging an existing structure, possibly without adding new content (e.g. changing a\nspreadsheet for use with a different problem).\nTranscription\nCopying content from one structure or notation to another notation (e.g. reading an equation\nout of a textbook, and converting it into a spreadsheet formula).\nExploratory design\nCombining incrementation and modification, with the further characteristic that the desired\nend state is not known in advance (e.g. programming a spreadsheet on the fly or \u201chacking\u201d).\nViscosity can make this kind of activity far more difficult. This is why good languages for\n29 hacking may not be strictly typed, or make greater use of type inference, as maintaining type\ndeclarations causes greater viscosity. Loosely typed languages are more likely to suffer from\nhidden dependencies (a trade-off with viscosity), but this is not such a problem for exploratory\ndesign, where the programmer can often hold this information in his head during the relatively\nshort development timescale.\nChapter 5 of the Carroll book gives a more extended description of Cognitive Dimensions,\nwith examples and theoretical background. Further information on Cognitive Dimensions\nresearch can be found on-line in the Cognitive Dimensions archive:\nhttp:\/\/www.cl.cam.ac.uk\/~afb21\/CognitiveDimensions\/\n30 "}