{
    "text": "Quantitative Genetics for\nPlant Breeding\nWalter Suza (Editor); Kendall Lamkey (Editor); William Beavis; Katherine\nEspinosa; Mark Newell; and Anthony Assibi Mahama\nIowa State University Digital Press\nAmes, Iowa Quantitative Genetics for Plant Breeding Copyright \u00a9 by Walter Suza (Editor); Kendall Lamkey (Editor); William Beavis; Katherine\nEspinosa; Mark Newell; and Anthony Assibi Mahama is licensed under a Creative Commons Attribution-NonCommercial 4.0\nInternational License, except where otherwise noted.\nYou are free to copy, share, adapt, remix, transform, and build upon the material, so long as you follow the terms of the license.\nHow to cite this publication:\nSuza, W., & Lamkey, K. (Eds.). (2023). Quantitative Genetics for Plant Breeding. Iowa State University Digital Press.\nThis is a publication of the\nIowa State University Digital Press\n701 Morrill Rd, Ames, IA 50011\nhttps://www.iastatedigitalpress.com\ndigipress@iastate.edu Contents\nAbout the PBEA Series ix\nChapter 1: Gene Frequencies 1\nWilliam Beavis; Kendall Lamkey; and Anthony Assibi Mahama\nAllelic and Genotypic Variation 2\nHardy-Weinberg Equilibrium 7\nFactors Affecting Allele Frequency 10\nSelection 13\nReferences 20\nChapter 2: Linkage 21\nWilliam Beavis and Anthony Assibi Mahama\nDisequilibrium 21\nDissipation of Disequilibrium 25\nChi-Square Statistic 27\nReferences 27\nChapter 3: Resemblance Between Relatives 28\nWilliam Beavis; Kendall Lamkey; and Anthony Assibi Mahama\nBackground 29\nCoefficient of Inbreeding 29\nCoefficient of Parentage 32\nSelf Pollination 33\nFull-Sibing 39\nReferences 39 Chapter 4: Measures of Similarity 41\nWilliam Beavis; Mark Newell; and Anthony Assibi Mahama\nPopulation Structure Based on Pedigree Information 41\nPopulation Structure Based on Markers 42\nMeasures of Distance 43\nPrincipal Component Analysis 44\nCluster Analysis 47\nHierarchical Clustering 49\nChapter 5: Gene Effects 51\nWilliam Beavis; Kendall Lamkey; Katherine Espinosa; and Anthony Assibi Mahama\nLinear Models for Phenotypic Values 51\nAverage Genetic (Allelic) Effects 58\nBreeding Value 60\nEpistasis 64\nSingle Locus Genotype 67\nReferences 70\nChapter 6: Components of Variance 71\nWilliam Beavis; Kendall Lamkey; Katherine Espinosa; and Anthony Assibi Mahama\nPhenotypic Components of Variance 72\nGenetic Components of Variance 74\nDeriving Variance Components 77\nInfluence of Epistasis 81\nReferences 85 Chapter 7: Estimates of Variance 86\nWilliam Beavis; Kendall Lamkey; Katherine Espinosa; and Anthony Assibi Mahama\nCovariance of Relatives 86\nF2 and F3 Progenies 89\nBi-Parental Progenies 93\nUsing the Algorithm 95\nReferences 100\nChapter 8: Mating Designs 102\nWilliam Beavis; Kendall Lamkey; and Anthony Assibi Mahama\nDesign Setup 103\nDiallel Crosses 103\nF-Tests 106\nGardner and Eberhart Diallel Analysis II 108\nNorth Carolina Design I 113\nNorth Carolina Design II 116\nNorth Carolina Design III 120\nF-Tests 121\nReferences 122\nChapter 9: Selection Response 123\nWilliam Beavis; Kendall Lamkey; and Anthony Assibi Mahama\nUnderlying Theory of Selection 123\nHeritability on an Entry-Mean Basis 127\nFamily Structure 129\nMethod of Moments 133\nReferences 135 Chapter 10: G x E 136\nWilliam Beavis; Kendall Lamkey; Katherine Espinosa; and Anthony Assibi Mahama\nEnvironmental Components of Variance 136\nSimple Types of GxE Interactions 139\nComplex Types of GxE Interactions 142\nPartition of GxE Variances 146\nInteraction Components 147\nFlux between Genotypic Variance and GE Interaction Variance 151\nReferences 153\nChapter 11: Multiple Trait Selection 155\nWilliam Beavis; Kendall Lamkey; and Anthony Assibi Mahama\nIndex Selection 155\nExpected Genetic Gains 160\nConstruction of a Selection Index 165\nSelection Index Efficiency 169\nReferences 171\nChapter 12: Multi Environment Trials: Linear Mixed Models 172\nWilliam Beavis and Anthony Assibi Mahama\nHenderson\u2019s Concept 172\nBLUEs and BLUPs 176\nLinear Mixed Model Solution 178\nReference 179\nChapter 13: Simulation Modeling 180\nWilliam Beavis and Anthony Assibi Mahama\nHistory of Simulations 181\nGenetic Architecture of the Trait 184\nPolygenic Trait Simulation 188\nQTL Simulations 189\nReferences 190 Plant Breeding Basics 191\nWilliam Beavis and Anthony Assibi Mahama\nDefining Plant Breeding 191\nA Brief History of Quantitative Genetics 195\nTrait Measures 197\nTypes of Models 199\nInstallation of R 213\nAnalysis of Covariance 234\nComputational Considerations 237\nMatrix Algebra 238\nReferences 242\nApplied Learning Activities 244\nChapter 1 244\nChapter 2 244\nChapter 3 244\nChapter 4 244\nChapter 5 245\nChapter 6 245\nChapter 7 245\nChapter 8 245\nChapter 9 246\nChapter 10 246\nChapter 11 246\nChapter 12 247\nChapter 13 247\nPlant Breeding Basics 247 Contributors 248\nEditors 248\nChapter Authors 248\nContributors 249 ABOUT THE PBEA SERIES | IX\nAbout the PBEA Series\nBackground\nThe Plant Breeding E-Learning in Africa (PBEA) e-modules were originally developed as part of\nthe Bill & Melinda Gates Foundation Contract No. 24576.\nBuilding on Iowa State University\u2019s expertise with online plant breeding education, the PBEA e-\nmodules were developed for use in curricula to train African students in the management of crop\nbreeding programs for public, local, and international organizations. Collaborating with faculty\nat Makerere University in Uganda, University of KwaZulu-Natal in South Africa, and Kwame\nNkrumah University of Science and Technology in Ghana, our team created several e-modules\nthat hone essential capabilities with real-world challenges of cultivar development in Africa\nusing Applied Learning Activities. Our collaboration embraces shared goals, sharing knowledge\nand building consensus. The pedagogical emphasis on application produces a coursework-\nintensive MSc program for Africa.\n\u2022 PBEA Project Director: Walter Suza\n\u2022 Original Module Coordinator: William Beavis\nCollaborating Faculty and Experts in Africa: Richard Akromah, Stephen Amoah, Maxwell\nAsante, Ben Banful, John Derera, Richard Edema, Paul Gibson, Sadik Kassim, Rufaro Madakadze,\nSettumba Mukasa, Margaret Nabasirye, Daniel Nyadanu, Thomas Odong, Patrick Ongom, Joseph\nSarkodie-Addo, Paul Shanahan, Husein Shimelis, Julia Sibiya, Pangirayi Tongoona, Phinehas\nTukamuhabwa.\nThe authors of this textbook series adapted and built upon the PBEA modules to develop a series\nof textbooks covering individual topic areas. It is our hope that this project will facilitate wider\ndissemination and reuse of the PBEA modules\u2019 content.\nExplore the Series\n\u2022 Crop Genetics\n\u2022 Quantitative Methods for Plant Breeding\n\u2022 Molecular Plant Breeding\n\u2022 Quantitative Genetics for Plant Breeding X | ABOUT THE PBEA SERIES\n\u2022 Crop Improvement\n\u2022 Cultivar Development CHAPTER 1: GENE FREQUENCIES | 1\nChapter 1: Gene Frequencies\nWilliam Beavis; Kendall Lamkey; and Anthony Assibi Mahama\nThe challenge of Quantitative Genetics is to connect traits measured on quantitative scales\nwith genes that are inherited and evaluated as discrete units. This challenge was addressed\nthrough the development of theory between 1918 and 1947. The theory is now referred to as the\nModern Synthesis and required another 50 years for technological innovations and experimental\nbiologists to validate. Luminaries such as RA Fisher, Sewell Wright, JBS Haldane, and John\nMaynard Smith were able to develop the theory that is still widely applied without the benefit\nof high throughput \u2018omics\u2019 technologies. Indeed, modern synthesis was developed before the\nknowledge of the structure of DNA.\nPopulation genetics characterizes how discrete units, i.e., alleles, change in breeding populations.\nSuch characterization is the basis for understanding the structure of genomes and breeding\npopulations. The forces of mutation (Fig. 1), migration, selection, and drift will alter the structure\nof breeding populations. Herein we will learn how to characterize population structure at one or\ntwo loci in diploid crop species. This will set the foundation for characterizing structure based\non any number of loci and for polyploid crops that you may encounter in more advanced courses.\nLearning Objectives\n\u2022 Demonstrate the relevance of population genetics concepts to plant breeding populations.\n\u2022 Demonstrate the relevance of a purely theoretical Ideal Population to plant breeding\npopulations.\n\u2022 Demonstrate understanding of the purpose of populations in Hardy-Weinberg Equilibrium\n\u2022 Distinguish populations in Hardy-Weinberg Equilibrium from the Ideal Population.\n\u2022 Describe the impact of mutation, selection, and drift on breeding populations. 2 | CHAPTER 1: GENE FREQUENCIES\nFig. 1 A red Darwin hybrid tulip \u201cAppeldoorn\u201d with a mutation\nresulting in half of one petal being yellow. Photo by LepoRello;\nLicensed under CC BY-SA 3.0 via Wikimedia Commons.\nAllelic and Genotypic Variation\nIdeal Population\nIn order to understand the genetic structure of a population, it is necessary to establish a\nstandard reference population so that the breeding population can be characterized relative to\nthe standard. For this purpose, an \u2018ideal\u2019 conceptual base population can be defined as infinitely\nlarge with the potential to extract finite sub-populations through sampling, such as depicted in\nthe following figure and described in Falconer and Mackay (1996):\nFig. 2 Reference population. Adapted from Falconer and Mackay, 1996.\nNote that the sub-populations depicted in Fig. 2 are based on a genetic sampling process that CHAPTER 1: GENE FREQUENCIES | 3\nis affected by the reproductive biology of the species. Unlike animal species, crop species can\nreproduce in a variety of ways:\n\u2022 Sexual\n\u25e6 Cross-Pollination\n\u25e6 Self-Pollination\n\u25e6 Mixtures of Self and Cross-pollination\n\u2022 Asexual\n\u25e6 Clonal\n\u25e6 Doubled haploids\n\u25e6 Apomixis\nAssumptions\nIn the ideal population depicted in Fig. 2, the following assumptions are true:\n1. The base population is infinite or at least too large to count.\n2. There is no migration between sub-populations.\n3. There is no breeding between overlapping generations.\n4. The number of breeding individuals is the same in each subpopulation.\n5. There is random mating within a subpopulation.\n6. There is no Selection.\n7. There is no Mutation.\nOf course, in real populations, these assumptions are violated.\nAllelic and Genotypic Frequencies\nWe first model a single locus with only two alleles in an ideal breeding population of diploid\nindividuals. Define the following:\nN = number of breeding individuals in a subpopulation (population size)\nt = time usually measured in terms of generations\nq = frequency of one of two alleles at a locus within a subpopulation\np = 1 \u2013 q = frequency of a second allele at a locus within a subpopulation\n= frequency of a second allele across the subpopulations (the mean of p)\np = frequency of a second allele in the base population\n0 4 | CHAPTER 1: GENE FREQUENCIES\nDue to the assumptions associated with an ideal reference population, = q at any stage or\n0\ngeneration of the sampling process, so q can be used interchangeably with .\n0\nThe alleles, allele frequencies, genotypes, and genotypic frequencies can be represented in Table\n1 and in Equations 1 and 2.\nTable 1 Alleles, allele frequencies, genotypes, and\ngenotypic frequencies.\nAlleles Genotypes\nA a AA Aa aa\nFrequencies p q P P P\nAA Aa aa\n.\nSum of allele frequencies.\n.\nSum of genotype frequencies,\nwhere:\nare as defined earlier,\n= frequencies of the three genotypes.\nVariance of Allele Frequency\nThe relationship between allele frequencies and genotype frequencies can be expressed as in\nEquation 3.\n,\nEquation for determining allele frequency,\nwhere:\nare as defined earlier.\nA particular sub-population is a random sample of N individuals or 2N gametes (for a diploid)\nfrom the base population. Therefore, the expected gene frequency of a particular allele in the sub-\npopulations is q and the variance of q is represented by Equation 4.\n0, CHAPTER 1: GENE FREQUENCIES | 5\n.\nEquation for estimating the variance of an allele,\nwhere:\n= the variance of an allele,\n= the number of individuals.\nSince q is a constant, the variance of the change in allele frequency (q \u2013 q ) is also:\n0 1 0\n.\nEquation for estimating the variance of change in allele frequency,\nwhere:\n= the change in allele frequency,\nare as defined before.\nFrequency Estimators\nIn addition to the genetic sampling process depicted in Fig. 2, a statistical sampling process can\nbe used to estimate frequencies, variances, and covariances of alleles and genotypes in a sub-\npopulation. If we sample n individuals from a population of size N, then notationally (Equation\n6),\nEquation for determining the number, n, of individuals and number of A\nindividuals in a sample from a population,\nwhere:\n= the sample size,\nare as defined before.\nEstimates of the frequency of the allele and genotype in the sample are obtained using\nEquation 7. 6 | CHAPTER 1: GENE FREQUENCIES\nEstimating the frequency of A allele and AA genotype in a sample,\nwhere:\n= the estimate of the A allele frequency,\n= the estimate of the AA genotype frequency.\nExpected Number of Alleles\nRecognizing that statistical sampling at a locus with two alleles in a diploid population is\nrepresented as a binomial random process, the expected number of A alleles and their frequency\nin a sample can be determined using Equation 8.\nEstimating the expected number of a allele,\nwhere:\n= the expectation of the A allele,\nare as defined previously.\nThus, is an unbiased estimator of the population parameter .\nUsing the definition of variance, we can likewise find the and using\nEquations 9 and 10. All terms have been defined previously.\nCalculating the variance of n number of the A allele.\nCalculating the variance of the estimated frequency of the A allele.\nNote that are usually unknown, so we often substitute in the\ncalculation of the . Also, note that is not the variance of a Binomial\ndistribution. If the population sampled is in Hardy-Weinberg Equilibrium (see below), the\ngenetic sampling of alleles will be random so that . The\nvariance of the estimated frequency of the A allele can be obtained using Equation 11, which has\nthe form of the variance from a binomial distribution. CHAPTER 1: GENE FREQUENCIES | 7\nAlternative equation for calculating the variance of estimated frequency of an\nallele.\nwhere:\nare as defined previously.\nHardy-Weinberg Equilibrium\nThe proof of the Hardy Weinberg Equilibrium (HWE) requires the following assumptions\n(Falconer and Mackay, 1996):\n1. Allele frequency in the parents is equal to the allele frequency in the gametes.\n\u25e6 Assumes normal gene segregation.\n\u25e6 Assumes equal fertility of parents.\n2. Allele frequency in gametes is equal to the allele frequency in gametes forming zygotes.\n\u25e6 Assumes equal fertilizing capacity of gametes.\n\u25e6 Assumes a large population.\n3. Allele frequency in gametes forming zygotes is equal to allele frequencies in zygotes.\n4. Genotype frequency in zygotes is equal to genotype frequency in progeny.\n\u25e6 Assumes random mating.\n\u25e6 Assumes equal gene frequencies in male and female parents.\n5. Genotype frequencies in progeny do not alter gene frequencies in progeny.\n\u25e6 Assumes equal viability.\nFor a two-allele locus in a population in HWE, .\nHWE at a given genetic locus is achieved in one generation of random mating. Genotype\nfrequencies in the progeny depend only on the allele frequencies in the parents and not on the\ngenotype frequencies of the parents. 8 | CHAPTER 1: GENE FREQUENCIES\nDisequilibrium\nAs discussed, there are several processes that can force allelic and genotypic frequencies to\ndeviate from HWE. Deviations from equilibrium are referred to as disequilibrium and are often\ndenoted with a disequilibrium coefficient, D. In the two allele case, the genotypic frequencies can\nbe represented as .\nThus, the disequilibrium coefficient can be estimated using Equation 12.\n.\nEquation for estimating D.\nwhere:\n= the estimate of the disequilibrium coefficient of the A allele,\nare as defined previously.\nNote that the expectation of can be obtained from Equation 13. All terms are defined earlier.\n.\nEquation for estimating D.\nThe, is biased. Although the estimate of is biased, as the sample size, n, becomes large,\nthe bias becomes small. Thus, emphasizing the need for large sample sizes in drawing inferences\nabout disequilibrium from Hardy-Weinberg.\nVariance\nThe can likewise be derived as (Equation 14:\n.\nEquation for estimating the variance of .\nIf is large, , and , a normal distribution.\nSo, a standard normal variate, can be constructed as: CHAPTER 1: GENE FREQUENCIES | 9\n.\nEquation for estimating the standard normal variate, Z.\nGoodness of Fit\nAlternatively, because , therefore Equation 16 allows the calculation of chi-square.\n,\nEquation for estimating chi-square.\nThis form enables the direct use of genotypic counts, , as shown in Table 2.\nTable 2 Representations of observed and expected genotypic counts and differences between the\ncounts.\nn/a Genotypes\nn/a AA Aa aa\nObserved (O)\nExpected (E)\nO-E\nThe Goodness of Fit Statistic\nAssessing the fit of observed data to expectation can be accomplished by using Equation 17.\nFormula for calculating chi-square goodness of fit statistic.\nwhere:\n= the observed data,\n= expected data. 10 | CHAPTER 1: GENE FREQUENCIES\nNon-Random Mating\nTwo methods of non-random mating that are important in plant breeding are assortative mating\nand disassortative mating.\nAssortative mating occurs when similar phenotypes mate more frequently than they would by\nchance. One example would be the tendency to mate early x early maturing plants and late x late\nmaturing plants. The effect of assortative mating is to increase the frequency of homozygotes and\ndecrease the frequency of heterozygotes in a population relative to what would be expected in a\nrandomly mating population. Assortative mating effectively divides the population into two or\nmore groups where matings are more frequent within groups than between groups.\nDisassortative mating occurs when unlike or dissimilar phenotypes mate more frequently than\nwould be expected under random mating. Its consequences are, in general, opposite those of\nassortative mating in that disassortative mating leads to an excess of heterozygotes and a\ndeficiency of homozygotes relative to random mating. Disassortative mating can also lead to the\nmaintenance of rare alleles in a population.\nFactors Affecting Allele Frequency\nThe factors affecting changes in allele frequency can be divided into two categories: systematic\nprocesses, which are predictable in both magnitude and direction, and dispersive processes,\nwhich are predictable in magnitude but not direction. The three systematic processes are\nmigration, mutation, and selection. Dispersive processes are a result of sampling in small\npopulations.\nMigration\nAssume a population has a frequency of m new immigrants each generation, with 1-m being the\nfrequency of natives. Let q be the frequency of a gene in the immigrant population and q the\nm 0\nfrequency of the same gene in the native population. Then the frequency in the mixed population\nwill be:\n.\nFormula for calculating the frequency of an allele in a mixed population.\nwhere: CHAPTER 1: GENE FREQUENCIES | 11\n= the frequency of the allele,\nare as defined.\nThe change in gene frequency brought about by migration is the difference between the allele\nfrequency before and after migration.\n.\nFormula for calculating the change in gene frequency.\nThus the change in gene frequency from migration is dependent on the rate of migration and the\ndifference in allele frequency between the native and immigrant populations.\nMutations\nMutations are the source of all genetic variation. Loci with only one allelic variant in a breeding\npopulation have no effect on phenotypic variability. While all allelic variants originated from\na mutational event, we tend to group mutational events into two classes: rare mutations and\nrecurrent mutations, where the mutation occurs repeatedly.\nRare Mutations\nBy definition, a rare mutation only occurs very infrequently in a population. Therefore, the\nmutant allele is carried only in a heterozygous condition and, since mutations are usually\nrecessive, will not have an observable phenotype. Rare mutations will usually be lost, although\ntheory indicates rare mutations can increase in frequency if they have a selective advantage.\nFate of a Single Mutation\nConsider a population of only AA individuals. Suppose that one A allele in the population\nmutates to a. Then there would only be one Aa individual in a population of AA individuals. So,\nthe Aa individual must mate with an AA individual, i.e.,\nThis mating has the following outcomes Li (1976; pp 388):\n1. No offspring are produced, in which case the mutation is lost.\n2. One offspring is produced: the probability of that offspring being is , so the\nprobability of losing the mutation is . 12 | CHAPTER 1: GENE FREQUENCIES\n3. Two offspring are produced: can mate with more than one of the individuals in\nthe population, thus if mates with two individuals, the probability of both\noffspring being AA is , so the probability of losing the mutation is .\nIf is the number of offspring from the above mating, then the probability of losing the mutation\namong the first generation of progeny is .\nProbability of Loss\nThe probability of losing the gene in the second generation can be calculated by making the\nfollowing assumptions:\n\u2022 The number of offspring per mating is distributed as a Poisson process (which means that\nthey follow a stochastic distribution in which events occur continuously and independently\nof one another).\n\u2022 With the average number of offspring per mating = 2.\n\u2022 New mutations are selectively neutral.\nWith these assumptions, the probabilities of extinction are as in Table 3:\nTable 3 Probability of extinction\nin different generations.\nGeneration Probability of Loss\n1 0.37\n7 0.79\n15 0.89\n31 0.94\n63 0.97\n120 0.98\nRecurrent Mutations\nLet the mutation frequencies be: CHAPTER 1: GENE FREQUENCIES | 13\nThen the change in gene frequency in one generation at equilibrium is determined using\nEquation 20, where , and .\n.\nFormula for calculating the change in gene frequency due to mutation.\nwhere:\n= the rate of mutation of A to a allele,\n= the rate of mutation of a to A allele\nOther terms are as defined.\nConclusions\n\u2022 Mutations alone produce very slow changes in allele frequency.\n\u2022 Since reverse mutations are generally rare, the general absence of mutations in a population\nis due to selection.\nSelection\nSelection is one of the primary forces that will alter allele frequencies in populations. Selection\nis essentially the differential reproduction of genotypes. In population genetics, this concept\nis referred to as fitness and is measured by the reproductive contribution of an individual (or\ngenotype) to the next generation. Individuals that have more progeny are more fit than those who\nhave less progeny because they contribute more of their genes to the population.\nThe change in allele frequency following selection is more complicated than for mutation and\nmigration because selection is based on phenotype. Thus, calculating the change in allele\nfrequency from selection requires knowledge of genotypes and the degree of dominance with\nrespect to fitness. Selection affects only the gene loci that affect the phenotype under\nselection\u2014rather than all loci in the entire genome\u2014but it also would affect any genes that are\nlinked to the genes under selection. 14 | CHAPTER 1: GENE FREQUENCIES\nEffects of Selection\nChange in allele frequency: The strength of selection is expressed as a coefficient of selection,\ns, which is the proportionate reduction in gametic output of a genotype compared to a standard\ngenotype, usually the most favored. Fitness (relative fitness) is the proportionate contribution of\noffspring.\nPartial selection against a completely recessive allele: To see how the change in allele frequency\nfollowing selection is calculated, consider the case of selection against a recessive allele:\nTable 4 Initial genotypic frequencies, coefficient of selection, fitness, and\ngametic contribution by genotypes.\nn/a Genotypes\nn/a AA Aa aa Total\n2 2\nInitial Frequencies p 2pq q 1\nCoefficient of Selection 0 0 s n/a\nFitness 1 1 1 \u2013 s n/a\n2 2 2\nGametic Contribution p 2pq q (1 \u2013 s) 1 \u2013 sq\nFrequency Equations\nThe frequency of allele after selection is estimated using Equation 21:\n.\nFormula for calculating the frequency of an allele following selection,\nwhere:\n= the selection differential (represented as a deviation from the population mean),\nare as defined.\nThe change in allele frequency is then represented as in Equation:\n. CHAPTER 1: GENE FREQUENCIES | 15\nFormula for calculating the change in allele frequency due to selection.\nwhere:\nare as defined.\nIn general, you can show that the number of generations, , required to reduce a recessive from a\nfrequency of to a frequency of , assuming complete elimination of the recessive, i.e.,\nis (Equation 23):\nFormula for calculating the number of generations required.\nSmall Population Size\nUnlike the three systematic forces that are predictable in both amount and direction, changes\ndue to small population size are predictable only in amount and are random in direction.\nThe effects of small population size can be understood from two different perspectives. It can be\nconsidered a sampling process, and it can be considered from the point of view of inbreeding.\nThe inbreeding perspective is more interesting, but looking at it from a sampling perspective lets\nus understand how the process works.\nConsequences of small population size\n1. Random genetic drift: random changes in allele frequency within a subpopulation\n2. Differentiation between subpopulations\n3. Uniformity within subpopulations\n4. Increased homozygosity\nExample 1: Let q = 0.5 and N = 50, then\nExample 2: Let q = 0.5 and N = 4, then 16 | CHAPTER 1: GENE FREQUENCIES\nInbreeding and Small Populations\nInbreeding is the mating together of individuals that are related by ancestry. The degree of\nrelationship among individuals in a population is determined by the size of the population. This\ncan be seen by examining the number of ancestors that a single individual has:\nTable 5 Number of ancestors of an individual in relation to the\nnumber of generations.\nGeneration Ancestors\n0 1\n1 2\n2 4\n3 8\n4 16\n5 32\n6 64\n10 1,024\n50 1,125,899,906,842,620\n100 1,267,650,600,228,230,000,000,000,000,000\nt\nt 2\nJust 50 generations ago, note that a single individual would have more ancestors than the number\nof people that have existed or could exist on earth.\nTherefore, in small populations, individuals are necessarily related to one another. Pairs mating\nat random in a small population are more closely related than pairs mating together in a large\npopulation. Small population size has the effect of forcing relatives to mate even under random\nmating; thus, with small population sizes inbreeding is inevitable.\nIdentical by Descent and Identical by State\nIn finite populations, there are two sorts of homozygotes: Those that arose as a consequence\nof the replication of a single ancestral gene \u2014 these genes are said to be identical by descent\n(Bernardo, 1996). If the two genes have the same function but did not arise from the replication\nof a single ancestral gene, they are said to be alike in state. It is the production of homozygotes\nthat are identical by descent that gives rise to inbreeding in a small population. CHAPTER 1: GENE FREQUENCIES | 17\nCoefficient of Inbreeding\nThe probability that two genes are identical by descent is called the coefficient of inbreeding\nand will be the measure of the relationship between mating pairs.\nThe coefficient of inbreeding (F) refers to the individual and expresses the degree of relationship\nbetween an individual\u2019s parents. The coefficient of inbreeding is always expressed relative to a\nspecified base population. The reference population is assumed to be non-inbred (F=0).\nConsider a base population consisting of N individuals, each shedding equal numbers of gametes\nuniting at random. Because the base is non-inbred, each individual in this population carries\ngenes that are non-identical. The only way a homozygote that carries genes that are identical by\ndescent can arise is by the mating of a male and female gamete from the same individual that\ncarries a replication of the same gene. Because there are 2N gametes, the probability that two\nmating gametes are identical by descent is .\nEquation of Coefficient of Inbreeding\nIn the second generation, there are two ways genes are identical by descent can be joined:\n1. by a new replication of the same ancestral gene; and\n2. by the previous replication that occurred in generation 1. 18 | CHAPTER 1: GENE FREQUENCIES\nThe probability of a new replication event is . The remaining proportion of zygotes,\n, carry genes that are independent in origin from generation 1 but may have been\nidentical in their origin in generation 0. The probability that the genes are identical by descent\nfrom generation 1 is the inbreeding coefficient of generation 1 is .\nTherefore, the probability of identical homozygotes in generation 2 is represent in Equation 23:\n,\nFormula for calculating the probability of identical homozygotes.\nwhere:\n= the inbreeding coefficients of generations 1 and progeny generation\n(PG),\n= the population size.\nThe same arguments apply to future generations, so we can write the recurrence equation as\n(Equation 24):\n.\nFormula for calculating the probability of identical homozygotes in future\ngeneration.\nwhere:\n= the inbreeding coefficients of generation t,\n= the population size.\nInbreeding Coefficient\nThe inbreeding of any generation is composed of two components: new inbreeding, which arises\nfrom self-fertilization, and the old, which was already there.\nNote that inbreeding is cumulative and that the absence of inbreeding in generation does not\nchange the fact that a population has inbreeding from prior generations.\nThrough a series of algebraic steps, we can write the inbreeding coefficient as a function of the\nnumber of generations removed from the reference populations (Equation 25): CHAPTER 1: GENE FREQUENCIES | 19\n,\nFormula for calculating the inbreeding coefficient.\nwhere:\nis the change in inbreeding coefficients.\nDispersion\nTo relate inbreeding back to population size, we can rewrite the variance of the change in allele\nfrequency as (Equation 26):\n,\nAlternative formula for calculating the variance of the change in allele\nfrequency.\nAnd also represent the variance of the allele frequency as in Equation 27,\n.\nAlternative formula for calculating the variance of the allele frequency.\nexpresses the rate of dispersion and expresses the amount of dispersion.\nChanges in Frequencies\nThe genotype frequencies in a population can then be expressed as:\nTable 6 Contribution of inbreeding coefficient F on genotypic frequencies for a two-allele locus case\n(Falconer and Mackay, 1996, p62).\nn/a n/a n/a Origin\nChange due to\nn/a Original Frequencies Independent Identical\ninbreeding\nAA\nAa\nAa 20 | CHAPTER 1: GENE FREQUENCIES\nThe algebra summarizes what is expected to happen \u201casymptotically\u201d. In any given breeding\npopulation, the results will vary due to sampling.\nReferences\nBernardo, R. 1996. Best Linear Unbiased Prediction of Maize Single-Cross Performance Given\nErroneous Inbred Relationships. Crop Sci. 36:862-866.\nFalconer, D.S., and T.F.C. Mackay. 1996. Introduction to quantitative genetics. 4th ed. Pearson,\nBurnt Mill, England.\nHow to cite this chapter: Beavis, W., K. Lamkey, and A. A. Mahama. 2023. Gene Frequencies. In W. P.\nSuza, & K. R. Lamkey (Eds.), Quantitative Genetics for Plant Breeding. Iowa State University Digital\nPress. CHAPTER 2: LINKAGE | 21\nChapter 2: Linkage\nWilliam Beavis and Anthony Assibi Mahama\nPlant breeding populations, by definition, employ methods that force populations into states of\ndisequilibrium. Plant breeders do not mate infinite (or even large) numbers of parents; thus, drift\nhas a major impact on population disequilibrium. They select the parents that will be used in\nmating; thus, selection, linkage, and pleiotropy affect the population structure. New lines from\nexternal breeding projects are often introduced to the breeding nurseries; thus, migration affects\nthe structure of plant breeding populations. After the passage of the Plant Variety Protection Act,\nplant breeders working in the commercial sector began to keep breeding records for purposes of\nprotecting intellectual property. An unintended consequence has been the application of linear\nmixed models to produce predictors of performance, originally developed by animal breeders.\nThese methods are predicated on the use of coefficients of relationship among cultivars with\nknown performance and progeny with unknown or limited information on performance.\nHerein we introduce gametic and linkage disequilibrium as measures of deviation\n(disequilibrium) from Hardy-Weinberg Equilibrium. In other words, the estimation of these\npopulation parameters is based on a reference population, and the reference population must be\ndefined, or else the calculated values have no meaning.\nLearning Objectives\n\u2022 Demonstrate understanding that linkage and linkage disequilibrium are properties of\npopulations, not individuals.\n\u2022 Distinguish gamete from linkage disequilibrium.\n\u2022 Demonstrate ability to estimate recombination and disequilibrium statistics.\nDisequilibrium\nThe motivation is to \u2018map\u2019 genetic loci based on how they are most likely to be inherited relative\nto each other. If alleles at two loci are on the same chromosome in close proximity to each other,\nthen they will be inherited together more often than not. It was recognized in the 1920s (Sax,\n1923) that markers could have value for selecting phenotypes that are difficult to assay, but 60\nyears passed before the theory could be evaluated on a genome-wide scale. Linkage represents a\nmechanism that results in Disequilibrium among alleles at more than a single locus on the same\nchromosome. It is also possible that Disequilibrium among alleles at more than a single locus 22 | CHAPTER 2: LINKAGE\ncan result from mechanisms other than linkage, e.g., selection and drift. Unfortunately, the term\n\u201clinkage disequilibrium\u201d has been applied to all forms of multi-locus disequilibrium. Herein we\ntry to use the term \u201clinkage disequilibrium\u201d only for cases where we know alleles are on the same\nchromosome and \u201cgametic disequilibrium\u201d for situations when we do not know whether the loci\nare on the same chromosome.\nDisequilibrium Example\nConsider parent 1 with genotype A1A1B1B1C1C1D1D1 and parent 2 with A2A2B2B2C2C2D2D2.\nLoci A, B, and C are on a homologous chromosome, and D is on a separate chromosome (Fig. 1).\nFig. 1 A, B, C, and D loci on two pairs of homologous chromosomes.\nThe genotype of the F1 generation resulting from the cross between parent 1 and 2 will be\nA1A2 B1B2 C1C2 D1D2. Loci A and D are located on different chromosomes and will segregate\nindependently according to the random segregation of chromosomes into gametes. For two\ndifferent alleles at each locus, four possible combinations can occur, each with a chance of\n25%. A and C are unlinked on the same chromosome. They are so far away from each other\nthat recombination occurs between them in 50% of the meioses. The frequencies of all gametes\ninvolving alleles at the A and C locus (A1C1, A1C2, A2C1, A2C2) is 0.25, just as it is for the\nalleles for the A and D loci and the B and D loci. Since locus A and C assort independently,\nthe frequency of double homozygous dominant and double homozygous recessive genotypes\n(A1A1C1C1, A2A2C2C2) is 0.25\u00d70.25, and the frequency of double heterozygous genotypes\n(A1A2C1C2) is 0.5 x 0.5.\nLoci A and B are linked because they are located in close proximity on the same chromosome\nresulting in recombination frequencies that are less than 0.5, e.g., 0.1. The difference between the\nexpectation for unlinked loci and the estimated recombination frequency can be used to classify\nlinkage, i.e., the likelihood of two loci being inherited together. To estimate recombination\nfrequencies, non-parental gametes can be counted and divided by the total number of gametes. CHAPTER 2: LINKAGE | 23\nGametic Disequilibrium\nDisequilibrium can be created by self-pollination, crossing relatives within a breeding\npopulation, mutation, drift, selection, and migration. For example, consider alleles at loci A and\nD. Let us assume that each contributes to phenotypic variability in flower initiation in an additive\nmanner. Let us also assume selection for earlier flowering (conferred by the A1 and D2 alleles).\nThe impact will be a negative covariance between the alleles at loci A and D, which reduces the\ngenetic variances and creates disequilibrium between those loci. Even though A1 and D2 alleles\nare physically independent, they become correlated by selection which results in D >0. This\nA1, D2\nis also referred to as the Bulmer effect.\nAlthough individual loci achieve HWE after one generation of random mating, genotype\nfrequencies at two or more loci do not achieve equilibrium jointly after one generation of random\nmating.\nTo illustrate this point, consider two populations, one consisting entirely of AABB genotypes\nand the other consisting entirely of aabb genotypes. Assume they are mixed equally and allowed\nto mate randomly. The first generation would consist of the three genotypes AABB, AaBb, and\naabb in the proportions . However, for two loci with two alleles, nine genotypes are\npossible.\n(For n alleles at each locus and k loci, there are possible genotypes).\nContinued random mating would produce the missing genotypes, but they would not appear at\nthe equilibrium frequencies immediately. 24 | CHAPTER 2: LINKAGE\nDisequilibrium Table\nConsider Table 1 below:\nTable 1 Alleles and gametic types, their actual and equilibrium frequencies, and the\ndifference between them.\nAlleles A a B b\nAllele\nFrequencies\nn/a n/a n/a n/a n/a\nGametic Types AB Ab aB ab\nFrequencies at\nEquilibrium\nActual\nR S T U\nFrequencies\nDifference\nfrom\nEquilibrium\nA coupling heterozygote would be and occur with frequency , and the repulsion\nheterozygote would be occurring with frequency . If the frequency of these two\ngenotypes is equal, the population is in equilibrium, and Equation 1 can be used to estimate the\ndisequilibrium coefficient, D:\n.\nFormula for estimating D.\nwhere:\n= the actual gamete frequencies.\nIt can be shown that after t generations of random mating, the disequilibrium is given by\nEquation 2:\n.\nFormula for estimating D after t generations of mating. CHAPTER 2: LINKAGE | 25\nwhere:\n= the disequilibrium in the 0 and t generations, respectively,\n= the recombination frequency, equals for independently segregating loci.\nDissipation of Disequilibrium\nThe dissipation of disequilibrium relative to generation 0 is given in the figure below:\nFig. 2 Dissipation of disequilibrium.\nDeviations from independence at multiple loci are often referred to as linkage disequilibrium,\neven if linkage is not the cause. Unless two loci are known to reside on the same chromosome the\nterm Gametic Disequilibrium is a less ambiguous term to describe disequilibrium among loci.\nEstimation and Testing\nDisequilibrium at the A and B loci is a comparison of gametic frequency, , with the product\nof allele frequencies, ; and is estimated with Equation 3,\nFormula for estimating disequilibrium at two loci.\nwhere:\n= the disequilibrium at loci A and B,\nare as defined previously. 26 | CHAPTER 2: LINKAGE\nThe expectation of the estimated disequilibrium between two loci is calculated using Equation 4.\n.\nFormula for obtaining the expectation of estimated disequilibrium between two\nloci.\nwhere:\nare as defined previously.\nThe variance of the estimated disequilibrium is calculated using Equation 5.\n.\nFormula for obtaining the variance of estimated disequilibrium between two loci.\nwhere:\nare as defined previously.\nNote the similarities to . Thus, the distribution of estimated disequilibrium between two loci\napproaches a normal distribution (Equation 6).\nNormal distribution equation of .\nwhere:\nare as defined previously.\nThe Z statistic can be obtained using Equation 7;\n.\nFormula for calculating Z statistic for.\nwhere:\nare as defined previously. CHAPTER 2: LINKAGE | 27\nChi-Square Statistic\nAgain, a chi-square statistic for the hypothesis of no disequilibrium can be calculated using\nEquation 8 and Table 2.\n.\nFormula for calculating chi-square statistic,\nwhere:\nare as defined previously.\nTable 2 Arrangement of gametic types, their observed and expected counts for calculating\nchi-square statistic.\nGamete AB AB AB AB\nObserved\nExpected\nReferences\nSax, K. 1923. The association of size differences with seed-coat pattern, and pigmentation in\nPhaseolus vulgaris. Genetics, 8, 552\u2013560.\nHow to cite this chapter: Beavis, W. and A. A. Mahama 2023. Linkage. In W. P. Suza, & K. R. Lamkey\n(Eds.), Quantitative Genetics for Plant Breeding. Iowa State University Digital Press. 28 | CHAPTER 3: RESEMBLANCE BETWEEN RELATIVES\nChapter 3: Resemblance Between Relatives\nWilliam Beavis; Kendall Lamkey; and Anthony Assibi Mahama\nPlant breeding populations, by definition, employ methods that force populations into states of\ndisequilibrium. Plant breeders do not mate infinite (or even large) numbers of parents; thus, drift\nhas a major impact on population disequilibrium. They select the parents that will be used in\nmating; thus, selection linkage and pleiotropy affect the population structure. New lines from\nexternal breeding projects are often introduced to the breeding nurseries, thus migration affects\nthe structure of plant breeding populations. After the passage of the Plant Variety Protection Act,\nplant breeders working in the commercial sector began to keep breeding records for purposes of\nprotecting intellectual property. An unintended consequence has been the application of mixed\nlinear models to produce predictors of performance, originally developed by animal breeders.\nThese methods are predicated on the use of coefficients of relationship among cultivars with\nknown performance and progeny with unknown or limited information on performance.\nFig. 1 Plant breeding specimens in a lab at Makerere\nUniversity in Uganda. Photo by Iowa State University.\nLearning Objectives\n\u2022 Utilize population genetic concepts as a foundation to understand coefficients of inbreeding,\nparentage, and relationship.\n\u2022 Calculate coefficients of parentage and inbreeding. CHAPTER 3: RESEMBLANCE BETWEEN RELATIVES | 29\nBackground\nFig. 2 Fractional relationships in the calculation of coefficients of relationships\nand inbreeding. CC BY-SA 4.0\nThe calculation of coefficients of relationships and inbreeding were originally developed as path\ncoefficients by Sewall Wright and identity by descent by Gustave Mal\u00e9cot. The calculations\nwere simplified by Emik and Terrill (1949) and extended to all possible measures of identity by\nCockerham (1971). Example relatedness in humans is shown in Fig. 2.\nHerein we introduce inbreeding and parentage as deviations (disequilibrium) from Hardy\nWeinberg Equilibrium. In other words, the calculations of all of these measures are based on a\nreference population and the reference population must be defined or else the calculated values\nhave no meaning.\nCoefficient of Inbreeding\nLet us consider a random mating diploid population consisting of individuals: Because there 30 | CHAPTER 3: RESEMBLANCE BETWEEN RELATIVES\nare gametes, the probability that two mating gametes are identical by descent is\n. Therefore, . The remaining proportion of zygotes carry genes that are\nindependent in origin from generation 1. Therefore, the probability of identical homozygotes in\ngeneration 2 is represented by Equation 1:\n,\nFormula for calculating the probability of identical homozygotes in generation 2.\nwhere:\n= the inbreeding coefficients of generations 1 and 2,\n= the number of individuals in the population.\nwhere F and F are the inbreeding coefficients of generations 1 and 2. The same arguments apply\n1 2\nto future generations, so we can write the recurrence equation as in Equation 2:\n.\nFormula, the recurrence equation, for calculating the probability of identical\nhomozygotes in generation t,\nwhere:\n= the inbreeding coefficients of generations t,\n= the number of individuals in the population.\nThe inbreeding of any generation is composed of two components: new inbreeding, which arises\nfrom self-fertilization, and the \u201cold\u201d that was already there.\nNote that inbreeding is cumulative and that the absence of inbreeding in generation t does not\nchange the fact that a population may be inbred relative to prior generations.\nGeneral Principle\nRather than considering a random mating population, let\u2019s consider a population that is\nexperiencing a systematic inbreeding process. In this case, F refers to the proportionate\nreduction in heterozygosity (relative to a population that is in HWE) through inbreeding\nprocesses. For example, let us consider self-pollination. Begin with an F from a cross of two\n1 CHAPTER 3: RESEMBLANCE BETWEEN RELATIVES | 31\nhomozygous lines. We can self the F to get an F . How about if we random mate F ? Does this\n1 2 1\ncreate a population in HWE? What is the reference population?\nIf F is the proportionate decrease in heterozygosity due to an inbreeding process, then with self-\npollination F can be easily calculated for any generation of selfing as shown in Equation 3:\n.\nFormula for calculating the proportionate reduction in heterozygosity.\nwhere:\n= the generation of interest.\nImpact on Disequilibrium\nThe impact on deviations, i.e., disequilibrium, relative to HWE can be summarized as:\nHWE Frequencies Change due to inbreeding.\nAA\nAa\nAa\nAlternatively, we can think of the coefficient of inbreeding as the probability of identity by\ndescent. In this case, the coefficient of inbreeding is the probability that two alleles at a locus\nin an individual are IBD. For two individuals and , the relationship is represented in\nEquation 4.\nFormula for calculating the coefficient of inbreeding (equivalent to IBD).\nwhere:\n= the coefficient of inbreeding of x,\n= the coefficient of inbreeding of y,\n= probability that a and b and c and d are IBD. 32 | CHAPTER 3: RESEMBLANCE BETWEEN RELATIVES\nCoefficient of Parentage\nWhat if two homozygous parents of an F used to create an F population are related? Let us\n1 2\nthink about the relationship, parentage, and co-ancestry between two individual people, dogs,\ncorn plants, soybean plants, etc. Refer to these individuals as X and Y. Also, let us use a\nshorthand for a quantitative measure of this relationship. This relationship is also known as the\ncoefficient of parentage and is defined as the probability that a random gene from an individual\nX is identical by descent (IBD) with a random allele at the same locus from an individual Y. That\nis, for , the probability of identity by descent is presented in Equation 5.\n.\nFormula for determining IBD of genes in individuals X and Y.\nwhere:\n= the probability that alleles in X and Y are identical by descent,\n= are as defined previously.\nHistorically, this measure has been denoted or . The inbreeding coefficient of the\nprogeny is the coefficient of parentage of the parents.\nCalculations\n\u0398 = 1 means that X and Y have the same identical alleles by descent across all loci. What is\nX,Y\nanother name for this condition? (twins). \u0398 = 0 means what? Is it possible that you and I have\nX,Y\nno alleles that are identical by descent?\nThere is a relationship between F and \u0398 . In an individual, if two alleles at a single locus are\nn X,Y\nidentical by descent, then this is a special case of \u0398 , where X and Y are the same individual,\nX,Y\ni.e., F = \u0398 . To return to the original question \u201cWhat if two homozygous parents of an F used\nX X,X 1\nto create an F population are related?\u201d The relationship is represented by Equation 6.\n2\n.\nFormular for calculating the coefficient of inbreeding in generation n.\nwhere:\n= the coefficient of parentage of the parents. CHAPTER 3: RESEMBLANCE BETWEEN RELATIVES | 33\nInbreeding Coefficient\nConsider the following pedigree:\nIndividual Z has the following probabilities of containing the various alleles (Equation 7):\n,\nFormula for the probability of individual Z containing various combinations of\nalleles.\nwhere:\nand are as defined previously.\nExample Calculations\nWhat is the probability that the two mating gametes A1A2 x A3A4 at locus A are identical by\ndescent in the F ? Assume here that the two parents are not related; that is,\n1\n.\n.\n.\nThe probability that the gametes are identical by descent in the F = 0.5\n1\nSelf Pollination\nThe relationship expressed in Equation 7 can be applied in determining the coefficient of\ninbreeding following self-pollination as in Equation 8. That is, .\n,\n,\n, 34 | CHAPTER 3: RESEMBLANCE BETWEEN RELATIVES\n,\n.\nAlternative formula for calculating Fz.\nwhere:\n= the coefficient of inbreeding of individual X.\nPanmictic Index\nPanmictic Index, P, is the probability that two alleles at a locus are not IBD and is related to F\nas, P = 1 \u2212 F. We can use the known equations and relationships in the earlier section and with\nth\nsubstitution of P for the n generation we can calculate P using Equation 9.\n,\n,\n,\n,\n.\nFormula for calculating the panmictic index.\nwhere:\n= the panmictic index in generation n,\n= the panmictic index of alleles in z.\n= the panmictic index in generation 0,\n= the panmictic index of alleles in X.\nFor diploids this is also the percent of heterozygotes at a locus CHAPTER 3: RESEMBLANCE BETWEEN RELATIVES | 35\nFull-Sib Mating (1)\nFig.3 and schematic of Full-sib mating design, from generation n to generation n+ 2\nFig. 3 Full-sib mating design.\nProbability that A & B both receive e from X =\nProbability that A & B both receive f from X =\nProbability that e and f are not IDB =\nProbability that A & B contain an identical allele from X by chance (given that e and f are not IBD)\n= 36 | CHAPTER 3: RESEMBLANCE BETWEEN RELATIVES\nFormula for calculating probability that an allele in A and B form parent X is\nIBD knowing that alleles e and f in X are not IBD.\nwhere:\n= the coefficient of inbreeding of X, i.e., the probability that e and f are IBD.\nFull-Sib Mating (2)\nProbability that e and f are identical =\nProbability A & B receive an identical allele from X (given that e and f are IBD) = 1\nTotal probability that A & B receive an identical allele from X, and from Y can be determined\nusing Equation 11.\nFormula for calculating the probability that progenies A and B received identical\nalleles from parents X and Y,\nwhere:\n= the probability of identical alleles from either parent,\n= the coefficient of inbreeding of x,\n= the coefficient of inbreeding of y.\nFull-Sib Mating (3)\nLet us consider full-sib mating where the relationship between genes in offspring from the two\nparents can be represented by Equation 12.\nFormula for calculating the probability that a gene from parent X to progeny A,\nand from parent Y to progeny B are IBD. CHAPTER 3: RESEMBLANCE BETWEEN RELATIVES | 37\nwhere:\nare as described below.\nProbability that a gene from X to A and one from Y to B are IBD is .\nProbability that a gene from Y to A and one from X to B are IBD is .\nFull-Sib Mating (4)\nThe relationship between X and Y, rxy, could be zero if the reference population from which\nX and Y are sampled is considered to be random mating and large. Note that if the population\nis random mating but is not large, then the relationship coefficient may not be zero. The\nrelationship can be represented as in Equation 13.\nFormula for calculating the relationship between two parents.\nwhere:\nhave been defined previously.\nIn the form represented in Equation 13, notice that the relationship between X and Y is equal to\nth\nthe average relationship in the n generation of random mating and the series of equations in\nEquation 14 shows the relationships in progression. 38 | CHAPTER 3: RESEMBLANCE BETWEEN RELATIVES\nFormula for calculating the relationship between two parents in different\ngenerations of random mating.\nwhere:\nare as defined previously.\nSelf-Pollination\nAssume original population is non-inbred (by definition = F ). Using Equation 9 the relationship\n2\nbetween the panmictic index and the coefficient of relationship can be calculated for different\ngenerations of self pollination assuming the original population is non-inbred (by definition = F2)\n(Table 2). Where:\n.\nTable 2 Relationship between panmictic index (P) and coefficient of\nrelationship (F) with self-poolination and F as original population.\n2\nGeneration P F\n0 1.00000000 0.00000000\n1 0.50000000 0.50000000\n2 0.25000000 0.75000000\n3 0.12500000 0.87500000\n4 0.06250000 0.93750000\n5 0.03125000 0.98437500\n6 0.01562500 0.98437500\n7 0.00781250 0.99218750\n8 0.00390625 0.99609375\n9 0.00195313 0.99804688\n10 0.00097656 0.99902344\n0.00000000 1.00000000 CHAPTER 3: RESEMBLANCE BETWEEN RELATIVES | 39\nFull-Sibing\nIn a similar manner, the relationship between the panmictic index and the coefficient of\nrelationship can be calculated for different generations for full-sib mating situation as shown in\nTable 3, where:\nTable 3 Relationship between panmictic index (P) and coefficient of\nrelationship (F) with full-sib mating and F as original population.\n2\nGeneration P F\n0 1.00000000 0.00000000\n1 1.00000000 0.00000000\n2 0.75000000 0.25000000\n3 0.62500000 0.37500000\n4 0.50000000 0.50000000\n5 0.40625000 0.59375000\n6 0.32812500 0.73437500\n7 0.26562500 0.78515625\n8 0.21484375 0.78515625\n9 0.17382813 0.82617188\n10 0.14062500 0.85937500\n0.00000000 1.00000000\nReferences\nEmik, l. O., and C. E. Terrill. 1949. Systematic procedures for calculating inbreeding coefficients.\nJ. Heredity, 40 (2): 51\u201355.\nCockerham, C. C. 1971. Higher order probability functions of identity of alleles by descent.\nGenetics 69:235\u2013246. 40 | CHAPTER 3: RESEMBLANCE BETWEEN RELATIVES\nHow to cite this chapter: Beavis, W., K. Lamkey, and A. A. Mahama. 2023. Resemblance Between\nRelatives. In W. P. Suza, & K. R. Lamkey (Eds.), Quantitative Genetics for Plant Breeding. Iowa State\nUniversity Digital Press. CHAPTER 4: MEASURES OF SIMILARITY | 41\nChapter 4: Measures of Similarity\nWilliam Beavis; Mark Newell; and Anthony Assibi Mahama\nIn an ideal reference breeding population, there is no structure consisting of sub-populations\nor aggregates of relatives organized into families and tribes. Plant Breeding populations, on the\nother hand, are organized into sub-populations. Perhaps the best-known example is represented\nby the heterotic germplasm pools in maize, e.g., Stiff Stalks, Non-Stiff Stalks, Lancasters, and\nIodents. In cytoplasmic male sterile hybrid systems such as sorghum, the restoration pattern\ncan be the primary divider of germplasm with additional subdivisions based on morphological\ncharacteristics and geographic origins, e.g., Kaoliang, Durra, and Feterita. Alternatively,\ncoefficients of relationship and inbreeding among members of a breeding population can be\nused to represent the structure of the breeding population. Also, with the emergence of high\nthroughput molecular marker technologies, it is possible to represent relationships among\nmembers of a breeding population using identity in state to produce a realized kinship matrix.\nLearning Objectives\n\u2022 Utilize coefficients of inbreeding and parentage to construct the numerator relationship matrix\n\u2022 Utilize molecular marker information to construct a realized kinship matrix\nPopulation Structure Based on Pedigree Information\nAnimal breeders were the first to utilize relationships among individuals for the purpose of\nproviding Best Linear Unbiased Predictions in linear mixed models. The \u201cA\u201d matrix in the linear\nmixed model equation, also known as the Numerator Relationships Matrix (NRM) was originally\nused by Henderson to capture information from relatives to predict breeding values of animals.\nIn essence, the A-matrix provides information on the proportion of alleles that are identical by\ndescent between all pairs of individuals in a breeding population.\nSpecifically, the numerator relationships are equal to twice the coefficient of coancestry between\nany pair of individuals. In other words, . Thus, if we know the pedigrees of\nall members of a breeding population, we can construct an A-matrix using a recursive tabular\nmethod. 42 | CHAPTER 4: MEASURES OF SIMILARITY\nRecursive Tabular Method\nA recursive tabular method for constructing the A-Matrix is described below:\n1. Order members of a pedigree chronologically, i.e., list parents before offspring. Assume that\nfounder lines are not inbred and are not related to each other.\n2. Transpose the list and use this to represent columns for the matrix.\n3. Beginning with the cell represented by compute .\n4. Move to cell and compute . This will be the same value that can be used for cell\n5. Move to cell and compute .\n6. Move to cell and compute . This will be the same value for\n7. Move to cell and compute . This will be the same value for\n8. Move to cell and compute\n9. Repeat until all elements of the matrix are completed.\nPopulation Structure Based on Markers\nThe Realized Kinship Matrix\nConsider two cultivars scored for 1400 SNPs. We can ask whether this pair of cultivars has the\nsame or different alleles at each locus. Intuitively, if they had the same allele at all 1400 loci, we\nwould say that there are no detectable allelic differences between the two genotypes, i.e., that\nthey are identical in state or that their similarity index = 1.0. Alternatively, if none of the alleles\nare the same at all 1400 loci, then we would say that the genotypes have no alleles in common,\ni.e., that their similarity index is zero. In practice, the two genotypes will exhibit a measure of\nsimilarity somewhere between these extremes.\nQuantitative Measure for Similarity\nLet us take this intuition and develop a quantitative measure for similarity. If the two cultivars (x\nand y) have the same pair of alleles at a locus, score the locus = 2; if one of the alleles is the same,\nscore the locus = 1; otherwise, the score = 0. If we sum these up across all loci, the maximum\nscore would be 2800. If we divide the summed score by 2800, we would obtain a proportion\nmeasure (designated ) to quantify the similarity between the pair of lines. This concept can\nbe represented algebraically as: CHAPTER 4: MEASURES OF SIMILARITY | 43\nFormula for calculating the similarity between pairs of lines.\nwhere:\n= the number of loci,\n= the two cultivars.\nSuch a similarity measure could be converted into an \u201cintuitive genetic distance\u201d measure by\nsubtracting from 1.\nMeasures of Distance\nOur intuitive genetic distance would make sense if:\n1. There are only two alleles per locus.\n2. Our interpretation of the result does not include inferences about identity by descent, and\n3. There is no LD among the SNP loci.\nHowever, most populations are more complex, requiring more nuanced measures of genetic\ndistance. Population geneticists tend to use three distance measures depending upon the\ninference about the population structure they are trying to understand. These are:\n\u2022 Nei\u2019s Distance assumes all loci have the same neutral rate of mutation, mutations are in\nequilibrium with genetic drift, and the effective population size is stable. The interpretation\nis a measure of the average number of changes per locus and that differences are due to\nmutation and genetic drift.\n\u2022 Cavalli-Sforza\u2019s Distance assumes differences are due to genetic drift between populations\nwith no mutation and interprets the genetic distance as an Euclidean Distance metric.\n\u2022 Reynolds Distance is applied to small populations; thus, it assumes differences are due to\ngenetic drift and is based on knowledge about coancestry, i.e., identity by descent for alleles\nthat are the same.\nApplication of Distance and Similarity Measures\nThere are a large number of additional distance and similarity measures that can be applied\nto molecular marker scores, including Euclidean, Mahalanobis, Manhattan, Chebyshev, and 44 | CHAPTER 4: MEASURES OF SIMILARITY\nGoldstein. Also, Bayesian Statistical approaches can be used to identify structure in the\npopulation (Pritchard et al, 2000) without resorting to the calculation of distance metrics. The\nchoice of an appropriate method depends upon the type of molecular marker data and the\nresearch question. A thorough presentation of distance measures is beyond the scope of this\ncourse, but there are graduate courses on multivariate statistics in which issues associated with\neach of the distance metrics can be explored.\nFor now, let us assume that we decided to use our to represent differences between all pairs\n( ) of breeding lines. Next, suppose we extend the example from two lines to 1800 lines scored\nfor 1400 SNPs. In this case, there are = = 1,619,100 estimates\nof pairwise distances among the lines.\nClearly, any attempt to find patterns in a data matrix consisting of all pairwise measures of\nsimilarity or distance will take considerable effort. Yet, these patterns in the data are essential\nto quantifying the structure in a breeding population because the structure will affect inferences\nabout genetic effects. It is the need to find patterns in such large data sets that motivated the\napplication of multivariate statistical methods such as principal components and cluster analyses\nin plant breeding populations.\nPrincipal Component Analysis\nThe primary purpose for applying principal component analysis (PCA) to genetic distance\nmatrices is to summarize, i.e., reduce dimensionality so that the underlying population structure\ncan be visualized.\nFig. 1 Effect of principal component analysis. CHAPTER 4: MEASURES OF SIMILARITY | 45\nConceptual Interpretation\nImagine we have two variables, denoted x1 and x2, where x1 represents the distance scores\nbetween cultivar 1 and all other cultivars, and x2 represents the distance scores between cultivar\n2 and all other cultivars. If we plot the x1 and x2 pairs of data, we might generate a plot such\nas seen in Fig. 1A. We could add distance data for a third cultivar and represent the data with a\n3-dimensional plot. We could obtain data for as many cultivars as we might have interest in, but\nthe ability to plot these in multi-dimensional space is not possible.\nFig. 2 Effect of principal component analysis with the first PC.\nWe refer to the first principal component (PC), also known as the first eigenvector, as a line (red)\nthat minimizes the perpendicular distances (blue line) between the red line and the data points\n(Fig. 2A).\nPrincipal Component Analysis \u2013 Interpretation\nThe second PC follows the same definition except that it represents a line through the data that\nminimizes the distance between a second line that is orthogonal (at a right angle) to PC1. The\nsecond PC minimizes the distance between the data and the second line. Since the second PC is\northogonal to the first, the distance among the data points represented by each PC is maximized.\nThus we can plot data points represented by the first two principal components (Fig. 3B). By\nplotting the PCs instead of the raw data, we often find hidden structures in the data (compare\nFig. 3A vs. 3B). 46 | CHAPTER 4: MEASURES OF SIMILARITY\nFig. 3 Effect of principal component analysis with second PC.\nSubsequent PCs represent lines that are orthogonal to all previous PCs and minimize the distance\nbetween each PC and data points that maximize the variability among the orthogonal PCs. This\nmeans that each PC is uncorrelated to all other PCs.\nA useful measure in PCA is the eigenvalue associated with each eigenvector (PC). The first\neigenvalue is the proportion of maximum variability among the multidimensional data that is\nexplained by the first PC. For the data depicted in Fig. 3B, the first eigenvalue is 0.997, and the\nsecond eigenvalue is equal to 0.003. Since the first PC is the vector (or line) that is plotted in the\ndirection of maximum variability among data points, the first eigenvalue is always the largest,\nand each consecutive eigenvalue accounts for less variability than the prior PCs. CHAPTER 4: MEASURES OF SIMILARITY | 47\nPCA Example\nFig. 4 Four distinct clusters produced by\nPCA.\nLet us consider an example from a set of 1816 barley lines scored for 1416 SNPs (Hamblin\net al. 2010). In this analysis, there were\nestimates of pairwise\ndistances based on 1416 SNP scores for each of the barley lines. Eigenvalues for PC1 and PC2\naccounted for 24.5% and 10.1% of the variability among pairwise genotypic distances. By plotting\nPC1 versus PC2 (Fig. 4), we observe four distinct clusters. Subsequent analyses of the lines\nrepresented by each point in the clusters revealed that the members of each cluster are from\n2-row, 6-row, spring, or winter barley types. From a breeding perspective, we can see that most\nbreeding for barley occurs within types rather than between types. The population structure is a\nresult of breeding processes of selection, drift, and non-random mating.\nCluster Analysis\nSimilar to PCA, the purpose of applying cluster analysis to matrices of pairwise distance\nmeasures among a set of genotypes is to segregate the observations into distinct clusters. There\nare many types of cluster analyses, and a primary distinction is between supervised and non-\nsupervised clustering. K-means is one of the supervised methods that have been widely adopted\nby plant population geneticists. The clustering method is supervised in the sense that K\nrepresents a pre-determined number of clusters. Designating the number of clusters is usually 48 | CHAPTER 4: MEASURES OF SIMILARITY\nbased on prior knowledge about groups of lines that are being clustered. For example, it might\nmake sense to designate the four clusters of barley lines based on known breeding history in\nwhich different barley agronomic types are not inter-mated. K-means represents an iterative\nprocedure with the following steps:\n1. An initial set number of K means (seed points) are determined (also called initialization);\nthese are the initial means for each of the K clusters.\n2. Each genotype is then assigned to the nearest cluster based on its pairwise distances to all\nother genotypes within and among clusters.\n3. Means for each cluster are then re-calculated, and genotypes are re-assigned to the nearest\ncluster.\n4. Steps ii and iii are then repeated until no more changes occur.\nCluster Analysis Example\nFig. 5 PCA-produced k-means.\nFor the barley data, since the inter-mating rule is not absolute, i.e., some agronomic types are\noccasionally inter-mated, it could be informative to designate K = 6 (Fig. 5). Note that a plot of\nPC1 vs PC3 (Fig. 5B) demonstrates the value of plotting PCs beyond the first two. While the third\nPC accounts for only 4.5% of the variability among genotypes, the third PC helps to distinguish\nwhat appears to be members of the same cluster in Fig. 5A. CHAPTER 4: MEASURES OF SIMILARITY | 49\nHierarchical Clustering\nFig. 6 Dendogram observations data\nAn unsupervised approach to clustering genotypic distance data is hierarchical clustering. This\napproach sequentially lumps or splits observations to make clusters. Applying the hierarchical\napproach to the barley data set, we can visualize the results using a dendrogram (Fig. 6). In\nthe dendrogram, observations are arrayed along the x-axis, and the y-axis refers to the average\ngenetic distance between breakpoints. For example, the horizontal line at 4e+05 indicates that\nthere are two major groups with a distance between them of 4e+05. The user determines the\nheight (distance along the y-axis) at which a horizontal line is drawn, and the number of clusters\nis chosen; this is drawn below in red for 6 clusters. The user may determine this by using the PC\nplots, cluster dendrogram, and any prior information that is known about the germplasm.\nHierarchical clustering can be implemented in many different ways. For genotypic data, the\nmost common method is Ward\u2019s, which attempts to minimize the variance within clusters and\nmaximize the variance between clusters. Similar to K-means clustering, we can look at the PC\nplots to explore the results for hierarchical clustering to see how the lines were assigned to\nclusters. 50 | CHAPTER 4: MEASURES OF SIMILARITY\nHow to cite this chapter: Beavis, W., M. Newell, and A. A. Mahama. 2023. Measures of Similarity. In\nW. P. Suza, & K. R. Lamkey (Eds.), Quantitative Genetics for Plant Breeding. Iowa State University Digital\nPress. CHAPTER 5: GENE EFFECTS | 51\nChapter 5: Gene Effects\nWilliam Beavis; Kendall Lamkey; Katherine Espinosa; and Anthony Assibi Mahama\nIn 1918, RA Fisher provided the first major contribution to the Modern Synthesis by proposing\na model that reconciled the inheritance of discrete characteristics (Mendel) and continuous,\nor quantitative, characteristics (Darwin) in breeding populations. Herein, the same theoretical\nfoundations are introduced.\nFor the beginning student, it will seem that the primary purpose of theory and modeling is\nto provide interpretations of observational and experimental results. Without this theoretical\nfoundation, there would be no genetic understanding of the results from plant breeding\nexperiments.\nHowever, there is a more important practical justification for learning theoretical models: Theory\nprovides predictions. Predictions are the basis for generating testable hypotheses. Also, with a\ntheoretical model, it is possible to simulate many different breeding strategies. These can be\ncompared, and the most promising can be used to design and implement the most effective\nand efficient breeding strategies. Thus, the theory provides a rational basis for designing plant\nbreeding programs.\nLearning Objectives\n\u2022 Model Genotypic and Phenotypic Values of individuals in crop breeding populations.\n\u2022 Integrate genotypic effect models with allele frequency models at single and multiple loci.\n\u2022 Distinguish and estimate genetic effects, effects of allele substitutions, and Breeding Values at\nsingle and multiple loci.\n\u2022 Distinguish and estimate dominance and epistatic deviations from additive effect models.\n\u2022 Integrate concepts to applied breeding programs with data sets consisting of genotypic (marker)\ninformation with phenotypic information for QTL analyses.\nLinear Models for Phenotypic Values\nSingle Locus\nThe phenotypic value of an individual, or group of individuals, is observed when a character or 52 | CHAPTER 5: GENE EFFECTS\ntrait is measured. For example, if a corn plant was measured and found to be 275 cm tall, then\nthat would be its phenotypic value for height.\nTo draw inferences about the genetic properties of a trait, we model phenotypic values using\nlinear components. The most common model consists of a part due to genetics and a part due to\nnon-genetic effects such as the environment. This is usually written as:\n,\nLinear model for evaluating phenotype.\nwhere:\nis the phenotypic value,\nis the genotypic value, and\nrepresents the non-genetic factors.\nIf we assume that , then , and .\nPopulation Mean\nThe mean phenotypic value of a population is equal to the mean genotypic value when the non-\ngenetic (environmental) deviations sum to zero.\nTo calculate the expected genotypic properties of a population for a single locus, we assign\narbitrary genotypic values to each locus.\nConsider a single locus with two alleles and .\nCoded genotypic value of one homozygote = .\nCoded genotypic value of the other homozygote =\nCoded genotypic value of a heterozygote = .\nWe can arbitrarily designate the A allele as the allele that increases the genotypic value. The\ngenotypic value of the heterozygotes (d) depends on the level of dominance: CHAPTER 5: GENE EFFECTS | 53\nDegree of Dominance\n1. No dominance when d = 0 (Fig. 1).\nFig. 1 Line diagrammatic representation of no dominance.\n2. If is dominant or partially dominant relative to the allele, then d is positive towards the\nAA genotype, as shown in Fig. 2.\nFig. 2 Line diagrammatic representation of dominance or partial dominance of A\nover a allele.\n3. If a is dominant or partially dominant to the A allele, then d is negative (Fig. 3).\nFig. 3 Line diagrammatic representation of dominance or partial dominance of a\nover A allele.\n4. If dominance is complete: d = +a or -a\nFig. 4 Line diagrammatic representation of complete dominance of A over a allele\nor a over A allele.\n5. If there is overdominance: d is greater than +a or less than -a 54 | CHAPTER 5: GENE EFFECTS\nFig. 5 Line diagrammatic representation of overdominance.\nAllele Frequencies and Population Mean (Table 1)\nTable 1 Influence of allele frequencies and dominance deviation on the average\nvalue of the trait in the population.\nn/a Genotype\nn/a AA Aa aa Total\nFrequency 1\nGenotypic\nn/a\nValue\nCoded GV n/a\nFreq. x\nCoded GV\nTable 2 An example of a genotype that controls the number of flowers and the\nexpected population value for number of flowers.\nn/a Genotype n/a\nn/a AA Aa aa Total\nFrequency\nGenotypic Value n/a\nCoded GV n/a\nFreq. x Coded GV\nNote: Coded Genotypic Values are obtained by subtracting the mid-parent value i.e., the\nmidpoint between the genotypic values of the two homozygotes (Table 2).\nPopulation mean , and\n.\nThe population mean is estimated using Equation 2.\n. CHAPTER 5: GENE EFFECTS | 55\nFormula for calculating population mean.\nwhere:\n= frequency of A allele,\n= frequency of a allele,\n= coded genotypic values of AA, aa genotypes,\n= coded genotypic value of Aa genotype.\nAdditive Gene Action\nApplying Equation 2 and the data in Table 2, the population mean is calculated as shown below.\nis both the mean genotypic value and the mean phenotypic value of the\npopulation with respect to the trait.\nNotice that if d = 0, the heterozygote genotype has no impact on the population mean, and we say\nthat completely additive gene action exists.\nTwo Loci\nNext, consider the contributions of alleles at more than one locus and find the joint effect on the\nmean (Table 3).\nConsider two single loci:\n\u2022 Genotypic value of is\n\u2022 Genotypic value of is 56 | CHAPTER 5: GENE EFFECTS\nConsider multiple loci:\n\u2022 Genotypic value of is .\n\u2022 Total genotypic value is = .\n\u2022 The mid-homozygote genotypic value is the average of double homozygotes\n.\nTable 3 Joint effects of coded genotypic values and frequencies of alleles at two loci.\nA locus genotype\nTwo-locus genotypic values\nand frequencies\nAA Aa aa\nB locus Coded Genotypic\ngenotype Value/Freq.\nBB\nBb\nbb\nPopulation Mean\nPopulation mean, = + Expected values of G and G\nA B\nG and G are weighted averages based on allele frequencies and coded genotypic values. The\nA B\npopulation mean is then represented by Equation 3.\nAlternative formula for calculating population mean.\nwhere:\n= weighted average of A allele,\n= weighted average of a allele, CHAPTER 5: GENE EFFECTS | 57\n= expectation of sum of the two values,\nare as defined previously.\nA numerical example is given in Table 4.\nTable 4 A numeric example involving dominance and allele frequencies that are\nnot equal at two loci.\nA locus genotype\nTwo-locus genotypic values and frequencies\nAA Aa aa\n8 4 -8\nB locus genotype Boded Genotypic Value/Freq.\n0.64 0.32 0.04\n4 12 8 -4\nBB\n0.04 0.0256 0.0128 0.0016\n2 10 6 -6\nBb\n0.32 0.2048 0.1024 0.0128\n-4 4 0 -12\nbb\n0.64 0.4096 0.2048 0.0256\nAverage at locus A 6.24 2.24 -9.76\nAverage at locus B 10.08 8.08 2.08\nExtension To More Than 2 Loci\nWe can extend the concept discussed above for two-locus case to more than two loci and calculate\nthe population mean in a similar manner; where\n\u2022\n\u2022 midpoint is the average of the most extreme multi-locus-homozygotes\n\u2022 Population mean = , is represented by Equation 4 58 | CHAPTER 5: GENE EFFECTS\nFormula for calculating population mean involving more than two loci.\nwhere:\n= expectation of the sum of all G values,\nare as defined previously.\nAverage Genetic (Allelic) Effects\nIndividuals chosen as parents transmit only a sample consisting of \u00bd of its alleles. With selection,\nwe are concerned with the transmission of value from parent to offspring. This cannot be\ndetermined based on genotypic value alone. Parents pass on their genes or alleles, NOT their\ngenotypes, to the next generation. Genotypes are created anew in each generation. One result is\nthat some aspects of the value of a particular genotype are unpredictable. Yet, selection theory\ncan work only with the predictable aspects of the union of two gametes. Therefore, we introduce\nthe average effect of a gene (allele) to represent this concept.\nFig. 6 Average effects of alleles on populations. CHAPTER 5: GENE EFFECTS | 59\nKey Concept: Although genotypes determine genotypic values, alleles and not genotypes are inherited\nby progeny.\nAverage effect of a gene (allele): The mean deviation from the population mean of individuals who\nreceived the gene (allele) from one parent is the average effect of the gene (allele). The effect of the\nother gene (allele) received from the remaining parent is represented as a random allele from the\npopulation \u2026 for this concept.\nFormula of Average Effect of an Allele\nConceptually, let a number of gametes carrying the A allele unite at random with gametes from\nthe population; then, the mean of the genotypes deviates from the population mean by an amount\nthat is the average effect of the A gene. This represents the average allele effect and is the average\ndeviation from the population mean of individuals who received a specific allele from one parent\nand the other allele at random from the population.\nTable 5 Average effect of an allele.\nGenotypes, coded Mean value of\nPopulation mean to Average effect of the\nAlleles genotypic values and genotypes\nbe deduced allele ( )\nfrequencies produced\nn/a next gen this gen (next gen) \u2014 (this gen)\nThe average effect of the A allele (or the \u03b1 allele) from a single locus is designated as \u03b1 (or \u03b1 ) and\nA a\ncalculated for data presented in Table 5 as:\nAllele Substitution Effect\nThe average effect of an allele substitution, often designated as , is the difference between the\naverage effects of each allele (Equation 5). 60 | CHAPTER 5: GENE EFFECTS\nFormula for calculating the average effect of allele substitution.\nwhere:\n= the average effects of allele A and a, respectively,\nare as defined previously.\nFor example, from Table 5, and represent average allele effects of and , respectively.\nAverage effects of each allele can be calculated as:\nThus, the average effect of allele substitution =\nNote the average genetic effect is:\n\u2022 Dependent on genotypic value,\n\u2022 Dependent on gene frequencies,\n\u2022 A property of the population as well as the genes concerned.\nBreeding Value\nBreeding value is a concept that is based on the following:\n\u2022 The average value of a parent is judged by its progeny.\n\u2022 Alleles carried by an individual and transmitted to its offspring can be inferred from the\nprogeny,\n\u2022 Which represents the sum of the average effects of all alleles an individual carries.\nLet us use the average effect of alleles to rewrite the Equation 1 as:\nAlternative formula for calculating phenotypic value based on average effect of\nalleles.\nwhere: CHAPTER 5: GENE EFFECTS | 61\n= the average effects of allele i, j in a diploid individual i, j, respectively,\nis the dominance deviation.\nBreeding value is the value of an individual judged by the average value of its progeny. The\nbreeding value of an individual is equal to the sum of the average effects of the alleles it carries.\nThe summation is over pairs of alleles at a locus and over all loci (Table 5). It is defined as twice\nthe expected deviation of the individual\u2019s progeny mean from the population mean when the\nindividual is mated at random to other individuals from the same population (Tables 6 and 7).\nMean Breeding Value in Random Population\nThe mean breeding value in a random mating population is zero.\nTable 6 Relationship of breeding values to genotypes.\nGenotype Breeding value\nTable 7 Theoretical example of calculations of breeding values.\nGenotype Breeding value\nDeviations for Average Genetic Effects\nDominance Deviation\nFor a single locus: the difference between the genotypic value and the breeding value of a\nparticular genotype is known as the dominance deviation. It is associated with the genotype.\nrepresents the deviation of genotypic value (i.e., ) from the regression-fitted genotypic\nvalue and is zero when dominance is absent ( )\nConsider Genotype . Recall that the Coded genotypic value of = , and the population 62 | CHAPTER 5: GENE EFFECTS\nmean equals .\nIf a is expressed as deviation relative to the population mean, then is can be calculated using\nEquation 7.\nAlternative formula for calculating average effect of allele substitution.\nwhere:\n= the average effect of allele substitution,\nare as defined previously.\nNotice that if d is not 0, and p is not equal to q, then a is affected by d. Also, recall that a can be\nexpressed in terms of the average effect of an allele substitution (Equations 8 and 9), where terms\nare as defined previously,\nAlternative formula for calculating average effect of allele substitution.\nThus\nAlternative formula for calculating average effect of allele substitution.\nUsing similar algebra, the dominance deviation is represented by Equation 10 as,\nAlternative formula for calculating the dominance deviation.\nObservations About Dominance\nNotice that\n\u2022 If there is no dominance, d is zero, and the dominance deviations are also zero.\n\u2022 In the absence of dominance, breeding values and genotypic values are the same.\n\u2022 Alleles involved with genotypes that show no dominance, i.e., d = 0, are sometimes called\n\u2018additive genes\u2019, or are said to \u2018act additively\u2019. CHAPTER 5: GENE EFFECTS | 63\nBreeding Values and Dominance Deviations\nFig. 7 Breeding values and dominance deviations.\nAs shown in Equation 6 the algebra of breeding values and dominance deviations provides the\ntheoretical basis for subdividing the G component of the Phenotypic model,\nThus, based on the algebra and substituting data from Table 7, dominance deviation is calculated\nas shwon below. 64 | CHAPTER 5: GENE EFFECTS\nEpistasis\nEpistasis exists when genotypes at two or more loci result in a genotypic value that is greater or\nless than the sum of the average genotypic effects at each of the individual loci. For example,\nTable 8 Two-locus genotypic values that do\nnot exhibit epistasis. The total genotypic\nvalue is the sum of the individual locus\ngenotypic values.\nGenotype at Locus A Genotype at Locus B\nn/a BB Bb bb\nAA 22 18 6\nAa 20 16 4\naa 14 10 -2\nTable 9 Two-locus genotypic values that\nexhibit epistasis. The total genotypic value is\nNOT equal to the sum of the genotypic values\nat the loci (G + G ).\nA B\nGenotype at Locus A Genotype at Locus B\nn/a BB Bb bb\nAA 24 18 6\nAa 20 16 4\naa 14 10 -2\nGraphical View of Epistasis\nEpistasis between loci within an individual can be represented as the reaction norm of different\ngenotypes at one locus, plotted against the genotypes at a second locus.\nFigure 8A: Epistasis between loci occurs because the reaction norms of the locus B genotypes\ndiffer in slope.\nFigure 8B: The reaction norms are parallel; thus, the effects of the two loci are independent, and\nno epistasis is present. CHAPTER 5: GENE EFFECTS | 65\nFig. 8 Graphs of epistasis (A) and no epistasis (B).\nPhysiological and Statistical Dominance\nCheverud and Routman (1995) identified two concepts associated with the term epistasis:\nphysiological epistasis and statistical epistasis.\nThe distinction between these two concepts is similar to that made between physiological and\nstatistical dominance:\nPhysiological Dominance:\n\u2022 Heterozygote is not midway between two homozygotes.\n\u2022 Values of a and d are not dependent on allele frequencies.\n\u2022 When d \u2260 0, it reflects intralocus interaction is present.\n\u2022 Least-squares solution of the unweighted regression of the number of genotypic values on\nthe number of \u201ca\u201d alleles.\n\u2022 Physiological dominance contributes to both additive and dominance values and variances.\nStatistical Dominance Deviations:\n\u2022 Deviations of single-locus from the additive combination of alleles contribute to the\ngenotype.\n\u2022 Depend on allele frequencies and will change with changes in allele frequencies.\n\u2022 Least-squares solution of a weighted (weighted by genotypic frequencies) regression of\ngenotypic value on the number of alleles. 66 | CHAPTER 5: GENE EFFECTS\nPhysiological Epistasis\nIn physiological epistasis (or mechanistic epistasis):\n\u2022 Interaction effects occur \u201cwithin\u201d genotypes, where genes expressed within a single\ngenome interact.\n\u2022 Simply recognizes that certain genotypes at two or more loci interact in the production of a\nphenotype.\n\u2022 If all possible genotypic classes are equally frequent in a population, the influence of\ngenetic interactions on phenotypes will be directly observable.\n\u2022 The contribution of physiological epistasis to populations is a function of the frequencies of\ninteracting genotypes in a population.\nModel for Physiological Epistasis\nLet the phenotypic value of an individual be determined by the combination of the alleles present\nat two loci. This model is used to illustrate how physiologically based gene interactions map to\ncomponents of genetic variation.\nConsider the two loci, each with two alleles per locus; the two-locus (physiological) genotypic\nth\nvalues, G , are the average phenotype of individuals with the ij genotype at the first locus,\nijkl\nth\nand the kl genotype at the second locus. Notice that we are not given the genotypic values for\nthe A locus nor for the B locus. So, we will determine the unweighted marginal means for each\ngenotype.\nTable 10 Example of Epistasis using Genotypic Values.\nUnweighted\nn/a\nmarginal mean\nUnweighted\nmarginal means CHAPTER 5: GENE EFFECTS | 67\nSingle Locus Genotype\nThe single-locus genotype is defined as the unweighted average across the genotypes at the\nsecond locus, for example, Equation 11.\nAlternative formula for calculating the dominance deviation.\nwhere:\n= the single locus genotype value.\nThus, the value at locus A is,\nand at locus B is,\nNon-Epistatic Genotypic Value\nSubscripts and or and refer to the two alleles at the interacting loci. The single locus\nvalues of and are computed as in Equation 12.\nFormula for calculating the single locus values,\nwhere:\n= the single locus values.\nThus, the value at locus A is,\nand 68 | CHAPTER 5: GENE EFFECTS\nSimilarly, and can be calculated to be 4 and 2, respectively. Try it!\nThe non-epistatic genotypic value, , is calculated using Equation 13, represented by,\nFormula for calculating the non-epistatic genotypic value,\nwhere:\nare as defined previously.\nFor AABB genotype,\n.\nEpistatic Genotypic Value\nTable 11 Non-epistatic values.\nn/a\nThe epistatic genotypic value is represented by Equation 14.\nFormula for calculating the epistatic genotypic value.\nExample from data in Table 11 is,\nA value of different from zero indicates that Physiological Epistasis is present. In this\nexample, there is little evidence for epistasis for this cell. Is there evidence for epistasis in the\nother cells? CHAPTER 5: GENE EFFECTS | 69\nStatistical Epistasis\nIn statistical epistasis (or population epistasis):\n\u2022 The term is used to refer to the amount of population variation in genotypic values\nassociated with variation among loci.\n\u2022 Notation that is often used includes , or , or .\n\u2022 The amount of statistical epistasis present in a population is a function of the frequencies\nof interacting multilocus genotypes and therefore is a function of population allele\nfrequencies as it is for additive and dominance variance ( ). It is calculated\nusing Equation 15.\nFormula for calculating the statistical genotypic value.\nwhere:\n= the total epistatic (statistical) value,\n= value at locus A,\n= value at locus B,\n= value due to A by B interaction.\nPresence of epistasis between locus and changes the population mean, ( ), mid-\nhomozygote value ( ), (additive), and (dominance) values.\nTable 12 An example with Epistatic effects.\nA locus genotype\nTwo-locus genotypic values and frequencies\nAA Aa aa\nB locus Coded Genotypic Value/\ngenotype Freq\nBB\nBb\nbb\nAverage at locus A\nAverage at locus B 70 | CHAPTER 5: GENE EFFECTS\nEpistasis Effects\nThe genotypic value of AABB has been increased from 22 to 24 due to epistatic effects.\nThere are changes in population mean, mid-homozygote values for A and B locus, and the average\nat A and B locus as shown in calculations below.\nReferences\nCheverud, J. M. and E. J. Routman. 1995. Epistasis and its contribution to genetic variance\ncomponents. Genetics, 139, 1455\u00b11461.\nHow to cite this chapter: Beavis, W., K. Lamkey, K. Espinosa, and A. A. Mahama. 2023. Gene Effects.\nIn W. P. Suza, & K. R. Lamkey (Eds.), Quantitative Genetics for Plant Breeding. Iowa State University\nDigital Press. CHAPTER 6: COMPONENTS OF VARIANCE | 71\nChapter 6: Components of Variance\nWilliam Beavis; Kendall Lamkey; Katherine Espinosa; and Anthony Assibi Mahama\nThis chapter explores sources of phenotypic variation (Fig. 1), whether genetic or environmental\nand how these contribute to the heritability of selected traits. It covers the derivation of variance\ncomponents and covariance, the relationship among variance components, and the role of\nepistasis.\nLearning Objectives\nType your learning objectives here.\n\u2022 Learn to model components of genetic variances for purposes of estimating heritability.\n\u2022 Be able to explain:\n\u25e6 The impact of allele frequencies on genetic components of genotypic variability,\n\u25e6 The reason estimates of components of genetic variability are limited to the population\nfrom which they are estimated,\n\u25e6 The reason additive variance does not imply additive gene action and\n\u25e6 How additive genetic variance can arise from genes with any degree of dominance or\nepistasis. 72 | CHAPTER 6: COMPONENTS OF VARIANCE\nFig. 1. Variation in height depicted in\nhuman and corn plants. From\n\u201cCritique of the Theory of Evolution\u201d\n(1915) by Thomas Hunt Morgan,\navailable freely at Project Gutenberg.\nLicensed under Public Domain\nvia Wikimedia Commons.\nPhenotypic Components of Variance\nRecall that our working model for the phenotype includes genotypic and non-genotypic\n(environmental) sources of variability (Equation 1):\nWorking model of phenotypic, genotypic, and environmental effects,\nwhere:\n= overall mean,\n= genotypic effect,\n= environmental effect.\nThe source of phenotypic variability determines whether selection for the trait will result in a\nheritable response, i.e., will be passed on to the next generation. CHAPTER 6: COMPONENTS OF VARIANCE | 73\nFor purposes of making decisions in plant breeding, if two populations have different phenotypic\nmeans, we want to know whether the differences are due to different environments, different\ngenotypes, or some combination of both. If the differences are due to genotypic differences, then\nwhat proportion of the genotypic differences is heritable?\nFig. 2 Comparing phenotypic traits of plant populations at the\nUniversity of KwaZulu-Natal in South Africa. Photo by Iowa State\nUniversity.\nAlgebraic Description\nUsing simple algebra and our working model, we can show that the phenotypic variance V\nP\nwithin a population is equal to the sum of the genotypic variance V and environmental variance\nG\nV , assuming that V and V are independent (Equation 2);\nE G E\nWorking model of phenotypic, genotypic, and environmental variances,\nIf the genotypic values and environmental deviations are not independent, the V(P)=V(G+E), and\nV(P) can be increased by twice the covariance of G with E if they are not independent (Equation\n3):\nComposition of phenotypic variance,\nwhere:\n= joint variation between G and E. 74 | CHAPTER 6: COMPONENTS OF VARIANCE\nGenetic Components of Variance\nGenetic components of variability can be divided into several subcategories, including additive\nvariance, V , dominance variance, V , and epistatic variance, V . Together, the values for each of\nA D I\nthese subcategories yield the total amount of genetic variation, V , responsible for a particular\nG\nphenotypic trait: V = V + V .\nP G E\nConsider the ratio of V to V . This was originally recognized by statistical geneticists (such as\nG P\nRA Fisher) as the genotypic intra-class correlation. To understand this, consider the evaluation\nof a line i, for a phenotype, Y. Next, imagine that you can evaluate line i repeatedly. Let\nus designate these repeated measurements as j. We can then designate these repeated\nmeasurements of the phenotype as Y . There is a Covariance among these repeated evaluations\nij\nthat we can represent as .\nExplanation of Formula\nThus the correlation among these repeated measures is (Equation 4)\n,\nCorrelation among repeated measures,\nwhere:\n= correlation between Y and Y ,\nij ij\u2019\n= covariance between Y and Y ,\nij ij\u2019\n& = variance ofY and Y .\nij ij\u2019\nBecause , the correlation is represented by Equation 5.\nCorrelation among repeated measures,\nwhere:\n= correlation between Y and Y ,\nij ij\u2019\n= genotypic variance of genotype i, CHAPTER 6: COMPONENTS OF VARIANCE | 75\n= variance of environment j for genotype i,\n= phenotypic variance of genotype i in environment j.\nHeritability in the Broad Sense\nBroad sense heritability is estimated by Equation 6.\nFormula for estimating broad sense heritability,\nwhere:\n= the total genetic variance,\n= the phenotypic variance.\nJL Lush, an animal breeder, also referred to this intra-class correlation coefficient as heritability\nin the broad sense (1937). He wanted to distinguish the application of intra-class correlation\nto animals from the concept of repeatability. Repeatability as an engineering concept refers\nto the same measurement procedure conducted by a single observer, using a single measuring\ninstrument, under the same conditions, at a single location, over a short period of time. As\na result, plant and animal breeders tend to prefer the use of broad sense heritability for the\ngenotypic intra-class correlation, although both plant and animal breeders routinely evaluate a\nsingle trait on individual genotypes repeatedly over time and space (locations and years).\nBroad-Sense Components\nThe genetic variance can be recognized as consisting of several components (Equation 7):\nComposition of total genotypic variance,\nwhere:\n= total genotypic variance,\n= additive genetic variance, that is, the variance of breeding values, and refers to the\ndeviation from the mean phenotype due to inheritance of a particular allele and this allele\u2019s\nrelative effect on phenotype, i.e., relative to the mean phenotype of the population,\n=dominance variance due to interactions between alternative alleles at a specific locus,\n= epistatic variance due to interaction between alleles at different loci. 76 | CHAPTER 6: COMPONENTS OF VARIANCE\nHeritability in the Narrow Sense\nHeritability in the narrow sense was defined by JL Lush (1937) to represent the extent to which\nphenotypes are determined by the genes transmitted from their parents (Equation 8):\nComposition of total genotypic variance,\nwhere:\n= heritability in the narrow sense,\n= additive genetic variance, that is, the variance of breeding values, and refers to the\ndeviation from the mean phenotype due to inheritance of a particular allele and this allele\u2019s\nrelative effect on phenotype, i.e., relative to the mean phenotype of the population,\n= phenotypic variance.\nSo, we can now expand our model for the phenotypic variance to include several genetic variance\ncomponents and environmental variance as in Equation 9.\nComposition of total genotypic variance,\nwhere:\n= environmental variance. Other variables are as described previously.\nTable 1 Variance components and sources of variation.\nVariance component Symbol Source of variation\nPhenotypic Phenotypic value\nGenotypic Genotypic Value\nAdditive Breeding Value\nDominance Dominance deviation\nInteraction Interaction deviation\nEnvironmental Non-genetic deviation CHAPTER 6: COMPONENTS OF VARIANCE | 77\nDeriving Variance Components\nThe genetic components of variance are influenced by the gene frequency and the assigned\ngenotypic values and . The information needed to derive and are:\nTable 2 Derivation of additive and dominance variance components of genetic variance.\nGenotypes\nFrequencies\nCoded GV\nGenotypic Value\nBreeding Value\nDominance Deviation\nThe variances are thus obtained by squaring the values in the table, multiplying by the frequency\nof the genotype concerned, and summing over the three genotypes (Equation 10).\nDerivation of additive and dominance variances,\nwhere:\n= frequency of allele ,\n= frequency of allele ,\n= average effect of an allele,\n= coded genotypic values. 78 | CHAPTER 6: COMPONENTS OF VARIANCE\nCovariance\nIf there is no dominance at the locus under consideration d = 0, then: .\nIf there is complete dominance d = a, the additive variance becomes .\nThe total genetic variance is estimated with Equation 11.\nTotal genetic variance formula including covariance of additive and dominance\nvariances,\nwhere:\nis the covariance of breeding values with dominance deviations, which can be\ndemonstrated to be zero. Thus substituting in Equation 12,\nTotal genetic variance formula relating additive and dominance variances to\nallele frequencies and coded genotypic values,\nwhere:\nare as described previously.\nComponent Relationships\nThe relationships among variance components, gene action, and allele frequencies for the two\nallele case can be graphically represented (Figs. 3, 4, 5). CHAPTER 6: COMPONENTS OF VARIANCE | 79\nAdditive Gene Action\nFig. 3 Genetic variance due to additive gene action\nonly and related to allele frequency changes.\nAdditive gene action: There is no dominance (a>0, d=0). In this case, the genetic variance is\nadditive, and it is greatest when p=q=0.5.\nComplete Dominance\nFig. 4 Variance changes in response to allele\nfrequency changes.\nComplete dominance: ( ). The dominance variance is maximal when p=q=0.5. The\nadditive is maximal when p=0.3. 80 | CHAPTER 6: COMPONENTS OF VARIANCE\nOverdominance\nFig. 5 Variance changes in response to allele\nfrequency changes.\nOverdominance: ( ). The dominance variance is the same as incomplete\ndominance.\nPrinciples to Remember\nImportant principles to remember:\n1. All the components of genetic variance are dependent on the gene frequencies.\n2. Estimates of components of genetic variances are valid only for the population from which they\nare estimated.\n3. The concept of additive variance does not carry with it the assumption of additive gene action;\nthe existence of additive variance is not an indication that genes act additively.\n4. Additive variance can arise from genes with any degree of dominance or epistasis. CHAPTER 6: COMPONENTS OF VARIANCE | 81\nFig. 6 Examining phenotypic traits of maize fields at the University of\nKwaZulu-Natal, South Africa. Photo by Iowa State University.\nInfluence of Epistasis\nTwo Or More Loci: Influence of Epistasis on Components of\nGenetic Variance\nWhen more than one locus is under consideration, then deviations due to interactions among loci\ngive rise to additional variance components due to epistatic interactions, V (Equation 13).\ni\nComponents of epistatic interaction variance,\nwhere:\n= additive \u00d7 additive variance is the interaction between two breeding value,\n= additive \u00d7 dominance variance is the interaction between the breeding value of one\nlocus and the dominance deviation of the other,\n= dominance \u00d7 dominance variance is the interaction between the two dominance\ndeviations. 82 | CHAPTER 6: COMPONENTS OF VARIANCE\nEpistatic Model\nA non-intuitive consequence of the epistatic models is that additive variance can arise from\npurely epistatic genetics. For example, let\u2019s consider the special case of an F population with\n2\nequal frequencies of two alleles at each of two independently segregating loci. Let\u2019s imagine that\nwe know the genotypes at each of these functional loci and analyze the F population using a\n2\nregression approach for each of the loci and their interactions.\nTable 3 Sources of genetic variability and associated df in an analysis of independently segregating loci\nin an F population based on a regression approach of analysis.\n2\nSource of variance Df\nLocus A 2\nLinear (Additive) 1\nQuadratic (Dominance) 1\nLocus B 2\nLinear (Additive) 1\nQuadratic (Dominance) 1\nEpistasis 4\nLinear A x Linear B (A * A) 1\nLinear A x Quadratic B (A * D) 1\nQuadratic A x Linear B (D * A) 1\nQuadratic A x quadratic B (B * D) 1\nTotal 8\nExample 1\nAnalysis of a phenotype in an F population with equal frequencies of alleles at two functionally\n2\npolymorphic loci, each contributing only additive coded genotypic values from the A locus and a\nB locus, i.e., , where = 5, = 3, and =1. CHAPTER 6: COMPONENTS OF VARIANCE | 83\nTable 4 Parameters and their Coded Genotypic\nvalues for P (example 1).\nParameter Value\n3\n0\n1\n0\n5\nTable 5 Coded Genotypic values for two functional bi-allelic loci in an F population derived\n2\nfrom a cross of two inbred lines (example 1).\nn/a n/a Mean\nn/a n/a 1/4 1/2 1/4 n/a\n1/4 9 6 3 6\n1/2 8 5 2 5\n1/4 7 4 1 4\nMean n/a 8 5 2 2.75 84 | CHAPTER 6: COMPONENTS OF VARIANCE\nTable 6 Calculated variances components for the\nF population described in example 1.\n2\nVariance component Variance\n4.5\n0.5\n0\n0\n0\n0\n0\n0\nExample 2\nAnalysis of a phenotype in an F population with equal frequencies of alleles at two functionally\n2\npolymorphic loci where only single epistatic interaction between the genotypes will produce an\naltered phenotype, , where = 0, = 0, = 0,\n= 0, = 0, = 50.\nTable 7 Coded Genotypic values for two functional bi-allelic loci in an F population derived\n2\nfrom a cross of two inbred lines (example 2).\nn/a n/a Mean\nn/a n/a 1/4 1/2 1/4 n/a\n1/4 0 0 0 0\n1/2 0 0 0 0\n1/4 0 0 50 12.5\nMean n/a 0 0 12.5 3.125 CHAPTER 6: COMPONENTS OF VARIANCE | 85\nTable 8 Calculated variances components for the\nF population described in example 2.\n2\nPopulation variances Population Percent\nTotal genetic 146.484 100.0%\n39.063 26.7%\nAdditive effects\n19.531 13.3%\nDominance\n87.891 60.0%\nEpistasis\nReferences\nLush J. L. 1937. Animal Breeding Plans, Collegiate Press Inc, Ames, IA.\nHow to cite this chapter: Beavis, W., K. Lamkey, K. Espinosa, and A. A. Mahama. 2023. Components\nof Variance. In W. P. Suza, & K. R. Lamkey (Eds.), Quantitative Genetics for Plant Breeding. Iowa State\nUniversity Digital Press. 86 | CHAPTER 7: ESTIMATES OF VARIANCE\nChapter 7: Estimates of Variance\nWilliam Beavis; Kendall Lamkey; Katherine Espinosa; and Anthony Assibi Mahama\nEstimating heritability is a fundamental concept of quantitative genetics. One method for\nobtaining estimates of heritability is the use of variance and covariance of a known collection of\nrelatives from various types of progeny.\nLearning Objectives\n\u2022 Model components of genetic variances and covariances for purposes of estimating heritability,\na fundamental concept of quantitative genetics.\n\u2022 Explain why estimates of components of genetic variability are limited to the population from\nwhich they are estimated.\n\u2022 Students will derive variance components and recognize the differences amongst components\nobtained from different progeny used for estimating heritability.\n\u2022 Students will write out the correct linear models for the correct mean squares and expected\nmean squares in a ANOVA table, and correctly interpret the ANOVA and algebraically extract\nthe correct values for estimating heritability.\n\u2022 Leverage of the powerful algebraic equivalence of covariances within groups of relatives to\nvariances among the same groups.\nCovariance of Relatives\nRecall that Cov(Y ,Y ) = Var(G), for j \u2260 j\u2019. In the context of genotypic sampling of relatives, this\nij ij\u2019 i\ngeneral relationship has a profound and powerful impact on interpretation of ANOVA. It means\nthat the covariance among a sample of relatives can be used to estimate components of genetic\nvariance associated with the genotypic effect. CHAPTER 7: ESTIMATES OF VARIANCE | 87\nTable 1 A general ANOVA table for any type of related progeny.\nn/a EMS\nSource df MS Variances Covariances\nReps n/a n/a n/a\nProgeny\nError\nTotal n/a n/a n/a\nWithin\nProgeny\nNote that there are p progeny grown in r reps. Cov(progeny) refers to the covariance of the\nprogeny, where the progeny can be full-sibs, half-sibs, S -progeny, S -progeny, testcross progeny,\n1 2\netc. The key is to know the progeny type and take advantage of the general rule that the variance\namong progeny is equal to the covariance of the progenies.\n\u03c32 \u03c32\nNote the use of instead of in the within progeny line of the ANOVA table. This is because\nT G\n\u03c32 \u03c32 \u03c32\nis usually equal to + the total variance in a non-inbred random mating population.\nG A D\nIf the population does not have a random mating structure, then the total variance will be\n\u03c32 \u03c32\nsomething other than + . For example, the total genetic variance for an F population is as\nA D 3\nin Equation 1.\nFormula for total genetic variance for and F population.\n3\nwhere:\n= total genetic variance for F population,\n3\n= additive variance,\n= dominance variance.\nLinear Models for Phenotypic Values\nThe covariance of relatives is simply that relatives tend to show more phenotypic similarities\nthan with each other than with unrelated individuals. For example let X represent an individual\nij\nfrom the mating of parent i and parent j: 88 | CHAPTER 7: ESTIMATES OF VARIANCE\nTable 2 Descriptions of relationships between individuals\nX and X .\nij i\u2019j\u2019\nConditions Description\nFull-sibs\nReciprocal Full-sibs\nMaternal half-sibs\nPaternal half-sibs\nSpecifying covariance of relatives in terms of genetic variances has the following assumptions:\n1. Regular diploid and solely Mendelian inheritance\n2. No environmental correlations among relatives\n3. No gametic disequilibrium\n4. The relatives are not inbred\n5. The relatives are considered to be random members of some non-inbred population\nWith these assumptions, we can specify the covariance of relatives as in Equation 2.\nFormula for covariance of relatives,\nwhere:\n= the coefficient of relative relationship,\n= additive genetic variance,\n= the dominance relationship coefficient,\n= the dominance variance,\n= the epistatic variances.\nCommon Types of Relatives\nUsing the result of Equation 1 for some common types of relatives, it can be shown that:\nCovariance of half-sibs with one common parent is represented by Equation 3. CHAPTER 7: ESTIMATES OF VARIANCE | 89\nFormula for calculating covariance of half-sibs,\nwhere:\n= inbreeding coefficient of parent A.\nCovariance of full-sibs with parents A and B is estimated using Equation 4.\nFormula for calculating covariance of full-sibs,\nwhere:\n= inbreeding coefficient of parent A,\n= inbreeding coefficient of parent B,\n= the epistatic variances.\nF and F Progenies\n2 3\nTable 3 F progeny genotypes, frequencies, genotypic values and progeny mean values\n3\nrepresentation.\nF3 Progeny\nF3\nGenotype Freq GV Progeny\nMean\n1/4 a 1 0 0 a\n1/2 d 1/4 1/2 1/4 1/2d\n1/4 -a 0 0 1 -a\nF and F Variances\n2 3\nTotal genetic variance among F individuals determined suing Eqaution 5:\n2 90 | CHAPTER 7: ESTIMATES OF VARIANCE\nFormula for calculating total genetic variance among F individuals,\n2\nwhere:\n= genotypic values of AA or aa and Aa genotypes, respectively.\nTotal F phenotypic variation:\n2\nFormula for calculating total phenotypic variance among F individuals,\n2\nwhere:\n= total phenotypic variance among F2 individuals,\n= the non-genetic variation among F2 plants,\nare as described previously.\nRecall that the F is our reference population for interpretation of genetic results. To estimate\n2\nthe total genetic variation of an F , we need the parents and the F (to estimate environmental\n2 1\neffects)as well as the F generation.\n2\nF Variances\n3\nF population mean is equal to\n3\nVariance among F progeny means is determined using Equation 7.\n3\n,\nFormula for calculating variance among F progeny means,\n3\nwhere:\n= variance among F progeny means,\n3\nare as described previously.\nVariance within F progeny means is determined using Equation 8.\n3\n,\nFormula for calculating total phenotypic variance within F progeny means,\n3 CHAPTER 7: ESTIMATES OF VARIANCE | 91\nwhere:\n= variance within F3 progeny means,\nare as described previously.\nTotal variance among F individuals is then estimated from Equation 9:\n3\nFormula for calculating total variance among F individuals,\n3\nwhere:\n= total variance among F3 individuals,\nare as described previously.\nF progenies can be grown in replicated trials, so a set of equations like the following could be\n3\nwritten to estimate the variance in different generations (Equation 10).\n,\n,\n,\n,\nFormulae for calculating variances for F and F ,\n2 3\nwhere:\n= non-genetic variation among mean variance of F progeny,\n3\n= number of replications,\nare as described previously.\nANOVA for F Progenies\n3\nANOVA for F progenies can be calculated from a replicated experiment.\n3 92 | CHAPTER 7: ESTIMATES OF VARIANCE\nTable 4 ANOVA for F Progenies.\n3\nSource df MS EMS\nReps n/a n/a\nProgeny M3\nError M2\nTotal n/a n/a\nWithin\nM1\nProgeny\nThen using Equation 11,\nFormulae for calculating variances using MS and EMS,\nis estimated as environmental variance within P1 or P2 plots (the inbred lines).\nNote that the phenotypic variance among F families is determined with Equation 12:\n3\n,\nFormula for calculating total phenotypic variance among F families,\n3\nwhere:\n= estimate of total phenotypic variance,\n= within progeny variance,\n= phenotypic variance among F families = the genotypic variance,\n3\n= number of replications,\n= individuals withing progeny type. CHAPTER 7: ESTIMATES OF VARIANCE | 93\nEstimate of Heritability\nA type of heritability estimates on a progeny mean basis can be calculated as shown in Equation\n13:\n,\nFormulae for calculating heritability on progeny entry mean basis,\nwhere:\nTerms are as described previously.\nNote that this estimate of heritability contains both additive and dominance variance. Recall that\nthis is an estimate of intra-class correlation, thus it is a type of broad-sense heritability.\nLimitations of this method (often referred to as Mather\u2019s methods)\n1. Estimates apply only to specific parents.\n\u03c32\n2. Estimates for may vary among generations.\nE1\n3. Estimates for a particular set of F plants can be obtained in only one environment.\n2\n4. Linkage will bias estimates.\n5. Epistasis is assumed to be absent.\nBi-Parental Progenies\nBi-parental progenies are just crosses between individual plants; thus, genetically, they are full-\nsibs. For example, in a random mating maize population, you could cross two individual plants\nreciprocally and bulk the seed from the two ears. This would produce enough seed to plant FS\nprogeny in 10-20 replications. We could then think about n plants and making n / 2 full-sib\nfamilies. The covariance then be computed using Equation 14. 94 | CHAPTER 7: ESTIMATES OF VARIANCE\nTable 5 ANOVA Table for bi-parental progenies\nSource df MS EMS\nReps n/a n/a\nAmong\nfamilies\nError\nTotal n/a n/a\nWithin\nfamilies\nExtracting different variance components.\nSummary\nTable 6 Data from Cockerham, 1983.\nProgeny\nCov(progeny) Total Variance,\nType\nHalf-sib\nFull-sib\nS1(F2:3)\nS2(F3:4)\nSn(F4:5)\nS\u221e CHAPTER 7: ESTIMATES OF VARIANCE | 95\nExpected Mean Squares\nThe AOV tables cannot be interpreted without understanding the expected sources of variability\nrepresented by the Mean Squares. In the case of balanced field plot designs with only a few\nsources of variation, the expected mean squares are easily determined. If a particular design\ninvolves many sources of random and fixed factors, students have found the approach of\nLorenzen and Anderson (1993, Design of Experiments: A No-Name Approach. p 71-72) to be\nuseful.\n1. Write the terms of the model with associated subscripts down the left side of the page.\nAcross the top, write the single letter subscripts (i,j,k, etc.). Above each subscript, place\neither F or R if the factor associated with that transcript is fixed or random. Above that,\nplace the number of levels associated with that subscript (I, J, K, etc.).\n2. Enter a 1 in every slot where the subscript at the top is contained within brackets in the\nterm at the left.\n3. Enter a 0 in every slot where the subscript at the top is fixed and also contained in the term\nas the left. Enter a 1 in every slot where the subscript at the top is random and also\ncontained in the terms at the left.\n4. Fill in the remaining slots with the number of levels at the top of each column.\n5. To compute the Expected Mean Squares (EMS) for a given term having df > 0, start at the\nbottom and work up. Only consider terms whose indices include all the indices in the term\nwhose EMS you are deriving. Compute the coefficient of this term by covering the columns\ncorresponding to the indices in the term whose EMS you are deriving and multiplying the\nvalues in the remaining columns. If there is a 0 column that is not covered, this term need\nnot be written in the EMS. A factor is considered fixed and denoted with a \u03a6 only if all of\n\u03c32\nits indices are fixed. Otherwise, it is considered random and denoted by the appropriate\nterm.\nUsing the Algorithm\nNotice that this algorithm can be used to compute EMS for all terms in the model, including\nthose that have zero df. A term that has zero df has no expected mean squares. For this reason, we\nwill not compute EMS for terms having zero df even though such terms are in the algorithm to\nmake the EMS of the other terms come out right. Note that this simple algorithm for determining\nthe EMS in an AOV assumes that the data are balanced, i.e., each of the sources of variability\n(model parameters) have data for all levels, i, j, and k. 96 | CHAPTER 7: ESTIMATES OF VARIANCE\nStep 1\nThe phenotype Y for this typical field trial will be something like in Equation 15:\n,\nLinear model for phenotype,\nwhere:\nth th\n= phenotypic measure of trait for the j genotype in the k block nested within\nth\nthe i environment,\n= overall mean,\nth\n= i environment,\nth th\n= represents the k block nested within the i environment,\nth\n= the j genotype,\nth th\n= interaction effect between the j genotype and the i environment,\nth th\n= the residual for genotype j in the k block nested within the i environment.\nNotice that this algorithm can be used to compute EMS for all terms in the model, including\nthose that have zero df. A term that has zero df has no expected mean squares. For this reason, we\nwill not compute EMS for terms having zero df even though such terms are in the algorithm to\nmake the EMS of the other terms come out right. Note that this simple algorithm for determining\nthe EMS in an AOV assumes that the data are balanced, i.e., each of the sources of variability\n(model parameters) have data for all levels, i, j, and k.\nTo illustrate, let us consider a slightly more complex but typical RCBD design used by plant\nbreeders to evaluate many genotypes grown in replicates at several environments for purposes\nof identifying and discarding poor-performing genotypes in a cultivar development project.\nEquation 15 will appear for each of the steps below.\nWrite the terms of the model with associated subscripts down the left side of the page. Across the\ntop, write the single letter subscripts (i,j,k, etc.). Above each subscript, place either F or R if the\nfactor associated with that transcript is fixed or random. Above that, place the number of levels\nassociated with that subscript (I, J, K, etc.).\nFactors:\n\u2022 Factor E \u2013 Fixed\n\u2022 Factor G \u2013 Random\n\u2022 Blocks \u2013 Random CHAPTER 7: ESTIMATES OF VARIANCE | 97\nE G R\nSource F R R EMS\ni j k\nn/a n/a n/a n/a\nn/a n/a n/a n/a\nn/a n/a n/a n/a\nn/a n/a n/a n/a\nn/a n/a n/a n/a\n,\nStep 2\nThe phenotype Y for this typical field trial will be something like:\n,\nEnter a 1 in every slot where the subscript at the top is contained within brackets in the term at\nthe left.\nFactors:\n\u2022 Factor E \u2013 Fixed\n\u2022 Factor G \u2013 Random\n\u2022 Blocks \u2013 Random 98 | CHAPTER 7: ESTIMATES OF VARIANCE\nE G R\nSource F R R EMS\ni j k\nn/a n/a n/a n/a n/a\n1 n/a n/a /n/a\n1 n/a n/a n/a\n1 1 n/a n/a\n1 1 1 n/a\nStep 3\nThe phenotype Y for this typical field trial will be something like:\n,\nEnter a 0 in every slot where the subscript at the top is fixed and also contained in the term as\nthe left. Enter a 1 in every slot where the subscript at the top is random and also contained in the\nterms at the left.\nFactors:\n\u2022 Factor E \u2013 Fixed\n\u2022 Factor G \u2013 Random\n\u2022 Blocks \u2013 Random CHAPTER 7: ESTIMATES OF VARIANCE | 99\nE G R\nSource F R R EMS\ni j k\nn/a n/a n/a\n0 1 n/a n/a\n1 n/a n/a n/a\n1 1 n/a n/a\n1 1 1 n/a\nStep 4\nThe phenotype Y for this typical field trial will be something like this:\n,\nFill in the remaining slots with the number of levels at the top of each column.\nFactors:\n\u2022 Factor E \u2013 Fixed\n\u2022 Factor G \u2013 Random\n\u2022 Blocks \u2013 Random\nE G R\nSource F R R EMS\ni j k\n0 G R n/a\n0 G 1 n/a\nE 1 R n/a\n1 1 R n/a\n1 1 1 n/a 100 | CHAPTER 7: ESTIMATES OF VARIANCE\nStep 5\nThe phenotype Y for this typical field trial will be something like:\n,\nTo compute the EMS for a given term having df > 0, start at the bottom and work up. Only\nconsider terms whose indices include all the indices in the term whose EMS you are deriving.\nCompute the coefficient of this term by covering the columns corresponding to the indices in the\nterm whose EMS you are deriving and multiplying the values in the remaining columns.\nIf there is a 0 column that is not covered, this term need not be written in the EMS. A factor\nis considered fixed and denoted with a \u03a6 only if all of its indices are fixed. Otherwise it is\n\u03c32\nconsidered random and denoted by the appropriate term.\nFactors:\n\u2022 Factor E \u2013 Fixed\n\u2022 Factor G \u2013 Random\n\u2022 Blocks \u2013 Random\nE G R\nSource F R R EMS\ni j k\n0 G R\n0 G 1\nE 1 R\n1 1 R\n1 1 1\nReferences\nCockerham, C.C. 1983. Covariances of relatives from self-fertilization. Crop Sci. 23: 1177-1180. CHAPTER 7: ESTIMATES OF VARIANCE | 101\nLorenzen, T., and V. Anderson. 1993. Design of Experiments: A No-Name Approach. Routledge\n& CRC Press.\nHow to cite this chapter: Beavis, W., K. Lamkey, K. Espinosa, and A. A. Mahama. 2023. Estimates of\nVariance. In W. P. Suza, & K. R. Lamkey (Eds.), Quantitative Genetics for Plant Breeding. Iowa State\nUniversity Digital Press. 102 | CHAPTER 8: MATING DESIGNS\nChapter 8: Mating Designs\nWilliam Beavis; Kendall Lamkey; and Anthony Assibi Mahama\nThere are many mating designs developed for the purpose of estimating the magnitude of\ngenetic variability in a reference population. This information is most often useful to the plant\nbreeder who is developing a new breeding program in a new crop species or developing a novel\ngermplasm resource for established crop species. For example, large estimates of additive genetic\nvariability and small estimates of genotype by environment variability suggest that rapid progress\nfrom selection can be made with minimal allocation of testing resources. While most recently\ntrained plant breeders will assume responsibilities for established plant breeding programs, most\nestablished programs begin with an evaluation of genetic variability using one of the many\nmating designs. Thus, we feel it is instructive to understand the genetic basis upon which these\nprograms were established.\nThe choice of mating designs is based on:\n1. The natural mode of reproduction and mating flexibilities of the species.\n2. The objective(s) in estimating genetic variances such as:\n\u25e6 General interest in knowledge of gene action for quantitative characters\n\u25e6 Making a choice among alternative selection and breeding procedures\n\u25e6 The prediction of response to selection.\n3. Joint purposes such as estimating genetic variances and simultaneously selecting among\nprogenies or evaluating hybrid combinations\n4. The precision of the estimates.\nLearning Objectives\nStudents will learn about methods used to evaluate the potential for genetic improvement in\ngermplasm with unknown estimates of heritability through the application of the Variance-\nCovariance principle in various types of mating designs. CHAPTER 8: MATING DESIGNS | 103\nDesign Setup\nSetting up the treatment and experimental designs for mating designs creates unique challenges.\nSeveral things need to be considered:\n\u2022 Ease of making crosses in the species.\n\u2022 Inbreeding generation of the parents of the crosses.\n\u2022 The number of parents that will be used (male and female).\n\u2022 Fixed versus random parents.\n\u2022 The type of mating design to be used.\n\u2022 The type of experimental design to be used.\n\u2022 The environmental design to be used.\nDiallel Crosses\nDiallel matings (Table 1) are used to make inferences regarding the types of gene effects\ncontrolling traits. Diallels are particularly important in cross-pollinated crops and for\ndetermining the importance of general combining ability and specific combining ability.\nConsider the following general mating scheme. This scheme is very similar in structure to the\ntwo-way tables we have seen for studying interactions.\nTable 1 General mating scheme for diallel\nParents P1 P2 P3 P4 Pn Totals\nP1 Y Y Y Y Y Y\n11 12 13 14 1n 1.\nP2 Y Y Y Y Y Y\n21 22 23 24 2n 2.\nP3 Y Y Y Y Y Y\n31 32 33 34 3n 3.\nP3 Y Y Y Y Y Y\n41 42 43 44 4n 4.\nPn Y Y Y Y Y Y\nn1 n2 n3 n4 nn n.\nTotals Y Y Y Y Y Y\n.1 .2 .3 .4 .n ..\nNumber of Diallel Crosses and Entries\nLet us consider the number of diallel crosses for n parents with and without reciprocal crosses.\nThe number of entries is the number that would have to be evaluated if the parents were included\nin the experiment (Table 2). 104 | CHAPTER 8: MATING DESIGNS\nTable 2 Number of crosses and entries possible with different numbers of parents with and without\nreciprocal matings.\nWithout Reciprocals With Reciprocals\nNo. of No. of Number of No. of Crosses Including Number of\nParents Crosses Entries Reciprocals Entries\n5 10 15 20 20\n6 15 21 30 30\n7 21 28 42 42\n8 28 36 56 56\n9 36 45 72 72\n10 45 55 90 90\n11 55 66 110 110\n12 66 78 132 132\n13 78 91 156 156\n14 91 105 182 182\n15 105 120 210 210\n20 190 210 380 380\n50 1225 1275 2450 2450\n100 4950 5050 9900 9900 CHAPTER 8: MATING DESIGNS | 105\nTypes of Diallel Analysis\nTable 3 Fixed versus random effects analysis based on method and entries makeup.\nModel Method Parents Included Crosses Reciprocals\nI (Fixed) 1 Yes Yes Yes\nI (Fixed) 2 Yes Yes No\nI (Fixed) 3 No Yes Yes\nI (Fixed) 4 No Yes No\nII (Random) 1 Yes Yes Yes\nII (Random) 2 Yes Yes No\nII (Random) 3 No Yes Yes\nII (Random) 4 No Yes No\nCommon Diallel Experiment\nThe most common diallel experiment is conducted with selected parents, which means a fixed\neffects analysis where only gene effects and not variance will be estimated (Table 3). The reason\nfor this is simple: It is very hard to sample a population adequately with a diallel. Diallels are\nuseful mating designs, however, despite this limitation.\nTherefore, we will not present any analyses related to estimating variance components \u2014 only\ngene effects. This makes this section somewhat out of place, but it fits in with the other mating\ndesigns from the structural point of view. The analyses we will present are a combination of those\npresented by Griffing (1956) and Gardner and Eberhart (1966).\nMethods 2 and 4 are the most common types of diallels. Most scientists grow the parents and\nthe crosses or just the crosses. The method 4 analysis is, however, the most commonly used\nanalysis because Griffing assigns specific combining ability effects to the parents per se, and\nthese are hard to interpret relative to Sprague and Tatum\u2019s (1942) definitions of general and\nspecific combining ability.\nThe general model underlying the diallel can be written as in Equation 1:\nGeneral linear model for diallel design experiments.\nwhere: 106 | CHAPTER 8: MATING DESIGNS\n= the mean,\nth\n= the general combining ability effect (marginal effect) of the i parent,\nth\n= the general combining ability effect (marginal effect) of the j parent,\nth th\n= the specific combining ability effect (interaction effect) of the i and j\nparents,\nth\n= the effect of the k replication,\n= the residual (or error).\nAn ANOVA Table for Diallels is shown in Table 4.\nTable 4 ANOVA Table for Diallels.\ndf\nEMS\nSource df (n = SS MS\n(Model I \u2013 Fixed Effects)\n10)\nReplications n/a n/a n/a\nEntries 44\nAmong\n9\nMargins\nAmong\ncells/ 35\nMargins\n44\nError\nF-Tests\nModel I F-Tests: For among cells/margins and among margins are, respectively,\nF = \\frac{M_{22}}{M_1} \\text{, and }\nThese F-tests evaluate whether differences among the parents and crosses within parents are\nsignificant. Also, it is possible to show that the effects can be estimated using Equation 2: CHAPTER 8: MATING DESIGNS | 107\n,\n,\n,\nFormulae for estimating mean, gca, and sca effects.\nwhere:\n= estimated mean,\n= number of parents,\n= estimate of gca effect of genotype i,\n= estimate of sca effect of genotypes i and j,\n= grand total,\n= sum of parent i across all parents,\n= sum of parent j across all parents,\n= phenotype of cross ij .\nThe variances of the effects can be estimated with Equation 3:\nFormulae for estimating variances of estimated of mean, gca, sca effects and error,\nwhere: 108 | CHAPTER 8: MATING DESIGNS\n= estimates variance of average phenotype,\nother terms are as defined previously.\nGardner and Eberhart Diallel Analysis II\nThe Gardner and Eberhart Analysis II for the diallel is a more general analysis designed for the\ncase of when the diallel includes random mating varieties. The model is best laid out by starting\nwith the following single locus theory for the variety and locus (Table 5):\nTable 5 Frequencies and genotypic values for genotypes.\nFrequency Genotype Genotypic value\nAA\nAa\naa\nWhere, .\nThe population mean can be written as in Equation 4:\n\\mu&#039; \\sum_i(2p_{ji}-1) \\alpha_{i} + 2\\sum_i(p_{ji}-p_{ji}^{2})\\delta_i\nFormula for calculating the population mean,\nwhere:\n= frequencies of the two allele.\n= average genotypic value,\n= coded genotypic values.\nEquations\nLet .\nSimilarly, let . CHAPTER 8: MATING DESIGNS | 109\nThen, the population mean can be written as in Equation 5:\nFormula for the population mean.\nwhere:\n= the mean,\n= average genotypic value of AA & aa,\n=genotypic value of AA, aa genotypes,\n= genotypic value of Aa genotype.\nA population cross mean can be written as in Equation 6:\n.\nFormula for the population cross mean.\nwhere:\n= mean of the \u201cvariety cross\u201d from two parents,\n= the mean of all crosses,\n= the additive effect,\n= the dominance effect,\n= the heterosis effect.\nIf the varieties, varieties selfed, population crosses, population crosses selfed, and population\ncrosses random mated are included in the analysis, then all of these genetic effects can be\nestimated. Usually, this is not the case, and only varieties and variety crosses are included in the\nanalysis, which are confounded, and they have to be estimated together. We can then define the\nfollowing parameters:\nThe mean of all parental varieties included in the analysis is written as in Equation 7:\nFormula for mean when all parental varieties are included.\nwhere:\n= the mean of all parental varieties included in the analysis,\n= the mean, 110 | CHAPTER 8: MATING DESIGNS\n= dominance effect,\n= average dominance effects.\nThe variety effect when parents are included in the analysis is written as in Equation 8:\nFormula for estimating variety effects with parents included.\nwhere:\n= the variety effect when parents are included,\nare as defined previously.\nModels\nWe can then fit the following four models to the data (Equation 9):\nLinear models for estimating different genetic effects on phenotype,\nwhere:\n,\n= phenotype of j by j\u2019 progeny.\nANOVA Table\nThe following ANOVA table can be written as in Table 6: CHAPTER 8: MATING DESIGNS | 111\nTable 6 ANOVA Table for Gardner and Eberhart Diallel Analysis II.\nSource df Sum of squares\nPopulations\nVarieties\nHeterosis\nAverage\nVariety\nSpecific\nEquivalent Analysis\nAn equivalent analysis can be made with just the crosses as follows:\nThe mean of crosses in the diallel can be estimated as follows:\n;\nThe variety effect in crosses = general combining ability effect = , then the mean of\ncrosses is written as in Equation 10:\nModel for analysis with only crosses included.\nwhere:\n= specific heterosis from variety j by variety j\u2019 mating,\n,\n. 112 | CHAPTER 8: MATING DESIGNS\nAnalysis III of Gardner and Eberhart\nThe following ANOVA (Table 7) can be written (Analysis III of Gardner and Eberhart)\nTable 7 ANOVA Table for Gardner and Eberhart Diallel Analysis III.\nSource Degrees of Freedom Sum of squares\nPopulation n/a\nVarieties ( )\nVarieties vs. crosses ( )\nCrosses( )\nGCA ( )\nSCA ( )\nThe analysis of Crosses, GCA, and SCA is all that can be done if only the crosses are included in\nthe analysis. This analysis is equivalent to the Model 4 analysis of Griffing. If varieties or parents\nare also included, then the analysis, Varieties, and Varieties vs. Crosses can also be calculated.\nAnalysis III is related to Analysis II in the following ways that the (s ) are the same in the two\njj\u2019\nanalyses S\u2019 = S\u201d , meaning that average heterosis is simply a contrast of the mean of the varieties\n21 2\nwith the mean of the crosses (Equation 11).\nFormula for calculating average heterosis.\nwhere:\n= effects of variety 1,\n= effects of variety 2,\n= effects of crosses,\n= GCA effects,\n= SCA effects. CHAPTER 8: MATING DESIGNS | 113\nNorth Carolina Design I\nFig. 1 North Carolina Design I (NC I), Nested Mating Design (A/B).\n\u2022 Consider m male plants:\n\u2022 each of which is mated to f female plants,\n\u2022 to produce n full-sib families within each male,\n\u2022 for a total of mf half-sib families.\n\u2022 There is a total of m half-sib families.\n\u2022 Different female plants are used to cross with each male.\n\u2022 The progeny P are grown in a replicated experiment design.\nThe model for analysis is written as in Equation 12:\nGeneral linear model for NC I experiments.\nwhere:\n= the mean,\n= the effect of male i,\n= the effect of female j when crossed to male i,\n= replication effect,\n= the residual.\nANOVA Table\nThen the ANOVA (Table 8) can be written as: 114 | CHAPTER 8: MATING DESIGNS\nTable 8 ANOVA Table for Gardner and Eberhart Diallel Analysis III.\nSource of\nd.f. MS EMS\nVariation\nReplications n/a n/a\nMales M4\nFemales/Males M3\nError M2\nTotal n/a n/a\nWithin M1\nThe table can be rewritten in terms of the covariance of relatives as follows (Table 9):\nTable 9 ANOVA Table for Gardner and Eberhart Diallel Analysis III.\nSource of\nd.f. MS EMS\nVariation\nReplications n/a n/a\nMales M4\nFemales/Males M3\nError M2\nTotal n/a n/a\nWithin M1\nVariance Estimates\nEstimation of variance of the various components is as in Equation 13: CHAPTER 8: MATING DESIGNS | 115\nFormulae for calculating variance components for NC I.\nwhere:\n= variance of males,\n= variance of males within females,\n= covariance of half-sibs,\n= covariance of full-sibs.\nSo, ignoring epistasis, the variances are written as in Equation 14:\n.\nAlternative formulae for calculating variance components.\nwhere:\n= the additive variance,\n= the dominance variance,\n= the inbreeding coefficient of the male parent,\n= estimated variance of male within females,\n= the inbreeding coefficient of the female parent,\n= effects of females.\nConsider the case when all the parents are noninbred, i.e., F = F = 0. The variances are written\nm f\nas in Equation 15:\nFormulae for calculating variance component when all the parents are\nnoninbred.\nwhere:\nare as defined previously.\nWhen both the male and female parents are inbred, i.e., F = F = 1, then the variances can be\nm f\nestimated as written in Equation 16: 116 | CHAPTER 8: MATING DESIGNS\nFormulae for calculating variance component when all the parents are inbred.\nwhere:\nare as defined previously.\nNorth Carolina Design II\nFig. 2 North Carolina Design II (NC II), Mating Design (AB).\n\u2022 Consider m male plants,\n\u2022 each of which is mated to f female plants,\n\u2022 to produce f full-sib families within each male,\n\u2022 for a total of mf half-sib families.\n\u2022 There is a total of m f half-sib families.\n\u2022 The same female plants are crossed with each male.\n\u2022 The progeny P are grown in a replicated experiment design.\nThe design is related to the diallel and another simpler way to represent the design is (Table 10):\nTable 10 North Carolina Design II arrangement.\nParents M1 M2 M3 M4 Totals\nF5 Y Y Y Y Y\n15 25 35 45 .5\nF6 Y Y Y Y Y\n16 26 36 46 .6\nF7 Y Y Y Y Y\n17 27 37 47 .7\nF8 Y Y Y Y Y\n18 28 38 48 .8\nTotals Y Y Y Y Y\n1. 2. 3. 4. .. CHAPTER 8: MATING DESIGNS | 117\nModel\nThe model for analysis is written as in Equation 17:\n\u00b5\nFormulae for calculating variance components for NC II.\nwhere:\n= mean,\n=the effect of male i,\n= the effect of female j,\n= the interaction effect of female j when crossed to male i,\n= replication effect,\n=the residual.\nANOVA Table\nThe ANOVA is shown in Table 11.\nTable 11 ANOVA Table for North Carolina Design II.\nSource of Variation d.f. MS EMS\nReplications n/a n/a\nMales (M) M5\nFemales (F) M4\nM x F M3\nError M2\nTotal n/a n/a\nWithin M1\nCovariance of Relatives\nThe table can be rewritten in terms of the covariance of relatives as follows (Table 12): 118 | CHAPTER 8: MATING DESIGNS\nTable 12 A general ANOVA table for covariance of relatives.\nSource of Variation d.f. MS EMS\nReplications n/a n/a\nMales (M) M5\nFemales (F) M4\nM x F M3\nError M2\nTotal n/a n/a\nWithin M1\nEstimation\nVariance components are estimated as written in Equation 18:\nFormula for estimating covariance of relatives,\nwhere:\n= estimated variance of males,\n= estimated variance of females,\n= estimated variance of male by female cross (full-sibs),\n= covariance of full-sibs,\n= covariance of half-sibs with common male,\n= covariance of half-sibs with common female.\nVariance Estimates\nIgnoring epistasis, variance components are estimated as written in Equation 19. CHAPTER 8: MATING DESIGNS | 119\nFormula for calculating variance estimates, ignoring epistasis.\nwhere:\nare as defined previously.\nConsider the case when all the parents are noninbred, i.e., F = F = 0. Variance components are\nm f\nestimated as written in Equation 20:\nFormulae for estimating variance components when male and female parents are\nnoninbred,\nwhere:\nare as defined previously.\nWhen both the male and female parents are inbred, i.e., F = F = 1, then variance components\nm f\nare estimated as written in Equation 21:\nFormulae for estimating variance components when male and female parents are\ninbred,\nwhere:\nare as defined previously. 120 | CHAPTER 8: MATING DESIGNS\nNorth Carolina Design III\nThe main use of Design III is for estimating the average degree of dominance.\nNorth Carolina Design III Backcross Design is shown in Fig. 3.\nFig. 3 North Carolina Design\nIII (NC III), Mating Design\n(F2 backcrossed to inbred\nparents).\nThis design involves crossing two inbred lines and obtaining the F and F generations. An\n1 2\nindividual F plant is then backcrossed to each of the inbred parents generating a pair of progeny\n2\nusing the F plants and pollen parents. Then for n F plants, there are 2n progenies produced,\n2 2\nand the model is as written in Equation 22:\n\u00b5\nLinear model for estimating average degree of dominance.\nwhere:\n= the mean,\n= contrast of the inbred parents i = 1, 2\n= the effect of F parent j,\n2\n= the interaction effect of inbred parent i and F plant i,\n2\n= replication effect,\n=the residual.\nANOVA Table\nAn ANOVA Table for North Carolina Design III is shown in Table 13. CHAPTER 8: MATING DESIGNS | 121\nTable 13 ANOVA Table for North Carolina Design III.\nSource of Variation d.f. MS EMS\nReplications n/a n/a\nInbred Lines n/a n/a\nF parents M3\n2\nF parent x inbred line M2\n2\nError M1\nTotal n/a n/a\nEstimation\nVariance components are estimated as written in Equation 23:\nFormulae for estimating effects of parents and the interaction effect of inbred\nparents and F plants,\n2\nwhere:\n= the summation is over i loci,\n= estimated variance of F parents i = 1, 2\n2\n= estimated variance of the interaction effect of the parent and F plant.\n2\nF-Tests\nRemember that in an F population, additive and dominance variances are written as in Equation\n2\n24\nFormula for total genetic variance for and F population,\n3\nso that variance components are estimated as written in Equation 25, 122 | CHAPTER 8: MATING DESIGNS\nFormula for total genetic variance for and F population.\n3\nwhere:\nare as defined previously,\n= additive variance,\n= dominance variance.\nNote that this design is very specialized for the specific case of F populations when p = q = 0.5.\n2\nThis design provides exact F-tests of two important hypotheses:\n1. The null hypothesis of no dominance. This is tested by: F=M2/M1, and if this F-test is\nsignificant, then it means that and there is no dominance.\n2. The null hypothesis is that dominance is complete. If there is complete dominance, then\nthe ratio, M3/M2=1.\nA significant departure of this ratio from one indicates that departs significantly from 1.\nReferences\nGardner, C. O, and A. S., Eberhart. 1966. Analysis and interpretation of the variety cross diallel\nand related populations. Biometrics, 22(3):439-52.\nGriffing, B. 1956. Concept of General and Specific Combining Ability in Relation to Diallel\nCrossing Systems. Aust. J. Biol. Sci. 9: 463-93.\nSprague, G. F., and Tatum, L. A. 1942. General Vs Specific Combining Ability in Single Crosses\nof Corn. J. Amer. Soc. Agron. 34: 923-32.\nHow to cite this chapter: Beavis, W., K. Lamkey, and A. A. Mahama. 2023. Mating Designs. In W. P.\nSuza, & K. R. Lamkey (Eds.), Quantitative Genetics for Plant Breeding. Iowa State University Digital\nPress. CHAPTER 9: SELECTION RESPONSE | 123\nChapter 9: Selection Response\nWilliam Beavis; Kendall Lamkey; and Anthony Assibi Mahama\nSelection is the crux of crop improvement and makes use of the art and science concepts to\nensure the success (gains derived) of a breeding program or specific project. This chapter explains\nprinciples and practices related to genetic gain.\nLearning Objectives\n\u2022 Explain the role of selection on genetic improvement.\n\u2022 Explain all of the components of realized and predicted genetic gain.\n\u2022 Explain why realized genetic gains are always less than predicted genetic gains.\n\u2022 Explain the role of replication in multi-environment tests on predicted and realized genetic\ngains.\nUnderlying Theory of Selection\nFig. 1 The normal distribution.\nLet be the mean phenotypic value of a quantitative trait that is normally distributed in a large\nrandom mating population (Fig. 1). Also, designate as the mean of a selected proportion P of\nthis population, where c is the truncation point of selection and Z is the height of the ordinate at\nthe selection truncation point. 124 | CHAPTER 9: SELECTION RESPONSE\nThe selection differential is defined as in Equation 1:\n.\nFormula for calculating selection differential.\nwhere:\n= selection differential,\n= mean of selected proportion,\n= mean of the population.\nIf is the phenotypic variance in the population, then the standardized selection differential\ncan be written as in Equation 2:\n.\nFormula for calculating standardized selection differential.\nwhere:\n= standardized selection differential; also, is the number of standard deviations\nrepresented by the selection differential, S,\n= square root of phenotypic variance in the population,\n= mean of selected proportion,\n= mean of the population.\nSelection Response\nWhile may be distinctive relative to , of greater interest are the phenotypes of the progeny\nderived from crosses among the selected parents . The predicted response of progeny to the\nselection of their parents can be derived from the relationship between parent and offspring as\nfollows (Fig. 2). Designate R as the response to selection measured in the offspring (represented as\na deviation from the population mean). S is the selection differential (represented as a deviation\nfrom the population mean) as described in the previous section. CHAPTER 9: SELECTION RESPONSE | 125\nFig. 2 Parent-offspring regression plot.\nGenetic Gain\nThe response to selection ( R ) can be written simply as in Equation 3:\n.\nFormula for calculating response to selection.\nwhere:\n= response to selection,\n= the regression coefficient of offspring on the mid-parent value,\n= the selection differential.\nThe regression coefficient of offspring on the mid-parent value can be written as in Equation 4:\nFormula for calculating the regression coefficient.\nwhere: 126 | CHAPTER 9: SELECTION RESPONSE\n= the covariance of offspring on mean of parents,\n= the variance of mid-parent,\n= the phenotypic variance.\nEquation 4 is written that way because:\nFormula for calculating the mean of parent phenotype.\nwhere:\n= the phenotype of the male parent,\n= the phenotype of the female parent.\nAlso, we can show that:\nTherefore, R is written as in Equation 6:\nFormula for calculating the change in genetic gain.\nwhere:\n= the rate of genetic gain per cycle,\n= the narrow sense heritability,\n= the standard deviation of the phenotype,\nare as defined previously.\nR is the selection response or Genetic Gain, as Lush defined it in 1940. This equation for \u0394G,\nalso known as the Breeder\u2019s Equation, based on the regression of offspring values on mid-\nparent values, is difficult to apply directly to plant breeding systems because plant breeders\ntypically evaluate hundreds of replicated individuals representing thousands of genotypes grown\nin replicated plots in dozens to hundreds of environments. Unlike most animal systems, it is\npossible to replicate progeny genotypes due to the diversity of reproductive biology that is\navailable to plant breeders: clonal propagation, doubled haploids, and tolerance to inbreeding\nthrough self-pollinations for multiple generations. In the last example, the response units can be\nseveral generations removed from the parental (crossing) generation. The type of reproductive\nbiology will affect the details of how we estimate the response to selection, \u0394G , also referred to\nc\nas the \u201cRate of Genetic Gain\u201d, per cycle. CHAPTER 9: SELECTION RESPONSE | 127\nHeritability on an Entry-Mean Basis\nRecall plant breeders often report heritability from field experiments on an entry-mean basis\nrepresented as in Equation 7:\nFormula for calculating heritability on entry mean basis.\nwhere:\n= the genotypic variance,\n= the genotype by environment variance,\n= the residual or error variance,\n= the number of replications\n= number of environments.\nAlthough Equation 7 is similar to Lush\u2019s broad sense heritability, it is not exactly the same\nconcept because it can be \u2018adjusted\u2019 by adding replicates and environments to reduce the impact\n\u03c32\nof and on the estimated phenotypic variance.\nGE\nThe problem for plant breeders is that the concept of evaluating individual plants and the\nperformance of their progeny to obtain an estimate of heritability simply is of no practical use for\nmost crops where plot performance is the basis for selection. Hanson attempted to address this\nby framing the multiple concepts of heritability within the context of genetic gain (1963).\nHanson defined heritability as \u201cthe fraction of the selection differential expected to be gained\nwhen selection is practiced on a defined reference unit.\u201d Given the standard definition for\n2\nselection response is , we can then solve for h using the\nexpression in Equation 8:\nFormula for estimating realized heritability, 128 | CHAPTER 9: SELECTION RESPONSE\nwhere:\nare as defined previously.\nThat is the standardized response to selection or realized heritability.\nContext of Heritability\nWithin the framework of genetic gain, Hanson defined heritability in such a manner as to be\nconsistent with the original concept while at the same time taking into consideration that it has\nlittle meaning unless the selection units (entry means) and response units are defined. Thus, when\nplant breeders wish to communicate information about heritability, they should specify:\n1. A reference population of genotypes.\n2. A reference population of environments. i.e., the target environments.\n3. Selection units\n4. Response units\nThis context emphasizes the purpose of obtaining variance component estimates, usually for the\npurpose of comparing genetic gains (\u0394G) under various possible breeding procedures. The results\nare used to make decisions about which procedure to employ. Indeed, it is in this context that\nvariance components of heritability are used as \u201cplug-in values\u201d (Sprague and Eberhart, 1977) for\na six-step decision-making algorithm that uses \u0394G as an arbiter for comparing breeding methods\n(Fehr, 1994; Chapter 17). Actually, this back-of-the-envelope algorithm is fairly insensitive to the\nestimated heritability values, and there are more effective means of optimizing genetic gain,\nnumber of generations, and costs. CHAPTER 9: SELECTION RESPONSE | 129\nHolland\u2019s Synthesis\nA thorough review of heritability and how it should be interpreted to compare \u0394G by plant breeders\nwas given by Holland et al, (2003). The review was essentially an update to a review by Nyquist (1991),\nwhere the updates were based on computational techniques, REML in particular, for obtaining\nappropriate estimates of variance components. He indicated that plant breeders have traditionally\nused the method of moments (covered in later slides) to estimate genotypic and phenotypic\ncorrelations between traits on the basis of a multivariate analysis of variance (MANOVA) and pointed\nout the key drawbacks of using the method that include the possibility of obtaining estimates outside\nof parameter bounds, reduced estimation efficiency, and ignorance of the estimators\u2019 distributional\nproperties when data are missing.\nWith Hanson\u2019s response, the response to selection can be rewritten as , where is\nthe regression coefficient of the response units on the selection units and is equal to\n.\nFamily Structure\nAssume our selection and response units are represented by some family structure, say half-\nsibs, or full-sibs, or recombinant inbred lines, as examples. Also, recall that we can equate the\ngenotypic variance component, designated as f for family relationships, to the genetic covariance\nof relatives. Thus, the Cov(R,S) = Var(f). Also, note that the Var(S) is the phenotypic variance among\nthe entry means. Thus, B is the proportion of variance among family units relative to the\nSR\nphenotypic variance among entry means. We might refer to this as the heritability of the family\nunits represented in Equation 9:\nFormula for estimating heritability of family units,\nwhere:\n= the family unit variance,\n= the phenotypic variance.\nIf the replicated plots consist of half-sibs from a random mating population, then the variance 130 | CHAPTER 9: SELECTION RESPONSE\ncomponent among half-sibs on an entry mean basis is equal to the covariance of the half-sibs\n(Equation 10), ignoring epistasis:\nFormula for estimating covariance of half-sibs, ignoring epistasis,\nwhere:\n= the inbreeding coefficient\n= the additive variance.\nNarrow-Sense Heritability of Half-Sibs\nThus, it is possible to utilize the estimated variance components from an ANOVA to estimate\n2\na \u201cnarrow sense heritability\u201d, h , by simply multiplying this variance component by 4/(1+F) and\nplugging the value into Equation 7 as in Equation 11; all terms are as defined previously:\nFormula for estimating narrow sense heritability of half-sibs.\nNotice that this is not the same as the original narrow sense heritability as defined by Lush (1940),\nbut is a narrow sense heritability for a population of half-sibs.\nNext, consider the numerator in Equation 11 above. In the case of half-sibs, we have learned that\nthe variance of family units is represented as in Equation 12.\nAlternative formula for estimating family units variance,\nwhere:\n= the additive by additive interaction variance. CHAPTER 9: SELECTION RESPONSE | 131\nCovariance Estimation\nAgain, if the data are not balanced, the variance component will not be estimated correctly unless\nREML is used. Let us assume that we obtain a \u2018best\u2019 estimate for \u03c3 , either because our data\nHS\nare balanced or we have used REML. Should we use the previous equation for the Cov(R,S)? To\nanswer this, we have to recognize that there is a genetic relationship between selection units\nand response units, i.e., there is a pedigree relationship or coefficient of coancestry between the\nselection and response units, and Equation 12 does not take this into consideration. In the case\nwhere both selection units and response units are half-sibs, the Cov(R,S) is represented as in\nEquation 13\nFormula for estimating covariance of response units and selection units,\nwhere:\nare as defined previously.\nNote that if Equation 13 is used, a slightly biased estimate of heritability will result even if the\nbest estimates of variance components are obtained. This is due to epistatic variance. For other\ntypes of progeny, the bias in the numerator can be much larger. Let us look at estimates based on\nEquation 13 for some example cases/progeny types.\nExample A\nEstimation of Narrow sense heritability from a half-sib family experiment with data obtained on\nindividual plants in one environment.\n1. Heritability on an individual plant basis\n\u25e6 Selection among individual plants\n\u25e6 1 Replication in 1 environment\n\u25e6 Response is measured in outbred progeny 132 | CHAPTER 9: SELECTION RESPONSE\nwhere: is the estimated family by environment interaction variance; is the estimated\nerror variance, and is the estimated within family variance.\nExample B\nEstimation of Narrow sense heritability from a half-sib family experiment with data obtained on\nindividual plants in multiple independent environments.\n2. Family heritability on a plot basis (half-sib family, single plot mean values)\n\u25e6 Selection among plot means\n\u25e6 1 Replication in 1 environment\n\u25e6 Response is measured in outbred progeny\nwhere: is the number of entries or plots; all other terms are described in example A. CHAPTER 9: SELECTION RESPONSE | 133\nComputational Considerations\nExample C\nEstimation: Narrow sense heritability estimated from a half-sib family experiment with data\nobtained on individual plants in multiple independent environments.\n3. Family heritability\n\u25e6 selection among half-sib family mean averaged over environments\n\u25e6 outbred progeny\nThe only way to remove the bias is to include both selection units and response units in the\nanalyses. This is not the same thing as including both groups in the same sets of\nenvironments.\nMethod of Moments\nNext, let us explore the computational nuances of these concepts in the context of plant breeding\npopulations. Consider first the evaluation of half-sibs from a random mating population in\na replicated Multi-Environment Trial. Let the phenotypic variance of the selection units be\ndesignated \u03c3 2 . From an introductory course in statistics, we were taught that the phenotypic\np\nvariance on an entry means basis can be obtained directly from Ordinary Least Squares (OLS)\nANOVA by equating the estimated Mean Squares (MS) with Expected Mean Squares (EMS). This\nis also known as the Method of Moments (MoM). Thus, an estimate of phenotypic variance, \u03c3 2\np\nrepresented as in Equation 14: 134 | CHAPTER 9: SELECTION RESPONSE\nFormula for estimating phenotypic variance,\nwhere:\n= the mean square, i.e., family variance,\nare as defined previously.\nWhen to Use Method of Moments\nIt turns out that the application of MoM is appropriate only if the data are from a balanced\nexperiment, i.e., the number of genotypes, in this case, families or genotypic entries, is the same\nacross reps and environments. Recall that lsmeans are useful for estimates of entry means in the\ncase of unequal replication per environment. Next, we need to learn how to obtain estimates of\nthe variance components for unbalanced data sets.\nThe most obvious problem is that the coefficients of the variance components are not equal to\nthe products of the numbers of reps and environments in the EMS. Addressing this problem is\nfairly straightforward (Milliken and Johnson, 1992). A more difficult problem is that the estimates\nof the variance components themselves are no longer the \u201cbest\u201d estimates. The solution, as\ndescribed by Holland et al (2003) is to obtain Restricted Expected Maximum Likelihood (REML)\nestimates in a Mixed Model Procedure (MMP).\nREML\nFor example, let us consider the case of half-sib progeny. Recall Equation 12:\nIf the data are not balanced, the variance component will not be estimated correctly unless REML\nis used. Let us assume that we obtain a \u2018best\u2019 estimate for \u03c3 ; either because our data are\nHS\nbalanced or we have used REML. Should we use Equation 8 for the Cov(R,S)? To answer this,\nwe have to recognize that there is a genetic relationship between selection units and response\nunits, i.e., there is a pedigree relationship or coefficient of coancestry between the selection and\nresponse units, and Equation 12 does not take this into consideration. In the case where both\nselection units and response units are half sibs, the Cov(R,S) is represented as in Equation 13: CHAPTER 9: SELECTION RESPONSE | 135\nThus, if Equation 13 is used, a slightly biased estimate of heritability will result even if REML-\nbased estimates of variance components are obtained. For other types of progeny, the bias in the\nnumerator can be much larger. Thus, the predicted genetic gain that might be used for planning\npurposes or comparison of possible breeding methods will be overestimated.\nReferences\nFehr, W. R. 1993. Principles of Cultivar Development. Vol. 1. Theory and Techniques. Macmillian\nPublishing Company.\nHolland, J.B., W.E. Nyquist, and C.T. Cervantes-Mart\u00ednez. 2003. Estimating and interpreting\nheritability for plant breeding: An update. Plant Breed. Rev. 2003:9\u2013112.\nLush, J. L. 1940. Intra-sire correlations or regressions of offspring on dam as a method of\nestimating heritability of characteristics. Am. Soc. Anim. Prod. Proc. 33: 293-301.\nMilliken, G. A., and D. E. Johnson. 1992 Analysis of Messy Data: Vol I, Design Experiments,\nChapman & Hall/CRC, London.\nNyquist, W. E. 1991. Estimation of heritability and prediction of selection response in plant\npopulations. Crit. Rev. Plant Sci. 10:235\u2013322.\nSprague, G. F., and S. A. Eberhart. 1977. Corn breeding. p. 305-362. In. G. F. Sprague and J. W.\nDufley (ed) Corn and corn improvement Agron. Monogr. 18. ASA, CSSA, and SSSA, Madison, WI.\nHow to cite this chapter: Beavis, W., K. Lamkey and A. A. Mahama. 2023. Selection Response. In W.\nP. Suza, & K. R. Lamkey (Eds.), Quantitative Genetics in Plant Breeding. Iowa State University Digital\nPress. 136 | CHAPTER 10: G X E\nChapter 10: G x E\nWilliam Beavis; Kendall Lamkey; Katherine Espinosa; and Anthony Assibi Mahama\nOne of the most difficult aspects of plant breeding involves making decisions about\nenvironments to target for the development of new cultivars. This challenge is not especially\ndifficult if cultivars are adapted to large geographic regions with little variability among\nenvironments or if there is significant variability among environments, but potential cultivars\nrespond similarly to the environmental differences. However, if potential cultivars do not respond\nsimilarly to environmental differences within a targeted set of environments, i.e., there are\ngenotype-by-environment interactions, decisions on which genotypes to develop can be difficult.\nHerein, we explore the impacts of environments on genotypes in cultivar development programs.\nLearning Objectives\n\u2022 Conceptual types of GxE interactions.\n\u2022 Decompose GxE interactions into heterogeneous variability and inconsistent rankings.\n\u2022 Leverage information on heterogeneous variance and inconsistent rankings to meet breeding\nobjectives.\nEnvironmental Components of Variance\nRecall that our working model for the phenotype includes genotypic and non-genotypic\n(environmental) sources of variability (Equation 1):\nLinear model for sources of variability in phenotype.\nwhere:\n= phenoype,\n= overall mean,\n= genotype effects ,\n= non-genetic of environment effects.\nBriefly, we consider the components of E as shown in Fig. 1. CHAPTER 10: G X E | 137\nFig. 1 Illustration of relationships among\nenvironments, genotypes, and phenotypes.\nMany Meanings of Environment\nMicro-environmental effects: the environment of a single organism as opposed to that of\nanother growing at the same time and in almost the same place.\n\u2022 All things except genotype affect a plant\u2019s development. Note that the probability that two\nplants in the same field will experience the same environment is infinitesimally small.\n\u25e6 Physical and chemical properties of the soil\n\u25e6 Climatic variables (rain, temperature, etc)\n\u25e6 Solar radiation\n\u25e6 Biotic stresses\nMacro-environmental effects: the general environment associated with a field site over a period\nof time.\n\u2022 Different class of environments in one area or time than another\n\u2022 A collection of micro-environments\nEnvironmental Sources of Variation\nEnvironmental sources of variation can also be hierarchically modeled to consist of variability\namong environments and within environments (Equation 2):\nFormula for among and within sources of variability. 138 | CHAPTER 10: G X E\nwhere:\n= environmental variance,\n= among environment variance,\n= within environment variance.\nFig. 2 No interaction type phenotype\nresponse in two environments.\nModel for phenotypic response.\nwhere:\n= phenotype,\n= overall mean,\n= genotype effect,\n= environment effect.\nFrom the beginning of field assessments of clonally propagated plants, we have been able to\nrecognize variability within and among locations (environments). As soon as we could evaluate\nmore than one replicated genotype, we also began to recognize patterns of phenotypic responses\nto environments. In 1964, Allard and Bradshaw provided a simple classification scheme of the\ntypes of phenotypic responses that could happen using two genotypes (designated A and B) and\ntwo environments (designated X and Y) and modeled as in Equation 3. The first type of response\n(Type 1) reveals that there is a difference of 2 units between the genotypes and a difference of 1 CHAPTER 10: G X E | 139\nunit between the environments (Fig. 2). They also recognized a second type of response (Type 2) in\nwhich the difference between genotypes was one unit while the difference between environments\nwas 2 units. Both types of responses indicate no interaction.\nSimple Types of GxE Interactions\nThese types of interaction can all be modeled as (Equation 4):\nModel for phenotype response with GxE present.\nwhere:\n= phenotypic response,\n= overall mean,\n= genotype effects,\n= environment effects,\n= genotype by environment interaction effect.\nType 3 GxE Interaction\nAllard and Bradshaw recognized that there could be a lack of consistent responses by two\ndifferent genotypes in two environments. The lack of consistent responses by genotypes to\ndifferent environments had been recognized as genotype by environment interactions since the\nbeginning of replicated trials, and Allard and Bradshaw classified these into four types of GxE\nfor two genotypes in two environments.\nA type 3 GxE response (Fig. 3) was based on the heterogeneity of genotypic variability between\nenvironments. Assuming that larger phenotypic values are desired, in GxE types 1,2, and 3,\ngenotype A is better adapted to both types of environments. 140 | CHAPTER 10: G X E\nFig. 3 Genotype response with\ninteraction in two environments.\nType 4 GxE Interaction\nA type 4 GxE interaction is due to a combination of heterogeneous genotypic variability and a\nfailure of the genotypes to have correlated responses (change of rank) across the environments\n(Fig. 4).\nFig. 4 Genotype response with\ninteraction (lines intersect) in two\nenvironments. CHAPTER 10: G X E | 141\nType 5 GxE Interaction\nA type 5 GxE is due to a failure of the genotypes to have correlated responses across the\nenvironments, while the genotypic variability is homogeneous between the environments (Fig.\n5). If environments X and Y represent typical types of environments in a market, then there\nare unique best genotypes for each type of environment; neither is broadly adapted to both\nenvironments.\nFig. 5 Genotype response with\ninteraction (lines intersect) in two\nenvironments.\nType 6 GxE Interaction\nA type 6 GxE interaction illustrates a \u2018racehorse\u2019 response by Genotype A. It takes full advantage\nof favorable environment X while failing under the stressful environment Y. This type of GxE also\nillustrates a more \u2018stable\u2019 response by Genotype B. The question for the plant breeder is whether\nto develop both types of cultivars or just one. 142 | CHAPTER 10: G X E\nFig. 6 Genotype response with\ninteraction (lines intersect) in two\nenvironments.\nComplex Types of GxE Interactions\nAs the number of environments and genotypes increases, the ability to sort out the types of\nGxE interactions becomes more difficult. For example, consider the data and plot in Fig. 7. It\nis likely that all types of GxE interactions are present in these data. If all types of GxE are\npresent, are there prevalent types of GxE? Do the genotypes behave the same way in some pairs\nof environments? What is the nature of GxE between all pairs of environments (1:2, 1:3, 2:3 \u2026)?\nTo answer these questions, multi-variate statistical techniques, sometimes referred to as\npattern analyses, are used to discover and summarize consistent patterns in large data sets. CHAPTER 10: G X E | 143\nFig. 7 Sample of yield values from ~100 entries taken at three environments from a 6500-entry trial grown\nat 20 environments.\nPattern Analysis Methods\nA partial listing of these pattern analysis methods would include Cluster Analyses (CA), Principal\nComponent Analysis (PCA), Additive Main and Multiplicative Interaction (AMMI) models, Sites\nRegression models, Partial Least Squares, Factorial Regression, Linear Bilinear Mixed Models,\nGeneralized Linear Models, Support Vector Machines, Bayesian Networks, Reproducing Kernel\nHilbert Spaces, etc.\nThe development of these methods was motivated by the need to find patterns in physical and\nchemical spectra 25 to 50 years ago. These methods began to be applied by ecologists in the 1970s,\nGxE challenges in plant breeding during the 1990s, and to find patterns in \u2018omics\u2019 data during the\n2000s. The application and interpretation of the methods in GxE continue to be an active area of\nresearch and well beyond the scope of an introduction to GxE.\nCluster Analyses\nHerein we introduce an application of multi-variate techniques to find patterns in GxE\ninteractions using Cluster Analyses (CA). The purpose of applying CA is to either: 1. organize the\nenvironments into homogeneous groups of environments so that there are no GxE interactions\nwithin environments and to emphasize (maximize) the differences between homogeneous groups 144 | CHAPTER 10: G X E\nof environments or 2. organize the genotypes into groups with no GxE within the groups and\nmaximize our ability to identify genotypes that have different responses to environments.\nCluster analyses require a metric that quantifies dissimilarities among all possible pairs of units\nthat we wish to cluster. There are many possible distance metrics that could be used. The most\ncommonly used metric for CA of environments, based on GxE, is the Euclidean distance which is\nbased on the Pythagoras\u2019 theorem (Fig. 8).\nFig. 8 Pythagoras\u2019 theorem extended into three-dimensional space.\nEuclidean Distance\nFor a trait such as yield, it is advisable to first standardize the values so that all of the yield values\nare on the same scale: Calculate the average and standard deviation for each location, subtract\nthe average value from the genotypic value, and divide by the standard deviation for each of the\ngenotypes by location. Next, calculate the Euclidean Distance of the standardized yield values\nbetween all possible pairs of environments.\nCA also requires that we choose a grouping algorithm. There are dozens of clustering algorithms,\nand none should be considered \u2018best\u2019 because there are no objective criteria that can be applied to\nall data sets. For purposes of interpretation using yield data with evidence of GxE, I prefer to use\nan agglomerative hierarchical clustering technique such as Ward\u2019s (also known as incremental\nsums of squares) or the Unweighted Pair Group Method with Arithmetic mean (UPGMA, also\nknown as average linkage clustering). Cooper and DeLacy (1994) prefer Ward\u2019s, but for the novice, CHAPTER 10: G X E | 145\nit is usually good to try several grouping algorithms for purposes of comparison based on the\ngoals of the breeding project.\nVariation Flux\nOne of the fundamental questions that a breeding project needs to decide is whether to develop\nbroadly adapted cultivars or specific cultivars for specific environments. Often this is determined\nby production and marketing considerations, but there is also an issue of identifying the types\nof environments that the crop will encounter within a marketing region. In order to assess the\ntypes of environments, the breeder needs to sample the total population of macro-environments\nusing a sample of genotypes. There will clearly need to be trade-offs between these two sampling\nobjectives. Decisions on the trade-offs could actually bias the results that one obtains\nbecause genotypic variances can be confounded with GxE variances and vice-versa.\nTo illustrate, consider Fig. 9, where A represents a population of macro-environments and S is a\nsubset of macro-environments.\nLet A serve as our reference population of environments.\nIt can be shown (with a little algebra) that\nFig. 9 Illustration of targeted sets of environments.\nA consequence is that if the subset population of environments, S, is made more homogeneous\n(smaller subsets of the total), then genotypic variance will increase because GS interaction\nvariance will decrease. Alternatively, expansion of the targeted subset S of environments will\nresult in a more heterogenous subset which will, in general, increase GS interaction variance at\nthe expense of genetic variance. The challenge is to subdivide an original set of environments 146 | CHAPTER 10: G X E\nso that subdivisions are clearly delineated and substantially more homogeneous. If the market\nanalysis then reveals that multiple sub-environments should be served, it will require an increase\nin the breeding effort since one breeding program needs to be replaced by multiple breeding\nprojects.\nPartition of GxE Variances\nGxE variances can be partitioned into two components:\n\u2022 due to heterogeneity of genotype variance among environments, and\n\u2022 due to lack of correlation of genetic performance among environments.\nMuir et al (1992), provided the means for calculating these two components.\nMuir\u2019s Partition Method\nGiven a model for the phenotype (Equation 5):\nModel for calculating components of phenotype.\nwhere:\n= phenotype,\n= overall mean,\n= genotype effect,\n= environment effect,\n= genotype by environment interaction effect,\n= residual (error).\nThen SS(GE) is determined as (Equation 6): CHAPTER 10: G X E | 147\nModel for calculating sum of squares GxE, SS(GE).\nwhere:\n= mean of ij\u2019s for all plots,\n= mean of i\u2019s for all jk\u2019s,\n= mean of j\u2019s for all ik\u2019s,\n= grand mean,\n= variance of i,\n= variance of i\u2019,\n= variance of ii\u2019,\n= all genotypes.\nGE Interaction Equation\nThe GE interaction can then be expressed as:\nThese results then allow the GE interaction sums of squares to be partitioned into a term due\nto heterogeneous variance, SS(HV) , and that due to imperfect positive correlation of the pair,\nii\u2019\nSS(IC) (Equation 7).\nii\u2019\nModel for calculating sum of squares GxE, SS(GE).\nwhere:\n= variance of i,\n= variance of i\u2019,\n= correlation between i and i\u2019,\n= all genotypes.\nInteraction Components\nWhile the first component can be derived from ANOVA results of single environments using the 148 | CHAPTER 10: G X E\ngenotypic variance in each environment and their average, the second component can then be\ncalculated using the estimated variance component (EMS) for G x E over all environments and\ngenetic variance component over all environments.\nExample Data Sets 1 and 2\nTo visualize the relative influence of heterogeneity of genotypic responses and lack of\ncorrelations among genotypes, consider the following example data sets.\nFig. 10 GxE due to heterogeneity of environments (data set 1) and lack of\ncorrelation among lines (data set 2).\nIn the first two examples, data sets, G x E is fully explained by one of its two components. In the\nfirst case, all G x E is due to the heterogeneity of genotypic variance among environments. In the\nsecond case, G x E is completely due to a lack of correlation of genotypic performance among\nenvironments.\nExample Data Set 3\nA more realistic example can be found in data set 3, where the GxE is due to a mixture of\nboth components. It is, however, worthwhile to look at their partial contribution; genotypic\nheterogeneity explains only 18% of GxE interaction, and lack of correlation (consistent ranking)\nexplains 82%. CHAPTER 10: G X E | 149\nFig. 11 GxE due to both heterogeneity of environments and lack of correlation\namong lines.\nIn the early stages of a breeding project, thousands of genotypes might be evaluated in two or\nthree environments, or a few hundred genotypes might be evaluated in dozens of environments.\nIn such situations, the simple graphical representations (Figs. 2-6) and partitioning of GxE\nvariances (Equations 1-5) become very difficult to interpret.\nAlternative Analyses\nThere are alternative analyses and visualization techniques that are used to interpret data from\nlarge numbers of genotypes grown in large numbers of environments. For example,\npattern analyses employ measures for similarity or dissimilarity to group environments and lines\nfor interpretable graphical representations of either genotypic or environmental performance.\nConsider the following as illustrated examples based on the data represented in Fig. 11.\nFig. 12 Grouping of environments based on similar genotypic responses.\nIn Fig. 12, we have clustered Environments A and C because the genotypes respond to these 150 | CHAPTER 10: G X E\nenvironments almost identically and very differently from the manner in which they respond to\nenvironment B.\nGrouping Similar Responses\nIn Fig. 12, we also recognize that Genotypes (lines) 2 and 4 respond to the environments in a\nsimilar manner, so we cluster these together and represent the response patterns of genotypes as\n3 distinct patterns.\nFig. 13 Grouping of lines with similar responses in 3 environments.\nGrouping of Lines and Environments\nIn Figures 12 and 13, either lines or environments are grouped for similar performance. In Fig.\n14, both groupings are shown in the same graph. Simple means of groups were taken to give an\nexample for simplification of G x E interactions.\nFig. 14 Grouping of lines and environments.\nFor more complex data sets, measures for similarity and dissimilarity of the performance of\ngenotypes can be used to summarize differences in genetic performance of the genotypes in\nenvironments j and j\u2019. We can denote such difference measures as D . We can also consider a\ng(jj-)\nmeasure of a difference, designated as D , between environments j and j , the way in which\ne(ii\u2019-) m\nthey discriminate between the genetic performance of genotypes. DeLacy and Cooper (1990) and CHAPTER 10: G X E | 151\nDeLacy et al. (1990a) discussed alternative forms of D , which have been used for pattern\ne(jj-)\nanalysis of relationships among environments in METs (Cooper and Delacey 1994).\nFlux between Genotypic Variance and GE Interaction\nVariance\nOne of the fundamental questions that a breeding project needs to decide is whether to develop\nbroadly adapted cultivars or specific cultivars for specific environments. Often this is determined\nby production and marketing considerations, but there is also an issue of identifying the types of\nenvironments that the crop will encounter within a marketing region. In order to assess the types\nof environments, the breeder needs to sample the total population of macro-environments using\na broad sample of genotypes. There will clearly need to be trade-offs between these two sampling\nobjectives. Decisions on the trade-offs could actually bias the results that one obtains because\ngenotypic variances can be confounded with GxE variances and vice-versa.\nTo illustrate, consider Fig. 9 above, where A represents a population of macro-environments and\nS is a subset of macro-environments.\nLet A serve as our reference population of environments.\nIt can be shown (with a little algebra) that .\nA consequence is that if the subset population of environments, S, is made more homogeneous (a\nsmaller subset of the total), then genotypic variance will increase because GS interaction variance\nwill decrease. Alternatively, expansion of the targeted subset S of environments will result in\na more heterogeneous subset which will, in general, increase GS interaction variance at the\nexpense of genetic variance. The challenge is to subdivide an original set of environments so that\nsubdivisions are clearly delineated and substantially more homogeneous. If the market analysis\nthen reveals that multiple sub-environments should be served, it will require an increase in the\nbreeding effort since one breeding program needs to be replaced by multiple breeding projects.\nImpact of Multiple Environments\nFrom an introductory course in statistics, we were taught that the phenotypic variance on an\nentry means basis can be obtained directly from Ordinary Least Squares (OLS) ANOVA by\nequating the estimated Mean Squares (MS) with Expected Mean Squares (EMS). This is also\nknown as the Method of Moments (MM see Review Module on Statistical Inference). Thus, an\nestimate of phenotypic variance, , on an entry mean basis is equal to 152 | CHAPTER 10: G X E\nFormula for estimating phenotypic variance on entry mean basis.\nwhere:\n= mean square value of genotype (g) in the ANOVA table,\n= genotypic variance,\n= genotype by environment interaction variance,\n= error variance of the error term, ,\n= number of replications.\nThe ability to replicate genotypes and grow them within plots that can be replicated enables the\nplant breeder to \u201cadjust\u201d their precision around their estimates.\nVariance Component Estimation Example\nLet us consider a typical plant breeding field trial in which location and year combinations are\nconsidered unique environments. (Table 1) Let there be e combinations of years and locations.\nAlso, assume the genotypes, g, are grown at the same locations each year.\nTable 1 Expected means squares for estimating variance components.\nMean\nSource d.f. EMS\nSquare\nEnvironments (E) n/a n/a\nReps within E (R) n/a n/a\nGenotypes (G) M1\nG x E M2\nResidual M5\nEstimators\nEmploying the MM approach, we can obtain estimates of the variance components and, thus, an\nestimate of the Covariance of the genotypic units (Table 2). CHAPTER 10: G X E | 153\nTable 2 Estimating variances.\nFunction Variance Estimated\nApplication Notes\nApplication of the MM is appropriate only if the data are from a balanced experiment, i.e., the\nnumber of genotypic units is the same across reps and environments. In the review chapter, we\nemployed lsmeans to obtain adjusted estimates of entry means in the case of unequal replication\nper environment. However, we did not learn how to obtain estimates of the variance components\nfor unbalanced data sets.\nIf there are only a few missing values (say < 5%) from some replicates, then the impact on the\nestimates of variance components will not be very great. However, we often design experiments\nto take advantage of seed supplies which may vary greatly among our genotypic units. In such\ncases, the coefficients of the variance components are not equal to the products of the numbers\nof reps and environments represented in the EMS. Addressing this issue is fairly straightforward\n(Milliken and Johnson, 1992). A more difficult problem is that the estimates of the variance\ncomponents themselves are no longer the \u201cbest\u201d estimates. The solution, as described by Holland\net al (2003) is to obtain Restricted Maximum Likelihood (REML) estimates in a Mixed Model\nProcedure (MMP).\nReferences\nCooper, M., and J.H. Delacy. 1994. Relationships among analytical methods used to study\ngenotypic variation and genotype-by-environment interaction in plant breeding multi-\nenvironment experiments. Theoretical and Applied Genetics 88: 561\u2013572.\nDeLacy, I. H., and M. Cooper. 1990. Pattern analysis for the analysis of regional variety trials.\nIn: Kang MS (ed) Genotype-by-environment interaction and plant breeding. Louisiana State\nUniversity, Baton Rouge, Louisiana, pp. 301\u2013334.\nDeLacy I. H., M. Cooper, and P. Lawrence. 1990a. Pattern analysis over years of regional variety 154 | CHAPTER 10: G X E\ntrials: relationship among sites. In: Kang MS (ed) Genotype-by-environment interaction and plant\nbreeding. Louisiana State University, Baton Rouge, Louisiana, pp 189\u2013213.\nHolland, J.B., W.E. Nyquist, and C.T. Cervantes-Mart\u00ednez. 2003. Estimating and interpreting\nheritability for plant breeding: An update. Plant Breed. Rev. 2003:9\u2013112.\nMilliken, G. A., and D. E. Johnson. 1992 Analysis of Messy Data: Vol I, Design Experiments,\nChapman & Hall/CRC, London.\nHow to cite this chapter: Beavis, W., K. Lamkey, K. Espinosa, and A. A. Mahama. 2023. G x E. In W. P.\nSuza, & K. R. Lamkey (Eds.), Quantitative Genetics for Plant Breeding. Iowa State University Digital\nPress. CHAPTER 11: MULTIPLE TRAIT SELECTION | 155\nChapter 11: Multiple Trait Selection\nWilliam Beavis; Kendall Lamkey; and Anthony Assibi Mahama\nMost cultivar improvement programs involve selection for multiple traits. Depending on the\nprogram or project goals, one of three types of multiple-trait selection can be employed. A brief\ndescription of these types follows.\n\u2022 Multistage selection: Selection for different traits at different stages during cultivar\ndevelopment.\n\u2022 Tandem selection: Selection for one trait until that trait is improved, then for a second,\netc., until finally each has been improved to the desired level.\n\u2022 Independent culling levels: A certain level of merit is established for each trait, and all\nindividuals below that level are discarded regardless of values for other traits.\n\u2022 Index selection: Select for all traits simultaneously by using some index of net merit.\nLearning Objectives\n\u2022 Explain the role of selection for multiple traits on genetic gain.\n\u2022 Learn methods for selecting multiple traits.\n\u2022 Learn how to evaluate the efficiency and effectiveness of selection on multiple traits.\nIndex Selection\nAn index is the best linear prediction of an individual\u2019s or line\u2019s breeding value and takes the\nform of the multiple regression of breeding value on all sources of information.\nThe objective of an index is to find the linear combination of phenotypic values that maximizes\nthe expected gain or, equivalently, that maximizes the correlation between the index value and\ntrue worth (breeding value).\nIndex Selection Theory\nDefine phenotype as affected by components in Equation 1. 156 | CHAPTER 11: MULTIPLE TRAIT SELECTION\nLinear model for sources of variability in phenotype.\nwhere:\n= the observed value of the attribute for an individual or line,\n= the average of phenotypic values over a population of environments,\n= non-genotypic contributions from environments.\nNote: Genotype x environment interactions are permitted, assuming genotypes and\nenvironments are associated entirely at random; such interactions are incorporated into E\ni\n(Equation 1). If GxE are not random, then see Cooper and DeLacy (1994).\nAssume G is composed entirely of additive effects of genes (breeding values). Define the\ni\ngenotypic economic value, H, of an individual as written in Equation 2:\nLinear model for sources of variability in phenotype.\nwhere:\n= known relative economic values.\nAssume the quantities P and H are such that the regression of P on H is linear. Selection will\ni i\nthen be based on the linear function, I (Equation 3).\nLinear model for index selection.\nwhere:\n= regression coefficient,\nare as defined previously.\nAssumptions\nAssume an equal amount of information on all individuals to be evaluated and selected. Also,\nassume that the distributions of P, G, and E are unknown but that the mean and covariances\ni i i\nare known.\nThen, CHAPTER 11: MULTIPLE TRAIT SELECTION | 157\nMean and Covariance of H and I\nWith these assumptions, we can derive the mean and covariance of H and I as in the set of\nequations written in Equation 4.\nFormulae for estimating mean, variance, and covariance of H, I, and G.\ni\nwhere:\nterms are as defined previously.\nThe objective of a selection index is to use some linear combination of trait values (I) to predict\ntrue genetic worth (H).\nThis can be accomplished by:\n\u2022 maximizing expected genetic gain.\n\u2022 maximizing the correlation of the sample index (I) with true worth (H).\n\u2022 maximizing the probability of correct selection.\n2\n\u2022 minimizing the E(I-H) . 158 | CHAPTER 11: MULTIPLE TRAIT SELECTION\nWilliams (1962) showed that maximizing the correlation between I and H also maximizes the\nexpected genetic gain and the probability of correct selection.\nDerivation of the Optimum Index\nMaximizing correlation of I with H (Equation 5):\nEquation for estimating the correlation between I and H.\nwhere:\nterms are as defined in equation 4.\nIt can be shown that maximizing r is equivalent to maximizing log(r ) (Equation 6).\nIH m\nEquation for maximizing correlation of I with H.\nwhere:\nterms are as defined previously.\nUsing least squares and differentiating with respect to b, we get Equation 7:\nj\nCalculating the correlation of I with H using least squares techniques.\nwhere:\nall terms are as defined previously.\nRewriting the Normal Equations\nThese equations are called the normal equations (unrelated to normal distribution) and constitute\nn equations in n unknowns. CHAPTER 11: MULTIPLE TRAIT SELECTION | 159\nThey can be rewritten as in Equation 8:\nCalculating the regression coefficient and observed value.\nwhere:\nall terms are as defined previously.\nBecause we are only interested in relative values of b, the constant term can be dropped, resulting\ni\nin Equation 9:\nDropping the constant values before calculating the regression coefficient and\nobserved value.\nwhere:\nall terms are as defined previously.\nFurther Calculations\nConsidering 2 traits (n=2), Equation 9 is written as:\nSolving the above equations, we get Equation 10:\n;\n.\nCalculating the regression coefficient,\nwhere:\nall terms are as defined previously. 160 | CHAPTER 11: MULTIPLE TRAIT SELECTION\n2\nMinimizing E(I-H)\n2\nThe same equations can be derived to get Equation 9 by minimizing E(I-H) as written below:\nbecause,\nApplying least squares:\nDividing through by 2 and rearranging, we get the normal equations:\nwhich are identical to the equations presented previously.\nExpected Genetic Gains\nTo derive the expected genetic gains, we need to make assumptions about the distributions of\n, , and .\nAssume:\n1. , , and are distributed normally with the mean and covariance structure given\nearlier.\n2. Truncation selection on .\nThen, the expected genetic gain is estimated with Equation 11: CHAPTER 11: MULTIPLE TRAIT SELECTION | 161\nCalculating the genetic gain of change in genetic worth,\nwhere:\n= the genetic gain, that is, the change in genetic worth,\n= the regression coefficient of H on I, which gives the mean value of H or any\nvalue of I.\nThis is the standard way to calculate predicted gains from univariate selection. See, for example,\nEmpig et al. (1972).\nTruncation Selection\nThe situation with regard to truncation selection is based on the following where,\n\u012a is the mean value of the index in the population; c is the truncation point; z is the height of\nordinate of the standard normal curve at the truncation point c;\nP is the proportion of the population selected, and \u012a is the mean of the selected individuals.\ns\nThen, the value of the frequency of the index is estimated with Equation 12:\nCalculating the genetic gain of change in genetic worth,\nwhere:\n= the frequency of individuals with index value I\nare as defined previously.\nSelection Relationships\nThe proportion saved is related to the truncation point by , and the mean\nvalue of the selected group is .\nThe selection differential (D) is given by Equation 13: 162 | CHAPTER 11: MULTIPLE TRAIT SELECTION\nFormula for calculating the selection differential, D,\nwhere:\nare as defined previously.\nStandardized Regression Coefficient\nLet,\n;\nthen, the standardized regression coefficient is derived as in Equation 14:\n;\n;\nCalculating the standardized regression coefficient,\nwhere:\n= the height of the ordinate at the truncation point.\nTypically, this is represented as , where, k is the standardized regression\ncoefficient.\nExpected Gain\nExpected gain can then be represented as in Equation 15:\n, CHAPTER 11: MULTIPLE TRAIT SELECTION | 163\nFormula for expected genetic gain,\nwhere:\n,\nFrom the normal equations derived earlier, we have\nSubstituting these terms,\n, we get\nexpected genetic as written in Equation 16:\n,\nAlternative formula for expected genetic gain,\nwhere:\nare as defined previously.\nPredicted Gain\nThe predicted gain is more useful when written in terms of the correlation between H and I\ndesignated as r (Equation 17):\nHI\n,\nFormula for predicted genetic gain,\nwhere:\n, as defined previously. 164 | CHAPTER 11: MULTIPLE TRAIT SELECTION\nIn the selection index literature, r is called the accuracy of selection because it is a measure of\nHI\nhow well the index, I, measures the true worth, H.\nAlternative selection indices, I, can be compared using r as long as the selection goal, H,\nHI\nremains the same for each of the indices.\nExpected Genetic Gains for Each Trait\nLet \u0394G be the expected genetic gain in trait i when selection is on I. From representations in\ni\nprevious equations as below, the expected genetic gains can be obtained as written in Equation\n18.\n,\nFormula for expected genetic gains for each trait,\nwhere:\n,\n,\n.\nThis index requires that you know the true values of the population parameters. However,\nestimates of the population parameters are often substituted for the true values, and the resulting\nindex is called the estimated index or the Smith-Hazel index.\nMatrix Representation of Selection Indices\nWith this notation, the normal equations can be written as .\nSome Results\nwhere G_i is the ith row of G. The genetic gain can be written as in Equation 19: CHAPTER 11: MULTIPLE TRAIT SELECTION | 165\n,\nMatrix notation for expected genetic gains,\nwhere:\n,\nother terms are as defined previously.\nConstruction of a Selection Index\nOptimum Index\nThe normal equations can be written for an optimum index as ,\nwhere P, G, and a are known without error, and the index is as in Equation 20:\n,\nFormula for optimum index,\nwhere:\n= trait x,\n= trait y.\nSmith-Hazel Index\nThis index is the same as the optimum index; only in this case (Equation 21), we use estimates of\nP, G, and a, designated as , , and , respectively.\n,\nFormula for Smith-Hazel index, 166 | CHAPTER 11: MULTIPLE TRAIT SELECTION\nwhere:\nare as defined previously.\nBase Index\nThe Base Index was apparently first suggested by Brim et al. (1959) and named the Base Index\nby Williams (1962). The base index is constructed simply by allowing b = a and written as in\nEquation 22, with all terms defined earlier.\n,\nFormula for Smith-Hazel index,\nSome results for this index include:\nThe foremost attribute of this index is its simplicity of construction and interpretation. Also, this\nindex does not require the estimation of genetic parameters.\nMultiplicative Index\nThe multiplicative index was first proposed by Elston (1963). This index is also sometimes called\nthe weight-free index because it does not require the specification of index weights or economic\nvalues.\nThe general form of this index is as in Equation 23:\n,\nFormula for multiplicative index,\nwhere:\n= the minimum value of trait X set by the breeder.\n1\nIn addition to being weight-free, this index also does not require the estimation of genetic or\nphenotypic parameters. Because this is a curvilinear index, theory is not available to predict\ngains. Baker (1974) found that this index can be approximated by using a linear index, where\nthe weights are the reciprocals of the phenotypic standard deviations of the traits in the index. CHAPTER 11: MULTIPLE TRAIT SELECTION | 167\nThis essentially amounts to an index with equal weighting per phenotypic standard deviation.\nApproximate, predicted gains can then be obtained for this index using the Smith-Hazel index\ntheory.\nDesired Gain Index\nThe desired gain index was suggested by Pesek and Baker (1969). This index allows the breeder to\nspecify a vector of desired gains, , and then substitute this into the predicted gain equation and\nsolve for the index weights. The solution for is as in Equation 24:\n,\nFormula for desired gain index,\nwhere:\n= the vector of desired gains,\nis as defined previously.\nThis index was proposed to eliminate the need to specify economic weights. However, in practice,\nthere are some difficulties with the index in specifying the vector of desired gains.\nThis index will result in maximum gains in each trait according to the relative importance\nassigned by the breeder in specifying the desired gains.\nPredicted gains can be obtained by substituting the vector of index weights into the conventional\nSmith-Hazel predicted gain equations.\nRestricted Selection Index\nRestricted selection indices were first derived by Kempthorne and Nordskog (1959). Since then,\nvarious restricted indices have been derived by Cunningham et al. (1970) and James (1968). See\nLin (1978) for a complete list. Basically, restricted selection indices involve holding the genetic\ngains in one or more traits to a constant or zero while changing the means of other traits in\nthe desired direction. The basic method is to impose the restriction on the index equations that\n.\nThe simplest procedure to accomplish this was given perhaps by Cunningham et al. (1970). Their\nmethod involved solving the following set of equations in Equation 25: 168 | CHAPTER 11: MULTIPLE TRAIT SELECTION\n,\nFormula for restricted selection index,\nwhere:\n= ,\n= ,\nare as defined previously.\nb is the vector of index weights to use in the index equation as , as before. The dummy\nvariable is not used in the index equation.\nThis method has the interesting consequence that the value obtained for the dummy variable\nis the negative of the economic weight needed to produce zero change in that trait in an\nunrestricted selection index.\nRank Summation Index\nThe rank summation index was first suggested by Mulamba and Mock (1978). Basically, this index\ninvolves obtaining the ranks of each of the traits to be included in the index and then calculating\nthe index by summing up the trait ranks, represented in Equation 26.\nFormula for the rank summation index,\nwhere:\nare as defined previously.\nThe primary advantages of this index are that genetic parameters need not be calculated, it\ntransforms the data so that the variances for each trait are identical, and it does not require the\nspecification of economic weights, although they can be used.\nAs with the multiplicative index, predicted gains cannot be calculated for this index. However,\nCrosbie et al. (1980) found that the same prediction equation used for the multiplicative index\nprovides a reasonably good approximation of the predicted gains for the rank summation index. CHAPTER 11: MULTIPLE TRAIT SELECTION | 169\nSelection Index Efficiency\nMethods to Compare Selection Index Efficiency\nCunningham (1969) provided a method for comparing the relative efficiencies of selection\nindices. He was primarily interested in deleting traits from the index so that their relative\ncontribution to the gain in the true worth (H) could be determined. Dropping traits from the\nindex means that fewer genetic parameters need to be estimated, providing considerable cost\nsavings.\nDefine the index containing all the traits of interest as the original index and define the index\nth th\nwith one trait dropped out as the i reduced index. Then the efficiency of the i reduced index\nrelative to that of the original index is the ratio of their standard deviations. Cunningham showed\nthis to be as in Equation 27:\nFormula for selection index efficiency,\nwhere:\nth\n= the i weighting factor in the original index,\n= the corresponding diagonal element in the inverse of P.\nth\nA more usual procedure is to compare the gain for the i trait when selection is on I, relative to\nth\nthe single trait selection for the i trait.\nEffect of Correlations on Index Weights\nTo determine the effects of correlation on index weights, we need to derive the index equations\nPb = Ga in terms of genetic and phenotypic correlation coefficients (Equations 28, 29, 30)\nfollowing the series of derivations as follows. Let, 170 | CHAPTER 11: MULTIPLE TRAIT SELECTION\nDerivation\nFormula for phenotypic and genotypic correlations,\nwhere:\nare as defined previously.\nFor 2 traits (n=2),\nFormula for estimated correlation between two traits,\nwhere:\nare as defined previously.\nP G\nWhen correlations are zero: r = r = 0,\n12 12 CHAPTER 11: MULTIPLE TRAIT SELECTION | 171\nFormula for estimated correlation between two traits when phenotypic and\ngenetic correlations are zero,\nwhere:\nare as defined previously.\nWhen | r | < 0.30, the use of the above index is nearly as efficient as using the Smith-Hazel index.\nReferences\nBrim, C. A., H. W. Johnson, and C. C. Cockerham. 1959. Multiple selection criteria in soybeans.\nAgron J 51: 42-46.\nCrosbie, T. M., J. J. Mock, and O. S. Smith. 1980. Comparison of gains predicted by several\nselection methods for cold tolerance traits in two maize populations. Crop Sci. 20:649-655.\nCunningham, E. P. 1969. The relative efficiencies of selection indexes. Acta Agriculturae\nScandinavica 19(1): 45-48.\nElston, R. C. 1963. A weight-free index for the purpose of ranking or selection with respect to\nseveral traits at a time. Biometrics 19(1): 85-97.\nJames, J. W. 1968. Index selection with restrictions. Biometrics 24, 1015-1018.\nKempthorne, O., and A. W. Nordskog. 1959. Restricted selection indices. Biometrics 15:10-19.\nLin, C. Y. 1978. Index selection for genetic improvement of quantitative characters. Theoretical\nand Applied Genetics, 52(2): 49-56.\nMulamba, N N., and J. J. Mock. 1978. Improvement of yield potential of the Eto Blanco maize (Zea\nmays L.) population by breeding for plant traits. Egypt. J. Genet. Cytol. 7: 40-51.\nPesek. J., and R. J. Baker. 1969. Desired improvement in relation to selection indices. Can J Plant\nSci, 9: 803\u2013804.\nWilliams, J. S. 1962. The evaluation of a selection index. Biometrics 18:375-393.\nHow to cite this module: Beavis, W., K. Lamkey, and A. A. Mahama. 2023. Multiple Trait Selection. In\nW. P. Suza, & K. R. Lamkey (Eds.), Quantitative Genetics in Plant Breeding. Iowa State University Digital\nPress. 172 | CHAPTER 12: MULTI ENVIRONMENT TRIALS: LINEAR MIXED MODELS\nChapter 12: Multi Environment Trials: Linear\nMixed Models\nWilliam Beavis and Anthony Assibi Mahama\nThe general linear model (Equation 1) can be applied to replicated trial data for basic prediction\npurposes. It is, however, not adequate in all experimental situations, especially where trials are\nconducted in more than one environment. This chapter explores the role of mixed linear models,\nBLUEs (Best Linear Unbiased Estimates), and BLUPs (Best Linear Unbiased Predictors) in the\nanalysis of multiple environment trial data to characterize and select among entries.\nGeneral linear model for basic prediction of replicated trials.\nwhere:\n= the phenotypic response,\n= population (or overall) mean,\n= genotypic units,\n= number of replicates of the genotypic units,\n= residual source of variability.\nLearning Objectives\n\u2022 Conceptual basis of mixed linear models\n\u2022 Review matrix algebra\n\u2022 The meaning of BLUE and BLUP\nHenderson\u2019s Concept\nC.R. Henderson recognized the challenge of prediction using models such as Equation 1 and\naddressed it using the concept of shrinkage estimators for the genotypic units in the model. Note\nthat the fitted regression line provides predictions that are \u2018shrunken\u2019 to the line rather than\nscattered around the line. Henderson\u2019s idea, first published in 1963, was framed in the context of\nthe matrix form of Equation 2, which can be explained using scalar algebra.\nFirst, let us obtain phenotypic averages for each genotypic unit. Next minimize the difference CHAPTER 12: MULTI ENVIRONMENT TRIALS: LINEAR MIXED MODELS | 173\nof N , where E represents the expectation, that is, the average for the genotypic unit, m is the\n1\npopulation mean and g is the genotypic value from the scalar version of model Equation 2. In this\ni\ncase, we need to find a value that will ensure that the sum of the squared differences is minimal.\nAs with Equation 2, a little knowledge of how to obtain partial derivatives provides the answer:\nFormula for calculating intra-class correlation.\nwhere:\n= intra-class correlation coefficient or broad sense heritability,\n= genotypic variance,\n= residual ( or error) variance,\n= number of replications.\nThis is known as the intra-class correlation coefficient. It is also known as the broad sense\nheritability, but for now we will refer to it as a shrinkage factor. When w is multiplied by\ni\n(Y \u2013 m) it will provide the Best Linear Unbiased Predictor of g. Notice that the predictions of\ni i\ngenotypic values are scaled towards the mean BV, which by definition is zero.\nExample Prediction 1\nIf the overall mean is the only fixed effect (one environment), all lines are unrelated to each other,\nand the data are balanced, then the predicted genotypic value is obtaining using Equation 3:\nFormula for calculating predicted genotypic value.\nwhere:\n= predicted genotypic value of genotypej,\n= the Shrinkage factor,\n= the phenotype of genotype j,\n= the predicted phenotype.\nIf is equal to zero, would be zero. 174 | CHAPTER 12: MULTI ENVIRONMENT TRIALS: LINEAR MIXED MODELS\nIf is equal to one, equals the phenotypic values.\nLet us demonstrate this with a simple data set in which four unrelated lines (A, B, C, D) were\nevaluated for yield (t/ha) in hybrid combination with a single tester (Z) in single rep tests at N\nenvironments (Table 1). For this simple example we are only interested in the impact of number\nof environments (replicates) on w and its subsequent impact on the predicted value for each g.\ni i\nAlso, assume that the residual variance, \u03c3 2 = 40.\ne\nSummary Data\nTable 1 Summary data of four inbreds evaluated in hybrid combination with one tester (Z) in single rep\ntests at 10 environments.\nHybrid\nAxZ 7 10 -3 10 -2.5 2 -1.5\nBxZ 9 10 -1 10 -0.83 2 -.05\nCxZ 11 10 1 10 0.83 2 0.5\nDxZ 13 10 3 10 2.5 2 1.5\nProve for yourself that the estimated \u03c3 2 = 20.\ne\nSome things to notice from the table:\n\u2022 The data are from balanced trials, i.e., all genotypic units are evaluated in the same number\nof environments (either 2 or 10).\n\u2022 With a large N, the observed differences will be equal to the predicted values.\n\u2022 For balanced trials, shrinkage does not change the relative ranking.\nIn essence the shrinkage predictor provides us with a value that not only includes the difference\nrelative to the mean, but also weights it by our confidence in the magnitudes of the differences\nfrom the overall mean.\nWe need to consider how to obtain predictions for genotypic units in the more likely situations\nwhere not all genotypic units (lines, cultivars, hybrids) will be evaluated equally in all\nenvironments. Indeed, we now find it possible with marker technologies to predict the values of\nthe genotypes before they have been grown. CHAPTER 12: MULTI ENVIRONMENT TRIALS: LINEAR MIXED MODELS | 175\nBest Linear Unbiased Prediction\nHenderson\u2019s shrinkage predictor can now be considered in the context of the matrix form of the\nmixed model equation (Equation 4):\nMixed model equation for predicting phenotype.\nwhere:\n= Vector of observations (phenotypes),\n= Design matrix for fixed effects,\n= Vector of unknown fixed effects (to be estimated),\n= Design matrix of random effects,\n= a vector of random effects (genotypic values to be estimated),\n= a vector of residual errors (random effects to be estimated).\nThe random effects are assumed to be distributed as\n.\nJust as estimates for in the matrix form of Equation 4 can be found using the normal equations,\nthe normal equations for Equation 4 can be used to find least squares estimates for the\nparameters in Equation 5.\nNormal equations in matrix notation.\nwhere:\n= the fixed effects parameters,\n= the random effects for the parameters,\n= incidence matrix,\n= a vector of observed phenotype (e.g., yield),\n= the additive relationship matrix,\n= the diagonal matrix,\n= incidence matrix,\n= the additive variance,\n= the residual variance. 176 | CHAPTER 12: MULTI ENVIRONMENT TRIALS: LINEAR MIXED MODELS\nBLUEs and BLUPs\nThe values for represent the Best Linear Unbiased Estimators (BLUE) of the fixed effects, while\nthe values for represent the Best Linear Unbiased Predictors (BLUP) of\nthe random effects. It is important to remember that BLUE\u2019s and BLUP\u2019s are not methods; they\nare statistical properties of methods (there are many) that are capable of producing such values.\nThese statistical properties include:\n\u2022 Best: the sampling variance of what is being estimated or predicted is minimized.\n\u2022 Linear: estimates or predictions are linear functions of the observations.\n\u2022 Unbiased: in BLUE indicates that the expected values of the estimates are equal to their\ntrue values. In BLUP, indicates that the sum of the predictions have an expectation of zero.\n\u2022 Estimators and Predictors: refer to algorithms that generate the estimated or predicted\nvalues.\nFor BLUE\u2019s the effects are considered fixed. Examples include the overall mean, effects of\ndifferent soil types, fertilizer treatments, etc. From a practical perspective, fixed effects do not\nhave a covariance structure. Due to the practical perspective, we often consider environments as\nfixed effects.\nEffects of BLUPs\nThe effects of BLUPs are considered random and it is possible to define covariance structures\nassociated with these effects. Examples include breeding values, dominance effects, tester effects,\netc. The challenge for application of methods that provide BLUPs is that Equation 3 assumes\ncovariances and variances are known. The truth is that the variances of genetic and non-genetic\nrandom effects are not known. Rather in practice we estimate these values. Thus, all\nimplementations of methods that provide BLUPs from mixed linear model equations provide\nonly approximations of the unknown vector values.\nNonetheless, BLUP values are useful in practical plant breeding trials where designs are\nunbalanced. Indeed, a method that produces a BLUP value enables the estimation of genetic\nvariances without having to resort to mating designs to obtain estimates of heritability. A typical\ntrial will have different numbers of genotypic units from different families evaluated in different\nsets of environments, some replicated some not. BLUPs utilize covariance structures (covariances\namong genotypic units grown in the same sets of environments and covariances among relatives)\nto maximize information on the traits of interest. Thus, the true purpose of a plant breeding\ntrial (to compare genotypes for purposes of selection), is enabled with the best possible values CHAPTER 12: MULTI ENVIRONMENT TRIALS: LINEAR MIXED MODELS | 177\nfor comparison because BLUPs maximize the correlation between the true genotypic values and\npredicted values.\nExample\nWhile Equation 5 initially appears to be daunting, with the little bit of matrix algebra, introduced\nabove, you have the skill to do these analyses using EXCEL. For example, consider the simple\ndata set in Table 2 (adapted from Chapter 11 of Bernardo, 2010).\nNote: This is an example of a self-pollinating crop (barley) and the number of environments does\nnot factor into solving of the equation.\nTable 2 Sample data from Chapter 11 of Bernardo,\n2010.\nEnvironments No. of Env Line Yield\nLow yield 18 1 4.45\nLow yield 18 2 4.61\nLow yield 18 4 5.27\nHigh yield 9 2 5.00\nHigh yield 9 3 5.82\nHigh yield 9 4 5.79\nWe desire to translate this into the following model in Equation 6.\nGeneral linear model for basic prediction of replicated trials.\nwhere:\n= the phenotypic response,\n= population (or overall) mean,\n= genotypic effect,\n= environmental effect,\n= genotypic by environment interaction effect,\n= residual source of variability.\nIn matrix notation, the data are represented in the model, (Equation 4, ) as\nin Equation 7: 178 | CHAPTER 12: MULTI ENVIRONMENT TRIALS: LINEAR MIXED MODELS\nMatrix notation of Equation 4 and Table 2 data.\nLinear Mixed Model Solution\nThe LMM solution is represented in Equation 8:\nLinear mixed model solution for Equation 7.\nwhere:\n,\n.\nThus, R represents a matrix that weights the calculations by the number of observations that\ncontribute to the estimated mean values of each cultivar in each type of environment. CHAPTER 12: MULTI ENVIRONMENT TRIALS: LINEAR MIXED MODELS | 179\nEstimated Residual Variance\nAssuming that the lines are unrelated to each other,\nis the ratio of the estimated residual variance (sometimes\nincorrectly referred to as the estimate of the experimental error) to the estimated additive\ngenetic variance. For purposes of illustration let us consider this estimated ratio to be 5, i.e., the\nestimated additive genetic variance is 20% as large as the residual variability.\nCalculations for the example have been implemented in an EXCEL spreadsheet: BLUEs and\nBLUPs of 4 barley varieties [XLSX].\nAs an exercise to conduct on your own, consider implementing the LMM.7 for the example on\nestimation of means using lsmeans discussed in the review module:\nReview of EDA and Estimation [DOC]:\n\u2022 Download R-code example eda aov lsmeans [TXT]\n\u2022 Download R-code example for mixed models [TXT]\n\u2022 Download R-code example Regression [TXT]\nReference\nHenderson, C. R. 1963. Estimation of variance and covariance components. Biometrics. 9: 226.\nHow to cite this chapter: Beavis, W. and A. A. Mahama. 2023. Multi Environment Trials: Linear\nMixed Models. In W. P. Suza, & K. R. Lamkey (Eds.), Quantitative Genetics for Plant Breeding. Iowa State\nUniversity Digital Press. 180 | CHAPTER 13: SIMULATION MODELING\nChapter 13: Simulation Modeling\nWilliam Beavis and Anthony Assibi Mahama\nQuantitative genetic models are used to represent, describe, and quantify the genetic\ncontributions to natural phenomena. These models can be arbitrarily simple, e.g., additive linear\nmodels, or complex, e.g., non-additive, non-linear models. R.A. Fisher and Sewell Wright had\na decades-long debate about which type of model should be considered in the study of natural\nand artificial selection. Fisher and his disciples argued that the more complex models were not\nneeded. Sewell Wright and his students argued that biology was inherently complex and needed\nnon-linear non-additive models to accurately understand adaptation and evolution. Of course,\nboth were correct, and both were wrong. As George Box reminds us, all models are wrong; some\nare useful. The choice of an appropriate model depends upon the purpose of the research.\nPrior to this chapter, we investigated the development of theoretical quantitative genetic models\nfor the purposes of conducting and interpreting analyses of plant breeding experiments. As\nnoted, without the theoretical models, there would be no genetic understanding of the results.\nTheory provides predictions, and predictions are the basis for generating testable hypotheses.\nIn this chapter, we introduce a far more practical justification for theoretical models: With a\ntheoretical model, it is possible to simulate many different data analysis techniques and breeding\nstrategies prior to conducting expensive experiments. In other words, natural and artificial\nsystems can be modeled in silico for purposes of predicting unknown outcomes. Many in silico\nexperiments can be compared, and the most promising can be used to compare methods or\nprocesses. If comparisons are based on objective criteria, such as accuracy, power, precision,\nefficacy, and efficiency, and if the model used for data analyses is the same as the model used for\nsimulating data sets, we can make rational decisions about which methods to implement in plant\nbreeding experiments.\nLearning Objectives\n\u2022 Recognize limitations of experimental research\n\u2022 Translate QG models to simulation models\n\u2022 Translate simulation models to EXCEL software functions\n\u2022 Build confidence in the use of simulation models CHAPTER 13: SIMULATION MODELING | 181\nHistory of Simulations\nGeneticists first used computers to implement simulation models to evaluate limits to artificial\nselection (Hill and Robertson, 1968; Bulmer, 1976) in closed breeding populations. By 1988,\nOscar Kempthorne, one of R.A. Fisher\u2019s disciples, pointed out that the classical experimental\nand algebraic approaches were limited to unrealistic assumptions in breeding and evolutionary\nsystems. Since 1988, plant and animal geneticists and breeders have used simulation models to\nevaluate the limits to emerging statistical methods (Beavis, 1994) and to choose among selection\nmethods because experimental evaluation of breeding methods is time and resource-limited\n(Podlich and Cooper, 1998). To date, there have been over 15,000 publications in which the terms\nsimulation and breeding occur in the title. Currently, there are numerous simulation software\npackages that have been developed and implemented for public and private research enterprises.\nSome are quite simple, while others are very flexible and complex. Generally, as the flexibility\nof the package increases, the learning curve associated with the complexity of the package also\nincreases. Thus, some of these simulation packages require entire courses and years to master.\nWhile it is beyond the scope of this chapter to either advocate or teach any particular simulation\npackage, we will learn how to implement the core quantitative and population genetic models\nthat are part of every useful simulation package.\nCore Elements\nThe core elements of simulation modeling include the genetic architecture of the trait(s), the\nstructure of segregating generations derived from breeding populations, and the organization of\nthe segregating genomes.\nBefore we decide on the genetic architecture of the trait, we need to know the structure of the\nsegregating generation derived from the breeding population. In diploid organisms, there are\nusually three genotypes at a SNP locus: {aa, ac, cc} or {tt, tg, gg}. Let us consider a locus with the\nsecond triplet, {tt, tg, gg}. If we decide to simulate a random mated population, then each of the\n2 2\nthree genotypes {tt, tg, gg} will occur at a frequency of p , 2pq, and q . To decide which genotype\nis going to be assigned to an individual, we should obtain a random sample of a number from the\n2\nUniform[0,1] distribution. If the random number is in the interval [0,p ], then we will assign the\n2 2\ngenotype \u2018gg\u2019 to individual i. If the random number is in the interval [p , p +2pq], then we will\nassign the genotype \u2018tg\u2019 to individual i; otherwise, we will assign the genotype \u2018tt\u2019 to individual i.\nObtaining a random sample from any distribution will depend on the syntax of the software\nsystem we decide to use for simulating the data. Since most students have experience with\nspreadsheet types of software, we will first learn how to use Excel for simulating SNP genotypes\nat a single locus in a random mating population, where p (frequency of g) = 0.3. The frequency 182 | CHAPTER 13: SIMULATION MODELING\nof \u2018gg\u2019 genotypes at this locus will be 0.09, the frequency of \u2018tt\u2019 genotypes will be 0.49, and the\nfrequency of heterozygous genotypes will be 0.42. Thus, if we sample a random number from the\nuniform distribution in the interval [0, 0.09], then we will assign the genotype \u2018gg\u2019 to individual i;\nin the interval from (0.09, 0.51], we will assign the genotype \u2018gt\u2019 to individual i; otherwise, we will\nassign the genotype \u2018tt\u2019 to individual i.\nExcel-Based Simulation\nStep 1\nWith these parameters, we will use the Excel functions \u201cIF\u201d and \u201cRAND\u201d in the following steps:\n1. Assign a sampleid designator to the first column.\nStep 2\nWith these parameters, we will use the Excel functions \u201cIF\u201d and \u201cRAND\u201d in the following steps:\n2. Obtain a random number from the Uniform Distribution for each of the sampleid\u2019s.\nSyntax: type =RAND() in cell B2, then drag across all cells in column B. CHAPTER 13: SIMULATION MODELING | 183\nStep 3\nWith these parameters, we will use the Excel functions \u201cIF\u201d and \u201cRAND\u201d in the following steps:\n3. Based on the random number assigned to each sampleid, assign a genotype to the locus.\nSyntax: type =IF(B2<=0.09,\u201dgg\u201d,IF(B2>0.51,\u201dtt\u201d,\u201dgt\u201d)) in cell C1, then drag across all relevant\ncells. Note that values in columns B and C will likely differ from those in the example\nbelow.\nStep 4\nIf we do not want the values generated by the RAND function to change as we add functions\nto the spreadsheet, we should plan to cut and paste a set of the actual values from one of the\nsampling events into new columns:\nOther Population Structures\nThere are many other possible breeding population structures; some are the result of designed\ncrosses (see Chapter 8 on mating designs), but most population structures emerge from long-term\nbreeding programs in which elite homozygous cultivars are crossed to promising homozygous\nlines through opportunistic networks of crosses. Simulating genotypes at segregating loci from\nany mating design or breeding program can be obtained in a manner described in the previous 184 | CHAPTER 13: SIMULATION MODELING\nparagraph. We need only decide on the frequencies of the genotypes in the segregating\npopulations. For example, in the specific case of an F generation derived from a cross of two\n2\ninbred lines, p = q = 0.5, and if a random number obtained from a uniform distribution, U[0,1],\nis greater than \u00bc and less than \u00be then the genotype of individual plant i, will be \u2018gt\u2019. We could\nbe interested in the case of recombinant inbred lines derived in the F generation of a cross of\n5\n2 2\ntwo inbred lines. In this case, the frequency of homozygotes is now p +pqF and q +pqF, and the\nfrequency of heterozygous lines is 2pq(1-F), where F = 0.875. Thus, if a random number obtained\nfrom U[0,1] is less than 0.4687, then RIL. i will be assigned the genotype \u2018gg\u2019. It should be obvious\nthat it should be possible to generate mixtures of segregating families from multiple independent\nor related crosses and simulate genotypes for any particular locus to all individuals in all families,\nregardless of how the families are derived.\nGenetic Architecture of the Trait\nNext, we need to decide how many loci will influence a trait and whether the alleles at the loci\nwill interact. Let us begin with a single-locus additive quantitative trait, designated P. Further,\nconsider a trait with an average phenotypic expression of 50 units and phenotypic variability\nin a diploid species that is due to additive genetic variability at a single locus and non-genetic\nvariability. Initially, let us plan to let half of the phenotypic variability be due to segregation at\nthe locus and half due to non-genetic sources of variability. The first step is to translate this brief\ndescription into a quantitative genetic model (Equation 1), preferably the same model that will be\nused in the eventual analysis of the phenotypic trait:\nQuantitative genetic model for phenotypic trait,\nwhere:\n= the phenotype,\n= the genotype effect,\n= one of three possible genotypes conferred by two alleles,\nth\n= one of the repeated samples of the i genotypes,\n= non-genetic source of variability and is ~ i.id N(0,\u03c3 \u03b5).\nThus, the variance model of Equation 1 is as written in Equation 2:\nVariance model for phenotypic trait, CHAPTER 13: SIMULATION MODELING | 185\nwhere:\n= the phenotypic variance,\n= the genotypic variance,\n= residual or non-genotypic variance.\nParameter Assignment\nThe next step is to assign values to each of the parameters in the model. Mean, \u00b5 is assigned the\nvalue of 50, and values for e are sampled from a Normal distribution with a mean of zero and a\nij\nstandard deviation of \u03c3 . We decided that we want to simulate data in which\ne\n(Equation 2 above).\n\u03c32\nWe can choose any value for , but it is often best to choose a value that is ~ consistent with\nP\nestimates from field trials for the crop of interest. In this case, let us say our field trials have\n\u03c32 \u03c32\ntypically produced estimates of phenotypic variance of ~ 98. Thus, both G and \u03b5 are ~ 49.\nThus, we can obtain values for \u03b5 by sampling a normal distribution with mean = 0 and standard\nij\ndeviation = 7.\nGenotypic Values\nWe also need numeric values for each of the genotypes. Recall from Quantitative Genetic Models\nTheory we can assign coded genotypic values to each genotype as follows:\n\u2022 Coded genotypic value of one homozygote (gg) = +a;\n\u2022 Coded genotypic value of the other homozygote (tt) = -a;\n\u2022 Coded genotypic value of heterozygotes (tg or gt) = d.\nSince we are simulating an additive genetic model, the genotypic value of the heterozygotes (d) is\nmidway between the two values for the homozygotes, i.e., d = 0. Thus,\nG = a for i = \u201cgg\u201d; -a for i = \u201ctt\u201d; and 0 for i = \u201cgt\u201d or \u201ctg\u201d.\ni\nNow we need a numeric value for a.\nCalculations\nRecall that (Equation 2), and the additive portion of genetic variance is\nrepresented by Equation 3: 186 | CHAPTER 13: SIMULATION MODELING\nFormula for calculating additive genetic variance.\nwhere:\n= the additive genetic variance,\n= the dominance variance,\n= the frequency of the two alleles (g or t),\n= coded genotypic values.\nSince we have decided to simulate d = 0, the genetic variance is all due to additive effects\n((Equation 4):\nAlgebraic formula for calculating genotypic variance.\nwhere:\nare as defined previously.\nThus,\nIf we assume that the frequency of \u2018t\u2019 (or \u2018g\u2019) in the population is , then a reasonable value for\nExcel Application\nNext, let us translate these values for the parameters into Excel functions.\nSyntax for assigning \u2018a\u2019: Type =IF(E3=\u201cgg\u201d,11.43, IF(E3=\u201ctt\u201d,-11.43,0)) in Cell\nG3, then drag across all relevant cells. CHAPTER 13: SIMULATION MODELING | 187\nIn order to fully understand how to sample from a normal distribution requires knowledge of\nprobability density functions, cumulative density functions, and integral calculus that enables the\ntranslation between the two. These are topics beyond our current scope but worth exploring by\nthose who wish to develop their own simulation capabilities.\nNormal Distribution Interval\nFor our immediate purpose, the Syntax for obtaining values for \u03b5 by sampling a Normal\nij\ndistribution with mean = 0 and standard deviation = 7 is the following:\nType =NORMINV(RAND(),0,7) in cell H3 and then drag across all relevant cells.\nSimulated Phenotypes\nWe now have values for all of the parameters in the model and need merely sum columns F, G,\nand H to obtain the simulated phenotypes (column I) for each of the sampleids.\nKeep in mind that if these were field trial data, we would only be able to obtain data found 188 | CHAPTER 13: SIMULATION MODELING\nin columns A and I. It should be immediately apparent that column F could be a mean value\nfor a particular replication or environment of the sampleids. Thus, it should be possible to\nsimulate data from multiple replicates and multiple environments with different mean values.\nIt should also be apparent that the sampling of \u03b5 could be derived from environments with\nij\na different plot-to-plot variability. For example, instead of using 7 in the function\n=NORMINV(RAND(),0,7), we could designate the standard deviation for some environments\nto be 14 and thus create a type of GxE that we discussed in Chapter 12 on Multi Environment\nTrials.\nExample Calculations\nFor the specific case of an F generation derived from a cross of two inbred lines, p = q = 0.5,\n2\nAlternatively, we could be interested in the case of recombinant inbred lines derived in the F\n5\ngeneration of a cross of two inbred lines. In this case, the additive portion of genetic variance is\nrepresented by Equation 5:\nFormula for calculating additive genetic variance involving inbreeding coefficient.\nwhere:\n= the coefficient of inbreeding,\nare as defined previously.\nAgain, let d = 0, p = q = 0.5, but F = \\small 0.875 and a reasonable value to simulate for as:\nPolygenic Trait Simulation\nLet us next simulate a polygenic trait P in which segregation at three loci will contribute additive\ngenotypic values that are responsible for 30% of the phenotypic variability. In this case, the\nphenotype is modeled where i, j, G, and \u03b5 are as before, and k represents each locus, while n is CHAPTER 13: SIMULATION MODELING | 189\n3. For simplicity, let us assume that segregation at each of the three loci contributes an equal\namount to the genotypic variability in an F population. Let us refer to these loci as quantitative\n2\ntrait loci (QTL). Using the quantitative genetic models we have already used, we learn that for\neach of the simulated QTL, a can be obtained using Equation 6:\nFormula for calculating additive genotypic value involving QTL,\nwhere:\nare as defined previously.\nIf we want the total phenotypic variability to be ~98, as before, and the frequency of each of the\n\u03c32 \u03c32\nalleles at all three loci is 0.5 (as in an F ), then = = 29 and a = 2.56 for each of the loci. We\n2 G A\nwould translate this information to the Excel spreadsheet as before, but now the spreadsheet will\nhave three columns for genotypes and three columns for genotypic values at each of the loci.\nQTL Simulations\nHow would you simulate genotypic effects if you wanted one of the QTLs to contribute 75%, a\nsecond QTL to contribute 20%, and the third to contribute 5% to the total genotypic variability?\nFor hybrid crops, the segregating progeny are often evaluated in testcross combination. For\nexample, in maize, it is routine to generate doubled haploids (DHs) from a cross of two elite\nStiff Stalk homozygous lines. The DH\u2019s are then crossed to an elite non-Stiff-Stalk homozygous\n\u2018tester\u2019. The resulting sample of Testcrossed DH (TDH) will be evaluated in an initial field trial.\nLet us simulate this situation for TDHs, grown in a field trial in which the CV for yield is ~ 7%\nand the mean is ~ 225 bu/ac. In order to simulate TDH\u2019s we need to recall that the additive genetic\nvariance for testcross progeny is represented by Equation 7:\nFormula for calculating additive genetic variance for testcross progeny,\nwhere: 190 | CHAPTER 13: SIMULATION MODELING\n= the additive genetic variance for testcross,\n= the coefficient of inbreeding,\n= the average effect of testcrossed allele.\nBecause the parents of the DH lines are fully homozygous, we can assume F=1. Thus,\n.\nOtherwise, the simulations will be generated as before, except we now have a different mean and\nphenotypic variance.\nReferences\nBeavis, W. D.1994. The power and deceit of QTL experiments: lessons from comparative QTL\nstudies, p.250-266. In Proceedings of the forty-ninth annual corn and sorghum industry research\nconference. American Seed Trade Association, Washington, DC.\nBulmer, M. G. 1976. The effect of selection on genetic variability: a simulation study. Genet. Res.,\nCamb. (1976), 28, pp. 101-117\nHill, W. G., and A. Robertson. 1968. Linkage disequilibrium in finite populations. Theor. Appl.\nGenet. 38, 226\u2013231.\nPodlich, D. W., and M. Cooper. 1998. Modelling plant breeding programs as search strategies on\na complex response surface, p.171-178. In Asia-Pacific Conference on Simulated Evolution and\nLearning. Springer, Berlin, Heidelberg\nHow to cite this chapter: Beavis, W., and A. A. Mahama. 2023. Simulation Modeling. In W. P. Suza, &\nK. R. Lamkey (Eds.), Quantitative Genetics for Plant Breeding. Iowa State University Digital Press. PLANT BREEDING BASICS | 191\nPlant Breeding Basics\nWilliam Beavis and Anthony Assibi Mahama\nThe materials in this chapter cover the basics of plant breeding and data management and\nanalysis concepts to serve as a refresher for helping the reader prepare to work through the main\nchapters of the book. Some readers may find it necessary to consult some introductory breeding\nand statistics texts for additional preparation.\nLearning Objectives\nPlace plant breeding activities within a framework of three categories based on goals:\n\u2022 Genetic improvement\n\u2022 Cultivar development\n\u2022 Product placement\nFig. 1 Plant breeding research activities at Makerere\nUniversity in Uganda. Photo by Iowa State University.\nDefining Plant Breeding\nPlant Breeding has many definitions. A working definition to consider: 192 | PLANT BREEDING BASICS\nPlant Breeding is the genetic improvement of crop species.\nThis definition implies that a process (breeding) is applied to a crop, resulting in genetic changes\nthat are valued because they confer desirable characteristics to the crop (Fig 1). Current breeding\nprograms are the result of thousands of years of refinements that have been implemented\nthrough considerable trial and error. Refinements to the breeding processes are constrained by\nlimited resources, technologies, and the reproductive biology of the species. Thus, the challenge\nof designing a plant breeding program might be thought of as the engineering counterpart to\nplant science (Fig. 2).\nFig. 2 Plant breeding research activities at Makerere University in Uganda. Photo\nby Iowa State University.\nOther definitions\n\u2022 Art of plant breeding: \u201c\u2026 the ability to discern fundamental differences of importance in the\nplant material available and to select and increase the more desirable types\u2026\u201d Hayes and\nImmer (1942)\n\u2022 \u201cPlant breeding, broadly defined, is the art and science of improving the genetic pattern of\nplants in relation to their economic use.\u201d Smith (1966).\n\u2022 \u201cPlant breeding is the science, art, and business of improving plants for human benefit.\u201d\nBernardo (2002). PLANT BREEDING BASICS | 193\nFig. 3 Individual plants of intermediate wheatgrass are tied into bundles to\nbe cut and threshed in order to select the plants with the highest yield and\nlargest seed. Photo by Dehaan; licensed under CC-BY-SA 3.0 via\nWikimedia Commons.\nOrganization of Plant Breeding Activities\nFor the purposes of applying appropriate quantitative genetic models in plant breeding, it is\nimportant to understand the distinctions among three types of plant breeding projects: genetic\nimprovement, cultivar development, and product placement. The distinctions among these three\ntypes of projects are nuanced aspects of every plant breeding program, yet the distinctions are\ncritical for applying the correct models for data analyses used in decision-making.\nCultivar Development\nThe primary goal of a genetic improvement (red arrows) project is to identify lines to cycle into\nthe breeding nursery (Fig. 3) for purposes of genetic improvement of the breeding population.\nIdentification of lines to select is accomplished through assays of segregating lines (synthetics,\nhybrids) with trait-based markers and small plot field trials in single and Multi-Environment\nfield Trials (METs) (Fig. 4). Data analyses will include analyses of binary traits with binomial and\nmultinomial models and quantitative traits with mixed linear models, where the segregating lines\nwill be modeled as random effects and the environments as fixed effects.\nThe primary goal of the cultivar development project (blue filters) is to identify cultivars that\nhave the potential to be grown throughout a targeted population of environments. Thus, in a 194 | PLANT BREEDING BASICS\ncultivar development project, selected lines from segregating populations will be evaluated for\nquantitative traits in multi-environment trials. Data analyses in the Regional Trials of a cultivar\ndevelopment project will also be based on mixed linear models. However, in this case, lines are\noften modeled as fixed effects, while the environments are modeled as random effects.\nFig. 4 A model of plant breeding activities.\nThe goals of a product placement project are again distinct from genetic improvement and\ncultivar development. In a product placement project, agronomic management practices, as well\nas cultivars, are selected for the field trials. These are often organized in hierarchical (split-plot)\nexperimental designs. Thus, the parameters of a mixed linear model associated with agronomic\npractices and cultivars will be modeled as fixed effects, while various levels of residual variability\nassociated with split-plot experimental units will be modeled as random effects.\nFor an introductory course on Quantitative Genetics, we will focus primarily on genetic\nimprovement, a little bit on cultivar development projects, and no time will be spent on product\nplacement projects.\nDecision-Making Process\nConceptually, genetic improvement consists of a simple two-step, iterative decision-making\nprocess (Fig. 5):\n1) selection of parents for crosses and 2) evaluation of their segregating progeny for the next PLANT BREEDING BASICS | 195\ngeneration of parents and development of cultivars (Fehr, 1991). Operational implementation of\ngenetic improvements for any given species requires far more detail.\nFor example, Comstock (1978) outlined the major activities involved in genetic improvement by\nplant breeders (below).\nThe details of any particular breeding program will likely consist of many activities. At the same\ntime, it is important to categorize these activities according to the goals that transcend all plant\nbreeding programs.\nFig. 5 Genetic Improvement Process by Plant Breeders\nA Brief History of Quantitative Genetics\nQuantitative genetics addresses the challenge of connecting traits measured on quantitative\nscales with genes that are inherited and measured as discrete units. This challenge was originally\naddressed through the development of theory between 1918 and 1947. The theory is now referred\nto as the modern synthesis and required another 50 years for technological innovations and\nexperimental biologists to validate. Luminaries such as RA Fisher, Sewell Wright, JBS Haldane,\nand John Maynard Smith were able to develop this theory without the benefit of high throughput 196 | PLANT BREEDING BASICS\n\u2018omics\u2019 technologies. Indeed, modern synthesis was developed before knowledge of the structure\nof DNA.\nUnlike animal breeders, plant breeders implement breeding processes in organisms that cannot\nbe protected from highly variable environments. Because plants are rooted in the sites in which\nthey are planted, they have evolved unique adaptive mechanisms, including whole-genome\nduplications that enable biochemical diversity through secondary metabolism and multiple forms\nof reproductive biology.\nBecause of the reproductive and biochemical diversity in domesticated crops, plant breeders felt\nlittle need to develop quantitative genetics beyond initial concepts associated with the Analysis\nof Variance (ANOVA; Fisher, 1925; 1935). Thus, plant breeders focused their efforts on the\ndevelopment of field plot designs and careful plot management practices to ensure balance in\nfield plot data for ANOVA.\nModern Synthesis Theory\nThe lack of reproductive and biochemical diversity in animal species created constraints that\nforced animal breeders to concentrate their efforts on the development of quantitative genetics\nbeyond the ideas of Fisher (1918, 1928). JL Lush (1948) and his student CR Henderson (1975)\nrealized that genetic improvement of quantitative traits in domesticated animal species could\nnot take advantage of replicated field trials that are based on access to cloning, inbreeding, and\nthe ability to produce dozens to thousands of progenies per individual. With these constraints, it\nwas not possible to obtain precise estimates of experimental error or Genotype by Environment\ninteraction effects using classical concepts from the ANOVA. So, they developed the statistical\nconcepts of Mixed Model Equations (MME) to estimate breeding values of individuals with\nstatistical properties of Best Linear Unbiased Prediction (BLUP). These very powerful statistical\napproaches were largely ignored by plant breeders until about 1995 (Bernardo, 1996).\nMarker Technologies\nThe power of these methods is derived from knowledge of genealogical relationships. For some\ncommercial plant breeding organizations, genealogical information had been carefully recorded\nfor purposes of protecting germplasm. Thus, it was relatively easy for commercial breeding\ncompanies such as Pioneer and Monsanto to implement these methods. Next, international plant\nbreeding institutes began to incorporate mixed linear models to estimate breeding values in\ntheir genetic improvement programs (Crossa et al, 2004); again, it was fairly easy to do this\nwith extensive pedigree information. Since about 2005, many plant quantitative geneticists have\npublished extensively on the benefits of this approach to genetic improvement of crops (Piepho, PLANT BREEDING BASICS | 197\n2009), although there remain many academic plant breeding organizations that do not utilize\nMME to estimate breeding values with BLUP statistical properties, primarily because pedigrees\nof lines developed by academic programs have not been widely shared, nor aggregated into a\nshared repository. In parallel to the adoption of MME by plant breeders, there has been the\ndevelopment of relatively inexpensive genetic marker technologies. These have enabled the use\nof MME for Genomic Estimates of Breeding Values (GEBV; Meuwissen, 2001), thus overcoming\nthe lack of genealogical knowledge for many crop and tree species.\nTrait Measures\nObjectives: Demonstrate ability to distinguish among the various types of phenotypic and\ngenotypic traits that are assessed routinely in a plant breeding program.\nCategorical Scales\nIn the context of plant breeding, quantitative genetics provides us with a genetic understanding\nof how quantitative traits change over generations of crossing and selection. Recall traits can be\nevaluated on categorical (Fig. 6) or quantitative scales. If the trait of interest is evaluated based\non some quality, for example, disease resistance, flower color, or developmental phase, then it\nis considered a categorical trait. There are three further distinctions that can be made among\ncategorical scales:\n\u2022 Binary consists of only two categories such as resistant and susceptible or small and large.\n\u2022 Nominal consists of unordered categories. For example, viral disease vectors might be\ncategorized as insects, fungi, or bacteria.\n\u2022 Ordinal consists of categorical data where the order is important. For example, disease\nsymptoms might be classified as none, low, intermediate, and severe. 198 | PLANT BREEDING BASICS\nFig. 6 Flower color variation in Aloe chabaudii from Manica province\nin Mozambique. Photo by Ton Rulkens, licensed under CC-SA 3.0 via\nWikimedia Commons.\nQuantitative Scales\nBinary, nominal, and ordinal data are typically analyzed using Generalized Linear Models. Such\nmodels require that we model the error structures using Poisson or Negative Binomial\ndistributions and are beyond the scope of introductory quantitative genetics. It is important to\nremember, however, that it is not advisable to apply General Linear Models to categorical data\ntypes.\nThere are two further distinctions of traits that are evaluated on quantitative scales:\n\u2022 Discrete data occur when there are gaps between possible values. This type of data usually\ninvolves counting. Examples include flowers per plant, number of seeds per pod, number of\ntranscripts per sample of a developing tissue, etc.\n\u2022 Continuous data can be measured and are only limited by the precision of the measuring\ninstrument. Examples include plant height, yield per unit of land, seed weight, seed size,\nprotein content, etc.\n\u25e6 In the context of measurement, Precision refers to the level of detail in the scale of the\nmeasurement.\n\u25e6 Accuracy refers to whether the measurement represents the true value. PLANT BREEDING BASICS | 199\nTypes of Models\nObjectives: Be able to place plant breeding activities within a framework of three categories\nbased on goals:\n\u2022 Genetic improvement\n\u2022 Cultivar development\n\u2022 Product placement\nDefinition and Purpose of Models\nModels are representations or abstractions of reality. Some models can be very useful, e.g.,\nprediction of phenotypes, even if they are not accurate. If data are modeled well, they can be\nused to generate useful graphics that will inform the breeder about data quality, integrity, and\nnovel discoveries. Most often, predictive models are in the form of mathematical functions. Also,\nthere are models for organizing data, analyses, processes, and systems. Yes, breeding systems and\ngenetic processes can be represented as sets of mathematical equations. Historically the subject\nof optimizing a breeding system has been approached through ad hoc management activities\nthat are often tested through trial and error. In the future, design and development of plant\nbreeding systems will need to be treated with the same rigor that engineers use to design optimal\nmanufacturing systems. Thus, it will be important to learn how to model breeding systems as\nmathematical functions.\nData Modeling\nEven if it were possible to record data without error, as soon as we evaluate a trait and record the\nvalue of a living organism, we lose information about the organism. The challenge is to develop a\ndata model that will minimize recording errors and loss of information.\nWhat Is Data Modeling?\n\u2022 Data modeling is the process of defining data requirements needed to support decisions.\n\u2022 Data modeling is used to assure standard, consistent, and predictable management of data\nas a resource for making decisions.\n\u2022 Data models support data and decision systems by providing definitions and formats. If the\ndata are modeled consistently throughout a plant breeding program, then compatibility of\ndata can be achieved. 200 | PLANT BREEDING BASICS\nIf a single data structure is used to store and access data, then multiple data analyses can share\ndata.\nSteps for Modeling Data in a Plant Breeding Project\n\u2022 Outline the plant breeding process.\n\u2022 Determine the experimental or sampling units that will be evaluated at each step in the\nprocess.\n\u2022 Determine the number of experimental or sampling units that will be evaluated.\n\u2022 Characterize the experimental and sampling units as well as the traits that will be evaluated\nat each step in the process.\nAn experimental unit is defined as the basic unit to which a treatment will be applied. A\nsampling unit is defined as a representative of a population of interest. In quantitative genetics,\nwe evaluate responses (traits) of experimental or sampling units on continuous scales, e.g., grain\nyield, plant height, harvest index, etc. Note that a measurement taken on a continuous scale\nis not the same as a continuously measured trait. Continuously measured traits such as grain\nfill, transpiration, disease progression, or gene expression are measured continuously over the\ngrowth and development of an organism. Historically, evaluation of continuously changing traits\nhas been too labor-intensive to justify their expense. The emergence of \u2018phenomics\u2019 using image\nprocessing will overcome the limitations of acquiring the data. However, the need to store and\nmanage \u2018big data\u2019 from phenomics is going to require novel data models and computational\ninfrastructure, or else the acquisition of such data will be meaningless.\nOrganizing Data\nData models address the need to organize data for subsequent analysis.\nA simple data model consists of a Row x Column matrix, where all experimental or sampling\nunits are represented in rows, and the evaluated characteristics or attributes for each unit are\nrecorded in the columns (Fig. 7): PLANT BREEDING BASICS | 201\nFig. 7 A matrix, A, for representing data.\nAlternatively, the A matrix can be represented as:\nr x c\n\u2022 A = {a }, for i = 1,2,3 \u2026 r and j = 1,2,3 \u2026 c\nij\n\u2022 i would represent line 1, line2, line3 \u2026 line r and\n\u2022 j would represent location, replication, SNP locus 1, disease rating, \u2026 yield, etc.\nPreserving Data\nWhile the A matrix is sufficient for small research projects, it is inadequate and cumbersome\n(r x c)\nfor breeding programs consisting of multiple types of evaluation trials at multiple stages of\ndevelopment. For such programs, relational databases are designed to optimize the ability to\nsearch and prepare data for analysis and interpretation using statistic and genetic models (Fig.\n7). Further unless data in an A matrix is disseminated through \u201cread-only\u201d access, there is\n(r x c)\npotential for alteration of originally recorded data. Thus, the use of Excel files, too commonly\nused to store experimental data in an A matrix, can create serious ethical issues. While\n(r x c)\nsuch issues do not disappear with relational databases, relational databases enable more effective\nprotection of data as originally recorded. Recently, a publicly available database designed for\norganizing data from plant breeding projects has been developed. Known as the Breeding\nManagement System, it is part of the Integrated Breeding Platform designed and developed\nby the Generation Challenge Program of the Consultative Group of International Agricultural\nResearch Centers. 202 | PLANT BREEDING BASICS\nA Relational Database\nFig. 8 A relational database for data management plant breeding research.\nWhile the development of relational databases (Fig. 8) is outside of the scope of this course,\nit is important to note that plant breeders routinely work with database developers to design,\nimplement, and populate relational databases.\nPhenotypic Models\nFor the most part, plant breeders rely on linear models to represent measured traits. While we\nwill concentrate on statistic and genetic models for continuous traits, it is important to recognize\nthat there are well-developed data analysis methods for binary, nominal, and ordinal traits (see\nMcCullagh and Nelder, 1989 or Christensen, 1997 for explanations of Generalized Linear Models).\nA general (not Generalized) linear model for the phenotype can be denoted as in Equation 1:\nGeneral linear model for the phenotype.\nwhere:\n= phenotype of individual i,\n= mean phenotype value of individual i,\n= random variability (or lack of precision) in the measurement of the phenotype of\nindividual i.\nFurther, we often assume that the variability associated with each measurement of variables, e, is\ni\ndistributed as random identical and independent Normal variables. This simple model is typically PLANT BREEDING BASICS | 203\nassociated with the hypothesis that the only source of variability is due to chance (noise). We\ncan extend the simple model to include genetic (G) and environmental (E) sources (signals) of\nvariability, .\nTwo Linear Models\nScalar Notation\nWe will utilize two types of models to analyze data Equations 2 and 3):\nLinear model for phenotype.\nwhere:\n= intercept at the y-axis,\n= the slope of the regression line,\n= the genotypic value,\n= unspecified or residual sources of variability.\nLinear model for phenotype.\nwhere:\n= the phenotypic response,\n= the population mean,\n= the genotypic unit,\n= replicates of the genotypic units,\n= unspecified or residual sources of variability.\nThe parameters of Equation 2 represent the intercept and slope of a line that can be fit to data\nconsisting of pairs of genotypic values, G, and phenotypic responses, where the genotypic values\ni\nare continuous and known (i.e., measured without error) while the phenotypic data are measured\nwith error in plots (experimental units). The parameters of Equation 3 represent a population\nmean, genotypic units, g, r replicates of the genotypic units, and the phenotypic, Y responses.\ni j ij\nThe genotypes are usually categorical designators of distinct segregating lines, hybrids, cultivars,\nclones, etc.\nWe typically estimate the parameters of Equations 2 and 3 using least squares methods. These\nmethods are based on the idea of minimizing the squared differences between the model 204 | PLANT BREEDING BASICS\nparameters and the measured phenotypic value (Equation 4). For example, using Equation 2, we\nwant to minimize the difference as:\nModel for squared differences.\nwhere:\nare as described previously.\nTaking the partial derivatives of Equation 3 with respect to \u03b2 and \u03b2 and setting the resulting\n0 1\ntwo equations = 0, we find the slope using Equation 5:\nFormulae for calculating intercept and slope.\nwhere:\n= the genotypic variance,\n= covariance of G and Y,\n= mean of Y,\n= mean of X.\nThe result is a prediction equation (Equation 6):\n</>\nPrediction model.\nwhere:\n= the estimates of these terms,\n= predictor variable, genotype, G.\ni\nNote that the predicted values are placed on the fitted line. Such values are sometimes referred to\nas \u2018shrunken\u2019 estimates because, relative to the observed values, they show much less variability.\nIf it were possible to obtain the true genotypic values, G, then we could routinely use Equation 2\ni\nto predict the phenotypic performance of individual i. Instead, plant breeders have used Equation\n3 and its expanded versions to evaluate segregating lines and cultivars. PLANT BREEDING BASICS | 205\nMatrix Notation\nEquation 2 also can be represented by Equation 7:\nMatrix model and solution for phenotype.\nwhere:\n= phenotype,\n= matrix or vector of genotype,\n= vector or residual or error.\nAnd Equation 3 could be represented by Equation Equation 8:\nModel for phenotype.\nwhere:\n= phenotype ,\n= vector of replication,\n= vector of genotype.\nWhen represented this way, though, Equation 3 is usually misinterpreted by beginning students\noften, as the matrix form of the equation with an added set of the parameter Z. The matrix form\nof Equation 2 is actually a mixed linear model equation (Equation 9) and not a simple expansion\nof the matrix form of Equation 1.\nMatrix model linear model for phenotype.\nwhere:\n= vector of observations (phenotypes),\n= Incidence matrix for fixed effects,\n= vector of unknown fixed effects (to be estimated),\n= Incidence matrix of random effects,\n= a vector of random effects (genotypic values to be predicted),\n= a vector of residual errors (random effects to be predicted). 206 | PLANT BREEDING BASICS\nExploratory Data Analysis (EDA)\nObjectives\n\u2022 Distinguish between descriptive and inferential statistics.\n\u2022 Conduct and interpret exploratory data analyses.\n\u2022 Distinguish parameters from estimators and estimates.\n\u2022 Estimate means in both balanced and unbalanced data sets.\n\u2022 Estimate variances, covariances, and correlations in balanced data sets.\nStatistical Inference\nThe purpose of statistical inference is to interpret the data we obtain from sampling or designed\nexperiments. Statistical Inference consists of two components: estimation and hypothesis testing.\nIn this section, we review some introductory estimation concepts.\nStatistical Inference: Hypothesis Testing\nThe purpose of statistical inference is to interpret the data we obtain from sampling or designed\nexperiments. Preliminary insights come from graphical data summaries such as bar charts,\nhistograms, box plots, stem-leaf plots, and simple descriptive statistics such as the range (maximum,\nminimum), quartiles, and the sample average, median, and mode. These exploratory data analysis\n(EDA) techniques should always be used prior to estimation and hypothesis testing. However, prior to\nconducting EDA, the phenotype should be modeled using the parameters in the experimental and\nsampling designs.\nExploratory Data Analyses\nPreliminary insights come from graphical data summaries such as bar charts, histograms, box\nplots, stem-leaf plots, and simple descriptive statistics such as the range (maximum, minimum),\nquartiles, correlations, and coefficients of variation. These exploratory data analysis (EDA)\ntechniques that provide descriptive statistics should always be used prior to estimation and\nhypothesis testing.\nEstimation: Sample Average\nIn population and quantitative genetics, parameters are quantities that are used to describe PLANT BREEDING BASICS | 207\ncentral tendencies and dispersion characteristics of populations. Parameters are usually\npresented in the context of theoretical models used to describe quantitative and population\ngenetics of breeding populations. Parameters of interest in population and quantitative genetics\ninclude frequencies, means, variances, and covariances.\nBecause populations often consist of an infinite or very large number of members, it may be\nimpossible to determine these quantities. Instead, statistical inferences, i.e., estimates, about the\ntrue but unknowable parameters are determined from samples. The rule by which a statistical\nestimate of a parameter is constructed is known as the estimator. For example, the description of\nhow to calculatea sample average given by Equation 10:\nFormula for estimating sample average.\nwhere:\n= the sample, i,\n= number of samples.\nThis average represents an estimator of the population mean, while the calculated value, e.g.,\n132.38, obtained from 25 (n) samples (X) from a population would be an estimate of the\ni\npopulation average.\nEstimation of Means\nThe most common inferential statistic is the estimate of a mean. Computing arithmetic means,\neither simple or weighted within-group averages represents a common approach to summarizing\nand comparing groups. Data from most agronomic experiments include multiple treatments\n(or samples) and sources of variability. Further, the numbers of observations per treatment\noften are not equal; even if designed for balance, some observations are lost during the course\nof an experiment. Thus, most data sets come from experiments that have multiple effects of\ninterest and are not balanced. In such situations, the arithmetic mean for a group may not\naccurately reflect the \u201ctypical\u201d response for that group because the arithmetic mean may be\nbiased by unequal weighting among multiple sources of variability. The calculation of Least\nSquare Means, lsmeans, was developed for such situations. In effect, lsmeans are within-group\nmeans appropriately adjusted for the other sources of variability. The adjustments made by\nlsmeans are meant to provide estimates as though the data were obtained from a balanced design.\nWhen an experiment is balanced, arithmetic averages and lsmeans agree. 208 | PLANT BREEDING BASICS\nEstimation of Means: Example\nConsider a data set consisting of 3 cultivars evaluated in a Randomized Complete Block Design\nconsisting of 5 replicates at each of 3 locations (Table 1). Despite exercising best agronomic\npractices, note that some plots at some locations did not produce phenotypic values.\nThe estimated means and number of observations for each cultivar indicate that there is very\nlittle difference among the cultivars, although cultivar C appears to have the highest yield (Table\n2).\nA closer investigation of the data reveals that the means are unequally weighted by location\neffects. Recalculating the means for the cultivars indicates more distinctive differences among\nthe cultivars once the differences among environments were taken into account (Table 3).\nTable 1 Sample data with missing\nvalues.\nCultivar Location Y\nj,k\nA Ames 17, 28, 19, 21, 19\nA Sutherland 43, 30, 39, 44, 44\nA Castana -, -, 16, -, \u2013\nB Ames 21,21, -, 24, 25\nB Sutherland 39, 45, 42, 47, \u2013\nB Castana -, 19, 22, -, 16\nC Ames 22, 30, -, 33, 31\nC Sutherland 46, -, -, -, \u2013\nC Castana 25, 31, 25, 33, 29\nTable 2 Trait average\nvalues for three\ncultivars, with the same\nsample number.\nCultivar N Average\nA 11 29.1\nB 11 29.2\nC 11 30.2 PLANT BREEDING BASICS | 209\nTable 3 Unequally\nweighted trait\naverage values.\nCultivar lsmeans\nA 25.6\nB 28.3\nC 34.4\nEstimation of Variances\nIf we model a trait value as (Equation 1), then the estimator of the variance of the\npopulation consisting of individuals, i = 1, 2, 3, \u2026. , N is written as in Equation 11:\nFormula for calculating the variance of a population of individuals,\nwhere:\n= the variance of the population of trait y values,\n= population size ,\n= population mean.\nSince it is not possible to evaluate a population of a crop species (think about it), we usually take\na sample of individuals representing the population, i = 1, 2, 3, \u2026, n, where n << N. The estimator\nof the sample variance from a sample of n values is represented in Equation 12:\nFormula for estimating sample variance,\nwhere:\n= the sample variance,\n= trait value of individual i,\n= trait mean value. 210 | PLANT BREEDING BASICS\nEstimation of Covariance\nThe covariance is a measure of the joint variation between two variables. Let us designate one\ntrait X and a second trait Y. We can model Y as before and we can model X in a similar manner,\ni.e.,\nFormula for estimating trait value,\nwhere:\nth\n= i value of trait X,\n= mean of trait X,\n= random variability in trait values.\nand the estimator of the variance of X is obtained using Equation 14:\nFormula for estimating variance of the trait, X.\nwhere:\nth\n= i value of trait X,\n= mean of trait X,\n= random variability in trait values.\nThus, the estimator of the covariance of X and Y is as in Equation 15:\nFormula for estimating covariance of the traits, X and Y.\nwhere:\n= covariance of X and Y,\n= mean of population of Xs,\n= mean of population of Ys,\nare as defined previously.\nAgain, it is not possible to evaluate a population, so we usually take a sample of individuals PLANT BREEDING BASICS | 211\nrepresenting the population, i = 1,2,3 \u2026 n, where n << N. So, the estimator of a sample covariance\nis obtained using Equation 16:\nFormula for calculating covariance of the traits, X and Y, from a sample.\nwhere:\n= the covariance of X and Y,\n= mean of sample of Xs,\n= mean of sample of Ys.\nEstimation of Variance Components\nIf we extend our simple model to include genetic and environmental sources of variability, as\nmentioned previously, as , then, noting that \u00b5 is a constant and applying\nsome algebra, we can show that the variance (V) of Y is as in Equation 17:\nFormula for estimating variance components.\nwhere:\n= the variance of trait Y,\n= the genotypic, environmental, and random variability,\nrespectively,\n= the covariance of G and E.\nThe assumption is that the errors are independently distributed. If we further assume that\ngenotype and environment are independent and that there is no genotype x environment\ninteraction, the variance and variance components are estimate with Equation 18:\nFormula for estimating variance components, in the absence of covariance.\nwhere:\nare as defined previously. 212 | PLANT BREEDING BASICS\nQuestions to Consider\nA question to consider is whether the parameters of the linear model Y = \u00b5 + G + E + e represent\nfixed or random effects, because this determination will affect the way in which we estimate\nvariance components and whether each is contributing significantly to the overall phenotypic\nvariability. This determination depends on the inference space to which results are going to be\napplied. Fixed effects denote components of the linear model with levels that are deliberately\narranged by the experimenter, rather than randomly sampled from a population of possible levels.\nInferences in fixed effect models are restricted to the set of conditions that the experimenter has\nchosen, whereas random effect models provide inferences for a population from which a sample\nis drawn.\nAs a practical matter, it is hard to justify designating a parameter as a random effect if the\nparameter space is not sampled well. Consider environments, for example, since we cannot\ncontrol the weather, it is tempting to designate environments as random effects, however\ndrawing inferences to a targeted population of environments will be difficult if we sample a small\nnumber of environments, say less than 40. Thus, as a practical matter, the genetic improvement\ncomponent of a breeding program will consider environments as fixed effects (or nuisance\nparameters), because our main interest is in drawing inferences about the members of a breeding\npopulation and their interactions with environments, wheras the product placement component\nof a breeding program will evaluate a relatively small number of selected genotypes in a large\nnumber of environments. Thus, for this phase the models will consider cultivars (lines, hybrids,\nsynthetics, etc.) as fixed effects and environments as random effects.\nMixed Models\nBecause the inference space of interest for genetic improvement is derived from random samples\nof genotypes obtained from a conceptually large breeding population, we do not consider\ngenotypes as fixed effects until the genotypes have been selected for a cultivar development\nprogram. At the same time it is a rare experimental design that does not include a fixed effect.\nOften random effects, such as environments are classified as fixed effects in mixed models so\nthat inferred predictions are determined using computational methods that provide restricted\nmaximum likelihood methods. More on this topic can be found in the section on Statistical\nInference. PLANT BREEDING BASICS | 213\nInstallation of R\nIntroduction and Objectives \u2013 Installation of R\n\u2022 Learn to download and install R and R Studio.\n\u2022 Learn to start an R analysis project.\n\u2022 Learn how to upload data that is CSV formatted.\nBackground\nR is a powerful language and environment for statistical computing and creating graphics. The\nmain advantages of R are the fact that R is a free software and that there is a lot of help\navailable. It is quite similar to other programming tools such as SAS (not freeware), but more\nuser-friendly than programming languages such as C++ or FORTRAN. You can use R as it is, but\nfor educational purposes we prefer to use R in combination with the RStudio interface (also free\nsoftware), which has an organized layout and several extra options.\nDirectory Of R Commands Used\n\u2022 getwd()\n\u2022 setwd()\n\u2022 ?\n\u2022 help.search()\n\u2022 example()\n\u2022 read.csv()\n\u2022 rm()\n\u2022 rm(list=())\n\u2022 head()\n\u2022 hist()\n\u2022 attach()\n\u2022 boxplot()\n\u2022 str()\n\u2022 as.factor()\n\u2022 aov()\n\u2022 summary()\nReferences\nUp and Running with R (Internet resource) 214 | PLANT BREEDING BASICS\nExercise\nImagine that you\u2019ve been recently hired as a data analyst for a brand new seed company and have\nbeen asked by your supervisor to conduct an analysis of variance (ANOVA) on yield trial data\nfrom 3 synthetic maize populations planted in 3 reps each. Your company does not have funds\nto purchase commercial statistical software, thus you must either do the analysis by hand or use\nfreely available software. Since you will have to analyze much larger data sets in the near future,\nyou opt to learn how to carry out the ANOVA using the freely available software R and R-Studio.\nInstall R\nTo install R on your computer, go to the home website of R and do the following (assuming you\nwork on a Windows computer):\n\u2022 Click CRAN under Download,Packages in the left bar\n\u2022 Choose a download site close to you (eg: USA: Iowa State University, Ames, IA)\n\u2022 Choose Download R for Windows\n\u2022 Click Base\n\u2022 Choose Download R 3.1.1 for Windows and choose default answers for all questions (click\n\u201cnext\u201d for all questions)\nInstall RStudio\nAfter finishing above setup, you should see an icon on your desktop. Clicking on this would\nstart up the standard interface. We recommend, however, using the RStudio interface. To install\nRStudio, go to the RStudio homepage and do the following (assuming you work on a windows\ncomputer):\n\u2022 Click Download RStudio\n\u2022 Click Desktop\n\u2022 Click RStudio 0.98.977 \u2013 Windows XP/Vista/7/8 under Installers for ALL Platforms to\ninitiate download\n\u2022 Open the .exe file from your computer\u2019s downloads and run it and choose default answers\nfor all questions (click \u201cnext\u201d for all questions)\nRStudio Layout\n1. Script Window: In this window, collections of commands (scripts) can be edited and saved.\nIf this window is not present upon opening RStudio, you can open it by clicking File\u2192New PLANT BREEDING BASICS | 215\nFile\u2192R Script. Just typing a command in the Script window and clicking enter will not\ncause R to run the command; the command has to get entered in the Console window\nbefore R executes the command. If you want to run a line from the script window, you can\nclick Run on the toolbar or press CTRL+ENTER to enter the line into the console view.\n2. Environment / History Window: Under the Environment tab you can see which data and\nvalues R has in its memory. The History tab shows what has been entered into the console.\n3. Console window: Here you can type simple commands after the > prompt and R will then\nexecute your command. This is the most important window, because this is where R\nactually runs commands.\n4. Files / Plots / Packages / Help: Here you can open files, view plots (also previous plots),\ninstall and load packages or use the help function.\n5. You can change the size of each of the windows by dragging the grey bars between the\nwindows.\nWorking Directory\nYour working directory is a folder on your computer from where files can be entered, or read,\ninto R. When you ask R to open a file with a read command, R will look in the working directory\nfolder for the specified file. When you tell R to save a data set or figure which you\u2019ve created, R\nwill also save the data or figure as a file in the same working directory folder. 216 | PLANT BREEDING BASICS\nSet your working directory to a folder where all of the example data files for this lesson are\nlocated.\n1. Create a folder on your desktop; for this example the folder will be called wd. Then, obtain\nthe default working directory by entering the command getwd() into the console\nwindow. R returns the default working directory below.\n> getwd()\n> [1] \u201cC:/Users/<Username>/Documents\u201d\n2. Next, set the working directory to the folder on your desktop, wd, using the setwd()\ncommand in the Console window:\n> setwd(\u201cC:/Users/<Username>/Desktop/wd\u201d)\nNotice that to set our working directory to a folder on our desktop, we enter everything that was\nreturned by R from the getwd()command before the word Documents, change Documents to\nDesktop, then add a forward slash followed by the name of our folder (wd).\nMake sure that the slashes are forward slashes and that you don\u2019t forget the quotation marks. R is\ncase sensitive, so make sure you write capitals where necessary. Within the RStudio interface you\ncan also go to Session \u2192 Set working directory to select a folder to be your working directory.\nLibraries\nR can do many kinds of statistical and data analyses. The analyses methods are organized in so-\ncalled packages. With the standard installation, most common packages are installed. To get a list\nof all installed packages, go to the packages window (lower right in RStudio). If the box in front of\nthe package name is ticked, the package is loaded (activated) and can be used. You can also type\nlibrary() in the console window to view the loaded packages.\nThere are many more packages available on the R-website. If you want to install and use a\npackage (for example, the package called \u201cgeometry\u201d) you should:\n1. Install the package: click on the \u201cpackages\u201d tab at the top of the lower-right window in\nRStudio. Click \u201cinstall\u201d, and in the text box under the heading \u201cpackages\u201d, type \u201cgeometry\u201d.\nYou can also simply enter install.packages(\u201dgeometry\u201d) in the console window to install the\npackage.\n2. Load the package: under the \u201cpackages\u201d tab at the top of the lower-right window in PLANT BREEDING BASICS | 217\nRStudio, check the boxes of the packages you wish to load (i.e. \u201cgeometry\u201d). You can also\nsimply type library(\u201dgeometry\u201d) in the console window to load the package.\nGetting Help in R\nIf you know the name of the function you want help with, you can just type a question mark\nfollowed by the name of the function in the console window. For example, to get help on aov, just\nenter:\n> ?aov\nSometimes you don\u2019t know the exact name of a function, but you know the subject on which you\nwant help (i.e., Analysis of Variance). The simplest way to get help in R is to click the \u201cHelp\u201d\ntab on the toolbar at the top of the bottom-right window in RStudio, then enter the subject or\nfunction that you want help within the search box at the right. This will return a list of help pages\npertaining to your query.\nAnother way to obtain the same list of help pages is by entering the help.search command\nin the Console. The subject or function which you\u2019d like information about is put inside of\nbrackets and quotation marks, directly following the help.search command. For example, to\nobtain information about Analysis of Variance, enter into the console:\n> help.search(\u201cAnalysis of Variance\u201d)\nIf you\u2019d like to see an example of how a function is used, enter \u201cexample\u201d followed by the function\nthat you\u2019d like to see an example of (within quotation marks and brackets). For instance, if we\nwanted to see an example of how the aov function can be used, we can enter into the console:\n> example(\u201caov\u201d)\nAn example is returned in the console window.\nReading the CSV File\nNow, we want to read the CSV file from our working directory into RStudio. At this point, we\nlearn an important operator: <-. This operator is used to name data that is being read into the R\ndata frame. The name you give to the file goes on the left side of this operator, while the command\nread.csv goes to its right. The name of the CSV file from your working directory is entered in the\nparenthesis and within quotations after the read.csv command. The command header = T is used\nin the function to tell R that the first row of the data file contains column names, and not data.\nRead the file into R by entering into the Console: 218 | PLANT BREEDING BASICS\ndata <- read.csv(\u201cReview Models Install R ALA data.csv\u201d, header = T)\nTip: If you are working out of the Console and received an error message because you typed\nsomething incorrectly, just press the \u2191 key to bring up the line which you previously entered.\nYou can then make corrections on the line of code without having to retype the entire line in the\nconsole window again. This can be an extremely useful and time saving tool when learning to use\na new function. Try it out.\nIf the data was successfully read into R, you will see the name that you assigned the data in the\nWorkspace/History window (top-right).\nExamining the Data\nLet us look at the first few rows of the data. We can do this by entering the command\nhead(data) in the console. If we want to look at a specific number of rows, let us say just the\nfirst 3 rows, we can enter head(data, n=3) in the Console. Try both ways.\nFirst, enter into the console:\n> head(data)\nPop Rep Yield\n1 30 1 137.1\n2 30 2 124.4\n3 30 3 145.9\n4 40 1 166.1\n5 40 2 147.4\n6 40 3 142.7\nNow, try looking at only the first 3 rows:\n> head(data)\nPop Rep Yield\n1 30 1 137.1\n2 30 2 124.4 PLANT BREEDING BASICS | 219\n3 30 3 145.9\nViewing and Removing Datasets\nNow, let us say we are finished using this dataset and want to remove it from the R data frame.\nTo accomplish this, we can use the rm command followed by the name of what we want removed\nin parenthesis. Let us remove the data from the R data frame. Enter into the console rm(data).\nrm(data)\nThe dataset data should no longer be present in the Workspace/History window.\nWhat if we have many things entered in the R data frame and want to remove them all? There are\ntwo ways that we can do this. To demonstrate how, let us first enter 3 variables (x,y, and z) into\nthe R data frame. Set x equal to 1, y equal to 2, and z equal to 3.\nx<-1\ny<-2\nz<-3\nClicking on \u2018clear\u2019 in the History/Environment window (top right) will clear everything in the R\ndata frame. Another way to remove all data from the R data frame is to enter in the console:\nrm(list=1s())\nTry both ways.\nEDA with R\nObjectives\n\u2022 Students will conduct exploratory data analyses (EDA) on data from a simple Completely\nRandomized Design (CRD).\n\u2022 Assess whether students know how to interpret results from EDA.\n\u2022 Students will conduct an Analysis of Variance (ANOVA) on data from a simple CRD.\nDirectory Of R Commands Used\n\u2022 getwd() 220 | PLANT BREEDING BASICS\n\u2022 setwd()\n\u2022 read.csv()\n\u2022 rm()\n\u2022 rm(list=())\n\u2022 hist()\n\u2022 attach()\n\u2022 boxplot()\n\u2022 str()\n\u2022 as.factor()\n\u2022 aov()\n\u2022 summary()\nSet Working Directory\nBefore you can conduct any analysis on data from a text file or spreadsheet, you must first enter,\nor read, the data file into the R data frame. For this activity, our data is in the form of an Excel\ncomma separated values (or CSV) file; a commonly used file type for inputting and exporting data\nfrom R.\nMake sure that the data file for this exercise is in the working directory folder on your desktop.\nNote: We previously discussed how to set the working directory to a folder named on your\ndesktop. For this activity, we will repeat the steps of setting the working directory to reinforce\nthe concept.\nIn the Console window, enter getwd(). R will return the current working directory below the\ncommand you entered:\ngetwd()\n[1] \u201cC:/Users/<Username>/Documents\u201d\nSet the working directory to the folder on your desktop by entering setwd(). For a folder\nnamed \u2018wd\u2019 on our desktop, we enter:\nsetwd(\u201cC:/Users/<Username>/Desktop/wd\u201d)\nPlease note that the working directory can be in any other folder as well, but the data file has to\nbe in that specific folder. PLANT BREEDING BASICS | 221\nReading the CSV File\nNow, we want to read the CSV file from our working directory into RStudio. At this point, we\nlearn an important operator: <-. This operator is used to name data that is being read into\nthe R data frame. The name you give to the file goes on the left side of this operator, while the\ncommand read.csv goes to its right. The name of the CSV file from your working directory, in\nthis case CRD.1.data.csv, is entered in the parenthesis and within quotations after the read.csv\ncommand. The command header = T is used in the function to tell R that the first row of the data\nfile contains column names, and not data.\ndata <- read.csv(\u201cCRD.1.data.csv\u201c, header = T)\nTip: If you are working out of the Console and received an error message because you typed\nsomething incorrectly, just press the \u2191 key to bring up the line which you previously entered.\nYou can then make corrections on the line of code without having to retype the entire line in the\nconsole window again. This can be an extremely useful and time saving tool when learning to use\na new function. Try it out.\nIf the data was successfully read into R, you will see the name that you assigned the data in the\nWorkspace/History window (top-right).\nExploring the Data\nLet us do some preliminary exploring of the data.\nRead the data set into the R data frame.\n> data <- read.csv(\u201cCRD.1.data.csv\u201c, header = T)\nFirst, let us look at a histogram of the yield data to see if they follow a normal distribution. We\ncan accomplish this using the hist command.\nEnter into the console:\n> hist(data$Yield, col=\u201dblue\u201d, main= \u201cHistogram of Yield of 3 Synthetic Maize Populations\u201d,\nxlab=\u201dYield (bushels/acre)\u201d, ylab=\u201dFrequency\u201d)\nR returns the histogram in the Files/Plots/Packages/Help window (bottom-right). 222 | PLANT BREEDING BASICS\nHistogram\nFig. 8\nLet us go through the command we just entered: data$Yield specifies that we want to plot\nthe values from the column Yield in the data, col=\u201dblue\u201d indicates which color the histogram\nshould be, the entry in quotations after main= indicates the title that you\u2019d like to give the\nhistogram, the entries after xlab= and ylab= indicate how the x and y axes of the histogram\nshould be labeled. The histogram appears in the bottom-right window in RStudio.\nThe histogram can be saved to your current working directory by clicking \u2018export\u2019 on the toolbar\nat the top of the lower-right window, then clicking \u201csave plot as PNG\u201d or \u201csave plot as a PDF\u201d.\nYou may then select the size dimensions you would like applied to the saved histogram.\nBoxplots\nLet us now look at some boxplots of yield by population for this data. First, enter into the Console\nwindow attach(data). The attach command specifies to R which data set we want to work\nwith, and simplifies some of the coding by allowing us just to use the names of columns in the\ndata, i.e. Yield vs. data$Yield. After we enter the attach command, we\u2019ll enter the boxplot\ncommand.\n> attach(data)\n> boxplot(Yield~Pop, col=\u201dred\u201d, main=\u201dYield by Population\u201d, xlab=\u201dSynthetic Population\u201d,\nylab=\u201dYield\u201d)\nR returns the boxplot in the bottom-right window. PLANT BREEDING BASICS | 223\nFig. 9\nLet us go through the boxplot command: Yield~Pop indicates that we want boxplots of the\nyield data for each of the 3 populations in our data, col= indicates the color that we want our\nboxplots to be, main= indicates the title we want to give the boxplots, and xlab= and ylab=\nindicate what we want the x and y axes labeled as.\nNote: Yield is capitalized in our data file, thus it MUST also be capitalized in the boxplot\ncommand.\nMean and Coefficient of Variance\nThe coefficient of variance can be calculated for each population in the data set. Looking at\nthe data, we can see that lines 1 to 3 pertain to population 30. We know that the coefficient of\nvariation for a sample is the mean of the sample divided by the standard deviation of the sample.\nBy using the command mean(), we can calculate the mean for a sample. Remember that to\nspecify a column from a data frame, we use the $ operator. If we want to calculate the mean of\npopulation 30 from the data (rows 1 to 3 in the data), we can enter\n> mean(data$Yield[1:3])\nTo calculate the standard deviation of the yield for population 30, enter\n> sd(data$Yield[1:3])\nThe coefficient of variance is therefore calculated by entering\n> mean(data$Yield[1:3])/sd(data$Yield[1:3]) 224 | PLANT BREEDING BASICS\nOne-Factor ANOVA of a CRD\nNow that we\u2019ve gained some intuition about how the data behave, let us carry out an ANOVA\nwith one factor (Pop) on the data. We first need to specify to R that we want Population to be a\nfactor. Enter into the Console\n> Pop<-as.factor(Pop)\nLet us go through the command above: as.factor(data$Pop) specifies that we want the\nPop column in dataset data to be a factor, which we\u2019ve called Pop.\nNow that we have population as a factor, we\u2019re ready to conduct the ANOVA. The model that we\nare using for this one-factor ANOVA is Yield=Population.\nIn the Console, enter\n> mean(data$Yield[1:3])/sd(data$Yield[1:3])\nInterpret the Results\nLet us look at the ANOVA table. Enter out in the Console window.\n> out\nIn this ANOVA table, the error row is labelled Residuals. In the second and subsequent columns\nyou see the degrees of freedom for Pop and Residuals (2 and 6), the treatment and error sums of\nsquares (6440 and 1011), the treatment mean square of 3220, the error variance = 169, the F ratio\nand the P value (19.1 and 0.0025). The double asterisks (**) next to the P value indicate that the\ndifference between the yield means of the three populations is significant at 0.1% (i.e. we reject\nthe null hypothesis that the yield means of each population are statistically equivalent). Notice\nthat R does not print the bottom row of the ANOVA table showing the total sum of squares and\ntotal degrees of freedom.\nHypothesis Tests\nObjectives\nDemonstrate ability to interpret types of errors that can be made from testing various kinds of\nhypotheses. PLANT BREEDING BASICS | 225\nStatistical Inference\nThe purpose of statistical inference is to interpret the data we obtain from sampling or designed\nexperiments. Preliminary insights come from graphical data summaries such as bar charts,\nhistograms, box plots, stem-leaf plots and simple descriptive statistics such as the range\n(maximum, minimum), quartiles, and the sample average, median, mode. These exploratory data\nanalysis (EDA) techniques should always be used prior to estimation and hypothesis testing.\nHowever, prior to conducting EDA, the phenotype should be modeled using the parameters in\nthe experimental and sampling designs.\nNull and Alternative Hypotheses\nHypotheses are questions about parameters in models. For example, \u201cIs the average value for a\ntrait different than zero?\u201d is a question about whether the parameter \u00b5 is non-zero. Formally,\nthe proposition , is called the null hypothesis, while a proposition is\ncalled an alternative hypothesis.\nA test statistic is used to quantify the plausibility of the data if the null hypothesis is true. For this\nsimple hypothesis the value of the test statistic should be close to zero if the null hypothesis is\ntrue and far from zero if the alternative hypothesis is true. Notice that in all linear models there is\na parameter, \u03b5, included to indicate that there is some random variability in the data that cannot\nbe ascribed to the other parameters in the model. It is entirely possible that the variability in the\ndata is due entirely to \u03b5 and that an estimate of \u00b5 that is not zero is due to this random variability.\nInferential Errors from Hypothesis Testing\nHow often will the estimate of \u00b5 be different from zero when H is true? We can answer\no\nthis question by rerunning an experiment in which we know \u00b5 = 0 a million times, generate a\nhistogram of the resulting distribution and then see how often (relative to 1 million) an estimated\nmean that is equal to or more extreme than our experimental estimate occurs. This is the\nfrequency associated with finding our estimated value or a more extreme value when H is true.\no\nThe good news is that we don\u2019t have to conduct a million such experiments because someone\nelse has already determined the distribution when \u00b5 = 0, is true. The frequency value associated\nwith a test statistic as extreme or more extreme than the one observed is often referred to as a \u2018p\u2019\nvalue. The smaller the p value, the more comfortable we should be in rejecting the null hypothesis\nin favor of an alternative hypothesis. Keep in mind that we can be wrong with making such a\ndecision. In fact we are admitting that such a decision will be incorrect at a frequency of p. 226 | PLANT BREEDING BASICS\nError Types\nConsider another simple example where we hypothesize that two genotypes have the same mean\nfor some trait of interest. The difference between two genotypes is tested by Equation 19:\nFormula for testing the difference between two genotypes.\nwhere:\nth th\n= the i and j true genotypic effects on the trait of interest.\nWhether or not a decision based on observed data is correct depends on the true value of the\ndifference between the means (Table 4).\nTable 4 Possible outcomes in testing the hypothesis that . Columns indicate the\nthree possible true states. Rows indicate the three possible decisions made on the basis of\nestimates from measured data.\nTrue Situation\nDecision based on empirical data\n1. Correct decision Type I error Type III error\n2. Type II error Correct decision Type II error\n3. Type III error Type I error Correct decision\nA Type I error is committed if the null hypothesis is rejected when it is true (\u03b4 =0 and the\nij\nhypothesis of equality is rejected). A Type II error is committed if the null hypothesis is accepted\nwhen it is really false (\u03b4 \u22600 and the hypothesis of equality is not rejected). Type I error is also\nij\ncalled \u201cfalse positive\u201d, and Type II error is also known as a \u201cfalse negative.\u201d A Type III error\noccurs if the first decision is made when the third decision should have been made. This error\nalso occurs if the third decision was made when the first decision was correct. Type III errors are\nsometimes called reverse decisions.\nSignificance Levels\nThe probability (or frequency) of a Type I error is the level of significance, denoted by .\nThe choice of can be any desirable value between 0 and 1.\nFor example, if a test is carried out at the 5% level, is 0.05.\nIf you carry out tests at 5% level you will reject 5% of the hypotheses you test when they are really PLANT BREEDING BASICS | 227\ntrue.\nThe rejection rate can be reduced by choosing a lower level of\nHowever, the choice of will affect the frequency of Type II and Type III errors.\nA Type III error rate, \u03b3, is the frequency of incorrect reverse decisions and is always less than\n\u03b1/2 even for the smallest magnitudes of the standardized true difference, \u03b4 /\u03c3 where \u03c3 is the\nij d d\nparameter value of the standard error of the mean difference. Representative values of \u03b3 are\nshown below in Table 5.\nTable 5 Type III error rates, y, when the df associated with a t-test is 40.\nSignificance Level ( )\nStandardized true difference\n0.05 0.10 0.20 0.40\n0.3 0.0127 0.0271 0.0584 0.1283\n0.9 0.002 00068 0.0167 0.0438\n1.5 0.0005 0.0014 0.0039 0.019\n2.1 0.0001 0.0002 0.0008 0.0026\n2.7 0.0000 0.0000 0.0001 0.0005\nPower of the Test\nLastly, consider the error that is committed if the null hypothesis is not rejected when it is truly\nfalse. This is also known as a Type II error, and the probability of this type of error is denoted by\n\u03b2. It is the frequency of failure to detect real differences and is also affected by both the choice of\n\u03b1 and the magnitude of the standardized true difference (Table 6).\nTable 6 Type II error rates, \u00df, or the frequencies of failure to detect\ndifferences when the test of significance is based on 40 df.\nSignificance Level\nStandardized true difference\n0.05 0.10 0.20 0.40\n0.3 0.941 0.886 0.781 0.579\n0.9 0.863 0.774 0.639 0.437\n1.5 0.697 0.571 0.419 0.248\n2.1 0.469 0.340 0.214 0.107\n2.7 0.251 0.158 0.085 0.035\nNotice that \u03b1 + \u03b2 \u2260 1.0. The power of the test is = 1 \u2013 \u03b2 and is denoted \u03c0, thus \u03b2 + \u03c0 = 1.0. The power\nof a test is the probability of rejecting the null hypothesis when it is false. It can be increased 228 | PLANT BREEDING BASICS\nby decreasing either the value of \u03b1 or decreasing the value of \u03c3 by increasing the number of\nd\nreplications per treatment or by improving the experimental design.\nAnalysis of Variance\nObjectives\nStudents will demonstrate the ability to conduct and interpret Analysis of Variance.\nStatistical Inference\nThe purpose of statistical inference is to interpret the data we obtain from sampling or designed\nexperiments. Preliminary insights come from graphical data summaries such as bar charts,\nhistograms, box plots, stem-leaf plots, and simple descriptive statistics such as the range\n(maximum, minimum), quartiles, and the sample average, median, mode. These exploratory data\nanalysis (EDA) techniques should always be used prior to estimation and hypothesis testing.\nHowever, prior to conducting EDA, the phenotype should be modeled using the parameters in\nthe experimental and sampling designs.\nBackground\nThe AOV has been the primary tool for testing hypotheses about parameters in linear models.\nThe AOV was originally developed and introduced for analyses of quantitative genetic questions\nby R.A. Fisher (1925). Since its introduction, the assumptions underlying the AOV have guided\ndevelopment of sophisticated experimental designs, and with increasing computational\ncapabilities the AOV has evolved to provide estimates of variance components from these\ndesigns. While the breadth and depth of experimental design and analyses of linear models are\nbeyond the scope of this class, it is worth recalling the salient features of experimental design\nand their impact on inferences from the AOV.\nExperimental Designs consist of design structures, treatment structures, and allocation of these\nstructures to experimental units. Typical design structures utilized by plant breeders include\nRandomized Complete Block, Lattice Incomplete Block and Augmented Designs. The primary\ntreatment designs of interest of plant breeders involve allocation of genotypes to experimental\nunits. This is accomplished primarily through mating, although with the emergence of\nbiotechnologies, such as protoplast fusion, tissue culture and various transgenic technologies,\nthere are many ways to allocate treatments (genotypes) to experimental units. Would you consider\ntreatments from these technologies as fixed or random effects? Why? Experimental units can be\nsplit in both time and space, resulting in the ability to apply treatment and design structures to\ndifferent sized experimental units. PLANT BREEDING BASICS | 229\nDesign Principles\nDesign principles in allocation of treatment and design structures to experimental units include\nRandomization, Replication and Blocking. These are principles rather than rigid rules. As such,\nthey provide flexibility in designing experiments to draw inferences about biological questions.\nAssuming that these principles are applied appropriately, experimental data can be used for\nobtaining unbiased estimates of treatment effects, variances, covariances, and even predict\nbreeding values.\nCompletely Random Design\nLet us imagine that we have two plant introduction accessions. We wish to evaluate whether\nthese two accessions are unique with respect to yield.\nAssume that we have 10 plots available for purposes of testing the null hypothesis that there is no\ndifference in their yield. Also, assume that we have enough seed to plant 200 seeds in each plot.\nLet us next assume that the 10 plots consist of two-row plots that are arranged in a 5\u00d72 grid\nconsisting of five ranges with 2 plots per range. We can randomly assign seed from each accession\nto the 10 plots. This would represent a Completely Random Design (CRD). Can you explain why?\nFixed and Random Effects\nPrior to execution of the experiment, we want to model the phenotypic data using a linear\nfunction. In this case we would model the phenotypic data using Equation 20:\nLinear model for phenotype.\nwhere:\n= the yield of plot i, j, where i = 1, 2 for accession and j = 1, \u2026, 5 for replicate,\n= represents the mean of accession i,\n\u03c32\n= the error, ~ i.i.d. N(0, ).\nIt is important to get in the habit of recognizing whether the parameters of the model are\nconsidered random or fixed effects.\nIn this first model, since we selected the two accessions, rather than sampled them from some\npopulation, we should consider them to be fixed effects. The parameter \u03b5 representing the\ni,j\nresidual or error in the model is based on a sample of plots to which experimental units are\nassigned, so \u03b5 is considered a random effect.\ni,j 230 | PLANT BREEDING BASICS\nAOV Based on Yield\nNext, let us say that we evaluate the plots for yield (bushels per acre) as well as stand counts\n(plants per plot) at the time of harvest. The resulting data might look something like in Table7.\nTable 7 Table 1 Yield (t per ha) as well as stand\ncounts (plants per plot).\nn/a PI accession 1 PI accession 2\nBlock (t/ha) (plants/plot) (t/ha) (plants/plot)\n1 1.69 91 1.88 102\n2 1.95 122 1.82 89\n3 2.20 143 2.01 139\n4 2.13 145 2.01 147\n5 1.76 110 1.95 112\nIf we conduct an AOV based on yield using the model for a CRD, we will generate a table that\nlooks something like Table 8.\nTable 8 ANOVA outline.\nSource df MS F Prob\nAccession 1 n/a n/a n/a\nResidual 8 n/a n/a n/a\nBlocking Ranges\nSuppose that we suspect a gradient for some soil factor (moisture, organic matter, fertility, etc.)\nacross the ranges. In order to remove the effect of the gradient on our comparisons between the\ntwo accessions, we should probably \u2018block\u2019 each range as a factor in our model.\nLet us further assume that we block the accession \u2018treatments\u2019 into five blocks consisting of\ntwo plots each. If we randomly group pairs of the accessions into 5 sets, next randomly assign\neach set to a range, and third, randomly assign each accession within a set to the plots within\nranges, we will have a randomized complete block design (RCBD) that can be modeled as\nwhere the definition of parameters is the same as the CRD model, but\nwith the added term for a blocking factor. PLANT BREEDING BASICS | 231\nMixed Linear Model\nIn this second model, the accessions are selected so we should consider them to be fixed effect\nparameters. Although the block parameter represents a sample of many possible blocks in the\nfield trial, there are only a few blocks that represent a \u201cnuisance\u201d source of variability, so we can\ntreat them as a fixed effect, while the parameter \u03b5 represents the residual or error in the model\nij\nwhich is based on a sample of plots to which experimental units are assigned.\nThus \u03b5 is considered random effects where \u03b5 ~ i.i.d. N(0, \u03c3 2 ), and the model is considered a\nij ij e\nmixed linear model.\nTable 9 ANOVA outline for mixed model.\nSource df MS F Prob\nBlock 4 n/a n/a n/a\nAccession 1 n/a n/a n/a\nResidual 4 n/a n/a n/a\nRegression and Prediction\nObjectives\nDemonstrate ability to conduct and interpret regression analyses.\nStatistical Inference\nThe purpose of statistical inference is to interpret the data we obtain from sampling or designed\nexperiments. Preliminary insights come from graphical data summaries such as bar charts,\nhistograms, box plots, stem-leaf plots, and simple descriptive statistics such as the range\n(maximum, minimum), quartiles, and the sample average, median, mode. These exploratory data\nanalysis (EDA) techniques should always be used prior to estimation and hypothesis testing.\nHowever, prior to conducting EDA, the phenotype should be modeled using the parameters in\nthe experimental and sampling designs.\nLinear Regression\nHistorically, linear (and non-linear) regression has not been utilized extensively by plant breeders,\nalthough it provides the conceptual foundation for understanding additive genetic models and\nanalysis of covariance. Recently, with the emergence of molecular marker technologies, the\nimportance of linear regression has manifested itself in the development of predictive methods 232 | PLANT BREEDING BASICS\nsuch as Genomic Prediction. Linear regression is an approach to modeling the relationship\nbetween a scalar dependent variable Y (e.g., harvestable grain yield per unit of land) and one or\nmore explanatory variables (e.g., breeding values of lines involved in crosses) denoted by X.\nBasic Assumptions\nIn linear regression, the phenotype is modeled using a linear function. There are four basic\nassumptions made about the relationship between a response variable Y and an explanatory\nvariable X.\n1. All Y values are from independent experimental or sample units.\n2. For each value of X, the possible Y values are distributed as normal random variables.\n3. The normal distribution for Y values corresponding to a particular value of X has a mean\n\u03bc{Y|X} that lies on a line (Equation 21):\n\u00b5\nFormula for estimating the mean.\nwhere:\n= the intercept and represents the mean of the Y values when X = 0,\n= the slope of the line, that is, the change in the mean of Y per unit increase in X.\n4. The distribution of Y values corresponding to a particular value of X has standard deviation\n\u03c3 {Y|X). The standard deviation is usually assumed to be the same for all values of X so that\nwe may write \u03c3{Y|X}=\u03c3. Violation of the last assumption is typical in plant breeding data and\nthe development of methods to account for unequal variances is an area of important\nresearch.\nSimple Linear Regression\nSuppose we have n observations of a response variable Y and an explanatory variable X: (X ,Y ), .\n1 1\n. . , (X ,Y ). The model can be rewritten as in Equation 22:\nn n\nLinear model for phenotype.\nwhere:\nare as defined previously, PLANT BREEDING BASICS | 233\nfor i = 1, . . . , n experimental units. e , . . . , e are assumed to be independent normal random\ni n\nvariables with mean 0 and standard deviation \u03c3{Y|X}=\u03c3. Thus, least-squares estimates of the Y\ni\nvalues are obtained using Equation 23:\nLeast squares estimates model for Y values.\nwhere:\nare as defined previously.\nThe residual e (e , . . . , e ) can be calculated as represented in Equation 24:\ni 1 n\nFormula for estimating residuals.\nwhere:\nare as defined previously.\nParameter Estimates\nThe estimators for parameters , , and \u03c3 are 234 | PLANT BREEDING BASICS\nFig. 10 Estimator plot.\nPrediction\nNotice that Equation 23, , provides a predicted value of Y. Imagine that the xi\ni\n(i=1, \u2026, n) values are a genotypic index for cultivar/individual i, such as the sum of all allelic values\n(+1 or -1) at quantitative trait loci throughout the genome. Some cultivars could have 60 positive\nallelic values and no negative allelic values, while other cultivars could have a genotypic index\nof -20 (see Figure 10). If the positive genotypic index values are associated with high phenotypic\nvalues, such as in the figure, then we will have a strong positive linear relationship between the\ngenotypic index and the phenotypes. A strong linear relationship can enable the plant breeder\nto predict phenotypes without having to spend resources on growing cultivars. The stronger the\nlinear relationship is between the genotypic index and the phenotype (less variability around the\nline), the better the ability to predict. This concept represents the foundation for what is widely\nreferred to as Genomic Prediction.\nThere are a number of details about how allelic values are estimated and combined into genotypic\nindices. The foundational concepts that address these details are covered in the Introduction to\nQuantitative Genetics section.\nAnalysis of Covariance\nObjective: Demonstrate ability to conduct and interpret Analysis of Covariance PLANT BREEDING BASICS | 235\nStatistical Inference\nThe purpose of statistical inference is to interpret the data we obtain from sampling or designed\nexperiments. Preliminary insights come from graphical data summaries such as bar charts,\nhistograms, box plots, stem-leaf plots, and simple descriptive statistics such as the range\n(maximum, minimum), quartiles, and the sample average, median, mode. These exploratory data\nanalysis (EDA) techniques should always be used prior to estimation and hypothesis testing.\nHowever, prior to conducting EDA, the phenotype should be modeled using the parameters in\nthe experimental and sampling designs.\nAOC is typically applied when there is a need to adjust results for variables that cannot be\ncontrolled by the experimenter. For example, imagine that we have two plant introduction\naccessions and we wish to evaluate whether these two accessions are unique with respect to yield.\nAlso, imagine that germination rates for each accession is different but unknown, especially\nunder field conditions in a new environment. We could decide to overplant each plot and reduce\nthe number of plants per plot to a constant number equal to a stand count that is typical under\ncurrent Agronomic practices. However, such an approach will be labor-intensive and not as\ninformative as simply adjusting plot yields for stand counts.\nExample\nAssume that we have 10 plots available for purposes of testing the null hypothesis that there is no\ndifference in their yield. Also, assume that we have enough seed to plant 200 seeds in each plot,\nalthough current agronomic practices are more closely aligned with stands of about 125 plants\nper plot. Let us next assume that the 10 plots consist of two-row plots that are arranged in a 5\u00d72\ngrid consisting of five ranges with 2 plots per range. Suppose that we suspect a gradient of some\nsoil factor (moisture, organic matter, fertility, etc.) across the ranges. In order to remove the effect\nof the gradient on our comparisons between the two accessions we should probably \u2018block\u2019 each\nrange as a factor in our model. If we randomly group pairs of the accessions into 5 sets, next\nrandomly assign each set to a range and third randomly assign each accession within a set to\nthe plots within ranges, we will have a RCBD. At the time of harvest, we evaluate the plots for\nyield (bushels per acre) as well as stand counts (plants per plot). The resulting data are arranged\nin Table 10. 236 | PLANT BREEDING BASICS\nTable 10 Data from RCBD plot.\nn/a PI accession 1 PI accession 2\nBlock (t/ha) (plants/plot) (t/ha) (plats/plot)\n1 1.69 91 1.88 102\n2 1.95 122 1.82 89\n3 2.20 143 2.01 139\n4 2.13 145 2.01 147\n5 1.76 110 1.95 112\nModel Equation\nIf we model the yield data as Y = b + \u00b5+ \u03b5 , where Y is the yield of plot ij , \u00b5 represents the\nij j i ij ij i\nmean of accession i, b represents the jth block in which each pair of accessions are grown and \u03b5\nj ij\nN(0,\u03c32\n~ i.i.d. ), the resulting analysis revealed that the variability between accessions is not much\ngreater than the residual variability. We might interpret this to mean that there is no difference\nin yield between the two accessions. However, our real interest is in whether there is a difference\nbetween the accessions at the same stand counts. A more appropriate model for the question of\ninterest is as in Equation 25:\nFormula for calculating phenotype in a plot.\nwhere:\n= intercept for accession i,\n= slope of accession i,\nth\n= the j stand count in accession i (i = 1, 2,)\n= random effect parameter.\nThe model has two intercepts, denoted \u03b1 (i = 1, 2) for each of the accessions, and two slopes\ni\nth\ndenoted \u00df (i = 1, 2), for each of the accessions. X is the j stand count in accession i (i = 1, 2).\ni ij\nThe model also has random effects parameters denoted by b and \u03b5 where b~ i.i.d. N(0,\u03c3 2 ) and\nj i,j j b\n\u03b5 ~ i.i.d. N(0, \u03c3 2 ). The resulting analyses of variability associated with each of the parameters\ni,j e\nis known as Analysis of Covariance, and can be thought of as an approach that takes advantage\nof both regression and ANOVA, i.e., an AOC model includes parameters representing both\nregression and factor variables. The result of the estimation procedure will enable us to evaluate\nwhether the accessions are equal at stand counts of interest. In other words it will be possible PLANT BREEDING BASICS | 237\nto adjust yield values to various stand counts of interest. As a matter of ethics in science, the\nvariable stand count of interest needs to be modeled prior to conducting the field trial.\nComputational Considerations\nKey Concepts\nAs long as data are balanced all computational algorithms will provide the same estimates of\nvariance components.\n\u2022 When data are not balanced, either by design or accident, simple algorithms implemented\nin many widely used software packages (EXCEL, JMP for exampes) will not provide correct\nestimates of variance components.\nStatistical Inference\nThe purpose of statistical inference is to interpret the data we obtain from sampling or designed\nexperiments. Preliminary insights come from graphical data summaries such as bar charts,\nhistograms, box plots, stem-leaf plots and simple descriptive statistics such as the range\n(maximum, minimum), quartiles, and the sample average, median, mode. These exploratory data\nanalysis (EDA) techniques should always be used prior to estimation and hypothesis testing.\nHowever, prior to conducting EDA, the phenotype should be modeled using the parameters in\nthe experimental and sampling designs.\nComputational Methods\nMost plant breeding data are obtained using a limited number of field plot designs consisting\nof lines (cultivars, hybrids, synthetics, etc.), environments and occasionally complete blocks, but\nusually incomplete blocks, within environments. Further, numbers of observations per source\nof variation are seldom balanced; even if designed for balance, some plots are lost during the\ncourse of a growing season. Thus, while the algorithm for obtaining EMS (described in the\nsection Statistical Inference: Analysis of Variance) is useful for learning basic concepts, it is of\nlittle practical use for most plant breeding projects. Just as the estimates of means need to be\nadjusted through use of lsmeans, advanced computational methods are needed to obtain accurate\nestimates of variance components of the linear model when data are obtained from unbalanced\nconditions. The computational methods are affected by fixed effects, random effects or a mixture\nof both types of effects. There are three primary computation methods for estimating variance 238 | PLANT BREEDING BASICS\ncomponents: Method of Moments (MM), Maximum Likelihood (ML), and Restricted Maximum\nLikelihood (REML).\nRegression, Anova, and AOC\nComputation of the MM estimators of variance components is essentially a matter of equating\nobserved mean squares, calculated using the sums of squared deviations and cross products,\nwith the expected mean squares, as demonstrated by Lorenzen and Anderson (1993, Design of\nExperiments: A No-Name Approach. p 71-72). These are appropriate if the data are balanced.\nMost advanced statistical software packages, e.g., SAS and R, calculate the sums of squares\nand cross products for the MM using the MIVQUE(0) algorithm (Minimum Variance Quadratic\nUnbiased Estimator, with no weighting for random effects).\nComputation of ML and REML are derived from MIVQUE(0); both use MIVQUE(0) estimates as\nstarting points in an iterative algorithm that maximizes the likelihood function, assuming that\nthe random effects are distributed as random normal variables. The difference between ML and\nREML is that the likelihood function in REML is maximized only for the random effects, i.e., the\nfixed effects are removed from the likelihood function. For a model consisting of only random\neffects, both ML and REML will provide the same results. Indeed, for completely balanced data\nfrom random effects models, all three computational methods provide the same results. When\ndealing with unbalanced data or mixed effect models, REML has been shown to be the best\ncomputational method.\nFurther Considerations\nAs a practical matter, if your data is missing less than 10% of the experimental units within\nany environment, the MM approach will provide estimates that are almost as good as REML.\nOtherwise, the estimates should be obtained with mixed model equations (MME) and a REML\nalgorithm. We encourage the use of R or SAS software for conducting data analyses. R is free,\nwhile the SAS license fees pay for more rigorous quality assurance.\nMatrix Algebra\nA matrix is a collection of numerical values arranged in rows and columns. Herein, the elements\nof a matrix are enclosed in brackets. For example, PLANT BREEDING BASICS | 239\n.\nis a matrix with 4 elements arranged in 2 rows and two columns.\nMatrices with more than two or more rows and columns are denoted with upper case bold letters.\nVectors are a special type of matrix with only one row or one column. For example,\n.\nSpecial Kinds of Matrices\nVector matrices are denoted with lower case bold italicized letters. A matrix consisting of only\none row and one column is referred to as a scalar matrix. A square matrix has the same number\nof rows and columns. A diagonal matrix is a square matrix with off-diagonal elements equal to 0.\nAn identity matrix is a diagonal matrix with diagonal elements = 1. The identity matrix is almost\nalways denoted I.\nOperations\nMatrices must be conformable, i.e., matrix operations have requirements on the numbers of rows\nand columns.\nIt is possible to add or subtract two matrices, but only if they have the same numbers of rows and\ncolumns. For example,\n.\nIt is possible to multiply a matrix by a scalar value (say \u2018v\u2019) by simply multiplying all elements of\nthe matrix by the scalar value, v. Thus, 240 | PLANT BREEDING BASICS\n.\nMultiplying Vectors\nIt is possible to multiply two vectors, but only if 1) one of the vectors is a row vector, 2) the second\nis a column vector, 3) the row vector has as many elements as the column vector. For example,\n.\nThe operation of vector multiplication in the first instance indicates that we have a 1\u00d73 matrix\nmultiplied by a 3\u00d71 matrix. The way we carry out the vector multiplication is to multiply the\nelements from each matrix in a pairwise manner, then sum the results of all 3 pairs:\n.\nMultiplying Vectors In Reverse\nWe could also apply the rule of multiplying and summing pairs of elements to the reverse\narrangement of these two vectors:\n.\nNotice that the order of arrangement of vectors matters. Likewise, the arrangement of matrices\nthat are to be multiplied matters. Virtually all types of matrix multiplication involve the\nmultiplication of a row vector by a column vector. In essence, we partition each matrix into a set\nof row and column vectors, then apply the rules of vector multiplication.\nMatrix Multiplication\nth th\nLet us consider C=AB. c = a.b , where a is the i row vector of A and b is the j column vector\nij i j i j\nof B. For example, PLANT BREEDING BASICS | 241\n.\nNotice that matrix multiplication requires that the first matrix must have as many columns as the\nsecond matrix has rows. Thus, AB is usually not equal to BA. Indeed, while AB may be possible,\nBA may not. Lastly verify for yourself that IA, IB and Ix = A, B and x respectively.\nAdditional Important Operations\nt T\nThe transpose of a matrix, denoted as A\u2019 (or A or A ) is a useful operation in which the first row\nof a matrix becomes the first column of its transpose, while the second, third, \u2026 etc rows become\nthe second, third, \u2026 etc columns of its transpose. For example,\n.\nThe inverse of a matrix is best understood by recalling that in scalar algebra the inverse of a\n-1\nnumber multiplied by the number will be = 1. Thus the inverse of x is x . In matrix algebra\n-1\nthe inverse of a matrix is a matrix when multiplied by the original matrix is I. That is AA =\n-1\nA A= I. Only square matrices will have an inverse, although not all square matrices will have an\ninverse. Bernardo describes how to calculate the inverse of a simple 2\u00d72 matrix and it is possible\nto calculate inverse matrices consisting of 3\u00d73 elements, but calculations of inverses of larger\nmatrices are better left to software. 242 | PLANT BREEDING BASICS\nReferences\nBernardo, R. 1996. Best linear unbiased prediction of maize single-cross performance. Crop Sci\n36:50\u201356.\nBernardo, R. 2002. Breeding for quantitative traits in plants. Stemma Press.\nComstock, R. E. 1978. Quantitative genetics in maize breeding. In: Walden DB (ed) Maize\nbreeding and genetics. Wiley, New York, p 191\u2013206.\nChristensen, R. 1997. Log-Linear Models and Logistic Regression (2nd ed.) New York: Springer-\nVerlag.\nCrossa, J., R. C. Yang, P. and Cornelius. L. 2004. Studying crossover genotype \u00d7 environment\ninteraction using linear-bilinear models and mixed models. J. Agric. Biol. Environ. Stat. 9 (3):362\u2013\n80.\nFehr, W. R. 1991. Principles of cultivar development vol. 1: Theory and technique. MacMillan\nPublishing Company, USA.\nFisher, R. A. 1918. The correlation between relatives on the supposition of Mendelian\ninheritance. Trans R Soc Edinb 52:399\u2013433.\nFisher, R. A. 1925. Theory of Statistical Estimation. Oliver & Boyd, Edinburgh.\nFisher, R. A. 1928. Statistical methods for research workers. Scotland: Oliver and Boyd.\nFisher, R. A. 1935. The Design of Experiments (8th ed., 1966), New York: Hafner Press.\nHayes, H. K., and F. R. Immer, 1942. Methods Of Plant Breeding. McGraw-Hill publications in\nthe agricultural sciences.\nHenderson, C. R. 1975. Best linear unbiased estimation and prediction under a selection model.\nBiometrics 31:423\u2013447.\nLorenzen, T., and V. Anderson. 1993. Design of Experiments: A No-Name Approach. p 71-72)\nLush, J. L. 1948.The genetics of populations. Mimeographed notes, Iowa State College, Ames,\nIowa.\nMcCullagh, P,. and, J. A. Nelder. 1989. Generalized Linear Models, volume 37 of Monographs on\nStatistics and Applied Probability. Chapman and Hall, London, 2nd edition PLANT BREEDING BASICS | 243\nMeuwissen, T. H. E., B. J. Hayes, and M. E. Goddard. 2001. Prediction of total genetic values using\ngenome-wide dense marker maps. Genetics 157: 1819\u20131829.\nPiepho, H. P. 2009. Data Transformation in Statistical Analysis of Field Trials with Changing\nTreatment Variance. Agron. J. 101:865-869.\nHow to cite this module: Beavis, W. and A. A. Mahama 2023. Plant Breeding Basics. In W. P. Suza, &\nK. R. Lamkey (Eds.), Quantitative Genetics for Plant Breeding. Iowa State University Digital Press. 244 | APPLIED LEARNING ACTIVITIES\nApplied Learning Activities\nThe following downloadable Applied Learning Activities (ALAs) and associated files are aligned\nwith the chapters linked below:\nChapter 1\n\u2022 Disequilibrium [PDF]\n\u2022 Ideal Population_HWE [PDF]\n\u2022 Fate of a Rare Allele [XLSX]\nChapter 2\n\u2022 Gametic and Linkage D_Likelihood [PDF]\n\u2022 Gametic and Linkage D [PDF]\n\u2022 Gamete and LD Expected frequencies with linkage and selfing [XLSX]\nChapter 3\n\u2022 Full-sib Mating [PDF]\n\u2022 Introgression [PDF]\n\u2022 Self-pollination [PDF]\n\u2022 Example \u2013 Self-pollination with Coefficient of Inbreeding [XLSX]\nChapter 4\n\u2022 Relationship-Coefficient_Solve-Example [PDF]\n\u25e6 Eval Dist Metrics [XLSX\n\u2022 Cluster Analysis [PDF]\n\u25e6 ALA 4.6_DS [CSV]\n\u25e6 Cluster Analysis [TXT] (For use as .R file) APPLIED LEARNING ACTIVITIES | 245\nChapter 5\n\u2022 Breeding Values ALA [PDF]\n\u25e6 ALA5.2 DS [XLSX]\n\u2022 Epistasis [PDF]\n\u2022 GV and Population Means_Solved-example [PDF]\nChapter 6\n\u2022 Genetic Variance Components [PDF]\n\u2022 Quantitative Genetic Models ds6 [CSV]\n\u2022 Genetic Covariance and Heritability [PDF]\n\u2022 Genetic Covariances and Heritability ds7 [CSV]\nChapter 7\n\u2022 Evaluating new breeding germplasm ALA [PDF]\n\u25e6 ALA7.1_ds [CSV]\n\u25e6 ALA7.1 [TXT] (For use as .R file)\n\u2022 Estimating heritability of testcross population ALA [PDF]\n\u25e6 ALA7.2_ds [CSV]\n\u25e6 ALA7.2 [TXT] (For use as .R file)\n\u2022 Deriving Expectation of Mean Squares [PDF]\nChapter 8\n\u2022 GCA and SCA_North Carolina Design II [PDF]\n\u25e6 QG_Mod8_ALA8.3_ds1 [CSV]\n\u2022 General and Specific Combining Ability [PDF]\n\u25e6 QG_Mod8_ALA8.1_ds1 [CSV]\n\u25e6 QG_Mod8_ALA8.1_ds2 [CSV]\n\u25e6 QG_Mod8_ALA8.1 R [TXT] (For use as .R file)\n\u2022 Using the North Carolina II Design [PDF]\n\u2022 QG_Mod8_ALA8.2 [TXT] (For use as .R file) 246 | APPLIED LEARNING ACTIVITIES\nChapter 9\n\u2022 Estimate heritability [PDF]\n\u2022 Estimates of heritability_unbalanced data set [PDF]\n\u25e6 QG_Mod9_ALA9.2_ds [CSV]\n\u25e6 QG_Mod9_ALA9_2 [TXT] (For use as .R file)\n\u2022 Response to Selection and Genetic Gain [PDF]\nChapter 10\n\u2022 Types of GxE \u2013 Cluster Analysis ALA [PDF]\n\u2022 Types of GxE \u2013 Partition GxE ALA [PDF]\n\u25e6 Multi Environment Trials Types of GxE ds5 [CSV]\n\u2022 QG_Mod10_ALAs_ds [CSV]\n\u2022 QG_Mod10_ALA10_1 [TXT] (For use as .R file)\n\u2022 QG_Mod10_ALA10_2 [TXT] (For use as .R file)\n\u2022 QG_Mod10_ALA10.2_env_pheno_ds [CSV]\n\u2022 QG_Mod10_ALA10.2_env_pheno_stdize_ds [CSV]\n\u2022 QG_Mod10_ALA10.2_env_gxe_ds [CSV]\n\u2022 QG_Mod10_ALA10.2_env_gxe_stdize_ds [CSV]\n\u2022 QG_Mod10_ALA10_3 [TXT] (For use as .R file)\nChapter 11\n\u2022 Multiple Trait Selection ALA [PDF]\n\u25e6 QG_Mod11_ALA11.2 [CSV]\n\u25e6 QG_Mod11_ALA11.2 [XLSX]\n\u2022 QG_Mod11_ALA11_2 [TXT] (For use as .R file)\n\u2022 Matrix Algebra ALA [PDF]\n\u25e6 QG_Mod11_ALA11.3_Review_Matrix_Algebra [PDF]\n\u2022 Matrix Algebra \u2013 Smith-Hazel Index [PDF]\n\u2022 QG_Mod11_ALA11.4 [CSV]\n\u2022 QG_Mod11_ls mean s t1 and t2 [CSV]\n\u25e6 QG_Mod11 ALA11.4 ls mean s t1 [CSV]\n\u25e6 QG_Mod11 ALA11.4 ls mean s t2 [CSV] APPLIED LEARNING ACTIVITIES | 247\nChapter 12\n\u2022 Linear Mixed Models \u2013 Samples of lines ALA [PDF]\n\u2022 Linear Mixed Models \u2013 Reduce plots by half ALA [PDF]\n\u25e6 MET ds5 [CSV]\n\u2022 MET ALA12_2 [TXT] (For use as .R file)\n\u2022 MET ALA12_3 [TXT] (For use as .R file)\n\u2022 MET ALA12_4 [TXT] (For use as .R file)\nChapter 13\n\u2022 Simulation modeling ALA [PDF]\n\u25e6 QG_Mod13_ALA13.1_ds [CSV]\n\u25e6 QG_Mod13_ALA13.1 [TXT] (For use as .R file)\n\u2022 QG_ALA selection GBLUP [PDF]\n\u25e6 DS8 RILs [XLSX]\n\u2022 Selection Heritability and Genetic Gain Simulation Comparison ALA [PDF]\nPlant Breeding Basics\n\u2022 Install R ALA [DOC]\n\u2022 RawDataEarPhenotypes_0 [XLSX]\n\u2022 Review EDA with R ALA P[PDF]\n\u25e6 Review EDA with R ds3 [CSV]\n\u2022 Review Exploratory Data Analysis ALA [PDF]\n\u25e6 Review Exploratory Data Analysis ds2 [CSV]\n\u2022 Review Statistical Inference Analysis of Variance ALA [PDF]\n\u2022 Review Statistical Inference Regression and Prediction ALA [PDF]\n\u25e6 Review Statistical Inference Regression and Prediction ds4 [CSV]\n\u2022 Review Trait Measures ALA [PDF]\n\u2022 Review Types of Models Data Management ALA [PDF]\n\u25e6 Review Types of Models Data Management ds1 [XLSX]\n\u2022 Review Types of Models Field Plot Design ALA [PDF] 248 | CONTRIBUTORS\nContributors\nEditors\nWalter Suza\nSuza is an Adjunct Associate Professor at Iowa State University. He teaches courses on Genetics\nand Crop Physiology in the Department of Agronomy. In addition to co-developing courses\nfor the ISU Distance MS in Plant Breeding Program, Suza also served as the director of Plant\nBreeding e-Learning in Africa Program (PBEA) for 8 years. With PBEA, Suza helped provide\naccess to open educational resources on topics related to the genetic improvement of crops. His\nresearch is on the metabolism and physiology of plant sterols. Suza holds a Ph.D. in the plant\nsciences area (with emphasis in molecular physiology) from the University of Nebraska-Lincoln.\nKendall Lamkey\nLamkey is the Associate Dean for Facilities and Operations for the College of Agriculture\nand Life Sciences at Iowa State University. He works in collaboration with the dean, associate\ndeans, department chairs, college-level centers, and other unit leaders to ensure that operations\ndirectly advance the mission of the college and that resources are deployed wisely and efficiently.\nPreviously, he served as the chair for the Department of Agronomy at Iowa State University,\nwhere, in addition to advocating for research and the PBEA program, he oversaw the Agronomy\nDepartment\u2019s educational direction, its faculty, and Agronomy Extension and Outreach. Dr.\nLamkey is a corn breeder and quantitative geneticist and conducts research on the quantitative\ngenetics of selection response, inbreeding depression, and heterosis. He holds a Ph.D. in plant\nbreeding from Iowa State University and a master\u2019s in plant breeding from the University of\nIllinois. Lamkey is a fellow of the American Society of Agronomy and the Crop Science Society\nof America and has served as an associate editor, technical editor, and editor for Crop Science.\nChapter Authors\nWilliam Beavis, Ursula Frei, M. L. Harbur, Reka Howard, Kendra Meade, Laura Merrick, Ken\nMoore, Ron Mowers, and Dennis Todey CONTRIBUTORS | 249\nContributors\nAnthony A. Mahama, Gretchen Anderson, Todd Hartnell, Andy Rohrback, Tyler Price, Glenn\nWiedenhoeft, and Abbey K. Elder"
}