{"text":"Module_4 - Physical Database Design and Database Security\n4.1.1 - Introduction\nIn this chapter, we describe physical database design, with special emphasis on the\nrelational data model. Physical database design is the process of transforming logical data\nmodels into physical data models. An experienced database designer will make a physical\ndatabase design in parallel with conceptual data modeling if they know the type of database\ntechnology that will be used. Conceptual data modeling is about understanding the organization\nand gathering the right requirements. Physical database design, on the other hand, is about\ncreating stable database structures correctly expressing the requirements in a technical language.\nAlthough there are other data models, we have two reasons for emphasizing the relational data\nmodel in this chapter. First, the relational data model is one most commonly used in\ncontemporary database applications. Second, some of the principles of logical database design\nfor the relational model apply to other logical models as well.\nWe introduced the relational data model informally through simple examples in earlier\nchapters. It is important, however, to note that the relational data model is a form of logical data\nmodel, and as such, it is different from the conceptual data models. Thus, an Entity-Relationship\n(E-R) data model is not a relational data model, and an E-R model may not obey the rules for a\nwell-structured relational data model, called normalization. The E-R model was developed for\nother purposes such as understanding data requirements and business rules about the data rather\nthan structuring the data for sound database processing.\nWe next describe and illustrate the process of transforming an EER (Enhanced Entity-\nRelationship) model into the relational model. Many CASE tools support this transformation\ntoday at the technical level; however, it is important to understand the underlying principles and\nprocedures. We then describe the concepts of normalization in detail. Normalization, which is\nthe process of designing well-structured relations, is an important component of logical design\nfor the relational model. Finally, we describe how to merge relations while avoiding common\npitfalls that may occur in this process.\nIn Chapters two and three, we learned how to describe and model organizational data\nduring the conceptual data modeling and logical database design phases of the database\ndevelopment process. We learned how to use EER notation, the relational data model, and\nnormalization to develop abstractions of organizational data that capture the meaning of data.\nHowever, these notations do not explain how the data will be processed or stored. The purpose of\nphysical database design is to translate the logical description of data into the technical\nspecifications for storing and retrieving data. The goal is to create a design for storing data that\nwill provide adequate performance and ensure database integrity, security, and recoverability.\nPhysical database design does not include implementing files and databases (i.e., creating\nthem and loading data into them). Physical database design produces the technical specifications\nthat programmers, database administrators, and others involved in information systems\nconstruction will use during the implementation phase. In this chapter, we study the basic steps\nrequired to develop an efficient and high-integrity physical database design. We concentrate in\nthis chapter on the design of a single, centralized database and how to estimate the amount of\ndata that users will require in the database. We will also learn about choices for storing attribute values and how to select from\namong these choices to achieve efficiency and data quality. Because of recent U.S. and\ninternational regulations (e.g., Sarbanes-Oxley) on financial reporting by organizations, proper\ncontrols specified in physical database design are required as a sound foundation for compliance.\nHence, we place special emphasis on data quality measures you can implement within the\nphysical design. We will also learn why normalized tables are not always the basis for the best\nphysical data files and how to de-normalize the data to improve the speed of data retrieval.\nFinally, we learn about the use of indexes, which are important in speeding up the retrieval of\ndata.\nWhen performing physical database design, the decisions made during this stage have a\nmajor impact on data accessibility, response times, data quality, security, user friendliness, and\nsimilarly important information system design factors. Database administration plays a major\nrole in physical database design. Advanced design will be discussed in this chapter.\nComparison between Logical and Physical Design:\nLogical Physical\nEntity Table\nRelationship Foreign Key\nAttribute Column\nUnique Identifier Primary Key\n4.1.2 - Background\nThe first database system was created in the 1960s as computers became affordable for\nprivate organizations. The two most popular data models during this time were the network\nmodel CODASYL (Conference\/Committee on Data Systems Languages) and the hierarchical\nmodel called IMS (Information Management System). These two systems were used by private organizations, but the SABRE (Semi-Automated Business Research Environment) system saw\ncommercial success through International Business Machines (IBM). IBM used the SABRE\nsystem to help American Airlines manage reservation data for customers. These database\nsystems began to change in the 1970s when E.F. Codd published a scientific paper that explored\nthe idea of a relational database model. E.F. Codd\u2019s ideas about a relational database\nrevolutionized the way people view databases. Codd's relational database schema separated the\nphysical information storage, which became the new standard for database systems.\nTwo major relational database systems emerged in 1974 and 1977. One of the databases\nwas called Ingres, which was created at UBC; the second was called System R, developed at\nIBM. Ingres used a query language known as QUEL, and it led to the creation of systems such\nas Ingres Crop., MS SQL Server, Sybase, Wang\u2019s PACE, and Britton-Lee (Quick Base). On the\nother hand, System R used the SEQUEL query language, and it contributed to the development\nof SQL\/DS, DB2, All base, Oracle, and Non-Stop SQL (Quick Base). Relational Database\nSystem became the standard and a recognized term in industry.\nThe next evolution in database systems came in 1976 through the Entity-Relationship (E-\nR) model. Entity-Relationship database model was created by P. Chen. The E-R model allowed\ndevelopers to focus less on logic table structure and more on research data application. In 1980,\nStructured Query Language (SQL) became the standard query language among databases. Also\nduring this time, computer sales saw commercial success, which aided the database market. This\nincrease in sales led to the decline in legacy network and hierarchical database models. With the\ndevelopment of the Internet, the database industry saw substantial growth. Databases started to\nbe accessed from client-servers. Online business was becoming popular, and with the demand\ncame the need for internet database connectors increased. Security also played an important part\nin the development of databases.\nBefore 1980, Government organizations such as the Department of Defense were the first\nto invest heavily into security of data. This was due to the type of data they were storing, such as\nmilitary data and census data. Most organizations framed security policies around the few\nvulnerabilities they detected. During this time, physical threats were understood and preventive\nmeasurements were put in place. Logical threats or digit threats were difficult to understand and\nwere very weak during this time. Mainstream research was focused on statistical databases.\nAccess Control was the first security control to come out of this research. \u201cAccess Control for\ndatabases was to be expressed in terms of logical data model with authorizations in terms of\nrelations and tuples. It also had to be content-aware to allow the system to determine whether\naccess should be granted based on the content of the data item\u201d (Lesov, 2010). This type of\naccess control became known as the Bell-LaPadula model or BLP. Systems were now required\nto store shared resources which also increased the need for security across those resources. Two\nnew models were created: the Mandatory Access Control (MAC) and the Discretionary Access\nControls (DAC). However, both of these did not create great success and were later reinforced\nwith encryptions. Two designs for encryptions were the Access Control Kernels and Encrypted\nDatabases. Kernels isolate and contain security policies inside different modules. Cryptography\nwas used through keys to encrypt and store data to maximize security. Paul Lesov from\nUniversity of Minnesota writes, \u201cAccess control was not sufficient by itself to address the issue\nof DBA being able to exercise complete control over the data residing in the database.\nEncryption provides a way to encrypt data in the database and store an encryption key external to\nthe database thereby preventing the DBA from accessing the data\u201d (2010).\nThe digital environment went through a massive transformation that was driven by\ncommercialization of the digital space. Windows was developed and adopted during this time\nalong with the World Wide Web. The idea of using the web as a place to conduct commerce made the security required of the early nineties very different from the late nineties. Another\nmajor development during this time was the idea of Object-Oriented Programming. Object-\nOriented Programming allowed for more complex and efficient ways to deal with complex data.\nEarly security for databases connected through the web took the form of firewalls which control\naccess to internal servers. Firewalls provided protection against direct attacks on the database\nbut the front-end were left susceptible to SQL Injections.\nSQL Injections allowed users to insert scripts into the database which would retrieve\ninformation. The idea of data mining extended the threat to individual privacy. Government\nregulations such as the Health Insurance Portability and Accountability Act (HIPAA) and the\nFederal Financial Institutions Examination Council (FFIEC) were enacted during this time to\nprotect individual privacy. Paul Lesov stated, \u201cIt is important to note that many problems with\nsecuring data stored in a database is not due to the lack of research but lacking security in\nimplementation of the database product or an application front ending the database. The shift\nfrom full trust to partial trust was driven in part by natural tendency to not provide full trust to\nanyone single individual based on dual control principle but also due to the inability of the users\nto keep their own PC computer secure and database frontend not being able to detect malicious\nattacks such as SQL injections\u201d (2010). Since then, the growth of the databases outgrew the pace\nof security research. Due to the amount of data and complexity around data security, research\nwas slow. Today research is still being conducted on database security and is ever evolving year\nto year.\n4.2.1 - Physical DB Design Process\nThe design of a physical database is heavily influenced on integrity and performance for\nthe system (Hoffer et al., 2015). To maximize efficiency for user interactions with an\ninformation system, minimizing time constraints associated with user interactions must be taken\ninto account. According to Adrienne Watt, database design starts with a conceptual data model\nand produces a specification of a logical schema; this will determine the specific type of database\nsystem (network, relational, object-oriented) that is required. The relational representation is still\nindependent of any specific DBMS; it is another conceptual data model (Watt, n.d.). The\ndatabase design process is initialized from the logical data model that will be used in the\ndatabase design and can be represented as an E-R (Entity-Relationship) diagram (Figure 1).\nPhysical database designs require preliminary pieces of information that are collected\nprior to development. For a physical file design and database design, normalized relations,\nattribute definition, descriptions of data handling, the expectations for response times, data\nsecurity, backup, recovery, retention, and integrity of data, and descriptions of the technology are\nutilized to implement the database (Hoffer et al., 2015). Before commencing analysis and\nimplementation of the database design, designers must first consult with customers to acquire\nrelevant documents and data to be used in analysis and implementation to achieve the desired\nresults based on the customers\u2019 needs.\nPhysical database design is also concerned with the design of fields. A field is the\nsmallest unit of application data recognized by system software. Fields are associated with the\ncolumns found in a relationship table and rely on the data types, coding techniques, data\nintegrity, and handling of empty data entry. Efficient database performance relies on adequately\ndefined fields and subsequently relies on adequate specifications for each field. Figure 1. E-R Diagram, by Visual Paradigm, 2020, https:\/\/www.visual-paradigm.com\/guide\/data-\nmodeling\/what-is-entity-relationship-diagram\/. Copyright 2020 by Visual Paradigm\nThe format for data storage is an imperative decision to be made when considering\nstorage space and data-related integrity as shown in Figure 1. This format can be integrated with\nthe selection of data types that are only needed for the database design to be implemented. Data\ntypes can be categorized as a string, numerical, and datetime data values, and can be further\ndecomposed into varying lengths of data for their respective categories as shown in Table 1.\nTable 1.\nString Numeric Date\/Time\nCHAR SMALLINT DATE VARCHAR INTEGER TIME\nCLOB DOUBLE TIMESTAMP\nCoding and compression techniques are critical for handling data storage within fields.\nAllowing for smaller-sized codes reduces the space usage of data values within databases. An\nexample of such a coding technique could involve cross-referencing to other tables in order to\nretrieve a field value. Aside from using data types as a form of data integrity, other forms of data\nintegrity to be considered in a physical database design include default values, which are\nautomatically assigned unless specified otherwise by a user; range control, which a preset range\nlimit specifies the set of values a field can be assigned; and null control, which restricts null\nvalues.\n4.2.2 - Data Partitioning\nPartitioning is a concept in databases in which very large tables and data are partitioned\ninto smaller, individual tables, and queries which helps the data process more quickly and\nefficiently. One form of partitioning is called horizontal partitioning. Horizontal partitioning is\nthe classification of the rows based on common characteristics into several, separate tables. For\nexample, students with scores less than 70 might be stored in the FAIL table, while students who\nscored greater than 70 might be stored in the PASS table. The two partitioned tables would be\nFAIL and PASS. In doing so, the database is partitioned into two tables, both of which will be\nprocessed more quickly than a single table. The two typical methods of horizontal partitioning\nare to partition a sole column value and to partition the date (as the date helps organize the query\nchronologically). The keyword SELECT represents a horizontal partition in SQL. Below is an\nexample of what horizontal partitioning would look like in SQL in which the first name, last\nname, class number and grade are specifically chosen as desired values:\nSELECT viewClassGradeReport.[FirstName],\nviewClassGradeReport.[LastName],\nviewClassGradeReport.[ClassName],\nviewClassGradeReport.[Grade] FROM viewClassGradeReport;\nThe Oracle DBMS (Database Management System) provides support for multiple forms\nof partitioning. The first one is range partitioning. In range partitioning, each partitioned portion\nis characterized by a range of values for one or multiple columns, such as IDs or dates. For\ninstance, rows for a column titled COUNTRY containing India, Sri Lanka, Pakistan, Nepal, and\nBangladesh could be a partition for South Asian countries.\nThe next form of partitioning is hash partitioning. Hash partitioning is the spreading of\ndata in even partitions autonomous of the key value. Hash partitioning outperforms the uneven\ndistributions or rows.\nThe next partitioning is known as list partitioning. List partitioning is a technique where a\nlist of distinct values is defined as the partitioning key in the characterization for each partition. The best use of list partitioning is when one requires specifically to highlight rows established on\ndiscrete values. For instance, a global distributor may only want data regarding deliveries to\nChina and Japan while ignoring deliveries to other nations.\nOracle provides another form of partitioning known as composite partitioning. Composite\npartitioning is a combination of the general data allocation methods in which a table is\npartitioned using one allocation method, and then each partition is subdivided into smaller\npartitions through another general allocation method.\nThe counterpart of horizontal partitioning, vertical partitioning is another form of\npartitioning. Vertical partitioning, as opposed to horizontal partitioning, is the distribution of\ncolumns based on their commonalities and separating them into independent tables. Vertical\npartitioning helps identify the dynamic data from the stable, immutable data. Vertical\npartitioning is represented with the keyword PROJECT.\nPartitioning is a useful tool that helps with the design of the database and has many\nadvantages. Partitioning is practical and helps manage the table because partitioning helps\nidentify the area where maintenance is needed and saves storage space. Partitioning is also\nsecure, as only the relevant and necessary data can be specifically chosen and accessed by the\nuser. Another benefit of implementing partitioning is that backing up and securing files is easier\ndue to their smaller size, and if one file is corrupted, the other is still accessible. Partitioning\nworks similarly with cars and replaceable parts: if one piece of equipment is damaged, the rest of\nthe car will still be functioning. Partitioning also helps with balancing the load. The partitioned\nfiles can be designated to different storage locations, reducing conflict and maximizing\nperformance.\nAlthough partitioning proves to be quite handy, it does have its drawbacks. The first\ndrawback of partitioning is the inconsistency of the access speed that it provides. All partitions\nare not identical; therefore, depending on the data the specific partition consists, access speeds\nwill differ. Another drawback is complexity. Due to the complex nature of partitions, the code\nrequired to program will need to be more complex and challenging to maintain. The final\ndisadvantage of partitioning is the excess storage space and time. Data can be replicated multiple\ntimes, which in turn takes up storage and can affect the time taken to process. Figure 2. Sharding, by Digital Ocean, 2019,\nhttps:\/\/www.digitalocean.com\/community\/tutorials\/understanding-database-sharding. Copyright 2020 by\nDigital Ocean\nThe user view, in a database, is a tool that helps visualize the tables in a database. Using\nthe user view, physical tables can be partitioned coherently. The main intention of utilizing the\nuser view is that it simplifies query editing and develops a secure database. In Oracle, a form of\nuser view is offered called partition view, which displays physically partitioned tables that can be\nlogically merged into one using the SQL union operator. This manner of partitioning does have\nits limitations. Firstly, there should not be any global index. Second, the physical tables should\nbe independently handled. Lastly, fewer choices are at your disposal with partition view.\n4.3.1 Describe three types of file organization\nA physical file contains the actual data that is stored on the system and a description of\nhow the data is to be presented to or received from a program. Physical files can be separated\ninto extents. Extents are a spreadable section of disk storage space. Many database management\nsystems store many different kinds of data in one operating system file. According to\n(Venkataraman, R., Topi, H. 2011) a \u201ctablespace is a named logical storage unit in which data\nfrom one or more database tables, views, or other database objects may be stored.\u201d A tablespace\nconsists of one or several physical operating system files. Thus, \u201cOracle has responsibility for\nmanaging the storage of data inside a tablespace, whereas the operating system has many\nresponsibilities for managing a tablespace, but they are all related to its responsibilities related to\nthe management of operating system files\u201d (Venkataraman, R., Topi, H. 2011). Because an instance of Oracle usually supports many databases for many users, a database\nadministrator usually will create many user tablespaces, which helps to achieve database\nsecurity, since the administrator can give each user selected rights to access each tablespace. As\nstated by Venkataraman, R. and Topi, H. (2011) \u201ceach tablespace consists of logical units called\nsegments consisting of one table, index, or partition, which, in turn, are divided into extents ....\nthese consist of a number of contiguous data blocks, which are the smallest unit of storage\u201d.\nEach table, index, or other so-called schema object belongs to a single tablespace, but a\ntablespace may contain one or more tables, indexes, and other schema objects. Physically, each\ntablespace can be stored in one or multiple data files, but each data file is associated with only\none tablespace and only one database.\nModern database management systems have an active role in managing the use of the\nphysical devices and files on them; for example, the allocation of schema objects (e.g., tables and\nindexes) to data files is typically fully controlled by the DBMS. A database administrator does\nhave the ability to manage the disk space allocated to tablespaces and a number of parameters\nrelated to the way free space is managed within a database. Because this is not a text on Oracle,\nwe do not cover specific details on managing tablespaces; however, the general principles of\nphysical database design apply to the design and management of Oracle tablespaces, as they do\nto whatever the physical storage unit is for any database management system\nFile Organization\nA \u201cfile organization is a technique for arranging the records of a file on secondary\nstorage devices\u201d (Venkataraman, R., Topi, H. 2011). With modern relational DBMS it is not\nnecessary to design file organizations, but you are to be allowed to select an organization and its\nparameters for a table or physical file. In choosing a file organization for a particular file in a\ndatabase, consider seven important factors: fast data retrieval, high throughput for processing\ndata input and maintenance transactions, efficient use of storage space, protection from failures\nor data loss, minimizing need for reorganization, accommodating growth, and security from\nunauthorized use.\nSEQUENTIAL FILE ORGANIZATIONS\nIn a sequential file organization, the records in the file are stored in sequence according to\na primary key value. To locate a particular record, a program must normally scan the file from\nthe beginning until the desired record is located. A common example of a sequential file is the\nalphabetical list of persons in the white pages of a telephone directory. A comparison of the\ncapabilities of sequential files with the other two types of files can be seen in Figure 1.3.\n\u201cBecause of their inflexibility, sequential files are not used in a database but may be used for\nfiles that back up data from a database\u201d (Venkataraman, R., Topi, H. 2011).\nINDEXED FILE ORGANIZATIONS\nIn an index file organization, records are stored either sequentially or nonsequentially,\nand an index is created that allows the application software to locate individual records. \u201cA card\ncatalog in a library, an index is a table that is used to determine in a file the location of records\nthat satisfy some condition\u201d (Venkataraman, R., Topi, H. 2011). Each index entry matches a key\nvalue with one or more records. An index can point to unique records or to potentially more than\none record, and an index that allows each entry to point to more than one record is called a\nsecondary key index. Secondary key indexes are important for supporting many reporting\nrequirements and for providing rapid ad hoc data retrieval. An example would be an index on the\nProductFinish column of a Product table. Because indexes are extensively used with relational DBMSs, the choice of what index and how to store the index entries matters greatly in database\nprocessing performance.\nFig 1.1 Modern Database Management (Venkataraman, R., Topi, H. (2011)) Fig 1.3 Modern Database Management 10th edition.((Venkataraman, R., Topi, H. 2011))\nWhen the address of a record within a file is to be determined or computed, the technique\nof hash file organization may be considered and implemented as an algorithm or function. A\nhash algorithm is an algorithm that takes an input of random size and proceeds to transform the\ninput such that the hash result is an output of fixed length. Once the output is determined or\ncomputed, the hash result is irreversible, meaning that the algorithm can only process data in one\ndirection. The use of hashing algorithms is commonly found in databases for practically any\nwebsite that requires a password to login to an account and is illustrated in Figure 1.4.\nFigure 1.4. Hashing Algorithm, by jscrambler, 2020, https:\/\/blog.jscrambler.com\/hashing-algorithms\/.\nCopyright 2020 by jscrambler\nIn other hashing algorithms, the primary key value of a record is typically divided by a\nprime number that is suitable for use and the remainder of the divided value is used as a relative\nstorage location. Due to limitations in which only one key is used for storage retrieval through\nhashing, hashing and indexing are used in combination to address this issue. According to Jeffrey\nA. Hoffer, Venkataraman Ramesh, and Heikki Topi, a hash index table uses hashing to map a\nkey into a location in an index (sometimes called a scatter index table), where there is a pointer\n(a field of data indicating a target address that can be used to locate a related field or record of\ndata) to the actual data record matching the hash key (Hoffer et al., 2015).\nIn an effort to preserve memory space, database management systems may allow for\nseveral rows of related tables to be joined together and store the same amount of units of storage\n(data block). An example of this is seen when a common primary key between related tables\nsuch as a CustomerID or ItemID are utilized to join rows of separate tables together which are\nrelated by these two primary keys. According to Jeffrey A. Hoffer, Venkataraman Ramesh, and\nHeikki Topi, time is reduced because related records will be closer to each other than if the\nrecords are stored in separate files in separate areas of the disk. Defining a table to be in only one\ncluster reduces retrieval time for only those tables stored in the same cluster. This technique of\nfile organization is known as clustering files and is illustrated in Figure 1.5. Figure 1.5. A clustering file for Ordering, by Hoffer et al., 2015, Modern Database Management 12e (E-\nReader Version). Copyright 2015 by Pearson Education, Inc.\nWhen considering security to protect files from damage, types of control are useful\nelements of database files to use for unforeseen file corruptions. Database files are stored in a\nproprietary format by the database which allows for access controls over the files. Some useful\nprocedures to consider are backups to ensure that stored data may be retrieved in the event that\ndata may be compromised. Another technique employs the utilization of encryption to encrypt\ndata contained within files and allow for only programs with access to decrypt them. There are\ntwo main methods of encryption: symmetric encryption and asymmetric encryption. Symmetric\nencryption makes use of a single key for all parties communicating and is used for both\nencrypting data and decrypting data. Asymmetric encryption makes use of two keys for all\nparties communicating where the first key is used for encryption and the second key is used for\ndecryption.\n4.4.1 - Translate a database model into efficient structures\nA majority of database manipulations demand the location of a row or a collection of rows that\nsatisfies a condition. Given the magnitude of a database, searching for data can be quite the\nlaborious task. Hence, using indexes can vastly increase the speed of the process and reduce the\ntime and work. The usage and definition of indexes are a crucial spoke on the wheel of physical\ndatabase design. Indexes are defined as either a primary key, secondary key, or both. It is\nordinary to define an index for the primary key of a table. The index is formed of two columns:\none column for the key and the other column for the address of the record that consists of the key\nvalue. In the case of a primary key, the index will only have one entry for each key value. Indexing in Databases, by GeeksforGeeks, 2020, https:\/\/www.geeksforgeeks.org\/indexing-in-databases-set-\n1\/ Copyright 2020 by GeeksforGeeks.org\nCREATING A UNIQUE KEY INDEX\nThe syntax to create a unique key index in SQL is \"CREATE [UNIQUE] INDEX index_name\nON table_name(column1, ... column_n);\". The UNIQUE modifier specifies the values in the\nindexed columns. Creating a non-unique key index is equivalent to a secondary key index. The\nterm UNIQUE isn't used to create a secondary key index, because values can be repeated.\nWHEN TO USE INDEXES\nIt is important to know when to use an index and which attributes to use when creating an index.\nUsing indexes come at the price of performance. Performance is compromised when using\nindexes due to the overload for maintenance for insertions, deletions, and updating records. For\nthis reason, indexes should be utilized mainly for data retrieval (Hoffer, Venkataraman, & Topi,\n2011). Here are some rules or conditions that suggest the use of indexes.\n1) Indexes are a lot more efficient and practical for substantial tables.\n2) Indexes are useful when there is a need to set out a unique index for the primary key.\n3) Indexes are frequently used for columns that appear in WHERE modifiers of SQL\ncommands.\n4) Indexes should be used when for attributes referenced in ORDER BY and GROUP BY\nstatements.\n5) Indexes are convenient when there is diversity in the values of an attribute. For Oracle's\nstandards, it is unproductive to use an index when an attribute has fewer than 30 values.\n6) One point to keep in mind is to consider developing a compressed version of the values.\nDoing this will ensure that the index isn't slower to process.\n7) If the index is used for finding the location of where the record will be stored, make sure\nthe key of this index is a surrogate key to ensure the records will be fairly spread across\nthe storage space.\n8) Make sure to check the limit of indexes on the DBMS because some systems do not\nallow for more than 16 indexes.\n9) Find a way to index attributes that have null values because rows with a null value won't\nbe referenced.\n4.4.1 Designing a Database for Optimal Query Performance Databases are used every day, which makes it important to optimize the performance of\ndatabase processing. Database processing can include adding, deleting, and modifying a\ndatabase along with methods of retrieving data. Since databases have more traffic retrieving data\nfrom the database than maintenance, it is important to optimize query performance. Producing\nreports and ad hoc screens for users is the primary goal. The amount of work required to\noptimize query performance heavily relies on DBMS. Some DBMS give little control to the\ndeveloper on how the database is designed and where the query can be processed. This can limit\nhow optimized data reads and writes are in the database. However, other systems give complete\ncontrol to the developer and require a lot of setup work. Since workloads often vary, it is\ndifficult to reach peak performance from a database unless the workload is focused. \u201cFor\nexample, the Teradata DBMS is highly turned for parallel processing in a data warehousing\nenvironment. In this case, rarely can a database designer or query writer improve on the\ncapabilities of the DBMS to store and process data\u201d (Hoffer, Venkataraman, & Topi, 2011).\nThis situation is rare; thus, it is important as a database designer to consider ways to improve the\nprocessing capabilities of the database.\nSince computer architecture has changed greatly over the years, the use of multiple\nprocessors in database servers has become standard. Symmetric multiprocessor (SMP)\narchitecture is commonly used in database servers to allow parallel processing. DBMS that use\nparallel query processing break up a query such that it can be processed in parallel by different\nprocessors. These partitions must be defined before the query by the database designer. An\nexample of instructing Oracle to execute a query using parallel processing is below:\nALTER TABLE ORDER_T PARALLEL 3;\n(Hoffer, Venkataraman, & Topi, 2011)\nEach table needs to be finely tuned to best support parallelism and can undergo multiple\nchanges to find the best performance. Schumacher reported, \u201con a test in which the time to\nperform a query was cut in half with parallel processing compared to using a normal table scan.\nBecause an index is a table, indexes can also be given the parallel structure, so that scans of an\nindex are also faster.\u201d Schumacher also reported an example of parallel processing reducing the\ntime of creating an index from seven minutes to five seconds (Hoffer, Venkataraman, & Topi,\n2011). Parallel processing not only improves the time of table scans but also can be used on\njoining tables, grouping query results, sorting, deleting, updating, and insertion. Oracle requires\nthe number or virtual parallel database servers to be defined beforehand. Once the servers are\ndefined, the processor will decide what is the best use of parallel processing for that specific\ncommand. (Burleson Consulting, 2014)\nOften the designer creating the query has information to better optimize the query that the\nquery module in the DBMS does not. In most relational DBMs, the optimizer\u2019s plan for\nprocessing the query can be known by the designer before actually running the query. This is\ndone through the command EXPLAIN or EXPLAIN PLAIN, which display all the information\nabout the optimizer\u2019s plans to process the query. The query optimizer makes its decision on how\nto process the query by looking at data from each table such as average row length or the number\nor rows. You can submit multiple EXPLAIN commands with a query written in different ways\nto see if the optimizer predicts different performance. That then allows you to find the best\nperformance and submit that for actual processing. With some DBMs you can force the\noptimizer to take different steps or use resources other than what the optimizer thinks is the best\nperformance. The clause \/**\/ can be used to override what the query determines is the best way\nto process the query.\n4.5 - Concise Summary\nPhysical database design translates the logical data model into a set of SQL statements\nthat define the database. For relational database systems, it is relatively easy to translate from a\nlogical data model into a physical database (\u201cPhysical Database Design,\u201d n.d.). The design\nprocess is a collaborative one where preliminary information is collected to determine technical\nproperties of the database, such as response times and data security. In the database design\nprocess, fields are heavily emphasized to determine the corresponding attributes and logical data\nmodel. Often, a database design is concerned with data partioning of large tables, which are\npartitioned into smaller, individual tables, allowing for more efficient processing of data and\ndatabase performance overall.\nThe database design process is also concerned with file organization of physical files.\nFile organization is responsible for managing records of a file on a secondary storage device.\nPhysical file organization is typically divided into three types of file organization: Sequential,\nIndexed, and Hashed file organization. In sequential file organization, records on an arbitrary file\nare stored in sequence onto a secondary storage device based on primary key values. In indexed\nfile organization, records may be stored sequentially or non-sequentially and an index is created "}