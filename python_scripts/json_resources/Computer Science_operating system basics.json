{"text":"1.1: Introduction to Operating Systems\nIntroduction to Operating System\nAn operating system acts as an intermediary between the user of a computer and computer hardware. The purpose of an operating\nsystem is to provide an environment in which a user can execute programs in a convenient and efficient manner.\nAn operating system is a software that manages the computer hardware. The hardware must provide appropriate mechanisms to\nensure the correct operation of the computer system and to prevent user programs from interfering with the proper operation of the\nsystem.\nOperating System \u2013 Definition:\nAn operating system is a program that controls the execution of application programs and acts as an interface between the user\nof a computer and the computer hardware.\nA more common definition is that the operating system is the one program running at all times on the computer (usually called\nthe kernel), with all else being application programs.\nAn operating system is concerned with the allocation of resources and services, such as memory, processors, devices, and\ninformation. The operating system correspondingly includes programs to manage these resources, such as a traffic controller, a\nscheduler, memory management module, I\/O programs, and a file system.\nFunctions of Operating system \u2013 Operating system performs three functions:\n1. Convenience: An OS makes a computer more convenient to use.\n2. Efficiency: An OS allows the computer system resources to be used in an efficient manner.\n3. Ability to Evolve: An OS should be constructed in such a way as to permit the effective development, testing and introduction\nof new system functions at the same time without interfering with service.\nOperating system as User Interface \u2013\n1. User\n2. System and application programs\n3. Operating system\n4. Hardware\nEvery general-purpose computer consists of the hardware, operating system, system programs, and application programs. The\nhardware consists of memory, CPU, ALU, and I\/O devices, peripheral device, and storage device. System program consists of\ncompilers, loaders, editors, OS, etc. The application program consists of business programs, database programs.\nFigure 1.1.1: Conceptual view of a computer system (\"Conceptual view of a computer system\" by Unknown, Geeks for Geeks is\nlicensed under CC BY-SA 4.0)\n1.1.1 https:\/\/eng.libretexts.org\/@go\/page\/45610 Every computer must have an operating system to run other programs. The operating system coordinates the use of the hardware\namong the various system programs and application programs for various users. It simply provides an environment within which\nother programs can do useful work.\nThe operating system is a set of special programs that run on a computer system that allows it to work properly. It performs basic\ntasks such as recognizing input from the keyboard, keeping track of files and directories on the disk, sending output to the display\nscreen and controlling peripheral devices.\nOS is designed to serve two basic purposes:\n1. It controls the allocation and use of the computing System\u2019s resources among the various user and tasks.\n2. It provides an interface between the computer hardware and the programmer that simplifies and makes feasible for coding,\ncreation, debugging of application programs.\nThe Operating system must support the following tasks. The task are:\n1. Provides the facilities to create, modification of programs and data files using an editor.\n2. Access to the compiler for translating the user program from high level language to machine language.\n3. Provide a loader program to move the compiled program code to the computer\u2019s memory for execution.\n4. Provide routines that handle the details of I\/O programming.\nI\/O System Management \u2013\nThe module that keeps track of the status of devices is called the I\/O traffic controller. Each I\/O device has a device handler that\nresides in a separate process associated with that device.\nThe I\/O subsystem consists of\nA memory Management component that includes buffering caching and spooling.\nA general device driver interface.\nDrivers for specific hardware devices.\nAssembler \u2013 The input to an assembler is an assembly language program. The output is an object program plus information that\nenables the loader to prepare the object program for execution. At one time, the computer programmer had at their disposal a basic\nmachine that interpreted, through hardware, certain fundamental instructions. The programmer would program the computer by\nwriting a series of ones and zeros (machine language), and place them into the memory of the machine.\nCompiler\/Interpreter \u2013 The high-level languages - for example C\/C++, are processed by compilers and interpreters. A compiler\nis a program that accepts source code written in a \u201chigh-level language \u201cand produces a corresponding object program. An\ninterpreter is a program that directly executes a source program as if it was machine language.\nLoader \u2013 A loader is a routine that loads an object program and prepares it for execution. There are various loading schemes:\nabsolute, relocating and direct-linking. In general, the loader must load, relocate and link the object program. The loader is a\nprogram that places programs into memory and prepares them for execution. In a simple loading scheme, the assembler outputs the\nmachine language translation of a program on a secondary device and a loader places it in the core. The loader places into memory\nthe machine language version of the user\u2019s program and transfers control to it. Since the loader program is much smaller than the\nassembler, those make more core available to the user\u2019s program.\nHistory of Operating system \u2013\nOperating system has been evolving through the years. Following Table shows the history of OS.\nGeneration Year Electronic device used Types of OS Device\nFirst 1945-55 Vacuum Tubes Plug Boards\nSecond 1955-65 Transistors Batch Systems\nThird 1965-80 Integrated Circuits(IC) Multiprogramming\nFourth Since 1980 Large Scale Integration PC\nAdapted from:\n\"Introduction of Operating System \u2013 Set 1\" by Unknown, Geeks for Geeks is licensed under CC BY-SA 4.0\n1.1.2 https:\/\/eng.libretexts.org\/@go\/page\/45610 This page titled 1.1: Introduction to Operating Systems is shared under a CC BY-SA license and was authored, remixed, and\/or curated by Patrick\nMcClanahan.\n1.1.3 https:\/\/eng.libretexts.org\/@go\/page\/45610 1.2 Starting with the Basics\nProcessor\nThe processor is an important part of a computer architecture, without it nothing would happen. It is a programmable device that\ntakes input, perform some arithmetic and logical operations and produce some output. In simple words, a processor is a digital\ndevice on a chip which can fetch instruction from memory, decode and execute them and provide results.\nBasics of a Processor \u2013\nA processor takes a bunch of instructions in machine language and executes them, telling the processor what it has to do.\nProcessors performs three basic operations while executing the instruction:\n1. It performs some basic operations like addition, subtraction, multiplication, division and some logical operations using its\nArithmetic and Logical Unit (ALU).\n2. Data in the processor can move from one location to another.\n3. It has a Program Counter (PC) register that stores the address of next instruction based on the value of PC.\nA typical processor structure looks like this.\nFigure 1: Von Neumann Architecture. (\"File:Computer Systems - Von Neumann Architecture Large poster anchor\nchart.svg\" by BotMultichillT, Wikimedia Commons is licensed under CC BY-NC-SA 4.0)\nBasic Processor Terminology\nControl Unit (CU)\nA control unit (CU) handles all processor control signals. It directs all input and output flow, fetches the code for instructions\nand controlling how data moves around the system.\nArithmetic and Logic Unit (ALU)\nThe arithmetic logic unit is that part of the CPU that handles all the calculations the CPU may need, e.g. Addition, Subtraction,\nComparisons. It performs Logical Operations, Bit Shifting Operations, and Arithmetic Operation.\nMain Memory Unit (Registers)\n1. Accumulator (ACC): Stores the results of calculations made by ALU.\n2. Program Counter (PC): Keeps track of the memory location of the next instructions to be dealt with. The PC then passes\nthis next address to Memory Address Register (MAR).\n3. Memory Address Register (MAR): It stores the memory locations of instructions that need to be fetched from memory or\nstored into memory.\n1 https:\/\/eng.libretexts.org\/@go\/page\/45611 4. Memory Data Register (MDR): It stores instructions fetched from memory or any data that is to be transferred to, and\nstored in, memory.\n5. Current Instruction Register (CIR): It stores the most recently fetched instructions while it is waiting to be coded and\nexecuted.\n6. Instruction Buffer Register (IBR): The instruction that is not to be executed immediately is placed in the instruction buffer\nregister IBR.\nInput\/Output Devices \u2013 Program or data is read into main memory from the input device or secondary storage under the\ncontrol of CPU input instruction. Output devices are used to output the information from a computer.\nBuses \u2013 Data is transmitted from one part of a computer to another, connecting all major internal components to the CPU and\nmemory, by the means of Buses. Types:\n1. Data Bus (Data): It carries data among the memory unit, the I\/O devices, and the processor.\n2. Address Bus (Address): It carries the address of data (not the actual data) between memory and processor.\n3. Control Bus (Control and Timing): It carries control commands from the CPU (and status signals from other devices) in\norder to control and coordinate all the activities within the computer.\nMemory\nMemory attached to the CPU is used for storage of data and instructions and is called internal memory The internal memory is\ndivided into many storage locations, each of which can store data or instructions. Each memory location is of the same size and has\nan address. With the help of the address, the computer can read any memory location easily without having to search the entire\nmemory. when a program is executed, it\u2019s data is copied to the internal memory and is stored in the memory till the end of the\nexecution. The internal memory is also called the Primary memory or Main memory. This memory is also called as RAM, i.e.\nRandom Access Memory. The time of access of data is independent of its location in memory, therefore this memory is also called\nRandom Access memory (RAM).\nI\/O Modules\nThe method that is used to transfer information between main memoryand external I\/O devices is known as the I\/O interface, or I\/O\nmodules. The CPU is interfaced using special communication links by the peripherals connected to any computer system. These\ncommunication links are used to resolve the differences between CPU and peripheral. There exists special hardware components\nbetween CPU and peripherals to supervise and synchronize all the input and output transfers that are called interface units.\nMode of Transfer:\nThe binary information that is received from an external device is usually stored in the memory unit. The information that is\ntransferred from the CPU to the external device is originated from the memory unit. CPU merely processes the information but the\nsource and target is always the memory unit. Data transfer between CPU and the I\/O devices may be done in different modes.\nData transfer to and from the peripherals may be done in any of the three possible ways\n1. Programmed I\/O: is the result of the I\/O instructions written in the program's code. Each data transfer is initiated by an\ninstruction in the program. Usually the transfer is from a CPU register and\/or memory. In this case it requires constant\nmonitoring by the CPU of the peripheral devices.\n2. Interrupt- initiated I\/O: using an interrupt facility and special commands to issue an interrupt request signal whenever data is\navailable from any device. In the meantime the CPU can proceed processing other programs. The interface meanwhile keeps\nmonitoring the device. When it is determined that the device is ready for a data transfer it initiates an interrupt request signal to\nthe CPU. Upon detection of an external interrupt signal the CPU momentarily stops the task it was processing, and services\nprogram that was waiting on the interrupt to process the I\/O transfer> Once the interrupt is satisfied, the CPU then return to the\ntask it was originally processing.\n3. Direct memory access( DMA): The data transfer between a fast storage media such as magnetic disk and main memory is\nlimited by the speed of the CPU. Thus we can allow the peripherals directly communicate with each other using the memory\nbuses, removing the intervention of the CPU. This type of data transfer technique is known as direct memory access, or DMA.\nDuring DMA the CPU is idle and it has no control over the memory buses. The DMA controller takes over the buses to manage\nthe transfer directly between the I\/O devices and the memory unit.\n2 https:\/\/eng.libretexts.org\/@go\/page\/45611 Adapted from:\n\"Introduction of Microprocessor\" by DikshaTewari, Geeks for Geeks is licensed under CC BY-SA 4.0\n\"Last Minute Notes Computer Organization\" by Geeks for Geeks is licensed under CC BY-SA 4.0\n\"Functional Components of a Computer\" by aishwaryaagarwal2, Geeks for Geeks is licensed under CC BY-SA 4.0\n\"System Bus Design\" by deepak, Geeks for Geeks is licensed under CC BY-SA 4.0\n\"I\/O Interface (Interrupt and DMA Mode)\" by Unknown, Geeks for Geeks is licensed under CC BY-SA 4.0\nThis page titled 1.2 Starting with the Basics is shared under a CC BY-SA license and was authored, remixed, and\/or curated by Patrick\nMcClanahan.\n3 https:\/\/eng.libretexts.org\/@go\/page\/45611 1.3 The Processor - History\nMicroprocessor Evolution\nGenerations of microprocessor:\n1. First generation: From 1971 to 1972 the era of the first generation came which brought microprocessors like INTEL 4004\nRockwell international PPS-4 INTEL 8008 etc.\n2. Second generation: The second generation marked the development of 8 bit microprocessors from 1973 to 1978. Processors\nlike INTEL 8085 Motorola 6800 and 6801 etc came into existence.\n3. Third generation: The third generation brought forward the 16 bit processors like INTEL 8086\/80186\/80286 Motorola 68000\n68010 etc. From 1979 to 1980 this generation used the HMOS technology.\n4. Fourth generation: The fourth generation came into existence from 1981 to 1995. The 32 bit processors using HMOS\nfabrication came into existence. INTEL 80386 and Motorola 68020 are some of the popular processors of this generation.\n5. Fifth generation: From 1995 till now we are in the fifth generation. 64 bit processors like Pentium, Celeron, dual, quad and\nocta core processors came into existence.\nTypes of microprocessors:\nComplex instruction set microprocessor: The processors are designed to minimize the number of instructions per program\nand ignore the number of cycles per instructions. The compiler is used to translate a high level language to assembly level\nlanguage because the length of code is relatively short and an extra RAM is used to store the instructions. These processors can\ndo tasks like downloading, uploading and recalling data from memory. Apart from these tasks these microprocessor can\nperform complex mathematical calculation in a single command.\nExample: IBM 370\/168, VAX 11\/780\nReduced instruction set microprocessor: These processor are made according to function. They are designed to reduce the\nexecution time by using the simplified instruction set. They can carry out small things in specific commands. These processors\ncomplete commands at faster rate. They require only one clock cycle to implement a result at uniform execution time. There are\nnumber of registers and less number of transistors. To access the memory location LOAD and STORE instructions are used.\nExample: Power PC 601, 604, 615, 620\nSuper scalar microprocessor: These processors can perform many tasks at a time. They can be used for ALUs and multiplier\nlike array. They have multiple operation unit and perform fast by executing multiple commands.\nApplication specific integrated circuit: These processors are application specific like for personal digital assistant computers.\nThey are designed according to proper specification.\nDigital signal multiprocessor: These processors are used to convert signals like analog to digital or digital to analog. The chips\nof these processors are used in many devices such as RADAR SONAR home theaters etc.\nAdvantages of microprocessor\n1. High processing speed\n2. Compact size\n3. Easy maintenance\n4. Can perform complex mathematics\n5. Flexible\n6. Can be improved according to requirement\nDisadvantages of microprocessors\n1. Overheating occurs due to overuse\n2. Performance depends on size of data\n3. Large board size than microcontrollers\n4. Most microprocessors do not support floating point operations\nAdapted from:\n\"Evolution of Microprocessors\" by Ayusharma0698, Geeks for Geeks is licensed under CC BY-SA 4.0\nThis page titled 1.3 The Processor - History is shared under a CC BY-SA license and was authored, remixed, and\/or curated by Patrick\nMcClanahan.\n1 https:\/\/eng.libretexts.org\/@go\/page\/45612 1.3.1: The Processor - Components\nAccumulator\nIn a computer's central processing unit (CPU), the accumulator (ACC in the image below) is a register in which intermediate\narithmetic and logic results are stored.\nWithout a register like an accumulator, it would be necessary to write the result of each calculation (addition, multiplication, shift,\netc.) to main memory, perhaps only to be read right back again for use in the next operation.\nAccess to main memory is slower than access to a register like an accumulator because the technology used for the large main\nmemory is slower (but cheaper) than that used for a register. Early electronic computer systems were often split into two groups,\nthose with accumulators and those without.\nModern computer systems often have multiple general-purpose registers that can operate as accumulators, and the term is no longer\nas common as it once was. However, to simplify their design, a number of special-purpose processors still use a single accumulator.\nArithmetic logic unit\nThe arithmetic logic unit (ALU) performs the arithmetic and logical functions that are the work of the computer. There are other\ngeneral purpose registers that hold the input data, and the accumulator receives the result of the operation. The instruction\nregister contains the instruction that the ALU is to perform.\nFor example, when adding two numbers, one number is placed in one of the general purpose registers register and the other in\nanother general purpose register. The ALU performs the addition and puts the result in the accumulator. If the operation is a logical\none, the data to be compared is placed into the one of the general purpose registers. The result of the comparison, a 1 or 0, is put in\nthe accumulator. Whether this is a logical or arithmetic operation, the accumulator content is then placed into the cache location\nreserved by the program for the result.\nInstruction register and pointer\nThe instruction pointer (CIR in the image below) specifies the location in memory containing the next instruction to be executed by\nthe CPU. When the CPU completes the execution of the current instruction, the next instruction is loaded into the instruction\nregister from the memory location pointed to by the instruction pointer.\nAfter the instruction is loaded into the instruction register, the instruction register pointer is incremented by one instruction address.\nIncrementing allows it to be ready to move the next instruction into the instruction register.\n1.3.1.1 https:\/\/eng.libretexts.org\/@go\/page\/46352 Memory Address Register\nIn a computer, the memory address register (MAR) is the CPU register that either stores the memory address from which data will\nbe fetched to the CPU, or the address to which data will be sent and stored.\nIn other words, This register is used to access data and instructions from memory during the execution phase of instruction. MAR\nholds the memory location of data that needs to be accessed. When reading from memory, data addressed by MAR is fed into the\nMDR (memory data register) and then used by the CPU. When writing to memory, the CPU writes data from MDR to the memory\nlocation whose address is stored in MAR. MAR, which is found inside the CPU, goes either to the RAM (random access memory)\nor cache.\nMemory Data Register\nThe memory data register (MDR) is the register that stores the data being transferred to and from the immediate access storage. It\ncontains the copy of designated memory locations specified by the memory address register. It acts as a buffer allowing the\nprocessor and memory units to act independently without being affected by minor differences in operation. A data item will be\ncopied to the MDR ready for use at the next clock cycle, when it can be either used by the processor for reading or writing or stored\nin main memory after being written.\nThis register holds the contents of the memory which are to be transferred from memory to other components or vice versa. A word\nto be stored must be transferred to the MDR, from where it goes to the specific memory location, and the arithmetic data to be\nprocessed in the ALU first goes to MDR and then to accumulated register, and then it is processed in the ALU.\nThe MDR is a two-way register. When data is fetched from memory and placed into the MDR, it is written to go in one direction.\nWhen there is a write instruction, the data to be written is placed into the MDR from another CPU register, which then puts the data\ninto memory.\nCache\nThe CPU never directly accesses RAM. Modern CPUs have one or more layers of cache. The CPU's ability to perform calculations\nis much faster than the RAM's ability to feed data to the CPU.\nCache memory is faster than the system RAM, and it is closer to the CPU because it is physically located on the processor chip.\nThe cache provides data storage and instructions to prevent the CPU from waiting for data to be retrieved from RAM. When the\nCPU needs data - program instructions are also considered to be data - the cache determines whether the data is already in\nresidence and provides it to the CPU.\nIf the requested data is not in the cache, it's retrieved from RAM and uses predictive algorithms to move more data from RAM into\nthe cache. The cache controller analyzes the requested data and tries to predict what additional data will be needed from RAM. It\nloads the anticipated data into the cache. By keeping some data closer to the CPU in a cache that is faster than RAM, the CPU can\nremain busy and not waste cycles waiting for data.\nThe simple example CPU has two levels of cache. Levels 2 is designed to predict what data and program instructions will be\nneeded next, move that data from RAM, and move it ever closer to the CPU to be ready when needed. These cache sizes typically\nrange from 1 MB to 32 MB, depending upon the speed and intended use of the processor.\nCPU clock and control unit (CU in the image)\nAll of the CPU components must be synchronized to work together smoothly. The control unit performs this function at a rate\ndetermined by the clock speed and is responsible for directing the operations of the other units by using timing signals that extend\nthroughout the CPU.\nRandom access memory (RAM)\nAlthough the RAM, or main storage, is shown in this diagram and the next, it is not truly a part of the CPU. Its function is to store\nprograms and data so that they are ready for use when the CPU needs them.\nAdapted from:\n\"Accumulator (computing)\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n\"Memory buffer register\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n\"Memory address register\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\nWilliam Lau, CC BY-SA 4.0, via Wikimedia Commons\n1.3.1.2 https:\/\/eng.libretexts.org\/@go\/page\/46352 1.3.1: The Processor - Components is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n1.3.1.3 https:\/\/eng.libretexts.org\/@go\/page\/46352 1.3.2: The Processor - Bus\nControl Bus\nIn computer architecture, a control bus is part of the system bus, used by CPUs for communicating with other devices within the\ncomputer. While the address bus carries the information about the device with which the CPU is communicating and the data bus\ncarries the actual data being processed, the control bus carries commands from the CPU and returns status signals from the devices.\nFor example, if the data is being read or written to the device the appropriate line (read or write) will be active (logic one).\nAddress bus\nAn address bus is a bus that is used to specify a physical address. When a processor or DMA-enabled device needs to read or write\nto a memory location, it specifies that memory location on the address bus (the value to be read or written is sent on the data bus).\nThe width of the address bus determines the amount of memory a system can address. For example, a system with a 32-bit address\nbus can address 232 (4,294,967,296) memory locations. If each memory location holds one byte, the addressable memory space is\n4 GiB.\nData \/ Memory Bus\nThe memory bus is the computer bus which connects the main memory to the memory controller in computer systems. Originally,\ngeneral-purpose buses like VMEbus and the S-100 bus were used, but to reduce latency, modern memory buses are designed to\nconnect directly to DRAM chips, and thus are designed by chip standards bodies such as JEDEC. Examples are the various\ngenerations of SDRAM, and serial point-to-point buses like SLDRAM and RDRAM. An exception is the Fully Buffered DIMM\nwhich, despite being carefully designed to minimize the effect, has been criticized for its higher latency.\nAdapted from:\n\"Bus (computing)\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n\"Memory bus\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n\"Control bus\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n1.3.2: The Processor - Bus is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n1.3.2.1 https:\/\/eng.libretexts.org\/@go\/page\/46353 1.4 Instruction Cycles\nInstruction Cycles\nThe instruction cycle (also known as the fetch\u2013decode\u2013execute cycle, or simply the fetch-execute cycle) is the cycle that\nthe central processing unit (CPU) follows from boot-up until the computer has shut down in order to process instructions. It is\ncomposed of three main stages: the fetch stage, the decode stage, and the execute stage.\nRole of components\nThe program counter (PC) is a special register that holds the memory address of the next instruction to be executed. During the\nfetch stage, the address stored in the PC is copied into the memory address register (MAR) and then the PC is incremented in order\nto \"point\" to the memory address of the next instruction to be executed. The CPU then takes the instruction at the memory address\ndescribed by the MAR and copies it into the memory data register (MDR). The MDR also acts as a two-way register that holds data\nfetched from memory or data waiting to be stored in memory (it is also known as the memory buffer register (MBR) because of\nthis). Eventually, the instruction in the MDR is copied into the current instruction register (CIR) which acts as a temporary holding\nground for the instruction that has just been fetched from memory.\nDuring the decode stage, the control unit (CU) will decode the instruction in the CIR. The CU then sends signals to other\ncomponents within the CPU, such as the arithmetic logic unit (ALU) and the floating point unit (FPU). The ALU performs\narithmetic operations such as addition and subtraction and also multiplication via repeated addition and division via repeated\nsubtraction. It also performs logic operations such as AND, OR, NOT, and binary shifts as well. The FPU is reserved for\nperforming floating-point operations.\nSummary of stages\nEach computer's CPU can have different cycles based on different instruction sets, but will be similar to the following cycle:\n1. Fetch Stage: The next instruction is fetched from the memory address that is currently stored in the program counter and stored\ninto the instruction register. At the end of the fetch operation, the PC points to the next instruction that will be read at the next\ncycle.\n2. Decode Stage: During this stage, the encoded instruction presented in the instruction register is interpreted by the decoder.\nRead the effective address: In the case of a memory instruction (direct or indirect), the execution phase will be during the\nnext clock pulse. If the instruction has an indirect address, the effective address is read from main memory, and any required\ndata is fetched from main memory to be processed and then placed into data registers (clock pulse: T ). If the instruction is\n3\ndirect, nothing is done during this clock pulse. If this is an I\/O instruction or a register instruction, the operation is\nperformed during the clock pulse.\n3. Execute Stage: The control unit of the CPU passes the decoded information as a sequence of control signals to the relevant\nfunctional units of the CPU to perform the actions required by the instruction, such as reading values from registers, passing\nthem to the ALU to perform mathematical or logic functions on them, and writing the result back to a register. If the ALU is\ninvolved, it sends a condition signal back to the CU. The result generated by the operation is stored in the main memory or sent\nto an output device. Based on the feedback from the ALU, the PC may be updated to a different address from which the next\ninstruction will be fetched.\n4. Repeat Cycle\nRegisters Involved In Each Instruction Cycle:\nMemory address registers(MAR) : It is connected to the address lines of the system bus. It specifies the address in memory\nfor a read or write operation.\nMemory Buffer Register(MBR) : It is connected to the data lines of the system bus. It contains the value to be stored in\nmemory or the last value read from the memory.\nProgram Counter(PC) : Holds the address of the next instruction to be fetched.\nInstruction Register(IR) : Holds the last instruction fetched.\nAdapted from:\n\"Instruction cycle\" by Multiple Contributors, Wikipedia is licensed under CC BY-NC 3.0\nThis page titled 1.4 Instruction Cycles is shared under a CC BY-SA license and was authored, remixed, and\/or curated by Patrick McClanahan.\n1 https:\/\/eng.libretexts.org\/@go\/page\/45615 1.4.1 Instruction Cycles - Fetch\nThe Fetch Cycle\nAt the beginning of the fetch cycle, the address of the next instruction to be executed is in the Program Counter(PC).\nFigure 1: Beginning of the Fetch Cycle. (\"Beginning of the Fetch Cycle\" by Astha_Singh, Geeks for Geeks is licensed under CC\nBY-SA 4.0)\nStep 1: The address in the program counter is moved to the memory address register(MAR), as this is the only register which is\nconnected to address lines of the system bus.\nFigure 1: Step #1 of Fetch Cycle. (\"Beginning of the Fetch Cycle\" by Astha_Singh, Geeks for Geeks is licensed under CC BY-SA\n4.0)\nStep 2: The address in MAR is placed on the address bus, now the control unit issues a READ command on the control bus, and the\nresult appears on the data bus and is then copied into the memory buffer register(MBR). Program counter is incremented by one, to\nget ready for the next instruction.(These two action can be performed simultaneously to save time)\nFigure 1: Step #2 of Fetch Cycle. (\"Step #2 of the Fetch Cycle\" by Astha_Singh, Geeks for Geeks is licensed under CC BY-SA 4.0)\n1 https:\/\/eng.libretexts.org\/@go\/page\/45613 Step 3: The content of the MBR is moved to the instruction register(IR).\nFigure 1: Step #3 of Fetch Cycle. (\"Step #3 of the Fetch Cycle\" by Astha_Singh, Geeks for Geeks is licensed under CC BY-SA 4.0)\nThus, a simple Fetch Cycle consist of three steps and four micro-operation. Symbolically, we can write these sequence of events as\nfollows:-\nFigure 1: Fetch Cycle Steps. (\"Fetch Cycle Steps\" by Astha_Singh, Geeks for Geeks is licensed under CC BY-SA 4.0)\nHere \u2018I\u2019 is the instruction length. The notations (t1, t2, t3) represents successive time units. We assume that a clock is available for\ntiming purposes and it emits regularly spaced clock pulses. Each clock pulse defines a time unit. Thus, all time units are of equal\nduration. Each micro-operation can be performed within the time of a single time unit.\nFirst time unit: Move the contents of the PC to MAR. The contents of the Program Counter contains the address (location) of\nthe instruction being executed at the current time. As each instruction gets fetched, the program counter increases its stored value\nby 1. After each instruction is fetched, the program counter points to the next instruction in the sequence. When the computer\nrestarts or is reset, the program counter normally reverts to 0. The MAR stores this address.\nSecond time unit: Move contents of memory location specified by MAR to MBR. Remember - the MDB contains the value to be\nstored in memory or the last value read from the memory, in this example it is an instruction to be executed. Also in this time unit\nthe PC content gets incremented by 1.\nThird time unit: Move contents of MBR to IR. Now the instructin register contains the instruction we need to execute.\nNote: Second and third micro-operations both take place during the second time unit.\nAdapted from:\n\"Computer Organization | Different Instruction Cycles\" by Astha_Singh, Geeks for Geeks is licensed under CC BY-SA 4.0\nThis page titled 1.4.1 Instruction Cycles - Fetch is shared under a CC BY-SA license and was authored, remixed, and\/or curated by Patrick\nMcClanahan.\n2 https:\/\/eng.libretexts.org\/@go\/page\/45613 1.4.2 Instruction Cycles - Instruction Primer\nHow Instructions Work\nOn the previous page we showed how the fetched instruction is loaded into the instruction register (IR). The instruction contains\nbits that specify the action the processor is to take. The processor will read the instruction and performs the necessary action. In\ngeneral, these actions fall into four categories:\nProcessor - memory: Data may be transferred from processor to memory, or from memory to processor.\nProcessor - I\/O: Data may be transferred to or from a peripheral device by transferring between the processor and the system's\nI\/O module.\nData processing: The processor may perform some arithmetic or logic operation on data, such as addition or comparisons.\nControl: An instruction may specify that the sequence of execution be altered. For example, the processor may fetch an\ninstruction from location 149, which specifies that the next instruction be from location 182. The processor sets the program\ncounter to 182. Thus, on the next fetch stage, the instruction will be fetched from location 182 rather than 150.\nIt is not the intent of this course to cover assembly language programming, or the concepts taught there. However, in order to\nprovide an example the following details are provided.\nIn this simplistic example, both the instructions and data are 16 bits long. The instructions have to following format:\nFigure 1: Instruction Layout. (\"Instruction Layout\" by Patrick McClanahan is licensed under CC BY-SA 4.0)\nThe opcode is a numerical representation of the instruction to be performed, instructions such as mathematical or logic operations.\nThe opcode tells the processor what it needs to do. The second part of the instructions tells the processor WHERE to find the data\nthat is being operated on. (we will see more specifically how this works in a moment).\nFigure 2: Data Layout. (\"Data Layout\" by Patrick McClanahan is licensed under CC BY-SA 4.0)\nWhen reading data from a memory location the first bit is a sign bit, and the other 15 bits are for the actual data.\nIn our example, we include an Accumulator (AC) which will be used as a temporary storage location for the data.\nThere will be 3 opcodes - PLEASE understand these are sample opcode for this example...do NOT confuse these with actual\nprocessor opcodes.\n1. 0001 - this opcode tells the processor to load the accumulator (AC) from the given memory address.\n2. 0011 - this opcode tells the processor to add to the value currently stored in the AC from the specified memory address.\n3. 0111 - this opcode tells the processor to move the value in the AC to the specified memory address.\nFigure 3: First Cycle. (\"First Cycle\" by Patrick McClanahan is licensed under CC BY-SA 4.0)\n1. At the beginning the PC contains 3254, which is the memory address of the next instruction to be executed. In this example we\nhave skipped the micro-steps, showing the IR receiving the value at the specified address.\n1 https:\/\/eng.libretexts.org\/@go\/page\/45614 2. The instruction at address 3254 is 172567. Remember from above - the first 4 bits are the opcode, in this case it is the number 1\n(0001 in binary).\n3. This opcode tells the processor to load the AC from the memory address located in the last 12 bits of the instruction - 72567.\n4. Go to address 72567 and load that value, 0099, into the accumulator.\nALSO NOTICE - the PC has been incremented by 1, so it now points to the next instruction in memory.\nFigure 4: Second Cylce (\"Second Cycle\" by Patrick McClanahan is licensed under CC BY-SA 4.0)\n1. Again - we start with the PC - and move the contents found at the memory address, 3255, into the IR.\n2. The instruction in the IR has an opcode of 3 (0011 in binary).\n3. This opcode in our example tells the processor to add the value currently stored in the AC, 0099, to the value stored at the given\nmemory address, 72568, which is the value 2.\n4. This value, 101, is stored back into the AC.\nAGAIN, the PC has been incremented by one as well.\nFigure 5: Third Cycle. (\"Third Cycle\" by Patrick McClanahan is licensed under CC BY-SA 4.0)\n1. The PC points to 3256, the value 772569 is moved to the IR.\n2. This instruction has an opcode of 7 (0111 in binary)\n3. This opcode tells the processor to move the value in the AC, 101, to the specified memory address, 72569.\nThe PC has again been incremented by one - and when our simple 3 instructions are completed, whatever instruction was at that\naddress would be executed.\nThis page titled 1.4.2 Instruction Cycles - Instruction Primer is shared under a CC BY-SA license and was authored, remixed, and\/or curated by\nPatrick McClanahan.\n2 https:\/\/eng.libretexts.org\/@go\/page\/45614 1.5 Interrupts\nWhat is an Interrupt\nAn interrupt is a signal emitted by hardware or software when a process or an event needs immediate attention. It alerts the\nprocessor to a high priority process requiring interruption of the current working process. In I\/O devices one of the bus control lines\nis dedicated for this purpose and is called the Interrupt Service Routine (ISR).\nWhen a device raises an interrupt at lets say process i, the processor first completes the execution of instruction i. Then it loads the\nProgram Counter (PC) with the address of the first instruction of the ISR. Before loading the Program Counter with the address, the\naddress of the interrupted instruction is moved to a temporary location. Therefore, after handling the interrupt the processor can\ncontinue with process i+1.\nWhile the processor is handling the interrupts, it must inform the device that its request has been recognized so that it stops sending\nthe interrupt request signal. Also, saving the registers so that the interrupted process can be restored in the future, increases the\ndelay between the time an interrupt is received and the start of the execution of the ISR. This is called Interrupt Latency.\nHardware Interrupts:\nIn a hardware interrupt, all the devices are connected to the Interrupt Request Line. A single request line is used for all the n\ndevices. To request an interrupt, a device closes its associated switch. When a device requests an interrupts, the value of INTR is\nthe logical OR of the requests from individual devices.\nSequence of events involved in handling an IRQ:\n1. Devices raise an IRQ.\n2. Processor interrupts the program currently being executed.\n3. Device is informed that its request has been recognized and the device deactivates the request signal.\n4. The requested action is performed.\n5. Interrupt is enabled and the interrupted program is resumed.\nConceptually an interrupt causes the following to happen:\nFigure 1: Concept of an interrupt. (\"Concept of an Interrupt\" by lemilxavier, WikiBooks is licensed under CC BY-SA 3.0)\nThe grey bars represent the control flow. The top line is the program that is currently running, and the bottom bar is the interrupt\nservice routine (ISR). Notice that when the interrupt (Int) occurs, the program stops executing and the microcontroller begins to\nexecute the ISR. Once the ISR is complete, the microcontroller returns to processing the program where it left off.\nHandling Multiple Devices:\nWhen more than one device raises an interrupt request signal, then additional information is needed to decide which which device\nto be considered first. The following methods are used to decide which device to select: Polling, Vectored Interrupts, and Interrupt\nNesting. These are explained as following below.\n1. Polling:\nIn polling, the first device encountered with with IRQ bit set is the device that is to be serviced first. Appropriate ISR is called\nto service the same. It is easy to implement but a lot of time is wasted by interrogating the IRQ bit of all devices.\n2. Vectored Interrupts:\nIn vectored interrupts, a device requesting an interrupt identifies itself directly by sending a special code to the processor over\nthe bus. This enables the processor to identify the device that generated the interrupt. The special code can be the starting\naddress of the ISR or where the ISR is located in memory, and is called the interrupt vector.\n3. Interrupt Nesting:\nIn this method, I\/O device is organized in a priority structure. Therefore, interrupt request from a higher priority device is\n1 https:\/\/eng.libretexts.org\/@go\/page\/45616 recognized where as request from a lower priority device is not. To implement this each process\/device (even the processor).\nProcessor accepts interrupts only from devices\/processes having priority more than it.\nWhat happens when external hardware requests another interrupt while the processor is already in the middle of executing the ISR\nfor a previous interrupt request?\nWhen the first interrupt was requested, hardware in the processor causes it to finish the current instruction, disable further\ninterrupts, and jump to the interrupt handler.\nThe processor ignores further interrupts until it gets to the part of the interrupt handler that has the \"return from interrupt\"\ninstruction, which re-enables interrupts.\nIf an interrupt request occurs while interrupts were turned off, some processors will immediately jump to that interrupt handler as\nsoon as interrupts are turned back on. With this sort of processor, an interrupt storm \"starves\" the main loop background task. Other\nprocessors execute at least one instruction of the main loop before handling the interrupt, so the main loop may execute extremely\nslowly, but at least it never \"starves\".\nA few processors have an interrupt controller that supports \"round robin scheduling\", which can be used to prevent a different kind\nof \"starvation\" of low-priority interrupt handlers.\nProcessors priority is encoded in a few bits of PS (Process Status register). It can be changed by program instructions that write into\nthe PS. Processor is in supervised mode only while executing OS routines. It switches to user mode before executing application\nprograms\nAdapted from:\n\"Interrupts\" by lemilxavier, Geeks for Geeks is licensed under CC BY-SA 4.0\n\"Microprocessor Design\/Interrupts\" by lemilxavier, WikiBooks is licensed under CC BY-SA 3.0\nThis page titled 1.5 Interrupts is shared under a CC BY-SA license and was authored, remixed, and\/or curated by Patrick McClanahan.\n2 https:\/\/eng.libretexts.org\/@go\/page\/45616 1.6 Memory Hierarchy\nMemory Hierarchy is an enhancement to organize the memory such that it can minimize the access time. The Memory Hierarchy\nwas developed based on a program behavior known as locality of references.The figure below clearly demonstrates the different\nlevels of memory hierarchy :\nThis Memory Hierarchy Design is divided into 2 main types:\n1. External Memory or Secondary Memory \u2013\nComprising of Magnetic Disk, Optical Disk, Magnetic Tape i.e. peripheral storage devices which are accessible by the\nprocessor via I\/O Module.\n2. Internal Memory or Primary Memory \u2013\nComprising of Main Memory, Cache Memory & CPU registers. This is directly accessible by the processor.\nWe can infer the following characteristics of Memory Hierarchy Design from above figure:\n1. Capacity:\nIt is the global volume of information the memory can store. As we move from top to bottom in the Hierarchy, the capacity\nincreases.\n2. Access Time:\nIt is the time interval between the read\/write request and the availability of the data. As we move from top to bottom in the\nHierarchy, the access time increases.\n3. Performance:\nEarlier when the computer system was designed without Memory Hierarchy design, the speed gap increases between the CPU\nregisters and Main Memory due to large difference in access time. This results in lower performance of the system and thus,\nenhancement was required. This enhancement was made in the form of Memory Hierarchy Design because of which the\nperformance of the system increases. One of the most significant ways to increase system performance is minimizing how far\ndown the memory hierarchy one has to go to manipulate data.\n4. Cost per bit:\nAs we move from bottom to top in the Hierarchy, the cost per bit increases i.e. Internal Memory is costlier than External\nMemory.\nAdapted from:\n\"Memory Hierarchy Design and its Characteristics\" by RishabhJain12, Geeks for Geeks is licensed under CC BY-SA 4.0\nThis page titled 1.6 Memory Hierarchy is shared under a CC BY-SA license and was authored, remixed, and\/or curated by Patrick McClanahan.\n1 https:\/\/eng.libretexts.org\/@go\/page\/45617 1.7 Cache Memory\nCache Memory\nCache Memory is a special very high-speed memory. It is used to speed up and synchronizing with high-speed CPU. Cache\nmemory is costlier than main memory or disk memory but economical than CPU registers. Cache memory is an extremely fast\nmemory type that acts as a buffer between RAM and the CPU. It holds frequently requested data and instructions so that they are\nimmediately available to the CPU when needed.\nCache memory is used to reduce the average time to access data from the Main memory. The cache is a smaller and faster memory\nwhich stores copies of the data from frequently used main memory locations. There are various different independent caches in a\nCPU, which store instructions and data.\nFigure 1: Cahce Memory. (\"Cache Memory\" by VaibhavRai3, Geeks for Geeks is licensed under CC BY-SA 4.0)\nLevels of memory:\nLevel 1 or Register\nIt is a type of memory in which data is stored and accepted that are immediately stored in CPU. Most commonly used register is\naccumulator, Program counter, address register etc.\nLevel 2 or Cache memory\nIt is the fastest memory which has faster access time where data is temporarily stored for faster access.\nLevel 3 or Main Memory (Primary Memory in the image above)\nIt is memory on which computer works currently. It is small in size and once power is off data no longer stays in this memory.\nLevel 4 or Secondary Memory\nIt is external memory which is not as fast as main memory but data stays permanently in this memory.\nCache Performance:\nWhen the processor needs to read or write a location in main memory, it first checks for a corresponding entry in the cache.\nIf the processor finds that the memory location is in the cache, a cache hit has occurred and data is read from cache\nIf the processor does not find the memory location in the cache, a cache miss has occurred. For a cache miss, the cache\nallocates a new entry and copies in data from main memory, then the request is fulfilled from the contents of the cache.\nThe performance of cache memory is frequently measured in terms of a quantity called Hit ratio.\nHit ratio = hit \/ (hit + miss) = no. of hits\/total accesses\nWe can improve Cache performance using higher cache block size, higher associativity, reduce miss rate, reduce miss penalty, and\nreduce the time to hit in the cache.\nApplication of Cache Memory\n1. Usually, the cache memory can store a reasonable number of blocks at any given time, but this number is small compared to the\ntotal number of blocks in the main memory.\n2. The correspondence between the main memory blocks and those in the cache is specified by a mapping function.\nTypes of Cache\n1 https:\/\/eng.libretexts.org\/@go\/page\/45620 Primary Cache\nA primary cache is always located on the processor chip. This cache is small and its access time is comparable to that of\nprocessor registers.\nSecondary Cache\nSecondary cache is placed between the primary cache and the rest of the memory. It is referred to as the level 2 (L2) cache.\nOften, the Level 2 cache is also housed on the processor chip.\nLocality of reference\nSince size of cache memory is less as compared to main memory. So to check which part of main memory should be given priority\nand loaded in cache is decided based on locality of reference. Locality will be discussed in greater detail later on.\nAdapted from:\n\"Cache Memory in Computer Organization\" by VaibhavRai3, Geeks for Geeks is licensed under CC BY-SA 4.0\nThis page titled 1.7 Cache Memory is shared under a CC BY-SA license and was authored, remixed, and\/or curated by Patrick McClanahan.\n2 https:\/\/eng.libretexts.org\/@go\/page\/45620 1.7.1 Cache Memory - Multilevel Cache\nMultilevel Cache\nMultilevel cache is one of the techniques to improve cache performance by reducing the \u201cmiss penalty\u201d. The term miss penalty\nrefers to the extra time required to bring the data into cache from the main memory whenever there is a \u201cmiss\u201d in cache .\nFor clear understanding let us consider an example where CPU requires 10 memory references for accessing the desired\ninformation and consider this scenario in the following 3 cases of System design :\nSystem Design without cache memory\nHere the CPU directly communicates with the main memory and no caches are involved.\nIn this case, the CPU needs to access the main memory 10 times to access the desired information.\nSystem Design with cache memory\nHere the CPU at first checks whether the desired data is present in the cache memory or not i.e. whether there is a \u201chit\u201d in cache\nor \u201cmiss\u201d in cache. Suppose there are 3 miss in cache memory then the main memory will be accessed only 3 times. We can see\nthat here the miss penalty is reduced because the main memory is accessed a lesser number of times than that in the previous case.\nSystem Design with Multilevel cache memory\nHere the cache performance is optimized further by introducing multilevel Caches. As shown in the above figure, we are\nconsidering 2 level cache Design. Suppose there are 3 miss in the L1 cache memory and out of these 3 misses there are 2 miss in\nthe L2 cache memory then the Main Memory will be accessed only 2 times. It is clear that here the miss penalty is reduced\nconsiderably than that in the previous case thereby improving the performance of cache memory.\nNOTE :\nWe can observe from the above 3 cases that we are trying to decrease the number of main memory references and thus decreasing\nthe miss penalty in order to improve the overall system performance. Also, it is important to note that in the multilevel\ncache design, L1 cache is attached to the CPU and it is small in size but fast. Although, L2 cache is attached to the primary cache\ni.e. L1 cache and it is larger in size and slower but still faster than the main memory.\nEffective Access Time = Hit rate * Cache access time\n+ Miss rate * Lower level access time\n1 https:\/\/eng.libretexts.org\/@go\/page\/45618 Average access Time For Multilevel Cache:(T )\navg\nT = H * C + (1 \u2013 H ) * (H * C +(1 \u2013 H ) *M )\navg 1 1 1 2 2 2\nwhere\nH1 is the Hit rate in the L1 caches.\nH2 is the Hit rate in the L2 cache.\nC1 is the Time to access information in the L1 caches.\nC2 is the Miss penalty to transfer information from the L2 cache to an L1 cache.\nM is the Miss penalty to transfer information from the main memory to the L2 cache.\nAdapted from:\n\"Multilevel Cache Organisation\" by shreya garg 4, Geeks for Geeks is licensed under CC BY-SA 4.0\nThis page titled 1.7.1 Cache Memory - Multilevel Cache is shared under a CC BY-SA license and was authored, remixed, and\/or curated by\nPatrick McClanahan.\n2 https:\/\/eng.libretexts.org\/@go\/page\/45618 1.7.2 Cache Memory - Locality of reference\nLocality of Reference\nLocality of reference refers to a phenomenon in which a computer program tends to access same set of memory locations for a\nparticular time period. In other words, Locality of Reference refers to the tendency of the computer program to access instructions\nwhose addresses are near one another. The property of locality of reference is mainly shown by:\n1. Loops in program cause the CPU to repeatedly execute a set of instructions that constitute the loop.\n2. Subroutine calls, cause the set of instructions are fetched from memory each time the subroutine gets called.\n3. References to data items also get localized, meaning the same data item is referenced again and again.\nEven though accessing memory is quite fast, it is possible for repeated calls for data from main memory can become a bottleneck.\nBy using faster cache memory, it is possible to speed up the retrieval of frequently used instructions or data.\nFigure 1: Cache Hit \/ Cache Miss. (\"Cache Hit \/ Miss\" by balwant_singh, Geeks for Geeks is licensed under CC BY-SA 4.0)\nIn the above figure, you can see that the CPU wants to read or fetch the data or instruction. First, it will access the cache memory as\nit is near to it and provides very fast access. If the required data or instruction is found, it will be fetched. This situation is known as\na cache hit. But if the required data or instruction is not found in the cache memory then this situation is known as a cache miss.\nNow the main memory will be searched for the required data or instruction that was being searched and if found will go through\none of the two ways:\n1. The inefficent method is to have the CPU fetch the required data or instruction from main memory and use it. When the same\ndata or instruction is required again the CPU again has to access the main memory to retrieve it again .\n2. A much more efficient method is to store the data or instruction in the cache memory so that if it is needed soon again in the\nnear future it could be fetched in a much faster manner.\nCache Operation:\nThis concept is based on the idea of locality of reference. There are two ways in which data or instruction are fetched from main\nmemory then get stored in cache memory:\n1. Temporal Locality\nTemporal locality means current data or instruction that is being fetched may be needed soon. So we should store that data or\ninstruction in the cache memory so that we can avoid again searching in main memory for the same data.\n1 https:\/\/eng.libretexts.org\/@go\/page\/45619 Figure 1: Temporal Locality. (\"Temporal Locality\" by balwant_singh, Geeks for Geeks is licensed under CC BY-SA 4.0)\n2. When CPU accesses the current main memory location for reading required data or instruction, it also gets stored in the cache\nmemory which is based on the fact that same data or instruction may be needed in near future. This is known as temporal\nlocality. If some data is referenced, then there is a high probability that it will be referenced again in the near future.\n3. Spatial Locality\nSpatial locality means instruction or data near to the current memory location that is being fetched, may be needed by the\nprocessor soon. This is different from the temporal locality in that we are making a guess that the data\/instructions will be\nneeded soon. With temporal locality we were talking about the actual memory location that was being fetched.\nFigure 1: Spatial Locality. (\"Spatial Locality\" by balwant_singh, Geeks for Geeks is licensed under CC BY-SA 4.0)\nAdapted From:\n\"Locality of Reference and Cache Operation in Cache Memory\" by balwant_singh, Geeks for Geeks is licensed under CC BY-SA\n4.0\nThis page titled 1.7.2 Cache Memory - Locality of reference is shared under a CC BY-SA license and was authored, remixed, and\/or curated by\nPatrick McClanahan.\n2 https:\/\/eng.libretexts.org\/@go\/page\/45619 1.8 Direct Memory Access\nDMA\nThere are three techniques used for I\/O operations: programmed I\/O, interrupt-driven I\/O, and direct memory access (DMA). As\nlong as we are discussing DMA, we will also discuss the other two techniques.\nThe method that is used to transfer information between internal storage and external I\/O devices is known as I\/O interface. The\nCPU is interfaced using special communication links by the peripherals connected to any computer system. These communication\nlinks are used to resolve the differences between CPU and peripheral. There exists special hardware components between CPU and\nperipherals to supervise and synchronize all the input and output transfers that are called interface units.\nMode of Transfer:\nThe binary information that is received from an external device is usually stored in the memory unit. The information that is\ntransferred from the CPU to the external device is originated from the memory unit. CPU merely processes the information but the\nsource and target is always the memory unit. Data transfer between CPU and the I\/O devices may be done in different modes.\nData transfer to and from the peripherals may be done in any of the three possible ways\n1. Programmed I\/O.\n2. Interrupt- initiated I\/O.\n3. Direct memory access( DMA).\nNow let\u2019s discuss each mode one by one.\n1. Programmed I\/O: It is due to the result of the I\/O instructions that are written in the computer program. Each data item\ntransfer is initiated by an instruction in the program. Usually the transfer is from a CPU register and memory. In this case it\nrequires constant monitoring by the CPU of the peripheral devices.\nExample of Programmed I\/O: In this case, the I\/O device does not have direct access to the memory unit. A transfer from I\/O\ndevice to memory requires the execution of several instructions by the CPU, including an input instruction to transfer the data\nfrom device to the CPU and store instruction to transfer the data from CPU to memory. In programmed I\/O, the CPU stays in\nthe program loop until the I\/O unit indicates that it is ready for data transfer. This is a time consuming process since it\nneedlessly keeps the CPU busy. This situation can be avoided by using an interrupt facility. This is discussed below.\n2. Interrupt- initiated I\/O: Since in the above case we saw the CPU is kept busy unnecessarily. This situation can very well be\navoided by using an interrupt driven method for data transfer. By using interrupt facility and special commands to inform the\ninterface to issue an interrupt request signal whenever data is available from any device. In the meantime the CPU can proceed\nfor any other program execution. The interface meanwhile keeps monitoring the device. Whenever it is determined that the\ndevice is ready for data transfer it initiates an interrupt request signal to the computer. Upon detection of an external interrupt\nsignal the CPU stops momentarily the task that it was already performing, branches to the service program to process the I\/O\ntransfer, and then return to the task it was originally performing. \\\nNote: Both the methods programmed I\/O and Interrupt-driven I\/O require the active intervention of the\nprocessor to transfer data between memory and the I\/O module, and any data transfer must transverse\na path through the processor. Thus both these forms of I\/O suffer from two inherent drawbacks.\nThe I\/O transfer rate is limited by the speed with which the processor can test and service a\ndevice.\nThe processor is tied up in managing an I\/O transfer; a number of instructions must be executed\nfor each I\/O transfer.\n3. Direct Memory Access: The data transfer between a fast storage media such as magnetic disk and memory unit is limited by\nthe speed of the CPU. Thus we can allow the peripherals directly communicate with each other using the memory buses,\nremoving the intervention of the CPU. This type of data transfer technique is known as DMA or direct memory access. During\nDMA the CPU is idle and it has no control over the memory buses. The DMA controller takes over the buses to manage the\ntransfer directly between the I\/O devices and the memory unit.\nBus Request : It is used by the DMA controller to request the CPU to relinquish the control of the buses.\n1 https:\/\/eng.libretexts.org\/@go\/page\/45621 Bus Grant : It is activated by the CPU to Inform the external DMA controller that the buses are in high impedance state and the\nrequesting DMA can take control of the buses. Once the DMA has taken the control of the buses it transfers the data. This transfer\ncan take place in many ways.\nTypes of DMA transfer using DMA controller (DMAC):\nBurst Transfer :\nDMA returns the bus after complete data transfer. A register is used as a byte count,\nbeing decremented for each byte transfer, and upon the byte count reaching zero, the DMAC will\nrelease the bus. When the DMAC operates in burst mode, the CPU is halted for the duration of the data\ntransfer.\n1. Bus grant request time.\n2. Transfer the entire block of data at transfer rate of device because the device is usually slow than the\nspeed at which the data can be transferred to CPU.\n3. Release the control of the bus back to CPU\nSo, total time taken to transfer the N bytes = Bus grant request time + (N) * (memory transfer rate) + Bus release control time.\nCyclic Stealing :An alternative method in which DMA controller transfers one word at a time after which it must return the control\nof the buses to the CPU. The CPU delays its operation only for one memory cycle to allow the direct memory I\/O transfer to\n\u201csteal\u201d one memory cycle.\nSteps Involved are:\n1. Buffer the byte into the buffer\n2. Inform the CPU that the device has 1 byte to transfer (i.e. bus grant request)\n3. Transfer the byte (at system bus speed)\n4. Release the control of the bus back to CPU.\nBefore moving on transfer next byte of data, device performs step 1 again so that bus isn\u2019t tied up and the transfer won\u2019t depend\nupon the transfer rate of device.\nAdapted from:\n\"I\/O Interface (Interrupt and DMA Mode)\" by saripallisriharsha2, Geeks for Geeks is licensed under CC BY-SA 4.0\nThis page titled 1.8 Direct Memory Access is shared under a CC BY-SA license and was authored, remixed, and\/or curated by Patrick\nMcClanahan.\n2 https:\/\/eng.libretexts.org\/@go\/page\/45621 1.9 Multiprocessor and Multicore Systems\nMultiProcessor System\nTwo or more processors or CPUs present in same computer, sharing system bus, memory and I\/O is called MultiProcessing\nSystem. It allows parallel execution of different processors. These systems are reliable since failure of any single processor does\nnot affect other processors. A quad-processor system can execute four processes at a time while an octa-processor can execute eight\nprocesses at a time. The memory and other resources may be shared or distributed among processes.\nFigure 1: Symmetric multiprocessing system. (\"Symmetric multiprocessing system\" by Multiple Contributors, Wikipedia\nCommons is licensed under CC BY-SA 3.0)\nAdvantages :\nSince more than one processor are working at the same time, throughput will get increased.\nMore reliable since failure in one CPU does not affect other.\nIt needs little complex configuration.\nParallel processing (more than one process executing at same time) is achieved through MultiProcessing.\nDisadvantages :\nIt will have more traffic (distances between two will require longer time).\nThroughput may get reduced in shared resources system where one processor using some I\/O then another processor has to wait\nfor its turn.\nAs more than processors are working at particular instant of time. So, coordination between these is very complex.\nMulticore System\nA processor that has more than one core is called Multicore Processor while one with single core is called Unicore Processor or\nUniprocessor. Nowadays, most of systems have four cores (Quad-core) or eight cores (Octa-core). These cores can individually\nread and execute program instructions, giving feel like computer system has several processors but in reality, they are cores and not\nprocessors. Instructions can be calculation, data transferring instruction, branch instruction, etc. Processor can run instructions on\nseparate cores at same time. This increases overall speed of program execution in system. Thus heat generated by processor gets\nreduced and increases overall speed of execution.\nMulticore systems support MultiThreading and Parallel Computing. Multicore processors are widely used across many application\ndomains, including general-purpose, embedded, network, digital signal processing (DSP), and graphics (GPU). Efficient software\nalgorithms should be used for implementation of cores to achieve higher performance. Software that can run in parallel is preferred\nbecause the desire is to achieve parallel execution with the help of multiple cores.\n1 https:\/\/eng.libretexts.org\/@go\/page\/45622 Figure 1: Quad Core Processor. (\"Diagram of a generic dual-core processor \" by Multiple Contributors, Wikipedia Commons is\nlicensed under CC BY-SA 3.0)\nAdvantages :\nThese cores are usually integrated into single IC (integrated circuit) die, or onto multiple dies but in single chip package. Thus\nallowing higher Cache Coherency.\nThese systems are energy efficient since they allow higher performance at lower energy. A challenge in this, however, is\nadditional overhead of writing parallel code.\nIt will have less traffic(cores integrated into single chip and will require less time).\nDisadvantages :\nDual-core processor do not work at twice speed of single processor. They get only 60-80% more speed.\nSome Operating systems are still using single core processor.\nOS compiled for multi-core processor will run slightly slower on single-core processor\nAdapted from:\n\"Difference between MultiCore and MultiProcessor System\" by Ganeshchowdharysadanala, Geeks for Geeks is licensed under CC\nBY-SA 4.0\nThis page titled 1.9 Multiprocessor and Multicore Systems is shared under a CC BY-SA license and was authored, remixed, and\/or curated by\nPatrick McClanahan.\n2 https:\/\/eng.libretexts.org\/@go\/page\/45622 CHAPTER OVERVIEW\n2: Operating System Overview\n2.1: Function of the Operating System\n2.2: Types of Operating Systems\n2.2.1: Types of Operating Systems (continued)\n2.2.2: Types of Operating Systems (continued)\n2.3: Difference between multitasking, multithreading and multiprocessing\n2.3.1: Difference between multitasking, multithreading and multiprocessing (continued)\n2.3.2: Difference between Multiprogramming, multitasking, multithreading and multiprocessing (continued)\nThis page titled 2: Operating System Overview is shared under a CC BY-SA license and was authored, remixed, and\/or curated by Patrick\nMcClanahan.\n1 2.1: Function of the Operating System\nWhat is the Purpose of an OS?\nAn operating system acts as a communication bridge (interface) between the user and computer hardware. The purpose of an\noperating system is to provide a platform on which a user can execute programs in a convenient and efficient manner.\nAn operating system is a piece of software that manages the allocation of computer hardware. The coordination of the hardware\nmust be appropriate to ensure the correct working of the computer system and to prevent user programs from interfering with the\nproper working of the system.\nExample: Just like a boss gives order to his employee, in the similar way we request or pass our orders to the operating system. The\nmain goal of the operating system is to thus make the computer environment more convenient to use and the secondary goal is to\nuse the resources in the most efficient manner.\nWhat is operating system ?\nAn operating system is a program on which application programs are executed and acts as an communication bridge (interface)\nbetween the user and the computer hardware.\nThe main task an operating system carries out is the allocation of resources and services, such as allocation of: memory, devices,\nprocessors and information. The operating system also includes programs to manage these resources, such as a traffic controller, a\nscheduler, memory management module, I\/O programs, and a file system.\nImportant functions of an operating system:\n1. Security\nThe operating system uses password protection to protect user data and similar other techniques. it also prevents unauthorized\naccess to programs and user data.\n2. Control over system performance\nMonitors overall system health to help improve performance. records the response time between service requests and system\nresponse to have a complete view of the system health. This can help improve performance by providing important information\nneeded to troubleshoot problems.\n3. Job accounting\nOperating system keeps track of time and resources used by various tasks and users, this information can be used to track\nresource usage for a particular user or group of user.\n4. Error detecting aids\nOperating system constantly monitors the system to detect errors and avoid the malfunctioning of computer system.\n5. Coordination between other software and users\nOperating systems also coordinate and assign interpreters, compilers, assemblers and other software to the various users of the\ncomputer systems.\n6. Memory Management\nThe operating system manages the primary memory or main memory. Main memory is made up of a large array of bytes or\nwords where each byte or word is assigned a certain address. Main memory is a fast storage and it can be accessed directly by\nthe CPU. For a program to be executed, it should be first loaded in the main memory. An operating system performs the\nfollowing activities for memory management:\nIt keeps tracks of primary memory, i.e., which bytes of memory are used by which user program. The memory addresses that\nhave already been allocated and the memory addresses of the memory that has not yet been used. In multi programming, the OS\ndecides the order in which process are granted access to memory, and for how long. It Allocates the memory to a process when\nthe process requests it and deallocates the memory when the process has terminated or is performing an I\/O operation.\n7. Processor Management\nIn a multi programming environment, the OS decides the order in which processes have access to the processor, and how much\nprocessing time each process has. This function of OS is called process scheduling. An operating system performs the following\nactivities for processor management.\nKeeps tracks of the status of processes. The program which perform this task is known as traffic controller. Allocates the CPU\nthat is processor to a process. De-allocates processor when a process is no more required.\n2.1.1 https:\/\/eng.libretexts.org\/@go\/page\/45624 8. Device Management\nAn OS manages device communication via their respective drivers. It performs the following activities for device management.\nKeeps tracks of all devices connected to system. designates a program responsible for every device known as the Input\/Output\ncontroller. Decides which process gets access to a certain device and for how long. Allocates devices in an effective and\nefficient way. Deallocates devices when they are no longer required.\n9. File Management\nA file system is organized into directories for efficient or easy navigation and usage. These directories may contain other\ndirectories and other files. An operating system carries out the following file management activities. It keeps track of where\ninformation is stored, user access settings and status of every file and more\u2026 These facilities are collectively known as the file\nsystem.\nMoreover, operating system also provides certain services to the computer system in one form or the other.\nThe operating system provides certain services to the users which can be listed in the following manner:\n1. Program Execution\nThe operating system is responsible for execution of all types of programs whether it be user programs or system programs. The\noperating system utilizes various resources available for the efficient running of all types of functionalities.\n2. Handling Input\/Output Operations\nThe operating system is responsible for handling all sort of inputs, i.e, from keyboard, mouse, desktop, etc. The operating\nsystem does all interfacing in the most appropriate manner regarding all kind of inputs and outputs.\nFor example, there is difference in nature of all types of peripheral devices such as mouse or keyboard, then operating system is\nresponsible for handling data between them.\n3. Manipulation of File System\nThe operating system is responsible for making of decisions regarding the storage of all types of data or files, i.e, floppy\ndisk\/hard disk\/pen drive, etc. The operating system decides as how the data should be manipulated and stored.\n4. Error Detection and Handling\nThe operating system is responsible for detection of any types of error or bugs that can occur while any task. The well secured\nOS sometimes also acts as countermeasure for preventing any sort of breach to the computer system from any external source\nand probably handling them.\n5. Resource Allocation\nThe operating system ensures the proper use of all the resources available by deciding which resource to be used by whom for\nhow much time. All the decisions are taken by the operating system.\n6. Accounting\nThe operating system tracks an account of all the functionalities taking place in the computer system at a time. All the details\nsuch as the types of errors occurred are recorded by the operating system.\n7. Information and Resource Protection\nThe operating system is responsible for using all the information and resources available on the machine in the most protected\nway. The operating system must foil an attempt from any external resource to hamper any sort of data or information.\nAll these services are ensured by the operating system for the convenience of the users to make the programming task easier. All\ndifferent kinds of operating system more or less provide the same services.\nAdapted from:\n\"Functions of operating system\" by Amaninder.Singh, Geeks for Geeks is licensed under CC BY-SA 4.0\nThis page titled 2.1: Function of the Operating System is shared under a CC BY-SA license and was authored, remixed, and\/or curated by Patrick\nMcClanahan.\n2.1.2 https:\/\/eng.libretexts.org\/@go\/page\/45624 2.2: Types of Operating Systems\nWhat are the Types of Operating Systems\nAn Operating System performs all the basic tasks like managing file,process, and memory. Thus operating system acts as manager\nof all the resources, i.e. resource manager. Thus operating system becomes an interface between user and machine.\nTypes of Operating Systems: Some of the widely used operating systems are as follows-\n1. Batch Operating System\nThis type of operating system does not interact with the computer directly. There is an operator which takes similar jobs having\nsame requirement and group them into batches. It is the responsibility of operator to sort the jobs with similar needs.\nFigure 2.2.1: Depiction of a batch operating system. (\"A batch operating system\" by akash1295, Geeks for Geeks is licensed\nunder CC BY-SA 4.0)\nAdvantages of Batch Operating System:\nIt is very difficult to guess or know the time required by any job to complete. Processors of the batch systems know how long\nthe job would be when it is in queue\nMultiple users can share the batch systems\nThe idle time for batch system is very less\nIt is easy to manage large work repeatedly in batch systems\nDisadvantages of Batch Operating System:\nThe computer operators should be well known with batch systems\nBatch systems are hard to debug\nIt is sometime costly\nThe other jobs will have to wait for an unknown time if any job fails\nExamples of Batch based Operating System: Payroll System, Bank Statements etc.\n2. Time-Sharing Operating Systems\nEach task is given some time to execute, so that all the tasks work smoothly. Each user gets time of CPU as they use single system.\nThese systems are also known as Multitasking Systems. The task can be from single user or from different users also. The time that\neach task gets to execute is called quantum. After this time interval is over OS switches over to next task.\n2.2.1 https:\/\/eng.libretexts.org\/@go\/page\/45627 Figure 2.2.1: Time sharing Operating System. (\"A time share operating system\" by akash1295, Geeks for Geeks is licensed\nunder CC BY-SA 4.0)\nAdvantages of Time-Sharing OS:\nEach task gets an equal opportunity\nLess chances of duplication of software\nCPU idle time can be reduced\nDisadvantages of Time-Sharing OS:\nReliability problem\nOne must have to take care of security and integrity of user programs and data\nData communication problem\nExamples of Time-Sharing OSs are: Linux, Unix etc.\nAdapted from:\n\"Types of Operating Systems\" by akash1295, Geeks for Geeks is licensed under CC BY-SA 4.0\nThis page titled 2.2: Types of Operating Systems is shared under a CC BY-SA license and was authored, remixed, and\/or curated by Patrick\nMcClanahan.\n2.2.2 https:\/\/eng.libretexts.org\/@go\/page\/45627 2.2.1: Types of Operating Systems (continued)\n3. Distributed Operating System\nVarious autonomous interconnected computers communicate each other using a shared communication network. Independent\nsystems possess their own memory unit and CPU. These are referred as loosely coupled systems or distributed systems. These\nsystem\u2019s processors differ in size and function. The major benefit of working with these types of operating system is that it is\nalways possible that one user can access the files or software which are not actually present on his system but on some other system\nconnected within this network i.e., remote access is enabled within the devices connected in that network.\nFigure 2.2.1.1: Distributed Operating System. (\"Distributed Operating System\" by akash1295, Geeks for Geeks is licensed\nunder CC BY-SA 4.0)\nAdvantages of Distributed Operating System:\nFailure of one will not affect the other network communication, as all systems are independent from each other\nElectronic mail increases the data exchange speed\nSince resources are being shared, computation is highly fast and durable\nLoad on host computer reduces\nThese systems are easily scalable as many systems can be easily added to the network\nDelay in data processing reduces\nDisadvantages of Distributed Operating System:\nFailure of the main network will stop the entire communication\nTo establish distributed systems the language which are used are not well defined yet\nThese types of systems are not readily available as they are very expensive. Not only that the underlying software is highly\ncomplex and not understood well yet\nExamples of Distributed Operating System are- LOCUS .\n4. Network Operating System\nHistorically operating systems with networking capabilities were described as network operating system, because they allowed\npersonal computers (PCs) to participate in computer networks and shared file and printer access within a local area network (LAN).\nThis description of operating systems is now largely historical, as common operating systems include a network stack to support\na client\u2013server model.\n2.2.1.1 https:\/\/eng.libretexts.org\/@go\/page\/45625 These limited client\/server networks were gradually replaced by Peer-to-peer networks, which used networking capabilities to\nshare resources and files located on a variety of computers of all sizes. A peer-to-peer network sets all connected computers equal;\nthey all share the same abilities to use resources available on the network. The most popular peer-to-peer networks as of 2020 are\nEthernet, Wi-Fi and the Internet protocol suite. Software that allowed users to interact with these networks, despite a lack of\nnetworking support in the underlying manufacturer's operating system, was sometimes called a network operating system.\nExamples of such add-on software include Phil Karn's KA9Q NOS (adding Internet support to CP\/M and MS-DOS), PC\/TCP\nPacket Drivers (adding Ethernet and Internet support to MS-DOS), and LANtastic (for MS-DOS, Microsoft Windows and OS\/2),\nand Windows for Workgroups (adding NetBIOS to Windows). Examples of early operating systems with peer-to-peer networking\ncapabilities built-in include MacOS (using AppleTalk and LocalTalk), and the Berkeley Software Distribution.\nToday, distributed computing and groupware applications have become the norm. Computer operating systems include a\nnetworking stack as a matter of course. During the 1980s the need to integrate dissimilar computers with network capabilities grew\nand the number of networked devices grew rapidly. Partly because it allowed for multi-vendor interoperability, and could route\npackets globally rather than being restricted to a single building, the Internet protocol suite became almost universally adopted in\nnetwork architectures. Thereafter, computer operating systems and the firmware of network devices tended to support Internet\nprotocols.\nFigure 2.2.1.1: Network Operating System. (\"Network Operating System\" by akash1295, Geeks for Geeks is licensed under CC\nBY-SA 4.0)\nAdvantages of Network Operating System:\nHighly stable centralized servers\nSecurity concerns are handled through servers\nNew technologies and hardware up-gradation are easily integrated to the system\nServer access are possible remotely from different locations and types of systems\nDisadvantages of Network Operating System:\nServers are costly\nUser has to depend on central location for most operations\nMaintenance and updates are required regularly\nExamples of Network Operating System are: Microsoft Windows Server 2003, Microsoft Windows Server 2008, UNIX, Linux,\nMac OS X, Novell NetWare, and BSD etc.\n2.2.1.2 https:\/\/eng.libretexts.org\/@go\/page\/45625 Adapted from:\n\"Types of Operating Systems\" by akash1295, Geeks for Geeks is licensed under CC BY-SA 4.0\n\"Network operating system\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\nThis page titled 2.2.1: Types of Operating Systems (continued) is shared under a CC BY-SA license and was authored, remixed, and\/or curated by\nPatrick McClanahan.\n2.2.1.3 https:\/\/eng.libretexts.org\/@go\/page\/45625 2.2.2: Types of Operating Systems (continued)\n5. Real-Time Operating System\nThese types of OSs are used in real-time systems. The time interval required to process and respond to inputs is very small. This\ntime interval is called response time.\nReal-time systems are used when there are time requirements are very strict like missile systems, air traffic control systems, robots\netc.\nTwo types of Real-Time Operating System which are as follows:\nHard Real-Time Systems:\nThese OSs are meant for the applications where time constraints are very strict and even the shortest possible delay is not\nacceptable. These systems are built for saving life like automatic parachutes or air bags which are required to be readily\navailable in case of any accident. Virtual memory is almost never found in these systems.\nSoft Real-Time Systems:\nThese OSs are for applications where for time-constraint is less strict.\nFigure 2.2.2.1: Real Time Operating System. (\"Real time operating system\" by akash1295, Geeks for Geeks is licensed under CC\nBY-SA 4.0)\nAdvantages of RTOS:\nMaximum Consumption: Maximum utilization of devices and system,thus more output from all the resources\nTask Shifting: Time assigned for shifting tasks in these systems are very less. For example in older systems it takes about 10\nmicro seconds in shifting one task to another and in latest systems it takes 3 micro seconds.\nFocus on Application: Focus on running applications and less importance to applications which are in queue.\nReal time operating system in embedded system: Since size of programs are small, RTOS can also be used in embedded\nsystems like in transport and others.\nError Free: These types of systems MUST be able to deal with any exceptions, so they are not really error free, but handle\nerror conditions without halting the system.\nMemory Allocation: Memory allocation is best managed in these type of systems.\nDisadvantages of RTOS:\nLimited Tasks: Very few tasks run at the same time and their concentration is very less on few applications to avoid errors.\nUse heavy system resources: Sometimes the system resources are not so good and they are expensive as well.\nComplex Algorithms: The algorithms are very complex and difficult for the designer to write on.\nDevice driver and interrupt signals: It needs specific device drivers and interrupt signals to response earliest to interrupts.\nThread Priority: It is not good to set thread priority as these systems are very less prone to switching tasks.\n2.2.2.1 https:\/\/eng.libretexts.org\/@go\/page\/45626 Examples of Real-Time Operating Systems are: Scientific experiments, medical imaging systems, industrial control systems,\nweapon systems, robots, air traffic control systems, etc.\nAdapted from:\n\"Types of Operating Systems\" by akash1295, Geeks for Geeks is licensed under CC BY-SA 4.0\nThis page titled 2.2.2: Types of Operating Systems (continued) is shared under a CC BY-SA license and was authored, remixed, and\/or curated by\nPatrick McClanahan.\n2.2.2.2 https:\/\/eng.libretexts.org\/@go\/page\/45626 2.3: Difference between multitasking, multithreading and multiprocessing\nMulti-programming\nIn a modern computing system, there are usually several concurrent application processes which want to execute. It is the\nresponsibility of the operating system to manage all the processes effectively and efficiently. One of the most important aspects of\nan operating system is to provide the capability to multi-program.\nIn a computer system, there are multiple processes waiting to be executed, i.e. they are waiting while the CPU is allocated to other\nprocesses. The main memory is too small to accommodate all of these processes or jobs. Thus, these processes are initially kept in\nan area called job pool. This job pool consists of all those processes awaiting allocation of main memory and CPU.\nThe scheduler selects a job out of the job pool, brings it into main memory and begins executing it. The processor executes one job\nuntil one of several factors interrupt its processing: 1) the process uses up its allotted time; 2) some other interrupt (we will talk\nmore about interrupts) causes the processor to stop executing this process; 3) the process goes into a wait state waiting on an I\/O\nrequest.\nNon-multi-programmed system concepts:\nIn a non multi-programmed system, as soon as one job hits any type of interrupt or wait state, the CPU becomes idle. The CPU\nkeeps waiting and waiting until this job (which was executing earlier) comes back and resumes its execution with the CPU. So\nCPU remains idle for a period of time.\nThere are drawbacks when the CPU remains idle for a very long period of time. Other jobs which are waiting for the\nprocessor will not get a chance to execute because the CPU is still allocated to the job that is in a wait state.\nThis poses a very serious problem - even though other jobs are ready to execute, the CPU is not available to them because it is\nstill allocated to a job which is not even utilizing it.\nIt is possible that one job is using the CPU for an extended period of time, while other jobs sit in the queue waiting for access\nto the CPU. In order to work around such scenarios like this the concept of multi-programming developed to increase the CPU\nutilization and thereby the overall efficiency of the system.\nThe main idea of multi-programming is to maximize the CPU time.\nMulti-programmed system concepts:\nIn a multi-programmed system, as soon as one job goes gets interrupted or goes into a wait state, the cpu selects the next job\nfrom the scheduler and starts its execution. Once the previous job resolves the reason for its interruption - perhaps the I\/O\ncompletes - goes back into the job pool. If the second job goes into a wait state, the CPU chooses a third job and starts\nexecuting it.\nThis makes for much more efficient use of the CPU. Therefore, the ultimate goal of multi-programming is to keep the CPU\nbusy as long as there are processes ready to execute. This way, multiple programs can be executed on a single processor by\nexecuting a part of a program at one time, a part of another program after this, then a part of another program and so on, hence\nexecuting multiple programs\nIn the image below, program A runs for some time and then goes to waiting state. In the mean time program B begins its\nexecution. So the CPU does not waste its resources and gives program B an opportunity to run. There is still time slots where\nthe processor is waiting - other programs could be run if necessary.\n2.3.1 https:\/\/eng.libretexts.org\/@go\/page\/45630 Figure 2.3.1: Multiprogramming. (\"Multiprogramming\" by Darshan L., Geeks for Geeks is licensed under CC BY-SA 4.0)\nAdapted from:\n\"Difference between Multiprogramming, multitasking, multithreading and multiprocessing\" by Darshan L., Geeks for Geeks is\nlicensed under CC BY-SA 4.0\nThis page titled 2.3: Difference between multitasking, multithreading and multiprocessing is shared under a CC BY-SA license and was authored,\nremixed, and\/or curated by Patrick McClanahan.\n2.3.2 https:\/\/eng.libretexts.org\/@go\/page\/45630 2.3.1: Difference between multitasking, multithreading and multiprocessing\n(continued)\n2. Multiprocessing\nIn a uni-processor system, only one process executes at a time. Multiprocessing makes use of two or more CPUs (processors)\nwithin a single computer system. The term also refers to the ability of a system to support more than one processor within a single\ncomputer system. Since there are multiple processors available, multiple processes can be executed at a time. These\nmultiprocessors share the computer bus, sometimes the clock, memory and peripheral devices also.\nMultiprocessing system\u2019s working \u2013\nWith the help of multiprocessing, many processes can be executed simultaneously. Say processes P1, P2, P3 and P4 are waiting\nfor execution. Now in a single processor system, firstly one process will execute, then the other, then the other and so on.\nBut with multiprocessing, each process can be assigned to a different processor for its execution. If its a dual-core processor (2\nprocessors), two processes can be executed simultaneously and thus will be two times faster, similarly a quad core processor\nwill be four times as fast as a single processor.\nWhy use multiprocessing\nThe main advantage of multiprocessor system is to get more work done in a shorter period of time. These types of systems are\nused when very high speed is required to process a large volume of data. multiprocessing systems can save money in\ncomparison to single processor systems because the processors can share peripherals and power supplies.\nIt also provides increased reliability in that if one processor fails, the work does not halt, it only slows down. e.g. if we have 10\nprocessors and 1 fails, then the work does not halt, rather the remaining 9 processors can share the work of the 10th processor.\nThus the whole system runs only 10 percent slower, rather than failing altogether\nMultiprocessing refers to the hardware (i.e., the CPU units) rather than the software (i.e., running processes). If the underlying\nhardware provides more than one processor then that is multiprocessing. It is the ability of the system to leverage multiple\nprocessors\u2019 computing power.\nDifference between multiprogramming and multiprocessing\nA system can be both multi programmed by having multiple programs running at the same time and multiprocessing by having\nmore than one physical processor. The difference between multiprocessing and multi programming is that Multiprocessing is\nbasically executing multiple processes at the same time on multiple processors, whereas multi programming is keeping several\nprograms in main memory and executing them concurrently using a single CPU only.\nMultiprocessing occurs by means of parallel processing whereas Multi programming occurs by switching from one process to\nother (phenomenon called as context switching).\n3. Multitasking\nAs the name itself suggests, multitasking refers to execution of multiple tasks (say processes, programs, threads etc.) at a time. In\nthe modern operating systems, we are able to play MP3 music, edit documents in Microsoft Word, surf the Google Chrome all\nsimultaneously, this is accomplished by means of multitasking.\nMultitasking is a logical extension of multi programming. The major way in which multitasking differs from multi programming is\nthat multi programming works solely on the concept of context switching whereas multitasking is based on time sharing alongside\nthe concept of context switching.\nMulti tasking system\u2019s concepts\nIn a time sharing system, each process is assigned some specific quantum of time for which a process is meant to execute. Say\nthere are 4 processes P1, P2, P3, P4 ready to execute. So each of them are assigned some time quantum for which they will\nexecute e.g time quantum of 5 nanoseconds (5 ns). As one process begins execution (say P2), it executes for that quantum of\ntime (5 ns). After 5 ns the CPU starts the execution of the other process (say P3) for the specified quantum of time.\nThus the CPU makes the processes to share time slices between them and execute accordingly. As soon as time quantum of one\nprocess expires, another process begins its execution.\n2.3.1.1 https:\/\/eng.libretexts.org\/@go\/page\/45628 Here also basically a context switch is occurring but it is occurring so fast that the user is able to interact with each program\nseparately while it is running. This way, the user is given the illusion that multiple processes\/ tasks are executing\nsimultaneously. But actually only one process\/ task is executing at a particular instant of time. In multitasking, time sharing is\nbest manifested because each running process takes only a fair quantum of the CPU time.\nIn a more general sense, multitasking refers to having multiple programs, processes, tasks, threads running at the same time. This\nterm is used in modern operating systems when multiple tasks share a common processing resource (e.g., CPU and Memory).\nFigure 2.3.1.1: Depiction of Multitasking System. (\"Multitasking\" by Darshan L., Geeks for Geeks is licensed under CC BY-SA\n4.0)\nAs depicted in the above image, At any time the CPU is executing only one task while other tasks are waiting for their turn. The\nillusion of parallelism is achieved when the CPU is reassigned to another task. i.e all the three tasks A, B and C are appearing to\noccur simultaneously because of time sharing.\nSo for multitasking to take place, firstly there should be multiprogramming i.e. presence of multiple programs ready for execution.\nAnd secondly the concept of time sharing.\nAdapted from:\n\"Difference between Multiprogramming, multitasking, multithreading and multiprocessing\" by Darshan L., Geeks for Geeks is\nlicensed under CC BY-SA 4.0\nThis page titled 2.3.1: Difference between multitasking, multithreading and multiprocessing (continued) is shared under a CC BY-SA license and\nwas authored, remixed, and\/or curated by Patrick McClanahan.\n2.3.1.2 https:\/\/eng.libretexts.org\/@go\/page\/45628 2.3.2: Difference between Multiprogramming, multitasking, multithreading and\nmultiprocessing (continued)\nMultithreading\nA thread is a basic unit of CPU utilization. Multithreading is an execution model that allows a single process to have multiple code\nsegments (i.e., threads) running concurrently within the \u201ccontext\u201d of that process.\ne.g. VLC media player, where one thread is used for opening the VLC media player, one thread for playing a particular song and\nanother thread for adding new songs to the playlist.\nMultithreading is the ability of a process to manage its use by more than one user at a time and to manage multiple requests by the\nsame user without having to have multiple copies of the program.\nMultithreading system examples\nExample 1\nSay there is a web server which processes client requests. Now if it executes as a single threaded process, then it will not be\nable to process multiple requests at a time. First one client will make its request and finish its execution and only then, the\nserver will be able to process another client request. This is quite inefficient, time consuming and tiring task. To avoid this, we\ncan take advantage of multithreading.\nNow, whenever a new client request comes in, the web server simply creates a new thread for processing this request and\nresumes its execution to process more client requests. So the web server has the task of listening to new client requests and\ncreating threads for each individual request. Each newly created thread processes one client request, thus reducing the burden\non web server.\nExample 2\nWe can think of threads as child processes that share the parent process resources but execute independently. Take the case of a\nGUI. Say we are performing a calculation on the GUI (which is taking very long time to finish). Now we can not interact with\nthe rest of the GUI until this command finishes its execution. To be able to interact with the rest of the GUI, this calculation\nshould be assigned to a separate thread. So at this point of time, 2 threads will be executing i.e. one for calculation, and one for\nthe rest of the GUI. Hence here in a single process, we used multiple threads for multiple functionality.\nThe image helps to describe the VLC player example:\nFigure 2.3.2.1: Example of Multithreading. (\"Multithreading\" by Darshan L., Geeks for Geeks is licensed under CC BY-SA 4.0)\nAdvantages of multithreading\nBenefits of multithreading include increased responsiveness. Since there are multiple threads in a program, so if one thread is\ntaking too long to execute or if it gets blocked, the rest of the threads keep executing without any problem. Thus the whole\nprogram remains responsive to the user by means of remaining threads.\n2.3.2.1 https:\/\/eng.libretexts.org\/@go\/page\/45629 Another advantage of multithreading is that it is less costly. Creating brand new processes and allocating resources is a time\nconsuming task, but since threads share resources of the parent process, creating threads and switching between them is\ncomparatively easy. Hence multithreading is the need of modern Operating Systems.\nAdapted from:\n\"Difference between Multiprogramming, multitasking, multithreading and multiprocessing\" by Darshan L., Geeks for Geeks is\nlicensed under CC BY-SA 4.0\nThis page titled 2.3.2: Difference between Multiprogramming, multitasking, multithreading and multiprocessing (continued) is shared under a CC\nBY-SA license and was authored, remixed, and\/or curated by Patrick McClanahan.\n2.3.2.2 https:\/\/eng.libretexts.org\/@go\/page\/45629 CHAPTER OVERVIEW\n3: Processes - What and How\n3.1: Processes\n3.2: Process States\n3.3 Process Description\n3.4: Process Control\n3.5: Execution within the Operating System\n3.5.1 : Execution within the Operating System - Dual Mode\nThis page titled 3: Processes - What and How is shared under a CC BY-SA license and was authored, remixed, and\/or curated by Patrick\nMcClanahan.\n1 3.1: Processes\nWhat is a Process\nIn computing, a process is the instance of a computer program that is being executed by one or many threads. It contains the\nprogram code and its activity. Depending on the operating system (OS), a process may be made up of multiple threads of execution\nthat execute instructions concurrently.\nWhile a computer program is a passive collection of instructions, a process is the actual execution of those instructions. Several\nprocesses may be associated with the same program; for example, opening up several instances of the same program often results\nin more than one process being executed.\nMultitasking is a method to allow multiple processes to share processors (CPUs) and other system resources. Each CPU (core)\nexecutes a single task at a time. However, multitasking allows each processor to switch between tasks that are being executed\nwithout having to wait for each task to finish (preemption). Depending on the operating system implementation, switches could be\nperformed when tasks initiate and wait for completion of input\/output operations, when a task voluntarily yields the CPU, on\nhardware interrupts, and when the operating system scheduler decides that a process has expired its fair share of CPU time (e.g, by\nthe Completely Fair Scheduler of the Linux kernel).\nA common form of multitasking is provided by CPU's time-sharing that is a method for interleaving the execution of users\nprocesses and threads, and even of independent kernel tasks - although the latter feature is feasible only in preemptive kernels such\nas Linux. Preemption has an important side effect for interactive process that are given higher priority with respect to CPU bound\nprocesses, therefore users are immediately assigned computing resources at the simple pressing of a key or when moving a mouse.\nFurthermore, applications like video and music reproduction are given some kind of real-time priority, preempting any other lower\npriority process. In time-sharing systems, context switches are performed rapidly, which makes it seem like multiple processes are\nbeing executed simultaneously on the same processor. This simultaneous execution of multiple processes is called concurrency.\nFor security and reliability, most modern operating systems prevent direct communication between independent processes,\nproviding strictly mediated and controlled inter-process communication functionality.\nFigure 3.1.1: Output of htop Linux Command. (PER9000, Per Erik Strandberg, CC BY-SA 3.0, via Wikimedia Commons)\nChild process\nA child process in computing is a process created by another process (the parent process). This technique pertains to multitasking\noperating systems, and is sometimes called a subprocess or traditionally a subtask.\nA child process inherits most of its attributes (described above), such as file descriptors, from its parent. In Linux, a child process is\ntypically created as a copy of the parent. The child process can then overlay itself with a different program as required.\nEach process may create many child processes but will have at most one parent process; if a process does not have a parent this\nusually indicates that it was created directly by the kernel. In some systems, including Linux-based systems, the very first process\nis started by the kernel at booting time and never terminates; other parentless processes may be launched to carry out various tasks\n3.1.1 https:\/\/eng.libretexts.org\/@go\/page\/45993 in userspace. Another way for a process to end up without a parent is if its parent dies, leaving an orphan process; but in this case it\nwill shortly be adopted by the main process.\nRepresentation\nIn general, a computer system process consists of (or is said to own) the following resources:\nOperating system descriptors of resources that are allocated to the process, such as file descriptors (Unix terminology) or\nhandles (Windows), and data sources and sinks.\nSecurity attributes, such as the process owner and the process' set of permissions (allowable operations).\nProcessor state (context), such as the content of registers and physical memory addressing. The state is typically stored in\ncomputer registers when the process is executing, and in memory otherwise.\nThe operating system holds most of this information about active processes in data structures called process control blocks. Any\nsubset of the resources, typically at least the processor state, may be associated with each of the process' threads in operating\nsystems that support threads or child processes.\nThe operating system keeps its processes separate and allocates the resources they need, so that they are less likely to interfere with\neach other and cause system failures (e.g., deadlock or thrashing). The operating system may also provide mechanisms for inter-\nprocess communication to enable processes to interact in safe and predictable ways.\nAttributes or Characteristics of a Process\nA process has following attributes.\n1. Process Id: A unique identifier assigned by the operating system\n2. Process State: Can be ready, running, etc.\n3. CPU registers: Like the Program Counter (CPU registers must be saved and restored w\n4. I\/O status information: For example, devices allocated to the process,\nopen files, etc\n5. CPU scheduling information: For example, Priority (Different processes may have dif\na short process may be assigned a low priority\nin the shortest job first scheduling)\n6. Various accounting information\nAll of the above attributes of a process are also known as the context of the process.\nContext Switching\nThe process of saving the context of one process and loading the context of another process is known as Context Switching. In\nsimple terms, it is like loading and unloading the process from running state to ready state.\nWhen does context switching happen?\n1. When a high-priority process comes to ready state (i.e. with higher priority than the running process)\n2. An Interrupt occurs\n3. User and kernel mode switch (It is not necessary though)\n4. Preemptive CPU scheduling used.\nContext Switch vs Mode Switch\nA mode switch occurs when CPU privilege level is changed, for example when a system call is made or a fault occurs. The kernel\nworks in more a privileged mode than a standard user task. If a user process wants to access things which are only accessible to the\nkernel, a mode switch must occur. The currently executing process need not be changed during a mode switch.\nA mode switch typically occurs for a process context switch to occur. Only the kernel can cause a context switch.\nCPU-Bound vs I\/O-Bound Processes\nA CPU-bound process requires more CPU time or spends more time in the running state.\nAn I\/O-bound process requires more I\/O time and less CPU time. An I\/O-bound process spends more time in the waiting state.\n3.1.2 https:\/\/eng.libretexts.org\/@go\/page\/45993 Adapted from:\n\"Process (computing)\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n\"Introduction of Process Management\" by SarthakSinghal1, Geeks for Geeks is licensed under CC BY-SA 4.0\n\"Child process\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n3.1: Processes is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n3.1.3 https:\/\/eng.libretexts.org\/@go\/page\/45993 3.2: Process States\nProcess state\nIn a multitasking computer system, processes may occupy a variety of states. These distinct states may not be recognized as such\nby the operating system kernel. However, they are a useful abstraction for the understanding of processes.\nPrimary process states\nIf we look at the following diagram there are several things to note:\nMemory:\nWe have some states of a process where the process is kept in main memory - RAM.\nWe have some states that are stored in secondary memory - that is what we call swap space and is actually part of the hard\ndisk set aside for use by the operating system.\nThere are numerous actions: Admint, dispatch, Event wait, Event occur, Suspend, Activate, Timeout, and Release. These all\nhave an impact on the process during its lifetime.\nIn the following diagram there are 7 states. Depending on who the author is there may be 5, 6 or 7. Sometimes the 2 Suspended\nstates are not shown, but we shown them for clarity here.\nLet's follow a process through a lifecycle.\n1. A new process gets created. For example, a user opens up a word processor, this requires a new process.\n2. Once the process has completed its initialization, it is placed in a Ready state with all of the other processes waiting to take its\nturn on the processor.\n3. When the process's turn comes, it is dispatched to the Running state and executes on the processor. From here one of 3 things\nhappens:\n1. the process completes and is Released and moves to the Exit state\n2. it uses up it turn and so Timeout and is returned to the Ready state\n3. some event happens, such as waiting for user input, and it is moved to the Blocked state.\n1. In the Blocked state, once the event it is waiting on occurs, it can return to the Ready state.\n2. If the event doesn't occur for an extended period of time, the process can get moved to the Suspended blocked state.\nSince it is suspended it is now in virtual memory - which is actually disk space set aside for temporary storage.\n4. Once the event this process is waiting on does occur it is moved to the Suspended ready state, then waits to get moved from\nsecondary storage, into main memory in the Ready state.\n1. On very busy systems processes can get moved from the Ready state to the Suspended ready state as well.\n5. Eventually every process will Exit.\nThe following typical process states are possible on computer systems of all kinds. In most of these states, processes are \"stored\"\non main memory.\n3.2.1 https:\/\/eng.libretexts.org\/@go\/page\/45994 Figure 3.2.1: States of a Process. (\"States of a Process\" by Aniket_Dusey, Geeks for Geeks is licensed under CC BY-SA 4.0)\nNew or Created\nWhen a process is first created, it occupies the \"created\" or \"new\" state. In this state, the process awaits admission to the \"ready\"\nstate. Admission will be approved or delayed by a long-term, or admission, scheduler. Typically in most desktop computer systems,\nthis admission will be approved automatically. However, for real-time operating systems this admission may be delayed. In a\nrealtime system, admitting too many processes to the \"ready\" state may lead to oversaturation and overcontention of the system's\nresources, leading to an inability to meet process deadlines.\nOperating systems need some ways to create processes. In a very simple system designed for running only a single application\n(e.g., the controller in a microwave oven), it may be possible to have all the processes that will ever be needed be present when the\nsystem comes up. In general-purpose systems, however, some way is needed to create and terminate processes as needed during\noperation.\nThere are four principal events that cause a process to be created:\nSystem initialization.\nExecution of process creation system call by a running process.\nA user request to create a new process.\nInitiation of a batch job.\nWhen an operating system is booted, typically several processes are created. Some of these are foreground processes, that interact\nwith a (human) user and perform work for them. Others are background processes, which are not associated with particular users,\nbut instead have some specific function. For example, one background process may be designed to accept incoming e-mails,\nsleeping most of the day but suddenly springing to life when an incoming e-mail arrives. Another background process may be\ndesigned to accept an incoming request for web pages hosted on the machine, waking up when a request arrives to service that\nrequest.\nThere are several steps involved in process creation. The first step is the validation of whether the parent process has sufficient\nauthorization to create a process. Upon successful validation, the parent process is copied almost entirely, with changes only to the\nunique process id, parent process, and user-space. Each new process gets its own user space.\nReady\nA \"ready\" or \"waiting\" process has been loaded into main memory and is awaiting execution on a CPU (to be context switched\nonto the CPU by the dispatcher, or short-term scheduler). There may be many \"ready\" processes at any one point of the system's\nexecution\u2014for example, in a one-processor system, only one process can be executing at any one time, and all other \"concurrently\nexecuting\" processes will be waiting for execution.\n3.2.2 https:\/\/eng.libretexts.org\/@go\/page\/45994 A ready queue or run queue is used in computer scheduling. Modern computers are capable of running many different programs or\nprocesses at the same time. However, the CPU is only capable of handling one process at a time. Processes that are ready for the\nCPU are kept in a queue for \"ready\" processes. Other processes that are waiting for an event to occur, such as loading information\nfrom a hard drive or waiting on an internet connection, are not in the ready queue.\nRunning\nA process moves into the running state when it is chosen for execution. The process's instructions are executed by one of the CPUs\n(or cores) of the system. There is at most one running process per CPU or core. A process can run in either of the two modes,\nnamely kernel mode or user mode.\nKernel mode\nProcesses in kernel mode can access both: kernel and user addresses.\nKernel mode allows unrestricted access to hardware including execution of privileged instructions.\nVarious instructions (such as I\/O instructions and halt instructions) are privileged and can be executed only in kernel mode.\nA system call from a user program leads to a switch to kernel mode.\nUser mode\nProcesses in user mode can access their own instructions and data but not kernel instructions and data (or those of other\nprocesses).\nWhen the computer system is executing on behalf of a user application, the system is in user mode. However, when a user\napplication requests a service from the operating system (via a system call), the system must transition from user to kernel mode\nto fulfill the request.\nUser mode avoids various catastrophic failures:\nThere is an isolated virtual address space for each process in user mode.\nUser mode ensures isolated execution of each process so that it does not affect other processes as such.\nNo direct access to any hardware device is allowed.\nBlocked\nA process transitions to a blocked state when it is waiting for some event, such as a resource becoming available or the completion\nof an I\/O operation. In a multitasking computer system, individual processes, must share the resources of the system. Shared\nresources include: the CPU, network and network interfaces, memory and disk.\nFor example, a process may block on a call to an I\/O device such as a printer, if the printer is not available. Processes also\ncommonly block when they require user input, or require access to a critical section which must be executed atomically.\nSuspend ready\nProcess that was initially in the ready state but were swapped out of main memory(refer Virtual Memory topic) and placed onto\nexternal storage by scheduler are said to be in suspend ready state. The process will transition back to ready state whenever the\nprocess is again brought onto the main memory.\nSuspend wait or suspend blocked\nSimilar to suspend ready but uses the process which was performing I\/O operation and lack of main memory caused them to move\nto secondary memory.\nWhen work is finished it may go to suspend ready.\nTerminated\nA process may be terminated, either from the \"running\" state by completing its execution or by explicitly being killed. In either of\nthese cases, the process moves to the \"terminated\" state. The underlying program is no longer executing, but the process remains in\nthe process table as a zombie process until its parent process calls the wait system call to read its exit status, at which point the\nprocess is removed from the process table, finally ending the process's lifetime. If the parent fails to call wait, this continues to\nconsume the process table entry (concretely the process identifier or PID), and causes a resource leak.\nThere are many reasons for process termination:\nBatch job issues halt instruction\nUser logs off\nProcess executes a service request to terminate\n3.2.3 https:\/\/eng.libretexts.org\/@go\/page\/45994 Error and fault conditions\nNormal completion\nTime limit exceeded\nMemory unavailable\nBounds violation; for example: attempted access of (non-existent) 11th element of a 10-element array\nProtection error; for example: attempted write to read-only file\nArithmetic error; for example: attempted division by zero\nTime overrun; for example: process waited longer than a specified maximum for an event\nI\/O failure\nInvalid instruction; for example: when a process tries to execute data (text)\nPrivileged instruction\nData misuse\nOperating system intervention; for example: to resolve a deadlock\nParent terminates so child processes terminate (cascading termination)\nParent request\nAdapted from:\n\"States of a Process in Operating Systems\" by Aniket_Dusey, Geeks for Geeks is licensed under CC BY-SA 4.0\n3.2: Process States is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n3.2.4 https:\/\/eng.libretexts.org\/@go\/page\/45994 3.3 Process Description\nSystem Processes \/ System Resources\nIn a multiprocessing system, there are numerous processes competing to the system's resources. As each process takes its turn\nexecuting in the processor, the state of the other processes must be kept in the state that they were interrupted at so as to restore the\nnext process to execute.\nProcesses run, make use of I\/O resources, consume memory. At times processes block waiting for I\/O, allowing other processes to\nrun on the processors. Some processes are swapped out in order to make room in physical memory for other processes' needs. P is\n1\nrunning, accessing I\/O and memory. P is blocked waiting for P1 to complete I\/O. P is swapped out waiting to return to main\n2 n\nmemory and further processing.\nFigure 1: Processes and Resources. (Unknown source)\nProcess description and control\nAs the operating system manages processes and resources, it must maintain information about the current status of each process\nand the resources in use. The approach to maintaining this information is for the operating system to construct and maintain various\ntables of information about each entity to be managed.\nOperating system maintains four internal components: 1) memory; 2) devices; 3) files; and 4) processes. Details differ from one\noperating system to another, but all operating systems maintain information in these four categories.\nMemory tables: Memory is central to the operation of a modern computer system. Memory is a large array of words or\nbytes, each with its own address. Interaction is achieved through a sequence of reads or writes of specific memory\naddress. The CPU fetches from and stores in memory.\nIn order to improve both the utilization of CPU and the speed of the computer's response to its users, several\nprocesses must be kept in memory. There are many different algorithms depends on the particular situation. Selection\nof a memory management scheme for a specific system depends upon many factor, but especially upon the hardware\ndesign of the system. Each algorithm requires its own hardware support.\nThe operating system is responsible for the following activities in connection with memory management.\nKeep track of which parts of memory are currently being used and by whom.\nDecide which processes are to be loaded into memory when memory space becomes available.\nAllocate and deallocate memory space as needed.\nWe will cover memory management in a later section.\n1 https:\/\/eng.libretexts.org\/@go\/page\/46441 Device tables: One of the purposes of an operating system is to hide the peculiarities of specific hardware devices from\nthe user. For example, in Linux, the peculiarities of I\/O devices are hidden from the bulk of the operating system itself\nby the I\/O system. The I\/O system consists of:\nA buffer caching system\nA general device driver code\nDrivers for specific hardware devices.\nOnly the device driver knows the peculiarities of a specific device. We will cover the details later in this course\nFile tables: File management is one of the most visible services of an operating system. Computers can store\ninformation in several different physical forms; disk - both magnetic disks and newer SSD devices, various USB\ndevices, and to the cloud. Each of these devices has it own characteristics and physical organization.\nFor convenient use of the computer system, the operating system provides a uniform logical view of information\nstorage. The operating system abstracts from the physical properties of its storage devices to define a logical storage\nunit, the file. Files are mapped, by the operating system, onto physical devices.\nA file is a collection of related information defined by its creator. Commonly, files represent programs (both source and\nobject forms) and data. Data files may be numeric, alphabetic or alphanumeric. Files may be free-form, such as text\nfiles, or may be rigidly formatted. In general a files is a sequence of bits, bytes, lines or records whose meaning is\ndefined by its creator and user. It is a very general concept.\nThe operating system implements the abstract concept of the file by managing mass storage device, such as types and\ndisks. Also files are normally organized into directories to ease their use. Finally, when multiple users have access to\nfiles, it may be desirable to control by whom and in what ways files may be accessed.\nThe operating system is responsible for the following activities in connection with file management:\nThe creation and deletion of files\nThe creation and deletion of directory\nThe support of primitives for manipulating files and directories\nThe mapping of files onto disk storage.\nBackup of files on stable (non volatile) storage.\nThis portion of the operating system will also be dealt with later.\nProcesses: The CPU executes a large number of programs. While its main concern is the execution of user programs,\nthe CPU is also needed for other system activities. These activities are called processes. A process is a program in\nexecution. Typically, a batch job is a process. A time-shared user program is a process. A system task, such as\nspooling, is also a process. For now, a process may be considered as a job or a time-shared program, but the concept\nis actually more general.\nIn general, a process will need certain resources such as CPU time, memory, files, I\/O devices, etc., to accomplish its\ntask. These resources are given to the process when it is created. In addition to the various physical and logical\nresources that a process obtains when its is created, some initialization data (input) may be passed along. For\nexample, a process whose function is to display on the screen of a terminal the status of a file, say F1, will get as an\ninput the name of the file F1 and execute the appropriate program to obtain the desired information.\nWe emphasize that a program by itself is not a process; a program is a passive entity, while a process is an active\nentity. It is known that two processes may be associated with the same program, they are nevertheless considered two\nseparate execution sequences.\nA process is the unit of work in a system. Such a system consists of a collection of processes, some of which are\noperating system processes, those that execute system code, and the rest being user processes, those that execute\nuser code. All of those processes can potentially execute concurrently.\nThe operating system is responsible for the following activities in connection with processes managed.\n2 https:\/\/eng.libretexts.org\/@go\/page\/46441 The creation and deletion of both user and system processes\nThe suspension are resumption of processes.\nThe provision of mechanisms for process synchronization\nThe provision of mechanisms for deadlock handling\n3.3 Process Description is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n3 https:\/\/eng.libretexts.org\/@go\/page\/46441 3.4: Process Control\nProcess Table and Process Control Block (PCB)\nWhile creating a process the operating system performs several operations. To identify the processes, it assigns a process\nidentification number (PID) to each process. As the operating system supports multi-programming, it needs to keep track of all the\nprocesses. For this task, the process control block (PCB) is used to track the process\u2019s execution status. Each block of memory\ncontains information about the process state, program counter, stack pointer, status of opened files, scheduling algorithms, etc. All\nthese information is required and must be saved when the process is switched from one state to another. When the process makes a\ntransition from one state to another, the operating system must update information in the process\u2019s PCB.\nA process control block (PCB) contains information about the process, i.e. registers, quantum, priority, etc. The process table is an\narray of PCB\u2019s, that means logically contains a PCB for all of the current processes in the system.\nFigure 3.4.1: Process Control Block. (\"Process Control Block\" by magbene, Geeks for Geeks is licensed under CC BY-SA 4.0)\nProcess scheduling state \u2013The state of the process in terms of \"ready\", \"suspended\", etc., and other scheduling information as\nwell, such as priority value, the amount of time elapsed since the process gained control of the CPU or since it was suspended.\nAlso, in case of a suspended process, event identification data must be recorded for the event the process is waiting for.\nProcess structuring information \u2013 the process's children id's, or the id's of other processes related to the current one in some\nfunctional way, which may be represented as a queue, a ring or other data structures\nPointer \u2013 It is a stack pointer which is required to be saved when the process is switched from one state to another to retain the\ncurrent position of the process.\nProcess number \u2013 Every process is assigned with a unique id known as process ID or PID which stores the process identifier.\nProgram counter \u2013 It stores the counter which contains the address of the next instruction that is to be executed for the\nprocess.\nRegister \u2013 These are the CPU registers which includes: accumulator, base, registers and general purpose registers.\nMemory Management Information \u2013 This field contains the information about memory management system used by\noperating system. This may include the page tables, segment tables etc.\nOpen files list \u2013 This information includes the list of files opened for a process.\nInterprocess communication information \u2013 flags, signals and messages associated with the communication among\nindependent processes\nProcess Privileges \u2013 allowed\/disallowed access to system resources\nProcess State \u2013 new, ready, running, waiting, dead\nProcess Number (PID) \u2013 unique identification number for each process (also known as Process ID)\nProgram Counter (PC) \u2013 A pointer to the address of the next instruction to be executed for this process\nCPU Scheduling Information \u2013 information scheduling CPU time\nAccounting Information \u2013 amount of CPU used for process execution, time limits, execution ID etc.\n3.4.1 https:\/\/eng.libretexts.org\/@go\/page\/46484 I\/O Status Information \u2013 list of I\/O devices allocated to the process.\nFigure 3.4.1: Process Table and Process Control Block. (\"Process Table and Process Control Block\" by magbene, Geeks for\nGeeks is licensed under CC BY-SA 4.0)\nAdapted from:\n\"Process Table and Process Control Block (PCB)\" by magbene, Geeks for Geeks is licensed under CC BY-SA 4.0\n\"Process control block\" by ultiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n3.4: Process Control is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n3.4.2 https:\/\/eng.libretexts.org\/@go\/page\/46484 3.5: Execution within the Operating System\nThere are some concepts we need to clarify as we discuss the concept of execution within the operating system.\nWhat is the kernel\nThe kernel is a computer program at the core of a computer's operating system that has complete control over everything in the\nsystem. It is the \"portion of the operating system code that is always resident in memory\", and facilitates interactions between\nhardware and software components. On most systems, the kernel is one of the first programs loaded on startup (after the\nbootloader). It handles the rest of startup as well as memory, peripherals, and input\/output (I\/O) requests from software, translating\nthem into data-processing instructions for the central processing unit.\nIntroduction to System Call\nIn computing, a system call when a program program requests a service from the kernel of the operating system it is executed on.\nA system call is a way for programs to interact with the operating system. Application programs are NOT allowed to\nperform certain tasks, such as open a file, or create a new process. System calls provide the services of the operating system to the\napplication programs via Application Program Interface(API). It provides an interface between a process and operating system to\nallow user-level processes, that is the applications that users are running on the system, to request services of the operating system.\nSystem calls are the only entry points into the kernel system. All programs needing resources must use system calls.\nServices Provided by System Calls :\n1. Process creation and management\n2. Main memory management\n3. File Access, Directory and File system management\n4. Device handling(I\/O)\n5. Protection\n6. Networking, etc.\nTypes of System Calls : There are 5 different categories of system calls\n1. Process control: end, abort, create, terminate, allocate and free memory.\n2. File management: create, open, close, delete, read file etc.\n3. Device management\n4. Information maintenance\n5. Communication\nThe following are some examples of system calls in Windows and Linux. So, if a user is running a word processing tool, and wants\nto save the document - the word processor asks the operating system to create a file, or open a file, to save the current set of\nchanges. If the application has permission to write to the requested file then the operating system performs the task. Otherwise, the\noperating system returns a status telling the user they do not have permission to write to the requested file. This concept of user\nversus kernel allows the operating system to maintain a certain level of control.\nWindows Linux\nCreateProcess() fork()\nProcess Control ExitProcess() exit()\nWaitForSingleObject() wait()\nCreateFile() open()\nReadFile() read()\nFile Manipulation\nWriteFile() write()\nCloseHandle() close()\nSetConsoleMode() ioctl()\nDevice Manipulation ReadConsole() read()\nWriteConsole() write()\n3.5.1 https:\/\/eng.libretexts.org\/@go\/page\/46486 Windows Linux\nGetCurrentProcessID() getpid()\nInformation Maintenance SetTimer() alarm()\nSleep() sleep()\nCreatePipe() pipe()\nCommunication CreateFileMapping() shmget()\nMapViewOfFile() mmap()\nSetFileSecurity()\nProtection InitlializeSecurityDescriptor()\nSetSecurityDescriptorGroup()\nAdapted from:\n\"Kernel (operating system)\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n\"Introduction of System Call\" by Samit Mandal, Geeks for Geeks is licensed under CC BY-SA 4.0\n3.5: Execution within the Operating System is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n3.5.2 https:\/\/eng.libretexts.org\/@go\/page\/46486 3.5.1 : Execution within the Operating System - Dual Mode\nDual Mode Operations in an Operating System\nAn error in one program can adversely affect many processes, it might modify data of another program, or also can affect the\noperating system. For example, if a process stuck in infinite loop then this infinite loop could affect correct operation of other\nprocesses. So to ensure the proper execution of the operating system, there are two modes of operation:\nUser mode \u2013\nWhen the computer system run user applications like creating a text document or using any application program, then the system is\nin the user mode. When the user application requests for a service from the operating system or an interrupt occurs or system call,\nthen there will be a transition from user to kernel mode to fulfill the requests.\nGiven below image describes what happen when an interrupt occurs: (do not worry about the comments about the mode bit)\nFigure 3.5.1.1: User Process Makes System Call. (\"User Mode to Kernel Mode Switch\" by shivani.mittal, Geeks for Geeks is\nlicensed under CC BY-SA 4.0)\nKernel Mode \u2013\nWhile the system is running the certain processes operate in kernel mode because the processes needs access to operating system\ncalls. This provides protection by controlling which processes can access kernel mode operations. As shown in the diagram above,\nthe system will allow certain user mode processes to execute system calls by allowing the process to temporarily run in kernel\nmode. While in kernel mode the process is allowed to have direct access to all hardware and memory in the system (also called\nprivileged mode). If a user process attempts to run privileged instructions in user mode then it will treat instruction as illegal and\ntraps to OS. Some of the privileged instructions are:\n1. Handling system interrupts\n2. To switch from user mode to kernel mode.\n3. Management of I\/O devices.\nUser Mode and Kernel Mode Switching\nIn it\u2019s life span a process executes in user mode AND kernel mode. The user mode is normal mode where the process has limited\naccess. While the kernel mode is the privileged mode where the process has unrestricted access to system resources like hardware,\nmemory, etc. A process can access services like hardware I\/O by executing accessing kernel data in kernel mode. Anything related\nto process management, I\/O hardware management, and memory management requires process to execute in Kernel mode.\nThe kernel provides System Call Interface (SCI), which are entry points for user processes to enter kernel mode. System calls are\nthe only way through which a process can go into kernel mode from user mode. Below diagram explains user mode to kernel mode\ntransition in detail.\n3.5.1 .1 https:\/\/eng.libretexts.org\/@go\/page\/46487 Figure 3.5.1.1: User Mode to Kernel Mode Transition. (\"user mode to kernel mode transition\" by sandeepjainlinux, Geeks for\nGeeks is licensed under CC BY-SA 4.0)\nWhen in user mode, the application process makes a call to Glibc, which is a library used by software programmers. This call alerts\nthe operating system kernel that the application desires to do something that only the kernel has the privilege to do. The operating\nsystem\/kernel will check to ensure that this application process has the proper authority to perform the requested action. If it does\nhave the necessary permission - the operating system allows the operation to proceed, otherwise an error message is sent to the\nuser.\nWhy?\nYou may be wondering why do operating system designers go to all of this trouble of creating dual modes. Why not just allow\neverything to operate in kernel mode and save the over head of all this switching?\nThere are 2 main reasons:\n1. If everything were to run in a single mode we end up with the issue that Microsoft had in the earlier versions of Windows. If\na process were able to exploit a vulnerability that process then had the ability to control the system.\n2. There are certain conditions known as a trap, also known as an exception or a system fault, is typically caused by an exceptional\ncondition such as division by zero, invalid memory access etc. If the process is running in kernel mode such a trap situation can\ncrash the entire operating system. A process in user mode that encounters a trap situation it only crashes the user mode process.\nSo, the overhead of switching is acceptable to ensure a more stable, secure system.\nAdapted from:\n\"Dual Mode operations in OS\" by shivani.mittal, Geeks for Geeks is licensed under CC BY-SA 4.0\n\"User mode and Kernel mode Switching\" by sandeepjainlinux, Geeks for Geeks is licensed under CC BY-SA 4.0\n3.5.1 : Execution within the Operating System - Dual Mode is shared under a not declared license and was authored, remixed, and\/or curated by\nLibreTexts.\n3.5.1 .2 https:\/\/eng.libretexts.org\/@go\/page\/46487 CHAPTER OVERVIEW\n4: Threads\n4.1: Process and Threads\n4.2: Thread Types\n4.2.1: Thread Types - Models\n4.3: Thread Relationships\n4.4: Benefits of Multithreading\nThis page titled 4: Threads is shared under a CC BY-SA license and was authored, remixed, and\/or curated by Patrick McClanahan.\n1 4.1: Process and Threads\nDefinition\nIn computer science, a thread of execution is the smallest sequence of programmed instructions that can be managed independently\nby a scheduler, which is typically a part of the operating system. The implementation of threads and processes differs between\noperating systems, but in most cases a thread is a component of a process. Multiple threads can exist within one process, executing\nconcurrently and sharing resources such as memory, while different processes do not share these resources. In particular, the\nthreads of a process share its executable code and the values of its dynamically allocated variables and non-thread-local global\nvariables at any given time.\nDifference between Process and Thread\nProcess:\nProcess means any program is in execution. Process control block controls the operation of any process. Process control block\ncontains information about processes for example Process priority, process id, process state, CPU, register, etc. A process can\ncreates other processes which are known as Child Processes. Process takes more time to terminate and it is isolated means it does\nnot share memory with any other process.\nThe process can have the following states like new, ready, running, waiting, terminated, suspended.\nThread:\nThread is the segment of a process means a process can have multiple threads and these multiple threads are contained within a\nprocess. A thread have 3 states: running, ready, and blocked.\nThread takes less time to terminate as compared to process and like process threads do not isolate.\nDifference between Process and Thread:\nProcess Thread\n1. Process means any program is in execution. Thread means segment of a process.\n2. Process takes more time to terminate. Thread takes less time to terminate.\n3. It takes more time for creation. It takes less time for creation.\n4. It also takes more time for context switching. It takes less time for context switching.\n5. Process is less efficient in term of communication. Thread is more efficient in term of communication.\n6. Process consume more resources. Thread consume less resources.\n7. Process is isolated. Threads share memory.\n8. Process is called heavy weight process. Thread is called light weight process.\n9. Process switching uses interface in operating system. Thread switching does not require to call a operating system\nand cause an interrupt to the kernel.\n10. If one server process is blocked no other server process can Second thread in the same task could run, while one server\nexecute until the first process unblocked. thread is blocked.\n11. Process has its own Process Control Block, Stack and Address Thread has Parents\u2019 PCB, its own Thread Control Block and\nSpace. Stack and common Address space.\n4.1.1 https:\/\/eng.libretexts.org\/@go\/page\/46801 Threads in Operating Systems\nWhat is a Thread?\nA thread is a path of execution within a process. A process can contain multiple threads.\nWhy Multithreading?\nA thread is also known as lightweight process. The idea is to achieve parallelism by dividing a process into multiple threads. For\nexample, in a browser, multiple tabs can be different threads. MS Word uses multiple threads: one thread to format the text, another\nthread to process inputs, etc. More advantages of multithreading are discussed below\nProcess vs Thread?\nThe primary difference is that threads within the same process run in a shared memory space, while processes run in separate\nmemory spaces.\nThreads are not independent of one another like processes are, and as a result threads share with other threads their code section,\ndata section, and OS resources (like open files and signals). But, like process, a thread has its own program counter (PC), register\nset, and stack space.\nThreads vs. processes pros and cons\nThreads differ from traditional multitasking operating-system processes in several ways:\nprocesses are typically independent, while threads exist as subsets of a process\nprocesses carry considerably more state information than threads, whereas multiple threads within a process share process state\nas well as memory and other resources\nprocesses have separate address spaces, whereas threads share their address space\nprocesses interact only through system-provided inter-process communication m`1echanisms\ncontext switching between threads in the same process typically occurs faster than context switching between processes\nSystems such as Windows NT and OS\/2 are said to have cheap threads and expensive processes; in other operating systems there is\nnot so great a difference except in the cost of an address-space switch, which on some architectures (notably x86) results in a\ntranslation lookaside buffer (TLB) flush.\nAdvantages and disadvantages of threads vs processes include:\nLower resource consumption of threads: using threads, an application can operate using fewer resources than it would need\nwhen using multiple processes.\nSimplified sharing and communication of threads: unlike processes, which require a message passing or shared memory\nmechanism to perform inter-process communication (IPC), threads can communicate through data, code and files they already\nshare.\nThread crashes a process: due to threads sharing the same address space, an illegal operation performed by a thread can crash\nthe entire process; therefore, one misbehaving thread can disrupt the processing of all the other threads in the application.\nAdapted from:\n\"Difference between Process and Thread\" by MKS075, Geeks for Geeks is licensed under CC BY-SA 4.0\n\"Thread in Operating System\" by chrismaher37, Geeks for Geeks is licensed under CC BY-SA 4.0\n\"Thread (computing)\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n4.1: Process and Threads is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n4.1.2 https:\/\/eng.libretexts.org\/@go\/page\/46801 4.2: Thread Types\nThreads and its types in Operating System\nThread is a single sequence stream within a process. Threads have same properties as of the process so they are called as light\nweight processes. Threads are executed one after another but gives the illusion as if they are executing in parallel. Each thread has\ndifferent states. Each thread has\n1. A program counter\n2. A register set\n3. A stack space\nThreads are not independent of each other as they share the code, data, OS resources etc.\nSimilarity between Threads and Processes \u2013\nOnly one thread or process is active at a time\nWithin process both execute sequentiall\nBoth can create children\nDifferences between Threads and Processes \u2013\nThreads are not independent, processes are.\nThreads are designed to assist each other, processes may or may not do it\nTypes of Threads:\n1. User Level thread (ULT)\nIs implemented in the user level library, they are not created using the system calls. Thread switching does not need to call OS\nand to cause interrupt to Kernel. Kernel doesn\u2019t know about the user level thread and manages them as if they were single-\nthreaded processes.\nAdvantages of ULT\nCan be implemented on an OS that does\u2019t support multithreading.\nSimple representation since thread has only program counter, register set, stack space.\nSimple to create since no intervention of kernel.\nThread switching is fast since no OS calls need to be made.\nDisadvantages of ULT\nNo or less co-ordination among the threads and Kernel.\nIf one thread causes a page fault, the entire process blocks.\nDifference between Process and User Level Thread:\nPROCESS USER LEVEL THREAD\nProcess is a program being executed. User level thread is the thread managed at user level.\nIt is high overhead. It is low overhead.\nThere is no sharing between processes. User level threads share address space.\nProcess is scheduled by operating system. User level thread is scheduled by thread library.\nBlocking one process does not affect the other processes. Blocking one user Level thread will block whole process of the thread.\nProcess is scheduled using process table. User level thread is scheduled using thread table.\nIt is heavy weight activity. It is light weight as compared to process.\nIt can be suspended. It can not be suspended.\nSuspension of a process does not affect other processes. Suspension of user level thread leads to all the threads stop running.\nIts types are \u2013 user process and system process. Its types are \u2013 user level single thread and user level multi thread.\n4.2.1 https:\/\/eng.libretexts.org\/@go\/page\/46802 PROCESS USER LEVEL THREAD\nEach process can run on different processor. All threads should run on only one processor.\nProcesses are independent from each other. User level threads are dependent.\nProcess supports parallelism. User level threads do not support parallelism.\n2. Kernel Level Thread (KLT)\nKernel knows and manages the threads. Instead of thread table in each process, the kernel itself has thread table (a master one)\nthat keeps track of all the threads in the system. In addition kernel also maintains the traditional process table to keep track of\nthe processes. OS kernel provides system call to create and manage threads.\nAdvantages of KLT\nSince kernel has full knowledge about the threads in the system, scheduler may decide to give more time to processes\nhaving large number of threads.\nGood for applications that frequently block.\nDisadvantages of KLT\nSlow and inefficient.\nIt requires thread control block so it is an overhead.\nDifference between Process and Kernel Thread:\nPROCESS KERNEL THREAD\nProcess is a program being executed. Kernel thread is the thread managed at kernel level.\nIt is high overhead. It is medium overhead.\nThere is no sharing between processes. Kernel threads share address space.\nProcess is scheduled by operating system using process table. Kernel thread is scheduled by operating system using thread table.\nIt is heavy weight activity. It is light weight as compared to process.\nIt can be suspended. It can not be suspended.\nSuspension of a process does not affect other processes. Suspension of kernel thread leads to all the threads stop running.\nIts types are \u2013 user process and system process. Its types are \u2013 kernel level single thread and kernel level multi thread.\n4.2: Thread Types is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n4.2.2 https:\/\/eng.libretexts.org\/@go\/page\/46802 4.2.1: Thread Types - Models\nMulti Threading Models in Process Management\nMany operating systems support kernel thread and user thread in a combined way. Example of such system is Solaris. Multi\nthreading model are of three types.\nMany to many model.\nMany to one model.\none to one model.\nMany to Many Model\nIn this model, we have multiple user threads connected to the same or lesser number of kernel level threads. The number of kernel\nlevel threads are specific to the type of hardware. The advantage of this model is if a user thread is blocked for any reason, we can\nschedule others user threads to other kernel threads and the process itself continues to execute. Therefore, the entire process doesn\u2019t\nblock if a single thread is blocked.\nFigure 4.2.1.1: Many to Many Thread Model. (\"Many to Many\" by Unknown, Geeks for Geeks is licensed under CC BY-SA 4.0)\nMany to One Model\nIn this model, we have multiple user threads mapped to a single kernel thread. In this model if a user thread gets blocked by a\nsystem call, the process itself is blocked. Since we have only one kernel thread then only one user thread can access kernel at a\ntime, so multiple user threads are not able access system calls at the same time.\nFigure 4.2.1.1: Many to One Thread Model. (\"Many to One\" by Unknown, Geeks for Geeks is licensed under CC BY-SA 4.0)\nOne to One Model\nIn this model, there is a one to one relationship between kernel and user threads. Multiple thread can run on their own processor in\na multiprocessor system. The problem with this model is that creating a user thread requires the creation of a kernel thread.\nFigure 4.2.1.1: One to One Thread Model. (\"One to One\" by Unknown, Geeks for Geeks is licensed under CC BY-SA 4.0)\n4.2.1.1 https:\/\/eng.libretexts.org\/@go\/page\/46804 Adapted from:\n\"Multi Threading Models in Process Management\" by Unknown, Geeks for Geeks is licensed under CC BY-SA 4.0\n4.2.1: Thread Types - Models is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n4.2.1.2 https:\/\/eng.libretexts.org\/@go\/page\/46804 4.3: Thread Relationships\nRelationship between User level thread and Kernel level thread\nA task is accomplished on the execution of a program, which results in a process. Every task incorporates one or many sub tasks,\nwhereas these sub tasks are carried out as functions within a program by the threads. The operating system (kernel) is unaware of\nthe threads in the user space.\nThere are two types of threads, User level threads (ULT) and Kernel level threads (KLT).\n1. User Level Threads :\nThreads in the user space designed by the application developer using a thread library to perform unique subtask.\n2. Kernel Level Threads :\nThreads in the kernel space designed by the os developer to perform unique functions of OS. Similar to a interrupt handler.\nThere exist a strong a relationship between user level threads and kernel level threads.\nDependencies between ULT and KLT :\n1. Use of Thread Library :\nThread library acts as an interface for the application developer to create number of threads (according to the number of\nsubtasks) and to manage those threads. This API for a process can be implemented in kernel space or user space. In real-time\napplication, the necessary thread library is implemented in user space. This reduces the system call to kernel whenever the\napplication is in need of thread creation, scheduling or thread management activities. Thus, the thread creation is faster as it\nrequires only function calls within the process. The user address space for each thread is allocated at run-time. Overall it\nreduces various interface and architectural overheads as all these functions are independent of kernel support.\n2. Synchronization :\nThe subtasks (functions) within each task (process) can be executed concurrently or in parallel depending on the application. In\nthat case, a single-threaded process is not suitable. These subtaks require multithreaded process. A unique subtask is allocated\nto every thread within the process. These threads may use the same data section or different data section. Typically, threads\nwithin the same process will share the code section, data section, address space, open files etc...BUT...each thread has its own\nset of registers, and its own stack memory.\nFigure 4.3.1: Single and Multi Thread Processes. (\"SIngle versus Multi Threads\" by maha93427, Geeks for Geeks is licensed\nunder CC BY-SA 4.0)\nWhen subtasks are concurrently performed by sharing the code section, it may result in data inconsistency. Ultimately, requires\nsuitable synchronization techniques to maintain the control flow to access the shared data.\nIn a multithreaded process, synchronization has four different models :\n4.3.1 https:\/\/eng.libretexts.org\/@go\/page\/46803 1. Mutex Locks \u2013 This allows only one thread at a time to access the shared resource.\n2. Read\/Write Locks \u2013 This allows exclusive writes and concurrent read of a shared resource.\n3. Counting Semaphore \u2013 This count refers to the number of shared resource that can be accessed simultaneously at a time. Once\nthe count limit is reached, the remaining threads are blocked.\n4. Conditional Variables \u2013 This blocks the thread until the condition satisfies (Busy Waiting).\nAll these synchronization models are carried out within each process using thread library. The memory space for the lock\nvariables is allocated in the user address space. Thus, requires no kernel intervention.\n1. Scheduling :\nThe application developer during the thread creation sets the priority and scheduling policy of each ULT thread using the thread\nlibrary. On the execution of program, based on the defined attributes the scheduling takes place by the thread library. In this case,\nthe system scheduler has no control over thread scheduling as the kernel is unaware of the ULT threads.\n2. Context Switching :\nSwitching from one ULT thread to other ULT thread is faster within the same process, as each thread has its own unique thread\ncontrol block, registers, stack. Thus, registers are saved and restored. Does not require any change of address space. Entire\nswitching takes place within the user address space under the control of thread library.\n3. Asynchronous I\/O :\nAfter an I\/O request ULT threads remains in blocked state, until it receives the acknowledgment(ack) from the receiver. Although it\nfollows asynchronous I\/O, it creates a synchronous environment to the application user. This is because the thread library itself\nschedules an other ULT to execute until the blocked thread sends sigpoll as an ack to the process thread library. Only then the\nthread library, reschedules the blocked thread.For example, consider a program to copy the content(read) from one file and to\npaste(write) in the other file. Additionally, a pop-up that displays the percentage of progress completion.\nDependency between ULT and KLT :\nThe one and only major dependency between KLT and ULT occurs when an ULT is in need of the Kernel resources. Every ULT\nthread is associated to a virtual processor called a Light-weight process. This is created and bound to the ULT by the thread library.\nWhenever a system call is invoked, a kernel level thread is created and scheduled by the system scheduler. These KLT are\nscheduled to access the kernel resources by the system scheduler - the scheduler is unaware of the ULT. Whereas the\nKLT themselves are aware of each ULT associated with each KLT.\nWhat if the relationship does not exist ?\nIf there is no association between KLT and ULT, then every process is a single-threaded process.\nAdapted from:\n\"Relationship between User level thread and Kernel level thread\" by maha93427, Geeks for Geeks is licensed under CC BY-SA 4.0\n4.3: Thread Relationships is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n4.3.2 https:\/\/eng.libretexts.org\/@go\/page\/46803 4.4: Benefits of Multithreading\nBenefits of Multithreading in Operating System\nThe benefits of multi threaded programming can be broken down into four major categories:\n1. Responsiveness \u2013\nMultithreading in an interactive application may allow a program to continue running even if a part of it is blocked or is\nperforming a lengthy operation, thereby increasing responsiveness to the user.\nIn a non multi threaded environment, a server listens to the port for some request and when the request comes, it processes the\nrequest and then resume listening to another request. The time taken while processing of request makes other users wait\nunnecessarily. Instead a better approach would be to pass the request to a worker thread and continue listening to port.\nFor example, a multi threaded web browser allow user interaction in one thread while an video is being loaded in another\nthread. So instead of waiting for the whole web-page to load the user can continue viewing some portion of the web-page.\n2. Resource Sharing \u2013\nProcesses may share resources only through techniques such as-\nMessage Passing\nShared Memory\nSuch techniques must be explicitly organized by programmer. However, threads share the memory and the resources of the\nprocess to which they belong by default.\nThe benefit of sharing code and data is that it allows an application to have several threads of activity within same address\nspace.\n3. Economy \u2013\nAllocating memory and resources for process creation is a costly job in terms of time and space.\nSince, threads share memory with the process it belongs, it is more economical to create and context switch threads. Generally\nmuch more time is consumed in creating and managing processes than in threads.\nIn Solaris, for example, creating process is 30 times slower than creating threads and context switching is 5 times slower.\n4. Scalability \u2013\nThe benefits of multi-programming greatly increase in case of multiprocessor architecture, where threads may be running\nparallel on multiple processors. If there is only one thread then it is not possible to divide the processes into smaller tasks that\ndifferent processors can perform.\nSingle threaded process can run only on one processor regardless of how many processors are available.\nMulti-threading on a multiple CPU machine increases parallelism.\nAdapted from:\n\"Benefits of Multithreading in Operating System\" by aastha98, Geeks for Geeks is licensed under CC BY-SA 4.0\n4.4: Benefits of Multithreading is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n4.4.1 https:\/\/eng.libretexts.org\/@go\/page\/46805 CHAPTER OVERVIEW\n5: Concurrency and Process Synchronization\n5.1: Introduction to Concurrency\n5.2: Process Synchronization\n5.3: Mutual Exclusion\n5.4: Interprocess Communication\n5.4.1: IPC - Semaphores\n5.4.2: IPC - Monitors\n5.4.3: IPC - Message Passing \/ Shared Memory\nThis page titled 5: Concurrency and Process Synchronization is shared under a CC BY-SA license and was authored, remixed, and\/or curated by\nPatrick McClanahan.\n1 5.1: Introduction to Concurrency\nConcurrency in Operating System\nConcurrency is the execution of a set of multiple instruction sequences at the same time. This occurs when there are several\nprocess threads running in parallel. These threads communicate with the other threads\/processes through a concept of shared\nmemory or through message passing. Because concurrency results in the sharing of system resources - instructions, memory, files -\nproblems can occur. like deadlocks and resources starvation. (we will talk about starvation and deadlocks in the next module).\nPrinciples of Concurrency :\nWith current technology such as multi core processors, and parallel processing, which allow for multiple processes\/threads to be\nexecuted concurrently - that is at the same time - it is possible to have more than a single process\/thread accessing the same space\nin memory, the same declared variable in the code, or even attempting to read\/write to the same file.\nThe amount of time it takes for a process to execute is not easily calculated, so we are unable to predict which process will\ncomplete first, thereby allowing us to implement algorithms to deal with the issues that concurrency creates. The amount of time a\nprocess takes to complete depends on the following:\nThe activities of other processes\nThe way operating system handles interrupts\nThe scheduling policies of the operating system\nProblems in Concurrency :\nSharing global resources\nSharing of global resources safely is difficult. If two processes both make use of a global variable and both make changes to the\nvariables value, then the order in which various changes take place are executed is critical.\nOptimal allocation of resources\nIt is difficult for the operating system to manage the allocation of resources optimally.\nLocating programming errors\nIt is very difficult to locate a programming error because reports are usually not reproducible due to the different states of the\nshared components each time the code runs.\nLocking the channel\nIt may be inefficient for the operating system to simply lock the resource and prevent its use by other processes.\nAdvantages of Concurrency :\nRunning of multiple applications\nHaving concurrency allows the operating system to run multiple applications at the same time.\nBetter resource utilization\nHaving concurrency allows the resources that are NOT being used by one application can be used for other applications.\nBetter average response time\nWithout concurrency, each application has to be run to completion before the next one can be run.\nBetter performance\nConcurrency provides better performance by the operating system. When one application uses only the processor and another\napplication uses only the disk drive then the time to concurrently run both applications to completion will be shorter than the\ntime to run each application consecutively.\nDrawbacks of Concurrency :\nWhen concurrency is used, it is pretty much required to protect multiple processes\/threads from one another.\nConcurrency requires the coordination of multiple processes\/threads through additional sequences of operations within the\noperating system.\nAdditional performance enhancements are necessary within the operating systems to provide for switching among applications.\nSometimes running too many applications concurrently leads to severely degraded performance.\nIssues of Concurrency :\n5.1.1 https:\/\/eng.libretexts.org\/@go\/page\/47609 Non-atomic\nOperations that are non-atomic but interruptible by multiple processes can cause problems. (an atomic operation is one that runs\ncompletely independently of any other processes\/threads - any process that is dependent on another process\/thread is non-\natomic)\nRace conditions\nA race condition is a behavior which occurs in software applications where the output is dependent on the timing or sequence of\nother uncontrollable events. Race conditions also occur in software which supports multithreading, use a distributed\nenvironment or are interdependent on shared resources\nBlocking\nA process that is blocked is one that is waiting for some event, such as a resource becoming available or the completion of\nan I\/O operation.[Processes can block waiting for resources. A process could be blocked for long period of time waiting for\ninput from a terminal. If the process is required to periodically update some data, this would be very undesirable.\nStarvation\nA problem encountered in concurrent computing where a process is perpetually denied necessary resources to process its\nwork. Starvation may be caused by errors in a scheduling or mutual exclusion algorithm, but can also be caused by resource\nleaks\nDeadlock\nIn concurrent computing, a deadlock is a state in which each member of a group waits for another member, including itself, to\ntake action, such as sending a message or more commonly releasing a lock. Deadlocks are a common problem in\nmultiprocessing systems, parallel computing, and distributed systems, where software and hardware locks are used to arbitrate\nshared resources and implement process synchronization\nAdapted from:\n\"Concurrency in Operating System\" by pp_pankaj, Geeks for Geeks is licensed under CC BY-SA 4.0\n5.1: Introduction to Concurrency is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n5.1.2 https:\/\/eng.libretexts.org\/@go\/page\/47609 5.2: Process Synchronization\nIntroduction of Process Synchronization\nWhen we discuss the concept of synchronization, processes are categorized as one of the following two types:\n1. Independent Process : Execution of one process does not affects the execution of other processes.\n2. Cooperative Process : Execution of one process affects the execution of other processes.\nProcess synchronization problems are most likely when dealing with cooperative processes because the process resources are\nshared between the multiple processes\/threads.\nRace Condition\nWhen more than one processes is executing the same code, accessing the same memory segment or a shared variable there is the\npossibility that the output or the value of the shared variable is incorrect. This can happen when multiple processes are attempting\nto alter a memory location, this can create a race condition - where multiple processes have accessed the current value at a memory\nlocation, each process has changed that value, and now they need to write the new back...BUT...each process has a different new\nvalue. So, which one is correct? Which one is going to be the new value.\nOperating system need to have a process to manage these shared components\/memory segments. This is called synchronization,\nand is a critical concept in operating system.\nUsually race conditions occur inside what is known as a critical section of the code. Race conditions can be avoided if the critical\nsection is treated as an atomic instruction, that is an operation that run completely independently of any other processes, making\nuse of software locks or atomic variables which will prevent race conditions. We will take a look at this concept below.\nCritical Section Problem\nIn concurrent programming, concurrent accesses to shared resources can lead to unexpected or erroneous behavior, so parts of the\nprogram where the shared resource is accessed need to be protected in ways that avoid the concurrent access. This protected section\nis the critical section or critical region. It cannot be executed by more than one process at a time. Typically, the critical section\naccesses a shared resource, such as a data structure, a peripheral device, or a network connection, that would not operate correctly\nin the context of multiple concurrent accesses.\nDifferent codes or processes may consist of the same variable or other resources that need to be read or written but whose results\ndepend on the order in which the actions occur. For example, if a variable x is to be read by process A, and process B has to write\nto the same variable x at the same time, process A might get either the old or new value of x.\nThe following example is VERY simple - sometimes the critical section can be more than a single line of code.\nProcess A:\n\/\/ Process A\n.\n.\nb = x + 5; \/\/ instruction executes at time = Tx, meaning some unknown ti\n.\nProcess B:\n\/\/ Process B\n.\n.\nx = 3 + z; \/\/ instruction executes at time = Tx, meaning some unknown ti\n.\n5.2.1 https:\/\/eng.libretexts.org\/@go\/page\/47598 In cases like these, a critical section is important. In the above case, if A needs to read the updated value of x, executing Process A\nand Process B at the same time may not give required results. To prevent this, variable x is protected by a critical section. First, B\ngets the access to the section. Once B finishes writing the value, A gets the access to the critical section and variable x can be read.\nBy carefully controlling which variables are modified inside and outside the critical section, concurrent access to the shared\nvariable are prevented. A critical section is typically used when a multi-threaded program must update multiple related variables\nwithout a separate thread making conflicting changes to that data. In a related situation, a critical section may be used to ensure that\na shared resource, for example, a printer, can only be accessed by one process at a time.\nAny solution to the critical section problem must satisfy three requirements:\nMutual Exclusion\nExclusive access of each process to the shared memory. Only one process can be in it's critical section at any given time.\nProgress\nIf no process is in its critical section, and if one or more threads want to execute their critical section then any one of these\nthreads must be allowed to get into its critical section.\nBounded Waiting\nAfter a process makes a request for getting into its critical section, there is a limit for how many other processes can get into\ntheir critical section, before this process's request is granted. So after the limit is reached, system must grant the process\npermission to get into its critical section. The purpose of this condition is to make sure that every process gets the chance to\nactually enter its critical section so that no process starves forever.\nAdapted from:\n\"Critical section\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n5.2: Process Synchronization is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n5.2.2 https:\/\/eng.libretexts.org\/@go\/page\/47598 5.3: Mutual Exclusion\nMutual Exclusion Explained\nThe problem which mutual exclusion addresses is a problem of resource sharing: how can a software system control multiple\nprocesses' access to a shared resource, when each process needs exclusive control of that resource while doing its work? The\nmutual-exclusion solution to this makes the shared resource available only while the process is in a specific code segment called\nthe critical section. It controls access to the shared resource by controlling each mutual execution of that part of its program where\nthe resource would be used.\nA successful solution to this problem must have at least these two properties:\nIt must implement mutual exclusion: only one process can be in the critical section at a time.\nIt must be free of deadlocks: if processes are trying to enter the critical section, one of them must eventually be able to do so\nsuccessfully, provided no process stays in the critical section permanently.\nHardware solutions\nOn single-processor systems, the simplest solution to achieve mutual exclusion is to disable interrupts when a process is in\na critical section. This will prevent any interrupt service routines (such as the system timer, I\/O interrupt request, etc) from running\n(effectively preventing a process from being interrupted). Although this solution is effective, it leads to many problems. If a critical\nsection is long, then the system clock will drift every time a critical section is executed because the timer interrupt (which keeps the\nsystem clock in sync) is no longer serviced, so tracking time is impossible during the critical section. Also, if a process halts during\nits critical section, control will never be returned to another process, effectively halting the entire system. A more elegant method\nfor achieving mutual exclusion is the busy-wait.\nBusy-waiting is effective for both single-processor and multiprocessor systems. The use of shared memory and an atomic\n(remember - we talked about atomic) test-and-set instruction provide the mutual exclusion. A process can test-and-set on a variable\nin a section of shared memory, and since the operation is atomic, only one process can set the flag at a time. Any process that is\nunsuccessful in setting the flag (it is unsuccessful because the process can NOT gain access to the variable until the other process\nreleases it) can either go on to do other tasks and try again later, release the processor to another process and try again later, or\ncontinue to loop while checking the flag until it is successful in acquiring it. Preemption is still possible, so this method allows the\nsystem to continue to function\u2014even if a process halts while holding the lock.\nSoftware solutions\nIn addition to hardware-supported solutions, some software solutions exist that use busy waiting to achieve mutual exclusion.\nIt is often preferable to use synchronization facilities provided by an operating system's multithreading library, which will take\nadvantage of hardware solutions if possible but will use software solutions if no hardware solutions exist. For example, when the\noperating system's lock library is used and a thread tries to acquire an already acquired lock, the operating system could suspend\nthe thread using a context switch and swap it out with another thread that is ready to be run, or could put that processor into a low\npower state if there is no other thread that can be run\nAdapted from:\n\"Mutual exclusion\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n5.3: Mutual Exclusion is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n5.3.1 https:\/\/eng.libretexts.org\/@go\/page\/47658 5.4: Interprocess Communication\nIPC (InterProcess Communication)\nIn computer science, inter-process communication or interprocess communication (IPC) refers specifically to the mechanisms an\noperating system provides to allow the processes to manage shared data. Typically, applications using IPC, are categorized as\nclients and servers, where the client requests data and the server responds to client requests. Many applications are both clients and\nservers, as commonly seen in distributed computing.\nAn independent process is not affected by the execution of other processes while cooperating processes can be affected by, and\nmay affect, other executing processes. Though one can think that those processes, which are running independently, will execute\nvery efficiently, in reality, there are many situations where the co-operative nature can be utilized for increasing computational\nspeed, convenience and modularity. Inter process communication (IPC) is a mechanism which allows processes to communicate\nwith each other and synchronize their actions.\nIPC is very important to the design process for operating system kernels that desire to be kept small, therefore reduce the number of\nfunctionalities provided by the kernel. Those functionalities are then obtained by communicating with servers via IPC, leading to a\nlarge increase in communication when compared to a regular type of operating system kernel, which provides a lot more\nfunctionality.\nMethods in Interprocess Communication\nThere are several different ways to implement IPC. IPC is set of programming interfaces, used by programs to communicate\nbetween series of processes. This allows running programs concurrently in an Operating System. Below are the methods in IPC:\n1. Pipes (Same Process)\nThis allows flow of data in one direction only. Analogous to simplex systems (Keyboard). Data from the output is usually\nbuffered until input process receives it which must have a common origin.\n2. Names Pipes (Different Processes)\nThis is a pipe with a specific name it can be used in processes that don\u2019t have a shared common process origin. E.g. is FIFO\nwhere the details written to a pipe is first named.\n3. Message Queuing\nThis allows messages to be passed between processes using either a single queue or several message queue. This is managed by\nsystem kernel these messages are coordinated using an API.\n4. Semaphores\nThis is used in solving problems associated with synchronization and to avoid race condition. These are integer values which\nare greater than or equal to 0.\n5. Shared memory\nThis allows the interchange of data through a defined area of memory. Semaphore values have to be obtained before data can\nget access to shared memory.\n6. Sockets\nThis method is mostly used to communicate over a network between a client and a server. It allows for a standard connection\nwhich is computer and OS independent.\nWe will discuss a couple of these concepts.\nAdapted from:\n\"Inter Process Communication (IPC)\" by ShubhamMaurya3, Geeks for Geeks is licensed under CC BY-SA 4.0\n\"Methods in Interprocess Communication\" by Aniket_Dusey, Geeks for Geeks is licensed under CC BY-SA 4.0\n5.4: Interprocess Communication is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n5.4.1 https:\/\/eng.libretexts.org\/@go\/page\/47659 5.4.1: IPC - Semaphores\nWhat is a sempahore\nIn computer science, a semaphore is a variable or abstract data type used to control access to a common resource by multiple\nprocesses and avoid critical section problems in a concurrent system such as a multitasking operating system. A trivial semaphore\nis a plain variable that is changed (for example, incremented or decremented, or toggled) depending on programmer-defined\nconditions.\nA useful way to think of a semaphore as used in a real-world system is as a record of how many units of a particular resource are\navailable, coupled with operations to adjust that record safely (i.e., to avoid race conditions) as units are acquired or become free,\nand, if necessary, wait until a unit of the resource becomes available.\nSemaphores are a useful tool in the prevention of race conditions; however, their use is by no means a guarantee that a program is\nfree from these problems. Semaphores which allow an arbitrary resource count are called counting semaphores, while semaphores\nwhich are restricted to the values 0 and 1 (or locked\/unlocked, unavailable\/available) are called binary semaphores and are used to\nimplement locks.\nLibrary analogy\nSuppose a library has 10 identical study rooms, to be used by one student at a time. Students must request a room from the front\ndesk if they wish to use a study room. If no rooms are free, students wait at the desk until someone relinquishes a room. When a\nstudent has finished using a room, the student must return to the desk and indicate that one room has become free.\nIn the simplest implementation, the clerk at the front desk knows only the number of free rooms available, which they only know\ncorrectly if all of the students actually use their room while they've signed up for them and return them when they're done. When a\nstudent requests a room, the clerk decreases this number. When a student releases a room, the clerk increases this number. The\nroom can be used for as long as desired, and so it is not possible to book rooms ahead of time.\nIn this scenario the front desk count-holder represents a counting semaphore, the rooms are the resource, and the students represent\nprocesses\/threads. The value of the semaphore in this scenario is initially 10, with all rooms empty. When a student requests a\nroom, they are granted access, and the value of the semaphore is changed to 9. After the next student comes, it drops to 8, then 7\nand so on. If someone requests a room and the current value of the semaphore is 0,[3] they are forced to wait until a room is freed\n(when the count is increased from 0). If one of the rooms was released, but there are several students waiting, then any method can\nbe used to select the one who will occupy the room (like FIFO or flipping a coin). And of course, a student needs to inform the\nclerk about releasing their room only after really leaving it, otherwise, there can be an awkward situation when such student is in\nthe process of leaving the room (they are packing their textbooks, etc.) and another student enters the room before they leave it.\nImportant observations\nWhen used to control access to a pool of resources, a semaphore tracks only how many resources are free; it does not keep track of\nwhich of the resources are free. Some other mechanism (possibly involving more semaphores) may be required to select a\nparticular free resource.\nThe paradigm is especially powerful because the semaphore count may serve as a useful trigger for a number of different actions.\nThe librarian above may turn the lights off in the study hall when there are no students remaining, or may place a sign that says the\nrooms are very busy when most of the rooms are occupied.\nThe success of the semaphore requires applications to follow it correctly. Fairness and safety are likely to be compromised (which\npractically means a program may behave slowly, act erratically, hang or crash) if even a single process acts incorrectly. This\nincludes:\nrequesting a resource and forgetting to release it;\nreleasing a resource that was never requested;\nholding a resource for a long time without needing it;\nusing a resource without requesting it first (or after releasing it).\nAdapted from:\n\"Semaphore (programming)\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n5.4.1.1 https:\/\/eng.libretexts.org\/@go\/page\/47660 5.4.1: IPC - Semaphores is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n5.4.1.2 https:\/\/eng.libretexts.org\/@go\/page\/47660 5.4.2: IPC - Monitors\nMonitors in Process Synchronization\nThe monitor is one of the ways to achieve process synchronization. The monitor is supported by programming languages to achieve\nmutual exclusion between processes. Not all programming languages provide for monitors.\nThe proper basic usage of a monitor is: (the italicized text are all comments explaining what is going on)\nacquire(m); \/\/ Acquire this monitor's lock - this prevents other processes from being\nwhile (!condition) { \/\/ While the condition that we are waiting for is not true (in pr\nwait(m, condition); \/\/ Wait on this monitor's lock (tht is the variable m) and\n}\n\/\/ ... Critical section of code goes here ...\nsignal(condition2); \/\/ condition2 might be the same as condition or different.\nrelease(m); \/\/ Release this monitor's lock, now one of the processes sitting in the wa\nAdvantages of Monitor:\nMonitors have the advantage of making parallel programming easier and less error prone than using techniques such as semaphore.\nDisadvantages of Monitor:\nMonitors have to be implemented as part of the programming language . The compiler must generate code for them. This gives the\ncompiler the additional burden of having to know what operating system facilities are available to control access to critical sections\nin concurrent processes. Some languages that do support monitors are Java,C#,Visual Basic,Ada and concurrent Euclid.\nAdapted from:\n\"Monitors in Process Synchronization\" by shivanshukumarsingh1, Geeks for Geeks is licensed under CC BY-SA 4.0\n\"Monitor (synchronization)\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n5.4.2: IPC - Monitors is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n5.4.2.1 https:\/\/eng.libretexts.org\/@go\/page\/47661 5.4.3: IPC - Message Passing \/ Shared Memory\nIPC through Message Passing\nAs we previously discussed - a process can be one of two different types:\nIndependent process.\nCo-operating process.\nAn independent process is not affected by the execution of other processes while a co-operating process can be affected by other\nexecuting processes. Though one can think that those processes, which are running independently, will execute very efficiently, in\nreality, there are many situations when co-operative nature can be utilised for increasing computational speed, convenience and\nmodularity. Inter process communication (IPC) is a mechanism which allows processes to communicate with each other and\nsynchronize their actions. The communication between these processes can be seen as a method of co-operation between them.\nProcesses can communicate with each other through either of these techniques:\n1. Shared Memory\n2. Message passing\nThe Figure 1 below shows a basic structure of communication between processes via the shared memory method and via the\nmessage passing method.\nAn operating system can implement both method of communication. First, there is the shared memory method of communication.\nCommunication between processes using shared memory requires processes to share some variable and it is usually left up to\nthe programmer to implement it. Sharing memory works in this manner: process1 and process2 are executing simultaneously and\nthey share some resources. Process1 generates data based on computations in the code. Process1 stores this data in shared memory.\nWhen process2 needs to use the shared data, it will check in the shared memory segment and use the data that process1 placed\nthere. Processes can use shared memory for extracting information as a record from another process as well as for delivering any\nspecific information to other processes.\nFigure 5.4.3.1: Shared Memory and Message Passing. (\"Shared Memory and Message Passing\" by ShubhamMaurya3, Geeks for\nGeeks is licensed under CC BY-SA 4.0)\nSecond, there is communication between processes via message passing. In this method, processes communicate with each other\nwithout using any kind of shared memory. If two processes p1 and p2 want to communicate with each other, they proceed as\nfollows:\nEstablish a communication link (if a link already exists, no need to establish it again.)\nStart exchanging messages using a system's library functions send() and receive().\nWe need at least two primitives:\n\u2013 send(message, destination) or send(message)\n\u2013 receive(message, host) or receive(message)\nTo send a message, Process A, sends a message via the communication link that has been opened between the 2 processes. Using\nthe send() function it send the necessary message. Process B, which is monitoring the communication link, uses the receive()\nfunction to pick up the message and performs the necessary processing based on the message it has received. The message size can\nbe of fixed size or of variable size. If it is of fixed size, it is easy for an OS designer but complicated for a programmer and if it is of\nvariable size then it is easy for a programmer but complicated for the OS designer.\n5.4.3.1 https:\/\/eng.libretexts.org\/@go\/page\/47662 Adapted from:\n\"Inter Process Communication (IPC)\" by ShubhamMaurya3, Geeks for Geeks is licensed under CC BY-SA 4.0\n5.4.3: IPC - Message Passing \/ Shared Memory is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n5.4.3.2 https:\/\/eng.libretexts.org\/@go\/page\/47662 CHAPTER OVERVIEW\n6: Concurrency: Deadlock and Starvation\n6.1: Concept and Principles of Deadlock\n6.2: Deadlock Detection and Prevention\n6.3: Starvation\n6.4: Dining Philosopher Problem\nThis page titled 6: Concurrency: Deadlock and Starvation is shared under a CC BY-SA license and was authored, remixed, and\/or curated by\nPatrick McClanahan.\n1 6.1: Concept and Principles of Deadlock\nDeadlock\nIn concurrent computing, a deadlock is a state in which each member of a group waits for another member, including itself, to take\naction, such as sending a message or more commonly releasing a lock. Deadlocks are a common problem in multiprocessing\nsystems, parallel computing, and distributed systems, where software and hardware locks are used to arbitrate shared resources and\nimplement process synchronization.\nIn an operating system, a deadlock occurs when a process or thread enters a waiting state because a requested system resource is\nheld by another waiting process, which in turn is waiting for another resource held by another waiting process. If a process is\nunable to change its state indefinitely because the resources requested by it are being used by another waiting process, then the\nsystem is said to be in a deadlock.\nIn a communications system, deadlocks occur mainly due to lost or corrupt signals rather than resource contention.\nFigure 6.1.1: Both processes need resources to continue execution. P1 requires additional resource R1 and is in possession of\nresource R2, P2 requires additional resource R2 and is in possession of R1; neither process can continue.\n(\"Process deadlock\" by Wikimedia Commons is licensed under CC BY-SA 4.0)\nThe previous image show a simple instance of deadlock. Two resources are \"stuck\", because the other process has control of the\nresource that the process needs to continue to process. While this can occure quite easily, there is usually code in place to keep this\nfrom happening. As we discussed in the previous module there are various inter-process communication techniques that can\nactually keep processes from becoming deadlocked due to resource contention. So, often times when we hit a deadlock like this it\nis something to do with the IPC that is not handling this situation properly.\nNecessary conditions\nA deadlock situation on a resource can arise if and only if all of the following conditions hold simultaneously in a system:\nMutual exclusion: At least one resource must be held in a non-shareable mode. Otherwise, the processes would not be\nprevented from using the resource when necessary. Only one process can use the resource at any given instant of time.\nHold and wait or resource holding: a process is currently holding at least one resource and requesting additional resources\nwhich are being held by other processes.\nNo preemption: a resource can be released only voluntarily by the process holding it.\nCircular wait: each process must be waiting for a resource which is being held by another process, which in turn is waiting for\nthe first process to release the resource. In general, there is a set of waiting processes, P = {P1, P2, \u2026, PN}, such that P1 is\nwaiting for a resource held by P2, P2 is waiting for a resource held by P3 and so on until PN is waiting for a resource held by\nP1.\nThese four conditions are known as the Coffman conditions from their first description in a 1971 article by Edward G. Coffman, Jr.\nWhile these conditions are sufficient to produce a deadlock on single-instance resource systems, they only indicate the possibility\nof deadlock on systems having multiple instances of resources.\nThe following image shows 4 processes and a single resource. The image shows 2 processes contending for the single reource (the\ngrey circl in the middle), the it shows 3 processes contending for that resource, then finally how it looks when 4 processes contend\nfor the same resource. The image depicts the processes waiting to gain access to the resource - only one resource at a time can have\naccess.\n6.1.1 https:\/\/eng.libretexts.org\/@go\/page\/48015 Figure 6.1.1: Four processes (blue lines) compete for one resource (grey circle), following a right-before-left policy. A deadlock\noccurs when all processes lock the resource simultaneously (black lines). The deadlock can be resolved by breaking the symmetry.\n(\"Marble Machine\" by Wikimedia Commons is licensed under CC BY-SA 4.0)\nAdapted from:\n\"Deadlock\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n6.1: Concept and Principles of Deadlock is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n6.1.2 https:\/\/eng.libretexts.org\/@go\/page\/48015 6.2: Deadlock Detection and Prevention\nDeadlock Handling\nMost current operating systems cannot prevent deadlocks. When a deadlock occurs, different operating systems respond to them in\ndifferent non-standard manners. Most approaches work by preventing one of the four Coffman conditions from occurring,\nespecially the fourth one. Major approaches are as follows.\nIgnoring deadlock\nIn this approach, it is assumed that a deadlock will never occur. This is also an application of the Ostrich algorithm. This approach\nwas initially used by MINIX and UNIX. This is used when the time intervals between occurrences of deadlocks are large and the\ndata loss incurred each time is tolerable.\nIgnoring deadlocks can be safely done if deadlocks are formally proven to never occur.\nDetection\nUnder the deadlock detection, deadlocks are allowed to occur. Then the state of the system is examined to detect that a deadlock\nhas occurred and subsequently it is corrected. An algorithm is employed that tracks resource allocation and process states, it rolls\nback and restarts one or more of the processes in order to remove the detected deadlock. Detecting a deadlock that has already\noccurred is easily possible since the resources that each process has locked and\/or currently requested are known to the resource\nscheduler of the operating system.\nAfter a deadlock is detected, it can be corrected by using one of the following methods\n1. Process termination: one or more processes involved in the deadlock may be aborted. One could choose to abort all competing\nprocesses involved in the deadlock. This ensures that deadlock is resolved with certainty and speed. But the expense is high as\npartial computations will be lost. Or, one could choose to abort one process at a time until the deadlock is resolved. This\napproach has high overhead because after each abort an algorithm must determine whether the system is still in deadlock.\nSeveral factors must be considered while choosing a candidate for termination, such as priority and age of the process.\n2. Resource preemption: resources allocated to various processes may be successively preempted and allocated to other processes\nuntil the deadlock is broken.\nFigure 6.2.1: Two processes concurring for two resources. A deadlock occurs when the first process locks the first resource at the\nsame time as the second process locks the second resource. The deadlock can be resolved by cancelling and restarting the first\nprocess.\n(\"Two Processes - Two Resources\" by Wikimedia Commons is licensed under CC BY-SA 4.0)\nIn the above image - there are 2 resources. Initially only one process is using both resources. When a second process attempts to\naccess one of the resources, it is temporarily blocked, until the resource is released by the other process.. When 2 processes each\n6.2.1 https:\/\/eng.libretexts.org\/@go\/page\/48016 have control of one resource there is a deadlock, as the process can not gain access to the other process it need to continue to\nprocess. Eventually, the one process is canceled, allowing the system to block the other resource and allow one of the processes to\ncomplete, which then frees up both resources for the other process.\nPrevention\nDeadlock prevention works by preventing one of the four Coffman conditions from occurring.\nRemoving the mutual exclusion condition means that no process will have exclusive access to a resource. This proves\nimpossible for resources that cannot be spooled. But even with spooled resources, the deadlock could still occur. Algorithms\nthat avoid mutual exclusion are called non-blocking synchronization algorithms.\nThe hold and wait or resource holding conditions may be removed by requiring processes to request all the resources they will\nneed before starting up (or before embarking upon a particular set of operations). This advance knowledge is frequently difficult\nto satisfy and, in any case, is an inefficient use of resources. Another way is to require processes to request resources only when\nit has none; First they must release all their currently held resources before requesting all the resources they will need from\nscratch. This too is often impractical. It is so because resources may be allocated and remain unused for long periods. Also, a\nprocess requiring a popular resource may have to wait indefinitely, as such a resource may always be allocated to some process,\nresulting in resource starvation.\nThe no preemption condition may also be difficult or impossible to avoid as a process has to be able to have a resource for a\ncertain amount of time, or the processing outcome may be inconsistent or thrashing may occur. However, the inability to\nenforce preemption may interfere with a priority algorithm. Preemption of a \"locked out\" resource generally implies a rollback,\nand is to be avoided since it is very costly in overhead. Algorithms that allow preemption include lock-free and wait-free\nalgorithms and optimistic concurrency control. If a process holding some resources and requests for some another resource(s)\nthat cannot be immediately allocated to it, the condition may be removed by releasing all the currently being held resources of\nthat process.\nThe final condition is the circular wait condition. Approaches that avoid circular waits include disabling interrupts during\ncritical sections and using a hierarchy to determine a partial ordering of resources. If no obvious hierarchy exists, even the\nmemory address of resources has been used to determine ordering and resources are requested in the increasing order of the\nenumeration.\nFigure 6.2.1: (A) Two processes concurring for one resource, following a first-come, first-served policy. (B) A deadlock occurs\nwhen both processes lock the resource simultaneously. (C) The deadlock can be resolved by breaking the symmetry of the locks.\n(D) The deadlock can be avoided by breaking the symmetry of the locking mechanism.\n(\"Avoiding Deadlock\" by Wikimedia Commons is licensed under CC BY-SA 4.0)\nIn the above image notice the yellow line - if it is the same on both sides, a deadlock can develop (scenario A shows that one\nprocess gets there first...it is difficult to see in the gif - but that is why there is NOT a deadlock - first come - first serve). Watch -\nwhen the yellow lines, representing the locking mechanism, are different on each side then we have a method to break the deadlock\nand allow the left side process tom complete and freeing up the resource for the right and resource to complete.\nAdapted from:\n\"Deadlock\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n6.2: Deadlock Detection and Prevention is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n6.2.2 https:\/\/eng.libretexts.org\/@go\/page\/48016 6.3: Starvation\nConcept of Starvation\nStarvation is usually caused by an overly simplistic scheduling algorithm. For example, if a system always switches between the\nfirst two tasks while a third never gets to run, then the third task is being starved of CPU time. The scheduling algorithm, which is\npart of the kernel, is supposed to allocate resources equally among all processes; that is, the algorithm should allocate resources so\nthat no process continually is blocked from accessing the resources it needs to execute to completion.\nMany operating system schedulers employ the concept of process priority. Each process gets a priority - usually the lower the\nnumber the higher the priority - making a priority of zero the highest priority a process can have. A high priority process A will run\nbefore a low priority process B. If the high priority process (process A) blocks and never gives up control of the processor, the low\npriority process (B) will (in some systems) never be scheduled\u2014it will experience starvation. If there is an even higher priority\nprocess X, which is dependent on a result from process B, then process X might never finish, even though it is the most important\nprocess in the system. This condition is called a priority inversion. Modern scheduling algorithms normally contain code to\nguarantee that all processes will receive a minimum amount of each important resource (most often CPU time) in order to prevent\nany process from being subjected to starvation.\nStarvation is normally caused by a deadlock that causes a process to freeze waiting for resources. Two or more processes become\ndeadlocked when each of them is doing nothing while waiting for a resource occupied by another program in the same set, the two\n(or more) processes that are waiting can starve while waiting on the one process that has control of the resource. On the other hand,\na process is in starvation when it is waiting for a resource that is continuously given to other processes because it can never\ncomplete without access to the necessary resource. Starvation-freedom is a stronger guarantee than the absence of deadlock: a\nmutual exclusion algorithm that must choose to allow one of two processes into a critical section and picks one arbitrarily is\ndeadlock-free, but not starvation-free.\nA possible solution to starvation is to use a scheduling algorithm with priority queue that also uses the aging technique. Aging is a\ntechnique of gradually increasing the priority of processes that wait in the system for a long time. For example, if a process X has a\npriority of 100, it would probably be near the bottom of the priority list, it would get very little processing time on a busy system.\nUsing the concept of aging, over some set period of time, process X's priority would decrease, to say 50. If process X still did not\nget enough resources or processing time, after another period of time the priority would again decrease, to say 25. Eventually\nprocess X would get to a high enough priority (low number) that it would be scheduled for access to resources\/processor and would\ncomplete in a proper fashion.\nAdapted from:\n\"Starvation (computer science)\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n6.3: Starvation is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n6.3.1 https:\/\/eng.libretexts.org\/@go\/page\/48017 6.4: Dining Philosopher Problem\nIn computer science, the dining philosophers problem is an example problem often used in concurrent algorithm design to illustrate\nsynchronization issues and techniques for resolving them.\nIt was originally formulated in 1965 by Edsger Dijkstra as a student exam exercise, presented in terms of computers competing for\naccess to tape drive peripherals.\nProblem statement\nFive silent philosophers sit at a round table with bowls of spaghetti. Forks are placed between each pair of adjacent philosophers.\nEach philosopher must alternately think and eat. However, a philosopher can only eat spaghetti when they have both left and right\nforks. Each fork can be held by only one philosopher and so a philosopher can use the fork only if it is not being used by another\nphilosopher. After an individual philosopher finishes eating, they need to put down both forks so that the forks become available to\nothers. A philosopher can only take the fork on their right or the one on their left as they become available and they cannot start\neating before getting both forks.\nEating is not limited by the remaining amounts of spaghetti or stomach space; an infinite supply and an infinite demand are\nassumed.\nThe problem is how to design a discipline of behavior (a concurrent algorithm) such that no philosopher will starve; i.e., each can\nforever continue to alternate between eating and thinking, assuming that no philosopher can know when others may want to eat or\nthink.\nProblems\nThe problem was designed to illustrate the challenges of avoiding deadlock, a system state in which no progress is possible. To see\nthat a proper solution to this problem is not obvious, consider a proposal in which each philosopher is instructed to behave as\nfollows:\nthink until the left fork is available; when it is, pick it up;\nthink until the right fork is available; when it is, pick it up;\nwhen both forks are held, eat for a fixed amount of time;\nthen, put the right fork down;\nthen, put the left fork down;\nrepeat from the beginning.\nThis attempted solution fails because it allows the system to reach a deadlock state, in which no progress is possible. This is a state\nin which each philosopher has picked up the fork to the left, and is waiting for the fork to the right to become available. With the\ngiven instructions, this state can be reached, and when it is reached, each philosopher will eternally wait for another (the one to the\nright) to release a fork.[4]\nResource starvation might also occur independently of deadlock if a particular philosopher is unable to acquire both forks because\nof a timing problem. For example, there might be a rule that the philosophers put down a fork after waiting ten minutes for the\nother fork to become available and wait a further ten minutes before making their next attempt. This scheme eliminates the\npossibility of deadlock (the system can always advance to a different state) but still suffers from the problem of livelock. If all five\nphilosophers appear in the dining room at exactly the same time and each picks up the left fork at the same time the philosophers\nwill wait ten minutes until they all put their forks down and then wait a further ten minutes before they all pick them up again.\nMutual exclusion is the basic idea of the problem; the dining philosophers create a generic and abstract scenario useful for\nexplaining issues of this type. The failures these philosophers may experience are analogous to the difficulties that arise in real\ncomputer programming when multiple programs need exclusive access to shared resources. These issues are studied in concurrent\nprogramming. The original problems of Dijkstra were related to external devices like tape drives. However, the difficulties\nexemplified by the dining philosophers problem arise far more often when multiple processes access sets of data that are being\nupdated. Complex systems such as operating system kernels use thousands of locks and synchronizations that require strict\nadherence to methods and protocols if such problems as deadlock, starvation, and data corruption are to be avoided.\n6.4.1 https:\/\/eng.libretexts.org\/@go\/page\/48018 Resource hierarchy solution\nThis solution to the problem is the one originally proposed by Dijkstra. It assigns a partial order to the resources (the forks, in this\ncase), and establishes the convention that all resources will be requested in order, and that no two resources unrelated by order will\never be used by a single unit of work at the same time. Here, the resources (forks) will be numbered 1 through 5 and each unit of\nwork (philosopher) will always pick up the lower-numbered fork first, and then the higher-numbered fork, from among the two\nforks they plan to use. The order in which each philosopher puts down the forks does not matter. In this case, if four of the five\nphilosophers simultaneously pick up their lower-numbered fork, only the highest-numbered fork will remain on the table, so the\nfifth philosopher will not be able to pick up any fork. Moreover, only one philosopher will have access to that highest-numbered\nfork, so he will be able to eat using two forks.\nWhile the resource hierarchy solution avoids deadlocks, it is not always practical, especially when the list of required resources is\nnot completely known in advance. For example, if a unit of work holds resources 3 and 5 and then determines it needs resource 2, it\nmust release 5, then 3 before acquiring 2, and then it must re-acquire 3 and 5 in that order. Computer programs that access large\nnumbers of database records would not run efficiently if they were required to release all higher-numbered records before accessing\na new record, making the method impractical for that purpose.\nThe resource hierarchy solution is not fair. If philosopher 1 is slow to take a fork, and if philosopher 2 is quick to think and pick its\nforks back up, then philosopher 1 will never get to pick up both forks. A fair solution must guarantee that each philosopher will\neventually eat, no matter how slowly that philosopher moves relative to the others.\nArbitrator solution\nAnother approach is to guarantee that a philosopher can only pick up both forks or none by introducing an arbitrator, e.g., a waiter.\nIn order to pick up the forks, a philosopher must ask permission of the waiter. The waiter gives permission to only one philosopher\nat a time until the philosopher has picked up both of their forks. Putting down a fork is always allowed. The waiter can be\nimplemented as a mutex. In addition to introducing a new central entity (the waiter), this approach can result in reduced\nparallelism: if a philosopher is eating and one of his neighbors is requesting the forks, all other philosophers must wait until this\nrequest has been fulfilled even if forks for them are still available.\nAdapted from:\n\"Dining philosophers problem\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n6.4: Dining Philosopher Problem is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n6.4.2 https:\/\/eng.libretexts.org\/@go\/page\/48018 CHAPTER OVERVIEW\n7: Memory Management\n7.1: Random Access Memory (RAM) and Read Only Memory (ROM)\n7.2: Memory Hierarchy\n7.3: Requirements for Memory Management\n7.4: Memory Partitioning\n7.4.1: Fixed Partitioning\n7.4.2: Variable Partitioning\n7.4.3: Buddy System\n7.5: Logical vs Physical Address\n7.6: Paging\n7.7: Segmentation\nThis page titled 7: Memory Management is shared under a CC BY-SA license and was authored, remixed, and\/or curated by Patrick McClanahan.\n1 7.1: Random Access Memory (RAM) and Read Only Memory (ROM)\nMemory Basics\nMemory is the most essential element of a computing system because without it computer can\u2019t perform simple tasks. Computer\nmemory is of two basic type \u2013 Primary memory(RAM and ROM) and Secondary memory(hard drive,CD,etc.). Random Access\nMemory (RAM) is primary-volatile memory and Read Only Memory (ROM) is primary-non-volatile memory.\nFigure 7.1.1: (\"Classification of Computer Memory\" by Deepanshi_Mittal, Geeks for Geeks is licensed under CC BY-SA 4.0)\n1. Random Access Memory (RAM)\nIt is also called as read write memory or the main memory or the primary memory.\nThe programs and data that the CPU requires during execution of a program are stored in this memory.\nIt is a volatile memory as the data loses when the power is turned off.\nRAM is further classified into two types- SRAM (Static Random Access Memory) and DRAM (Dynamic Random Access\nMemory).\nFigure 7.1.1: Difference between SRAM and DRAM. (\"Difference between SRAM and DRAM\" by Deepanshi_Mittal, Geeks for\nGeeks is licensed under CC BY-SA 4.0)\n2. Read Only Memory (ROM)\nStores crucial information essential to operate the system, like the program essential to boot the computer.\nIt is not volatile.\nAlways retains its data.\nUsed in embedded systems or where the programming needs no change.\nUsed in calculators and peripheral devices.\nROM is further classified into 4 types- ROM, PROM, EPROM, and EEPROM.\n7.1.1 https:\/\/eng.libretexts.org\/@go\/page\/45636 Types of Read Only Memory (ROM) \u2013\n1. PROM (Programmable read-only memory) \u2013 It can be programmed by user. Once programmed, the data and instructions in\nit cannot be changed.\n2. EPROM (Erasable Programmable read only memory) \u2013 It can be reprogrammed. To erase data from it, expose it to ultra\nviolet light. To reprogram it, erase all the previous data.\n3. EEPROM (Electrically erasable programmable read only memory) \u2013 The data can be erased by applying electric field, no\nneed of ultra violet light. We can erase only portions of the chip.\nFigure 7.1.1: (\"Difference between RAM and ROM\" by Deepanshi_Mittal, Geeks for Geeks is licensed under CC BY-SA 4.0)\nAdapted from:\n\"Random Access Memory (RAM) and Read Only Memory (ROM)\" by Deepanshi_Mittal, Geeks for Geeks is licensed under CC\nBY-SA 4.0\nThis page titled 7.1: Random Access Memory (RAM) and Read Only Memory (ROM) is shared under a CC BY-SA license and was authored,\nremixed, and\/or curated by Patrick McClanahan.\n7.1.2 https:\/\/eng.libretexts.org\/@go\/page\/45636 7.2: Memory Hierarchy\nMemory Hierarchy Design and its Characteristics\nIn computer systems design, the concept of memory hierarchy is an enhancement to organize the computer's memory such that\naccess time to memory is minimized. Memory hierarchy was developed based on a software program's behavior known as locality\nof references.The figure below depicts the different levels of memory hierarchy :\nFigure 7.2.1: (\"Memory Hierarchy\" by RishabhJain12, Geeks for Geeks is licensed under CC BY-SA 4.0)\nThis Memory Hierarchy Design is divided into 2 main types:\n1. External Memory or Secondary Memory\nThis level is comprised of peripheral storage devices which are accessible by the processor via I\/O Module.\n2. Internal Memory or Primary Memory\nThis level is comprised of memory that is directly accessible by the processor.\nWe can infer the following characteristics of Memory Hierarchy Design from the above figure:\n1. Capacity:\nAs we move from top to bottom in the hierarchy, the capacity increases.\n2. Access Time:\nThis represents the time interval between the read\/write request and the availability of the data. As we move from top to bottom\nin the hierarchy, the access time increases.\n3. Performance:\nIn erly computer systems that were designed without the idea of memory hierarchy design, the speed gap increased between the\nCPU registers and main memory due to difference in access time. This results in lower system performance, an enhancement\nwas required. This enhancement was memory hierarchy design which provided the system with greater performance. One of the\nmost significant ways to increase system performance is to minimize how far down the memory hierarchy one has to go to\nmanipulate data. If we can keep system using lower numbered levels (higher up the hierarchy) then we get better performance.\n4. Cost per bit:\nAs we move up the hierarchy - from bottom to top - the cost per bit increases i.e. internal memory is costlier than external\nmemory.\n7.2.1 https:\/\/eng.libretexts.org\/@go\/page\/48673 Adapted from:\n\"Memory Hierarchy Design and its Characteristics\" by RishabhJain12, Geeks for Geeks is licensed under CC BY-SA 4.0\n7.2: Memory Hierarchy is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n7.2.2 https:\/\/eng.libretexts.org\/@go\/page\/48673 7.3: Requirements for Memory Management\nRequirements of Memory Management System\nMemory management keeps track of the status of each memory location, whether it is allocated or free. It allocates the memory\ndynamically to the programs at their request and frees it for reuse when it is no longer needed.\nMemory management is meant to satisfy the following requirements:\n1. Relocation \u2013 The available memory is generally shared among a number of processes in a multiprogramming system, so it is\nnot possible to know in advance which other programs will be resident in main memory at the time of execution of his program.\nSwapping the active processes in and out of the main memory enables the operating system to have a larger pool of ready-to-\nexecute process.\nWhen a program gets swapped out to a disk memory, then it is not always possible that when it is swapped back into main\nmemory then it occupies the previous memory location, since the location may still be occupied by another process. We may\nneed to relocate the process to a different area of memory. Thus there is a possibility that program may be moved in main\nmemory due to swapping.\nFigure 7.3.1: A process occupying a continuous region of main memory.\n(\"Process Image\" by Aditya_04, Geeks for Geeks is licensed under CC BY-SA 4.0)\nThe figure depicts a process image. Every process looks like this in memory. Each process contains: 1) process control blocks;\n2) a program entry point - this is the instruction where the program starts execution; 3) a program section; 4) a data section; and\n5) a stack. The process image is occupying a continuous region of main memory. The operating system will need to know many\nthings including the location of process control information, the execution stack, and the code entry. Within a program, there are\nmemory references in various instructions and these are called logical addresses.\nAfter loading of the program into main memory, the processor and the operating system must be able to translate logical\naddresses into physical addresses. Branch instructions contain the address of the next instruction to be executed. Data reference\ninstructions contain the address of byte or word of data referenced.\n2. Protection \u2013 There is always a danger when we have multiple programs executing at the same time - one program may write to\nthe address space of another program. So every process must be protected against unwanted interference if one process tries to\nwrite into the memory space of another process - whether accidental or incidental. The operating system makes a trade-off\nbetween relocation and protection requirement: in order to satisfy the relocation requirement the difficulty of satisfying the\nprotection requirement increases in difficulty.\nIt is impossible to predict the location of a program in main memory, which is why it is impossible to determine the absolute\naddress at compile time and thereby attempt to assure protection. Most programming languages provide for dynamic calculation\nof address at run time. The memory protection requirement must be satisfied by the processor rather than the operating system\n7.3.1 https:\/\/eng.libretexts.org\/@go\/page\/48679 because the operating system does not necessarily control a process when it occupies the processor. Thus it is not possible to\ncheck the validity of memory references.\n3. Sharing \u2013 A protection mechanism must allow several processes to access the same portion of main memory. This must allow\nfor each processes the ability to access the same copy of the program rather than have their own separate copy.\nThis concept has an advantage. For example, multiple processes may use the same system file and it is natural to load one copy\nof the file in main memory and let it shared by those processes. It is the task of memory management to allow controlled access\nto the shared areas of memory without compromising the protection. Mechanisms are used to support relocation supported\nsharing capabilities.\n4. Logical organization \u2013 Main memory is organized as linear or it can be a one-dimensional address space which consists of a\nsequence of bytes or words. Most of the programs can be organized into modules, some of those are unmodifiable (read-only,\nexecute only) and some of those contain data that can be modified. To effectively deal with a user program, the operating\nsystem and computer hardware must support a basic module to provide the required protection and sharing. It has the following\nadvantages:\nModules are written and compiled independently and all the references from one module to another module are resolved by\nthe system at run time.\nDifferent modules are provided with different degrees of protection.\nThere are mechanisms by which modules can be shared among processes. Sharing can be provided on a module level that\nlets the user specify the sharing that is desired.\n5. Physical organization \u2013 The structure of computer memory has two levels referred to as main memory and secondary memory.\nMain memory is relatively very fast and costly as compared to the secondary memory. Main memory is volatile. Thus\nsecondary memory is provided for storage of data on a long-term basis while the main memory holds currently used programs.\nThe major system concern between main memory and secondary memory is the flow of information and it is impractical for\nprogrammers to understand this for two reasons:\nThe programmer may engage in a practice known as overlaying when the main memory available for a program and its data\nmay be insufficient. It allows different modules to be assigned to the same region of memory. One disadvantage is that it is\ntime-consuming for the programmer.\nIn a multiprogramming environment, the programmer does not know how much space will be available at the time of coding\nand where that space will be located inside the memory.\nAdapted from:\n\"Requirements of Memory Management System\" by Aditya_04, Geeks for Geeks is licensed under CC BY-SA 4.0\n7.3: Requirements for Memory Management is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n7.3.2 https:\/\/eng.libretexts.org\/@go\/page\/48679 7.4: Memory Partitioning\nPartition Allocation Methods in Memory Management\nIn the world of computer operating system, there are four common memory management techniques. They are:\n1. Single contiguous allocation: Simplest allocation method used by MS-DOS. All memory (except some reserved for OS) is\navailable to a process.\n2. Partitioned allocation: Memory is divided into different blocks or partitions. Each process is allocated according to the\nrequirement.\n3. Paged memory management: Memory is divided into fixed-sized units called page frames, used in a virtual memory\nenvironment.\n4. Segmented memory management: Memory is divided into different segments (a segment is a logical grouping of the process\u2019\ndata or code).In this management, allocated memory doesn\u2019t have to be contiguous.\nMost of the operating systems (for example Windows and Linux) use segmentation with paging. A process is divided into segments\nand individual segments have pages.\nIn partition allocation, when there is more than one partition freely available to accommodate a process\u2019s request, a partition must\nbe selected. To choose a particular partition, a partition allocation method is needed. A partition allocation method is considered\nbetter if it avoids internal fragmentation.\nWhen it is time to load a process into the main memory and if there is more than one free block of memory of sufficient size then\nthe OS decides which free block to allocate.\nThere are different Placement Algorithm:\n1. First Fit: In the first fit, the partition is allocated which is the first sufficient block from the top of main memory. It scans\nmemory from the beginning and chooses the first available block that is large enough. Thus it allocates the first hole that is large\nenough.\nFigure 7.4.1: First Fit. (\"First Fit\" by deepakmkoshy, Geeks for Geeks is licensed under CC BY-SA 4.0)\n2. Best Fit Allocate the process to the partition which is the first smallest sufficient partition among the free available partition. It\nsearches the entire list of holes to find the smallest hole whose size is greater than or equal to the size of the process.\nFigure 7.4.1: Best Fit. (\"Best Fit\" by deepakmkoshy, Geeks for Geeks is licensed under CC BY-SA 4.0)\n3. Worst Fit Allocate the process to the partition which is the largest sufficient among the freely available partitions available in\nthe main memory. It is opposite to the best-fit algorithm. It searches the entire list of holes to find the largest hole and allocate it\n7.4.1 https:\/\/eng.libretexts.org\/@go\/page\/48680 to process.\nFigure 7.4.1: Worst Fit. (\"Worst Fit\" by deepakmkoshy, Geeks for Geeks is licensed under CC BY-SA 4.0)\n4. Next Fit: Next fit is similar to the first fit but it will search for the first sufficient partition from the last allocation point.\nAdapted from:\n\"Partition Allocation Methods in Memory Management\" by deepakmkoshy, Geeks for Geeks is licensed under CC BY-SA 4.0\n7.4: Memory Partitioning is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n7.4.2 https:\/\/eng.libretexts.org\/@go\/page\/48680 7.4.1: Fixed Partitioning\nFixed (or static) Partitioning in Operating System\nThis is the oldest and simplest technique that allows more than one processes to be loaded into main memory. In this partitioning\nmethod the number of partitions (non-overlapping) in RAM are all a fixed size, but they may or may not be same size. This method\nof partitioning provides for contiguous allocation, hence no spanning is allowed. The partition sizes are made before execution or\nduring system configuration.\nFigure 7.4.1.1: Fixed Sized partition Example. (\"Fixed Sized Partition\" by Vidhayak_Chacha, Geeks for Geeks is licensed\nunder CC BY-SA 4.0)\nAs illustrated in above figure, first process is only consuming 1MB out of 4MB in the main memory.\nHence, Internal Fragmentation in first block is (4-1) = 3MB.\nSum of Internal Fragmentation in every block = (4-1)+(8-7)+(8-7)+(16-14)= 3+1+1+2 = 7MB.\nSuppose process P5 of size 7MB comes. But this process cannot be accommodated inspite of available free space because of\ncontiguous allocation (as spanning is not allowed). Hence, 7MB becomes part of External Fragmentation.\nThere are some advantages and disadvantages of fixed partitioning.\nAdvantages of Fixed Partitioning\nEasy to implement:\nAlgorithms needed to implement Fixed Partitioning are easy to implement. It simply requires putting a process into certain\npartition without focussing on the emergence of Internal and External Fragmentation.\nLittle OS overhead:\nProcessing of Fixed Partitioning require lesser excess and indirect computational power.\nDisadvantages of Fixed Partitioning\nInternal Fragmentation:\nMain memory use is inefficient. Any program, no matter how small, occupies an entire partition. This can cause internal\nfragmentation.\nExternal Fragmentation:\nThe total unused space (as stated above) of various partitions cannot be used to load the processes even though there is space\navailable but not in the contiguous form (as spanning is not allowed).\nLimit process size:\nProcess of size greater than size of partition in Main Memory cannot be accommodated. Partition size cannot be varied\naccording to the size of incoming process\u2019s size. Hence, process size of 32MB in above stated example is invalid.\nLimitation on Degree of Multiprogramming:\nPartition in Main Memory are made before execution or during system configure. Main Memory is divided into fixed number\n7.4.1.1 https:\/\/eng.libretexts.org\/@go\/page\/48686 of partition. Suppose if there are partitions in RAM and are the number of processes, then condition must\nbe fulfilled. Number of processes greater than number of partitions in RAM is invalid in Fixed Partitioning.\nAdapted from:\n\"Fixed (or static) Partitioning in Operating System\" by Vidhayak_Chacha, Geeks for Geeks is licensed under CC BY-SA 4.0\n7.4.1: Fixed Partitioning is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n7.4.1.2 https:\/\/eng.libretexts.org\/@go\/page\/48686 7.4.2: Variable Partitioning\nVariable (or dynamic) Partitioning in Operating System\nVariable partitioning is part of the contiguous allocation technique. It is used to alleviate the problem faced by fixed partitioning.\nAs opposed to fixed partitioning, in variable partitioning, partitions are not created until a process executes. At the time it is read\ninto main memory, the process is given exactly the amount of memory needed. This technique, like the fixed partitioning scheme\npreviously discussed have been replaced by more complex and efficient techniques.\nVarious features associated with variable partitioning.\nInitially RAM is empty and partitions are made during the run-time according to process\u2019s need instead of partitioning during\nsystem configure.\nThe size of partition will be equal to incoming process.\nThe partition size varies according to the need of the process so that the internal fragmentation can be avoided to ensure\nefficient utilisation of RAM.\nNumber of partitions in RAM is not fixed and depends on the number of incoming process and Main Memory\u2019s size.\nFigure 7.4.2.1: Variable Partitioned Memory. (\"Variable Patitiong\" by Vidhayak_Chacha, Geeks for Geeks is licensed under CC\nBY-SA 4.0)\nAdvantages of Variable Partitioning\n1. No Internal Fragmentation:\nIn variable Partitioning, space in main memory is allocated strictly according to the need of process, hence there is no case of\ninternal fragmentation. There will be no unused space left in the partition.\n2. No restriction on Degree of Multiprogramming:\nMore number of processes can be accommodated due to absence of internal fragmentation. A process can be loaded until the\nmemory is empty.\n3. No Limitation on the size of the process:\nIn Fixed partitioning, the process with the size greater than the size of the largest partition could not be loaded and process can\nnot be divided as it is invalid in contiguous allocation technique. Here, In variable partitioning, the process size can\u2019t be\nrestricted since the partition size is decided according to the process size.\nDisadvantages of Variable Partitioning\n1. Difficult Implementation:\nImplementing variable Partitioning is difficult as compared to Fixed Partitioning as it involves allocation of memory during\nrun-time rather than during system configure.\n2. External Fragmentation:\nThere will be external fragmentation inspite of absence of internal fragmentation.\nFor example, suppose in above example- process P1(2MB) and process P3(1MB) completed their execution. Hence two spaces\nare left i.e. 2MB and 1MB. Let\u2019s suppose process P5 of size 3MB comes. The empty space in memory cannot be allocated as no\n7.4.2.1 https:\/\/eng.libretexts.org\/@go\/page\/48687 spanning is allowed in contiguous allocation. The rule says that process must be contiguously present in main memory to get\nexecuted. Hence it results in External Fragmentation.\nAdapted from:\n\"Variable (or dynamic) Partitioning in Operating System\" by Vidhayak_Chacha, Geeks for Geeks is licensed under CC BY-SA 4.0\n7.4.2: Variable Partitioning is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n7.4.2.2 https:\/\/eng.libretexts.org\/@go\/page\/48687 7.4.3: Buddy System\nBuddy System \u2013 Memory allocation technique\nStatic partition schemes suffer from the limitation of having the fixed number of active processes and the usage of space may\nalso not be optimal. The buddy system is a memory allocation and management algorithm that manages memory in power of two\nincrements. Assume the memory size is 2U, suppose a size of S is required.\nIf 2U-1<S<=2U: Allocate the whole block\nElse: Recursively divide the block equally and test the condition at each time, when it satisfies, allocate the block and get out\nthe loop.\nSystem also keep the record of all the unallocated blocks each and can merge these different size blocks to make one big chunk.\nAdvantage\nEasy to implement a buddy system\nAllocates block of correct size\nIt is easy to merge adjacent holes\nFast to allocate memory and de-allocating memory\nDisadvantage\nIt requires all allocation unit to be powers of two\nIt leads to internal fragmentation\nExample\nConsider a system having buddy system with physical address space 128 KB.Calculate the size of partition for 18 KB process.\nSolution\nFigure 7.4.3.1: Buddy System Partitioning\n(\"Buddy System Partitioning\" by Samit Mandal, Geeks for Geeks is licensed under CC BY-SA 4.0)\nSo, size of partition for 18 KB process = 32 KB. It divides by 2, till possible to get minimum block to fit 18 KB.\nAdapted from:\n\"Buddy System \u2013 Memory allocation technique\" by Samit Mandal, Geeks for Geeks is licensed under CC BY-SA 4.0\n7.4.3: Buddy System is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n7.4.3.1 https:\/\/eng.libretexts.org\/@go\/page\/48688 7.5: Logical vs Physical Address\nLogical and Physical Addresses in an Operating System\nA logical address is generated by CPU while a program is running. Since a logical address does not physically exists it is also\nknown as a virtual address. This address is used as a reference by the CPU to access the actual physical memory location.\nThere is a hardware device called Memory-Management Unit is used for mapping logical address to its corresponding physical\naddress.\nA physical address identifies the physical location of a specific data element in memory. The user never directly deals with the\nphysical address but can determine the physical address by its corresponding logical address. The user program generates the\nlogical address and believes that the program is running in this logical address space, but the program needs physical memory for\nits execution, therefore, the logical address must be mapped to the physical address by the MMU before the addresses are used. The\nterm physical address space is used for all physical addresses corresponding to the logical addresses in a logical address space.\nFigure 7.5.1: MMU Operation.\n(\"MMU Operation\" by Ankit_Bisht, Geeks for Geeks is licensed under CC BY-SA 4.0)\nDifferences Between Logical and Physical Address in Operating System\n1. The basic difference between Logical and physical address is that Logical address is generated by CPU in perspective of a\nprogram whereas the physical address is a location that exists in the memory unit.\n2. Logical Address Space is the set of all logical addresses generated by CPU for a program whereas the set of all physical address\nmapped to corresponding logical addresses is called Physical Address Space.\n3. The logical address does not exist physically in the memory whereas physical address is a location in the memory that can be\naccessed physically.\n4. Identical logical addresses are generated by Compile-time and Load time address binding methods whereas they differs from\neach other in run-time address binding method.\n5. The logical address is generated by the CPU while the program is running whereas the physical address is computed by the\nMemory Management Unit (MMU).\nComparison Chart:\nParamenter LOGICAL ADDRESS PHYSICAL ADDRESS\nBasic generated by CPU location in a memory unit\nAddress Logical Address Space is set of all logical addresses Physical Address is set of all physical addresses mapped to\nSpace generated by CPU in reference to a program. the corresponding logical addresses.\nVisibility User can view the logical address of a program. User can never view physical address of program.\n7.5.1 https:\/\/eng.libretexts.org\/@go\/page\/48689 Paramenter LOGICAL ADDRESS PHYSICAL ADDRESS\nGeneration generated by the CPU Computed by MMU\nAccess The user can use the logical address to access the physical The user can indirectly access physical address but not\naddress. directly.\nMapping Virtual Addresses to Physical Addresses\nMemory consists of large array addresses. It is the responsibility of the CPU to fetch instruction address from the program counter.\nThese instruction may cause loading or storing to specific memory address.\nAddress binding is the process of mapping from one address space to another address space. Logical addresses are generated by\nCPU during execution whereas physical address refers to location in a physical memory unit (the one that is loaded into memory).\nNote that users deal only with logical address (virtual address). The logical address is translated by the MMU. The output of this\nprocess is the appropriate physical address of the data in RAM.\nAn address binding can be done in three different ways:\nCompile Time \u2013 If at compile time you know where a process will reside in memory then an absolute address can be generated -\nthat is a physical address is generated in the program executable during compilation. Loading such an executable into memory is\nvery fast. But if the generated address space is occupied by other process, then the program crashes and it becomes necessary to\nrecompile the program to use a virtual address space.\nLoad time \u2013 If it is not known at the compile time where process will reside then relocatable addresses will be generated. The\nloader translates the relocatable address to absolute address. The base address of the process in main memory is added to all logical\naddresses by the loader to generate absolute address. If the base address of the process changes then we need to reload the process\nagain.\nExecution time- The instructions are already loaded into memory and are processed by the CPU. Additional memory may be\nallocated and\/or deallocated at this time. This process is used if the process can be moved from one memory to another during\nexecution (dynamic linking-Linking that is done during load or run time). e.g \u2013 Compaction.\nMMU(Memory Management Unit)-\nThe run time mapping between virtual address and physical address is done by a hardware device known as MMU.\nIn memory management, Operating System will handle the processes and moves the processes between disk and memory for\nexecution . It keeps the track of available and used memory.\nAdapted from:\n\"Logical and Physical Address in Operating System\" by Ankit_Bisht, Geeks for Geeks is licensed under CC BY-SA 4.0\n\"Mapping Virtual Addresses to Physical Addresses\" by NEERAJ NEGI, Geeks for Geeks is licensed under CC BY-SA 4.0\n7.5: Logical vs Physical Address is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n7.5.2 https:\/\/eng.libretexts.org\/@go\/page\/48689 7.6: Paging\nMemory Paging\nA page, memory page, or virtual page is a fixed-length contiguous block of virtual memory, described by a single entry in the page\ntable. It is the smallest unit of data for memory management in a virtual memory operating system. Similarly, a page frame is the\nsmallest fixed-length contiguous block of physical memory into which memory pages are mapped by the operating system\nIn computer operating systems, memory paging is a memory management scheme by which a computer stores and retrieves data\nfrom secondary storage for use in main memory. In this scheme, the operating system retrieves data from secondary storage\n(usually the swap space on the disk) in same-size blocks called pages. Paging is an important part of virtual memory\nimplementations in modern operating systems, using secondary storage to let programs exceed the size of available physical\nmemory.\nPage Table\nPart of the concept of paging is the page table, which is a data structure used by the virtual memory system to store the mapping\nbetween virtual addresses and physical addresses. Virtual addresses are used by the executed program, while physical addresses are\nused by the hardware, or more specifically, by the RAM subsystem. The page table is a key component of virtual address\ntranslation which is necessary to access data in memory.\nRole of the page table\nIn operating systems that use virtual memory, every process is given the impression that it is working with large, contiguous\nsections of memory. Physically, the memory of each process may be dispersed across different areas of physical memory, or may\nhave been moved (paged out) to another storage, typically to a hard disk drive or solid state drive.\nWhen a process requests access to data in its memory, it is the responsibility of the operating system to map the virtual address\nprovided by the process to the physical address of the actual memory where that data is stored. The page table is where the\noperating system stores its mappings of virtual addresses to physical addresses, with each mapping also known as a page table\nentry (PTE).\n7.6.1 https:\/\/eng.libretexts.org\/@go\/page\/48700 Figure 7.6.1: Mapping Virtual Memory to Physical Memory.\n(\"Mapping Virtual Addresses to Physical Addresses\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0)\nThe above image shows the relationship between pages addressed by virtual addresses and the pages in physical memory, within a\nsimple address space scheme. Physical memory can contain pages belonging to many processes. If a page is not used for a period\nof time, the operating system can, if deemed necessary, move that page to secondary storage. The purple indicates where in\nphysical memory the pieces of the executing processes reside - BUT - in the virtual environments, the memory is contiguous.\nThe translation process\nThe CPU's memory management unit (MMU) stores a cache of recently used mappings from the operating system's page table.\nThis is called the translation lookaside buffer (TLB), which is an associative cache.\nFigure 7.6.1: Actions taken upon a virtual to physical address translation request.\n(\"Actions taken upon a virtual to physical address translation request\" by Multiple Contributors, Wikipedia is licensed under CC\nBY-SA 3.0)\n7.6.2 https:\/\/eng.libretexts.org\/@go\/page\/48700 When a virtual address needs to be translated into a physical address, the TLB is searched first. If a match is found (a TLB hit), the\nphysical address is returned and memory access can continue. However, if there is no match (called a TLB miss), the memory\nmanagement unit, or the operating system TLB miss handler, will typically look up the address mapping in the page table to see\nwhether a mapping exists (a page walk). If one exists, it is written back to the TLB (this must be done, as the hardware accesses\nmemory through the TLB in a virtual memory system), and the faulting instruction is restarted (this may happen in parallel as\nwell). The subsequent translation will find a TLB hit, and the memory access will continue.\nTranslation failures\nThe page table lookup may fail, triggering a page fault, for two reasons:\nThe lookup may fail if there is no translation available for the virtual address, meaning that virtual address is invalid. This will\ntypically occur because of a programming error, and the operating system must take some action to deal with the problem. On\nmodern operating systems, it will cause a segmentation fault signal being sent to the offending program.\nThe lookup may also fail if the page is currently not resident in physical memory. This will occur if the requested page has\nbeen moved out of physical memory to make room for another page. In this case the page is paged out to a secondary store\nlocated on a medium such as a hard disk drive (this secondary store, or \"backing store\", is often called a \"swap partition\" if it is\na disk partition, or a swap file, \"swapfile\" or \"page file\" if it is a file). When this happens the page needs to be taken from disk\nand put back into physical memory. A similar mechanism is used for memory-mapped files, which are mapped to virtual\nmemory and loaded to physical memory on demand.\nWhen physical memory is not full this is a simple operation; the page is written back into physical memory, the page table and TLB\nare updated, and the instruction is restarted. However, when physical memory is full, one or more pages in physical memory will\nneed to be paged out to make room for the requested page. The page table needs to be updated to mark that the pages that were\npreviously in physical memory are no longer there, and to mark that the page that was on disk is now in physical memory. The TLB\nalso needs to be updated, including removal of the paged-out page from it, and the instruction restarted. Which page to page out is\nthe subject of page replacement algorithms.\nSome MMUs trigger a page fault for other reasons, whether or not the page is currently resident in physical memory and mapped\ninto the virtual address space of a process:\nAttempting to write when the page table has the read-only bit set causes a page fault. This is a normal part of many operating\nsystem's implementation of copy-on-write; it may also occur when a write is done to a location from which the process is\nallowed to read but to which it is not allowed to write, in which case a signal is delivered to the process.\nAttempting to execute code when the page table has the NX bit (no-execute bit) set in the page table causes a page fault. This\ncan be used by an operating system, in combination with the read-only bit, to provide a Write XOR Execute feature that stops\nsome kinds of exploits\nAdapted from:\n\"Memory paging\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n\"Page (computer memory)\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n\"Page table\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n7.6: Paging is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n7.6.3 https:\/\/eng.libretexts.org\/@go\/page\/48700 7.7: Segmentation\nSegmentation in Operating System\nA process is divided into segments. The segments are not required to be of the same sizes.\nThere are 2 types of segmentation:\n1. Virtual memory segmentation\nEach process is divided into a number of segments, not all of which are resident at any one point in time.\n2. Simple segmentation\nEach process is divided into a number of segments, all of which are loaded into memory at run time, though not necessarily\ncontiguously.\nThere is no simple relationship between logical addresses and physical addresses in segmentation. A table stores the information\nabout all such segments and is called Segment Table.\nFigure 7.7.1: Segmentation Table Mapping to Physical Address.\n(\"Segmentation Table Mapping to Physical Address\" by VaibhavRai3, Geeks for Geeks is licensed under CC BY-SA 4.0)\nThe Segment Table maps the logical address, made up of the base address and the limit, into one-dimensional physical address. It\u2019s\neach table entry has:\nBase Address: It contains the starting physical address where the segments reside in memory.\nLimit: It specifies the length of the segment.\nTranslation of a two dimensional Logical Address to one dimensional Physical Address.\n7.7.1 https:\/\/eng.libretexts.org\/@go\/page\/48712 Figure 7.7.1: Translate Logical Address to Physical Address.\n(\"Translate Logical Address to Physical Address\" by VaibhavRai3, Geeks for Geeks is licensed under CC BY-SA 4.0)\nAddress generated by the CPU is divided into:\nSegment number (s): Number of bits required to represent the segment.\nSegment offset (d): Number of bits required to represent the size of the segment.\nWalking through the diagram above:\n1. CPU generates a 2 part logival address.\n2. The segment number is used to get the Limit and the Base Address value from the segment table.\n3. If the segment offset (d) is less than the Limit value from the segment table then\nThe Base Address returned from the segment table, points to the beginning of the segment\nThe Limit value points to the end of the segment in physcial memory.\nAdvantages of Segmentation\nNo Internal fragmentation.\nSegment Table consumes less space in comparison to Page table in paging.\nDisadvantage of Segmentation\nAs processes are loaded and removed from the memory, the free memory space is broken into little pieces, causing External\nfragmentation.\nAdapted from:\n\"Segmentation in Operating System\" by VaibhavRai3, Geeks for Geeks is licensed under CC BY-SA 4.0\n7.7: Segmentation is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n7.7.2 https:\/\/eng.libretexts.org\/@go\/page\/48712 CHAPTER OVERVIEW\n8: Virtual Memory\n8.1: Memory Paging\n8.1.1: Memory Paging - Page Replacement\n8.2: Virtual Memory in the Operating System\nThis page titled 8: Virtual Memory is shared under a CC BY-SA license and was authored, remixed, and\/or curated by Patrick McClanahan.\n1 8.1: Memory Paging\nAlthough memory paging is NOT specific to virtual memory, paging is discussed a lot in the discussion of virtual memory. So, we\nwill spend a moment making sure we are up on the concepts we need to properly study virtual memory.\nMemory Paging\nIn computer operating systems, memory paging is a memory management scheme by which a computer stores and retrieves data\nfrom secondary storage for use in main memory. In this scheme, the operating system retrieves data from secondary storage in\nsame-size blocks called pages. Paging is an important part of virtual memory implementations in modern operating systems, using\nsecondary storage to let programs exceed the size of available physical memory.\nFor simplicity, main memory is called \"RAM\" (an acronym of \"random-access memory\") and secondary storage is called \"disk\" (a\nshorthand for \"hard disk drive, drum memory or solid-state drive\"), but the concepts do not depend on whether these terms apply\nliterally to a specific computer system.\nPage faults\nWhen a process tries to reference a page not currently present in RAM, the processor treats this invalid memory reference as a page\nfault and transfers control from the program to the operating system. The operating system must:\nDetermine the location of the data on disk.\nObtain an empty page frame in RAM to use as a container for the data.\nLoad the requested data into the available page frame.\nUpdate the page table to refer to the new page frame.\nReturn control to the program, transparently retrying the instruction that caused the page fault.\nWhen all page frames are in use, the operating system must select a page frame to reuse for the page the program now needs. If the\nevicted page frame was dynamically allocated by a program to hold data, or if a program modified it since it was read into RAM\n(in other words, if it has become \"dirty\"), it must be written out to disk before being freed. If a program later references the evicted\npage, another page fault occurs and the page must be read back into RAM.\nThe method the operating system uses to select the page frame to reuse, which is its page replacement algorithm, is important to\nefficiency. The operating system predicts the page frame least likely to be needed soon, often through the least recently used (LRU)\nalgorithm or an algorithm based on the program's working set. To further increase responsiveness, paging systems may predict\nwhich pages will be needed soon, preemptively loading them into RAM before a program references them.\nThrashing\nAfter completing initialization, most programs operate on a small number of code and data pages compared to the total memory the\nprogram requires. The pages most frequently accessed are called the working set.\nWhen the working set is a small percentage of the system's total number of pages, virtual memory systems work most efficiently\nand an insignificant amount of computing is spent resolving page faults. As the working set grows, resolving page faults remains\nmanageable until the growth reaches a critical point. Then faults go up dramatically and the time spent resolving them overwhelms\ntime spent on the computing the program was written to do. This condition is referred to as thrashing. Thrashing occurs on a\nprogram that works with huge data structures, as its large working set causes continual page faults that drastically slow down the\nsystem. Satisfying page faults may require freeing pages that will soon have to be re-read from disk. \"Thrashing\" is also used in\ncontexts other than virtual memory systems; for example, to describe cache issues in computing or silly window syndrome in\nnetworking.\nA worst case might occur on VAX processors. A single MOVL crossing a page boundary could have a source operand using a\ndisplacement deferred addressing mode, where the longword containing the operand address crosses a page boundary, and a\ndestination operand using a displacement deferred addressing mode, where the longword containing the operand address crosses a\npage boundary, and the source and destination could both cross page boundaries. This single instruction references ten pages; if not\nall are in RAM, each will cause a page fault. As each fault occurs the operating system needs to go through the extensive memory\nmanagement routines perhaps causing multiple I\/Os which might including writing other process pages to disk and reading pages\nof the active process from disk. If the operating system could not allocate ten pages to this program, then remedying the page fault\nwould discard another page the instruction needs, and any restart of the instruction would fault again.\n8.1.1 https:\/\/eng.libretexts.org\/@go\/page\/48916 To decrease excessive paging and resolve thrashing problems, a user can increase the number of pages available per program,\neither by running fewer programs concurrently or increasing the amount of RAM in the computer.\nSharing\nIn multi-programming or in a multi-user environment, many users may execute the same program, written so that its code and data\nare in separate pages. To minimize RAM use, all users share a single copy of the program. Each process's page table is set up so\nthat the pages that address code point to the single shared copy, while the pages that address data point to different physical pages\nfor each process.\nDifferent programs might also use the same libraries. To save space, only one copy of the shared library is loaded into physical\nmemory. Programs which use the same library have virtual addresses that map to the same pages (which contain the library's code\nand data). When programs want to modify the library's code, they use copy-on-write, so memory is only allocated when needed.\nShared memory is an efficient way of communication between programs. Programs can share pages in memory, and then write and\nread to exchange data.\nAdapted from:\n\"Virtual memory\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n\"Demand paging\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n\"Memory paging\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n\"Page replacement algorithm\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n8.1: Memory Paging is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n8.1.2 https:\/\/eng.libretexts.org\/@go\/page\/48916 8.1.1: Memory Paging - Page Replacement\nPage replacement algorithm\nIn a computer operating system that uses paging for virtual memory management, page replacement algorithms decide which\nmemory pages to page out, sometimes called swap out, or write to disk, when a page of memory needs to be allocated. Page\nreplacement happens when a requested page is not in memory (page fault) and a free page cannot be used to satisfy the allocation,\neither because there are none, or because the number of free pages is lower than some threshold.\nWhen the page that was selected for replacement and paged out is referenced again it has to be paged in (read in from disk), and\nthis involves waiting for I\/O completion. This determines the quality of the page replacement algorithm: the less time waiting for\npage-ins, the better the algorithm. A page replacement algorithm looks at the limited information about accesses to the pages\nprovided by hardware, and tries to guess which pages should be replaced to minimize the total number of page misses, while\nbalancing this with the costs (primary storage and processor time) of the algorithm itself.\nLocal vs. global replacement\nReplacement algorithms can be local or global.\nWhen a process incurs a page fault, a local page replacement algorithm selects for replacement some page that belongs to that same\nprocess (or a group of processes sharing a memory partition). A global replacement algorithm is free to select any page in memory.\nLocal page replacement assumes some form of memory partitioning that determines how many pages are to be assigned to a given\nprocess or a group of processes. Most popular forms of partitioning are fixed partitioning and balanced set algorithms based on the\nworking set model. The advantage of local page replacement is its scalability: each process can handle its page faults\nindependently, leading to more consistent performance for that process. However global page replacement is more efficient on an\noverall system basis.\nDetecting which pages are referenced and modified\nModern general purpose computers and some embedded processors have support for virtual memory. Each process has its own\nvirtual address space. A page table maps a subset of the process virtual addresses to physical addresses. In addition, in most\narchitectures the page table holds an \"access\" bit and a \"dirty\" bit for each page in the page table. The CPU sets the access bit when\nthe process reads or writes memory in that page. The CPU sets the dirty bit when the process writes memory in that page. The\noperating system can modify the access and dirty bits. The operating system can detect accesses to memory and files through the\nfollowing means:\nBy clearing the access bit in pages present in the process' page table. After some time, the OS scans the page table looking for\npages that had the access bit set by the CPU. This is fast because the access bit it set automatically by the CPU and inaccurate\nbecause the OS does not immediately receives notice of the access nor does it have information about the order in which the\nprocess accessed these pages.\nBy removing pages from the process' page table without necessarily removing them from physical memory. The next access to that\npage is detected immediately because it causes a page fault. This is slow because a page fault involves a context switch to the OS,\nsoftware lookup for the corresponding physical address, modification of the page table and a context switch back to the process and\naccurate because the access is detected immediately after it occurs.\nDirectly when the process makes system calls that potentially access the page cache like read and write in POSIX.\nPrecleaning\nMost replacement algorithms simply return the target page as their result. This means that if target page is dirty (that is, contains\ndata that have to be written to the stable storage before page can be reclaimed), I\/O has to be initiated to send that page to the stable\nstorage (to clean the page). In the early days of virtual memory, time spent on cleaning was not of much concern, because virtual\nmemory was first implemented on systems with full duplex channels to the stable storage, and cleaning was customarily\noverlapped with paging. Contemporary commodity hardware, on the other hand, does not support full duplex transfers, and\ncleaning of target pages becomes an issue.\nTo deal with this situation, various precleaning policies are implemented. Precleaning is the mechanism that starts I\/O on dirty\npages that are (likely) to be replaced soon. The idea is that by the time the precleaned page is actually selected for the replacement,\n8.1.1.1 https:\/\/eng.libretexts.org\/@go\/page\/48917 the I\/O will complete and the page will be clean. Precleaning assumes that it is possible to identify pages that will be replaced next.\nPrecleaning that is too eager can waste I\/O bandwidth by writing pages that manage to get re-dirtied before being selected for\nreplacement.\nDemand Paging Basic concept\nDemand paging follows that pages should only be brought into memory if the executing process demands them. This is often\nreferred to as lazy evaluation as only those pages demanded by the process are swapped from secondary storage to main memory.\nContrast this to pure swapping, where all memory for a process is swapped from secondary storage to main memory during the\nprocess startup.\nCommonly, to achieve this process a page table implementation is used. The page table maps logical memory to physical memory.\nThe page table uses a bitwise operator to mark if a page is valid or invalid. A valid page is one that currently resides in main\nmemory. An invalid page is one that currently resides in secondary memory. When a process tries to access a page, the following\nsteps are generally followed:\nAttempt to access page.\nIf page is valid (in memory) then continue processing instruction as normal.\nIf page is invalid then a page-fault trap occurs.\nCheck if the memory reference is a valid reference to a location on secondary memory. If not, the process is terminated (illegal\nmemory access). Otherwise, we have to page in the required page.\nSchedule disk operation to read the desired page into main memory.\nRestart the instruction that was interrupted by the operating system trap.\nAdvantages\nDemand paging, as opposed to loading all pages immediately:\nOnly loads pages that are demanded by the executing process.\nAs there is more space in main memory, more processes can be loaded, reducing the context switching time, which utilizes\nlarge amounts of resources.\nLess loading latency occurs at program startup, as less information is accessed from secondary storage and less information is\nbrought into main memory.\nAs main memory is expensive compared to secondary memory, this technique helps significantly reduce the bill of material\n(BOM) cost in smart phones for example. Symbian OS had this feature.\nDisadvantages\nIndividual programs face extra latency when they access a page for the first time.\nLow-cost, low-power embedded systems may not have a memory management unit that supports page replacement.\nMemory management with page replacement algorithms becomes slightly more complex.\nPossible security risks, including vulnerability to timing attacks; see Percival, Colin (2005-05-13). \"Cache missing for fun and\nprofit\" (PDF). BSDCan 2005. (specifically the virtual memory attack in section 2).\nThrashing which may occur due to repeated page faults.\nAnticipatory paging\nSome systems attempt to reduce latency of Demand paging by guessing which pages not in RAM are likely to be needed soon, and\npre-loading such pages into RAM, before that page is requested. (This is often in combination with pre-cleaning, which guesses\nwhich pages currently in RAM are not likely to be needed soon, and pre-writing them out to storage).\nWhen a page fault occurs, \"anticipatory paging\" systems will not only bring in the referenced page, but also the next few\nconsecutive pages (analogous to a prefetch input queue in a CPU).\nThe swap prefetch mechanism goes even further in loading pages (even if they are not consecutive) that are likely to be needed\nsoon.\nAdapted from:\n\"Virtual memory\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n\"Demand paging\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n8.1.1.2 https:\/\/eng.libretexts.org\/@go\/page\/48917 \"Memory paging\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n\"Page replacement algorithm\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n8.1.1: Memory Paging - Page Replacement is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n8.1.1.3 https:\/\/eng.libretexts.org\/@go\/page\/48917 8.2: Virtual Memory in the Operating System\nVirtual Memory Intro\nIn computing, virtual memory, or virtual storage is a memory management technique that provides an \"idealized abstraction of the\nstorage resources that are actually available on a given machine\" which \"creates the illusion to users of a very large (main)\nmemory\".\nThe computer's operating system, using a combination of hardware and software, maps memory addresses used by a program,\ncalled virtual addresses, into physical addresses in computer memory. Main storage, as seen by a process or task, appears as a\ncontiguous address space or collection of contiguous segments. The operating system manages virtual address spaces and the\nassignment of real memory to virtual memory. Address translation hardware in the CPU, often referred to as a memory\nmanagement unit (MMU), automatically translates virtual addresses to physical addresses. Software within the operating system\nmay extend these capabilities to provide a virtual address space that can exceed the capacity of real memory and thus reference\nmore memory than is physically present in the computer.\nThe primary benefits of virtual memory include freeing applications from having to manage a shared memory space, ability to\nshare memory used by libraries between processes, increased security due to memory isolation, and being able to conceptually use\nmore memory than might be physically available, using the technique of paging or segmentation.\nProperties of Virtual Memory\nVirtual memory makes application programming easier by hiding fragmentation of physical memory; by delegating to the kernel\nthe burden of managing the memory hierarchy (eliminating the need for the program to handle overlays explicitly); and, when each\nprocess is run in its own dedicated address space, by obviating the need to relocate program code or to access memory with relative\naddressing.\nPaged virtual memory\nNearly all current implementations of virtual memory divide a virtual address space into pages, blocks of contiguous virtual\nmemory addresses. Pages on contemporary systems are usually at least 4 kilobytes in size; systems with large virtual address\nranges or amounts of real memory generally use larger page sizes\nPage tables\nPage tables are used to translate the virtual addresses seen by the application into physical addresses used by the hardware to\nprocess instructions; such hardware that handles this specific translation is often known as the memory management unit. Each\nentry in the page table holds a flag indicating whether the corresponding page is in real memory or not. If it is in real memory, the\npage table entry will contain the real memory address at which the page is stored. When a reference is made to a page by the\nhardware, if the page table entry for the page indicates that it is not currently in real memory, the hardware raises a page fault\nexception, invoking the paging supervisor component of the operating system.\nSystems can have one page table for the whole system, separate page tables for each application and segment, a tree of page tables\nfor large segments or some combination of these. If there is only one page table, different applications running at the same time use\ndifferent parts of a single range of virtual addresses. If there are multiple page or segment tables, there are multiple virtual address\nspaces and concurrent applications with separate page tables redirect to different real addresses.\nSome earlier systems with smaller real memory sizes, such as the SDS 940, used page registers instead of page tables in memory\nfor address translation.\nPaging supervisor\nThis part of the operating system creates and manages page tables. If the hardware raises a page fault exception, the paging\nsupervisor accesses secondary storage, returns the page that has the virtual address that resulted in the page fault, updates the page\ntables to reflect the physical location of the virtual address and tells the translation mechanism to restart the request.\nWhen all physical memory is already in use, the paging supervisor must free a page in primary storage to hold the swapped-in\npage. The supervisor uses one of a variety of page replacement algorithms such as least recently used to determine which page to\nfree.\n8.2.1 https:\/\/eng.libretexts.org\/@go\/page\/48839 Pinned pages\nOperating systems have memory areas that are pinned (never swapped to secondary storage). Other terms used are locked, fixed, or\nwired pages. For example, interrupt mechanisms rely on an array of pointers to their handlers, such as I\/O completion and page\nfault. If the pages containing these pointers or the code that they invoke were pageable, interrupt-handling would become far more\ncomplex and time-consuming, particularly in the case of page fault interruptions. Hence, some part of the page table structures is\nnot pageable.\nSome pages may be pinned for short periods of time, others may be pinned for long periods of time, and still others may need to be\npermanently pinned. For example:\nThe paging supervisor code and drivers for secondary storage devices on which pages reside must be permanently pinned, as\notherwise paging wouldn't even work because the necessary code wouldn't be available.\nTiming-dependent components may be pinned to avoid variable paging delays.\nData buffers that are accessed directly by peripheral devices that use direct memory access or I\/O channels must reside in\npinned pages while the I\/O operation is in progress because such devices and the buses to which they are attached expect to find\ndata buffers located at physical memory addresses; regardless of whether the bus has a memory management unit for I\/O,\ntransfers cannot be stopped if a page fault occurs and then restarted when the page fault has been processed.\nThrashing\nWhen paging and page stealing are used, a problem called \"thrashing\" can occur, in which the computer spends an unsuitably large\namount of time transferring pages to and from a backing store, hence slowing down useful work. A task's working set is the\nminimum set of pages that should be in memory in order for it to make useful progress. Thrashing occurs when there is insufficient\nmemory available to store the working sets of all active programs. Adding real memory is the simplest response, but improving\napplication design, scheduling, and memory usage can help. Another solution is to reduce the number of active tasks on the system.\nThis reduces demand on real memory by swapping out the entire working set of one or more processes.\nSegmented virtual memory\nSome systems use segmentation instead of paging, dividing virtual address spaces into variable-length segments. A virtual address\nhere consists of a segment number and an offset within the segment. Segmentation and paging can be used together by dividing\neach segment into pages; systems with this memory structure are usually paging-predominant, segmentation providing memory\nprotection.\nIn some processors, the segments reside in a 32-bit linear, paged address space. Segments can be moved in and out of that space;\npages there can \"page\" in and out of main memory, providing two levels of virtual memory; few if any operating systems do so,\ninstead using only paging. Early non-hardware-assisted virtualization solutions combined paging and segmentation because paging\noffers only two protection domains whereas a VMM \/ guest OS \/ guest applications stack needs three. The difference between\npaging and segmentation systems is not only about memory division; segmentation is visible to user processes, as part of memory\nmodel semantics. Hence, instead of memory that looks like a single large space, it is structured into multiple spaces.\nThis difference has important consequences; a segment is not a page with variable length or a simple way to lengthen the address\nspace. Segmentation that can provide a single-level memory model in which there is no differentiation between process memory\nand file system consists of only a list of segments (files) mapped into the process's potential address space.\nAdapted from:\n\"Virtual memory\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n8.2: Virtual Memory in the Operating System is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n8.2.2 https:\/\/eng.libretexts.org\/@go\/page\/48839 CHAPTER OVERVIEW\n9: Uniprocessor CPU Scheduling\n9.1: Types of Processor Scheduling\n9.2: Scheduling Algorithms\nThis page titled 9: Uniprocessor CPU Scheduling is shared under a CC BY-SA license and was authored, remixed, and\/or curated by Patrick\nMcClanahan.\n1 9.1: Types of Processor Scheduling\nIn computing, scheduling is the method by which work is assigned to resources that complete the work. The work may be virtual\ncomputation elements such as threads, processes or data flows, which are in turn scheduled onto hardware resources such as\nprocessors, network links or expansion cards.\nA scheduler is what carries out the scheduling activity. Schedulers are often implemented so they keep all computer resources busy\n(as in load balancing), allow multiple users to share system resources effectively, or to achieve a target quality of service.\nScheduling is fundamental to computation itself, and an intrinsic part of the execution model of a computer system; the concept of\nscheduling makes it possible to have computer multitasking with a single central processing unit (CPU).\nGoals of a Scheduler\nA scheduler may aim at one or more goals, for example: maximizing throughput (the total amount of work completed per time\nunit); minimizing wait time (time from work becoming ready until the first point it begins execution); minimizing latency or\nresponse time (time from work becoming ready until it is finished in case of batch activity, or until the system responds and hands\nthe first output to the user in case of interactive activity); or maximizing fairness (equal CPU time to each process, or more\ngenerally appropriate times according to the priority and workload of each process). In practice, these goals often conflict (e.g.\nthroughput versus latency), thus a scheduler will implement a suitable compromise. Preference is measured by any one of the\nconcerns mentioned above, depending upon the user's needs and objectives.\nIn real-time environments, such as embedded systems for automatic control in industry (for example robotics), the scheduler also\nmust ensure that processes can meet deadlines; this is crucial for keeping the system stable. Scheduled tasks can also be distributed\nto remote devices across a network and managed through an administrative back end.\nTypes of operating system schedulers\nThe scheduler is an operating system module that selects the next jobs to be admitted into the system and the next process to run.\nOperating systems may feature up to three distinct scheduler types: a long-term scheduler (also known as an admission scheduler or\nhigh-level scheduler), a mid-term or medium-term scheduler, and a short-term scheduler. The names suggest the relative frequency\nwith which their functions are performed.\nProcess scheduler\nThe process scheduler is a part of the operating system that decides which process runs at a certain point in time. It usually has the\nability to pause a running process, move it to the back of the running queue and start a new process; such a scheduler is known as a\npreemptive scheduler, otherwise it is a cooperative scheduler.\nWe distinguish between \"long-term scheduling\", \"medium-term scheduling\", and \"short-term scheduling\" based on how often\ndecisions must be made.\nLong-term scheduling\nThe long-term scheduler, or admission scheduler, decides which jobs or processes are to be admitted to the ready queue (in main\nmemory); that is, when an attempt is made to execute a program, its admission to the set of currently executing processes is either\nauthorized or delayed by the long-term scheduler. Thus, this scheduler dictates what processes are to run on a system, and the\ndegree of concurrency to be supported at any one time \u2013 whether many or few processes are to be executed concurrently, and how\nthe split between I\/O-intensive and CPU-intensive processes is to be handled. The long-term scheduler is responsible for\ncontrolling the degree of multiprogramming.\nIn general, most processes can be described as either I\/O-bound or CPU-bound. An I\/O-bound process is one that spends more of\nits time doing I\/O than it spends doing computations. A CPU-bound process, in contrast, generates I\/O requests infrequently, using\nmore of its time doing computations. It is important that a long-term scheduler selects a good process mix of I\/O-bound and CPU-\nbound processes. If all processes are I\/O-bound, the ready queue will almost always be empty, and the short-term scheduler will\nhave little to do. On the other hand, if all processes are CPU-bound, the I\/O waiting queue will almost always be empty, devices\nwill go unused, and again the system will be unbalanced. The system with the best performance will thus have a combination of\nCPU-bound and I\/O-bound processes. In modern operating systems, this is used to make sure that real-time processes get enough\nCPU time to finish their tasks.\n9.1.1 https:\/\/eng.libretexts.org\/@go\/page\/48918 Long-term scheduling is also important in large-scale systems such as batch processing systems, computer clusters,\nsupercomputers, and render farms. For example, in concurrent systems, co-scheduling of interacting processes is often required to\nprevent them from blocking due to waiting on each other. In these cases, special-purpose job scheduler software is typically used to\nassist these functions, in addition to any underlying admission scheduling support in the operating system.\nSome operating systems only allow new tasks to be added if it is sure all real-time deadlines can still be met. The specific heuristic\nalgorithm used by an operating system to accept or reject new tasks is the admission control mechanism.\nMedium-term scheduling\nThe medium-term scheduler temporarily removes processes from main memory and places them in secondary memory (such as a\nhard disk drive) or vice versa, which is commonly referred to as \"swapping out\" or \"swapping in\" (also incorrectly as \"paging out\"\nor \"paging in\"). The medium-term scheduler may decide to swap out a process which has not been active for some time, or a\nprocess which has a low priority, or a process which is page faulting frequently, or a process which is taking up a large amount of\nmemory in order to free up main memory for other processes, swapping the process back in later when more memory is available,\nor when the process has been unblocked and is no longer waiting for a resource.\nIn many systems today (those that support mapping virtual address space to secondary storage other than the swap file), the\nmedium-term scheduler may actually perform the role of the long-term scheduler, by treating binaries as \"swapped out processes\"\nupon their execution. In this way, when a segment of the binary is required it can be swapped in on demand, or \"lazy loaded\", also\ncalled demand paging.\nShort-term scheduling\nThe short-term scheduler (also known as the CPU scheduler) decides which of the ready, in-memory processes is to be executed\n(allocated a CPU) after a clock interrupt, an I\/O interrupt, an operating system call or another form of signal. Thus the short-term\nscheduler makes scheduling decisions much more frequently than the long-term or mid-term schedulers \u2013 a scheduling decision\nwill at a minimum have to be made after every time slice, and these are very short. This scheduler can be preemptive, implying that\nit is capable of forcibly removing processes from a CPU when it decides to allocate that CPU to another process, or non-preemptive\n(also known as \"voluntary\" or \"co-operative\"), in which case the scheduler is unable to \"force\" processes off the CPU.\nA preemptive scheduler relies upon a programmable interval timer which invokes an interrupt handler that runs in kernel mode and\nimplements the scheduling function.\nDispatcher\nAnother component that is involved in the CPU-scheduling function is the dispatcher, which is the module that gives control of the\nCPU to the process selected by the short-term scheduler. It receives control in kernel mode as the result of an interrupt or system\ncall. The functions of a dispatcher mop the following:\nContext switches, in which the dispatcher saves the state (also known as context) of the process or thread that was previously\nrunning; the dispatcher then loads the initial or previously saved state of the new process.\nSwitching to user mode.\nJumping to the proper location in the user program to restart that program indicated by its new state.\nThe dispatcher should be as fast as possible, since it is invoked during every process switch. During the context switches, the\nprocessor is virtually idle for a fraction of time, thus unnecessary context switches should be avoided. The time it takes for the\ndispatcher to stop one process and start another is known as the dispatch latency.\nAdapted from:\n\"Scheduling (computing)\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n9.1: Types of Processor Scheduling is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n9.1.2 https:\/\/eng.libretexts.org\/@go\/page\/48918 9.2: Scheduling Algorithms\nScheduling Algorithms\nScheduling algorithms are used for distributing resources among parties which simultaneously and asynchronously request them.\nScheduling disciplines are used in routers (to handle packet traffic) as well as in operating systems (to share CPU time among both\nthreads and processes), disk drives (I\/O scheduling), printers (print spooler), most embedded systems, etc.\nThe main purposes of scheduling algorithms are to minimize resource starvation and to ensure fairness amongst the parties utilizing\nthe resources. Scheduling deals with the problem of deciding which of the outstanding requests is to be allocated resources. There\nare many different scheduling algorithms. In this section, we introduce several of them.\nIn packet-switched computer networks and other statistical multiplexing, the notion of a scheduling algorithm is used as an\nalternative to first-come first-served queuing of data packets.\nThe simplest best-effort scheduling algorithms are round-robin, fair queuing (a max-min fair scheduling algorithm), proportionally\nfair scheduling and maximum throughput. If differentiated or guaranteed quality of service is offered, as opposed to best-effort\ncommunication, weighted fair queuing may be utilized.\nIn advanced packet radio wireless networks such as HSDPA (High-Speed Downlink Packet Access) 3.5G cellular system, channel-\ndependent scheduling may be used to take advantage of channel state information. If the channel conditions are favourable, the\nthroughput and system spectral efficiency may be increased. In even more advanced systems such as LTE, the scheduling is\ncombined by channel-dependent packet-by-packet dynamic channel allocation, or by assigning OFDMA multi-carriers or other\nfrequency-domain equalization components to the users that best can utilize them.\nFirst come, first served\nFirst in, first out (FIFO), also known as first come, first served (FCFS), is the simplest scheduling algorithm. FIFO simply queues\nprocesses in the order that they arrive in the ready queue. This is commonly used for a task queue, for example as illustrated in this\nsection.\nSince context switches only occur upon process termination, and no reorganization of the process queue is required, scheduling\noverhead is minimal.\nThroughput can be low, because long processes can be holding the CPU, causing the short processes to wait for a long time\n(known as the convoy effect).\nNo starvation, because each process gets chance to be executed after a definite time.\nTurnaround time, waiting time and response time depend on the order of their arrival and can be high for the same reasons\nabove.\nNo prioritization occurs, thus this system has trouble meeting process deadlines.\nThe lack of prioritization means that as long as every process eventually completes, there is no starvation. In an environment\nwhere some processes might not complete, there can be starvation.\nIt is based on queuing.\nShortest remaining time first\nSimilar to shortest job first (SJF). With this strategy the scheduler arranges processes with the least estimated processing time\nremaining to be next in the queue. This requires advanced knowledge or estimations about the time required for a process to\ncomplete.\nIf a shorter process arrives during another process' execution, the currently running process is interrupted (known as\npreemption), dividing that process into two separate computing blocks. This creates excess overhead through additional context\nswitching. The scheduler must also place each incoming process into a specific place in the queue, creating additional overhead.\nThis algorithm is designed for maximum throughput in most scenarios.\nWaiting time and response time increase as the process's computational requirements increase. Since turnaround time is based\non waiting time plus processing time, longer processes are significantly affected by this. Overall waiting time is smaller than\nFIFO, however since no process has to wait for the termination of the longest process.\nNo particular attention is given to deadlines, the programmer can only attempt to make processes with deadlines as short as\npossible.\n9.2.1 https:\/\/eng.libretexts.org\/@go\/page\/48920 Starvation is possible, especially in a busy system with many small processes being run.\nTo use this policy we should have at least two processes of different priority\nFixed priority pre-emptive scheduling\nThe operating system assigns a fixed priority rank to every process, and the scheduler arranges the processes in the ready queue in\norder of their priority. Lower-priority processes get interrupted by incoming higher-priority processes.\nOverhead is not minimal, nor is it significant.\nFPPS has no particular advantage in terms of throughput over FIFO scheduling.\nIf the number of rankings is limited, it can be characterized as a collection of FIFO queues, one for each priority ranking.\nProcesses in lower-priority queues are selected only when all of the higher-priority queues are empty.\nWaiting time and response time depend on the priority of the process. Higher-priority processes have smaller waiting and\nresponse times.\nDeadlines can be met by giving processes with deadlines a higher priority.\nStarvation of lower-priority processes is possible with large numbers of high-priority processes queuing for CPU time.\nRound-robin scheduling\nThe scheduler assigns a fixed time unit per process, and cycles through them. If process completes within that time-slice it gets\nterminated otherwise it is rescheduled after giving a chance to all other processes.\nRR scheduling involves extensive overhead, especially with a small time unit.\nBalanced throughput between FCFS\/ FIFO and SJF\/SRTF, shorter jobs are completed faster than in FIFO and longer processes\nare completed faster than in SJF.\nGood average response time, waiting time is dependent on number of processes, and not average process length.\nBecause of high waiting times, deadlines are rarely met in a pure RR system.\nStarvation can never occur, since no priority is given. Order of time unit allocation is based upon process arrival time, similar to\nFIFO.\nIf Time-Slice is large it becomes FCFS \/FIFO or if it is short then it becomes SJF\/SRTF.\nMultilevel queue scheduling\nThis is used for situations in which processes are easily divided into different groups. For example, a common division is made\nbetween foreground (interactive) processes and background (batch) processes. These two types of processes have different\nresponse-time requirements and so may have different scheduling needs. It is very useful for shared memory problems.\nWork-conserving schedulers\nA work-conserving scheduler is a scheduler that always tries to keep the scheduled resources busy, if there are submitted jobs ready\nto be scheduled. In contrast, a non-work conserving scheduler is a scheduler that, in some cases, may leave the scheduled resources\nidle despite the presence of jobs ready to be scheduled.\nChoosing a scheduling algorithm\nWhen designing an operating system, a programmer must consider which scheduling algorithm will perform best for the use the\nsystem is going to see. There is no universal \"best\" scheduling algorithm, and many operating systems use extended or\ncombinations of the scheduling algorithms above.\nFor example, Windows NT\/XP\/Vista uses a multilevel feedback queue, a combination of fixed-priority preemptive scheduling,\nround-robin, and first in, first out algorithms. In this system, threads can dynamically increase or decrease in priority depending on\nif it has been serviced already, or if it has been waiting extensively. Every priority level is represented by its own queue,\nwith round-robin scheduling among the high-priority threads and FIFO among the lower-priority ones. In this sense, response time\nis short for most threads, and short but critical system threads get completed very quickly. Since threads can only use one time unit\nof the round-robin in the highest-priority queue, starvation can be a problem for longer high-priority threads.\nAdapted from:\n\"Scheduling (computing)\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n9.2: Scheduling Algorithms is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n9.2.2 https:\/\/eng.libretexts.org\/@go\/page\/48920 CHAPTER OVERVIEW\n10: Multiprocessor Scheduling\n10.1: The Question\n10.2: Multiprocessor Scheduling\nThis page titled 10: Multiprocessor Scheduling is shared under a CC BY-SA license and was authored, remixed, and\/or curated by Patrick\nMcClanahan.\n1 10.1: The Question\nWhat is the purpose of multiprocessing\nIn computer science, multiprocessor scheduling is an optimization problem involving the scheduling of computational tasks in\na multiprocessor environment. The problem statement is: \"Given a set J of jobs where job j\ni\nhas length l\ni\nand a number of\nprocessors m, what is the minimum possible time required to schedule all jobs in J on m processors such that none\noverlap?\". The problem is often called the minimum makespan problem: the makespan of a schedule is defined as the time it takes\nthe system to complete all processes, and the goal is to find a schedule that minimizes the makespan. The problem has many\nvariants.\nApproaches to Multiple-Processor Scheduling\nAsymmetric multiprocessing\nAn asymmetric multiprocessing (AMP or ASMP) system is a multiprocessor computer system where not all of the multiple\ninterconnected central processing units (CPUs) are treated equally. For example, a system might allow (either at the hardware or\noperating system level) only one CPU to execute operating system code or might allow only one CPU to perform I\/O operations.\nOther AMP systems might allow any CPU to execute operating system code and perform I\/O operations, so that they were\nsymmetric with regard to processor roles, but attached some or all peripherals to particular CPUs, so that they were asymmetric\nwith respect to the peripheral attachment.\nFigure 10.1.1: Asymmetric multiprocessing. (\"Asmp 2.gif\" by G7a, Wikimedia Commons is licensed under CC BY-SA 3.0)\nAsymmetric multiprocessing was the only method for handling multiple CPUs before symmetric multiprocessing (SMP) was\navailable. It has also been used to provide less expensive options on systems where SMP was available.\nSymmetric multiprocessing\nSymmetric multiprocessing (SMP) involves a multiprocessor computer hardware and software architecture where two or more\nidentical processors are connected to a single, shared main memory, have full access to all input and output devices, and are\ncontrolled by a single operating system instance that treats all processors equally, reserving none for special purposes. Most\nmultiprocessor systems today use an SMP architecture. In the case of multi-core processors, the SMP architecture applies to the\ncores, treating them as separate processors.\nProfessor John D. Kubiatowicz considers traditionally SMP systems to contain processors without caches. Culler and Pal-Singh in\ntheir 1998 book \"Parallel Computer Architecture: A Hardware\/Software Approach\" mention: \"The term SMP is widely used but\ncauses a bit of confusion. The more precise description of what is intended by SMP is a shared memory multiprocessor where the\ncost of accessing a memory location is the same for all processors; that is, it has uniform access costs when the access actually is to\nmemory. If the location is cached, the access will be faster, but cache access times and memory access times are the same on all\nprocessors.\"\n10.1.1 https:\/\/eng.libretexts.org\/@go\/page\/48923 Figure 10.1.1: SMP - Symmetric Multiprocessor System. (\"SMP - Symmetric Multiprocessor\nSystem\" by Ferry24.Milan, Wikimedia Commons is licensed under CC BY-SA 3.0)\nSMP systems are tightly coupled multiprocessor systems with a pool of homogeneous processors running independently of each\nother. Each processor, executing different programs and working on different sets of data, has the capability of sharing common\nresources (memory, I\/O device, interrupt system and so on) that are connected using a system bus or a crossbar.\nAdapted from:\n\"Asymmetric multiprocessing\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n\"Symmetric multiprocessing\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n10.1: The Question is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n10.1.2 https:\/\/eng.libretexts.org\/@go\/page\/48923 10.2: Multiprocessor Scheduling\nMP Scheduling\nSo, we come back to the question: \"Given a set J of jobs where job ji has length li and a number of processors m, what is the\nminimum possible time required to schedule all jobs in J on m processors such that none overlap?\".\nThis is a complex question when we have multiple processors, some of which may run at different speeds. Scheduling is not as\nstraight forward as it was with the single processor, the algorithms are more complex due to the nature of multiprocessors being\npresent.\nThere are several different concepts that have been studied and implemented for multiprocessor thread scheduling and processor\nassignment. A few of these concepts are discussed below approaches seem to be well accepted:\nGang scheduling\nIn computer science, gang scheduling is a scheduling algorithm for parallel systems that schedules related threads or processes\nto run simultaneously on different processors. Usually these will be threads all belonging to the same process, but they may also\nbe from different processes, where the processes could have a producer-consumer relationship or come from the same MPI\nprogram.\nGang scheduling is used to ensure that if two or more threads or processes communicate with each other, they will all be ready\nto communicate at the same time. If they were not gang-scheduled, then one could wait to send or receive a message to another\nwhile it is sleeping, and vice versa. When processors are over-subscribed and gang scheduling is not used within a group of\nprocesses or threads which communicate with each other, each communication event could suffer the overhead of a context\nswitch.\nGang scheduling is based on a data structure called the Ousterhout matrix. In this matrix each row represents a time slice, and\neach column a processor. The threads or processes of each job are packed into a single row of the matrix. During execution,\ncoordinated context switching is performed across all nodes to switch from the processes in one row to those in the next row.\nGang scheduling is stricter than co-scheduling. It requires all threads of the same process to run concurrently, while co-\nscheduling allows for fragments, which are sets of threads that do not run concurrently with the rest of the gang.\nProcessor affinity\nProcessor affinity, or CPU pinning or \"cache affinity\", enables the binding and unbinding of a process or a thread to a central\nprocessing unit (CPU) or a range of CPUs, so that the process or thread will execute only on the designated CPU or CPUs\nrather than any CPU. This can be viewed as a modification of the native central queue scheduling algorithm in a symmetric\nmultiprocessing operating system. Each item in the queue has a tag indicating its kin processor. At the time of resource\nallocation, each task is allocated to its kin processor in preference to others.\nProcessor affinity takes advantage of the fact that remnants of a process that was run on a given processor may remain in that\nprocessor's state (for example, data in the cache memory) after another process was run on that processor. Scheduling that\nprocess to execute on the same processor improves its performance by reducing performance-degrading events such as cache\nmisses. A practical example of processor affinity is executing multiple instances of a non-threaded application, such as some\ngraphics-rendering software.\nScheduling-algorithm implementations vary in adherence to processor affinity. Under certain circumstances, some\nimplementations will allow a task to change to another processor if it results in higher efficiency. For example, when two\nprocessor-intensive tasks (A and B) have affinity to one processor while another processor remains unused, many schedulers\nwill shift task B to the second processor in order to maximize processor use. Task B will then acquire affinity with the second\nprocessor, while task A will continue to have affinity with the original processor.\nUsage\nProcessor affinity can effectively reduce cache problems, but it does not reduce the persistent load-balancing problem.[1] Also\nnote that processor affinity becomes more complicated in systems with non-uniform architectures. For example, a system with\ntwo dual-core hyper-threaded CPUs presents a challenge to a scheduling algorithm.\n10.2.1 https:\/\/eng.libretexts.org\/@go\/page\/48925 There is complete affinity between two virtual CPUs implemented on the same core via hyper-threading, partial affinity\nbetween two cores on the same physical processor (as the cores share some, but not all, cache), and no affinity between separate\nphysical processors. As other resources are also shared, processor affinity alone cannot be used as the basis for CPU\ndispatching. If a process has recently run on one virtual hyper-threaded CPU in a given core, and that virtual CPU is currently\nbusy but its partner CPU is not, cache affinity would suggest that the process should be dispatched to the idle partner CPU.\nHowever, the two virtual CPUs compete for essentially all computing, cache, and memory resources. In this situation, it would\ntypically be more efficient to dispatch the process to a different core or CPU, if one is available. This could incur a penalty\nwhen process repopulates the cache, but overall performance could be higher as the process would not have to compete for\nresources within the CPU.\nLoad balancing\nIn computing, load balancing refers to the process of distributing a set of tasks over a set of resources (computing units), with\nthe aim of making their overall processing more efficient. Load balancing techniques can optimize the response time for each\ntask, avoiding unevenly overloading compute nodes while other compute nodes are left idle.\nLoad balancing is the subject of research in the field of parallel computers. Two main approaches exist: static algorithms, which\ndo not take into account the state of the different machines, and dynamic algorithms, which are usually more general and more\nefficient, but require exchanges of information between the different computing units, at the risk of a loss of efficiency.\nLoad Balancing Considerations\nA load balancing algorithm always tries to answer a specific problem. Among other things, the nature of the tasks, the\nalgorithmic complexity, the hardware architecture on which the algorithms will run as well as required error tolerance, must be\ntaken into account. Therefore compromise must be found to best meet application-specific requirements.\nNature of tasks\nThe efficiency of load balancing algorithms critically depends on the nature of the tasks. Therefore, the more information about\nthe tasks is available at the time of decision making, the greater the potential for optimization.\nSize of tasks\nA perfect knowledge of the execution time of each of the tasks allows to reach an optimal load distribution (see algorithm of\nprefix sum). Unfortunately, this is in fact an idealized case. Knowing the exact execution time of each task is an extremely rare\nsituation.\nFor this reason, there are several techniques to get an idea of the different execution times. First of all, in the fortunate scenario\nof having tasks of relatively homogeneous size, it is possible to consider that each of them will require approximately the\naverage execution time. If, on the other hand, the execution time is very irregular, more sophisticated techniques must be used.\nOne technique is to add some metadata to each task. Depending on the previous execution time for similar metadata, it is\npossible to make inferences for a future task based on statistics.\nDependencies\nIn some cases, tasks depend on each other. These interdependencies can be illustrated by a directed acyclic graph. Intuitively,\nsome tasks cannot begin until others are completed.\nAssuming that the required time for each of the tasks is known in advance, an optimal execution order must lead to the\nminimization of the total execution time. Although this is an NP-hard problem and therefore can be difficult to be solved\nexactly. There are algorithms, like job scheduler, that calculate optimal task distributions using metaheuristic methods.\nSegregation of tasks\nAnother feature of the tasks critical for the design of a load balancing algorithm is their ability to be broken down into subtasks\nduring execution. The \"Tree-Shaped Computation\" algorithm presented later takes great advantage of this specificity.\nLoad Balancing Approaches\nStatic distribution with full knowledge of the tasks\nIf the tasks are independent of each other, and if their respective execution time and the tasks can be subdivided, there is a\nsimple and optimal algorithm.\nBy dividing the tasks in such a way as to give the same amount of computation to each processor, all that remains to be done is\nto group the results together. Using a prefix sum algorithm, this division can be calculated in logarithmic time with respect to\n10.2.2 https:\/\/eng.libretexts.org\/@go\/page\/48925 the number of processors.\nStatic load distribution without prior knowledge\nEven if the execution time is not known in advance at all, static load distribution is always possible.\nRound-Robin\nIn this simple algorithm, the first request is sent to the first server, then the next to the second, and so on down to the last. Then\nit is started again, assigning the next request to the first server, and so on.\nThis algorithm can be weighted such that the most powerful units receive the largest number of requests and receive them first.\nRandomized static\nRandomized static load balancing is simply a matter of randomly assigning tasks to the different servers. This method works\nquite well. If, on the other hand, the number of tasks is known in advance, it is even more efficient to calculate a random\npermutation in advance. This avoids communication costs for each assignment. There is no longer a need for a distribution\nmaster because every processor knows what task is assigned to it. Even if the number of tasks is unknown, it is still possible to\navoid communication with a pseudo-random assignment generation known to all processors.\nThe performance of this strategy (measured in total execution time for a given fixed set of tasks) decreases with the maximum\nsize of the tasks\nMaster-Worker Scheme\nMaster-Worker schemes are among the simplest dynamic load balancing algorithms. A master distributes the workload to all\nworkers (also sometimes referred to as \"slaves\"). Initially, all workers are idle and report this to the master. The master answers\nworker requests and distributes the tasks to them. When he has no more tasks to give, he informs the workers so that they stop\nasking for tasks.\nThe advantage of this system is that it distributes the burden very fairly. In fact, if one does not take into account the time\nneeded for the assignment, the execution time would be comparable to the prefix sum seen above.\nThe problem of this algorithm is that it has difficulty to adapt to a large number of processors because of the high amount of\nnecessary communications. This lack of scalability makes it quickly inoperable in very large servers or very large parallel\ncomputers. The master acts as a bottleneck.\nAdapted from:\n\"Multiprocessor scheduling\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n\"Processor affinity\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n\"Load balancing (computing)\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n\"Gang scheduling\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n10.2: Multiprocessor Scheduling is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n10.2.3 https:\/\/eng.libretexts.org\/@go\/page\/48925 CHAPTER OVERVIEW\n12: File Management\n12.1: Overview\n12.2: Files\n12.2.1: Files (continued)\n12.3: Directory\n12.4: File Sharing\nThis page titled 12: File Management is shared under a CC BY-SA license and was authored, remixed, and\/or curated by Patrick McClanahan.\n1 12.1: Overview\nFile system\nA file system controls how data is stored and retrieved. Without a file system, data placed in a storage medium would be one large\nbody of data with no way to tell where one piece of data stops and the next begins. By separating the data into pieces and giving\neach piece a name, the data is easily isolated and identified. Taking its name from the way paper-based data management system is\nnamed, each group of data is called a \"file.\" The structure and logic rules used to manage the groups of data and their names is\ncalled a \"file system.\"\nThere are many different kinds of file systems. Each one has different structure and logic, properties of speed, flexibility, security,\nsize and more. Some file systems have been designed to be used for specific applications. For example, the ISO 9660 file system is\ndesigned specifically for optical disks.\nFile systems can be used on numerous different types of storage devices that use different kinds of media. As of 2019, hard disk\ndrives have been key storage devices and are projected to remain so for the foreseeable future. Other kinds of media that are used\ninclude SSDs, magnetic tapes, and optical disks. In some cases, such as with tmpfs, the computer's main memory (random-access\nmemory, RAM) is used to create a temporary file system for short-term use.\nSome file systems are used on local data storage devices; others provide file access via a network protocol (for example, NFS,\nSMB, or 9P clients). Some file systems are \"virtual\", meaning that the supplied \"files\" (called virtual files) are computed on request\n(such as procfs and sysfs) or are merely a mapping into a different file system used as a backing store. The file system manages\naccess to both the content of files and the metadata about those files. It is responsible for arranging storage space; reliability,\nefficiency, and tuning with regard to the physical storage medium are important design considerations.\nFigure 12.1.1: Linux file system layout. (\"Linux file system layout\" by Machtelt Garrels, The Linux Documentation Project is\nlicensed under LDP Manifesto)\n12.1.1 https:\/\/eng.libretexts.org\/@go\/page\/50047 Architecture\nA file system consists of two or three layers. Sometimes the layers are explicitly separated, and sometimes the functions are\ncombined.\nThe logical file system is responsible for interaction with the user application. It provides the application program interface (API)\nfor file operations \u2014 OPEN, CLOSE, READ, etc., and passes the requested operation to the layer below it for processing. The\nlogical file system \"manage open file table entries and per-process file descriptors\". This layer provides \"file access, directory\noperations, security and protection\".\nThe second optional layer is the virtual file system. \"This interface allows support for multiple concurrent instances of physical file\nsystems, each of which is called a file system implementation\".\nThe third layer is the physical file system. This layer is concerned with the physical operation of the storage device (e.g. disk). It\nprocesses physical blocks being read or written. It handles buffering and memory management and is responsible for the physical\nplacement of blocks in specific locations on the storage medium. The physical file system interacts with the device drivers or with\nthe channel to drive the storage device.\nAspects of file systems\nSpace management\nFile systems allocate space in a granular manner, usually multiple physical units on the device. The file system is responsible for\norganizing files and directories, and keeping track of which areas of the media belong to which file and which are not being\nused. This results in unused space when a file is not an exact multiple of the allocation unit, sometimes referred to as slack space.\nFor a 512-byte allocation, the average unused space is 256 bytes. For 64 KB clusters, the average unused space is 32 KB. The size\nof the allocation unit is chosen when the file system is created. Choosing the allocation size based on the average size of the files\nexpected to be in the file system can minimize the amount of unusable space. Frequently the default allocation may provide\nreasonable usage. Choosing an allocation size that is too small results in excessive overhead if the file system will contain mostly\nvery large files.\nFile system fragmentation occurs when unused space or single files are not contiguous. As a file system is used, files are created,\nmodified and deleted. When a file is created, the file system allocates space for the data. Some file systems permit or require\nspecifying an initial space allocation and subsequent incremental allocations as the file grows. As files are deleted, the space they\nwere allocated eventually is considered available for use by other files. This creates alternating used and unused areas of various\nsizes. This is free space fragmentation. When a file is created and there is not an area of contiguous space available for its initial\nallocation, the space must be assigned in fragments. When a file is modified such that it becomes larger, it may exceed the space\ninitially allocated to it, another allocation must be assigned elsewhere and the file becomes fragmented.\nFilenames\nA filename (or file name) is used to identify a storage location in the file system. Most file systems have restrictions on the length\nof filenames. In some file systems, filenames are not case sensitive (i.e., the names MYFILE and myfile refer to the same\nfile in a directory); in others, filenames are case sensitive (i.e., the names MYFILE , MyFile , and myfile refer to three\nseparate files that are in the same directory).\nMost modern file systems allow filenames to contain a wide range of characters from the Unicode character set. However, they\nmay have restrictions on the use of certain special characters, disallowing them within filenames; those characters might be used to\nindicate a device, device type, directory prefix, file path separator, or file type.\nDirectories\nFile systems typically have directories (also called folders) which allow the user to group files into separate collections. This may\nbe implemented by associating the file name with an index in a table of contents or an inode in a Unix-like file system. Directory\nstructures may be flat (i.e. linear), or allow hierarchies where directories may contain subdirectories.\nMetadata\nOther bookkeeping information is typically associated with each file within a file system. The length of the data contained in a file\nmay be stored as the number of blocks allocated for the file or as a byte count. The time that the file was last modified may be\nstored as the file's timestamp. File systems might store the file creation time, the time it was last accessed, the time the file's\n12.1.2 https:\/\/eng.libretexts.org\/@go\/page\/50047 metadata was changed, or the time the file was last backed up. Other information can include the file's device type (e.g. block,\ncharacter, socket, subdirectory, etc.), its owner user ID and group ID, its access permissions and other file attributes (e.g. whether\nthe file is read-only, executable, etc.).\nA file system stores all the metadata associated with the file\u2014including the file name, the length of the contents of a file, and the\nlocation of the file in the folder hierarchy\u2014separate from the contents of the file.\nMost file systems store the names of all the files in one directory in one place\u2014the directory table for that directory\u2014which is\noften stored like any other file. Many file systems put only some of the metadata for a file in the directory table, and the rest of the\nmetadata for that file in a completely separate structure, such as the inode.\nAdapted from:\n\"File system\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n12.1: Overview is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n12.1.3 https:\/\/eng.libretexts.org\/@go\/page\/50047 12.2: Files\nA computer file is a computer resource for recording data in a computer storage device. Just as words can be written to paper, so\ncan data be written to a computer file. Files can be edited and transferred through the Internet on that particular computer system.\nDifferent types of computer files are designed for different purposes. A file may be designed to store a picture, a written message, a\nvideo, a computer program, or a wide variety of other kinds of data. Certain files can store multiple data types at once.\nBy using computer programs, a person can open, read, change, save, and close a computer file. Computer files may be reopened,\nmodified, and copied an arbitrary number of times.\nFiles are typically organized in a file system, which tracks file locations on the disk and enables user access.\nFile contents\nOn most modern operating systems, files are organized into one-dimensional arrays of bytes. The format of a file is defined by its\ncontent since a file is solely a container for data, although on some platforms the format is usually indicated by its filename\nextension, specifying the rules for how the bytes must be organized and interpreted meaningfully. For example, the bytes of a plain\ntext file (.txt in Windows) are associated with either ASCII or UTF-8 characters, while the bytes of image, video, and audio files\nare interpreted otherwise. Most file types also allocate a few bytes for metadata, which allows a file to carry some basic\ninformation about itself.\nSome file systems can store arbitrary (not interpreted by the file system) file-specific data outside of the file format, but linked to\nthe file, for example extended attributes or forks. On other file systems this can be done via sidecar files or software-specific\ndatabases. All those methods, however, are more susceptible to loss of metadata than are container and archive file formats.\nFile Size\nAt any instant in time, a file might have a size, normally expressed as number of bytes, that indicates how much storage is\nassociated with the file. In most modern operating systems the size can be any non-negative whole number of bytes up to a system\nlimit. Many older operating systems kept track only of the number of blocks or tracks occupied by a file on a physical storage\ndevice. In such systems, software employed other methods to track the exact byte count.\nOperations\nThe most basic operations that programs can perform on a file are:\nCreate a new file\nChange the access permissions and attributes of a file\nOpen a file, which makes the file contents available to the program\nRead data from a file\nWrite data to a file\nDelete a file\nClose a file, terminating the association between it and the program\nTruncate a file, shortening it to a specified size within the file system without rewriting any content\nFiles on a computer can be created, moved, modified, grown, shrunk (truncated), and deleted. In most cases, computer programs\nthat are executed on the computer handle these operations, but the user of a computer can also manipulate files if necessary. For\ninstance, Microsoft Word files are normally created and modified by the Microsoft Word program in response to user commands,\nbut the user can also move, rename, or delete these files directly by using a file manager program such as Windows Explorer (on\nWindows computers) or by command lines (CLI).\nIn Unix-like systems, user space programs do not operate directly, at a low level, on a file. Only the kernel deals with files, and it\nhandles all user-space interaction with files in a manner that is transparent to the user-space programs. The operating system\nprovides a level of abstraction, which means that interaction with a file from user-space is simply through its filename (instead of\nits inode). For example, rm filename will not delete the file itself, but only a link to the file. There can be many links to a file, but\nwhen they are all removed, the kernel considers that file's memory space free to be reallocated. This free space is commonly\nconsidered a security risk (due to the existence of file recovery software). Any secure-deletion program uses kernel-space (system)\nfunctions to wipe the file's data.\n12.2.1 https:\/\/eng.libretexts.org\/@go\/page\/50050 File moves within a file system complete almost immediately because the data content does not need to be rewrittten. Only the\npaths need to be changed.\nFile Organization\nContinuous Allocation\nA single continuous set of blocks is allocated to a file at the time of file creation. Thus, this is a pre-allocation strategy, using\nvariable size portions. The file allocation table needs just a single entry for each file, showing the starting block and the length of\nthe file. This method is best from the point of view of the individual sequential file. Multiple blocks can be read in at a time to\nimprove I\/O performance for sequential processing. It is also easy to retrieve a single block. For example, if a file starts at block b,\nand the ith block of the file is wanted, its location on secondary storage is simply b+i-1.\nDisadvantage\nExternal fragmentation will occur, making it difficult to find contiguous blocks of space of sufficient length. Compaction\nalgorithm will be necessary to free up additional space on disk.\nAlso, with pre-allocation, it is necessary to declare the size of the file at the time of creation.\nLinked Allocation(Non-contiguous allocation)\nAllocation is on an individual block basis. Each block contains a pointer to the next block in the chain. Again the file table needs\njust a single entry for each file, showing the starting block and the length of the file. Although pre-allocation is possible, it is more\ncommon simply to allocate blocks as needed. Any free block can be added to the chain. The blocks need not be continuous.\nIncrease in file size is always possible if free disk block is available. There is no external fragmentation because only one block at a\ntime is needed but there can be internal fragmentation but it exists only in the last disk block of file.\nDisadvantage\nInternal fragmentation exists in last disk block of file.\nThere is an overhead of maintaining the pointer in every disk block.\nIf the pointer of any disk block is lost, the file will be truncated.\nIt supports only the sequential access of files.\nIndexed Allocation\nIt addresses many of the problems of contiguous and chained allocation. In this case, the file allocation table contains a separate\none-level index for each file: The index has one entry for each block allocated to the file. Allocation may be on the basis of fixed-\nsize blocks or variable-sized blocks. Allocation by blocks eliminates external fragmentation, whereas allocation by variable-size\nblocks improves locality. This allocation technique supports both sequential and direct access to the file and thus is the most\npopular form of file allocation.\nJust as the space that is allocated to files must be managed ,so the space that is not currently allocated to any file must be managed.\nTo perform any of the file allocation techniques,it is necessary to know what blocks on the disk are available. Thus we need a disk\nallocation table in addition to a file allocation table.The following are the approaches used for free space management.\nBit Tables\nThis method uses a vector containing one bit for each block on the disk. Each entry for a 0 corresponds to a free block and each 1\ncorresponds to a block in use.\nFor example: 00011010111100110001\nIn this vector every bit correspond to a particular block and 0 implies that, that particular block is free and 1 implies that the block\nis already occupied. A bit table has the advantage that it is relatively easy to find one or a contiguous group of free blocks. Thus, a\nbit table works well with any of the file allocation methods. Another advantage is that it is as small as possible.\nFree Block List\nIn this method, each block is assigned a number sequentially and the list of the numbers of all free blocks is maintained in a\nreserved block of the disk.\nAdaptred from:\n\"Computer file\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n12.2.2 https:\/\/eng.libretexts.org\/@go\/page\/50050 \"File Systems in Operating System\" by Aakansha yadav, Geeks for Geeks is licensed under CC BY-SA 4.0\n12.2: Files is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n12.2.3 https:\/\/eng.libretexts.org\/@go\/page\/50050 12.2.1: Files (continued)\nFile Access Methods\nWhen a file is used, information is read and accessed into computer memory and there are several ways to access this information\nof the file. Some systems provide only one access method for files. Other systems, such as those of IBM, support many access\nmethods, and choosing the right one for a particular application is a major design problem.\nSequential Access\nIt is the simplest access method. Information in the file is processed in order starting at the beginning of the file, one record after\nthe other. This mode of access is by far the most common; for example, editor and compiler usually access the file in this fashion.\nRead and write make up the bulk of the operation on a file. A read operation reads from the current file pointer position. Usually,\nthe software reads some pre-determined amount of bytes. The read operation also moves the file pointer to the new position where\nthe system has stopped reading the file. Similarly, for the write command writes at the point of the current pointer.\nDirect Access\nAnother method is direct access method also known as relative access method. A fixed-length logical record that allows the\nprogram to read and write records rapidly, in no particular order. Direct access is based on the disk model of a file since disk allows\nrandom access to any file block. For direct access, the file is viewed as a numbered sequence of block or record. Thus, we may read\nblock 14 then block 59 and then we can write block 17. There is no restriction on the order of reading and writing for a direct\naccess file.\nA block number provided by the user to the operating system is normally a relative block number, the first relative block of the file\nis 0 and then 1 and so on.\nIndex Sequential Access\nIt is a method of accessing a file which is built on top of the sequential access method. This method constructs an index for the file.\nThe index, like an index in the back of a book, contains the pointer to the various blocks. To find a record in the file, we first search\nthe index and then by the help of pointer we access the file directly.\nAdapted from:\n\"File Access Methods in Operating System\" by AshishVishwakarma1, Geeks for Geeks is licensed under CC BY-SA 4.0\n12.2.1: Files (continued) is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n12.2.1.1 https:\/\/eng.libretexts.org\/@go\/page\/50051 12.3: Directory\nA directory is a file system cataloging structure which contains references to other computer files, and possibly other directories.\nOn many computers, directories are known as folders, or drawers, analogous to a workbench or the traditional office filing cabinet.\nFiles are organized by storing related files in the same directory. In a hierarchical file system (that is, one in which files and\ndirectories are organized in a manner that resembles a tree), a directory contained inside another directory is called a subdirectory.\nThe terms parent and child are often used to describe the relationship between a subdirectory and the directory in which it is\ncataloged, the latter being the parent. The top-most directory in such a filesystem, which does not have a parent of its own, is called\nthe root directory.\nMFD\nDir 1 Dir 2\nFile 1 Dir 3\nFile 3 File 2\nFigure 12.3.1: A typical Files-11 directory hierarchy. (\"A typical Files-11 directory hierarchy.\" by Stannered, Wikimedia\nCommons is in the Public Domain, CC0)\nFILE DIRECTORIES\nCollection of files is a file directory. The directory contains information about the files, including attributes, location and\nownership. Much of this information, especially that is concerned with storage, is managed by the operating system. The directory\nis itself a file, accessible by various file management routines. Different operating systems have different structures for their\ndirectories.\nInformation frequently contained in a directories structure\nName of the directory\nType of file - not supported on all file systems\nCurrent length - of the directory\nMaximum length\nDate last accessed\nDate last updated\nOwner id\nProtection information\nOperation usually allowed on directory\nSearch for a file\nCreate a file or directory\nDelete a file or directory\nList the contents of the directory or sub-directory\nRename a file\nAdvantages of maintaining directories are\nEfficiency: A file can be located more quickly.\nNaming: It becomes convenient for users- for example two users can have same name for different files or may have different\nname for same file.\nGrouping: Logical grouping of files can be done by properties e.g. all java programs, all games etc.\nAdapted from:\n\"File system\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n\"File Systems in Operating System\" by Aakansha yadav, Geeks for Geeks is licensed under CC BY-SA 4.0\n12.3: Directory is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n12.3.1 https:\/\/eng.libretexts.org\/@go\/page\/50048 12.4: File Sharing\nRestricting and Permitting Access\nThere are several mechanisms used by file systems to control access to data. Usually the intent is to prevent reading or modifying\nfiles by a user or group of users. Another reason is to ensure data is modified in a controlled way so access may be restricted to a\nspecific program. Examples include passwords stored in the metadata of the file or elsewhere and file permissions in the form of\npermission bits, access control lists, or capabilities. The need for file system utilities to be able to access the data at the media level\nto reorganize the structures and provide efficient backup usually means that these are only effective for polite users but are not\neffective against intruders.\nMethods for encrypting file data are sometimes included in the file system. This is very effective since there is no need for file\nsystem utilities to know the encryption seed to effectively manage the data. The risks of relying on encryption include the fact that\nan attacker can copy the data and use brute force to decrypt the data. Additionally, losing the seed means losing the data.\nMaintaining Integrity\nOne significant responsibility of a file system is to ensure that the file system structures in secondary storage remain consistent,\nregardless of the actions by programs accessing the file system. This includes actions taken if a program modifying the file system\nterminates abnormally or neglects to inform the file system that it has completed its activities. This may include updating the\nmetadata, the directory entry and handling any data that was buffered but not yet updated on the physical storage media.\nOther failures which the file system must deal with include media failures or loss of connection to remote systems.\nIn the event of an operating system failure or \"soft\" power failure, special routines in the file system must be invoked similar to\nwhen an individual program fails.\nThe file system must also be able to correct damaged structures. These may occur as a result of an operating system failure for\nwhich the OS was unable to notify the file system, a power failure, or a reset.\nThe file system must also record events to allow analysis of systemic issues as well as problems with specific files or directories.\nUser Data\nThe most important purpose of a file system is to manage user data. This includes storing, retrieving and updating data.\nSome file systems accept data for storage as a stream of bytes which are collected and stored in a manner efficient for the media.\nWhen a program retrieves the data, it specifies the size of a memory buffer and the file system transfers data from the media to the\nbuffer. A runtime library routine may sometimes allow the user program to define a record based on a library call specifying a\nlength. When the user program reads the data, the library retrieves data via the file system and returns a record.\nSome file systems allow the specification of a fixed record length which is used for all writes and reads. This facilitates locating the\nnth record as well as updating records.\nAn identification for each record, also known as a key, makes for a more sophisticated file system. The user program can read,\nwrite and update records without regard to their location. This requires complicated management of blocks of media usually\nseparating key blocks and data blocks. Very efficient algorithms can be developed with pyramid structures for locating records.\nAdapted from:\n\"File system\" by Multiple Contributors, Wikipedia is licensed under CC BY-SA 3.0\n12.4: File Sharing is shared under a not declared license and was authored, remixed, and\/or curated by LibreTexts.\n12.4.1 https:\/\/eng.libretexts.org\/@go\/page\/50052 "}