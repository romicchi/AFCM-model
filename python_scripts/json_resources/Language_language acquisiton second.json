{"text":"Language Acquisition Page 1of 39\nLanguage Acquisition\nSteven Pinker\nMassachusetts Institute of Technology\nChapter to appear in L. R. Gleitman, M. Liberman, and D. N. Osherson (Eds.),\nAn Invitation to Cognitive Science, 2nd Ed. Volume 1: Language. Cambridge, MA: MIT Press.\nNONFINAL VERSION: PLEASE DO NOTE QUOTE.\nPreparation of the chapter was supported by NIH grant HD 18381 and NSF grant BNS 91-09766, and\nby the McDonnell-Pew Center for Cognitive Neuroscience at MIT.\n1 Introduction\nLanguage acquisition is one of the central topics in cognitive science. Every theory of cognition has\ntried to explain it; probably no other topic has aroused such controversy. Possessing a language is the\nquintessentially human trait: all normal humans speak, no nonhuman animal does. Language is the main\nvehicle by which we know about other people's thoughts, and the two must be intimately related. Every\ntime we speak we are revealing something about language, so the facts of language structure are easy to\ncome by; these data hint at a system of extraordinary complexity. Nonetheless, learning a first language\nis something every child does successfully, in a matter of a few years and without the need for formal\nlessons. With language so close to the core of what it means to be human, it is not surprising that\nchildren's acquisition of language has received so much attention. Anyone with strong views about the\nhuman mind would like to show that children's first few steps are steps in the right direction.\nLanguage acquisition is not only inherently interesting; studying it is one way to look for concrete\nanswers to questions that permeate cognitive science:\nModularity. Do children learn language using a \"mental organ,\" some of whose principles of\norganization are not shared with other cognitive systems such as perception, motor control, and\nreasoning (Chomsky, 1975, 1991; Fodor, 1983)? Or is language acquisition just another problem to be\nsolved by general intelligence, in this case, the problem of how to communicate with other humans over\nthe auditory channel (Putnam, 1971; Bates, 1989)?\nHuman Uniqueness. A related question is whether language is unique to humans. At first glance the\nanswer seems obvious. Other animals communication with a fixed repertoire of symbols, or with\nanalogue variation like the mercury in a thermometer. But none appears to have the combinatorial rule\nsystem of human language, in which symbols are permuted into an unlimited set of combinations, each\nwith a determinate meaning. On the other hand, many other claims about human uniqueness, such as\nthat humans were the only animals to use tools or to fabricate them, have turned out to be false. Some\nresearchers have thought that apes have the capacity for language but never profited from a humanlike\ncultural milieu in which language was taught, and they have thus tried to teach apes language-like\nsystems. Whether they have succeeded, and whether human children are really \"taught\" language\nthemselves, are questions we will soon come to.\nLanguage and Thought. Is language simply grafted on top of cognition as a way of sticking\ncommunicable labels onto thoughts (Fodor, 1975; Piaget, 1926)? Or does learning a language somehow\nmean learning to think in that language? A famous hypothesis, outlined by Benjamin Whorf (1956),\nasserts that the categories and relations that we use to understand the world come from our particular\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 2of 39\nlanguage, so that speakers of different languages conceptualize the world in different ways. Language\nacquisition, then, would be learning to think, not just learning to talk.\nThis is an intriguing hypothesis, but virtually all modern cognitive scientists believe it is false (see\nPinker, 1994a). Babies can think before they can talk (Chapter X). Cognitive psychology has shown that\npeople think not just in words but in images (see Chapter X) and abstract logical propositions (see the\nchapter by Larson). And linguistics has shown that human languages are too ambiguous and schematic\nto use as a medium of internal computation: when people think about \"spring,\" surely they are not\nconfused as to whether they are thinking about a season or something that goes \"boing\" -- and if one\nword can correspond to two thoughts, thoughts can't be words.\nBut language acquisition has a unique contribution to make to this issue. As we shall see, it is virtually\nimpossible to show how children could learn a language unless you assume they have a considerable\namount of nonlinguistic cognitive machinery in place before they start.\nLearning and Innateness. All humans talk but no house pets or house plants do, no matter how\npampered, so heredity must be involved in language. But a child growing up in Japan speaks Japanese\nwhereas the same child brought up in California would speak English, so the environment is also\ncrucial. Thus there is no question about whether heredity or environment is involved in language, or\neven whether one or the other is \"more important.\" Instead, language acquisition might be our best hope\nof finding out how heredity and environment interact. We know that adult language is intricately\ncomplex, and we know that children become adults. Therefore something in the child's mind must be\ncapable of attaining that complexity. Any theory that posits too little innate structure, so that its\nhypothetical child ends up speaking something less than a real language, must be false. The same is true\nfor any theory that posits too much innate structure, so that the hypothetical child can acquire English\nbut not, say, Bantu or Vietnamese.\nAnd not only do we know about the output of language acquisition, we know a fair amount about the\ninput to it, namely, parent's speech to their children. So even if language acquisition, like all cognitive\nprocesses, is essentially a \"black box,\" we know enough about its input and output to be able to make\nprecise guesses about its contents.\nThe scientific study of language acquisition began around the same time as the birth of cognitive\nscience, in the late 1950's. We can see now why that is not a coincidence. The historical catalyst was\nNoam Chomsky's review of Skinner's Verbal Behavior (Chomsky, 1959). At that time, Anglo-American\nnatural science, social science, and philosophy had come to a virtual consensus about the answers to the\nquestions listed above. The mind consisted of sensorimotor abilities plus a few simple laws of learning\ngoverning gradual changes in an organism's behavioral repertoire. Therefore language must be learned,\nit cannot be a module, and thinking must be a form of verbal behavior, since verbal behavior is the\nprime manifestation of \"thought\" that can be observed externally. Chomsky argued that language\nacquisition falsified these beliefs in a single stroke: children learn languages that are governed by highly\nsubtle and abstract principles, and they do so without explicit instruction or any other environmental\nclues to the nature of such principles. Hence language acquisition depends on an innate, species-specific\nmodule that is distinct from general intelligence. Much of the debate in language acquisition has\nattempted to test this once-revolutionary, and still controversial, collection of ideas. The implications\nextend to the rest of human cognition.\n2 The Biology of Language Acquisition\nHuman language is made possible by special adaptations of the human mind and body that occurred in\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 3of 39\nthe course of human evolution, and which are put to use by children in acquiring their mother tongue.\n2.1 Evolution of Language\nMost obviously, the shape of the human vocal tract seems to have been modified in evolution for the\ndemands of speech. Our larynxes are low in our throats, and our vocal tracts have a sharp right angle\nbend that creates two independently-modifiable resonant cavities (the mouth and the pharynx or throat)\nthat defines a large two-dimensional range of vowel sounds (see the chapter by Liberman). But it comes\nat a sacrifice of efficiency for breathing, swallowing, and chewing (Lieberman, 1984). Before the\ninvention of the Heimlich maneuver, choking on food was a common cause of accidental death in\nhumans, causing 6,000 deaths a year in the United States. The evolutionary selective advantages for\nlanguage must have been very large to outweigh such a disadvantage.\nIt is tempting to think that if language evolved by gradual Darwinian natural selection, we must be able\nto find some precursor of it in our closest relatives, the chimpanzees. In several famous and\ncontroversial demonstrations, chimpanzees have been taught some hand-signs based on American Sign\nLanguage, to manipulate colored switches or tokens, and to understand some spoken commands\n(Gardner & Gardner, 1969; Premack & Premack, 1983; Savage-Rumbaugh, 1991). Whether one wants\nto call their abilities \"language\" is not really a scientific question, but a matter of definition: how far we\nare willing to stretch the meaning of the word \"language\".\nThe scientific question is whether the chimps' abilities are homologous to human language -- that is,\nwhether the two systems show the same basic organization owing to descent from a single system in\ntheir common ancestor. For example, biologists don't debate whether the wing-like structures of gliding\nrodents may be called \"genuine wings\" or something else (a boring question of definitions). It's clear\nthat these structures are not homologous to the wings of bats, because they have a fundamentally\ndifferent anatomical plan, reflecting a different evolutionary history. Bats' wings are modifications of the\nhands of the common mammalian ancestor; flying squirrels' wings are modifications of its rib cage. The\ntwo structures are merely analogous: similar in function.\nThough artificial chimp signaling systems have some analogies to human language (e.g., use in\ncommunication, combinations of more basic signals), it seems unlikely that they are homologous.\nChimpanzees require massive regimented teaching sequences contrived by humans to acquire quite\nrudimentary abilities, mostly limited to a small number of signs, strung together in repetitive, quasi-\nrandom sequences, used with the intent of requesting food or tickling (Terrace, Petitto, Sanders, &\nBever, 1979; Seidenberg & Petitto, 1979, 1987; Seidenberg, 1986; Wallman, 1992; Pinker, 1994a). This\ncontrasts sharply with human children, who pick up thousands of words spontaneously, combine them in\nstructured sequences where every word has a determinate role, respect the word order of the adult\nlanguage, and use sentences for a variety of purposes such as commenting on interesting objects.\nThis lack of homology does not, by the way, cast doubt on a gradualistic Darwinian account of language\nevolution. Humans did not evolve directly from chimpanzees. Both derived from common ancestor,\nprobably around 6-7 million years ago. This leaves about 300,000 generations in which language could\nhave evolved gradually in the lineage leading to humans, after it split off from the lineage leading to\nchimpanzees. Presumably language evolved in the human lineage for two reasons: our ancestors\ndeveloped technology and knowledge of the local environment in their lifetimes, and were involved in\nextensive reciprocal cooperation. This allowed them to benefit by sharing hard-won knowledge with\ntheir kin and exchanging it with their neighbors (Pinker & Bloom, 1990).\n2.2 Dissociations between Language and General Intelligence\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 4of 39\nHumans evolved brain circuitry, mostly in the left hemisphere surrounding the sylvian fissure, that\nappears to be designed for language, though how exactly their internal wiring gives rise to rules of\nlanguage is unknown (see the Chapter by Zurif). The brain mechanisms underlying language are not just\nthose allowing us to be smart in general. Strokes often leave adults with catastrophic losses in language\n(see the Chapter by Zurif, and Pinker, 1994a), though not necessarily impaired in other aspects of\nintelligence, such as those measured on the nonverbal parts of IQ tests. Similarly, there is an inherited\nset of syndromes called Specific Language Impairment (Gopnik and Crago, 1993; Tallal, Ross, &\nCurtiss, 1989) which is marked by delayed onset of language, difficulties in articulation in childhood,\nand lasting difficulties in understanding, producing, and judging grammatical sentences. By definition,\nSpecifically Language Impaired people show such deficits despite the absence of cognitive problems\nlike retardation, sensory problems like hearing loss, or social problems like autism.\nMore interestingly, there are syndromes showing the opposite dissociation, where intact language\ncoexists with severe retardation. These cases show that language development does not depend on fully\nfunctioning general intelligence. One example comes from children with Spina Bifida, a malformation\nof the vertebrae that leaves the spinal cord unprotected, often resulting in hydrocephalus, an increase in\npressure in the cerebrospinal fluid filling the ventricles (large cavities) of the brain, distending the brain\nfrom within. Hydrocephalic children occasionally end up significantly retarded but can carry on long,\narticulate, and fully grammatical conversations, in which they earnestly recount vivid events that are, in\nfact, products of their imaginations (Cromer, 1992; Curtiss, 1989; Pinker, 1994a). Another example is\nWilliams Syndrome, an inherited condition involving physical abnormalities, significant retardation (the\naverage IQ is about 50), incompetence at simple everyday tasks (tying shoelaces, finding one's way,\nadding two numbers, and retrieving items from a cupboard), social warmth and gregariousness, and\nfluent, articulate language abilities (Bellugi, et al., 1990).\n2.3 Maturation of the Language System\nAs the chapter by Newport and Gleitman suggests, the maturation of language circuits during a child's\nearly years may be a driving force underlying the course of language acquisition (Pinker, 1994, Chapter\n9; Bates, Thal, & Janowsky, 1992; Locke, 1992; Huttenlocher, 1990). Before birth, virtually all the\nneurons (nerve cells) are formed, and they migrate into their proper locations in the brain. But head size,\nbrain weight, and thickness of the cerebral cortex (gray matter), where the synapses (junctions)\nsubserving mental computation take place, continue to increase rapidly in the year after birth. Long-\ndistance connections (white matter) are not complete until nine months, and they continue to grow their\nspeed-inducing myelin insulation throughout childhood. Synapses continue to develop, peaking in\nnumber between nine months and two years (depending on the brain region), at which point the child\nhas 50% more synapses than the adult. Metabolic activity in the brain reaches adult levels by nine to ten\nmonths, and soon exceeds it, peaking around the age of four. In addition, huge numbers of neurons die\nin utero, and the dying continues during the first two years before leveling off at age seven. Synapses\nwither from the age of two through the rest of childhood and into adolescence, when the brain's\nmetabolic rate falls back to adult levels. Perhaps linguistic milestones like babbling, first words, and\ngrammar require minimum levels of brain size, long-distance connections, or extra synapses,\nparticularly in the language centers of the brain.\nSimilarly, one can conjecture that these changes are responsible for the decline in the ability to learn a\nlanguage over the lifespan. The language learning circuitry of the brain is more plastic in childhood;\nchildren learn or recover language when the left hemisphere of the brain is damaged or even surgically\nremoved (though not quite at normal levels), but comparable damage in an adult usually leads to\npermanent aphasia (Curtiss, 1989; Lenneberg, 1967). Most adults never master a foreign language,\nespecially the phonology, giving rise to what we call a \"foreign accent.\" Their development often\nfossilizes into permanent error patterns that no teaching or correction can undo. There are great\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 5of 39\nindividual differences, which depend on effort, attitudes, amount of exposure, quality of teaching, and\nplain talent.\nMany explanations have been advanced for children's superiority: they can exploit the special ways that\ntheir mothers talk them, they make errors unself-consciously, they are more motivated to communicate,\nthey like to conform, they are not xenophobic or set in their ways, and they have no first language to\ninterfere. But some of these accounts are unlikely, based on what we learn about how language\nacquisition works later in this chapter. For example, children can learn a language without the special\nindulgent speech from their mothers; they make few errors; and they get no feedback for the errors they\ndo make. And it can't be an across-the-board decline in learning. There is no evidence, for example, that\nlearning words (as opposed to phonology or grammar) declines in adulthood.\nThe chapter by Newport and Gleitman shows how sheer age seems to play an important role. Successful\nacquisition of language typically happens by 4 (as we shall see in the next section), is guaranteed for\nchildren up to the age of six, is steadily compromised from then until shortly after puberty, and is rare\nthereafter. Maturational changes in the brain, such as the decline in metabolic rate and number of\nneurons during the early school age years, and the bottoming out of the number of synapses and\nmetabolic rate around puberty, are plausible causes. Thus, there may be a neurologically-determined\n\"critical period\" for successful language acquisition, analogous to the critical periods documented in\nvisual development in mammals and in the acquisition of songs by some birds.\n3 The Course of Language Acquisition\nAlthough scholars have kept diaries of their children's speech for over a century (Charles Darwin was\none of the first), it was only after portable tape-recorders became available in the late 1950's that\nchildren's spontaneous speech began to be analyzed systematically within developmental psychology.\nThese naturalistic studies of children's spontaneous speech have become even more accessible now that\nthey can be put into computer files and can be disseminated and analyzed automatically (MacWhinney\n& Snow, 1985, 1990; MacWhinney, 1991). They are complemented by experimental methods. In\nproduction tasks, children utter sentences to describe pictures or scenes, in response to questions, or to\nimitate target sentences. In comprehension tasks, they listen to sentences and then point to pictures or act\nout events with toys. In judgement tasks, they indicate whether or which sentences provided by an\nexperimenter sound \"silly\" to them.\nAs the chapter by Werker shows, language acquisition begins very early in the human lifespan, and\nbegins, logically enough, with the acquisition of a language's sound patterns. The main linguistic\naccomplishments during the first year of life are control of the speech musculature and sensitivity to the\nphonetic distinctions used in the parents' language. Interestingly, babies achieve these feats before they\nproduce or understand words, so their learning cannot depend on correlating sound with meaning. That\nis, they cannot be listening for the difference in sound between a word they think means bit and a word\nthey think means beet, because they have learned neither word. They must be sorting the sounds\ndirectly, somehow tuning their speech analysis module to deliver the phonemes used in their language\n(Kuhl, et al., 1992). The module can then serve as the front end of the system that learns words and\ngrammar.\nShortly before their first birthday, babies begin to understand words, and around that birthday, they start\nto produce them (see Clark, 1993; Ingram, 1989). Words are usually produced in isolation; this one-\nword stage can last from two months to a year. Children's first words are similar all over the planet.\nAbout half the words are for objects: food (juice, cookie, body parts (eye, nose), clothing (diaper, sock),\nvehicles (car, boat), toys (doll, block), household items (bottle, light, animals (dog, kitty), and people\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 6of 39\n(dada, baby). There are words for actions, motions, and routines, like (up, off, open, peekaboo, eat, and\ngo, and modifiers, like hot, allgone, more, dirty, and cold. Finally, there are routines used in social\ninteraction, like yes, no, want, bye-bye, and hi -- a few of which, like look at that and what is that, are\nwords in the sense of memorized chunks, though they are not single words for the adult. Children differ\nin how much they name objects or engage in social interaction using memorized routines, though all\nchildren do both.\nAround 18 months, language changes in two ways. Vocabulary growth increases; the child begins to\nlearn words at a rate of one every two waking hours, and will keep learning that rate or faster through\nadolescence (Clark, 1993; Pinker, 1994). And primitive syntax begins, with two-word strings like the\nfollowing:\nAll dry. All messy. All wet.\nI sit. I shut. No bed.\nNo pee. See baby. See pretty.\nMore cereal. More hot. Hi Calico.\nOther pocket. Boot off. Siren by.\nMail come. Airplane allgone. Bybebye car.\nOur car. Papa away. Dry pants.\nOur car. Papa away. Dry pants. Children's two-word combinations are highly similar across cultures.\nEverywhere, children announce when objects appear, disappear, and move about, point out their\nproperties and owners, comment on people doing things and seeing things, reject and request objects and\nactivities, and ask about who, what, and where. These sequences already reflect the language being\nacquired: in 95% of them, the words are properly ordered (Braine, 1976; Brown, 1973; Pinker, 1984;\nIngram, 1989).\nEven before they put words together, babies can comprehend a sentence using its syntax. For example,\nin one experiment, babies who spoke only in single words were seated in front of two television screens,\neach of which featured a pair of adults dressed up as Cookie Monster and Big Bird from Sesame Street.\nOne screen showed Cookie Monster tickling Big Bird; the other showed Big Bird tickling Cookie\nMonster. A voice-over said, \"OH LOOK!!! BIG BIRD IS TICKLING COOKIE MONSTER!! FIND\nBIG BIRD TICKLING COOKIE MONSTER!!\" (Or vice-versa.) The children must have understood the\nmeaning of the ordering of subject, verb, and object, because they looked more at the screen that\ndepicted the sentence in the voice-over (Hirsh-Pasek & Golinkoff, 1991).\nChildren's output seems to meet up with a bottleneck at the output end (Brown, 1973; Bloom, 1970;\nPinker, 1984). Their two- and three-word utterances look like samples drawn from longer potential\nsentences expressing a complete and more complicated idea. Roger Brown, one of the founders of the\nmodern study of language development, noted that although the three children he studied intensively\nnever produced a sentence as complicated as Mother gave John lunch in the kitchen, they did produce\nstrings containing all of its components, and in the correct order: (Brown, 1973, p. 205):\nAgent Action Recipient Object Location\n(Mother gave John lunch in the kitchen.)\nMommy fix.\nMommy pumpkin.\nBaby table.\nGive doggie.\nPut light.\nPut floor.\nI ride horsie.\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 7of 39\nTractor go floor.\nGive doggie paper.\nPut truck window.\nAdam put it box.\nBetween the late two's and mid-three's, children's language blooms into fluent grammatical conversation\nso rapidly that it overwhelms the researchers who study it, and no one has worked out the exact\nsequence. Sentence length increases steadily, and because grammar is a combinatorial system, the\nnumber of syntactic types increases exponentially, doubling every month, reaching the thousands before\nthe third birthday (Ingram, 1989, p. 235; Brown, 1973; Limber, 1973; Pinker, 1984). For example, here\nare snapshots of the development of one of Brown's longitudinal subjects, Adam, in the year following\nhis first word combinations at the age of 2 years and 3 months (Pinker, 1994a):\n2;3: Play checkers. Big drum. I got horn.\n2;4: See marching bear go? Screw part machine.\n2;5: Now put boots on. Where wrench go? What that paper clip doing?\n2;6: Write a piece a paper. What that egg doing? No, I don't want to sit seat.\n2;7: Where piece a paper go? Dropped a rubber band. Rintintin don't fly, Mommy.\n2;8: Let me get down with the boots on. How tiger be so healthy and\nfly like kite? Joshua throw like a penguin.\n2;9: Where Mommy keep her pocket book? Show you something funny.\n2;10: Look at that train Ursula brought. You don't have paper. Do you want little bit,\nCromer?\n2;11: Do want some pie on your face? Why you mixing baby chocolate? I said why not you\ncoming in? We going turn light on so you can't - see.\n3;0: I going come in fourteen minutes. I going wear that to wedding. Those are not strong\nmens. You dress me up like a baby elephant.\n3;1: I like to play with something else. You know how to put it back together. I gon' make it\nlike a rocket to blast off with. You want - to give me some carrots and some beans? Press\nthe button and catch - it, sir. Why you put the pacifier in his mouth?\n3;2: So it can't be cleaned? I broke my racing car. Do you know the light wents off? When\nit's got a flat tire it's need a go to the station. I'm going to mail this so the letter can't come\noff. I - want to have some espresso. Can I put my head in the mailbox so - the mailman can\nknow where I are and put me in the mailbox? Can I - keep the screwdriver just like a\ncarpenter keep the screwdriver?\nNormal children can differ by a year or more in their rate of language development, though the stages\nthey pass through are generally the same regardless of how stretched out or compressed. Adam's\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 8of 39\nlanguage development, for example, was relatively leisurely; many children speak in complex sentences\nbefore they turn two.\nDuring the grammar explosion, children's sentences are getting not only longer but more complex, with\nfuller trees, because the children can embed one constituent inside another. Whereas before they might\nhave said Give doggie paper (a three-branch Verb Phrase) and Big doggie (a two-branch Noun Phrase),\nthey now say Give big doggie paper, with the two-branch NP embedded inside the three-branch VP. The\nearlier sentences resembled telegrams, missing unstressed function words like of, the, on, and does, as\nwell as inflections like -ed, -ing, and -s. By the 3's, children are using these function words more often\nthan they are omitting them, many in more than 90% of the sentences that require them. A full range of\nsentence types flower -- questions with words like who, what and where, relative clauses, comparatives,\nnegations, complements, conjunctions, and passives. These constructions appear to display the most,\nperhaps even all, of the grammatical machinery needed to account for adult grammar.\nThough many of the young 3-year-old's sentences are ungrammatical for one reason or another, it is\nbecause there are many things that can go wrong in any single sentence. When researchers focus on a\nsingle grammatical rule and count how often a child obeys it and how often he or she versus flouts it, the\nresults are very impressive: for just about every rule that has been looked at, three-year olds obey it a\nmajority of the time (Stromswold, 1990; Pinker, 1984, 1989; Crain, 1992; Marcus, et al., 1992). As we\nhave seen, children rarely scramble word orders and, by the age of three, come to supply most\ninflections and function words in sentences that require them. Though our ears perk up when we hear\nerrors like mens, wents, Can you broke those?, What he can ride in?, That's a furniture, Button me the\nrest, and Going to see kitten, the errors occur in anywhere from 0.1% to 8% of the opportunities for\nmaking them; more than 90% of the time, the child is on target. The next chapter follows one of those\nerrors in detail.\nChildren do not seem to favor any particular kind of language (indeed, it would be puzzling how any\nkind of language could survive if children did not easily learn it!). They swiftly acquire free word order,\nSOV and VSO orders, rich systems of case and agreement, strings of agglutinated suffixes, ergative case\nmarking, and whatever else their language throws at them, with no lag relative to their English-speaking\ncounterparts. Even grammatical gender, which many adults learning a second language find mystifying,\npresents no problem: children acquiring language like French, German, and Hebrew acquire gender\nmarking quickly, make few errors, and never use the association with maleness and femaleness as a false\ncriterion (Levy, 1983). It is safe to say that except for constructions that are rare, predominantly used in\nwritten language, or mentally taxing even to an adult (like The horse that the elephant tickled kissed the\npig), all parts of all languages are acquired before the child turns four (Slobin, 1985\/1992).\n4 Explaining Language Acquisition\nHow do we explain children's course of language acquisition -- most importantly, their inevitable and\nearly mastery? Several kinds of mechanisms are at work. As we saw in section (), the brain changes after\nbirth, and these maturational changes may govern the onset, rate, and adult decline of language\nacquisition capacity. General changes in the child's information processing abilities (attention, memory,\nshort-term buffers for acoustic input and articulatory output) could leave their mark as well. In the next\nchapter, I show how a memory retrieval limitation -- children are less reliable at recalling that broke is\nthe past tense of break -- can account for a conspicuous and universal error pattern, overregularizations\nlike breaked (see also Marcus, et al., 1992).\nMany other small effects have been documented where changes in information processing abilities\naffect language development. For example, children selectively pick up information at the ends of words\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 9of 39\n(Slobin, 1973), and at the beginnings and ends of sentences (Newport, et al, 1977), presumably because\nthese are the parts of strings that are best retained in short term memory. Similarly, the progressively\nwidening bottleneck for early word combinations presumably reflects a general increase in motor\nplanning capacity. Conceptual development (see Chapter X), too, might affect language development: if\na child has not yet mastered a difficult semantic distinction, such as the complex temporal relations\ninvolved in John will have gone, he or she may be unable to master the syntax of the construction\ndedicated to expressing it.\nThe complexity of a grammatical form has a demonstrable role in development: simpler rules and forms\nappear in speech before more complex ones, all other things being equal. For example, the plural marker\n-s in English (e.g. cats), which requires knowing only whether the number of referents is singular or\nplural, is used consistently before the present tense marker -s (he walks), which requires knowing\nwhether the subject is singular or plural and whether it is a first, second, or third person and whether the\nevent is in the present tense (Brown, 1973). Similarly, complex forms are sometimes first used in\nsimpler approximations. Russian contains one case marker for masculine nominative (i.e., a suffix on a\nmasculine noun indicating that it is the subject of the sentence), one for feminine nominative, one for\nmasculine accusative (used to indicate that a noun is a direct object), and one for feminine accusative.\nChildren often use each marker with the correct case, never using a nominative marker for accusative\nnouns or vice-versa, but don't properly use the masculine and feminine variants with masculine and\nfeminine nouns (Slobin, 1985).\nBut these global trends do not explain the main event: how children succeed. Language acquisition is so\ncomplex that one needs a precise framework for understanding what it involves -- indeed, what learning\nin general involves.\n4.1 Learnability Theory\nWhat is language acquisition, in principle? A branch of theoretical computer science called Learnability\nTheory attempts to answer this question (Gold, 1967; Osherson, Stob, & Weinstein, 1985; Pinker,\n1979). Learnability theory has defined learning as a scenario involving four parts (the theory embraces\nall forms of learning, but I will use language as the example):\n1. A class of languages. One of them is the \"target\" language, to be - attained by the learner, but the\nlearner does not, of course, know - which it is. In the case of children, the class of languages\nwould - consist of the existing and possible human languages; the target - language is the one\nspoken in their community.\n2. An environment. This is the information in the world that the learner has to go on in trying to\nacquire the language. In the case of children, it might include the sentences parents utter, the\ncontext in which they utter them, feedback to the child (verbal or nonverbal) in response to the\nchild's own speech, and so on. Parental utterances can be a random sample of the language, or\nthey might have some special properties: they might be ordered in certain ways, sentences might\nbe repeated or only uttered once, and so on.\n3. A learning strategy. The learner, using information in the environment, tries out \"hypotheses\"\nabout the target language. The learning strategy is the algorithm that creates the hypotheses and\ndetermines whether they are consistent with the input information from the environment. For\nchildren, it is the \"grammar-forming\" mechanism in their brains; their \"language acquisition\ndevice.\"\n4. A success criterion. If we want to say that \"learning\" occurs, presumably it is because the learners'\nhypotheses are not random, - but that by some time the hypotheses are related in some systematic\n- way to the target language. Learners may arrive at a hypothesis - identical to the target language\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 10of 39\nafter some fixed period of time; - they may arrive at an approximation to it; they may waiver\namong a - set of hypotheses one of which is correct.\nTheorems in learnability theory show how assumptions about any of the three components imposes\nlogical constraints on the fourth. It is not hard to show why learning a language, on logical grounds\nalone, is so hard. Like all \"induction problems\" (uncertain generalizations from instances), there are an\ninfinite number of hypotheses consistent with any finite sample of environmental information.\nLearnability theory shows which induction problems are solvable and which are not.\nA key factor is the role of negative evidence, or information about which strings of words are not\nsentences in the language to be acquired. Human children might get such information by being corrected\nevery time they speak ungrammatically. If they aren't -- and as we shall see, they probably aren't -- the\nacquisition problem is all the harder. Consider Figure 1, where languages are depicted as circles\ncorresponding to sets of word strings, and all the logical possibilities for how the child's language could\ndiffer from the adult language are depicted. There are four possibilities. (a) The child's hypothesis\nlanguage (H) is disjoint from the language to be acquired (the \"target language,\" T). That would\ncorrespond to the state of child learning English who cannot say a single well-formed English sentence.\nFor example, the child might be able only to say things like we breaked it, and we goed, never we broke\nit or we went. (b) The child's hypothesis and the target language intersect. Here the child would be able\nto utter some English sentences, like he went. However, he or she also uses strings of words that are not\nEnglish, such as we breaked it; and some sentences of English, such as we broke it, would still be\noutside their abilities. (c) The child's hypothesis language is a subset of the target language. That would\nmean that the child would have mastered some of English, but not all of it, but that everything the child\nhad mastered would be part of English. The child might not be able to say we broke it, but he or she\nwould be able to say some grammatical sentences, such as we went; no errors such as she breaked it or\nwe goed would occur. The final logical possibility is (d), where The child's hypothesis language is a\nsuperset of the target language. That would occur, for example, if the child could say we broke it, we\nwent, we breaked it and we goed.\nIn cases (a-c), the child can realize that the hypothesis is incorrect by hearing sentences from parental\n\"positive evidence,\" (indicated by the \"+\" symbol) that are in the target language but not the\nhypothesized one: sentences such as we broke it. This is impossible in case (d); negative evidence (such\nas corrections of the child's ungrammatical sentences by his or her parents) would be needed. In other\nwords, without negative evidence, if a child guesses too large a language, the world can never tell him\nhe's wrong.\nThis has several consequences. For one thing, the most general learning algorithm one might conceive\nof -- one that is capable of hypothesizing any grammar, or any computer program capable of generating\na language -- is in trouble. Without negative evidence (and even in many cases with it), there is no\ngeneral-purpose, all-powerful learning machine; a machine must in some sense \"know\" something about\nthe constraints in the domain in which it is learning.\nMore concretely, if children don't receive negative evidence (see Section ) we have a lot of explaining to\ndo, because overly large hypotheses are very easy for the child to make. For example, children actually\ndo go through stages in which they use two or more past tense forms for a given verb, such as broke and\nbreaked -- this case is discussed in detail in my other chapter in this volume. They derive transitive verbs\nfrom intransitives too freely: where an adult might say both The ice melted and I melted the ice, children\nalso can say The girl giggled and Don't giggle me! (Bowerman, 1982b; Pinker, 1989). In each case they\nare in situation (d) in Figure 1, and unless their parents slip them some signal in every case that lets them\nknow they are not speaking properly, it is puzzling that they eventually stop. That is, we would need to\nexplain how they grow into adults who are more restrictive in their speech -- or another way of putting is\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 11of 39\nthat it's puzzling that the English language doesn't allow don't giggle me and she eated given that\nchildren are tempted to grow up talking that way. If the world isn't telling children to stop, something in\ntheir brains is, and we have to find out who or what is causing the change.\nLet's now examine language acquisition in the human species by breaking it down into the four elements\nthat give a precise definition to learning: the target of learning, the input, the degree of success, and the\nlearning strategy.\n5 What is Learned\nTo understand how X is learned, you first have to understand what X is. Linguistic theory is thus an\nessential part of the study of language acquisition (see the Chapter by Lasnik). Linguistic research tries\ndo three things. First, it must characterize the facts of English, and all the other languages whose\nacquisition we are interested in explaining. Second, since children are not predisposed to learn English\nor any other language, linguistics has to examine the structure of other languages. In particular, linguists\ncharacterize which aspects of grammar are universal, prevalent, rare, and nonexistent across languages.\nContrary to early suspicions, languages do not vary arbitrarily and without limit; there is by now a large\ncatalogue of language universals, properties shared exactly, or in a small number of variations, by all\nlanguages (see Comrie, 1981; Greenberg, 1978; Shopen, 1985). This obviously bears on what children's\nlanguage acquisition mechanisms find easy or hard to learn.\nAnd one must go beyond a mere list of universals. Many universal properties of language are not\nspecific to language but are simply reflections of universals of human experience. All languages have\nwords for \"water\" and \"foot\" because all people need to refer to water and feet; no language has a word\na million syllables long because no person would have time to say it. But others might be specific to the\ninnate design of language itself. For example, if a language has both derivational suffixes (which create\nnew words from old ones, like -ism) and inflectional suffixes (which modify a word to fit its role in the\nsentence, like plural -s), then the derivational suffixes are always closer to the word stem than the\ninflectional ones. For example, in English one can say Darwinisms (derivational -ism closer to the stem\nthan inflectional -s) but not Darwinsism. It is hard to think of a reason how this law would fit in to any\nuniversal law of thought or memory: why would the concept of two ideologies based on one Darwin\nshould be thinkable, but the concept of one ideology based on two Darwins (say, Charles and Erasmus)\nnot be thinkable (unless one reasons in a circle and declares that the mind must find -ism to be more\ncognitively basic than the plural, because that's the order we see in language). Universals like this, that\nare specifically linguistic, should be captured in a theory of Universal Grammar (UG) (Chomsky, 1965,\n1981, 1991). UG specifies the allowable mental representations and operations that all languages are\nconfined to use. The theory of universal grammar is closely tied to the theory of the mental mechanisms\nchildren use in acquiring language; their hypotheses about language must be couched in structures\nsanctioned by UG.\nTo see how linguistic research can't be ignored in understanding language acquisition, consider the\nsentences below. In each of the examples, a learner who heard the (a) and (b) sentences could quite\nsensibly extract a general rule that, when applied to the (c) sentence, yield version (d). Yet the result is\nan odd sentence that no one would say:\n1. (a) John saw Mary with her best friend's husband.\n(b) Who did John see Mary with?\n(c) John saw Mary and her best friend's husband.\n(d) *Who did John see Mary and?\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 12of 39\n2. (a) Irv drove the car into the garage.\n(b) Irv drove the car.\n(c) Irv put the car into the garage.\n(d) *Irv put the car.\n3. (a) I expect the fur to fly.\n(b) I expect the fur will fly.\n(c) The fur is expected to fly.\n(d) *The fur is expected will fly.\n4. (a) The baby seems to be asleep.\n(b) The baby seems asleep.\n(c) The baby seems to be sleeping.\n(d) *The baby seems sleeping.\n5. (a) John liked the pictures of Bill that Mary took.\n(b) John liked Mary's pictures of Bill.\n(c) John liked the pictures of himself that Mary took.\n(d) *John liked Mary's pictures of himself.\nThe solution to the problem must be that children's learning mechanisms ultimately don't allow them to\nmake what would otherwise be a tempting generalization. For example, in (1), constraints that prevent\nextraction of a single phrase out of a coordinate structure (phrases joined by a word like and or or)\nwould block would what otherwise be a natural generalization from other examples of extraction, such\nas 1(a-b). The other examples presents other puzzles that the theory of universal grammar, as part of a\ntheory of language acquisition, must solve. It is because of the subtlety of these examples, and the\nabstractness of the principles of universal grammar that must be posited to explain them, that Chomsky\nhas claimed that the overall structure of language must be innate, based on his paper-and-pencil\nexamination of the facts of language alone.\n6 Input\nTo understand how children learn language, we have to know what aspects of language (from their\nparents or peers) they have access to.\n6.1 Positive Evidence\nChildren clearly need some kind of linguistic input to acquire a language. There have been occasional\ncases in history where abandoned children have somehow survived in forests, such as Victor, the Wild\nBoy of Aveyron (subject of a film by Francois Truffaut). Occasionally other modern children have\ngrown up wild because depraved parents have raised them silently in dark rooms and attics; the chapter\nby Newport and Gleitman discuss some of those cases. The outcome is always the same: the children,\nwhen found, are mute. Whatever innate grammatical abilities there are, they are too schematic to\ngenerate concrete speech, words, and grammatical constructions on their own.\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 13of 39\nChildren do not, however, need to hear a full-fledged language; as long as they are in a community with\nother children, and have some source for individual words, they will invent one on their own, often in a\nsingle generation. Children who grew up in plantations and slave colonies were often exposed to a crude\npidgin that served as the lingua franca in these Babels of laborers. But they grew up to speak genuinely\nnew languages, expressive \"creoles\" with their own complex grammars (Bickerton, 1984; see also the\nChapter by Newport and Gleitman). The sign languages of the deaf arose in similar ways. Indeed, they\narise spontaneously and quickly wherever there is a community of deaf children (Senghas, 1994; Kegl,\n1994).\nChildren most definitely do need to hear an existing language to learn that language, of course. Children\nwith Japanese genes do not find Japanese any easier than English, or vice-versa; they learn whichever\nlanguage they are exposed to. The term \"positive evidence\" refers to the information available to the\nchild about which strings of words are grammatical sentences of the target language.\nBy \"grammatical,\" incidentally, linguists and psycholinguists mean only those sentences that sound\nnatural in colloquial speech, not necessarily those that would be deemed \"proper English\" in formal\nwritten prose. Thus split infinitives, dangling participles, slang, and so on, are \"grammatical\" in this\nsense (and indeed, are as logical, systematic, expressive, and precise as \"correct\" written English, often\nmore so; see Pinker, 1994a). Similarly, elliptical utterances, such as when the question Where are you\ngoing? is answered with To the store), count as grammatical. Ellipsis is not just random snipping from\nsentences, but is governed by rules that are part of the grammar of one's language or dialect. For\nexample, the grammar of casual British English allows you to answer the question Will he go? by saying\nHe might do, whereas the grammar of American English doesn't allow it.\nGiven this scientific definition of \"grammatical,\" do we find that parents' speech counts as \"positive\nevidence\"? That is, when a parent uses a sentence, can the child assume that it is part of the language to\nbe learned, or do parents use so many ungrammatical sentences random fragments, slips of the tongue,\nhesitations, and false starts that the child would have to take much of it with a grain of salt? Fortunately\nfor the child, the vast majority of the speech they hear during the language-learning years is fluent,\ncomplete, and grammatically well-formed: 99.93%, according to one estimate (Newport, Gleitman, &\nGleitman, 1977). Indeed, this is true of conversation among adults in general (Labov, 1969).\nThus language acquisition is ordinarily driven by a grammatical sample of the target language. Note that\nhis is true even for forms of English that people unthinkingly call \"ungrammatical,\" \"fractured,\" or \"bad\nEnglish,\" such as rural American English (e.g., them books; he don't; we ain't; they drug him away) and\nurban black English (e.g., She walking; He be working; see the Chapter by Labov). These are not\ncorrupted versions of standard English; to a linguist they look just like different dialects, as rule-\ngoverned as the southern-England dialect of English that, for historical reasons, became the standard\nseveral centuries ago. Scientifically speaking, the grammar of working-class speech -- indeed, every\nhuman language system that has been studied -- is intricately complex, though different languages are\ncomplex in different ways.\n6.2 Negative Evidence\nNegative evidence refers to information about which strings of words are not grammatical sentences in\nthe language, such as corrections or other forms of feedback from a parent that tell the child that one of\nhis or her utterances is ungrammatical. As mentioned in Section ), it's very important for us to know\nwhether children get and need negative, because in the absence of negative evidence, any child who\nhypothesizes a rule that generates a superset of the language will have no way of knowing that he or she\nis wrong Gold, 1967; Pinker, 1979, 1989). If children don't get, or don't use, negative evidence, they\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 14of 39\nmust have some mechanism that either avoids generating too large a language the child would be\nconservative -- or that can recover from such overgeneration.\nRoger Brown and Camille Hanlon (1970) attempted to test B. F. Skinner's behaviorist claim that\nlanguage learning depends on parents' reinforcement of children's grammatical behaviors. Using\ntranscripts of naturalistic parent-child dialogue, they divided children's sentences into ones that were\ngrammatically well-formed and ones that contained grammatical errors. They then divided adults'\nresponses to those sentences into ones that expressed some kind of approval (e.g., \"yes, that's good\") and\nthose that expressed some kind of disapproval. They looked for a correlation, but failed to find one:\nparents did not differentially express approval or disapproval to their children contingent on whether the\nchild's prior utterance was well-formed or not (approval depends, instead, on whether the child's\nutterance was true). Brown and Hanlon also looked at children's well-formed and badly-formed\nquestions, and whether parents seemed to answer them appropriately, as if they understood them, or with\nnon sequiturs. They found parents do not understand their children's well-formed questions better than\ntheir badly-formed ones.\nOther studies (e.g. Hirsh-Pasek, Treiman, and Schneiderman, 1984; Demetras, Post, and Snow, 1986;\nPenner, 1987; Bohannon & Stanowicz, 1988) have replicated that result, but with a twist. Some have\nfound small statistical contingencies between the grammaticality of some children's sentence and the\nkind of follow-up given by their parents; for example, whether the parent repeats the sentence verbatim,\nasks a follow-up question, or changes the topic. But Marcus (1993) has found that these patterns fall far\nshort of negative evidence (reliable information about the grammatical status of any word string).\nDifferent parents react in opposite ways to their children's ungrammatical sentences, and many forms of\nungrammaticality are not reacted to at all -- leaving a given child unable to know what to make of any\nparental reaction. Even when a parent does react differentially, a child would have to repeat a particular\nerror, verbatim, hundreds of times to eliminate the error, because the parent's reaction is only statistical:\nthe feedback signals given to ungrammatical signals are also given nearly as often to grammatical\nsentences.\nStromswold (1994) has an even more dramatic demonstration that parental feedback cannot be crucial.\nShe studied a child who, for unknown neurological reasons, was congenitally unable to talk. He was a\ngood listener, though, and when tested he was able to understand complicated sentences perfectly, and to\njudge accurately whether a sentence was grammatical or ungrammatical. The boy's abilities show that\nchildren certainly do not need negative evidence to learn grammatical rules properly, even in the\nunlikely event that their parents provided it.\nThese results, though of profound importance, should not be too surprising. Every speaker of English\njudges sentences such as I dribbled the floor with paint and Ten pounds was weighed by the boy and\nWho do you believe the claim that John saw? and John asked Mary to look at himself to be\nungrammatical. But it is unlikely that every such speaker has at some point uttered these sentences and\nbenefited from negative feedback. The child must have some mental mechanisms that rule out vast\nnumbers of \"reasonable\" strings of words without any outside intervention.\n6.3 Motherese\nParents and caretakers in most parts of the world modify their speech when talking to young children,\none example of how people in general use several \"registers\" in different social settings. Speech to\nchildren is slower, shorter, in some ways (but not all) simpler, higher-pitched, more exaggerated in\nintonation, more fluent and grammatically well-formed, and more directed in content to the present\nsituation, compared to speech among adults (Snow & Ferguson, 1977). Many parents also expand their\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 15of 39\nchildren's utterances into full sentences, or offer sequences of paraphrases of a given sentence.\nOne should not, though, consider this speech register, sometimes called \"Motherese,\" to be a set of\n\"language lessons.\" Though mother's speech may seem simple at first glance, in many ways it is not. For\nexample, speech to children is full of questions -- sometimes a majority of the sentences. If you think\nquestions are simple, just try to write a set of rules that accounts for the following sentences and non-\nsentences:\n1. He can go somewhere.\nWhere can he go?\n*Where can he go somewhere?\n*Where he can go?\n*Where did he can go?\n2. He went somewhere.\nWhere did he go?\nHe went WHERE?\n*Where went he?\n*Where did he went?\n*Where he went?\n*He did go WHERE?\n3. He went home.\nWhy did he go home?\nHow come he went home?\n*Why he went home?\n*How come did he go home?\nLinguists struggle over these facts (see the Chapters by Lasnik and Larson), some of the most puzzling\nin the English language. But these are the constructions that infants are bombarded with and that they\nmaster in their preschool years.\nThe chapter by Newport and Gleitman gives another reason for doubting that Motherese is a set of\nlanguage lessons. Children whose mothers use Motherese more consistently don't pass through the\nmilestones of language development any faster (Newport, et al, 1977). Furthermore, there are some\ncommunities with radically different ideas about children's proper place in society. In some societies, for\nexample, people tacitly assume that that children aren't worth speaking to, and don't have anything to\nsay that is worth listening to. Such children learn to speak by overhearing streams of adult-to-adult\nspeech (Heath, 1983). In some communities in New Guinea, mothers consciously try to teach their\nchildren language, but not in the style familiar to us, of talking to them indulgently. Rather, they wait\nuntil a third party is present, and coach the child as to the proper, adultlike sentences they should use\n(see Schieffelin & Eisenberg, 1981). Nonetheless, those children, like all children, grow up to be fluent\nlanguage speakers. It surely must help children when their parents speak slowly, clearly, and succinctly\nto them, but their success at learning can't be explained by any special grammar-unveiling properties of\nparental babytalk.\n6.4 Prosody\nParental speech is not a string of printed words on a ticker-tape, nor is it in a monotone like science-\nfiction robots. Normal human speech has a pattern of melody, timing, and stress called prosody. And\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 16of 39\nmotherese directed to young infants has a characteristic, exaggerated prosody of its own: a rise and fall\ncontour for approving, a set of sharp staccato bursts for prohibiting, a rise pattern for directing attention,\nand smooth, low legato murmurs for comforting. Fernald (1992) has shown that these patterns are very\nwidespread across language communities, and may be universal. The melodies seem to attract the child's\nattention, mark the sounds as speech as opposed to stomach growlings or other noises, and might\ndistinguish statements, questions, and imperatives, delineate major sentence boundaries, and highlight\nnew words. When given a choice, babies prefer to listen to speech with these properties than to speech\nintended for adults (Fernald, 1984, 1992; Hirsh-Pasek, Nelson, Jusczyk, Cassidy, Druss, & Kennedy,\n1987).\nIn all speech, a number of prosodic properties of the speech wave, such as lengthening, intonation, and\npausing, are influenced by the syntactic structure of the sentence (Cooper & Paccia-Cooper, 1980). Just\nlisten to how you would say the word like in the sentence The boy I like slept compared to The boy I\nsaw likes sleds. In the first sentence, the word like is at the boundary of a relative clause and is drawn\nout, exaggerated in intonation, and followed by a pause; in the second, it is in the middle of a verb\nphrase and is pronounced more quickly, uniformly in intonation, and is run together with the following\nword. Some psychologists (e.g., Gleitman & Wanner, 1984; Gleitman, 1990) have suggested that\nchildren use this information in the reverse direction, and read the syntactic structure of a sentence\ndirectly off its melody and timing. We will examine the hypothesis in Section .\n6.5 Context\nChildren do not hear sentences in isolation, but in a context. No child has learned language from the\nradio; indeed, children rarely if ever learn language from television. Ervin-Tripp (1973) studied hearing\nchildren of deaf parents whose only access to English was from radio or television broadcasts. The\nchildren did not learn any speech from that input. One reason is that without already knowing the\nlanguage, it would be difficult for a child to figure out what the characters in the unresponsive televised\nworlds are talking about. In interacting with live human speakers, who tend to talk about the here and\nnow in the presence of children, the child can be more of a mind-reader, guessing what the speaker\nmight have meant (Macnamara, 1972, 1982; Schlesinger, 1971). That is, before children have learned\nsyntax, they know the meaning of many words, and they might be able to make good guesses as to what\ntheir parents are saying based on their knowledge of how the referents of these words typically act (for\nexample, people tend to eat apples, but not vice-versa). In fact, parental speech to young children is so\nredundant with its context that a person with no knowledge of the order in which parents' words are\nspoken, only the words themselves, can infer from transcripts, with high accuracy, what was being said\n(Slobin, 1977).\nMany models of language acquisition assume that the input to the child consists of a sentence and a\nrepresentation of the meaning of that sentence, inferred from context and from the child's knowledge of\nthe meanings of the words (e.g. Anderson, 1977; Berwick, 1986; Pinker, 1982, 1984; Wexler &\nCulicover, 1980). Of course, this can't literally be true -- children don't hear every word of every\nsentence, and surely don't, to begin with, perceive the entire meaning of a sentence from context. Blind\nchildren, whose access to the nonlinguistic world is obviously severely limited, learn language without\nmany problems (Landau & Gleitman, 1985). And when children do succeed in guessing a parent's\nmeaning, it can't be by simple temporal contiguity. For example, Gleitman (1990) points out that when a\nmother arriving home from work opens the door, she is likely to say, \"What did you do today?,\" not I'm\nopening the door. Similarly, she is likely to say \"Eat your peas\" when her child is, say, looking at the\ndog, and certainly not when the child is already eating peas.\nStill, the assumption of context-derived semantic input is a reasonable idealization, if one considers the\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 17of 39\nabilities of the whole child. The child must keep an updated mental model of the current situation,\ncreated by mental faculties for perceiving objects and events and the states of mind and communicative\nintentions of other humans. The child can use this knowledge, plus the meanings of any familiar words\nin the sentence, to infer what the parent probably meant. In Section we will discuss how children might\nfill the important gaps in what they can infer from context.\n7 What and When Children Learn\nPeople do not reproduce their parents' language exactly. If they did, we would all still be speaking like\nChaucer. But in any generation, in most times, the differences between parents' language and the one\ntheir children ultimately acquire is small. And remember that, judging by their spontaneous speech, we\ncan conclude that most children have mastered their mother tongue (allowing for performance errors due\nto complexity or rarity of a construction) some time in their threes. It seems that the success criterion for\nhuman language is something close to full mastery, and in a short period of time.\nTo show that young children really have grasped the design plan of language, rather than merely\napproximating it with outwardly-convincing routines or rules of thumb which would have to be\nsupplanted later in life, we can't just rely on what they say; we need to use clever experimental\ntechniques. Let's look at two examples that illustrate how even very young children seem to obey the\ninnate complex design of Universal Grammar.\nEarlier I mentioned that in all languages, if there are derivational affixes that build new words out of old\nones, like -ism, -er, and -able, and inflectional affixes that modify a word according to its role in the\nsentence, like -s, -ed, and -ing, then the derivational affix appears inside the inflectional one:\nDarwinisms is possible, Darwinsism is not. This and many other grammatical quirks were nicely\nexplained in a theory of word structure proposed by Paul Kiparsky (1982).\nKiparsky showed that words are built in layers or \"levels.\" To build a word, you can start with a root\n(like Darwin). Then you can rules of a certain kind to it, called \"Level 1 Rules,\" to yield a more complex\nword. For example, there is a rule adding the suffix -ian, turning the word into Darwinian. Level 1\nRules, according to the theory, can affect the sound of the stem; in this case, the syllable carrying the\nstress shifts from Dar to win. Level 2 rules apply to a word after any Level 1 rules have been applied.\nAn example of a Level 2 rule is the one that adds the suffix -ism, yielding, for example, Darwinism.\nLevel 2 rules generally do not affect the pronunciation of the words they apply to; they just add material\nonto the word, leaving the pronunciation intact. (The stress in Darwinism is the same as it was in\nDarwin.) Finally, Level 3 rules apply to a word after any Level 2 rules have been applied. The regular\nrules of inflectional morphology are examples of Level 3 rules. An example is the rule that adds an -s to\nthe end of a noun to form its plural -- for example, Darwinians or Darwinisms.\nCrucially, the rules cannot apply out of order. The input to a Level 1 rules must be a word root. The\ninput to a level 2 rule must be either a root or the output of Level 1 rules. The input to a Level 3 rule\nmust be a root, the output of Level 1 rules, or the output of Level 2 rules. That constraint yields\npredictions about what kinds of words are possible and which are impossible. For example, the ordering\nmakes it impossible to derive Darwinianism and Darwinianisms, but not Darwinsian, Darwinsism, and\nDarwinismian.\nNow, irregular inflection, such as the pairing of mouse with mice, belongs to Level 1, whereas regular\ninflectional rules, such as the one that relates rat to rats, belongs to Level 3. Compounding, the rule that\nwould produce Darwin-lover and mousetrap, is a Level 2 rule, in between. This correctly predicts that an\nirregular plural can easily appear inside a compound, but a regular plural cannot. Compare the\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 18of 39\nfollowing:\nice-infested (OK); rats-infested (bad)\nmen-bashing (OK); guys-bashing (bad)\nteethmarks (OK); clawsmarks (bad)\nfeet-warmer (OK); hand-warmer (bad)\npurple people-eater (OK); purple babies-eater (bad)\nMice-infested is a possible word, because the process connecting mouse with mice comes before the rule\ncombining the noun with infested. However, rats-infested, even though it is cognitively quite similar to\nmice-infested, sounds strange; we can say only rat-infested (even though by definition one rat does not\nmake an infestation).\nPeter Gordon (1986) had children between the ages of 3 and 5 participate in an elicited-production\nexperiment in which he would say, \"Here is a puppet who likes to eat _____. What would you call\nhim?\" He provided a response for several singular mass nouns, like mud, beforehand, so that the\nchildren were aware of the existence of the \"x-eater\" compound form. Children behaved just like adults:\na puppet who likes to eat a mouse was called a mouse-eater, a puppet who likes to eat a rat was called a\nrat-eater, a puppet who likes to eat mice was called either a mouse-eater or a mice-eater -- but -- a\npuppet who likes to eat rats was called a rat-eater, never a rats-eater. Interestingly, children treated their\nown overregularizations, such as mouses, exactly as they treated legitimate regular plurals: they would\nnever call the puppet a mouses-eater, even if they used mouses in their own speech.\nEven more interestingly, Gordon examined how children could have acquired the constraint. Perhaps, he\nreasoned, they had learned the fact that compounds can contain either singulars or irregular plurals,\nnever regular plurals, by paying keeping track of all the kinds of compounds that do and don't occur in\ntheir parents' speech. It turns out that they would have no way of learning that fact. Although there is no\ngrammatical reason why compounds would not contain irregular plurals, the speech that most children\nhear does not contain any. Compounds like toothbrush abound; compounds containing irregular plurals\nlike teethmarks, people-eater, and men-bashing, though grammatically possible, are statistically rare,\naccording to the standardized frequency data that Gordon examined, and he found none that was likely\nto appear in the speech children hear. Therefore children were willing to say mice-eater and unwilling to\nsay rats-eater with no good evidence from the input that that is the pattern required in English. Gordon\nsuggests that this shows that the constraints on level-ordering may be innate.\nLet's now go from words to sentences. Sentence are ordered strings of words. No child could fail to\nnotice word order in learning and understanding language. But most regularities of language govern\nhierarchically-organized structures -- words grouped into phrases, phrases grouped into clauses, clauses\ngrouped into sentences (see the Chapters by Lasnik, by Larson, and by Newport & Gleitman). If the\nstructures of linguistic theory correspond to the hypotheses that children formulate when they analyze\nparental speech and form rules, children should create rules defined over hierarchical structures, not\nsimple properties of linear order such as which word comes before which other word or how close two\nwords are in a sentence. The chapter by Gleitman and Newport discusses one nice demonstration of how\nadults (who are, after all, just grown-up children) respect constituent structure, not simple word order,\nwhen forming questions. Here is an example making a similar point that has been tried out with\nchildren.\nLanguages often have embedded clauses missing a subject, such as John told Mary to leave, where the\nembedded \"downstairs\" clause to leave has no subject. The phenomenon of control governs how the\nmissing subject is interpreted. In this sentence it is Mary who is understood as having the embedded\nsubject's role, that is, the person doing the leaving. We say that the phrase Mary \"controls\" the missing\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 19of 39\nsubject position of the lower clause. For most verbs, there is a simple principle defining control. If the\nupstairs verb has no object, then the subject of the upstairs verb controls the missing subject of the\ndownstairs verb. For example, in John tried to leave, John is interpreted as the subject of both try and\nleave. If the upstairs verb has a subject and an object, then it is the object that controls the missing\nsubject of the downstairs verb, as we saw in John told Mary to leave.\nIn 1969, Carol Chomsky published a set of classic experiments in developmental psycholinguistics. She\nshowed that children apply this principle quite extensively, even for the handful of verbs that are\nexceptions to it. In act-out comprehension experiments on children between the ages of 5 and 10, she\nshowed that even relatively old children were prone to this kind of mistake. When told \"Mickey\npromised Donald to jump; Make him jump,\" the children made Donald, the object of the first verb, do\nthe jumping, in accord with the general principle. The \"right answer\" in this case would have been\nMickey, because promise is an exception to the principle, calling for an unusual kind of control where\nthe subject of the upstairs verb, not the object of the upstairs verb, should act as controller.\nBut what, exactly, is the principle that children are over-applying? One possibility can be called the\nMinimal Distance Principle: the controller of the downstairs verb is the noun phrase nearest to it in the\nlinear string of words in the sentence. If children analyze sentences in terms of linear order, this should\nbe a natural generalization. However, it isn't right for the adult language. Consider the passive sentence\nMary was told by John to leave. The phrase John is closest to the subject position for leave, but adult\nEnglish speakers understand the sentence as meaning that Mary is the one leaving. The Minimal\nDistance Principle gives the wrong answer here. Instead, for the adult language, we need a principle\nsensitive to grammatical structure, such as the \"c-control\" structural relation discussed in the Chapter by\nLasnik [?]. Let's consider a simplified version, which we can call the Structural Principle. It might say\nthat the controller of a missing subject is the grammatical object of the upstairs verb if it has one;\notherwise it is the grammatical subject of the upstairs verb (both of them c-command the missing\nsubject). The object of a preposition in the higher clause, however, is never allowed to be a controller,\nbasically because it is embedded \"too deeply\" in the sentence's tree structure to c-command the missing\nsubject. That's why Mary was told by John to leave has Mary as the controller. (It is also why,\nincidentally, the sentence Mary was promised by John to leave is unintelligible -- it would require a\nprepositional phrase to be the controller, which is ruled out by the Structural Principle.)\nIt would certainly be understandable if children were to follow the Minimal Distance Principle. Not only\nis it easily stated in terms of surface properties that children can easily perceive, but sentences that\nwould disconfirm it like Mary was told by John to leave are extremely rare in parents' speech. Michael\nMaratsos (1974) did the crucial experiment. He gave children such sentences and asked them who was\nleaving. Of course, on either account children would have to be able to understand the passive\nconstruction to interpret these sentences, and Maratsos gave them a separate test of comprehension of\nsimple passive sentences to select out only those children who could do so. And indeed, he found that\nthose children interpreted passive sentences with missing embedded subjects just as adults would. That\nis, in accord with the Structural Principle and in violation of the Minimal Distance Principle, they\ninterpreted Mary was told by John to leave as having the subject, Mary, do the leaving; that is, as the\ncontroller. The experiment shows how young children have grasped the abstract structural relations in\nsentences, and have acquired a grammar of the same design as that spoken by their parents.\n8 The Child's Language-Learning Algorithm\nHere is the most basic problem in understanding how children learn a language: The input to language\nacquisition consists of sounds and situations; the output is a grammar specifying, for that language, the\norder and arrangement of abstract entities like nouns, verbs, subjects, phrase structures, control, and c-\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 20of 39\ncommand (see the Chapters by Lasnik and Larson, and the demonstrations in this chapter and the one by\nGleitman and Newport). Somehow the child must discover these entities to learn the language. We know\nthat even preschool children have an extensive unconscious grasp of grammatical structure, to the\nexperiments on discussed in the previous section, but how has the child managed to go from sounds and\nsituations to syntactic structure?\nInnate knowledge of grammar itself is not sufficient. It does no good for the child to have written down\nin his brain \"There exist nouns\"; children need some way of finding them in parents' speech, so that they\ncan determine, among other things, whether the nouns come before the verb, as in English, or after, as in\nIrish. Once the child finds nouns and verbs, any innate knowledge would immediately be helpful,\nbecause the child could then deduce all kinds of implications about how they can be used. But finding\nthem is the crucial first step, and it is not an easy one.\nIn English, nouns can be identified as those things that come after articles, get suffixed with -s in the\nplural, and so on. But the infant obviously doesn't know that yet. Nouns don't occur in any constant\nposition in a sentence across the languages of the world, and they aren't said with any particular tone of\nvoice. Nor do nouns have a constant meaning -- they often refer to physical things, like dogs, but don't\nhave to, as in The days of our lives and The warmth of the sun. The same is true for other linguistic\nentities, such as verbs, subjects, objects, auxiliaries, and tense. Since the child must somehow \"lift\nhimself up by his bootstraps\" to get started in formulating a grammar for the language, this is called the\n\"bootstrapping problem\" (see Pinker, 1982, 1984, 1987b, 1989, 1994; Morgan, 1986; Gleitman, 1990;\nand the contributors to Morgan and Demuth, 1995). Several solutions can be envisioned.\n8.1 Extracting Simple Correlations\nOne possibility is that the child sets up a massive correlation matrix, and tallies which words appear in\nwhich positions, which words appear next to which other words, which words get which prefixes and\nsuffixes in which circumstances, and so on. Syntactic categories would arise implicitly as the child\ndiscovered that certain sets of properties are mutually intercorrelated in large sets of words. For\nexample, many words tend to occur between a subject and an object, are inflected with -s when the\nsubject is singular and in the third person and the tense is present, and often appear after the word to.\nThis set of words would be grouped together as the equivalent of the \"verb\" category (Maratsos &\nChalkley, 1981).\nThere are two problems with this proposal. The main one is that the features that the prelinguistic child\nis supposed to be cross-referencing are not audibly marked in parental speech. Rather, they are\nperceptible only to child who has already analyzed the grammar of the language -- just what the\nproposal is trying to explain in the first place! How is a prelinguistic child supposed to find the \"subject\"\nof the sentence in order to correlate it with the ending on the words he or she is focusing on? A subject\nis not the same thing as the first word or two of the sentence (e.g., The big bad wolf huffed and puffed)\nor even the first phrase (e.g., What did the big bad wolf do?). We have a dilemma. If the features\ndefining the rows and columns of the correlation matrix are things that are perceptible to the child, like\n\"first word in a sentence,\" then grammatical categories will never emerge, because they have no\nconsistent correlation with these features. But if the features are the things that do define grammatical\ncategories, like agreement and phrase structure position, the proposal assumes just what it sets out to\nexplain, namely that the child has analyzed the input into its correct grammatical structures. Somehow,\nthe child must break into this circle. It is a general danger that pops up in cognitive psychology\nwhenever anyone proposes a model that depends on correlations among features: there is always a\ntemptation to glibly endow the features with the complex, abstract representations whose acquisition one\nis trying to explain.\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 21of 39\nThe second problem is that, without prior constraints on the design of the feature-correlator, there are an\nastronomical number of possible intercorrelations among linguistic properties for the child to test. To\ntake just two, the child would have to determine whether a sentence containing the word cat in third\nposition must have a plural word at the end, and whether sentences ending in words ending in d are\ninvariably preceded by words referring to plural entities. Most of these correlations never occur in any\nnatural language. It would be mystery, then, why children are built with complex machinery designed to\ntest for them -- though another way of putting it is that it would be a mystery why there are no languages\nexhibiting certain kinds of correlations given that children are capable of finding them.\n8.2 Using Prosody\nA second way in which the child could begin syntax learning would be to attend to the prosody of\nsentences, and to posit phrase boundaries at points in the acoustic stream marked by lengthening,\npausing, and drops in fundamental frequency. The proposal seems attractive, because prosodic\nproperties are perceptible in advance of knowing any syntax, so at first glance prosody seems like a\nstraightforward way for a child to break into the language system.\nBut on closer examination, the proposal does not seem to work (Pinker, 1987, 1994b; Fernald and\nMcRoberts, in press; Steedman, in press). Just as gold glitters, but all that glitters is not gold, syntactic\nstructure affects aspects of prosody, but aspects of prosody are affected by many things besides syntax.\nThe effects of emotional state of the speaker, intent of the speaker, word frequency, contrastive stress,\nand syllabic structure of individual words, are all mixed together, and there is no way for a child to\ndisentangle them from the sound wave alone. For example, in the sentence The baby ate the slug, the\nmain pause coincides with the major syntactic boundary between the subject and the predicate. But a\nchild cannot work backwards and assume that the main pause in an input sentence marks the boundary\nbetween the subject and the predicate. In the similar sentence He ate the slug, the main pause is at the\nmore embedded boundary between the verb and its object.\nWorse, the mapping between syntax and prosody, even when it is consistent, is consistent in different\nways in different languages. So a young child cannot use any such consistency, at least not at the very\nbeginning of language acquisition, to decipher the syntax of the sentence, because it itself is one of the\nthings that has to be learned.\n8.3 Using Context and Semantics\nA third possibility (see Pinker, 1982, 1984, 1989; Macnamara, 1982; Grimshaw 1981; Wexler &\nCulicover, 1980; Bloom, in press) exploits the fact that there is a one-way contingency between syntax\nand semantics in the basic sentences of most of the world's languages. Though not all nouns are physical\nobjects, all physical objects are named by nouns. Similarly, if a verb has an argument playing the\nsemantic role of 'agent', then that argument will be expressed as the subject of basic sentences in\nlanguage after language. (Again, this does not work in reverse: the subject is not necessarily an agent. In\nJohn liked Mary the subject is an \"experiencer\"; in John pleased Mary it is an object of experience; in\nJohn received a package it is a goal or recipient; in John underwent an operation it is a patient.)\nSimilarly, entities directly affected by an action are expressed as objects (but not all objects are entities\naffected by an action); actions themselves are expressed as verbs (though not all verbs express actions).\nEven phrase structure configurations have semantic correlates: arguments of verbs reliably appear as\n\"sisters\" to them inside the verb phrase in phrase structure trees (see the chapter by Lasnik).\nIf children assume that semantic and syntactic categories are related in restricted ways in the early input,\nthey could use semantic properties of words and phrases (inferred from context; see Section ) as\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 22of 39\nevidence that they belong to certain syntactic categories. For example, a child can infer that a word that\ndesignated a person, place or thing is a noun, that a word designating an action is a verb, that a word\nexpressing the agent argument of an action predicate is the subject of its sentence, and so on. For\nexample, upon hearing the sentence The cat chased the rat, the child can deduce that in English the\nsubject comes before the verb, that the object comes after the verb, and so on. This would give the child\nthe basis for creating the phrase structure trees that allow him or her to analyze the rules of the language.\nOf course, a child cannot literally create a grammar that contains rules like \"Agent words come before\naction words.\" This would leave the child no way of knowing how to order the words in sentences such\nas Apples appeal to Mary or John received a package. But once an initial set of rules is learned, items\nthat are more abstract or that don't follow the usual patterns relating syntax and semantic could be\nlearned through their distribution in already-learned structures. That is, the child could now infer that\nApples is the subject of appeal, and that John is the subject of receive, because they are in subject\nposition, a fact the child now knows thanks to the earlier cat-chased-rat sentences. Similarly, the child\ncould infer that appeal is a verb to begin with because it is in the \"verb\" position.\n9 Acquisition in Action\nWhat do all these arguments mean for what goes on in a child's mind moment by moment as he or she is\nacquiring rules from parental speech? Let's look at the process as concretely as possible.\n9.1 Bootstrapping the First Rules\nFirst imagine a hypothetical child trying to extract patterns from the following sentences, without any\ninnate guidance as to how human grammar works.\nMyron eats lamb.\nMyron eats fish.\nMyron likes fish.\nAt first glance, one might think that the child could analyze the input as follows. Sentences consist of\nthree words: the first must be Myron, the second either eats or likes, the third lamb or fish. With these\nmicro-rules, the child can already generalize beyond the input, to the brand new sentence Myron likes\nchicken.\nBut let's say the next two sentences are\nMyron eats loudly.\nMyron might fish.\nThe word might gets added to the list of words that can appear in second position, and the word loudly is\nadded to the list that can appear in third position. But look at the generalizations this would allow:\nMyron might loudly.\nMyron likes loudly.\nMyron might lamb.\nThis is not working. The child must couch rules in grammatical categories like noun, verb, and\nauxiliary, not in actual words. That way, fish as a noun and fish as a verb can be kept separate, and the\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 23of 39\nchild would not adulterate the noun rule with instances of verbs and vice-versa. If children are willing to\nguess that words for objects are nouns, words for actions are verbs, and so on, they would have a leg up\non the rule-learning problem.\nBut words are not enough; they must be ordered. Imagine the child trying to figure out what kind of\nword can occur before the verb bother. It can't be done:\nThat dog bothers me. [dog, a noun]\nWhat she wears bothers me. [wears, a verb]\nMusic that is too loud bothers me. [loud, an adjective]\nCheering too loudly bothers me. [loudly, an adverb]\nThe guy she hangs out with bothers me. [with, a preposition]\nThe problem is obvious. There is a certain something that must come before the verb bother, but that\nsomething is not a kind of word; it is a kind of phrase, a noun phrase. A noun phrase always contains a\nhead noun, but that noun can be followed by many other phrases. So it is useless of try to learn a\nlanguage by analyzing sentences word by word. The child must look for phrases -- and the experiments\non grammatical control discussed earlier shows that they do.\nWhat does it mean to look for phrases? A phrase is a group of words. Most of the logically possible\ngroups of words in a sentence are useless for constructing new sentences, such as wears bothers and\ncheering too, but the child, unable to rely on parental feedback, has no way of knowing this. So once\nagain, children cannot attack the language learning task like some logician free of preconceptions; they\nneed prior constraints. We have already seen where such constraints could come. First, the child could\nassume that parents' speech respects the basic design of human phrase structure: phrases contain heads\n(e.g., a noun phrase is built around a head noun); arguments are grouped with heads in small phrases,\nsometimes called X-bars (see the chapter by Lasnik); X-bars are grouped with their modifiers inside\nlarge phrases (Noun Phrase, Verb Phrase, and so on); phrases can have subjects. Second, since the\nmeanings of parents' sentences are guessable in context, the child could use the meanings to help set up\nthe right phrase structure. Imagine that a parent says The big dog ate ice cream. If the child already\nknows the words big, dog, ate, and ice cream, he or she can guess their categories and grow the first\nbranches of a tree: In turn, nouns and verbs must belong to noun phrases and verb phrases, so the child\ncan posit one for each of these words. And if there is a big dog around, the child can guess that the and\nbig modify dog, and connect them properly inside the noun phrase: If the child knows that the dog just\nate ice cream, he or she can also guess that ice cream and dog are arguments of the verb eat. Dog is a\nspecial kind of argument, because it is the causal agent of the action and the topic of the sentence, and\nhence it is likely to be the subject of the sentence, and therefore attaches to the \"S.\" A tree for the\nsentence has been completed: The rules and dictionary entries can be peeled off the tree:\nS --> NP VP\nNP --> (det) (A) N\nVP --> V NP\ndog: N\nice cream: N\nate: V; eater = subject, thing eaten = object\nthe: det\nbig: A\nThis hypothetical example shows how a child, if suitably equipped, could learn three rules and five\nwords from a single sentence in context.\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 24of 39\nThe use of part-of-speech categories, phrase structure, and meaning guessed from context are powerful\ntools that can help the child in the daunting task of learning grammar quickly and without systematic\nparental feedback (Pinker, 1984). In particular, there are many benefits to using a small number of\ncategories like N and V to organize incoming speech. By calling both the subject and object phrases\n\"NP,\" rather than, say Phrase#1 and Phrase#2, the child automatically can apply knowledge about nouns\nin subject position to nouns in object position, and vice-versa. For example, our model child can already\ngeneralize, and use dog as an object, without having heard an adult do so, and the child tacitly knows\nthat adjectives precede nouns not just in subjects but in objects, again without direct evidence. The child\nknows that if more than one dog is dogs in subject position, more than one dog is dogs in object\nposition.\nMore generally, English allows at least eight possible phrasemates of a head noun inside a noun phrase,\nsuch as John's dog; dogs in the park; big dogs; dogs that I like, and so on. In turn, there are about eight\nplaces in a sentence where the whole noun phrase can go, such as Dog bites man; Man bites dog; A\ndog's life; Give the boy a dog; Talk to the dog; and so on. There are three ways to inflect a noun: dog,\ndogs, dog's. And a typical child by the time he or she is in high school has learned something like 20,000\ndifferent nouns (Miller, 1991; Pinker, 1994a). If children had to learn all the combinations separately,\nthey would need to listen to about 140 million different sentences. At a rate of a sentence every ten\nseconds, ten hours a day, it would take over a century. But by unconsciously labeling all nouns as \"N\"\nand all noun phrases as \"NP,\" the child has only to hear about twenty-five different kinds of noun phrase\nand learn the nouns one by one, and the millions of possible combinations fall out automatically.\nIndeed, if children are constrained to look for only a small number of phrase types, they automatically\ngain the ability to produce an infinite number of sentences, one of the hallmarks of human language.\nTake the phrase the tree in the park. If the child mentally labels the park as an NP, and also labels the\ntree in the park as an NP, the resulting rules generate an NP inside a PP inside an NP -- a loop that can\nbe iterated indefinitely, as in the tree near the ledge by the lake in the park in the city in the east of the\nstate .... In contrast, a child who was free to to label in the park as one kind of phrase, and the tree in the\npark another, would be deprived of the insight that the phrase contains an example of itself. The child\nwould be limited to reproducing that phrase structure alone.\nWith a rudimentary but roughly accurate analysis of sentence structure set up, the other parts of\nlanguage can be acquired systematically. Abstract words, such as nouns that do not refer to objects and\npeople, -- can be learned by paying attention to where they sit inside a sentence. Since situation in The\nsituation justifies drastic measures occurs inside a phrase in NP position, it must be a noun. If the\nlanguage allows phrases to be scrambled around the sentence, like Latin or the Australian aboriginal\nlanguage Warlpiri, the child can discover this feature upon coming across a word that cannot be\nconnected to a tree in the expected place without crossing branches (in Section , we will see that\nchildren do seem to proceed in this order). The child's mind can also know what to focus on in decoding\ncase and agreement inflections: a noun's inflection can be checked to see if it appears whenever the noun\nappears in subject position, in object position, and so on; a verb's inflection might can be checked for\ntense, aspect, and the number, person, and gender of its subject and object. The child need not bother\nchecking whether the third word in the sentence referred to a reddish or a bluish object, whether the last\nword was long or short, whether the sentence was being uttered indoors or outdoors, and billions of\nother fruitless possibilities that a purely correlational learner would have to check.\n9.2 The Organization of Grammar as a Guide to Acquisition\nA grammar is not a bag of rules; there are principles that link the various parts together into a\nfunctioning whole. The child can use such principles of Universal Grammar to allow one bit of\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 25of 39\nknowledge about language to affect another. This helps solve the problem of how the child can avoid\ngeneralizing to too large a language, which in the absence of negative evidence would be incorrigible. In\ncases were children do overgeneralize, these principles can help the child recover: if there is a principle\nthat says that A and B cannot coexist in a language, a child acquiring B can use it to catapult A out of\nthe grammar.\n9.2.1 Blocking and Inflectional Overregularization\nThe next chapter presents a good example. The Blocking principle in morphology dictates that an\nirregular form listed in the mental dictionary as corresponding to a particular inflectional category (say,\npast tense), blocks the application of the corresponding general rule. For example, adults know the\nirregular form broke, and that prevents them from applying the regular \"add -ed\" rule to break and\nsaying breaked. Children, who have not heard broke enough times to remember it reliably on demand,\nthus fail to block the rule and occasionally say breaked. As they hear broke enough times to recall it\nreliably, Blocking would suppress the regular rule, and they would gradually recover from these\novergeneralization errors (Marcus, et al., 1992).\n9.2.2 Interactions between Word Meaning and Syntax\nHere is another example in which a general principle rules out a form in the adult grammar, but in the\nchild's grammar, the crucial information allowing the principle to apply is missing. As the child's\nknowledge increases, the relevance of the principle to the errant form manifests itself, and the form can\nbe ruled out so as to make the grammar as a whole consistent with the principle.\nEvery verb has an \"argument structure\": a specification of what kinds of phrases it can appear with\n(Pinker, 1989). A familiar example is the distinction between a transitive verb like devour, which\nrequires a direct object (you can say He devoured the steak but not just He devoured) and an intransitive\nverb like dine, which does not (you can say He dined but not He dined the steak). Children sometimes\nmake errors with the argument structures of verbs that refer to the act of moving something to a\nspecified location (Bowerman, 1982b; Gropen, Pinker, Hollander, and Goldberg, 1991a):\nI didn't fill water up to drink it; I filled it up for the flowers to drink it.\nCan I fill some salt into the bear? [a bear-shaped salt shaker]\nI'm going to cover a screen over me.\nFeel your hand to that.\nTerri said if this [a rhinestone on a shirt] were a diamond then people would be trying to rob\nthe shirt.\nA general principle of argument structure is that the argument that is affected in some way specified by\nthe verb gets mapped onto the syntactic object. This is an example of a \"linking rule,\" which links\nsemantics with syntax (and which is an example of the contingency a young child would have employed\nto use semantic information to bootstrap into the syntax). For example, for adults, the \"container\"\nargument (where the water goes) is the direct object of fill -- fill the glass with water, not fill water into\nthe glass -- because the mental definition of the verb fill says that the glass becomes full, but says\nnothing about how that happens (one can fill a glass by pouring water into it, by dripping water into it,\nby dipping it into a pond, and so on). In contrast, for a verb like pour, it is the \"content\" argument (the\nwater) that is the object -- pour water into the glass, not pour the glass with water -- because the mental\ndefinition of the verb pour says that the water must move in a certain manner (downward, in a stream)\nbut does not specify what happens to the container (the water might fill the glass, merely wet it, end up\nbeside it, and so on). In both cases, the entity specified as \"affected\" ends up as the object, but for fill, it\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 26of 39\nis the object whose state is affected (going from not full to full), whereas for pour, it is the object whose\nlocation is affected (going from one place to a lower one).\nNow, let's say children mistakenly think that fill refers to a manner of motion (presumably, some kind of\ntipping or pouring), instead of an end state of fullness. (Children commonly use end state verbs as\nmanner verbs: for example, they think that mix just means stir, regardless of whether the stirred\ningredients end up mixed together; Gentner, 1978). If so, the linking rule for direct objects would cause\nthem to make the error we observe: fill x into y. How could they recover? When children observe the\nverb fill in enough contexts to realize that it actually encodes the end state of fullness, not a manner of\npouring or any other particular manner (for example eventually they may hear someone talking about\nfilling a glass by leaving it on a window sill during a storm), they can change their mental dictionary\nentry for fill. As a result, they would withdraw it from eligibility to take the argument structure with the\ncontents as direct object, on the grounds that it violates the constraint that \"direct object = specifically\naffected entity.\" The principle could have existed all along, but only been deemed relevant to the verb\nfill when more information about its definition had been accumulated (Gropen, et al., 1991a, b; Pinker,\n1989).\nThere is evidence that the process works in just that way. Gropen et al. (1991a) asked preschool children\nto select which picture corresponded to the sentence She filled the glass with water. Most children\nindiscriminately chose any picture showing water pouring; they did not care whether the glass ended up\nfull. This shows that they do misconstrue the meaning of fill. In a separate task, the children were asked\nto describe in their own words what was happening in a picture showing a glass being filled. Many of\nthese children used incorrect sentences like He's filling water into the glass. Older children tended to\nmake fewer errors of both verb meaning and verb syntax, and children who got the verb meaning right\nwere less likely to make syntax errors and vice-versa. In an even more direct demonstration, Gropen, et\nal. (1991b) taught children new verbs like to pilk, referring to actions like moving a sponge over to a\ncloth. For some children, the motion had a distinctive zigzag manner, but the cloth remained unchanged.\nFor others, the motion was nondescript, but the cloth changed color in a litmus-like reaction when the\nsponge ended up on it. Though none of the children heard the verb used in a sentence, when asked to\ndescribe the event, the first group said that the experimenter was pilking the sponge, whereas the second\ngroup said that he was pilking the cloth. This is just the kind of inference that would cause a child who\nfinally figured out what fill means to stop using it with the wrong direct object.\nInterestingly, the connections between verbs' syntax and semantics go both ways. Gleitman (1990)\npoints out that there are some aspects of a verb's meaning that are difficult, if not impossible, for a child\nto learn by observing only the situations in which the verb is used. For example, verb pairs like push and\nmove, give and receive, win and beat, buy and sell, chase and flee, and drop and fall often can be used to\ndescribe the same event; only the perspective assumed by the verb differs. Also, mental verbs like see,\nknow, and want, are difficult to infer by merely observing their contexts. Gleitman suggests that the\ncrucial missing information comes from the syntax of the sentence. For example, fall is intransitive (it\nfell, not John fell the ball); drop can be transitive (He dropped the ball). This reflects the fact that the\nmeaning of fall involves the mere act of plummeting, independent of who if anyone caused it, whereas\nthe extra argument of drop refers to an agent who is causing the descent. A child could figure out the\nmeaning difference between the two by paying attention to the transitive and intransitive syntax -- an\nexample of using syntax to learn semantics, rather than vice-versa. (Of course, it can only work if the\nchild has acquired some syntax to begin with.) Similarly, a verb that appears with a clause as its\ncomplement (as in I think that ...) must refer to a state involving a proposition, and not, say, of motion\n(there is no verb like He jumped that he was in the room). Therefore a child hearing a verb appearing\nwith a clausal complement can infer that it might be a mental verb.\nNaigles (1990) conducted an experiment that suggest that children indeed can learn some of a verb's\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 27of 39\nmeaning from the syntax of a sentence it is used in. Twenty-four-month-olds first saw a video of a rabbit\npushing a duck up and down, while both made large circles with one arm. One group of children heard a\nvoice saying \"The rabbit is gorping the duck\"; another heard \"The rabbit and the duck are gorping.\"\nThen both groups saw a pair of screens, one showing the rabbit pushing the duck up and down, neither\nmaking arm circles, the other showing the two characters making arm circles, neither pushing down the\nother. In response to the command \"Where's gorping now? Find gorping!\", the children who heard the\ntransitive sentence looked at the screen showing the up-and-down action, and the children who heard the\nintransitive sentence looked at the screen showing the making-circles action. For a general discussion of\nhow children could use verb syntax to learn verb semantics, and vice-versa, see Pinker (1994b).\n9.3 Parameter-Setting and the Subset Principle\nA striking discovery of modern generative grammar is that natural languages seem to be built on the\nsame basic plan. Many differences among languages represent not separate designs but different settings\nof a few \"parameters\" that allow languages to vary, or different choices of rule types from a fairly small\ninventory of possibilities. The notion of a \"parameter\" is borrowed from mathematics. For example, all\nof the equations of the form \"y = 3x + b,\" when graphed, correspond to a family of parallel lines with a\nslope of 3; the parameter b takes on a different value for each line, and corresponds to how high or low it\nis on the graph. Similarly, languages may have parameters (see the chapter by Lasnik).\nFor example, all languages in some sense have subjects, but there is a parameter corresponding to\nwhether a language allows the speaker to omit the subject in a tensed sentence with an inflected verb.\nThis \"null subject\" parameter (sometimes called \"PRO-drop\") is set to \"off\" in English and \"on\" in\nSpanish and Italian (Chomsky, 1981). In English, one can't say Goes to the store, but in Spanish, one can\nsay the equivalent. The reason this difference is a \"parameter\" rather than an isolated fact is that it\npredicts a variety of more subtle linguistic facts. For example, in null subject languages, one can also\nuse sentences like Who do you think that left? and Ate John the apple, which are ungrammatical in\nEnglish. This is because the rules of a grammar interact tightly; if one thing changes, it will have series\nof cascading effects throughout the grammar. (For example, Who do you think that left? is\nungrammatical in English because the surface subject of left is an inaudible \"trace\" left behind when the\nunderlying subject, who, was moved to the front of the sentence. For reasons we need not cover here, a\ntrace cannot appear after a word like that, so its presence taints the sentence. Recall that in Spanish, one\ncan delete subjects. Therefore, one can delete the trace subject of left, just like any other subject (yes,\none can \"delete\" a mental symbol even it would have made no sound to begin with). The is trace no\nlonger there, so the principle that disallows a trace in that position is no longer violated, and the sentence\nsounds fine in Spanish.\nOn this view, the child would set parameters on the basis of a few examples from the parental input, and\nthe full complexity of a language will ensue when those parameterized rules interact with one another\nand with universal principles. The parameter-setting view can help explain the universality and rapidity\nof the acquisition of language, despite the arcane complexity of what is and is not grammatical (e.g., the\nungrammaticality of Who do you think that left?). When children learn one fact about a language, they\ncan deduce that other facts are also true of it without having to learn them one by one.\nThis raises the question of how the child sets the parameters. One suggestion is that parameter settings\nare ordered, with children assuming a particular setting as the default case, moving to other settings as\nthe input evidence forces them to (Chomsky, 1981). But how would the parameter settings be ordered?\nOne very general rationale comes from the fact that children have no systematic access to negative\nevidence. Thus for every case in which parameter setting A generates a subset of the sentences\ngenerated by setting B (as in diagrams (c) and (d) of Figure 1), the child must first hypothesize A, then\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 28of 39\nabandon it for B only if a sentence generated by B but not by A was encountered in the input (Pinker,\n1984; Berwick, 1985; Osherson, et al, 1985). The child would then have no need for negative evidence;\nhe or she would never guess too large a language. (For settings that generate languages that intersect or\nare disjoint, as in diagrams (a) and (b) of Figure 1, either setting can be discarded if incorrect, because\nthe child will eventually encounter a sentence that one grammar generates but the other does not).\nMuch interesting research in language acquisition hinges on whether children's first guess from among a\nset of nested possible languages really is the smallest subset. For example, some languages, like English,\nmandate strict word orders; others, such as Russian or Japanese, list a small set of admissible orders; still\nothers, such as the Australian aborigine language Warlpiri, allow almost total scrambling of word order\nwithin a clause. Word order freedom thus seems to be a parameter of variation, and the setting\ngenerating the smallest language would obviously be the one dictating fixed word order. If children\nfollow the Subset Principle, they should assume, by default, that languages have a fixed constituent\norder. They would back off from that prediction if and only if they hear alternative word orders, which\nindicate that the language does permit constituent order freedom. The alternative is that the child could\nassume that the default case was constituent order freedom.\nIf fixed-order is indeed the default, children should make few word order errors for a fixed-order\nlanguage like English, and might be conservative in learning freer-word order languages, sticking with a\nsubset of the sanctioned orders (whether they in fact are conservative would depend on how much\nevidence of multiple orders they need before leaping to the conclusion that multiple orders are\npermissible, and on how frequent in parental speech the various orders are). If, on the other hand, free-\norder is the default, children acquiring fixed-word-order languages might go through a stage of\novergenerating (saying, give doggie paper; give paper doggie, paper doggie give; doggie paper give, and\nso on), while children acquiring free word-order languages would immediately be able to use all the\norders. In fact, as I have mentioned, children learning English never leap to the conclusion that it is a\nfree-word order language and speak in all orders (Brown, 1973; Braine, 1976; Pinker, 1984; Bloom,\nLightbown, & Hood, 1975). Logically speaking, though, that would be consistent with what they hear if\nthey were willing to entertain the possibility that their parents were just conservative speakers of\nKorean, Russian or Swedish, where several orders are possible. But children learning Korean, Russian,\nand Swedish do sometimes (though not always) err on the side of caution, and use only one of the orders\nallowed in the language, pending further evidence (Brown, 1973). It looks like fixed-order is the default,\njust as the Subset Principle would predict.\nWexler & Manzini (1987) present a particularly nice example concerning the difference between\n\"anaphors\" like herself and \"pronouns\" like her. An anaphor has to be have its antecedent lie a small\ndistance away (measured in terms of phrase size, of course, not number of words); the antecedent is said\nto be inside the anaphor's \"governing category.\" That is why the sentence John liked himself is fine, but\nJohn thought that Mary liked himself is ungrammatical: himself needs an antecedent (like John) within\nthe same clause as itself, which it has in the first example but not the second. Different languages permit\ndifferent-size governing categories for the equivalents of anaphors like himself; in some languages, the\ntranslations of both sentences are grammatical. The Subset Principle predicts that children should start\noff assuming that their language requires the tiniest possible governing category for anaphors, and then\nto expand the possibilities outward as they hear the telltale sentences. Interestingly, for pronouns like\n\"her,\" the ordering is predicted to be the opposite. Pronouns may not have an antecedent within their\ngoverning categories: John liked him (meaning John liked himself] is ungrammatical, because the\nantecedent of him is too close, but John thought that Mary liked him is fine. Sets of languages with\nbigger and bigger governing categories for pronouns allow fewer and fewer grammatical possibilities,\nbecause they define larger ranges in which a pronoun prohibits its antecedent from appearing -- an effect\nof category size on language size that is in the opposite direction to what happens for anaphors. Wexler\nand Manzini thus predict that for pronouns, children should start off assuming that their language\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 29of 39\nrequires the largest possible governing category, and then to shrink the possibilities inward as they hear\nthe telltale sentences. They review experiments and spontaneous speech studies that provide some\nsupport for this subtle pattern of predictions.\n10 Conclusion\nThe topic of language acquisition implicate the most profound questions about our understanding of the\nhuman mind, and its subject matter, the speech of children, is endlessly fascinating. But the attempt to\nunderstand it scientifically is guaranteed to bring on a certain degree of frustration. Languages are\ncomplex combinations of elegant principles and historical accidents. We cannot design new ones with\nindependent properties; we are stuck with the confounded ones entrenched in communities. Children,\ntoo, were not designed for the benefit of psychologists: their cognitive, social, perceptual, and motor\nskills are all developing at the same time as their linguistic systems are maturing and their knowledge of\na particular language is increasing, and none of their behavior reflects one of these components acting in\nisolation.\nGiven these problems, it may be surprising that we have learned anything about language acquisition at\nall, but we have. When we have, I believe, it is only because a diverse set of conceptual and\nmethodological tools has been used to trap the elusive answers to our questions: neurobiology, ethology,\nlinguistic theory, naturalistic and experimental child psychology, cognitive psychology, philosophy of\ninduction, theoretical and applied computer science. Language acquisition, then, is one of the best\nexamples of the indispensability of the multidisciplinary approach called cognitive science.\n11 Further Reading\nA general introduction to language can be found in my book The Language Instinct (Pinker, 1994), from\nwhich several portions of this chapter were adapted. There is a chapter on language acquisition, and\nchapters on syntactic structure, word structure, universals and change, prescriptive grammar, neurology\nand genetics, and other topics.\nThe logical problem of language acquisition is discussed in detail by Wexler and Culicover (1980),\nPinker (1979, 1984, 1987, 1989), Osherson, Stob, & Weinstein (1985), Berwick (1985), and Morgan\n(1986). Pinker (1979) is a nontechnical introduction. The study of learnability within theoretical\ncomputer science has recently taken on interesting new turns, reviewed in Kearns & Vazirani (1994),\nthough with little discussion of the special case we are interested in, language acquisition. Brent (1995)\ncontains state-of-the-art work on computer models of language acquisition.\nThe most comprehensive recent textbook on language development is Ingram (1989). Among other\nrecent textbooks, Gleason (1993) has a focus on children's and mothers' behavior, whereas Atkinson\n(1992), Goodluck (1991), and Crain and Lillo-Martin (in press), have more of a focus on linguistic\ntheory. Bloom (1993) is an excellent collection of reprinted articles, organized around the acquisition of\nwords and grammar. Hoekstra and Schwartz (1994) is a collection of recent papers more closely tied to\ntheories of generative grammar. Fletcher & MacWhinney's The Handbook of Child Language (1995),\nhas many useful survey chapters; see also the surveys by Paul Bloom in Gernsbacher's Handbook of\nPsycholinguistics (1994) and by Michael Maratsos in Mussen's Carmichael's Manual of Child\nPsychology (4th edition 1983; 5th edition in preparation at the time of this writing).\nEarlier collections of important articles include Krasnegor, et al., (1991), MacWhinney (1987), Roeper\n& Williams (1987), Wanner & Gleitman (1982), Baker & McCarthy (1981), Fletcher and Garman\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 30of 39\n(1979), Ferguson & Slobin (1973), Hayes (1970), Brown & Bellugi (1964), and Lenneberg (1964).\nSlobin (1985a\/1993) is a large collection of major reviews on the acquisition of particular languages.\nThe most ambitious attempts to synthesize large amounts of data on language development into a\ncohesive framework are Brown (1973), Pinker (1984), and Slobin (1985b). Clark (1993) reviews the\nacquisition of words. Locke (1993) covers the earliest stages of acquisition, with a focus on speech input\nand output. Morgan & Demuth (in press) contains papers on children's perception of input speech and its\ninteraction with their language development.\n12 Problems\n1. \"Negative evidence\" is reliable information available to a language learner about which strings of\nwords are ungrammatical in the language to be acquired. Which of the following would, and\nwould not, count as negative evidence. Justify your answers.\na. Mother expresses disapproval every time Junior speaks ungrammatically.\nb. Father often rewards Junior when he speaks grammatically, and often punishes him when he\nspeaks ungrammatically.\nc. Mother wrinkles her nose every time Junior speaks ungrammatically, and never wrinkles her\nnose any other time.\nd. Father repeats all of Junior's grammatical sentences verbatim, and converts all of his\nungrammatical sentences into grammatical ones.\ne. Mother blathers incessantly, uttering all the grammatical sentences of English in order of length\n-- all the two word sentences, then all the three-word sentences, and so on.\nf. Father corrects Junior whenever he produces an overregularization like breaked, but never\ncorrects him when he produces a correct past tense form like broke.\ng. Whenever Junior speaks ungrammatically, Mother responds by correcting the sentence to the\ngrammatical version. When he speaks grammatically, Mother responds with a follow-up that\nmerely recasts the sentence in different words.\nh. Whenever Junior speaks ungrammatically, Father changes the subject.\ni. Mother never repeats Junior's ungrammatical sentences verbatim, but sometimes repeats his\ngrammatical sentences verbatim.\nj. Father blathers incessantly, producing all possible strings of English words, furrowing his brows\nafter every ungrammatical string and pursing his lips after every grammatical sentence.\n2. Consider three languages. Language A is is English, in which sentence must contain a\ngrammatical subject: He ate the apple is good; Ate the apple is ungrammatical. In Language B, the\nsubject is optional, but the verb always has a suffix which agrees with the subject (whether it is\npresent or absent) in person, number, and gender. Thus He ate-3MS the apple is good (assume that\n\"3MS\" is a suffix, like -o or -ik, that is used only when the subject is 3rd person masculine\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 Language Acquisition Page 31of 39\nsingular), as is Ate-3MS the apple. (Those of you who speak Spanish or Italian will see that this\nhypothetical language is similar to them.) Language C has no inflection on the verb, but allows\nthe subject to be omitted: He ate the apple and Ate the apple are both good. Assuming a child has\nno access to negative evidence, but knows that the language to be learned is one of these three.\nDoes the child have to entertain these hypotheses in any fixed order? If so, what is it? What\nlearning strategy would guarantee that the child would arrive at the correct language? Show why.\n3. Imagine a verb pilk that means \"to have both of one's elbows grabbed by someone else,\" so John\npilked Bill meant that Bill grabbed John's elbows.\na. Why is this verb unlikely to occur in English?\nb. If children use semantic context and semantic-syntax linking rules to bootstrap their way into a\nlanguage, what would a languageless child infer about English upon hearing \"This is pilking\" and\nseeing Bill grab John's elbows?\nc. If children use semantic context and semantics-syntax linking rules to bootstrap their way into a\nlanguage, what would a languageless child infer about English upon hearing \"John pilked Bill\"\nand seeing Bill grab John's elbows?\nd. If children use semantic context and semantics-syntax linking rules to bootstrap their way into a\nlanguage, what would a child have to experience in order to learn English syntax and the correct\nuse of the word pilk?\n13 Answers to Problems\n1. a. No. Presumably Mother also expresses disapproval for other reasons, such as Junior uttering a\nrude or false -- but grammatical -- sentence. If Junior were to assume that disapproved-of\nsentences were ungrammatical, he would spuriously eliminate many grammatical sentences from\nhis language.\nb. No, because Father may also reward him when he speaks ungrammatically and punish him\nwhen he speaks grammatically.\nc. Yes, because Junior can deduce that any nose-wrinkle-eliciting sentence is grammatical.\nd. Yes, because Junior can deduce that any sentence that is not repeated verbatim is\nungrammatical.\ne. Yes, because for any sentence that Junior is unsure about, he can keep listening to mother until\nshe begins to utter sentences longer than that one. If, by that time, Mother has uttered his sentence,\nit is grammatical; if she hasn't, it's ungrammatical.\nf. No, because we don't know what Father does for the rest of the language.\ng. No, because while we know whether the changeover in Junior's sentence is a \"correction\" or a\n\"recasting,\" because we know what's ungrammatical (hence corrected) or grammatical (hence\nrecast), Junior has no way of knowing that from his point of view, Mother just changes everything\nhe says into different words.\nhttp:\/\/users.ecs.soton.ac.uk\/harnad\/Papers\/Py104\/pinker.langacq.html 1\/18\/2008 "}