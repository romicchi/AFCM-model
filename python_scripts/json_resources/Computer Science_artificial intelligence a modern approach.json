{"text":"1\nINTRODUCTION\nIn which we try to explain why we consider artificial intelligence to be a subject\nmostworthyofstudy,andinwhichwetrytodecidewhatexactlyitis,thisbeinga\ngoodthingtodecidebeforeembarking.\nWe call ourselves Homo sapiens\u2014man the wise\u2014because our intelligence is so important\nINTELLIGENCE\ntous. Forthousands ofyears, wehavetried tounderstand how wethink; that is, how amere\nhandful of matter can perceive, understand, predict, and manipulate a world far larger and\nARTIFICIAL more complicated than itself. The field of artificial intelligence, or AI, goes further still: it\nINTELLIGENCE\nattemptsnotjusttounderstand butalsotobuildintelligent entities.\nAIisoneofthe newestfieldsinscience and engineering. Work started inearnest soon\nafter World War II, and the name itself was coined in 1956. Along with molecular biology,\nAIisregularlycitedasthe\u201cfieldIwouldmostliketobein\u201dbyscientistsinotherdisciplines.\nAstudentinphysicsmightreasonably feelthatallthegoodideashavealreadybeentakenby\nGalileo, Newton, Einstein, and the rest. AI, on the other hand, still has openings forseveral\nfull-timeEinsteinsandEdisons.\nAIcurrentlyencompassesahugevarietyofsubfields,rangingfromthegeneral(learning\nandperception)tothespecific,suchasplayingchess,provingmathematicaltheorems,writing\npoetry, driving a car on a crowded street, and diagnosing diseases. AI is relevant to any\nintellectual task;itistrulyauniversal field.\n1.1 WHAT IS AI?\nWe have claimed that AI is exciting, but we have not said what it is. In Figure 1.1 we see\neight definitions of AI, laid out along twodimensions. The definitions on top are concerned\nwiththoughtprocessesandreasoning,whereastheonesonthebottomaddressbehavior. The\ndefinitions on the left measure success in terms of fidelity to human performance, whereas\nthe ones on the right measure against an ideal performance measure, called rationality. A\nRATIONALITY\nsystemisrationalifitdoesthe\u201crightthing,\u201dgivenwhatitknows.\nHistorically, all four approaches to AI have been followed, each by different people\nwithdifferentmethods. Ahuman-centered approachmustbeinpartanempiricalscience,in-\n1 2 Chapter 1. Introduction\nThinkingHumanly ThinkingRationally\n\u201cTheexcitingnewefforttomakecomput- \u201cThestudyofmentalfacultiesthroughthe\ners think ... machines with minds, in the useofcomputational models.\u201d\nfullandliteralsense.\u201d (Haugeland, 1985) (Charniak andMcDermott,1985)\n\u201c[The automation of] activities that we \u201cThestudyofthecomputationsthatmake\nassociate with human thinking, activities itpossibletoperceive, reason,andact.\u201d\nsuch as decision-making, problem solv- (Winston, 1992)\ning,learning ...\u201d(Bellman,1978)\nActingHumanly ActingRationally\n\u201cThe art of creating machines that per- \u201cComputational Intelligence is the study\nform functions that require intelligence ofthedesignofintelligentagents.\u201d (Poole\nwhen performed by people.\u201d (Kurzweil, etal.,1998)\n1990)\n\u201cThestudy ofhowtomakecomputers do \u201cAI ...is concerned with intelligent be-\nthingsatwhich,atthemoment,peopleare haviorinartifacts.\u201d (Nilsson,1998)\nbetter.\u201d (RichandKnight,1991)\nFigure1.1 Somedefinitionsofartificialintelligence,organizedintofourcategories.\nvolvingobservationsandhypothesesabouthumanbehavior. Arationalist1 approachinvolves\nacombination ofmathematicsandengineering. Thevariousgrouphavebothdisparaged and\nhelpedeachother. Letuslookatthefourapproaches inmoredetail.\n1.1.1 Acting humanly: The Turing Test approach\nThe Turing Test, proposed by Alan Turing (1950), was designed to provide a satisfactory\nTURINGTEST\noperationaldefinitionofintelligence. Acomputerpassesthetestifahumaninterrogator,after\nposingsomewrittenquestions, cannottellwhetherthewrittenresponsescomefromaperson\norfromacomputer. Chapter26discussesthedetailsofthetestandwhetheracomputerwould\nreally be intelligent if it passed. For now, we note that programming a computer to pass a\nrigorously applied test provides plenty towork on. Thecomputerwould need to possess the\nfollowingcapabilities:\nNATURALLANGUAGE \u2022 naturallanguageprocessing toenableittocommunicatesuccessfully inEnglish;\nPROCESSING\nKNOWLEDGE \u2022 knowledgerepresentation tostorewhatitknowsorhears;\nREPRESENTATION\nAUTOMATED \u2022 automated reasoning to use the stored information to answer questions and to draw\nREASONING\nnewconclusions;\n\u2022 machinelearningtoadapttonewcircumstancesandtodetectandextrapolatepatterns.\nMACHINELEARNING\n1 Bydistinguishingbetweenhumanandrationalbehavior, wearenotsuggestingthathumansarenecessarily\n\u201cirrational\u201dinthesenseof \u201cemotionallyunstable\u201d or\u201cinsane.\u201d Onemerelyneednotethat wearenot perfect:\nnotallchessplayersaregrandmasters;and,unfortunately,noteveryonegetsanAontheexam.Somesystematic\nerrorsinhumanreasoningarecatalogedbyKahnemanetal.(1982). Section1.1. WhatIsAI? 3\nTuring\u2019stestdeliberately avoideddirectphysicalinteraction betweentheinterrogatorandthe\ncomputer, because physical simulation ofaperson isunnecessary forintelligence. However,\nthe so-called total Turing Test includes a video signal so that the interrogator can test the\nTOTALTURINGTEST\nsubject\u2019s perceptual abilities, as well as the opportunity for the interrogator to pass physical\nobjects\u201cthrough thehatch.\u201d TopassthetotalTuringTest,thecomputerwillneed\n\u2022 computervisiontoperceiveobjects, and\nCOMPUTERVISION\n\u2022 roboticstomanipulate objectsandmoveabout.\nROBOTICS\nThese six disciplines compose most of AI, and Turing deserves credit for designing a test\nthat remains relevant 60 years later. Yet AI researchers have devoted little effort to passing\nthe Turing Test, believing that it is more important to study the underlying principles of in-\ntelligence thantoduplicate anexemplar. Thequest for\u201cartificial flight\u201dsucceeded whenthe\nWrightbrothers andothers stopped imitating birdsandstarted using windtunnels andlearn-\ning about aerodynamics. Aeronautical engineering texts do not define the goal of their field\nasmaking\u201cmachinesthatflysoexactlylikepigeonsthattheycanfoolevenotherpigeons.\u201d\n1.1.2 Thinking humanly: The cognitivemodeling approach\nIfwearegoing to saythat agiven program thinks like ahuman, wemusthave some wayof\ndetermining how humans think. Weneed toget inside the actual workings of human minds.\nThere are three ways to do this: through introspection\u2014trying to catch our own thoughts as\nthey go by; through psychological experiments\u2014observing a person in action; and through\nbrain imaging\u2014observing the brain in action. Once wehave a sufficiently precise theory of\nthemind,itbecomespossible toexpress thetheory asacomputer program. Iftheprogram\u2019s\ninput\u2013output behavior matchescorresponding human behavior, thatisevidence thatsomeof\nthe program\u2019s mechanisms could also be operating in humans. For example, Allen Newell\nandHerbertSimon,whodeveloped GPS,the\u201cGeneralProblemSolver\u201d(NewellandSimon,\n1961), were not content merely to have their program solve problems correctly. They were\nmore concerned with comparing the trace of its reasoning steps to traces of human subjects\nsolving the same problems. The interdisciplinary field of cognitive science brings together\nCOGNITIVESCIENCE\ncomputermodelsfromAIandexperimentaltechniques frompsychologytoconstructprecise\nandtestabletheories ofthehumanmind.\nCognitivescienceisafascinating fieldinitself,worthyofseveraltextbooksandatleast\none encyclopedia (Wilson andKeil, 1999). Wewilloccasionally comment onsimilarities or\ndifferences betweenAItechniques andhumancognition. Realcognitive science, however,is\nnecessarily based on experimental investigation of actual humans or animals. We will leave\nthatforotherbooks, asweassumethereaderhasonlyacomputerforexperimentation.\nIn the early days of AI there was often confusion between the approaches: an author\nwould argue that an algorithm performs well on a task and that it is therefore a good model\nof human performance, or vice versa. Modern authors separate the two kinds of claims;\nthis distinction has allowed both AI and cognitive science to develop more rapidly. Thetwo\nfields continue to fertilize each other, most notably in computer vision, which incorporates\nneurophysiological evidence intocomputational models. 4 Chapter 1. Introduction\n1.1.3 Thinking rationally: The \u201clawsofthought\u201d approach\nTheGreekphilosopherAristotlewasoneofthefirsttoattempttocodify\u201crightthinking,\u201dthat\nis, irrefutable reasoning processes. His syllogisms provided patterns forargument structures\nSYLLOGISM\nthatalwaysyieldedcorrectconclusionswhengivencorrectpremises\u2014forexample,\u201cSocrates\nis a man; all men are mortal; therefore, Socrates is mortal.\u201d These laws of thought were\nsupposed togoverntheoperation ofthemind;theirstudyinitiated thefieldcalledlogic.\nLOGIC\nLogiciansinthe19thcenturydevelopedaprecisenotationforstatementsaboutallkinds\nofobjects intheworldandtherelations amongthem. (Contrastthiswithordinaryarithmetic\nnotation, which provides only for statements about numbers.) By 1965, programs existed\nthat could, inprinciple, solve anysolvable problem described inlogical notation. (Although\nifnosolution exists, theprogram mightloop forever.) Theso-called logicist tradition within\nLOGICIST\nartificialintelligence hopestobuildonsuchprogramstocreateintelligent systems.\nThere are two main obstacles to this approach. First, it is not easy to take informal\nknowledge and state it in the formal terms required by logical notation, particularly when\nthe knowledge is less than 100% certain. Second, there is a big difference between solving\na problem \u201cin principle\u201d and solving it in practice. Even problems with just a few hundred\nfacts can exhaust the computational resources of any computer unless it has some guidance\nastowhichreasoningstepstotryfirst. Althoughbothoftheseobstaclesapplytoanyattempt\ntobuildcomputational reasoning systems, theyappearedfirstinthelogicisttradition.\n1.1.4 Acting rationally: The rational agentapproach\nAn agent is just something that acts (agent comes from the Latin agere, to do). Of course,\nAGENT\nall computer programs do something, but computer agents are expected to do more: operate\nautonomously, perceive their environment, persist over a prolonged time period, adapt to\nchange, and create and pursue goals. A rational agent is one that acts so as to achieve the\nRATIONALAGENT\nbestoutcomeor,whenthereisuncertainty, thebestexpectedoutcome.\nInthe\u201clawsofthought\u201d approach toAI,theemphasiswasoncorrectinferences. Mak-\ning correct inferences is sometimes part of being a rational agent, because one way to act\nrationally istoreason logically totheconclusion thatagivenaction willachieve one\u2019s goals\nand then to act on that conclusion. On the other hand, correct inference is not all of ration-\nality; insomesituations, thereisnoprovably correct thingtodo, butsomething muststillbe\ndone. There are also ways of acting rationally that cannot be said to involve inference. For\nexample, recoiling from a hot stove is a reflex action that is usually more successful than a\nsloweractiontakenaftercarefuldeliberation.\nAlltheskillsneededfortheTuringTestalsoallowanagenttoactrationally. Knowledge\nrepresentation and reasoning enable agents to reach good decisions. We need to be able to\ngenerate comprehensible sentences in natural language to get by in a complex society. We\nneed learning not only for erudition, but also because it improves our ability to generate\neffectivebehavior.\nThe rational-agent approach has two advantages over the other approaches. First, it\nis more general than the \u201claws of thought\u201d approach because correct inference is just one\nof several possible mechanisms for achieving rationality. Second, it is more amenable to Section1.2. TheFoundations ofArtificialIntelligence 5\nscientificdevelopmentthanareapproaches basedonhumanbehaviororhumanthought. The\nstandard of rationality is mathematically well defined and completely general, and can be\n\u201cunpacked\u201d togenerateagentdesignsthatprovablyachieve it. Humanbehavior, ontheother\nhand, is well adapted for one specific environment and is defined by, well, the sum total\nof all the things that humans do. This book therefore concentrates on general principles\nof rational agents and on components for constructing them. We will see that despite the\napparent simplicity with which the problem can be stated, an enormous variety of issues\ncomeupwhenwetrytosolveit. Chapter2outlines someoftheseissuesinmoredetail.\nOneimportantpointtokeepinmind: Wewillseebeforetoolongthatachievingperfect\nrationality\u2014always doing the right thing\u2014is notfeasible in complicated environments. The\ncomputational demands are just too high. Formost of the book, however, we will adopt the\nworking hypothesis that perfect rationality isa good starting point foranalysis. It simplifies\nthe problem and provides the appropriate setting for most of the foundational material in\nLIMITED the field. Chapters 5 and 17 deal explicitly with the issue of limited rationality\u2014acting\nRATIONALITY\nappropriately whenthereisnotenoughtimetodoallthecomputations onemightlike.\n1.2 THE FOUNDATIONS OF ARTIFICIAL INTELLIGENCE\nInthissection,weprovideabriefhistoryofthedisciplinesthatcontributedideas,viewpoints,\nand techniques to AI. Like any history, this one is forced to concentrate on a small number\nof people, events, and ideas and to ignore others that also were important. We organize the\nhistory aroundaseriesofquestions. Wecertainly wouldnot wishtogivetheimpression that\nthese questions arethe only ones the disciplines address or that the disciplines have allbeen\nworkingtowardAIastheirultimatefruition.\n1.2.1 Philosophy\n\u2022 Canformalrulesbeusedtodrawvalidconclusions?\n\u2022 Howdoesthemindarisefromaphysical brain?\n\u2022 Wheredoesknowledgecomefrom?\n\u2022 Howdoesknowledgeleadtoaction?\nAristotle (384\u2013322 B.C.), whose bust appears on the front cover of this book, was the first\nto formulate a precise set of laws governing the rational part of the mind. He developed an\ninformalsystemofsyllogismsforproperreasoning, whichinprincipleallowedonetogener-\nate conclusions mechanically, given initial premises. Much later, Ramon Lull (d. 1315) had\ntheideathatuseful reasoning couldactually becarried out byamechanical artifact. Thomas\nHobbes (1588\u20131679) proposed thatreasoning waslike numerical computation, that\u201cweadd\nand subtract in our silent thoughts.\u201d The automation of computation itself was already well\nunder way. Around 1500, Leonardo da Vinci (1452\u20131519) designed but did not build a me-\nchanical calculator; recent reconstructions have shown the design to be functional. Thefirst\nknown calculating machine was constructed around 1623 by the German scientist Wilhelm\nSchickard (1592\u20131635), although thePascaline, builtin1642byBlaisePascal(1623\u20131662), 6 Chapter 1. Introduction\nis more famous. Pascal wrote that \u201cthe arithmetical machine produces effects which appear\nnearer to thought than all the actions of animals.\u201d Gottfried Wilhelm Leibniz (1646\u20131716)\nbuilt amechanical device intended to carry out operations on concepts rather than numbers,\nbut its scope was rather limited. Leibniz did surpass Pascal by building a calculator that\ncould add, subtract, multiply, and take roots, whereas the Pascaline could only add and sub-\ntract. Some speculated that machines might not just do calculations but actually be able to\nthink and act on their own. In his 1651 book Leviathan, Thomas Hobbes suggested the idea\nof an \u201cartificial animal,\u201d arguing \u201cFor what is the heart but a spring; and the nerves, but so\nmanystrings; andthejoints,butsomanywheels.\u201d\nIt\u2019sonethingtosaythatthemindoperates,atleastinpart,accordingtologicalrules,and\nto build physical systems that emulate some of those rules; it\u2019s another to say that the mind\nitself is such a physical system. Rene\u00b4 Descartes (1596\u20131650) gave the first clear discussion\nofthedistinction betweenmindandmatterandoftheproblemsthatarise. Oneproblemwith\na purely physical conception of the mind is that it seems to leave little room for free will:\nif the mind is governed entirely by physical laws, then it has no more free will than a rock\n\u201cdeciding\u201dtofalltowardthecenteroftheearth. Descarteswasastrongadvocateofthepower\nof reasoning in understanding theworld, aphilosophy now called rationalism, and one that\nRATIONALISM\ncounts Aristotle and Leibnitz as members. But Descartes was also a proponent of dualism.\nDUALISM\nHe held that there is a part of the human mind (or soul or spirit) that is outside of nature,\nexempt from physical laws. Animals, on the other hand, did not possess this dual quality;\nthey could be treated as machines. An alternative to dualism is materialism, which holds\nMATERIALISM\nthat the brain\u2019s operation according to the laws of physics constitutes the mind. Free will is\nsimplythewaythattheperception ofavailable choicesappears tothechoosing entity.\nGiven a physical mind that manipulates knowledge, the next problem is to establish\nthe source of knowledge. The empiricism movement, starting with Francis Bacon\u2019s (1561\u2013\nEMPIRICISM\n1626)NovumOrganum,2 ischaracterizedbyadictumofJohnLocke(1632\u20131704): \u201cNothing\nis in the understanding, which was not first in the senses.\u201d David Hume\u2019s (1711\u20131776) A\nTreatise of Human Nature (Hume, 1739) proposed what is now known as the principle of\ninduction: thatgeneralrulesareacquiredbyexposuretorepeatedassociations betweentheir\nINDUCTION\nelements. Building on the work of Ludwig Wittgenstein (1889\u20131951) and Bertrand Russell\n(1872\u20131970), the famous Vienna Circle, led by Rudolf Carnap (1891\u20131970), developed the\ndoctrineoflogicalpositivism. Thisdoctrineholdsthatallknowledgecanbecharacterizedby\nLOGICALPOSITIVISM\nOBSERVATION logical theories connected, ultimately, to observation sentences that correspond to sensory\nSENTENCES\ninputs;thuslogicalpositivismcombinesrationalismandempiricism.3 Theconfirmationthe-\nCONFIRMATION oryofCarnapandCarlHempel(1905\u20131997) attempted toanalyze theacquisition ofknowl-\nTHEORY\nedge from experience. Carnap\u2019s book The Logical Structure of the World (1928) defined an\nexplicit computational procedure for extracting knowledge from elementary experiences. It\nwasprobably thefirsttheoryofmindasacomputational process.\n2 TheNovum Organum isanupdate of Aristotle\u2019sOrganon, orinstrument ofthought. ThusAristotlecanbe\nseenasbothanempiricistandarationalist.\n3 Inthispicture,allmeaningfulstatementscanbeverifiedorfalsifiedeitherbyexperimentationorbyanalysis\nofthemeaningofthewords.Becausethisrulesoutmostofmetaphysics,aswastheintention,logicalpositivism\nwasunpopularinsomecircles. Section1.2. TheFoundations ofArtificialIntelligence 7\nThe final element in the philosophical picture of the mind is the connection between\nknowledgeandaction. ThisquestionisvitaltoAIbecauseintelligencerequiresactionaswell\nas reasoning. Moreover, only by understanding how actions are justified can we understand\nhowtobuildanagentwhoseactionsarejustifiable(orrational). Aristotleargued(inDeMotu\nAnimalium)thatactionsarejustifiedbyalogicalconnectionbetweengoalsandknowledgeof\ntheaction\u2019soutcome(thelastpartofthisextractalsoappearsonthefrontcoverofthisbook,\nintheoriginal Greek):\nButhowdoesithappenthatthinkingissometimesaccompaniedbyactionandsometimes\nnot, sometimes by motion, and sometimes not? It looks as if almost the same thing\nhappensasinthecaseofreasoningandmakinginferencesaboutunchangingobjects.But\nin that case the end is a speculative proposition ... whereas here the conclusionwhich\nresultsfromthe twopremisesisan action. ...I needcovering;a cloakis a covering. I\nneedacloak. WhatIneed,Ihavetomake;Ineedacloak. Ihavetomakeacloak. And\ntheconclusion,the\u201cIhavetomakeacloak,\u201disanaction.\nIn the Nicomachean Ethics (Book III. 3, 1112b), Aristotle further elaborates on this topic,\nsuggesting analgorithm:\nWedeliberatenotaboutends,butaboutmeans. Foradoctordoesnotdeliberatewhether\nhe shall heal, nor an orator whether he shall persuade, ... They assume the end and\nconsiderhowandbywhatmeansitisattained,andifitseemseasilyandbestproduced\nthereby;whileifitisachievedbyonemeansonlytheyconsiderhowitwillbeachieved\nbythisandbywhatmeansthiswillbeachieved,tilltheycometothefirstcause,...and\nwhatislastintheorderofanalysisseemstobefirstintheorderofbecoming. Andifwe\ncomeonanimpossibility,wegiveupthesearch,e.g.,ifweneedmoneyandthiscannot\nbegot;butifathingappearspossiblewetrytodoit.\nAristotle\u2019s algorithm was implemented 2300 years later by Newell and Simon in their GPS\nprogram. Wewouldnowcallitaregression planning system(seeChapter10).\nGoal-based analysis is useful, but does not say what to do when several actions will\nachievethegoalorwhennoactionwillachieveitcompletely. AntoineArnauld(1612\u20131694)\ncorrectly described a quantitative formula for deciding what action to take in cases like this\n(seeChapter16). JohnStuartMill\u2019s(1806\u20131873) bookUtilitarianism (Mill,1863)promoted\ntheideaofrationaldecisioncriteriainallspheresofhumanactivity. Themoreformaltheory\nofdecisions isdiscussed inthefollowingsection.\n1.2.2 Mathematics\n\u2022 Whataretheformalrulestodrawvalidconclusions?\n\u2022 Whatcanbecomputed?\n\u2022 Howdowereasonwithuncertain information?\nPhilosophersstakedoutsomeofthefundamentalideasofAI,buttheleaptoaformalscience\nrequired a level of mathematical formalization in three fundamental areas: logic, computa-\ntion,andprobability.\nTheidea of formal logic can be traced back to the philosophers of ancient Greece, but\nitsmathematicaldevelopmentreallybeganwiththeworkofGeorgeBoole(1815\u20131864),who 8 Chapter 1. Introduction\nworked out the details of propositional, or Boolean, logic (Boole, 1847). In 1879, Gottlob\nFrege(1848\u20131925) extendedBoole\u2019slogictoincludeobjectsandrelations, creatingthefirst-\norder logic that is used today.4 Alfred Tarski (1902\u20131983) introduced a theory of reference\nthatshowshowtorelatetheobjectsinalogictoobjectsintherealworld.\nThe next step was to determine the limits of what could be done with logic and com-\nputation. The first nontrivial algorithm is thought to be Euclid\u2019s algorithm for computing\nALGORITHM\ngreatest common divisors. Theword algorithm (and the idea of studying them) comes from\nal-Khowarazmi, a Persian mathematician of the 9th century, whose writings also introduced\nArabic numerals and algebra to Europe. Boole and others discussed algorithms for logical\ndeduction, and, bythelate 19thcentury, efforts wereunder waytoformalize general mathe-\nmatical reasoning as logical deduction. In 1930, KurtGo\u00a8del (1906\u20131978) showed that there\nexists aneffective procedure toprove anytrue statement in the first-order logic ofFregeand\nRussell, but that first-order logic could not capture the principle of mathematical induction\nneeded to characterize the natural numbers. In 1931, Go\u00a8del showed that limits on deduc-\nINCOMPLETENESS tion do exist. His incompleteness theorem showed that in any formal theory as strong as\nTHEOREM\nPeano arithmetic (the elementary theory of natural numbers), there are true statements that\nareundecidable inthesensethattheyhavenoproofwithinthetheory.\nThis fundamental result can also be interpreted as showing that some functions on the\nintegers cannot be represented by an algorithm\u2014that is, they cannot be computed. This\nmotivated Alan Turing (1912\u20131954) to try to characterize exactly which functions are com-\nputable\u2014capable of being computed. This notion is actually slightly problematic because\nCOMPUTABLE\nthenotionofacomputation oreffectiveprocedure reallycannotbegivenaformaldefinition.\nHowever, the Church\u2013Turing thesis, which states that the Turing machine (Turing, 1936) is\ncapableofcomputinganycomputablefunction,isgenerallyacceptedasprovidingasufficient\ndefinition. Turing also showed that there were some functions that no Turing machine can\ncompute. For example, no machine can tell in general whether a given program will return\nanansweronagiveninputorrunforever.\nAlthoughdecidabilityandcomputabilityareimportanttoanunderstandingofcomputa-\ntion,thenotionoftractability hashadanevengreaterimpact. Roughlyspeaking, aproblem\nTRACTABILITY\niscalledintractableifthetimerequiredtosolveinstancesoftheproblemgrowsexponentially\nwith the size of the instances. The distinction between polynomial and exponential growth\nincomplexity wasfirstemphasized inthemid-1960s (Cobham, 1964; Edmonds, 1965). Itis\nimportant because exponential growthmeansthatevenmoderately largeinstances cannot be\nsolved in any reasonable time. Therefore, one should strive to divide the overall problem of\ngenerating intelligent behaviorintotractable subproblems ratherthanintractable ones.\nHowcanone recognize an intractable problem? Thetheory of NP-completeness, pio-\nNP-COMPLETENESS\nneeredbyStevenCook(1971)andRichardKarp(1972), provides amethod. CookandKarp\nshowed theexistence oflarge classes ofcanonical combinatorial search andreasoning prob-\nlems that are NP-complete. Anyproblem class to which the class of NP-complete problems\ncanbereducedislikelytobeintractable. (AlthoughithasnotbeenprovedthatNP-complete\n4 Frege\u2019sproposed notation for first-orderlogic\u2014an arcane combination of textual and geometric features\u2014\nneverbecamepopular. Section1.2. TheFoundations ofArtificialIntelligence 9\nproblems are necessarily intractable, most theoreticians believe it.) These results contrast\nwith the optimism with which the popular press greeted the first computers\u2014\u201cElectronic\nSuper-Brains\u201d that were \u201cFaster than Einstein!\u201d Despite the increasing speed of computers,\ncareful use of resources will characterize intelligent systems. Put crudely, the world is an\nextremely large problem instance! Work in AI has helped explain why some instances of\nNP-completeproblems arehard,yetothersareeasy(Cheesemanetal.,1991).\nBesideslogicandcomputation, thethirdgreatcontribution ofmathematics toAIisthe\ntheory of probability. The Italian Gerolamo Cardano (1501\u20131576) first framed the idea of\nPROBABILITY\nprobability, describing it in terms of the possible outcomes of gambling events. In 1654,\nBlaise Pascal (1623\u20131662), in a letter to Pierre Fermat (1601\u20131665), showed how to pre-\ndict the future of an unfinished gambling game and assign average payoffs to the gamblers.\nProbabilityquicklybecameaninvaluable partofallthequantitative sciences, helpingtodeal\nwithuncertain measurements and incomplete theories. James Bernoulli (1654\u20131705), Pierre\nLaplace (1749\u20131827), and others advanced the theory and introduced new statistical meth-\nods. Thomas Bayes (1702\u20131761), who appears on the front cover of this book, proposed\na rule for updating probabilities in the light of new evidence. Bayes\u2019 rule underlies most\nmodernapproaches touncertain reasoning inAIsystems.\n1.2.3 Economics\n\u2022 Howshouldwemakedecisions soastomaximizepayoff?\n\u2022 Howshouldwedothiswhenothersmaynotgoalong?\n\u2022 Howshouldwedothiswhenthepayoffmaybefarinthefuture?\nThe science of economics got its start in 1776, when Scottish philosopher Adam Smith\n(1723\u20131790) published An Inquiry into the Nature and Causes of the Wealth of Nations.\nWhiletheancientGreeksandothershadmadecontributions toeconomicthought,Smithwas\nthe first to treat it as a science, using the idea that economies can be thought of as consist-\ning of individual agents maximizing their own economic well-being. Most people think of\neconomics as being about money, but economists will say that they are really studying how\npeople makechoices thatleadtopreferred outcomes. WhenMcDonald\u2019s offersahamburger\nforadollar,theyareassertingthattheywouldpreferthedollarandhopingthatcustomerswill\nprefer the hamburger. The mathematical treatment of \u201cpreferred outcomes\u201d or utility was\nUTILITY\nfirstformalized byLe\u00b4onWalras(pronounced \u201cValrasse\u201d) (1834-1910) andwasimproved by\nFrank Ramsey(1931) and laterby John von Neumann and OskarMorgenstern in theirbook\nTheTheoryofGamesandEconomicBehavior(1944).\nDecisiontheory,whichcombinesprobability theorywithutilitytheory,providesafor-\nDECISIONTHEORY\nmalandcompleteframeworkfordecisions(economicorotherwise)madeunderuncertainty\u2014\nthat is, in cases where probabilistic descriptions appropriately capture the decision maker\u2019s\nenvironment. This issuitable for\u201clarge\u201d economies whereeach agent need pay no attention\nto the actions of other agents as individuals. For \u201csmall\u201d economies, the situation is much\nmore like a game: the actions of one player can significantly affect the utility of another\n(either positively or negatively). Von Neumann and Morgenstern\u2019s development of game\ntheory (see also Luceand Raiffa, 1957) included the surprising result that, forsomegames,\nGAMETHEORY 10 Chapter 1. Introduction\narational agent should adoptpolicies thatare(orleast appeartobe)randomized. Unlikede-\ncisiontheory, gametheorydoesnotofferanunambiguous prescription forselecting actions.\nFor the most part, economists did not address the third question listed above, namely,\nhow tomake rational decisions whenpayoffs from actions are not immediate but instead re-\nsultfromseveralactionstakeninsequence. Thistopicwaspursuedinthefieldofoperations\nOPERATIONS research, which emerged in World War II from efforts in Britain to optimize radar installa-\nRESEARCH\ntions, and later found civilian applications in complex management decisions. The work of\nRichard Bellman (1957) formalized a class of sequential decision problems called Markov\ndecisionprocesses, whichwestudyinChapters17and21.\nWork in economics and operations research has contributed much to our notion of ra-\ntional agents, yet for many years AI research developed along entirely separate paths. One\nreason was the apparent complexity of making rational decisions. The pioneering AI re-\nsearcherHerbertSimon(1916\u20132001) wontheNobelPrizeineconomicsin1978forhisearly\nwork showing that models based on satisficing\u2014making decisions that are \u201cgood enough,\u201d\nSATISFICING\nrather than laboriously calculating an optimal decision\u2014gave a better description of actual\nhuman behavior (Simon, 1947). Since the 1990s, there has been a resurgence of interest in\ndecision-theoretic techniques foragentsystems(Wellman,1995).\n1.2.4 Neuroscience\n\u2022 Howdobrainsprocess information?\nNeuroscience is the study of the nervous system, particularly the brain. Although the exact\nNEUROSCIENCE\nwayinwhichthebrainenablesthoughtisoneofthegreatmysteriesofscience,thefactthatit\ndoesenablethoughthasbeenappreciated forthousands ofyearsbecauseoftheevidencethat\nstrong blowsto the head can lead tomental incapacitation. Ithas also long been known that\nhumanbrains aresomehow different; inabout 335 B.C. Aristotle wrote, \u201cOfalltheanimals,\nman has the largest brain in proportion to his size.\u201d5 Still, it was not until the middle of the\n18th century thatthebrain waswidely recognized astheseat ofconsciousness. Before then,\ncandidate locations included theheartandthespleen.\nPaul Broca\u2019s (1824\u20131880) study of aphasia (speech deficit) in brain-damaged patients\nin 1861 demonstrated the existence of localized areas of the brain responsible for specific\ncognitive functions. In particular, he showed that speech production was localized to the\nportion of the left hemisphere now called Broca\u2019s area.6 By that time, it was known that\nthe brain consisted of nerve cells, or neurons, but it was not until 1873 that Camillo Golgi\nNEURON\n(1843\u20131926) developed a staining technique allowing the observation of individual neurons\nin the brain (see Figure 1.2). This technique was used by Santiago Ramon y Cajal (1852\u2013\n1934)inhispioneering studiesofthebrain\u2019sneuronalstructures.7 NicolasRashevsky(1936,\n1938)wasthefirsttoapplymathematical modelstothestudyofthenervous sytem.\n5 Sincethen,ithasbeendiscoveredthatthetreeshrew(Scandentia)hasahigherratioofbraintobodymass.\n6 ManyciteAlexanderHood(1824)asapossiblepriorsource.\n7 Golgipersistedinhisbeliefthatthebrain\u2019sfunctionswerecarriedoutprimarilyinacontinuousmediumin\nwhichneuronswereembedded, whereasCajalpropounded the\u201cneuronal doctrine.\u201d ThetwosharedtheNobel\nprizein1906butgavemutuallyantagonisticacceptancespeeches. Section1.2. TheFoundations ofArtificialIntelligence 11\nAxonal arborization\nAxon from another cell\nSynapse\nDendrite Axon\nNucleus\nSynapses\nCell body or Soma\nFigure 1.2 The parts of a nerve cell or neuron. Each neuron consists of a cell body,\nor soma, that contains a cell nucleus. Branching out from the cell body are a number of\nfiberscalleddendritesanda single longfibercalled the axon. Theaxonstretchesoutfora\nlong distance, much longer than the scale in this diagram indicates. Typically, an axon is\n1cmlong(100timesthediameterofthecellbody),butcanreachupto1meter. Aneuron\nmakesconnectionswith10to100,000otherneuronsatjunctionscalledsynapses.Signalsare\npropagatedfrom neuronto neuronby a complicated electrochemicalreaction. The signals\ncontrolbrainactivityintheshorttermandalsoenablelong-termchangesintheconnectivity\nofneurons. Thesemechanismsarethoughttoformthebasisforlearninginthebrain. Most\ninformationprocessinggoesoninthecerebralcortex,theouterlayerofthebrain. Thebasic\norganizationalunit appears to be a column of tissue about0.5 mm in diameter, containing\nabout20,000neuronsandextendingthefulldepthofthecortexabout4mminhumans).\nWenowhavesomedataonthemappingbetweenareasofthebrain andthepartsofthe\nbody that they control orfrom which they receive sensory input. Such mappings are able to\nchange radically over the course of a few weeks, and some animals seem to have multiple\nmaps. Moreover, we do not fully understand how other areas can take over functions when\noneareaisdamaged. Thereisalmostnotheoryonhowanindividual memoryisstored.\nThe measurement of intact brain activity began in 1929 with the invention by Hans\nBergerofthe electroencephalograph (EEG).Therecent development offunctional magnetic\nresonance imaging (fMRI) (Ogawa et al., 1990; Cabeza and Nyberg, 2001) is giving neu-\nroscientists unprecedentedly detailed images of brain activity, enabling measurements that\ncorrespond in interesting ways to ongoing cognitive processes. These are augmented by\nadvances in single-cell recording of neuron activity. Individual neurons can be stimulated\nelectrically, chemically, orevenoptically(HanandBoyden,2007),allowingneuronal input\u2013\noutput relationships to be mapped. Despite these advances, we are still a long way from\nunderstanding howcognitiveprocesses actually work.\nThe truly amazing conclusion is that a collection of simple cells can lead to thought,\naction, and consciousness or, in the pithy words of John Searle (1992), brains cause minds. 12 Chapter 1. Introduction\nSupercomputer PersonalComputer HumanBrain\nComputational units 104 CPUs,1012 transistors 4CPUs,109 transistors 1011 neurons\nStorageunits 1014 bitsRAM 1011 bitsRAM 1011 neurons\n1015 bitsdisk 1013 bitsdisk 1014 synapses\nCycletime\n10\u22129\nsec\n10\u22129\nsec\n10\u22123\nsec\nOperations\/sec 1015 1010 1017\nMemoryupdates\/sec 1014 1010 1014\nFigure1.3 AcrudecomparisonoftherawcomputationalresourcesavailabletotheIBM\nBLUE GENEsupercomputer,atypicalpersonalcomputerof2008,andthehumanbrain.The\nbrain\u2019s numbersare essentially fixed, whereas the supercomputer\u2019snumbers have been in-\ncreasing by a factorof 10 every 5 years or so, allowing it to achieveroughparity with the\nbrain.Thepersonalcomputerlagsbehindonallmetricsexceptcycletime.\nTheonlyrealalternative theoryismysticism: thatmindsoperateinsomemysticalrealmthat\nisbeyondphysical science.\nBrainsanddigitalcomputershavesomewhatdifferentproperties. Figure1.3showsthat\ncomputers have a cycle time that is a million times faster than a brain. The brain makes up\nfor that with far more storage and interconnection than even a high-end personal computer,\nalthough the largest supercomputers have a capacity that is similar to the brain\u2019s. (It should\nbe noted, however, that the brain does not seem to use all of its neurons simultaneously.)\nFuturists make much of these numbers, pointing to an approaching singularity at which\nSINGULARITY\ncomputers reach asuperhuman level ofperformance (Vinge, 1993; Kurzweil, 2005), but the\nrawcomparisons arenotespecially informative. Evenwitha computerofvirtuallyunlimited\ncapacity, westillwouldnotknowhowtoachievethebrain\u2019slevelofintelligence.\n1.2.5 Psychology\n\u2022 Howdohumansandanimalsthinkandact?\nThe origins of scientific psychology are usually traced to the work of the German physi-\ncist Hermann von Helmholtz (1821\u20131894) and his student Wilhelm Wundt (1832\u20131920).\nHelmholtz applied the scientific method to the study of human vision, and his Handbook\nof Physiological Optics is even now described as \u201cthe single most important treatise on the\nphysics and physiology of human vision\u201d (Nalwa, 1993, p.15). In 1879, Wundt opened the\nfirst laboratory of experimental psychology, at the University of Leipzig. Wundt insisted\non carefully controlled experiments in which his workers would perform a perceptual oras-\nsociative task while introspecting on their thought processes. The careful controls went a\nlong way toward making psychology a science, but the subjective nature of the data made\nit unlikely that an experimenter would ever disconfirm his or her own theories. Biologists\nstudying animal behavior, onthe otherhand, lacked introspective data and developed an ob-\njectivemethodology,asdescribedbyH.S.Jennings(1906)inhisinfluentialworkBehaviorof\nthe Lower Organisms. Applying this viewpoint to humans, the behaviorism movement, led\nBEHAVIORISM\nbyJohnWatson(1878\u20131958), rejectedanytheoryinvolvingmentalprocessesonthegrounds Section1.2. TheFoundations ofArtificialIntelligence 13\nthatintrospection couldnotprovidereliableevidence. Behavioristsinsistedonstudyingonly\nobjective measures of the percepts (or stimulus) given to an animal and its resulting actions\n(or response). Behaviorism discovered a lot about rats and pigeons but had less success at\nunderstanding humans.\nCOGNITIVE Cognitive psychology, which views the brain as an information-processing device,\nPSYCHOLOGY\ncan be traced back at least to the works of William James (1842\u20131910). Helmholtz also\ninsisted that perception involved a form of unconscious logical inference. The cognitive\nviewpoint waslargely eclipsed by behaviorism in the United States, but atCambridge\u2019s Ap-\nplied Psychology Unit, directed by Frederic Bartlett (1886\u20131969), cognitive modeling was\nable to flourish. The Nature of Explanation, by Bartlett\u2019s student and successor Kenneth\nCraik (1943), forcefully reestablished the legitimacy of such \u201cmental\u201d terms as beliefs and\ngoals, arguing that they are just as scientific as, say, using pressure and temperature to talk\nabout gases, despite their being made of molecules that have neither. Craik specified the\nthreekeystepsofaknowledge-based agent: (1)thestimulusmustbetranslatedintoaninter-\nnalrepresentation, (2)therepresentation ismanipulated bycognitiveprocessestoderivenew\ninternal representations, and (3) these are in turn retranslated back into action. He clearly\nexplained whythiswasagooddesignforanagent:\nIftheorganismcarriesa\u201csmall-scalemodel\u201dofexternalrealityandofits ownpossible\nactionswithinitshead,itisabletotryoutvariousalternatives,concludewhichisthebest\nofthem,reacttofuturesituationsbeforetheyarise,utilizetheknowledgeofpastevents\nindealingwith thepresentandfuture,andin everywaytoreactin amuchfuller,safer,\nandmorecompetentmannertotheemergencieswhichfaceit. (Craik,1943)\nAfterCraik\u2019s death inabicycle accident in1945, hisworkwascontinued byDonaldBroad-\nbent,whosebookPerceptionandCommunication(1958)wasoneofthefirstworkstomodel\npsychological phenomena as information processing. Meanwhile, in the United States, the\ndevelopment ofcomputermodeling ledtothecreation ofthefieldofcognitive science. The\nfieldcanbe saidtohave started ataworkshop inSeptember 1956 atMIT.(Weshall see that\nthisisjusttwomonthsaftertheconference atwhichAIitselfwas\u201cborn.\u201d) Attheworkshop,\nGeorgeMillerpresented TheMagicNumberSeven,NoamChomskypresented ThreeModels\nof Language, and Allen Newell and Herbert Simon presented The Logic Theory Machine.\nThese three influential papers showed how computer models could be used to address the\npsychology of memory, language, and logical thinking, respectively. It is now a common\n(although far from universal) view among psychologists that \u201ca cognitive theory should be\nlikeacomputerprogram\u201d(Anderson,1980);thatis,itshoulddescribeadetailedinformation-\nprocessing mechanismwherebysomecognitivefunction mightbeimplemented.\n1.2.6 Computer engineering\n\u2022 Howcanwebuildanefficientcomputer?\nFor artificial intelligence to succeed, we need two things: intelligence and an artifact. The\ncomputer has been the artifact of choice. The modern digital electronic computer was in-\nventedindependently andalmostsimultaneously byscientistsinthreecountriesembattledin 14 Chapter 1. Introduction\nWorld War II. The first operational computer was the electromechanical Heath Robinson,8\nbuilt in 1940 byAlan Turing\u2019s team forasingle purpose: deciphering German messages. In\n1943, the same group developed the Colossus, a powerful general-purpose machine based\non vacuum tubes.9 The first operational programmable computer was the Z-3, the inven-\ntionofKonrad ZuseinGermanyin1941. Zusealsoinvented floating-point numbers andthe\nfirst high-level programming language, Plankalku\u00a8l. The first electronic computer, the ABC,\nwas assembled by John Atanasoff and his student Clifford Berry between 1940 and 1942\nat Iowa State University. Atanasoff\u2019s research received little support or recognition; it was\nthe ENIAC, developed as part of a secret military project at the University of Pennsylvania\nby a team including John Mauchly and John Eckert, that proved to be the most influential\nforerunnerofmoderncomputers.\nSincethattime,eachgenerationofcomputerhardwarehasbroughtanincreaseinspeed\nandcapacityandadecreaseinprice. Performancedoubledevery18monthsorsountilaround\n2005,whenpowerdissipationproblemsledmanufacturers to startmultiplyingthenumberof\nCPUcoresratherthantheclockspeed. Currentexpectations arethatfutureincreasesinpower\nwillcomefrommassiveparallelism\u2014a curious convergence withtheproperties ofthebrain.\nOf course, there were calculating devices before the electronic computer. The earliest\nautomated machines, dating from the 17th century, were discussed on page 6. The first pro-\ngrammable machine was a loom, devised in 1805 by Joseph Marie Jacquard (1752\u20131834),\nthat used punched cards to store instructions for the pattern to be woven. In the mid-19th\ncentury, Charles Babbage (1792\u20131871) designed two machines, neither of which he com-\npleted. TheDifference Enginewasintended tocomputemathematical tables forengineering\nandscientificprojects. Itwasfinallybuiltandshowntoworkin1991attheScienceMuseum\ninLondon(Swade,2000). Babbage\u2019s Analytical Enginewasfarmoreambitious: itincluded\naddressable memory, stored programs, andconditional jumpsand wasthefirstartifact capa-\nble ofuniversal computation. Babbage\u2019s colleague AdaLovelace, daughter ofthepoet Lord\nByron,wasperhapstheworld\u2019sfirstprogrammer. (TheprogramminglanguageAdaisnamed\nafterher.) ShewroteprogramsfortheunfinishedAnalytical Engineandevenspeculated that\nthemachinecouldplaychessorcomposemusic.\nAI also owes a debt to the software side of computer science, which has supplied the\noperatingsystems,programminglanguages, andtoolsneededtowritemodernprograms(and\npapers aboutthem). Butthisisoneareawherethedebthasbeenrepaid: workinAIhaspio-\nneeredmanyideasthathavemadetheirwaybacktomainstream computerscience,including\ntime sharing, interactive interpreters, personal computers with windows and mice, rapid de-\nvelopment environments, the linked list data type, automatic storage management, and key\nconcepts ofsymbolic, functional, declarative, andobject-oriented programming.\n8 HeathRobinsonwasacartoonistfamousforhisdepictionsofwhimsicalandabsurdlycomplicatedcontrap-\ntionsforeverydaytaskssuchasbutteringtoast.\n9 Inthepostwarperiod, Turingwantedtousethesecomputers forAIresearch\u2014forexample, oneofthefirst\nchessprograms(Turingetal.,1953).HiseffortswereblockedbytheBritishgovernment. Section1.2. TheFoundations ofArtificialIntelligence 15\n1.2.7 Control theory andcybernetics\n\u2022 Howcanartifactsoperate undertheirowncontrol?\nKtesibios of Alexandria (c. 250 B.C.) built the first self-controlling machine: a water clock\nwith a regulator that maintained a constant flow rate. This invention changed the definition\nof what an artifact could do. Previously, only living things could modify their behavior in\nresponse to changes inthe environment. Otherexamples of self-regulating feedback control\nsystems include the steam engine governor, created by James Watt (1736\u20131819), and the\nthermostat, invented by Cornelis Drebbel (1572\u20131633), who also invented the submarine.\nThemathematical theoryofstablefeedback systemswasdevelopedinthe19thcentury.\nThe central figure in the creation of what is now called control theory was Norbert\nCONTROLTHEORY\nWiener(1894\u20131964). Wienerwasabrilliant mathematician whoworkedwithBertrandRus-\nsell,amongothers,beforedevelopinganinterestinbiologicalandmechanicalcontrolsystems\nandtheirconnectiontocognition. LikeCraik(whoalsousedcontrolsystemsaspsychological\nmodels), Wiener and his colleagues Arturo Rosenblueth and Julian Bigelow challenged the\nbehaviorist orthodoxy (Rosenblueth et al., 1943). They viewed purposive behavior as aris-\ningfromaregulatorymechanismtryingtominimize\u201cerror\u201d\u2014thedifferencebetweencurrent\nstate and goal state. In the late 1940s, Wiener, along with Warren McCulloch, Walter Pitts,\nand John von Neumann, organized a series of influential conferences that explored the new\nmathematicalandcomputational modelsofcognition. Wiener\u2019sbookCybernetics(1948)be-\nCYBERNETICS\ncame a bestseller and awoke the public to the possibility of artificially intelligent machines.\nMeanwhile, in Britain, W. Ross Ashby (Ashby, 1940) pioneered similar ideas. Ashby, Alan\nTuring, Grey Walter, and others formed the Ratio Club for \u201cthose who had Wiener\u2019s ideas\nbeforeWiener\u2019s bookappeared.\u201d Ashby\u2019s DesignforaBrain(1948, 1952)elaborated onhis\nidea that intelligence could be created by the use of homeostatic devices containing appro-\nHOMEOSTATIC\npriatefeedback loopstoachievestableadaptivebehavior.\nModern control theory, especially the branch known as stochastic optimal control, has\nOBJECTIVE asitsgoalthedesignofsystemsthatmaximizeanobjectivefunctionovertime. Thisroughly\nFUNCTION\nmatches our view of AI: designing systems that behave optimally. Why, then, are AI and\ncontrol theory twodifferent fields, despite the close connections among theirfounders? The\nanswer lies in the close coupling between the mathematical techniques that were familiar to\ntheparticipantsandthecorresponding setsofproblemsthatwereencompassedineachworld\nview. Calculusandmatrixalgebra,thetoolsofcontroltheory,lendthemselvestosystemsthat\naredescribablebyfixedsetsofcontinuousvariables,whereasAIwasfoundedinpartasaway\ntoescapefromthetheseperceivedlimitations. Thetoolsoflogicalinferenceandcomputation\nallowed AIresearchers toconsider problems suchaslanguage, vision, andplanning that fell\ncompletely outsidethecontroltheorist\u2019s purview.\n1.2.8 Linguistics\n\u2022 Howdoeslanguage relatetothought?\nIn 1957, B. F. Skinner published Verbal Behavior. This was a comprehensive, detailed ac-\ncount of the behaviorist approach to language learning, written by the foremost expert in 16 Chapter 1. Introduction\nthe field. But curiously, a review of the book became as well known as the book itself, and\nserved to almost kill off interest in behaviorism. The author of the review was the linguist\nNoam Chomsky, who had just published a book on his own theory, Syntactic Structures.\nChomsky pointed out that the behaviorist theory did not address the notion of creativity in\nlanguage\u2014it didnotexplain howachild could understand and makeupsentences thatheor\nshehadneverheardbefore. Chomsky\u2019stheory\u2014based onsyntacticmodelsgoingbacktothe\nIndian linguist Panini(c. 350 B.C.)\u2014could explain this, andunlike previous theories, itwas\nformalenoughthatitcouldinprinciple beprogrammed.\nModern linguistics and AI, then, were \u201cborn\u201d at about the same time, and grew up\nCOMPUTATIONAL together, intersectinginahybridfieldcalled computationallinguisticsornaturallanguage\nLINGUISTICS\nprocessing. Theproblemofunderstandinglanguagesoonturnedouttobeconsiderablymore\ncomplex than it seemed in 1957. Understanding language requires an understanding of the\nsubjectmatterandcontext,notjustanunderstandingofthestructureofsentences. Thismight\nseem obvious, but it was not widely appreciated until the 1960s. Much of the early work in\nknowledge representation (the study of how to put knowledge into a form that a computer\ncan reason with) was tied to language and informed by research in linguistics, which was\nconnected inturntodecades ofworkonthephilosophical analysis oflanguage.\n1.3 THE HISTORY OF ARTIFICIAL INTELLIGENCE\nWiththebackground materialbehindus,wearereadytocover thedevelopment ofAIitself.\n1.3.1 The gestationofartificialintelligence (1943\u20131955)\nThe first work that is now generally recognized as AI was done by Warren McCulloch and\nWalter Pitts (1943). They drew on three sources: knowledge of the basic physiology and\nfunction of neurons in the brain; a formal analysis of propositional logic due to Russell and\nWhitehead; andTuring\u2019stheoryofcomputation. Theyproposed amodelofartificialneurons\ninwhicheachneuronischaracterized asbeing\u201con\u201dor\u201coff,\u201d withaswitchto\u201con\u201doccurring\nin response to stimulation by a sufficient number of neighboring neurons. The state of a\nneuronwasconceivedofas\u201cfactuallyequivalenttoapropositionwhichproposeditsadequate\nstimulus.\u201d They showed, for example, that any computable function could be computed by\nsome network of connected neurons, and that all the logical connectives (and, or, not, etc.)\ncould be implemented by simple net structures. McCulloch and Pitts also suggested that\nsuitably defined networks could learn. Donald Hebb(1949) demonstrated asimple updating\nruleformodifying theconnection strengths between neurons. Hisrule, nowcalled Hebbian\nlearning,remainsaninfluential modeltothisday.\nHEBBIANLEARNING\nTwoundergraduate students at Harvard, Marvin Minsky and Dean Edmonds, built the\nfirst neural network computer in 1950. The SNARC, as it was called, used 3000 vacuum\ntubesandasurplusautomaticpilotmechanismfromaB-24bombertosimulateanetworkof\n40 neurons. Later, at Princeton, Minsky studied universal computation in neural networks.\nHis Ph.D. committee was skeptical about whether this kind of work should be considered Section1.3. TheHistoryofArtificialIntelligence 17\nmathematics, butvonNeumannreportedly said,\u201cIfitisn\u2019tnow,itwillbesomeday.\u201d Minsky\nwaslatertoproveinfluentialtheoremsshowingthelimitationsofneuralnetworkresearch.\nThere were a number of early examples of work that can be characterized as AI, but\nAlanTuring\u2019s vision wasperhaps themostinfluential. Hegavelectures onthetopic asearly\nas1947attheLondonMathematical Societyandarticulated apersuasive agenda inhis1950\narticle \u201cComputing Machinery and Intelligence.\u201d Therein, he introduced the Turing Test,\nmachine learning, genetic algorithms, and reinforcement learning. He proposed the Child\nProgrammeidea,explaining \u201cInsteadoftryingtoproduceaprogrammetosimulatetheadult\nmind,whynotrathertrytoproduce onewhichsimulated thechild\u2019s?\u201d\n1.3.2 The birth ofartificialintelligence (1956)\nPrinceton was home to another influential figure in AI, John McCarthy. After receiving his\nPhD there in 1951 and working for two years as an instructor, McCarthy moved to Stan-\nfordandthentoDartmouthCollege,whichwastobecometheofficialbirthplace ofthefield.\nMcCarthy convinced Minsky, Claude Shannon, and Nathaniel Rochester to help him bring\ntogether U.S. researchers interested in automata theory, neural nets, and the study of intel-\nligence. They organized a two-month workshop at Dartmouth in the summer of 1956. The\nproposal states:10\nWe propose that a 2 month, 10 man study of artificial intelligence be carried\nout during the summer of 1956 at Dartmouth College in Hanover, New Hamp-\nshire. The study is to proceed on the basis of the conjecture that every aspect of\nlearning or any other feature of intelligence can in principle be so precisely de-\nscribedthatamachinecanbemadetosimulateit. Anattemptwillbemadetofind\nhowtomakemachinesuselanguage, formabstractions andconcepts, solvekinds\nof problems now reserved for humans, and improve themselves. We think that a\nsignificant advance can be made in one or more of these problems if a carefully\nselectedgroupofscientists workonittogetherforasummer.\nThere were 10 attendees in all, including Trenchard More from Princeton, Arthur Samuel\nfromIBM,andRaySolomonoffandOliverSelfridgefromMIT.\nTwo researchers from Carnegie Tech,11 Allen Newell and Herbert Simon, rather stole\nthe show. Although the others had ideas and in some cases programs for particular appli-\ncations such as checkers, Newell and Simon already had a reasoning program, the Logic\nTheorist (LT), about whichSimonclaimed, \u201cWehave invented acomputerprogram capable\nofthinking non-numerically, andtherebysolvedthevenerable mind\u2013body problem.\u201d12 Soon\naftertheworkshop,theprogramwasabletoprovemostofthetheoremsinChapter2ofRus-\n10 ThiswasthefirstofficialusageofMcCarthy\u2019stermartificialintelligence.Perhaps\u201ccomputationalrationality\u201d\nwouldhavebeenmorepreciseandlessthreatening,but\u201cAI\u201dhasstuck.Atthe50thanniversaryoftheDartmouth\nconference, McCarthystatedthatheresistedtheterms\u201ccomputer\u201dor\u201ccomputational\u201dindeferencetoNorbert\nWeiner,whowaspromotinganalogcyberneticdevicesratherthandigitalcomputers.\n11 NowCarnegieMellonUniversity(CMU).\n12 Newell and Simon also invented a list-processing language, IPL, to write LT. They had no compiler and\ntranslateditintomachinecodebyhand. Toavoiderrors,theyworkedinparallel,callingoutbinarynumbersto\neachotherastheywroteeachinstructiontomakesuretheyagreed. 18 Chapter 1. Introduction\nsell andWhitehead\u2019s Principia Mathematica. Russell wasreportedly delighted whenSimon\nshowedhimthattheprogramhadcomeupwithaproofforonetheoremthatwasshorterthan\ntheoneinPrincipia. Theeditors ofthe Journal ofSymbolic Logicwerelessimpressed; they\nrejectedapapercoauthored byNewell,Simon,andLogicTheorist.\nThe Dartmouth workshop did not lead to any new breakthroughs, but it did introduce\nall the major figures to each other. For the next 20 years, the field would be dominated by\nthesepeopleandtheirstudents andcolleagues atMIT,CMU,Stanford, andIBM.\nLooking at the proposal for the Dartmouth workshop (McCarthy et al., 1955), we can\nsee whyitwasnecessary forAIto become aseparate field. Whycouldn\u2019t all the workdone\nin AI have taken place under the name of control theory or operations research or decision\ntheory, which, after all, have objectives similar to those of AI? Or why isn\u2019t AI a branch\nof mathematics? The first answer is that AI from the start embraced the idea of duplicating\nhuman faculties such as creativity, self-improvement, and language use. None of the other\nfields were addressing these issues. The second answer is methodology. AI is the only one\nofthesefieldsthatisclearlyabranchofcomputerscience(althoughoperationsresearchdoes\nshare an emphasis on computer simulations), and AI is the only field to attempt to build\nmachinesthatwillfunction autonomously incomplex, changing environments.\n1.3.3 Early enthusiasm, great expectations (1952\u20131969)\nTheearly yearsofAIwerefullofsuccesses\u2014in alimitedway. Giventheprimitive comput-\ners and programming tools of the time and the fact that only a few years earlier computers\nwereseenasthingsthatcoulddoarithmeticandnomore,itwasastonishingwheneveracom-\nputerdidanything remotely clever. Theintellectual establishment, byandlarge, preferred to\nbelieve that \u201ca machine can never do X.\u201d (See Chapter 26 for a long list of X\u2019s gathered\nby Turing.) AI researchers naturally responded by demonstrating one X after another. John\nMcCarthyreferredtothisperiodasthe\u201cLook,Ma,nohands!\u201d era.\nNewell and Simon\u2019s early success was followed up with the General Problem Solver,\nor GPS. Unlike Logic Theorist, this program was designed from the start to imitate human\nproblem-solving protocols. Within the limited class of puzzles it could handle, it turned out\nthat the order in which the program considered subgoals and possible actions was similarto\nthatinwhichhumansapproached thesameproblems. Thus, GPS wasprobably thefirstpro-\ngramtoembodythe\u201cthinkinghumanly\u201dapproach. Thesuccess ofGPS andsubsequentpro-\ngramsasmodelsofcognitionledNewellandSimon(1976)toformulatethefamousphysical\nPHYSICALSYMBOL symbolsystemhypothesis,whichstatesthat\u201caphysicalsymbolsystemhasthenecessaryand\nSYSTEM\nsufficient means forgeneral intelligent action.\u201d What they meant is that any system (human\nor machine) exhibiting intelligence must operate by manipulating data structures composed\nofsymbols. Wewillseelaterthatthishypothesis hasbeenchallenged frommanydirections.\nAt IBM, Nathaniel Rochester and his colleagues produced some of the first AI pro-\ngrams. Herbert Gelernter (1959) constructed the Geometry Theorem Prover, which was\nable to prove theorems that many students of mathematics would find quite tricky. Starting\nin 1952, Arthur Samuel wrote a series of programs for checkers (draughts) that eventually\nlearned toplay atastrong amateurlevel. Along theway, he disproved theidea that comput- Section1.3. TheHistoryofArtificialIntelligence 19\nerscandoonlywhattheyaretoldto: hisprogram quickly learned toplayabettergamethan\nitscreator. Theprogram wasdemonstrated ontelevision inFebruary 1956, creating astrong\nimpression. Like Turing, Samuel had trouble finding computer time. Working at night, he\nused machines that were still on the testing floor at IBM\u2019s manufacturing plant. Chapter 5\ncoversgameplaying, andChapter21explainsthelearning techniques usedbySamuel.\nJohn McCarthy movedfrom Dartmouth toMITand there madethree crucial contribu-\ntionsinonehistoricyear: 1958. InMITAILabMemoNo.1,McCarthydefinedthehigh-level\nlanguageLisp,whichwastobecomethedominantAIprogramminglanguageforthenext30\nLISP\nyears. WithLisp,McCarthyhadthetoolheneeded, butaccess toscarceandexpensivecom-\nputingresourceswasalsoaseriousproblem. Inresponse, he andothersatMITinventedtime\nsharing. Also in 1958, McCarthy published a paper entitled Programs with Common Sense,\nin which he described the Advice Taker, ahypothetical program that can be seen as the first\ncomplete AI system. Like the Logic Theorist and Geometry Theorem Prover, McCarthy\u2019s\nprogram was designed to use knowledge to search forsolutions to problems. But unlike the\nothers, it was to embody general knowledge of the world. For example, he showed how\nsomesimpleaxiomswouldenabletheprogram togenerate aplantodrivetotheairport. The\nprogram wasalso designed toaccept new axioms inthe normal course ofoperation, thereby\nallowing it to achieve competence in new areas without being reprogrammed. The Advice\nTaker thus embodied the central principles of knowledge representation and reasoning: that\nit is useful to have a formal, explicit representation of the world and its workings and to be\nable to manipulate that representation with deductive processes. It is remarkable how much\nofthe1958paperremainsrelevanttoday.\n1958alsomarkedtheyearthatMarvinMinskymovedtoMIT.Hisinitialcollaboration\nwithMcCarthydidnotlast,however. McCarthystressedrepresentation andreasoning infor-\nmal logic, whereas Minsky was more interested in getting programs to work and eventually\ndeveloped an anti-logic outlook. In 1963, McCarthy started the AIlab at Stanford. Hisplan\nto use logic to build the ultimate Advice Taker was advanced by J. A. Robinson\u2019s discov-\nery in 1965 of the resolution method (a complete theorem-proving algorithm for first-order\nlogic; see Chapter 9). Work at Stanford emphasized general-purpose methods for logical\nreasoning. Applications of logic included Cordell Green\u2019s question-answering and planning\nsystems (Green, 1969b) and the Shakey robotics project at the Stanford Research Institute\n(SRI). The latter project, discussed further in Chapter 25, was the first to demonstrate the\ncompleteintegration oflogicalreasoning andphysicalactivity.\nMinsky supervised a series of students who chose limited problems that appeared to\nrequire intelligence tosolve. Theselimited domains became known asmicroworlds. James\nMICROWORLD\nSlagle\u2019s SAINT program (1963) wasable tosolveclosed-form calculus integration problems\ntypical of first-year college courses. Tom Evans\u2019s ANALOGY program (1968) solved geo-\nmetricanalogyproblemsthatappearinIQtests. DanielBobrow\u2019sSTUDENT program(1967)\nsolvedalgebrastoryproblems,suchasthefollowing:\nIf the number of customers Tom gets is twice the square of 20 percent of the number\nof advertisementshe runs, and the numberof advertisements he runs is 45, what is the\nnumberofcustomersTomgets? 20 Chapter 1. Introduction\nBlue\nRed\nRed Green\nBlue\nGreen\nGreen\nRed\nFigure1.4 Ascenefromtheblocksworld. SHRDLU(Winograd,1972)hasjustcompleted\nthecommand\u201cFindablockwhichistallerthantheoneyouareholdingandputitinthebox.\u201d\nThe most famous microworld was the blocks world, which consists of a set of solid blocks\nplaced on a tabletop (or more often, a simulation of a tabletop), as shown in Figure 1.4.\nA typical task in this world is to rearrange the blocks in a certain way, using a robot hand\nthat can pick up one block at a time. The blocks world was home to the vision project of\nDavid Huffman (1971), the vision and constraint-propagation work of David Waltz (1975),\nthe learning theory of Patrick Winston (1970), the natural-language-understanding program\nofTerryWinograd(1972), andtheplannerofScottFahlman(1974).\nEarly work building on the neural networks of McCulloch and Pitts also flourished.\nThe work of Winograd and Cowan (1963) showed how a large number of elements could\ncollectively represent anindividual concept, withacorresponding increaseinrobustness and\nparallelism. Hebb\u2019s learning methods were enhanced by Bernie Widrow (Widrow and Hoff,\n1960; Widrow, 1962), who called his networks adalines, and by Frank Rosenblatt (1962)\nwith his perceptrons. The perceptron convergence theorem (Block et al., 1962) says that\nthelearningalgorithmcanadjusttheconnection strengths ofaperceptrontomatchanyinput\ndata,providedsuchamatchexists. Thesetopicsarecovered inChapter20.\n1.3.4 A doseofreality(1966\u20131973)\nFrom the beginning, AI researchers were not shy about making predictions of their coming\nsuccesses. Thefollowingstatement byHerbertSimonin1957 isoftenquoted:\nItisnotmyaimtosurpriseorshockyou\u2014butthesimplestwayIcansummarizeistosay\nthattherearenowintheworldmachinesthatthink,thatlearnandthatcreate. Moreover, Section1.3. TheHistoryofArtificialIntelligence 21\ntheirabilitytodothesethingsisgoingtoincreaserapidlyuntil\u2014inavisiblefuture\u2014the\nrangeofproblemstheycanhandlewillbecoextensivewiththerangetowhichthehuman\nmindhasbeenapplied.\nTerms such as \u201cvisible future\u201d can be interpreted in various ways, but Simon also made\nmore concrete predictions: that within 10 years a computer would be chess champion, and\na significant mathematical theorem would be proved by machine. These predictions came\ntrue(orapproximately true)within40yearsratherthan10. Simon\u2019soverconfidence wasdue\nto the promising performance of early AI systems on simple examples. In almost all cases,\nhowever, these early systems turned out to fail miserably when tried out onwiderselections\nofproblems andonmoredifficultproblems.\nThe first kind of difficulty arose because most early programs knew nothing of their\nsubject matter; they succeeded by means of simple syntactic manipulations. A typical story\noccurred inearlymachinetranslation efforts, whichweregenerously fundedbytheU.S.Na-\ntionalResearchCouncilinanattempttospeedupthetranslation ofRussianscientificpapers\ninthewakeoftheSputniklaunchin1957. Itwasthoughtinitiallythatsimplesyntactictrans-\nformations based on the grammars of Russian and English, and word replacement from an\nelectronic dictionary, would suffice to preserve the exact meanings of sentences. The fact is\nthat accurate translation requires background knowledge in order to resolve ambiguity and\nestablish the content of the sentence. The famous retranslation of \u201cthe spirit is willing but\nthe fleshisweak\u201d as\u201cthe vodka isgood but themeat isrotten\u201d illustrates the difficulties en-\ncountered. In1966,areportbyanadvisorycommitteefoundthat\u201ctherehasbeennomachine\ntranslationofgeneralscientifictext,andnoneisinimmediateprospect.\u201d AllU.S.government\nfundingforacademictranslation projectswascanceled. Today,machinetranslation isanim-\nperfectbutwidelyusedtoolfortechnical, commercial, government, andInternetdocuments.\nThesecondkindofdifficultywastheintractabilityofmanyoftheproblemsthatAIwas\nattempting to solve. Most of the early AI programs solved problems by trying out different\ncombinations of steps until the solution was found. This strategy worked initially because\nmicroworlds contained very few objects and hence very few possible actions and very short\nsolution sequences. Before the theory of computational complexity was developed, it was\nwidely thought that \u201cscaling up\u201d to larger problems was simply a matter of faster hardware\nandlargermemories. Theoptimismthataccompaniedthedevelopmentofresolutiontheorem\nproving,forexample,wassoondampenedwhenresearchers failedtoprovetheoremsinvolv-\ningmorethanafewdozenfacts. Thefactthataprogramcanfindasolutioninprincipledoes\nnotmeanthattheprogramcontains anyofthemechanismsneededtofinditinpractice.\nThe illusion of unlimited computational power was not confined to problem-solving\nprograms. Earlyexperiments in machineevolution(nowcalledgeneticalgorithms)(Fried-\nMACHINEEVOLUTION\nGENETIC berg, 1958; Friedberg et al., 1959) were based on the undoubtedly correct belief that by\nALGORITHM\nmaking an appropriate series of small mutations to a machine-code program, one can gen-\nerate a program with good performance for any particular task. The idea, then, was to try\nrandom mutations with a selection process to preserve mutations that seemed useful. De-\nspitethousandsofhoursofCPUtime,almostnoprogresswasdemonstrated. Moderngenetic\nalgorithms usebetterrepresentations andhaveshownmoresuccess. 22 Chapter 1. Introduction\nFailure to come to grips with the \u201ccombinatorial explosion\u201d was one of the main criti-\ncismsofAIcontainedintheLighthillreport(Lighthill,1973),whichformedthebasisforthe\ndecision bytheBritishgovernment toendsupport forAIresearch inallbuttwouniversities.\n(Oraltraditionpaintsasomewhatdifferentandmorecolorfulpicture,withpoliticalambitions\nandpersonal animosities whosedescription isbesidethepoint.)\nAthirddifficultyarosebecauseofsomefundamentallimitationsonthebasicstructures\nbeing used togenerate intelligent behavior. Forexample, Minsky andPapert\u2019s book Percep-\ntrons (1969) proved that, although perceptrons (a simple form of neural network) could be\nshowntolearnanythingtheywerecapableofrepresenting, theycouldrepresentverylittle. In\nparticular,atwo-inputperceptron(restrictedtobesimplerthantheformRosenblattoriginally\nstudied) couldnotbetrained torecognize whenitstwoinputs weredifferent. Althoughtheir\nresults did not apply to more complex, multilayer networks, research funding for neural-net\nresearch soon dwindled to almost nothing. Ironically, the newback-propagation learning al-\ngorithms for multilayer networks that were to cause an enormous resurgence in neural-net\nresearch inthelate1980swereactually discoveredfirstin1969(BrysonandHo,1969).\n1.3.5 Knowledge-based systems: The key to power? (1969\u20131979)\nThe picture of problem solving that had arisen during the first decade of AI research was of\na general-purpose search mechanism trying to string together elementary reasoning steps to\nfindcompletesolutions. Suchapproacheshavebeencalledweakmethodsbecause,although\nWEAKMETHOD\ngeneral, they do not scale up tolarge ordifficult problem instances. Thealternative to weak\nmethods is to use more powerful, domain-specific knowledge that allows larger reasoning\nstepsandcanmoreeasilyhandle typically occurring casesinnarrowareas ofexpertise. One\nmightsaythattosolveahardproblem,youhavetoalmostknow theansweralready.\nTheDENDRALprogram(Buchananetal.,1969)wasanearlyexampleofthisapproach.\nIt was developed at Stanford, where Ed Feigenbaum (a former student of Herbert Simon),\nBruce Buchanan (a philosopher turned computer scientist), and Joshua Lederberg (a Nobel\nlaureate geneticist) teameduptosolvetheproblem ofinferring molecularstructure from the\ninformation provided by a mass spectrometer. The input to the program consists of the ele-\nmentaryformulaofthemolecule(e.g.,C H NO )andthemassspectrumgivingthemasses\n6 13 2\nofthevariousfragmentsofthemoleculegeneratedwhenitisbombardedbyanelectronbeam.\nForexample, themassspectrum mightcontain apeakat m = 15,corresponding tothemass\nofamethyl(CH )fragment.\n3\nThe naive version of the program generated all possible structures consistent with the\nformula,andthenpredictedwhatmassspectrumwouldbeobservedforeach,comparingthis\nwith the actual spectrum. As one might expect, this is intractable for even moderate-sized\nmolecules. The DENDRAL researchers consulted analytical chemists and found that they\nworkedbylookingforwell-knownpatternsofpeaksinthespectrumthatsuggested common\nsubstructures inthe molecule. Forexample, the following rule isused torecognize aketone\n(C=O)subgroup (whichweighs28):\niftherearetwopeaksatx andx suchthat\n1 2\n(a)x +x =M +28(M isthemassofthewholemolecule);\n1 2 Section1.3. TheHistoryofArtificialIntelligence 23\n(b)x \u221228isahighpeak;\n1\n(c)x \u221228isahighpeak;\n2\n(d)Atleastoneofx andx ishigh.\n1 2\nthenthereisaketonesubgroup\nRecognizing thatthe molecule contains aparticular substructure reduces thenumberofpos-\nsiblecandidates enormously. DENDRAL waspowerfulbecause\nAlltherelevanttheoreticalknowledgetosolvetheseproblemshasbeenmappedoverfrom\nits general form in the [spectrum prediction component] (\u201cfirst principles\u201d) to efficient\nspecialforms(\u201ccookbookrecipes\u201d). (Feigenbaumetal.,1971)\nThe significance of DENDRAL was that it was the first successful knowledge-intensive sys-\ntem: its expertise derived from large numbers of special-purpose rules. Later systems also\nincorporated themainthemeofMcCarthy\u2019sAdviceTakerapproach\u2014the cleanseparation of\ntheknowledge(intheformofrules)fromthereasoning component.\nWith this lesson in mind, Feigenbaum and others at Stanford began the Heuristic Pro-\ngramming Project (HPP) to investigate the extent to which the new methodology of expert\nsystems could be applied to other areas of human expertise. The next major effort was in\nEXPERTSYSTEMS\ntheareaofmedical diagnosis. Feigenbaum, Buchanan, andDr.EdwardShortliffe developed\nMYCIN to diagnose blood infections. With about 450 rules, MYCIN was able to perform\nas well as some experts, and considerably better than junior doctors. It also contained two\nmajor differences from DENDRAL. First, unlike the DENDRAL rules, no general theoretical\nmodelexistedfromwhichtheMYCIN rulescouldbededuced. Theyhadtobeacquiredfrom\nextensive interviewing of experts, who in turn acquired them from textbooks, other experts,\nanddirectexperienceofcases. Second,theruleshadtoreflecttheuncertaintyassociatedwith\nCERTAINTYFACTOR\nmedical knowledge. MYCIN incorporated acalculus of uncertainty called certainty factors\n(seeChapter14),whichseemed(atthetime)tofitwellwithhowdoctorsassessedtheimpact\nofevidence onthediagnosis.\nThe importance of domain knowledge was also apparent in the area of understanding\nnatural language. AlthoughWinograd\u2019s SHRDLU system forunderstanding natural language\nhadengendered agooddealofexcitement,itsdependence onsyntacticanalysiscausedsome\nof the same problems as occurred in the early machine translation work. It was able to\novercome ambiguity and understand pronoun references, but this wasmainly because itwas\ndesigned specifically foronearea\u2014the blocks world. Several researchers, including Eugene\nCharniak, a fellow graduate student of Winograd\u2019s at MIT, suggested that robust language\nunderstanding would require general knowledge about the world and a general method for\nusingthatknowledge.\nAt Yale, linguist-turned-AI-researcher Roger Schank emphasized this point, claiming,\n\u201cThereisnosuchthingassyntax,\u201dwhichupsetalotoflinguistsbutdidservetostartauseful\ndiscussion. Schank and his students built a series of programs (Schank and Abelson, 1977;\nWilensky, 1978; Schank and Riesbeck, 1981; Dyer, 1983) that all had the task of under-\nstanding naturallanguage. Theemphasis, however,wasless onlanguage perseandmoreon\ntheproblemsofrepresenting andreasoning withtheknowledgerequiredforlanguageunder-\nstanding. The problems included representing stereotypical situations (Cullingford, 1981), 24 Chapter 1. Introduction\ndescribing human memory organization (Rieger, 1976; Kolodner, 1983), and understanding\nplansandgoals(Wilensky, 1983).\nThewidespread growthof applications toreal-world problems caused aconcurrent in-\ncrease in the demands for workable knowledge representation schemes. A large number\nof different representation and reasoning languages were developed. Some were based on\nlogic\u2014forexample,theProloglanguage becamepopularinEurope,andthe PLANNER fam-\nily in the United States. Others, following Minsky\u2019s idea of frames (1975), adopted a more\nFRAMES\nstructured approach, assembling facts about particular object and event types and arranging\nthetypesintoalargetaxonomichierarchy analogous toabiological taxonomy.\n1.3.6 AIbecomes anindustry (1980\u2013present)\nThefirstsuccessfulcommercialexpertsystem, R1,beganoperationattheDigitalEquipment\nCorporation (McDermott, 1982). The program helped configure orders for new computer\nsystems; by 1986, it was saving the company an estimated $40 million a year. By 1988,\nDEC\u2019sAIgroup had40expert systems deployed, withmoreontheway. DuPonthad100in\nuseand500indevelopment,savinganestimated$10millionayear. NearlyeverymajorU.S.\ncorporation haditsownAIgroupandwaseitherusingorinvestigating expertsystems.\nIn1981,theJapaneseannouncedthe\u201cFifthGeneration\u201dproject,a10-yearplantobuild\nintelligent computers running Prolog. In response, the United States formed the Microelec-\ntronics and ComputerTechnology Corporation (MCC)as aresearch consortium designed to\nassure national competitiveness. In both cases, AI was part of a broad effort, including chip\ndesign andhuman-interface research. InBritain, theAlvey report reinstated thefunding that\nwascutbytheLighthillreport.13 Inallthreecountries, however,theprojectsnevermettheir\nambitiousgoals.\nOverall,theAIindustryboomedfromafewmilliondollarsin1980tobillionsofdollars\nin 1988, including hundreds of companies building expert systems, vision systems, robots,\nand software and hardware specialized for these purposes. Soon after that came a period\ncalledthe\u201cAIWinter,\u201dinwhichmanycompaniesfellbythewaysideastheyfailedtodeliver\nonextravagant promises.\n1.3.7 The return ofneural networks (1986\u2013present)\nIn the mid-1980s at least four different groups reinvented the back-propagation learning\nBACK-PROPAGATION\nalgorithm first found in 1969 by Bryson and Ho. The algorithm was applied to many learn-\ning problems in computer science and psychology, and the widespread dissemination of the\nresults in the collection Parallel Distributed Processing (Rumelhart and McClelland, 1986)\ncausedgreatexcitement.\nThese so-called connectionist models of intelligent systems were seen by some as di-\nCONNECTIONIST\nrect competitors both to the symbolic models promoted by Newell and Simon and to the\nlogicist approach of McCarthy and others (Smolensky, 1988). It might seem obvious that\nat some level humans manipulate symbols\u2014in fact, Terrence Deacon\u2019s book The Symbolic\n13 Tosaveembarrassment,anewfieldcalledIKBS(IntelligentKnowledge-BasedSystems)wasinventedbecause\nArtificialIntelligencehadbeenofficiallycanceled. Section1.3. TheHistoryofArtificialIntelligence 25\nSpecies (1997) suggests that this is the defining characteristic of humans\u2014but the most ar-\ndentconnectionistsquestionedwhethersymbolmanipulationhadanyrealexplanatoryrolein\ndetailed modelsofcognition. Thisquestion remainsunanswered, butthecurrent viewisthat\nconnectionistandsymbolicapproachesarecomplementary, notcompeting. Asoccurredwith\nthe separation of AI and cognitive science, modern neural network research has bifurcated\ninto two fields, one concerned with creating effective network architectures and algorithms\nand understanding theirmathematical properties, the other concerned withcareful modeling\noftheempiricalproperties ofactualneuronsandensembles ofneurons.\n1.3.8 AIadopts thescientific method (1987\u2013present)\nRecent years have seen a revolution in both the content and the methodology of work in\nartificialintelligence.14 Itisnowmorecommontobuild onexisting theories thantopropose\nbrand-new ones, to base claims on rigorous theorems or hard experimental evidence rather\nthanonintuition, andtoshowrelevance toreal-world applications ratherthantoyexamples.\nAIwasfoundedinpartasarebellionagainstthelimitationsofexistingfieldslikecontrol\ntheoryandstatistics, butnowitisembracingthosefields. AsDavidMcAllester(1998)putit:\nIn the early period of AI it seemed plausible that new forms of symbolic computation,\ne.g.,framesandsemanticnetworks,mademuchofclassicaltheoryobsolete. Thisledto\na formof isolationism in which AI became largelyseparatedfromthe rest of computer\nscience. This isolationism is currently being abandoned. There is a recognition that\nmachinelearningshouldnotbeisolatedfrominformationtheory,thatuncertainreasoning\nshouldnotbeisolatedfromstochasticmodeling,thatsearchshouldnotbeisolatedfrom\nclassical optimizationand control, and that automated reasoning should not be isolated\nfromformalmethodsandstaticanalysis.\nIn terms of methodology, AI has finally come firmly under the scientific method. To be ac-\ncepted,hypothesesmustbesubjectedtorigorousempirical experiments,andtheresultsmust\nbe analyzed statistically for their importance (Cohen, 1995). It is now possible to replicate\nexperiments byusingsharedrepositories oftestdataandcode.\nThe field of speech recognition illustrates the pattern. In the 1970s, a wide variety of\ndifferent architectures and approaches were tried. Many of these were rather ad hoc and\nfragile, and were demonstrated on only a few specially selected examples. In recent years,\nHIDDENMARKOV approachesbasedonhiddenMarkovmodels(HMMs)havecometodominatethearea. Two\nMODELS\naspects ofHMMsarerelevant. First, theyarebased onarigorous mathematical theory. This\nhasallowedspeechresearcherstobuildonseveraldecadesofmathematicalresultsdeveloped\nin other fields. Second, they are generated by a process of training on a large corpus of\nreal speech data. This ensures that the performance is robust, and in rigorous blind tests the\nHMMshavebeenimprovingtheirscoressteadily. Speechtechnology andtherelatedfieldof\nhandwritten character recognition are already making the transition towidespread industrial\n14 Somehavecharacterizedthischange asavictoryofthe neats\u2014thosewhothinkthatAItheoriesshouldbe\ngrounded inmathematicalrigor\u2014overthe scruffies\u2014thosewhowouldrathertryoutlotsof ideas, writesome\nprograms, andthenassesswhatseemstobeworking. Bothapproachesareimportant. Ashifttowardneatness\nimpliesthatthefieldhasreachedalevelofstabilityandmaturity. Whetherthatstabilitywillbedisruptedbya\nnewscruffyideaisanotherquestion. 26 Chapter 1. Introduction\nand consumer applications. Note that there is no scientific claim that humans use HMMsto\nrecognize speech; rather, HMMs provide a mathematical framework for understanding the\nproblem andsupporttheengineering claimthattheyworkwellinpractice.\nMachine translation follows the samecourse asspeech recognition. In the1950s there\nwas initial enthusiasm for an approach based on sequences of words, with models learned\naccording to the principles of information theory. That approach fell out of favor in the\n1960s, butreturnedinthelate1990sandnowdominatesthefield.\nNeural networks also fitthis trend. Much of the work on neural nets in the 1980s was\ndone inanattempt toscope out whatcould bedone and tolearn howneural netsdifferfrom\n\u201ctraditional\u201d techniques. Usingimprovedmethodology and theoretical frameworks, thefield\narrived at an understanding in which neural nets can now be compared with corresponding\ntechniquesfromstatistics,patternrecognition,andmachinelearning,andthemostpromising\ntechnique can be applied to each application. As a result of these developments, so-called\ndataminingtechnology hasspawnedavigorous newindustry.\nDATAMINING\nJudeaPearl\u2019s(1988)Probabilistic ReasoninginIntelligent Systemsledtoanewaccep-\ntance ofprobability anddecision theory inAI,following aresurgence ofinterest epitomized\nby Peter Cheeseman\u2019s (1985) article \u201cIn Defense of Probability.\u201d The Bayesian network\nBAYESIANNETWORK\nformalism was invented to allow efficient representation of, and rigorous reasoning with,\nuncertain knowledge. This approach largely overcomes many problems of the probabilistic\nreasoningsystemsofthe1960sand1970s;itnowdominatesAIresearchonuncertainreason-\ning and expert systems. The approach allows for learning from experience, and it combines\nthebestofclassicalAIandneuralnets. WorkbyJudeaPearl(1982a)andbyEricHorvitzand\nDavidHeckerman(HorvitzandHeckerman,1986;Horvitzetal.,1986)promotedtheideaof\nnormative expert systems: ones that act rationally according to the laws of decision theory\nanddonottrytoimitatethethoughtstepsofhumanexperts. TheWindowsTM operatingsys-\ntem includes several normative diagnostic expert systems for correcting problems. Chapters\n13to16coverthisarea.\nSimilar gentle revolutions have occurred in robotics, computer vision, and knowledge\nrepresentation. Abetterunderstanding oftheproblemsand theircomplexityproperties,com-\nbined withincreased mathematical sophistication, has led to workable research agendas and\nrobustmethods. Althoughincreasedformalizationandspecializationledfieldssuchasvision\nandroboticstobecomesomewhatisolatedfrom\u201cmainstream\u201d AIinthe1990s,thistrendhas\nreversedinrecentyearsastoolsfrommachinelearninginparticularhaveprovedeffectivefor\nmanyproblems. Theprocessofreintegration isalreadyyielding significantbenefits\n1.3.9 The emergence ofintelligentagents (1995\u2013present)\nPerhaps encouraged bytheprogress insolving thesubproblems ofAI,researchers havealso\nstarted to look at the \u201cwhole agent\u201d problem again. The work of Allen Newell, John Laird,\nandPaulRosenbloomonSOAR(Newell,1990;Lairdetal.,1987)isthebest-knownexample\nof a complete agent architecture. One of the most important environments for intelligent\nagents is the Internet. AI systems have become so common in Web-based applications that\nthe \u201c-bot\u201d suffix has entered everyday language. Moreover, AI technologies underlie many Section1.3. TheHistoryofArtificialIntelligence 27\nInternettools, suchassearchengines, recommendersystems, andWebsiteaggregators.\nOneconsequenceoftryingtobuildcompleteagentsistherealizationthatthepreviously\nisolated subfields of AI might need to be reorganized somewhat when their results are to be\ntied together. In particular, it is now widely appreciated that sensory systems (vision, sonar,\nspeechrecognition, etc.) cannotdeliverperfectlyreliableinformationabouttheenvironment.\nHence, reasoning and planning systems must be able to handle uncertainty. Asecond major\nconsequence of the agent perspective is that AI has been drawn into much closer contact\nwith other fields, such as control theory and economics, that also deal with agents. Recent\nprogressinthecontrolofroboticcarshasderivedfromamixtureofapproachesrangingfrom\nbetter sensors, control-theoretic integration of sensing, localization and mapping, as well as\nadegreeofhigh-level planning.\nDespite these successes, some influential founders of AI, including John McCarthy\n(2007), Marvin Minsky (2007), Nils Nilsson (1995, 2005) and Patrick Winston (Beal and\nWinston,2009),haveexpresseddiscontentwiththeprogressofAI.TheythinkthatAIshould\nput less emphasis on creating ever-improved versions of applications that are good at a spe-\ncific task, such as driving a car, playing chess, or recognizing speech. Instead, they believe\nAIshouldreturntoitsrootsofstrivingfor,inSimon\u2019swords,\u201cmachinesthatthink,thatlearn\nandthatcreate.\u201d Theycalltheeffort human-levelAIorHLAI;theirfirstsymposium wasin\nHUMAN-LEVELAI\n2004(Minskyetal.,2004). Theeffortwillrequireverylargeknowledgebases; Hendleretal.\n(1995)discusswheretheseknowledgebasesmightcomefrom.\nARTIFICIALGENERAL A related idea is the subfield of Artificial General Intelligence or AGI (Goertzel and\nINTELLIGENCE\nPennachin,2007),whichhelditsfirstconferenceandorganizedtheJournalofArtificialGen-\neral Intelligence in 2008. AGI looks for a universal algorithm for learning and acting in\nany environment, and has its roots in the work of Ray Solomonoff (1964), one of the atten-\ndees of the original 1956 Dartmouth conference. Guaranteeing that what we create is really\nFriendlyAI isalso aconcern (Yudkowsky, 2008; Omohundro, 2008), one we will return to\nFRIENDLYAI\ninChapter26.\n1.3.10 The availabilityofvery largedata sets (2001\u2013present)\nThroughoutthe60-yearhistoryofcomputerscience,theemphasishasbeenonthealgorithm\nasthemainsubject ofstudy. ButsomerecentworkinAIsuggests thatformanyproblems,it\nmakes more sense to worry about the data and be less picky about what algorithm to apply.\nThis is true because of the increasing availability of very large data sources: for example,\ntrillionsofwordsofEnglishandbillionsofimagesfromtheWeb(KilgarriffandGrefenstette,\n2006);orbillionsofbasepairsofgenomicsequences (Collinsetal.,2003).\nOne influential paper in this line was Yarowsky\u2019s (1995) work on word-sense disam-\nbiguation: giventheuseoftheword\u201cplant\u201d inasentence, doesthatrefertofloraorfactory?\nPrevious approaches to the problem had relied on human-labeled examples combined with\nmachine learning algorithms. Yarowsky showed that the task can be done, with accuracy\nabove 96%, with no labeled examples at all. Instead, given a very large corpus of unanno-\ntatedtextandjustthedictionary definitions ofthetwosenses\u2014\u201cworks, industrial plant\u201d and\n\u201cflora, plant life\u201d\u2014one can label examples in the corpus, and from there bootstrap to learn 28 Chapter 1. Introduction\nnew patterns that help label new examples. Banko and Brill (2001) show that techniques\nlike this perform even better as the amount of available text goes from a million words to a\nbillion and that the increase in performance from using more data exceeds any difference in\nalgorithm choice; a mediocre algorithm with 100 million words of unlabeled training data\noutperforms thebestknownalgorithm with1millionwords.\nAsanotherexample, HaysandEfros(2007)discuss theproblem offillinginholesina\nphotograph. Suppose you use Photoshop to mask out an ex-friend from a group photo, but\nnow you need to fill in the masked area with something that matches the background. Hays\nandEfrosdefinedanalgorithmthatsearchesthroughacollectionofphotostofindsomething\nthat will match. They found the performance of their algorithm was poor when they used\na collection of only ten thousand photos, but crossed a threshold into excellent performance\nwhentheygrewthecollection totwomillionphotos.\nWorklikethissuggests thatthe\u201cknowledge bottleneck\u201d inAI\u2014theproblem ofhowto\nexpressalltheknowledgethatasystemneeds\u2014maybesolvedinmanyapplicationsbylearn-\ningmethodsratherthanhand-codedknowledgeengineering, providedthelearningalgorithms\nhaveenoughdatatogoon(Halevyetal.,2009). Reportershavenoticedthesurgeofnewap-\nplications and have written that \u201cAI Winter\u201d may be yielding to a new Spring (Havenstein,\n2005). As Kurzweil (2005) writes, \u201ctoday, many thousands of AI applications are deeply\nembeddedintheinfrastructure ofeveryindustry.\u201d\n1.4 THE STATE OF THE ART\nWhat can AI do today? A concise answer isdifficult because there are so many activities in\nsomanysubfields. Herewesampleafewapplications; othersappearthroughout thebook.\nRobotic vehicles: A driverless robotic car named STANLEY sped through the rough\nterrain of the Mojave dessert at 22 mph, finishing the 132-mile course first to win the 2005\nDARPAGrandChallenge. STANLEY isaVolkswagen Touaregoutfittedwithcameras,radar,\nandlaserrangefinders tosensetheenvironment andonboard softwaretocommandthesteer-\ning, braking, andacceleration (Thrun, 2006). Thefollowing yearCMU\u2019s BOSS wonthe Ur-\nbanChallenge,safelydrivingintrafficthroughthestreets ofaclosedAirForcebase,obeying\ntrafficrulesandavoiding pedestrians andothervehicles.\nSpeechrecognition: AtravelercallingUnitedAirlinestobookaflightcanhavetheen-\ntireconversationguidedbyanautomatedspeechrecognitionanddialogmanagementsystem.\nAutonomousplanningandscheduling: AhundredmillionmilesfromEarth,NASA\u2019s\nRemote Agent program became the first on-board autonomous planning program to control\nthe scheduling of operations for a spacecraft (Jonsson et al., 2000). REMOTE AGENT gen-\nerated plansfrom high-level goals specified fromtheground andmonitored theexecution of\nthoseplans\u2014detecting, diagnosing, andrecoveringfromproblemsastheyoccurred. Succes-\nsorprogram MAPGEN (Al-Chang etal.,2004)plans thedailyoperations forNASA\u2019sMars\nExplorationRovers,andMEXAR2(Cestaetal.,2007)didmissionplanning\u2014both logistics\nandscienceplanning\u2014for theEuropeanSpaceAgency\u2019sMarsExpressmissionin2008. Section1.5. Summary 29\nGame playing: IBM\u2019s DEEP BLUE became the first computer program to defeat the\nworld champion in a chess match when it bested Garry Kasparov by a score of 3.5 to 2.5 in\nanexhibition match (Goodman and Keene, 1997). Kasparov said that hefelt a\u201cnew kind of\nintelligence\u201d across the board from him. Newsweek magazine described the match as \u201cThe\nbrain\u2019s last stand.\u201d The value of IBM\u2019s stock increased by $18 billion. Human champions\nstudied Kasparov\u2019s loss and were able to draw a few matches in subsequent years, but the\nmostrecenthuman-computer matcheshavebeenwonconvincingly bythecomputer.\nSpamfighting: Eachday,learningalgorithmsclassifyoverabillionmessagesasspam,\nsavingtherecipientfromhavingtowastetimedeletingwhat,formanyusers,couldcomprise\n80%or90%ofallmessages,ifnotclassifiedawaybyalgorithms. Becausethespammersare\ncontinually updating theirtactics, itisdifficultforastaticprogrammed approach tokeepup,\nandlearning algorithmsworkbest(Sahami etal.,1998;GoodmanandHeckerman, 2004).\nLogistics planning: During the Persian Gulf crisis of 1991, U.S. forces deployed a\nDynamic Analysis and Replanning Tool, DART (Cross and Walker, 1994), to doautomated\nlogistics planning and scheduling for transportation. This involved up to 50,000 vehicles,\ncargo, and people at a time, and had to account for starting points, destinations, routes, and\nconflict resolution among all parameters. The AI planning techniques generated in hours\na plan that would have taken weeks with older methods. The Defense Advanced Research\nProject Agency (DARPA) stated that this single application more than paid back DARPA\u2019s\n30-yearinvestment inAI.\nRobotics: The iRobot Corporation has sold overtwomillion Roomba robotic vacuum\ncleaners for home use. The company also deploys the more rugged PackBot to Iraq and\nAfghanistan, where it is used to handle hazardous materials, clear explosives, and identify\nthelocationofsnipers.\nMachine Translation: A computer program automatically translates from Arabic to\nEnglish, allowing an English speaker to see the headline \u201cArdogan Confirms That Turkey\nWould Not Accept Any Pressure, Urging Them to Recognize Cyprus.\u201d The program uses a\nstatisticalmodelbuiltfromexamplesofArabic-to-English translationsandfromexamplesof\nEnglishtexttotalingtwotrillionwords(Brants etal.,2007). Noneofthecomputerscientists\nontheteamspeakArabic,buttheydounderstand statisticsandmachinelearningalgorithms.\nThese are just a few examples of artificial intelligence systems that exist today. Not\nmagic or science fiction\u2014but rather science, engineering, and mathematics, to which this\nbookprovidesanintroduction.\n1.5 SUMMARY\nThis chapter defines AI and establishes the cultural background against which it has devel-\noped. Someoftheimportantpointsareasfollows:\n\u2022 Differentpeopleapproach AIwithdifferent goalsinmind. Twoimportant questions to\naskare: Areyouconcerned withthinking orbehavior? Doyou wanttomodelhumans\norworkfromanidealstandard? 30 Chapter 1. Introduction\n\u2022 In this book, we adopt the view that intelligence is concerned mainly with rational\naction. Ideally, an intelligent agent takes the best possible action in a situation. We\nstudytheproblem ofbuilding agentsthatareintelligent inthissense.\n\u2022 Philosophers (going back to 400 B.C.) made AI conceivable by considering the ideas\nthatthemindisinsomewayslikeamachine,thatitoperatesonknowledgeencodedin\nsomeinternal language, andthatthought canbeusedtochoosewhatactionstotake.\n\u2022 Mathematiciansprovidedthetoolstomanipulatestatementsoflogicalcertaintyaswell\nasuncertain, probabilistic statements. Theyalso setthe groundwork forunderstanding\ncomputation andreasoning aboutalgorithms.\n\u2022 Economists formalized the problem of making decisions that maximize the expected\noutcometothedecisionmaker.\n\u2022 Neuroscientistsdiscoveredsomefactsabouthowthebrainworksandthewaysinwhich\nitissimilartoanddifferentfromcomputers.\n\u2022 Psychologistsadoptedtheideathathumansandanimalscanbeconsideredinformation-\nprocessing machines. Linguists showedthatlanguage usefitsintothismodel.\n\u2022 Computerengineers provided the ever-more-powerful machines that make AI applica-\ntionspossible.\n\u2022 Controltheorydealswithdesigning devicesthatactoptimally onthebasisoffeedback\nfrom the environment. Initially, the mathematical tools of control theory were quite\ndifferentfromAI,butthefieldsarecomingclosertogether.\n\u2022 ThehistoryofAIhashadcyclesofsuccess,misplacedoptimism,andresultingcutbacks\nin enthusiasm and funding. There have also been cycles of introducing new creative\napproaches andsystematically refiningthebestones.\n\u2022 AIhasadvancedmorerapidlyinthepastdecadebecauseofgreateruseofthescientific\nmethodinexperimenting withandcomparing approaches.\n\u2022 Recentprogressinunderstanding thetheoretical basisfor intelligence hasgonehandin\nhand with improvements in the capabilities of real systems. The subfields of AI have\nbecomemoreintegrated, andAIhasfoundcommonground withotherdisciplines.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nThemethodologicalstatusofartificialintelligenceisinvestigatedinTheSciencesoftheArtifi-\ncial,byHerbSimon(1981),whichdiscussesresearchareasconcernedwithcomplexartifacts.\nIt explains how AI can be viewed as both science and mathematics. Cohen (1995) gives an\noverviewofexperimental methodology withinAI.\nTheTuringTest(Turing, 1950) isdiscussed byShieber(1994), whoseverely criticizes\nthe usefulness of its instantiation in the Loebner Prize competition, and by Ford and Hayes\n(1995), whoarguethatthetestitselfisnothelpfulforAI.Bringsjord (2008) givesadvicefor\na Turing Test judge. Shieber (2004) and Epstein et al. (2008) collect a number of essays on\nthe Turing Test. Artificial Intelligence: The Very Idea, by John Haugeland (1985), gives a Exercises 31\nreadable account of the philosophical and practical problems of AI. Significant early papers\ninAIareanthologizedinthecollectionsbyWebberandNilsson(1981)andbyLuger(1995).\nThe Encyclopedia of AI (Shapiro, 1992) contains survey articles on almost every topic in\nAI, as does Wikipedia. These articles usually provide a good entry point into the research\nliterature on each topic. An insightful and comprehensive history of AI is given by Nils\nNillson(2009), oneoftheearlypioneers ofthefield.\nThe most recent work appears in the proceedings of the majorAI conferences: the bi-\nennial International JointConference onAI(IJCAI),theannual European Conference onAI\n(ECAI),andtheNationalConferenceonAI,moreoftenknownasAAAI,afteritssponsoring\norganization. The major journals for general AI are Artificial Intelligence, Computational\nIntelligence, theIEEETransactions onPatternAnalysis andMachine Intelligence, IEEEIn-\ntelligentSystems,andtheelectronicJournalofArtificialIntelligenceResearch. Therearealso\nmany conferences and journals devoted to specific areas, which we cover in the appropriate\nchapters. The main professional societies forAI are the American Association for Artificial\nIntelligence (AAAI), the ACM Special Interest Group in Artificial Intelligence (SIGART),\nand the Society for Artificial Intelligence and Simulation of Behaviour (AISB). AAAI\u2019s AI\nMagazine contains manytopical andtutorial articles, anditsWebsite, aaai.org,contains\nnews,tutorials, andbackground information.\nEXERCISES\nTheseexercisesareintendedtostimulatediscussion, andsomemightbesetastermprojects.\nAlternatively, preliminary attempts can be made now, and these attempts can be reviewed\nafterthecompletion ofthebook.\n1.1 Define in your own words: (a) intelligence, (b) artificial intelligence, (c) agent, (d)\nrationality, (e)logicalreasoning.\n1.2 Read Turing\u2019s original paper on AI (Turing, 1950). In the paper, he discusses several\nobjectionstohisproposedenterpriseandhistestforintelligence. Whichobjectionsstillcarry\nweight? Are his refutations valid? Can you think of new objections arising from develop-\nments since he wrotethe paper? Inthe paper, hepredicts that, bythe year2000, acomputer\nwill have a 30% chance of passing a five-minute Turing Test with an unskilled interrogator.\nWhatchancedoyouthinkacomputerwouldhavetoday? Inanother50years?\n1.3 Arereflexactions(suchasflinchingfromahotstove)rational? Aretheyintelligent?\n1.4 Suppose weextend Evans\u2019s ANALOGY program so that it can score 200 on a standard\nIQtest. Wouldwethenhaveaprogram moreintelligent thanahuman? Explain.\n1.5 The neural structure of the sea slug Aplysia has been widely studied (first by Nobel\nLaureate Eric Kandel) because it has only about 20,000 neurons, most of them large and\neasily manipulated. Assuming that the cycle time foran Aplysia neuron is roughly the same\nas fora human neuron, how does the computational power, in terms of memory updates per\nsecond, comparewiththehigh-end computerdescribed inFigure1.3? 32 Chapter 1. Introduction\n1.6 How could introspection\u2014reporting on one\u2019s inner thoughts\u2014be inaccurate? Could I\nbewrongaboutwhatI\u2019mthinking? Discuss.\n1.7 Towhatextentarethefollowingcomputersystemsinstances ofartificialintelligence:\n\u2022 Supermarketbarcodescanners.\n\u2022 Websearchengines.\n\u2022 Voice-activated telephone menus.\n\u2022 Internetroutingalgorithms thatrespond dynamically tothestateofthenetwork.\n1.8 Many of the computational models of cognitive activities that have been proposed in-\nvolve quite complex mathematical operations, such asconvolving animage withaGaussian\norfindingaminimumoftheentropyfunction. Mosthumans(andcertainlyallanimals)never\nlearn this kind of mathematics at all, almost no one learns it before college, and almost no\none can compute the convolution of a function with a Gaussian in their head. What sense\ndoes it make to say that the \u201cvision system\u201d is doing this kind of mathematics, whereas the\nactualpersonhasnoideahowtodoit?\n1.9 Whywouldevolution tendtoresultinsystemsthatactrationally? Whatgoalsaresuch\nsystemsdesigned toachieve?\n1.10 IsAIascience, orisitengineering? Orneitherorboth? Explain.\n1.11 \u201cSurely computers cannot be intelligent\u2014they can do only what their programmers\ntellthem.\u201d Isthelatterstatement true,anddoesitimplytheformer?\n1.12 \u201cSurely animals cannot be intelligent\u2014they can do only what their genes tell them.\u201d\nIsthelatterstatementtrue,anddoesitimplytheformer?\n1.13 \u201cSurelyanimals,humans,andcomputerscannotbeintelligent\u2014theycandoonlywhat\ntheirconstituent atomsaretoldtodobythelawsofphysics.\u201d Isthelatterstatementtrue,and\ndoesitimplytheformer?\n1.14 Examine the AI literature to discover whether the following tasks can currently be\nsolvedbycomputers:\na. Playingadecentgameoftabletennis(Ping-Pong).\nb. DrivinginthecenterofCairo,Egypt.\nc. DrivinginVictorville, California.\nd. Buyingaweek\u2019sworthofgroceries atthemarket.\ne. Buyingaweek\u2019sworthofgroceries ontheWeb.\nf. Playingadecentgameofbridgeatacompetitive level.\ng. Discoveringandprovingnewmathematical theorems.\nh. Writinganintentionally funnystory.\ni. Givingcompetentlegaladviceinaspecialized areaoflaw.\nj. Translatingspoken Englishintospoken Swedishinrealtime.\nk. Performingacomplexsurgical operation. Exercises 33\nForthecurrently infeasible tasks, trytofindoutwhatthedifficultiesareandpredict when,if\never,theywillbeovercome.\n1.15 Varioussubfields ofAIhaveheldcontests bydefining astandard taskandinviting re-\nsearchers to do their best. Examples include the DARPA Grand Challenge for robotic cars,\nTheInternationalPlanningCompetition,theRobocuproboticsoccerleague,theTRECinfor-\nmation retrieval event, and contests in machine translation, speech recognition. Investigate\nfiveofthesecontests,anddescribetheprogressmadeovertheyears. Towhatdegreehavethe\ncontestsadvancedtoestateoftheartinAI?Dowhatdegreedotheyhurtthefieldbydrawing\nenergyawayfromnewideas? 2\nINTELLIGENT AGENTS\nIn which we discuss the nature of agents, perfect or otherwise, the diversity of\nenvironments, andtheresulting menagerieofagenttypes.\nChapter 1 identified the concept of rational agents as central to our approach to artificial\nintelligence. Inthischapter,wemakethisnotionmoreconcrete. Wewillseethattheconcept\nofrationality canbeappliedtoawidevarietyofagentsoperating inanyimaginable environ-\nment. Ourplanin thisbook istousethis concept todevelop asmallsetof design principles\nforbuilding successful agents\u2014systems thatcanreasonably becalled intelligent.\nWe begin by examining agents, environments, and the coupling between them. The\nobservationthatsomeagentsbehavebetterthanothersleadsnaturallytotheideaofarational\nagent\u2014one that behaves as well as possible. How well an agent can behave depends on\nthe nature of the environment; some environments are more difficult than others. We give a\ncrude categorization of environments and show how properties of an environment influence\nthedesignofsuitableagentsforthatenvironment. Wedescribe anumberofbasic\u201cskeleton\u201d\nagentdesigns, whichwefleshoutintherestofthebook.\n2.1 AGENTS AND ENVIRONMENTS\nAnagentisanything thatcanbeviewedasperceiving itsenvironmentthrough sensors and\nENVIRONMENT\nactinguponthatenvironmentthrough actuators. ThissimpleideaisillustratedinFigure2.1.\nSENSOR\nAhumanagenthaseyes,ears,andotherorgansforsensorsandhands,legs,vocaltract,andso\nACTUATOR\non for actuators. A robotic agent might have cameras and infrared range finders for sensors\nand various motors for actuators. A software agent receives keystrokes, file contents, and\nnetwork packets as sensory inputs and acts on the environment by displaying on the screen,\nwritingfiles,andsending networkpackets.\nWeusethetermpercepttorefertotheagent\u2019sperceptualinputsatanygiveninstant. An\nPERCEPT\nagent\u2019sperceptsequenceisthecompletehistoryofeverythingtheagenthaseverperceived.\nPERCEPTSEQUENCE\nIngeneral, an agent\u2019s choice ofaction atany given instant candepend on theentire percept\nsequenceobservedtodate,butnotonanything ithasn\u2019tperceived. Byspecifying theagent\u2019s\nchoice of action for every possible percept sequence, we have said more or less everything\n34 Section2.1. AgentsandEnvironments 35\nAgent\nSensors\nPercepts\n?\nActions\nActuators\nFigure2.1 Agentsinteractwithenvironmentsthroughsensorsandactuators.\nthere is to say about the agent. Mathematically speaking, we say that an agent\u2019s behavior is\ndescribed bytheagentfunctionthatmapsanygivenperceptsequence toanaction.\nAGENTFUNCTION\nWecan imagine tabulating the agent function that describes any given agent; formost\nagents, this would be a very large table\u2014infinite, in fact, unless we place a bound on the\nlengthofperceptsequenceswewanttoconsider. Givenanagenttoexperimentwith,wecan,\nin principle, construct this table by trying out all possible percept sequences and recording\nwhichactionstheagentdoesinresponse.1 Thetableis,ofcourse,anexternalcharacterization\nof the agent. Internally, the agent function for an artificial agent will be implemented by an\nagent program. It is important to keep these two ideas distinct. The agent function is an\nAGENTPROGRAM\nabstract mathematical description; the agent program is a concrete implementation, running\nwithinsomephysicalsystem.\nTo illustrate these ideas, we use a very simple example\u2014the vacuum-cleaner world\nshown in Figure 2.2. This world is so simple that we can describe everything that happens;\nit\u2019salsoamade-upworld,sowecaninventmanyvariations. Thisparticularworldhasjusttwo\nlocations: squares A and B. The vacuum agent perceives which square it is in and whether\nthere is dirt in the square. It can choose to move left, move right, suck up the dirt, or do\nnothing. One very simple agent function is the following: if the current square is dirty, then\nsuck;otherwise,movetotheothersquare. Apartialtabulationofthisagentfunctionisshown\ninFigure2.3andanagentprogram thatimplementsitappears inFigure2.8onpage48.\nLookingatFigure2.3,weseethatvarious vacuum-world agentscanbedefinedsimply\nbyfillingintheright-handcolumninvariousways. Theobviousquestion,then,isthis: What\nis the right way to fill out the table? In other words, what makes an agent good or bad,\nintelligent orstupid? Weanswerthesequestions inthenextsection.\n1 Iftheagentusessomerandomizationtochoose itsactions, thenwewouldhavetotryeachsequence many\ntimestoidentifytheprobabilityofeachaction. Onemightimaginethatactingrandomlyisrathersilly,butwe\nshowlaterinthischapterthatitcanbeveryintelligent.\nEnvironment 36 Chapter 2. Intelligent Agents\nA B\nFigure2.2 Avacuum-cleanerworldwithjusttwolocations.\nPerceptsequence Action\n[A,Clean] Right\n[A,Dirty] Suck\n[B,Clean] Left\n[B,Dirty] Suck\n[A,Clean],[A,Clean] Right\n[A,Clean],[A,Dirty] Suck\n. .\n. .\n. .\n[A,Clean],[A,Clean],[A,Clean] Right\n[A,Clean],[A,Clean],[A,Dirty] Suck\n. .\n. .\n. .\nFigure 2.3 Partial tabulation of a simple agent function for the vacuum-cleaner world\nshowninFigure2.2.\nBeforeclosingthissection,weshouldemphasizethatthenotionofanagentismeantto\nbe a tool for analyzing systems, not an absolute characterization that divides the world into\nagents and non-agents. One could view a hand-held calculator as an agent that chooses the\naction of displaying \u201c4\u201d when given the percept sequence \u201c2 + 2 =,\u201d but such an analysis\nwouldhardly aidourunderstanding ofthecalculator. Inasense, allareasofengineering can\nbe seen as designing artifacts that interact with the world; AI operates at (what the authors\nconsider to be) the most interesting end of the spectrum, where the artifacts have significant\ncomputational resources andthetaskenvironment requires nontrivial decision making.\n2.2 GOOD BEHAVIOR: THE CONCEPT OF RATIONALITY\nArational agent is one that does the right thing\u2014conceptually speaking, every entry in the\nRATIONALAGENT\ntable for the agent function is filled out correctly. Obviously, doing the right thing is better\nthandoingthewrongthing,butwhatdoesitmeantodotherightthing? Section2.2. GoodBehavior: TheConceptofRationality 37\nWe answer this age-old question in an age-old way: by considering the consequences\nof the agent\u2019s behavior. When an agent is plunked down in an environment, it generates a\nsequenceofactionsaccordingtotheperceptsitreceives. Thissequenceofactionscausesthe\nenvironment to go through a sequence of states. If the sequence is desirable, then the agent\nPERFORMANCE has performed well. This notion of desirability is captured by aperformance measure that\nMEASURE\nevaluatesanygivensequence ofenvironment states.\nNotice that we said environment states, not agent states. If we define success in terms\nof agent\u2019s opinion ofits own performance, an agent could achieve perfect rationality simply\nbydeluding itselfthatitsperformance wasperfect. Humanagentsinparticular arenotorious\nfor \u201csour grapes\u201d\u2014believing they did not really want something (e.g., a Nobel Prize) after\nnotgettingit.\nObviously,thereisnotonefixedperformancemeasureforalltasksandagents;typically,\nadesigner willdevise one appropriate tothe circumstances. Thisis notas easy asit sounds.\nConsider, for example, the vacuum-cleaner agent from the preceding section. We might\nproposetomeasureperformancebytheamountofdirtcleaned upinasingleeight-hourshift.\nWith a rational agent, of course, what you ask for is what you get. A rational agent can\nmaximizethisperformance measurebycleaning upthedirt,thendumpingitallonthefloor,\nthencleaningitupagain,andsoon. Amoresuitableperformancemeasurewouldrewardthe\nagentforhavingacleanfloor. Forexample,onepointcouldbeawardedforeachcleansquare\nat each time step (perhaps with a penalty for electricity consumed and noise generated). As\na general rule, it is better to design performance measures according to what one actually\nwantsintheenvironment, ratherthanaccording tohowonethinkstheagentshouldbehave.\nEvenwhentheobviouspitfallsareavoided,thereremainsomeknottyissuestountangle.\nFor example, the notion of \u201cclean floor\u201d in the preceding paragraph is based on average\ncleanliness over time. Yet the same average cleanliness can be achieved by two different\nagents,oneofwhichdoesamediocrejoballthetimewhiletheothercleansenergetically but\ntakeslongbreaks. Whichispreferablemightseemtobeafinepointofjanitorialscience, but\nin fact it is a deep philosophical question with far-reaching implications. Which is better\u2014\na reckless life of highs and lows, or a safe but humdrum existence? Which is better\u2014an\neconomy where everyone lives in moderate poverty, or one in which some live in plenty\nwhileothersareverypoor? Weleavethesequestions asanexerciseforthediligentreader.\n2.2.1 Rationality\nWhatisrationalatanygiventimedepends onfourthings:\n\u2022 Theperformance measurethatdefinesthecriterionofsuccess.\n\u2022 Theagent\u2019spriorknowledge oftheenvironment.\n\u2022 Theactionsthattheagentcanperform.\n\u2022 Theagent\u2019sperceptsequence todate.\nDEFINITIONOFA Thisleadstoadefinitionofarationalagent:\nRATIONALAGENT\nForeachpossible perceptsequence, a rationalagentshouldselect anaction thatis ex-\npectedtomaximizeitsperformancemeasure,giventheevidenceprovidedbythepercept\nsequenceandwhateverbuilt-inknowledgetheagenthas. 38 Chapter 2. Intelligent Agents\nConsiderthesimplevacuum-cleaner agent thatcleans asquare ifitisdirty andmovestothe\nothersquare ifnot; thisistheagentfunction tabulated inFigure2.3. Isthisarational agent?\nThat depends! First, we need to say what the performance measure is, what is known about\ntheenvironment, andwhatsensorsandactuators theagenthas. Letusassumethefollowing:\n\u2022 The performance measure awards one point for each clean square at each time step,\novera\u201clifetime\u201dof1000timesteps.\n\u2022 The \u201cgeography\u201d of the environment is known a priori (Figure 2.2) but the dirt distri-\nbutionandtheinitiallocationoftheagentarenot. Cleansquaresstaycleanandsucking\ncleans the current square. The Left and Right actions move the agent left and right\nexceptwhenthiswouldtaketheagentoutsidetheenvironment, inwhichcasetheagent\nremainswhereitis.\n\u2022 Theonlyavailable actionsareLeft,Right,andSuck.\n\u2022 Theagentcorrectly perceivesitslocation andwhetherthat location contains dirt.\nWe claim that under these circumstances the agent is indeed rational; its expected perfor-\nmanceisatleastashighasanyotheragent\u2019s. Exercise2.2asksyoutoprovethis.\nOne can see easily that the same agent would be irrational under different circum-\nstances. Forexample, once allthedirtiscleaned up, theagent willoscillate needlessly back\nandforth;iftheperformancemeasureincludesapenaltyofonepointforeachmovementleft\nor right, the agent will fare poorly. A better agent for this case would do nothing once it is\nsure that allthe squares are clean. Ifclean squares can become dirty again, the agent should\noccasionally check and re-clean them if needed. If the geography of the environment is un-\nknown, the agent will need to explore it rather than stick to squares A and B. Exercise 2.2\nasksyoutodesignagentsforthesecases.\n2.2.2 Omniscience, learning, andautonomy\nWe need to be careful to distinguish between rationality and omniscience. An omniscient\nOMNISCIENCE\nagent knows the actual outcome of its actions and can act accordingly; but omniscience is\nimpossible in reality. Consider the following example: I am walking along the Champs\nElyse\u00b4es one day and I see an old friend across the street. There is no traffic nearby and I\u2019m\nnot otherwise engaged, so, being rational, I start to cross the street. Meanwhile, at 33,000\nfeet, a cargo door falls off a passing airliner,2 and before I make it to the other side of the\nstreetIamflattened. WasIirrationaltocrossthestreet? Itisunlikelythatmyobituarywould\nread\u201cIdiotattemptstocrossstreet.\u201d\nThis example shows that rationality is not the same as perfection. Rationality max-\nimizes expected performance, while perfection maximizes actual performance. Retreating\nfrom a requirement of perfection is not just a question of being fair to agents. The point is\nthat if we expect an agent to do what turns out to be the best action after the fact, it will be\nimpossibletodesignanagenttofulfillthisspecification\u2014unlessweimprovetheperformance\nofcrystal ballsortimemachines.\n2 SeeN.Henderson,\u201cNewdoorlatchesurgedforBoeing747jumbojets,\u201dWashingtonPost,August24,1989. Section2.2. GoodBehavior: TheConceptofRationality 39\nOur definition of rationality does not require omniscience, then, because the rational\nchoice depends only on the percept sequence to date. We must also ensure that we haven\u2019t\ninadvertently allowedtheagenttoengageindecidedly underintelligent activities. Forexam-\nple,ifanagentdoesnotlookbothwaysbeforecrossingabusyroad,thenitsperceptsequence\nwill not tell it that there is a large truck approaching at high speed. Does our definition of\nrationality saythatit\u2019snowOK tocross theroad? Farfromit! First,itwould notberational\ntocross the road given thisuninformative percept sequence: the riskofaccident from cross-\ningwithoutlookingistoogreat. Second,arationalagentshouldchoosethe\u201clooking\u201d action\nbefore stepping into the street, because looking helps maximize the expected performance.\nDoing actions in order to modify future percepts\u2014sometimes called information gather-\nINFORMATION ing\u2014is an important part of rationality and is covered in depth in Chapter 16. A second\nGATHERING\nexampleofinformationgathering isprovidedbythe exploration thatmustbeundertaken by\nEXPLORATION\navacuum-cleaning agentinaninitially unknownenvironment.\nOurdefinition requires arational agentnotonlytogatherinformation butalsotolearn\nLEARNING\nas much as possible from what it perceives. The agent\u2019s initial configuration could reflect\nsome prior knowledge of the environment, but as the agent gains experience this may be\nmodified and augmented. There are extreme cases in which the environment is completely\nknown a priori. In such cases, the agent need not perceive orlearn; it simply acts correctly.\nOfcourse,suchagentsarefragile. Considerthelowlydungbeetle. Afterdiggingitsnestand\nlayingitseggs,itfetchesaballofdungfromanearbyheaptoplugtheentrance. Iftheballof\ndung isremoved from itsgrasp en route, the beetle continues its task and pantomimes plug-\nging the nest withthe nonexistent dung ball, never noticing that it is missing. Evolution has\nbuiltanassumption intothebeetle\u2019sbehavior, andwhenitisviolated, unsuccessful behavior\nresults. Slightly more intelligent is the sphex wasp. The female sphex will dig a burrow, go\nout and sting a caterpillar and drag it to the burrow, enter the burrow again to check all is\nwell,dragthecaterpillarinside,andlayitseggs. Thecaterpillarservesasafoodsourcewhen\nthe eggs hatch. So far so good, but if an entomologist moves the caterpillar a few inches\naway while the sphex isdoing the check, it willrevert to the \u201cdrag\u201d step ofits plan and will\ncontinuetheplanwithoutmodification,evenafterdozensofcaterpillar-movinginterventions.\nThesphexisunabletolearnthatitsinnateplanisfailing, andthuswillnotchange it.\nTo the extent that an agent relies on the prior knowledge of its designer rather than\nAUTONOMY on its own percepts, we say that the agent lacks autonomy. A rational agent should be\nautonomous\u2014it should learn what it can to compensate forpartial or incorrect prior knowl-\nedge. Forexample,avacuum-cleaningagentthatlearnstoforeseewhereandwhenadditional\ndirt will appear will do better than one that does not. As a practical matter, one seldom re-\nquires complete autonomy from the start: when the agent has had little or no experience, it\nwould have to act randomly unless the designer gave some assistance. So, just as evolution\nprovidesanimalswithenoughbuilt-inreflexestosurvivelongenoughtolearnforthemselves,\nit would be reasonable to provide an artificial intelligent agent with some initial knowledge\nas well as an ability to learn. After sufficient experience of its environment, the behavior\nof a rational agent can become effectively independent of its prior knowledge. Hence, the\nincorporation of learning allows one to design a single rational agent that will succeed in a\nvastvarietyofenvironments. 40 Chapter 2. Intelligent Agents\n2.3 THE NATURE OF ENVIRONMENTS\nNow that we have a definition of rationality, we are almost ready to think about building\nrational agents. First, however, we must think about task environments, which are essen-\nTASKENVIRONMENT\ntiallythe\u201cproblems\u201d towhichrational agentsarethe\u201csolutions.\u201d Webeginbyshowinghow\nto specify a task environment, illustrating the process with a number of examples. We then\nshowthattask environments comeinavarietyofflavors. Theflavorofthetaskenvironment\ndirectlyaffectstheappropriate designfortheagentprogram.\n2.3.1 Specifying thetaskenvironment\nIn our discussion of the rationality of the simple vacuum-cleaner agent, we had to specify\ntheperformance measure, theenvironment, and theagent\u2019s actuators and sensors. Wegroup\nalltheseundertheheading ofthetaskenvironment. Fortheacronymically minded, wecall\nthisthePEAS(Performance, Environment, Actuators, Sensors)description. Indesigning an\nPEAS\nagent,thefirststepmustalwaysbetospecify thetaskenvironment asfullyaspossible.\nThevacuum worldwasasimpleexample;letusconsider amorecomplexproblem: an\nautomated taxi driver. We should point out, before the reader becomes alarmed, that a fully\nautomatedtaxiiscurrentlysomewhatbeyondthecapabilitiesofexistingtechnology. (page28\ndescribes anexisting driving robot.) Thefulldriving task isextremely open-ended. Thereis\nnolimittothenovelcombinations ofcircumstances thatcanarise\u2014another reasonwechose\nit as a focus for discussion. Figure 2.4 summarizes the PEAS description for the taxi\u2019s task\nenvironment. Wediscuss eachelementinmoredetailinthefollowingparagraphs.\nAgentType Performance Environment Actuators Sensors\nMeasure\nTaxidriver Safe,fast,legal, Roads,other Steering, Cameras,sonar,\ncomfortabletrip, traffic, accelerator, speedometer,\nmaximizeprofits pedestrians, brake,signal, GPS,odometer,\ncustomers horn,display accelerometer,\nenginesensors,\nkeyboard\nFigure2.4 PEASdescriptionofthetaskenvironmentforanautomatedtaxi.\nFirst, what isthe performance measure towhich wewould like ourautomated driver\ntoaspire? Desirablequalities includegetting tothecorrect destination; minimizingfuelcon-\nsumptionandwearandtear;minimizingthetriptimeorcost; minimizingviolationsoftraffic\nlaws and disturbances to other drivers; maximizing safety and passenger comfort; maximiz-\ningprofits. Obviously, someofthesegoalsconflict, sotradeoffs willberequired.\nNext, what is the driving environment that the taxi will face? Any taxi driver must\ndeal with a variety of roads, ranging from rural lanes and urban alleys to 12-lane freeways.\nThe roads contain other traffic, pedestrians, stray animals, road works, police cars, puddles, Section2.3. TheNatureofEnvironments 41\nandpotholes. Thetaximustalsointeractwithpotentialandactualpassengers. Therearealso\nsome optional choices. The taxi might need to operate in Southern California, where snow\nis seldom aproblem, orin Alaska, where it seldom isnot. Itcould always be driving on the\nright, orwemightwantittobeflexibleenough todriveontheleftwheninBritain orJapan.\nObviously, themorerestricted theenvironment, theeasier thedesignproblem.\nTheactuatorsforanautomatedtaxiincludethoseavailable toahumandriver: control\novertheengine through theaccelerator andcontrol oversteering andbraking. Inaddition, it\nwill need output to a display screen or voice synthesizer to talk back to the passengers, and\nperhapssomewaytocommunicatewithothervehicles, politely orotherwise.\nThe basic sensors for the taxi will include one or more controllable video cameras so\nthat it can see the road; it might augment these with infrared or sonar sensors to detect dis-\ntancestoothercarsandobstacles. Toavoidspeedingtickets,thetaxishouldhaveaspeedome-\nter,andtocontrolthevehicleproperly, especiallyoncurves,itshouldhaveanaccelerometer.\nTodetermine themechanical state ofthevehicle, itwillneed theusual array ofengine, fuel,\nand electrical system sensors. Like many human drivers, it might want a global positioning\nsystem (GPS) so that it doesn\u2019t get lost. Finally, it will need a keyboard or microphone for\nthepassengertorequest adestination.\nIn Figure 2.5, we have sketched the basic PEAS elements for a number of additional\nagenttypes. FurtherexamplesappearinExercise2.4. Itmaycomeasasurprisetosomeread-\ners that our list of agent types includes some programs that operate in the entirely artificial\nenvironmentdefinedbykeyboardinputandcharacteroutputonascreen. \u201cSurely,\u201donemight\nsay,\u201cthisisnotarealenvironment, isit?\u201d Infact,whatmattersisnotthedistinction between\n\u201creal\u201dand\u201cartificial\u201denvironments, butthecomplexityof therelationship amongthebehav-\nior of the agent, the percept sequence generated by the environment, and the performance\nmeasure. Some\u201creal\u201denvironments areactuallyquitesimple. Forexample,arobotdesigned\ntoinspectpartsastheycomebyonaconveyorbeltcanmakeuse ofanumberofsimplifying\nassumptions: that the lighting is always just so, that the only thing onthe conveyor belt will\nbepartsofakindthatitknowsabout,andthatonlytwoactions(acceptorreject)arepossible.\nIncontrast, somesoftwareagents(orsoftwarerobotsor softbots)existinrich,unlim-\nSOFTWAREAGENT\niteddomains. ImagineasoftbotWebsiteoperatordesignedtoscanInternetnewssourcesand\nSOFTBOT\nshow the interesting items to its users, while selling advertising space to generate revenue.\nTo do well, that operator will need some natural language processing abilities, it will need\nto learn what each user and advertiser is interested in, and it will need to change its plans\ndynamically\u2014for example, when the connection for one news source goes down or when a\nnew one comes online. The Internet is an environment whose complexity rivals that of the\nphysicalworldandwhoseinhabitants includemanyartificialandhumanagents.\n2.3.2 Properties oftaskenvironments\nThe range of task environments that might arise in AI is obviously vast. We can, however,\nidentify a fairly small number of dimensions along which task environments can be catego-\nrized. These dimensions determine, to a large extent, the appropriate agent design and the\napplicability of each of the principal families of techniques foragent implementation. First, 42 Chapter 2. Intelligent Agents\nAgentType Performance Environment Actuators Sensors\nMeasure\nMedical Healthypatient, Patient,hospital, Displayof Keyboardentry\ndiagnosissystem reducedcosts staff questions,tests, ofsymptoms,\ndiagnoses, findings,patient\u2019s\ntreatments, answers\nreferrals\nSatelliteimage Correctimage Downlinkfrom Displayofscene Colorpixel\nanalysissystem categorization orbitingsatellite categorization arrays\nPart-picking Percentageof Conveyorbelt Jointedarmand Camera,joint\nrobot partsincorrect withparts;bins hand anglesensors\nbins\nRefinery Purity,yield, Refinery, Valves,pumps, Temperature,\ncontroller safety operators heaters,displays pressure,\nchemicalsensors\nInteractive Student\u2019sscore Setofstudents, Displayof Keyboardentry\nEnglishtutor ontest testingagency exercises,\nsuggestions,\ncorrections\nFigure2.5 ExamplesofagenttypesandtheirPEASdescriptions.\nwelistthedimensions, thenweanalyzeseveraltaskenvironments toillustrate theideas. The\ndefinitionshereareinformal;laterchaptersprovidemoreprecisestatementsandexamplesof\neachkindofenvironment.\nFully observable vs. partially observable: If an agent\u2019s sensors give it access to the\nFULLYOBSERVABLE\nPARTIALLY complete state of the environment at each point in time, then we say that the task environ-\nOBSERVABLE\nment is fully observable. A task environment is effectively fully observable if the sensors\ndetect all aspects that are relevant tothe choice of action; relevance, inturn, depends on the\nperformance measure. Fullyobservable environments areconvenient because theagentneed\nnotmaintainanyinternal statetokeeptrackoftheworld. An environment mightbepartially\nobservable because of noisy and inaccurate sensors or because parts of the state are simply\nmissing from the sensor data\u2014for example, a vacuum agent with only a local dirt sensor\ncannottellwhetherthereisdirtinothersquares,andanautomatedtaxicannotseewhatother\ndrivers are thinking. If the agent has no sensors at all then the environment is unobserv-\nable. One might think that in such cases the agent\u2019s plight is hopeless, but, as wediscuss in\nUNOBSERVABLE\nChapter4,theagent\u2019s goalsmaystillbeachievable, sometimeswithcertainty.\nSingleagent vs. multiagent: Thedistinction between single-agent andmultiagent en-\nSINGLEAGENT\nMULTIAGENT Section2.3. TheNatureofEnvironments 43\nvironments may seem simple enough. Forexample, anagent solving acrossword puzzle by\nitself is clearly in a single-agent environment, whereas an agent playing chess is in a two-\nagentenvironment. Thereare,however, somesubtle issues. First,wehavedescribed howan\nentity may be viewed as an agent, but we have not explained which entities must be viewed\nas agents. Does an agent A (the taxi driver for example) have to treat an object B (another\nvehicle)asanagent,orcanitbetreatedmerelyasanobjectbehavingaccordingtothelawsof\nphysics, analogous towavesatthebeach orleaves blowing inthewind? Thekeydistinction\niswhetherB\u2019sbehaviorisbestdescribedasmaximizingaperformancemeasurewhosevalue\ndepends on agent A\u2019s behavior. For example, in chess, the opponent entity B is trying to\nmaximize its performance measure, which, by the rules of chess, minimizes agent A\u2019s per-\nformancemeasure. Thus,chessisa competitivemultiagent environment. Inthetaxi-driving\nCOMPETITIVE\nenvironment, on the other hand, avoiding collisions maximizes the performance measure of\nall agents, so it is a partially cooperative multiagent environment. It is also partially com-\nCOOPERATIVE\npetitive because, for example, only one car can occupy a parking space. The agent-design\nproblems inmultiagent environments areoftenquite different fromthose insingle-agent en-\nvironments;forexample, communicationoftenemergesasarationalbehaviorinmultiagent\nenvironments; insomecompetitive environments, randomizedbehaviorisrational because\nitavoids thepitfallsofpredictability.\nDeterministic vs. stochastic. If the next state of the environment is completely deter-\nDETERMINISTIC\nminedbythecurrentstateandtheactionexecutedbytheagent,thenwesaytheenvironment\nSTOCHASTIC\nisdeterministic;otherwise,itisstochastic. Inprinciple,anagentneednotworryaboutuncer-\ntainty in a fully observable, deterministic environment. (In our definition, we ignore uncer-\ntainty that arises purely from the actions of other agents in a multiagent environment; thus,\na game can be deterministic even though each agent may be unable to predict the actions of\nthe others.) If the environment is partially observable, however, then it could appear to be\nstochastic. Most real situations are so complex that it is impossible to keep track of all the\nunobserved aspects;forpracticalpurposes, theymustbetreatedasstochastic. Taxidrivingis\nclearly stochastic inthissense, because one canneverpredict thebehavior oftrafficexactly;\nmoreover, one\u2019s tires blow out and one\u2019s engine seizes up without warning. The vacuum\nworldaswedescribed itisdeterministic, butvariations caninclude stochastic elementssuch\nasrandomly appearing dirtandanunreliable suction mechanism (Exercise 2.13). Wesayan\nenvironment is uncertain if it is not fully observable or not deterministic. One final note:\nUNCERTAIN\nour use of the word \u201cstochastic\u201d generally implies that uncertainty about outcomes is quan-\ntified in terms of probabilities; a nondeterministic environment is one in which actions are\nNONDETERMINISTIC\ncharacterized by their possible outcomes, but no probabilities are attached to them. Nonde-\nterministic environment descriptions are usually associated with performance measures that\nrequiretheagenttosucceed forallpossible outcomesofitsactions.\nEpisodic vs. sequential: In an episodic task environment, the agent\u2019s experience is\nEPISODIC\ndividedintoatomicepisodes. Ineachepisodetheagentreceivesaperceptandthenperforms\nSEQUENTIAL\nasingle action. Crucially, the next episode does not depend on theactions taken inprevious\nepisodes. Many classification tasks are episodic. For example, an agent that has to spot\ndefective parts on an assembly line bases each decision on the current part, regardless of\nprevious decisions; moreover, the current decision doesn\u2019t affect whether the next part is 44 Chapter 2. Intelligent Agents\ndefective. In sequential environments, on the other hand, the current decision could affect\nallfuture decisions.3 Chessand taxidriving aresequential: inboth cases, short-term actions\ncan have long-term consequences. Episodic environments are much simpler than sequential\nenvironments becausetheagentdoesnotneedtothinkahead.\nStaticvs.dynamic: Iftheenvironment canchangewhileanagentisdeliberating, then\nSTATIC\nwesaytheenvironment isdynamic forthatagent;otherwise, itisstatic. Staticenvironments\nDYNAMIC\nareeasytodealwithbecausetheagentneednotkeeplookingattheworldwhileitisdeciding\non an action, nor need it worry about the passage of time. Dynamic environments, on the\nother hand, are continuously asking the agent what it wants to do; if it hasn\u2019t decided yet,\nthat counts as deciding to do nothing. If the environment itself does not change with the\npassage of time but the agent\u2019s performance score does, then we say the environment is\nsemidynamic. Taxidrivingisclearlydynamic: theothercarsandthetaxi itselfkeepmoving\nSEMIDYNAMIC\nwhilethe driving algorithm dithers about whattodonext. Chess, whenplayed withaclock,\nissemidynamic. Crosswordpuzzlesarestatic.\nDiscretevs.continuous: Thediscrete\/continuous distinction applies tothestateofthe\nDISCRETE\nenvironment, to the way time is handled, and to the percepts and actions of the agent. For\nCONTINUOUS\nexample, the chess environment has a finite number of distinct states (excluding the clock).\nChess also has a discrete set of percepts and actions. Taxi driving is a continuous-state and\ncontinuous-time problem: the speed and location ofthe taxi and ofthe other vehicles sweep\nthrough arangeofcontinuous values anddososmoothlyovertime. Taxi-driving actions are\nalso continuous (steering angles, etc.). Input from digital cameras is discrete, strictly speak-\ning,butistypically treatedasrepresenting continuously varyingintensities andlocations.\nKnownvs. unknown: Strictly speaking, this distinction refers not to the environment\nKNOWN\nitself but to the agent\u2019s (or designer\u2019s) state of knowledge about the \u201claws of physics\u201d of\nUNKNOWN\nthe environment. In a known environment, the outcomes (or outcome probabilities if the\nenvironmentisstochastic)forallactionsaregiven. Obviously,iftheenvironmentisunknown,\nthe agent will have to learn how it works in order to make good decisions. Note that the\ndistinction between known and unknown environments is not the same as the one between\nfully and partially observable environments. It is quite possible for a known environment\nto be partially observable\u2014for example, in solitaire card games, I know the rules but am\nstill unable to see the cards that have not yet been turned over. Conversely, an unknown\nenvironment can be fully observable\u2014in a new video game, the screen may show the entire\ngamestatebutIstilldon\u2019tknowwhatthebuttonsdountilItrythem.\nAs one might expect, the hardest case is partially observable, multiagent, stochastic,\nsequential,dynamic,continuous,andunknown. Taxidrivingishardinallthesesenses,except\nthatforthemostpartthedriver\u2019senvironmentisknown. Drivingarentedcarinanewcountry\nwithunfamiliargeography andtrafficlawsisalotmoreexciting.\nFigure 2.6 lists the properties of a number of familiar environments. Note that the\nanswers are not always cut and dried. For example, we describe the part-picking robot as\nepisodic, because itnormally considers each part in isolation. Butif oneday there isalarge\n3 Theword\u201csequential\u201disalsousedincomputerscienceastheantonymof\u201cparallel.\u201d Thetwomeaningsare\nlargelyunrelated. Section2.3. TheNatureofEnvironments 45\nTaskEnvironment Observable Agents Deterministic Episodic Static Discrete\nCrosswordpuzzle Fully Single Deterministic Sequential Static Discrete\nChesswithaclock Fully Multi Deterministic Sequential Semi Discrete\nPoker Partially Multi Stochastic Sequential Static Discrete\nBackgammon Fully Multi Stochastic Sequential Static Discrete\nTaxidriving Partially Multi Stochastic Sequential Dynamic Continuous\nMedicaldiagnosis Partially Single Stochastic Sequential Dynamic Continuous\nImageanalysis Fully Single Deterministic Episodic Semi Continuous\nPart-pickingrobot Partially Single Stochastic Episodic Dynamic Continuous\nRefinerycontroller Partially Single Stochastic Sequential Dynamic Continuous\nInteractiveEnglishtutor Partially Multi Stochastic Sequential Dynamic Discrete\nFigure2.6 Examplesoftaskenvironmentsandtheircharacteristics.\nbatchofdefectiveparts,therobotshouldlearnfromseveralobservations thatthedistribution\nof defects has changed, and should modify its behavior for subsequent parts. We have not\nincludeda\u201cknown\/unknown\u201dcolumnbecause,asexplainedearlier,thisisnotstrictlyaprop-\nerty of the environment. Forsome environments, such aschess and poker, it isquite easy to\nsupplytheagentwithfullknowledgeoftherules,butitisnonetheless interesting toconsider\nhowanagentmightlearntoplaythesegameswithoutsuchknowledge.\nSeveraloftheanswersinthetabledepend onhowthetaskenvironment isdefined. We\nhavelistedthemedical-diagnosis taskassingle-agentbecausethediseaseprocessinapatient\nis not profitably modeled as an agent; but a medical-diagnosis system might also have to\ndealwithrecalcitrantpatientsandskepticalstaff,sotheenvironmentcouldhaveamultiagent\naspect. Furthermore, medicaldiagnosis isepisodic ifoneconceives ofthetaskasselecting a\ndiagnosisgivenalistofsymptoms;theproblemissequentialifthetaskcanincludeproposing\na series of tests, evaluating progress over the course of treatment, and so on. Also, many\nenvironments are episodic at higher levels than the agent\u2019s individual actions. For example,\na chess tournament consists of a sequence of games; each game is an episode because (by\nand large) the contribution of the moves in one game to the agent\u2019s overall performance is\nnotaffected bythemovesinitsprevious game. Ontheotherhand, decision makingwithina\nsinglegameiscertainly sequential.\nThe code repository associated with this book (aima.cs.berkeley.edu) includes imple-\nmentationsofanumberofenvironments, togetherwithageneral-purpose environmentsimu-\nlatorthatplacesoneormoreagentsinasimulatedenvironment, observestheirbehaviorover\ntime, and evaluates them according to a given performance measure. Such experiments are\noftencarried outnotforasingle environment butformanyenvironments drawnfroman en-\nENVIRONMENT vironmentclass. Forexample,toevaluateataxidriverinsimulatedtraffic, wewouldwantto\nCLASS\nrunmanysimulations withdifferent traffic, lighting, and weatherconditions. Ifwedesigned\nthe agent for a single scenario, we might be able to take advantage of specific properties\nof the particular case but might not identify a good design for driving in general. For this 46 Chapter 2. Intelligent Agents\nENVIRONMENT reason, the code repository also includes an environment generator for each environment\nGENERATOR\nclassthatselectsparticularenvironments (withcertainlikelihoods) inwhichtoruntheagent.\nForexample,thevacuumenvironmentgeneratorinitializes thedirtpatternandagentlocation\nrandomly. We are then interested in the agent\u2019s average performance over the environment\nclass. A rational agent for a given environment class maximizes this average performance.\nExercises 2.8 to 2.13 take you through the process of developing an environment class and\nevaluating variousagentstherein.\n2.4 THE STRUCTURE OF AGENTS\nSofarwehavetalkedaboutagentsbydescribing behavior\u2014theactionthatisperformedafter\nany given sequence ofpercepts. Nowwemust bite the bullet and talk about how theinsides\nwork. The job of AI is to design an agent program that implements the agent function\u2014\nAGENTPROGRAM\nthe mapping from percepts to actions. We assume this program will run on some sort of\ncomputing devicewithphysicalsensorsandactuators\u2014we callthisthearchitecture:\nARCHITECTURE\nagent = architecture+program .\nObviously,theprogramwechoosehastobeonethatisappropriateforthearchitecture. Ifthe\nprogram isgoing torecommend actions like Walk,thearchitecture hadbetterhave legs. The\narchitecture might be just an ordinary PC, or it might be a robotic car with several onboard\ncomputers, cameras, and other sensors. In general, the architecture makes the percepts from\nthesensorsavailabletotheprogram,runstheprogram,andfeedstheprogram\u2019sactionchoices\nto the actuators asthey are generated. Most ofthis book is about designing agent programs,\nalthough Chapters24and25dealdirectly withthesensorsandactuators.\n2.4.1 Agentprograms\nThe agent programs that we design in this book all have the same skeleton: they take the\ncurrent percept as input from the sensors and return an action to the actuators.4 Notice the\ndifferencebetweentheagentprogram,whichtakesthecurrentperceptasinput,andtheagent\nfunction, which takes the entire percept history. The agent program takes just the current\nperceptasinputbecausenothingmoreisavailablefromtheenvironment;iftheagent\u2019sactions\nneedtodependontheentireperceptsequence, theagentwillhavetorememberthepercepts.\nWe describe the agent programs in the simple pseudocode language that is defined in\nAppendix B. (The online code repository contains implementations in real programming\nlanguages.) Forexample, Figure 2.7shows arathertrivial agent program thatkeeps track of\nthe percept sequence and then uses it to index into a table of actions to decide what to do.\nThe table\u2014an example of which is given for the vacuum world in Figure 2.3\u2014represents\nexplicitly the agent function that the agent program embodies. To build a rational agent in\n4 Thereareotherchoices fortheagent program skeleton; forexample, wecouldhavetheagent programsbe\ncoroutinesthatrunasynchronouslywiththeenvironment. Eachsuchcoroutinehasaninputandoutputportand\nconsistsofaloopthatreadstheinputportforperceptsandwritesactionstotheoutputport. Section2.4. TheStructureofAgents 47\nfunctionTABLE-DRIVEN-AGENT(percept)returnsanaction\npersistent: percepts,asequence,initiallyempty\ntable,atableofactions,indexedbyperceptsequences,initiallyfullyspecified\nappendpercept totheendofpercepts\naction\u2190LOOKUP(percepts,table)\nreturnaction\nFigure 2.7 The TABLE-DRIVEN-AGENT program is invoked for each new percept and\nreturnsanactioneachtime. Itretainsthecompleteperceptsequenceinmemory.\nthisway,weasdesignersmustconstructatablethatcontainstheappropriateactionforevery\npossible perceptsequence.\nIt is instructive to consider why the table-driven approach to agent construction is\ndoomed to failure. Let P be the set of possible percepts and let T be the lifet(cid:2)ime of the\nagent(thetotalnumberofperceptsitwillreceive). Thelookuptablewillcontain T |P|t\nt=1\nentries. Consider the automated taxi: the visual input from a single camera comes in at the\nrate of roughly 27 megabytes per second (30 frames per second, 640\u00d7480 pixels with 24\nbitsofcolorinformation). Thisgivesalookup table withover10250,000,000,000 entries foran\nhour\u2019s driving. Even the lookup table for chess\u2014a tiny, well-behaved fragment of the real\nworld\u2014would have at least 10150 entries. The daunting size of these tables (the number of\natoms in the observable universe is less than 1080) means that (a) no physical agent in this\nuniverse willhavethespacetostorethetable, (b)thedesignerwouldnothavetimetocreate\nthe table, (c)no agent could everlearn allthe right table entries from itsexperience, and (d)\neven if the environment is simple enough to yield a feasible table size, the designer still has\nnoguidance abouthowtofillinthetableentries.\nDespite all this, TABLE-DRIVEN-AGENT does do what we want: it implements the\ndesired agent function. The key challenge for AI is to find out how to write programs that,\nto the extent possible, produce rational behavior from a smallish program rather than from\na vast table. We have many examples showing that this can be done successfully in other\nareas: forexample,thehugetablesofsquarerootsusedbyengineersandschoolchildrenprior\nto the 1970s have now been replaced by a five-line program for Newton\u2019s method running\non electronic calculators. The question is, can AI do for general intelligent behavior what\nNewtondidforsquareroots? Webelievetheanswerisyes.\nIn the remainder of this section, we outline four basic kinds of agent programs that\nembodytheprinciples underlying almostallintelligent systems:\n\u2022 Simplereflexagents;\n\u2022 Model-based reflexagents;\n\u2022 Goal-basedagents; and\n\u2022 Utility-based agents.\nEach kind of agent program combines particular components in particular ways to generate\nactions. Section2.4.6explains ingeneral termshowtoconvert alltheseagents into learning 48 Chapter 2. Intelligent Agents\nfunctionREFLEX-VACUUM-AGENT([location,status])returnsanaction\nifstatus =Dirty thenreturnSuck\nelseiflocation =AthenreturnRight\nelseiflocation =B thenreturnLeft\nFigure2.8 Theagentprogramforasimplereflexagentinthetwo-statevacuumenviron-\nment.ThisprogramimplementstheagentfunctiontabulatedinFigure2.3.\nagentsthatcanimprovetheperformanceoftheircomponentssoasto generatebetteractions.\nFinally, Section2.4.7describes thevariety ofwaysinwhichthecomponents themselves can\nbe represented within the agent. This variety provides a major organizing principle for the\nfieldandforthebookitself.\n2.4.2 Simplereflex agents\nSIMPLEREFLEX Thesimplestkindofagentisthesimplereflexagent. Theseagentsselectactionsonthebasis\nAGENT\nofthecurrentpercept,ignoringtherestofthepercepthistory. Forexample,thevacuumagent\nwhose agent function is tabulated in Figure 2.3is asimple reflexagent, because its decision\nis based only on the current location and on whether that location contains dirt. An agent\nprogram forthisagentisshowninFigure2.8.\nNoticethatthevacuumagentprogramisverysmallindeedcomparedtothecorrespond-\ning table. The most obvious reduction comes from ignoring the percept history, which cuts\ndown the number of possibilities from 4T to just 4. A further, small reduction comes from\nthefactthatwhenthecurrent squareisdirty,theactiondoesnotdepend onthelocation.\nSimple reflex behaviors occur even in more complex environments. Imagine yourself\nasthedriveroftheautomatedtaxi. Ifthecarinfrontbrakes anditsbrakelightscomeon,then\nyou should notice this and initiate braking. In other words, some processing is done on the\nvisualinputtoestablishtheconditionwecall\u201cThecarinfrontisbraking.\u201d Then,thistriggers\nsome established connection in the agent program to the action \u201cinitiate braking.\u201d We call\nCONDITION\u2013ACTION suchaconnection acondition\u2013action rule,5 writtenas\nRULE\nifcar-in-front-is-braking theninitiate-braking.\nHumansalsohavemanysuchconnections, someofwhicharelearnedresponses (asfordriv-\ning)andsomeofwhichareinnatereflexes(suchasblinking whensomething approaches the\neye). In the course of the book, we show several different ways in which such connections\ncanbelearned andimplemented.\nThe program in Figure 2.8 is specific to one particular vacuum environment. A more\ngeneral and flexible approach is first to build a general-purpose interpreter for condition\u2013\naction rules and then to create rule sets for specific task environments. Figure 2.9 gives the\nstructure ofthisgeneral programinschematicform,showinghowthecondition\u2013action rules\nallow the agent to make the connection from percept to action. (Do not worry if this seems\n5 Alsocalledsituation\u2013actionrules,productions,orif\u2013thenrules. Section2.4. TheStructureofAgents 49\nAgent Sensors\nWhat the world\nis like now\nWhat action I\nCondition-action rules\nshould do now\nActuators\nFigure2.9 Schematicdiagramofasimplereflexagent.\nfunctionSIMPLE-REFLEX-AGENT(percept)returnsanaction\npersistent: rules,asetofcondition\u2013actionrules\nstate\u2190INTERPRET-INPUT(percept)\nrule\u2190RULE-MATCH(state,rules)\naction\u2190rule.ACTION\nreturnaction\nFigure 2.10 A simple reflex agent. It acts accordingto a rule whose conditionmatches\nthecurrentstate,asdefinedbythepercept.\ntrivial;itgetsmoreinteresting shortly.) Weuserectangles todenotethecurrentinternalstate\nof the agent\u2019s decision process, and ovals to represent the background information used in\nthe process. The agent program, which is also very simple, is shown in Figure 2.10. The\nINTERPRET-INPUT function generates anabstracted description ofthecurrentstatefromthe\npercept, and the RULE-MATCH function returns the firstrule inthe setof rules that matches\nthe given state description. Note that the description in terms of \u201crules\u201d and \u201cmatching\u201d is\npurely conceptual; actual implementations can be as simple as a collection of logic gates\nimplementing aBooleancircuit.\nSimplereflexagentshavetheadmirablepropertyofbeingsimple,buttheyturnouttobe\noflimitedintelligence. TheagentinFigure2.10willworkonlyifthecorrectdecisioncanbe\nmadeonthebasisofonlythecurrentpercept\u2014thatis,onlyiftheenvironmentisfullyobserv-\nable. Evenalittle bitofunobservability cancause serious trouble. Forexample, thebraking\nrule given earlier assumes that the condition car-in-front-is-braking can be determined from\nthe current percept\u2014a single frame of video. This works if the car in front has a centrally\nmounted brake light. Unfortunately, older models have different configurations of taillights,\nEnvironment 50 Chapter 2. Intelligent Agents\nbrake lights, and turn-signal lights, and it is not always possible to tell from a single image\nwhetherthecarisbraking. Asimplereflexagentdrivingbehindsuchacarwouldeitherbrake\ncontinuously andunnecessarily, or,worse,neverbrakeatall.\nWecanseeasimilarproblemarisinginthevacuumworld. Supposethatasimplereflex\nvacuum agent is deprived of its location sensor and has only a dirt sensor. Such an agent\nhasjusttwopossiblepercepts: [Dirty]and[Clean]. ItcanSuck inresponseto[Dirty];what\nshoulditdoinresponseto[Clean]? MovingLeft fails(forever)ifithappenstostartinsquare\nA,andmovingRight fails(forever)ifithappenstostartinsquare B. Infiniteloopsareoften\nunavoidable forsimplereflexagentsoperating inpartially observable environments.\nEscape from infinite loops is possible if the agent can randomize its actions. For ex-\nRANDOMIZATION\nample,ifthevacuumagentperceives[Clean],itmightflipacointochoosebetweenLeft and\nRight. Itiseasytoshowthattheagentwillreachtheothersquareinanaverageoftwosteps.\nThen, if that square is dirty, the agent will clean it and the task will be complete. Hence, a\nrandomized simplereflexagentmightoutperform adeterministic simplereflexagent.\nWementionedinSection2.3thatrandomizedbehavioroftherightkindcanberational\ninsomemultiagentenvironments. Insingle-agentenvironments,randomizationisusuallynot\nrational. It is a useful trick that helps a simple reflex agent in some situations, but in most\ncaseswecandomuchbetterwithmoresophisticated deterministic agents.\n2.4.3 Model-based reflex agents\nThe most effective way to handle partial observability is for the agent to keep track of the\npart of the world it can\u2019t see now. That is, the agent should maintain some sort of internal\nstatethatdepends onthepercepthistoryandtherebyreflectsatleastsomeoftheunobserved\nINTERNALSTATE\naspects ofthecurrentstate. Forthebraking problem, theinternal stateisnottooextensive\u2014\njust the previous frame from the camera, allowing the agent to detect when twored lights at\ntheedgeofthevehicle goonoroffsimultaneously. Forother driving taskssuchaschanging\nlanes,theagentneedstokeeptrackofwheretheothercarsareifitcan\u2019tseethemallatonce.\nAndforanydrivingtobepossibleatall,theagentneedstokeeptrackofwhereitskeysare.\nUpdating this internal state information as time goes by requires two kinds of knowl-\nedge to be encoded in the agent program. First, we need some information about how the\nworldevolvesindependently oftheagent\u2014forexample,thatanovertakingcargenerallywill\nbe closer behind than it was a moment ago. Second, we need some information about how\ntheagent\u2019sownactionsaffecttheworld\u2014forexample,thatwhentheagentturnsthesteering\nwheel clockwise, the car turns to the right, or that after driving for five minutes northbound\nonthefreeway,oneisusuallyaboutfivemilesnorthofwhereonewasfiveminutesago. This\nknowledge about \u201chow the world works\u201d\u2014whether implemented in simple Boolean circuits\norincomplete scientific theories\u2014is called a modelofthe world. Anagent that uses such a\nMODEL-BASED modeliscalledamodel-basedagent.\nAGENT\nFigure2.11givesthestructureofthemodel-basedreflexagentwithinternalstate,show-\ning how the current percept is combined with the old internal state to generate the updated\ndescriptionofthecurrentstate,basedontheagent\u2019smodelofhowtheworldworks. Theagent\nprogramisshowninFigure2.12. Theinterestingpartisthefunction UPDATE-STATE,which Section2.4. TheStructureofAgents 51\nSensors\nState\nHow the world evolves What the world\nis like now\nWhat my actions do\nWhat action I\nCondition-action rules\nshould do now\nAgent Actuators\nFigure2.11 Amodel-basedreflexagent.\nfunctionMODEL-BASED-REFLEX-AGENT(percept)returnsanaction\npersistent: state,theagent\u2019scurrentconceptionoftheworldstate\nmodel,adescriptionofhowthenextstatedependsoncurrentstateandaction\nrules,asetofcondition\u2013actionrules\naction,themostrecentaction,initiallynone\nstate\u2190UPDATE-STATE(state,action,percept,model)\nrule\u2190RULE-MATCH(state,rules)\naction\u2190rule.ACTION\nreturnaction\nFigure2.12 Amodel-basedreflexagent. Itkeepstrackofthecurrentstateoftheworld,\nusinganinternalmodel.Itthenchoosesanactioninthesamewayasthereflexagent.\nis responsible forcreating the new internal state description. Thedetails of how models and\nstates are represented vary widely depending on the type of environment and the particular\ntechnology used in the agent design. Detailed examples of models and updating algorithms\nappearinChapters4,12,11,15,17,and25.\nRegardless of the kind of representation used, it is seldom possible for the agent to\ndetermine the current state of a partially observable environment exactly. Instead, the box\nlabeled \u201cwhat the world is like now\u201d (Figure 2.11) represents the agent\u2019s \u201cbest guess\u201d (or\nsometimes best guesses). Forexample, an automated taxi may not beable tosee around the\nlarge truck that has stopped in front of it and can only guess about what may be causing the\nhold-up. Thus,uncertainty aboutthecurrent statemaybeunavoidable, buttheagentstillhas\ntomakeadecision.\nA perhaps less obvious point about the internal \u201cstate\u201d maintained by a model-based\nagent is that it does not have to describe \u201cwhat the world is like now\u201d in a literal sense. For\nEnvironment 52 Chapter 2. Intelligent Agents\nSensors\nState\nWhat the world\nHow the world evolves is like now\nWhat it will be like\nWhat my actions do if I do action A\nWhat action I\nGoals should do now\nAgent\nActuators\nFigure2.13 Amodel-based,goal-basedagent.Itkeepstrackoftheworldstateaswellas\nasetofgoalsitistryingtoachieve,andchoosesanactionthatwill(eventually)leadtothe\nachievementofitsgoals.\nexample, the taxi may be driving back home, and it may have a rule telling it to fill up with\ngas on the way home unless it has at least half a tank. Although \u201cdriving back home\u201d may\nseem toan aspect of theworld state, the fact ofthe taxi\u2019s destination isactually an aspect of\nthe agent\u2019s internal state. Ifyou findthis puzzling, consider that the taxi could be in exactly\nthesameplaceatthesametime,butintending toreachadifferent destination.\n2.4.4 Goal-basedagents\nKnowingsomethingaboutthecurrentstateoftheenvironmentisnotalwaysenoughtodecide\nwhat to do. For example, at a road junction, the taxi can turn left, turn right, or go straight\non. Thecorrectdecisiondependsonwherethetaxiistryingtogetto. Inotherwords,aswell\nas a current state description, the agent needs some sort of goal information that describes\nGOAL\nsituations that are desirable\u2014for example, being at the passenger\u2019s destination. The agent\nprogram can combine this with the model (the same information as was used in the model-\nbasedreflexagent)tochooseactionsthatachievethegoal. Figure2.13showsthegoal-based\nagent\u2019sstructure.\nSometimesgoal-basedactionselectionisstraightforward\u2014forexample,whengoalsat-\nisfaction results immediately from a single action. Sometimes it will be more tricky\u2014for\nexample,whentheagenthastoconsiderlongsequences oftwistsandturnsinordertofinda\nwaytoachievethegoal. Search(Chapters3to5)andplanning(Chapters10and11)arethe\nsubfieldsofAIdevoted tofindingactionsequences thatachievetheagent\u2019sgoals.\nNoticethatdecisionmakingofthiskindisfundamentally differentfromthecondition\u2013\nactionrulesdescribed earlier,inthatitinvolvesconsideration ofthefuture\u2014both\u201cWhatwill\nhappen ifIdosuch-and-such?\u201d and\u201cWillthatmakemehappy?\u201d Inthereflexagentdesigns,\nthis information is not explicitly represented, because the built-in rules map directly from\nEnvironment Section2.4. TheStructureofAgents 53\npercepts toactions. Thereflexagentbrakeswhenitseesbrakelights. Agoal-based agent, in\nprinciple, couldreasonthatifthecarinfronthasitsbrake lightson,itwillslowdown. Given\nthe way the world usually evolves, the only action that will achieve the goal of not hitting\nothercarsistobrake.\nAlthough the goal-based agent appears less efficient, it is more flexible because the\nknowledgethatsupportsitsdecisionsisrepresentedexplicitlyandcanbemodified. Ifitstarts\ntorain,theagentcanupdateitsknowledgeofhoweffectivelyitsbrakeswilloperate;thiswill\nautomatically cause all of the relevant behaviors to be altered to suit the new conditions.\nFor the reflex agent, on the other hand, we would have to rewrite many condition\u2013action\nrules. Thegoal-based agent\u2019sbehaviorcaneasilybechanged togotoadifferentdestination,\nsimply by specifying that destination as the goal. The reflex agent\u2019s rules for when to turn\nand when to go straight will work only fora single destination; they must all be replaced to\ngosomewherenew.\n2.4.5 Utility-basedagents\nGoals alone are not enough to generate high-quality behavior in most environments. For\nexample, many action sequences will get the taxi to its destination (thereby achieving the\ngoal) but somearequicker, safer, morereliable, orcheaper thanothers. Goalsjust provide a\ncrudebinarydistinctionbetween\u201chappy\u201dand\u201cunhappy\u201dstates. Amoregeneralperformance\nmeasureshouldallowacomparisonofdifferentworldstates accordingtoexactlyhowhappy\ntheywouldmaketheagent. Because\u201chappy\u201d doesnotsoundveryscientific, economists and\ncomputerscientists usetheterm utilityinstead.6\nUTILITY\nWehavealreadyseenthataperformancemeasureassignsascoretoanygivensequence\nof environment states, so it can easily distinguish between more and less desirable ways of\ngetting to the taxi\u2019s destination. An agent\u2019s utility function is essentially an internalization\nUTILITYFUNCTION\nof the performance measure. If the internal utility function and the external performance\nmeasure are in agreement, then an agent that chooses actions to maximize its utility will be\nrationalaccording totheexternalperformance measure.\nLet us emphasize again that this is not the only way to be rational\u2014we have already\nseen a rational agent program for the vacuum world (Figure 2.8) that has no idea what its\nutility function is\u2014but, likegoal-based agents, autility-based agent hasmanyadvantages in\ntermsofflexibilityandlearning. Furthermore,intwokinds ofcases,goalsareinadequatebut\nautility-based agentcanstillmakerationaldecisions. First,whenthereareconflictinggoals,\nonly some of which can be achieved (for example, speed and safety), the utility function\nspecifies the appropriate tradeoff. Second, when there are several goals that the agent can\naim for, none of which can be achieved with certainty, utility provides a way in which the\nlikelihood ofsuccesscanbeweighedagainsttheimportance ofthegoals.\nPartialobservabilityandstochasticityareubiquitousintherealworld,andso,therefore,\nis decision making under uncertainty. Technically speaking, a rational utility-based agent\nchooses the action that maximizes the expected utility of the action outcomes\u2014that is, the\nEXPECTEDUTILITY\nutility the agent expects to derive, on average, given the probabilities and utilities of each\n6 Theword\u201cutility\u201dhererefersto\u201cthequalityofbeinguseful,\u201dnottotheelectriccompanyorwaterworks. 54 Chapter 2. Intelligent Agents\nSensors\nState\nWhat the world\nHow the world evolves is like now\nWhat it will be like\nWhat my actions do if I do action A\nHow happy I will be\nUtility\nin such a state\nWhat action I\nshould do now\nAgent Actuators\nFigure2.14 Amodel-based,utility-basedagent. Itusesamodeloftheworld,alongwith\nautilityfunctionthatmeasuresitspreferencesamongstatesoftheworld.Thenitchoosesthe\nactionthatleadstothebestexpectedutility,whereexpectedutilityiscomputedbyaveraging\noverallpossibleoutcomestates,weightedbytheprobabilityoftheoutcome.\noutcome. (AppendixAdefinesexpectation moreprecisely.) InChapter16,weshowthatany\nrational agent must behave as if it possesses a utility function whose expected value it tries\ntomaximize. Anagentthatpossesses anexplicit utility function canmakerational decisions\nwith a general-purpose algorithm that does not depend on the specific utility function being\nmaximized. In this way, the \u201cglobal\u201d definition of rationality\u2014designating as rational those\nagent functions that have the highest performance\u2014is turned into a \u201clocal\u201d constraint on\nrational-agent designsthatcanbeexpressedinasimpleprogram.\nTheutility-based agent structure appears in Figure 2.14. Utility-based agent programs\nappearinPartIV,wherewedesign decision-making agents thatmusthandle theuncertainty\ninherent instochastic orpartially observable environments.\nAtthispoint,thereadermaybewondering, \u201cIsitthatsimple? Wejustbuildagentsthat\nmaximize expected utility, and we\u2019re done?\u201d It\u2019s true that such agents would be intelligent,\nbut it\u2019s not simple. A utility-based agent has to model and keep track of its environment,\ntasks that have involved a great deal of research on perception, representation, reasoning,\nand learning. The results of this research fill many of the chapters of this book. Choosing\ntheutility-maximizing courseofactionisalsoadifficulttask,requiringingeniousalgorithms\nthat fill several more chapters. Even with these algorithms, perfect rationality is usually\nunachievable inpractice becauseofcomputational complexity, aswenotedinChapter1.\n2.4.6 Learning agents\nWe have described agent programs with various methods for selecting actions. We have\nnot, so far, explained how the agent programs come into being. In his famous early paper,\nTuring (1950) considers the idea of actually programming his intelligent machines by hand.\nEnvironment Section2.4. TheStructureofAgents 55\nPerformance standard\nCritic Sensors\nfeedback\nchanges\nLearning Performance\nelement element\nknowledge\nlearning\ngoals\nProblem\ngenerator\nAgent Actuators\nFigure2.15 Agenerallearningagent.\nHeestimateshowmuchworkthismighttakeandconcludes\u201cSomemoreexpeditiousmethod\nseems desirable.\u201d The method he proposes is to build learning machines and then to teach\nthem. In many areas of AI, this is now the preferred method for creating state-of-the-art\nsystems. Learning has another advantage, as wenoted earlier: it allows the agent to operate\nininitially unknown environments andtobecomemorecompetent thanitsinitial knowledge\nalone might allow. In this section, we briefly introduce the main ideas of learning agents.\nThroughout the book, we comment on opportunities and methods for learning in particular\nkindsofagents. PartVgoesintomuchmoredepthonthelearningalgorithms themselves.\nA learning agent can be divided into four conceptual components, as shown in Fig-\nure 2.15. The most important distinction is between the learning element, which is re-\nLEARNINGELEMENT\nPERFORMANCE sponsibleformakingimprovements,andtheperformanceelement,whichisresponsiblefor\nELEMENT\nselecting external actions. The performance element is what we have previously considered\ntobetheentire agent: ittakes inpercepts and decides onactions. Thelearning element uses\nfeedback from the critic on how the agent is doing and determines how the performance\nCRITIC\nelementshouldbemodifiedtodobetterinthefuture.\nThedesignofthelearningelementdependsverymuchonthedesignoftheperformance\nelement. When trying to design an agent that learns a certain capability, the first question is\nnot\u201cHowamIgoingtogetittolearnthis?\u201d but\u201cWhatkindofperformanceelementwillmy\nagentneedtodothisonceithaslearned how?\u201d Givenanagentdesign, learningmechanisms\ncanbeconstructed toimproveeverypartoftheagent.\nThecritic tellsthe learning element how welltheagent isdoing withrespect toafixed\nperformance standard. The critic is necessary because the percepts themselves provide no\nindication of the agent\u2019s success. For example, a chess program could receive a percept\nindicating that it has checkmated its opponent, but it needs a performance standard to know\nthatthisisagoodthing;theperceptitselfdoesnotsayso. Itisimportantthattheperformance\nEnvironment 56 Chapter 2. Intelligent Agents\nstandard be fixed. Conceptually, one should think of it as being outside the agent altogether\nbecausetheagentmustnotmodifyittofititsownbehavior.\nPROBLEM The last component of the learning agent is the problem generator. It is responsible\nGENERATOR\nfor suggesting actions that will lead to new and informative experiences. The point is that\nif the performance element had its way, it would keep doing the actions that are best, given\nwhatitknows. Butiftheagent iswillingtoexplore alittle anddosomeperhaps suboptimal\nactions inthe short run, itmight discover muchbetteractions forthelong run. Theproblem\ngenerator\u2019s job is to suggest these exploratory actions. This is what scientists do when they\ncarry out experiments. Galileo did not think that dropping rocks from the top of a tower in\nPisa was valuable in itself. He was not trying to break the rocks or to modify the brains of\nunfortunate passers-by. His aim was to modify his own brain by identifying a better theory\nofthemotionofobjects.\nTomake theoverall design moreconcrete, letusreturn tothe automated taxi example.\nThe performance element consists of whatever collection of knowledge and procedures the\ntaxi has for selecting its driving actions. The taxi goes out on the road and drives, using\nthisperformance element. Thecritic observes theworldand passes information along tothe\nlearningelement. Forexample,afterthetaximakesaquickleftturnacrossthreelanesoftraf-\nfic,thecriticobservestheshockinglanguageusedbyotherdrivers. Fromthisexperience, the\nlearningelementisabletoformulatearulesayingthiswasabadaction,andtheperformance\nelement is modified by installation of the new rule. The problem generator might identify\ncertainareasofbehaviorinneedofimprovementandsuggest experiments,suchastryingout\nthebrakesondifferentroadsurfaces underdifferentconditions.\nThelearning elementcanmakechanges toanyofthe\u201cknowledge\u201d components shown\nintheagentdiagrams(Figures2.9,2.11,2.13,and2.14). Thesimplestcasesinvolvelearning\ndirectly from the percept sequence. Observation of pairs of successive states of the environ-\nmentcanallow theagent tolearn \u201cHowtheworldevolves,\u201d and observation oftheresults of\nitsactions canallowtheagenttolearn\u201cWhatmyactions do.\u201d Forexample, ifthetaxiexerts\na certain braking pressure when driving on a wet road, then it will soon find out how much\ndeceleration is actually achieved. Clearly, these two learning tasks are more difficult if the\nenvironment isonlypartially observable.\nThe forms of learning in the preceding paragraph do not need to access the external\nperformance standard\u2014in a sense, the standard is the universal one of making predictions\nthat agree with experiment. The situation is slightly more complex for a utility-based agent\nthatwishestolearnutilityinformation. Forexample,suppose thetaxi-driving agentreceives\nno tips from passengers who have been thoroughly shaken up during the trip. The external\nperformance standard mustinform theagentthatthelossoftipsisanegativecontribution to\nits overall performance; then the agent might be able to learn that violent maneuvers do not\ncontribute to its own utility. In a sense, the performance standard distinguishes part of the\nincomingperceptasareward(orpenalty)thatprovidesdirectfeedbackonthequalityofthe\nagent\u2019s behavior. Hard-wired performance standards such aspainandhungerinanimals can\nbeunderstood inthisway. Thisissueisdiscussed furtherin Chapter21.\nInsummary, agents haveavarietyofcomponents, andthose components canberepre-\nsented in many ways within the agent program, so there appears to be great variety among Section2.4. TheStructureofAgents 57\nlearning methods. Thereis, however, asingle unifying theme. Learning inintelligent agents\ncanbesummarized asaprocess ofmodification ofeachcomponent oftheagenttobring the\ncomponents into closer agreement withthe available feedback information, thereby improv-\ningtheoverallperformance oftheagent.\n2.4.7 Howthe components ofagentprograms work\nWehavedescribedagentprograms(inveryhigh-levelterms) asconsistingofvariouscompo-\nnents,whosefunctionitistoanswerquestionssuchas: \u201cWhatistheworldlikenow?\u201d \u201cWhat\naction should I do now?\u201d \u201cWhat do my actions do?\u201d The next question for a student of AI\nis, \u201cHow on earth do these components work?\u201d It takes about a thousand pages to begin to\nanswerthat question properly, but here wewantto draw thereader\u2019s attention to somebasic\ndistinctions among thevarious waysthatthecomponents can represent theenvironment that\ntheagentinhabits.\nRoughly speaking, we can place the representations along an axis of increasing com-\nplexity and expressive power\u2014atomic, factored, and structured. To illustrate these ideas,\nit helps to consider a particular agent component, such as the one that deals with \u201cWhat my\nactions do.\u201d This component describes the changes that might occur in the environment as\nthe result of taking an action, and Figure 2.16 provides schematic depictions of how those\ntransitions mightberepresented.\nB C\nB C\n(a) Atomic (b) Factored (b) Structured\nFigure2.16 Threewaystorepresentstatesandthetransitionsbetweenthem. (a)Atomic\nrepresentation:astate(suchasBorC)isablackboxwithnointernalstructure;(b)Factored\nrepresentation: a state consists of a vectorof attribute values; valuescan be Boolean, real-\nvalued, or one of a fixed set of symbols. (c) Structured representation: a state includes\nobjects,eachofwhichmayhaveattributesofitsownaswellasrelationshipstootherobjects.\nATOMIC In an atomic representation each state of the world is indivisible\u2014it has no internal\nREPRESENTATION\nstructure. Consider the problem of finding a driving route from one end of a country to the\notherviasomesequenceofcities(weaddressthisprobleminFigure3.2onpage68). Forthe\npurposes ofsolving this problem, itmaysufficetoreduce the stateofworldtojustthename\nof the city we are in\u2014a single atom of knowledge; a \u201cblack box\u201d whose only discernible\nproperty is that of being identical to or different from another black box. The algorithms 58 Chapter 2. Intelligent Agents\nunderlyingsearchandgame-playing(Chapters3\u20135),HiddenMarkovmodels(Chapter15),\nand Markov decision processes (Chapter 17) all work with atomic representations\u2014or, at\nleast,theytreatrepresentations asiftheywereatomic.\nNowconsider a higher-fidelity description for the same problem, where weneed to be\nconcerned with more than just atomic location in one city oranother; wemight need to pay\nattention tohowmuchgasisinthetank, ourcurrent GPScoordinates, whetherornottheoil\nwarning light isworking, howmuch spare change wehave fortoll crossings, whatstation is\nFACTORED on the radio, and so on. A factored representation splits up each state into a fixed set of\nREPRESENTATION\nvariables or attributes, each of which can have a value. While two different atomic states\nVARIABLE\nhave nothing in common\u2014they are just different black boxes\u2014two different factored states\nATTRIBUTE\ncansharesomeattributes(suchasbeingatsomeparticularGPSlocation)andnotothers(such\nVALUE\nas having lots of gas or having no gas); this makes it much easier to work out how to turn\nonestateintoanother. Withfactored representations, wecanalsorepresent uncertainty\u2014for\nexample, ignorance about the amount of gas in the tank can be represented by leaving that\nattribute blank. Manyimportant areasofAIarebased onfactored representations, including\nconstraint satisfaction algorithms (Chapter 6), propositional logic (Chapter 7), planning\n(Chapters 10 and 11), Bayesian networks (Chapters 13\u201316), and the machine learning al-\ngorithmsinChapters18,20,and21.\nFor many purposes, we need to understand the world as having things in it that are\nrelated to each other, not just variables with values. For example, we might notice that a\nlargetruck aheadofusisreversing intothedrivewayofadairy farmbutacowhasgotloose\nand is blocking the truck\u2019s path. A factored representation is unlikely to be pre-equipped\nwiththeattributeTruckAheadBackingIntoDairyFarmDrivewayBlockedByLooseCow with\nSTRUCTURED value true or false. Instead, we would need a structured representation, in which ob-\nREPRESENTATION\njects such as cows and trucks and their various and varying relationships can be described\nexplicitly. (See Figure 2.16(c).) Structured representations underlie relational databases\nand first-order logic (Chapters 8, 9, and 12), first-order probability models (Chapter 14),\nknowledge-based learning (Chapter 19) and much of natural language understanding\n(Chapters 22 and 23). In fact, almost everything that humans express in natural language\nconcerns objectsandtheirrelationships.\nAs we mentioned earlier, the axis along which atomic, factored, and structured repre-\nsentations lieistheaxis ofincreasing expressiveness. Roughly speaking, amoreexpressive\nEXPRESSIVENESS\nrepresentation cancapture,atleastasconcisely, everythingalessexpressiveonecancapture,\nplussomemore. Often,themoreexpressivelanguageismuchmoreconcise;forexample,the\nrules of chess can be written in a page or two of a structured-representation language such\nas first-order logic but require thousands of pages when written in a factored-representation\nlanguage such as propositional logic. On the other hand, reasoning and learning become\nmore complex as the expressive power of the representation increases. To gain the benefits\nofexpressive representations whileavoiding theirdrawbacks, intelligent systems forthereal\nworldmayneedtooperateatallpointsalongtheaxissimultaneously. Section2.5. Summary 59\n2.5 SUMMARY\nThis chapter has been something of a whirlwind tour of AI, which we have conceived of as\nthescienceofagentdesign. Themajorpointstorecallareas follows:\n\u2022 Anagent issomething that perceives and acts in anenvironment. The agent function\nforanagentspecifiestheactiontakenbytheagentinresponsetoanyperceptsequence.\n\u2022 The performance measure evaluates the behavior of the agent in an environment. A\nrational agentactssoastomaximize theexpected valueoftheperformance measure,\ngiventhepercept sequence ithasseensofar.\n\u2022 A task environment specification includes the performance measure, the external en-\nvironment, the actuators, and the sensors. In designing an agent, the first step must\nalwaysbetospecifythetaskenvironment asfullyaspossible.\n\u2022 Task environments vary along several significant dimensions. They can be fully or\npartiallyobservable, single-agent ormultiagent, deterministic orstochastic, episodicor\nsequential, staticordynamic,discrete orcontinuous, and knownorunknown.\n\u2022 The agent program implements the agent function. There exists a variety of basic\nagent-program designs reflecting thekindofinformation madeexplicit andusedinthe\ndecision process. The designs vary in efficiency, compactness, and flexibility. The\nappropriate designoftheagentprogram depends onthenatureoftheenvironment.\n\u2022 Simplereflexagentsresponddirectlytopercepts, whereas model-basedreflexagents\nmaintain internal state to track aspects of the world that are not evident in the current\npercept. Goal-based agents act to achieve their goals, and utility-based agents try to\nmaximizetheirownexpected \u201chappiness.\u201d\n\u2022 Allagentscanimprovetheirperformance through learning.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nThe central role of action in intelligence\u2014the notion of practical reasoning\u2014goes back at\nleast as far as Aristotle\u2019s Nicomachean Ethics. Practical reasoning was also the subject of\nMcCarthy\u2019s(1958)influential paper\u201cProgramswithCommonSense.\u201d Thefieldsofrobotics\nandcontrol theory are, bytheirverynature, concerned principally withphysical agents. The\nconcept of a controller in control theory is identical to that of an agent in AI. Perhaps sur-\nCONTROLLER\nprisingly, AI has concentrated for most of its history on isolated components of agents\u2014\nquestion-answering systems, theorem-provers, vision systems, and so on\u2014rather than on\nwholeagents. Thediscussion ofagents inthetextbyGenesereth andNilsson(1987) wasan\ninfluentialexception. Thewhole-agentviewisnowwidelyacceptedandisacentralthemein\nrecenttexts(Pooleetal.,1998;Nilsson,1998;PadghamandWinikoff, 2004;Jones,2007).\nChapter1tracedtherootsoftheconceptofrationalityinphilosophyandeconomics. In\nAI,theconceptwasofperipheralinterestuntilthemid-1980s,whenitbegantosuffusemany 60 Chapter 2. Intelligent Agents\ndiscussions aboutthepropertechnical foundations ofthefield. ApaperbyJonDoyle(1983)\npredicted that rational agent design would come to be seen as the core mission of AI, while\notherpopulartopicswouldspinofftoformnewdisciplines.\nCareful attention to the properties of the environment and their consequences for ra-\ntional agent design is most apparent in the control theory tradition\u2014for example, classical\ncontrol systems (Dorf and Bishop, 2004; Kirk, 2004) handle fully observable, deterministic\nenvironments; stochastic optimal control (Kumar and Varaiya, 1986; Bertsekas and Shreve,\n2007) handles partially observable, stochastic environments; and hybrid control (Henzinger\nand Sastry, 1998; Cassandras and Lygeros, 2006) deals with environments containing both\ndiscrete andcontinuous elements. Thedistinction between fully andpartially observable en-\nvironments is also central in the dynamic programming literature developed in the field of\noperations research (Puterman,1994), whichwediscuss inChapter17.\nReflex agents were the primary model for psychological behaviorists such as Skinner\n(1953),whoattemptedtoreducethepsychologyoforganismsstrictlytoinput\/outputorstim-\nulus\/response mappings. The advance from behaviorism to functionalism in psychology,\nwhich wasatleast partly driven bythe application ofthe computer metaphortoagents (Put-\nnam, 1960; Lewis, 1966), introduced the internal state of the agent into the picture. Most\nwork in AI views the idea of pure reflex agents with state as too simple to provide much\nleverage, but work by Rosenschein (1985) and Brooks (1986) questioned this assumption\n(see Chapter 25). In recent years, a great deal of work has gone into finding efficient algo-\nrithmsforkeepingtrackofcomplexenvironments(Hamscheretal.,1992;Simon,2006). The\nRemoteAgentprogram(describedonpage28)thatcontrolled theDeepSpaceOnespacecraft\nisaparticularly impressiveexample(Muscettola etal.,1998;Jonssonetal.,2000).\nGoal-basedagentsarepresupposedineverythingfromAristotle\u2019sviewofpracticalrea-\nsoning to McCarthy\u2019s early papers on logical AI. Shakey the Robot (Fikes and Nilsson,\n1971; Nilsson, 1984) was the first robotic embodiment of a logical, goal-based agent. A\nfull logical analysis of goal-based agents appeared in Genesereth and Nilsson (1987), and a\ngoal-basedprogrammingmethodologycalledagent-orientedprogrammingwasdevelopedby\nShoham (1993). The agent-based approach is now extremely popular in software engineer-\ning (Ciancarini and Wooldridge, 2001). It has also infiltrated the area of operating systems,\nAUTONOMIC whereautonomiccomputingreferstocomputersystemsandnetworksthatmonitorandcon-\nCOMPUTING\ntrolthemselves withaperceive\u2013act loopandmachinelearning methods(KephartandChess,\n2003). Noting that a collection of agent programs designed to work well together in a true\nmultiagentenvironmentnecessarilyexhibitsmodularity\u2014theprogramssharenointernalstate\nand communicate with each other only through the environment\u2014it is common within the\nMULTIAGENT field of multiagent systems to design the agent program of asingle agent as a collection of\nSYSTEMS\nautonomous sub-agents. In some cases, one can even prove that the resulting system gives\nthesameoptimalsolutions asamonolithicdesign.\nThegoal-based viewofagentsalsodominatesthecognitivepsychology traditioninthe\narea of problem solving, beginning with the enormously influential Human Problem Solv-\ning(NewellandSimon,1972)andrunningthroughallofNewell\u2019slaterwork(Newell,1990).\nGoals, furtheranalyzed as desires (general) and intentions (currently pursued), arecentral to\nthe theory of agents developed by Bratman (1987). This theory has been influential both in Exercises 61\nnaturallanguage understanding andmultiagent systems.\nHorvitz et al. (1988) specifically suggest the use of rationality conceived as the maxi-\nmization of expected utility as a basis forAI. Thetext by Pearl (1988) was the first in AI to\ncoverprobabilityandutilitytheoryindepth;itsexpositionofpracticalmethodsforreasoning\nand decision making under uncertainty was probably the single biggest factor in the rapid\nshifttowardsutility-based agentsinthe1990s(seePartIV).\nThegeneraldesignforlearningagentsportrayedinFigure2.15isclassicinthemachine\nlearning literature (Buchanan et al., 1978; Mitchell, 1997). Examples of the design, as em-\nbodiedinprograms,gobackatleastasfarasArthurSamuel\u2019s (1959,1967)learningprogram\nforplayingcheckers. Learningagentsarediscussed indepthinPartV.\nInterestinagentsandinagentdesignhasrisenrapidlyinrecentyears,partlybecauseof\nthegrowth ofthe Internet and theperceived needforautomated and mobilesoftbot (Etzioni\nand Weld, 1994). Relevant papers are collected in Readings in Agents (Huhns and Singh,\n1998)andFoundationsofRationalAgency(WooldridgeandRao,1999). Textsonmultiagent\nsystems usually provide agoodintroduction tomanyaspects ofagent design (Weiss, 2000a;\nWooldridge,2002). Severalconferenceseriesdevotedtoagentsbeganinthe1990s,including\nthe International Workshop on Agent Theories, Architectures, and Languages (ATAL), the\nInternational Conference on Autonomous Agents (AGENTS), and the International Confer-\nenceonMulti-AgentSystems(ICMAS).In2002,thesethreemergedtoformtheInternational\nJoint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS).Thejournal\nAutonomous Agents and Multi-Agent Systems was founded in 1998. Finally, Dung Beetle\nEcology (Hanski and Cambefort, 1991) provides a wealth of interesting information on the\nbehaviorofdungbeetles. YouTubefeatures inspiring video recordings oftheiractivities.\nEXERCISES\n2.1 Suppose that the performance measure is concerned with just the first T time steps of\nthe environment and ignores everything thereafter. Show that a rational agent\u2019s action may\ndependnotjustonthestateoftheenvironment butalsoonthetimestepithasreached.\n2.2 Letusexaminetherationality ofvariousvacuum-cleaner agentfunctions.\na. Show that the simple vacuum-cleaner agent function described in Figure 2.3 is indeed\nrationalundertheassumptions listedonpage38.\nb. Describearationalagentfunctionforthecaseinwhicheachmovementcostsonepoint.\nDoesthecorresponding agentprogram requireinternal state?\nc. Discuss possible agent designs for the cases in which clean squares can become dirty\nandthegeography oftheenvironment isunknown. Doesitmake sensefortheagentto\nlearnfromitsexperience inthesecases? Ifso,whatshoulditlearn? Ifnot,whynot?\n2.3 For each of the following assertions, say whether it is true or false and support your\nanswerwithexamplesorcounterexamples whereappropriate.\na. Anagentthatsensesonlypartialinformationaboutthestatecannotbeperfectlyrational. 62 Chapter 2. Intelligent Agents\nb. Thereexisttaskenvironments inwhichnopurereflexagentcanbehaverationally.\nc. Thereexistsataskenvironment inwhicheveryagentisrational.\nd. Theinputtoanagentprogram isthesameastheinputtotheagentfunction.\ne. Everyagentfunctionisimplementable bysomeprogram\/machine combination.\nf. Supposeanagentselectsitsactionuniformlyatrandomfromthesetofpossibleactions.\nThereexistsadeterministic taskenvironment inwhichthis agentisrational.\ng. Itispossibleforagivenagenttobeperfectlyrationalintwodistincttaskenvironments.\nh. Everyagentisrationalinanunobservable environment.\ni. Aperfectly rationalpoker-playing agentneverloses.\n2.4 For each of the following activities, give a PEAS description of the task environment\nandcharacterize itintermsoftheproperties listedinSection2.3.2.\n\u2022 Playingsoccer.\n\u2022 Exploringthesubsurface oceansofTitan.\n\u2022 ShoppingforusedAIbooksontheInternet.\n\u2022 Playingatennismatch.\n\u2022 Practicingtennisagainstawall.\n\u2022 Performingahighjump.\n\u2022 Knittingasweater.\n\u2022 Biddingonanitematanauction.\n2.5 Define in your own words the following terms: agent, agent function, agent program,\nrationality, autonomy,reflexagent,model-based agent,goal-based agent,utility-based agent,\nlearning agent.\n2.6 Thisexerciseexploresthedifferences betweenagentfunctions andagentprograms.\na. Can there be more than one agent program that implements a given agent function?\nGiveanexample,orshowwhyoneisnotpossible.\nb. Arethereagentfunctions thatcannotbeimplemented byanyagentprogram?\nc. Given a fixed machine architecture, does each agent program implement exactly one\nagentfunction?\nd. Given an architecture with n bits of storage, how many different possible agent pro-\ngramsarethere?\ne. Supposewekeeptheagentprogram fixedbutspeedupthemachinebyafactoroftwo.\nDoesthatchangetheagentfunction?\n2.7 Writepseudocode agentprogramsforthegoal-based andutility-based agents.\nThe following exercises all concern the implementation of environments and agents for the\nvacuum-cleaner world. Exercises 63\n2.8 Implement a performance-measuring environment simulator for the vacuum-cleaner\nworlddepictedinFigure2.2andspecifiedonpage38. Yourimplementationshouldbemodu-\nlarsothatthesensors,actuators,andenvironmentcharacteristics(size,shape,dirtplacement,\netc.) canbechangedeasily. (Note: forsomechoicesofprogramminglanguageandoperating\nsystemtherearealready implementations intheonlinecode repository.)\n2.9 Implement a simple reflex agent for the vacuum environment in Exercise 2.8. Run the\nenvironment with this agent for all possible initial dirt configurations and agent locations.\nRecordtheperformance scoreforeachconfiguration andtheoverallaveragescore.\n2.10 Consideramodifiedversionofthevacuum environment inExercise 2.8,inwhichthe\nagentispenalized onepointforeachmovement.\na. Canasimplereflexagentbeperfectlyrational forthisenvironment? Explain.\nb. Whataboutareflexagentwithstate? Designsuchanagent.\nc. How do your answers to a and b change if the agent\u2019s percepts give it the clean\/dirty\nstatusofeverysquareintheenvironment?\n2.11 Consideramodifiedversionofthevacuum environment inExercise 2.8,inwhichthe\ngeography of the environment\u2014its extent, boundaries, and obstacles\u2014is unknown, asis the\ninitialdirtconfiguration. (TheagentcangoUp andDown aswellasLeft andRight.)\na. Canasimplereflexagentbeperfectlyrational forthisenvironment? Explain.\nb. Canasimplereflexagentwitharandomizedagentfunction outperform asimplereflex\nagent? Designsuchanagentandmeasureitsperformance onseveralenvironments.\nc. Canyou design an environment in which yourrandomized agent willperform poorly?\nShowyourresults.\nd. Can a reflex agent with state outperform a simple reflex agent? Design such an agent\nandmeasureitsperformance onseveralenvironments. Canyoudesignarational agent\nofthistype?\n2.12 Repeat Exercise 2.11 for the case in which the location sensor is replaced with a\n\u201cbump\u201d sensor that detects the agent\u2019s attempts to move into an obstacle or to cross the\nboundaries of the environment. Suppose the bump sensor stops working; how should the\nagentbehave?\n2.13 Thevacuumenvironmentsintheprecedingexerciseshaveallbeendeterministic. Dis-\ncusspossible agentprogramsforeachofthefollowingstochastic versions:\na. Murphy\u2019slaw: twenty-fivepercentofthetime,theSuck actionfailstocleanthefloorif\nitisdirtyanddepositsdirtontothefloorifthefloorisclean. Howisyouragentprogram\naffectedifthedirtsensorgivesthewronganswer10%ofthetime?\nb. Small children: At each time step, each clean square has a 10% chance of becoming\ndirty. Canyoucomeupwitharational agentdesignforthiscase? 3\nSOLVING PROBLEMS BY\nSEARCHING\nIn which we see how an agent can find a sequence of actions that achieves its\ngoalswhennosingleactionwilldo.\nThesimplestagentsdiscussedinChapter2werethereflexagents,whichbasetheiractionson\nadirectmappingfromstatestoactions. Suchagentscannotoperate wellinenvironments for\nwhichthismappingwouldbetoolargetostoreandwouldtaketoolongtolearn. Goal-based\nagents, ontheotherhand,considerfutureactions andthedesirability oftheiroutcomes.\nPROBLEM-SOLVING This chapter describes one kind of goal-based agent called a problem-solving agent.\nAGENT\nProblem-solving agents use atomic representations, as described in Section 2.4.7\u2014that is,\nstatesoftheworldareconsideredaswholes,withnointernalstructurevisibletotheproblem-\nsolving algorithms. Goal-based agents that use more advanced factored or structured rep-\nresentations areusuallycalled planningagentsandarediscussed inChapters7and10.\nOurdiscussionofproblemsolvingbeginswithprecisedefinitionsofproblemsandtheir\nsolutions and give several examples to illustrate these definitions. We then describe several\ngeneral-purpose search algorithms that can be used to solve these problems. We will see\nseveral uninformed search algorithms\u2014algorithms that are given no information about the\nproblem otherthan itsdefinition. Although some ofthese algorithms can solve anysolvable\nproblem, noneofthemcandosoefficiently. Informedsearchalgorithms, ontheotherhand,\ncandoquitewellgivensomeguidance onwheretolookforsolutions.\nIn this chapter, we limit ourselves to the simplest kind of task environment, for which\nthesolutiontoaproblemisalwaysafixedsequenceofactions. Themoregeneralcase\u2014where\ntheagent\u2019sfutureactionsmayvarydepending onfuturepercepts\u2014is handledinChapter4.\nThis chapter uses the concepts of asymptotic complexity (that is, O() notation) and\nNP-completeness. Readersunfamiliarwiththeseconcepts shouldconsult AppendixA.\n3.1 PROBLEM-SOLVING AGENTS\nIntelligent agents are supposed to maximize their performance measure. As we mentioned\ninChapter2, achieving this issometimes simplified iftheagent canadopt agoal andaim at\nsatisfying it. Letusfirstlookatwhyandhowanagentmightdothis.\n64 Section3.1. Problem-Solving Agents 65\nImagineanagentinthecityofArad,Romania,enjoying atouringholiday. Theagent\u2019s\nperformance measure contains manyfactors: itwants toimprove itssuntan, improve its Ro-\nmanian,takeinthesights,enjoythenightlife (suchasitis),avoidhangovers, andsoon. The\ndecision problem is a complex one involving many tradeoffs and careful reading of guide-\nbooks. Now,suppose theagenthasanonrefundable tickettoflyoutofBucharest thefollow-\ning day. In that case, it makes sense for the agent to adopt the goal of getting to Bucharest.\nCourses of action that don\u2019t reach Bucharest on timecan berejected without further consid-\neration and the agent\u2019s decision problem is greatly simplified. Goals help organize behavior\nby limiting the objectives that the agent is trying to achieve and hence the actions it needs\nto consider. Goal formulation, based on the current situation and the agent\u2019s performance\nGOALFORMULATION\nmeasure,isthefirststepinproblem solving.\nWe will consider a goal to be a set of world states\u2014exactly those states in which the\ngoal is satisfied. The agent\u2019s task is to find out how to act, now and in the future, so that it\nreaches a goal state. Before it can do this, it needs to decide (or we need to decide on its\nbehalf) what sorts of actions and states it should consider. If it were to consider actions at\nthelevelof\u201cmovetheleftfootforwardaninch\u201dor\u201cturnthesteering wheelonedegreeleft,\u201d\nthe agent would probably never find its way out of the parking lot, let alone to Bucharest,\nbecause at that level of detail there is too much uncertainty in the world and there would be\nPROBLEM too many steps in a solution. Problem formulation is the process of deciding what actions\nFORMULATION\nandstatestoconsider, givenagoal. Wediscuss thisprocess inmoredetaillater. Fornow,let\nusassume thattheagent willconsider actions atthe levelof driving from onemajortownto\nanother. Eachstatetherefore corresponds tobeinginaparticulartown.\nOur agent has now adopted the goal of driving to Bucharest and is considering where\ntogofrom Arad. Threeroads lead outofArad, onetowardSibiu, onetoTimisoara, and one\ntoZerind. Noneoftheseachievesthegoal,sounlesstheagentisfamiliarwiththegeography\nofRomania, itwillnotknow whichroadtofollow.1 Inotherwords, theagent willnotknow\nwhich of its possible actions is best, because it does not yet know enough about the state\nthat results from taking each action. If the agent has no additional information\u2014i.e., if the\nenvironment is unknowninthesense defined in Section 2.3\u2014then itishas nochoice but to\ntryoneoftheactionsatrandom. Thissadsituationisdiscussed inChapter4.\nBut suppose the agent has a map of Romania. The point of a map is to provide the\nagentwithinformationaboutthestatesitmightgetitselfintoandtheactionsitcantake. The\nagent can use this information to consider subsequent stages of a hypothetical journey via\neachofthethreetowns,tryingtofindajourneythateventuallygetstoBucharest. Onceithas\nfound a path on the map from Arad to Bucharest, it can achieve its goal by carrying out the\ndriving actions that correspond to the legs of the journey. In general, an agent with several\nimmediate options ofunknown valuecan decide whattodobyfirst examining future actions\nthateventually leadtostatesofknownvalue.\nTo be more specific about what we mean by \u201cexamining future actions,\u201d we have to\nbe more specific about properties of the environment, as defined in Section 2.3. For now,\n1 Weareassumingthatmostreadersareinthesamepositionandcaneasilyimaginethemselvestobeasclueless\nasouragent.WeapologizetoRomanianreaderswhoareunabletotakeadvantageofthispedagogicaldevice. 66 Chapter 3. SolvingProblemsbySearching\nweassume that the environment is observable, so the agent always knows the current state.\nForthe agent driving in Romania, it\u2019s reasonable to suppose that each city on the maphas a\nsign indicating its presence to arriving drivers. Wealso assume the environment is discrete,\nso at any given state there are only finitely many actions to choose from. This is true for\nnavigating in Romania because each city isconnected toa small numberof other cities. We\nwillassumetheenvironment isknown,sotheagentknowswhichstatesarereachedbyeach\naction. (Having an accurate map suffices to meet this condition for navigation problems.)\nFinally, we assume that the environment is deterministic, so each action has exactly one\noutcome. Under ideal conditions, this is true for the agent in Romania\u2014it means that if it\nchooses to drive from Arad to Sibiu, it does end up in Sibiu. Of course, conditions are not\nalwaysideal, asweshowinChapter4.\nUnder these assumptions, the solution to any problem is a fixed sequence of actions.\n\u201cOfcourse!\u201d onemightsay,\u201cWhatelsecoulditbe?\u201d Well,ingeneralitcouldbeabranching\nstrategy that recommends different actions in the future depending on what percepts arrive.\nFor example, under less than ideal conditions, the agent might plan to drive from Arad to\nSibiu and then to Rimnicu Vilcea but may also need to have a contingency plan in case it\narrivesbyaccidentinZerindinstead ofSibiu. Fortunately, iftheagentknowstheinitialstate\nand the environment is known and deterministic, it knows exactly where it will be after the\nfirstaction and whatitwillperceive. Sinceonlyonepercept ispossible afterthefirstaction,\nthesolution canspecify onlyonepossible secondaction, andsoon.\nTheprocess oflooking forasequence ofactions thatreaches thegoaliscalledsearch.\nSEARCH\nA search algorithm takes a problem as input and returns a solution in the form of an action\nSOLUTION\nsequence. Once a solution is found, the actions it recommends can be carried out. This\nis called the execution phase. Thus, we have a simple \u201cformulate, search, execute\u201d design\nEXECUTION\nfor the agent, as shown in Figure 3.1. After formulating a goal and a problem to solve,\nthe agent calls a search procedure to solve it. It then uses the solution to guide its actions,\ndoingwhateverthesolutionrecommendsasthenextthingtodo\u2014typically, thefirstactionof\nthe sequence\u2014and then removing that step from the sequence. Once the solution has been\nexecuted, theagentwillformulate anewgoal.\nNotice that while the agent is executing the solution sequence it ignores its percepts\nwhen choosing an action because it knows in advance what they will be. An agent that\ncarries out its plans with its eyes closed, so to speak, must be quite certain of what is going\non. Controltheorists callthisan open-loopsystem,because ignoringthepercepts breaksthe\nOPEN-LOOP\nloopbetweenagentandenvironment.\nWe first describe the process of problem formulation, and then devote the bulk of the\nchapter to various algorithms for the SEARCH function. We do not discuss the workings of\nthe UPDATE-STATE and FORMULATE-GOAL functions furtherinthischapter.\n3.1.1 Well-defined problems and solutions\nAproblemcanbedefinedformallybyfivecomponents:\nPROBLEM\n\u2022 The initial state that the agent starts in. For example, the initial state for our agent in\nINITIALSTATE\nRomaniamightbedescribed asIn(Arad). Section3.1. Problem-Solving Agents 67\nfunctionSIMPLE-PROBLEM-SOLVING-AGENT(percept)returnsanaction\npersistent: seq,anactionsequence,initiallyempty\nstate,somedescriptionofthecurrentworldstate\ngoal,agoal,initiallynull\nproblem,aproblemformulation\nstate\u2190UPDATE-STATE(state,percept)\nifseq isemptythen\ngoal\u2190FORMULATE-GOAL(state)\nproblem\u2190FORMULATE-PROBLEM(state,goal)\nseq\u2190SEARCH(problem)\nifseq =failure thenreturnanullaction\naction\u2190FIRST(seq)\nseq\u2190REST(seq)\nreturnaction\nFigure 3.1 A simple problem-solving agent. It first formulates a goal and a problem,\nsearchesforasequenceofactionsthatwouldsolvetheproblem,andthenexecutestheactions\noneatatime. Whenthisiscomplete,itformulatesanothergoalandstartsover.\n\u2022 A description of the possible actions available to the agent. Given a particular state s,\nACTIONS\nACTIONS(s) returns the set of actions that can be executed in s. We say that each of\nthese actions is applicable in s. Forexample, from the state In(Arad), the applicable\nAPPLICABLE\nactionsare{Go(Sibiu),Go(Timisoara),Go(Zerind)}.\n\u2022 A description of what each action does; the formal name for this is the transition\nTRANSITIONMODEL\nmodel, specified by a function RESULT(s,a) that returns the state that results from\ndoingactionainstates. Wealsousethetermsuccessortorefertoanystatereachable\nSUCCESSOR\nfromagivenstatebyasingleaction.2 Forexample,wehave\nRESULT(In(Arad),Go(Zerind)) = In(Zerind).\nTogether,theinitialstate,actions,andtransitionmodelimplicitlydefinethestatespace\nSTATESPACE\nof the problem\u2014the set of all states reachable from the initial state by any sequence\nof actions. The state space forms a directed network or graph in which the nodes\nGRAPH\nare states and the links between nodes are actions. (The map of Romania shown in\nFigure 3.2 can be interpreted as a state-space graph if we view each road as standing\nfortwodriving actions, one ineach direction.) A pathin the state space is asequence\nPATH\nofstatesconnected byasequence ofactions.\n\u2022 Thegoal test, whichdetermines whetheragiven stateisagoal state. Sometimesthere\nGOALTEST\nis an explicit set of possible goal states, and the test simply checks whether the given\nstateisoneofthem. Theagent\u2019sgoalinRomaniaisthesingletonset{In(Bucharest)}.\n2 Manytreatmentsofproblemsolving,includingpreviouseditionsofthisbook,useasuccessorfunction,which\nreturnsthe set of allsuccessors, instead of separate ACTIONSand RESULTfunctions. The successor function\nmakesitdifficulttodescribeanagentthatknowswhatactionsitcantrybutnotwhattheyachieve. Also,note\nsomeauthoruseRESULT(a,s)insteadofRESULT(s,a),andsomeuseDOinsteadofRESULT. 68 Chapter 3. SolvingProblemsbySearching\nOradea\n71\nNeamt\nZerind 87\n151\n75\nIasi\nArad\n140\n92\nSibiu Fagaras\n99\n118\nVaslui\n80\nRimnicu Vilcea\nTimisoara\n142\n111 Lugoj 97 Pitesti 211\n70 98\nHirsova\nMehadia 146 101 85 Urziceni\n86\n75 138\nBucharest\nDrobeta 120\n90\nCraiova Giurgiu Eforie\nFigure3.2 AsimplifiedroadmapofpartofRomania.\nSometimesthegoalisspecifiedbyanabstractpropertyratherthananexplicitlyenumer-\natedsetofstates. Forexample,inchess,thegoalistoreach astatecalled\u201ccheckmate,\u201d\nwheretheopponent\u2019s kingisunderattackandcan\u2019tescape.\n\u2022 A path cost function that assigns a numeric cost to each path. The problem-solving\nPATHCOST\nagent chooses acost function that reflects its ownperformance measure. Forthe agent\ntryingtogettoBucharest,timeisoftheessence,sothecostofapathmightbeitslength\ninkilometers. Inthischapter, weassumethatthecostofapathcanbedescribed asthe\nsumofthecostsoftheindividualactionsalongthepath.3 Thestepcostoftakingaction\nSTEPCOST\n(cid:2) (cid:2)\na in state s to reach state s is denoted by c(s,a,s). The step costs for Romania are\nshowninFigure3.2asroutedistances. Weassumethatstepcostsarenonnegative.4\nThe preceding elements define a problem and can be gathered into a single data structure\nthat is given as input to a problem-solving algorithm. A solution to a problem is an action\nsequence that leads from the initial state to agoal state. Solution quality is measured by the\npathcostfunction, andanoptimalsolutionhasthelowestpathcostamongallsolutions.\nOPTIMALSOLUTION\n3.1.2 Formulatingproblems\nIntheprecedingsectionweproposed aformulationoftheproblemofgettingtoBucharestin\ntermsoftheinitial state, actions, transition model, goal test, andpath cost. Thisformulation\nseems reasonable, but it is still a model\u2014an abstract mathematical description\u2014and not the\n3 Thisassumptionisalgorithmicallyconvenientbutalsotheoreticallyjustifiable\u2014seepage649inChapter17.\n4 TheimplicationsofnegativecostsareexploredinExercise3.8. Section3.2. ExampleProblems 69\nrealthing. Comparethesimplestatedescriptionwehavechosen,In(Arad),toanactualcross-\ncountrytrip,wherethestateoftheworldincludessomanythings: thetravelingcompanions,\nthe current radio program, the scenery out of the window, the proximity of law enforcement\nofficers, the distance to the next rest stop, the condition of the road, the weather, and so on.\nAlltheseconsiderations areleftoutofourstatedescriptionsbecausetheyareirrelevanttothe\nproblemoffindingaroutetoBucharest. Theprocessofremovingdetailfromarepresentation\niscalledabstraction.\nABSTRACTION\nInadditiontoabstractingthestatedescription, wemustabstracttheactionsthemselves.\nA driving action has many effects. Besides changing the location of the vehicle and its oc-\ncupants, ittakes uptime, consumes fuel, generates pollution, andchanges the agent (as they\nsay, travel is broadening). Our formulation takes into account only the change in location.\nAlso, there are many actions that we omit altogether: turning on the radio, looking out of\nthewindow,slowing downforlawenforcement officers, andso on. Andofcourse, wedon\u2019t\nspecifyactions atthelevelof\u201cturnsteeringwheeltotheleftbyonedegree.\u201d\nCanwebemorepreciseaboutdefiningtheappropriatelevelofabstraction? Thinkofthe\nabstract states and actions we have chosen as corresponding to large sets of detailed world\nstates and detailed action sequences. Now consider a solution to the abstract problem: for\nexample,thepathfromAradtoSibiutoRimnicuVilceatoPitestitoBucharest. Thisabstract\nsolution corresponds to alarge numberofmore detailed paths. Forexample, wecould drive\nwith the radio on between Sibiu and Rimnicu Vilcea, and then switch it off for the rest of\nthetrip. Theabstraction is valid ifwecanexpand anyabstract solution intoasolution inthe\nmore detailed world; a sufficient condition is that for every detailed state that is \u201cin Arad,\u201d\nthere is a detailed path to some state that is \u201cin Sibiu,\u201d and so on.5 The abstraction is useful\nif carrying out each of the actions in the solution is easier than the original problem; in this\ncase they are easy enough that they can be carried out without further search orplanning by\nan average driving agent. The choice of a good abstraction thus involves removing as much\ndetail as possible while retaining validity and ensuring that the abstract actions are easy to\ncarryout. Wereitnotfortheabilitytoconstruct usefulabstractions, intelligent agentswould\nbecompletelyswampedbytherealworld.\n3.2 EXAMPLE PROBLEMS\nThe problem-solving approach has been applied to a vast array of task environments. We\nlist some of the best known here, distinguishing between toy and real-world problems. A\ntoyproblem isintended toillustrate orexercise various problem-solving methods. Itcanbe\nTOYPROBLEM\ngivenaconcise,exactdescriptionandhenceisusablebydifferentresearcherstocomparethe\nREAL-WORLD performance of algorithms. A real-world problem is one whose solutions people actually\nPROBLEM\ncareabout. Suchproblemstendnottohaveasingleagreed-upondescription, butwecangive\nthegeneralflavoroftheirformulations.\n5 SeeSection11.2foramorecompletesetofdefinitionsandalgorithms. 70 Chapter 3. SolvingProblemsbySearching\nR\nL R\nL\nS S\nR R\nL R L R\nL L\nS S\nS S\nR\nL R\nL\nS S\nFigure 3.3 The state space for the vacuum world. Links denote actions: L = Left, R =\nRight,S=Suck.\n3.2.1 Toy problems\nThe first example we examine is the vacuum world first introduced in Chapter 2. (See\nFigure2.2.) Thiscanbeformulatedasaproblem asfollows:\n\u2022 States: The state is determined by both the agent location and the dirt locations. The\nagent is in one of two locations, each of which might or might not contain dirt. Thus,\nthere are 2\u00d722 = 8 possible world states. A larger environment with n locations has\nn\u00b72n states.\n\u2022 Initialstate: Anystatecanbedesignated astheinitialstate.\n\u2022 Actions: In this simple environment, each state has just three actions: Left, Right, and\nSuck. Largerenvironments mightalsoinclude UpandDown.\n\u2022 Transition model: Theactions have their expected effects, except that moving Left in\ntheleftmostsquare,movingRightintherightmostsquare,andSuckinginacleansquare\nhavenoeffect. ThecompletestatespaceisshowninFigure3.3.\n\u2022 Goaltest: Thischeckswhetherallthesquares areclean.\n\u2022 Pathcost: Eachstepcosts1,sothepathcostisthenumberofstepsinthepath.\nCompared with the real world, this toy problem has discrete locations, discrete dirt, reliable\ncleaning, anditnevergetsanydirtier. Chapter4relaxessomeoftheseassumptions.\nThe8-puzzle,aninstanceofwhichisshowninFigure3.4,consistsofa3\u00d73boardwith\n8-PUZZLE\neight numbered tiles and a blank space. A tile adjacent to the blank space can slide into the\nspace. Theobject istoreach aspecified goalstate, such astheoneshownontherightofthe\nfigure. Thestandard formulation isasfollows: Section3.2. ExampleProblems 71\n7 2 4 1 2\n5 6 3 4 5\n8 3 1 6 7 8\nStart State Goal State\nFigure3.4 Atypicalinstanceofthe8-puzzle.\n\u2022 States: Astatedescription specifiesthelocationofeachoftheeighttilesandtheblank\ninoneoftheninesquares.\n\u2022 Initial state: Any state can be designated as the initial state. Note that any given goal\ncanbereachedfromexactlyhalfofthepossible initialstates(Exercise3.4).\n\u2022 Actions: Thesimplestformulationdefinestheactionsasmovementsoftheblankspace\nLeft, Right, Up, or Down. Different subsets of these are possible depending on where\ntheblankis.\n\u2022 Transitionmodel: Givenastateandaction,thisreturnstheresultingstate;forexample,\nifweapplyLefttothestartstateinFigure3.4,theresultingstatehasthe5andtheblank\nswitched.\n\u2022 Goaltest: Thischecks whetherthe state matches thegoal configuration showninFig-\nure3.4. (Othergoalconfigurations arepossible.)\n\u2022 Pathcost: Eachstepcosts1,sothepathcostisthenumberofstepsinthepath.\nWhat abstractions have weincluded here? The actions are abstracted to their beginning and\nfinalstates,ignoringtheintermediatelocationswheretheblockissliding. Wehaveabstracted\naway actions such as shaking the board when pieces get stuck and ruled out extracting the\npieceswithaknifeandputtingthembackagain. Weareleftwithadescriptionoftherulesof\nthepuzzle, avoiding allthedetailsofphysicalmanipulations.\nSLIDING-BLOCK The 8-puzzle belongs to the family of sliding-block puzzles, which are often used as\nPUZZLES\ntest problems for new search algorithms in AI. This family is known to be NP-complete,\nso one does not expect to find methods significantly better in the worst case than the search\nalgorithmsdescribedinthischapterandthenext. The8-puzzlehas9!\/2=181,440reachable\nstatesandiseasilysolved. The15-puzzle(ona4\u00d74board)hasaround1.3trillionstates,and\nrandominstancescanbesolvedoptimallyinafewmillisecondsbythebestsearchalgorithms.\nThe24-puzzle (on a 5\u00d75board) has around 1025 states, and random instances take several\nhourstosolveoptimally.\nThe goal of the 8-queens problem is to place eight queens on a chessboard such that\n8-QUEENSPROBLEM\nno queen attacks any other. (A queen attacks any piece in the same row, column or diago-\nnal.) Figure3.5shows anattempted solution thatfails: the queen inthe rightmost column is\nattacked bythequeenatthetopleft. 72 Chapter 3. SolvingProblemsbySearching\nFigure3.5 Almostasolutiontothe8-queensproblem.(Solutionisleftasanexercise.)\nAlthough efficient special-purpose algorithms exist forthis problem and forthe whole\nn-queens family, it remains auseful test problem forsearch algorithms. Thereare twomain\nINCREMENTAL kindsofformulation. Anincrementalformulationinvolvesoperatorsthataugmentthestate\nFORMULATION\ndescription, starting with an empty state; for the 8-queens problem, this means that each\nCOMPLETE-STATE action adds a queen to the state. A complete-state formulation starts with all 8 queens on\nFORMULATION\ntheboard andmovesthemaround. Ineithercase,thepathcost isofnointerest because only\nthefinalstatecounts. Thefirstincremental formulation one mighttryisthefollowing:\n\u2022 States: Anyarrangement of0to8queensontheboardisastate.\n\u2022 Initialstate: Noqueensontheboard.\n\u2022 Actions: Addaqueentoanyemptysquare.\n\u2022 Transitionmodel: Returnstheboardwithaqueenaddedtothespecifiedsquare.\n\u2022 Goaltest: 8queensareontheboard, noneattacked.\nInthisformulation, wehave64\u00b763\u00b7\u00b7\u00b757 \u2248 1.8\u00d71014 possible sequences toinvestigate. A\nbetterformulation wouldprohibitplacing aqueeninanysquarethatisalready attacked:\n\u2022 States: All possible arrangements of n queens (0 \u2264 n \u2264 8), one per column in the\nleftmostncolumns,withnoqueenattacking another.\n\u2022 Actions: Add a queen to any square in the leftmost empty column such that it is not\nattackedbyanyotherqueen.\nThisformulationreducesthe8-queensstatespacefrom1.8\u00d71014tojust2,057,andsolutions\nareeasytofind. Ontheotherhand,for100queensthereductionisfromroughly 10400 states\ntoabout1052 states(Exercise3.5)\u2014abigimprovement,butnotenoughtomaketheproblem\ntractable. Section4.1describes thecomplete-state formulation, andChapter6givesasimple\nalgorithm thatsolveseventhemillion-queens problemwith ease. Section3.2. ExampleProblems 73\nOurfinaltoyproblem wasdevisedbyDonaldKnuth(1964)andillustrates howinfinite\nstatespacescanarise. Knuthconjectured that,starting withthenumber4,asequence offac-\ntorial, square root, andflooroperations willreach anydesired positive integer. Forexample,\nwecanreach5from4asfollows:\n(cid:4)\n(cid:5)(cid:7)\n(cid:8)\n(cid:3)(cid:5) (cid:6) (cid:9) (cid:10) (cid:11)\n(4!)! = 5.\nTheproblem definitionisverysimple:\n\u2022 States: Positivenumbers.\n\u2022 Initialstate: 4.\n\u2022 Actions: Applyfactorial, squareroot,orflooroperation (factorial forintegersonly).\n\u2022 Transitionmodel: Asgivenbythemathematicaldefinitions oftheoperations.\n\u2022 Goaltest: Stateisthedesiredpositiveinteger.\nToourknowledge thereisnobound onhowlarge anumbermightbeconstructed inthepro-\ncessofreaching agiventarget\u2014for example, thenumber620,448,401,733,239,439,360,000\nis generated in the expression for 5\u2014so the state space for this problem is infinite. Such\nstate spaces arise frequently in tasks involving the generation of mathematical expressions,\ncircuits, proofs, programs, andotherrecursively definedobjects.\n3.2.2 Real-worldproblems\nROUTE-FINDING We have already seen how the route-finding problem is defined in terms of specified loca-\nPROBLEM\ntionsandtransitionsalonglinksbetweenthem. Route-findingalgorithmsareusedinavariety\nof applications. Some, such as Web sites and in-car systems that provide driving directions,\nare relatively straightforward extensions of the Romania example. Others, such as routing\nvideostreamsincomputernetworks,militaryoperationsplanning,andairlinetravel-planning\nsystems,involvemuchmorecomplexspecifications. Considertheairlinetravelproblemsthat\nmustbesolvedbyatravel-planning Website:\n\u2022 States: Eachstate obviously includes alocation (e.g., anairport) and the current time.\nFurthermore, because the cost of an action (a flight segment) may depend on previous\nsegments, their fare bases, and their status as domestic or international, the state must\nrecordextrainformation aboutthese\u201chistorical\u201d aspects.\n\u2022 Initialstate: Thisisspecifiedbytheuser\u2019squery.\n\u2022 Actions: Take any flight from the current location, in any seat class, leaving after the\ncurrenttime,leavingenough timeforwithin-airport transferifneeded.\n\u2022 Transition model: The state resulting from taking a flight will have the flight\u2019s desti-\nnationasthecurrentlocation andtheflight\u2019sarrivaltimeasthecurrenttime.\n\u2022 Goaltest: Areweatthefinaldestination specifiedbytheuser?\n\u2022 Path cost: This depends on monetary cost, waiting time, flight time, customs and im-\nmigrationprocedures, seatquality, timeofday,typeofairplane, frequent-flyer mileage\nawards,andsoon. 74 Chapter 3. SolvingProblemsbySearching\nCommercial travel advice systems use a problem formulation of this kind, with many addi-\ntional complications to handle the byzantine fare structures that airlines impose. Any sea-\nsoned traveler knows, however, that not all air travel goes according to plan. A really good\nsystemshouldincludecontingencyplans\u2014suchasbackupreservationsonalternateflights\u2014\ntotheextentthatthesearejustifiedbythecostandlikelihood offailureoftheoriginalplan.\nTouring problems are closely related to route-finding problems, but with an impor-\nTOURINGPROBLEM\ntant difference. Consider, for example, the problem \u201cVisit every city in Figure 3.2 at least\nonce, starting and ending in Bucharest.\u201d As with route finding, the actions correspond\nto trips between adjacent cities. The state space, however, is quite different. Each state\nmust include not just the current location but also the set of cities the agent has visited.\nSo the initial state would be In(Bucharest),Visited({Bucharest}), a typical intermedi-\nate state would be In(Vaslui),Visited({Bucharest,Urziceni,Vaslui}), and the goal test\nwouldcheckwhethertheagentisinBucharestandall20citieshavebeenvisited.\nTRAVELING\nThe traveling salesperson problem (TSP) is a touring problem in which each city\nSALESPERSON\nPROBLEM\nmust be visited exactly once. The aim is to find the shortest tour. The problem is known to\nbeNP-hard,butanenormousamountofefforthasbeenexpendedtoimprovethecapabilities\nof TSP algorithms. In addition to planning trips for traveling salespersons, these algorithms\nhavebeenusedfortaskssuchasplanningmovementsofautomaticcircuit-board drillsandof\nstocking machinesonshopfloors.\nA VLSIlayout problem requires positioning millions of components and connections\nVLSILAYOUT\non a chip to minimize area, minimize circuit delays, minimize stray capacitances, and max-\nimize manufacturing yield. The layout problem comes after the logical design phase and is\nusually split into two parts: cell layout and channel routing. In cell layout, the primitive\ncomponents of the circuit are grouped into cells, each of which performs some recognized\nfunction. Each cell has a fixed footprint (size and shape) and requires a certain number of\nconnectionstoeachoftheothercells. Theaimistoplacethecellsonthechipsothattheydo\nnotoverlapandsothatthereisroomfortheconnecting wires tobeplacedbetweenthecells.\nChannelroutingfindsaspecificrouteforeachwirethroughthegapsbetweenthecells. These\nsearch problems are extremely complex, but definitely worth solving. Later in this chapter,\nwepresentsomealgorithmscapable ofsolving them.\nRobot navigation is a generalization of the route-finding problem described earlier.\nROBOTNAVIGATION\nRather than following a discrete set of routes, a robot can move in a continuous space with\n(in principle) an infinite set of possible actions and states. For a circular robot moving on a\nflat surface, the space is essentially two-dimensional. When the robot has arms and legs or\nwheelsthatmustalsobecontrolled, thesearchspacebecomesmany-dimensional. Advanced\ntechniques are required just to make the search space finite. We examine some of these\nmethods in Chapter 25. In addition to the complexity of the problem, real robots must also\ndealwitherrorsintheirsensorreadings andmotorcontrols.\nAUTOMATIC\nASSEMBLY\nAutomaticassemblysequencingofcomplexobjectsbyarobotwasfirstdemonstrated\nSEQUENCING\nby FREDDY (Michie, 1972). Progress since then has been slow but sure, to the point where\ntheassemblyofintricateobjectssuchaselectricmotorsiseconomicallyfeasible. Inassembly\nproblems, the aim is to find an order in which to assemble the parts of some object. If the\nwrong order is chosen, there will be no way to add some part later in the sequence without Section3.3. SearchingforSolutions 75\nundoing some of the work already done. Checking a step in the sequence for feasibility is a\ndifficultgeometricalsearchproblemcloselyrelatedtorobotnavigation. Thus,thegeneration\nof legal actions is the expensive part of assembly sequencing. Any practical algorithm must\navoidexploringallbutatinyfractionofthestatespace. Anotherimportantassemblyproblem\nisprotein design, inwhichthegoalistofindasequence ofaminoacids thatwillfoldinto a\nPROTEINDESIGN\nthree-dimensional proteinwiththerightproperties tocuresomedisease.\n3.3 SEARCHING FOR SOLUTIONS\nHaving formulated some problems, we now need to solve them. A solution is an action\nsequence, so search algorithms work by considering various possible action sequences. The\npossible action sequences starting at the initial state form asearch tree with the initial state\nSEARCHTREE\nat the root; the branches are actions and the nodes correspond to states in the state space of\nNODE\ntheproblem. Figure3.6showsthefirstfewstepsingrowingthesearchtreeforfindingaroute\nfrom Arad to Bucharest. The root node of the tree corresponds to the initial state, In(Arad).\nThe first step is to test whether this is a goal state. (Clearly it is not, but it is important to\ncheck so that we can solve trick problems like \u201cstarting in Arad, get to Arad.\u201d) Then we\nneed to consider taking various actions. We do this by expanding the current state; that is,\nEXPANDING\napplying each legal action to the current state, thereby generating a new set of states. In\nGENERATING\nthis case, we add three branches from the parent node In(Arad) leading to three new child\nPARENTNODE\nnodes: In(Sibiu), In(Timisoara), and In(Zerind). Now we must choose which of these three\nCHILDNODE\npossibilities toconsiderfurther.\nThisistheessenceofsearch\u2014followinguponeoptionnowandputtingtheothersaside\nfor later, in case the first choice does not lead to a solution. Suppose we choose Sibiu first.\nWe check to see whether it is a goal state (it is not) and then expand it to get In(Arad),\nIn(Fagaras), In(Oradea),andIn(RimnicuVilcea). Wecanthenchooseanyofthesefourorgo\nback and choose Timisoara orZerind. Each ofthese sixnodes isa leaf node, that is, anode\nLEAFNODE\nwith no children in the tree. The set of all leaf nodes available for expansion at any given\npointiscalledthefrontier. (Manyauthors callittheopenlist, whichisbothgeographically\nFRONTIER\nlessevocativeandlessaccurate, becauseotherdatastructures arebettersuitedthanalist.) In\nOPENLIST\nFigure3.6,thefrontierofeachtreeconsists ofthosenodes withboldoutlines.\nTheprocessofexpandingnodesonthefrontiercontinuesuntileitherasolutionisfound\northerearenomorestatestoexpand. Thegeneral TREE-SEARCH algorithm isshowninfor-\nmally in Figure 3.7. Search algorithms all share this basic structure; they vary primarily\naccording tohowtheychoosewhichstatetoexpandnext\u2014theso-called search strategy.\nSEARCHSTRATEGY\nTheeagle-eyedreaderwillnoticeonepeculiarthingaboutthesearchtreeshowninFig-\nure3.6: itincludesthepathfromAradtoSibiuandbacktoAradagain! WesaythatIn(Arad)\nis a repeated state in the search tree, generated in this case by a loopy path. Considering\nREPEATEDSTATE\nsuch loopy paths means that the complete search tree for Romania is infinite because there\nLOOPYPATH\nis no limit to how often one can traverse a loop. On the other hand, the state space\u2014the\nmapshowninFigure3.2\u2014hasonly 20states. Aswediscuss inSection 3.4,loops cancause 76 Chapter 3. SolvingProblemsbySearching\ncertainalgorithmstofail,makingotherwisesolvableproblemsunsolvable. Fortunately,there\nisnoneedtoconsiderloopypaths. Wecanrelyonmorethanintuition forthis: because path\ncosts are additive and step costs are nonnegative, a loopy path to any given state is never\nbetterthanthesamepathwiththeloopremoved.\nLoopypathsareaspecialcaseofthemoregeneralconceptofredundantpaths,which\nREDUNDANTPATH\nexistwheneverthereismorethanonewaytogetfromonestatetoanother. Considerthepaths\nArad\u2013Sibiu (140 km long) and Arad\u2013Zerind\u2013Oradea\u2013Sibiu (297 km long). Obviously, the\nsecondpathisredundant\u2014it\u2019s justaworsewaytogettothesamestate. Ifyouareconcerned\nabout reaching the goal, there\u2019s never any reason to keep more than one path to any given\nstate, because any goal state that is reachable by extending one path is also reachable by\nextending theother.\nIn some cases, it is possible to define the problem itself so as to eliminate redundant\npaths. Forexample, if we formulate the 8-queens problem (page 71) so that a queen can be\nplacedinanycolumn,theneachstatewithnqueenscanbereachedbyn!differentpaths;but\nifwereformulatetheproblemsothateachnewqueenisplacedintheleftmostemptycolumn,\ntheneachstatecanbereached onlythrough onepath.\n(a) The initial state Arad\nSibiu Timisoara Zerind\nArad Fagaras Oradea Rimnicu Vilcea Arad Lugoj Arad Oradea\n(b) After expanding Arad Arad\nSibiu Timisoara Zerind\nArad Fagaras Oradea Rimnicu Vilcea Arad Lugoj Arad Oradea\n(c) After expanding Sibiu Arad\nSibiu Timisoara Zerind\nArad Fagaras Oradea Rimnicu Vilcea Arad Lugoj Arad Oradea\nFigure 3.6 Partial search trees for finding a route from Arad to Bucharest. Nodes that\nhave been expanded are shaded; nodes that have been generated but not yet expanded are\noutlinedinbold;nodesthathavenotyetbeengeneratedareshowninfaintdashedlines. Section3.3. SearchingforSolutions 77\nfunctionTREE-SEARCH(problem)returnsasolution,orfailure\ninitializethefrontierusingtheinitialstateofproblem\nloopdo\nifthefrontierisemptythenreturnfailure\nchoosealeafnodeandremoveitfromthefrontier\nifthenodecontainsagoalstatethenreturnthecorrespondingsolution\nexpandthechosennode,addingtheresultingnodestothefrontier\nfunctionGRAPH-SEARCH(problem)returnsasolution,orfailure\ninitializethefrontierusingtheinitialstateofproblem\ninitializetheexploredsettobeempty\nloopdo\nifthefrontierisemptythenreturnfailure\nchoosealeafnodeandremoveitfromthefrontier\nifthenodecontainsagoalstatethenreturnthecorrespondingsolution\naddthenodetotheexploredset\nexpandthechosennode,addingtheresultingnodestothefrontier\nonlyifnotinthefrontierorexploredset\nFigure 3.7 An informal description of the general tree-search and graph-search algo-\nrithms. The parts of GRAPH-SEARCH marked in bold italic are the additions needed to\nhandlerepeatedstates.\nIn other cases, redundant paths are unavoidable. This includes all problems where\nthe actions are reversible, such as route-finding problems and sliding-block puzzles. Route-\nfindingonarectangular grid(liketheoneusedlaterforFigure3.9)isaparticularly impor-\nRECTANGULARGRID\ntant example in computer games. In such a grid, each state has four successors, so a search\ntreeofdepthdthatincludesrepeatedstateshas4dleaves;butthereareonlyabout2d2distinct\nstateswithindstepsofanygivenstate. Ford= 20,thismeansaboutatrillionnodesbutonly\nabout 800 distinct states. Thus, following redundant paths can cause a tractable problem to\nbecomeintractable. Thisistrueevenforalgorithms thatknowhowtoavoidinfiniteloops.\nAs the saying goes, algorithms that forget their history are doomed to repeat it. The\nway to avoid exploring redundant paths is to remember where one has been. To do this, we\nEXPLOREDSET\naugment the TREE-SEARCH algorithm with a data structure called the explored set (also\nknown as the closed list), which remembers every expanded node. Newly generated nodes\nCLOSEDLIST\nthatmatchpreviously generated nodes\u2014ones intheexplored setorthefrontier\u2014can bedis-\ncardedinsteadofbeingaddedtothefrontier. Thenewalgorithm, called GRAPH-SEARCH,is\nshown informally in Figure 3.7. Thespecific algorithms inthis chapter draw onthis general\ndesign.\nClearly,thesearchtreeconstructedbytheGRAPH-SEARCH algorithmcontainsatmost\nonecopyofeachstate,sowecanthinkofitasgrowingatreedirectlyonthestate-spacegraph,\nas shown in Figure 3.8. The algorithm has another nice property: the frontier separates the\nSEPARATOR\nstate-space graphintotheexploredregionandtheunexplored region,sothateverypathfrom 78 Chapter 3. SolvingProblemsbySearching\nFigure3.8 AsequenceofsearchtreesgeneratedbyagraphsearchontheRomaniaprob-\nlemofFigure3.2. Ateachstage,wehaveextendedeachpathbyonestep. Noticethatatthe\nthirdstage,thenorthernmostcity(Oradea)hasbecomeadeadend:bothofitssuccessorsare\nalreadyexploredviaotherpaths.\n(a) (b) (c)\nFigure3.9 TheseparationpropertyofGRAPH-SEARCH,illustratedonarectangular-grid\nproblem. Thefrontier(whitenodes)alwaysseparatestheexploredregionofthestatespace\n(black nodes) from the unexplored region (gray nodes). In (a), just the root has been ex-\npanded.In(b),oneleafnodehasbeenexpanded.In(c),theremainingsuccessorsoftheroot\nhavebeenexpandedinclockwiseorder.\nthe initial state to an unexplored state has to pass through a state in the frontier. (If this\nseemscompletely obvious, tryExercise 3.13now.) Thisproperty isillustrated inFigure 3.9.\nAs every step moves a state from the frontier into the explored region while moving some\nstatesfromtheunexploredregionintothefrontier,weseethatthealgorithmissystematically\nexaminingthestatesinthestatespace, onebyone,untilitfindsasolution.\n3.3.1 Infrastructure forsearch algorithms\nSearch algorithms require a data structure to keep track of the search tree that is being con-\nstructed. Foreachnode nofthetree,wehaveastructure thatcontains fourcomponents:\n\u2022 n.STATE: thestateinthestatespacetowhichthenodecorresponds;\n\u2022 n.PARENT: thenodeinthesearchtreethatgenerated thisnode;\n\u2022 n.ACTION: theactionthatwasappliedtotheparenttogeneratethenode;\n\u2022 n.PATH-COST: thecost,traditionally denotedbyg(n),ofthepathfromtheinitialstate\ntothenode,asindicated bytheparentpointers. Section3.3. SearchingforSolutions 79\nPARENT\nNode\n55 44 ACTION = Right\nPATH-COST = 6\n66 11 88\nSTATE\n77 33 22\nFigure3.10 Nodesarethedatastructuresfromwhichthesearchtreeisconstructed.Each\nhasaparent,astate,andvariousbookkeepingfields. Arrowspointfromchildtoparent.\nGiven the components for a parent node, it is easy to see how to compute the necessary\ncomponents fora child node. The function CHILD-NODE takes a parent node and an action\nandreturnstheresulting childnode:\nfunctionCHILD-NODE(problem,parent,action)returnsanode\nreturnanodewith\nSTATE=problem.RESULT(parent.STATE,action),\nPARENT=parent,ACTION=action,\nPATH-COST=parent.PATH-COST+problem.STEP-COST(parent.STATE,action)\nThe node data structure is depicted in Figure 3.10. Notice how the PARENT pointers\nstringthenodestogetherintoatreestructure. Thesepointersalsoallowthesolutionpathtobe\nextracted when agoal node is found; weuse the SOLUTION function to return the sequence\nofactions obtained byfollowingparentpointers backtotheroot.\nUptonow,wehavenotbeenverycarefultodistinguishbetweennodesandstates,butin\nwriting detailed algorithms it\u2019s important to make that distinction. A node isa bookkeeping\ndata structure usedtorepresent the search tree. Astate corresponds toaconfiguration ofthe\nworld. Thus, nodes are on particular paths, as defined by PARENT pointers, whereas states\nare not. Furthermore, two different nodes can contain the same world state if that state is\ngenerated viatwodifferent searchpaths.\nNow that we have nodes, we need somewhere to put them. The frontier needs to be\nstored in such a way that the search algorithm can easily choose the next node to expand\naccording to its preferred strategy. The appropriate data structure for this is a queue. The\nQUEUE\noperations onaqueueareasfollows:\n\u2022 EMPTY?(queue)returnstrueonlyiftherearenomoreelementsinthequeue.\n\u2022 POP(queue)removesthefirstelementofthequeueandreturnsit.\n\u2022 INSERT(element,queue)insertsanelementandreturns theresulting queue. 80 Chapter 3. SolvingProblemsbySearching\nQueuesarecharacterized bythe orderinwhichtheystoretheinsertednodes. Threecommon\nvariantsarethefirst-in,first-outor FIFOqueue,whichpopstheoldestelementofthequeue;\nFIFOQUEUE\nthelast-in, first-outorLIFOqueue(also knownasastack), whichpops thenewestelement\nLIFOQUEUE\nof the queue; and the priority queue,which pops the element of the queue with the highest\nPRIORITYQUEUE\npriorityaccording tosomeorderingfunction.\nThe explored set can be implemented with a hash table to allow efficient checking for\nrepeated states. With a good implementation, insertion and lookup can be done in roughly\nconstant time no matter how many states are stored. One must take care to implement the\nhash table with the right notion of equality between states. For example, in the traveling\nsalesperson problem (page 74), the hash table needs to know that the set of visited cities\n{Bucharest,Urziceni,Vaslui} isthesameas{Urziceni,Vaslui,Bucharest}. Sometimesthiscan\nbe achieved most easily by insisting that the data structures for states be in some canonical\nform; that is, logically equivalent states should map to the same data structure. In the case\nCANONICALFORM\nof states described by sets, for example, a bit-vector representation or a sorted list without\nrepetition wouldbecanonical, whereasanunsorted listwouldnot.\n3.3.2 Measuring problem-solving performance\nBefore we get into the design of specific search algorithms, we need to consider the criteria\nthat might be used to choose among them. We can evaluate an algorithm\u2019s performance in\nfourways:\n\u2022 Completeness: Isthealgorithm guaranteed tofindasolution whenthereisone?\nCOMPLETENESS\n\u2022 Optimality: Doesthestrategyfindtheoptimalsolution, asdefinedonpage68?\nOPTIMALITY\n\u2022 Timecomplexity: Howlongdoesittaketofindasolution?\nTIMECOMPLEXITY\n\u2022 Spacecomplexity: Howmuchmemoryisneededtoperformthesearch?\nSPACECOMPLEXITY\nTimeandspacecomplexityarealwaysconsidered withrespecttosomemeasureoftheprob-\nlem difficulty. In theoretical computer science, the typical measure is the size of the state\nspace graph, |V|+|E|, where V is the set of vertices (nodes) of the graph and E is the set\nofedges (links). Thisisappropriate whenthe graph isan explicit data structure that isinput\ntothesearchprogram. (ThemapofRomaniaisanexampleofthis.) InAI,thegraphisoften\nrepresented implicitly bytheinitialstate, actions, andtransition modelandisfrequently infi-\nnite. Forthesereasons,complexityisexpressedintermsofthreequantities: b,thebranching\nfactor or maximum number of successors of any node; d, the depth of the shallowest goal\nBRANCHINGFACTOR\nnode(i.e.,thenumberofstepsalong thepathfromtheroot); andm,themaximumlength of\nDEPTH\nanypathinthestatespace. Timeisoftenmeasuredintermsofthenumberofnodesgenerated\nduring the search, and space in terms of the maximum number of nodes stored in memory.\nFor the most part, we describe time and space complexity for search on a tree; for a graph,\ntheanswerdepends onhow\u201credundant\u201d thepathsinthestatespaceare.\nToassesstheeffectivenessofasearchalgorithm,wecanconsiderjustthesearchcost\u2014\nSEARCHCOST\nwhich typically depends on the time complexity but can also include a term for memory\nusage\u2014orwecanusethetotalcost,whichcombinesthesearchcostandthepathcostofthe\nTOTALCOST\nsolution found. For the problem of finding a route from Arad to Bucharest, the search cost\nisthe amount of timetaken bythe search and thesolution cost isthe total length of thepath Section3.4. UninformedSearchStrategies 81\nin kilometers. Thus, to compute the total cost, we have to add milliseconds and kilometers.\nThereisno\u201cofficialexchangerate\u201dbetweenthetwo,butitmightbereasonableinthiscaseto\nconvertkilometersintomillisecondsbyusinganestimateofthecar\u2019saveragespeed(because\ntime is what the agent cares about). This enables the agent to find an optimal tradeoff point\nat which further computation to find a shorter path becomes counterproductive. The more\ngeneralproblem oftradeoffs betweendifferentgoodsistakenupinChapter16.\n3.4 UNINFORMED SEARCH STRATEGIES\nThis section covers several search strategies that come under the heading of uninformed\nUNINFORMED search (also called blind search). The term means that the strategies have no additional\nSEARCH\ninformation about states beyond that provided in the problem definition. All they can do is\nBLINDSEARCH\ngenerate successors and distinguish a goal state from a non-goal state. All search strategies\nare distinguished by the order in which nodes are expanded. Strategies that know whether\nonenon-goalstateis\u201cmorepromising\u201dthananotherarecalledinformedsearchorheuristic\nINFORMEDSEARCH\nsearch strategies; theyarecoveredinSection3.5.\nHEURISTICSEARCH\n3.4.1 Breadth-first search\nBREADTH-FIRST Breadth-firstsearchisasimplestrategyinwhichtherootnodeisexpandedfirst,thenallthe\nSEARCH\nsuccessors of the root node are expanded next, then their successors, and so on. In general,\nall the nodes are expanded at a given depth in the search tree before any nodes at the next\nlevelareexpanded.\nBreadth-firstsearchisaninstanceofthegeneralgraph-search algorithm (Figure3.7)in\nwhichtheshallowestunexpandednodeischosenforexpansion. Thisisachievedverysimply\nbyusingaFIFOqueueforthefrontier. Thus,newnodes(which arealwaysdeeperthantheir\nparents)gotothebackofthequeue,andoldnodes,whichareshallowerthanthenewnodes,\ngetexpandedfirst. Thereisoneslighttweakonthegeneralgraph-search algorithm,whichis\nthatthegoaltestisappliedtoeachnodewhenitisgeneratedratherthanwhenitisselectedfor\nexpansion. This decision is explained below, where we discuss time complexity. Note also\nthatthealgorithm, followingthegeneral templateforgraphsearch, discards anynewpathto\na state already in the frontier or explored set; it is easy to see that any such path must be at\nleast as deep as the one already found. Thus, breadth-first search always has the shallowest\npathtoeverynodeonthefrontier.\nPseudocode isgiven inFigure 3.11. Figure 3.12 shows the progress ofthe search on a\nsimplebinarytree.\nHowdoesbreadth-first search rateaccording tothefourcriteria from theprevious sec-\ntion? Wecaneasilyseethatitiscomplete\u2014iftheshallowestgoalnodeisatsomefinitedepth\nd, breadth-first search will eventually find it after generating all shallower nodes (provided\nthe branching factor b is finite). Note that as soon as a goal node is generated, we know it\nis the shallowest goal node because all shallower nodes must have been generated already\nand failed the goal test. Now, the shallowest goal node is not necessarily the optimal one; 82 Chapter 3. SolvingProblemsbySearching\nfunctionBREADTH-FIRST-SEARCH(problem)returnsasolution,orfailure\nnode\u2190anodewithSTATE=problem.INITIAL-STATE,PATH-COST=0\nifproblem.GOAL-TEST(node.STATE)thenreturnSOLUTION(node)\nfrontier\u2190aFIFOqueuewithnode astheonlyelement\nexplored\u2190anemptyset\nloopdo\nifEMPTY?(frontier)thenreturnfailure\nnode\u2190POP(frontier) \/*choosestheshallowestnodeinfrontier *\/\naddnode.STATEtoexplored\nforeachaction inproblem.ACTIONS(node.STATE)do\nchild\u2190CHILD-NODE(problem,node,action)\nifchild.STATEisnotinexplored orfrontier then\nifproblem.GOAL-TEST(child.STATE)thenreturnSOLUTION(child)\nfrontier\u2190INSERT(child,frontier)\nFigure3.11 Breadth-firstsearchonagraph.\ntechnically, breadth-first search isoptimal ifthe path cost is anondecreasing function of the\ndepthofthenode. Themostcommonsuchscenarioisthatallactions havethesamecost.\nSo far, the news about breadth-first search has been good. The news about time and\nspace is not so good. Imagine searching a uniform tree where every state has b successors.\nTherootofthesearchtreegenerates bnodesatthefirstlevel,eachofwhichgenerates bmore\nnodes, foratotalofb2 atthesecond level. Eachofthesegenerates bmorenodes, yielding b3\nnodes atthe third level, and soon. Nowsuppose that the solution isatdepth d. Inthe worst\ncase,itisthelastnodegenerated atthatlevel. Thenthetotalnumberofnodesgenerated is\nb+b2+b3+\u00b7\u00b7\u00b7+bd = O(bd).\n(Ifthealgorithmweretoapplythegoaltesttonodeswhenselectedforexpansion,ratherthan\nwhengenerated, thewholelayerofnodesatdepth dwouldbeexpanded before thegoalwas\ndetected andthetimecomplexitywouldbeO(bd+1).)\nAs for space complexity: for any kind of graph search, which stores every expanded\nnode in the explored set, the space complexity is always within a factor of b of the time\ncomplexity. For breadth-first graph search in particular, every node generated remains in\nmemory. There will be O(bd\u22121) nodes in the explored set and O(bd) nodes in the frontier,\nA A A A\nB C B C B C B C\nD E F G D E F G D E F G D E F G\nFigure 3.12 Breadth-first search on a simple binary tree. At each stage, the node to be\nexpandednextisindicatedbyamarker. Section3.4. UninformedSearchStrategies 83\nso the space complexity is O(bd), i.e., it is dominated by the size of the frontier. Switching\ntoatreesearch would notsavemuchspace, andinastate space withmanyredundant paths,\nswitchingcouldcostagreatdealoftime.\nAn exponential complexity bound such as O(bd) is scary. Figure 3.13 shows why. It\nlists, forvarious values ofthesolution depth d,thetimeandmemoryrequired forabreadth-\nfirst search with branching factor b = 10. The table assumes that 1 million nodes can be\ngenerated persecond and that anode requires 1000 bytes ofstorage. Manysearch problems\nfit roughly within these assumptions (give or take a factor of 100) when run on a modern\npersonal computer.\nDepth Nodes Time Memory\n2 110 .11 milliseconds 107 kilobytes\n4 11,110 11 milliseconds 10.6 megabytes\n6 106 1.1 seconds 1 gigabyte\n8 108 2 minutes 103 gigabytes\n10 1010 3 hours 10 terabytes\n12 1012 13 days 1 petabyte\n14 1014 3.5 years 99 petabytes\n16 1016 350 years 10 exabytes\nFigure3.13 Timeandmemoryrequirementsforbreadth-firstsearch.Thenumbersshown\nassumebranchingfactorb=10;1millionnodes\/second;1000bytes\/node.\nTwo lessons can be learned from Figure 3.13. First, the memory requirements are a\nbigger problem for breadth-first search than is the execution time. One might wait 13 days\nforthesolution toanimportant problem withsearch depth 12,butnopersonal computerhas\nthepetabyte ofmemoryitwouldtake. Fortunately, otherstrategies requirelessmemory.\nThe second lesson is that time is still a major factor. If your problem has a solution at\ndepth16,then(givenourassumptions)itwilltakeabout350yearsforbreadth-firstsearch(or\nindeedanyuninformedsearch)tofindit. Ingeneral, exponential-complexity searchproblems\ncannotbesolvedbyuninformed methodsforanybutthesmallestinstances.\n3.4.2 Uniform-costsearch\nWhen all step costs are equal, breadth-first search is optimal because it always expands the\nshallowestunexpandednode. Byasimpleextension,wecanfindanalgorithmthatisoptimal\nUNIFORM-COST withanystep-cost function. Instead ofexpanding theshallowest node, uniform-costsearch\nSEARCH\nexpands the node n with the lowest path cost g(n). This is done by storing the frontier as a\npriorityqueueordered by g. Thealgorithm isshowninFigure3.14.\nIn addition to the ordering of the queue by path cost, there are two other significant\ndifferences from breadth-first search. Thefirstisthat thegoal test isapplied toanode when\nit is selected for expansion (as in the generic graph-search algorithm shown in Figure 3.7)\nrather than when it is first generated. The reason is that the first goal node that is generated 84 Chapter 3. SolvingProblemsbySearching\nfunctionUNIFORM-COST-SEARCH(problem)returnsasolution,orfailure\nnode\u2190anodewithSTATE=problem.INITIAL-STATE,PATH-COST=0\nfrontier\u2190apriorityqueueorderedbyPATH-COST,withnode astheonlyelement\nexplored\u2190anemptyset\nloopdo\nifEMPTY?(frontier)thenreturnfailure\nnode\u2190POP(frontier) \/*choosesthelowest-costnodeinfrontier *\/\nifproblem.GOAL-TEST(node.STATE)thenreturnSOLUTION(node)\naddnode.STATEtoexplored\nforeachaction inproblem.ACTIONS(node.STATE)do\nchild\u2190CHILD-NODE(problem,node,action)\nifchild.STATEisnotinexplored orfrontier then\nfrontier\u2190INSERT(child,frontier)\nelseifchild.STATEisinfrontier withhigherPATH-COSTthen\nreplacethatfrontier nodewithchild\nFigure 3.14 Uniform-cost search on a graph. The algorithm is identical to the general\ngraphsearchalgorithminFigure3.7,exceptfortheuseofapriorityqueueandtheaddition\nofanextracheckincaseashorterpathtoafrontierstateisdiscovered.Thedatastructurefor\nfrontier needstosupportefficientmembershiptesting,soitshouldcombinethecapabilities\nofapriorityqueueandahashtable.\nSibiu Fagaras\n99\n80\nRimnicu Vilcea\n211\nPitesti\n97\n101\nBucharest\nFigure3.15 PartoftheRomaniastatespace,selectedtoillustrateuniform-costsearch.\nmay be on a suboptimal path. The second difference is that a test is added in case a better\npathisfoundtoanodecurrently onthefrontier.\nBothofthesemodificationscomeintoplayintheexampleshowninFigure3.15,where\ntheproblemistogetfromSibiutoBucharest. Thesuccessors ofSibiuareRimnicuVilceaand\nFagaras,withcosts80and99,respectively. Theleast-cost node,RimnicuVilcea,isexpanded\nnext, adding Pitesti with cost 80 + 97=177. The least-cost node is now Fagaras, so it is\nexpanded, addingBucharestwithcost99+211=310. Nowagoalnodehasbeengenerated,\nbutuniform-costsearchkeepsgoing,choosingPitestiforexpansionandaddingasecondpath Section3.4. UninformedSearchStrategies 85\ntoBucharestwithcost80+97+101=278. Nowthealgorithmcheckstoseeifthisnewpath\nis better than the old one; it is, so the old one is discarded. Bucharest, now with g-cost 278,\nisselected forexpansion andthesolution isreturned.\nIt is easy to see that uniform-cost search is optimal in general. First, we observe that\nwhenever uniform-cost search selects a node n for expansion, the optimal path to that node\nhas been found. (Were this not the case, there would have to be another frontier node\nn(cid:2)\non\nthe optimal path from the start node to n, by the graph separation property of Figure 3.9;\n(cid:2)\nby definition, n would have lower g-cost than n and would have been selected first.) Then,\nbecause step costs are nonnegative, paths never get shorter as nodes are added. These two\nfacts together imply that uniform-cost search expands nodes in order of their optimal path\ncost. Hence,thefirstgoalnodeselectedforexpansion mustbetheoptimalsolution.\nUniform-costsearchdoesnotcareaboutthe numberofstepsapathhas,butonlyabout\ntheirtotalcost. Therefore,itwillgetstuckinaninfiniteloopifthereisapathwithaninfinite\nsequence ofzero-cost actions\u2014for example, asequence of NoOp actions.6 Completeness is\nguaranteed providedthecostofeverystepexceedssomesmallpositiveconstant (cid:2).\nUniform-costsearchisguidedbypathcostsratherthandepths,soitscomplexityisnot\neasily characterized in terms of band d. Instead, let C\u2217 bethe cost ofthe optimal solution,7\nandassumethateveryactioncostsatleast(cid:2). Thenthealgorithm\u2019s worst-casetimeandspace\ncomplexity is O(b1+(cid:4)C\u2217\/(cid:2)(cid:5) ), which can be much greater than bd. This is because uniform-\ncost search can explore large trees of small steps before exploring paths involving large and\nperhaps useful steps. When all step costs are equal, b1+(cid:4)C\u2217\/(cid:2)(cid:5) is just bd+1. When all step\ncostsarethesame,uniform-costsearchissimilartobreadth-firstsearch,exceptthatthelatter\nstops as soon as it generates a goal, whereas uniform-cost search examines all the nodes at\nthe goal\u2019s depth to see if one has a lower cost; thus uniform-cost search does strictly more\nworkbyexpanding nodesatdepthdunnecessarily.\n3.4.3 Depth-first search\nDEPTH-FIRST Depth-firstsearchalwaysexpandsthedeepestnodeinthecurrentfrontierofthesearchtree.\nSEARCH\nThe progress of the search is illustrated in Figure 3.16. The search proceeds immediately\nto the deepest level of the search tree, where the nodes have no successors. As those nodes\nare expanded, they are dropped from the frontier, so then the search \u201cbacks up\u201d to the next\ndeepestnodethatstillhasunexplored successors.\nThe depth-first search algorithm is an instance of the graph-search algorithm in Fig-\nure3.7;whereasbreadth-first-searchusesaFIFOqueue,depth-firstsearchusesaLIFOqueue.\nA LIFO queue means that the most recently generated node is chosen for expansion. This\nmustbethedeepestunexpandednodebecauseitisonedeeperthanitsparent\u2014which,inturn,\nwasthedeepestunexpanded nodewhenitwasselected.\nAs an alternative to the GRAPH-SEARCH-style implementation, it is common to im-\nplement depth-first search witharecursive function that callsitself oneach ofitschildren in\nturn. (Arecursivedepth-first algorithm incorporating adepthlimitisshowninFigure3.17.)\n6 NoOp,or\u201cnooperation,\u201disthenameofanassemblylanguageinstructionthatdoesnothing.\n7 Here,andthroughoutthebook,the\u201cstar\u201dinC\u2217meansanoptimalvalueforC. 86 Chapter 3. SolvingProblemsbySearching\nA A A\nB C B C B C\nD E F G D E F G D E F G\nH I J K L M N O H I J K L M N O H I J K L M N O\nA A A\nB C B C C\nD E F G D E F G E F G\nH I J K L M N O I J K L M N O J K L M N O\nA A\nC B C C\nE F G E F G F G\nJ K L M N O K L M N O L M N O\nA A A\nC C C\nF G F G F G\nL M N O L M N O M N O\nFigure3.16 Depth-firstsearchonabinarytree. Theunexploredregionis showninlight\ngray. Explorednodeswithnodescendantsinthefrontierare removedfrommemory. Nodes\natdepth3havenosuccessorsandM istheonlygoalnode.\nThe properties of depth-first search depend strongly on whether the graph-search or\ntree-search version is used. The graph-search version, which avoids repeated states and re-\ndundantpaths,iscompleteinfinitestatespacesbecauseitwilleventuallyexpandeverynode.\nThe tree-search version, on the other hand, is not complete\u2014for example, in Figure 3.6 the\nalgorithmwillfollowtheArad\u2013Sibiu\u2013Arad\u2013Sibiuloopforever. Depth-firsttreesearchcanbe\nmodifiedatnoextra memorycostsothatitchecks newstates against those onthepathfrom\ntheroottothecurrent node;thisavoids infiniteloopsinfinitestatespaces butdoesnotavoid\nthe proliferation of redundant paths. In infinite state spaces, both versions fail if an infinite\nnon-goal path is encountered. For example, in Knuth\u2019s 4 problem, depth-first search would\nkeepapplying thefactorial operatorforever.\nForsimilarreasons, both versions arenonoptimal. Forexample, inFigure3.16, depth-\nfirstsearch willexplore theentire left subtree evenifnode C isagoal node. Ifnode J were\nalso a goal node, then depth-first search would return it as a solution instead of C, which\nwouldbeabettersolution; hence, depth-firstsearchisnotoptimal. Section3.4. UninformedSearchStrategies 87\nThetimecomplexityofdepth-firstgraphsearchisboundedbythesizeofthestatespace\n(whichmaybeinfinite,ofcourse). Adepth-firsttreesearch, ontheotherhand,maygenerate\nall of the O(bm) nodes in the search tree, where m is the maximum depth of any node; this\ncan be much greater than the size of the state space. Note that m itself can be much larger\nthand(thedepthoftheshallowest solution) andisinfiniteifthetreeisunbounded.\nSo far, depth-first search seems to have no clear advantage over breadth-first search,\nso why do we include it? The reason is the space complexity. For a graph search, there is\nno advantage, but a depth-first tree search needs to store only a single path from the root\nto a leaf node, along with the remaining unexpanded sibling nodes for each node on the\npath. Once a node has been expanded, it can be removed from memory as soon as all its\ndescendants have been fully explored. (See Figure 3.16.) For a state space with branching\nfactor b and maximum depth m, depth-first search requires storage of only O(bm) nodes.\nUsingthesameassumptions asforFigure3.13andassumingthatnodesatthesamedepthas\nthegoalnodehavenosuccessors, wefindthatdepth-firstsearchwouldrequire156kilobytes\ninstead of 10 exabytes at depth d = 16, a factor of 7 trillion times less space. This has\nled to the adoption of depth-first tree search as the basic workhorse of many areas of AI,\nincludingconstraintsatisfaction(Chapter6),propositionalsatisfiability(Chapter7),andlogic\nprogramming (Chapter 9). Forthe remainder ofthis section, wefocus primarily on thetree-\nsearchversionofdepth-first search.\nBACKTRACKING Avariantofdepth-firstsearchcalled backtrackingsearchusesstilllessmemory. (See\nSEARCH\nChapter6formoredetails.) Inbacktracking, onlyonesuccessor isgenerated atatimerather\nthan all successors; each partially expanded node remembers which successor to generate\nnext. In this way, only O(m) memory is needed rather than O(bm). Backtracking search\nfacilitates yet another memory-saving (and time-saving) trick: the idea of generating a suc-\ncessor by modifying the current state description directly rather than copying it first. This\nreduces thememoryrequirements tojustonestatedescription andO(m)actions. Forthisto\nwork,wemustbeabletoundoeachmodification whenwegobacktogenerate thenextsuc-\ncessor. Forproblemswithlargestatedescriptions, suchas roboticassembly,thesetechniques\narecriticaltosuccess.\n3.4.4 Depth-limited search\nThe embarrassing failure of depth-first search in infinite state spaces can be alleviated by\nsupplying depth-first searchwithapredetermined depth limit(cid:3). Thatis,nodes atdepth(cid:3)are\nDEPTH-LIMITED treated as if they have no successors. This approach is called depth-limited search. The\nSEARCH\ndepth limit solves the infinite-path problem. Unfortunately, it also introduces an additional\nsourceofincompleteness ifwechoose (cid:3) < d,thatis,theshallowest goalisbeyondthedepth\nlimit. (This is likely when d is unknown.) Depth-limited search will also be nonoptimal if\nwechoose(cid:3) > d. ItstimecomplexityisO(b(cid:3))anditsspacecomplexityisO(b(cid:3)). Depth-first\nsearchcanbeviewedasaspecialcaseofdepth-limited searchwith(cid:3)=\u221e.\nSometimes, depth limits can be based on knowledge of the problem. Forexample, on\nthemapofRomaniathereare20cities. Therefore,weknowthatifthereisasolution,itmust\nbe of length 19 at the longest, so (cid:3) = 19 is a possible choice. But in fact if we studied the 88 Chapter 3. SolvingProblemsbySearching\nfunctionDEPTH-LIMITED-SEARCH(problem,limit)returnsasolution,orfailure\/cutoff\nreturnRECURSIVE-DLS(MAKE-NODE(problem.INITIAL-STATE),problem,limit)\nfunctionRECURSIVE-DLS(node,problem,limit)returnsasolution,orfailure\/cutoff\nifproblem.GOAL-TEST(node.STATE)thenreturnSOLUTION(node)\nelseiflimit =0thenreturncutoff\nelse\ncutoff occurred?\u2190false\nforeachaction inproblem.ACTIONS(node.STATE)do\nchild\u2190CHILD-NODE(problem,node,action)\nresult\u2190RECURSIVE-DLS(child,problem,limit \u22121)\nifresult =cutoff thencutoff occurred?\u2190true\nelseifresult (cid:7)=failure thenreturnresult\nifcutoff occurred?thenreturncutoff elsereturnfailure\nFigure3.17 Arecursiveimplementationofdepth-limitedtreesearch.\nmapcarefully, wewoulddiscoverthatanycitycanbereached fromanyothercityinatmost\n9steps. Thisnumber, knownasthediameterofthestatespace,givesusabetterdepthlimit,\nDIAMETER\nwhich leads to a more efficient depth-limited search. For most problems, however, we will\nnotknowagooddepthlimituntilwehavesolvedtheproblem.\nDepth-limited search can beimplemented asasimple modification to thegeneral tree-\nor graph-search algorithm. Alternatively, it can be implemented as a simple recursive al-\ngorithm as shown in Figure 3.17. Notice that depth-limited search can terminate with two\nkinds of failure: the standard failure value indicates no solution; the cutoff value indicates\nnosolutionwithinthedepthlimit.\n3.4.5 Iterativedeepening depth-first search\nITERATIVE Iterative deepening search (or iterative deepening depth-first search) is a general strategy,\nDEEPENINGSEARCH\noften used incombination withdepth-first tree search, that findsthe bestdepth limit. Itdoes\nthisbygraduallyincreasingthelimit\u2014first0,then1,then2,andsoon\u2014untilagoalisfound.\nThis will occur when the depth limit reaches d, the depth of the shallowest goal node. The\nalgorithm is shown in Figure 3.18. Iterative deepening combines the benefits of depth-first\nandbreadth-firstsearch. Likedepth-firstsearch,itsmemoryrequirementsaremodest: O(bd)\ntobeprecise. Likebreadth-first search, itiscomplete when thebranching factorisfiniteand\noptimalwhenthepathcostisanondecreasing function ofthedepthofthenode. Figure3.19\nshowsfouriterationsofITERATIVE-DEEPENING-SEARCH onabinarysearchtree,wherethe\nsolution isfoundonthefourthiteration.\nIterative deepening search may seem wasteful because states are generated multiple\ntimes. Itturns out this isnot too costly. Thereason is that inasearch tree with thesame (or\nnearly the same) branching factor at each level, most of the nodes are in the bottom level,\nso it does not mattermuch that the upper levels are generated multiple times. In an iterative\ndeepening search, the nodes on the bottom level (depth d) are generated once, those on the Section3.4. UninformedSearchStrategies 89\nfunctionITERATIVE-DEEPENING-SEARCH(problem)returnsasolution,orfailure\nfordepth =0to\u221edo\nresult\u2190DEPTH-LIMITED-SEARCH(problem,depth)\nifresult (cid:7)=cutoffthenreturnresult\nFigure 3.18 The iterative deepening search algorithm, which repeatedly applies depth-\nlimitedsearchwithincreasinglimits. Itterminateswhena solutionisfoundorifthedepth-\nlimitedsearchreturnsfailure,meaningthatnosolutionexists.\nLimit = 0 A A\nLimit = 1 A A A A\nB C B C B C B C\nLimit = 2 A A A A\nB C B C B C B C\nD E F G D E F G D E F G D E F G\nA A A A\nB C B C B C B C\nD E F G D E F G D E F G D E F G\nLimit = 3 A A A A\nB C B C B C B C\nD E F G D E F G D E F G D E F G\nH I J K L M N O H I J K L M N O H I J K L M N O H I J K L M N O\nA A A A\nB C B C B C B C\nD E F G D E F G D E F G D E F G\nH I J K L M N O H I J K L M N O H I J K L M N O H I J K L M N O\nA A A A\nB C B C B C B C\nD E F G D E F G D E F G D E F G\nH I J K L M N O H I J K L M N O H I J K L M N O H I J K L M N O\nFigure3.19 Fouriterationsofiterativedeepeningsearchonabinarytree. 90 Chapter 3. SolvingProblemsbySearching\nnext-to-bottom levelaregenerated twice, andsoon, uptothechildren oftheroot, whichare\ngenerated dtimes. Sothetotalnumberofnodesgenerated intheworstcaseis\nN(IDS) = (d)b+(d\u22121)b2+\u00b7\u00b7\u00b7+(1)bd ,\nwhich gives a time complexity of O(bd)\u2014asymptotically the same as breadth-first search.\nThereissomeextracostforgeneratingtheupperlevelsmultipletimes,butitisnotlarge. For\nexample,ifb = 10andd = 5,thenumbersare\nN(IDS) = 50+400+3,000+20,000+100,000 = 123,450\nN(BFS) = 10+100+1,000+10,000+100,000 = 111,110.\nIf you are really concerned about repeating the repetition, you can use a hybrid approach\nthat runs breadth-first search until almost all the available memory is consumed, and then\nruns iterative deepening from all the nodes in the frontier. In general, iterative deepening is\nthe preferred uninformed search method when thesearch space islarge and thedepth ofthe\nsolution isnotknown.\nIterativedeepeningsearchisanalogous tobreadth-first searchinthatitexploresacom-\nplete layer of new nodes at each iteration before going on to the next layer. It would seem\nworthwhile to develop an iterative analog to uniform-cost search, inheriting the latter algo-\nrithm\u2019s optimality guarantees while avoiding its memory requirements. The idea is to use\nincreasing path-cost limitsinstead ofincreasing depth limits. Theresulting algorithm, called\nITERATIVE\niterative lengthening search, is explored in Exercise 3.17. It turns out, unfortunately, that\nLENGTHENING\nSEARCH\niterativelengthening incurssubstantial overheadcomparedtouniform-cost search.\n3.4.6 Bidirectional search\nTheideabehindbidirectional searchistoruntwosimultaneous searches\u2014one forwardfrom\nthe initial state andthe otherbackward from thegoal\u2014hoping that thetwosearches meetin\nthe middle (Figure 3.20). The motivation is that bd\/2 +bd\/2 is much less than bd, or in the\nfigure, the areaofthe twosmall circles isless thanthe areaofone bigcircle centered onthe\nstartandreaching tothegoal.\nBidirectional search is implemented by replacing the goal test with a check to see\nwhether the frontiers of the two searches intersect; if they do, a solution has been found.\n(It is important to realize that the first such solution found may not be optimal, even if the\ntwo searches are both breadth-first; some additional search is required to make sure there\nisn\u2019t another short-cut across the gap.) The check can be done when each node is generated\nor selected for expansion and, with a hash table, will take constant time. For example, if a\nproblem has solution depth d=6, and each direction runs breadth-first search one node at a\ntime,thenintheworstcasethetwosearches meetwhentheyhavegenerated allofthenodes\natdepth3. Forb=10,thismeansatotalof2,220nodegenerations,comparedwith1,111,110\nfor a standard breadth-first search. Thus, the time complexity of bidirectional search using\nbreadth-first searches in both directions is O(bd\/2). The space complexity is also O(bd\/2).\nWecanreducethisbyroughly halfifoneofthetwosearches is donebyiterative deepening,\nbutatleastoneofthefrontiers mustbekeptinmemorysothat theintersection checkcanbe\ndone. Thisspacerequirement isthemostsignificant weaknessofbidirectional search. Section3.4. UninformedSearchStrategies 91\nStart Goal\nFigure 3.20 A schematic view of a bidirectionalsearch thatis aboutto succeed when a\nbranchfromthestartnodemeetsabranchfromthegoalnode.\nThereduction intimecomplexity makesbidirectional search attractive, buthowdowe\nsearch backward? This is not as easy as it sounds. Let the predecessors of a state x be all\nPREDECESSOR\nthosestatesthathavexasasuccessor. Bidirectional searchrequiresamethodforcomputing\npredecessors. Whenalltheactionsinthestatespacearereversible, thepredecessors of xare\njustitssuccessors. Othercasesmayrequiresubstantial ingenuity.\nConsiderthequestion ofwhatwemeanby\u201cthegoal\u201dinsearching \u201cbackwardfromthe\ngoal.\u201d Forthe8-puzzleandforfindingarouteinRomania,thereisjustonegoalstate,sothe\nbackward search is very much like the forward search. If there are several explicitly listed\ngoalstates\u2014forexample,thetwodirt-freegoalstatesinFigure3.3\u2014thenwecanconstructa\nnewdummygoalstatewhoseimmediatepredecessors arealltheactualgoalstates. Butifthe\ngoal isan abstract description, such as thegoal that \u201cno queen attacks another queen\u201d in the\nn-queensproblem,thenbidirectional searchisdifficulttouse.\n3.4.7 Comparing uninformed search strategies\nFigure 3.21 compares search strategies in terms of the four evaluation criteria set forth in\nSection 3.3.2. This comparison is for tree-search versions. For graph searches, the main\ndifferencesarethatdepth-firstsearchiscompleteforfinitestatespacesandthatthespaceand\ntimecomplexities arebounded bythesizeofthestatespace.\nBreadth- Uniform- Depth- Depth- Iterative Bidirectional\nCriterion\nFirst Cost First Limited Deepening (ifapplicable)\nComplete? Yesa Yesa,b No No Yesa Yesa,d\nTime O(bd) O(b1+(cid:2)C\u2217\/(cid:2)(cid:3)) O(bm) O(b(cid:3)) O(bd) O(bd\/2)\nSpace O(bd) O(b1+(cid:2)C\u2217\/(cid:2)(cid:3)) O(bm) O(b(cid:3)) O(bd) O(bd\/2)\nOptimal? Yesc Yes No No Yesc Yesc,d\nFigure3.21 Evaluationoftree-searchstrategies. bisthebranchingfactor; disthedepth\nofthe shallowestsolution; m isthe maximumdepthofthe searchtree; l isthe depthlimit.\nSuperscriptcaveatsareasfollows: a completeifbisfinite; b completeifstepcosts\u2265 (cid:2)for\npositive(cid:2);coptimalifstepcostsareallidentical;difbothdirectionsusebreadth-firstsearch. 92 Chapter 3. SolvingProblemsbySearching\n3.5 INFORMED (HEURISTIC) SEARCH STRATEGIES\nThissectionshowshowaninformedsearchstrategy\u2014onethatusesproblem-specificknowl-\nINFORMEDSEARCH\nedgebeyondthedefinitionoftheproblemitself\u2014canfindsolutionsmoreefficientlythancan\nanuninformedstrategy.\nThe general approach we consider is called best-first search. Best-first search is an\nBEST-FIRSTSEARCH\ninstance of the general TREE-SEARCH or GRAPH-SEARCH algorithm in which a node is\nEVALUATION selected for expansion based on an evaluation function, f(n). The evaluation function is\nFUNCTION\nconstrued as a cost estimate, so the node with the lowest evaluation is expanded first. The\nimplementation of best-first graph search is identical to that for uniform-cost search (Fig-\nure3.14),exceptfortheuseoff insteadofg toorderthepriority queue.\nThechoice off determines thesearch strategy. (Forexample, asExercise 3.21shows,\nbest-first tree search includes depth-first search asaspecial case.) Mostbest-first algorithms\nHEURISTIC includeasacomponent off aheuristicfunction,denoted h(n):\nFUNCTION\nh(n) = estimatedcostofthecheapest pathfromthestateatnode ntoagoalstate.\n(Notice that h(n) takes anode as input, but, unlike g(n), itdepends only on the state at that\nnode.) Forexample,inRomania,onemightestimatethecostofthecheapestpathfromArad\ntoBucharest viathestraight-line distance fromAradtoBucharest.\nHeuristic functions are the most common form in which additional knowledge of the\nproblemisimpartedtothesearchalgorithm. WestudyheuristicsinmoredepthinSection3.6.\nFornow,weconsiderthemtobearbitrary, nonnegative, problem-specificfunctions, withone\nconstraint: ifnisagoalnode,thenh(n)=0. Theremainderofthissection coverstwoways\ntouseheuristic information toguidesearch.\n3.5.1 Greedy best-first search\nGREEDYBEST-FIRST Greedybest-firstsearch8 triestoexpandthenodethatisclosest tothegoal, onthegrounds\nSEARCH\nthat this is likely to lead to a solution quickly. Thus, it evaluates nodes by using just the\nheuristic function; thatis,f(n)= h(n).\nLetusseehowthisworksforroute-finding problemsinRomania;weusethestraight-\nSTRAIGHT-LINE line distance heuristic, which we will call h . If the goal is Bucharest, we need to\nDISTANCE SLD\nknow the straight-line distances to Bucharest, which are shown in Figure 3.22. For exam-\nple, h (In(Arad))=366. Notice that the values of h cannot be computed from the\nSLD SLD\nproblem description itself. Moreover, it takes a certain amount of experience to know that\nh iscorrelated withactualroaddistances andis,therefore, ausefulheuristic.\nSLD\nFigure 3.23 shows the progress of a greedy best-first search using h to find a path\nSLD\nfrom Arad to Bucharest. The first node to be expanded from Arad will be Sibiu because it\nis closer to Bucharest than either Zerind or Timisoara. The next node to be expanded will\nbe Fagaras because it is closest. Fagaras in turn generates Bucharest, which is the goal. For\nthis particular problem, greedy best-first search using h finds a solution without ever\nSLD\n8 Ourfirsteditioncalledthisgreedysearch; otherauthorshavecalleditbest-firstsearch. Ourmoregeneral\nusageofthelattertermfollowsPearl(1984). Section3.5. Informed(Heuristic)SearchStrategies 93\nArad 366 Mehadia 241\nBucharest 0 Neamt 234\nCraiova 160 Oradea 380\nDrobeta 242 Pitesti 100\nEforie 161 Rimnicu Vilcea 193\nFagaras 176 Sibiu 253\nGiurgiu 77 Timisoara 329\nHirsova 151 Urziceni 80\nIasi 226 Vaslui 199\nLugoj 244 Zerind 374\nFigure3.22 ValuesofhSLD\u2014straight-linedistancestoBucharest.\nexpanding a node that is not on the solution path; hence, its search cost is minimal. It is\nnot optimal, however: the path via Sibiu and Fagaras to Bucharest is 32 kilometers longer\nthan the path through Rimnicu Vilcea and Pitesti. This shows why the algorithm is called\n\u201cgreedy\u201d\u2014at eachstepittriestogetasclosetothegoalasitcan.\nGreedy best-first tree search is also incomplete even in a finite state space, much like\ndepth-first search. Consider the problem of getting from Iasi to Fagaras. The heuristic sug-\ngests that Neamt be expanded first because it is closest to Fagaras, but it is a dead end. The\nsolution is to go first to Vaslui\u2014a step that is actually farther from the goal according to\nthe heuristic\u2014and then to continue to Urziceni, Bucharest, and Fagaras. The algorithm will\nnever find this solution, however, because expanding Neamt puts Iasi back into the frontier,\nIasiisclosertoFagarasthanVasluiis,andsoIasiwillbeexpanded again, leading toaninfi-\nniteloop. (Thegraphsearchversion iscompleteinfinitespaces,butnotininfiniteones.) The\nworst-casetimeandspacecomplexityforthetreeversionisO(bm),wheremisthemaximum\ndepth of the search space. With a good heuristic function, however, the complexity can be\nreducedsubstantially. Theamountofthereductiondepends ontheparticularproblemandon\nthequalityoftheheuristic.\n3.5.2 A* search: Minimizing the totalestimated solutioncost\n\u2217 The most widely known form of best-first search is called A\u2217 search (pronounced \u201cA-star\nA SEARCH\nsearch\u201d). Itevaluatesnodesbycombiningg(n),thecosttoreachthenode,andh(n),thecost\ntogetfromthenodetothegoal:\nf(n)= g(n)+h(n).\nSince g(n) gives the path cost from the start node to node n, and h(n) is the estimated cost\nofthecheapest pathfrom ntothegoal,wehave\nf(n)= estimatedcostofthecheapest solution through n.\nThus, if we are trying to find the cheapest solution, a reasonable thing to try first is the\nnode with the lowest value of g(n)+ h(n). It turns out that this strategy is more than just\n\u2217\nreasonable: providedthattheheuristicfunction h(n)satisfiescertainconditions, A searchis\nboth complete and optimal. The algorithm is identical to UNIFORM-COST-SEARCH except\n\u2217\nthatA usesg+hinsteadofg. 94 Chapter 3. SolvingProblemsbySearching\n(a) The initial state Arad\n366\n(b) After expanding Arad Arad\nSibiu Timisoara Zerind\n253 329 374\n(c) After expanding Sibiu Arad\nSibiu Timisoara Zerind\n329 374\nArad Fagaras Oradea Rimnicu Vilcea\n366 176 380 193\n(d) After expanding Fagaras Arad\nSibiu Timisoara Zerind\n329 374\nArad Fagaras Oradea Rimnicu Vilcea\n366 380 193\nSibiu Bucharest\n253 0\nFigure3.23 Stages in a greedybest-first tree search forBucharestwith the straight-line\ndistanceheuristichSLD. Nodesarelabeledwiththeirh-values.\nConditionsforoptimality: Admissibilityandconsistency\nADMISSIBLE The first condition we require for optimality is that h(n) be an admissible heuristic. An\nHEURISTIC\nadmissible heuristic isonethat never overestimates thecost toreach the goal. Because g(n)\nis the actual cost to reach n along the current path, and f(n)=g(n)+h(n), we have as an\nimmediate consequence that f(n) never overestimates the true cost of a solution along the\ncurrentpaththrough n.\nAdmissible heuristics are by nature optimistic because they think the cost of solving\nthe problem is less than it actually is. An obvious example of an admissible heuristic is the\nstraight-line distance h that we used in getting to Bucharest. Straight-line distance is\nSLD\nadmissible because theshortest pathbetweenanytwopoints isastraight line, sothestraight Section3.5. Informed(Heuristic)SearchStrategies 95\n\u2217\nlinecannot beanoverestimate. InFigure3.24,weshowtheprogressofanA treesearchfor\nBucharest. Thevaluesof g arecomputed fromthestepcosts inFigure3.2,andthevalues of\nh aregiveninFigure3.22. NoticeinparticularthatBucharestfirstappearsonthefrontier\nSLD\nat step (e), but it is not selected forexpansion because its f-cost (450) is higher than that of\nPitesti (417). Anotherwaytosay this isthatthere mightbeasolution through Pitesti whose\ncostisaslowas417,sothealgorithm willnotsettleforasolution thatcosts450.\nAsecond, slightly strongercondition called consistency (orsometimes monotonicity)\nCONSISTENCY\nisrequired only forapplications ofA\u2217 tograph search.9 Aheuristic h(n)isconsistent if, for\nMONOTONICITY\n(cid:2)\nevery node n and every successor n of n generated by any action a, the estimated cost of\n(cid:2)\nreaching the goal from n is no greater than the step cost of getting to n plus the estimated\n(cid:2)\ncostofreachingthegoalfrom n:\nh(n) \u2264 c(n,a,n(cid:2) )+h(n(cid:2) ).\nTRIANGLE Thisisaformofthegeneral triangleinequality,whichstipulatesthateachsideofatriangle\nINEQUALITY\n(cid:2)\ncannot be longer than the sum of the other two sides. Here, the triangle is formed by n, n,\nandthegoalG closestton. Foranadmissibleheuristic, theinequality makesperfect sense:\nn\n(cid:2)\nif there were a route from n to G via n that was cheaper than h(n), that would violate the\nn\nproperty that h(n)isalowerboundonthecosttoreach G .\nn\nItisfairlyeasytoshow(Exercise3.29)thateveryconsistentheuristicisalsoadmissible.\nConsistency is therefore a stricter requirement than admissibility, but one has to work quite\nhardtoconcoctheuristicsthatareadmissiblebutnotconsistent. Alltheadmissibleheuristics\nwe discuss in this chapter are also consistent. Consider, for example, h . We know that\nSLD\nthe general triangle inequality is satisfied when each side is measured by the straight-line\n(cid:2) (cid:2)\ndistance and that the straight-line distance between n and n is no greater than c(n,a,n).\nHence,h isaconsistent heuristic.\nSLD\nOptimalityofA*\n\u2217 \u2217\nAs we mentioned earlier, A has the following properties: the tree-search version of A is\noptimalifh(n)isadmissible, whilethegraph-search versionisoptimalifh(n)isconsistent.\nWe show the second of these two claims since it is more useful. The argument es-\nsentially mirrors the argument for the optimality of uniform-cost search, with g replaced by\n\u2217\nf\u2014justasintheA algorithm itself.\nThe first step is to establish the following: if h(n) is consistent, then the values of\nf(n) along any path are nondecreasing. The proof follows directly from the definition of\n(cid:2) (cid:2) (cid:2)\nconsistency. Suppose n isasuccessor of n; then g(n)=g(n)+c(n,a,n)forsome action\na,andwehave\nf(n(cid:2) )= g(n(cid:2) )+h(n(cid:2) ) = g(n)+c(n,a,n(cid:2) )+h(n(cid:2) )\u2265 g(n)+h(n) = f(n).\n\u2217\nThe next step is to prove that whenever A selects a node n for expansion, the optimal path\nto that node has been found. Were this not the case, there would have to be another frontier\n(cid:2)\nnode n on the optimal path from the start node to n, by the graph separation property of\n9 Withanadmissiblebutinconsistentheuristic,A\u2217requiressomeextrabookkeepingtoensureoptimality. 96 Chapter 3. SolvingProblemsbySearching\n(a) The initial state\nArad\n366=0+366\n(b) After expanding Arad\nArad\nSibiu Timisoara Zerind\n393=140+253 447=118+329 449=75+374\n(c) After expanding Sibiu\nArad\nSibiu Timisoara Zerind\n447=118+329 449=75+374\nArad Fagaras Oradea Rimnicu Vilcea\n646=280+366 415=239+176671=291+380 413=220+193\n(d) After expanding Rimnicu Vilcea\nArad\nSibiu Timisoara Zerind\n447=118+329 449=75+374\nArad Fagaras Oradea Rimnicu Vilcea\n646=280+366 415=239+176671=291+380\nCraiova Pitesti Sibiu\n526=366+160417=317+100 553=300+253\n(e) After expanding Fagaras\nArad\nSibiu Timisoara Zerind\n447=118+329 449=75+374\nArad Fagaras Oradea Rimnicu Vilcea\n646=280+366 671=291+380\nSibiu Bucharest Craiova Pitesti Sibiu\n591=338+253 450=450+0 526=366+160 417=317+100 553=300+253\n(f) After expanding Pitesti\nArad\nSibiu Timisoara Zerind\n447=118+329 449=75+374\nArad Fagaras Oradea Rimnicu Vilcea\n646=280+366 671=291+380\nSibiu Bucharest Craiova Pitesti Sibiu\n591=338+253 450=450+0 526=366+160 553=300+253\nBucharest Craiova Rimnicu Vilcea\n418=418+0 615=455+160 607=414+193\nFigure3.24 StagesinanA\u2217searchforBucharest.Nodesarelabeledwithf =g+h. The\nhvaluesarethestraight-linedistancestoBucharesttakenfromFigure3.22. Section3.5. Informed(Heuristic)SearchStrategies 97\nO\nN\nZ\nA I\n380 S\nF\nV\n400\nT R\nP\nL\nH\nM U\nB\n420\nD\nE\nC\nG\nFigure3.25 MapofRomaniashowingcontoursatf =380,f =400,andf =420,with\nAradas the start state. Nodesinside a givencontourhave f-costsless than orequalto the\ncontourvalue.\n(cid:2)\nFigure 3.9; because f is nondecreasing along any path, n would have lower f-cost than n\nandwouldhavebeenselectedfirst.\nFrom the two preceding observations, it follows that the sequence of nodes expanded\n\u2217\nby A using GRAPH-SEARCH is in nondecreasing order of f(n). Hence, the first goal node\nselected forexpansion must be an optimal solution because f is the true cost forgoal nodes\n(whichhaveh=0)andalllatergoalnodeswillbeatleastasexpensive.\nThe fact that f-costs are nondecreasing along any path also means that we can draw\ncontours in the state space, just like the contours in a topographic map. Figure 3.25 shows\nCONTOUR\nan example. Inside the contour labeled 400, all nodes have f(n) less than or equal to 400,\n\u2217\nand so on. Then, because A expands the frontier node of lowest f-cost, we can see that an\n\u2217\nA searchfansoutfromthestartnode,addingnodesinconcentricbandsofincreasingf-cost.\n\u2217\nWith uniform-cost search (A search using h(n) = 0), the bands will be \u201ccircular\u201d\naround the start state. With more accurate heuristics, the bands will stretch toward the goal\n\u2217\nstate and become more narrowly focused around the optimal path. If C is the cost of the\noptimalsolutionpath,thenwecansaythefollowing:\n\u2022 A\u2217 expandsallnodeswithf(n)< C\u2217 .\n\u2022 A\u2217 mightthenexpandsomeofthenodesrightonthe\u201cgoalcontour\u201d(wheref(n)= C\u2217 )\nbeforeselecting agoalnode.\nCompleteness requires that there be only finitely many nodes with cost less than orequal to\n\u2217\nC ,acondition thatistrueifallstepcostsexceedsomefinite(cid:2)andifbisfinite.\n\u2217 \u2217\nNotice that A expands no nodes with f(n) > C \u2014for example, Timisoara is not\nexpanded in Figure 3.24 even though it is a child of the root. Wesay that the subtree below 98 Chapter 3. SolvingProblemsbySearching\nTimisoaraispruned;becauseh isadmissible,thealgorithmcansafelyignorethissubtree\nPRUNING SLD\nwhile still guaranteeing optimality. The concept of pruning\u2014eliminating possibilities from\nconsideration withouthavingtoexaminethem\u2014isimportant formanyareasofAI.\nOne final observation is that among optimal algorithms of this type\u2014algorithms that\n\u2217\nextend search paths from the root and use the same heuristic information\u2014A is optimally\nOPTIMALLY efficient for any given consistent heuristic. That is, no other optimal algorithm is guaran-\nEFFICIENT\n\u2217\nteedtoexpand fewernodes thanA (except possibly through tie-breaking amongnodeswith\n\u2217 \u2217\nf(n)=C ). This is because any algorithm that does not expand all nodes with f(n) < C\nrunstheriskofmissingtheoptimalsolution.\n\u2217\nThatA search iscomplete, optimal, andoptimally efficientamongallsuchalgorithms\n\u2217\nisrathersatisfying. Unfortunately, itdoesnotmeanthatA istheanswertoalloursearching\nneeds. The catch is that, for most problems, the number of states within the goal contour\nsearch space is still exponential in the length of the solution. The details of the analysis are\nbeyondthescopeofthisbook,butthebasicresultsareasfollows. Forproblemswithconstant\nstepcosts, thegrowthinruntimeasafunction oftheoptimal solution depthdisanalyzed in\nterms of the the absolute error or the relative error of the heuristic. The absolute error is\nABSOLUTEERROR\ndefined as \u0394 \u2261 h\u2217 \u2212h, where h\u2217 is the actual cost of getting from the root to the goal, and\nRELATIVEERROR\ntherelativeerrorisdefinedas (cid:2) \u2261 (h\u2217 \u2212h)\/h\u2217 .\nThe complexity results depend very strongly on the assumptions made about the state\nspace. The simplest model studied is a state space that has a single goal and is essentially a\ntree with reversible actions. (The 8-puzzle satisfies the first and third of these assumptions.)\n\u2217\nInthiscase, thetimecomplexity ofA isexponential inthemaximum absolute error, that is,\nO(b\u0394). For constant step costs, we can write this as O(b(cid:2)d), where d is the solution depth.\nForalmostallheuristics inpracticaluse,theabsoluteerrorisatleastproportional tothepath\n\u2217\ncost h , so (cid:2) is constant or growing and the time complexity is exponential in d. We can\nalsoseetheeffectofamoreaccurateheuristic: O(b(cid:2)d)=O((b(cid:2))d),sotheeffectivebranching\nfactor(definedmoreformallyinthenextsection) is b(cid:2).\nWhenthestate spacehasmanygoalstates\u2014particularly near-optimal goalstates\u2014the\nsearchprocesscanbeledastrayfromtheoptimalpathandthereisanextracostproportional\nto the number of goals whose cost is within a factor (cid:2) of the optimal cost. Finally, in the\ngeneral case ofa graph, the situation is even worse. There can be exponentially many states\n\u2217\nwith f(n) < C even if the absolute error is bounded by a constant. Forexample, consider\naversionofthevacuum worldwheretheagentcanclean upanysquare forunitcostwithout\nevenhavingtovisitit: inthatcase,squarescanbecleanedinanyorder. WithN initiallydirty\nsquares, there are 2N states where some subset has been cleaned and all of them are on an\n\u2217\noptimalsolutionpath\u2014andhencesatisfyf(n)< C \u2014eveniftheheuristichasanerrorof1.\n\u2217\nThecomplexityofA oftenmakesitimpracticaltoinsistonfindinganoptimalsolution.\n\u2217\nOne can use variants of A that find suboptimal solutions quickly, or one can sometimes\ndesign heuristics that are more accurate but not strictly admissible. In any case, the use of a\ngoodheuristic stillprovidesenormoussavingscomparedto theuseofanuninformed search.\nInSection3.6,welookatthequestion ofdesigning goodheuristics.\n\u2217\nComputation timeisnot, however,A \u2019smaindrawback. Becauseitkeepsallgenerated\n\u2217\nnodes inmemory (asdo all GRAPH-SEARCH algorithms), A usually runs outof space long Section3.5. Informed(Heuristic)SearchStrategies 99\nfunctionRECURSIVE-BEST-FIRST-SEARCH(problem)returnsasolution,orfailure\nreturnRBFS(problem,MAKE-NODE(problem.INITIAL-STATE),\u221e)\nfunctionRBFS(problem,node,f limit)returnsasolution,orfailureandanewf-costlimit\nifproblem.GOAL-TEST(node.STATE)thenreturnSOLUTION(node)\nsuccessors\u2190[]\nforeachaction inproblem.ACTIONS(node.STATE)do\naddCHILD-NODE(problem,node,action) intosuccessors\nifsuccessors isemptythenreturnfailure,\u221e\nforeachs insuccessors do \/*updatef withvaluefromprevioussearch,ifany*\/\ns.f \u2190max(s.g + s.h, node.f))\nloopdo\nbest\u2190thelowestf-valuenodeinsuccessors\nifbest.f > f limit thenreturnfailure,best.f\nalternative\u2190thesecond-lowestf-valueamongsuccessors\nresult,best.f\u2190RBFS(problem,best,min(f limit,alternative))\nifresult (cid:7)=failure thenreturnresult\nFigure3.26 Thealgorithmforrecursivebest-firstsearch.\n\u2217\nbefore it runs out of time. For this reason, A is not practical for many large-scale prob-\nlems. There are, however, algorithms that overcome the space problem without sacrificing\noptimality orcompleteness, atasmallcostinexecution time. Wediscussthesenext.\n3.5.3 Memory-bounded heuristic search\n\u2217\nThe simplest way to reduce memory requirements for A is to adapt the idea of iterative\nITERATIVE- \u2217 \u2217\nD\u2217EEPENING deepening to the heuristic search context, resulting in the iterative-deepening A (IDA)al-\nA gorithm. ThemaindifferencebetweenIDA\u2217 andstandarditerativedeepeningisthatthecutoff\nusedisthef-cost(g+h)ratherthanthedepth;ateachiteration,thecutoffvalueisthesmall-\n\u2217\nest f-cost of any node that exceeded the cutoff on the previous iteration. IDA is practical\nfor many problems with unit step costs and avoids the substantial overhead associated with\nkeepingasortedqueueofnodes. Unfortunately,itsuffersfromthesamedifficultieswithreal-\nvalued costs as does the iterative version of uniform-cost search described in Exercise 3.17.\n\u2217\nThissectionbrieflyexaminestwoothermemory-boundedalgorithms,calledRBFSandMA.\nRECURSIVE Recursive best-first search (RBFS) is a simple recursive algorithm that attempts to\nBEST-FIRSTSEARCH\nmimictheoperation ofstandard best-first search, but using only linearspace. Thealgorithm\nis shown in Figure 3.26. Its structure is similar to that of a recursive depth-first search, but\nratherthan continuing indefinitely downthecurrent path, ituses thef limit variable tokeep\ntrack of the f-value of the best alternative path available from any ancestor of the current\nnode. If the current node exceeds this limit, the recursion unwinds back to the alternative\npath. As the recursion unwinds, RBFS replaces the f-value of each node along the path\nwithabacked-upvalue\u2014thebestf-valueofitschildren. Inthisway,RBFSremembersthe\nBACKED-UPVALUE\nf-value of the best leaf in the forgotten subtree and can therefore decide whether it\u2019s worth 100 Chapter 3. SolvingProblemsbySearching\n(a) After expanding Arad, Sibiu, \u221e\nand Rimnicu Vilcea Arad 366\n447\nSibiu Timisoara Zerind\n393\n447 449\n415\nArad Fagaras Oradea Rimnicu Vilcea\n413\n646 415 671\nCraiova Pitesti Sibiu\n526 417 553\n(b) After unwinding back to Sibiu \u221e\nand expanding Fagaras Arad\n366\n447\nSibiu Timisoara Zerind\n393 447 449\n417\nArad Fagaras Oradea Rimnicu Vilcea\n646 415 671 413 417\nSibiu Bucharest\n591 450\n(c) After switching back to Rimnicu Vilcea \u221e\nand expanding Pitesti Arad\n366\n447\nSibiu Timisoara Zerind\n393 447 449\n447\nArad Fagaras Oradea Rimnicu Vilcea\n646 415 450 671 417\n447\nCraiova Pitesti Sibiu\n526 417 553\nBucharest Craiova Rimnicu Vilcea\n418 615 607\nFigure 3.27 Stages in an RBFS search for the shortest route to Bucharest. The f-limit\nvalueforeachrecursivecallisshownontopofeachcurrentnode,andeverynodeislabeled\nwithitsf-cost.(a)ThepathviaRimnicuVilceaisfolloweduntilthecurrentbestleaf(Pitesti)\nhasavaluethatisworsethanthebestalternativepath(Fagaras). (b)Therecursionunwinds\nandthe bestleaf value of the forgottensubtree(417)is backedup to Rimnicu Vilcea; then\nFagarasis expanded,revealinga bestleafvalueof 450. (c) Therecursionunwindsandthe\nbestleafvalueoftheforgottensubtree(450)isbackeduptoFagaras;thenRimnicuVilceais\nexpanded.Thistime,becausethebestalternativepath(throughTimisoara)costsatleast447,\ntheexpansioncontinuestoBucharest.\nreexpandingthesubtreeatsomelatertime. Figure3.27showshowRBFSreachesBucharest.\n\u2217\nRBFS is somewhat more efficient than IDA, but still suffers from excessive node re-\ngeneration. In the example in Figure 3.27, RBFS follows the path via Rimnicu Vilcea, then Section3.5. Informed(Heuristic)SearchStrategies 101\n\u201cchanges its mind\u201d and tries Fagaras, and then changes its mind back again. These mind\nchanges occur because every time the current best path is extended, its f-value is likely to\nincrease\u2014h is usually less optimistic for nodes closer to the goal. When this happens, the\nsecond-best path might become the best path, so the search has to backtrack to follow it.\n\u2217\nEachmind change corresponds to aniteration of IDA and could require manyreexpansions\nofforgotten nodestorecreatethebestpathandextenditone morenode.\n\u2217\nLike A tree search, RBFS is an optimal algorithm if the heuristic function h(n) is\nadmissible. Its space complexity is linear in the depth of the deepest optimal solution, but\nits time complexity is rather difficult to characterize: it depends both on the accuracy of the\nheuristic function andonhowoftenthebestpathchanges asnodesareexpanded.\n\u2217 \u2217\nIDA and RBFS suffer from using too little memory. Between iterations, IDA retains\nonly a single number: the current f-cost limit. RBFS retains more information in memory,\nbutitusesonlylinearspace: evenifmorememorywereavailable,RBFShasnowaytomake\nuseofit. Becausetheyforgetmostofwhattheyhavedone,bothalgorithmsmayendupreex-\npandingthesamestatesmanytimesover. Furthermore,theysufferthepotentiallyexponential\nincrease incomplexityassociated withredundant pathsingraphs(seeSection3.3).\nIt seems sensible, therefore, to use all available memory. Two algorithms that do this\n\u2217 \u2217 \u2217 \u2217 \u2217\nare MA (memory-bounded A ) and SMA (simplified MA). SMA is\u2014well\u2014simpler, so\nMA*\n\u2217 \u2217\nwewilldescribeit. SMA proceedsjustlikeA,expanding thebestleafuntilmemoryisfull.\nSMA*\n\u2217\nAtthispoint,itcannotaddanewnodetothesearchtreewithoutdropping anoldone. SMA\n\u2217\nalways drops the worst leaf node\u2014the one with the highest f-value. Like RBFS, SMA\nthen backs up the value of the forgotten node to its parent. In this way, the ancestor of a\nforgotten subtree knows the quality of the best path in that subtree. With this information,\n\u2217\nSMA regenerates thesubtree onlywhenallotherpaths havebeenshowntolookworsethan\nthepathithasforgotten. Anotherwayofsayingthisisthat, ifallthedescendants ofanoden\nare forgotten, then we will not know which way to go from n, but we will still have an idea\nofhowworthwhileitistogoanywherefrom n.\nThecompletealgorithmistoocomplicatedtoreproducehere,10 butthereisonesubtlety\n\u2217\nworthmentioning. WesaidthatSMA expands thebestleafanddeletestheworstleaf. What\nif all the leaf nodes have the same f-value? To avoid selecting the same node for deletion\n\u2217\nand expansion, SMA expands the newest best leaf and deletes the oldest worst leaf. These\ncoincide whenthereisonlyoneleaf,butinthatcase,thecurrentsearchtreemustbeasingle\npathfromroottoleafthatfillsallofmemory. Iftheleafisnotagoalnode,thenevenifitison\nanoptimalsolutionpath,thatsolutionisnotreachablewiththeavailablememory. Therefore,\nthenodecanbediscarded exactlyasifithadnosuccessors.\n\u2217\nSMA is complete if there is any reachable solution\u2014that is, if d, the depth of the\nshallowest goal node, is less than the memory size (expressed in nodes). It is optimal if any\noptimal solution is reachable; otherwise, it returns the best reachable solution. In practical\n\u2217\nterms,SMA isafairlyrobustchoiceforfindingoptimalsolutions,particularlywhenthestate\nspace is a graph, step costs are not uniform, and node generation is expensive compared to\ntheoverheadofmaintaining thefrontierandtheexploredset.\n10 Aroughsketchappearedinthefirsteditionofthisbook. 102 Chapter 3. SolvingProblemsbySearching\n\u2217\nOnveryhardproblems,however,itwilloftenbethecasethat SMA isforcedtoswitch\nbackandforthcontinuallyamongmanycandidatesolutionpaths,onlyasmallsubsetofwhich\ncanfitinmemory. (Thisresembles the problem of thrashingindisk paging systems.) Then\nTHRASHING\nthe extra time required for repeated regeneration of the same nodes means that problems\n\u2217\nthat would be practically solvable by A , given unlimited memory, become intractable for\n\u2217\nSMA. That is to say, memory limitations can make a problem intractable from the point\nof view of computation time. Although no current theory explains the tradeoff between time\nand memory, it seems that this is an inescapable problem. The only way out is to drop the\noptimality requirement.\n3.5.4 Learning to search better\nWe have presented several fixed strategies\u2014breadth-first, greedy best-first, and so on\u2014that\nhave been designed by computer scientists. Could anagent learn how to search better? The\nMETALEVELSTATE answerisyes,andthemethodrestsonanimportantconceptcalledthemetalevelstatespace.\nSPACE\nEachstateinametalevelstatespacecaptures theinternal (computational) stateofaprogram\nOBJECT-LEVELSTATE that is searching in an object-level state space such as Romania. For example, the internal\nSPACE\n\u2217\nstateoftheA algorithmconsistsofthecurrentsearchtree. Eachactioninthemetalevelstate\nspace is acomputation step that alters the internal state; forexample, each computation step\n\u2217\ninA expandsaleafnodeandaddsitssuccessors tothetree. Thus,Figure3.24,whichshows\nasequence of larger and larger search trees, can beseen asdepicting apath inthe metalevel\nstatespacewhereeachstateonthepathisanobject-level searchtree.\nNow,thepathinFigure3.24hasfivesteps,includingonestep,theexpansionofFagaras,\nthat is not especially helpful. Forharder problems, there will be many such missteps, and a\nMETALEVEL metalevellearningalgorithmcanlearnfromtheseexperiencestoavoidexploringunpromis-\nLEARNING\ning subtrees. Thetechniques used forthis kind of learning are described inChapter21. The\ngoal of learning is to minimize the total cost of problem solving, trading off computational\nexpenseandpathcost.\n3.6 HEURISTIC FUNCTIONS\nIn this section, we look at heuristics for the 8-puzzle, in order to shed light on the nature of\nheuristics ingeneral.\nThe 8-puzzle was one of the earliest heuristic search problems. As mentioned in Sec-\ntion 3.2, the object of the puzzle is toslide the tiles horizontally orvertically into the empty\nspaceuntiltheconfiguration matchesthegoalconfiguration (Figure3.28).\nTheaveragesolutioncostforarandomlygenerated8-puzzle instanceisabout22steps.\nThe branching factor is about 3. (When the empty tile is in the middle, four moves are\npossible; when it is in a corner, two; and when it is along an edge, three.) This means\nthat an exhaustive tree search to depth 22 would look at about 322 \u2248 3.1\u00d71010 states.\nA graph search would cut this down by a factor of about 170,000 because only 9!\/2 =\n181,440 distinct states are reachable. (See Exercise 3.4.) This is a manageable number, but Section3.6. HeuristicFunctions 103\n7 2 4 1 2\n5 6 3 4 5\n8 3 1 6 7 8\nStart State Goal State\nFigure3.28 Atypicalinstanceofthe8-puzzle.Thesolutionis26stepslong.\nthe corresponding numberforthe 15-puzzle isroughly 1013,so thenext orderof business is\n\u2217\nto find a good heuristic function. If we want to find the shortest solutions by using A, we\nneedaheuristic function thatneveroverestimates thenumberofstepstothegoal. Thereisa\nlonghistory ofsuchheuristics forthe15-puzzle; herearetwocommonlyusedcandidates:\n\u2022 h = the number of misplaced tiles. For Figure 3.28, all of the eight tiles are out of\n1\nposition, so the start state would have h = 8. h is an admissible heuristic because it\n1 1\nisclearthatanytilethatisoutofplacemustbemovedatleastonce.\n\u2022 h = the sum of the distances of the tiles from their goal positions. Because tiles\n2\ncannot move along diagonals, the distance we will count is the sum of the horizontal\nandverticaldistances. ThisissometimescalledthecityblockdistanceorManhattan\nMANHATTAN distance. h is also admissible because all any move can do is move one tile one step\nDISTANCE 2\nclosertothegoal. Tiles1to8inthestartstategiveaManhattandistance of\nh = 3+1+2+2+2+3+3+2 =18.\n2\nAsexpected, neitheroftheseoverestimates thetruesolution cost,whichis26.\n3.6.1 The effect ofheuristic accuracy onperformance\n\u2217\nEFFECTIVE Onewaytocharacterize thequalityofaheuristicisthe effective branchingfactorb . Ifthe\nBRANCHINGFACTOR\n\u2217\ntotalnumberofnodesgeneratedbyA foraparticularproblemis N andthesolutiondepthis\n\u2217\nd, then b is the branching factor that a uniform tree of depth d would have to have in order\ntocontain N +1nodes. Thus,\nN +1 = 1+b\u2217 +(b\u2217 )2+\u00b7\u00b7\u00b7+(b\u2217 )d .\n\u2217\nFor example, if A finds a solution at depth 5 using 52 nodes, then the effective branching\nfactor is 1.92. Theeffective branching factor can vary across problem instances, but usually\nit is fairly constant for sufficiently hard problems. (The existence of an effective branching\n\u2217\nfactor follows from the result, mentioned earlier, that the number of nodes expanded by A\n\u2217\ngrows exponentially withsolution depth.) Therefore, experimental measurements of b on a\nsmallsetofproblems canprovide agood guide tothe heuristic\u2019s overall usefulness. Awell-\n\u2217\ndesigned heuristic would have a value of b close to 1, allowing fairly large problems to be\nsolvedatreasonable computational cost. 104 Chapter 3. SolvingProblemsbySearching\nTo test the heuristic functions h and h , we generated 1200 random problems with\n1 2\nsolution lengths from 2 to 24 (100 for each even number) and solved them with iterative\n\u2217\ndeepeningsearchandwithA treesearchusingbothh andh . Figure3.29givestheaverage\n1 2\nnumber of nodes generated by each strategy and the effective branching factor. The results\nsuggestthath isbetterthanh ,andisfarbetterthanusingiterativedeepening search. Even\n2 1\nfor small problems with d=12, A\u2217 with h is 50,000 times more efficient than uninformed\n2\niterativedeepening search.\nSearchCost(nodes generated) EffectiveBranching Factor\n\u2217 \u2217 \u2217 \u2217\nd IDS A(h 1) A(h 2) IDS A(h 1) A(h 2)\n2 10 6 6 2.45 1.79 1.79\n4 112 13 12 2.87 1.48 1.45\n6 680 20 18 2.73 1.34 1.30\n8 6384 39 25 2.80 1.33 1.24\n10 47127 93 39 2.79 1.38 1.22\n12 3644035 227 73 2.78 1.42 1.24\n14 \u2013 539 113 \u2013 1.44 1.23\n16 \u2013 1301 211 \u2013 1.45 1.25\n18 \u2013 3056 363 \u2013 1.46 1.26\n20 \u2013 7276 676 \u2013 1.47 1.27\n22 \u2013 18094 1219 \u2013 1.48 1.28\n24 \u2013 39135 1641 \u2013 1.48 1.26\nFigure 3.29 Comparison of the search costs and effective branching factors for the\nITERATIVE-DEEPENING-SEARCH and A\u2217 algorithmswith h 1, h 2. Data are averagedover\n100instancesofthe8-puzzleforeachofvarioussolutionlengthsd.\nOnemightaskwhetherh isalwaysbetterthanh . Theansweris\u201cEssentially, yes.\u201d It\n2 1\niseasytoseefromthedefinitions ofthetwoheuristics that, foranynode n,h (n) \u2265 h (n).\n2 1\n\u2217\nWe thus say that h dominates h . Domination translates directly into efficiency: A using\nDOMINATION 2 1\n\u2217\nh will never expand more nodes than A using h (except possibly for some nodes with\n2 1\n\u2217\nf(n)=C ). The argument is simple. Recall the observation on page 97 that every node\n\u2217\nwith f(n) < C will surely be expanded. This is the same as saying that every node with\nh(n) < C\u2217 \u2212g(n) will surely be expanded. But because h is at least as big as h for all\n2 1\n\u2217\nnodes, everynodethat issurely expanded byA search withh willalsosurely beexpanded\n2\nwith h , and h might cause other nodes to be expanded as well. Hence, it is generally\n1 1\nbetter to use a heuristic function with higher values, provided it is consistent and that the\ncomputation timefortheheuristic isnottoolong.\n3.6.2 Generating admissibleheuristics from relaxed problems\nWe have seen that both h (misplaced tiles) and h (Manhattan distance) are fairly good\n1 2\nheuristics forthe 8-puzzle and that h isbetter. Howmight one have comeup with h ? Isit\n2 2\npossible foracomputertoinventsuchaheuristicmechanically?\nh andh areestimates oftheremaining pathlengthforthe8-puzzle, buttheyarealso\n1 2\nperfectlyaccurate pathlengthsfor simplifiedversionsofthepuzzle. Iftherulesofthepuzzle Section3.6. HeuristicFunctions 105\nwerechangedsothatatilecouldmoveanywhereinsteadofjusttotheadjacentemptysquare,\nthenh wouldgivetheexactnumberofstepsintheshortestsolution. Similarly,ifatilecould\n1\nmoveonesquareinanydirection,evenontoanoccupiedsquare,thenh wouldgivetheexact\n2\nnumber of steps in the shortest solution. A problem with fewerrestrictions on the actions is\ncalled a relaxed problem. The state-space graph of the relaxed problem is a supergraph of\nRELAXEDPROBLEM\ntheoriginalstatespacebecause theremovalofrestrictions createsaddededgesinthegraph.\nBecause the relaxed problem adds edges to the state space, anyoptimal solution in the\noriginal problem is, by definition, also a solution in the relaxed problem; but the relaxed\nproblem may have better solutions if the added edges provide short cuts. Hence, the cost of\nanoptimal solution toarelaxedproblem isanadmissible heuristic fortheoriginal problem.\nFurthermore, because the derived heuristic is an exact cost for the relaxed problem, it must\nobeythetriangleinequality andistherefore consistent(seepage95).\nIfaproblem definition iswritten down inaformal language, itis possible to construct\nrelaxedproblemsautomatically.11 Forexample,ifthe8-puzzle actionsaredescribed as\nAtilecanmovefromsquareAtosquareBif\nAishorizontally orvertically adjacent toB andBisblank,\nwecangeneratethreerelaxedproblemsbyremovingoneorbothoftheconditions:\n(a)AtilecanmovefromsquareAtosquareBifAisadjacenttoB.\n(b)AtilecanmovefromsquareAtosquareBifBisblank.\n(c)AtilecanmovefromsquareAtosquareB.\nFrom (a), we can derive h (Manhattan distance). The reasoning is that h would be the\n2 2\nproperscoreifwemovedeachtileinturntoitsdestination. Theheuristic derivedfrom(b)is\ndiscussed inExercise3.31. From(c),wecanderive h (misplaced tiles)because itwouldbe\n1\ntheproperscore iftilescouldmovetotheirintended destination inonestep. Noticethatitis\ncrucialthattherelaxedproblemsgeneratedbythistechniquecanbesolvedessentiallywithout\nsearch,becausetherelaxedrulesallowtheproblemtobedecomposedintoeightindependent\nsubproblems. If the relaxed problem is hard to solve, then the values of the corresponding\nheuristic willbeexpensivetoobtain.12\nAprogram called ABSOLVER cangenerate heuristics automatically from problem def-\ninitions, using the \u201crelaxed problem\u201d method and various other techniques (Prieditis, 1993).\nABSOLVER generated a new heuristic for the 8-puzzle that was better than any preexisting\nheuristic andfoundthefirstusefulheuristicforthefamous Rubik\u2019sCubepuzzle.\nOne problem with generating new heuristic functions is that one often fails to get a\nsingle \u201cclearly best\u201d heuristic. If a collection of admissible heuristics h ...h is available\n1 m\nforaproblem andnoneofthemdominates anyoftheothers, whichshould wechoose? Asit\nturnsout,weneednotmakeachoice. Wecanhavethebestofallworlds,bydefining\nh(n) = max{h (n),...,h (n)}.\n1 m\n11 InChapters8and10,wedescribeformallanguagessuitableforthistask;withformaldescriptionsthatcanbe\nmanipulated,theconstructionofrelaxedproblemscanbeautomated.Fornow,weuseEnglish.\n12 Notethataperfect heuristiccanbeobtainedsimplybyallowinghtorunafullbreadth-first search\u201conthe\nsly.\u201dThus,thereisatradeoffbetweenaccuracyandcomputationtimeforheuristicfunctions. 106 Chapter 3. SolvingProblemsbySearching\n2 4 1 2\n5 6 3 54 6\n8 3 1 7 8\nStart State Goal State\nFigure3.30 A subproblemofthe 8-puzzleinstance givenin Figure 3.28. Thetask is to\ngettiles 1, 2, 3, and4 intotheircorrectpositions,without worryingaboutwhathappensto\ntheothertiles.\nThis composite heuristic uses whichever function is most accurate on the node in question.\nBecausethecomponentheuristicsareadmissible, hisadmissible;itisalsoeasytoprovethat\nhisconsistent. Furthermore, hdominatesallofitscomponent heuristics.\n3.6.3 Generating admissibleheuristics from subproblems: Patterndatabases\nAdmissible heuristics canalsobederived fromthesolution costofasubproblemofagiven\nSUBPROBLEM\nproblem. For example, Figure 3.30 shows a subproblem of the 8-puzzle instance in Fig-\nure3.28. Thesubproblem involves gettingtiles1,2,3,4intotheircorrectpositions. Clearly,\nthe cost of the optimal solution of this subproblem is a lower bound on the cost of the com-\npleteproblem. ItturnsouttobemoreaccuratethanManhattan distance insomecases.\nTheideabehind patterndatabases istostore theseexactsolution costs foreverypos-\nPATTERNDATABASE\nsible subproblem instance\u2014in our example, every possible configuration of the four tiles\nand the blank. (The locations of the other four tiles are irrelevant for the purposes of solv-\ning the subproblem, but moves of those tiles do count toward the cost.) Then we compute\nan admissible heuristic h for each complete state encountered during a search simply by\nDB\nlookingupthecorrespondingsubproblemconfigurationinthedatabase. Thedatabaseitselfis\nconstructed bysearchingback13 fromthegoalandrecordingthecostofeachnewpatternen-\ncountered; theexpenseofthissearchisamortized overmany subsequent problem instances.\nThechoiceof1-2-3-4 isfairlyarbitrary; wecouldalsoconstruct databases for5-6-7-8,\nfor2-4-6-8, andsoon. Eachdatabaseyieldsanadmissibleheuristic, andtheseheuristics can\nbe combined, as explained earlier, by taking the maximum value. A combined heuristic of\nthiskindismuchmoreaccuratethantheManhattandistance; thenumberofnodesgenerated\nwhensolvingrandom 15-puzzles canbereducedbyafactorof1000.\nOne might wonder whether the heuristics obtained from the 1-2-3-4 database and the\n5-6-7-8couldbeadded,sincethetwosubproblems seemnottooverlap. Wouldthisstillgive\nan admissible heuristic? The answer is no, because the solutions of the 1-2-3-4 subproblem\nand the 5-6-7-8 subproblem for a given state will almost certainly share some moves\u2014it is\n13 By working backward from the goal, the exact solution cost of every instance encountered is immediately\navailable.Thisisanexampleofdynamicprogramming,whichwediscussfurtherinChapter17. Section3.6. HeuristicFunctions 107\nunlikely that1-2-3-4 can bemovedinto place without touching 5-6-7-8, andvice versa. But\nwhatifwedon\u2019t count those moves? Thatis, werecord notthetotal costofsolving the1-2-\n3-4 subproblem, but just the number of moves involving 1-2-3-4. Then it is easy to see that\nthesumofthetwocostsisstillalowerboundonthecostofsolvingtheentireproblem. This\nDISJOINTPATTERN is the idea behind disjoint pattern databases. With such databases, it is possible to solve\nDATABASES\nrandom 15-puzzles in a few milliseconds\u2014the number of nodes generated is reduced by a\nfactorof10,000compared withtheuseofManhattan distance. For24-puzzles, aspeedup of\nroughly afactorofamillioncanbeobtained.\nDisjoint pattern databases work for sliding-tile puzzles because the problem can be\ndividedupinsuchawaythateachmoveaffectsonlyonesubproblem\u2014because onlyonetile\nismoved atatime. Foraproblem such as Rubik\u2019s Cube, this kind ofsubdivision isdifficult\nbecause each move affects 8 or 9 of the 26 cubies. More general ways of defining additive,\nadmissible heuristics have been proposed that do apply to Rubik\u2019s cube (Yang et al., 2008),\nbuttheyhavenotyieldedaheuristicbetterthanthebestnonadditiveheuristicfortheproblem.\n3.6.4 Learning heuristics from experience\nA heuristic function h(n) is supposed to estimate the cost of a solution beginning from the\nstate at node n. How could an agent construct such a function? One solution was given in\nthe preceding sections\u2014namely, to devise relaxed problems for which an optimal solution\ncan befound easily. Anothersolution isto learn from experience. \u201cExperience\u201d here means\nsolvinglotsof8-puzzles,forinstance. Eachoptimalsolutiontoan8-puzzleproblemprovides\nexamples from which h(n) can be learned. Each example consists of a state from the solu-\ntionpathandtheactualcostofthesolution fromthatpoint. Fromtheseexamples, alearning\nalgorithm canbeusedtoconstruct afunction h(n)thatcan(withluck)predictsolution costs\nforotherstates thatarise during search. Techniques fordoing just thisusing neural nets, de-\ncisiontrees,andothermethodsaredemonstrated inChapter 18. (Thereinforcement learning\nmethodsdescribed inChapter21arealsoapplicable.)\nInductive learning methods work best when supplied with features of a state that are\nFEATURE\nrelevant to predicting the state\u2019s value, rather than with just the raw state description. For\nexample, the feature \u201cnumber of misplaced tiles\u201d might be helpful in predicting the actual\ndistance of a state from the goal. Let\u2019s call this feature x (n). Wecould take 100 randomly\n1\ngenerated 8-puzzle configurations and gather statistics on their actual solution costs. We\nmight find that when x (n) is 5, the average solution cost is around 14, and so on. Given\n1\nthesedata,thevalueofx canbeusedtopredicth(n). Ofcourse,wecanuseseveralfeatures.\n1\nAsecondfeaturex (n)mightbe\u201cnumberofpairsofadjacenttilesthatarenotadjacentinthe\n2\ngoalstate.\u201d Howshouldx (n)andx (n)becombinedtopredicth(n)? Acommonapproach\n1 2\nistousealinearcombination:\nh(n) = c x (n)+c x (n).\n1 1 2 2\nThe constants c and c are adjusted to give the best fit to the actual data on solution costs.\n1 2\nOneexpectsbothc andc tobepositivebecausemisplacedtilesandincorrectadjacentpairs\n1 2\nmake the problem harder to solve. Notice that this heuristic does satisfy the condition that\nh(n)=0forgoalstates,butitisnotnecessarily admissibleorconsistent. 108 Chapter 3. SolvingProblemsbySearching\n3.7 SUMMARY\nThis chapter has introduced methods that an agent can use to select actions in environments\nthataredeterministic, observable, static,andcompletelyknown. Insuchcases,theagentcan\nconstruct sequences ofactionsthatachieveitsgoals;thisprocess iscalled search.\n\u2022 Before anagent can start searching forsolutions, a goal must beidentified and awell-\ndefinedproblemmustbeformulated.\n\u2022 Aproblem consists of fiveparts: the initial state, aset ofactions, atransition model\ndescribing the results of those actions, a goal test function, and a path cost function.\nThe environment of the problem is represented by a state space. A path through the\nstatespacefromtheinitialstatetoagoalstateisasolution.\n\u2022 Search algorithms treat states and actions as atomic: theydo not consider anyinternal\nstructuretheymightpossess.\n\u2022 A general TREE-SEARCH algorithm considers all possible paths to find a solution,\nwhereasaGRAPH-SEARCH algorithm avoids consideration ofredundant paths.\n\u2022 Searchalgorithmsarejudgedonthebasisofcompleteness,optimality,timecomplex-\nity, and space complexity. Complexity depends onb, the branching factor inthe state\nspace,andd,thedepthoftheshallowestsolution.\n\u2022 Uninformed search methods have access only to the problem definition. The basic\nalgorithmsareasfollows:\n\u2013 Breadth-first search expands the shallowest nodes first; it is complete, optimal\nforunitstepcosts, buthasexponential spacecomplexity.\n\u2013 Uniform-costsearchexpandsthenodewithlowestpathcost,g(n),andisoptimal\nforgeneralstepcosts.\n\u2013 Depth-firstsearch expands the deepest unexpanded node first. It is neither com-\nplete nor optimal, but has linear space complexity. Depth-limited search adds a\ndepthbound.\n\u2013 Iterative deepening search calls depth-first search with increasing depth limits\nuntilagoalisfound. Itiscomplete,optimalforunitstepcosts,hastimecomplexity\ncomparable tobreadth-first search,andhaslinearspacecomplexity.\n\u2013 Bidirectionalsearchcanenormously reducetimecomplexity,butitisnotalways\napplicable andmayrequiretoomuchspace.\n\u2022 Informedsearchmethodsmayhaveaccesstoaheuristicfunctionh(n)thatestimates\nthecostofasolution fromn.\n\u2013 Thegeneric best-firstsearch algorithm selectsanodeforexpansion according to\nanevaluationfunction.\n\u2013 Greedybest-first search expands nodes withminimalh(n). Itisnot optimal but\nisoftenefficient. Bibliographical andHistorical Notes 109\n\u2217 \u2217\n\u2013 A search expands nodes withminimal f(n) = g(n)+h(n). A iscomplete and\noptimal, provided that h(n) is admissible (for TREE-SEARCH) or consistent (for\n\u2217\nGRAPH-SEARCH). ThespacecomplexityofA isstillprohibitive.\n\u2217 \u2217\n\u2013 RBFS (recursive best-first search) and SMA (simplified memory-bounded A )\nare robust, optimal search algorithms that use limited amounts of memory; given\n\u2217\nenough time, they can solve problems that A cannot solve because it runs out of\nmemory.\n\u2022 Theperformance ofheuristic search algorithms depends on the quality ofthe heuristic\nfunction. One can sometimes construct good heuristics by relaxing the problem defi-\nnition, bystoring precomputed solution costs forsubproblems inapattern database, or\nbylearning fromexperience withtheproblemclass.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nThetopicofstate-spacesearchoriginatedinmoreorlessitscurrentformintheearlyyearsof\nAI.NewellandSimon\u2019sworkontheLogicTheorist(1957)andGPS(1961)ledtotheestab-\nlishmentofsearchalgorithms astheprimaryweaponsinthearmoryof1960sAIresearchers\nand to the establishment of problem solving as the canonical AI task. Work in operations\nresearch by Richard Bellman (1957) showed the importance of additive path costs in sim-\nplifying optimization algorithms. The text on Automated Problem Solving by Nils Nilsson\n(1971)established theareaonasolidtheoretical footing.\nMost of the state-space search problems analyzed in this chapter have a long history\nin the literature and are less trivial than they might seem. The missionaries and cannibals\nproblem used in Exercise 3.9 was analyzed in detail by Amarel (1968). It had been consid-\nered earlier\u2014in AIby Simonand Newell(1961) and inoperations research by Bellman and\nDreyfus(1962).\nThe8-puzzle isasmallercousin ofthe15-puzzle, whosehistory isrecounted atlength\nby Slocum and Sonneveld (2006). It was widely believed to have been invented by the fa-\nmous American game designer Sam Loyd, based on his claims to that effect from 1891 on-\nward(Loyd, 1959). Actually itwasinvented byNoyesChapman, apostmasterinCanastota,\nNew York, in the mid-1870s. (Chapman was unable to patent his invention, as a generic\npatentcoveringslidingblockswithletters,numbers,orpictureswasgrantedtoErnestKinsey\nin1878.) Itquickly attracted theattention ofthepublic andofmathematicians (Johnson and\nStory, 1879; Tait, 1880). The editors of the American Journal of Mathematics stated, \u201cThe\n\u201815\u2019puzzleforthelastfewweekshasbeenprominentlybeforetheAmericanpublic,andmay\nsafely be said to have engaged the attention of nine out of ten persons of both sexes and all\nagesandconditions ofthecommunity.\u201d RatnerandWarmuth(1986)showedthatthegeneral\nn\u00d7nversionofthe15-puzzle belongs totheclassofNP-complete problems.\nThe 8-queens problem was first published anonymously in the German chess maga-\nzine Schach in 1848; it was later attributed to one Max Bezzel. It was republished in 1850\nand at that time drew the attention of the eminent mathematician Carl Friedrich Gauss, who 110 Chapter 3. SolvingProblemsbySearching\nattempted to enumerate all possible solutions; initially he found only 72, but eventually he\nfound the correct answer of 92, although Nauck published all 92 solutions first, in 1850.\nNetto(1901)generalized theproblem tonqueens, andAbramsonandYung(1989)foundan\nO(n)algorithm.\nEach of the real-world search problems listed in the chapter has been the subject of a\ngooddeal ofresearch effort. Methods forselecting optimal airline flightsremainproprietary\nforthemostpart,butCarldeMarcken(personalcommunication)hasshownthatairlineticket\npricing and restrictions have become so convoluted that the problem of selecting an optimal\nflight is formally undecidable. The traveling-salesperson problem is a standard combinato-\nrial problem in theoretical computer science (Lawler et al., 1992). Karp (1972) proved the\nTSPto be NP-hard, but effective heuristic approximation methods were developed (Lin and\nKernighan, 1973). Arora (1998) devised a fully polynomial approximation scheme for Eu-\nclidean TSPs. VLSIlayout methods are surveyed by Shahookar and Mazumder (1991), and\nmanylayout optimization papers appearinVLSIjournals. Robotic navigation and assembly\nproblemsarediscussed inChapter25.\nUninformedsearchalgorithmsforproblemsolvingareacentraltopicofclassicalcom-\nputerscience(HorowitzandSahni,1978)andoperations research(Dreyfus, 1969). Breadth-\nfirst search was formulated for solving mazes by Moore (1959). The method of dynamic\nprogramming (Bellman, 1957; Bellman and Dreyfus, 1962), which systematically records\nsolutions for all subproblems of increasing lengths, can be seen as a form of breadth-first\nsearch on graphs. The two-point shortest-path algorithm of Dijkstra (1959) is the origin\nof uniform-cost search. These works also introduced the idea of explored and frontier sets\n(closedandopenlists).\nA version of iterative deepening designed to make efficient use of the chess clock was\nfirst used by Slate and Atkin (1977) in the CHESS 4.5 game-playing program. Martelli\u2019s\n\u2217\nalgorithmB(1977)includesaniterativedeepeningaspectandalsodominatesA\u2019sworst-case\nperformance with admissible but inconsistent heuristics. The iterative deepening technique\ncame to the fore in work by Korf (1985a). Bidirectional search, which was introduced by\nPohl(1971), canalsobeeffectiveinsomecases.\nTheuseofheuristicinformationinproblemsolvingappears inanearlypaperbySimon\nand Newell (1958), but the phrase \u201cheuristic search\u201d and the use of heuristic functions that\nestimate the distance to the goal came somewhat later (Newell and Ernst, 1965; Lin, 1965).\nDoran and Michie (1966) conducted extensive experimental studies of heuristic search. Al-\nthoughtheyanalyzed pathlengthand\u201cpenetrance\u201d (theratioofpathlengthtothetotalnum-\nber of nodes examined so far), they appear to have ignored the information provided by the\n\u2217\npath cost g(n). The A algorithm, incorporating the current path cost into heuristic search,\nwasdevelopedbyHart,Nilsson,andRaphael(1968),withsomelatercorrections(Hartetal.,\n\u2217\n1972). DechterandPearl(1985)demonstrated theoptimalefficiencyofA.\n\u2217\nTheoriginal A paperintroduced theconsistency condition onheuristic functions. The\nmonotoneconditionwasintroducedbyPohl(1977)asasimplerreplacement,butPearl(1984)\nshowedthatthetwowereequivalent.\nPohl(1977) pioneered thestudyoftherelationship between theerrorinheuristic func-\n\u2217\ntionsandthetimecomplexityofA. Basicresultswereobtainedfortreesearchwithunitstep Bibliographical andHistorical Notes 111\ncostsandasinglegoalnode(Pohl,1977;Gaschnig,1979;Huynetal.,1980;Pearl,1984)and\nwithmultiple goalnodes (Dinhetal., 2007). The\u201ceffective branching factor\u201d wasproposed\nby Nilsson (1971) as an empirical measure of the efficiency; it is equivalent to assuming a\ntimecostofO((b\u2217 )d). Fortreesearchappliedtoagraph,Korfetal.(2001)arguethatthetime\ncost is better modeled as\nO(bd\u2212k),\nwhere k depends on the heuristic accuracy; this analysis\nhas elicited some controversy, however. For graph search, Helmert and Ro\u00a8ger (2008) noted\nthat several well-known problems contained exponentially many nodes on optimal solution\n\u2217\npaths,implyingexponential timecomplexity forA evenwithconstant absolute errorin h.\n\u2217\nTherearemanyvariationsontheA algorithm. Pohl(1973)proposedtheuseofdynamic\nweighting, whichusesaweightedsumf (n)=w g(n)+w h(n)ofthecurrentpathlength\nw g h\nandtheheuristicfunctionasanevaluationfunction,ratherthanthesimplesumf(n)=g(n)+\n\u2217\nh(n)used inA. Theweights w and w are adjusted dynamically asthe search progresses.\ng h\nPohl\u2019salgorithmcanbeshowntobe(cid:2)-admissible\u2014thatis,guaranteedtofindsolutionswithin\nafactor 1+(cid:2)of the optimal solution, where (cid:2) isaparameter supplied tothe algorithm. The\n\u2217\nsamepropertyisexhibitedbytheA algorithm(Pearl,1984),whichcanselectanynodefrom\n(cid:2)\nthefrontierprovideditsf-costiswithinafactor1+(cid:2)ofthelowest-f-costfrontiernode. The\nselection canbedonesoastominimizesearchcost.\n\u2217 \u2217\nBidirectional versions of A have been investigated; a combination of bidirectional A\nand known landmarks was used to efficiently find driving routes for Microsoft\u2019s online map\nservice(Goldbergetal.,2006). Aftercachingasetofpathsbetweenlandmarks,thealgorithm\ncanfindanoptimalpathbetweenanypairofpointsina24millionpointgraphoftheUnited\nStates, searching less than 0.1% of the graph. Others approaches to bidirectional search\ninclude a breadth-first search backward from the goal up to a fixed depth, followed by a\n\u2217\nforwardIDA search(Dillenburg andNelson,1994;Manzini,1995).\n\u2217\nA andotherstate-space searchalgorithms areclosely relatedtothebranch-and-bound\ntechniques that are widely used in operations research (Lawler and Wood, 1966). The\nrelationships between state-space search and branch-and-bound have been investigated in\ndepth (Kumarand Kanal, 1983; Nauet al., 1984; Kumaret al., 1988). Martelli and Monta-\nnari (1978) demonstrate a connection between dynamic programming (see Chapter 17) and\ncertaintypesofstate-spacesearch. KumarandKanal(1988)attempta\u201cgrandunification\u201dof\nheuristic search, dynamic programming, and branch-and-bound techniques under the name\nofCDP\u2014the\u201ccomposite decision process.\u201d\nBecausecomputersinthelate1950sandearly1960shadatmostafewthousandwords\nof main memory, memory-bounded heuristic search wasan early research topic. The Graph\nTraverser (Doran and Michie, 1966), one of the earliest search programs, commits to an\n\u2217\noperatoraftersearchingbest-firstuptothememorylimit. IDA (Korf,1985a,1985b)wasthe\nfirst widely used optimal, memory-bounded heuristic search algorithm, and a large number\n\u2217\nof variants have been developed. Ananalysis of the efficiency of IDA and of its difficulties\nwithreal-valued heuristics appearsinPatrick etal.(1992).\nRBFS (Korf, 1993) is actually somewhat more complicated than the algorithm shown\nin Figure 3.26, which is closer to anindependently developed algorithm called iterative ex-\nITERATIVE pansion (Russell, 1992). RBFSuses a lower bound as well as the upper bound; the two al-\nEXPANSION\ngorithmsbehaveidentically withadmissible heuristics, butRBFSexpandsnodesinbest-first 112 Chapter 3. SolvingProblemsbySearching\norder even with an inadmissible heuristic. The idea of keeping track of the best alternative\n\u2217 \u2217\npathappearedearlierinBratko\u2019s(1986)elegantPrologimplementationofA andintheDTA\nalgorithm (Russell and Wefald, 1991). Thelatter work also discusses metalevel state spaces\nandmetalevellearning.\n\u2217 \u2217 \u2217\nThe MA algorithm appeared in Chakrabarti et al. (1989). SMA, or Simplified MA,\n\u2217\nemergedfromanattempttoimplementMA asacomparisonalgorithmforIE(Russell,1992).\n\u2217\nKaindlandKhorsand (1994) have applied SMA toproduce abidirectional search algorithm\nthatissubstantiallyfasterthanpreviousalgorithms. KorfandZhang(2000)describeadivide-\n\u2217\nand-conquer approach, and Zhou and Hansen (2002) introduce memory-bounded A graph\nsearch and a strategy for switching to breadth-first search to increase memory-efficiency\n(ZhouandHansen,2006). Korf(1995)surveysmemory-bounded searchtechniques.\nTheideathatadmissibleheuristics canbederivedbyproblem relaxation appearsinthe\nseminal paper by Held and Karp (1970), who used the minimum-spanning-tree heuristic to\nsolvetheTSP.(SeeExercise3.30.)\nThe automation of the relaxation process was implemented successfully by Priedi-\ntis (1993), building on earlier work with Mostow (Mostow and Prieditis, 1989). Holte and\nHernadvolgyi (2001) describe morerecent steps towardsautomating theprocess. Theuseof\npattern databases to derive admissible heuristics is due to Gasser (1995) and Culberson and\nSchaeffer (1996, 1998); disjoint pattern databases are described by Korf and Felner (2002);\na similar method using symbolic patterns is due to Edelkamp (2009). Felner et al. (2007)\nshow how to compress pattern databases to save space. The probabilistic interpretation of\nheuristics wasinvestigated indepthbyPearl(1984)andHanssonandMayer(1989).\nBy far the most comprehensive source on heuristics and heuristic search algorithms\nis Pearl\u2019s (1984) Heuristics text. This book provides especially good coverage of the wide\n\u2217\nvarietyofoffshootsandvariationsofA ,includingrigorousproofsoftheirformalproperties.\nKanal and Kumar(1988) present an anthology of important articles on heuristic search, and\nRayward-Smithetal.(1996)coverapproaches fromOperations Research. Papersaboutnew\nsearch algorithms\u2014which, remarkably, continue to be discovered\u2014appear in journals such\nasArtificialIntelligence andJournal oftheACM.\nThetopic ofparallel search algorithms wasnot coveredinthechapter, partly because\nPARALLELSEARCH\nit requires a lengthy discussion of parallel computer architectures. Parallel search became a\npopulartopicinthe1990sinbothAIandtheoreticalcomputerscience(MahantiandDaniels,\n1993; Grama and Kumar, 1995; Crauser et al., 1998) and is making a comeback in the era\nof new multicore and cluster architectures (Ralphs et al., 2004; Korf and Schultze, 2005).\nAlso of increasing importance are search algorithms for very large graphs that require disk\nstorage(Korf,2008).\nEXERCISES\n3.1 Explainwhyproblem formulation mustfollowgoalformulation.\n3.2 Yourgoalistonavigatearobotoutofamaze. Therobotstarts inthecenterofthemaze Exercises 113\nfacing north. You can turn the robot to face north, east, south, or west. You can direct the\nrobottomoveforwardacertaindistance, although itwillstopbeforehittingawall.\na. Formulatethisproblem. Howlargeisthestatespace?\nb. In navigating a maze, the only place we need to turn is at the intersection of two or\nmorecorridors. Reformulatethisproblemusingthisobservation. Howlargeisthestate\nspacenow?\nc. Fromeachpointinthemaze,wecanmoveinanyofthefourdirectionsuntilwereacha\nturningpoint, andthisistheonlyactionweneedtodo. Reformulatetheproblemusing\ntheseactions. Doweneedtokeeptrackoftherobot\u2019sorientation now?\nd. In our initial description of the problem we already abstracted from the real world,\nrestricting actionsandremovingdetails. Listthreesuchsimplifications wemade.\n3.3 Suppose twofriends livein different cities ona map, such as the Romania map shown\nin Figure 3.2. Onevery turn, wecan simultaneously move each friend to a neighboring city\nonthemap. Theamountoftimeneededtomovefromcityitoneighborj isequaltotheroad\ndistanced(i,j)betweenthecities,butoneachturnthefriendthatarrivesfirstmustwaituntil\ntheotheronearrives(andcallsthefirstonhis\/hercellphone) before thenextturncanbegin.\nWewantthetwofriendstomeetasquicklyaspossible.\na. Writeadetailed formulation forthissearch problem. (You willfindithelpful todefine\nsomeformalnotation here.)\nb. LetD(i,j) bethestraight-line distance between cities iandj. Whichofthefollowing\nheuristicfunctions areadmissible? (i)D(i,j);(ii)2\u00b7D(i,j);(iii)D(i,j)\/2.\nc. Aretherecompletely connected mapsforwhichnosolution exists?\nd. Aretheremapsinwhichallsolutions requireonefriendtovisitthesamecitytwice?\n3.4 Show that the 8-puzzle states are divided into two disjoint sets, such that any state is\nreachable from any other state in the same set, while no state is reachable from any state in\ntheotherset. (Hint: SeeBerlekamp etal. (1982).) Deviseaprocedure todecide whichset a\ngivenstateisin,andexplainwhythisisusefulforgenerating random states.\n3.5 Consider the n-queens problem using the \u201ceffi\u221acient\u201d incremental formulation given on\npage 72. Explain why the state space has at least 3 n! states and estimate the largest n for\nwhichexhaustiveexplorationisfeasible. (Hint: Derivealowerboundonthebranchingfactor\nbyconsideringthemaximumnumberofsquaresthataqueencanattackinanycolumn.)\n3.6 Givea complete problem formulation foreach of the following. Choose aformulation\nthatisprecise enoughtobeimplemented.\na. Using only four colors, you have to color a planar map in such a way that no two\nadjacentregionshavethesamecolor.\nb. A 3-foot-tall monkey is in a room where some bananas are suspended from the 8-foot\nceiling. Hewould like to get the bananas. Theroom contains twostackable, movable,\nclimbable3-foot-high crates. 114 Chapter 3. SolvingProblemsbySearching\nG\nS\nFigure3.31 Ascenewithpolygonalobstacles. S andGarethestartandgoalstates.\nc. Youhave aprogram that outputs the message \u201cillegal input record\u201d when feda certain\nfile of input records. You know that processing of each record is independent of the\notherrecords. Youwanttodiscoverwhatrecordisillegal.\nd. Youhavethreejugs,measuring12gallons,8gallons,and3gallons,andawaterfaucet.\nYoucanfillthejugsuporemptythemoutfromonetoanotherorontotheground. You\nneedtomeasureoutexactlyonegallon.\n3.7 Considertheproblemoffindingtheshortestpathbetweentwopointsonaplanethathas\nconvex polygonal obstacles as shown in Figure 3.31. This is an idealization of the problem\nthatarobothastosolvetonavigateinacrowdedenvironment.\na. Suppose the state space consists of all positions (x,y) in the plane. How many states\narethere? Howmanypathsaretheretothegoal?\nb. Explainbrieflywhytheshortestpathfromonepolygonvertextoanyotherinthescene\nmust consist of straight-line segments joining some of the vertices of the polygons.\nDefineagoodstatespacenow. Howlargeisthisstatespace?\nc. Definethenecessaryfunctionstoimplementthesearchproblem,includinganACTIONS\nfunctionthattakesavertexasinputandreturnsasetofvectors,eachofwhichmapsthe\ncurrentvertextooneoftheverticesthatcanbereachedinastraightline. (Donotforget\nthe neighbors on the same polygon.) Use the straight-line distance for the heuristic\nfunction.\nd. Applyoneormoreofthealgorithms inthischaptertosolvea rangeofproblemsinthe\ndomain,andcommentontheirperformance.\n3.8 Onpage 68, wesaid that wewould not consider problems withnegative path costs. In\nthisexercise, weexplorethisdecision inmoredepth.\na. Suppose that actions can have arbitrarily large negative costs; explain why this possi-\nbilitywouldforceanyoptimalalgorithm toexploretheentirestatespace. Exercises 115\nb. Does it help if we insist that step costs must be greater than or equal to some negative\nconstantc? Considerbothtreesandgraphs.\nc. Supposethatasetofactionsformsaloopinthestatespacesuchthatexecutingthesetin\nsomeorderresultsinnonetchangetothestate. Ifalloftheseactionshavenegativecost,\nwhatdoesthisimplyabouttheoptimalbehaviorforanagentinsuchanenvironment?\nd. Onecan easily imagine actions withhigh negative cost, even indomains such asroute\nfinding. For example, some stretches of road might have such beautiful scenery as to\nfar outweigh the normal costs in terms of time and fuel. Explain, in precise terms,\nwithinthecontext ofstate-space search, whyhumans donotdrivearound scenic loops\nindefinitely, and explain how to define the state space and actions for route finding so\nthatartificialagentscanalsoavoid looping.\ne. Canyouthinkofarealdomaininwhichstepcostsaresuchastocauselooping?\n3.9 Themissionaries and cannibals problem is usually stated as follows. Three mission-\naries and three cannibals are on one side of a river, along with a boat that can hold one or\ntwopeople. Findawaytogeteveryonetotheothersidewithouteverleavingagroupofmis-\nsionariesinoneplaceoutnumbered bythecannibals inthatplace. Thisproblemisfamousin\nAIbecauseitwasthesubjectofthefirstpaperthatapproached problemformulation froman\nanalytical viewpoint(Amarel,1968).\na. Formulate the problem precisely, making only those distinctions necessary to ensure a\nvalidsolution. Drawadiagram ofthecompletestatespace.\nb. Implementandsolvetheproblemoptimallyusinganappropriatesearchalgorithm. Isit\nagoodideatocheckforrepeated states?\nc. Whydoyouthinkpeoplehaveahardtimesolvingthispuzzle,giventhatthestatespace\nissosimple?\n3.10 Define in your own words the following terms: state, state space, search tree, search\nnode,goal,action, transition model,andbranching factor.\n3.11 What\u2019s the difference between a world state, a state description, and a search node?\nWhyisthisdistinction useful?\n3.12 AnactionsuchasGo(Sibiu)reallyconsistsofalongsequenceoffiner-grainedactions:\nturn on the car, release the brake, accelerate forward, etc. Having composite actions of this\nkind reduces the number of steps in a solution sequence, thereby reducing the search time.\nSupposewetakethistothelogicalextreme,bymakingsuper-composite actions outofevery\npossible sequence of Go actions. Then every problem instance is solved by a single super-\ncomposite action, such as Go(Sibiu)Go(Rimnicu Vilcea)Go(Pitesti)Go(Bucharest). Explain\nhow search would work in this formulation. Is this a practical approach for speeding up\nproblem solving?\n3.13 Prove that GRAPH-SEARCH satisfies the graph separation property illustrated in Fig-\nure3.9. (Hint: Beginbyshowingthatthepropertyholdsatthestart,thenshowthatifitholds\nbefore an iteration of the algorithm, it holds afterwards.) Describe a search algorithm that\nviolates theproperty. 116 Chapter 3. SolvingProblemsbySearching\nx 12\nx 2 x 2\nx 16\nFigure3.32 Thetrackpiecesinawoodenrailwayset;eachislabeledwiththenumberof\ncopiesintheset. Notethatcurvedpiecesand\u201cfork\u201dpieces(\u201cswitches\u201dor\u201cpoints\u201d)canbe\nflippedoversotheycancurveineitherdirection.Eachcurvesubtends45degrees.\n3.14 Whichofthefollowingaretrueandwhicharefalse? Explainyouranswers.\n\u2217\na. Depth-firstsearchalwaysexpandsatleastasmanynodesasA searchwithanadmissi-\nbleheuristic.\nb. h(n)= 0isanadmissibleheuristic forthe8-puzzle.\n\u2217\nc. A isofnouseinrobotics becausepercepts, states, andactionsarecontinuous.\nd. Breadth-firstsearchiscompleteevenifzerostepcostsare allowed.\ne. Assumethatarookcanmoveonachessboard anynumberofsquaresinastraight line,\nvertically orhorizontally, but cannot jump overother pieces. Manhattan distance is an\nadmissible heuristic for the problem of moving the rook from square A to square B in\nthesmallestnumberofmoves.\n3.15 Consider a state space where the start state is number 1 and each state k has two\nsuccessors: numbers 2k and2k+1.\na. Drawtheportionofthestatespaceforstates1to15.\nb. Suppose the goal state is 11. List the order in which nodes will be visited forbreadth-\nfirstsearch,depth-limited searchwithlimit3,anditerativedeepening search.\nc. How well would bidirectional search work on this problem? What is the branching\nfactorineachdirection ofthebidirectional search?\nd. Doestheanswerto(c)suggest areformulation oftheproblem thatwouldallow youto\nsolvetheproblemofgettingfromstate1toagivengoalstate withalmostnosearch?\ne. Calltheactiongoingfrom k to2k Left,andtheactiongoingto2k+1Right. Canyou\nfindanalgorithm thatoutputsthesolution tothisproblem withoutanysearchatall?\n3.16 A basic wooden railway set contains the pieces shown in Figure 3.32. The task is to\nconnect these piecesintoarailwaythathasnooverlapping tracksandnoloose endswherea\ntraincouldrunoffontothefloor.\na. Supposethatthepiecesfittogetherexactlywithnoslack. Giveapreciseformulationof\nthetaskasasearchproblem.\nb. Identifyasuitable uninformed searchalgorithm forthistaskandexplainyourchoice.\nc. Explainwhyremovinganyoneofthe\u201cfork\u201dpiecesmakestheproblem unsolvable. Exercises 117\nd. Give an upper bound on the total size of the state space defined by your formulation.\n(Hint: think about the maximum branching factor for the construction process and the\nmaximumdepth, ignoring theproblem ofoverlapping pieces andloose ends. Beginby\npretending thateverypieceisunique.)\n3.17 On page 90, we mentioned iterative lengthening search, an iterative analog of uni-\nform cost search. The idea is to use increasing limits on path cost. If a node is generated\nwhose path cost exceeds the current limit, it is immediately discarded. For each new itera-\ntion,thelimitissettothelowestpathcostofanynodediscarded inthepreviousiteration.\na. Showthatthisalgorithm isoptimalforgeneralpathcosts.\nb. Consider a uniform tree with branching factor b, solution depth d, and unit step costs.\nHowmanyiterations williterativelengthening require?\nc. Nowconsiderstepcostsdrawnfromthecontinuousrange [(cid:2),1],where0 < (cid:2) < 1. How\nmanyiterations arerequiredintheworstcase?\nd. Implement the algorithm and apply it to instances of the 8-puzzle and traveling sales-\npersonproblems. Comparethealgorithm\u2019sperformance tothatofuniform-cost search,\nandcommentonyourresults.\n3.18 Describe astate space inwhichiterative deepening search performs muchworsethan\ndepth-firstsearch(forexample, O(n2)vs. O(n)).\n3.19 Write a program that will take as input two Web page URLs and find a path of links\nfrom onetotheother. Whatisanappropriate search strategy? Isbidirectional search agood\nidea? Couldasearchenginebeusedtoimplementapredecessor function?\n3.20 Considerthevacuum-world problemdefinedinFigure2.2.\na. Whichofthealgorithms definedinthischapterwouldbeappropriate forthisproblem?\nShouldthealgorithm usetreesearchorgraphsearch?\nb. Apply your chosen algorithm to compute an optimal sequence of actions for a 3\u00d73\nworldwhoseinitialstatehasdirtinthethreetopsquares andtheagentinthecenter.\nc. Constructasearchagentforthevacuumworld,andevaluate itsperformance inasetof\n3\u00d73worldswithprobability 0.2ofdirtineachsquare. Include thesearchcostaswell\naspathcostintheperformance measure,usingareasonable exchange rate.\nd. Compare your best search agent with a simple randomized reflex agent that sucks if\nthereisdirtandotherwise movesrandomly.\ne. Consider what would happen if the world were enlarged to n\u00d7n. How does the per-\nformanceofthesearchagentandofthereflexagentvarywith n?\n3.21 Proveeachofthefollowingstatements, orgiveacounterexample:\na. Breadth-firstsearchisaspecialcaseofuniform-cost search.\nb. Depth-firstsearchisaspecialcaseofbest-firsttreesearch.\n\u2217\nc. Uniform-costsearchisaspecial caseofA search. 118 Chapter 3. SolvingProblemsbySearching\n\u2217\n3.22 Compare the performance of A and RBFS on a set of randomly generated problems\ninthe8-puzzle(withManhattandistance)andTSP(withMST\u2014seeExercise3.30)domains.\nDiscussyourresults. Whathappenstotheperformance ofRBFSwhenasmallrandomnum-\nberisaddedtotheheuristic valuesinthe8-puzzle domain?\n\u2217\n3.23 Trace the operation of A search applied to the problem of getting to Bucharest from\nLugojusing thestraight-line distance heuristic. Thatis, show thesequence ofnodes that the\nalgorithm willconsiderandthe f,g,andhscoreforeachnode.\n\u2217\n3.24 DeviseastatespaceinwhichA usingGRAPH-SEARCH returnsasuboptimalsolution\nwithanh(n)function thatisadmissible butinconsistent.\nHEURISTICPATH 3.25 The heuristic path algorithm (Pohl, 1977) is a best-first search in which the evalu-\nALGORITHM\nation function is f(n) = (2 \u2212 w)g(n) + wh(n). For what values of w is this complete?\nForwhat values is it optimal, assuming that h is admissible? What kind of search does this\nperform forw = 0,w = 1,andw = 2?\n3.26 Considertheunbounded version oftheregular2DgridshowninFigure3.9. Thestart\nstateisattheorigin, (0,0),andthegoalstateisat(x,y).\na. Whatisthebranching factorbinthisstatespace?\nb. Howmanydistinct statesarethereatdepth k(fork > 0)?\nc. Whatisthemaximumnumberofnodesexpandedbybreadth-first treesearch?\nd. Whatisthemaximumnumberofnodesexpandedbybreadth-first graphsearch?\ne. Ish = |u\u2212x|+|v\u2212y|anadmissible heuristic forastateat(u,v)? Explain.\n\u2217\nf. HowmanynodesareexpandedbyA graphsearchusing h?\ng. Doeshremainadmissible ifsomelinksareremoved?\nh. Doeshremainadmissible ifsomelinksareaddedbetweennonadjacent states?\n3.27 nvehiclesoccupysquares(1,1)through(n,1)(i.e.,thebottomrow)ofann\u00d7ngrid.\nThevehicles mustbemovedtothetoprowbutinreverseorder; sothevehicleithatstartsin\n(i,1)mustendupin(n\u2212i+1,n). Oneachtimestep,everyoneofthenvehiclescanmove\none square up, down, left, or right, orstay put; but if a vehicle stays put, one other adjacent\nvehicle(butnotmorethanone)canhopoverit. Twovehiclescannotoccupythesamesquare.\na. Calculatethesizeofthestatespaceasafunction ofn.\nb. Calculatethebranching factorasafunction ofn.\nc. Suppose that vehicle i is at (x ,y ); write a nontrivial admissible heuristic h for the\ni i i\nnumberof moves itwillrequire to get toits goal location (n\u2212i+1,n), assuming no\nothervehiclesareonthegrid.\nd. Whichofthefollowingheuristics areadmissible fortheproblem ofmovingall nvehi-\nclestotheirdestinations? Explain.\n(cid:2)\n(i) n h .\ni=1 i\n(ii) max{h ,...,h }.\n1 n\n(iii) min{h ,...,h }.\n1 n Exercises 119\n3.28 Invent a heuristic function for the 8-puzzle that sometimes overestimates, and show\nhowitcanleadtoasuboptimalsolution onaparticularproblem. (Youcanuseacomputerto\n\u2217\nhelp if you want.) Prove that if h never overestimates by more than c, A using h returns a\nsolution whosecostexceedsthatoftheoptimalsolution bynomorethan c.\n3.29 Prove that if a heuristic is consistent, it must be admissible. Construct an admissible\nheuristic thatisnotconsistent.\n3.30 The traveling salesperson problem (TSP)can be solved with the minimum-spanning-\ntree (MST)heuristic, which estimates the cost of completing atour, given that a partial tour\nhas already been constructed. TheMSTcost ofaset ofcities isthe smallest sum of thelink\ncostsofanytreethatconnects allthecities.\na. Showhowthisheuristic canbederivedfromarelaxedversionoftheTSP.\nb. ShowthattheMSTheuristicdominates straight-line distance.\nc. Write a problem generator for instances of the TSP where cities are represented by\nrandompointsintheunitsquare.\n\u2217\nd. Findanefficientalgorithmintheliteratureforconstructing theMST,anduseitwithA\ngraphsearchtosolveinstances oftheTSP.\n3.31 Onpage 105,wedefinedtherelaxation ofthe8-puzzle inwhich atilecanmovefrom\nsquare A to square B if B is blank. The exact solution of this problem defines Gaschnig\u2019s\nheuristic (Gaschnig, 1979). Explain why Gaschnig\u2019s heuristic is at least as accurate as h\n1\n(misplaced tiles), and show cases where it is moreaccurate than both h and h (Manhattan\n1 2\ndistance). Explainhowtocalculate Gaschnig\u2019s heuristicefficiently.\n3.32 We gave two simple heuristics for the 8-puzzle: Manhattan distance and misplaced\ntiles. Several heuristics in the literature purport to improve on this\u2014see, for example, Nils-\nson (1971), Mostow and Prieditis (1989), and Hansson et al. (1992). Test these claims by\nimplementingtheheuristicsandcomparingtheperformance oftheresultingalgorithms. 4\nBEYOND CLASSICAL\nSEARCH\nIn which we relax the simplifying assumptions of the previous chapter, thereby\ngettingclosertotherealworld.\nChapter 3 addressed a single category of problems: observable, deterministic, known envi-\nronmentswherethesolutionisasequenceofactions. Inthischapter,welookatwhathappens\nwhentheseassumptionsarerelaxed. Webeginwithafairlysimplecase: Sections4.1and4.2\ncoveralgorithms thatperform purely localsearch inthestate space, evaluating andmodify-\ningoneormorecurrentstatesratherthansystematically exploringpathsfromaninitialstate.\nThesealgorithms are suitable forproblems inwhich allthat mattersisthe solution state, not\nthepathcost toreach it. Thefamilyoflocal search algorithms includes methods inspired by\nstatistical physics(simulatedannealing)andevolutionary biology (geneticalgorithms).\nThen, in Sections 4.3\u20134.4, we examine what happens when we relax the assumptions\nofdeterminismandobservability. Thekeyideaisthatifanagentcannotpredictexactlywhat\npercept it will receive, then it will need to consider what to do under each contingency that\nits percepts may reveal. With partial observability, the agent will also need to keep track of\nthestatesitmightbein.\nFinally, Section 4.5investigates onlinesearch, inwhichtheagent isfaced withastate\nspacethatisinitially unknownandmustbeexplored.\n4.1 LOCAL SEARCH ALGORITHMS AND OPTIMIZATION PROBLEMS\nThe search algorithms that we have seen so far are designed to explore search spaces sys-\ntematically. Thissystematicity isachieved by keeping one ormore paths in memory and by\nrecordingwhichalternativeshavebeenexploredateachpointalongthepath. Whenagoalis\nfound,thepathtothatgoalalsoconstitutesasolutiontotheproblem. Inmanyproblems,how-\never, the path to the goal is irrelevant. Forexample, in the 8-queens problem (see page 71),\nwhatmattersisthefinalconfiguration ofqueens, nottheorderinwhichthey areadded. The\nsame general property holds for many important applications such as integrated-circuit de-\nsign,factory-floorlayout,job-shopscheduling,automaticprogramming,telecommunications\nnetworkoptimization, vehiclerouting, andportfolio management.\n120 Section4.1. LocalSearchAlgorithmsandOptimization Problems 121\nIf the path to the goal does not matter, we might consider a different class of algo-\nrithms, ones that do not worry about paths at all. Local search algorithms operate using\nLOCALSEARCH\na single current node (rather than multiple paths) and generally move only to neighbors\nCURRENTNODE\nof that node. Typically, the paths followed by the search are not retained. Although local\nsearch algorithms are not systematic, they have two key advantages: (1) they use very little\nmemory\u2014usuallyaconstantamount;and(2)theycanoftenfindreasonablesolutionsinlarge\norinfinite(continuous) statespacesforwhichsystematic algorithmsareunsuitable.\nIn addition to finding goals, local search algorithms are useful for solving pure op-\nOPTIMIZATION timization problems, in which the aim is to find the best state according to an objective\nPROBLEM\nOBJECTIVE function. Many optimization problems do not fitthe \u201cstandard\u201d search model introduced in\nFUNCTION\nChapter 3. For example, nature provides an objective function\u2014reproductive fitness\u2014that\nDarwinian evolution could be seen as attempting tooptimize, but there isno \u201cgoal test\u201d and\nno\u201cpathcost\u201dforthisproblem.\nSTATE-SPACE Tounderstand localsearch, wefindituseful toconsiderthe state-space landscape(as\nLANDSCAPE\ninFigure4.1). Alandscapehasboth\u201clocation\u201d(definedbythestate)and\u201celevation\u201d(defined\nby the value ofthe heuristic cost function orobjective function). Ifelevation corresponds to\ncost, then the aim is to find the lowest valley\u2014a global minimum; if elevation corresponds\nGLOBALMINIMUM\ntoanobjective function, thentheaim istofindthehighest peak\u2014a globalmaximum. (You\nGLOBALMAXIMUM\ncan convert from one to the other just by inserting a minus sign.) Local search algorithms\nexplore this landscape. A complete local search algorithm always finds a goal ifone exists;\nanoptimalalgorithm alwaysfindsaglobalminimum\/maximum.\nobjective function\nglobal maximum\nshoulder\nlocal maximum\n\u201cflat\u201d local maximum\nstate space\ncurrent\nstate\nFigure4.1 Aone-dimensionalstate-spacelandscapeinwhichelevationcorrespondstothe\nobjectivefunction. The aim is to find the globalmaximum. Hill-climbingsearch modifies\nthecurrentstatetotrytoimproveit,asshownbythearrow.Thevarioustopographicfeatures\naredefinedinthetext. 122 Chapter 4. BeyondClassicalSearch\nfunctionHILL-CLIMBING(problem)returnsastatethatisalocalmaximum\ncurrent\u2190MAKE-NODE(problem.INITIAL-STATE)\nloopdo\nneighbor\u2190ahighest-valuedsuccessorofcurrent\nifneighbor.VALUE\u2264current.VALUEthenreturncurrent.STATE\ncurrent\u2190neighbor\nFigure4.2 Thehill-climbingsearchalgorithm,whichisthemostbasiclocalsearchtech-\nnique. At each step the currentnode is replaced by the best neighbor; in this version, that\nmeans the neighborwith the highest VALUE, but if a heuristic cost estimate h is used, we\nwouldfindtheneighborwiththelowesth.\n4.1.1 Hill-climbingsearch\nThe hill-climbing search algorithm (steepest-ascent version) is shown in Figure 4.2. It is\nHILLCLIMBING\nsimply a loop that continually moves in the direction of increasing value\u2014that is, uphill. It\nSTEEPESTASCENT\nterminates when it reaches a \u201cpeak\u201d where no neighbor has a higher value. The algorithm\ndoes not maintain a search tree, so the data structure for the current node need only record\nthe state and the value of the objective function. Hill climbing does not look ahead beyond\nthe immediate neighbors of thecurrent state. Thisresembles trying tofind thetop ofMount\nEverestinathickfogwhilesufferingfromamnesia.\nTo illustrate hill climbing, we will use the 8-queens problem introduced on page 71.\nLocal search algorithms typically use a complete-state formulation, where each state has\n8 queens on the board, one per column. The successors of a state are all possible states\ngenerated bymoving asingle queen toanothersquare inthesamecolumn (soeachstate has\n8\u00d77=56 successors). The heuristic cost function h is the number of pairs of queens that\nare attacking each other, either directly or indirectly. The global minimum of this function\niszero, whichoccurs only atperfect solutions. Figure 4.3(a) showsastate withh=17. The\nfigure also shows the values of all its successors, with the best successors having h=12.\nHill-climbingalgorithmstypicallychooserandomlyamongthesetofbestsuccessorsifthere\nismorethanone.\nGREEDYLOCAL Hillclimbingissometimescalledgreedylocalsearchbecauseitgrabsagoodneighbor\nSEARCH\nstatewithoutthinkingaheadaboutwheretogonext. Althoughgreedisconsideredoneofthe\nsevendeadly sins,itturnsoutthatgreedy algorithms often perform quitewell. Hillclimbing\noftenmakesrapidprogresstowardasolutionbecauseitisusuallyquiteeasytoimproveabad\nstate. For example, from the state in Figure 4.3(a), it takes just five steps to reach the state\nin Figure 4.3(b), which has h=1 and is very nearly a solution. Unfortunately, hill climbing\noftengetsstuckforthefollowingreasons:\n\u2022 Local maxima: a local maximum is a peak that is higher than each of its neighboring\nLOCALMAXIMUM\nstates but lower than the global maximum. Hill-climbing algorithms that reach the\nvicinity of a local maximum will be drawn upward toward the peak but will then be\nstuck with nowhere else to go. Figure 4.1 illustrates the problem schematically. More Section4.1. LocalSearchAlgorithmsandOptimization Problems 123\n18 12 14 13 13 12 14 14\n14 16 13 15 12 14 12 16\n14 12 18 13 15 12 14 14\n15 14 14 13 16 13 16\n14 17 15 14 16 16\n17 16 18 15 15\n18 14 15 15 14 16\n14 14 13 17 12 14 12 18\n(a) (b)\nFigure4.3 (a)An8-queensstatewithheuristiccostestimateh=17,showingthevalueof\nhforeachpossiblesuccessorobtainedbymovingaqueenwithinitscolumn.Thebestmoves\naremarked. (b)Alocalminimuminthe8-queensstatespace;thestatehash=1butevery\nsuccessorhasahighercost.\nconcretely, thestate inFigure 4.3(b)isalocal maximum(i.e.,alocalminimum forthe\ncosth);everymoveofasinglequeenmakesthesituationworse.\n\u2022 Ridges: a ridge is shown in Figure 4.4. Ridges result in a sequence of local maxima\nRIDGE\nthatisverydifficultforgreedyalgorithms tonavigate.\n\u2022 Plateaux: a plateau is a flat area of the state-space landscape. It can be a flat local\nPLATEAU\nmaximum, from which no uphill exit exists, or a shoulder, from which progress is\nSHOULDER\npossible. (SeeFigure4.1.) Ahill-climbing searchmightgetlostontheplateau.\nIneachcase,thealgorithmreachesapointatwhichnoprogressisbeingmade. Startingfrom\narandomlygenerated8-queensstate,steepest-ascenthillclimbinggetsstuck86%ofthetime,\nsolvingonly14%ofprobleminstances. Itworksquickly,takingjust4stepsonaveragewhen\nitsucceeds and3whenitgetsstuck\u2014not badforastatespacewith88 \u2248 17millionstates.\nThe algorithm in Figure 4.2 halts if it reaches a plateau where the best successor has\nthe same value as the current state. Might it not be a good idea to keep going\u2014to allow a\nsidewaysmoveinthehopethattheplateauisreallyashoulder, asshowninFigure4.1? The\nSIDEWAYSMOVE\nanswerisusuallyyes,butwemusttakecare. Ifwealwaysallowsidewaysmoveswhenthere\nare no uphill moves, an infinite loop will occur whenever the algorithm reaches a flat local\nmaximumthatisnotashoulder. Onecommonsolutionistoputalimitonthenumberofcon-\nsecutive sideways moves allowed. Forexample, we could allow up to, say, 100 consecutive\nsideways moves in the 8-queens problem. This raises the percentage of problem instances\nsolved by hill climbing from 14% to 94%. Success comes at a cost: the algorithm averages\nroughly 21stepsforeachsuccessful instance and64foreach failure. 124 Chapter 4. BeyondClassicalSearch\nFigure4.4 Illustrationofwhyridgescausedifficultiesforhillclimbing.Thegridofstates\n(darkcircles)issuperimposedonaridgerisingfromlefttoright,creatingasequenceoflocal\nmaxima that are not directly connected to each other. From each local maximum, all the\navailableactionspointdownhill.\nSTOCHASTICHILL Manyvariantsofhillclimbinghavebeeninvented. Stochastichillclimbingchoosesat\nCLIMBING\nrandomfromamongtheuphillmoves;theprobabilityofselectioncanvarywiththesteepness\nof the uphill move. This usually converges more slowly than steepest ascent, but in some\nFIRST-CHOICEHILL state landscapes, it finds better solutions. First-choice hill climbing implements stochastic\nCLIMBING\nhillclimbing bygenerating successors randomly until oneisgenerated thatisbetterthanthe\ncurrentstate. Thisisagoodstrategywhenastatehasmany(e.g.,thousands) ofsuccessors.\nThe hill-climbing algorithms described so far are incomplete\u2014they often fail to find\na goal when one exists because they can get stuck on local maxima. Random-restart hill\nRANDOM-RESTART climbingadopts thewell-known adage, \u201cIfatfirstyou don\u2019t succeed, try, try again.\u201d Itcon-\nHILLCLIMBING\nducts a series of hill-climbing searches from randomly generated initial states,1 until a goal\nis found. It is trivially complete with probability approaching 1, because it will eventually\ngenerate a goal state as the initial state. If each hill-climbing search has a probability p of\nsuccess, then the expected number of restarts required is 1\/p. For 8-queens instances with\nnosideways movesallowed, p \u2248 0.14,soweneed roughly 7iterations tofindagoal (6fail-\nuresand1success). Theexpectednumberofstepsisthecostofonesuccessful iterationplus\n(1\u2212p)\/ptimesthecostoffailure,orroughly22stepsinall. Whenweallowsidewaysmoves,\n1\/0.94 \u2248 1.06iterationsareneededonaverageand (1\u00d721)+(0.06\/0.94)\u00d764 \u224825steps.\nFor8-queens, then, random-restart hillclimbingisveryeffectiveindeed. Evenforthreemil-\nlionqueens, theapproach canfindsolutions inunderaminute.2\n1 Generatingarandomstatefromanimplicitlyspecifiedstatespacecanbeahardprobleminitself.\n2 Lubyetal.(1993)provethatitisbest,insomecases,torestartarandomizedsearchalgorithmafteraparticular,\nfixed amount of time and that this can be much more efficient than letting each search continue indefinitely.\nDisallowingorlimitingthenumberofsidewaysmovesisanexampleofthisidea. Section4.1. LocalSearchAlgorithmsandOptimization Problems 125\nThe success of hill climbing depends very much on the shape of the state-space land-\nscape: if there are few local maxima and plateaux, random-restart hill climbing will find a\ngood solution very quickly. On the other hand, many real problems have a landscape that\nlooksmorelikeawidelyscatteredfamilyofbaldingporcupinesonaflatfloor,withminiature\nporcupines living on the tip of each porcupine needle, ad infinitum. NP-hard problems typi-\ncallyhaveanexponential numberoflocalmaximatogetstuckon. Despitethis,areasonably\ngoodlocalmaximumcanoftenbefoundafterasmallnumberofrestarts.\n4.1.2 Simulated annealing\nAhill-climbingalgorithmthatnevermakes\u201cdownhill\u201dmovestowardstateswithlowervalue\n(or higher cost) is guaranteed to be incomplete, because it can get stuck on a local maxi-\nmum. In contrast, a purely random walk\u2014that is, moving to a successor chosen uniformly\nat random from the set of successors\u2014is complete but extremely inefficient. Therefore, it\nseemsreasonabletotrytocombinehillclimbingwitharandomwalkinsomewaythatyields\nSIMULATED bothefficiencyandcompleteness. Simulatedannealingissuchanalgorithm. Inmetallurgy,\nANNEALING\nannealing is the process used to temper or harden metals and glass by heating them to a\nhightemperature andthengradually coolingthem,thusallowingthematerialtoreachalow-\nenergy crystalline state. To explain simulated annealing, we switch our point of view from\nhill climbing to gradient descent (i.e., minimizing cost) and imagine the task of getting a\nGRADIENTDESCENT\nping-pong ball into thedeepest crevice inabumpy surface. Ifwejustlet theball roll, itwill\ncome to rest at a local minimum. If weshake the surface, wecan bounce the ball out of the\nlocal minimum. The trick is to shake just hard enough to bounce the ball out of local min-\nima but not hard enough to dislodge it from the global minimum. The simulated-annealing\nsolution istostartbyshaking hard(i.e.,atahightemperature) andthengradually reduce the\nintensity oftheshaking (i.e.,lowerthetemperature).\nTheinnermostloopofthesimulated-annealing algorithm(Figure4.5)isquitesimilarto\nhillclimbing. Insteadofpickingthebestmove,however,itpicksarandommove. Ifthemove\nimprovesthesituation,itisalwaysaccepted. Otherwise,thealgorithmacceptsthemovewith\nsome probability less than 1. The probability decreases exponentially with the \u201cbadness\u201d of\nthe move\u2014the amount \u0394E by which the evaluation is worsened. The probability also de-\ncreasesasthe\u201ctemperature\u201d T goesdown: \u201cbad\u201dmovesaremorelikelytobeallowedatthe\nstartwhenT ishigh,andtheybecomemoreunlikely asT decreases. Iftheschedule lowers\nT slowlyenough, thealgorithm willfindaglobaloptimumwithprobability approaching 1.\nSimulated annealing was first used extensively to solve VLSI layout problems in the\nearly1980s. Ithasbeenapplied widelytofactoryscheduling andotherlarge-scale optimiza-\ntiontasks. InExercise4.4,youareaskedtocompareitsperformancetothatofrandom-restart\nhillclimbingonthe8-queens puzzle.\n4.1.3 Local beam search\nKeeping just one node in memory might seem to be an extreme reaction to the problem of\nLOCALBEAM memory limitations. The local beam search algorithm3 keeps track of k states rather than\nSEARCH\n3 Localbeamsearchisanadaptationofbeamsearch,whichisapath-basedalgorithm. 126 Chapter 4. BeyondClassicalSearch\nfunctionSIMULATED-ANNEALING(problem,schedule)returnsasolutionstate\ninputs:problem,aproblem\nschedule,amappingfromtimeto\u201ctemperature\u201d\ncurrent\u2190MAKE-NODE(problem.INITIAL-STATE)\nfort =1to\u221edo\nT\u2190schedule(t)\nifT =0thenreturncurrent\nnext\u2190arandomlyselectedsuccessorofcurrent\n\u0394E\u2190next.VALUE\u2013current.VALUE\nif\u0394E >0thencurrent\u2190next\nelsecurrent\u2190next onlywithprobabilitye\u0394E\/T\nFigure4.5 Thesimulatedannealingalgorithm,aversionofstochastichillclimbingwhere\nsomedownhillmovesareallowed. Downhillmovesareacceptedreadilyearlyintheanneal-\ningscheduleandthenlessoftenastimegoeson.Theschedule inputdeterminesthevalueof\nthetemperatureT asafunctionoftime.\njustone. Itbegins withk randomly generated states. Ateach step, allthesuccessors ofallk\nstates aregenerated. Ifanyoneisagoal, thealgorithm halts. Otherwise, itselects the k best\nsuccessors fromthecompletelistandrepeats.\nAt first sight, a local beam search with k states might seem to be nothing more than\nrunning k random restarts in parallel instead of in sequence. In fact, the two algorithms\nare quite different. In a random-restart search, each search process runs independently of\nthe others. In a local beam search, useful information is passed among the parallel search\nthreads. In effect, the states that generate the best successors say to the others, \u201cCome over\nhere, the grass is greener!\u201d The algorithm quickly abandons unfruitful searches and moves\nitsresources towherethemostprogress isbeingmade.\nIn its simplest form, local beam search can suffer from a lack of diversity among the\nk states\u2014they canquickly becomeconcentrated inasmallregionofthestate space, making\nthesearch little morethananexpensive version ofhillclimbing. Avariant called stochastic\nSTOCHASTICBEAM beam search, analogous to stochastic hill climbing, helps alleviate this problem. Instead\nSEARCH\nof choosing the best k from the the pool of candidate successors, stochastic beam search\nchooses k successors at random, with the probability of choosing a given successor being\nan increasing function of its value. Stochastic beam search bears some resemblance to the\nprocess of natural selection, whereby the \u201csuccessors\u201d (offspring) of a \u201cstate\u201d (organism)\npopulate thenextgeneration according toits\u201cvalue\u201d(fitness).\n4.1.4 Geneticalgorithms\nGENETIC Ageneticalgorithm(orGA)isavariantofstochasticbeamsearchinwhichsuccessorstates\nALGORITHM\nare generated by combining two parent states rather than by modifying a single state. The\nanalogy tonatural selection isthesameasinstochastic beamsearch, exceptthatnowweare\ndealingwithsexualratherthanasexualreproduction. Section4.1. LocalSearchAlgorithmsandOptimization Problems 127\n24748552 32752411 32748552 32748152\n24 31%\n32752411 24748552 24752411 24752411\n23 29%\n24415124 32752411 32752124 32252124\n20 26%\n32543213 24415124 24415411 24415417\n11 14%\n(a) (b) (c) (d) (e)\nInitial Population Fitness Function Selection Crossover Mutation\nFigure4.6 Thegeneticalgorithm,illustratedfordigitstringsrepresenting8-queensstates.\nThe initial population in (a) is ranked by the fitness function in (b), resulting in pairs for\nmatingin(c). Theyproduceoffspringin(d),whicharesubjecttomutationin(e).\n+ =\nFigure4.7 The8-queensstatescorrespondingtothefirsttwoparentsinFigure4.6(c)and\nthefirstoffspringinFigure4.6(d).Theshadedcolumnsarelostinthecrossoverstepandthe\nunshadedcolumnsareretained.\nLike beam searches, GAs begin with a set of k randomly generated states, called the\npopulation. Eachstate,orindividual,isrepresented asastringoverafinitealphabet\u2014most\nPOPULATION\ncommonly,astringof0sand1s. Forexample,an8-queensstatemustspecifythepositionsof\nINDIVIDUAL\n8queens, each inacolumn of 8squares, and so requires 8\u00d7 log 8=24 bits. Alternatively,\n2\nthestatecouldberepresentedas8digits,eachintherangefrom1to8. (Wedemonstratelater\nthat the two encodings behave differently.) Figure 4.6(a) shows a population of four 8-digit\nstringsrepresenting 8-queens states.\nThe production of the next generation of states is shown in Figure 4.6(b)\u2013(e). In (b),\neach state is rated by the objective function, or(in GAterminology) the fitnessfunction. A\nFITNESSFUNCTION\nfitness function should return higher values for better states, so, for the 8-queens problem\nwe use the number of nonattacking pairs of queens, which has a value of 28 for a solution.\nThe values of the four states are 24, 23, 20, and 11. In this particular variant of the genetic\nalgorithm, the probability of being chosen for reproducing is directly proportional to the\nfitnessscore,andthepercentages areshownnexttotherawscores.\nIn (c), two pairs are selected at random for reproduction, in accordance with the prob- 128 Chapter 4. BeyondClassicalSearch\nabilities in (b). Notice that one individual is selected twice and one not at all.4 For each\npair to be mated, a crossover point is chosen randomly from the positions in the string. In\nCROSSOVER\nFigure4.6,thecrossoverpointsareafterthethirddigitin thefirstpairandafterthefifthdigit\ninthesecondpair.5\nIn (d), the offspring themselves are created by crossing over the parent strings at the\ncrossoverpoint. Forexample,thefirstchildofthefirstpair getsthefirstthreedigitsfromthe\nfirst parent and the remaining digits from the second parent, whereas the second child gets\nthe first three digits from the second parent and the rest from the first parent. The 8-queens\nstates involved in this reproduction step are shown in Figure 4.7. The example shows that\nwhentwoparent statesarequitedifferent, thecrossover operation canproduce astatethatis\na long way from either parent state. It is often the case that the population is quite diverse\nearlyonintheprocess,socrossover(likesimulatedannealing)frequentlytakeslargestepsin\nthe state space early in the search process and smaller steps later on when most individuals\narequitesimilar.\nFinally, in (e), each location is subject to random mutation with a small independent\nMUTATION\nprobability. One digit was mutated in the first, third, and fourth offspring. In the 8-queens\nproblem, this corresponds to choosing a queen at random and moving it to a random square\ninitscolumn. Figure4.8describes analgorithm thatimplementsallthesesteps.\nLike stochastic beam search, genetic algorithms combine an uphill tendency with ran-\ndom exploration and exchange of information among parallel search threads. The primary\nadvantage, if any, of genetic algorithms comes from the crossover operation. Yet it can be\nshown mathematically that, if the positions of the genetic code are permuted initially in a\nrandom order, crossover conveys no advantage. Intuitively, the advantage comes from the\nabilityofcrossovertocombinelargeblocksoflettersthat haveevolvedindependently toper-\nform useful functions, thus raising the level of granularity at which the search operates. For\nexample,itcouldbethatputtingthefirstthreequeensinpositions 2,4,and6(wheretheydo\nnot attack each other) constitutes a useful block that can be combined with other blocks to\nconstruct asolution.\nThe theory of genetic algorithms explains how this works using the idea of a schema,\nSCHEMA\nwhich is a substring in which some of the positions can be left unspecified. For example,\nthe schema 246***** describes all 8-queens states in which the first three queens are in\npositions 2, 4, and 6, respectively. Strings that match the schema (such as 24613578) are\ncalledinstancesoftheschema. Itcanbeshownthatiftheaveragefitnessoftheinstances of\nINSTANCE\naschemaisabovethemean,thenthenumberofinstancesoftheschemawithinthepopulation\nwillgrowovertime. Clearly,thiseffectisunlikelytobesignificantifadjacentbitsaretotally\nunrelated to each other, because then there will be few contiguous blocks that provide a\nconsistent benefit. Genetic algorithms work best when schemata correspond to meaningful\ncomponentsofasolution. Forexample,ifthestringisarepresentationofanantenna,thenthe\nschematamayrepresentcomponentsoftheantenna,suchasreflectorsanddeflectors. Agood\n4 Therearemanyvariantsofthisselectionrule. Themethodofculling,inwhichallindividualsbelowagiven\nthresholdarediscarded,canbeshowntoconvergefasterthantherandomversion(Baumetal.,1995).\n5 Itisherethattheencodingmatters. Ifa24-bitencodingisusedinsteadof8digits,thenthecrossoverpoint\nhasa2\/3chanceofbeinginthemiddleofadigit,whichresultsinanessentiallyarbitrarymutationofthatdigit. Section4.2. LocalSearchinContinuous Spaces 129\nfunctionGENETIC-ALGORITHM(population,FITNESS-FN)returnsanindividual\ninputs:population,asetofindividuals\nFITNESS-FN,afunctionthatmeasuresthefitnessofanindividual\nrepeat\nnew population\u2190emptyset\nfori =1toSIZE(population)do\nx\u2190RANDOM-SELECTION(population,FITNESS-FN)\ny\u2190RANDOM-SELECTION(population,FITNESS-FN)\nchild\u2190REPRODUCE(x,y)\nif(smallrandomprobability)thenchild\u2190MUTATE(child)\naddchild tonew population\npopulation\u2190new population\nuntilsomeindividualisfitenough,orenoughtimehaselapsed\nreturnthebestindividualinpopulation,accordingtoFITNESS-FN\nfunctionREPRODUCE(x,y)returnsanindividual\ninputs:x,y,parentindividuals\nn\u2190LENGTH(x);c\u2190randomnumberfrom1ton\nreturnAPPEND(SUBSTRING(x,1,c),SUBSTRING(y,c+1,n))\nFigure 4.8 A genetic algorithm. The algorithm is the same as the one diagrammed in\nFigure 4.6, with one variation: in this more popular version, each mating of two parents\nproducesonlyoneoffspring,nottwo.\ncomponentislikelytobegoodinavarietyofdifferentdesigns. Thissuggeststhatsuccessful\nuseofgenetic algorithmsrequires carefulengineering oftherepresentation.\nInpractice,geneticalgorithmshavehadawidespreadimpactonoptimizationproblems,\nsuch ascircuit layout and job-shop scheduling. Atpresent, itisnot clearwhetherthe appeal\nofgeneticalgorithmsarisesfromtheirperformanceorfrom their\u00e6stheticallypleasingorigins\nin the theory of evolution. Much work remains to be done to identify the conditions under\nwhichgeneticalgorithms performwell.\n4.2 LOCAL SEARCH IN CONTINUOUS SPACES\nIn Chapter 2, we explained the distinction between discrete and continuous environments,\npointing out that most real-world environments are continuous. Yet none of the algorithms\nwehavedescribed (exceptforfirst-choice hillclimbingand simulated annealing) canhandle\ncontinuous stateandactionspaces,becausetheyhaveinfinitebranchingfactors. Thissection\nprovides a very brief introduction to some local search techniques for finding optimal solu-\ntions in continuous spaces. Theliterature on this topic is vast; many of the basic techniques 130 Chapter 4. BeyondClassicalSearch\nEVOLUTION AND SEARCH\nThe theory of evolution was developed in Charles Darwin\u2019s On the Origin of\nSpeciesbyMeansofNaturalSelection(1859)andindependently byAlfredRussel\nWallace (1858). The central idea is simple: variations occur in reproduction and\nwill be preserved in successive generations approximately in proportion to their\neffectonreproductive fitness.\nDarwin\u2019stheorywasdevelopedwithnoknowledgeofhowthetraitsoforgan-\nisms can be inherited and modified. The probabilistic laws governing these pro-\ncesses were first identified by Gregor Mendel (1866), a monk who experimented\nwithsweetpeas. Muchlater,WatsonandCrick(1953)identifiedthestructureofthe\nDNA molecule and its alphabet, AGTC (adenine, guanine, thymine, cytosine). In\nthestandardmodel,variationoccursbothbypointmutationsinthelettersequence\nand by \u201ccrossover\u201d (in which the DNAof an offspring is generated by combining\nlongsections ofDNAfromeachparent).\nTheanalogytolocalsearchalgorithmshasalreadybeendescribed; theprinci-\npaldifferencebetweenstochasticbeamsearchandevolutionistheuseofsexualre-\nproduction, wherein successors are generated from multiple organisms rather than\njust one. The actual mechanisms of evolution are, however, far richer than most\ngenetic algorithms allow. For example, mutations can involve reversals, duplica-\ntions,andmovementoflargechunksofDNA;somevirusesborrowDNAfromone\norganism andinsert itinanother; and therearetransposable genes thatdonothing\nbut copy themselves many thousands of times within the genome. There are even\ngenesthatpoison cellsfrompotential matesthatdonotcarrythegene,thereby in-\ncreasingtheirownchancesofreplication. Mostimportantisthefactthatthegenes\nthemselves encode the mechanisms whereby the genome is reproduced and trans-\nlated into an organism. In genetic algorithms, those mechanisms are a separate\nprogramthatisnotrepresented withinthestrings beingmanipulated.\nDarwinian evolution may appear inefficient, having generated blindly some\n1045 or so organisms without improving its search heuristics one iota. Fifty\nyearsbeforeDarwin,however,theotherwisegreatFrenchnaturalist JeanLamarck\n(1809) proposed a theory of evolution whereby traits acquired by adaptation dur-\ning an organism\u2019s lifetime would be passed on to its offspring. Such a process\nwould be effective but does not seem to occur in nature. Much later, James Bald-\nwin(1896)proposedasuperficiallysimilartheory: thatbehaviorlearnedduringan\norganism\u2019slifetimecouldacceleratetherateofevolution. UnlikeLamarck\u2019s,Bald-\nwin\u2019stheoryisentirelyconsistentwithDarwinianevolutionbecauseitreliesonse-\nlection pressures operating onindividuals that have found local optimaamong the\nset of possible behaviors allowed by their genetic makeup. Computer simulations\nconfirm that the \u201cBaldwin effect\u201d is real, once \u201cordinary\u201d evolution has created\norganismswhoseinternal performance measurecorrelates withactualfitness. Section4.2. LocalSearchinContinuous Spaces 131\noriginatedinthe17thcentury,afterthedevelopmentofcalculusbyNewtonandLeibniz.6 We\nfindusesforthesetechniquesatseveralplacesinthebook,includingthechaptersonlearning,\nvision, androbotics.\nWe begin with an example. Suppose we want to place three new airports anywhere\nin Romania, such that the sum of squared distances from each city on the map (Figure 3.2)\nto its nearest airport is minimized. The state space is then defined by the coordinates of\nthe airports: (x ,y ), (x ,y ), and (x ,y ). This is a six-dimensional space; we also say\n1 1 2 2 3 3\nthat states are defined by six variables. (In general, states are defined by an n-dimensional\nVARIABLE\nvector of variables, x.) Moving around in this space corresponds to moving one or more of\nthe airports on the map. The objective function f(x ,y ,x ,y ,x ,y ) is relatively easy to\n1 1 2 2 3 3\ncompute for any particular state once we compute the closest cities. Let C be the set of\ni\ncitieswhoseclosestairport(inthecurrentstate)isairporti. Then,intheneighborhood ofthe\ncurrentstate,wheretheC sremainconstant, wehave\ni\n(cid:12)3 (cid:12)\nf(x ,y ,x ,y ,x ,y ) = (x \u2212x )2+(y \u2212y )2 . (4.1)\n1 1 2 2 3 3 i c i c\ni=1c\u2208Ci\nThis expression is correct locally, but not globally because the sets C are (discontinuous)\ni\nfunctions ofthestate.\nOnewaytoavoidcontinuousproblemsissimplytodiscretizetheneighborhoodofeach\nDISCRETIZATION\nstate. Forexample, we can move only one airport at a time in either the x or y direction by\na fixed amount \u00b1\u03b4. With 6 variables, this gives 12 possible successors for each state. We\ncan then apply any of the local search algorithms described previously. We could also ap-\nply stochastic hill climbing and simulated annealing directly, without discretizing the space.\nThesealgorithmschoosesuccessorsrandomly,whichcanbedonebygeneratingrandomvec-\ntorsoflength \u03b4.\nMany methods attempt to use the gradient of the landscape to find a maximum. The\nGRADIENT\ngradientoftheobjectivefunctionisavector\u2207f thatgivesthemagnitudeanddirectionofthe\nsteepest slope. Forourproblem,wehave\n(cid:13) (cid:14)\n\u2202f \u2202f \u2202f \u2202f \u2202f \u2202f\n\u2207f = , , , , , .\n\u2202x \u2202y \u2202x \u2202y \u2202x \u2202y\n1 1 2 2 3 3\nInsomecases,wecanfindamaximumbysolvingtheequation\u2207f=0. (Thiscouldbedone,\nforexample,ifwewereplacingjustoneairport;thesolutionisthearithmeticmeanofallthe\ncities\u2019 coordinates.) In many cases, however, this equation cannot be solved in closed form.\nFor example, with three airports, the expression for the gradient depends on what cities are\nclosest to each airport in the current state. This means we can compute the gradient locally\n(butnotglobally); forexample,\n(cid:12)\n\u2202f\n= 2 (x \u2212x ). (4.2)\ni c\n\u2202x\n1 c\u2208C1\nGivenalocallycorrectexpressionforthegradient,wecanperformsteepest-ascenthillclimb-\n6 Abasicknowledgeofmultivariatecalculusandvectorarithmeticisusefulforreadingthissection. 132 Chapter 4. BeyondClassicalSearch\ningbyupdatingthecurrentstateaccording totheformula\nx \u2190 x+\u03b1\u2207f(x),\nwhere \u03b1 is a small constant often called the step size. In other cases, the objective function\nSTEPSIZE\nmightnotbeavailableinadifferentiableformatall\u2014forexample,thevalueofaparticularset\nof airport locations might be determined by running some large-scale economic simulation\nEMPIRICAL package. In those cases, we can calculate a so-called empirical gradient by evaluating the\nGRADIENT\nresponse to small increments and decrements in each coordinate. Empirical gradient search\nisthesameassteepest-ascent hillclimbinginadiscretized versionofthestatespace.\nHidden beneath the phrase \u201c\u03b1 is a small constant\u201d lies a huge variety of methods for\nadjusting \u03b1. The basic problem is that, if \u03b1 is too small, too many steps are needed; if \u03b1\nis too large, the search could overshoot the maximum. The technique of line search tries to\nLINESEARCH\novercome this dilemma by extending the current gradient direction\u2014usually by repeatedly\ndoubling\u03b1\u2014untilf startstodecreaseagain. Thepointatwhichthisoccursbecomesthenew\ncurrent state. There are several schools of thought about how the new direction should be\nchosenatthispoint.\nFor many problems, the most effective algorithm is the venerable Newton\u2013Raphson\nNEWTON\u2013RAPHSON\nmethod. Thisisageneral technique forfindingrootsoffunctions\u2014that is,solvingequations\nof the form g(x)=0. It works by computing a new estimate for the root x according to\nNewton\u2019sformula\nx\u2190 x\u2212g(x)\/g(cid:2) (x).\nTo find a maximum or minimum of f, we need to find x such that the gradient is zero (i.e.,\n\u2207f(x)=0). Thus, g(x) in Newton\u2019s formula becomes \u2207f(x), and the update equation can\nbewritteninmatrix\u2013vector formas\nx \u2190 x\u2212H\u22121(x)\u2207f(x),\nf\nwhere H (x) is the Hessian matrix of second derivatives, whose elements H are given\nHESSIAN f ij\nby \u22022f\/\u2202x \u2202x . For our airport example, we can see from Equation (4.2) that H (x) is\ni j f\nparticularly simple: theoff-diagonal elements are zero and thediagonal elements forairport\ni are just twice the number of cities in C . A moment\u2019s calculation shows that one step of\ni\nthe update moves airport i directly to the centroid of C , which is the minimum of the local\ni\nexpression forf fromEquation (4.1).7 Forhigh-dimensional problems, however, computing\nthen2entriesoftheHessianandinvertingitmaybeexpensive,somanyapproximateversions\noftheNewton\u2013Raphson methodhavebeendeveloped.\nLocal search methods suffer from local maxima, ridges, and plateaux in continuous\nstatespacesjustasmuchasindiscrete spaces. Randomrestartsandsimulatedannealing can\nbe used and are often helpful. High-dimensional continuous spaces are, however, big places\ninwhichitiseasytogetlost.\nCONSTRAINED Afinaltopicwithwhichapassingacquaintance isusefulisconstrainedoptimization.\nOPTIMIZATION\nAnoptimizationproblemisconstrainedifsolutionsmustsatisfysomehardconstraintsonthe\nvalues of the variables. Forexample, in ourairport-siting problem, wemight constrain sites\n7 Ingeneral,theNewton\u2013Raphsonupdatecanbeseenasfittingaquadraticsurfacetof atxandthenmoving\ndirectlytotheminimumofthatsurface\u2014whichisalsotheminimumoff iff isquadratic. Section4.3. SearchingwithNondeterministic Actions 133\nto be inside Romania and on dry land (rather than in the middle of lakes). The difficulty of\nconstrained optimization problemsdepends onthenatureof theconstraints andtheobjective\nLINEAR function. Thebest-known category isthat of linear programming problems, in which con-\nPROGRAMMING\nstraints must be linear inequalities forming a convex set 8 and the objective function is also\nCONVEXSET\nlinear. Thetimecomplexityoflinearprogrammingispolynomialinthenumberofvariables.\nLinear programming is probably the most widely studied and broadly useful class of\noptimization problems. It is a special case of the more general problem of convex opti-\nCONVEX mization, which allows the constraint region to be any convex region and the objective to\nOPTIMIZATION\nbeanyfunction thatisconvexwithintheconstraint region. Undercertainconditions, convex\noptimization problems are also polynomially solvable and may be feasible in practice with\nthousands of variables. Several important problems in machine learning and control theory\ncanbeformulated asconvexoptimization problems(seeChapter20).\n4.3 SEARCHING WITH NONDETERMINISTIC ACTIONS\nInChapter3,weassumedthattheenvironment isfullyobservable anddeterministic andthat\ntheagentknowswhattheeffectsofeachactionare. Therefore,theagentcancalculateexactly\nwhich state results from any sequence of actions and always knows which state it is in. Its\npercepts provide nonewinformation aftereachaction, although ofcourse theytelltheagent\ntheinitialstate.\nWhentheenvironment iseitherpartially observable ornondeterministic (orboth), per-\nceptsbecomeuseful. Inapartiallyobservableenvironment, everypercepthelpsnarrowdown\nthesetofpossible states theagent mightbein, thus makingiteasierfortheagent toachieve\nitsgoals. Whentheenvironmentisnondeterministic, perceptstelltheagentwhichofthepos-\nsible outcomes ofits actions hasactually occurred. In both cases, the future percepts cannot\nbedeterminedinadvanceandtheagent\u2019sfutureactionswilldependonthosefuturepercepts.\nSothesolutiontoaproblemisnotasequencebutacontingencyplan(alsoknownasastrat-\nCONTINGENCYPLAN\negy) that specifies what to do depending on what percepts are received. In this section, we\nSTRATEGY\nexaminethecaseofnondeterminism, deferring partialobservability toSection4.4.\n4.3.1 The erraticvacuum world\nAs an example, we use the vacuum world, first introduced in Chapter 2 and defined as a\nsearch problem in Section 3.2.1. Recall that the state space has eight states, as shown in\nFigure 4.9. There are three actions\u2014Left, Right, and Suck\u2014and the goal is to clean up all\nthe dirt (states 7 and 8). If the environment is observable, deterministic, and completely\nknown, then the problem is trivially solvable by any of the algorithms in Chapter 3 and the\nsolution isanaction sequence. Forexample, iftheinitial state is1, then theaction sequence\n[Suck,Right,Suck]willreachagoalstate, 8.\n8 AsetofpointsSisconvexifthelinejoininganytwopointsinSisalsocontainedinS.Aconvexfunctionis\noneforwhichthespace\u201cabove\u201ditformsaconvexset;bydefinition,convexfunctionshavenolocal(asopposed\ntoglobal)minima. 134 Chapter 4. BeyondClassicalSearch\n1 2\n3 4\n5 6\n7 8\nFigure4.9 Theeightpossiblestatesofthevacuumworld;states7and8aregoalstates.\nNow suppose that we introduce nondeterminism in the form of a powerful but erratic\nERRATICVACUUM vacuumcleaner. Intheerraticvacuumworld,theSuckactionworksasfollows:\nWORLD\n\u2022 When applied to a dirty square the action cleans the square and sometimes cleans up\ndirtinanadjacentsquare, too.\n\u2022 Whenapplied toacleansquaretheactionsometimesdeposits dirtonthecarpet.9\nToprovideapreciseformulation ofthisproblem,weneedtogeneralize thenotionofatran-\nsitionmodelfromChapter3. Insteadofdefiningthetransition modelbyaRESULT function\nthat returns asingle state, weuse a RESULTS function thatreturns a setofpossible outcome\nstates. Forexample, intheerratic vacuum world, the Suck action instate1leads toastatein\ntheset{5,7}\u2014thedirtintheright-hand squaremayormaynotbevacuumedup.\nWealsoneed togeneralize thenotion ofasolution totheproblem. Forexample, ifwe\nstart in state 1, there is no single sequence of actions that solves the problem. Instead, we\nneedacontingency plansuchasthefollowing:\n[Suck,ifState=5then[Right,Suck]else[]]. (4.3)\nThus, solutions for nondeterministic problems can contain nested if\u2013then\u2013else statements;\nthis means that they are trees rather than sequences. This allows the selection of actions\nbased oncontingencies arising during execution. Manyproblems in thereal, physical world\nare contingency problems because exact prediction is impossible. For this reason, many\npeoplekeeptheireyesopenwhilewalkingaroundordriving.\n9 We assume that most readers face similar problems and can sympathize with our agent. We apologize to\nownersofmodern,efficienthomeapplianceswhocannottakeadvantageofthispedagogicaldevice. Section4.3. SearchingwithNondeterministic Actions 135\n4.3.2 AND\u2013OR searchtrees\nThe next question is how to find contingent solutions to nondeterministic problems. As in\nChapter3,webeginbyconstructingsearchtrees,butherethetreeshaveadifferentcharacter.\nIn adeterministic environment, the only branching isintroduced by the agent\u2019s ownchoices\nORNODE in each state. We call these nodes OR nodes. In the vacuum world, for example, at an OR\nnode the agent chooses Left orRight orSuck. In anondeterministic environment, branching\nis also introduced by the environment\u2019s choice of outcome for each action. We call these\nANDNODE nodes AND nodes. For example, the Suck action in state 1 leads to a state in the set {5,7},\nso the agent would need to find a plan for state 5 and for state 7. These two kinds of nodes\nAND\u2013ORTREE\nalternate, leading toan AND\u2013OR treeasillustrated inFigure4.10.\nAsolution foran AND\u2013OR searchproblemisasubtreethat(1)hasagoalnodeatevery\nleaf, (2) specifies one action ateach of its OR nodes, and (3) includes every outcome branch\nat each of its AND nodes. The solution is shown in bold lines in the figure; it corresponds\nto the plan given in Equation (4.3). (The plan uses if\u2013then\u2013else notation to handle the AND\nbranches,butwhentherearemorethantwobranchesatanode, itmightbebettertouseacase\n1\nSuck Right\n7 5 2\nGOAL\nSuck Right Left Suck\n5 1 6 1 8 4\nSuck Left\nLOOP LOOP LOOP GOAL\n8 5\nGOAL LOOP\nFigure 4.10 The first two levels of the search tree for the erratic vacuum world. State\nnodesareORnodeswheresomeactionmustbechosen.AttheANDnodes,shownascircles,\neveryoutcomemustbehandled,asindicatedbythearclinkingtheoutgoingbranches. The\nsolutionfoundisshowninboldlines. 136 Chapter 4. BeyondClassicalSearch\nfunctionAND-OR-GRAPH-SEARCH(problem)returnsa conditional plan, or failure\nOR-SEARCH(problem.INITIAL-STATE,problem,[])\nfunctionOR-SEARCH(state,problem,path)returnsa conditional plan, or failure\nifproblem.GOAL-TEST(state)thenreturntheemptyplan\nifstate isonpath thenreturnfailure\nforeachaction inproblem.ACTIONS(state)do\nplan\u2190AND-SEARCH(RESULTS(state,action),problem,[state |path])\nifplan (cid:7)=failure thenreturn[action | plan]\nreturnfailure\nfunctionAND-SEARCH(states,problem,path)returnsa conditional plan, or failure\nforeachsiinstates do\nplan i\u2190OR-SEARCH(si,problem,path)\nifplan =failure thenreturnfailure\ni\nreturn[ifs 1thenplan 1elseifs 2thenplan 2else ...ifsn\u22121thenplan n\u22121elseplan n]\nFigure 4.11 An algorithmforsearching AND\u2013OR graphsgeneratedby nondeterministic\nenvironments.Itreturnsaconditionalplanthatreachesagoalstateinallcircumstances.(The\nnotation[x|l]referstothelistformedbyaddingobjectxtothefrontoflistl.)\nconstruct.) Modifying the basic problem-solving agent shown in Figure 3.1 to execute con-\ntingentsolutionsofthiskindisstraightforward. Onemayalsoconsiderasomewhatdifferent\nagentdesign, inwhichtheagentcanactbeforeithasfoundaguaranteed plananddealswith\nsome contingencies only as they arise during execution. Thistype of interleaving of search\nINTERLEAVING\nandexecution isalsousefulforexploration problems (seeSection4.5)andforgameplaying\n(seeChapter5).\nFigure 4.11 gives a recursive, depth-first algorithm for AND\u2013OR graph search. One\nkey aspect of the algorithm is the way in which it deals with cycles, which often arise in\nnondeterministic problems (e.g., if an action sometimes has no effect or if an unintended\neffect can be corrected). If the current state is identical to a state on the path from the root,\nthenitreturnswithfailure. Thisdoesn\u2019tmeanthatthereis nosolutionfromthecurrentstate;\nit simply means that if there is a noncyclic solution, it must be reachable from the earlier\nincarnation ofthecurrentstate,sothenewincarnation can bediscarded. Withthischeck,we\nensurethatthealgorithmterminatesineveryfinitestatespace,becauseeverypathmustreach\nagoal, adead end, orarepeated state. Notice that the algorithm does notcheck whetherthe\ncurrentstateisarepetitionofastateonsomeotherpathfromtheroot,whichisimportantfor\nefficiency. Exercise4.5investigates thisissue.\nAND\u2013ORgraphscanalsobeexploredbybreadth-firstorbest-firstmethods. Theconcept\nof a heuristic function must be modified to estimate the cost of a contingent solution rather\n\u2217\nthan asequence, but the notion of admissibility carries over and there is an analog of the A\nalgorithm forfindingoptimal solutions. Pointers aregiven inthebibliographical notes atthe\nendofthechapter. Section4.3. SearchingwithNondeterministic Actions 137\n1\nSuck Right\n5 2\nRight\n6\nFigure4.12 Partofthesearchgraphfortheslipperyvacuumworld,wherewehaveshown\n(some)cyclesexplicitly. Allsolutionsforthisproblemarecyclicplansbecausethereis no\nwaytomovereliably.\n4.3.3 Try,try again\nConsider the slippery vacuum world, which is identical to the ordinary (non-erratic) vac-\nuumworldexcept thatmovementactionssometimes fail,leaving theagentinthesameloca-\ntion. For example, moving Right in state 1 leads to the state set {1,2}. Figure 4.12 shows\npart of the search graph; clearly, there are no longer any acyclic solutions from state 1, and\nCYCLICSOLUTION\nAND-OR-GRAPH-SEARCH would return with failure. There is, however, a cyclic solution,\nwhichistokeeptryingRight untilitworks. Wecanexpressthissolutionbyaddingalabelto\nLABEL\ndenote someportion oftheplanandusing thatlabel laterinstead ofrepeating theplanitself.\nThus,ourcyclicsolutionis\n[Suck,L : Right,ifState=5thenL elseSuck].\n1 1\n(A better syntax for the looping part of this plan would be \u201cwhile State=5 do Right.\u201d)\nIn general a cyclic plan may be considered a solution provided that every leaf is a goal\nstate and that a leaf is reachable from every point in the plan. The modifications needed\nto AND-OR-GRAPH-SEARCH arecoveredinExercise4.6. Thekeyrealization isthataloop\ninthestatespacebacktoastateLtranslates toaloopintheplanbacktothepointwherethe\nsubplan forstate Lisexecuted.\nGiventhedefinitionofacyclicsolution,anagentexecutingsuchasolutionwilleventu-\nallyreachthegoalprovidedthateachoutcomeofanondeterministicactioneventuallyoccurs.\nIs this condition reasonable? It depends on the reason forthe nondeterminism. If the action\nrollsadie,then it\u2019sreasonable tosuppose thateventually asixwillberolled. Iftheaction is\ntoinsertahotelcardkeyintothedoorlock,butitdoesn\u2019t workthefirsttime,thenperhaps it\nwilleventuallywork,orperhapsonehasthewrongkey(orthe wrongroom!). Aftersevenor 138 Chapter 4. BeyondClassicalSearch\neighttries,mostpeoplewillassumetheproblem iswiththekeyandwillgobacktothefront\ndesk to getanew one. Onewayto understand this decision isto saythat the initial problem\nformulation (observable, nondeterministic) is abandoned in favor of a different formulation\n(partially observable, deterministic) where the failure is attributed to an unobservable prop-\nertyofthekey. WehavemoretosayonthisissueinChapter13.\n4.4 SEARCHING WITH PARTIAL OBSERVATIONS\nWe now turn to the problem of partial observability, where the agent\u2019s percepts do not suf-\nfice to pin down the exact state. As noted at the beginning of the previous section, if the\nagent is in one of several possible states, then an action may lead to one of several possible\noutcomes\u2014even if the environment is deterministic. The key concept required for solving\npartiallyobservableproblemsisthe beliefstate,representing theagent\u2019scurrentbeliefabout\nBELIEFSTATE\nthe possible physical states it might be in, given the sequence of actions and percepts up to\nthat point. Webegin withthe simplest scenario forstudying belief states, which iswhen the\nagenthasnosensorsatall;thenweaddinpartialsensingaswellasnondeterministic actions.\n4.4.1 Searching withno observation\nWhen the agent\u2019s percepts provide no information at all, we have what is called a sensor-\nless problem or sometimes a conformant problem. At first, one might think the sensorless\nSENSORLESS\nagent has no hope ofsolving aproblem if ithas no idea whatstate it\u2019s in; in fact, sensorless\nCONFORMANT\nproblems are quite often solvable. Moreover, sensorless agents can be surprisingly useful,\nprimarily because they don\u2019t rely on sensors working properly. In manufacturing systems,\nforexample,manyingeniousmethodshavebeendevelopedfororientingpartscorrectlyfrom\nan unknown initial position by using a sequence of actions with no sensing at all. The high\ncost of sensing is another reason to avoid it: for example, doctors often prescribe a broad-\nspectrum antibiotic rather than using the contingent plan of doing an expensive blood test,\nthenwaitingfortheresults tocomeback, andthenprescribing amorespecificantibiotic and\nperhapshospitalization becausetheinfection hasprogressed toofar.\nWecan make a sensorless version of the vacuum world. Assume that the agent knows\nthe geography of its world, but doesn\u2019t know its location or the distribution of dirt. In that\ncase,itsinitialstatecouldbeanyelementoftheset{1,2,3,4,5,6,7,8}. Now,considerwhat\nhappensifittriestheactionRight. Thiswillcauseittobeinoneofthestates{2,4,6,8}\u2014the\nagentnowhasmoreinformation! Furthermore, theactionsequence [Right,Suck]willalways\nendup inone ofthestates {4,8}. Finally, thesequence [Right,Suck,Left,Suck]isguaranteed\nto reach the goal state 7 no matter what the start state. We say that the agent can coerce the\nCOERCION\nworldintostate7.\nTosolvesensorlessproblems,wesearchinthespaceofbeliefstatesratherthanphysical\nstates.10 Notice that in belief-state space, the problem is fully observable because the agent\n10 Inafullyobservableenvironment,eachbeliefstatecontainsonephysicalstate. Thus,wecanviewthealgo-\nrithmsinChapter3assearchinginabelief-statespaceofsingletonbeliefstates. Section4.4. SearchingwithPartialObservations 139\nalwaysknowsitsownbeliefstate. Furthermore, thesolution (ifany)isalwaysasequence of\nactions. Thisisbecause,asintheordinaryproblemsofChapter3,theperceptsreceivedafter\neachactionarecompletelypredictable\u2014they\u2019re alwaysempty! Sotherearenocontingencies\ntoplanfor. Thisistrue eveniftheenvironment isnondeterminstic.\nIt is instructive to see how the belief-state search problem is constructed. Suppose\ntheunderlying physical problem P isdefined by ACTIONSP, RESULTP, GOAL-TESTP,and\nSTEP-COSTP. Thenwecandefinethecorresponding sensorless problem asfollows:\n\u2022 Beliefstates: Theentirebelief-statespacecontainseverypossiblesetofphysicalstates.\nIfP hasN states,thenthesensorless problemhasupto2N states,although manymay\nbeunreachable fromtheinitialstate.\n\u2022 Initial state: Typically the set of allstates in P, although insome cases the agent will\nhavemoreknowledge thanthis.\n\u2022 Actions: This is slightly tricky. Suppose the agent is in belief state b={s ,s }, but\n1 2\nACTIONSP(s 1) (cid:7)= ACTIONSP(s 2);thentheagent isunsure ofwhichactions arelegal.\nIf we assume that illegal actions have no effect on the environment, then it is safe to\ntaketheunionofalltheactionsinanyofthephysicalstatesinthecurrent beliefstateb:\n(cid:15)\nACTIONS(b) = ACTIONSP(s).\ns\u2208b\nOntheotherhand, ifanillegalactionmightbetheendoftheworld, itissafertoallow\nonly the intersection, that is, the set of actions legal in all the states. For the vacuum\nworld,everystatehasthesamelegalactions, sobothmethodsgivethesameresult.\n\u2022 Transition model: The agent doesn\u2019t know which state in the belief state is the right\none; so as far as it knows, it might get to any of the states resulting from applying the\naction tooneofthe physical states inthebelief state. Fordeterministic actions, theset\nofstatesthatmightbereached is\nb(cid:2) = RESULT(b,a)= {s(cid:2) : s(cid:2) =RESULTP(s,a)ands \u2208 b}. (4.4)\n(cid:2)\nWithdeterministic actions, b isneverlargerthan b. Withnondeterminism, wehave\nb(cid:2) = RESULT(b,a) = { (cid:15)s(cid:2) : s(cid:2) \u2208 RESULTSP(s,a)ands \u2208 b}\n= RESULTSP(s,a),\ns\u2208b\nwhich may be larger than b, as shown in Figure 4.13. The process of generating\n(cid:2)\nthe new belief state after the action is called the prediction step; the notation b =\nPREDICTION\nPREDICTP(b,a)willcomeinhandy.\n\u2022 Goaltest: The agent wants a plan that is sure to work, which means that a belief state\nsatisfies the goal only if all the physical states in it satisfy GOAL-TESTP. The agent\nmayaccidentally achievethegoalearlier, butitwon\u2019t knowthatithasdoneso.\n\u2022 Path cost: This is also tricky. If the same action can have different costs in different\nstates, then the cost of taking an action in a given belief state could be one of several\nvalues. (Thisgives risetoanew class ofproblems, which weexplore inExercise 4.9.)\nFor now we assume that the cost of an action is the same in all states and so can be\ntransferred directlyfromtheunderlying physical problem. 140 Chapter 4. BeyondClassicalSearch\n1\n1 2 1 2\n3 4 3 4\n3\n(a) (b)\nFigure 4.13 (a) Predicting the next belief state for the sensorless vacuum world with a\ndeterministicaction,Right. (b)Predictionforthesamebeliefstateandactionintheslippery\nversionofthesensorlessvacuumworld.\nFigure 4.14 shows the reachable belief-state space for the deterministic, sensorless vacuum\nworld. Thereareonly12reachable beliefstatesoutof 28=256possible beliefstates.\nThepreceding definitions enabletheautomaticconstruction ofthebelief-state problem\nformulation from the definition of the underlying physical problem. Once this is done, we\ncan apply any of the search algorithms of Chapter 3. In fact, we can do a little bit more\nthan that. In \u201cordinary\u201d graph search, newly generated states are tested to see if they are\nidentical toexisting states. Thisworksforbelief states, too; forexample, inFigure 4.14, the\naction sequence [Suck,Left,Suck] starting at the initial state reaches the same belief state as\n[Right,Left,Suck], namely, {5,7}. Now, consider the belief state reached by [Left], namely,\n{1,3,5,7}. Obviously, this is not identical to {5,7}, but it is a superset. It is easy to prove\n(Exercise4.8)thatifanactionsequenceisasolutionforabeliefstateb,itisalsoasolutionfor\nanysubset ofb. Hence, wecandiscardapathreaching {1,3,5,7} if{5,7} hasalready been\ngenerated. Conversely, if {1,3,5,7} has already been generated and found to be solvable,\nthenanysubset, suchas{5,7}, isguaranteed tobesolvable. Thisextralevelofpruningmay\ndramatically improvetheefficiencyofsensorless problem solving.\nEvenwiththisimprovement,however,sensorlessproblem-solvingaswehavedescribed\nitisseldom feasible inpractice. Thedifficulty isnotsomuchthevastness ofthebelief-state\nspace\u2014even though it is exponentially larger than the underlying physical state space; in\nmost cases the branching factor and solution length in the belief-state space and physical\nstate space are not so different. The real difficulty lies with the size of each belief state. For\nexample, the initial belief state forthe 10\u00d710 vacuum worldcontains 100\u00d72100 oraround\n1032 physical states\u2014far too many if we use the atomic representation, which is an explicit\nlistofstates.\nOne solution is to represent the belief state by some more compact description. In\nEnglish, we could say the agent knows \u201cNothing\u201d in the initial state; after moving Left, we\ncould say, \u201cNot in the rightmost column,\u201d and so on. Chapter 7 explains how to do this in a\nformal representation scheme. Another approach is to avoid the standard search algorithms,\nwhichtreatbeliefstatesasblackboxesjustlikeanyotherproblemstate. Instead,wecanlook Section4.4. SearchingwithPartialObservations 141\nL\nR\n1 3 1 2 3 2 4\nL R\n5 7 4 5 6 6 8\n7 8\nS\n4 5\nS S\n7 8\nL R\nL\n5 S 5 3 6 4 S 4\n7 7 8 8\nR\nR L L R\nL\n6 S S 3\n8 7\n8 R 7\nFigure4.14 Thereachableportionofthebelief-statespaceforthedeterministic,sensor-\nlessvacuumworld.Eachshadedboxcorrespondstoasinglebeliefstate. Atanygivenpoint,\nthe agentis in a particularbeliefstate butdoesnotknowwhichphysicalstate it is in. The\ninitial belief state (complete ignorance) is the top center box. Actions are represented by\nlabeledlinks. Self-loopsareomittedforclarity.\nINCREMENTAL\ninsidethebeliefstatesanddevelopincrementalbelief-statesearch algorithms thatbuildup\nBELIEF-STATE\nSEARCH\nthe solution one physical state at a time. For example, in the sensorless vacuum world, the\ninitial belief state is{1,2,3,4,5,6,7,8}, and wehave tofindan action sequence that works\ninall8states. Wecandothisbyfirstfindingasolution thatworksforstate1;thenwecheck\nifitworksforstate 2;ifnot, goback andfindadifferent solution forstate1, andsoon. Just\nasan AND\u2013OR search hastofindasolution forevery branch at an AND node, thisalgorithm\nhastofindasolution forevery stateinthebelief state; thedifference isthat AND\u2013OR search\ncan find a different solution for each branch, whereas an incremental belief-state search has\ntofindonesolutionthatworksforallthestates.\nThe main advantage of the incremental approach is that it is typically able to detect\nfailurequickly\u2014when abeliefstateisunsolvable, itisusuallythecasethatasmallsubsetof\nthebeliefstate,consisting ofthefirstfewstatesexamined, isalsounsolvable. Insomecases, 142 Chapter 4. BeyondClassicalSearch\nthisleads toaspeedup proportional tothesizeofthebelief states, whichmaythemselves be\naslargeasthephysicalstatespaceitself.\nEven the most efficient solution algorithm is not of much use when no solutions exist.\nMany things just cannot be done without sensing. For example, the sensorless 8-puzzle is\nimpossible. On the other hand, a little bit of sensing can go a long way. Forexample, every\n8-puzzle instanceissolvableifjustonesquareisvisible\u2014the solution involves movingeach\ntileinturnintothevisiblesquareandthenkeeping trackof itslocation.\n4.4.2 Searching withobservations\nForageneralpartiallyobservableproblem,wehavetospecifyhowtheenvironmentgenerates\npercepts for the agent. Forexample, we might define the local-sensing vacuum world to be\noneinwhichtheagenthasapositionsensorandalocaldirtsensorbuthasnosensorcapable\nof detecting dirt in other squares. The formal problem specification includes a PERCEPT(s)\nfunction that returns the percept received in a given state. (If sensing is nondeterministic,\nthenweuseaPERCEPTS functionthatreturnsasetofpossiblepercepts.) Forexample,inthe\nlocal-sensingvacuumworld,thePERCEPTinstate1is[A,Dirty]. Fullyobservableproblems\nare aspecial case in which PERCEPT(s)=sforevery state s, whilesensorless problems are\naspecialcaseinwhich PERCEPT(s)=null.\nWhenobservations are partial, itwillusually be the case that several states could have\nproduced any given percept. For example, the percept [A,Dirty] is produced by state 3 as\nwell as by state 1. Hence, given this as the initial percept, the initial belief state for the\nlocal-sensing vacuum world will be {1,3}. The ACTIONS, STEP-COST, and GOAL-TEST\nareconstructedfromtheunderlying physicalproblemjustasforsensorlessproblems,butthe\ntransition model isabitmorecomplicated. Wecan think of transitions from one belief state\ntothenextforaparticularactionasoccurring inthreestages, asshowninFigure4.15:\n\u2022 Thepredictionstageisthesameasforsensorlessproblems: giventheactionainbelief\nstateb,thepredicted beliefstateis\u02c6b=PREDICT(b,a).11\n\u2022 The observation prediction stage determines the set of percepts o that could be ob-\nservedinthepredicted beliefstate:\nPOSSIBLE-PERCEPTS(\u02c6b) = {o: o=PERCEPT(s)ands \u2208\u02c6b}.\n\u2022 The update stage determines, for each possible percept, the belief state that would\nresult from the percept. The new belief state b is just the set of states in\u02c6b that could\no\nhaveproduced thepercept:\nb\no\n= UPDATE(\u02c6b,o) = {s :o=PERCEPT(s)ands \u2208\u02c6b}.\nNoticethateachupdatedbeliefstateb canbenolargerthanthepredictedbeliefstate\u02c6b;\no\nobservations can only help reduce uncertainty compared to the sensorless case. More-\nover, for deterministic sensing, the belief states for the different possible percepts will\nbedisjoint, formingapartition oftheoriginal predicted beliefstate.\n11 Here,andthroughoutthebook,the\u201chat\u201din\u02c6bmeansanestimatedorpredictedvalueforb. Section4.4. SearchingwithPartialObservations 143\n[B,Dirty] 2\nRight\n1 2\n(a)\n3 4\n[B,Clean]\n4\n[B,Dirty] 2\nRight 2\n1 1 [A,Dirty] 1\n(b)\n3 3 3\n4\n[B,Clean]\n4\nFigure 4.15 Two example of transitions in local-sensing vacuum worlds. (a) In the de-\nterministic world, Right is applied in the initial belief state, resulting in a new belief state\nwith two possible physicalstates; forthose states, the possible perceptsare [B,Dirty] and\n[B,Clean], leading to two belief states, each of which is a singleton. (b) In the slippery\nworld, Right is applied in the initial belief state, giving a new belief state with four physi-\ncalstates; forthosestates, thepossibleperceptsare [A,Dirty], [B,Dirty], and[B,Clean],\nleadingtothreebeliefstatesasshown.\nPuttingthesethreestagestogether, weobtainthepossible beliefstatesresultingfromagiven\nactionandthesubsequent possible percepts:\nRESULTS(b,a) = {b\no\n:b\no\n= UPDATE(PREDICT(b,a),o)and\no \u2208 POSSIBLE-PERCEPTS(PREDICT(b,a))}. (4.5)\nAgain,thenondeterminisminthepartiallyobservableproblemcomesfromtheinability\ntopredict exactly whichpercept willbereceived afteracting; underlying nondeterminism in\nthe physical environment may contribute to this inability by enlarging the belief state at the\nprediction stage, leadingtomorepercepts attheobservation stage.\n4.4.3 Solvingpartially observableproblems\nThe preceding section showed how to derive the RESULTS function for a nondeterministic\nbelief-state problem fromanunderlying physicalproblem andthe PERCEPT function. Given 144 Chapter 4. BeyondClassicalSearch\n1\n3\nSuck Right\n[A,Clean] [B,Dirty] [B,Clean]\n5\n2 4\n7\nFigure4.16 Thefirstlevelofthe AND\u2013OR searchtreeforaprobleminthelocal-sensing\nvacuumworld;Suck isthefirststepofthesolution.\nsuch a formulation, the AND\u2013OR search algorithm of Figure 4.11 can be applied directly to\nderive a solution. Figure 4.16 shows part of the search tree for the local-sensing vacuum\nworld,assuminganinitialpercept [A,Dirty]. Thesolution istheconditional plan\n[Suck,Right,ifBstate={6}thenSuckelse[]].\nNotice that, because we supplied a belief-state problem to the AND\u2013OR search algorithm, it\nreturned a conditional plan that tests the belief state rather than the actual state. This isas it\nshouldbe: inapartiallyobservableenvironmenttheagentwon\u2019tbeabletoexecuteasolution\nthatrequires testingtheactualstate.\nAsinthecaseofstandard search algorithms applied tosensorless problems, the AND\u2013\nOR search algorithm treats belief states as black boxes, just like any other states. One can\nimproveonthisbycheckingforpreviouslygeneratedbeliefstatesthataresubsetsorsupersets\nof the current state, just as for sensorless problems. One can also derive incremental search\nalgorithms, analogous to those described for sensorless problems, that provide substantial\nspeedups overtheblack-box approach.\n4.4.4 Anagentforpartiallyobservable environments\nThedesign ofaproblem-solving agent forpartially observable environments isquitesimilar\nto the simple problem-solving agent in Figure 3.1: the agent formulates a problem, calls a\nsearchalgorithm (such as AND-OR-GRAPH-SEARCH)tosolveit,andexecutes thesolution.\nThere are two main differences. First, the solution to a problem will be a conditional plan\nrather than a sequence; if the first step is an if\u2013then\u2013else expression, the agent will need to\ntesttheconditionintheif-partandexecutethethen-partortheelse-partaccordingly. Second,\nthe agent will need to maintain its belief state as it performs actions and receives percepts.\nThis process resembles the prediction\u2013observation\u2013update process in Equation (4.5) but is\nactuallysimplerbecausetheperceptisgivenbytheenvironmentratherthancalculatedbythe Section4.4. SearchingwithPartialObservations 145\nSuck [A,Clean] Right 2 [B,Dirty]\n1 5 5 6 2\n3 7 7 4 6\n8\nFigure4.17 Twoprediction\u2013updatecyclesofbelief-statemaintenanceinthekindergarten\nvacuumworldwithlocalsensing.\nagent. Givenaninitialbeliefstateb,anactiona,andapercept o,thenewbeliefstateis:\n(cid:2)\nb = UPDATE(PREDICT(b,a),o). (4.6)\nFigure 4.17 shows the belief state being maintained in the kindergarten vacuum world with\nlocal sensing, wherein any square may become dirty at any time unless the agent is actively\ncleaning itatthatmoment.12\nIn partially observable environments\u2014which include the vast majority of real-world\nenvironments\u2014maintaining one\u2019s belief state is a core function of any intelligent system.\nThis function goes under various names, including monitoring, filtering and state estima-\nMONITORING\ntion. Equation (4.6) iscalled arecursive state estimator because it computes the new belief\nFILTERING\nstatefromthepreviousoneratherthanbyexaminingtheentireperceptsequence. Iftheagent\nSTATEESTIMATION\nis not to \u201cfall behind,\u201d the computation has to happen as fast as percepts are coming in. As\nRECURSIVE\nthe environment becomes more complex, the exact update computation becomes infeasible\nand the agent willhave to compute an approximate belief state, perhaps focusing on the im-\nplications ofthe percept fortheaspects ofthe environment that are ofcurrent interest. Most\nwork on this problem has been done for stochastic, continuous-state environments with the\ntools of probability theory, as explained in Chapter 15. Here we will show an example in a\ndiscreteenvironment withdetrministic sensorsandnondeterministic actions.\nThe example concerns a robot with the task of localization: working out where it is,\nLOCALIZATION\ngiven amap of the world and a sequence of percepts and actions. Ourrobot is placed in the\nmaze-like environment of Figure 4.18. The robot is equipped with four sonar sensors that\ntell whether there is an obstacle\u2014the outer wall or a black square in the figure\u2014in each of\nthefourcompassdirections. Weassumethatthesensors give perfectly correct data, andthat\nthe robot has a correct map of the enviornment. But unfortunately the robot\u2019s navigational\nsystemisbroken,sowhenitexecutesaMoveaction,itmovesrandomlytooneoftheadjacent\nsquares. Therobot\u2019staskistodetermineitscurrentlocation.\nSuppose the robot has just been switched on, so it does not know where it is. Thus its\ninitial belief state b consists of the set of all locations. The the robot receives the percept\n12 Theusualapologiestothosewhoareunfamiliarwiththeeffectofsmallchildrenontheenvironment. 146 Chapter 4. BeyondClassicalSearch\n(a)Possiblelocations ofrobotafterE = NSW\n1\n(b)Possiblelocations ofrobotAfterE = NSW,E = NS\n1 2\nFigure4.18 Possiblepositionsoftherobot,(cid:13),(a)afteroneobservationE =NSW and\n1\n(b)afterasecondobservationE =NS.Whensensorsarenoiselessandthetransitionmodel\n2\nisaccurate,therearenootherpossiblelocationsfortherobotconsistentwiththissequence\noftwoobservations.\nNSW,meaningthereareobstaclestothenorth,west,andsouth,anddoesanupdateusingthe\nequation b o=UPDATE(b),yielding the4locations showninFigure4.18(a). Youcaninspect\nthemazetoseethatthosearetheonlyfourlocations thatyieldthepercept NWS.\nNexttherobot executes aMove action, buttheresultisnondeterministic. Thenewbe-\nliefstate,b a=PREDICT(b o,Move),containsallthelocationsthatareonestepawayfromthe\nlocations in b o. When the second percept, NS, arrives, the robot does UPDATE(b a,NS) and\nfinds that the belief state has collapsed down to the single location shown in Figure 4.18(b).\nThat\u2019stheonlylocation thatcouldbetheresultof\nUPDATE(PREDICT(UPDATE(b,NSW),Move),NS).\nWith nondetermnistic actions the PREDICT step grows the belief state, but the UPDATE step\nshrinks it back down\u2014as long as the percepts provide some useful identifying information.\nSometimes the percepts don\u2019t help much for localization: If there were one or more long\neast-west corridors, then a robot could receive a long sequence of NS percepts, but never\nknowwhereinthecorridor(s) itwas. Section4.5. OnlineSearchAgentsandUnknownEnvironments 147\n4.5 ONLINE SEARCH AGENTS AND UNKNOWN ENVIRONMENTS\nSo far we have concentrated on agents that use offline search algorithms. They compute\nOFFLINESEARCH\na complete solution before setting foot in the real world and then execute the solution. In\ncontrast,anonlinesearch13agentinterleavescomputationandaction: firstittakesanaction,\nONLINESEARCH\nthen itobserves theenvironment and computes the nextaction. Online search isagoodidea\nin dynamic or semidynamic domains\u2014domains where there is a penalty for sitting around\nand computing too long. Online search is also helpful in nondeterministic domains because\nit allows the agent to focus its computational efforts on the contingencies that actually arise\nrather than those that might happen but probably won\u2019t. Of course, there is a tradeoff: the\nmoreanagentplansahead, thelessoftenitwillfinditselfupthecreekwithoutapaddle.\nOnlinesearchisanecessary ideaforunknownenvironments, wheretheagentdoesnot\nknow what states exist or what its actions do. In this state of ignorance, the agent faces an\nEXPLORATION exploration problem and must use its actions as experiments in order to learn enough to\nPROBLEM\nmakedeliberation worthwhile.\nThecanonical example of online search is a robot that is placed in anew building and\nmustexplore ittobuild amapthatitcanuse forgetting from AtoB. Methods forescaping\nfromlabyrinths\u2014required knowledge foraspiring heroesof antiquity\u2014are alsoexamplesof\nonline search algorithms. Spatial exploration is not the only form of exploration, however.\nConsider a newborn baby: it has many possible actions but knows the outcomes of none of\nthem, and it has experienced only a few of the possible states that it can reach. The baby\u2019s\ngradualdiscovery ofhowtheworldworksis,inpart,anonlinesearchprocess.\n4.5.1 Onlinesearchproblems\nAnonline search problem mustbesolved byan agent executing actions, rather than bypure\ncomputation. We assume a deterministic and fully observable environment (Chapter 17 re-\nlaxestheseassumptions), butwestipulate thattheagentknowsonlythefollowing:\n\u2022 ACTIONS(s),whichreturnsalistofactions allowedinstate s;\n\u2022 The step-cost function c(s,a,s(cid:2) )\u2014note that this cannot be used until the agent knows\n(cid:2)\nthats istheoutcome; and\n\u2022 GOAL-TEST(s).\nNote in particular that the agent cannot determine RESULT(s,a) except by actually being\nin s and doing a. For example, in the maze problem shown in Figure 4.19, the agent does\nnot know that going Up from (1,1) leads to (1,2); nor, having done that, does it know that\ngoing Down will take it back to (1,1). This degree of ignorance can be reduced in some\napplications\u2014for example,arobotexplorermightknowhowitsmovementactionsworkand\nbeignorantonlyofthelocations ofobstacles.\n13 Theterm\u201conline\u201discommonlyusedincomputersciencetorefertoalgorithmsthatmustprocessinputdata\nastheyarereceivedratherthanwaitingfortheentireinputdatasettobecomeavailable. 148 Chapter 4. BeyondClassicalSearch\n3 G\n2\n1 S\n1 2 3\nFigure4.19 Asimplemazeproblem. TheagentstartsatS andmustreachGbutknows\nnothingoftheenvironment.\nG\nS A\nS G\nS A\nG\n(a) (b)\nFigure4.20 (a)Twostatespacesthatmightleadanonlinesearchagentintoadeadend.\nAnygivenagentwillfailinatleastoneofthesespaces. (b)Atwo-dimensionalenvironment\nthat can cause an online search agent to follow an arbitrarily inefficient route to the goal.\nWhicheverchoice the agentmakes, the adversaryblocksthat route with anotherlong, thin\nwall,sothatthepathfollowedismuchlongerthanthebestpossiblepath.\nFinally, the agent might have access to an admissible heuristic function h(s) that es-\ntimates the distance from the current state to a goal state. For example, in Figure 4.19, the\nagentmightknowthelocationofthegoalandbeabletousetheManhattan-distanceheuristic.\nTypically,theagent\u2019sobjectiveistoreachagoalstatewhileminimizingcost. (Another\npossibleobjectiveissimplytoexploretheentireenvironment.) Thecostisthetotalpathcost\nof the path that the agent actually travels. It is common to compare this cost with the path\ncost of the path the agent would follow if it knew the search space in advance\u2014that is, the\nactual shortest path (orshortest complete exploration). Inthelanguage ofonline algorithms,\nthisiscalledthecompetitiveratio;wewouldlikeittobeassmallaspossible.\nCOMPETITIVERATIO Section4.5. OnlineSearchAgentsandUnknownEnvironments 149\nAlthoughthissounds likeareasonable request, itiseasytoseethatthebestachievable\ncompetitive ratio is infinite in some cases. For example, if some actions are irreversible\u2014\nIRREVERSIBLE\ni.e., they lead to a state from which no action leads back to the previous state\u2014the online\nsearchmightaccidentally reachadead-endstatefromwhichnogoalstateisreachable. Per-\nDEADEND\nhaps the term \u201caccidentally\u201d is unconvincing\u2014after all, there might be an algorithm that\nhappensnottotakethedead-endpathasitexplores. Ourclaim,tobemoreprecise,isthatno\nalgorithm canavoiddeadendsinallstatespaces. Considerthetwodead-end statespacesin\nFigure 4.20(a). To an online search algorithm that has visited states S and A, the two state\nspaces look identical, so it must make the same decision in both. Therefore, it will fail in\nADVERSARY one of them. This isan example of an adversary argument\u2014wecan imagine an adversary\nARGUMENT\nconstructing the state space while the agent explores it and putting the goals and dead ends\nwhereveritchooses.\nDeadendsarearealdifficultyforrobotexploration\u2014staircases, ramps,cliffs,one-way\nstreets, andallkindsofnaturalterrainpresentopportunities forirreversibleactions. Tomake\nprogress,wesimplyassumethatthestatespaceissafelyexplorable\u2014thatis,somegoalstate\nSAFELYEXPLORABLE\nis reachable from every reachable state. State spaces with reversible actions, such as mazes\nand8-puzzles, canbeviewedasundirected graphsandareclearlysafelyexplorable.\nEven in safely explorable environments, no bounded competitive ratio can be guaran-\nteed if there are paths of unbounded cost. This is easy to show in environments with irre-\nversible actions, but in fact it remains true for the reversible case as well, as Figure 4.20(b)\nshows. Forthisreason,itiscommontodescribetheperformanceofonlinesearchalgorithms\nintermsofthesizeoftheentirestatespaceratherthanjust thedepthoftheshallowest goal.\n4.5.2 Onlinesearchagents\nAftereachaction, anonlineagentreceivesapercepttelling itwhatstateithasreached; from\nthis information, it can augment its map of the environment. The current map is used to\ndecide where to go next. This interleaving of planning and action means that online search\nalgorithmsarequitedifferentfromtheofflinesearchalgorithmswehaveseenpreviously. For\n\u2217\nexample, offline algorithms such as A can expand a node in one part of the space and then\nimmediately expand a node in another part of the space, because node expansion involves\nsimulated rather than real actions. An online algorithm, on the other hand, can discover\nsuccessors only for a node that it physically occupies. To avoid traveling all the way across\nthetreetoexpandthenextnode,itseemsbettertoexpandnodesinalocalorder. Depth-first\nsearchhasexactlythisproperty because(exceptwhenbacktracking) thenextnodeexpanded\nisachildoftheprevious nodeexpanded.\nAn online depth-first search agent is shown in Figure 4.21. This agent stores its map\nin a table, RESULT[s,a], that records the state resulting from executing action a in state s.\nWhenever an action from the current state has not been explored, the agent tries that action.\nThe difficulty comes when the agent has tried all the actions in a state. In offline depth-first\nsearch, the state is simply dropped from the queue; in an online search, the agent has to\nbacktrack physically. Indepth-firstsearch,thismeansgoingbacktothestatefromwhichthe\nagentmostrecentlyenteredthecurrentstate. Toachievethat,thealgorithmkeepsatablethat 150 Chapter 4. BeyondClassicalSearch\nfunctionONLINE-DFS-AGENT(s(cid:5))returnsanaction\ninputs:s(cid:5),aperceptthatidentifiesthecurrentstate\npersistent: result,atableindexedbystateandaction,initiallyempty\nuntried,atablethatlists,foreachstate,theactionsnotyettried\nunbacktracked,atablethatlists,foreachstate,thebacktracksnotyettried\ns,a,thepreviousstateandaction,initiallynull\nifGOAL-TEST(s(cid:5))thenreturnstop\nifs(cid:5)isanewstate(notinuntried)thenuntried[s(cid:5)]\u2190ACTIONS(s(cid:5))\nifs isnotnullthen\nresult[s,a]\u2190s(cid:5)\nadds tothefrontofunbacktracked[s(cid:5)]\nifuntried[s(cid:5)]isemptythen\nifunbacktracked[s(cid:5)]isemptythenreturnstop\nelsea\u2190anactionb suchthatresult[s(cid:5),b]=POP(unbacktracked[s(cid:5)])\nelsea\u2190POP(untried[s(cid:5)])\ns\u2190s(cid:5)\nreturna\nFigure4.21 Anonlinesearchagentthatusesdepth-firstexploration. Theagentisappli-\ncableonlyinstatespacesinwhicheveryactioncanbe\u201cundone\u201dbysomeotheraction.\nlists, foreach state, the predecessor states to which the agent has not yet backtracked. If the\nagenthasrunoutofstatestowhichitcanbacktrack, thenits searchiscomplete.\nWe recommend that the reader trace through the progress of ONLINE-DFS-AGENT\nwhen applied to the maze given in Figure 4.19. It is fairly easy to see that the agent will, in\ntheworstcase, enduptraversing everylinkinthestatespace exactly twice. Forexploration,\nthis is optimal; for finding a goal, on the other hand, the agent\u2019s competitive ratio could be\narbitrarily bad if it goes off on a long excursion when there is a goal right next to the initial\nstate. Anonlinevariantofiterativedeepeningsolvesthis problem;foranenvironment thatis\nauniform tree,thecompetitive ratioofsuchanagentisasmallconstant.\nBecause of its method of backtracking, ONLINE-DFS-AGENT works only in state\nspaces where the actions are reversible. There are slightly more complex algorithms that\nworkingeneralstatespaces,butnosuchalgorithm hasabounded competitiveratio.\n4.5.3 Onlinelocalsearch\nLike depth-first search, hill-climbing search has the property of locality in its node expan-\nsions. In fact, because it keeps just one current state in memory, hill-climbing search is\nalready an online search algorithm! Unfortunately, it is not very useful in its simplest form\nbecause it leaves the agent sitting at local maxima with nowhere to go. Moreover, random\nrestartscannotbeused,becausetheagentcannottransport itselftoanewstate.\nInstead of random restarts, one might consider using a random walk to explore the\nRANDOMWALK\nenvironment. Arandom walksimply selects atrandom one oftheavailable actions from the Section4.5. OnlineSearchAgentsandUnknownEnvironments 151\nS G\nFigure4.22 Anenvironmentinwhicharandomwalkwilltakeexponentiallymanysteps\ntofindthegoal.\ncurrent state; preference can be given to actions that have not yet been tried. It is easy to\nprove that a random walk will eventually find a goal or complete its exploration, provided\nthatthespaceisfinite.14 Ontheotherhand, theprocess canbeveryslow. Figure4.22shows\nan environment in which a random walk will take exponentially many steps to find the goal\nbecause, ateachstep,backwardprogressistwiceaslikelyasforwardprogress. Theexample\nis contrived, of course, but there are many real-world state spaces whose topology causes\nthesekindsof\u201ctraps\u201dforrandomwalks.\nAugmenting hill climbing withmemoryrather than randomness turns out to beamore\neffective approach. The basic idea is to store a \u201ccurrent best estimate\u201d H(s) of the cost to\nreach the goal from each state that has been visited. H(s) starts out being just the heuristic\nestimate h(s) and is updated as the agent gains experience in the state space. Figure 4.23\nshows a simple example in a one-dimensional state space. In (a), the agent seems to be\nstuck in a flat local minimum at the shaded state. Rather than staying where it is, the agent\nshould follow what seems to bethe best path tothe goal given the current cost estimates for\n(cid:2)\nits neighbors. The estimated cost to reach the goal through a neighbor s is the cost to get\n(cid:2) (cid:2) (cid:2)\nto s plus the estimated cost to get to a goal from there\u2014that is, c(s,a,s)+H(s). In the\nexample,therearetwoactions,withestimatedcosts1+9and1+2,soitseemsbesttomove\nright. Now, it is clear that the cost estimate of 2 for the shaded state was overly optimistic.\nSince the best move cost 1 and led to a state that is at least 2 steps from a goal, the shaded\nstate must be at least 3 steps from agoal, so its H should be updated accordingly, as shown\nin Figure 4.23(b). Continuing this process, the agent will move back and forth twice more,\nupdating H eachtimeand\u201cflattening out\u201dthelocalminimumuntilitescapes totheright.\n\u2217 \u2217\nAnagent implementing this scheme, whichiscalled learning real-time A (LRTA), is\nLRTA*\nshown in Figure 4.24. Like ONLINE-DFS-AGENT, it builds a map of the environment in\ntheresult table. Itupdates thecostestimate forthestate ithas justleft andthen chooses the\n\u201capparently best\u201d move according to its current cost estimates. One important detail is that\nactionsthathavenotyetbeentriedinastatesarealwaysassumedtoleadimmediatelytothe\nOPTIMISMUNDER goalwiththeleastpossiblecost,namelyh(s). Thisoptimismunderuncertaintyencourages\nUNCERTAINTY\ntheagenttoexplorenew,possiblypromising paths.\n\u2217\nAnLRTA agentisguaranteedtofindagoalinanyfinite,safelyexplorableenvironment.\n\u2217\nUnlikeA,however,itisnotcompleteforinfinitestatespaces\u2014therearecaseswhereitcanbe\nledinfinitelyastray. ItcanexploreanenvironmentofnstatesinO(n2)stepsintheworstcase,\n14 Randomwalksarecompleteoninfiniteone-dimensionalandtwo-dimensionalgrids. Onathree-dimensional\ngrid,theprobabilitythatthewalkeverreturnstothestartingpointisonlyabout0.3405(Hughes,1995). 152 Chapter 4. BeyondClassicalSearch\n1 1 1 1 1 1 1\n(a) 8 9 2 2 4 3\n1 1 1 1 1 1 1\n(b) 8 9 3 2 4 3\n1 1 1 1 1 1 1\n(c) 8 9 3 4 4 3\n1 1 1 1 1 1 1\n(d) 8 9 5 4 4 3\n1 1 1 1 1 1 1\n(e) 8 9 5 5 4 3\nFigure 4.23 Five iterations of LRTA\u2217 on a one-dimensional state space. Each state is\nlabeledwithH(s),thecurrentcostestimatetoreachagoal,andeachlinkislabeledwithits\nstepcost. Theshadedstatemarksthelocationoftheagent,andtheupdatedcostestimatesat\neachiterationarecircled.\nfunctionLRTA*-AGENT(s(cid:5))returnsanaction\ninputs:s(cid:5),aperceptthatidentifiesthecurrentstate\npersistent: result,atable,indexedbystateandaction,initiallyempty\nH,atableofcostestimatesindexedbystate,initiallyempty\ns,a,thepreviousstateandaction,initiallynull\nifGOAL-TEST(s(cid:5))thenreturnstop\nifs(cid:5)isanewstate(notinH)thenH[s(cid:5)]\u2190h(s(cid:5))\nifs isnotnull\nresult[s,a]\u2190s(cid:5)\nH[s]\u2190 min LRTA*-COST(s,b,result[s,b],H)\nb\u2208ACTIONS(s)\na\u2190anactionb inACTIONS(s(cid:5))thatminimizesLRTA*-COST(s(cid:5),b,result[s(cid:5),b],H)\ns\u2190s(cid:5)\nreturna\nfunctionLRTA*-COST(s,a,s(cid:5),H)returnsacostestimate\nifs(cid:5)isundefinedthenreturnh(s)\nelsereturnc(s,a,s(cid:5)) + H[s(cid:5)]\nFigure 4.24 LRTA*-AGENT selects an action according to the values of neighboring\nstates,whichareupdatedastheagentmovesaboutthestatespace. Section4.6. Summary 153\n\u2217\nbutoftendoesmuchbetter. TheLRTA agentisjustoneofalargefamilyofonlineagentsthat\none can define by specifying the action selection rule and the update rule in different ways.\nWediscussthisfamily,developed originally forstochastic environments, inChapter21.\n4.5.4 Learning inonlinesearch\nTheinitialignoranceofonlinesearchagentsprovidesseveralopportunitiesforlearning. First,\ntheagents learn a\u201cmap\u201dofthe environment\u2014more precisely, theoutcome ofeach action in\neach state\u2014simply by recording each of their experiences. (Notice that the assumption of\ndeterministic environments means that one experience is enough for each action.) Second,\nthelocalsearchagentsacquiremoreaccurateestimatesofthecostofeachstatebyusinglocal\n\u2217\nupdating rules, as inLRTA. InChapter 21, weshow that these updates eventually converge\nto exact values for every state, provided that the agent explores the state space in the right\nway. Onceexact values are known, optimal decisions can be taken simply by moving to the\nlowest-cost successor\u2014that is,purehillclimbingisthenanoptimalstrategy.\nIf you followed our suggestion to trace the behavior of ONLINE-DFS-AGENT in the\nenvironment of Figure 4.19, you will have noticed that the agent is not very bright. For\nexample, after it has seen that the Up action goes from (1,1) to (1,2), the agent still has no\nidea that the Down action goes back to (1,1) or that the Up action also goes from (2,1) to\n(2,2), from (2,2) to (2,3), and so on. In general, we would like the agent to learn that Up\nincreases they-coordinate unless thereisawallintheway,that Down reduces it,andsoon.\nFor this to happen, we need two things. First, we need a formal and explicitly manipulable\nrepresentation forthese kinds ofgeneral rules; sofar, wehavehidden the information inside\ntheblack boxcalled the RESULT function. PartIII isdevoted tothis issue. Second, weneed\nalgorithms that can construct suitable general rules from the specific observations made by\ntheagent. ThesearecoveredinChapter18.\n4.6 SUMMARY\nThis chapter has examined search algorithms for problems beyond the \u201cclassical\u201d case of\nfindingtheshortest pathtoagoalinanobservable, deterministic, discreteenvironment.\n\u2022 Local search methods such as hill climbing operate on complete-state formulations,\nkeeping only a small number of nodes in memory. Several stochastic algorithms have\nbeendeveloped,includingsimulatedannealing,whichreturnsoptimalsolutionswhen\ngivenanappropriate coolingschedule.\n\u2022 Many local search methods apply also to problems in continuous spaces. Linear pro-\ngramming and convex optimization problems obey certain restrictions on the shape\nof the state space and the nature of the objective function, and admit polynomial-time\nalgorithmsthatareoftenextremelyefficientinpractice.\n\u2022 Ageneticalgorithmisastochastic hill-climbing searchinwhichalargepopulation of\nstates is maintained. New states are generated by mutation and by crossover, which\ncombinespairsofstatesfromthepopulation. 154 Chapter 4. BeyondClassicalSearch\n\u2022 Innondeterministicenvironments, agents canapply AND\u2013OR search togenerate con-\ntingentplansthatreachthegoalregardlessofwhichoutcomesoccur duringexecution.\n\u2022 When the environment is partially observable, the belief state represents the set of\npossiblestatesthattheagentmightbein.\n\u2022 Standardsearchalgorithmscanbeapplieddirectlytobelief-statespacetosolvesensor-\nless problems, and belief-state AND\u2013OR search can solve general partially observable\nproblems. Incremental algorithms thatconstruct solutions state-by-state withinabelief\nstateareoftenmoreefficient.\n\u2022 Explorationproblemsarisewhentheagenthasnoideaaboutthestatesandactionsof\nitsenvironment. Forsafely explorable environments, onlinesearch agents can build a\nmapandfindagoalifoneexists. Updatingheuristicestimatesfromexperienceprovides\naneffectivemethodtoescapefromlocalminima.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nLocal search techniques have a long history in mathematics and computer science. Indeed,\nthe Newton\u2013Raphson method (Newton, 1671; Raphson, 1690) can be seen as a very effi-\ncient local search method for continuous spaces in which gradient information is available.\nBrent (1973) is a classic reference for optimization algorithms that do not require such in-\nformation. Beam search, which we have presented as a local search algorithm, originated\nas a bounded-width variant of dynamic programming for speech recognition in the HARPY\nsystem(Lowerre,1976). Arelatedalgorithm isanalyzed indepthbyPearl(1984,Ch.5).\nThetopic oflocal search wasreinvigorated intheearly 1990s bysurprisingly good re-\nsults for large constraint-satisfaction problems such as n-queens (Minton et al., 1992) and\nlogical reasoning (Selman et al., 1992) and by the incorporation of randomness, multiple\nsimultaneoussearches,andotherimprovements. ThisrenaissanceofwhatChristosPapadim-\nitriou has called \u201cNew Age\u201d algorithms also sparked increased interest among theoretical\ncomputer scientists (Koutsoupias and Papadimitriou, 1992; Aldous and Vazirani, 1994). In\nthefieldofoperationsresearch,avariantofhillclimbingcalledtabusearchhasgainedpopu-\nTABUSEARCH\nlarity(GloverandLaguna,1997). Thisalgorithmmaintains atabulistofkpreviouslyvisited\nstatesthatcannotberevisited;aswellasimprovingefficiencywhensearchinggraphs,thislist\ncanallow the algorithm toescape from somelocal minima. Anotheruseful improvement on\nhill climbing is the STAGE algorithm (Boyan and Moore, 1998). Theidea is touse the local\nmaximafoundbyrandom-restart hillclimbingtogetanideaoftheoverallshapeoftheland-\nscape. Thealgorithm fitsasmoothsurface tothesetoflocalmaximaandthencalculates the\nglobal maximum of that surface analytically. This becomes the new restart point. The algo-\nrithmhasbeenshowntoworkinpracticeonhardproblems. Gomesetal.(1998)showedthat\nHEAVY-TAILED the runtimes ofsystematic backtracking algorithms often have aheavy-tailed distribution,\nDISTRIBUTION\nwhich means that the probability of a very long run time is more than would be predicted if\ntheruntimes wereexponentially distributed. Whentherun timedistribution isheavy-tailed,\nrandomrestarts findasolution faster, onaverage, thanasingleruntocompletion. Bibliographical andHistorical Notes 155\nSimulated annealing was first described by Kirkpatrick et al. (1983), who borrowed\ndirectly from the Metropolis algorithm (which is used to simulate complex systems in\nphysics(Metropolisetal.,1953)andwassupposedlyinventedataLosAlamosdinnerparty).\nSimulatedannealing isnowafieldinitself,withhundreds ofpaperspublished everyyear.\nFinding optimal solutions in continuous spaces is the subject matter of several fields,\nincludingoptimizationtheory,optimalcontroltheory,andthecalculusofvariations. The\nbasictechniques areexplained wellbyBishop(1995);Pressetal.(2007)coverawiderange\nofalgorithms andprovideworkingsoftware.\nAs Andrew Moore points out, researchers have taken inspiration for search and opti-\nmizationalgorithmsfromawidevarietyoffieldsofstudy: metallurgy(simulatedannealing),\nbiology (geneticalgorithms), economics(market-based algorithms), entomology (antcolony\noptimization),neurology(neuralnetworks),animalbehavior(reinforcementlearning),moun-\ntaineering (hillclimbing), andothers.\nLinearprogramming(LP)wasfirststudiedsystematically bytheRussianmathemati-\ncian Leonid Kantorovich (1939). It was one of the first applications of computers; the sim-\nplexalgorithm(Dantzig,1949)isstilluseddespiteworst-caseexponential complexity. Kar-\nmarkar(1984)developedthefarmoreefficientfamilyof interior-pointmethods, whichwas\nshowntohavepolynomialcomplexityforthemoregeneralclassofconvexoptimizationprob-\nlemsbyNesterovandNemirovski(1994). Excellentintroductionstoconvexoptimizationare\nprovided byBen-TalandNemirovski(2001)andBoydandVandenberghe (2004).\nWork by Sewall Wright (1931) on the concept of a fitness landscape was an impor-\ntant precursor to the development of genetic algorithms. In the 1950s, several statisticians,\nincluding Box (1957) and Friedman (1959), used evolutionary techniques for optimization\nEVOLUTION problems,butitwasn\u2019tuntilRechenberg(1965)introduced evolutionstrategies tosolveop-\nSTRATEGY\ntimization problemsforairfoils thattheapproach gainedpopularity. Inthe1960s and1970s,\nJohn Holland (1975) championed genetic algorithms, both as a useful tool and as a method\ntoexpand ourunderstanding ofadaptation, biological orotherwise (Holland, 1995). Thear-\ntificial life movement (Langton, 1995) takes this idea one step further, viewing the products\nARTIFICIALLIFE\nof genetic algorithms as organisms rather than solutions to problems. Work in this field by\nHintonandNowlan(1987) andAckleyandLittman(1991) hasdonemuchtoclarify theim-\nplicationsoftheBaldwineffect. Forgeneralbackground onevolution, werecommendSmith\nandSzathma\u00b4ry(1999),Ridley(2004), andCarroll(2007).\nMostcomparisons ofgenetic algorithms tootherapproaches (especially stochastic hill\nclimbing) have found that the genetic algorithms are slower to converge (O\u2019Reilly and Op-\npacher,1994;Mitchelletal.,1996;JuelsandWattenberg,1996;Baluja,1997). Suchfindings\nare not universally popular within the GA community, but recent attempts within that com-\nmunity to understand population-based search as an approximate form of Bayesian learning\n(see Chapter 20) might help close the gap between the field and its critics (Pelikan et al.,\n1999). The theory of quadratic dynamical systems may also explain the performance of\nGAs(Rabani etal.,1998). SeeLohnetal.(2001) foranexampleofGAsapplied toantenna\ndesign, andRennerandEkart(2003)foranapplication tocomputer-aided design.\nGENETIC Thefieldofgeneticprogrammingiscloselyrelatedtogeneticalgorithms. Theprinci-\nPROGRAMMING\npal difference is that the representations that are mutated and combined are programs rather 156 Chapter 4. BeyondClassicalSearch\nthanbitstrings. Theprogramsarerepresentedintheformof expressiontrees;theexpressions\ncanbeinastandard language suchasLisporcanbespecially designed torepresent circuits,\nrobot controllers, and so on. Crossover involves splicing together subtrees rather than sub-\nstrings. This form of mutation guarantees that the offspring are well-formed expressions,\nwhichwouldnotbethecaseifprogramsweremanipulated asstrings.\nInterestingeneticprogrammingwasspurredbyJohnKoza\u2019swork(Koza,1992,1994),\nbut it goes back at least to early experiments with machine code by Friedberg (1958) and\nwithfinite-state automata byFogeletal. (1966). Aswithgenetic algorithms, there isdebate\nabout the effectiveness of the technique. Kozaet al. (1999) describe experiments in the use\nofgenetic programmingtodesigncircuitdevices.\nThejournals Evolutionary Computation andIEEETransactions onEvolutionary Com-\nputation covergenetic algorithms and genetic programming; articles are alsofound in Com-\nplex Systems, Adaptive Behavior, and Artificial Life. The main conference is the Genetic\nandEvolutionary Computation Conference(GECCO).Goodoverviewtextsongeneticalgo-\nrithmsaregivenbyMitchell (1996), Fogel(2000), andLangdon andPoli(2002), andbythe\nfreeonlinebookbyPolietal.(2008).\nThe unpredictability and partial observability of real environments were recognized\nearly on in robotics projects that used planning techniques, including Shakey (Fikes et al.,\n1972) and FREDDY (Michie, 1974). Theproblems received moreattention after thepublica-\ntionofMcDermott\u2019s(1978a) influentialarticle, PlanningandActing.\nThefirstworktomakeexplicituseofAND\u2013ORtreesseemstohavebeenSlagle\u2019sSAINT\nprogram for symbolic integration, mentioned in Chapter 1. Amarel (1967) applied the idea\nto propositional theorem proving, a topic discussed in Chapter 7, and introduced a search\nalgorithm similar to AND-OR-GRAPH-SEARCH. The algorithm was further developed and\n\u2217\nformalized by Nilsson (1971), who also described AO \u2014which, as its name suggests, finds\n\u2217\noptimalsolutionsgivenanadmissibleheuristic. AO wasanalyzedandimprovedbyMartelli\n\u2217 \u2217\nand Montanari (1973). AO is a top-down algorithm; a bottom-up generalization of A is\n\u2217 \u2217\nALD,forA LightestDerivation (Felzenszwalb andMcAllester, 2007). Interest in AND\u2013OR\nsearch has undergone a revival in recent years, with new algorithms for finding cyclic solu-\ntions(JimenezandTorras,2000;HansenandZilberstein, 2001)andnewtechniques inspired\nbydynamicprogramming (BonetandGeffner,2005).\nTheideaoftransforming partially observable problemsintobelief-state problemsorig-\ninated withAstrom(1965) forthemuchmorecomplexcaseofprobabilistic uncertainty (see\nChapter17). Erdmann andMason (1988) studied theproblem of robotic manipulation with-\noutsensors, usingacontinuous formofbelief-state search. Theyshowedthatitwaspossible\ntoorientapartonatablefromanarbitraryinitialposition byawell-designedsequenceoftilt-\ningactions. Morepracticalmethods,basedonaseriesofpreciselyorienteddiagonalbarriers\nacrossaconveyorbelt,usethesamealgorithmic insights(Wiegleyetal.,1996).\nThe belief-state approach was reinvented in the context of sensorless and partially ob-\nservable searchproblems byGenesereth andNourbakhsh (1993). Additional workwasdone\nonsensorless problems inthe logic-based planning community (Goldman and Boddy, 1996;\nSmithand Weld, 1998). Thiswork has emphasized concise representations forbelief states,\nasexplainedinChapter11. BonetandGeffner(2000)introduced thefirsteffectiveheuristics Exercises 157\nfor belief-state search; these were refined by Bryce et al. (2006). The incremental approach\nto belief-state search, in which solutions are constructed incrementally for subsets of states\nwithineachbeliefstate,wasstudiedintheplanningliteraturebyKurienetal.(2002);several\nnewincrementalalgorithms wereintroduced fornondeterministic, partiallyobservable prob-\nlemsbyRussellandWolfe(2005). Additional references for planning instochastic, partially\nobservable environments appearinChapter17.\nAlgorithmsforexploringunknownstatespaceshavebeenofinterestformanycenturies.\nDepth-firstsearchinamazecanbeimplementedbykeepingone\u2019slefthandonthewall;loops\ncan be avoided by marking each junction. Depth-first search fails with irreversible actions;\nthemoregeneralproblemofexploring Euleriangraphs(i.e.,graphsinwhicheachnodehas\nEULERIANGRAPH\nequalnumbersofincomingandoutgoingedges)wassolvedbyanalgorithmduetoHierholzer\n(1873). Thefirst thorough algorithmic study of the exploration problem forarbitrary graphs\nwas carried out by Deng and Papadimitriou (1990), who developed a completely general\nalgorithm but showed that no bounded competitive ratio is possible for exploring a general\ngraph. PapadimitriouandYannakakis(1991)examinedthequestionoffindingpathstoagoal\ningeometricpath-planningenvironments(whereallactionsarereversible). Theyshowedthat\na small competitive ratio is achievable with square obstacles, but with general rectangular\nobstacles noboundedratiocanbeachieved. (SeeFigure4.20.)\n\u2217\nThe LRTA algorithm was developed by Korf (1990) as part of an investigation into\nreal-time search for environments in which the agent must act after searching for only a\nREAL-TIMESEARCH\n\u2217\nfixed amount of time (a common situation in two-player games). LRTA is in fact a special\ncaseofreinforcementlearningalgorithmsforstochasticenvironments(Bartoetal.,1995). Its\npolicyofoptimismunderuncertainty\u2014alwaysheadfortheclosestunvisitedstate\u2014canresult\nin an exploration pattern that is less efficient in the uninformed case than simple depth-first\nsearch (Koenig, 2000). Dasguptaetal.(1994) show thatonline iterative deepening search is\noptimally efficient for finding a goal in a uniform tree with no heuristic information. Sev-\n\u2217\neral informed variants on the LRTA theme have been developed with different methods for\nsearching and updating within the known portion of the graph (Pemberton and Korf, 1992).\nAs yet, there is no good understanding of how to find goals with optimal efficiency when\nusingheuristic information.\nEXERCISES\n4.1 Givethenameofthealgorithm thatresults fromeachofthefollowingspecialcases:\na. Localbeamsearchwithk = 1.\nb. Localbeamsearchwithoneinitialstateandnolimitonthenumberofstatesretained.\nc. Simulatedannealing withT = 0atalltimes(andomittingthetermination test).\nd. Simulatedannealing withT = \u221eatalltimes.\ne. Geneticalgorithm withpopulation sizeN = 1. 158 Chapter 4. BeyondClassicalSearch\n4.2 Exercise 3.16 considers the problem of building railway tracks under the assumption\nthat pieces fit exactly with no slack. Now consider the real problem, in which pieces don\u2019t\nfitexactly butallow forupto10degrees ofrotation toeither sideofthe\u201cproper\u201d alignment.\nExplainhowtoformulatetheproblem soitcouldbesolvedbysimulatedannealing.\n4.3 In this exercise, we explore the use of local search methods to solve TSPs of the type\ndefinedinExercise3.30.\na. Implementandtestahill-climbingmethodtosolveTSPs. Comparetheresultswithop-\n\u2217\ntimalsolutions obtained fromtheA algorithm withtheMSTheuristic(Exercise3.30).\nb. Repeat part (a) using a genetic algorithm instead of hill climbing. You may want to\nconsultLarran\u02dcaga etal.(1999)forsomesuggestions forrepresentations.\n4.4 Generatealargenumberof8-puzzleand8-queensinstancesandsolvethem(wherepos-\nsible) by hill climbing (steepest-ascent and first-choice variants), hill climbing with random\nrestart, andsimulated annealing. Measurethesearchcostandpercentage ofsolvedproblems\nandgraphtheseagainsttheoptimalsolution cost. Commentonyourresults.\n4.5 The AND-OR-GRAPH-SEARCH algorithm in Figure 4.11 checks for repeated states\nonly on the path from the root to the current state. Suppose that, in addition, the algorithm\nweretostore every visited state and check against that list. (See BREADTH-FIRST-SEARCH\ninFigure3.11foranexample.) Determinetheinformation thatshouldbestoredandhowthe\nalgorithmshouldusethatinformationwhenarepeatedstate isfound. (Hint: Youwillneedto\ndistinguish atleastbetweenstatesforwhichasuccessfulsubplanwasconstructed previously\nand states for which no subplan could be found.) Explain how to use labels, as defined in\nSection4.3.3,toavoidhaving multiplecopiesofsubplans.\n4.6 Explainpreciselyhowtomodifythe AND-OR-GRAPH-SEARCH algorithm togenerate\nacyclicplanifnoacyclicplanexists. Youwillneedtodealwiththreeissues: labelingtheplan\nstepssothatacyclicplancanpointbacktoanearlierpartoftheplan,modifyingOR-SEARCH\nso that it continues to look for acyclic plans after finding a cyclic plan, and augmenting the\nplan representation toindicate whether aplan iscyclic. Showhow your algorithm works on\n(a)theslipperyvacuumworld,and(b)theslippery,erratic vacuumworld. Youmightwishto\nuseacomputerimplementation tocheckyourresults.\n4.7 In Section 4.4.1 we introduced belief states to solve sensorless search problems. A\nsequence of actions solves a sensorless problem if it maps every physical state in the initial\n\u2217\nbeliefstatebtoagoalstate. Supposetheagentknowsh (s),thetrueoptimalcostofsolving\nthephysicalstatesinthefullyobservable problem,foreverystate sinb. Findanadmissible\nheuristic h(b) for the sensorless problem in terms of these costs, and prove its admissibilty.\nCommentontheaccuracyofthisheuristiconthesensorless vacuumproblemofFigure4.14.\n\u2217\nHowwelldoesA perform?\n4.8 This exercise explores subset\u2013superset relations between belief states in sensorless or\npartially observable environments.\na. Provethatifanactionsequence isasolution forabeliefstateb,itisalsoasolution for\nanysubsetofb. Cananythingbesaidaboutsupersets ofb? Exercises 159\nb. Explainindetailhowtomodifygraphsearchforsensorless problemstotakeadvantage\nofyouranswersin(a).\nc. Explain in detail how to modify AND\u2013OR search for partially observable problems,\nbeyondthemodifications youdescribe in(b).\n4.9 On page 139 it was assumed that a given action would have the same cost when ex-\necuted in any physical state within a given belief state. (This leads to a belief-state search\nproblem with well-defined step costs.) Now consider what happens when the assumption\ndoesnothold. Doesthenotionofoptimalitystillmakesenseinthiscontext,ordoesitrequire\nmodification? Consideralso various possible definitions of the\u201ccost\u201d ofexecuting anaction\nin a belief state; for example, we could use the minimum of the physical costs; or the maxi-\nmum; or a cost interval with the lower bound being the minimum cost and the upper bound\nbeingthemaximum;orjustkeepthesetofallpossiblecostsforthataction. Foreachofthese,\n\u2217\nexplorewhetherA (withmodifications ifnecessary) canreturnoptimalsolutions.\n4.10 Consider the sensorless version of the erratic vacuum world. Draw the belief-state\nspacereachablefromtheinitialbeliefstate{1,2,3,4,5,6,7,8},andexplainwhytheproblem\nisunsolvable.\n4.11 Wecanturnthenavigation problem inExercise3.7intoanenvironment asfollows:\n\u2022 The percept will be a list of the positions, relative to the agent, of the visible vertices.\nTheperceptdoesnotincludethepositionoftherobot! Therobotmustlearnitsownpo-\nsitionfromthemap;fornow,youcanassumethateachlocation hasadifferent \u201cview.\u201d\n\u2022 Each action will be a vector describing a straight-line path to follow. If the path is\nunobstructed, the action succeeds; otherwise, the robot stops at the point where its\npath first intersects an obstacle. If the agent returns a zero motion vector and is at the\ngoal(which isfixedand known), then theenvironment teleports theagent toarandom\nlocation(notinsideanobstacle).\n\u2022 Theperformance measure charges the agent 1point foreach unit ofdistance traversed\nandawards1000pointseachtimethegoalisreached.\na. Implement this environment and a problem-solving agent for it. After each teleporta-\ntion,theagentwillneedtoformulateanewproblem,whichwillinvolvediscoveringits\ncurrentlocation.\nb. Documentyouragent\u2019sperformance(byhavingtheagentgeneratesuitablecommentary\nasitmovesaround)andreportitsperformance over100episodes.\nc. Modify the environment so that 30% of the time the agent ends up at an unintended\ndestination(chosenrandomlyfromtheothervisibleverticesifany;otherwise,nomove\nat all). This is a crude model of the motion errors of a real robot. Modify the agent\nso that when such an error is detected, it finds out where it is and then constructs a\nplan to get back to where it was and resume the old plan. Remember that sometimes\ngettingbacktowhereitwasmightalsofail! Showanexampleoftheagentsuccessfully\novercomingtwosuccessive motionerrorsandstillreaching thegoal. 160 Chapter 4. BeyondClassicalSearch\nd. Nowtrytwodifferentrecoveryschemesafteranerror: (1)headfortheclosestvertexon\ntheoriginalroute;and(2)replanaroutetothegoalfromthe newlocation. Comparethe\nperformance ofthe three recovery schemes. Would theinclusion ofsearch costs affect\nthecomparison?\ne. Now suppose that there are locations from which the view is identical. (For example,\nsupposetheworldisagridwithsquareobstacles.) Whatkindofproblemdoestheagent\nnowface? Whatdosolutions looklike?\n4.12 Suppose that an agent is in a 3\u00d73 maze environment like the one shown in Fig-\nure4.19. Theagentknowsthatitsinitiallocationis(1,1),thatthegoalisat(3,3),andthatthe\nactions Up, Down, Left,Right have theirusual effects unless blocked byawall. Theagent\ndoes notknow wheretheinternal wallsare. Inanygiven state, theagent perceives thesetof\nlegalactions;itcanalsotellwhetherthestateisoneithasvisitedbefore.\na. Explainhowthisonlinesearchproblemcanbeviewedasanofflinesearchinbelief-state\nspace, where the initial belief state includes all possible environment configurations.\nHowlargeistheinitialbeliefstate? Howlargeisthespaceofbeliefstates?\nb. Howmanydistinct perceptsarepossible intheinitialstate?\nc. Describe the first few branches of a contingency plan for this problem. How large\n(roughly) isthecompleteplan?\nNoticethatthiscontingencyplanisasolutionforeverypossibleenvironmentfittingthegiven\ndescription. Therefore, interleaving of search and execution is not strictly necessary even in\nunknownenvironments.\n4.13 Inthisexercise,weexaminehillclimbinginthecontextofrobotnavigation, usingthe\nenvironment inFigure3.31asanexample.\na. Repeat Exercise 4.11 using hill climbing. Does your agent ever get stuck in a local\nminimum? Isitpossible forittogetstuckwithconvexobstacles?\nb. Constructanonconvex polygonal environment inwhichtheagentgetsstuck.\nc. Modifythehill-climbing algorithm sothat, instead ofdoing adepth-1 search todecide\nwhere to go next, it does a depth-k search. It should find the best k-step path and do\nonestepalongit,andthenrepeattheprocess.\nd. Istheresomekforwhichthenewalgorithmisguaranteedtoescapefromlocalminima?\n\u2217\ne. ExplainhowLRTA enablestheagenttoescapefromlocalminimainthiscase.\n4.14 LikeDFS,onlineDFSisincompleteforreversiblestatespaceswithinfinitepaths. For\nexample, suppose that states are points on the infinite two-dimensional grid and actions are\nunitvectors (1,0), (0,1), (\u22121,0), (0,\u22121), triedinthatorder. ShowthatonlineDFSstarting\nat (0,0) will not reach (1,\u22121). Suppose the agent can observe, in addition to its current\nstate, all successor states and the actions that would lead to them. Write an algorithm that\nis complete even for bidirected state spaces with infinite paths. What states does it visit in\nreaching (1,\u22121)? 5\nADVERSARIAL SEARCH\nInwhichweexaminetheproblemsthatarisewhenwetrytoplanaheadinaworld\nwhereotheragentsareplanning againstus.\n5.1 GAMES\nChapter 2 introduced multiagent environments, in which each agent needs to consider the\nactions of other agents and how they affect its own welfare. The unpredictability of these\nother agents can introduce contingencies into the agent\u2019s problem-solving process, as dis-\ncussedinChapter4. Inthischapterwecovercompetitiveenvironments,inwhichtheagents\u2019\ngoalsareinconflict,givingrisetoadversarial search problems\u2014often knownasgames.\nGAME\nMathematicalgametheory,abranchofeconomics,viewsanymultiagentenvironment\nas a game, provided that the impact of each agent on the others is \u201csignificant,\u201d regardless\nof whether the agents are cooperative or competitive.1 In AI, the most common games are\nofaratherspecialized kind\u2014whatgametheorists calldeterministic, turn-taking, two-player,\nzero-sum games of perfect information (such as chess). In our terminology, this means\nZERO-SUMGAMES\nPERFECT deterministic,fullyobservableenvironmentsinwhichtwoagentsactalternatelyandinwhich\nINFORMATION\nthe utility values at the end of the game are always equal and opposite. Forexample, if one\nplayerwinsagameofchess, theotherplayernecessarily loses. Itisthisopposition between\ntheagents\u2019utilityfunctions thatmakesthesituation adversarial.\nGames have engaged the intellectual faculties of humans\u2014sometimes to an alarming\ndegree\u2014for as long as civilization has existed. For AI researchers, the abstract nature of\ngames makes them an appealing subject for study. The state of a game is easy to represent,\nandagentsareusuallyrestrictedtoasmallnumberofactionswhoseoutcomesaredefinedby\nprecise rules. Physicalgames,such ascroquet andicehockey, havemuchmorecomplicated\ndescriptions, a much larger range of possible actions, and rather imprecise rules defining\nthe legality of actions. With the exception of robot soccer, these physical games have not\nattracted muchinterest intheAIcommunity.\n1 Environmentswithverymanyagentsareoftenviewedaseconomiesratherthangames.\n161 162 Chapter 5. Adversarial Search\nGames, unlike most of the toy problems studied in Chapter 3, are interesting because\nthey are too hard to solve. For example, chess has an average branching factor of about 35,\nand games often go to 50 moves by each player, so the search tree has about 35100 or 10154\nnodes(although thesearchgraphhas\u201conly\u201d about 1040 distinct nodes). Games,likethereal\nworld,thereforerequiretheabilitytomake somedecision evenwhencalculating theoptimal\ndecisionisinfeasible. Gamesalsopenalizeinefficiencyseverely. Whereasanimplementation\n\u2217\nofA searchthatishalfasefficientwillsimplytaketwiceaslongtoruntocompletion,achess\nprogram that is half as efficient in using its available time probably will be beaten into the\nground, otherthingsbeingequal. Game-playingresearchhasthereforespawnedanumberof\ninteresting ideasonhowtomakethebestpossible useoftime.\nWe begin with a definition of the optimal move and an algorithm for finding it. We\nthen look at techniques for choosing a good move when time is limited. Pruningallows us\nPRUNING\ntoignore portions ofthesearch treethatmakenodifference tothefinalchoice, andheuristic\nevaluationfunctionsallowustoapproximate thetrueutilityofastatewithoutdoingacom-\nplete search. Section 5.5 discusses games such as backgammon that include an element of\nIMPERFECT chance; wealso discuss bridge, which includes elements of imperfect information because\nINFORMATION\nnotallcardsarevisible toeachplayer. Finally,welookathowstate-of-the-art game-playing\nprogramsfareagainsthumanopposition andatdirections forfuturedevelopments.\nWefirstconsidergameswithtwoplayers,whomwecall MAXandMINforreasonsthat\nwillsoonbecomeobvious. MAX movesfirst,andthentheytaketurnsmovinguntilthegame\nis over. At the end of the game, points are awarded to the winning player and penalties are\ngiven to the loser. A game can be formally defined as a kind of search problem with the\nfollowingelements:\n\u2022 S : Theinitialstate, whichspecifieshowthegameissetupatthestart.\n0\n\u2022 PLAYER(s): Defineswhichplayerhasthemoveinastate.\n\u2022 ACTIONS(s): Returnsthesetoflegalmovesinastate.\n\u2022 RESULT(s,a): Thetransitionmodel,whichdefinestheresultofamove.\nTERMINALTEST\n\u2022 TERMINAL-TEST(s): A terminal test, which is true when the game is over and false\notherwise. Stateswherethegamehasendedarecalled terminalstates.\nTERMINALSTATES\n\u2022 UTILITY(s,p): Autilityfunction(alsocalledanobjectivefunctionorpayofffunction),\ndefinesthefinalnumericvalueforagamethatendsinterminal statesforaplayerp. In\nchess,theoutcomeisawin,loss,ordraw,withvalues +1,0,or 1. Somegameshavea\n2\nwidervarietyofpossibleoutcomes;thepayoffsinbackgammonrangefrom0to+192.\nAzero-sum game is (confusingly) defined as one where the total payoff toall players\nisthe sameforevery instance ofthegame. Chessiszero-sum because everygamehas\npayoffofeither0+1,1+0or 1 + 1. \u201cConstant-sum\u201d wouldhavebeenabetterterm,\n2 2\nbut zero-sum is traditional and makes sense if you imagine each player is charged an\nentryfeeof 1.\n2\nGAMETREE\nThe initial state, ACTIONS function, and RESULT function define the game tree for the\ngame\u2014a tree where the nodes are game states and the edges are moves. Figure 5.1 shows\npart of the game tree for tic-tac-toe (noughts and crosses). From the initial state, MAX has\nnine possible moves. Play alternates between MAX\u2019s placing an X and MIN\u2019s placing an O Section5.2. OptimalDecisions inGames 163\nuntil we reach leaf nodes corresponding to terminal states such that one player has three in\na row or all the squares are filled. The number on each leaf node indicates the utility value\nofthe terminal state from the point ofview of MAX; high values are assumed tobe good for\nMAX andbadfor MIN(whichishowtheplayersgettheirnames).\nFor tic-tac-toe the game tree is relatively small\u2014fewer than 9! = 362,880 terminal\nnodes. But for chess there are over 1040 nodes, so the game tree is best thought of as a\ntheoretical construct that we cannot realize in the physical world. But regardless of the size\nSEARCHTREE\nofthegametree,itisMAX\u2019sjobtosearchforagoodmove. Weusethetermsearchtreefora\ntreethatissuperimposed onthefullgametree,andexamines enoughnodestoallowaplayer\ntodetermine whatmovetomake.\nMAX (X)\nX X X\nMIN (O)\nX X X\nX X X\nXO X O X . . .\nMAX (X) O\nX O X XO X O . . .\nMIN (O) X X\n. . . . . . . . . . . .\nX O X X O X X O X . . .\nTERMINAL O X OO X X\nO X X O X O O\nUtility \u20131 0 +1\nFigure 5.1 A (partial) game tree forthe game of tic-tac-toe. The top node is the initial\nstate,andMAXmovesfirst,placinganXinanemptysquare.Weshowpartofthetree,giving\nalternatingmovesbyMIN(O)andMAX(X),untilweeventuallyreachterminalstates,which\ncanbeassignedutilitiesaccordingtotherulesofthegame.\n5.2 OPTIMAL DECISIONS IN GAMES\nIn a normal search problem, the optimal solution would be a sequence of actions leading to\na goal state\u2014a terminal state that is a win. In adversarial search, MIN has something to say\nSTRATEGY about it. MAX therefore must find a contingent strategy, which specifies MAX\u2019s move in\nthe initial state, then MAX\u2019s moves in the states resulting from every possible response by 164 Chapter 5. Adversarial Search\nMAX 3 A\na a\n1 3\na\n2\nMIN 3 B 2 C 2 D\nb b c c d d\n1 3 1 3 1 3\nb c d\n2 2 2\n3 12 8 2 4 6 14 5 2\nFigure 5.2 A two-ply game tree. The (cid:14) nodes are \u201cMAX nodes,\u201d in which it is MAX\u2019s\n(cid:15)\nturntomove,andthe nodesare\u201cMINnodes.\u201d Theterminalnodesshowtheutilityvalues\nforMAX;theothernodesarelabeledwiththeirminimaxvalues. MAX\u2019sbestmoveattheroot\nisa 1,becauseitleadstothestatewiththehighestminimaxvalue,andMIN\u2019sbestreplyisb 1,\nbecauseitleadstothestatewiththelowestminimaxvalue.\nMIN,then MAX\u2019smovesinthestates resulting fromeverypossible response by MIN tothose\nmoves, and so on. This is exactly analogous to the AND\u2013OR search algorithm (Figure 4.11)\nwith MAX playing theroleof OR and MIN equivalent to AND. Roughly speaking, anoptimal\nstrategy leads to outcomes at least as good as any other strategy when one is playing an\ninfallible opponent. Webeginbyshowinghowtofindthisoptimalstrategy.\nEven a simple game like tic-tac-toe is too complex for us to draw the entire game tree\nononepage,sowewillswitchtothetrivialgameinFigure5.2. ThepossiblemovesforMAX\nat the root node are labeled a 1, a 2, and a 3. The possible replies to a 1 for MIN are b 1, b 2,\nb 3, and so on. This particular game ends after one move each by MAX and MIN. (In game\nparlance,wesaythatthistreeisonemovedeep,consistingoftwohalf-moves,eachofwhich\niscalledaply.) Theutilitiesoftheterminalstatesinthisgamerangefrom2to14.\nPLY\nGiven a game tree, the optimal strategy can be determined from the minimax value\nMINIMAXVALUE\nof each node, which we write as MINIMAX(n). The minimax value of a node is the utility\n(for MAX) of being in the corresponding state, assuming that both players play optimally\nfrom there to the end of the game. Obviously, the minimax value of a terminal state is just\nits utility. Furthermore, given a choice, MAX prefers to move to a state of maximum value,\nwhereas MIN prefersastateofminimumvalue. Sowehavethefollowing:\nMINIMAX(s) =\n\u23a7\n\u23a8 UTILITY(s) if TERMINAL-TEST(s)\n\u23a9\nmax a\u2208Actions(s)MINIMAX(RESULT(s,a)) if PLAYER(s) =MAX\nmin a\u2208Actions(s)MINIMAX(RESULT(s,a)) if PLAYER(s) =MIN\nLetusapplythesedefinitionstothegametreeinFigure5.2. Theterminalnodesonthebottom\nlevel get their utility values from the game\u2019s UTILITY function. The first MIN node, labeled\nB, has three successor states with values 3, 12, and 8, so its minimax value is 3. Similarly,\nthe othertwo MIN nodes have minimaxvalue 2. Theroot node isa MAX node; its successor\nstates have minimax values 3, 2, and2; soithas aminimax value of3. Wecanalso identify Section5.2. OptimalDecisions inGames 165\nMINIMAXDECISION\ntheminimaxdecisionattheroot: actiona 1istheoptimalchoiceforMAXbecauseitleadsto\nthestatewiththehighest minimaxvalue.\nThis definition of optimal play for MAX assumes that MIN also plays optimally\u2014it\nmaximizestheworst-caseoutcomeforMAX. WhatifMINdoesnotplayoptimally? Thenitis\neasytoshow(Exercise5.7)thatMAXwilldoevenbetter. Otherstrategiesagainstsuboptimal\nopponents maydobetterthantheminimaxstrategy,butthese strategiesnecessarily doworse\nagainstoptimalopponents.\n5.2.1 The minimaxalgorithm\nTheminimaxalgorithm(Figure5.3)computestheminimaxdecisionfromthecurrent state.\nMINIMAXALGORITHM\nItusesasimplerecursivecomputationoftheminimaxvaluesofeachsuccessorstate,directly\nimplementing thedefining equations. Therecursion proceeds allthewaydowntotheleaves\nof the tree, and then the minimax values are backed up through the tree as the recursion\nunwinds. For example, in Figure 5.2, the algorithm first recurses down to the three bottom-\nleftnodes andusesthe UTILITY function onthemtodiscoverthattheirvaluesare3,12,and\n8, respectively. Then it takes the minimum of these values, 3, and returns it as the backed-\nup value of node B. A similar process gives the backed-up values of 2 for C and 2 for D.\nFinally,wetakethemaximumof3,2,and2togetthebacked-upvalueof3fortherootnode.\nThe minimax algorithm performs a complete depth-first exploration of the game tree.\nIf the maximum depth of the tree is m and there are b legal moves at each point, then the\ntimecomplexityoftheminimaxalgorithm isO(bm). ThespacecomplexityisO(bm)foran\nalgorithm that generates allactions atonce, or O(m)foran algorithm that generates actions\none at a time (see page 87). For real games, of course, the time cost is totally impractical,\nbut this algorithm serves as the basis for the mathematical analysis of games and for more\npractical algorithms.\n5.2.2 Optimaldecisions inmultiplayergames\nManypopulargamesallowmorethantwoplayers. Letusexaminehowtoextendtheminimax\nidea to multiplayer games. This is straightforward from the technical viewpoint, but raises\nsomeinteresting newconceptual issues.\nFirst, we need to replace the single value for each node with a vector of values. For\nexample,inathree-playergamewithplayersA,B,andC,avector(cid:16)v ,v ,v (cid:17)isassociated\nA B C\nwitheachnode. Forterminalstates,thisvectorgivestheutilityofthestatefromeachplayer\u2019s\nviewpoint. (Intwo-player,zero-sumgames,thetwo-elementvectorcanbereducedtoasingle\nvaluebecausethevaluesarealwaysopposite.) Thesimplestwaytoimplementthisistohave\nthe UTILITY function returnavectorofutilities.\nNowwehavetoconsidernonterminalstates. ConsiderthenodemarkedX inthegame\ntree shown in Figure 5.4. In that state, player C chooses what to do. The two choices lead\ntoterminal states with utility vectors (cid:16)v =1,v =2,v =6(cid:17)and (cid:16)v =4,v =2,v =3(cid:17).\nA B C A B C\nSince6isbiggerthan3,Cshouldchoosethefirstmove. ThismeansthatifstateX isreached,\nsubsequent play will lead to a terminal state with utilities (cid:16)v =1,v =2,v =6(cid:17). Hence,\nA B C\nthebacked-upvalueofX isthisvector. Thebacked-upvalueofanodenisalwaystheutility 166 Chapter 5. Adversarial Search\nfunctionMINIMAX-DECISION(state)returnsan action\nreturnargmax\na \u2208 ACTIONS(s)\nMIN-VALUE(RESULT(state,a))\nfunctionMAX-VALUE(state)returnsa utility value\nifTERMINAL-TEST(state)thenreturnUTILITY(state)\nv\u2190\u2212\u221e\nforeacha inACTIONS(state)do\nv\u2190MAX(v,MIN-VALUE(RESULT(s,a)))\nreturnv\nfunctionMIN-VALUE(state)returnsa utility value\nifTERMINAL-TEST(state)thenreturnUTILITY(state)\nv\u2190\u221e\nforeacha inACTIONS(state)do\nv\u2190MIN(v,MAX-VALUE(RESULT(s,a)))\nreturnv\nFigure 5.3 An algorithm forcalculating minimax decisions. It returns the action corre-\nsponding to the best possible move, that is, the move that leads to the outcome with the\nbestutility,undertheassumptionthattheopponentplaystominimizeutility. Thefunctions\nMAX-VALUE and MIN-VALUE go throughthe whole game tree, all the way to the leaves,\nto determine the backed-up value of a state. The notation argmax f(a) computes the\na\u2208S\nelementaofsetS thathasthemaximumvalueoff(a).\nto move\nA (1, 2, 6)\nB (1, 2, 6) (1, 5, 2)\nC (1, 2, 6) X (6, 1, 2) (1, 5, 2) (5, 4, 5)\nA\n(1, 2, 6) (4, 2, 3) (6, 1, 2) (7, 4,1) (5,1,1) (1, 5, 2) (7, 7,1) (5, 4, 5)\nFigure5.4 Thefirstthreepliesofagametreewiththreeplayers(A,B,C). Eachnodeis\nlabeledwithvaluesfromtheviewpointofeachplayer. Thebestmoveismarkedattheroot.\nvector of the successor state with the highest value for the player choosing at n. Anyone\nwho plays multiplayer games, such as Diplomacy, quickly becomes aware that much more\nisgoing onthan intwo-player games. Multiplayer gamesusually involve alliances, whether\nALLIANCE\nformalorinformal,amongtheplayers. Alliancesaremadeandbrokenasthegameproceeds.\nHow are we to understand such behavior? Are alliances a natural consequence of optimal\nstrategies foreach player in a multiplayer game? It turns out that they can be. Forexample, Section5.3. Alpha\u2013BetaPruning 167\nsuppose A and B are in weak positions and C is in a stronger position. Then it is often\noptimal for both A and B to attack C rather than each other, lest C destroy each of them\nindividually. In this way, collaboration emerges from purely selfish behavior. Of course,\nas soon as C weakens under the joint onslaught, the alliance loses its value, and either A\nor B could violate the agreement. In some cases, explicit alliances merely make concrete\nwhat would have happened anyway. In other cases, a social stigma attaches to breaking an\nalliance,soplayersmustbalancetheimmediateadvantageofbreakinganallianceagainstthe\nlong-term disadvantage of being perceived as untrustworthy. See Section 17.5 for more on\nthesecomplications.\nIf the game is not zero-sum, then collaboration can also occur with just two players.\nSuppose,forexample,thatthereisaterminalstatewithutilities(cid:16)v =1000,v =1000(cid:17)and\nA B\nthat1000 isthehighest possible utility foreachplayer. Thentheoptimalstrategy isforboth\nplayers to do everything possible to reach this state\u2014that is, the players will automatically\ncooperate toachieveamutuallydesirable goal.\n5.3 ALPHA\u2013BETA PRUNING\nThe problem with minimax search is that the number of game states it has to examine is\nexponential in the depth of the tree. Unfortunately, we can\u2019t eliminate the exponent, but it\nturnsoutwecaneffectivelycutitinhalf. Thetrickisthatitispossibletocomputethecorrect\nminimaxdecisionwithoutlookingateverynodeinthegametree. Thatis,wecanborrowthe\nidea of pruningfrom Chapter 3 to eliminate large parts of the tree from consideration. The\nALPHA\u2013BETA particular technique weexamine iscalled alpha\u2013beta pruning. Whenapplied toastandard\nPRUNING\nminimax tree, it returns the same move as minimax would, but prunes away branches that\ncannotpossibly influencethefinaldecision.\nConsideragainthetwo-plygametreefromFigure5.2. Let\u2019sgothroughthecalculation\nof the optimal decision once more, this time paying careful attention to what we know at\neachpoint intheprocess. Thestepsareexplained inFigure5.5. Theoutcome isthatwecan\nidentify theminimaxdecision withouteverevaluating twooftheleafnodes.\nAnotherwaytolookatthisisasasimplification oftheformula forMINIMAX. Letthe\ntwo unevaluated successors of node C in Figure 5.5 have values x and y. Then the value of\ntherootnodeisgivenby\nMINIMAX(root) = max(min(3,12,8),min(2,x,y),min(14,5,2))\n= max(3,min(2,x,y),2)\n= max(3,z,2) wherez = min(2,x,y) \u2264 2\n= 3.\nIn otherwords, thevalue ofthe root and hence the minimax decision are independent of the\nvaluesoftheprunedleaves xandy.\nAlpha\u2013beta pruning can be applied to trees of any depth, and it is often possible to\nprune entire subtrees ratherthanjust leaves. Thegeneral principle isthis: consider anode n 168 Chapter 5. Adversarial Search\n(a) [\u2212\u221e, +\u221e] A (b) [\u2212\u221e, +\u221e] A\n[\u2212\u221e, 3] B [\u2212\u221e, 3] B\n3 3 12\n(c)\n[3, +\u221e]\nA (d)\n[3, +\u221e]\nA\n[3, 3] B [3, 3] B [\u2212\u221e, 2] C\n3 12 8 3 12 8 2\n(e) [3, 14] A (f) [3, 3] A\n[3, 3] B [\u2212\u221e, 2] C [\u2212\u221e, 14] D [3, 3] B [\u2212\u221e, 2] C [2, 2] D\n3 12 8 2 14 3 12 8 2 14 5 2\nFigure5.5 StagesinthecalculationoftheoptimaldecisionforthegametreeinFigure5.2.\nAteachpoint,weshowtherangeofpossiblevaluesforeachnode.(a)ThefirstleafbelowB\nhasthevalue3. Hence,B,whichisaMINnode,hasavalueofatmost3.(b)Thesecondleaf\nbelowB hasavalueof12; MIN wouldavoidthismove,sothevalueofB isstillatmost3.\n(c) The third leaf below B has a value of 8; we have seen all B\u2019s successor states, so the\nvalue of B is exactly 3. Now, we can inferthat the value of the rootis at least 3, because\nMAX has a choice worth 3 at the root. (d) The first leaf below C has the value 2. Hence,\nC, whichisa MIN node,hasavalueofatmost2. ButweknowthatB isworth3,so MAX\nwouldneverchooseC. Therefore,thereisnopointinlookingattheothersuccessorstates\nofC. Thisisanexampleofalpha\u2013betapruning. (e)ThefirstleafbelowDhasthevalue14,\nsoDisworthatmost14. ThisisstillhigherthanMAX\u2019sbestalternative(i.e.,3),soweneed\nto keep exploringD\u2019s successorstates. Notice also that we nowhave boundson all of the\nsuccessorsoftheroot,sotheroot\u2019svalueisalsoatmost14. (f)ThesecondsuccessorofD\nisworth5,soagainweneedtokeepexploring. Thethirdsuccessorisworth2,sonowDis\nworthexactly2. MAX\u2019sdecisionattherootistomovetoB,givingavalueof3.\nsomewhereinthetree(seeFigure5.6),suchthatPlayerhasa choice ofmovingtothatnode.\nIfPlayerhasabetterchoicemeitherattheparentnodeofnoratanychoicepointfurtherup,\nthennwillneverbereached inactualplay. Sooncewehavefoundoutenough aboutn(by\nexaminingsomeofitsdescendants) toreachthisconclusion, wecanpruneit.\nRemember that minimax search is depth-first, so at any one time we just have to con-\nsider the nodes along a single path in the tree. Alpha\u2013beta pruning gets its name from the\nfollowingtwoparametersthatdescribeboundsonthebacked-upvaluesthatappearanywhere\nalongthepath: Section5.3. Alpha\u2013BetaPruning 169\nPlayer\nOpponent m\n\u2022\n\u2022\n\u2022\nPlayer\nOpponent n\nFigure5.6 Thegeneralcaseforalpha\u2013betapruning. If misbetterthannforPlayer,we\nwillnevergettoninplay.\n\u03b1= thevalueofthebest(i.e.,highest-value)choicewehavefoundsofaratanychoicepoint\nalongthepathfor MAX.\n\u03b2 = thevalueofthebest(i.e.,lowest-value)choicewehavefoundsofaratanychoicepoint\nalongthepathfor MIN.\nAlpha\u2013beta search updates the values of \u03b1 and \u03b2 as it goes along and prunes the remaining\nbranches at a node (i.e., terminates the recursive call) as soon as the value of the current\nnode is known to be worse than the current \u03b1 or \u03b2 value for MAX or MIN, respectively. The\ncomplete algorithm is given in Figure 5.7. We encourage you to trace its behavior when\nappliedtothetreeinFigure5.5.\n5.3.1 Moveordering\nTheeffectiveness ofalpha\u2013beta pruning ishighly dependent ontheorderinwhich thestates\nareexamined. Forexample,inFigure5.5(e)and(f),wecould notpruneanysuccessors ofD\nat all because the worst successors (from the point of view of MIN) were generated first. If\nthethirdsuccessorofDhadbeengeneratedfirst,wewouldhavebeenabletoprunetheother\ntwo. Thissuggests that it might beworthwhile to try toexamine firstthe successors that are\nlikelytobebest.\nIf this can be done,2 then it turns out that alpha\u2013beta needs to examine only O(bm\/2)\nnodes to pick the best mov\u221ae, instead of O(bm) for minimax. This means that the effective\nbranching factor becomes b instead of b\u2014for chess, about 6 instead of 35. Put another\nway, alpha\u2013beta can solve a tree roughly twice as deep as minimax in the same amount of\ntime. If successors are examined in random order rather than best-first, the total number of\nnodesexaminedwillberoughlyO(b3m\/4)formoderateb. Forchess,afairlysimpleordering\nfunction (such as trying captures first, then threats, then forward moves, and then backward\nmoves)getsyoutowithinaboutafactorof2ofthebest-case O(bm\/2)result.\n2 Obviously,itcannotbedoneperfectly;otherwise,theorderingfunctioncouldbeusedtoplayaperfectgame! 170 Chapter 5. Adversarial Search\nfunctionALPHA-BETA-SEARCH(state)returnsanaction\nv\u2190MAX-VALUE(state,\u2212\u221e,+\u221e)\nreturntheaction inACTIONS(state)withvaluev\nfunctionMAX-VALUE(state,\u03b1,\u03b2)returnsa utility value\nifTERMINAL-TEST(state)thenreturnUTILITY(state)\nv\u2190\u2212\u221e\nforeacha inACTIONS(state)do\nv\u2190MAX(v,MIN-VALUE(RESULT(s,a),\u03b1,\u03b2))\nifv \u2265 \u03b2 thenreturnv\n\u03b1\u2190MAX(\u03b1,v)\nreturnv\nfunctionMIN-VALUE(state,\u03b1,\u03b2)returnsa utility value\nifTERMINAL-TEST(state)thenreturnUTILITY(state)\nv\u2190+\u221e\nforeacha inACTIONS(state)do\nv\u2190MIN(v,MAX-VALUE(RESULT(s,a),\u03b1,\u03b2))\nifv \u2264 \u03b1thenreturnv\n\u03b2\u2190MIN(\u03b2,v)\nreturnv\nFigure 5.7 The alpha\u2013betasearch algorithm. Notice that these routinesare the same as\nthe MINIMAX functionsinFigure5.3,exceptforthetwolinesineachof MIN-VALUE and\nMAX-VALUEthatmaintain\u03b1and\u03b2(andthebookkeepingtopasstheseparametersalong).\nAddingdynamicmove-orderingschemes,suchastryingfirstthemovesthatwerefound\nto be best in the past, brings us quite close to the theoretical limit. The past could be the\nprevious move\u2014often thesame threats remain\u2014or itcould come from previous exploration\nof the current move. One way to gain information from the current move is with iterative\ndeepening search. First, search 1 ply deep and record the best path of moves. Then search\n1 ply deeper, but use the recorded path to inform move ordering. As we saw in Chapter 3,\niterative deepening on an exponential game tree adds only a constant fraction to the total\nsearchtime,whichcanbemorethanmadeupfrombettermoveordering. Thebestmovesare\noftencalledkillermovesandtotrythemfirstiscalledthekillermoveheuristic.\nKILLERMOVES\nIn Chapter 3, we noted that repeated states in the search tree can cause an exponential\nincreaseinsearchcost. Inmanygames,repeatedstatesoccurfrequentlybecauseoftranspo-\nsitions\u2014different permutations of the move sequence that end up in the same position. For\nTRANSPOSITION\nexample, if White has one move, a , that can be answered by Black with b and an unre-\n1 1\nlated movea on the other side of the board that can be answered by b , then the sequences\n2 2\n[a ,b ,a ,b ] and [a ,b ,a ,b ] both end up in the same position. It is worthwhile to store\n1 1 2 2 2 2 1 1\nthe evaluation of the resulting position in a hash table the first time it is encountered so that\nwedon\u2019t havetorecompute itonsubsequent occurrences. The hashtableofpreviously seen\nTRANSPOSITION positionsistraditionallycalledatranspositiontable;itisessentiallyidenticaltotheexplored\nTABLE Section5.4. ImperfectReal-TimeDecisions 171\nlistinGRAPH-SEARCH (Section3.3). Usingatransposition tablecanhaveadramaticeffect,\nsometimesasmuchasdoublingthereachablesearchdepthinchess. Ontheotherhand,ifwe\nareevaluatingamillionnodespersecond,atsomepointitisnotpracticaltokeepallofthem\nin the transposition table. Various strategies have been used to choose which nodes to keep\nandwhichtodiscard.\n5.4 IMPERFECT REAL-TIME DECISIONS\nTheminimaxalgorithmgeneratestheentiregamesearchspace,whereasthealpha\u2013betaalgo-\nrithm allowsustoprune large partsofit. However, alpha\u2013beta stillhastosearch alltheway\ntoterminalstatesforatleastaportionofthesearchspace. Thisdepthisusuallynotpractical,\nbecause moves must be made in a reasonable amount of time\u2014typically a few minutes at\nmost. ClaudeShannon\u2019spaperProgrammingaComputerforPlayingChess(1950)proposed\ninsteadthatprogramsshouldcutoffthesearchearlierandapplyaheuristic evaluationfunc-\nEVALUATION tion to states in the search, effectively turning nonterminal nodes into terminal leaves. In\nFUNCTION\notherwords,thesuggestion istoalterminimaxoralpha\u2013beta intwoways: replacetheutility\nfunction by a heuristic evaluation function EVAL, which estimates the position\u2019s utility, and\nCUTOFFTEST\nreplace theterminaltestbya cutofftestthatdecides whentoapply EVAL. Thatgivesusthe\nfollowingforheuristic minimaxforstate sandmaximumdepthd:\nH-MINIMAX(s,d) =\n\u23a7\n\u23a8 EVAL(s) ifCUTOFF-TEST(s,d)\n\u23a9\nmax a\u2208Actions(s)H-MINIMAX(RESULT(s,a),d+1) ifPLAYER(s) =MAX\nmin a\u2208Actions(s)H-MINIMAX(RESULT(s,a),d+1) ifPLAYER(s) =MIN.\n5.4.1 Evaluationfunctions\nAn evaluation function returns an estimate of the expected utility of the game from a given\nposition, just as the heuristic functions of Chapter 3 return an estimate of the distance to\nthe goal. The idea of an estimator was not new when Shannon proposed it. For centuries,\nchess players (and aficionados of othergames) have developed ways ofjudging the value of\na position because humans are even more limited in the amount of search they can do than\nare computer programs. It should be clear that the performance of a game-playing program\ndepends strongly onthequality ofitsevaluation function. Aninaccurate evaluation function\nwillguideanagenttowardpositions thatturnouttobelost. Howexactlydowedesigngood\nevaluation functions?\nFirst, the evaluation function should order the terminal states in the same way as the\ntrueutility function: states thatarewinsmustevaluate betterthan draws,whichinturnmust\nbe better than losses. Otherwise, an agent using the evaluation function might err even if it\ncan see ahead all the way to the end of the game. Second, the computation must not take\ntoo long! (The whole point is to search faster.) Third, for nonterminal states, the evaluation\nfunction shouldbestrongly correlated withtheactualchances ofwinning. 172 Chapter 5. Adversarial Search\nOnemightwellwonderaboutthephrase\u201cchancesofwinning.\u201d Afterall,chessisnota\ngameofchance: weknowthecurrentstatewithcertainty,andnodiceareinvolved. Butifthe\nsearchmustbecutoffatnonterminal states, thenthealgorithm willnecessarily beuncertain\naboutthefinaloutcomesofthosestates. Thistypeofuncertaintyisinducedbycomputational,\nrather than informational, limitations. Given the limited amount of computation that the\nevaluationfunctionisallowedtodoforagivenstate,thebestitcandoismakeaguessabout\nthefinaloutcome.\nLet us make this idea more concrete. Most evaluation functions work by calculating\nvarious features ofthestate\u2014for example, inchess, wewouldhave features forthenumber\nof white pawns, black pawns, white queens, black queens, and so on. The features, taken\ntogether,definevariouscategoriesorequivalenceclassesofstates: thestatesineachcategory\nhave the same values for all the features. For example, one category contains all two-pawn\nvs. one-pawn endgames. Any given category, generally speaking, will contain some states\nthat lead to wins, some that lead to draws, and some that lead to losses. The evaluation\nfunction cannot knowwhichstatesarewhich,butitcanreturnasinglevaluethatreflectsthe\nproportion of states with each outcome. Forexample, suppose our experience suggests that\n72% ofthe states encountered inthetwo-pawns vs. one-pawn category lead toawin(utility\n+1); 20% to a loss (0), and 8% to a draw (1\/2). Then a reasonable evaluation for states in\nthe category is the expected value: (0.72 \u00d7+1)+(0.20 \u00d70)+(0.08 \u00d71\/2) = 0.76. In\nEXPECTEDVALUE\nprinciple, the expected value canbe determined foreach category, resulting inan evaluation\nfunction that works for any state. As with terminal states, the evaluation function need not\nreturnactualexpectedvaluesaslongasthe orderingofthestatesisthesame.\nIn practice, this kind of analysis requires too many categories and hence too much\nexperience to estimate all the probabilities of winning. Instead, most evaluation functions\ncompute separate numerical contributions from each feature and then combine them to find\nthe total value. Forexample, introductory chess books give an approximate material value\nMATERIALVALUE\nforeachpiece: eachpawnisworth1,aknightorbishopisworth3,arook5,andthequeen9.\nOtherfeatures suchas\u201cgood pawnstructure\u201d and\u201ckingsafety\u201d mightbeworthhalfapawn,\nsay. Thesefeaturevaluesarethensimplyaddeduptoobtaintheevaluation oftheposition.\nAsecure advantage equivalent toapawngivesasubstantial likelihood ofwinning, and\nasecureadvantageequivalenttothreepawnsshouldgivealmostcertainvictory,asillustrated\ninFigure5.8(a). Mathematically, thiskindofevaluation functioniscalledaweightedlinear\nWEIGHTEDLINEAR functionbecauseitcanbeexpressed as\nFUNCTION\n(cid:12)n\nEVAL(s) = w 1f 1(s)+w 2f 2(s)+\u00b7\u00b7\u00b7+w nf n(s) = w if i(s),\ni=1\nwhereeach w isaweight andeach f isafeature oftheposition. Forchess, the f could be\ni i i\nthenumbers ofeach kind ofpiece ontheboard, andthe w could bethevalues ofthe pieces\ni\n(1forpawn,3forbishop, etc.).\nAdding up the values of features seems like a reasonable thing to do, but in fact it\ninvolves a strong assumption: that the contribution of each feature is independent of the\nvalues of the other features. Forexample, assigning the value 3 to a bishop ignores the fact\nthat bishops are more powerful in the endgame, when they have a lot of space to maneuver. Section5.4. ImperfectReal-TimeDecisions 173\n(a) White to move (b) White to move\nFigure5.8 Twochesspositionsthatdifferonlyinthepositionoftherookatlowerright.\nIn(a), Black has an advantageofa knightand two pawns, whichshouldbe enoughto win\nthegame. In(b),Whitewillcapturethequeen,givingitanadvantagethatshouldbestrong\nenoughtowin.\nForthisreason,currentprogramsforchessandothergamesalsousenonlinearcombinations\noffeatures. Forexample,apairofbishopsmightbeworthslightlymorethantwicethevalue\nofasinglebishop,andabishopisworthmoreintheendgame(thatis,whenthemovenumber\nfeatureishighorthe numberofremainingpiecesfeatureislow).\nTheastutereaderwillhavenoticedthatthefeaturesandweightsarenotpartoftherules\nofchess! Theycomefromcenturiesofhumanchess-playingexperience. Ingameswherethis\nkind of experience is not available, the weights of the evaluation function can be estimated\nby the machine learning techniques of Chapter 18. Reassuringly, applying these techniques\ntochesshasconfirmedthatabishopisindeedworthaboutthreepawns.\n5.4.2 Cutting offsearch\nThe next step is to modify ALPHA-BETA-SEARCH so that it will call the heuristic EVAL\nfunction when it is appropriate to cut off the search. We replace the two lines in Figure 5.7\nthatmention TERMINAL-TEST withthefollowingline:\nifCUTOFF-TEST(state,depth)thenreturn EVAL(state)\nWealsomustarrangeforsomebookkeepingsothatthecurrent depth isincrementedoneach\nrecursivecall. Themoststraightforwardapproachtocontrollingtheamountofsearchistoset\nafixeddepthlimitsothatCUTOFF-TEST(state,depth)returnstrue foralldepth greaterthan\nsomefixeddepthd. (Itmustalsoreturn true forallterminalstates,justasTERMINAL-TEST\ndid.) The depth d is chosen so that a move is selected within the allocated time. A more\nrobust approach is to apply iterative deepening. (See Chapter 3.) When time runs out, the\nprogram returns the move selected by the deepest completed search. As a bonus, iterative\ndeepening alsohelpswithmoveordering. 174 Chapter 5. Adversarial Search\nThese simple approaches can lead to errors due to the approximate nature of the eval-\nuation function. Consider again the simple evaluation function for chess based on material\nadvantage. Suppose the program searches to the depth limit, reaching the position in Fig-\nure 5.8(b), where Black is ahead by a knight and two pawns. It would report this as the\nheuristic value of the state, thereby declaring that the state is a probable win by Black. But\nWhite\u2019s next move captures Black\u2019s queen with no compensation. Hence, the position is\nreallywonforWhite,butthiscanbeseenonlybylooking aheadonemoreply.\nObviously,amoresophisticatedcutofftestisneeded. Theevaluationfunctionshouldbe\nappliedonlytopositions thatarequiescent\u2014thatis,unlikely toexhibitwildswingsinvalue\nQUIESCENCE\nin the nearfuture. Inchess, forexample, positions inwhich favorable captures can be made\narenotquiescentforanevaluation functionthatjustcountsmaterial. Nonquiescent positions\ncan be expanded further until quiescent positions are reached. This extra search is called a\nQUIESCENCE quiescence search; sometimes it is restricted to consider only certain types of moves, such\nSEARCH\nascapture moves,thatwillquickly resolvetheuncertainties intheposition.\nThe horizon effect is more difficult to eliminate. It arises when the program is facing\nHORIZONEFFECT\nan opponent\u2019s move that causes serious damage and is ultimately unavoidable, but can be\ntemporarily avoided by delaying tactics. Consider the chess game in Figure 5.9. It is clear\nthat there isno wayforthe black bishop toescape. Forexample, the white rook cancapture\nitbymovingtoh1,thena1,thena2;acaptureatdepth6ply. ButBlackdoeshaveasequence\nof moves that pushes the capture of the bishop \u201cover the horizon.\u201d Suppose Black searches\ntodepth8ply. MostmovesbyBlackwillleadtotheeventualcaptureofthebishop, andthus\nwill be marked as \u201cbad\u201d moves. But Black will consider checking the white king with the\npawnate4. Thiswillleadtothekingcapturingthepawn. NowBlackwillconsiderchecking\nagain, with the pawn at f5, leading to another pawn capture. That takes up 4 ply, and from\nthere the remaining 4 ply is not enough to capture the bishop. Black thinks that the line of\nplayhassavedthebishop atthepriceoftwopawns,whenactually allithasdoneispushthe\ninevitable captureofthebishopbeyondthehorizonthatBlackcansee.\nSINGULAR One strategy to mitigate the horizon effect is the singular extension, a move that is\nEXTENSION\n\u201cclearly better\u201d than all other moves in a given position. Once discovered anywhere in the\ntreeinthecourseofasearch,thissingularmoveisremembered. Whenthesearchreachesthe\nnormaldepth limit,thealgorithm checks toseeifthesingularextension isalegalmove;ifit\nis, the algorithm allows the moveto be considered. This makes the tree deeper, but because\ntherewillbefewsingularextensions, itdoesnotaddmanytotalnodestothetree.\n5.4.3 Forwardpruning\nSo far, we have talked about cutting off search at a certain level and about doing alpha\u2013\nbeta pruning that provably has no effect on the result (at least with respect to the heuristic\nevaluation values). It is also possible to do forward pruning, meaning that some moves at\nFORWARDPRUNING\na given node are pruned immediately without further consideration. Clearly, most humans\nplaying chess consider only a few moves from each position (at least consciously). One\napproach to forward pruning is beam search: on each ply, consider only a \u201cbeam\u201d of the n\nBEAMSEARCH\nbestmoves(according totheevaluation function)ratherthanconsidering allpossible moves. Section5.4. ImperfectReal-TimeDecisions 175\n1\n2\n3\n4\n5\n6\n7\n8\na b c d e f g h\nFigure5.9 Thehorizoneffect. With Black tomove,the blackbishopissurelydoomed.\nButBlackcanforestallthateventbycheckingthewhitekingwithitspawns,forcingtheking\ntocapturethepawns.Thispushestheinevitablelossofthebishopoverthehorizon,andthus\nthepawnsacrificesareseenbythesearchalgorithmasgoodmovesratherthanbadones.\nUnfortunately, this approach is rather dangerous because there is no guarantee that the best\nmovewillnotbeprunedaway.\nThe PROBCUT, or probabilistic cut, algorithm (Buro, 1995) is a forward-pruning ver-\nsionofalpha\u2013betasearchthatusesstatisticsgainedfrompriorexperiencetolessenthechance\nthat the best move will be pruned. Alpha\u2013beta search prunes any node that is provably out-\nside the current (\u03b1,\u03b2) window. PROBCUT also prunes nodes that are probably outside the\nwindow. It computes this probability by doing a shallow search to compute the backed-up\nvaluev ofanodeandthenusing pastexperience toestimatehowlikely itisthatascoreof v\natdepthdinthetreewouldbeoutside (\u03b1,\u03b2). BuroappliedthistechniquetohisOthellopro-\ngram,LOGISTELLO,andfoundthataversionofhisprogramwithPROBCUTbeattheregular\nversion64%ofthetime,evenwhentheregularversionwasgiventwiceasmuchtime.\nCombining all the techniques described here results in a program that can play cred-\nitablechess(orothergames). Letusassumewehaveimplementedanevaluationfunctionfor\nchess, a reasonable cutoff test with a quiescence search, and a large transposition table. Let\nusalsoassumethat,aftermonthsoftediousbit-bashing,wecangenerateandevaluatearound\namillionnodespersecondonthelatestPC,allowingustosearchroughly 200millionnodes\nper move under standard time controls (three minutes per move). The branching factor for\nchess is about 35, on average, and 355 is about 50 million, so if we used minimax search,\nwe could look ahead only about fiveplies. Though not incompetent, such a program can be\nfooledeasilybyanaverage humanchessplayer, whocanoccasionally plansixoreightplies\nahead. With alpha\u2013beta search we get to about 10 plies, which results in an expert level of\nplay. Section5.8describes additional pruningtechniques thatcanextendtheeffectivesearch\ndepth to roughly 14 plies. To reach grandmaster status we would need an extensively tuned\nevaluation function andalargedatabaseofoptimalopening andendgamemoves. 176 Chapter 5. Adversarial Search\n5.4.4 Search versus lookup\nSomehowitseemslikeoverkillforachessprogramtostartagamebyconsidering atreeofa\nbilliongamestates,onlytoconclude thatitwillmoveitspawntoe4. Booksdescribing good\nplayintheopeningandendgameinchesshavebeenavailableforaboutacentury(Tattersall,\n1911). It is not surprising, therefore, that many game-playing programs use table lookup\nratherthansearchfortheopening andendingofgames.\nForthe openings, thecomputer ismostly relying onthe expertise ofhumans. Thebest\nadvice ofhumanexpertsonhowtoplayeachopening iscopied frombooks andentered into\ntables forthecomputer\u2019s use. However, computers canalso gatherstatistics from adatabase\nof previously played games to see which opening sequences most often lead to a win. In\ntheearly movestherearefewchoices, andthusmuchexpert commentaryandpastgameson\nwhichtodraw. Usuallyaftertenmovesweendupinararelyseenposition, andtheprogram\nmustswitchfromtablelookuptosearch.\nNeartheendofthegamethereareagainfewerpossiblepositions,andthusmorechance\nto do lookup. But here it is the computer that has the expertise: computer analysis of\nendgames goes far beyond anything achieved by humans. A human can tell you the gen-\neral strategy forplaying aking-and-rook-versus-king (KRK)endgame: reduce the opposing\nking\u2019s mobility bysqueezing ittowardone edgeoftheboard, using yourking toprevent the\nopponent fromescaping thesqueeze. Otherendings, such asking, bishop, andknight versus\nking (KBNK),aredifficult tomasterandhave nosuccinct strategy description. Acomputer,\nontheotherhand,cancompletely solvetheendgamebyproducing apolicy,whichisamap-\nPOLICY\npingfromeverypossiblestatetothebestmoveinthatstate. Thenwecanjustlookupthebest\nmove rather than recompute it anew. How big will the KBNK lookup table be? It turns out\nthere are 462 ways that two kings can be placed on the board without being adjacent. After\nthe kings are placed, there are 62 empty squares for the bishop, 61 for the knight, and two\npossible players to move next, so there are just 462 \u00d7 62\u00d761 \u00d72 = 3,494,568 possible\npositions. Someofthesearecheckmates;markthemassuchinatable. Thendoaretrograde\nRETROGRADE\nminimax search: reverse the rules of chess to do unmoves rather than moves. Any move by\nWhitethat,nomatterwhatmoveBlackrespondswith,endsupinapositionmarkedasawin,\nmust also be a win. Continue this search until all 3,494,568 positions are resolved as win,\nloss,ordraw,andyouhaveaninfallible lookuptableforall KBNKendgames.\nUsing this technique and a tour de force of optimization tricks, Ken Thompson (1986,\n1996) and Lewis Stiller (1992, 1996) solved all chess endgames with up to five pieces and\nsome with six pieces, making them available on the Internet. Stiller discovered one case\nwhereaforcedmateexistedbutrequired262moves;thiscausedsomeconsternation because\nthe rules of chess require a capture or pawn move to occur within 50 moves. Laterwork by\nMarc Bourzutschky and Yakov Konoval (Bourzutschky, 2006) solved all pawnless six-piece\nandsomeseven-pieceendgames;thereisaKQNKRBNendgamethatwithbestplayrequires\n517movesuntilacapture, whichthenleadstoamate.\nIf we could extend the chess endgame tables from 6 pieces to 32, then White would\nknow onthe opening movewhether itwould be awin, loss, ordraw. This has not happened\nsofarforchess,butithashappenedforcheckers, asexplained inthehistoricalnotessection. Section5.5. StochasticGames 177\n5.5 STOCHASTIC GAMES\nIn real life, many unpredictable external events can put us into unforeseen situations. Many\ngames mirror this unpredictability by including a random element, such as the throwing of\ndice. We call these stochastic games. Backgammon is a typical game that combines luck\nSTOCHASTICGAMES\nandskill. Dicearerolledatthebeginning ofaplayer\u2019s turn todetermine thelegalmoves. In\nthe backgammon position of Figure 5.10, for example, White has rolled a 6\u20135 and has four\npossible moves.\n0 1 2 3 4 5 6 7 8 9 10 11 12\n25 24 23 22 21 20 19 18 17 16 15 14 13\nFigure5.10 A typicalbackgammonposition. Thegoalofthegameistomoveallone\u2019s\npiecesofftheboard. Whitemovesclockwisetoward25,andBlackmovescounterclockwise\ntoward0.Apiececanmovetoanypositionunlessmultipleopponentpiecesarethere;ifthere\nisoneopponent,itiscapturedandmuststartover. Inthepositionshown,White hasrolled\n6\u20135 and must choose among four legal moves: (5\u201310,5\u201311), (5\u201311,19\u201324),(5\u201310,10\u201316),\nand(5\u201311,11\u201316),wherethenotation(5\u201311,11\u201316)meansmoveonepiecefromposition5\nto11,andthenmoveapiecefrom11to16.\nAlthoughWhiteknowswhathisorherownlegalmovesare,Whitedoesnotknowwhat\nBlackisgoing torollandthus doesnotknow whatBlack\u2019s legalmoveswillbe. Thatmeans\nWhite cannot construct a standard game tree of the sort we saw in chess and tic-tac-toe. A\nCHANCENODES game tree in backgammon must include chance nodes in addition to MAX and MIN nodes.\nChance nodes are shown as circles in Figure 5.11. The branches leading from each chance\nnode denote the possible dice rolls; each branch is labeled with the roll and its probability.\nThereare36waystorolltwodice,eachequallylikely;butbecausea6\u20135isthesameasa5\u20136,\nthereareonly21distinctrolls. Thesixdoubles(1\u20131through 6\u20136)eachhaveaprobability of\n1\/36,sowesayP(1\u20131) = 1\/36. Theother15distinctrollseachhavea1\/18probability. 178 Chapter 5. Adversarial Search\nMAX\nCHANCE . . .\nB\n... ... ... ...\n1\/36 1\/18 1\/18 1\/36\n1,1 1,2 6,5 6,6\nMIN . . .\n... ... ...\nCHANCE C . . .\n... ... ...\n1\/36 1\/18 1\/18 1\/36\n1,1 1,2 6,5 6,6\nMAX . . .\n... ... ...\nTERMINAL 2 \u20131 1 \u20131 1\nFigure5.11 Schematicgametreeforabackgammonposition.\nThenextstepistounderstand howtomakecorrect decisions. Obviously, westillwant\nto pick the move that leads to the best position. However, positions do not have definite\nminimaxvalues. Instead,wecanonlycalculatetheexpectedvalueofaposition: theaverage\nEXPECTEDVALUE\noverallpossible outcomesofthechancenodes.\nThis leads us to generalize the minimax value for deterministic games to an expecti-\nEXPECTIMINIMAX minimaxvalueforgameswithchance nodes. Terminalnodes and MAX and MIN nodes (for\nVALUE\nwhich the dice roll is known) work exactly the same way as before. For chance nodes we\ncompute the expected value, which is the sum of the value over all outcomes, weighted by\ntheprobability ofeachchanceaction:\nEXPECTIMINIMAX(s)=\n\u23a7\n\u23aa \u23aa UTILITY(s) ifTERMINAL-TEST(s)\n\u23a8\nmax aEXPECTIMINIMAX(RESULT(s,a)) ifPLAYER(s)= MAX\n\u23aa \u23aa \u23a9 m (cid:2)in aEXPECTIMINIMAX(RESULT(s,a)) ifPLAYER(s)= MIN\nP(r)EXPECTIMINIMAX(RESULT(s,r)) ifPLAYER(s)= CHANCE\nr\nwhere r represents apossible diceroll(orotherchance event)and RESULT(s,r)isthesame\nstateass,withtheadditional factthattheresultofthedicerollisr.\n5.5.1 Evaluationfunctions forgames ofchance\nAs with minimax, the obvious approximation to make with expectiminimax is to cut the\nsearch offat some point and apply an evaluation function to each leaf. Onemight think that\nevaluation functions forgamessuchasbackgammonshould bejustlikeevaluation functions Section5.5. StochasticGames 179\nforchess\u2014theyjustneedtogivehigherscorestobetterpositions. Butinfact,thepresenceof\nchance nodes means that one has tobe more careful about what the evaluation values mean.\nFigure 5.12 shows what happens: with an evaluation function that assigns the values [1, 2,\n3, 4] to the leaves, move a is best; with values [1, 20, 30, 400], move a is best. Hence,\n1 2\nthe program behaves totally differently if we make a change in the scale of some evaluation\nvalues! It turns out that to avoid this sensitivity, the evaluation function must be a positive\nlineartransformation oftheprobabilityofwinningfromaposition(or,moregenerally, ofthe\nexpected utility of the position). This is an important and general property of situations in\nwhichuncertainty isinvolved, andwediscussitfurtherinChapter16.\nMAX\na a a a\n1 2 1 2\nCHANCE 2.1 1.3 21 40.9\n.9 .1 .9 .1 .9 .1 .9 .1\nMIN 2 3 1 4 20 30 1 400\n2 2 3 3 1 1 4 4 20 20 30 30 1 1 400 400\nFigure5.12 Anorder-preservingtransformationonleafvalueschangesthebestmove.\nIf the program knew in advance all the dice rolls that would occur for the rest of the\ngame,solving agamewithdicewouldbejustlikesolving agamewithout dice,whichmini-\nmaxdoesinO(bm)time,wherebisthebranchingfactorandmisthemaximumdepthofthe\ngame tree. Because expectiminimax is also considering all the possible dice-roll sequences,\nitwilltakeO(bmnm),wherenisthenumberofdistinctrolls.\nEvenifthesearchdepthislimitedtosomesmalldepth d,theextracostcomparedwith\nthat of minimax makes it unrealistic to consider looking ahead very far in most games of\nchance. Inbackgammon nis21andb isusually around 20, butinsome situations canbeas\nhighas4000fordicerollsthataredoubles. Threepliesisprobably allwecouldmanage.\nAnother way to think about the problem is this: the advantage of alpha\u2013beta is that\nit ignores future developments that just are not going to happen, given best play. Thus, it\nconcentrates on likely occurrences. In games with dice, there are no likely sequences of\nmoves,because forthosemovestotakeplace, thedicewouldfirsthavetocomeouttheright\nway to make them legal. This is a general problem whenever uncertainty enters the picture:\nthe possibilities are multiplied enormously, and forming detailed plans of action becomes\npointless becausetheworldprobably willnotplayalong.\nIt may have occurred to you that something like alpha\u2013beta pruning could be applied 180 Chapter 5. Adversarial Search\nto game trees with chance nodes. It turns out that it can. The analysis for MIN and MAX\nnodes is unchanged, but wecan also prune chance nodes, using a bit of ingenuity. Consider\nthechance node C inFigure5.11and whathappens toitsvalue asweexamine and evaluate\nits children. Is it possible to find an upper bound on the value of C before we have looked\natallitschildren? (Recall that thisiswhatalpha\u2013beta needs inordertoprune anode andits\nsubtree.) Atfirstsight, itmightseem impossible because thevalueofC istheaverage ofits\nchildren\u2019s values, and in order to compute the average of a set of numbers, we must look at\nall the numbers. But ifweput bounds on the possible values of the utility function, then we\ncan arrive atbounds forthe average without looking atevery number. Forexample, say that\nallutilityvaluesarebetween\u22122and+2;thenthevalueofleafnodesisbounded, andinturn\nwecanplaceanupperboundonthevalueofachancenodewithoutlookingatallitschildren.\nMONTECARLO An alternative is to do Monte Carlo simulation to evaluate a position. Start with\nSIMULATION\nan alpha\u2013beta (or other) search algorithm. From a start position, have the algorithm play\nthousands of games against itself, using random dice rolls. In the case of backgammon, the\nresulting win percentage has been shown to be a good approximation of the value of the\nposition, even if the algorithm has an imperfect heuristic and is searching only a few plies\n(Tesauro,1995). Forgameswithdice,thistypeofsimulation iscalledarollout.\nROLLOUT\n5.6 PARTIALLY OBSERVABLE GAMES\nChess has often been described as war in miniature, but it lacks at least one major charac-\nteristic of real wars, namely, partial observability. In the \u201cfog of war,\u201d the existence and\ndisposition of enemy units is often unknown until revealed by direct contact. As a result,\nwarfareincludestheuseofscoutsandspiestogatherinformationandtheuseofconcealment\nand bluff to confuse the enemy. Partially observable games share these characteristics and\narethusqualitatively differentfromthegamesdescribed inthepreceding sections.\n5.6.1 Kriegspiel: Partiallyobservablechess\nIndeterministicpartiallyobservablegames,uncertaintyaboutthestateoftheboardarisesen-\ntirelyfromlackofaccesstothechoicesmadebytheopponent. Thisclassincludeschildren\u2019s\ngamessuchasBattleships(whereeachplayer\u2019sshipsareplacedinlocationshiddenfromthe\nopponentbutdonotmove)andStratego(wherepiecelocationsareknownbutpiecetypesare\nhidden). Wewill examine the game of Kriegspiel, apartially observable variant of chess in\nKRIEGSPIEL\nwhichpiecescanmovebutarecompletelyinvisible totheopponent.\nThe rules of Kriegspiel are as follows: White and Black each see a board containing\nonlytheirownpieces. Areferee,whocanseeallthepieces,adjudicatesthegameandperiod-\nically makes announcements that are heard by both players. On his turn, White proposes to\nthereferee anymovethatwouldbelegalifthere werenoblack pieces. Ifthemoveisinfact\nnot legal (because of the black pieces), the referee announces \u201cillegal.\u201d In this case, White\nmaykeepproposing movesuntilalegaloneisfound\u2014and learnsmoreaboutthelocation of\nBlack\u2019s pieces in the process. Once a legal move is proposed, the referee announces one or Section5.6. PartiallyObservable Games 181\nmore of the following: \u201cCapture on square X\u201dif there is a capture, and \u201cCheck by D\u201d if the\nblack king is in check, where D is the direction of the check, and can be one of \u201cKnight,\u201d\n\u201cRank,\u201d \u201cFile,\u201d \u201cLong diagonal,\u201d or \u201cShort diagonal.\u201d (In case of discovered check, the ref-\neree may make two \u201cCheck\u201d announcements.) If Black is checkmated or stalemated, the\nrefereesaysso;otherwise, itisBlack\u2019sturntomove.\nKriegspielmayseemterrifyinglyimpossible,buthumansmanageitquitewellandcom-\nputer programs are beginning to catch up. It helps to recall the notion of a belief state as\ndefined in Section 4.4 and illustrated in Figure 4.14\u2014the set of all logically possible board\nstates given the complete history of percepts to date. Initially, White\u2019s belief state is a sin-\ngleton because Black\u2019s pieces haven\u2019t moved yet. After White makes a move and Black re-\nsponds, White\u2019s belief state contains 20positions because Blackhas20replies toanyWhite\nmove. Keepingtrackofthebeliefstateasthegameprogressesisexactlytheproblemofstate\nestimation, for which the update step is given in Equation (4.6). We can map Kriegspiel\nstate estimation directly onto the partially observable, nondeterministic framework of Sec-\ntion 4.4 if we consider the opponent as the source of nondeterminism; that is, the RESULTS\nofWhite\u2019smovearecomposedfromthe(predictable) outcome ofWhite\u2019sownmoveandthe\nunpredictable outcomegivenbyBlack\u2019sreply.3\nGiven a current belief state, White may ask, \u201cCan I win the game?\u201d For a partially\nobservable game, the notion of a strategy is altered; instead of specifying a move to make\nforeachpossible movetheopponent mightmake,weneedamoveforeverypossible percept\nsequence that might be received. For Kriegspiel, a winning strategy, or guaranteed check-\nGUARANTEED mate,isonethat, foreachpossible percept sequence, leadstoan actual checkmate forevery\nCHECKMATE\npossible board state in the current belief state, regardless of how the opponent moves. With\nthis definition, the opponent\u2019s belief state is irrelevant\u2014the strategy has to work even if the\nopponent can see all the pieces. This greatly simplifies the computation. Figure 5.13 shows\npart of a guaranteed checkmate for the KRK (king and rook against king) endgame. In this\ncase, Blackhasjustonepiece(theking), soabelief stateforWhitecanbeshowninasingle\nboardbymarkingeachpossible position oftheBlackking.\nThe general AND-OR search algorithm can be applied to the belief-state space to find\nguaranteed checkmates, just as in Section 4.4. The incremental belief-state algorithm men-\ntioned inthatsection oftenfindsmidgamecheckmates uptodepth9\u2014probably wellbeyond\ntheabilitiesofhumanplayers.\nIn addition to guaranteed checkmates, Kriegspiel admits an entirely new concept that\nPROBABILISTIC makes no sense in fully observable games: probabilistic checkmate. Such checkmates are\nCHECKMATE\nstillrequiredtoworkineveryboardstateinthebeliefstate;theyareprobabilisticwithrespect\ntorandomization ofthewinning player\u2019s moves. Togetthebasic idea, consider theproblem\nof finding a lone black king using just the white king. Simply by moving randomly, the\nwhite king will eventually bump into the black king even if the latter tries to avoid this fate,\nsince Blackcannot keep guessing therightevasivemovesindefinitely. Intheterminology of\nprobability theory, detection occurs with probability 1. The KBNK endgame\u2014king, bishop\n3 Sometimes,thebeliefstatewillbecometoolargetorepresentjustasalistofboardstates,butwewillignore\nthisissuefornow;Chapters7and8suggestmethodsforcompactlyrepresentingverylargebeliefstates. 182 Chapter 5. Adversarial Search\n4\n3\n2\n1\na b c d\nKc3 ?\n\u201cOK\u201d \u201cIllegal\u201d\nRc3 ?\n\u201cOK\u201d \u201cCheck\u201d\nFigure 5.13 Part of a guaranteedcheckmatein the KRK endgame, shown on a reduced\nboard. In the initial belief state, Black\u2019s king is in one of three possible locations. By a\ncombination of probing moves, the strategy narrows this down to one. Completion of the\ncheckmateisleftasanexercise.\nandknight against king\u2014is woninthis sense; Whitepresents Blackwithaninfiniterandom\nsequence of choices, for one of which Black will guess incorrectly and reveal his position,\nleadingtocheckmate. TheKBBKendgame,ontheotherhand,iswonwithprobability1\u2212(cid:2).\nWhite can force a win only by leaving one of his bishops unprotected for one move. If\nBlackhappens tobeintherightplaceandcaptures thebishop (amovethatwouldloseifthe\nbishopsareprotected), thegameisdrawn. Whitecanchoosetomaketheriskymoveatsome\nrandomlychosenpointinthemiddleofaverylongsequence,thusreducing(cid:2)toanarbitrarily\nsmallconstant, butcannotreduce (cid:2)tozero.\nIt is quite rare that a guaranteed or probabilistic checkmate can be found within any\nreasonabledepth,exceptintheendgame. Sometimesacheckmatestrategyworksforsomeof\ntheboardstatesinthecurrentbeliefstatebutnotothers. Tryingsuchastrategymaysucceed,\nACCIDENTAL leadingtoanaccidentalcheckmate\u2014accidental inthesensethatWhitecouldnotknowthat\nCHECKMATE\nitwouldbecheckmate\u2014ifBlack\u2019spieceshappentobeintherightplaces. (Mostcheckmates\nin games between humans are of this accidental nature.) This idea leads naturally to the\nquestion ofhow likely itis that agiven strategy willwin, whichleads inturn tothe question\nofhowlikelyitisthateachboardstateinthecurrentbeliefstateisthetrueboardstate. Section5.6. PartiallyObservable Games 183\nOne\u2019sfirstinclinationmightbetoproposethatallboardstatesinthecurrentbeliefstate\nare equally likely\u2014but this can\u2019t be right. Consider, for example, White\u2019s belief state after\nBlack\u2019s first move of the game. By definition (assuming that Black plays optimally), Black\nmusthaveplayedanoptimalmove,soallboardstatesresultingfromsuboptimalmovesought\ntobeassignedzeroprobability. Thisargumentisnotquiterighteither,because eachplayer\u2019s\ngoal is not just to move pieces tothe right squares but also tominimize the information that\nthe opponent has about their location. Playing any predictable \u201coptimal\u201d strategy provides\nthe opponent with information. Hence, optimal play in partially observable games requires\na willingness to play somewhat randomly. (This is why restaurant hygiene inspectors do\nrandominspection visits.) Thismeansoccasionally selecting movesthatmayseem\u201cintrinsi-\ncally\u201dweak\u2014buttheygainstrengthfromtheirveryunpredictability, becausetheopponentis\nunlikely tohaveprepared anydefenseagainstthem.\nFrom these considerations, it seems that the probabilities associated with the board\nstates in the current belief state can only be calculated given an optimal randomized strat-\negy; in turn, computing that strategy seems to require knowing the probabilities of the var-\nious states the board might be in. This conundrum can be resolved by adopting the game-\ntheoretic notion of an equilibrium solution, which we pursue further in Chapter 17. An\nequilibrium specifies an optimal randomized strategy for each player. Computing equilib-\nria is prohibitively expensive, however, even for small games, and is out of the question for\nKriegspiel. At present, the design of effective algorithms for general Kriegspiel play is an\nopen research topic. Most systems perform bounded-depth lookahead in their own belief-\nstatespace, ignoring theopponent\u2019s beliefstate. Evaluation functions resemble those forthe\nobservable gamebutincludeacomponent forthesizeofthebeliefstate\u2014smallerisbetter!\n5.6.2 Cardgames\nCard games provide many examples of stochastic partial observability, where the missing\ninformationisgeneratedrandomly. Forexample,inmanygames,cardsaredealtrandomlyat\nthe beginning of the game, with each player receiving a hand that is not visible to the other\nplayers. Suchgamesincludebridge, whist,hearts, andsome formsofpoker.\nAtfirstsight,itmightseemthatthesecardgamesarejustlikedicegames: thecardsare\ndealtrandomlyanddeterminethemovesavailabletoeachplayer,butallthe\u201cdice\u201darerolled\natthe beginning! Even though this analogy turns out tobe incorrect, itsuggests an effective\nalgorithm: consider all possible deals of the invisible cards; solve each one as if it were a\nfullyobservablegame;andthenchoosethemovethathasthebestoutcomeaveragedoverall\nthedeals. Supposethateachdealsoccurswithprobability P(s);thenthemovewewantis\n(cid:12)\nargmax P(s)MINIMAX(RESULT(s,a)). (5.1)\na\ns\nHere,werunexact MINIMAX ifcomputationally feasible;otherwise, werun H-MINIMAX.\nNow, in most card games, the number of possible deals is rather large. For example,\ninbridge play, each player sees just two(cid:20)of(cid:21)the fourhands; there aretwounseen hands of13\ncards each, so the number of deals is 26 = 10,400,600. Solving even one deal is quite\n13\ndifficult, so solving ten million is out of the question. Instead, we resort to a Monte Carlo 184 Chapter 5. Adversarial Search\napproximation: instead of adding up all the deals, we take a random sample of N deals,\nwheretheprobability ofdeal sappearing inthesampleisproportional to P(s):\n(cid:12)N\n1\nargmax MINIMAX(RESULT(s i,a)). (5.2)\nN\na\ni=1\n(Notice that P(s) does not appear explicitly in the summation, because the samples are al-\nready drawn according to P(s).) As N grows large, the sum over the random sample tends\ntotheexact value, but evenforfairly small N\u2014say, 100to1,000\u2014the method gives agood\napproximation. Itcanalso beapplied todeterministic gamessuchasKriegspiel, givensome\nreasonable estimateofP(s).\nForgameslikewhistandhearts,wherethereisnobidding orbetting phasebeforeplay\ncommences, each deal will be equally likely and so the values of P(s) are all equal. For\nbridge, play ispreceded byabidding phase inwhicheachteam indicates howmanytricks it\nexpects towin. Since players bid based on the cards they hold, the other players learn more\nabouttheprobability ofeachdeal. Takingthisintoaccount indeciding howtoplaythehand\nistricky, forthereasons mentioned inourdescription ofKriegspiel: players maybidinsuch\nawayastominimize theinformation conveyed totheiropponents. Evenso, the approach is\nquiteeffectiveforbridge, asweshowinSection5.7.\nThe strategy described in Equations 5.1 and 5.2 is sometimes called averaging over\nclairvoyance because it assumes that the game will become observable to both players im-\nmediately after the first move. Despite its intuitive appeal, the strategy can lead one astray.\nConsiderthefollowingstory:\nDay 1: Road A leads to a heap of gold; Road B leads to a fork. Take the left fork and\nyou\u2019llfindabiggerheapofgold,buttaketherightforkandyou\u2019llberunoverbyabus.\nDay2: RoadA leadsto aheapofgold; RoadBleadstoa fork. Takethe rightforkand\nyou\u2019llfindabiggerheapofgold,buttaketheleftforkandyou\u2019llberunoverbyabus.\nDay 3: Road A leads to a heap of gold; Road B leads to a fork. One branch of the\nforkleads to a biggerheap of gold, but take the wrong forkand you\u2019llbe hit by a bus.\nUnfortunatelyyoudon\u2019tknowwhichforkiswhich.\nAveragingoverclairvoyanceleadstothefollowingreasoning: onDay1,Bistherightchoice;\nonDay2,Bistheright choice; onDay3,thesituation isthesameaseitherDay1orDay2,\nsoBmuststillbetherightchoice.\nNow we can see how averaging overclairvoyance fails: it does not consider the belief\nstatethattheagentwillbeinafteracting. Abeliefstateoftotalignoranceisnotdesirable,es-\npecially whenonepossibility iscertain death. Becauseitassumesthateveryfuturestatewill\nautomatically beoneofperfectknowledge, theapproach neverselectsactionsthatgatherin-\nformation(likethefirstmoveinFigure5.13);norwillitchooseactionsthathideinformation\nfrom the opponent or provide information to a partner because it assumes that they already\nknowtheinformation; anditwillneverbluffinpoker,4 because itassumestheopponent can\nBLUFF\nseeitscards. InChapter17, weshowhowtoconstruct algorithms thatdoallthese things by\nvirtueofsolving thetruepartiallyobservable decisionproblem.\n4 Bluffing\u2014bettingasifone\u2019shandisgood,evenwhenit\u2019snot\u2014isacorepartofpokerstrategy. Section5.7. State-of-the-Art GamePrograms 185\n5.7 STATE-OF-THE-ART GAME PROGRAMS\nIn 1965, the Russian mathematician Alexander Kronrod called chess \u201cthe Drosophila of ar-\ntificial intelligence.\u201d John McCarthy disagrees: whereas geneticists use fruit flies to make\ndiscoveries that apply to biology more broadly, AI has used chess to do the equivalent of\nbreeding very fast fruit flies. Perhaps a better analogy is that chess is to AI as Grand Prix\nmotorracingistothecarindustry: state-of-the-art gameprogramsareblindingly fast,highly\noptimized machines that incorporate the latest engineering advances, but they aren\u2019t much\nuse for doing the shopping or driving off-road. Nonetheless, racing and game-playing gen-\nerate excitement and a steady stream of innovations that have been adopted by the wider\ncommunity. Inthissectionwelookatwhatittakestocomeoutontopinvariousgames.\nCHESS\nChess: IBM\u2019s DEEP BLUE chess program, now retired, is well known for defeating world\nchampion GarryKasparov inawidely publicized exhibition match. DeepBlueranonapar-\nallel computer with 30 IBM RS\/6000 processors doing alpha\u2013beta search. The unique part\nwas a configuration of 480 custom VLSI chess processors that performed move generation\nandmoveorderingforthelastfewlevelsofthetree,andevaluatedtheleafnodes. DeepBlue\nsearched up to 30 billion positions per move, reaching depth 14 routinely. The key to its\nsuccess seemstohavebeen itsability togenerate singular extensions beyond thedepth limit\nforsufficiently interesting lines offorcing\/forced moves. Insomecases thesearch reached a\ndepth of 40 plies. Theevaluation function had over8000 features, many of them describing\nhighly specific patterns of pieces. An \u201copening book\u201d of about 4000 positions was used, as\nwell as a database of 700,000 grandmaster games from which consensus recommendations\ncould beextracted. Thesystem also used alarge endgame database of solved positions con-\ntaining all positions with five pieces and many with six pieces. This database had the effect\nofsubstantially extending theeffectivesearchdepth,allowingDeepBluetoplayperfectlyin\nsomecasesevenwhenitwasmanymovesawayfromcheckmate.\nThesuccessofDEEP BLUEreinforcedthewidelyheldbeliefthatprogressincomputer\ngame-playing has come primarily from ever-more-powerful hardware\u2014a view encouraged\nby IBM. But algorithmic improvements have allowed programs running on standard PCs\nto win World Computer Chess Championships. A variety of pruning heuristics are used to\nreducetheeffectivebranchingfactortolessthan3(comparedwiththeactualbranchingfactor\nofabout35). Themostimportantoftheseisthenullmoveheuristic, whichgenerates agood\nNULLMOVE\nlower bound on the value of a position, using a shallow search in which the opponent gets\nto move twice at the beginning. This lower bound often allows alpha\u2013beta pruning without\ntheexpenseofafull-depth search. Alsoimportant is futilitypruning,whichhelpsdecidein\nFUTILITYPRUNING\nadvancewhichmoveswillcauseabetacutoffinthesuccessor nodes.\nHYDRA can be seen as the successor to DEEP BLUE. HYDRA runs on a 64-processor\ncluster with1gigabyte perprocessor andwithcustom hardware intheform ofFPGA(Field\nProgrammableGateArray)chips. HYDRAreaches200millionevaluationspersecond,about\nthe same as Deep Blue, but HYDRA reaches 18 plies deep rather than just 14 because of\naggressive useofthenullmoveheuristic andforwardpruning. 186 Chapter 5. Adversarial Search\nRYBKA, winnerof the 2008 and 2009 World Computer Chess Championships, iscon-\nsidered the strongest current computer player. It uses an off-the-shelf 8-core 3.2 GHz Intel\nXeon processor, but little is known about the design of the program. RYBKA\u2019s main ad-\nvantage appears to be its evaluation function, which has been tuned by its main developer,\nInternational MasterVasikRajlich,andatleastthreeothergrandmasters.\nThe most recent matches suggest that the top computer chess programs have pulled\naheadofallhumancontenders. (Seethehistorical notesfordetails.)\nCHECKERS\nCheckers: Jonathan Schaeffer and colleagues developed CHINOOK, which runs on regular\nPCsand uses alpha\u2013beta search. Chinook defeated the long-running human champion in an\nabbreviatedmatchin1990,andsince2007CHINOOKhasbeenabletoplayperfectlybyusing\nalpha\u2013beta searchcombined withadatabase of39trillionendgamepositions.\nOthello, also called Reversi, is probably more popular as a computer game than as a board\nOTHELLO\ngame. It has a smaller search space than chess, usually 5 to 15 legal moves, but evaluation\nexpertisehadtobedevelopedfromscratch. In1997,theLOGISTELLO program(Buro,2002)\ndefeatedthehumanworldchampion,TakeshiMurakami,bysixgamestonone. Itisgenerally\nacknowledged thathumansarenomatchforcomputers atOthello.\nBackgammon: Section5.5explainedwhytheinclusionofuncertaintyfromdicerollsmakes\nBACKGAMMON\ndeep search an expensive luxury. Most work on backgammon has gone into improving the\nevaluation function. Gerry Tesauro (1992) combined reinforcement learning with neural\nnetworks to develop a remarkably accurate evaluator that is used with a search to depth 2\nor 3. After playing more than a million training games against itself, Tesauro\u2019s program,\nTD-GAMMON,iscompetitivewithtophumanplayers. Theprogram\u2019sopinionsontheopen-\ningmovesofthegamehaveinsomecasesradicallyalteredthe receivedwisdom.\nGo is the most popular board game in Asia. Because the board is 19 \u00d7 19 and moves are\nGO\nallowed into (almost) every empty square, the branching factor starts at 361, which is too\ndaunting for regular alpha\u2013beta search methods. In addition, it is difficult to write an eval-\nuation function because control of territory is often very unpredictable until the endgame.\nTherefore thetopprograms, suchas MOGO,avoid alpha\u2013beta search andinstead useMonte\nCarlorollouts. Thetrickistodecidewhatmovestomakeinthecourseoftherollout. Thereis\nnoaggressive pruning; allmovesarepossible. TheUCT(upperconfidence bounds ontrees)\nmethod works by making random moves in the first few iterations, and over time guiding\nthesampling process toprefermovesthathaveledtowinsinprevious samples. Sometricks\nare added, including knowledge-based rules that suggest particular moves whenever a given\npattern isdetected andlimitedlocal searchtodecide tactical questions. Someprogramsalso\nCOMBINATORIAL include special techniques from combinatorial game theory to analyze endgames. These\nGAMETHEORY\ntechniques decompose aposition intosub-positions thatcanbeanalyzed separately andthen\ncombined (Berlekamp and Wolfe, 1994; Mu\u00a8ller, 2003). The optimal solutions obtained in\nthis way have surprised many professional Go players, who thought they had been playing\noptimally allalong. Current Goprograms play atthemasterlevelonareduced 9\u00d79board,\nbutarestillatadvanced amateurlevelonafullboard.\nBridge is a card game of imperfect information: a player\u2019s cards are hidden from the other\nBRIDGE\nplayers. Bridge is also a multiplayer game with four players instead of two, although the Section5.8. AlternativeApproaches 187\nplayers are paired into two teams. As in Section 5.6, optimal play in partially observable\ngameslikebridgecanincludeelementsofinformationgathering,communication,andcareful\nweighing of probabilities. Many of these techniques are used in the Bridge Baron program\n(Smith et al., 1998), which won the 1997 computer bridge championship. While it does\nnot play optimally, Bridge Baron is one of the few successful game-playing systems to use\ncomplex,hierarchicalplans(seeChapter11)involvinghigh-levelideas,suchasfinessingand\nsqueezing,thatarefamiliartobridgeplayers.\nTheGIBprogram(Ginsberg,1999)wonthe2000computerbridgechampionshipquite\ndecisivelyusingtheMonteCarlomethod. Sincethen,otherwinningprogramshavefollowed\nEXPLANATION-\nGIB\u2019slead. GIB\u2019smajorinnovation isusingexplanation-based generalization tocompute\nBASED\nGENERALIZATION\nand cache general rules foroptimal play in various standard classes of situations rather than\nevaluating each situation individually. For example, in a situation where one player has the\ncards A-K-Q-J-4-3-2 of one suit and another player has 10-9-8-7-6-5, there are 7\u00d76 = 42\nways that the first player can lead from that suit and the second player can follow. But GIB\ntreats these situations as just two: the first player can lead either a high card or a low card;\ntheexactcardsplayeddon\u2019tmatter. Withthisoptimization (andafewothers), GIBcansolve\na52-card, fullyobservable deal exactly inabout asecond. GIB\u2019stactical accuracy makesup\nforitsinabilitytoreasonaboutinformation. Itfinished12thinafieldof35intheparcontest\n(involving just play of the hand, not bidding) at the 1998 human world championship, far\nexceeding theexpectations ofmanyhumanexperts.\nThereare several reasons why GIB plays atexpert level withMonte Carlosimulation,\nwhereas Kriegspiel programs donot. First, GIB\u2019s evaluation ofthe fully observable version\nofthegameisexact,searching thefullgametree, whileKriegspiel programs relyoninexact\nheuristics. But far more important is the fact that in bridge, most of the uncertainty in the\npartially observable information comesfromtherandomness ofthedeal,notfromtheadver-\nsarial play of the opponent. Monte Carlo simulation handles randomness well, but does not\nalwayshandlestrategy well,especially whenthestrategy involves thevalueofinformation.\nScrabble: MostpeoplethinkthehardpartaboutScrabbleiscomingupwithgoodwords,but\nSCRABBLE\ngiventheofficialdictionary, itturnsouttoberathereasytoprogramamovegeneratortofind\nthe highest-scoring move (Gordon, 1994). That doesn\u2019t mean the game is solved, however:\nmerely taking the top-scoring move each turn results in a good but not expert player. The\nproblem is that Scrabble is both partially observable and stochastic: you don\u2019t know what\nletters the other player has or what letters you will draw next. So playing Scrabble well\ncombines the difficulties of backgammon and bridge. Nevertheless, in 2006, the QUACKLE\nprogram defeatedtheformerworldchampion, DavidBoys,3\u20132.\n5.8 ALTERNATIVE APPROACHES\nBecause calculating optimal decisions in games is intractable in most cases, all algorithms\nmust make some assumptions and approximations. The standard approach, based on mini-\nmax,evaluationfunctions, andalpha\u2013beta, isjustonewaytodothis. Probablybecauseithas 188 Chapter 5. Adversarial Search\nMAX\nMIN 99 100\n99 1000 1000 1000 100 101 102 100\nFigure5.14 Atwo-plygametreeforwhichheuristicminimaxmaymakeanerror.\nbeen worked on for so long, the standard approach dominates other methods in tournament\nplay. Some believe that this has caused game playing to become divorced from the main-\nstream ofAIresearch: thestandard approach nolongerprovides muchroom fornewinsight\nintogeneralquestions ofdecision making. Inthissection, welookatthealternatives.\nFirst, let us consider heuristic minimax. It selects an optimal move in a given search\ntree provided that the leaf node evaluations are exactly correct. In reality, evaluations are\nusually crude estimates of the value of a position and can be considered to have large errors\nassociated with them. Figure 5.14 shows a two-ply game tree for which minimax suggests\ntaking the right-hand branch because 100 > 99. That is the correct move if the evaluations\nare all correct. But of course the evaluation function is only approximate. Suppose that\nthe evaluation of each node has an error that is independent of other nodes and is randomly\ndistributed with mean zero and standard deviation of \u03c3. Then when \u03c3 = 5, the left-hand\nbranch is actually better 71% of the time, and 58% of the time when \u03c3 = 2. The intuition\nbehind this is that the right-hand branch has four nodes that are close to 99; if an error in\nthe evaluation of any one of the four makes the right-hand branch slip below 99, then the\nleft-hand branchisbetter.\nInreality,circumstances areactuallyworsethanthisbecausetheerrorintheevaluation\nfunctionisnotindependent. Ifwegetonenodewrong,thechancesarehighthatnearbynodes\nin the tree will also be wrong. The fact that the node labeled 99 has siblings labeled 1000\nsuggests that in fact it might have a higher true value. We can use an evaluation function\nthatreturns aprobability distribution overpossible values, butitisdifficulttocombinethese\ndistributions properly, because wewon\u2019thaveagoodmodeloftheverystrongdependencies\nthatexistbetweenthevaluesofsiblingnodes\nNext,weconsiderthesearchalgorithmthatgeneratesthetree. Theaimofanalgorithm\ndesigneristospecifyacomputationthatrunsquicklyandyieldsagoodmove. Thealpha\u2013beta\nalgorithmisdesignednotjusttoselectagoodmovebutalsotocalculateboundsonthevalues\nof all the legal moves. Tosee whythis extra information is unnecessary, consider aposition\nin which there is only one legal move. Alpha\u2013beta search still will generate and evaluate a\nlarge search tree, telling usthatthe onlymoveisthebest moveand assigning itavalue. But\nsince we have to make the move anyway, knowing the move\u2019s value is useless. Similarly, if\nthereisoneobviouslygoodmoveandseveralmovesthatarelegalbutleadtoaquickloss,we Section5.9. Summary 189\nwouldnotwantalpha\u2013betatowastetimedeterminingaprecisevalueforthelonegoodmove.\nBettertojustmakethemovequicklyandsavethetimeforlater. Thisleadstotheideaofthe\nutility of a node expansion. A good search algorithm should select node expansions of high\nutility\u2014that is, ones that are likely tolead tothe discovery ofa significantly better move. If\nthere are no node expansions whose utility is higher than their cost (in terms of time), then\nthe algorithm should stop searching and make a move. Notice that this works not only for\nclear-favorite situations but also forthe case of symmetrical moves, forwhich no amount of\nsearchwillshowthatonemoveisbetterthananother.\nThis kind of reasoning about what computations to do is called metareasoning (rea-\nMETAREASONING\nsoning about reasoning). It applies not just to game playing but to any kind of reasoning\nat all. All computations are done in the service of trying to reach better decisions, all have\ncosts, andallhave somelikelihood ofresulting inacertain improvement indecision quality.\nAlpha\u2013beta incorporates thesimplest kindofmetareasoning, namely, atheorem totheeffect\nthatcertainbranches ofthetreecanbeignored withoutloss. Itispossible todomuchbetter.\nInChapter16,weseehowtheseideascanbemadepreciseandimplementable.\nFinally, let us reexamine the nature of search itself. Algorithms for heuristic search\nand for game playing generate sequences of concrete states, starting from the initial state\nand then applying an evaluation function. Clearly, this is not how humans play games. In\nchess,oneoftenhasaparticulargoalinmind\u2014forexample,trappingtheopponent\u2019squeen\u2014\nand can use this goal to selectively generate plausible plans for achieving it. This kind of\ngoal-directed reasoning or planning sometimes eliminates combinatorial search altogether.\nDavid Wilkins\u2019 (1980) PARADISE is the only program to have used goal-directed reasoning\nsuccessfully in chess: it was capable of solving some chess problems requiring an 18-move\ncombination. As yet there is no good understanding of how to combine the two kinds of\nalgorithms into a robust and efficient system, although Bridge Baron might be a step in the\nright direction. A fully integrated system would be a significant achievement not just for\ngame-playing research but also forAI research in general, because it would be a good basis\nforageneralintelligent agent.\n5.9 SUMMARY\nWe have looked at a variety of games to understand what optimal play means and to under-\nstandhowtoplaywellinpractice. Themostimportant ideasareasfollows:\n\u2022 A game can be defined by the initial state (how the board is set up), the legal actions\nin each state, the result of each action, a terminal test (which says when the game is\nover),andautilityfunctionthatapplies toterminalstates.\n\u2022 In two-player zero-sum games with perfect information, the minimax algorithm can\nselectoptimalmovesbyadepth-firstenumeration ofthegametree.\n\u2022 The alpha\u2013beta search algorithm computes the same optimal move as minimax, but\nachievesmuchgreaterefficiencybyeliminating subtreesthatareprovablyirrelevant.\n\u2022 Usually,itisnotfeasibletoconsiderthewholegametree(evenwithalpha\u2013beta),sowe 190 Chapter 5. Adversarial Search\nneed tocutthe search offatsomepoint andapply aheuristic evaluation function that\nestimatestheutilityofastate.\n\u2022 Manygameprogramsprecompute tablesofbestmovesintheopening andendgameso\nthattheycanlookupamoveratherthansearch.\n\u2022 Games of chance can be handled by an extension to the minimax algorithm that eval-\nuates a chance node by taking the average utility of all its children, weighted by the\nprobability ofeachchild.\n\u2022 Optimal play in games of imperfect information, such as Kriegspiel and bridge, re-\nquires reasoning about the current and future belief states of each player. A simple\napproximation can be obtained by averaging the value of an action over each possible\nconfiguration ofmissinginformation.\n\u2022 Programshavebestedevenchampionhumanplayers atgamessuchaschess,checkers,\nand Othello. Humans retain the edge in several games of imperfect information, such\nas poker, bridge, and Kriegspiel, and in games with very large branching factors and\nlittlegoodheuristicknowledge, suchasGo.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nThe early history of mechanical game playing was marred by numerous frauds. The most\nnotoriousofthesewasBaronWolfgangvonKempelen\u2019s(1734\u20131804)\u201cTheTurk,\u201dasupposed\nchess-playing automaton that defeated Napoleon before being exposed as amagician\u2019s trick\ncabinet housing a human chess expert (see Levitt, 2000). It played from 1769 to 1854. In\n1846, Charles Babbage (who had been fascinated by the Turk) appears to have contributed\nthe first serious discussion of the feasibility of computer chess and checkers (Morrison and\nMorrison,1961). Hedidnotunderstandtheexponentialcomplexityofsearchtrees,claiming\n\u201cthe combinations involved in the Analytical Engine enormously surpassed any required,\neven by the game of chess.\u201d Babbage also designed, but did not build, a special-purpose\nmachine for playing tic-tac-toe. The first true game-playing machine was built around 1890\nby theSpanish engineer Leonardo TorresyQuevedo. Itspecialized inthe \u201cKRK\u201d(king and\nrookvs.king)chessendgame,guaranteeing awinwithkingandrookfromanyposition.\nThe minimax algorithm is traced to a 1912 paper by Ernst Zermelo, the developer of\nmodernsettheory. Thepaperunfortunatelycontainedseveralerrorsanddidnotdescribemin-\nimaxcorrectly. Ontheotherhand,itdidlayouttheideasofretrogradeanalysisandproposed\n(but did not prove) what became known as Zermelo\u2019s theorem: that chess is determined\u2014\nWhitecan force awinorBlack canoritisadraw; wejust don\u2019t know which. Zermelosays\nthatshould weeventually know,\u201cChesswouldofcourse losethecharacterofagameatall.\u201d\nA solid foundation for game theory was developed in the seminal work Theory of Games\nandEconomicBehavior (vonNeumannandMorgenstern, 1944), whichincluded ananalysis\nshowingthatsomegamesrequirestrategiesthatarerandomized(orotherwiseunpredictable).\nSeeChapter17formoreinformation. Bibliographical andHistorical Notes 191\nJohn McCarthy conceived the idea of alpha\u2013beta search in 1956, although he did not\npublish it. TheNSSchess program (Newell etal.,1958) used asimplified version ofalpha\u2013\nbeta; itwasthe firstchess program todo so. Alpha\u2013beta pruning wasdescribed by Hartand\nEdwards(1961)andHartetal.(1972). Alpha\u2013betawasusedbythe\u201cKotok\u2013McCarthy\u201dchess\nprogram written by a student of John McCarthy (Kotok, 1962). Knuth and Moore (1975)\nproved the correctness ofalpha\u2013beta and analysed its timecomplexity. Pearl(1982b) shows\nalpha\u2013beta tobeasymptotically optimalamongallfixed-depthgame-treesearchalgorithms.\nSeveral attempts have been made to overcome the problems with the \u201cstandard ap-\nproach\u201d thatwereoutlined inSection5.8. Thefirstnonexhaustive heuristic searchalgorithm\n\u2217\nwithsome theoretical grounding wasprobably B (Berliner, 1979), which attempts tomain-\ntain interval bounds on the possible value of a node in the game tree rather than giving it\na single point-valued estimate. Leaf nodes are selected for expansion in an attempt to re-\n\u2217\nfine the top-level bounds until one move is \u201cclearly best.\u201d Palay (1985) extends the B idea\nusing probability distributions on values in place of intervals. David McAllester\u2019s (1988)\nconspiracy number search expands leaf nodes that, by changing their values, could cause\n\u2217\nthe program to prefer a new move at the root. MGSS (Russell and Wefald, 1989) uses the\ndecision-theoretic techniques of Chapter 16 to estimate the value of expanding each leaf in\nterms of the expected improvement in decision quality at the root. It outplayed an alpha\u2013\n\u2217\nbetaalgorithm atOthellodespite searching anorderofmagnitude fewernodes. TheMGSS\napproach is,inprinciple, applicable tothecontrolofanyformofdeliberation.\nAlpha\u2013beta search is in many ways the two-player analog of depth-first branch-and-\n\u2217 \u2217\nbound, which is dominated by A in the single-agent case. The SSS algorithm (Stockman,\n\u2217\n1979) can be viewed as a two-player A and never expands more nodes than alpha\u2013beta to\nreachthesamedecision. Thememoryrequirementsandcomputationaloverheadofthequeue\n\u2217\nmake SSS in its original form impractical, but a linear-space version has been developed\nfrom the RBFSalgorithm (Korf and Chickering, 1996). Plaat et al. (1996) developed a new\n\u2217\nviewofSSS asacombination ofalpha\u2013beta andtransposition tables, showinghowtoover-\ncome the drawbacks of the original algorithm and developing a new variant called MTD(f)\nthathasbeenadopted byanumberoftopprograms.\nD. F.Beal (1980) and Dana Nau (1980, 1983) studied the weaknesses of minimax ap-\npliedtoapproximate evaluations. Theyshowedthatundercertainassumptions aboutthedis-\ntributionofleafvaluesinthetree,minimaxingcanyieldvaluesattherootthatareactuallyless\nreliable than the direct use of the evaluation function itself. Pearl\u2019s book Heuristics (1984)\npartially explains this apparent paradox and analyzes many game-playing algorithms. Baum\nand Smith (1997) propose a probability-based replacement for minimax, showing that it re-\nsults in better choices in certain games. The expectiminimax algorithm was proposed by\nDonald Michie (1966). Bruce Ballard (1983) extended alpha\u2013beta pruning to cover trees\nwithchancenodesandHauk(2004)reexaminesthisworkandprovides empiricalresults.\nKoller and Pfeffer (1997) describe a system for completely solving partially observ-\nable games. The system is quite general, handling games whose optimal strategy requires\nrandomized moves and games that are more complex than those handled by any previous\nsystem. Still, it can\u2019t handle games as complex as poker, bridge, and Kriegspiel. Frank\net al. (1998) describe several variants of Monte Carlo search, including one where MIN has 192 Chapter 5. Adversarial Search\ncomplete information but MAX does not. Among deterministic, partially observable games,\nKriegspiel has received the most attention. Ferguson demonstrated hand-derived random-\nizedstrategiesforwinningKriegspielwithabishopandknight(1992)ortwobishops(1995)\nagainst a king. The first Kriegspiel programs concentrated on finding endgame checkmates\nand performed AND\u2013OR search in belief-state space (Sakuta and Iida, 2002; Bolognesi and\nCiancarini,2003). Incrementalbelief-statealgorithmsenabledmuchmorecomplexmidgame\ncheckmates to be found (Russell and Wolfe, 2005; Wolfe and Russell, 2007), but efficient\nstateestimation remainstheprimaryobstacletoeffective generalplay(Parker etal.,2005).\nChesswasoneofthefirsttasksundertakeninAI,withearlyefforts bymanyofthepio-\nneersofcomputing,including KonradZusein1945,NorbertWienerinhisbookCybernetics\n(1948), and Alan Turing in 1950 (see Turing et al., 1953). But it was Claude Shannon\u2019s\narticle Programming a Computer for Playing Chess (1950) that had the most complete set\nof ideas, describing a representation for board positions, an evaluation function, quiescence\nsearch, andsomeideasforselective (nonexhaustive) game-treesearch. Slater(1950)andthe\ncommentators onhisarticlealsoexplored thepossibilities forcomputerchessplay.\nD. G. Prinz (1952) completed a program that solved chess endgame problems but did\nnot play a full game. Stan Ulam and a group at the Los Alamos National Lab produced a\nprogram that played chess on a 6\u00d76 board with no bishops (Kister et al., 1957). It could\nsearch 4pliesdeepinabout12minutes. AlexBernstein wrote thefirstdocumented program\ntoplayafullgameofstandard chess(BernsteinandRoberts, 1958).5\nThefirstcomputerchessmatchfeaturedtheKotok\u2013McCarthyprogramfromMIT(Ko-\ntok, 1962) and the ITEP program written in the mid-1960s at Moscow\u2019s Institute of Theo-\nretical and Experimental Physics (Adelson-Velsky etal., 1970). Thisintercontinental match\nwasplayedbytelegraph. Itendedwitha3\u20131victoryfortheITEPprogramin1967. Thefirst\nchess program to compete successfully with humans was MIT\u2019s MACHACK-6 (Greenblatt\netal.,1967). ItsEloratingofapproximately 1400waswellabovethenovicelevelof1000.\nThe Fredkin Prize, established in 1980, offered awards for progressive milestones in\nchess play. The$5,000 prize forthe firstprogram toachieve a masterrating wentto BELLE\n(Condon and Thompson, 1982), which achieved a rating of2250. The$10,000 prize forthe\nfirst program to achieve a USCF (United States Chess Federation) rating of 2500 (near the\ngrandmaster level) was awarded to DEEP THOUGHT (Hsu et al., 1990) in 1989. The grand\nprize, $100,000, went to DEEP BLUE (Campbell et al., 2002; Hsu, 2004) for its landmark\nvictoryoverworldchampionGarryKasparovina1997exhibition match. Kasparovwrote:\nThedecisivegameofthematchwasGame2,whichleftascarinmymemory...wesaw\nsomethingthatwentwellbeyondourwildestexpectationsofhowwellacomputerwould\nbe able to foresee the long-termpositionalconsequencesof its decisions. The machine\nrefusedtomovetoapositionthathadadecisiveshort-termadvantage\u2014showingavery\nhumansenseofdanger.(Kasparov,1997)\nProbably the most complete description of a modern chess program is provided by Ernst\nHeinz (2000), whose DARKTHOUGHT program was the highest-ranked noncommercial PC\nprogram atthe1999worldchampionships.\n5 ARussianprogram,BESMmayhavepredatedBernstein\u2019sprogram. Bibliographical andHistorical Notes 193\n(a) (b)\nFigure5.15 Pioneersincomputerchess:(a)HerbertSimonandAllenNewell,developers\nof the NSS program (1958); (b) John McCarthy and the Kotok\u2013McCarthy program on an\nIBM7090(1967).\nIn recent years, chess programs are pulling ahead of even the world\u2019s best humans.\nIn 2004\u20132005 HYDRA defeated grand master Evgeny Vladimirov 3.5\u20130.5, world champion\nRuslanPonomariov2\u20130,andseventh-ranked MichaelAdams5.5\u20130.5. In2006, DEEP FRITZ\nbeat world champion Vladimir Kramnik 4\u20132, and in 2007 RYBKA defeated several grand\nmasters in games in which it gave odds (such as a pawn) to the human players. Asof 2009,\nthe highest Elorating everrecorded wasKasparov\u2019s 2851. HYDRA (Donninger and Lorenz,\n2004)isratedsomewherebetween2850and3000, basedmostly onitstrouncing ofMichael\nAdams. The RYBKA program is rated between 2900 and 3100, but this is based on a small\nnumberofgamesandisnotconsideredreliable. Ross(2004)showshowhumanplayershave\nlearnedtoexploitsomeoftheweaknessesofthecomputerprograms.\nCheckers was the first of the classic games fully played by a computer. Christopher\nStrachey (1952) wrote the first working program for checkers. Beginning in 1952, Arthur\nSamuel of IBM, working in his spare time, developed a checkers program that learned its\nown evaluation function by playing itself thousands of times (Samuel, 1959, 1967). We\ndescribe this idea in more detail in Chapter 21. Samuel\u2019s program began as a novice but\nafter only a few days\u2019 self-play had improved itself beyond Samuel\u2019s own level. In 1962 it\ndefeated Robert Nealy, a champion at \u201cblind checkers,\u201d through an error on his part. When\none considers that Samuel\u2019s computing equipment (an IBM 704) had 10,000 words of main\nmemory,magnetictapeforlong-termstorage,anda.000001GHzprocessor,thewinremains\nagreataccomplishment.\nThechallenge started bySamuelwastaken upbyJonathan SchaefferoftheUniversity\nof Alberta. His CHINOOK program came in second in the 1990 U.S. Open and earned the\nright to challenge forthe world championship. It then ran up against a problem, in the form\nof Marion Tinsley. Dr. Tinsley had been world champion for over 40 years, losing only\nthree gamesinallthat time. Inthefirstmatchagainst CHINOOK,Tinsleysuffered hisfourth 194 Chapter 5. Adversarial Search\nand fifth losses, but won the match 20.5\u201318.5. A rematch at the 1994 world championship\nended prematurely when Tinsley had to withdraw forhealth reasons. CHINOOK became the\nofficial world champion. Schaeffer kept on building on his database of endgames, and in\n2007\u201csolved\u201dcheckers(Schaeffer etal.,2007;Schaeffer,2008). Thishadbeenpredictedby\nRichard Bellman (1965). In the paper that introduced the dynamic programming approach\nto retrograde analysis, he wrote, \u201cIn checkers, the number of possible moves in any given\nsituation is so small that we can confidently expect a complete digital computer solution to\nthe problem of optimal play in this game.\u201d Bellman did not, however, fully appreciate the\nsize of the checkers game tree. There are about 500 quadrillion positions. After 18 years\nof computation on a cluster of 50 or more machines, Jonathan Schaeffer\u2019s team completed\nan endgame table forall checkers positions with 10 orfewerpieces: over39 trillion entries.\nFrom there, they were able to do forward alpha\u2013beta search to derive a policy that proves\nthat checkers is in fact a draw with best play by both sides. Note that this is an application\nof bidirectional search (Section 3.4.6). Building an endgame table forall ofcheckers would\nbe impractical: it would require a billion gigabytes of storage. Searching without any table\nwouldalsobeimpractical: thesearch treehasabout 847 positions, andwouldtakethousands\nof years to search with today\u2019s technology. Only a combination of clever search, endgame\ndata,andadropinthepriceofprocessors andmemorycouldsolvecheckers. Thus,checkers\njoins Qubic (Patashnik, 1980), Connect Four(Allis, 1988), and Nine-Men\u2019s Morris (Gasser,\n1998)asgamesthathavebeensolvedbycomputeranalysis.\nBackgammon,agameofchance,wasanalyzed mathematically byGerolamoCardano\n(1663), but only taken up for computer play in the late 1970s, first with the BKG pro-\ngram (Berliner, 1980b); it used a complex, manually constructed evaluation function and\nsearchedonlytodepth1. Itwasthefirstprogramtodefeatahumanworldchampionatama-\njorclassic game(Berliner, 1980a). Berlinerreadily acknowledged that BKGwasverylucky\nwiththedice. GerryTesauro\u2019s(1995) TD-GAMMON played consistently atworldchampion\nlevel. TheBGBLITZ program wasthewinnerofthe2008ComputerOlympiad.\nGoisadeterministic game,butthelargebranchingfactormakesitchalleging. Thekey\nissuesandearlyliteratureincomputerGoaresummarizedbyBouzyandCazenave(2001)and\nMu\u00a8ller (2002). Up to 1997 there were no competent Go programs. Now the best programs\nplay most of their moves at the master level; the only problem is that over the course of a\ngame they usually make at least one serious blunder that allows a strong opponent to win.\nWhereas alpha\u2013beta search reigns in most games, many recent Go programs have adopted\nMonteCarlomethodsbasedontheUCT(upperconfidence bounds ontrees)scheme(Kocsis\nand Szepesvari, 2006). The strongest Go program as of 2009 is Gelly and Silver\u2019s MOGO\n(WangandGelly,2007;GellyandSilver,2008). InAugust2008, MOGOscoredasurprising\nwin against top professional Myungwan Kim, albeit with MOGO receiving a handicap of\nnine stones (about the equivalent of a queen handicap in chess). Kim estimated MOGO\u2019s\nstrength at 2\u20133 dan, the low end of advanced amateur. For this match, MOGO was run on\nan 800-processor 15 teraflop supercomputer (1000 times Deep Blue). A few weeks later,\nMOGO,withonlyafive-stonehandicap, wonagainsta6-danprofessional. Inthe9\u00d79form\nof Go, MOGO is at approximately the 1-dan professional level. Rapid advances are likely\nas experimentation continues with new forms of Monte Carlo search. The Computer Go Exercises 195\nNewsletter,published bytheComputerGoAssociation, describes currentdevelopments.\nBridge: Smithetal.(1998) reportonhowtheirplanning-based program wonthe1998\ncomputerbridgechampionship,and(Ginsberg,2001)describeshowhisGIBprogram,based\non Monte Carlo simulation, wonthe following computer championship and did surprisingly\nwellagainsthumanplayersandstandard bookproblem sets. From2001\u20132007, thecomputer\nbridge championship was won five times by JACK and twice by WBRIDGE5. Neither has\nhadacademicarticlesexplainingtheirstructure,butboth arerumoredtousetheMonteCarlo\ntechnique, whichwasfirstproposed forbridgebyLevy(1989).\nScrabble: Agood description ofatopprogram, MAVEN, isgivenby itscreator, Brian\nSheppard (2002). Generating the highest-scoring move is described by Gordon (1994), and\nmodelingopponents iscoveredbyRichardsandAmir(2007).\nSoccer (Kitano et al., 1997b; Visser et al., 2008) and billiards (Lam and Greenspan,\n2008; Archibald et al., 2009) and other stochastic games with a continuous space of actions\narebeginning toattractattention inAI,bothinsimulation andwithphysicalrobotplayers.\nComputergamecompetitions occurannually, andpapersappearinavarietyofvenues.\nThe rather misleadingly named conference proceedings Heuristic Programming in Artificial\nIntelligence reportontheComputerOlympiads, whichincludeawidevarietyofgames. The\nGeneral GameCompetition (Love etal.,2006) tests programs that mustlearn toplay anun-\nknowngamegivenonlyalogicaldescription oftherulesofthegame. Therearealsoseveral\neditedcollections ofimportantpapersongame-playing research(Levy,1988a,1988b;Mars-\nland and Schaeffer, 1990). The International Computer Chess Association (ICCA), founded\nin 1977, publishes the ICGA Journal (formerly the ICCA Journal). Important papers have\nbeen published in the serial anthology Advances in Computer Chess, starting with Clarke\n(1977). Volume 134 of the journal Artificial Intelligence (2002) contains descriptions of\nstate-of-the-art programs forchess, Othello, Hex, shogi, Go, backgammon, poker, Scrabble,\nandothergames. Since1998,abiennial ComputersandGamesconference hasbeenheld.\nEXERCISES\n5.1 Suppose you have an oracle, OM(s), that correctly predicts the opponent\u2019s move in\nany state. Using this, formulate the definition of a game as a (single-agent) search problem.\nDescribeanalgorithm forfindingtheoptimalmove.\n5.2 Considertheproblem ofsolvingtwo8-puzzles.\na. Giveacompleteproblem formulation inthestyleofChapter 3.\nb. Howlargeisthereachable statespace? Giveanexactnumericalexpression.\nc. Suppose wemake the problem adversarial as follows: the two players take turns mov-\ning;acoinisflippedtodeterminethepuzzleonwhichtomakeamoveinthatturn;and\nthe winner is the first to solve one puzzle. Which algorithm can be used to choose a\nmoveinthissetting?\nd. Giveaninformalproofthatsomeonewilleventuallywinifbothplayperfectly. 196 Chapter 5. Adversarial Search\ne\n(a) a b c d\nP E\nf\nbd\ncd ad\n(b)\nce cf cc ae af ac\n? ? ? ? ?\nde df\ndd dd\nFigure5.16 (a)Amapwherethecostofeveryedgeis1.InitiallythepursuerP isatnode\nbandtheevaderE isatnoded. (b)Apartialgametreeforthismap. Eachnodeislabeled\nwiththeP,E positions.P movesfirst. Branchesmarked\u201c?\u201d haveyettobeexplored.\n5.3 Imagine that, inExercise 3.3, oneof thefriends wantstoavoid the other. Theproblem\nthen becomes a two-player pursuit\u2013evasion game. We assume now that the players take\nPURSUIT\u2013EVASION\nturns moving. The game ends only when the players are on the same node; the terminal\npayoff tothepursuer isminusthe totaltimetaken. (Theevader\u201cwins\u201d byneverlosing.) An\nexampleisshowninFigure5.16.\na. Copythegametreeandmarkthevaluesoftheterminalnodes.\nb. Next to each internal node, write the strongest fact you can infer about its value (a\nnumber,oneormoreinequalities suchas\u201c\u2265 14\u201d,ora\u201c?\u201d).\nc. Beneatheachquestion mark,writethenameofthenodereached bythatbranch.\nd. Explainhowaboundonthevalueofthenodesin(c)canbederivedfromconsideration\nofshortest-pathlengthsonthemap,andderivesuchboundsforthesenodes. Remember\nthecosttogettoeachleafaswellasthecosttosolveit.\ne. Nowsupposethatthetreeasgiven,withtheleafboundsfrom(d),isevaluatedfromleft\nto right. Circle those \u201c?\u201d nodes that would not need to be expanded further, given the\nboundsfrompart(d),andcrossoutthosethatneednotbeconsidered atall.\nf. Canyouproveanythingingeneralaboutwhowinsthegameonamapthatisatree? Exercises 197\n5.4 Describeandimplementstatedescriptions,movegenerators,terminaltests,utilityfunc-\ntions,andevaluationfunctionsforoneormoreofthefollowingstochasticgames: Monopoly,\nScrabble, bridgeplaywithagivencontract, orTexashold\u2019em poker.\n5.5 Describe and implement a real-time, multiplayer game-playing environment, where\ntimeispartoftheenvironment stateandplayersaregivenfixedtimeallocations.\n5.6 Discusshowwellthestandardapproachtogameplayingwouldapplytogamessuchas\ntennis, pool,andcroquet, whichtakeplaceinacontinuous physicalstatespace.\n5.7 Prove the following assertion: Forevery game tree, the utility obtained by MAX using\nminimaxdecisions against asuboptimal MIN willbeneverbelowerthan theutility obtained\nplaying against an optimal MIN. Can you come up with a game tree in which MAX can do\nstillbetterusingasuboptimal strategyagainstasuboptimal MIN?\nA B\n1 2 3 4\nFigure5.17 Thestartingpositionofasimplegame.PlayerAmovesfirst.Thetwoplayers\ntaketurnsmoving,andeachplayermustmovehistokentoanopenadjacentspaceineither\ndirection. If the opponent occupies an adjacent space, then a player may jump over the\nopponenttothenextopenspaceifany.(Forexample,ifAison3andBison2,thenAmay\nmovebackto1.) Thegameendswhenoneplayerreachestheoppositeendoftheboard. If\nplayer A reaches space 4 first, then the value of the game to A is +1; if player B reaches\nspace1first,thenthevalueofthegametoAis\u22121.\n5.8 Considerthetwo-playergamedescribed inFigure5.17.\na. Drawthecompletegametree,usingthefollowingconventions:\n\u2022 Writeeachstateas(s ,s ),wheres ands denotethetokenlocations.\nA B A B\n\u2022 Puteachterminalstateinasquareboxandwriteitsgamevalueinacircle.\n\u2022 Putloopstates(statesthatalreadyappearonthepathtotheroot)indouble square\nboxes. Sincetheirvalueisunclear, annotate eachwitha\u201c?\u201d inacircle.\nb. Nowmarkeachnodewithitsbacked-up minimaxvalue(alsoinacircle). Explainhow\nyouhandledthe\u201c?\u201d valuesandwhy.\nc. Explain why the standard minimax algorithm would fail on this game tree and briefly\nsketch how you might fixit, drawing on youranswerto(b). Does yourmodified algo-\nrithmgiveoptimaldecisions forallgameswithloops?\nd. This4-square game can be generalized to n squares forany n > 2. Prove that Awins\nifnisevenandlosesifnisodd.\n5.9 This problem exercises the basic concepts of game playing, using tic-tac-toe (noughts\nand crosses) as an example. We define X as the number of rows, columns, or diagonals\nn 198 Chapter 5. Adversarial Search\nwith exactly n X\u2019s and no O\u2019s. Similarly, O is the number of rows, columns, ordiagonals\nn\nwithjust nO\u2019s. Theutility function assigns +1toany position with X = 1 and \u22121toany\n3\nposition withO = 1. Allotherterminal positions have utility 0. Fornonterminal positions,\n3\nweusealinearevaluationfunctiondefinedasEval(s)= 3X (s)+X (s)\u2212(3O (s)+O (s)).\n2 1 2 1\na. Approximately howmanypossible gamesoftic-tac-toe arethere?\nb. Show the whole game tree starting from an empty board down to depth 2 (i.e., one X\nandoneOontheboard),takingsymmetryintoaccount.\nc. Markonyourtreetheevaluations ofallthepositions atdepth2.\nd. Usingtheminimaxalgorithm,markonyourtreethebacked-up valuesforthepositions\natdepths1and0,andusethosevaluestochoosethebeststartingmove.\ne. Circle the nodes at depth 2 that would not be evaluated if alpha\u2013beta pruning were\napplied, assumingthenodesaregenerated intheoptimalorderforalpha\u2013beta pruning.\n5.10 Considerthefamilyofgeneralized tic-tac-toe games,definedasfollows. Eachpartic-\nular game is specified by a set S of squares and a collection W of winning positions. Each\nwinningpositionisasubsetofS. Forexample,instandardtic-tac-toe, S isasetof9squares\nandW isacollectionof8subsetsofW: thethreerows,thethreecolumns,andthetwodiag-\nonals. Inotherrespects, thegameisidentical tostandard tic-tac-toe. Startingfromanempty\nboard, players alternate placing their marks on an empty square. A player who marks every\nsquare in a winning position wins the game. It is a tie if all squares are marked and neither\nplayerhaswon.\na. LetN = |S|, the number of squares. Give an upper bound on the number of nodes in\nthecompletegametreeforgeneralized tic-tac-toe asafunction ofN.\nb. Givealowerboundonthesizeofthegametreefortheworstcase,whereW ={}.\nc. Proposeaplausibleevaluationfunctionthatcanbeusedforanyinstanceofgeneralized\ntic-tac-toe. Thefunction maydependonS andW.\nd. Assume that it is possible to generate a new board and check whether it is a winning\nposition in 100N machine instructions and assume a 2 gigahertz processor. Ignore\nmemory limitations. Using your estimate in (a), roughly how large agame tree can be\ncompletelysolvedbyalpha\u2013beta inasecondofCPUtime? aminute? anhour?\n5.11 Developageneralgame-playing program,capableofplaying avarietyofgames.\na. Implement move generators and evaluation functions forone ormore of the following\ngames: Kalah,Othello, checkers, andchess.\nb. Constructageneralalpha\u2013beta game-playing agent.\nc. Compare theeffect ofincreasing search depth, improving moveordering, and improv-\ningtheevaluationfunction. Howclosedoesyoureffectivebranchingfactorcometothe\nidealcaseofperfectmoveordering?\nd. Implementaselectivesearchalgorithm,suchasB*(Berliner,1979),conspiracynumber\nsearch (McAllester, 1988), or MGSS* (Russell and Wefald, 1989) and compare its\nperformance toA*. Exercises 199\nn\n1\nn\n2\nn\nj\nFigure5.18 Situationwhenconsideringwhethertoprunenodenj.\n5.12 Describe how the minimax and alpha\u2013beta algorithms change for two-player, non-\nzero-sumgamesinwhicheachplayerhasadistinctutilityfunction andbothutilityfunctions\nareknowntobothplayers. Iftherearenoconstraintsonthetwoterminalutilities,isitpossible\nfor any node to be pruned by alpha\u2013beta? What if the player\u2019s utility functions on any state\ndifferbyatmostaconstant k,makingthegamealmostcooperative?\n5.13 Developaformalproofofcorrectness foralpha\u2013betapruning. Todothis,considerthe\nsituation shown in Figure 5.18. The question is whether to prune node n , which is a max-\nj\nnode and a descendant of node n . The basic idea is to prune it if and only if the minimax\n1\nvalueofn canbeshowntobeindependent ofthevalueofn .\n1 j\na. Moden takesontheminimumvalueamongitschildren: n =min(n ,n ,...,n ).\n1 1 2 21 2b2\nFindasimilarexpression forn andhenceanexpression forn intermsofn .\n2 1 j\nb. Letl betheminimum(ormaximum)valueofthenodestotheleftofnoden atdepthi,\ni i\nwhoseminimaxvalueisalreadyknown. Similarly,letr betheminimum(ormaximum)\ni\nvalueoftheunexplored nodes totherightof n atdepthi. Rewriteyourexpression for\ni\nn intermsofthel andr values.\n1 i i\nc. Nowreformulate the expression to show that in orderto affect n , n must not exceed\n1 j\nacertainboundderivedfromthe l values.\ni\nd. Repeattheprocessforthecasewhere n isamin-node.\nj\n5.14 Provethatalpha\u2013betapruningtakestimeO(2m\/2)withoptimalmoveordering,where\nmisthemaximumdepthofthegametree.\n5.15 Suppose you have a chess program that can evaluate 10 million nodes per second.\nDecideonacompactrepresentationofagamestateforstorageinatranspositiontable. About\nhow many entries can you fit in a 2-gigabyte in-memory table? Will that be enough for the 200 Chapter 5. Adversarial Search\n0.5 0.5 0.5 0.5\n2 2 1 2 0 2 -1 0\nFigure5.19 Thecompletegametreeforatrivialgamewithchancenodes.\nthree minutes of search allocated forone move? Howmany table lookups can you do in the\ntime it would take to do one evaluation? Now suppose the transposition table is stored on\ndisk. Abouthowmanyevaluations couldyoudointhetimeittakes todoonediskseekwith\nstandard diskhardware?\n5.16 This question considers pruning in games with chance nodes. Figure 5.19 shows the\ncompletegametreeforatrivialgame. Assumethattheleafnodesaretobeevaluated inleft-\nto-rightorder,andthatbeforealeafnodeisevaluated, weknownothingaboutitsvalue\u2014the\nrangeofpossible valuesis\u2212\u221eto\u221e.\na. Copythe figure, mark thevalue ofall theinternal nodes, and indicate the best moveat\ntherootwithanarrow.\nb. Given the values of the first six leaves, do we need to evaluate the seventh and eighth\nleaves? Given the values of the first seven leaves, do we need to evaluate the eighth\nleaf? Explainyouranswers.\nc. Suppose the leaf node values are known to lie between \u20132 and 2 inclusive. After the\nfirsttwoleavesareevaluated, whatisthevaluerangeforthe left-hand chancenode?\nd. Circlealltheleavesthatneednotbeevaluatedundertheassumption in(c).\n5.17 Implement the expectiminimax algorithm and the *-alpha\u2013beta algorithm, which is\ndescribed byBallard(1983), forpruning gametreeswithchance nodes. Trythemonagame\nsuchasbackgammonandmeasurethepruning effectiveness of *-alpha\u2013beta.\n5.18 Prove that with a positive linear transformation of leaf values (i.e., transforming a\nvaluextoax+bwherea > 0),thechoiceofmoveremainsunchanged inagametree,even\nwhentherearechancenodes.\n5.19 Considerthefollowingprocedure forchoosing movesingameswithchance nodes:\n\u2022 Generatesomedice-roll sequences (say,50)downtoasuitabledepth(say,8).\n\u2022 With known dice rolls, the game tree becomes deterministic. For each dice-roll se-\nquence,solvetheresultingdeterministic gametreeusingalpha\u2013beta. Exercises 201\n\u2022 Usetheresults toestimatethevalueofeachmoveandtochoosethebest.\nWillthisprocedure workwell? Why(orwhynot)?\n5.20 In the following, a \u201cmax\u201d tree consists only of max nodes, whereas an \u201cexpectimax\u201d\ntree consists of a max node at the root with alternating layers of chance and max nodes. At\nchance nodes, alloutcome probabilities arenonzero. Thegoal istofindthevalue oftheroot\nwithabounded-depth search. Foreachof(a)\u2013(f), eithergiveanexampleorexplainwhythis\nisimpossible.\na. Assuming that leaf values are finite but unbounded, is pruning (as in alpha\u2013beta) ever\npossibleinamaxtree?\nb. Ispruningeverpossible inanexpectimaxtreeunderthesameconditions?\nc. If leaf values are all nonnegative, is pruning ever possible in a max tree? Give an\nexample,orexplainwhynot.\nd. Ifleaf values are allnonnegative, ispruning everpossible inanexpectimax tree? Give\nanexample,orexplainwhynot.\ne. Ifleaf values areallintherange [0,1], ispruning everpossible inamaxtree? Givean\nexample,orexplainwhynot.\nf. Ifleafvaluesareallintherange [0,1],ispruning everpossibleinanexpectimax tree?\ng. Considertheoutcomesofachancenodeinanexpectimaxtree. Whichofthefollowing\nevaluationordersismostlikelytoyieldpruning opportunities?\n(i) Lowestprobability first\n(ii) Highestprobability first\n(iii) Doesn\u2019tmakeanydifference\n5.21 Whichofthefollowingaretrueandwhicharefalse? Givebriefexplanations.\na. Inafully observable, turn-taking, zero-sum gamebetween twoperfectly rational play-\ners, itdoesnot helpthe firstplayer toknowwhatstrategy the second playerisusing\u2014\nthatis,whatmovethesecondplayerwillmake,giventhefirstplayer\u2019smove.\nb. In a partially observable, turn-taking, zero-sum game between two perfectly rational\nplayers, it does not help the first player to know what move the second player will\nmake,giventhefirstplayer\u2019s move.\nc. Aperfectly rationalbackgammonagentneverloses.\n5.22 Considercarefullytheinterplayofchanceeventsandpartialinformationineachofthe\ngamesinExercise5.4.\na. Forwhichisthestandardexpectiminimaxmodelappropriate? Implementthealgorithm\nand run it in your game-playing agent, with appropriate modifications to the game-\nplayingenvironment.\nb. Forwhichwouldtheschemedescribed inExercise5.19beappropriate?\nc. Discusshowyoumightdealwiththefactthatinsomeofthegames,theplayers donot\nhavethesameknowledgeofthecurrentstate. 6\nCONSTRAINT\nSATISFACTION PROBLEMS\nInwhichweseehowtreatingstatesasmorethanjustlittleblackboxesleadstothe\ninventionofarangeofpowerfulnewsearchmethodsandadeeperunderstanding\nofproblem structureandcomplexity.\nChapters 3 and 4 explored the idea that problems can be solved by searching in a space of\nstates. Thesestatescanbeevaluated bydomain-specific heuristics andtestedtoseewhether\nthey are goal states. From the point of view of the search algorithm, however, each state is\natomic,orindivisible\u2014a blackboxwithnointernalstructure.\nThis chapter describes away to solve a wide variety of problems more efficiently. We\nuse a factored representation for each state: a set of variables, each of which has a value.\nA problem is solved when each variable has a value that satisfies all the constraints on the\nCONSTRAINT\nvariable. Aproblem described thiswayiscalleda constraintsatisfaction problem,orCSP.\nSATISFACTION\nPROBLEM\nCSPsearchalgorithmstakeadvantageofthestructureofstatesandusegeneral-purpose\nratherthanproblem-specific heuristicstoenablethesolutionofcomplexproblems. Themain\nideaistoeliminatelargeportionsofthesearchspaceallatoncebyidentifyingvariable\/value\ncombinations thatviolatetheconstraints.\n6.1 DEFINING CONSTRAINT SATISFACTION PROBLEMS\nAconstraint satisfaction problemconsists ofthreecomponents, X,D,andC:\nX isasetofvariables, {X ,...,X }.\n1 n\nDisasetofdomains,{D ,...,D },oneforeachvariable.\n1 n\nC isasetofconstraints thatspecifyallowablecombinations ofvalues.\nEach domain D consists of a set of allowable values, {v ,...,v } for variable X . Each\ni 1 k i\nconstraintC consistsofapair(cid:16)scope,rel(cid:17),wherescope isatupleofvariablesthatparticipate\ni\nintheconstraintandrel isarelationthatdefinesthevaluesthatthosevariablescantakeon. A\nrelationcanberepresented asanexplicitlistofalltuples ofvaluesthatsatisfytheconstraint,\nor as an abstract relation that supports two operations: testing if a tuple is a member of the\nrelation andenumerating themembersoftherelation. Forexample, ifX and X bothhave\n1 2\n202 Section6.1. DefiningConstraintSatisfaction Problems 203\nthe domain {A,B}, then the constraint saying the two variables must have different values\ncanbewrittenas(cid:16)(X ,X ),[(A,B),(B,A)](cid:17) oras(cid:16)(X ,X ),X (cid:7)= X (cid:17).\n1 2 1 2 1 2\nTo solve a CSP, we need to define a state space and the notion of a solution. Each\nstate in a CSP is defined by an assignment of values to some or all of the variables, {X =\nASSIGNMENT i\nv ,X = v ,...}. Anassignment that does not violate anyconstraints iscalled aconsistent\nCONSISTENT i j j\nCOMPLETE orlegalassignment. Acompleteassignmentisoneinwhicheveryvariable isassigned, and\nASSIGNMENT\na solution to a CSP is a consistent, complete assignment. A partial assignment is one that\nSOLUTION\nPARTIAL assignsvaluestoonlysomeofthevariables.\nASSIGNMENT\n6.1.1 Exampleproblem: Mapcoloring\nSuppose that, having tired of Romania, we are looking at a map of Australia showing each\nof its states and territories (Figure 6.1(a)). We are given the task of coloring each region\neither red, green, orblue in such away that no neighboring regions have the samecolor. To\nformulatethisasaCSP,wedefinethevariables tobetheregions\nX = {WA,NT,Q,NSW,V,SA,T}.\nThe domain of each variable is the set D = {red,green,blue}. The constraints require\ni\nneighboring regionstohavedistinctcolors. Sincetherearenineplaceswhereregionsborder,\ntherearenineconstraints:\nC = {SA (cid:7)= WA,SA (cid:7)= NT,SA (cid:7)= Q,SA (cid:7)= NSW,SA (cid:7)= V,\nWA (cid:7)= NT,NT (cid:7)= Q,Q(cid:7)= NSW,NSW (cid:7)= V}.\nHereweareusingabbreviations; SA (cid:7)= WAisashortcutfor(cid:16)(SA,WA),SA (cid:7)= WA(cid:17),where\nSA (cid:7)= WAcanbefullyenumerated inturnas\n{(red,green),(red,blue),(green,red),(green,blue),(blue,red),(blue,green)}.\nTherearemanypossible solutions tothisproblem, suchas\n{WA=red,NT =green,Q=red,NSW =green,V =red,SA=blue,T =red }.\nIt can be helpful to visualize a CSP as a constraint graph, as shown in Figure 6.1(b). The\nCONSTRAINTGRAPH\nnodes ofthegraph correspond tovariables oftheproblem, andalink connects anytwovari-\nablesthatparticipate inaconstraint.\nWhy formulate a problem as a CSP? One reason is that the CSPs yield a natural rep-\nresentation for a wide variety of problems; if you already have a CSP-solving system, it is\nofteneasiertosolveaproblemusingitthantodesignacustomsolutionusinganothersearch\ntechnique. Inaddition, CSPsolverscanbefasterthanstate-space searchers because theCSP\nsolver canquickly eliminate large swatches ofthe search space. Forexample, once wehave\nchosen{SA=blue}intheAustraliaproblem,wecanconcludethatnoneofthefiveneighbor-\ningvariablescantakeonthevalueblue. Withouttakingadvantageofconstraintpropagation,\na search procedure would have to consider 35=243 assignments for the five neighboring\nvariables; withconstraint propagation weneverhavetoconsider blue asavalue, sowehave\nonly25=32assignments tolookat,areduction of87%.\nIn regular state-space search we can only ask: is this specific state a goal? No? What\naboutthisone? WithCSPs,oncewefindoutthatapartialassignmentisnotasolution,wecan 204 Chapter 6. ConstraintSatisfaction Problems\nNT\nQ\nWA\nNorthern\nTerritory\nQueensland\nWestern SA NSW\nAustralia\nSouth\nAustralia New\nSouth V\nWales\nVictoria\nT\nTasmania\n(a) (b)\nFigure 6.1 (a) The principal states and territories of Australia. Coloring this map can\nbe viewed as a constraint satisfaction problem (CSP). The goal is to assign colors to each\nregionso that no neighboringregionshave the same color. (b) The map-coloringproblem\nrepresentedasaconstraintgraph.\nimmediately discard further refinements of the partial assignment. Furthermore, we can see\nwhytheassignmentisnotasolution\u2014weseewhichvariablesviolateaconstraint\u2014sowecan\nfocus attention on the variables that matter. As a result, many problems that are intractable\nforregularstate-space searchcanbesolvedquickly whenformulated asaCSP.\n6.1.2 Exampleproblem: Job-shopscheduling\nFactorieshavetheproblemofschedulingaday\u2019sworthofjobs,subjecttovariousconstraints.\nInpractice,manyoftheseproblemsaresolvedwithCSPtechniques. Considertheproblemof\nschedulingtheassemblyofacar. Thewholejobiscomposedoftasks,andwecanmodeleach\ntask as avariable, where the value of each variable isthe time that the task starts, expressed\nas an integer number of minutes. Constraints can assert that one task must occur before\nanother\u2014for example, awheel must beinstalled before the hubcap isput on\u2014and that only\nso many tasks can go on at once. Constraints can also specify that a task takes a certain\namountoftimetocomplete.\nWeconsiderasmallpartofthecarassembly, consisting of15 tasks: installaxles(front\nand back), affix all four wheels (right and left, front and back), tighten nuts for each wheel,\naffixhubcaps, andinspect thefinalassembly. Wecanrepresent thetaskswith15variables:\nX = {Axle ,Axle ,Wheel ,Wheel ,Wheel ,Wheel ,Nuts ,\nF B RF LF RB LB RF\nNuts ,Nuts ,Nuts ,Cap ,Cap ,Cap ,Cap ,Inspect}.\nLF RB LB RF LF RB LB\nThe value of each variable is the time that the task starts. Next we represent precedence\nPRECEDENCE constraints between individual tasks. Whenever a task T must occur before task T , and\nCONSTRAINTS 1 2\ntaskT takesduration d tocomplete,weaddanarithmeticconstraint oftheform\n1 1\nT +d \u2264 T .\n1 1 2 Section6.1. DefiningConstraintSatisfaction Problems 205\nIn our example, the axles have to be in place before the wheels are put on, and it takes 10\nminutestoinstallanaxle,sowewrite\nAxle +10 \u2264 Wheel ; Axle +10 \u2264Wheel ;\nF RF F LF\nAxle +10 \u2264 Wheel ; Axle +10 \u2264 Wheel .\nB RB B LB\nNextwesaythat,foreachwheel,wemustaffixthewheel(whichtakes1minute),thentighten\nthenuts(2minutes),andfinallyattachthehubcap(1minute,butnotrepresented yet):\nWheel +1 \u2264 Nuts ; Nuts +2 \u2264 Cap ;\nRF RF RF RF\nWheel +1\u2264 Nuts ; Nuts +2 \u2264 Cap ;\nLF LF LF LF\nWheel +1 \u2264 Nuts ; Nuts +2 \u2264 Cap ;\nRB RB RB RB\nWheel +1 \u2264Nuts ; Nuts +2\u2264 Cap .\nLB LB LB LB\nSupposewehavefourworkerstoinstallwheels,buttheyhavetoshareonetoolthathelpsput\nDISJUNCTIVE the axle in place. We need a disjunctive constraint to say that Axle and Axle must not\nCONSTRAINT F B\noverlapintime;eitheronecomesfirstortheotherdoes:\n(Axle +10 \u2264 Axle ) or (Axle +10 \u2264 Axle ).\nF B B F\nThis looks like a more complicated constraint, combining arithmetic and logic. But it still\nreducestoasetofpairsofvaluesthat Axle andAxle cantakeon.\nF F\nWe also need to assert that the inspection comes last and takes 3 minutes. For every\nvariableexceptInspect weaddaconstraintoftheform X+d \u2264 Inspect. Finally,suppose\nX\nthereisarequirement togetthewholeassembly donein30minutes. Wecanachievethatby\nlimitingthedomainofallvariables:\nD = {1,2,3,...,27}.\ni\nThis particular problem is trivial to solve, but CSPs have been applied to job-shop schedul-\ning problems like this with thousands of variables. In some cases, there are complicated\nconstraints that are difficult to specify in the CSP formalism, and more advanced planning\ntechniques areused,asdiscussed inChapter11.\n6.1.3 Variationsonthe CSP formalism\nThe simplest kind of CSP involves variables that have discrete, finite domains. Map-\nDISCRETEDOMAIN\ncoloring problems andscheduling withtimelimitsarebothofthiskind. The8-queens prob-\nFINITEDOMAIN\nlem described in Chapter 3 can also be viewed as a finite-domain CSP, where the variables\nQ ,...,Q are the positions of each queen in columns 1,...,8 and each variable has the\n1 8\ndomainD = {1,2,3,4,5,6,7,8}.\ni\nAdiscretedomaincanbeinfinite,suchasthesetofintegersorstrings. (Ifwedidn\u2019tput\nINFINITE\na deadline on the job-scheduling problem, there would be an infinite number of start times\nfor each variable.) With infinite domains, it is no longer possible to describe constraints by\nCONSTRAINT enumerating all allowed combinations of values. Instead, a constraint language must be\nLANGUAGE\nused that understands constraints such as T + d \u2264 T directly, without enumerating the\n1 1 2\nset of pairs of allowable values for (T ,T ). Special solution algorithms (which we do not\n1 2\nLINEAR discuss here) exist for linear constraints on integer variables\u2014that is, constraints, such as\nCONSTRAINTS\nthe one just given, in which each variable appears only in linear form. It can be shown that\nNONLINEAR noalgorithm existsforsolving general nonlinearconstraintsonintegervariables.\nCONSTRAINTS 206 Chapter 6. ConstraintSatisfaction Problems\nCONTINUOUS Constraint satisfaction problems with continuous domains are common in the real\nDOMAINS\nworldandarewidelystudiedinthefieldofoperations research. Forexample,thescheduling\nofexperiments on theHubble Space Telescope requires very precise timingofobservations;\nthe start and finish of each observation and maneuver are continuous-valued variables that\nmust obey a variety of astronomical, precedence, and power constraints. The best-known\ncategory of continuous-domain CSPs is that of linear programming problems, where con-\nstraintsmustbelinearequalitiesorinequalities. Linearprogrammingproblemscanbesolved\nin time polynomial in the number of variables. Problems with different types of constraints\nandobjectivefunctions havealsobeenstudied\u2014quadratic programming, second-order conic\nprogramming, andsoon.\nIn addition to examining the types of variables that can appear in CSPs, it is useful to\nlook at the types of constraints. The simplest type is the unary constraint, which restricts\nUNARYCONSTRAINT\nthevalueofasinglevariable. Forexample,inthemap-coloring problem itcouldbethecase\nthat South Australians won\u2019t tolerate the color green; we can express that with the unary\nconstraint (cid:16)(SA),SA (cid:7)= green(cid:17)\nA binary constraint relates two variables. For example, SA (cid:7)= NSW is a binary\nBINARYCONSTRAINT\nconstraint. A binary CSP is one with only binary constraints; it can be represented as a\nconstraint graph, asinFigure6.1(b).\nWe can also describe higher-order constraints, such as asserting that the value of Y is\nbetweenX andZ,withtheternaryconstraint Between(X,Y,Z).\nGLOBAL A constraint involving an arbitrary number of variables is called a global constraint.\nCONSTRAINT\n(Thenameistraditional butconfusing becauseitneednotinvolveallthevariablesinaprob-\nlem). One of the most common global constraints is Alldiff, which says that all of the\nvariables involved in the constraint must have different values. In Sudoku problems (see\nSection 6.2.6), all variables in a row or column must satisfy an Alldiff constraint. An-\nother example isprovided by cryptarithmetic puzzles. (See Figure 6.2(a).) Each letter in a\nCRYPTARITHMETIC\ncryptarithmetic puzzle represents a different digit. For the case in Figure 6.2(a), this would\nbe represented as the global constraint Alldiff(F,T,U,W,R,O). The addition constraints\nonthefourcolumnsofthepuzzlecanbewrittenasthefollowingn-aryconstraints:\nO+O = R+10\u00b7C\n10\nC +W +W = U +10\u00b7C\n10 100\nC +T +T = O+10\u00b7C\n100 1000\nC = F ,\n1000\nwhereC ,C ,andC areauxiliaryvariablesrepresentingthedigitcarriedoverintothe\n10 100 1000\ntens, hundreds, or thousands column. These constraints can be represented in a constraint\nCONSTRAINT hypergraph,suchastheoneshowninFigure6.2(b). Ahypergraphconsistsofordinarynodes\nHYPERGRAPH\n(thecirclesinthefigure)andhypernodes (thesquares), whichrepresent n-aryconstraints.\nAlternatively, as Exercise 6.6 asks you to prove, every finite-domain constraint can be\nreducedtoasetofbinaryconstraintsifenoughauxiliaryvariablesareintroduced,sowecould\ntransform anyCSPinto one withonly binary constraints; thismakes the algorithms simpler.\nAnotherwaytoconvertann-aryCSPtoabinaryoneisthedualgraphtransformation: create\nDUALGRAPH\nanewgraphinwhichtherewillbeonevariableforeachconstraint intheoriginal graph, and Section6.1. DefiningConstraintSatisfaction Problems 207\nT W O F T U W R O\n+ T W O\nF O U R\nC C C\n3 2 1\n(a) (b)\nFigure6.2 (a)Acryptarithmeticproblem.Eachletterstandsforadistinctdigit;theaimis\ntofindasubstitutionofdigitsforletterssuchthattheresultingsumisarithmeticallycorrect,\nwiththeaddedrestrictionthatnoleadingzeroesareallowed. (b)Theconstrainthypergraph\nfor the cryptarithmetic problem, showing the Alldiff constraint (square box at the top) as\nwellasthecolumnadditionconstraints(foursquareboxesinthemiddle). ThevariablesC ,\n1\nC ,andC representthecarrydigitsforthethreecolumns.\n2 3\nonebinaryconstraintforeachpairofconstraintsintheoriginalgraphthatsharevariables. For\nexample, if the original graph has variables {X,Y,Z} and constraints (cid:16)(X,Y,Z),C (cid:17) and\n1\n(cid:16)(X,Y),C (cid:17) then the dual graph would have variables {C ,C } with the binary constraint\n2 1 2\n(cid:16)(X,Y),R (cid:17),where(X,Y)arethesharedvariablesand R isanewrelationthatdefinesthe\n1 1\nconstraint betweenthesharedvariables, asspecifiedbythe original C andC .\n1 2\nTherearehowevertworeasonswhywemightpreferaglobalconstraintsuchasAlldiff\nrather than a set of binary constraints. First, it is easier and less error-prone to write the\nproblem description using Alldiff. Second,itispossible todesignspecial-purpose inference\nalgorithmsforglobalconstraintsthatarenotavailableforasetofmoreprimitiveconstraints.\nWedescribetheseinference algorithmsinSection6.2.5.\nTheconstraintswehavedescribedsofarhaveallbeenabsoluteconstraints, violationof\nPREFERENCE which rules out apotential solution. Many real-world CSPsinclude preference constraints\nCONSTRAINTS\nindicating whichsolutions arepreferred. Forexample,ina university class-scheduling prob-\nlem there are absolute constraints that no professor can teach two classes at the same time.\nButwealsomayallowpreference constraints: Prof.Rmightpreferteaching inthemorning,\nwhereas Prof. N prefers teaching in the afternoon. A schedule that has Prof. R teaching at\n2p.m.wouldstillbeanallowablesolution(unlessProf.Rhappenstobethedepartmentchair)\nbutwouldnotbeanoptimalone. Preference constraints canoftenbeencoded ascostsonin-\ndividual variable assignments\u2014for example, assigning an afternoon slot for Prof. R costs\n2 points against the overall objective function, whereas a morning slot costs 1. With this\nformulation, CSPs with preferences can be solved with optimization search methods, either\nCONSTRAINT\npath-based or local. We call such a problem a constraint optimization problem, or COP.\nOPTIMIZATION\nPROBLEM\nLinearprogramming problemsdothiskindofoptimization. 208 Chapter 6. ConstraintSatisfaction Problems\n6.2 CONSTRAINT PROPAGATION: INFERENCE IN CSPS\nInregular state-space search, analgorithm can doonly one thing: search. In CSPsthere is a\nchoice: analgorithmcansearch(chooseanewvariableassignmentfromseveralpossibilities)\nor do a specific type of inference called constraint propagation: using the constraints to\nINFERENCE\nCONSTRAINT reduce the number of legal values for a variable, which in turn can reduce the legal values\nPROPAGATION\nforanothervariable, andsoon. Constraint propagation may beintertwined withsearch, orit\nmaybedoneasapreprocessing step, before searchstarts. Sometimesthispreprocessing can\nsolvethewholeproblem,sonosearchisrequired atall.\nLOCAL The key idea is local consistency. If we treat each variable as a node in a graph (see\nCONSISTENCY\nFigure 6.1(b)) and each binary constraint as an arc, then the process of enforcing local con-\nsistency ineach part of the graph causes inconsistent values tobe eliminated throughout the\ngraph. Therearedifferent typesoflocalconsistency, whichwenowcoverinturn.\n6.2.1 Nodeconsistency\nA single variable (corresponding to a node in the CSP network) is node-consistent if all\nNODECONSISTENCY\nthe values in the variable\u2019s domain satisfy the variable\u2019s unary constraints. For example,\nin the variant of the Australia map-coloring problem (Figure 6.1) where South Australians\ndislike green, the variable SA starts with domain {red,green,blue}, and we can make it\nnode consistent byeliminating green,leaving SAwiththe reduced domain {red,blue}. We\nsaythatanetworkisnode-consistent ifeveryvariableinthenetworkisnode-consistent.\nIt is always possible to eliminate all the unary constraints in a CSP by running node\nconsistency. It is also possible to transform all n-ary constraints into binary ones (see Ex-\nercise 6.6). Because of this, it is common to define CSP solvers that work with only binary\nconstraints; wemakethatassumption fortherestofthischapter, exceptwherenoted.\n6.2.2 Arcconsistency\nA variable in a CSP is arc-consistent if every value in its domain satisfies the variable\u2019s\nARCCONSISTENCY\nbinaryconstraints. Moreformally, X isarc-consistent withrespecttoanothervariable X if\ni j\nforevery value in the current domain D there is some value in the domain D that satisfies\ni j\nthebinary constraint onthearc (X ,X ). Anetworkisarc-consistent ifeveryvariable isarc\ni j\nconsistentwitheveryothervariable. Forexample,considertheconstraint Y =X2 wherethe\ndomainofbothX andY isthesetofdigits. Wecanwritethisconstraint explicitly as\n(cid:16)(X,Y),{(0,0),(1,1),(2,4),(3,9))}(cid:17) .\nTo make X arc-consistent with respect to Y, we reduce X\u2019s domain to {0,1,2,3}. If we\nalsomakeY arc-consistent withrespect to X,thenY\u2019sdomainbecomes {0,1,4,9} andthe\nwholeCSPisarc-consistent.\nOntheotherhand,arcconsistency candonothingfortheAustraliamap-coloring prob-\nlem. Considerthefollowinginequality constraint on(SA,WA):\n{(red,green),(red,blue),(green,red),(green,blue),(blue,red),(blue,green)}. Section6.2. ConstraintPropagation: InferenceinCSPs 209\nfunctionAC-3(csp)returnsfalseifaninconsistencyisfoundandtrueotherwise\ninputs:csp,abinaryCSPwithcomponents(X, D, C)\nlocalvariables: queue,aqueueofarcs,initiallyallthearcsincsp\nwhilequeue isnotemptydo\n(Xi, Xj)\u2190REMOVE-FIRST(queue)\nifREVISE(csp,Xi, Xj)then\nifsizeofDi = 0thenreturnfalse\nforeachXk inXi.NEIGHBORS-{Xj}do\nadd(Xk, Xi)toqueue\nreturntrue\nfunctionREVISE(csp,Xi, Xj)returnstrueiffwerevisethedomainofXi\nrevised\u2190false\nforeachx inDido\nifnovalueyinDj allows(x,y)tosatisfytheconstraintbetweenXiandXj then\ndeletex fromDi\nrevised\u2190true\nreturnrevised\nFigure 6.3 The arc-consistency algorithm AC-3. After applying AC-3, either every arc\nisarc-consistent,orsomevariablehasanemptydomain,indicatingthattheCSP cannotbe\nsolved. Thename\u201cAC-3\u201dwasusedbythealgorithm\u2019sinventor(Mackworth,1977)because\nit\u2019sthethirdversiondevelopedinthepaper.\nNo matter what value you choose for SA (or for WA), there is a valid value for the other\nvariable. Soapplying arcconsistency hasnoeffectonthedomainsofeithervariable.\nThe most popular algorithm for arc consistency is called AC-3 (see Figure 6.3). To\nmakeeveryvariablearc-consistent, theAC-3algorithmmaintainsaqueueofarcstoconsider.\n(Actually, theorderofconsideration isnotimportant, sothedatastructure isreallyaset, but\ntraditioncallsitaqueue.) Initially,thequeuecontainsallthearcsintheCSP.AC-3thenpops\noffanarbitraryarc (X ,X )fromthequeueandmakesX arc-consistent withrespecttoX .\ni j i j\nIf this leaves D unchanged, the algorithm just moves on to the next arc. But if this revises\ni\nD (makes the domain smaller), then we add to the queue all arcs (X ,X ) where X is a\ni k i k\nneighborofX . WeneedtodothatbecausethechangeinD mightenablefurtherreductions\ni i\nin the domains of D , even if we have previously considered X . If D is revised down to\nk k i\nnothing, thenweknowthewholeCSPhasnoconsistentsolution, andAC-3canimmediately\nreturn failure. Otherwise, we keep checking, trying to remove values from the domains of\nvariables until no more arcs are in the queue. At that point, we are left with a CSP that is\nequivalent to the original CSP\u2014they both have the same solutions\u2014but the arc-consistent\nCSPwillinmostcasesbefastertosearchbecauseitsvariables havesmallerdomains.\nThe complexity of AC-3 can be analyzed as follows. Assume a CSP with n variables,\neachwithdomainsizeatmostd,andwithcbinaryconstraints (arcs). Eacharc (X ,X )can\nk i\nbe inserted in the queue only d times because X has at most d values to delete. Checking\ni 210 Chapter 6. ConstraintSatisfaction Problems\nconsistency ofanarccanbedoneinO(d2)time,sowegetO(cd3)totalworst-casetime.1\nIt is possible to extend the notion of arc consistency to handle n-ary rather than just\nbinary constraints; this is called generalized arc consistency or sometimes hyperarc consis-\nGENERALIZEDARC tency, depending onthe author. A variable X is generalized arc consistent with respect to\nCONSISTENT i\nann-aryconstraint ifforeveryvalue v inthedomainofX thereexistsatupleofvaluesthat\ni\nisamemberoftheconstraint, hasallitsvalues takenfromthedomains ofthecorresponding\nvariables, and has its X component equal to v. For example, if all variables have the do-\ni\nmain {0,1,2,3}, then to make the variable X consistent with the constraint X < Y < Z,\nwewould have to eliminate 2 and 3from the domain of X because the constraint cannot be\nsatisfiedwhenX is2or3.\n6.2.3 Pathconsistency\nArc consistency can go a long way toward reducing the domains of variables, sometimes\nfinding a solution (by reducing every domain to size 1) and sometimes finding that the CSP\ncannotbesolved(byreducingsomedomaintosize0). Butforothernetworks,arcconsistency\nfails to make enough inferences. Consider the map-coloring problem on Australia, but with\nonlytwocolorsallowed,redandblue. Arcconsistencycandonothingbecauseeveryvariable\nisalreadyarcconsistent: eachcanberedwithblueattheotherendofthearc(orviceversa).\nButclearlythereisnosolutiontotheproblem: becauseWesternAustralia,NorthernTerritory\nandSouthAustraliaalltoucheachother,weneedatleastthreecolorsforthemalone.\nArc consistency tightens down the domains (unary constraints) using the arcs (binary\nconstraints). Tomakeprogress onproblems like mapcoloring, weneed astronger notion of\nconsistency. Path consistency tightens the binary constraints by using implicit constraints\nPATHCONSISTENCY\nthatareinferredbylookingattriplesofvariables.\nA two-variable set {X ,X } is path-consistent with respect to a third variable X if,\ni j m\nforeveryassignment {X = a,X = b}consistent withtheconstraints on{X ,X },thereis\ni j i j\nanassignmenttoX thatsatisfiestheconstraintson{X ,X }and{X ,X }. Thisiscalled\nm i m m j\npath consistency because one can think of it aslooking at apath from X to X withX in\ni j m\nthemiddle.\nLet\u2019sseehowpathconsistency faresincoloring theAustraliamapwithtwocolors. We\nwillmaketheset{WA,SA}pathconsistentwithrespecttoNT. Westartbyenumeratingthe\nconsistent assignments totheset. Inthis case, there areonly two: {WA = red,SA = blue}\nand {WA = blue,SA = red}. We can see that with both of these assignments NT can be\nneither red nor blue (because it would conflict with either WA or SA). Because there is no\nvalidchoiceforNT,weeliminatebothassignments,andweendupwithnovalidassignments\nfor{WA,SA}. Therefore, weknow that there canbenosolution tothisproblem. ThePC-2\nalgorithm (Mackworth, 1977) achieves path consistency in much the same way that AC-3\nachievesarcconsistency. Becauseitissosimilar, wedonot showithere.\n1 TheAC-4algorithm(MohrandHenderson,1986)runsinO(cd2)worst-casetimebutcanbeslowerthanAC-3\nonaveragecases.SeeExercise6.13. Section6.2. ConstraintPropagation: InferenceinCSPs 211\n6.2.4 K-consistency\nStronger forms of propagation can be defined with the notion of k-consistency. A CSP is\nK-CONSISTENCY\nk-consistent if, for any set of k \u2212 1 variables and for any consistent assignment to those\nvariables, a consistent value can always be assigned to any kth variable. 1-consistency says\nthat, given the empty set, we can make any set of one variable consistent: this is what we\ncalled node consistency. 2-consistency is the same as arc consistency. Forbinary constraint\nnetworks, 3-consistency isthesameaspathconsistency.\nSTRONGLY A CSP is strongly k-consistent if it is k-consistent and is also (k \u2212 1)-consistent,\nK-CONSISTENT\n(k \u22122)-consistent, ... all the way down to 1-consistent. Now suppose we have a CSPwith\nn nodes and make it strongly n-consistent (i.e., strongly k-consistent for k=n). We can\nthen solve the problem as follows: First, we choose a consistent value for X . We are then\n1\nguaranteed to be able to choose a value for X because the graph is 2-consistent, for X\n2 3\nbecauseitis3-consistent, andsoon. ForeachvariableX ,weneedonlysearchthroughthed\ni\nvaluesinthedomaintofindavalueconsistent withX 1,...,X i\u22121. Weareguaranteed tofind\na solution in time O(n2d). Ofcourse, there is no free lunch: any algorithm for establishing\nn-consistency must take time exponential in n in the worst case. Worse, n-consistency also\nrequires space thatisexponential in n. Thememoryissue isevenmoreseverethanthetime.\nInpractice, determining theappropriate levelofconsistency checking ismostlyanempirical\nscience. It can be said practitioners commonly compute 2-consistency and less commonly\n3-consistency.\n6.2.5 Globalconstraints\nRememberthataglobalconstraintisoneinvolvinganarbitrarynumberofvariables(butnot\nnecessarily all variables). Global constraints occur frequently in real problems and can be\nhandledbyspecial-purpose algorithmsthataremoreefficientthanthegeneral-purpose meth-\nods described so far. Forexample, the Alldiff constraint says that all the variables involved\nmust have distinct values (as in the cryptarithmetic problem above and Sudoku puzzles be-\nlow). One simple form of inconsistency detection for Alldiff constraints works as follows:\nifmvariables areinvolved inthe constraint, and iftheyhave npossible distinct values alto-\ngether, andm > n,thentheconstraint cannotbesatisfied.\nThis leads to the following simple algorithm: First, remove any variable in the con-\nstraint that has a singleton domain, and delete that variable\u2019s value from the domains of the\nremainingvariables. Repeataslongastherearesingleton variables. Ifatanypointanempty\ndomainisproducedortherearemorevariablesthandomainvaluesleft,thenaninconsistency\nhasbeendetected.\nThismethodcandetect theinconsistency intheassignment {WA=red,NSW =red}\nfor Figure 6.1. Notice that the variables SA, NT, and Q are effectively connected by an\nAlldiff constraint because each pair must have two different colors. After applying AC-3\nwith the partial assignment, the domain of each variable is reduced to {green,blue}. That\nis, we have three variables and only two colors, so the Alldiff constraint is violated. Thus,\na simple consistency procedure for a higher-order constraint is sometimes more effective\nthan applying arc consistency to an equivalent set of binary constraints. There are more 212 Chapter 6. ConstraintSatisfaction Problems\ncomplex inference algorithms for Alldiff (see van Hoeve and Katriel, 2006) that propagate\nmoreconstraints butaremorecomputationally expensiveto run.\nRESOURCE Anotherimportanthigher-orderconstraintisthe resourceconstraint,sometimescalled\nCONSTRAINT\nthe atmost constraint. For example, in a scheduling problem, let P ,...,P denote the\n1 4\nnumbers of personnel assigned to each of four tasks. The constraint that no more than 10\npersonnel are assigned in total is written as Atmost(10,P ,P ,P ,P ). We can detect an\n1 2 3 4\ninconsistency simply by checking the sum of the minimum values of the current domains;\nfor example, if each variable has the domain {3,4,5,6}, the Atmost constraint cannot be\nsatisfied. Wecanalsoenforceconsistencybydeletingthemaximumvalueofanydomainifit\nisnotconsistent withtheminimumvaluesoftheotherdomains. Thus,ifeachvariableinour\nexamplehasthedomain{2,3,4,5,6}, thevalues5and6canbedeletedfromeachdomain.\nFor large resource-limited problems with integer values\u2014such as logistical problems\ninvolving moving thousands of people in hundreds of vehicles\u2014it is usually not possible to\nrepresentthedomainofeachvariableasalargesetofintegersandgraduallyreducethatsetby\nconsistency-checking methods. Instead, domainsarerepresented byupperandlowerbounds\nBOUNDS and are managed by bounds propagation. For example, in an airline-scheduling problem,\nPROPAGATION\nlet\u2019s suppose there are two flights, F and F , for which the planes have capacities 165 and\n1 2\n385,respectively. Theinitialdomainsforthenumbersofpassengers oneachflightarethen\nD = [0,165] and D = [0,385].\n1 2\nNow suppose we have the additional constraint that the two flights together must carry 420\npeople: F +F = 420. Propagating boundsconstraints, wereducethedomainsto\n1 2\nD = [35,165] and D = [255,385].\n1 2\nBOUNDS We say that a CSP is bounds consistent if for every variable X, and for both the lower-\nCONSISTENT\nboundandupper-bound valuesofX,thereexistssomevalueofY thatsatisfiestheconstraint\nbetween X and Y for every variable Y. This kind of bounds propagation is widely used in\npractical constraint problems.\n6.2.6 Sudoku example\nThepopularSudokupuzzlehasintroducedmillionsofpeopletoconstraintsatisfactionprob-\nSUDOKU\nlems, although they may not recognize it. A Sudoku board consists of 81 squares, some of\nwhich are initially filled with digits from 1 to 9. The puzzle is to fill in all the remaining\nsquaressuchthatnodigitappearstwiceinanyrow,column,or3\u00d73box(seeFigure6.4). A\nrow,column,orboxiscalledaunit.\nTheSudokupuzzles thatareprintedinnewspapers andpuzzle bookshavetheproperty\nthat thereisexactly onesolution. Although somecanbetricky tosolve byhand, taking tens\nofminutes, eventhehardest SudokuproblemsyieldtoaCSPsolverinlessthan0.1second.\nASudoku puzzle can beconsidered a CSPwith 81 variables, one foreach square. We\nusethevariable namesA1 through A9 forthetoprow(lefttoright), downto I1 through I9\nforthebottom row. Theemptysquares have the domain {1,2,3,4,5,6,7,8,9} and the pre-\nfilled squares have a domain consisting of a single value. In addition, there are 27 different Section6.2. ConstraintPropagation: InferenceinCSPs 213\n1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9\nA 3 2 6 A 4 8 3 9 2 1 6 5 7\nB 9 3 5 1 B 9 6 7 3 4 5 8 2 1\nC 1 8 6 4 C 2 5 1 8 7 6 4 9 3\nD 8 1 2 9 D 5 4 8 1 3 2 9 7 6\nE 7 8 E 7 2 9 5 6 4 1 3 8\nF 6 7 8 2 F 1 3 6 7 9 8 2 4 5\nG 2 6 9 5 G 3 7 2 6 8 9 5 1 4\nH 8 2 3 9 H 8 1 4 2 5 3 7 6 9\nI 5 1 3 I 6 9 5 4 1 7 3 8 2\n(a) (b)\nFigure6.4 (a)ASudokupuzzleand(b)itssolution.\nAlldiff constraints: oneforeachrow,column, andboxof9squares.\nAlldiff(A1,A2,A3,A4,A5,A6,A7,A8,A9)\nAlldiff(B1,B2,B3,B4,B5,B6,B7,B8,B9)\n\u00b7\u00b7\u00b7\nAlldiff(A1,B1,C1,D1,E1,F1,G1,H1,I1)\nAlldiff(A2,B2,C2,D2,E2,F2,G2,H2,I2)\n\u00b7\u00b7\u00b7\nAlldiff(A1,A2,A3,B1,B2,B3,C1,C2,C3)\nAlldiff(A4,A5,A6,B4,B5,B6,C4,C5,C6)\n\u00b7\u00b7\u00b7\nLetusseehowfararcconsistencycantakeus. AssumethattheAlldiff constraintshavebeen\nexpandedintobinaryconstraints(suchasA1 (cid:7)= A2)sothatwecanapplytheAC-3algorithm\ndirectly. Consider variable E6 from Figure 6.4(a)\u2014the empty square between the 2 and the\n8inthemiddlebox. Fromtheconstraintsinthebox,wecanremovenotonly2and8butalso\n1 and 7 from E6\u2019s domain. From the constraints in its column, we can eliminate 5, 6, 2, 8,\n9,and3. Thatleaves E6 withadomainof{4};inotherwords, weknowtheanswerfor E6.\nNow consider variable I6\u2014the square in the bottom middle box surrounded by 1, 3, and 3.\nApplyingarcconsistencyinitscolumn,weeliminate5,6,2,4(sincewenowknowE6 must\nbe 4), 8, 9, and 3. We eliminate 1 by arc consistency with I5, and we are left with only the\nvalue 7 inthe domain of I6. Now there are 8known values in column 6, so arc consistency\ncaninferthatA6 mustbe1. Inference continues along theselines, andeventually, AC-3can\nsolve the entire puzzle\u2014all the variables have their domains reduced to a single value, as\nshowninFigure6.4(b).\nOf course, Sudoku would soon lose its appeal if every puzzle could be solved by a 214 Chapter 6. ConstraintSatisfaction Problems\nmechanicalapplicationofAC-3,andindeedAC-3worksonlyfortheeasiestSudokupuzzles.\nSlightly harder ones can be solved by PC-2, but at a greater computational cost: there are\n255,960differentpathconstraintstoconsiderinaSudokupuzzle. Tosolvethehardestpuzzles\nandtomakeefficientprogress, wewillhavetobemoreclever.\nIndeed,theappealofSudokupuzzlesforthehumansolveristheneedtoberesourceful\nin applying more complex inference strategies. Aficionados give them colorful names, such\nas \u201cnaked triples.\u201d That strategy works as follows: in any unit (row, column or box), find\nthree squares that each have a domain that contains the same three numbers or a subset of\nthose numbers. Forexample, thethree domains mightbe {1,8}, {3,8}, and {1,3,8}. From\nthat wedon\u2019t know which square contains 1,3, or8, but wedoknow that thethree numbers\nmust be distributed among the three squares. Therefore we can remove 1, 3, and 8 from the\ndomainsofeveryothersquareintheunit.\nIt is interesting to note how far we can go without saying much that is specific to Su-\ndoku. Wedoofcoursehavetosaythatthereare81variables,thattheirdomainsarethedigits\n1to9,andthatthereare27 Alldiff constraints. Butbeyond that, allthestrategies\u2014arc con-\nsistency, path consistency, etc.\u2014apply generally to all CSPs, not just to Sudoku problems.\nEven naked triples is really a strategy for enforcing consistency of Alldiff constraints and\nhasnothingtodowithSudokuperse. ThisisthepoweroftheCSPformalism: foreachnew\nproblem area, we only need to define the problem in terms of constraints; then the general\nconstraint-solving mechanismscantakeover.\n6.3 BACKTRACKING SEARCH FOR CSPS\nSudoku problems are designed to be solved by inference over constraints. But many other\nCSPs cannot be solved by inference alone; there comes a time when we must search for a\nsolution. In this section we look at backtracking search algorithms that work on partial as-\nsignments;inthenextsectionwelookatlocalsearchalgorithmsovercompleteassignments.\nWe could apply a standard depth-limited search (from Chapter 3). A state would be a\npartial assignment, andanaction wouldbeadding var = value totheassignment. Butfora\nCSPwith nvariables of domain size d, wequickly notice something terrible: the branching\nfactoratthetoplevelisndbecauseanyofdvaluescanbeassignedtoanyofnvariables. At\nthe next level, the branching factor is (n \u22121)d, and so on for n levels. We generate a tree\nwithn!\u00b7dn leaves,eventhoughthereareonly dn possible completeassignments!\nOur seemingly reasonable but naive formulation ignores crucial property common to\nallCSPs: commutativity. Aproblemiscommutativeiftheorderofapplication ofanygiven\nCOMMUTATIVITY\nset of actions has no effect on the outcome. CSPsare commutative because when assigning\nvalues to variables, wereach the samepartial assignment regardless of order. Therefore, we\nneed onlyconsider a single variable ateach node inthe search tree. Forexample, attheroot\nnode of a search tree for coloring the map of Australia, we might make a choice between\nSA=red, SA=green, and SA=blue, but we would never choose between SA=red and\nWA=blue. Withthisrestriction, thenumberofleavesis dn,aswewouldhope. Section6.3. Backtracking SearchforCSPs 215\nfunctionBACKTRACKING-SEARCH(csp)returnsasolution,orfailure\nreturnBACKTRACK({},csp)\nfunctionBACKTRACK(assignment,csp)returnsasolution,orfailure\nifassignment iscompletethenreturnassignment\nvar\u2190SELECT-UNASSIGNED-VARIABLE(csp)\nforeachvalue inORDER-DOMAIN-VALUES(var,assignment,csp)do\nifvalue isconsistentwithassignment then\nadd{var =value}toassignment\ninferences\u2190INFERENCE(csp,var,value)\nifinferences (cid:7)= failure then\naddinferences toassignment\nresult\u2190BACKTRACK(assignment,csp)\nifresult (cid:7)=failure then\nreturnresult\nremove{var =value}andinferences fromassignment\nreturnfailure\nFigure6.5 Asimplebacktrackingalgorithmforconstraintsatisfactionproblems. Theal-\ngorithmismodeledontherecursivedepth-firstsearchofChapter3.Byvaryingthefunctions\nSELECT-UNASSIGNED-VARIABLE and ORDER-DOMAIN-VALUES, we can implementthe\ngeneral-purposeheuristicsdiscussedinthetext. ThefunctionINFERENCEcanoptionallybe\nused to impose arc-, path-, or k-consistency, as desired. If a value choice leads to failure\n(noticedeitherbyINFERENCEorbyBACKTRACK),thenvalueassignments(includingthose\nmadebyINFERENCE)areremovedfromthecurrentassignmentandanewvalueistried.\nBACKTRACKING The term backtracking search is used for a depth-first search that chooses values for\nSEARCH\none variable atatime and backtracks when avariable has nolegal values left to assign. The\nalgorithmisshowninFigure6.5. Itrepeatedlychoosesanunassigned variable,andthentries\nallvaluesinthedomainofthatvariableinturn,tryingtofindasolution. Ifaninconsistencyis\ndetected,thenBACKTRACKreturnsfailure,causingthepreviouscalltotryanothervalue. Part\nof the search tree forthe Australia problem is shown in Figure 6.6, where wehave assigned\nvariables in the order WA,NT,Q,.... Because the representation of CSPs is standardized,\nthere is no need to supply BACKTRACKING-SEARCH with a domain-specific initial state,\nactionfunction, transition model,orgoaltest.\nNoticethatBACKTRACKING-SEARCH keepsonlyasinglerepresentation ofastateand\naltersthatrepresentation ratherthancreating newones,asdescribed onpage87.\nIn Chapter 3 we improved the poor performance of uninformed search algorithms by\nsupplying them with domain-specific heuristic functions derived from ourknowledge of the\nproblem. ItturnsoutthatwecansolveCSPsefficientlywithoutsuchdomain-specific knowl-\nedge. Instead, we can add some sophistication to the unspecified functions in Figure 6.5,\nusingthemtoaddressthefollowingquestions:\n1. Which variable should be assigned next (SELECT-UNASSIGNED-VARIABLE), and in\nwhatordershoulditsvaluesbetried(ORDER-DOMAIN-VALUES)? 216 Chapter 6. ConstraintSatisfaction Problems\nWA=red WA=green WA=blue\nWA=red WA=red\nNT=green NT=blue\nWA=red WA=red\nNT=green NT=green\nQ=red Q=blue\nFigure6.6 Partofthesearchtreeforthemap-coloringprobleminFigure6.1.\n2. Whatinferences should beperformed ateachstepinthesearch(INFERENCE)?\n3. Whenthesearcharrivesatanassignmentthatviolatesaconstraint,canthesearchavoid\nrepeating thisfailure?\nThesubsections thatfollowanswereachofthesequestions inturn.\n6.3.1 Variableand valueordering\nThebacktracking algorithm containstheline\nvar\u2190SELECT-UNASSIGNED-VARIABLE(csp) .\nThesimpleststrategyforSELECT-UNASSIGNED-VARIABLE istochoosethenextunassigned\nvariableinorder, {X ,X ,...}. Thisstaticvariable ordering seldomresultsinthemosteffi-\n1 2\ncientsearch. Forexample,aftertheassignmentsforWA=red andNT =greeninFigure6.6,\nthereisonlyonepossiblevalueforSA,soitmakessensetoassignSA=blue nextratherthan\nassigningQ. Infact,afterSAisassigned,thechoicesforQ,NSW,andV areallforced. This\nintuitiveidea\u2014choosingthevariablewiththefewest\u201clegal\u201dvalues\u2014iscalledtheminimum-\nMINIMUM- remaining-values(MRV)heuristic. Italsohasbeencalledthe\u201cmostconstrainedvariable\u201dor\nREMAINING-VALUES\n\u201cfail-first\u201d heuristic, thelatterbecause itpicksavariablethatismostlikelytocauseafailure\nsoon, thereby pruning the search tree. If some variable X has no legal values left, the MRV\nheuristicwillselectX andfailurewillbedetectedimmediately\u2014avoiding pointlesssearches\nthrough other variables. The MRV heuristic usually performs better than a random or static\nordering,sometimesbyafactorof1,000ormore,althoughtheresultsvarywidelydepending\nontheproblem.\nTheMRVheuristic doesn\u2019t help atallinchoosing thefirstregion tocolorinAustralia,\nbecauseinitiallyeveryregionhasthreelegalcolors. Inthiscase,thedegreeheuristiccomes\nDEGREEHEURISTIC\nin handy. It attempts to reduce the branching factor on future choices by selecting the vari-\nable that is involved in the largest number of constraints on other unassigned variables. In\nFigure 6.1, SAis the variable with highest degree, 5; the other variables have degree 2 or3,\nexcept for T, which has degree 0. In fact, once SA is chosen, applying the degree heuris-\ntic solves the problem without any false steps\u2014you can choose any consistent color at each\nchoice point and still arrive at a solution with no backtracking. The minimum-remaining- Section6.3. Backtracking SearchforCSPs 217\nvalues heuristic isusually amorepowerful guide, but the degree heuristic canbe useful as a\ntie-breaker.\nOnce a variable has been selected, the algorithm must decide on the order in which to\nLEAST-\nexamine itsvalues. Forthis, the least-constraining-value heuristic can beeffective insome\nCONSTRAINING-\nVALUE\ncases. It prefers the value that rules out the fewest choices for the neighboring variables in\nthe constraint graph. For example, suppose that in Figure 6.1 we have generated the partial\nassignment with WA=red and NT =green and that our next choice is for Q. Blue would\nbe a bad choice because it eliminates the last legal value left for Q\u2019s neighbor, SA. The\nleast-constraining-value heuristic therefore prefers red to blue. In general, the heuristic is\ntryingtoleavethemaximumflexibilityforsubsequentvariableassignments. Ofcourse,ifwe\nare trying to find all the solutions to a problem, not just the first one, then the ordering does\nnot matter because we have to consider every value anyway. The same holds if there are no\nsolutions totheproblem.\nWhyshould variable selection be fail-first, but value selection be fail-last? It turns out\nthat, for a wide variety of problems, a variable ordering that chooses a variable with the\nminimumnumberofremainingvalueshelpsminimizethenumberofnodesinthesearchtree\nby pruning larger parts of the tree earlier. Forvalue ordering, the trick is that we only need\nonesolution;thereforeitmakessensetolookforthemostlikelyvaluesfirst. Ifwewantedto\nenumerateallsolutions ratherthanjustfindone,thenvalue orderingwouldbeirrelevant.\n6.3.2 Interleaving searchand inference\nSo far we have seen how AC-3 and other algorithms can infer reductions in the domain of\nvariables before webeginthesearch. Butinference canbeevenmorepowerfulinthecourse\nof a search: every time we make a choice of a value for a variable, we have a brand-new\nopportunity toinfernewdomainreductions ontheneighboring variables.\nFORWARD One of the simplest forms of inference is called forward checking. Whenever a vari-\nCHECKING\nable X isassigned, the forward-checking process establishes arc consistency forit: foreach\nunassigned variable Y that is connected to X by a constraint, delete from Y\u2019s domain any\nvalue that is inconsistent withthe value chosen for X. Because forward checking only does\narcconsistencyinferences,thereisnoreasontodoforward checkingifwehavealreadydone\narcconsistency asapreprocessing step.\nFigure 6.7 shows the progress of backtracking search on the Australia CSP with for-\nward checking. There are two important points to notice about this example. First, notice\nthat after WA=red and Q=green are assigned, the domains of NT and SA are reduced\nto a single value; we have eliminated branching on these variables altogether by propagat-\ning information from WA and Q. A second point to notice is that after V =blue, the do-\nmain of SA is empty. Hence, forward checking has detected that the partial assignment\n{WA=red,Q=green,V =blue} is inconsistent with the constraints of the problem, and\nthealgorithm willtherefore backtrack immediately.\nFor many problems the search will be more effective if we combine the MRV heuris-\ntic with forward checking. Consider Figure 6.7 after assigning {WA=red}. Intuitively, it\nseems that that assignment constrains itsneighbors, NT and SA, soweshould handle those 218 Chapter 6. ConstraintSatisfaction Problems\nWA NT Q NSW V SA T\nInitial domains R G B R G B R G B R G B R G B R G B R G B\nAfterWA=red R G B R G B R G B R G B G B R G B\nAfterQ=green R B G R B R G B B R G B\nAfterV=blue R B G R B R G B\nFigure 6.7 The progress of a map-coloring search with forward checking. WA=red\nis assigned first; then forward checking deletes red from the domains of the neighboring\nvariables NT and SA. After Q=green is assigned, green is deleted from the domainsof\nNT,SA,andNSW. AfterV =blue isassigned,blue isdeletedfromthedomainsofNSW\nandSA,leavingSAwithnolegalvalues.\nvariables next, and then all the other variables will fall into place. That\u2019s exactly what hap-\npens with MRV:NT and SAhave two values, soone of them ischosen first, then the other,\nthen Q, NSW, and V in order. Finally T still has three values, and any one of them works.\nWecanviewforwardchecking asanefficientwaytoincrementally compute theinformation\nthattheMRVheuristic needstodoitsjob.\nAlthoughforwardcheckingdetectsmanyinconsistencies, itdoesnotdetectallofthem.\nThe problem is that it makes the current variable arc-consistent, but doesn\u2019t look ahead and\nmakealltheothervariablesarc-consistent. Forexample,considerthethirdrowofFigure6.7.\nItshowsthatwhenWAisred andQisgreen,bothNT andSAareforcedtobeblue. Forward\nchecking does notlook farenough ahead tonotice that thisis aninconsistency: NT andSA\nareadjacentandsocannothavethesamevalue.\nMAINTAININGARC The algorithm called MAC (for Maintaining Arc Consistency (MAC)) detects this\nCONSISTENCY(MAC)\ninconsistency. Afteravariable X\ni\nisassignedavalue,theINFERENCE procedurecallsAC-3,\nbut instead of a queue of all arcs in the CSP, we start with only the arcs (X ,X ) for all\nj i\nX thatareunassigned variables thatareneighbors of X . Fromthere, AC-3doesconstraint\nj i\npropagation intheusualway,andifanyvariablehasitsdomainreducedtotheemptyset,the\ncall to AC-3 fails and we know to backtrack immediately. We can see that MAC is strictly\nmorepowerfulthanforwardcheckingbecauseforwardcheckingdoesthesamethingasMAC\non the initial arcs in MAC\u2019s queue; but unlike MAC, forward checking does not recursively\npropagate constraints whenchanges aremadetothedomainsofvariables.\n6.3.3 Intelligentbacktracking: Looking backward\nThe BACKTRACKING-SEARCH algorithm inFigure6.5hasaverysimple policy forwhatto\ndo when a branch of the search fails: back up to the preceding variable and try a different\nCHRONOLOGICAL value for it. This is called chronological backtracking because the most recent decision\nBACKTRACKING\npointisrevisited. Inthissubsection, weconsiderbetterpossibilities.\nConsider what happens when we apply simple backtracking in Figure 6.1 with a fixed\nvariable ordering Q, NSW, V, T, SA, WA, NT. Suppose we have generated the partial\nassignment {Q=red,NSW =green,V =blue,T =red}. When we try the next variable,\nSA, we see that every value violates a constraint. We back up to T and try a new color for Section6.3. Backtracking SearchforCSPs 219\nTasmania! Obviouslythisissilly\u2014recoloring Tasmaniacannotpossiblyresolvetheproblem\nwithSouthAustralia.\nAmoreintelligent approach tobacktracking istobacktrack toavariable thatmightfix\nthe problem\u2014a variable that was responsible for making one of the possible values of SA\nimpossible. To do this, we will keep track of a set of assignments that are in conflict with\nsomevalueforSA. Theset(inthiscase{Q=red,NSW =green,V =blue,}),iscalledthe\nconflict set for SA. The backjumpingmethod backtracks to the most recent assignment in\nCONFLICTSET\nthe conflict set; in this case, backjumping would jump over Tasmania and try a new value\nBACKJUMPING\nfor V. This method is easily implemented by a modification to BACKTRACK such that it\naccumulates the conflict set while checking for a legal value to assign. If no legal value is\nfound, the algorithm should return the most recent element of the conflict setalong with the\nfailureindicator.\nThe sharp-eyed reader will have noticed that forward checking can supply the conflict\nsetwithnoextrawork: wheneverforwardchecking basedonan assignment X=xdeletes a\nvalue from Y\u2019s domain, it should add X=x to Y\u2019s conflict set. If the last value is deleted\nfrom Y\u2019s domain, then the assignments in the conflict set of Y are added to the conflict set\nofX. Then,whenwegettoY,weknowimmediately wheretobacktrack ifneeded.\nThe eagle-eyed reader will have noticed something odd: backjumping occurs when\nevery value in a domain is in conflict with the current assignment; but forward checking\ndetects this event and prevents the search from ever reaching such a node! In fact, it can be\nshownthateverybranchprunedbybackjumping isalsoprunedbyforwardchecking. Hence,\nsimple backjumping is redundant in a forward-checking search or, indeed, in a search that\nusesstrongerconsistency checking, suchas MAC.\nDespite the observations of the preceding paragraph, the idea behind backjumping re-\nmainsagoodone: tobacktrackbasedonthereasonsforfailure. Backjumpingnoticesfailure\nwhenavariable\u2019sdomainbecomesempty,butinmanycasesabranchisdoomedlongbefore\nthis occurs. Consider again the partial assignment {WA=red,NSW =red} (which, from\nourearlierdiscussion, isinconsistent). Supposewetry T =red nextandthenassignNT,Q,\nV,SA. Weknowthatnoassignment canworkfortheselastfourvariables, soeventually we\nrunoutofvaluestotryatNT. Now,thequestionis,wheretobacktrack? Backjumpingcannot\nwork, because NT does have values consistent with the preceding assigned variables\u2014NT\ndoesn\u2019t have a complete conflict set of preceding variables that caused it to fail. We know,\nhowever,thatthefourvariables NT,Q,V,andSA,takentogether, failedbecauseofasetof\npreceding variables, which must be those variables that directly conflict with the four. This\nleads toadeepernotion oftheconflict setforavariable such asNT: itisthat setofpreced-\ning variables that caused NT, together with any subsequent variables, to have no consistent\nsolution. In this case, the set is WA and NSW, so the algorithm should backtrack to NSW\nandskipoverTasmania. Abackjumping algorithm thatusesconflictsets definedinthisway\nCONFLICT-DIRECTED iscalledconflict-directed backjumping.\nBACKJUMPING\nWe must now explain how these new conflict sets are computed. The method is in\nfact quite simple. The \u201cterminal\u201d failure of a branch of the search always occurs because a\nvariable\u2019s domain becomes empty; that variable has a standard conflict set. In our example,\nSA fails, and its conflict set is (say) {WA,NT,Q}. We backjump to Q, and Q absorbs 220 Chapter 6. ConstraintSatisfaction Problems\nthe conflict set from SA (minus Q itself, of course) into its own direct conflict set, which is\n{NT,NSW}; the new conflict set is {WA,NT,NSW}. That is, there is no solution from\nQ onward, given the preceding assignment to {WA,NT,NSW}. Therefore, we backtrack\nto NT, the most recent of these. NT absorbs {WA,NT,NSW} \u2212 {NT} into its own\ndirect conflict set {WA}, giving {WA,NSW} (as stated in the previous paragraph). Now\nthe algorithm backjumps to NSW, as wewould hope. Tosummarize: let X be the current\nj\nvariable, and let conf(X ) be its conflict set. If every possible value for X fails, backjump\nj j\ntothemostrecentvariable X inconf(X ),andset\ni j\nconf(X ) \u2190 conf(X )\u222aconf(X )\u2212{X }.\ni i j i\nWhen we reach a contradiction, backjumping can tell us how far to back up, so we don\u2019t\nwaste time changing variables that won\u2019t fix the problem. But we would also like to avoid\nrunning into the same problem again. When the search arrives at a contradiction, we know\nCONSTRAINT thatsomesubsetoftheconflictsetisresponsiblefortheproblem. Constraintlearningisthe\nLEARNING\nideaoffindingaminimumsetofvariablesfromtheconflictsetthatcausestheproblem. This\nset of variables, along with their corresponding values, is called a no-good. Wethen record\nNO-GOOD\nthe no-good, either byadding anew constraint tothe CSPorby keeping aseparate cache of\nno-goods.\nFor example, consider the state {WA = red,NT = green,Q = blue} in the bottom\nrow of Figure 6.6. Forward checking can tell us this state is a no-good because there is no\nvalidassignmenttoSA. Inthisparticularcase,recordingtheno-goodwouldnothelp,because\nonce we prune this branch from the search tree, we will never encounter this combination\nagain. ButsupposethatthesearchtreeinFigure6.6wereactuallypartofalargersearchtree\nthat started by first assigning values for V and T. Then it would be worthwhile to record\n{WA = red,NT = green,Q = blue} as a no-good because we are going to run into the\nsameproblemagainforeachpossible setofassignments toV andT.\nNo-goods can be effectively used by forward checking orby backjumping. Constraint\nlearning is one of the most important techniques used by modern CSP solvers to achieve\nefficiencyoncomplexproblems.\n6.4 LOCAL SEARCH FOR CSPS\nLocalsearchalgorithms(seeSection4.1)turnouttobeeffectiveinsolvingmanyCSPs. They\nuse a complete-state formulation: the initial state assigns a value to every variable, and the\nsearchchangesthevalueofonevariableatatime. Forexample,inthe8-queensproblem(see\nFigure 4.3), the initial state might be a random configuration of 8 queens in 8 columns, and\neach step moves a single queen to a new position in its column. Typically, the initial guess\nviolatesseveralconstraints. Thepointoflocalsearchistoeliminatetheviolatedconstraints.2\nInchoosing anewvalueforavariable, themostobvious heuristic istoselect thevalue\nthat results in the minimum number of conflicts with other variables\u2014the min-conflicts\nMIN-CONFLICTS\n2 Localsearchcaneasilybeextendedtoconstraintoptimizationproblems(COPs).Inthatcase,allthetechniques\nforhillclimbingandsimulatedannealingcanbeappliedtooptimizetheobjectivefunction. Section6.4. LocalSearchforCSPs 221\nfunctionMIN-CONFLICTS(csp,max steps)returnsasolutionorfailure\ninputs:csp,aconstraintsatisfactionproblem\nmax steps,thenumberofstepsallowedbeforegivingup\ncurrent\u2190aninitialcompleteassignmentforcsp\nfori =1tomax steps do\nifcurrent isasolutionforcsp thenreturncurrent\nvar\u2190arandomlychosenconflictedvariablefromcsp.VARIABLES\nvalue\u2190thevaluev forvar thatminimizesCONFLICTS(var,v,current,csp)\nsetvar=value incurrent\nreturnfailure\nFigure6.8 TheMIN-CONFLICTSalgorithmforsolvingCSPsbylocalsearch. Theinitial\nstate may be chosen randomlyor by a greedy assignmentprocess that choosesa minimal-\nconflict value for each variable in turn. The CONFLICTS function counts the number of\nconstraintsviolatedbyaparticularvalue,giventherestofthecurrentassignment.\n2 3\n2 3\n1\n2 2\n3 3\n1 2\n2 3\n0\nFigure 6.9 A two-step solution using min-conflicts for an 8-queens problem. At each\nstage, a queen is chosen for reassignment in its column. The number of conflicts (in this\ncase, the number of attacking queens) is shown in each square. The algorithm moves the\nqueentothemin-conflictssquare,breakingtiesrandomly.\nheuristic. Thealgorithm isshowninFigure6.8anditsapplication toan8-queens problem is\ndiagrammedinFigure6.9.\nMin-conflicts is surprisingly effective for many CSPs. Amazingly, on the n-queens\nproblem, if you don\u2019t count the initial placement of queens, the run time of min-conflicts is\nroughly independent of problem size. It solves even the million-queens problem in an aver-\nage of 50 steps (after the initial assignment). This remarkable observation was the stimulus\nleading to a great deal of research in the 1990s on local search and the distinction between\neasy and hard problems, which we take up in Chapter 7. Roughly speaking, n-queens is\neasy for local search because solutions are densely distributed throughout the state space.\nMin-conflicts also works well for hard problems. Forexample, it has been used to schedule\nobservations fortheHubble SpaceTelescope, reducing the timetaken toschedule aweekof\nobservations fromthreeweeks(!) toaround10minutes. 222 Chapter 6. ConstraintSatisfaction Problems\nAllthelocalsearchtechniquesfromSection4.1arecandidatesforapplicationtoCSPs,\nand some of those have proved especially effective. The landscape of aCSPunder the min-\nconflicts heuristic usually has a series of plateaux. There may be millions of variable as-\nsignments that are only one conflict away from a solution. Plateau search\u2014allowing side-\nwaysmovestoanother state withthesamescore\u2014can helplocal search finditswayoffthis\nplateau. This wandering on the plateau can be directed with tabu search: keeping a small\nlistofrecentlyvisitedstatesandforbidding thealgorithm toreturntothosestates. Simulated\nannealing canalsobeusedtoescapefromplateaux.\nCONSTRAINT Anothertechnique,calledconstraintweighting,canhelpconcentratethesearchonthe\nWEIGHTING\nimportant constraints. Eachconstraint isgivenanumeric weight, W ,initially all1. Ateach\ni\nstepofthesearch,thealgorithmchoosesavariable\/value pairtochangethatwillresultinthe\nlowesttotalweightofallviolatedconstraints. Theweightsarethenadjustedbyincrementing\ntheweightofeachconstraintthatisviolatedbythecurrentassignment. Thishastwobenefits:\nit adds topography to plateaux, making sure that it is possible to improve from the current\nstate,anditalso,overtime,addsweighttotheconstraints thatareprovingdifficulttosolve.\nAnother advantage of local search is that it can be used in an online setting when the\nproblem changes. This is particularly important in scheduling problems. A week\u2019s airline\nschedule may involve thousands of flights and tens of thousands of personnel assignments,\nbutbadweatheratoneairportcanrenderthescheduleinfeasible. Wewouldliketorepairthe\nschedule with a minimum number of changes. This can be easily done with a local search\nalgorithm starting from the current schedule. A backtracking search with the new set of\nconstraints usually requires much more time and might find a solution with many changes\nfromthecurrentschedule.\n6.5 THE STRUCTURE OF PROBLEMS\nIn this section, we examine ways in which the structure of the problem, as represented by\ntheconstraint graph, can beused tofindsolutions quickly. Mostofthe approaches herealso\napplytootherproblemsbesidesCSPs,suchasprobabilistic reasoning. Afterall,theonlyway\nwecan possibly hope todeal withthe realworld istodecompose itintomany subproblems.\nLookingagainattheconstraintgraphforAustralia(Figure6.1(b),repeatedasFigure6.12(a)),\nonefactstandsout: Tasmaniaisnotconnectedtothemainland.3 Intuitively, itisobviousthat\nINDEPENDENT coloring Tasmania and coloring the mainland are independentsubproblems\u2014any solution\nSUBPROBLEMS\nfor the mainland combined with any solution for Tasmania yields a solution for the whole\nCONNECTED map. Independence can be ascertained simply by finding connected components of the\nCOMPONENT\nconstraint graph. Each component corresponds to a subproblem CSP . If assignment S is\n(cid:22) (cid:22) i i\na solution of CSP , then S is a solution of CSP . Why is this important? Consider\ni i i i i\nthe following: suppose each CSP has c variables from the total of n variables, where c is\ni\na constant. Then there are n\/c subproblems, each of which takes at most dc work to solve,\n3 AcarefulcartographerorpatrioticTasmanianmightobject thatTasmaniashouldnotbecoloredthesameas\nitsnearestmainlandneighbor,toavoidtheimpressionthatitmightbepartofthatstate. Section6.5. TheStructureofProblems 223\nwhere d is the size of the domain. Hence, the total work is O(dcn\/c), which is linear in n;\nwithout the decomposition, the total work is O(dn), which is exponential in n. Let\u2019s make\nthismoreconcrete: dividingaBooleanCSPwith80variables intofoursubproblems reduces\ntheworst-casesolution timefromthelifetimeoftheuniversedowntolessthanasecond.\nCompletely independent subproblems are delicious, then, but rare. Fortunately, some\nother graph structures are also easy to solve. Forexample, a constraint graph isa tree when\nanytwovariablesareconnectedbyonlyonepath. Weshowthatanytree-structured CSPcan\nbesolved intimelinear inthenumberofvariables.4 Thekeyisanewnotion ofconsistency,\nDIRECTEDARC calleddirectedarcconsistencyorDAC.ACSPisdefinedtobedirectedarc-consistentunder\nCONSISTENCY\nan ordering of variables X ,X ,...,X if and only if every X is arc-consistent with each\n1 2 n i\nX forj > i.\nj\nTo solve a tree-structured CSP, first pick any variable to be the root of the tree, and\nchooseanorderingofthevariablessuchthateachvariableappearsafteritsparentinthetree.\nSuch an ordering is called a topological sort. Figure 6.10(a) shows a sample tree and (b)\nTOPOLOGICALSORT\nshowsonepossibleordering. Anytreewithnnodeshasn\u22121arcs,sowecanmakethisgraph\ndirected arc-consistent in O(n) steps, each of which must compare up to d possible domain\nvalues fortwo variables, foratotal time of O(nd2). Once we have a directed arc-consistent\ngraph, we can just march down the list of variables and choose any remaining value. Since\neachlinkfromaparenttoitschildisarcconsistent,weknowthatforanyvaluewechoosefor\ntheparent, there willbeavalidvalue lefttochoose forthechild. Thatmeanswewon\u2019thave\nto backtrack; we can move linearly through the variables. Thecomplete algorithm is shown\ninFigure6.11.\nA E\nB D A B C D E F\nC F\n(a) (b)\nFigure6.10 (a)Theconstraintgraphofatree-structuredCSP.(b)Alinearorderingofthe\nvariablesconsistentwiththetreewithAastheroot. Thisisknownasatopologicalsortof\nthevariables.\nNowthatwehaveanefficientalgorithmfortrees,wecanconsiderwhethermoregeneral\nconstraint graphs can be reduced to trees somehow. There are two primary ways to do this,\nonebasedonremovingnodesandonebasedoncollapsing nodes together.\nThe first approach involves assigning values to some variables so that the remaining\nvariables form a tree. Consider the constraint graph for Australia, shown again in Fig-\nure 6.12(a). If we could delete South Australia, the graph would become a tree, as in (b).\nFortunately, we can do this (in the graph, not the continent) by fixing a value for SA and\n4 Sadly,veryfewregionsoftheworldhavetree-structuredmaps,althoughSulawesicomesclose. 224 Chapter 6. ConstraintSatisfaction Problems\nfunctionTREE-CSP-SOLVER(csp)returnsasolution,orfailure\ninputs: csp,aCSPwithcomponentsX, D, C\nn\u2190numberofvariablesinX\nassignment\u2190anemptyassignment\nroot\u2190anyvariableinX\nX \u2190TOPOLOGICALSORT(X,root)\nforj =n downto2do\nMAKE-ARC-CONSISTENT(PARENT(Xj),Xj)\nifitcannotbemadeconsistentthenreturnfailure\nfori =1ton do\nassignment[Xi]\u2190anyconsistentvaluefromDi\nifthereisnoconsistentvaluethenreturnfailure\nreturnassignment\nFigure6.11 The TREE-CSP-SOLVER algorithmforsolvingtree-structuredCSPs. Ifthe\nCSPhasasolution,wewillfinditinlineartime;ifnot,wewilldetectacontradiction.\nNT NT\nQ Q\nWA WA\nSA NSW NSW\nV V\nT T\n(a) (b)\nFigure6.12 (a) TheoriginalconstraintgraphfromFigure6.1. (b)The constraintgraph\naftertheremovalofSA.\ndeleting from the domains of the other variables any values that are inconsistent with the\nvaluechosen forSA.\nNow, any solution for the CSP after SA and its constraints are removed will be con-\nsistent with the value chosen for SA. (This works for binary CSPs; the situation is more\ncomplicated withhigher-order constraints.) Therefore, wecan solve theremaining treewith\nthe algorithm given above and thus solve the whole problem. Ofcourse, in the general case\n(asopposed tomapcoloring), thevaluechosenforSAcouldbethewrongone, sowewould\nneedtotryeachpossible value. Thegeneralalgorithm isasfollows: Section6.5. TheStructureofProblems 225\n1. ChooseasubsetS oftheCSP\u2019svariables suchthattheconstraint graphbecomesatree\nafterremovalofS. S iscalledacyclecutset.\nCYCLECUTSET\n2. Foreachpossible assignment tothevariables inS thatsatisfiesallconstraints onS,\n(a) remove from thedomains oftheremaining variables anyvalues that are inconsis-\ntentwiththeassignment forS,and\n(b) Iftheremaining CSPhasasolution, returnittogetherwiththeassignment forS.\nIfthecyclecutsethassizec,thenthetotalruntimeisO(dc\u00b7(n\u2212c)d2): wehavetotryeach\nof the dc combinations of values for the variables in S, and for each combination we must\nsolveatreeproblemofsize n\u2212c. Ifthegraphis\u201cnearlyatree,\u201dthen cwillbesmallandthe\nsavingsoverstraightbacktracking willbehuge. Intheworstcase,however,ccanbeaslarge\nas(n\u22122). Finding the smallest cycle cutset isNP-hard, but several efficient approximation\nCUTSET algorithms are known. The overall algorithmic approach is called cutset conditioning; it\nCONDITIONING\ncomesupagaininChapter14,whereitisusedforreasoning aboutprobabilities.\nTREE The second approach is based on constructing a tree decomposition of the constraint\nDECOMPOSITION\ngraphintoasetofconnectedsubproblems. Eachsubproblemissolvedindependently, andthe\nresulting solutions are then combined. Likemostdivide-and-conquer algorithms, this works\nwell if no subproblem is too large. Figure 6.13 shows a tree decomposition of the map-\ncoloring problem into five subproblems. A tree decomposition must satisfy the following\nthreerequirements:\n\u2022 Everyvariableintheoriginalproblem appearsinatleastoneofthesubproblems.\n\u2022 Iftwovariablesareconnectedbyaconstraint intheoriginalproblem,theymustappear\ntogether(alongwiththeconstraint) inatleastoneofthesubproblems.\n\u2022 Ifavariableappearsintwosubproblemsinthetree,itmustappearineverysubproblem\nalongthepathconnecting thosesubproblems.\nThe first two conditions ensure that all the variables and constraints are represented in the\ndecomposition. Thethird condition seemsrathertechnical, butsimply reflectstheconstraint\nthat any given variable must have the same value in every subproblem in which it appears;\nthelinksjoining subproblems inthetreeenforcethisconstraint. Forexample, SAappears in\nall four of the connected subproblems in Figure 6.13. You can verify from Figure 6.12 that\nthisdecomposition makessense.\nWesolve eachsubproblem independently; ifanyone hasnosolution, weknow theen-\ntireproblemhasnosolution. Ifwecansolveallthesubproblems,thenweattempttoconstruct\naglobalsolutionasfollows. First,wevieweachsubproblem asa\u201cmega-variable\u201dwhosedo-\nmain isthe set of all solutions forthe subproblem. Forexample, the leftmost subproblem in\nFigure6.13isamap-coloring problemwiththreevariablesandhencehassixsolutions\u2014one\nis {WA = red,SA = blue,NT = green}. Then, we solve the constraints connecting the\nsubproblems, using the efficient algorithm for trees given earlier. The constraints between\nsubproblems simply insistthatthesubproblem solutions agreeontheirshared variables. For\nexample,giventhesolution{WA = red,SA = blue,NT = green}forthefirstsubproblem,\ntheonlyconsistentsolutionforthenextsubproblemis{SA = blue,NT = green,Q = red}.\nA given constraint graph admits many tree decompositions; in choosing a decompo-\nsition, the aim is to make the subproblems as small as possible. The tree width of a tree\nTREEWIDTH 226 Chapter 6. ConstraintSatisfaction Problems\nNT\nNT\nQ\nWA\nSA\nSA\nQ\nSA NSW\nSA NSW\nT\nV\nFigure6.13 AtreedecompositionoftheconstraintgraphinFigure6.12(a).\ndecomposition of a graph is one less than the size of the largest subproblem; the tree width\nofthegraphitselfisdefinedtobetheminimumtreewidthamongallitstreedecompositions.\nIfagraph has treewidth w andwearegiven thecorresponding tree decomposition, then the\nproblem can be solved in O(ndw+1) time. Hence, CSPs with constraint graphs of bounded\ntree width are solvable in polynomial time. Unfortunately, finding the decomposition with\nminimaltreewidthisNP-hard,butthereareheuristic methodsthatworkwellinpractice.\nSofar, wehave looked atthestructure oftheconstraint graph. There canbeimportant\nstructureinthevaluesofvariablesaswell. Considerthemap-coloringproblemwithncolors.\nForeveryconsistent solution, there isactually aset of n!solutions formed bypermuting the\ncolornames. Forexample,ontheAustraliamapweknowthatWA,NT,andSAmustallhave\ndifferent colors, but there are 3! = 6 ways to assign the three colors to these three regions.\nThis is called value symmetry. We would like to reduce the search space by a factor of\nVALUESYMMETRY\nSYMMETRY-\nn!by breaking the symmetry. Wedo this by introducing a symmetry-breaking constraint.\nBREAKING\nCONSTRAINT\nForourexample, we might impose an arbitrary ordering constraint, NT < SA < WA, that\nrequires the three values tobeinalphabetical order. Thisconstraint ensures that only one of\nthen!solutions ispossible: {NT = blue,SA = green,WA = red}.\nFor map coloring, it was easy to find a constraint that eliminates the symmetry, and\nin general it is possible to find constraints that eliminate all but one symmetric solution in\npolynomial time, but it is NP-hard to eliminate all symmetry among intermediate sets of\nvalues during search. In practice, breaking value symmetry has proved to be important and\neffectiveonawiderangeofproblems. Section6.6. Summary 227\n6.6 SUMMARY\n\u2022 Constraintsatisfaction problems(CSPs)represent astatewithasetofvariable\/value\npairsandrepresent theconditions forasolutionbyasetofconstraints onthevariables.\nManyimportantreal-world problemscanbedescribed asCSPs.\n\u2022 Anumberofinferencetechniquesusetheconstraintstoinferwhichvariable\/valuepairs\nareconsistent andwhicharenot. Theseinclude node,arc,path,andk-consistency.\n\u2022 Backtrackingsearch,aformofdepth-firstsearch,iscommonlyusedforsolvingCSPs.\nInferencecanbeinterwovenwithsearch.\n\u2022 Theminimum-remaining-valuesanddegreeheuristicsaredomain-independentmeth-\nods for deciding which variable to choose next in a backtracking search. The least-\nconstraining-value heuristic helps in deciding which value to try first for a given\nvariable. Backtracking occurs when no legal assignment can be found for a variable.\nConflict-directedbackjumpingbacktracks directlytothesourceoftheproblem.\n\u2022 Localsearchusingthemin-conflictsheuristic hasalsobeenappliedtoconstraint satis-\nfactionproblemswithgreatsuccess.\n\u2022 The complexity of solving a CSP is strongly related to the structure of its constraint\ngraph. Tree-structured problemscanbesolvedinlineartime. Cutsetconditioningcan\nreduceageneral CSPtoatree-structured oneandisquiteefficientifasmallcutset can\nbefound. TreedecompositiontechniquestransformtheCSPintoatreeofsubproblems\nandareefficientifthetreewidthoftheconstraint graphissmall.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nThe earliest work related to constraint satisfaction dealt largely with numerical constraints.\nEquationalconstraints withintegerdomainswerestudiedbytheIndianmathematicianBrah-\nDIOPHANTINE maguptaintheseventhcentury;theyareoftencalledDiophantineequations,aftertheGreek\nEQUATIONS\nmathematician Diophantus (c. 200\u2013284), whoactually considered thedomainofpositive ra-\ntionals. Systematicmethodsforsolvinglinearequationsbyvariableeliminationwerestudied\nbyGauss(1829);thesolutionoflinearinequality constraints goesbacktoFourier(1827).\nFinite-domain constraint satisfaction problems also have a long history. For example,\ngraph coloring (ofwhich mapcoloring isaspecial case) is anold problem in mathematics.\nGRAPHCOLORING\nThefour-color conjecture (that every planar graph can be colored with fourorfewercolors)\nwas first made by Francis Guthrie, a student of De Morgan, in 1852. It resisted solution\u2014\ndespite several published claims to the contrary\u2014until a proof was devised by Appel and\nHaken (1977) (see the book Four Colors Suffice (Wilson, 2004)). Purists were disappointed\nthat part of the proof relied on a computer, so Georges Gonthier (2008), using the COQ\ntheorem prover,derivedaformalproofthatAppelandHaken\u2019sproofwascorrect.\nSpecific classes of constraint satisfaction problems occur throughout the history of\ncomputer science. One of the most influential early examples was the SKETCHPAD sys- 228 Chapter 6. ConstraintSatisfaction Problems\ntem (Sutherland, 1963), which solved geometric constraints in diagrams and was the fore-\nrunnerofmoderndrawingprogramsandCADtools. Theidentification ofCSPsasageneral\nclass is due to Ugo Montanari (1974). The reduction of higher-order CSPsto purely binary\nCSPswithauxiliaryvariables(seeExercise6.6)isdueoriginallytothe19th-centurylogician\nCharles Sanders Peirce. It was introduced into the CSP literature by Dechter (1990b) and\nwaselaboratedbyBacchusandvanBeek(1998). CSPswithpreferencesamongsolutionsare\nstudied widely in the optimization literature; see Bistarelli et al. (1997) for a generalization\nof the CSPframework to allow forpreferences. The bucket-elimination algorithm (Dechter,\n1999)canalsobeapplied tooptimization problems.\nConstraint propagation methods were popularized by Waltz\u2019s (1975) success on poly-\nhedral line-labeling problems for computer vision. Waltz showed that, in many problems,\npropagation completely eliminates the need for backtracking. Montanari (1974) introduced\nthe notion of constraint networks and propagation by path consistency. Alan Mackworth\n(1977)proposedtheAC-3algorithmforenforcingarcconsistencyaswellasthegeneralidea\nof combining backtracking with some degree of consistency enforcement. AC-4, a more\nefficientarc-consistency algorithm,wasdevelopedbyMohrandHenderson(1986). Soonaf-\nterMackworth\u2019s paperappeared, researchers beganexperimenting withthetradeoffbetween\nthe cost of consistency enforcement and the benefits in terms of search reduction. Haralick\nand Elliot (1980) favored the minimal forward-checking algorithm described by McGregor\n(1979), whereas Gaschnig (1979) suggested full arc-consistency checking after each vari-\nable assignment\u2014an algorithm later called MAC by Sabin and Freuder (1994). The latter\npaper provides somewhat convincing evidence that, on harder CSPs, full arc-consistency\nchecking pays off. Freuder (1978, 1982) investigated the notion of k-consistency and its\nrelationship to the complexity of solving CSPs. Apt (1999) describes a generic algorithmic\nframework within which consistency propagation algorithms can be analyzed, and Bessie`re\n(2006)presents acurrentsurvey.\nSpecial methods for handling higher-order or global constraints were developed first\nwithin the context of constraint logic programming. Marriott and Stuckey (1998) provide\nexcellent coverage of research in this area. The Alldiff constraint was studied by Regin\n(1994),StergiouandWalsh(1999),andvanHoeve(2001). Boundsconstraintswereincorpo-\nratedintoconstraint logicprogrammingbyVanHentenryck etal.(1998). Asurveyofglobal\nconstraints isprovided byvanHoeveandKatriel(2006).\nSudokuhasbecomethemostwidelyknownCSPandwasdescribedassuchbySimonis\n(2005). Agerbeck and Hansen (2008) describe some of the strategies and show that Sudoku\non an n2 \u00d7 n2 board is in the class of NP-hard problems. Reeson et al. (2007) show an\ninteractive solverbasedonCSPtechniques.\nThe idea of backtracking search goes back to Golomb and Baumert (1965), and its\napplicationtoconstraintsatisfactionisduetoBitnerandReingold(1975),althoughtheytrace\nthe basic algorithm back to the 19th century. Bitner and Reingold also introduced the MRV\nheuristic, which they called the most-constrained-variable heuristic. Brelaz (1979) used the\ndegree heuristic as a tiebreaker after applying the MRV heuristic. The resulting algorithm,\ndespite its simplicity, is still the best method for k-coloring arbitrary graphs. Haralick and\nElliot(1980)proposed theleast-constraining-value heuristic. Bibliographical andHistorical Notes 229\nThe basic backjumping method is due to John Gaschnig (1977, 1979). Kondrak and\nvan Beek (1997) showed that this algorithm is essentially subsumed by forward checking.\nConflict-directed backjumping was devised by Prosser (1993). The most general and pow-\nerful form of intelligent backtracking was actually developed very early on by Stallman and\nDEPENDENCY-\nSussman(1977). Theirtechniqueofdependency-directedbacktrackingledtothedevelop-\nDIRECTED\nBACKTRACKING\nmentoftruthmaintenancesystems(Doyle,1979),whichwediscussinSection12.6.2. The\nconnection betweenthetwoareasisanalyzed bydeKleer(1989).\nThe work of Stallman and Sussman also introduced the idea of constraint learning,\nin which partial results obtained by search can be saved and reused later in the search. The\nideawasformalized Dechter(1990a). Backmarking(Gaschnig, 1979) isaparticularly sim-\nBACKMARKING\nple method in which consistent and inconsistent pairwise assignments are saved and used\nto avoid rechecking constraints. Backmarking can be combined with conflict-directed back-\njumping; Kondrak and van Beek (1997) present a hybrid algorithm that provably subsumes\nDYNAMIC eithermethodtaken separately. Themethodof dynamicbacktracking(Ginsberg, 1993) re-\nBACKTRACKING\ntains successful partial assignments from later subsets of variables when backtracking over\nanearlierchoicethatdoesnotinvalidate thelatersuccess.\nEmpirical studies of several randomized backtracking methods were done by Gomes\netal.(2000)andGomesandSelman(2001). VanBeek(2006)surveysbacktracking.\nLocal search in constraint satisfaction problems was popularized by the work of Kirk-\npatricketal.(1983)onsimulatedannealing(seeChapter4),whichiswidelyusedforschedul-\ningproblems. Themin-conflictsheuristicwasfirstproposed byGu(1989)andwasdeveloped\nindependently byMintonetal.(1992). SosicandGu(1994)showedhowitcouldbeapplied\nto solve the 3,000,000 queens problem in less than a minute. The astounding success of\nlocal search using min-conflicts on the n-queens problem led to a reappraisal of the nature\nand prevalence of \u201ceasy\u201d and \u201chard\u201d problems. Peter Cheeseman et al. (1991) explored the\ndifficulty of randomly generated CSPs and discovered that almost all such problems either\nare trivially easy or have no solutions. Only if the parameters of the problem generator are\nsetin acertain narrow range, within which roughly half oftheproblems aresolvable, dowe\nfind \u201chard\u201d problem instances. We discuss this phenomenon further in Chapter 7. Konolige\n(1994)showedthatlocalsearchisinferiortobacktracking searchonproblemswithacertain\ndegree of local structure; this led to work that combined local search and inference, such as\nthatbyPinkasandDechter(1995). HoosandTsang(2006)surveylocalsearchtechniques.\nWorkrelatingthestructureandcomplexityofCSPsoriginateswithFreuder(1985),who\nshowedthat search onarcconsistent treesworks without any backtracking. Asimilarresult,\nwith extensions to acyclic hypergraphs, was developed in the database community (Beeri\net al., 1983). Bayardo and Miranker (1994) present an algorithm for tree-structured CSPs\nthatrunsinlineartimewithoutanypreprocessing.\nSincethosepaperswerepublished,therehasbeenagreatdealofprogressindeveloping\nmoregeneralresultsrelatingthecomplexityofsolvingaCSPtothestructureofitsconstraint\ngraph. ThenotionoftreewidthwasintroducedbythegraphtheoristsRobertsonandSeymour\n(1986). Dechter and Pearl (1987, 1989), building on the work of Freuder, applied a related\nnotion (whichthey called inducedwidth)toconstraint satisfaction problems and developed\nthetreedecompositionapproachsketchedinSection6.5. Drawingonthisworkandonresults 230 Chapter 6. ConstraintSatisfaction Problems\nfromdatabasetheory,Gottlobetal.(1999a,1999b)developedanotion,hypertreewidth,that\nisbased onthecharacterization of theCSPasahypergraph. Inaddition toshowing that any\nCSP with hypertree width w can be solved in time O(nw+1logn), they also showed that\nhypertree width subsumes all previously defined measures of \u201cwidth\u201d in the sense that there\narecaseswherethehypertreewidthisboundedandtheothermeasuresareunbounded.\nInterestinlook-backapproachestobacktrackingwasrekindledbytheworkofBayardo\nandSchrag(1997),whoseRELSAT algorithmcombinedconstraintlearningandbackjumping\nand was shown to outperform many other algorithms of the time. This led to AND\/OR\nsearch algorithms applicable to both CSPs and probabilistic reasoning (Dechter and Ma-\nteescu, 2007). Brown et al. (1988) introduce the idea of symmetry breaking in CSPs, and\nGentetal.(2006)givearecentsurvey.\nDISTRIBUTED\nThefieldofdistributed constraint satisfaction looks atsolving CSPswhenthere isa\nCONSTRAINT\nSATISFACTION\ncollection of agents, each of which controls a subset of the constraint variables. There have\nbeen annual workshops on this problem since 2000, and good coverage elsewhere (Collin\netal.,1999;Pearceetal.,2008;ShohamandLeyton-Brown,2009).\nComparingCSPalgorithmsismostlyanempiricalscience: fewtheoreticalresultsshow\nthat one algorithm dominates another on all problems; instead, we need to run experiments\nto see which algorithms perform better on typical instances of problems. AsHooker (1995)\npoints out, we need to be careful to distinguish between competitive testing\u2014as occurs in\ncompetitions among algorithms based on run time\u2014and scientific testing, whose goal is to\nidentify theproperties ofanalgorithm thatdetermineitsefficacyonaclassofproblems.\nThe recent textbooks by Apt (2003) and Dechter (2003), and the collection by Rossi\netal. (2006) are excellent resources on constraint processing. There are several good earlier\nsurveys,includingthosebyKumar(1992),DechterandFrost(2002),andBartak(2001);and\nthe encyclopedia articles by Dechter (1992) and Mackworth (1992). Pearson and Jeavons\n(1997) survey tractable classes of CSPs, covering both structural decomposition methods\nand methods that rely on properties of the domains or constraints themselves. Kondrak and\nvan Beek (1997) give an analytical survey of backtracking search algorithms, and Bacchus\nandvanRun(1995) giveamoreempirical survey. Constraint programming iscoveredinthe\nbooksbyApt(2003)andFruhwirthandAbdennadher(2003). Severalinterestingapplications\naredescribedinthecollectioneditedbyFreuderandMackworth(1994). Papersonconstraint\nsatisfactionappearregularlyinArtificialIntelligenceandinthespecialistjournalConstraints.\nThe primary conference venue is the International Conference on Principles and Practice of\nConstraint Programming,oftencalled CP.\nEXERCISES\n6.1 Howmanysolutions arethere forthemap-coloring problem in Figure6.1? Howmany\nsolutions iffourcolorsareallowed? Twocolors?\n6.2 Consider the problem of placing k knights on an n\u00d7n chessboard such that no two\nknightsareattacking eachother, where k isgivenandk \u2264 n2. Exercises 231\na. ChooseaCSPformulation. Inyourformulation, whatarethe variables?\nb. Whatarethepossiblevaluesofeachvariable?\nc. Whatsetsofvariablesareconstrained, andhow?\nd. Now consider the problem of putting as many knights as possible on the board with-\nout any attacks. Explain how to solve this with local search by defining appropriate\nACTIONS and RESULT functions andasensibleobjective function.\n6.3 Consider the problem of constructing (not solving) crossword puzzles:5 fitting words\ninto a rectangular grid. The grid, which is given as part of the problem, specifies which\nsquares are blank and which are shaded. Assume that a list of words (i.e., a dictionary)\nis provided and that the task is to fill in the blank squares by using any subset of the list.\nFormulatethisproblem precisely intwoways:\na. As a general search problem. Choose an appropriate search algorithm and specify a\nheuristicfunction. Isitbettertofillinblanksoneletteratatimeoronewordatatime?\nb. Asaconstraint satisfaction problem. Shouldthevariablesbewordsorletters?\nWhichformulation doyouthinkwillbebetter? Why?\n6.4 Givepreciseformulationsforeachofthefollowingasconstraint satisfaction problems:\na. Rectilinearfloor-planning: findnon-overlappingplacesinalargerectangleforanumber\nofsmallerrectangles.\nb. Classscheduling: Thereisafixednumberofprofessorsandclassrooms,alistofclasses\nto be offered, and a list of possible time slots for classes. Each professor has a set of\nclassesthatheorshecanteach.\nc. Hamiltoniantour: givenanetworkofcitiesconnectedbyroads,chooseanordertovisit\nallcitiesinacountry withoutrepeating any.\n6.5 Solve the cryptarithmetic problem in Figure 6.2 by hand, using the strategy of back-\ntracking withforwardcheckingandtheMRVandleast-constraining-value heuristics.\n6.6 Show how a single ternary constraint such as \u201cA+B = C\u201d can be turned into three\nbinary constraints by using an auxiliary variable. You may assume finite domains. (Hint:\nConsider a new variable that takes on values that are pairs of other values, and consider\nconstraints such as \u201cX is the first element of the pair Y.\u201d) Next, show how constraints with\nmorethanthreevariablescanbetreatedsimilarly. Finally,showhowunaryconstraintscanbe\neliminated by altering the domains of variables. This completes the demonstration that any\nCSPcanbetransformed intoaCSPwithonlybinaryconstraints.\n6.7 Considerthefollowinglogicpuzzle: Infivehouses,eachwithadifferentcolor,livefive\npersonsofdifferentnationalities, eachofwhomprefersadifferentbrandofcandy,adifferent\ndrink,andadifferentpet. Giventhefollowingfacts,thequestionstoanswerare\u201cWheredoes\nthezebralive,andinwhichhousedotheydrinkwater?\u201d\n5 Ginsbergetal.(1990)discussseveralmethodsforconstructingcrosswordpuzzles.Littmanetal.(1999)tackle\ntheharderproblemofsolvingthem. 232 Chapter 6. ConstraintSatisfaction Problems\nTheEnglishmanlivesintheredhouse.\nTheSpaniardownsthedog.\nTheNorwegianlivesinthefirsthouseontheleft.\nThegreenhouseisimmediately totherightoftheivoryhouse.\nThemanwhoeatsHersheybarslivesinthehousenexttothemanwiththefox.\nKitKatsareeatenintheyellowhouse.\nTheNorwegianlivesnexttothebluehouse.\nTheSmartieseaterownssnails.\nTheSnickerseaterdrinksorangejuice.\nTheUkrainiandrinkstea.\nTheJapaneseeatsMilkyWays.\nKitKatsareeateninahousenexttothehousewherethehorseiskept.\nCoffeeisdrunkinthegreenhouse.\nMilkisdrunkinthemiddlehouse.\nDiscussdifferent representations ofthisproblem asaCSP. Whywouldonepreferonerepre-\nsentation overanother?\n6.8 Consider the graph with 8 nodes A , A , A , A , H, T, F , F . A is connected to\n1 2 3 4 1 2 i\nA for all i, each A is connected to H, H is connected to T, and T is connected to each\ni+1 i\nF . Find a 3-coloring of this graph by hand using the following strategy: backtracking with\ni\nconflict-directed backjumping, the variable order A , H, A , F , A , F , A , T, and the\n1 4 1 2 2 3\nvalueorder R,G,B.\n6.9 Explainwhyitisagoodheuristictochoosethevariablethatismostconstrainedbutthe\nvaluethatisleastconstraining inaCSPsearch.\n6.10 Generate random instances of map-coloring problems as follows: scatter n points on\ntheunit square; select apoint X atrandom, connect X byastraight linetothenearest point\nY such that X is not already connected to Y and the line crosses no other line; repeat the\nprevious step until no more connections are possible. The points represent regions on the\nmap and the lines connect neighbors. Now try to find k-colorings of each map, for both\nk=3andk=4,usingmin-conflicts, backtracking, backtracking withforwardchecking, and\nbacktracking withMAC.Constructatableofaverageruntimesforeachalgorithmforvalues\nofnuptothelargestyoucanmanage. Commentonyourresults.\n6.11 Use the AC-3 algorithm to show that arc consistency can detect the inconsistency of\nthepartialassignment {WA=green,V =red}fortheproblem showninFigure6.1.\n6.12 Whatistheworst-casecomplexity ofrunning AC-3onatree-structured CSP?\n6.13 AC-3 puts back on the queue every arc (X ,X ) whenever any value is deleted from\nk i\nthedomainofX ,evenifeachvalueofX isconsistentwithseveralremainingvaluesofX .\ni k i\nSupposethat,foreveryarc(X ,X ),wekeeptrackofthenumberofremainingvaluesof X\nk i i\nthat are consistent with each value of X . Explain how to update these numbers efficiently\nk\nandhenceshowthatarcconsistency canbeenforced intotaltimeO(n2d2). Exercises 233\n6.14 TheTREE-CSP-SOLVER (Figure6.10)makesarcsconsistentstartingattheleavesand\nworking backwards towards the root. Whydoes itdothat? What would happen ifitwentin\ntheopposite direction?\n6.15 We introduced Sudoku as a CSP to be solved by search over partial assignments be-\ncausethatisthewaypeoplegenerallyundertakesolvingSudokuproblems. Itisalsopossible,\nof course, to attack these problems with local search over complete assignments. How well\nwouldalocalsolverusingthemin-conflictsheuristic doonSudokuproblems?\n6.16 Define in your own words the terms constraint, backtracking search, arc consistency,\nbackjumping, min-conflicts, andcyclecutset.\n6.17 Supposethatagraphisknowntohaveacyclecutsetofnomorethanknodes. Describe\nasimple algorithm forfinding aminimalcyclecutset whoserun timeisnotmuchmorethan\nO(nk)foraCSPwithnvariables. Searchtheliteratureformethodsforfindingapproximately\nminimal cycle cutsets intimethat ispolynomial inthe size ofthe cutset. Doesthe existence\nofsuchalgorithmsmakethecyclecutsetmethodpractical? 7\nLOGICAL AGENTS\nInwhichwedesignagentsthatcanformrepresentationsofacomplexworld,usea\nprocessofinference toderivenewrepresentations abouttheworld,andusethese\nnewrepresentations todeduce whattodo.\nHumans, it seems, know things; and what they know helps them do things. These are\nnot empty statements. They make strong claims about how the intelligence of humans is\nachieved\u2014not by purely reflex mechanisms but by processes of reasoning that operate on\nREASONING\ninternal representations of knowledge. In AI, this approach to intelligence is embodied in\nREPRESENTATION\nKNOWLEDGE-BASED knowledge-basedagents.\nAGENTS\nTheproblem-solvingagentsofChapters3and4knowthings,butonlyinaverylimited,\ninflexiblesense. Forexample, thetransition modelforthe8-puzzle\u2014knowledge ofwhatthe\nactions do\u2014is hidden inside the domain-specific code of the RESULT function. It can be\nused to predict the outcome of actions but not to deduce that two tiles cannot occupy the\nsamespaceorthatstateswithoddparitycannotbereachedfromstateswithevenparity. The\natomic representations used by problem-solving agents are also very limiting. In a partially\nobservable environment, an agent\u2019s only choice for representing what it knows about the\ncurrentstateistolistallpossibleconcretestates\u2014ahopelessprospectinlargeenvironments.\nChapter 6 introduced the idea of representing states as assignments of values to vari-\nables; this is a step in the right direction, enabling some parts of the agent to work in a\ndomain-independent way and allowing for more efficient algorithms. In this chapter and\nthose that follow, we take this step to its logical conclusion, so to speak\u2014we develop logic\nLOGIC\nas a general class of representations to support knowledge-based agents. Such agents can\ncombineandrecombineinformationtosuitmyriadpurposes. Often,thisprocesscanbequite\nfar removed from the needs of the moment\u2014as when a mathematician proves a theorem or\nanastronomercalculatestheearth\u2019slifeexpectancy. Knowledge-basedagentscanacceptnew\ntasksintheformofexplicitlydescribedgoals;theycanachievecompetencequicklybybeing\ntoldorlearning newknowledge about theenvironment; andtheycanadapttochanges inthe\nenvironment byupdating therelevantknowledge.\nWe begin in Section 7.1 with the overall agent design. Section 7.2 introduces a sim-\nplenewenvironment, thewumpusworld, andillustrates theoperation ofaknowledge-based\nagentwithoutgoingintoanytechnicaldetail. Thenweexplainthegeneralprinciplesoflogic\n234 Section7.1. Knowledge-Based Agents 235\nin Section 7.3 and the specifics of propositional logic in Section 7.4. While less expressive\nthan first-order logic (Chapter 8), propositional logic illustrates all the basic concepts of\nlogic; it also comes with well-developed inference technologies, which we describe in sec-\ntions7.5and7.6. Finally,Section7.7combinestheconceptofknowledge-based agentswith\nthetechnology ofpropositional logictobuildsomesimpleagentsforthewumpusworld.\n7.1 KNOWLEDGE-BASED AGENTS\nThecentralcomponent ofaknowledge-based agentisitsknowledgebase,orKB.Aknowl-\nKNOWLEDGEBASE\nedge base is a set of sentences. (Here \u201csentence\u201d is used as a technical term. It is related\nSENTENCE\nbut not identical to the sentences of English and other natural languages.) Each sentence is\nKNOWLEDGE\nexpressed in a language called a knowledge representation language and represents some\nREPRESENTATION\nLANGUAGE\nassertion about theworld. Sometimeswedignify asentence withthenameaxiom, whenthe\nAXIOM\nsentence istakenasgivenwithoutbeingderivedfromothersentences.\nThere must be a way to add new sentences to the knowledge base and a way to query\nwhat is known. The standard names for these operations are TELL and ASK, respectively.\nBothoperations mayinvolve inference\u2014that is,deriving newsentences fromold. Inference\nINFERENCE\nmustobeytherequirementthatwhenoneASKsaquestionoftheknowledgebase,theanswer\nshould follow from whathasbeen told(or TELLed)totheknowledge basepreviously. Later\nin this chapter, we will be more precise about the crucial word \u201cfollow.\u201d Fornow, take it to\nmeanthattheinferenceprocess shouldnotmakethingsupasitgoesalong.\nFigure7.1showstheoutlineofaknowledge-based agentprogram. Likeallouragents,\nittakesaperceptasinputandreturnsanaction. Theagentmaintainsaknowledgebase, KB,\nBACKGROUND whichmayinitially containsomebackgroundknowledge.\nKNOWLEDGE\nEach time the agent program is called, it does three things. First, it TELLs the knowl-\nedge base what it perceives. Second, it ASKs the knowledge base what action it should\nperform. In the process of answering this query, extensive reasoning may be done about\nthe current state of the world, about the outcomes of possible action sequences, and so on.\nThird,theagentprogram TELLstheknowledgebasewhichactionwaschosen, andtheagent\nexecutestheaction.\nThedetails oftherepresentation language arehidden insidethreefunctions thatimple-\nmenttheinterface between thesensors andactuators ononesideandthecorerepresentation\nand reasoning system on the other. MAKE-PERCEPT-SENTENCE constructs a sentence as-\nsertingthattheagentperceivedthegivenperceptatthegiventime. MAKE-ACTION-QUERY\nconstructs a sentence that asks what action should be done at the current time. Finally,\nMAKE-ACTION-SENTENCE constructs a sentence asserting that the chosen action was ex-\necuted. The details of the inference mechanisms are hidden inside TELL and ASK. Later\nsections willrevealthesedetails.\nTheagentinFigure7.1appearsquitesimilartotheagentswithinternalstatedescribed\nin Chapter 2. Because of the definitions of TELL and ASK, however, the knowledge-based\nagent is not an arbitrary program for calculating actions. It is amenable to a description at 236 Chapter 7. LogicalAgents\nfunctionKB-AGENT(percept)returnsanaction\npersistent: KB,aknowledgebase\nt,acounter,initially0,indicatingtime\nTELL(KB,MAKE-PERCEPT-SENTENCE(percept,t))\naction\u2190ASK(KB,MAKE-ACTION-QUERY(t))\nTELL(KB,MAKE-ACTION-SENTENCE(action,t))\nt\u2190t +1\nreturnaction\nFigure7.1 Agenericknowledge-basedagent.Givenapercept,theagentaddsthepercept\ntoitsknowledgebase,askstheknowledgebaseforthebestaction,andtellstheknowledge\nbasethatithasinfacttakenthataction.\nthe knowledge level, where we need specify only what the agent knows and what its goals\nKNOWLEDGELEVEL\nare, in order to fix its behavior. For example, an automated taxi might have the goal of\ntaking a passenger from San Francisco to Marin County and might know that the Golden\nGate Bridge is the only link between the two locations. Then we can expect it to cross the\nGoldenGateBridgebecauseitknowsthatthatwillachieveitsgoal. Noticethatthisanalysis\nIMPLEMENTATION isindependent ofhowthetaxiworksattheimplementationlevel. Itdoesn\u2019t matterwhether\nLEVEL\nitsgeographicalknowledgeisimplementedaslinkedlistsorpixelmaps,orwhetheritreasons\nby manipulating strings of symbols stored in registers or by propagating noisy signals in a\nnetworkofneurons.\nA knowledge-based agent can be built simply by TELLing it what it needs to know.\nStarting with an empty knowledge base, the agent designer can TELL sentences one by one\nuntil the agent knows how to operate in its environment. This is called the declarative ap-\nDECLARATIVE\nproach to system building. In contrast, the procedural approach encodes desired behaviors\ndirectly asprogram code. Inthe1970s and1980s, advocates ofthetwoapproaches engaged\ninheateddebates. Wenowunderstandthatasuccessfulagentoftencombinesbothdeclarative\nandprocedural elements initsdesign, andthatdeclarative knowledge canoften becompiled\nintomoreefficientprocedural code.\nWe can also provide a knowledge-based agent with mechanisms that allow it to learn\nfor itself. These mechanisms, which are discussed in Chapter 18, create general knowledge\nabouttheenvironment fromaseriesofpercepts. Alearning agentcanbefullyautonomous.\n7.2 THE WUMPUS WORLD\nInthissection wedescribe anenvironment inwhichknowledge-based agents canshowtheir\nworth. Thewumpusworldisacaveconsistingofroomsconnectedbypassageways. Lurking\nWUMPUSWORLD\nsomewhere in the cave isthe terrible wumpus, a beast that eats anyone who enters its room.\nThewumpuscanbeshotbyanagent,buttheagenthasonlyonearrow. Someroomscontain Section7.2. TheWumpusWorld 237\nbottomless pitsthatwilltrapanyone whowanders intothese rooms(except forthewumpus,\nwhich is too big to fall in). The only mitigating feature of this bleak environment is the\npossibility of finding a heap of gold. Although the wumpus world is rather tame by modern\ncomputergamestandards, itillustrates someimportant pointsaboutintelligence.\nA sample wumpus world is shown in Figure 7.2. The precise definition of the task\nenvironment isgiven,assuggested inSection2.3,bythePEASdescription:\n\u2022 Performance measure: +1000 for climbing out of the cave with the gold, \u20131000 for\nfalling into a pit or being eaten by the wumpus, \u20131 for each action taken and \u201310 for\nusingupthearrow. Thegameendseitherwhentheagentdiesorwhentheagentclimbs\noutofthecave.\n\u2022 Environment: A 4\u00d74 grid of rooms. The agent always starts in the square labeled\n[1,1], facing to the right. The locations of the gold and the wumpus are chosen ran-\ndomly, with a uniform distribution, from the squares other than the start square. In\naddition, eachsquareotherthanthestartcanbeapit,withprobability 0.2.\n\u2022 Actuators: The agent can move Forward, TurnLeft by 90\u25e6 , or TurnRight by 90\u25e6 . The\nagent dies a miserable death if it enters a square containing apit ora live wumpus. (It\nissafe, albeit smelly, toenter asquare with adead wumpus.) Ifan agent tries to move\nforward and bumps into awall, then the agent does notmove. Theaction Grabcan be\nused to pick up the gold if it is in the same square as the agent. The action Shoot can\nbeusedtofireanarrowinastraight lineinthedirection theagentisfacing. Thearrow\ncontinues untiliteitherhits(andhence kills) thewumpusorhits awall. Theagent has\nonly one arrow, so only the first Shoot action has any effect. Finally, the action Climb\ncanbeusedtoclimboutofthecave,butonlyfromsquare[1,1].\n\u2022 Sensors: Theagenthasfivesensors, eachofwhichgivesasinglebitofinformation:\n\u2013 Inthe square containing the wumpusand in thedirectly (not diagonally) adjacent\nsquares, theagentwillperceiveaStench.\n\u2013 Inthesquares directlyadjacent toapit,theagentwillperceiveaBreeze.\n\u2013 Inthesquarewherethegoldis,theagentwillperceivea Glitter.\n\u2013 Whenanagentwalksintoawall,itwillperceiveaBump.\n\u2013 When the wumpus is killed, it emits a woeful Scream that can be perceived any-\nwhereinthecave.\nThe percepts will be given to the agent program in the form of a list of five symbols;\nforexample, ifthereisastenchandabreeze, butnoglitter, bump,orscream,theagent\nprogramwillget [Stench,Breeze,None,None,None].\nWe can characterize the wumpus environment along the various dimensions given in Chap-\nter2. Clearly,itisdiscrete,static,andsingle-agent. (Thewumpusdoesn\u2019tmove,fortunately.)\nIt is sequential, because rewards may come only after many actions are taken. It is partially\nobservable, because some aspects of the state are not directly perceivable: the agent\u2019s lo-\ncation, the wumpus\u2019s state of health, and the availability of an arrow. As for the locations\nof the pits and the wumpus: we could treat them as unobserved parts of the state that hap-\npentobeimmutable\u2014in whichcase, thetransition modelfortheenvironment iscompletely 238 Chapter 7. LogicalAgents\n4 Stench Breeze PIT\nBreeze\n3 Stench PIT Breeze\nGold\n2 Stench Breeze\n1 Breeze PIT Breeze\nSTART\n1 2 3 4\nFigure7.2 Atypicalwumpusworld.Theagentisinthebottomleftcorner,facingright.\nknown;orwecouldsaythatthetransitionmodelitselfisunknownbecausetheagentdoesn\u2019t\nknow which Forward actions are fatal\u2014in which case, discovering the locations of pits and\nwumpuscompletestheagent\u2019sknowledge ofthetransition model.\nForan agent in the environment, the main challenge is its initial ignorance of the con-\nfigurationoftheenvironment; overcomingthisignorance seemstorequirelogicalreasoning.\nInmostinstancesofthewumpusworld,itispossiblefortheagenttoretrievethegoldsafely.\nOccasionally, theagentmustchoosebetweengoinghomeempty-handedandriskingdeathto\nfind the gold. About 21% of the environments are utterly unfair, because the gold is in a pit\norsurrounded bypits.\nLet us watch a knowledge-based wumpus agent exploring the environment shown in\nFigure 7.2. We use an informal knowledge representation language consisting of writing\ndownsymbolsinagrid(asinFigures7.3and7.4).\nThe agent\u2019s initial knowledge base contains the rules of the environment, as described\npreviously; inparticular, itknowsthat itisin[1,1] and that [1,1]is asafe square; wedenote\nthatwithan\u201cA\u201dand\u201cOK,\u201drespectively, insquare[1,1].\nThefirstpercept is [None,None,None,None,None], from which the agent can con-\nclude that its neighboring squares, [1,2] and [2,1], are free of dangers\u2014they are OK. Fig-\nure7.3(a)showstheagent\u2019sstateofknowledgeatthispoint.\nA cautious agent will move only into a square that it knows to be OK. Let us suppose\ntheagentdecidestomoveforwardto[2,1]. Theagentperceivesabreeze(denotedby\u201cB\u201d)in\n[2,1],sotheremustbeapitinaneighboringsquare. Thepitcannotbein[1,1],bytherulesof\nthe game, so there must be apitin [2,2] or[3,1] orboth. The notation \u201cP?\u201d inFigure 7.3(b)\nindicates apossible pitinthosesquares. Atthispoint, thereisonlyoneknownsquare thatis\nOKandthathasnotyetbeenvisited. Sotheprudentagentwillturnaround, gobackto[1,1],\nandthenproceed to[1,2].\nThe agent perceives a stench in [1,2], resulting in the state of knowledge shown in\nFigure 7.4(a). The stench in [1,2] means that there must be a wumpus nearby. But the Section7.2. TheWumpusWorld 239\n1,4 2,4 3,4 4,4 A = Agent 1,4 2,4 3,4 4,4\nB = Breeze\nG = Glitter, Gold\nOK = Safe square\n1,3 2,3 3,3 4,3 P = Pit 1,3 2,3 3,3 4,3\nS = Stench\nV = Visited\nW = Wumpus\n1,2 2,2 3,2 4,2 1,2 2,2 3,2 4,2\nP?\nOK OK\n1,1 2,1 3,1 4,1 1,1 2,1 3,1 4,1\nA P?\nA\nV B\nOK OK OK OK\n(a) (b)\nFigure 7.3 The first step taken by the agent in the wumpus world. (a) The initial sit-\nuation, after percept [None,None,None,None,None]. (b) After one move, with percept\n[None,Breeze,None,None,None].\n1,4 2,4 3,4 4,4 A = Agent 1,4 2,4 3,4 4,4\nP?\nB = Breeze\nG = Glitter, Gold\nOK = Safe square\n1,3 W! 2,3 3,3 4,3 P = Pit 1,3 W! 2,3 A 3,3 P? 4,3\nS = Stench\nS G\nV = Visited\nB\nW = Wumpus\n1,2 2,2 3,2 4,2 1,2 2,2 3,2 4,2\nA S\nS V V\nOK OK OK OK\n1,1 2,1 3,1 4,1 1,1 2,1 3,1 4,1\nB P! B P!\nV V V V\nOK OK OK OK\n(a) (b)\nFigure 7.4 Two later stages in the progress of the agent. (a) After the third move,\nwith percept [Stench,None,None,None,None]. (b) After the fifth move, with percept\n[Stench,Breeze,Glitter,None,None].\nwumpus cannot be in [1,1], by the rules of the game, and it cannot be in [2,2] (or the agent\nwould have detected a stench when it was in [2,1]). Therefore, the agent can infer that the\nwumpusisin[1,3]. Thenotation W!indicates thisinference. Moreover, thelack ofabreeze\nin[1,2]impliesthatthereisnopitin[2,2]. Yettheagenthasalready inferredthattheremust\nbe a pit in either [2,2] or [3,1], so this means it must be in [3,1]. This is a fairly difficult\ninference, because it combines knowledge gained at different times in different places and\nreliesonthelackofapercepttomakeonecrucialstep. 240 Chapter 7. LogicalAgents\nTheagenthasnowprovedtoitselfthatthereisneitherapitnorawumpusin[2,2],soit\nisOKtomovethere. Wedonotshowtheagent\u2019sstateofknowledgeat[2,2];wejustassume\nthat the agent turns and moves to [2,3], giving us Figure 7.4(b). In [2,3], the agent detects a\nglitter, soitshouldgrabthegoldandthenreturnhome.\nNote that in each case for which the agent draws a conclusion from the available in-\nformation, that conclusion is guaranteed tobecorrect iftheavailable information iscorrect.\nThis is a fundamental property of logical reasoning. In the rest of this chapter, we describe\nhowtobuildlogicalagentsthatcanrepresentinformationanddrawconclusionssuchasthose\ndescribed inthepreceding paragraphs.\n7.3 LOGIC\nThis section summarizes the fundamental concepts of logical representation and reasoning.\nThese beautiful ideas are independent of any of logic\u2019s particular forms. We therefore post-\npone the technical details of those forms until the next section, using instead the familiar\nexampleofordinary arithmetic.\nIn Section 7.1, we said that knowledge bases consist of sentences. These sentences\nareexpressed according tothe syntax oftherepresentation language, whichspecifies allthe\nSYNTAX\nsentences that are well formed. The notion of syntax is clear enough in ordinary arithmetic:\n\u201cx+y =4\u201disawell-formed sentence, whereas\u201cx4y+ =\u201disnot.\nAlogicmustalsodefinethesemanticsormeaningofsentences. Thesemanticsdefines\nSEMANTICS\nthe truth of each sentence with respect to each possible world. Forexample, the semantics\nTRUTH\nfor arithmetic specifies that the sentence \u201cx +y=4\u201d is true in a world where x is 2 and y\nPOSSIBLEWORLD\nis 2, but false in a world where x is 1 and y is1. In standard logics, every sentence must be\neithertrueorfalseineachpossible world\u2014there isno\u201cinbetween.\u201d1\nWhen we need to be precise, we use the term model in place of \u201cpossible world.\u201d\nMODEL\nWhereaspossibleworldsmightbethoughtofas(potentially)realenvironmentsthattheagent\nmight or might not be in, models are mathematical abstractions, each of which simply fixes\nthetruthorfalsehoodofeveryrelevantsentence. Informally,wemaythinkofapossibleworld\nas,forexample,havingxmenandywomensittingatatableplayingbridge,andthesentence\nx+y=4 is true when there are four people in total. Formally, the possible models are just\nallpossibleassignmentsofrealnumberstothevariables xandy. Eachsuchassignmentfixes\nthetruthofanysentence ofarithmetic whosevariables are xandy. Ifasentence \u03b1istruein\nmodel m, we say that m satisfies \u03b1 or sometimes m is a model of \u03b1. We use the notation\nSATISFACTION\nM(\u03b1)tomeanthesetofallmodelsof\u03b1.\nNowthat wehave a notion of truth, we are ready to talk about logical reasoning. This\ninvolves the relation of logical entailment between sentences\u2014the idea that a sentence fol-\nENTAILMENT\nlowslogically fromanothersentence. Inmathematicalnotation, wewrite\n\u03b1 |= \u03b2\n1 Fuzzylogic,discussedinChapter14,allowsfordegreesoftruth. Section7.3. Logic 241\n2 2 PIT 2 2 PIT \u03b1\nKB 1 1 Bree 2ze P 3IT \u03b1 1 1 Bre 2eze 3 KB 1 1 Bree 2ze P 3IT 1 1 Bree 2ze 3 2\n1\n2 PIT 2 2 PIT 2 PIT 2 2 PIT\n1 1 Bree 2ze 3 1 1 Bree 2ze 3 1 1 Bree 2ze P 3IT 1 1 Bre 2eze 3 1 1 Bree 2ze 3 1 1 Bre 2eze P 3IT\n2 PIT 2 PIT PIT 2 PIT 2 PIT PIT\n1 Breeze PIT 2 PIT PIT 1 1 Bre 2eze 3 1 Breeze PIT 2 PIT PIT 1 1 Bre 2eze 3\n1 2 3 1 2 3\n1 Breeze PIT 1 Breeze PIT\n1 2 3 1 2 3\n(a) (b)\nFigure7.5 Possiblemodelsforthepresenceofpitsinsquares[1,2],[2,2],and[3,1]. The\nKB correspondingto theobservationsofnothingin [1,1]anda breezein[2,1]isshownby\nthe solid line. (a) Dotted line shows modelsof \u03b1 (no pit in [1,2]). (b) Dotted line shows\n1\nmodelsof\u03b1 (nopitin[2,2]).\n2\ntomeanthatthesentence\u03b1entailsthesentence\u03b2. Theformaldefinitionofentailmentisthis:\n\u03b1 |= \u03b2 ifandonlyif,ineverymodelinwhich\u03b1istrue,\u03b2 isalsotrue. Usingthenotationjust\nintroduced, wecanwrite\n\u03b1 |= \u03b2 ifandonlyifM(\u03b1) \u2286 M(\u03b2).\n(Notethedirection ofthe\u2286here: if\u03b1 |= \u03b2,then\u03b1isastronger assertionthan\u03b2: itrulesout\nmore possible worlds.) The relation of entailment is familiar from arithmetic; we are happy\nwith the idea that the sentence x = 0 entails the sentence xy = 0. Obviously, in any model\nwherexiszero,itisthecasethatxy iszero(regardlessofthevalueof y).\nWecanapplythesamekindofanalysistothewumpus-worldreasoning examplegiven\nin the preceding section. Consider the situation in Figure 7.3(b): the agent has detected\nnothing in[1,1]andabreezein[2,1]. Thesepercepts, combined withtheagent\u2019s knowledge\nof the rules of the wumpus world, constitute the KB. The agent is interested (among other\nthings) in whether the adjacent squares [1,2], [2,2], and [3,1] contain pits. Each of the three\nsquaresmightormightnotcontainapit,so(forthepurposes ofthisexample)thereare23=8\npossible models. TheseeightmodelsareshowninFigure7.5.2\nThe KB can be thought of as a set of sentences or as a single sentence that asserts all\nthe individual sentences. The KB is false in models that contradict what the agent knows\u2014\nfor example, the KB is false in any model in which [1,2] contains a pit, because there is\nno breeze in[1,1]. Thereare in fact just three models inwhich the KBis true, and these are\n2 Althoughthefigureshowsthemodelsaspartialwumpusworlds,theyarereallynothingmorethanassignments\noftrue andfalsetothesentences\u201cthereisapitin[1,2]\u201detc. Models,inthemathematicalsense,donotneedto\nhave\u2019orrible\u2019airywumpusesinthem. 242 Chapter 7. LogicalAgents\nshownsurroundedbyasolidlineinFigure7.5. Nowletusconsidertwopossibleconclusions:\n\u03b1 = \u201cThereisnopitin[1,2].\u201d\n1\n\u03b1 = \u201cThereisnopitin[2,2].\u201d\n2\nWehavesurrounded themodelsof \u03b1 and\u03b1 withdotted linesinFigures7.5(a)and 7.5(b),\n1 2\nrespectively. Byinspection, weseethefollowing:\nineverymodelinwhichKB istrue, \u03b1 isalsotrue.\n1\nHence,KB |= \u03b1 : thereisnopitin[1,2]. Wecanalsoseethat\n1\ninsomemodelsinwhichKB istrue, \u03b1 isfalse.\n2\nHence,KB (cid:7)|= \u03b1 : theagentcannotconcludethatthereisnopitin[2,2]. (Norcanitconclude\n2\nthatthere isapitin[2,2].)3\nTheprecedingexamplenotonlyillustratesentailmentbutalsoshowshowthedefinition\nof entailment can be applied to derive conclusions\u2014that is, to carry out logical inference.\nLOGICALINFERENCE\nThe inference algorithm illustrated in Figure 7.5 is called model checking, because it enu-\nMODELCHECKING\nmeratesallpossible modelstocheckthat \u03b1istrueinallmodelsinwhich KB istrue, thatis,\nthatM(KB)\u2286 M(\u03b1).\nInunderstanding entailmentandinference,itmighthelptothinkofthesetofallconse-\nquences ofKB asahaystack andof\u03b1asaneedle. Entailmentisliketheneedlebeing inthe\nhaystack;inferenceislikefindingit. Thisdistinctionisembodiedinsomeformalnotation: if\naninferencealgorithm icanderive\u03b1from KB,wewrite\nKB (cid:20) \u03b1,\ni\nwhichispronounced \u201c\u03b1isderivedfrom KB byi\u201dor\u201ciderives \u03b1fromKB.\u201d\nAn inference algorithm that derives only entailed sentences is called sound or truth-\nSOUND\npreserving. Soundness is a highly desirable property. An unsound inference procedure es-\nTRUTH-PRESERVING\nsentiallymakesthingsupasitgoesalong\u2014itannouncesthediscoveryofnonexistentneedles.\nItiseasytoseethatmodelchecking, whenitisapplicable,4 isasoundprocedure.\nThe property of completeness is also desirable: an inference algorithm is complete if\nCOMPLETENESS\nit can derive any sentence that is entailed. For real haystacks, which are finite in extent,\nit seems obvious that a systematic examination can always decide whether the needle is in\nthehaystack. Formanyknowledge bases, however, thehaystack ofconsequences isinfinite,\nand completeness becomes an important issue.5 Fortunately, there are complete inference\nprocedures forlogicsthataresufficiently expressivetohandle manyknowledgebases.\nWe have described a reasoning process whose conclusions are guaranteed to be true\nin any world in which the premises are true; in particular, if KB is true in the real world,\nthenanysentence \u03b1derived fromKB byasoundinference procedure isalsotrueinthereal\nworld. So,whileaninferenceprocessoperateson\u201csyntax\u201d\u2014internal physicalconfigurations\nsuch as bits in registers or patterns of electrical blips in brains\u2014the process corresponds\n3 Theagentcancalculatetheprobabilitythatthereisapitin[2,2];Chapter13showshow.\n4 Model checking works if the space of models isfinite\u2014forexample, inwumpus worlds of fixedsize. For\narithmetic,ontheotherhand,thespaceofmodelsisinfinite: evenifwerestrictourselvestotheintegers,there\nareinfinitelymanypairsofvaluesforxandyinthesentencex+y=4.\n5 ComparewiththecaseofinfinitesearchspacesinChapter3,wheredepth-firstsearchisnotcomplete. Section7.4. Propositional Logic: AVerySimpleLogic 243\nSentences Sentence\nEntails\nRepresentation\nWorld\nAspects of the Aspect of the\nreal world Follows real world\nFigure7.6 Sentencesarephysicalconfigurationsoftheagent,andreasoningisaprocess\nof constructing new physical configurations from old ones. Logical reasoning should en-\nsurethatthenewconfigurationsrepresentaspectsoftheworldthatactuallyfollowfromthe\naspectsthattheoldconfigurationsrepresent.\nto the real-world relationship whereby some aspect of the real world is the case6 by virtue\nof other aspects of the real world being the case. This correspondence between world and\nrepresentation isillustrated inFigure7.6.\nThe final issue to consider is grounding\u2014the connection between logical reasoning\nGROUNDING\nprocesses andthe real environment inwhich theagent exists. Inparticular, how do weknow\nthat KB is true in the real world? (After all, KB is just \u201csyntax\u201d inside the agent\u2019s head.)\nThis is a philosophical question about which many, many books have been written. (See\nChapter26.) Asimpleansweristhattheagent\u2019ssensors createtheconnection. Forexample,\nour wumpus-world agent has a smell sensor. The agent program creates a suitable sentence\nwhenever there is a smell. Then, whenever that sentence is in the knowledge base, it is\ntrue in the real world. Thus, the meaning and truth of percept sentences are defined by the\nprocessesofsensingandsentenceconstruction thatproducethem. Whatabouttherestofthe\nagent\u2019s knowledge, such as its belief that wumpuses cause smells in adjacent squares? This\nis not a direct representation of a single percept, but a general rule\u2014derived, perhaps, from\nperceptual experience but not identical to a statement of that experience. General rules like\nthis are produced by a sentence construction process called learning, which is the subject\nof Part V. Learning is fallible. It could be the case that wumpuses cause smells except on\nFebruary29inleapyears,whichiswhentheytaketheirbaths. Thus,KB maynotbetruein\ntherealworld,butwithgoodlearning procedures, thereisreasonforoptimism.\n7.4 PROPOSITIONAL LOGIC: A VERY SIMPLE LOGIC\nPROPOSITIONAL Wenowpresentasimplebutpowerfullogiccalled propositional logic. Wecoverthesyntax\nLOGIC\nof propositional logic and its semantics\u2014the way in which the truth of sentences is deter-\nmined. Then we look at entailment\u2014the relation between a sentence and another sentence\nthatfollows from it\u2014andseehow thisleads toasimple algorithm forlogical inference. Ev-\nerything takesplace, ofcourse, inthewumpusworld.\n6 AsWittgenstein(1922)putitinhisfamousTractatus:\u201cTheworldiseverythingthatisthecase.\u201d\nSemantics Semantics 244 Chapter 7. LogicalAgents\n7.4.1 Syntax\nThe syntax of propositional logic defines the allowable sentences. The atomic sentences\nATOMICSENTENCES\nPROPOSITION consist of a single proposition symbol. Each such symbol stands for a proposition that can\nSYMBOL\nbe true or false. We use symbols that start with an uppercase letter and may contain other\nletters or subscripts, for example: P, Q, R, W and North. The names are arbitrary but\n1,3\nare often chosen to have some mnemonic value\u2014we use W to stand for the proposition\n1,3\nthat the wumpus is in [1,3]. (Remember that symbols such as W are atomic, i.e., W, 1,\n1,3\nand3arenotmeaningfulpartsofthesymbol.) Therearetwoproposition symbolswithfixed\nmeanings: True is the always-true proposition and False is the always-false proposition.\nCOMPLEX Complex sentences are constructed from simpler sentences, using parentheses and logical\nSENTENCES\nLOGICAL connectives. Therearefiveconnectives incommonuse:\nCONNECTIVES\n\u00ac (not). A sentence such as \u00acW is called the negation of W . A literal is either an\nNEGATION 1,3 1,3\natomicsentence(apositiveliteral)oranegatedatomicsentence (anegative literal).\nLITERAL\n\u2227 (and). A sentence whose main connective is \u2227, such as W \u2227P , is called a con-\n1,3 3,1\njunction;itspartsaretheconjuncts. (The\u2227lookslikean\u201cA\u201dfor\u201cAnd.\u201d)\nCONJUNCTION\n\u2228 (or). Asentenceusing\u2228,suchas(W \u2227P )\u2228W ,isadisjunctionofthedisjuncts\nDISJUNCTION 1,3 3,1 2,2\n(W \u2227P )andW . (Historically, the \u2228comes fromtheLatin\u201cvel,\u201dwhich means\n1,3 3,1 2,2\n\u201cor.\u201d Formostpeople, itiseasiertoremember \u2228asanupside-down \u2227.)\n\u21d2 (implies). Asentencesuchas(W \u2227P ) \u21d2 \u00acW iscalledanimplication(orcon-\nIMPLICATION 1,3 3,1 2,2\nditional). Itspremiseorantecedentis(W \u2227P ),anditsconclusionorconsequent\nPREMISE 1,3 3,1\nis\u00acW . Implications arealsoknownas rulesorif\u2013thenstatements. Theimplication\nCONCLUSION 2,2\nsymbolissometimeswritteninotherbooksas \u2283or\u2192.\nRULES\n\u21d4 (if and only if). The sentence W \u21d4 \u00acW is a biconditional. Some other books\nBICONDITIONAL 1,3 2,2\nwritethisas\u2261.\nSentence \u2192 AtomicSentence | ComplexSentence\nAtomicSentence \u2192 True | False | P | Q | R | ...\nComplexSentence \u2192 (Sentence )| [Sentence ]\n| \u00acSentence\n| Sentence \u2227Sentence\n| Sentence \u2228Sentence\n| Sentence \u21d2 Sentence\n| Sentence \u21d4 Sentence\nOPERATORPRECEDENCE : \u00ac,\u2227,\u2228,\u21d2,\u21d4\nFigure 7.7 A BNF (Backus\u2013Naur Form) grammar of sentences in propositional logic,\nalongwithoperatorprecedences,fromhighesttolowest. Section7.4. Propositional Logic: AVerySimpleLogic 245\nFigure7.7givesaformalgrammarofpropositional logic; seepage1060 ifyouarenot\nfamiliar with the BNF notation. The BNF grammar by itself is ambiguous; a sentence with\nseveraloperatorscanbeparsedbythegrammarinmultipleways. Toeliminatetheambiguity\nwedefineaprecedenceforeachoperator. The\u201cnot\u201doperator(\u00ac)hasthehighestprecedence,\nwhich means that in the sentence \u00acA\u2227B the \u00acbinds most tightly, giving us the equivalent\nof(\u00acA)\u2227Bratherthan\u00ac(A\u2227B). (Thenotationforordinaryarithmeticisthesame: \u22122+4\nis2,not\u20136.) Whenindoubt,useparentheses tomakesureoftherightinterpretation. Square\nbrackets meanthesamething asparentheses; thechoice ofsquare brackets orparentheses is\nsolelytomakeiteasierforahumantoreadasentence.\n7.4.2 Semantics\nHaving specified the syntax of propositional logic, we now specify its semantics. The se-\nmantics defines the rules for determining the truth of a sentence with respect to a particular\nmodel. Inpropositional logic, amodelsimply fixesthetruthvalue\u2014true orfalse\u2014forev-\nTRUTHVALUE\neryproposition symbol. Forexample,ifthesentencesinthe knowledgebasemakeuseofthe\nproposition symbols P ,P ,andP ,thenonepossible modelis\n1,2 2,2 3,1\nm = {P =false, P =false, P =true}.\n1 1,2 2,2 3,1\nWith three proposition symbols, there are 23=8 possible models\u2014exactly those depicted\nin Figure 7.5. Notice, however, that the models are purely mathematical objects with no\nnecessary connection to wumpus worlds. P is just asymbol; itmight mean \u201cthere is apit\n1,2\nin[1,2]\u201dor\u201cI\u2019minParistodayandtomorrow.\u201d\nThe semantics for propositional logic must specify how to compute the truth value of\nany sentence, given a model. This is done recursively. All sentences are constructed from\natomicsentences andthefiveconnectives; therefore, weneed tospecify how tocompute the\ntruthofatomicsentences andhowtocomputethetruthofsentences formedwitheachofthe\nfiveconnectives. Atomicsentences areeasy:\n\u2022 True istrueineverymodeland False isfalseineverymodel.\n\u2022 The truth value of every other proposition symbol must be specified directly in the\nmodel. Forexample,inthemodelm givenearlier, P isfalse.\n1 1,2\nForcomplexsentences, wehave fiverules, whichhold foranysubsentences P andQinany\nmodelm(here\u201ciff\u201dmeans\u201cifandonlyif\u201d):\n\u2022 \u00acP istrueiffP isfalseinm.\n\u2022 P \u2227QistrueiffbothP andQaretruein m.\n\u2022 P \u2228Qistrueiffeither P orQistrueinm.\n\u2022 P \u21d2 Qistrueunless P istrueandQisfalseinm.\n\u2022 P \u21d4 QistrueiffP andQarebothtrueorbothfalsein m.\nThe rules can also be expressed with truth tables that specify the truth value of a complex\nTRUTHTABLE\nsentence foreach possible assignment oftruth values toits components. Truthtables forthe\nfiveconnectives aregiven inFigure7.8. Fromthese tables, thetruth value ofanysentence s\ncanbecomputedwithrespecttoanymodelmbyasimplerecursiveevaluation. Forexample, 246 Chapter 7. LogicalAgents\nP Q \u00acP P \u2227Q P \u2228Q P \u21d2 Q P \u21d4 Q\nfalse false true false false true true\nfalse true true false true true false\ntrue false false false true false false\ntrue true false true true true true\nFigure7.8 Truthtablesforthefivelogicalconnectives. Tousethetabletocompute,for\nexample,thevalueofP \u2228QwhenP istrueandQisfalse,firstlookontheleftfortherow\nwhereP istrueandQisfalse(thethirdrow).ThenlookinthatrowundertheP\u2228Qcolumn\ntoseetheresult: true.\nthe sentence \u00acP \u2227(P \u2228 P ), evaluated in m , gives true \u2227(false \u2228true)=true \u2227\n1,2 2,2 3,1 1\ntrue=true. Exercise7.3asksyoutowritethealgorithm PL-TRUE?(s,m),whichcomputes\nthetruthvalueofapropositional logicsentence sinamodelm.\nThetruth tables for\u201cand,\u201d \u201cor,\u201d and\u201cnot\u201d areinclose accord withourintuitions about\ntheEnglishwords. ThemainpointofpossibleconfusionisthatP \u2228QistruewhenP istrue\nor Q is true or both. A different connective, called \u201cexclusive or\u201d (\u201cxor\u201d for short), yields\nfalse when both disjuncts are true.7 There is no consensus on the symbol for exclusive or;\nsomechoicesare\u2228\u02d9 or(cid:7)=or\u2295.\nThetruth table for \u21d2 maynot quite fitone\u2019s intuitive understanding of \u201cP implies Q\u201d\nor\u201cifP thenQ.\u201d Foronething,propositionallogicdoesnotrequireanyrelationofcausation\norrelevancebetweenP andQ. Thesentence\u201c5isoddimpliesTokyoisthecapitalofJapan\u201d\nis a true sentence of propositional logic (under the normal interpretation), even though it is\na decidedly odd sentence of English. Another point of confusion is that any implication is\ntrue whenever its antecedent is false. Forexample, \u201c5 is even implies Sam is smart\u201d is true,\nregardless of whether Sam is smart. This seems bizarre, but it makes sense if you think of\n\u201cP \u21d2 Q\u201dassaying,\u201cIfP istrue,thenIamclaimingthatQistrue. OtherwiseIammaking\nnoclaim.\u201d Theonlywayforthissentence tobefalseisifP istruebutQisfalse.\nThe biconditional, P \u21d4 Q, is true whenever both P \u21d2 Q and Q \u21d2 P are true. In\nEnglish,thisisoftenwrittenas\u201cP ifandonlyifQ.\u201d Manyoftherulesofthewumpusworld\nare best written using \u21d4. Forexample, a square is breezy if a neighboring square has a pit,\nandasquareisbreezy onlyifaneighboring squarehasapit. Soweneedabiconditional,\nB \u21d4 (P \u2228P ),\n1,1 1,2 2,1\nwhereB meansthatthereisabreezein[1,1].\n1,1\n7.4.3 A simpleknowledge base\nNowthatwehavedefinedthesemanticsforpropositionallogic,wecanconstructaknowledge\nbase for the wumpus world. We focus first on the immutable aspects of the wumpus world,\nleaving the mutable aspects for a later section. Fornow, weneed the following symbols for\neach[x,y]location:\n7 Latinhasaseparateword,aut,forexclusiveor. Section7.4. Propositional Logic: AVerySimpleLogic 247\nP istrueifthereisapitin[x,y].\nx,y\nW istrueifthereisawumpusin [x,y],deadoralive.\nx,y\nB istrueiftheagentperceives abreezein [x,y].\nx,y\nS istrueiftheagentperceivesastenchin [x,y].\nx,y\nThe sentences we write will suffice to derive \u00acP (there is no pit in [1,2]), as was done\n1,2\ninformally inSection7.3. Welabeleachsentence R sothatwecanrefertothem:\ni\n\u2022 Thereisnopitin[1,1]:\nR : \u00acP .\n1 1,1\n\u2022 A square is breezy if and only if there is a pit in a neighboring square. This has to be\nstatedforeachsquare; fornow,weincludejusttherelevant squares:\nR : B \u21d4 (P \u2228P ).\n2 1,1 1,2 2,1\nR : B \u21d4 (P \u2228P \u2228P ).\n3 2,1 1,1 2,2 3,1\n\u2022 The preceding sentences are true in all wumpus worlds. Now we include the breeze\nperceptsforthefirsttwosquaresvisitedinthespecificworldtheagentisin,leadingup\ntothesituation inFigure7.3(b).\nR : \u00acB .\n4 1,1\nR : B .\n5 2,1\n7.4.4 A simpleinference procedure\nOur goal now is to decide whether KB |= \u03b1 for some sentence \u03b1. For example, is \u00acP\n1,2\nentailedbyourKB? Ourfirstalgorithm forinference isamodel-checking approachthatisa\ndirect implementation of the definition of entailment: enumerate the models, and check that\n\u03b1 is true in every model in which KB is true. Models are assignments of true or false to\nevery proposition symbol. Returning to our wumpus-world example, the relevant proposi-\ntion symbols are B , B , P , P , P , P , and P . With seven symbols, there are\n1,1 2,1 1,1 1,2 2,1 2,2 3,1\n27=128 possible models; in three of these, KB is true (Figure 7.9). In those three models,\n\u00acP istrue,hencethereisnopitin[1,2]. Ontheotherhand, P istrueintwoofthethree\n1,2 2,2\nmodelsandfalseinone,sowecannotyettellwhetherthereisapitin[2,2].\nFigure7.9reproduces inamoreprecise form thereasoning illustrated inFigure7.5. A\ngeneralalgorithmfordecidingentailmentinpropositionallogicisshowninFigure7.10. Like\nthe BACKTRACKING-SEARCH algorithm on page 215, TT-ENTAILS? performs a recursive\nenumeration of afinite space of assignments to symbols. Thealgorithm is soundbecause it\nimplements directly thedefinition ofentailment, and completebecause itworksforany KB\nand\u03b1andalwaysterminates\u2014there areonlyfinitelymanymodelsto examine.\nOf course, \u201cfinitely many\u201d is not always the same as \u201cfew.\u201d If KB and \u03b1 contain n\nsymbols in all, then there are 2n models. Thus, the time complexity of the algorithm is\nO(2n). (ThespacecomplexityisonlyO(n)becausetheenumeration isdepth-first.) Laterin\nthis chapter weshow algorithms that are much more efficient in many cases. Unfortunately,\npropositional entailment isco-NP-complete (i.e.,probably noeasierthanNP-complete\u2014see\nAppendix A), so every known inference algorithm for propositional logic has a worst-case\ncomplexity thatisexponential inthesizeoftheinput. 248 Chapter 7. LogicalAgents\nB 1,1 B 2,1 P 1,1 P 1,2 P 2,1 P 2,2 P 3,1 R 1 R 2 R 3 R 4 R 5 KB\nfalse false false false false false false true true true true false false\nfalse false false false false false true true true false true false false\n. . . . . . . . . . . . .\n. . . . . . . . . . . . .\n. . . . . . . . . . . . .\nfalse true false false false false false true true false true true false\nfalse true false false false false true true true true true true true\nfalse true false false false true false true true true true true true\nfalse true false false false true true true true true true true true\nfalse true false false true false false true false false true true false\n. . . . . . . . . . . . .\n. . . . . . . . . . . . .\n. . . . . . . . . . . . .\ntrue true true true true true true false true true false true false\nFigure7.9 Atruthtableconstructedfortheknowledgebasegiveninthetext. KBistrue\nifR throughR aretrue,whichoccursinjust3ofthe128rows(theonesunderlinedinthe\n1 5\nright-handcolumn).Inall3rows,P 1,2isfalse,sothereisnopitin[1,2].Ontheotherhand,\ntheremight(ormightnot)beapitin[2,2].\nfunctionTT-ENTAILS?(KB,\u03b1)returnstrue orfalse\ninputs:KB,theknowledgebase,asentenceinpropositionallogic\n\u03b1,thequery,asentenceinpropositionallogic\nsymbols\u2190alistofthepropositionsymbolsinKB and\u03b1\nreturnTT-CHECK-ALL(KB,\u03b1,symbols,{})\nfunctionTT-CHECK-ALL(KB,\u03b1,symbols,model)returnstrue orfalse\nifEMPTY?(symbols)then\nifPL-TRUE?(KB,model)thenreturnPL-TRUE?(\u03b1,model)\nelsereturntrue \/\/ whenKBisfalse,alwaysreturntrue\nelsedo\nP\u2190FIRST(symbols)\nrest\u2190REST(symbols)\nreturn(TT-CHECK-ALL(KB,\u03b1,rest,model \u222a{P = true})\nand\nTT-CHECK-ALL(KB,\u03b1,rest,model \u222a{P = false }))\nFigure 7.10 A truth-table enumeration algorithm for deciding propositionalentailment.\n(TTstandsfortruthtable.) PL-TRUE? returnstrueifasentenceholdswithinamodel. The\nvariablemodelrepresentsapartialmodel\u2014anassignmenttosomeofthesymbols. Thekey-\nword\u201cand\u201disusedhereasalogicaloperationonitstwoarguments,returningtrue orfalse. Section7.5. Propositional TheoremProving 249\n(\u03b1\u2227\u03b2) \u2261 (\u03b2 \u2227\u03b1) commutativity of\u2227\n(\u03b1\u2228\u03b2) \u2261 (\u03b2 \u2228\u03b1) commutativity of\u2228\n((\u03b1\u2227\u03b2)\u2227\u03b3) \u2261 (\u03b1\u2227(\u03b2\u2227\u03b3)) associativity of\u2227\n((\u03b1\u2228\u03b2)\u2228\u03b3) \u2261 (\u03b1\u2228(\u03b2\u2228\u03b3)) associativity of\u2228\n\u00ac(\u00ac\u03b1) \u2261 \u03b1 double-negation elimination\n(\u03b1 \u21d2 \u03b2) \u2261 (\u00ac\u03b2 \u21d2 \u00ac\u03b1) contraposition\n(\u03b1 \u21d2 \u03b2) \u2261 (\u00ac\u03b1\u2228\u03b2) implication elimination\n(\u03b1 \u21d4 \u03b2) \u2261 ((\u03b1 \u21d2 \u03b2)\u2227(\u03b2 \u21d2 \u03b1)) biconditional elimination\n\u00ac(\u03b1\u2227\u03b2) \u2261 (\u00ac\u03b1\u2228\u00ac\u03b2) DeMorgan\n\u00ac(\u03b1\u2228\u03b2) \u2261 (\u00ac\u03b1\u2227\u00ac\u03b2) DeMorgan\n(\u03b1\u2227(\u03b2 \u2228\u03b3)) \u2261 ((\u03b1\u2227\u03b2)\u2228(\u03b1\u2227\u03b3)) distributivity of \u2227 over\u2228\n(\u03b1\u2228(\u03b2 \u2227\u03b3)) \u2261 ((\u03b1\u2228\u03b2)\u2227(\u03b1\u2228\u03b3)) distributivity of \u2228 over\u2227\nFigure7.11 Standardlogicalequivalences. Thesymbols\u03b1, \u03b2, and\u03b3 standforarbitrary\nsentencesofpropositionallogic.\n7.5 PROPOSITIONAL THEOREM PROVING\nSofar,wehaveshownhowtodetermineentailmentbymodelchecking: enumeratingmodels\nand showing that the sentence must hold in all models. In this section, weshow how entail-\nmentcanbedonebytheoremproving\u2014applying rulesofinferencedirectlytothesentences\nTHEOREMPROVING\ninourknowledgebasetoconstructaproofofthedesiredsentencewithoutconsultingmodels.\nIfthenumberofmodelsislargebutthelengthoftheproofisshort,thentheoremprovingcan\nbemoreefficientthanmodelchecking.\nBefore we plunge into the details of theorem-proving algorithms, we will need some\nLOGICAL additional concepts related toentailment. Thefirstconcept islogical equivalence: twosen-\nEQUIVALENCE\ntences \u03b1 and \u03b2 are logically equivalent if they are true in the same set of models. We write\nthis as \u03b1 \u2261 \u03b2. Forexample, wecan easily show (using truth tables) that P \u2227Q and Q\u2227P\nare logically equivalent; other equivalences are shown in Figure 7.11. These equivalences\nplay much the same role in logic as arithmetic identities do in ordinary mathematics. An\nalternative definition of equivalence is asfollows: any two sentences \u03b1 and \u03b2 are equivalent\nonlyifeachofthementailstheother:\n\u03b1 \u2261 \u03b2 ifandonlyif \u03b1 |= \u03b2 and\u03b2 |= \u03b1.\nThesecondconceptwewillneedisvalidity. Asentenceisvalidifitistrueinallmodels. For\nVALIDITY\nexample,thesentenceP \u2228\u00acP isvalid. Validsentencesarealsoknownastautologies\u2014they\nTAUTOLOGY\nare necessarily true. Because the sentence True is true in all models, every valid sentence\nis logically equivalent to True. What good are valid sentences? From our definition of\nDEDUCTION entailment, wecanderivethedeductiontheorem,whichwasknowntotheancientGreeks:\nTHEOREM\nForanysentences \u03b1and\u03b2,\u03b1 |= \u03b2 ifandonlyifthesentence(\u03b1 \u21d2 \u03b2)isvalid.\n(Exercise7.5asksforaproof.) Hence, wecandecide if \u03b1 |= \u03b2 bychecking that(\u03b1 \u21d2 \u03b2)is\ntrueineverymodel\u2014whichisessentiallywhattheinference algorithminFigure7.10does\u2014 250 Chapter 7. LogicalAgents\norby proving that (\u03b1 \u21d2 \u03b2) isequivalent toTrue. Conversely, the deduction theorem states\nthateveryvalidimplication sentence describes alegitimateinference.\nThe final concept we will need is satisfiability. A sentence is satisfiable if it is true\nSATISFIABILITY\nin, orsatisfied by, some model. Forexample, the knowledge base given earlier, (R \u2227R \u2227\n1 2\nR \u2227 R \u2227 R ), is satisfiable because there are three models in which it is true, as shown\n3 4 5\nin Figure 7.9. Satisfiability can be checked by enumerating the possible models until one is\nfound that satisfies the sentence. The problem of determining the satisfiability of sentences\ninpropositional logic\u2014the SATproblem\u2014was thefirstproblem proved tobeNP-complete.\nSAT\nMany problems in computer science are really satisfiability problems. For example, all the\nconstraint satisfaction problems in Chapter 6 ask whether the constraints are satisfiable by\nsomeassignment.\nValidity and satisfiability are of course connected: \u03b1 is valid iff \u00ac\u03b1 is unsatisfiable;\ncontrapositively, \u03b1issatisfiable iff\u00ac\u03b1isnotvalid. Wealsohavethefollowingusefulresult:\n\u03b1|= \u03b2 ifandonlyifthesentence (\u03b1\u2227\u00ac\u03b2)isunsatisfiable.\nProving \u03b2 from \u03b1 by checking the unsatisfiability of (\u03b1 \u2227 \u00ac\u03b2) corresponds exactly to the\nREDUCTIOAD standard mathematical proof technique of reductio ad absurdum (literally, \u201creduction to an\nABSURDUM\nabsurdthing\u201d). Itisalsocalledproofbyrefutationorproofbycontradiction. Oneassumesa\nREFUTATION\nsentence\u03b2tobefalseandshowsthatthisleadstoacontradiction withknownaxioms\u03b1. This\nCONTRADICTION\ncontradiction isexactlywhatismeantbysayingthatthesentence (\u03b1\u2227\u00ac\u03b2)isunsatisfiable.\n7.5.1 Inference andproofs\nThissectioncoversinferencerulesthatcanbeappliedtoderiveaproof\u2014achainofconclu-\nINFERENCERULES\nsions that leads tothe desired goal. Thebest-known rule is called ModusPonens(Latin for\nPROOF\nmodethataffirms)andiswritten\nMODUSPONENS\n\u03b1 \u21d2 \u03b2, \u03b1\n.\n\u03b2\nThe notation means that, whenever any sentences of the form \u03b1 \u21d2 \u03b2 and \u03b1 are given, then\nthesentence\u03b2canbeinferred. Forexample,if(WumpusAhead\u2227WumpusAlive) \u21d2 Shoot\nand(WumpusAhead \u2227WumpusAlive)aregiven,thenShoot canbeinferred.\nAnotherusefulinferenceruleisAnd-Elimination,whichsaysthat,fromaconjunction,\nAND-ELIMINATION\nanyoftheconjuncts canbeinferred:\n\u03b1\u2227\u03b2\n.\n\u03b1\nForexample,from (WumpusAhead \u2227WumpusAlive),WumpusAlive canbeinferred.\nBy considering the possible truth values of \u03b1 and \u03b2, one can show easily that Modus\nPonens and And-Elimination are sound once and for all. These rules can then be used in\nany particular instances where they apply, generating sound inferences without the need for\nenumerating models.\nAllofthelogicalequivalences inFigure7.11canbeusedasinferencerules. Forexam-\nple,theequivalence forbiconditional elimination yields thetwoinference rules\n\u03b1 \u21d4 \u03b2 (\u03b1 \u21d2 \u03b2)\u2227(\u03b2 \u21d2 \u03b1)\nand .\n(\u03b1 \u21d2 \u03b2)\u2227(\u03b2 \u21d2 \u03b1) \u03b1 \u21d4 \u03b2 Section7.5. Propositional TheoremProving 251\nNotall inference rules work inboth directions like this. Forexample, wecannot run Modus\nPonensintheoppositedirection toobtain \u03b1 \u21d2 \u03b2 and\u03b1from\u03b2.\nLetusseehowtheseinferencerulesandequivalencescanbeusedinthewumpusworld.\nWe start with the knowledge base containing R through R and show how to prove \u00acP ,\n1 5 1,2\nthatis,thereisnopitin[1,2]. First,weapplybiconditional elimination toR toobtain\n2\nR : (B \u21d2 (P \u2228P )) \u2227 ((P \u2228P ) \u21d2 B ).\n6 1,1 1,2 2,1 1,2 2,1 1,1\nThenweapplyAnd-Elimination toR toobtain\n6\nR : ((P \u2228P ) \u21d2 B ).\n7 1,2 2,1 1,1\nLogicalequivalence forcontrapositives gives\nR : (\u00acB \u21d2 \u00ac(P \u2228P )).\n8 1,1 1,2 2,1\nNowwecanapplyModusPonenswithR andthepercept R (i.e.,\u00acB ),toobtain\n8 4 1,1\nR : \u00ac(P \u2228P ).\n9 1,2 2,1\nFinally,weapplyDeMorgan\u2019srule,givingtheconclusion\nR : \u00acP \u2227\u00acP .\n10 1,2 2,1\nThatis,neither[1,2]nor[2,1]contains apit.\nWefoundthisproofbyhand,butwecanapplyanyofthesearchalgorithmsinChapter3\ntofindasequence ofstepsthatconstitutes aproof. Wejustneedtodefineaproofproblemas\nfollows:\n\u2022 INITIAL STATE: theinitialknowledgebase.\n\u2022 ACTIONS: the set of actions consists of all the inference rules applied to all the sen-\ntencesthatmatchthetophalfoftheinferencerule.\n\u2022 RESULT: theresultofanactionistoaddthesentenceinthebottomhalfoftheinference\nrule.\n\u2022 GOAL: thegoalisastatethatcontainsthesentence wearetryingtoprove.\nThus, searching for proofs is an alternative to enumerating models. In many practical cases\nfindingaproofcanbemoreefficient becausetheproofcanignoreirrelevantpropositions, no\nmatter how manyof them there are. Forexample, the proof given earlier leading to \u00acP \u2227\n1,2\n\u00acP does not mention the propositions B , P , P , or P . They can be ignored\n2,1 2,1 1,1 2,2 3,1\nbecausethegoalproposition, P ,appearsonlyinsentence R ;theotherpropositions inR\n1,2 2 2\nappearonlyinR andR ;soR ,R ,andR havenobearingontheproof. Thesamewould\n4 2 1 3 5\nholdevenifweaddedamillionmoresentencestotheknowledgebase;thesimpletruth-table\nalgorithm,ontheotherhand,wouldbeoverwhelmedbytheexponentialexplosionofmodels.\nOne final property of logical systems is monotonicity, which says that the set of en-\nMONOTONICITY\ntailed sentences can only increase as information is added to the knowledge base.8 Forany\nsentences \u03b1and\u03b2,\nif KB |= \u03b1 then KB \u2227\u03b2 |= \u03b1.\n8 Nonmonotoniclogics, whichviolatethemonotonicity property, capturea commonproperty ofhuman rea-\nsoning:changingone\u2019smind.TheyarediscussedinSection12.6. 252 Chapter 7. LogicalAgents\nForexample,supposetheknowledgebasecontainstheadditionalassertion\u03b2statingthatthere\nareexactlyeightpitsintheworld. Thisknowledgemighthelptheagentdrawadditionalcon-\nclusions, but it cannot invalidate any conclusion \u03b1 already inferred\u2014such as the conclusion\nthatthereisnopitin[1,2]. Monotonicitymeansthatinferencerulescanbeappliedwhenever\nsuitable premises are found in the knowledge base\u2014the conclusion of the rule must follow\nregardless ofwhatelseisintheknowledge base.\n7.5.2 Proofby resolution\nWehave argued that the inference rules covered so farare sound, but wehave not discussed\nthe question of completeness for the inference algorithms that use them. Search algorithms\nsuch as iterative deepening search (page 89) are complete in the sense that they will find\nany reachable goal, but if the available inference rules are inadequate, then the goal is not\nreachable\u2014no proof existsthatuses onlythose inference rules. Forexample, ifweremoved\nthe biconditional elimination rule, the proof in the preceding section would not go through.\nThe current section introduces a single inference rule, resolution, that yields a complete\ninference algorithm whencoupledwithanycompletesearchalgorithm.\nWebeginbyusingasimpleversion oftheresolution ruleinthewumpusworld. Letus\nconsider the steps leading up to Figure 7.4(a): the agent returns from [2,1] to [1,1] and then\ngoes to [1,2], where it perceives a stench, but no breeze. We add the following facts to the\nknowledgebase:\nR : \u00acB .\n11 1,2\nR : B \u21d4 (P \u2228P \u2228P ).\n12 1,2 1,1 2,2 1,3\nBy the same process that led to R earlier, we can now derive the absence of pits in [2,2]\n10\nand[1,3](rememberthat[1,1]isalreadyknowntobepitless):\nR : \u00acP .\n13 2,2\nR : \u00acP .\n14 1,3\nWe can also apply biconditional elimination to R , followed by Modus Ponens with R , to\n3 5\nobtainthefactthatthereisapitin[1,1],[2,2],or[3,1]:\nR : P \u2228P \u2228P .\n15 1,1 2,2 3,1\nNow comes the firstapplication of the resolution rule: the literal \u00acP in R resolves with\n2,2 13\ntheliteral P inR togivetheresolvent\nRESOLVENT 2,2 15\nR : P \u2228P .\n16 1,1 3,1\nInEnglish;ifthere\u2019sapitinoneof[1,1],[2,2],and[3,1]andit\u2019snotin[2,2],thenit\u2019sin[1,1]\nor[3,1]. Similarly,theliteral \u00acP inR resolveswiththeliteral P inR togive\n1,1 1 1,1 16\nR : P .\n17 3,1\nIn English: if there\u2019s a pit in [1,1] or [3,1] and it\u2019s not in [1,1], then it\u2019s in [3,1]. These last\ntwoinference stepsareexamplesoftheunitresolution inference rule,\nUNITRESOLUTION\n(cid:3) \u2228\u00b7\u00b7\u00b7\u2228(cid:3) , m\n1 k\n,\n(cid:3) 1\u2228\u00b7\u00b7\u00b7\u2228(cid:3) i\u22121\u2228(cid:3) i+1\u2228\u00b7\u00b7\u00b7\u2228(cid:3)\nk\nCOMPLEMENTARY where each (cid:3) is a literal and (cid:3) and m are complementary literals (i.e., one is the negation\nLITERALS i Section7.5. Propositional TheoremProving 253\nof the other). Thus, the unit resolution rule takes a clause\u2014a disjunction of literals\u2014and a\nCLAUSE\nliteralandproduces anewclause. Notethatasingleliteral canbeviewedasadisjunction of\noneliteral, alsoknownasaunitclause.\nUNITCLAUSE\nTheunitresolution rulecanbegeneralized tothefull resolution rule,\nRESOLUTION\n(cid:3) \u2228\u00b7\u00b7\u00b7\u2228(cid:3) , m \u2228\u00b7\u00b7\u00b7\u2228m\n1 k 1 n\n,\n(cid:3) 1\u2228\u00b7\u00b7\u00b7\u2228(cid:3) i\u22121\u2228(cid:3) i+1\u2228\u00b7\u00b7\u00b7\u2228(cid:3)\nk\n\u2228m 1\u2228\u00b7\u00b7\u00b7\u2228m j\u22121\u2228m j+1\u2228\u00b7\u00b7\u00b7\u2228m\nn\nwhere (cid:3) and m are complementary literals. Thissays that resolution takes twoclauses and\ni j\nproduces a new clause containing all the literals of the two original clauses except the two\ncomplementary literals. Forexample,wehave\nP \u2228P , \u00acP \u2228\u00acP\n1,1 3,1 1,1 2,2\n.\nP \u2228\u00acP\n3,1 2,2\nThereisonemoretechnical aspectoftheresolution rule: theresulting clauseshould contain\nonlyonecopyofeachliteral.9 Theremovalofmultiplecopies ofliterals iscalled factoring.\nFACTORING\nForexample,ifweresolve (A\u2228B)with(A\u2228\u00acB),weobtain(A\u2228A),whichisreducedto\njustA.\nThesoundnessoftheresolutionrulecanbeseeneasilybyconsidering theliteral(cid:3) that\ni\nis complementary to literal m in the other clause. If (cid:3) is true, then m is false, and hence\nj i j\nm 1\u2228\u00b7\u00b7\u00b7\u2228m j\u22121\u2228m j+1\u2228\u00b7\u00b7\u00b7\u2228m\nn\nmustbetrue, because m 1\u2228\u00b7\u00b7\u00b7\u2228m\nn\nisgiven. If(cid:3)\ni\nis\nfalse,then(cid:3) 1\u2228\u00b7\u00b7\u00b7\u2228(cid:3) i\u22121\u2228(cid:3) i+1\u2228\u00b7\u00b7\u00b7\u2228(cid:3)\nk\nmustbetruebecause (cid:3) 1\u2228\u00b7\u00b7\u00b7\u2228(cid:3)\nk\nisgiven. Now\n(cid:3) iseithertrueorfalse,sooneorotheroftheseconclusions holds\u2014exactly astheresolution\ni\nrulestates.\nWhatis moresurprising about theresolution rule is that itforms the basis forafamily\nofcomplete inference procedures. Aresolution-based theorem prover can, foranysentences\n\u03b1 and \u03b2 in propositional logic, decide whether \u03b1 |= \u03b2. The next two subsections explain\nhowresolution accomplishes this.\nConjunctivenormalform\nTheresolution ruleapplies onlytoclauses (that is,disjunctions ofliterals), soitwouldseem\nto be relevant only to knowledge bases and queries consisting of clauses. How, then, can\nit lead to a complete inference procedure for all of propositional logic? The answer is that\nevery sentence of propositional logic is logically equivalent to a conjunction of clauses. A\nCONJUNCTIVE sentence expressed as a conjunction of clauses is said to be in conjunctive normal form or\nNORMALFORM\nCNF (see Figure 7.14). We now describe a procedure for converting to CNF. We illustrate\nthe procedure by converting the sentence B \u21d4 (P \u2228P ) into CNF. The steps are as\n1,1 1,2 2,1\nfollows:\n1. Eliminate\u21d4,replacing \u03b1 \u21d4 \u03b2 with(\u03b1 \u21d2 \u03b2)\u2227(\u03b2 \u21d2 \u03b1).\n(B \u21d2 (P \u2228P ))\u2227((P \u2228P ) \u21d2 B ).\n1,1 1,2 2,1 1,2 2,1 1,1\n2. Eliminate\u21d2,replacing \u03b1 \u21d2 \u03b2 with\u00ac\u03b1\u2228\u03b2:\n(\u00acB \u2228P \u2228P )\u2227(\u00ac(P \u2228P )\u2228B ).\n1,1 1,2 2,1 1,2 2,1 1,1\n9 Ifaclauseisviewedasasetofliterals,thenthisrestrictionisautomaticallyrespected. Usingsetnotationfor\nclausesmakestheresolutionrulemuchcleaner,atthecostofintroducingadditionalnotation. 254 Chapter 7. LogicalAgents\n3. CNFrequires \u00ac to appear only in literals, so we \u201cmove \u00ac inwards\u201d by repeated appli-\ncationofthefollowingequivalences fromFigure7.11:\n\u00ac(\u00ac\u03b1) \u2261 \u03b1 (double-negation elimination)\n\u00ac(\u03b1\u2227\u03b2)\u2261 (\u00ac\u03b1\u2228\u00ac\u03b2) (DeMorgan)\n\u00ac(\u03b1\u2228\u03b2)\u2261 (\u00ac\u03b1\u2227\u00ac\u03b2) (DeMorgan)\nIntheexample,werequirejustoneapplication ofthelastrule:\n(\u00acB \u2228P \u2228P )\u2227((\u00acP \u2227\u00acP )\u2228B ).\n1,1 1,2 2,1 1,2 2,1 1,1\n4. Now we have a sentence containing nested \u2227 and \u2228 operators applied to literals. We\napplythedistributivity lawfromFigure7.11,distributing \u2228over\u2227whereverpossible.\n(\u00acB \u2228P \u2228P )\u2227(\u00acP \u2228B )\u2227(\u00acP \u2228B ).\n1,1 1,2 2,1 1,2 1,1 2,1 1,1\nThe original sentence is now in CNF,as aconjunction of three clauses. It is much harder to\nread,butitcanbeusedasinputtoaresolution procedure.\nAresolution algorithm\nInference procedures based on resolution work by using the principle of proof bycontradic-\ntion introduced on page 250. That is, to show that KB |= \u03b1, we show that (KB \u2227\u00ac\u03b1) is\nunsatisfiable. Wedothisbyproving acontradiction.\nA resolution algorithm is shown in Figure 7.12. First, (KB \u2227 \u00ac\u03b1) is converted into\nCNF. Then, the resolution rule is applied to the resulting clauses. Each pair that contains\ncomplementary literals is resolved to produce a new clause, which is added to the set if it is\nnotalreadypresent. Theprocesscontinues untiloneoftwothingshappens:\n\u2022 therearenonewclauses thatcanbeadded, inwhichcase KB doesnotentail\u03b1;or,\n\u2022 twoclausesresolvetoyieldtheemptyclause,inwhichcaseKB entails\u03b1.\nTheemptyclause\u2014adisjunctionofnodisjuncts\u2014isequivalenttoFalse becauseadisjunction\nis true only if at least one of its disjuncts is true. Another way to see that an empty clause\nrepresents acontradiction istoobservethatitarisesonly fromresolving twocomplementary\nunitclausessuchasP and\u00acP.\nWecanapplytheresolutionproceduretoaverysimpleinferenceinthewumpusworld.\nWhentheagentisin[1,1],thereisnobreeze, sotherecanbenopitsinneighboring squares.\nTherelevantknowledgebaseis\nKB = R \u2227R = (B \u21d4 (P \u2228P ))\u2227\u00acB\n2 4 1,1 1,2 2,1 1,1\nand we wish to prove \u03b1 which is, say, \u00acP . When we convert (KB \u2227\u00ac\u03b1) into CNF, we\n1,2\nobtain the clauses shown at the top of Figure 7.13. The second row of the figure shows\nclauses obtained by resolving pairs in the firstrow. Then, whenP isresolved with \u00acP ,\n1,2 1,2\nwe obtain the empty clause, shown as a small square. Inspection of Figure 7.13 reveals that\nmanyresolutionstepsarepointless. Forexample,theclauseB \u2228\u00acB \u2228P isequivalent\n1,1 1,1 1,2\nto True \u2228P which is equivalent to True. Deducing that True is true is not very helpful.\n1,2\nTherefore, anyclauseinwhichtwocomplementary literalsappearcanbediscarded. Section7.5. Propositional TheoremProving 255\nfunctionPL-RESOLUTION(KB,\u03b1)returnstrue orfalse\ninputs:KB,theknowledgebase,asentenceinpropositionallogic\n\u03b1,thequery,asentenceinpropositionallogic\nclauses\u2190thesetofclausesintheCNFrepresentationofKB \u2227\u00ac\u03b1\nnew\u2190{}\nloopdo\nforeachpairofclausesCi,Cj inclauses do\nresolvents\u2190PL-RESOLVE(Ci,Cj)\nifresolvents containstheemptyclausethenreturntrue\nnew\u2190new\u222a resolvents\nifnew \u2286clauses thenreturnfalse\nclauses\u2190clauses\u222anew\nFigure 7.12 A simple resolution algorithm for propositional logic. The function\nPL-RESOLVEreturnsthesetofallpossibleclausesobtainedbyresolvingitstwoinputs.\n\u00acP ^ B \u00acB ^ P ^ P \u00acP ^ B \u00acB P\n2,1 1,1 1,1 1,2 2,1 1,2 1,1 1,1 1,2\n\u00acB ^ P ^ B P ^ P ^ \u00acP \u00acB ^ P ^ B P ^ P ^ \u00acP \u00acP \u00acP\n1,1 1,2 1,1 1,2 2,1 2,1 1,1 2,1 1,1 1,2 2,1 1,2 2,1 1,2\nFigure7.13 PartialapplicationofPL-RESOLUTIONtoasimpleinferenceinthewumpus\nworld. \u00acP 1,2 isshowntofollowfromthefirstfourclausesinthetoprow.\nCompletenessofresolution\nToconclude ourdiscussion of resolution, wenow show why PL-RESOLUTION iscomplete.\nRESOLUTION Todothis,weintroducetheresolutionclosureRC(S)ofasetofclausesS,whichistheset\nCLOSURE\nof all clauses derivable by repeated application of the resolution rule to clauses in S ortheir\nderivatives. The resolution closure is what PL-RESOLUTION computes as the final value of\nthevariableclauses. ItiseasytoseethatRC(S)mustbefinite,becausethereareonlyfinitely\nmanydistinctclausesthatcanbeconstructed outofthesymbolsP ,...,P thatappearinS.\n1 k\n(Noticethatthiswouldnotbetruewithoutthefactoring stepthatremovesmultiplecopiesof\nliterals.) Hence, PL-RESOLUTION alwaysterminates.\nThe completeness theorem for resolution in propositional logic is called the ground\nGROUND\nresolution theorem:\nRESOLUTION\nTHEOREM\nIf a set of clauses is unsatisfiable, then the resolution closure of those clauses\ncontainstheemptyclause.\nThis theorem is proved by demonstrating its contrapositive: if the closure RC(S) does not 256 Chapter 7. LogicalAgents\ncontain the empty clause, then S is satisfiable. In fact, we can construct a model for S with\nsuitable truthvaluesforP ,...,P . Theconstruction procedure isasfollows:\n1 k\nForifrom1tok,\n\u2013 IfaclauseinRC(S)containstheliteral\u00acP andallitsotherliteralsarefalseunder\ni\ntheassignment chosenforP 1,...,P i\u22121,thenassignfalse toP i.\n\u2013 Otherwise,assign true toP .\ni\nThis assignment to P ,...,P is a model of S. To see this, assume the opposite\u2014that, at\n1 k\nsome stage i in the sequence, assigning symbol P causes some clause C to become false.\ni\nForthis tohappen, itmust bethe case that all the other literals in C mustalready have been\nfalsifiedbyassignments toP 1,...,P i\u22121. Thus,C mustnowlooklikeeither(false \u2228false \u2228\n\u00b7\u00b7\u00b7false\u2228P )orlike(false\u2228false\u2228\u00b7\u00b7\u00b7false\u2228\u00acP ). IfjustoneofthesetwoisinRC(S),then\ni i\nthe algorithm will assign the appropriate truth value to P to make C true, so C can only be\ni\nfalsifiedifbothoftheseclausesareinRC(S). Now,sinceRC(S)isclosedunderresolution,\nitwillcontaintheresolventofthesetwoclauses,andthatresolventwillhaveallofitsliterals\nalready falsified by the assignments to P 1,...,P i\u22121. This contradicts our assumption that\nthefirstfalsifiedclauseappearsatstage i. Hence,wehaveprovedthattheconstruction never\nfalsifies a clause in RC(S); that is, it produces a model of RC(S) and thus a model of S\nitself(sinceS iscontained inRC(S)).\n7.5.3 Hornclauses and definite clauses\nThecompletenessofresolutionmakesitaveryimportantinferencemethod. Inmanypractical\nsituations, however, the full power of resolution is not needed. Some real-world knowledge\nbases satisfy certain restrictions on the form of sentences they contain, which enables them\ntouseamorerestricted andefficientinference algorithm.\nOne such restricted form is the definite clause, which is a disjunction of literals of\nDEFINITECLAUSE\nwhichexactlyoneispositive. Forexample,theclause(\u00acL \u2228\u00acBreeze\u2228B )isadefinite\n1,1 1,1\nclause, whereas(\u00acB \u2228P \u2228P )isnot.\n1,1 1,2 2,1\nSlightlymoregeneral isthe Hornclause, whichisadisjunction ofliterals ofwhich at\nHORNCLAUSE\nmostone is positive. Soall definite clauses areHorn clauses, as areclauses with no positive\nliterals;thesearecalledgoalclauses. Hornclausesareclosedunderresolution: ifyouresolve\nGOALCLAUSES\ntwoHornclauses, yougetbackaHornclause.\nKnowledgebasescontaining onlydefiniteclausesareinteresting forthreereasons:\n1. Every definite clause can be written as an implication whose premise is a conjunction\nofpositiveliteralsandwhoseconclusionisasinglepositiveliteral. (SeeExercise7.13.)\nFor example, the definite clause (\u00acL \u2228 \u00acBreeze \u2228 B ) can be written as the im-\n1,1 1,1\nplication (L \u2227 Breeze) \u21d2 B . In the implication form, the sentence is easier to\n1,1 1,1\nunderstand: itsaysthatiftheagentisin[1,1]andthereisabreeze,then[1,1]isbreezy.\nIn Horn form, the premise is called the body and the conclusion is called the head. A\nBODY\nsentence consisting of a single positive literal, such as L , is called a fact. It too can\nHEAD 1,1\nbewritteninimplication formas True \u21d2 L ,butitissimplertowritejust L .\nFACT 1,1 1,1 Section7.5. Propositional TheoremProving 257\nCNFSentence \u2192 Clause\n1\n\u2227\u00b7\u00b7\u00b7\u2227Clausen\nClause \u2192 Literal\n1\n\u2228\u00b7\u00b7\u00b7\u2228Literalm\nLiteral \u2192 Symbol | \u00acSymbol\nSymbol \u2192 P | Q| R| ...\nHornClauseForm \u2192 DefiniteClauseForm | GoalClauseForm\nDefiniteClauseForm \u2192 (Symbol \u2227\u00b7\u00b7\u00b7\u2227Symbol ) \u21d2 Symbol\n1 l\nGoalClauseForm \u2192 (Symbol \u2227\u00b7\u00b7\u00b7\u2227Symbol ) \u21d2 False\n1 l\nFigure7.14 Agrammarforconjunctivenormalform,Hornclauses,anddefiniteclauses.\nAclausesuchasA\u2227B \u21d2 C isstilladefiniteclausewhenitiswrittenas\u00acA\u2228\u00acB\u2228C,\nbutonlytheformerisconsideredthecanonicalformfordefiniteclauses. Onemoreclassis\nthek-CNFsentence,whichisaCNFsentencewhereeachclausehasatmostkliterals.\n2. InferencewithHornclausescanbedonethroughtheforward-chainingandbackward-\nFORWARD-CHAINING\nBACKWARD- chaining algorithms, which we explain next. Both of these algorithms are natural,\nCHAINING\nin that the inference steps are obvious and easy for humans to follow. This type of\ninferenceisthebasisforlogicprogramming, whichisdiscussed inChapter9.\n3. Deciding entailment with Hornclauses can be done in timethat is linear inthe size of\ntheknowledge base\u2014apleasant surprise.\n7.5.4 Forwardandbackward chaining\nThe forward-chaining algorithm PL-FC-ENTAILS?(KB,q) determines if a single proposi-\ntion symbol q\u2014the query\u2014is entailed by a knowledge base of definite clauses. It begins\nfrom knownfacts (positive literals) intheknowledge base. Ifallthepremises ofanimplica-\ntion are known, then its conclusion is added to the set of known facts. For example, if L\n1,1\nandBreeze areknownand (L \u2227Breeze) \u21d2 B isintheknowledge base, thenB can\n1,1 1,1 1,1\nbeadded. Thisprocesscontinues untilthequery q isaddedoruntilnofurtherinferences can\nbemade. Thedetailedalgorithm isshowninFigure7.15;themainpointtorememberisthat\nitrunsinlineartime.\nThe best way to understand the algorithm is through an example and a picture. Fig-\nure 7.16(a) shows a simple knowledge base of Horn clauses with A and B as known facts.\nFigure 7.16(b) shows the same knowledge base drawn as an AND\u2013OR graph (see Chap-\nter 4). In AND\u2013OR graphs, multiple links joined by an arc indicate a conjunction\u2014every\nlink must be proved\u2014while multiple links without an arc indicate a disjunction\u2014any link\ncanbeproved. Itiseasytoseehowforwardchaining worksinthegraph. Theknownleaves\n(here, A and B) are set, and inference propagates up the graph as far as possible. Wher-\never a conjunction appears, the propagation waits until all the conjuncts are known before\nproceeding. Thereaderisencouraged toworkthroughtheexampleindetail. 258 Chapter 7. LogicalAgents\nfunctionPL-FC-ENTAILS?(KB,q)returnstrue orfalse\ninputs:KB,theknowledgebase,asetofpropositionaldefiniteclauses\nq,thequery,apropositionsymbol\ncount\u2190atable,wherecount[c]isthenumberofsymbolsinc\u2019spremise\ninferred\u2190atable,whereinferred[s]isinitiallyfalse forallsymbols\nagenda\u2190aqueueofsymbols,initiallysymbolsknowntobetrueinKB\nwhileagenda isnotemptydo\np\u2190POP(agenda)\nifp =q thenreturntrue\nifinferred[p]=false then\ninferred[p]\u2190true\nforeachclausec inKB wherep isinc.PREMISEdo\ndecrementcount[c]\nifcount[c]=0thenaddc.CONCLUSIONtoagenda\nreturnfalse\nFigure7.15 Theforward-chainingalgorithmforpropositionallogic. Theagenda keeps\ntrackofsymbolsknownto betruebutnotyet\u201cprocessed.\u201d The count tablekeepstrackof\nhowmanypremisesofeachimplicationareasyetunknown.Wheneveranewsymbolpfrom\ntheagendaisprocessed,thecountisreducedbyoneforeachimplicationinwhosepremise\np appears(easily identified in constanttime with appropriateindexing.) If a countreaches\nzero, all the premises of the implication are known, so its conclusion can be added to the\nagenda.Finally,weneedtokeeptrackofwhichsymbolshavebeenprocessed;asymbolthat\nisalreadyinthesetofinferredsymbolsneednotbeaddedtotheagendaagain. Thisavoids\nredundantworkandpreventsloopscausedbyimplicationssuchasP \u21d2QandQ\u21d2P.\nItiseasytoseethatforward chaining issound: everyinference isessentially anappli-\ncationofModusPonens. Forwardchaining isalsocomplete: everyentailed atomicsentence\nwillbederived. Theeasiest waytoseethis istoconsider the finalstate oftheinferred table\n(after the algorithm reaches a fixed point where no new inferences are possible). The table\nFIXEDPOINT\ncontains true for each symbol inferred during the process, and false for all other symbols.\nWecanviewthetableasalogicalmodel;moreover,everydefiniteclauseintheoriginalKBis\ntrueinthismodel. Toseethis,assumetheopposite,namelythatsomeclausea \u2227...\u2227a \u21d2 b\n1 k\nis false in the model. Then a \u2227...\u2227a must be true in the model and b must be false in\n1 k\nthe model. But this contradicts ourassumption that the algorithm has reached a fixed point!\nWecanconclude,therefore,thatthesetofatomicsentences inferredatthefixedpointdefines\na model of the original KB. Furthermore, any atomic sentence q that is entailed by the KB\nmust be true in all its models and in this model in particular. Hence, every entailed atomic\nsentence q mustbeinferred bythealgorithm.\nForwardchainingisanexampleofthegeneralconceptof data-drivenreasoning\u2014that\nDATA-DRIVEN\nis,reasoninginwhichthefocusofattention startswiththeknowndata. Itcanbeusedwithin\nan agent to derive conclusions from incoming percepts, often without a specific query in\nmind. Forexample, thewumpusagent might TELL itspercepts totheknowledge baseusing Section7.6. EffectivePropositional ModelChecking 259\nQ\nP \u21d2 Q\nL\u2227M \u21d2 P P\nB\u2227L \u21d2 M\nA\u2227P \u21d2 L M\nA\u2227B \u21d2 L\nL\nA\nB\nA B\n(a) (b)\nFigure7.16 (a)AsetofHornclauses. (b)ThecorrespondingAND\u2013ORgraph.\nanincremental forward-chaining algorithm inwhichnewfactscanbeaddedtotheagendato\ninitiate newinferences. Inhumans, acertain amount ofdata-driven reasoning occurs asnew\ninformation arrives. Forexample,ifIamindoorsandhearrainstartingtofall,itmightoccur\ntomethatthepicnicwillbecanceled. Yetitwillprobablynotoccurtomethattheseventeenth\npetalonthelargestroseinmyneighbor\u2019sgardenwillgetwet;humanskeepforwardchaining\nundercarefulcontrol, lesttheybeswampedwithirrelevant consequences.\nThe backward-chaining algorithm, as its name suggests, works backward from the\nquery. Ifthe query q is known to be true, then no work is needed. Otherwise, the algorithm\nfinds those implications inthe knowledge base whose conclusion is q. If all the premises of\none of those implications can be proved true (by backward chaining), then q is true. When\napplied to the query Q in Figure 7.16, it works back downthe graph until it reaches a set of\nknownfacts,AandB,thatformsthebasisforaproof. Thealgorithm isessentially identical\nto the AND-OR-GRAPH-SEARCH algorithm in Figure 4.11. As with forward chaining, an\nefficientimplementation runsinlineartime.\nGOAL-DIRECTED Backward chaining is a form of goal-directed reasoning. It is useful for answering\nREASONING\nspecificquestionssuchas\u201cWhatshallIdonow?\u201d and\u201cWherearemykeys?\u201d Often,thecost\nofbackward chaining is muchlessthanlinearinthesizeoftheknowledge base, because the\nprocesstouches onlyrelevantfacts.\n7.6 EFFECTIVE PROPOSITIONAL MODEL CHECKING\nIn this section, we describe two families of efficient algorithms for general propositional\ninference based on model checking: One approach based on backtracking search, and one\nonlocalhill-climbing search. Thesealgorithms arepartof the\u201ctechnology\u201d ofpropositional\nlogic. Thissectioncanbeskimmedonafirstreadingofthechapter. 260 Chapter 7. LogicalAgents\nThealgorithmswedescribeareforcheckingsatisfiability: theSATproblem. (Asnoted\nearlier, testing entailment, \u03b1 |= \u03b2, can be done by testing unsatisfiability of \u03b1\u2227 \u00ac\u03b2.) We\nhave already noted the connection between finding a satisfying model fora logical sentence\nandfindingasolutionforaconstraintsatisfactionproblem,soitisperhapsnotsurprisingthat\nthe two families of algorithms closely resemble the backtracking algorithms of Section 6.3\nand the local search algorithms of Section 6.4. They are, however, extremely important in\ntheirownrightbecausesomanycombinatorialproblemsincomputersciencecanbereduced\nto checking the satisfiability of a propositional sentence. Any improvement in satisfiability\nalgorithms hashugeconsequences forourabilitytohandlecomplexityingeneral.\n7.6.1 A completebacktracking algorithm\nDAVIS\u2013PUTNAM Thefirstalgorithm weconsider isoften called the Davis\u2013Putnamalgorithm, afterthesem-\nALGORITHM\ninal paper by Martin Davis and Hilary Putnam (1960). The algorithm is in fact the version\ndescribed by Davis, Logemann, and Loveland (1962), so we will call it DPLL after the ini-\ntials of all four authors. DPLL takes as input a sentence in conjunctive normal form\u2014a set\nof clauses. Like BACKTRACKING-SEARCH and TT-ENTAILS?, it is essentially a recursive,\ndepth-first enumeration ofpossible models. Itembodies threeimprovements overthesimple\nschemeof TT-ENTAILS?:\n\u2022 Early termination: The algorithm detects whether the sentence must be true or false,\neven with a partially completed model. A clause is true if any literal is true, even if\nthe other literals do not yet have truth values; hence, the sentence as a whole could be\njudged true even before the model is complete. For example, the sentence (A\u2228B)\u2227\n(A\u2228C)istrue if Aistrue, regardless of thevalues of B and C. Similarly, asentence\nisfalseifanyclauseisfalse,whichoccurswheneachofitsliteralsisfalse. Again,this\ncan occurlong before the model iscomplete. Early termination avoids examination of\nentiresubtrees inthesearchspace.\n\u2022 Pure symbol heuristic: A puresymbol is a symbol that always appears with the same\nPURESYMBOL\n\u201csign\u201d in all clauses. For example, in the three clauses (A \u2228 \u00acB), (\u00acB \u2228 \u00acC), and\n(C \u2228 A), the symbol A is pure because only the positive literal appears, B is pure\nbecause only the negative literal appears, and C is impure. It is easy to see that if\na sentence has a model, then it has a model with the pure symbols assigned so as to\nmake their literals true, because doing so can never make a clause false. Note that, in\ndetermining the purity of a symbol, the algorithm can ignore clauses that are already\nknown to be true in the model constructed so far. For example, if the model contains\nB=false, then the clause (\u00acB \u2228\u00acC)is already true, and in the remaining clauses C\nappearsonlyasapositiveliteral;therefore C becomespure.\n\u2022 Unit clause heuristic: A unit clause was defined earlier as a clause with just one lit-\neral. In the context of DPLL, it also means clauses in which all literals but one are\nalready assigned false by the model. For example, if the model contains B=true,\nthen (\u00acB \u2228\u00acC) simplifies to \u00acC, which is a unit clause. Obviously, for this clause\nto be true, C must be set to false. The unit clause heuristic assigns all such symbols\nbefore branching on the remainder. Oneimportant consequence ofthe heuristic is that Section7.6. EffectivePropositional ModelChecking 261\nfunctionDPLL-SATISFIABLE?(s)returnstrue orfalse\ninputs:s,asentenceinpropositionallogic\nclauses\u2190thesetofclausesintheCNFrepresentationofs\nsymbols\u2190alistofthepropositionsymbolsins\nreturnDPLL(clauses,symbols,{})\nfunctionDPLL(clauses,symbols,model)returnstrue orfalse\nifeveryclauseinclauses istrueinmodel thenreturntrue\nifsomeclauseinclauses isfalseinmodel thenreturnfalse\nP,value\u2190FIND-PURE-SYMBOL(symbols,clauses,model)\nifP isnon-nullthenreturnDPLL(clauses,symbols \u2013P,model \u222a{P=value})\nP,value\u2190FIND-UNIT-CLAUSE(clauses,model)\nifP isnon-nullthenreturnDPLL(clauses,symbols \u2013P,model \u222a{P=value})\nP\u2190FIRST(symbols);rest\u2190REST(symbols)\nreturnDPLL(clauses,rest,model \u222a{P=true})or\nDPLL(clauses,rest,model \u222a{P=false}))\nFigure7.17 TheDPLLalgorithmforcheckingsatisfiabilityofasentenceinpropositional\nlogic. The ideas behind FIND-PURE-SYMBOL and FIND-UNIT-CLAUSE are described in\nthe text; each returnsa symbol(or null) and the truth value to assign to that symbol. Like\nTT-ENTAILS?,DPLLoperatesoverpartialmodels.\nany attempt toprove (by refutation) a literal that is already in the knowledge base will\nsucceed immediately (Exercise 7.23). Notice also that assigning one unit clause can\ncreate another unit clause\u2014for example, when C is set to false, (C \u2228 A) becomes a\nunit clause, causing true to be assigned to A. This \u201ccascade\u201d of forced assignments\nis called unitpropagation. It resembles the process of forward chaining with definite\nUNITPROPAGATION\nclauses, and indeed, if the CNF expression contains only definite clauses then DPLL\nessentially replicates forwardchaining. (SeeExercise7.24.)\nThe DPLL algorithm is shown in Figure 7.17, which gives the the essential skeleton of the\nsearchprocess.\nWhat Figure 7.17 does not show are the tricks that enable SAT solvers to scale up to\nlarge problems. It is interesting that most of these tricks are in fact rather general, and we\nhaveseenthembeforeinotherguises:\n1. Componentanalysis (asseenwithTasmaniainCSPs): AsDPLL assigns truthvalues\ntovariables, thesetofclauses maybecomeseparated intodisjoint subsets, calledcom-\nponents,thatsharenounassigned variables. Givenanefficientwaytodetectwhenthis\noccurs,asolvercangainconsiderablespeedbyworkingoneachcomponentseparately.\n2. Variable and value ordering (as seen in Section 6.3.1 for CSPs): Our simple imple-\nmentation of DPLL uses an arbitrary variable ordering and always tries the value true\nbefore false. The degree heuristic (see page 216) suggests choosing the variable that\nappearsmostfrequently overallremainingclauses. 262 Chapter 7. LogicalAgents\n3. Intelligent backtracking (as seen in Section 6.3 for CSPs): Many problems that can-\nnot be solved in hours of run time with chronological backtracking can be solved in\nseconds with intelligent backtracking that backs up all the way to the relevant point of\nconflict. All SAT solvers that do intelligent backtracking use some form of conflict\nclause learning to record conflicts so that they won\u2019t be repeated later in the search.\nUsuallyalimited-size setofconflictsiskept,andrarelyusedonesaredropped.\n4. Randomrestarts(asseenonpage124forhill-climbing): Sometimesarunappearsnot\nto be making progress. In this case, we can start over from the top of the search tree,\nrather than trying to continue. After restarting, different random choices (in variable\nandvalueselection) aremade. Clausesthatarelearnedinthefirstrunareretainedafter\nthe restart and can help prune the search space. Restarting does not guarantee that a\nsolutionwillbefoundfaster, butitdoesreducethevariance onthetimetosolution.\n5. Clever indexing (as seen in many algorithms): The speedup methods used in DPLL\nitself, aswell asthe tricks used in modern solvers, require fast indexing ofsuch things\nas \u201cthe set of clauses in which variable X appears as a positive literal.\u201d This task is\ni\ncomplicated by the fact that the algorithms are interested only in the clauses that have\nnot yet been satisfied by previous assignments to variables, so the indexing structures\nmustbeupdateddynamically asthecomputation proceeds.\nWiththeseenhancements, modernsolverscanhandleproblemswithtensofmillionsofvari-\nables. They have revolutionized areas such as hardware verification and security protocol\nverification, whichpreviously required laborious, hand-guided proofs.\n7.6.2 Local search algorithms\nWehaveseenseverallocalsearchalgorithmssofarinthisbook,including HILL-CLIMBING\n(page 122) and SIMULATED-ANNEALING (page 126). These algorithms can be applied di-\nrectly to satisfiability problems, provided that we choose the right evaluation function. Be-\ncause thegoalistofindanassignment that satisfieseveryclause, anevaluation function that\ncounts the number of unsatisfied clauses will do the job. In fact, this is exactly the measure\nusedbytheMIN-CONFLICTS algorithmforCSPs(page221). Allthesealgorithmstakesteps\nin the space of complete assignments, flipping the truth value of one symbol at a time. The\nspace usually contains many local minima, to escape from which various forms of random-\nness are required. In recent years, there has been a great deal of experimentation to find a\ngoodbalancebetweengreediness andrandomness.\nOneofthesimplestandmosteffectivealgorithmstoemergefromallthisworkiscalled\nWALKSAT (Figure 7.18). On every iteration, the algorithm picks an unsatisfied clause and\npicks a symbol in the clause to flip. It chooses randomly between two ways to pick which\nsymboltoflip: (1)a\u201cmin-conflicts\u201d stepthatminimizesthenumberofunsatisfied clausesin\nthenewstateand(2)a\u201crandom walk\u201dstepthatpicksthesymbolrandomly.\nWhen WALKSAT returns a model, the input sentence is indeed satisfiable, but when\nit returns failure, there are two possible causes: either the sentence is unsatisfiable or we\nneed togive the algorithm more time. If weset max flips=\u221e and p > 0, WALKSAT will\neventually return a model (if one exists), because the random-walk steps will eventually hit Section7.6. EffectivePropositional ModelChecking 263\nfunctionWALKSAT(clauses,p,max flips)returnsasatisfyingmodelorfailure\ninputs:clauses,asetofclausesinpropositionallogic\np,theprobabilityofchoosingtodoa\u201crandomwalk\u201dmove,typicallyaround0.5\nmax flips,numberofflipsallowedbeforegivingup\nmodel\u2190arandomassignmentoftrue\/false tothesymbolsinclauses\nfori= 1tomax flips do\nifmodel satisfiesclauses thenreturnmodel\nclause\u2190arandomlyselectedclausefromclauses thatisfalseinmodel\nwithprobabilitypflipthevalueinmodel ofarandomlyselectedsymbolfromclause\nelseflipwhicheversymbolinclause maximizesthenumberofsatisfiedclauses\nreturnfailure\nFigure 7.18 The WALKSAT algorithm for checking satisfiability by randomly flipping\nthevaluesofvariables.Manyversionsofthealgorithmexist.\nupon the solution. Alas, if max flips is infinity and the sentence is unsatisfiable, then the\nalgorithm neverterminates!\nForthisreason, WALKSAT ismostuseful whenweexpect asolution toexist\u2014forex-\nample,theproblemsdiscussedinChapters3and6usuallyhavesolutions. Ontheotherhand,\nWALKSAT cannot always detect unsatisfiability, which is required for deciding entailment.\nFor example, an agent cannot reliably use WALKSAT to prove that a square is safe in the\nwumpusworld. Instead, itcansay,\u201cIthoughtaboutitforanhourandcouldn\u2019tcomeupwith\napossible worldinwhichthesquare isn\u2019tsafe.\u201d Thismaybeagoodempirical indicator that\nthesquareissafe,butit\u2019scertainly notaproof.\n7.6.3 The landscapeofrandom SATproblems\nSome SAT problems are harder than others. Easy problems can be solved by any old algo-\nrithm, but because weknow that SATisNP-complete, atleast someproblem instances must\nrequireexponential runtime. InChapter6,wesawsomesurprisingdiscoveries aboutcertain\nkindsofproblems. Forexample,the n-queensproblem\u2014thought tobequitetrickyforback-\ntracking search algorithms\u2014turned outtobetrivially easy forlocal search methods, such as\nmin-conflicts. This is because solutions are very densely distributed in the space of assign-\nments, and anyinitial assignment isguaranteed tohaveasolution nearby. Thus, n-queens is\neasybecause itisunderconstrained.\nUNDERCONSTRAINED\nWhen we look at satisfiability problems in conjunctive normal form, an undercon-\nstrained problem is one with relatively few clauses constraining the variables. For example,\nhereisarandomlygenerated 3-CNFsentence withfivesymbols andfiveclauses:\n(\u00acD\u2228\u00acB\u2228C)\u2227(B\u2228\u00acA\u2228\u00acC)\u2227(\u00acC \u2228\u00acB\u2228E)\n\u2227(E \u2228\u00acD\u2228B)\u2227(B\u2228E \u2228\u00acC).\nSixteen of the 32 possible assignments are models of this sentence, so, on average, it would\ntake just two random guesses to find a model. This is an easy satisfiability problem, as are 264 Chapter 7. LogicalAgents\nmost such underconstrained problems. On the other hand, an overconstrained problem has\nmanyclauses relativetothenumberofvariablesandislikelytohavenosolutions.\nTo go beyond these basic intuitions, we must define exactly how random sentences\nare generated. The notation CNF (m,n) denotes a k-CNF sentence with m clauses and n\nk\nsymbols, where the clauses are chosen uniformly, independently, and without replacement\nfromamongallclauseswithkdifferentliterals,whicharepositiveornegativeatrandom. (A\nsymbolmaynotappeartwiceinaclause,normayaclauseappeartwiceinasentence.)\nGiven a source of random sentences, we can measure the probability of satisfiability.\nFigure 7.19(a) plots the probability for CNF (m,50), that is, sentences with 50 variables\n3\nand 3 literals per clause, as a function of the clause\/symbol ratio, m\/n. As we expect, for\nsmall m\/n the probability of satisfiability is close to 1, and at large m\/n the probability\nis close to 0. The probability drops fairly sharply around m\/n=4.3. Empirically, we find\nthat the \u201ccliff\u201d stays in roughly the same place (for k=3)and gets sharper and sharper as n\nSATISFIABILITY increases. Theoretically, the satisfiability threshold conjecture says that for every k \u2265 3,\nTHRESHOLD\nCONJECTURE\nthereisathreshold ratio r suchthat,asngoestoinfinity,theprobability thatCNF (n,rn)\nk k\nis satisfiable becomes 1 for all values of r below the threshold, and 0 for all values above.\nTheconjecture remainsunproven.\n2000\n1\n1800 DPLL\nWalkSAT\n0.8 1600\n1400\n0.6 1200\n1000\n0.4 800\n600\n0.2 400\n200\n0\n0\n0 1 2 3 4 5 6 7 8 0 1 2 3 4 5 6 7 8\nClause\/symbol ratio m\/n Clause\/symbol ratio m\/n\n(a) (b)\nFigure7.19 (a)Graphshowingtheprobabilitythatarandom3-CNFsentencewithn=50\nsymbolsissatisfiable,asafunctionoftheclause\/symbolratiom\/n.(b)Graphofthemedian\nruntime(measuredinnumberofrecursivecallstoDPLL,agoodproxy)onrandom3-CNF\nsentences.Themostdifficultproblemshaveaclause\/symbolratioofabout4.3.\nNowthatwehaveagood ideawherethesatisfiable andunsatisfiable problems are, the\nnext question is, where are the hard problems? It turns out that they are also often at the\nthreshold value. Figure7.19(b) showsthat 50-symbol problems atthe threshold value of4.3\nare about 20 times more difficult to solve than those at a ratio of 3.3. The underconstrained\nproblems are easiest to solve (because it is so easy to guess a solution); the overconstrained\nproblemsarenotaseasyastheunderconstrained, butstillaremucheasierthantheonesright\natthethreshold.\n)elbaifsitas(P\nemitnuR Section7.7. AgentsBasedonPropositional Logic 265\n7.7 AGENTS BASED ON PROPOSITIONAL LOGIC\nIn this section, webring together what we have learned so far in order to construct wumpus\nworldagentsthatusepropositionallogic. Thefirststepistoenabletheagenttodeduce,tothe\nextent possible, thestate oftheworldgiven itspercept history. Thisrequires writing downa\ncompletelogicalmodeloftheeffectsofactions. Wealsoshowhowtheagentcankeeptrackof\nthe world efficiently without going back into the percept history for each inference. Finally,\nwe show how the agent can use logical inference to construct plans that are guaranteed to\nachieveitsgoals.\n7.7.1 The current stateofthe world\nAs stated at the beginning of the chapter, a logical agent operates by deducing what to do\nfrom a knowledge base of sentences about the world. The knowledge base is composed of\naxioms\u2014general knowledge about how the world works\u2014and percept sentences obtained\nfromtheagent\u2019sexperienceinaparticularworld. Inthissection,wefocusontheproblemof\ndeducing thecurrentstateofthewumpusworld\u2014whereamI,is thatsquaresafe,andsoon.\nWebegancollecting axioms inSection 7.4.3. Theagent knows thatthe starting square\ncontainsnopit(\u00acP )andnowumpus(\u00acW ). Furthermore, foreachsquare, itknowsthat\n1,1 1,1\nthesquareisbreezyifandonlyifaneighboringsquarehasapit;andasquareissmellyifand\nonlyifaneighboring squarehasawumpus. Thus,weinclude alargecollection ofsentences\nofthefollowingform:\nB \u21d4 (P \u2228P )\n1,1 1,2 2,1\nS \u21d4 (W \u2228W )\n1,1 1,2 2,1\n\u00b7\u00b7\u00b7\nTheagentalsoknowsthatthereisexactlyonewumpus. Thisisexpressed intwoparts. First,\nwehavetosaythatthereisatleastonewumpus:\nW \u2228W \u2228\u00b7\u00b7\u00b7\u2228W \u2228W .\n1,1 1,2 4,3 4,4\nThen, wehave tosaythat there is atmostonewumpus. Foreach pairoflocations, weadd a\nsentence sayingthatatleastoneofthemmustbewumpus-free:\n\u00acW \u2228\u00acW\n1,1 1,2\n\u00acW \u2228\u00acW\n1,1 1,3\n\u00b7\u00b7\u00b7\n\u00acW \u2228\u00acW .\n4,3 4,4\nSo far, so good. Now let\u2019s consider the agent\u2019s percepts. If there is currently a stench, one\nmightsuppose thataproposition Stench shouldbeaddedtotheknowledgebase. Thisisnot\nquiteright,however: iftherewasnostenchattheprevioustimestep,then\u00acStench wouldal-\nreadybeasserted,andthenewassertionwouldsimplyresult inacontradiction. Theproblem\nissolvedwhenwerealizethataperceptassertssomething onlyaboutthecurrenttime. Thus,\nifthetimestep(assupplied to MAKE-PERCEPT-SENTENCE inFigure7.1)is4,thenweadd 266 Chapter 7. LogicalAgents\nStench4 totheknowledge base, ratherthan Stench\u2014neatlyavoiding anycontradiction with\n\u00acStench3. Thesamegoesforthebreeze, bump,glitter, andscream percepts.\nTheideaofassociating propositions withtimesteps extends toanyaspect oftheworld\nthatchangesovertime. Forexample,theinitialknowledgebaseincludesL0 \u2014theagentisin\n1,1\nsquare[1,1]attime0\u2014aswellasFacingEast0,HaveArrow0,andWumpusAlive0. Weuse\ntheword fluent(from theLatin fluens, flowing) toreferanaspect of theworld thatchanges.\nFLUENT\n\u201cFluent\u201disasynonymfor\u201cstatevariable,\u201dinthesensedescribedinthediscussionoffactored\nrepresentations in Section 2.4.7 on page 57. Symbols associated with permanent aspects of\nATEMPORAL theworlddonotneedatimesuperscript andaresometimescalled atemporalvariables.\nVARIABLE\nWe can connect stench and breeze percepts directly to the properties of the squares\nwhere they areexperienced through the location fluentas follows.10 Forany timestep t and\nanysquare [x,y],weassert\nLt \u21d2 (Breezet \u21d4 B )\nx,y x,y\nLt \u21d2 (Stencht \u21d4 S ).\nx,y x,y\nNow, of course, we need axioms that allow the agent to keep track of fluents such as Lt .\nx,y\nThese fluents change as the result of actions taken by the agent, so, in the terminology of\nChapter 3, we need to write down the transition model of the wumpus world as a set of\nlogicalsentences.\nFirst, we need proposition symbols for the occurrences of actions. As with percepts,\nthesesymbolsareindexedbytime;thus,Forward0meansthattheagentexecutestheForward\naction at time 0. Byconvention, the percept fora given time step happens first, followed by\ntheactionforthattimestep,followedbyatransition tothe nexttimestep.\nTo describe how the world changes, we can try writing effect axioms that specify the\nEFFECTAXIOM\noutcomeofanactionatthenexttimestep. Forexample,iftheagentisatlocation[1,1]facing\neast at time0 and goes Forward, the result isthat the agent is in square [2,1] and no longer\nisin[1,1]:\nL0 \u2227FacingEast0\u2227Forward0 \u21d2 (L1 \u2227\u00acL1 ). (7.1)\n1,1 2,1 1,1\nWe would need one such sentence for each possible time step, for each of the 16 squares,\nandeachofthefourorientations. Wewouldalsoneedsimilarsentencesfortheotheractions:\nGrab,Shoot,Climb,TurnLeft,andTurnRight.\nLet us suppose that the agent does decide to move Forward at time 0 and asserts this\nfact into its knowledge base. Given the effect axiom in Equation (7.1), combined with the\ninitial assertions about the state at time 0, the agent can now deduce that it is in [2,1]. That\nis, ASK(KB,L1 )=true. Sofar, so good. Unfortunately, the news elsewhere isless good:\n2,1\nif we ASK(KB,HaveArrow1), the answer is false, that is, the agent cannot prove it still\nhas the arrow; norcan it prove it doesn\u2019t have it! Theinformation has been lost because the\neffectaxiom fails tostatewhatremains unchanged astheresultofanaction. Theneed todo\nthis gives rise to the frame problem.11 One possible solution to the frame problem would\nFRAMEPROBLEM\n10 Section7.4.3convenientlyglossedoverthisrequirement.\n11 Thename\u201cframeproblem\u201dcomesfrom\u201cframeofreference\u201dinphysics\u2014theassumedstationarybackground\nwithrespecttowhichmotionismeasured. Italsohasananalogytotheframesofamovie, inwhichnormally\nmostofthebackgroundstaysconstantwhilechangesoccurintheforeground. Section7.7. AgentsBasedonPropositional Logic 267\nbe to add frame axioms explicitly asserting all the propositions that remain the same. For\nFRAMEAXIOM\nexample,foreachtimetwewouldhave\nForwardt \u21d2 (HaveArrowt \u21d4 HaveArrowt+1)\nForwardt \u21d2 (WumpusAlivet \u21d4 WumpusAlivet+1)\n\u00b7\u00b7\u00b7\nwhere we explicitly mention every proposition that stays unchanged from time t to time\nt + 1 under the action Forward. Although the agent now knows that it still has the arrow\naftermovingforwardandthatthewumpushasn\u2019t diedorcomebacktolife,theproliferation\nof frame axioms seems remarkably inefficient. In a world with m different actions and n\nfluents, the set of frame axioms will be of size O(mn). This specific manifestation of the\nREPRESENTATIONAL frame problem is sometimes called the representational frame problem. Historically, the\nFRAMEPROBLEM\nproblemwasasignificantoneforAIresearchers; weexploreitfurtherinthenotesattheend\nofthechapter.\nTherepresentational frameproblemissignificantbecausetherealworldhasverymany\nfluents, to put it mildly. Fortunately for us humans, each action typically changes no more\nthan some small number k of those fluents\u2014the world exhibits locality. Solving the repre-\nLOCALITY\nsentational frameproblem requires definingthetransition modelwithasetofaxiomsofsize\nINFERENTIALFRAME O(mk) rather than size O(mn). There is also an inferential frame problem: the problem\nPROBLEM\nofprojecting forwardtheresultsofa tstepplanofactionintimeO(kt)ratherthan O(nt).\nThe solution to the problem involves changing one\u2019s focus from writing axioms about\nactions towriting axioms about fluents. Thus, foreach fluent F, wewillhavean axiom that\ndefinesthetruthvalueofFt+1intermsoffluents(includingF itself)attimetandtheactions\nthatmayhaveoccurredattimet. Now,thetruthvalueofFt+1 canbesetinoneoftwoways:\neithertheactionattimetcausesF tobetrueatt+1,orF wasalreadytrueattimetandthe\nactionattimetdoesnotcauseittobefalse. Anaxiomofthisformiscalledasuccessor-state\nSUCCESSOR-STATE axiomandhasthisschema:\nAXIOM\nFt+1 \u21d4 ActionCausesFt\u2228(Ft\u2227\u00acActionCausesNotFt).\nOne of the simplest successor-state axioms is the one for HaveArrow. Because there is no\nactionforreloading, the ActionCausesFt partgoesawayandweareleftwith\nHaveArrowt+1 \u21d4 (HaveArrowt\u2227\u00acShoott). (7.2)\nFor the agent\u2019s location, the successor-state axioms are more elaborate. For example, Lt+1\n1,1\nis true if either (a) the agent moved Forward from [1,2] when facing south, or from [2,1]\nwhenfacingwest;or(b)Lt wasalreadytrueandtheactiondidnotcausemovement(either\n1,1\nbecause the action wasnot Forward orbecause theaction bumped into awall). Written out\ninpropositional logic,thisbecomes\nLt+1 \u21d4 (Lt \u2227(\u00acForwardt\u2228Bumpt+1))\n1,1 1,1\n\u2228 (Lt \u2227(Southt\u2227Forwardt)) (7.3)\n1,2\n\u2228 (Lt \u2227(Westt\u2227Forwardt)).\n2,1\nExercise7.26asksyoutowriteoutaxiomsfortheremaining wumpusworldfluents. 268 Chapter 7. LogicalAgents\nGivenacompletesetofsuccessor-state axiomsandtheotheraxiomslistedatthebegin-\nningofthissection, theagentwillbeabletoASK andansweranyanswerablequestionabout\nthecurrentstateoftheworld. Forexample,inSection7.2theinitialsequenceofperceptsand\nactionsis\n\u00acStench0\u2227\u00acBreeze0\u2227\u00acGlitter0\u2227\u00acBump0\u2227\u00acScream0 ; Forward0\n\u00acStench1\u2227Breeze1\u2227\u00acGlitter1\u2227\u00acBump1\u2227\u00acScream1 ; TurnRight1\n\u00acStench2\u2227Breeze2\u2227\u00acGlitter2\u2227\u00acBump2\u2227\u00acScream2 ; TurnRight2\n\u00acStench3\u2227Breeze3\u2227\u00acGlitter3\u2227\u00acBump3\u2227\u00acScream3 ; Forward3\n\u00acStench4\u2227\u00acBreeze4\u2227\u00acGlitter4\u2227\u00acBump4\u2227\u00acScream4 ; TurnRight4\n\u00acStench5\u2227\u00acBreeze5\u2227\u00acGlitter5\u2227\u00acBump5\u2227\u00acScream5 ; Forward5\nStench6\u2227\u00acBreeze6\u2227\u00acGlitter6\u2227\u00acBump6\u2227\u00acScream6\nAt this point, we have ASK(KB,L6 )=true, so the agent knows where it is. Moreover,\n1,2\nASK(KB,W 1,3)=true andASK(KB,P 3,1)=true,sotheagenthasfoundthewumpusand\noneofthepits. ThemostimportantquestionfortheagentiswhetherasquareisOKtomove\ninto, that is, the square contains no pit nor live wumpus. It\u2019s convenient to add axioms for\nthis,havingtheform\nOKt \u21d4 \u00acP \u2227\u00ac(W \u2227WumpusAlivet).\nx,y x,y x,y\nFinally, ASK(KB,OK6 )=true, so the square [2,2] is OK to move into. In fact, given a\n2,2\nsoundandcompleteinferencealgorithmsuchasDPLL,theagentcanansweranyanswerable\nquestionaboutwhichsquaresareOK\u2014andcandosoinjustafewmilliseconds forsmall-to-\nmediumwumpusworlds.\nSolving the representational and inferential frame problems is a big step forward, but\napernicious problem remains: weneed toconfirm that allthenecessary preconditions ofan\nactionholdforittohaveitsintendedeffect. WesaidthattheForward actionmovestheagent\naheadunlessthereisawallintheway,buttherearemanyotherunusualexceptionsthatcould\ncausetheactiontofail: theagentmighttripandfall,bestrickenwithaheartattack,becarried\nQUALIFICATION awaybygiant bats, etc. Specifying allthese exceptions iscalled thequalification problem.\nPROBLEM\nThere is no complete solution within logic; system designers have to use good judgment in\ndeciding how detailed they want to be in specifying their model, and what details they want\ntoleaveout. WewillseeinChapter13thatprobability theoryallowsustosummarizeallthe\nexceptions withoutexplicitly namingthem.\n7.7.2 A hybridagent\nTheabilitytodeducevariousaspectsofthestateoftheworldcanbecombinedfairlystraight-\nforwardly withcondition\u2013action rulesandwithproblem-solving algorithms fromChapters3\nand4toproduceahybridagentforthewumpusworld. Figure7.20showsonepossibleway\nHYBRIDAGENT\nto do this. The agent program maintains and updates a knowledge base as well as a current\nplan. The initial knowledge base contains the atemporal axioms\u2014those that don\u2019t depend\non t, such as the axiom relating the breeziness of squares to the presence of pits. At each\ntimestep,thenewperceptsentenceisaddedalongwithalltheaxiomsthatdependont,such Section7.7. AgentsBasedonPropositional Logic 269\nasthesuccessor-state axioms. (Thenextsectionexplainswhytheagentdoesn\u2019t needaxioms\nfor future time steps.) Then, the agent uses logical inference, by ASKing questions of the\nknowledgebase,toworkoutwhichsquares aresafeandwhichhaveyettobevisited.\nThemainbodyoftheagentprogramconstructsaplanbasedonadecreasingpriorityof\ngoals. First,ifthereisaglitter, theprogram constructs a plantograbthegold,followaroute\nback tothe initial location, and climb out of the cave. Otherwise, ifthere isno current plan,\nthe program plans a route to the closest safe square that it has not visited yet, making sure\n\u2217\nthe route goes through only safe squares. Route planning is done with A search, not with\nASK. Iftherearenosafesquarestoexplore,thenextstep\u2014ifthe agentstillhasanarrow\u2014is\ntotry to makea safe square byshooting at one ofthe possible wumpus locations. These are\ndetermined by asking where ASK(KB,\u00acW x,y) is false\u2014that is, where it is not known that\nthere is not awumpus. The function PLAN-SHOT (not shown) uses PLAN-ROUTE to plan a\nsequence ofactions thatwillline upthis shot. Ifthis fails, the program looks forasquare to\nexplore that isnot provably unsafe\u2014that is, asquare forwhich ASK(KB,\u00acOKt x,y)returns\nfalse. Ifthereisnosuchsquare, thenthemissionisimpossibleandtheagentretreatsto[1,1]\nandclimbsoutofthecave.\n7.7.3 Logicalstateestimation\nThe agent program in Figure 7.20 works quite well, but it has one major weakness: as time\ngoesby,thecomputationalexpenseinvolvedinthecallstoASKgoesupandup. Thishappens\nmainlybecausetherequiredinferenceshavetogobackfurtherandfurtherintimeandinvolve\nmore and more proposition symbols. Obviously, this is unsustainable\u2014we cannot have an\nagent whose timetoprocess each percept grows inproportion to the length ofits life! What\nwereallyneedisaconstant updatetime\u2014thatis,independent oft. Theobviousansweristo\nsave,orcache,theresultsofinference, sothattheinferenceprocessatthenexttimestepcan\nCACHING\nbuildontheresultsofearlierstepsinstead ofhavingtostartagainfromscratch.\nAs we saw in Section 4.4, the past history of percepts and all their ramifications can\nbereplaced bythe beliefstate\u2014that is,somerepresentation ofthesetofallpossible current\nstatesoftheworld.12 Theprocessofupdating thebeliefstateasnewpercepts arriveiscalled\nstate estimation. Whereas in Section 4.4 the belief state was an explicit list of states, here\nwe can use a logical sentence involving the proposition symbols associated with the current\ntimestep,aswellastheatemporal symbols. Forexample,the logicalsentence\nWumpusAlive1\u2227L1 \u2227B \u2227(P \u2228P ) (7.4)\n2,1 2,1 3,1 2,2\nrepresents the set of all states at time 1 in which the wumpus is alive, the agent is at [2,1],\nthatsquareisbreezy, andthereisapitin [3,1]or[2,2]orboth.\nMaintaining an exact belief state as a logical formula turns out not to be easy. If there\narenfluentsymbolsfortimet,thenthereare2npossiblestates\u2014thatis,assignmentsoftruth\nvaluestothosesymbols. Now,thesetofbeliefstatesisthepowerset(setofallsubsets)ofthe\nset of physical states. There are 2n physical states, hence 22n belief states. Even if weused\nthe most compact possible encoding of logical formulas, with each belief state represented\n12 Wecanthinkofthepercepthistoryitselfasarepresentationofthebeliefstate,butonethatmakesinference\nincreasinglyexpensiveasthehistorygetslonger. 270 Chapter 7. LogicalAgents\nfunctionHYBRID-WUMPUS-AGENT(percept)returnsanaction\ninputs:percept,alist,[stench,breeze,glitter,bump,scream]\npersistent: KB,aknowledgebase,initiallytheatemporal\u201cwumpusphysics\u201d\nt,acounter,initially0,indicatingtime\nplan,anactionsequence,initiallyempty\nTELL(KB,MAKE-PERCEPT-SENTENCE(percept,t))\nTELLtheKB thetemporal\u201cphysics\u201dsentencesfortimet\nsafe\u2190{[x,y] : ASK(KB,OKt x,y) = true}\nt\nifASK(KB,Glitter ) = true then\nplan\u2190[Grab]+PLAN-ROUTE(current,{[1,1]},safe)+[Climb]\nifplan isemptythen\nunvisited\u2190{[x,y] : ASK(KB,Lt x(cid:3) ,y) = false forall t(cid:5) \u2264 t}\nplan\u2190PLAN-ROUTE(current,unvisited\u2229safe,safe)\nt\nifplan isemptyandASK(KB,HaveArrow ) = true then\npossible wumpus\u2190{[x,y] : ASK(KB,\u00acWx,y) = false}\nplan\u2190PLAN-SHOT(current,possible wumpus,safe)\nifplan isemptythen \/\/nochoicebuttotakearisk\nnot unsafe\u2190{[x,y] : ASK(KB,\u00acOKt x,y) = false}\nplan\u2190PLAN-ROUTE(current,unvisited\u2229not unsafe,safe)\nifplan isemptythen\nplan\u2190PLAN-ROUTE(current,{[1,1]},safe)+[Climb]\naction\u2190POP(plan)\nTELL(KB,MAKE-ACTION-SENTENCE(action,t))\nt\u2190t +1\nreturnaction\nfunctionPLAN-ROUTE(current,goals,allowed)returnsanactionsequence\ninputs:current,theagent\u2019scurrentposition\ngoals,asetofsquares;trytoplanaroutetooneofthem\nallowed,asetofsquaresthatcanformpartoftheroute\nproblem\u2190ROUTE-PROBLEM(current,goals,allowed)\nreturnA*-GRAPH-SEARCH(problem)\nFigure7.20 Ahybridagentprogramforthewumpusworld.Itusesapropositionalknowl-\nedgebase to inferthestate ofthe world, anda combinationof problem-solvingsearch and\ndomain-specificcodetodecidewhatactionstotake.\nby a unique binary number, we would need numbers with log (22n )=2n bits to label the\n2\ncurrentbeliefstate. Thatis,exactstateestimationmayrequirelogicalformulaswhosesizeis\nexponential inthenumberofsymbols.\nOnevery common and natural scheme for approximate state estimation is to represent\nbeliefstatesasconjunctionsofliterals,thatis,1-CNFformulas. Todothis,theagentprogram\nsimply tries to prove Xt and \u00acXt for each symbol Xt (as well as each atemporal symbol\nwhose truth value is not yet known), given the belief state at t \u2212 1. The conjunction of Section7.7. AgentsBasedonPropositional Logic 271\nFigure 7.21 Depiction of a 1-CNF belief state (bold outline) as a simply representable,\nconservative approximation to the exact (wiggly) belief state (shaded region with dashed\noutline).Eachpossibleworldisshownasacircle;theshadedonesareconsistentwithallthe\npercepts.\nprovableliteralsbecomesthenewbeliefstate,andthepreviousbeliefstateisdiscarded.\nItis important to understand that this scheme maylose some information as timegoes\nalong. Forexample, if the sentence in Equation (7.4) were the true belief state, then neither\nP nor P would be provable individually and neither would appear in the 1-CNF belief\n3,1 2,2\nstate. (Exercise 7.27 explores one possible solution to this problem.) On the other hand,\nbecause every literal in the 1-CNF belief state is proved from the previous belief state, and\nthe initial belief state is a true assertion, we know that entire 1-CNF belief state must be\ntrue. Thus, thesetofpossible statesrepresented bythe1-CNFbeliefstateincludes allstates\nthat are in fact possible given the full percept history. As illustrated in Figure 7.21, the 1-\nCONSERVATIVE CNFbeliefstateactsasasimpleouterenvelope,orconservativeapproximation,aroundthe\nAPPROXIMATION\nexact belief state. We see this idea of conservative approximations to complicated sets as a\nrecurring themeinmanyareasofAI.\n7.7.4 Making plans by propositional inference\nTheagentinFigure7.20useslogicalinference todetermine whichsquaresaresafe,butuses\n\u2217\nA search to make plans. In this section, we show how to make plans by logical inference.\nThebasicideaisverysimple:\n1. Constructasentence thatincludes\n(a) Init0,acollection ofassertions abouttheinitialstate;\n(b) Transition1,...,Transitiont, the successor-state axioms for all possible actions\nateachtimeuptosomemaximumtimet;\n(c) theassertion thatthegoalisachieved attimet: HaveGoldt\u2227ClimbedOutt. 272 Chapter 7. LogicalAgents\n2. Presentthewholesentence toaSATsolver. Ifthesolverfindsasatisfying model, then\nthe goal is achievable; if the sentence is unsatisfiable, then the planning problem is\nimpossible.\n3. Assuming a model is found, extract from the model those variables that represent ac-\ntionsandareassigned true. Togethertheyrepresent aplantoachievethegoals.\nA propositional planning procedure, SATPLAN, is shown in Figure 7.22. It implements the\nbasic idea just given, with one twist. Because the agent does not know how many steps it\nwill take to reach the goal, the algorithm tries each possible number of steps t, up to some\nmaximumconceivableplanlengthT . Inthisway,itisguaranteedtofindtheshortestplan\nmax\nif one exists. Because of the way SATPLAN searches for a solution, this approach cannot\nbe used in a partially observable environment; SATPLAN would just set the unobservable\nvariables tothevaluesitneedstocreateasolution.\nfunctionSATPLAN(init, transition, goal,T max)returnssolutionorfailure\ninputs:init, transition, goal,constituteadescriptionoftheproblem\nT ,anupperlimitforplanlength\nmax\nfort =0toT do\nmax\ncnf \u2190TRANSLATE-TO-SAT(init, transition, goal,t)\nmodel\u2190SAT-SOLVER(cnf)\nifmodel isnotnullthen\nreturnEXTRACT-SOLUTION(model)\nreturnfailure\nFigure 7.22 The SATPLAN algorithm. The planning problem is translated into a CNF\nsentenceinwhichthegoalisassertedtoholdatafixedtimesteptandaxiomsareincluded\nforeachtimestepuptot. Ifthesatisfiabilityalgorithmfindsamodel,thenaplanisextracted\nby looking at those proposition symbols that refer to actions and are assigned true in the\nmodel.Ifnomodelexists,thentheprocessisrepeatedwiththegoalmovedonesteplater.\nThe key step in using SATPLAN is the construction of the knowledge base. It might\nseem, on casual inspection, that the wumpus world axioms in Section 7.7.1 suffice forsteps\n1(a)and1(b)above. Thereis,however,asignificantdifference betweentherequirements for\nentailment (as tested by ASK)and those forsatisfiability. Consider, forexample, theagent\u2019s\nlocation, initially [1,1], and suppose the agent\u2019s unambitious goal isto be in[2,1] attime 1.\nTheinitialknowledgebasecontainsL0 andthegoalisL1 . UsingASK,wecanproveL1\n1,1 2,1 2,1\nif Forward0 is asserted, and, reassuringly, we cannot prove L1 if, say, Shoot0 is asserted\n2,1\ninstead. Now, SATPLAN will find the plan [Forward0]; so far, so good. Unfortunately,\nSATPLANalsofindstheplan[Shoot0]. Howcouldthisbe? Tofindout,weinspectthemodel\nthat SATPLAN constructs: it includes the assignment L0 , that is, the agent can be in [2,1]\n2,1\nattime1bybeingthereattime0andshooting. Onemightask,\u201cDidn\u2019twesaytheagentisin\n[1,1]attime0?\u201d Yes,wedid,butwedidn\u2019ttelltheagentthatitcan\u2019tbeintwoplacesatonce!\nFor entailment, L0 is unknown and cannot, therefore, be used in a proof; for satisfiability,\n2,1 Section7.7. AgentsBasedonPropositional Logic 273\non the other hand, L0 is unknown and can, therefore, be set to whatever value helps to\n2,1\nmakethegoaltrue. Forthisreason, SATPLAN isagooddebuggingtoolforknowledgebases\nbecause it reveals places where knowledge is missing. Inthis particular case, wecan fixthe\nknowledgebasebyassertingthat,ateachtimestep,theagentisinexactlyonelocation,using\nacollection ofsentences similartothoseusedtoasserttheexistence ofexactlyonewumpus.\nAlternatively,wecanassert\u00acL0 foralllocationsotherthan[1,1];thesuccessor-stateaxiom\nx,y\nfor location takes care of subsequent time steps. The same fixes also work to make sure the\nagenthasonlyoneorientation.\nSATPLAN has more surprises in store, however. The first is that it finds models with\nimpossibleactions,suchasshootingwithnoarrow. Tounderstandwhy,weneedtolookmore\ncarefullyatwhatthesuccessor-state axioms(suchasEquation(7.3))sayaboutactionswhose\npreconditionsarenotsatisfied. Theaxiomsdopredictcorrectlythatnothingwillhappenwhen\nsuchanaction isexecuted (seeExercise10.14), buttheydonotsaythattheactioncannot be\nPRECONDITION executed! Toavoid generating plans withillegal actions, wemustadd precondition axioms\nAXIOMS\nstatingthatanactionoccurrencerequiresthepreconditionstobesatisfied.13 Forexample,we\nneedtosay,foreachtimet,that\nShoott \u21d2 HaveArrowt .\nThis ensures that if a plan selects the Shoot action at any time, it must be the case that the\nagenthasanarrowatthattime.\nSATPLAN\u2019ssecondsurpriseisthecreationofplanswithmultiplesimultaneousactions.\nFor example, it may come up with a model in which both Forward0 and Shoot0 are true,\nACTIONEXCLUSION whichisnotallowed. Toeliminate thisproblem, weintroduce action exclusion axioms: for\nAXIOM\neverypairofactions At andAt weaddtheaxiom\ni j\n\u00acAt\u2228\u00acAt .\ni j\nItmight bepointed out that walking forward and shooting at the same timeis not so hard to\ndo, whereas, say, shooting and grabbing atthesametimeis ratherimpractical. Byimposing\naction exclusion axioms only on pairs of actions that really do interfere with each other, we\ncanallowforplansthatincludemultiplesimultaneousactions\u2014andbecauseSATPLANfinds\ntheshortestlegalplan,wecanbesurethatitwilltakeadvantage ofthiscapability.\nTo summarize, SATPLAN finds models for a sentence containing the initial state, the\ngoal, the successor-state axioms, the precondition axioms, and the action exclusion axioms.\nIt can be shown that this collection of axioms is sufficient, in the sense that there are no\nlonger any spurious \u201csolutions.\u201d Any model satisfying the propositional sentence will be a\nvalid plan for the original problem. Modern SAT-solving technology makes the approach\nquite practical. Forexample, aDPLL-stylesolverhasno difficulty in generating the11-step\nsolution forthewumpusworldinstance showninFigure7.2.\nThissectionhasdescribedadeclarativeapproachtoagentconstruction: theagentworks\nbyacombination ofasserting sentences intheknowledge baseandperforming logical infer-\nence. This approach has some weaknesses hidden in phrases such as \u201cfor each time t\u201d and\n13 Noticethattheadditionofpreconditionaxiomsmeansthatweneednotincludepreconditionsforactionsin\nthesuccessor-stateaxioms. 274 Chapter 7. LogicalAgents\n\u201cfor each square [x,y].\u201d For any practical agent, these phrases have to be implemented by\ncodethatgenerates instances ofthegeneral sentence schemaautomatically forinsertion into\ntheknowledge base. Forawumpusworldofreasonable size\u2014one comparable toasmallish\ncomputer game\u2014we might need a 100\u00d7100 board and 1000 time steps, leading to knowl-\nedge bases withtens orhundreds ofmillions ofsentences. Notonly does thisbecome rather\nimpractical, but it also illustrates a deeper problem: we know something about the wum-\npus world\u2014namely, that the \u201cphysics\u201d works the same way across all squares and all time\nsteps\u2014that we cannot express directly in the language of propositional logic. To solve this\nproblem, we need a more expressive language, one in which phrases like \u201cfor each time t\u201d\nand \u201cfor each square [x,y]\u201d can be written in a natural way. First-order logic, described in\nChapter 8, is such a language; in first-order logic a wumpus world of any size and duration\ncanbedescribed inabouttensentences ratherthantenmillionortentrillion.\n7.8 SUMMARY\nWe have introduced knowledge-based agents and have shown how to define a logic with\nwhichsuchagentscanreasonabouttheworld. Themainpoints areasfollows:\n\u2022 Intelligent agentsneedknowledge abouttheworldinordertoreachgooddecisions.\n\u2022 Knowledge is contained in agents in the form of sentences in a knowledge represen-\ntationlanguagethatarestoredinaknowledgebase.\n\u2022 A knowledge-based agent is composed of a knowledge base and an inference mecha-\nnism. Itoperates bystoring sentences abouttheworldinits knowledge base, using the\ninference mechanism toinfernew sentences, and using these sentences todecide what\nactiontotake.\n\u2022 A representation language is defined by its syntax, which specifies the structure of\nsentences,anditssemantics,whichdefinesthetruthofeachsentenceineachpossible\nworldormodel.\n\u2022 The relationship of entailment between sentences is crucial to our understanding of\nreasoning. A sentence \u03b1 entails another sentence \u03b2 if \u03b2 is true in all worlds where\n\u03b1 is true. Equivalent definitions include the validity of the sentence \u03b1 \u21d2 \u03b2 and the\nunsatisfiabilityofthesentence \u03b1\u2227\u00ac\u03b2.\n\u2022 Inferenceistheprocessofderivingnewsentencesfromoldones. Soundinferencealgo-\nrithmsderiveonlysentencesthatareentailed; completealgorithmsderiveallsentences\nthatareentailed.\n\u2022 Propositionallogicisasimplelanguageconsistingofpropositionsymbolsandlogical\nconnectives. Itcanhandlepropositionsthatareknowntrue,knownfalse,orcompletely\nunknown.\n\u2022 The set of possible models, given a fixed propositional vocabulary, is finite, so en-\ntailment can be checked by enumerating models. Efficient model-checking inference\nalgorithms for propositional logic include backtracking and local search methods and\ncanoftensolvelargeproblemsquickly. Bibliographical andHistorical Notes 275\n\u2022 Inference rules are patterns of sound inference that can be used to find proofs. The\nresolution rule yields a complete inference algorithm for knowledge bases that are\nexpressed in conjunctive normal form. Forward chaining and backward chaining\nareverynaturalreasoning algorithmsforknowledgebasesinHornform.\n\u2022 Local search methods such as WALKSAT can be used to find solutions. Such algo-\nrithmsaresoundbutnotcomplete.\n\u2022 Logical state estimation involves maintaining alogical sentence that describes the set\nof possible states consistent with the observation history. Each update step requires\ninferenceusingthetransitionmodeloftheenvironment, whichisbuiltfromsuccessor-\nstateaxiomsthatspecifyhoweachfluentchanges.\n\u2022 Decisionswithinalogical agentcanbemadebySATsolving: findingpossible models\nspecifying future action sequences that reach the goal. This approach works only for\nfullyobservable orsensorless environments.\n\u2022 Propositional logic does not scale to environments of unbounded size because it lacks\ntheexpressive powertodeal concisely withtime, space, and universal patterns ofrela-\ntionships amongobjects.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nJohn McCarthy\u2019s paper \u201cPrograms with Common Sense\u201d (McCarthy, 1958, 1968) promul-\ngatedthenotionofagentsthatuselogicalreasoningtomediatebetweenperceptsandactions.\nItalsoraisedtheflagofdeclarativism,pointingoutthattellinganagentwhatitneedstoknow\nis an elegant way to build software. Allen Newell\u2019s (1982) article \u201cThe Knowledge Level\u201d\nmakesthecasethatrationalagentscanbedescribedandanalyzedatanabstractleveldefined\nbytheknowledgetheypossessratherthantheprogramstheyrun. Thedeclarativeandproce-\ndural approaches to AI are analyzed in depth by Boden (1977). The debate was revived by,\namongothers, Brooks(1991) andNilsson(1991), andcontinues tothisday(Shaparau etal.,\n2008). Meanwhile, thedeclarative approach has spread into otherareas ofcomputerscience\nsuchasnetworking (Looetal.,2006).\nLogicitself haditsorigins inancient Greek philosophy and mathematics. Variouslog-\nical principles\u2014principles connecting the syntactic structure of sentences with their truth\nand falsity, with their meaning, or with the validity of arguments in which they figure\u2014are\nscattered in the works of Plato. The first known systematic study of logic was carried out\nby Aristotle, whose work was assembled by his students after his death in 322 B.C. as a\ntreatise called the Organon. Aristotle\u2019s syllogisms were what we would now call inference\nSYLLOGISM\nrules. Although thesyllogisms included elements ofbothpropositional andfirst-orderlogic,\nthe system as a whole lacked the compositional properties required to handle sentences of\narbitrary complexity.\nThe closely related Megarian and Stoic schools (originating in the fifth century B.C.\nandcontinuingforseveralcenturiesthereafter)beganthe systematicstudyofthebasiclogical\nconnectives. The use of truth tables fordefining connectives is due to Philo of Megara. The 276 Chapter 7. LogicalAgents\nStoics took five basic inference rules as valid without proof, including the rule we now call\nModus Ponens. They derived a number of other rules from these five, using, among other\nprinciples, the deduction theorem (page 249) and were much clearer about the notion of\nproofthanwasAristotle. Agoodaccount ofthehistory ofMegarianandStoiclogic isgiven\nbyBensonMates(1953).\nTheidea of reducing logical inference toapurely mechanical process applied to afor-\nmallanguageisduetoWilhelmLeibniz(1646\u20131716), althoughhehadlimitedsuccessinim-\nplementing theideas. GeorgeBoole(1847)introduced thefirstcomprehensive andworkable\nsystem of formal logic in his book The Mathematical Analysis of Logic. Boole\u2019s logic was\nclosely modeled on the ordinary algebra of real numbers and used substitution of logically\nequivalent expressions as its primary inference method. Although Boole\u2019s system still fell\nshortoffullpropositional logic,itwascloseenoughthatothermathematicianscouldquickly\nfill in the gaps. Schro\u00a8der (1877) described conjunctive normal form, while Horn form was\nintroduced muchlaterbyAlfredHorn(1951). Thefirstcomprehensive exposition ofmodern\npropositional logic (and first-order logic) is found in Gottlob Frege\u2019s (1879) Begriffschrift\n(\u201cConceptWriting\u201dor\u201cConceptual Notation\u201d).\nThefirstmechanical devicetocarryoutlogicalinferences wasconstructed bythethird\nEarl of Stanhope (1753\u20131816). The Stanhope Demonstrator could handle syllogisms and\ncertain inferences in the theory of probability. William Stanley Jevons, one of those who\nimproved upon and extended Boole\u2019s work, constructed his \u201clogical piano\u201d in 1869 to per-\nform inferences in Boolean logic. An entertaining and instructive history of these and other\nearly mechanical devices for reasoning is given by Martin Gardner (1968). The first pub-\nlished computer program for logical inference was the Logic Theorist of Newell, Shaw,\nand Simon (1957). This program was intended to model human thought processes. Mar-\ntinDavis(1957) hadactually designed aprogram thatcameup withaproof in1954, but the\nLogicTheorist\u2019sresultswerepublished slightlyearlier.\nTruthtablesasamethodoftestingvalidityorunsatisfiabilityinpropositionallogicwere\nintroducedindependentlybyEmilPost(1921)andLudwigWittgenstein(1922). Inthe1930s,\na great deal of progress was made on inference methods for first-order logic. In particular,\nGo\u00a8del (1930) showed that a complete procedure for inference in first-order logic could be\nobtained via areduction topropositional logic, using Herbrand\u2019s theorem (Herbrand, 1930).\nWe take up this history again in Chapter 9; the important point here is that the development\nof efficient propositional algorithms in the 1960s was motivated largely by the interest of\nmathematicians inaneffectivetheorem proverforfirst-orderlogic. TheDavis\u2013Putnam algo-\nrithm(DavisandPutnam,1960)wasthefirsteffectivealgorithm forpropositional resolution\nbutwasinmostcasesmuch lessefficient thanthe DPLL backtracking algorithm introduced\ntwoyearslater(1962). Thefullresolution ruleandaproof ofitscompleteness appeared ina\nseminal paper by J. A. Robinson (1965), which also showed how to do first-order reasoning\nwithoutresorttopropositional techniques.\nStephen Cook (1971) showed that deciding satisfiability ofa sentence in propositional\nlogic (the SAT problem) is NP-complete. Since deciding entailment is equivalent to decid-\ningunsatisfiability, itisco-NP-complete. Manysubsets ofpropositional logic areknownfor\nwhich the satisfiability problem is polynomially solvable; Horn clauses are one such subset. Bibliographical andHistorical Notes 277\nThe linear-time forward-chaining algorithm for Horn clauses is due to Dowling and Gallier\n(1984), whodescribe theiralgorithm asadataflow process similartothepropagation of sig-\nnalsinacircuit.\nEarly theoretical investigations showed that DPLL has polynomial average-case com-\nplexity for certain natural distributions of problems. This potentially exciting fact became\nless exciting when Franco and Paull (1983) showed that the same problems could be solved\nin constant time simply by guessing random assignments. The random-generation method\ndescribedinthechapterproducesmuchharderproblems. Motivatedbytheempiricalsuccess\noflocalsearchontheseproblems,Koutsoupias andPapadimitriou (1992)showedthatasim-\nplehill-climbing algorithmcansolvealmostallsatisfiabilityprobleminstancesveryquickly,\nsuggesting that hard problems are rare. Moreover, Scho\u00a8ning (1999) exhibited a randomized\nhill-climbingalgorithm whoseworst-caseexpectedruntimeon3-SATproblems(thatis,sat-\nisfiability of 3-CNF sentences) is O(1.333n)\u2014still exponential, but substantially faster than\nprevious worst-case bounds. The current record is O(1.324n) (Iwama and Tamaki, 2004).\nAchlioptas et al. (2004) and Alekhnovich et al. (2005) exhibit families of 3-SAT instances\nforwhichallknownDPLL-likealgorithms requireexponential running time.\nOnthepracticalside,efficiencygainsinpropositionalsolvershavebeenmarked. Given\ntenminutesofcomputingtime,theoriginal DPLL algorithm in1962couldonlysolveprob-\nlems with no more than 10 or 15 variables. By 1995 the SATZ solver (Li and Anbulagan,\n1997) could handle 1,000 variables, thanks to optimized data structures for indexing vari-\nables. Twocrucial contributions were the watched literal indexing technique of Zhang and\nStickel (1996), which makes unit propagation very efficient, and the introduction of clause\n(i.e.,constraint)learningtechniquesfromtheCSPcommunitybyBayardoandSchrag(1997).\nUsing these ideas, andspurred by theprospect ofsolving industrial-scale circuit verification\nproblems, Moskewicz et al. (2001) developed the CHAFF solver, which could handle prob-\nlems with millions of variables. Beginning in 2002, SAT competitions have been held reg-\nularly; most of the winning entries have either been descendants of CHAFF orhave used the\nsamegeneral approach. RSAT (Pipatsrisawat andDarwiche, 2007), the2007winner, falls in\nthelattercategory. AlsonoteworthyisMINISAT (EenandSo\u00a8rensson,2003),anopen-source\nimplementation available at http:\/\/minisat.sethat is designed to be easily modified\nandimproved. Thecurrentlandscape ofsolversissurveyedbyGomesetal.(2008).\nLocal search algorithms for satisfiability were tried by various authors throughout the\n1980s; all ofthe algorithms werebased onthe idea ofminimizing the number ofunsatisfied\nclauses (Hansen and Jaumard, 1990). A particularly effective algorithm was developed by\nGu(1989) and independently bySelman etal. (1992), whocalled it GSAT and showed that\nit wascapable of solving a wide range of very hard problems very quickly. The WALKSAT\nalgorithm described inthechapterisduetoSelman etal.(1996).\nThe \u201cphase transition\u201d in satisfiability of random k-SAT problems was first observed\nby Simon and Dubois (1989) and has given rise to a great deal of theoretical and empirical\nresearch\u2014due, inpart,totheobviousconnection tophasetransitionphenomenainstatistical\nphysics. Cheeseman etal. (1991) observed phase transitions in several CSPsand conjecture\nthat all NP-hard problems have a phase transition. Crawford and Auton (1993) located the\n3-SAT transition at a clause\/variable ratio of around 4.26, noting that this coincides with a 278 Chapter 7. LogicalAgents\nsharppeakintheruntimeoftheirSATsolver. CookandMitchell(1997)provideanexcellent\nsummaryoftheearlyliterature ontheproblem.\nThe current state of theoretical understanding is summarized by Achlioptas (2009).\nSATISFIABILITY\nThe satisfiability threshold conjecture states that, for each k, there is a sharp satisfiability\nTHRESHOLD\nCONJECTURE threshold r ,suchthatasthenumberofvariables n \u2192 \u221e,instances belowthethreshold are\nk\nsatisfiable with probability 1, while those above the threshold are unsatisfiable with proba-\nbility1. Theconjecture wasnotquiteprovedbyFriedgut(1999): asharpthresholdexistsbut\nits location might depend on n even as n \u2192 \u221e. Despite significant progress in asymptotic\nanalysis of the threshold location for large k (Achlioptas and Peres, 2004; Achlioptas et al.,\n2007), all that can be proved for k=3 is that it lies in the range [3.52,4.51]. Current theory\nsuggests that apeak in the run time of aSATsolver is not necessarily related to the satisfia-\nbility threshold, but instead to a phase transition in the solution distribution and structure of\nSAT instances. Empirical results due to Coarfa et al. (2003) support this view. In fact, al-\nSURVEY gorithms suchas surveypropagation (ParisiandZecchina, 2002; Manevaetal.,2007)take\nPROPAGATION\nadvantage ofspecial properties ofrandom SATinstances nearthesatisfiability threshold and\ngreatlyoutperform generalSATsolversonsuchinstances.\nThebestsources forinformation onsatisfiability, boththeoretical andpractical, arethe\nHandbook of Satisfiability (Biere et al., 2009) and the regular International Conferences on\nTheoryandApplications ofSatisfiability Testing,knownasSAT.\nThe idea of building agents with propositional logic can be traced back to the seminal\npaper of McCulloch and Pitts (1943), which initiated the field of neural networks. Con-\ntrary topopular supposition, thepaperwasconcerned withthe implementation ofaBoolean\ncircuit-based agentdesign inthebrain. Circuit-based agents, whichperform computation by\npropagating signals in hardware circuits rather than running algorithms in general-purpose\ncomputers, have received little attention in AI, however. The most notable exception is the\nwork of Stan Rosenschein (Rosenschein, 1985; Kaelbling andRosenschein, 1990), who de-\nveloped ways to compile circuit-based agents from declarative descriptions of the task envi-\nronment. (Rosenschein\u2019s approach is described at some length in the second edition of this\nbook.) TheworkofRodBrooks(1986,1989)demonstratestheeffectivenessofcircuit-based\ndesigns for controlling robots\u2014a topic we take up in Chapter 25. Brooks (1991) argues\nthat circuit-based designs are all that is needed for AI\u2014that representation and reasoning\nare cumbersome, expensive, and unnecessary. In our view, neither approach is sufficient by\nitself. Williams et al. (2003) show how a hybrid agent design not too different from our\nwumpusagenthasbeenusedtocontrolNASAspacecraft, planningsequencesofactionsand\ndiagnosing andrecoveringfromfaults.\nThegeneral problem ofkeeping track of apartially observable environment wasintro-\nduced for state-based representations in Chapter 4. Its instantiation for propositional repre-\nsentations was studied by Amir and Russell (2003), who identified several classes of envi-\nronments that admit efficient state-estimation algorithms and showed that for several other\nTEMPORAL- classes theproblem isintractable. The temporal-projection problem, whichinvolves deter-\nPROJECTION\nmining what propositions hold true after an action sequence is executed, can be seen as a\nspecialcaseofstateestimationwithemptypercepts. Manyauthorshavestudiedthisproblem\nbecause of its importance in planning; some important hardness results were established by Exercises 279\nLiberatore (1997). The idea of representing a belief state with propositions can be traced to\nWittgenstein (1922).\nLogical state estimation, of course, requires a logical representation of the effects of\nactions\u2014a key problem in AI since the late 1950s. The dominant proposal has been the sit-\nuation calculusformalism (McCarthy, 1963), whichiscouched within first-order logic. We\ndiscusssituationcalculus,andvariousextensionsandalternatives,inChapters10and12. The\napproach taken in this chapter\u2014using temporal indices on propositional variables\u2014is more\nrestrictivebuthasthebenefitofsimplicity. ThegeneralapproachembodiedintheSATPLAN\nalgorithm wasproposed by Kautzand Selman (1992). Latergenerations of SATPLAN were\nable to take advantage of the advances in SAT solvers, described earlier, and remain among\nthemosteffectivewaysofsolving difficultproblems(Kautz,2006).\nThe frame problem was first recognized by McCarthy and Hayes (1969). Many re-\nsearchers considered the problem unsolvable within first-order logic, and it spurred a great\ndeal of research into nonmonotonic logics. Philosophers from Dreyfus (1972) to Crockett\n(1994) have cited the frame problem as one symptom of the inevitable failure of the entire\nAI enterprise. The solution of the frame problem with successor-state axioms is due to Ray\nReiter (1991). Thielscher (1999) identifies the inferential frame problem as a separate idea\nand provides a solution. In retrospect, one can see that Rosenschein\u2019s (1985) agents were\nusing circuits that implemented successor-state axioms, but Rosenschein did not notice that\nthe frame problem was thereby largely solved. Foo (2001) explains why the discrete-event\ncontrol theory models typically used by engineers do not have to explicitly deal with the\nframe problem: because they are dealing with prediction and control, not with explanation\nandreasoning aboutcounterfactual situations.\nModernpropositionalsolvershavewideapplicabilityinindustrialapplications. Theap-\nplication ofpropositional inference inthe synthesis ofcomputer hardware isnow astandard\ntechnique having manylarge-scale deployments (Nowick etal.,1993). The SATMC satisfi-\nabilitycheckerwasusedtodetectapreviously unknownvulnerability inaWebbrowseruser\nsign-onprotocol (Armando etal.,2008).\nThewumpus world wasinvented byGregory Yob(1975). Ironically, Yob developed it\nbecause he was bored with games played on a rectangular grid: the topology of his original\nwumpus world was a dodecahedron, and we put it back in the boring old grid. Michael\nGenesereth wasthefirsttosuggest thatthewumpusworldbeusedasanagenttestbed.\nEXERCISES\n7.1 SupposetheagenthasprogressedtothepointshowninFigure 7.4(a),page239,having\nperceivednothingin[1,1],abreezein[2,1],andastenchin[1,2],andisnowconcernedwith\nthe contents of [1,3], [2,2], and [3,1]. Each of these can contain a pit, and at most one can\ncontainawumpus. FollowingtheexampleofFigure7.5,constructthesetofpossibleworlds.\n(You should find 32 of them.) Mark the worlds in which the KB is true and those in which 280 Chapter 7. LogicalAgents\neachofthefollowingsentences istrue:\n\u03b1 = \u201cThereisnopitin[2,2].\u201d\n2\n\u03b1 = \u201cThereisawumpusin[1,3].\u201d\n3\nHenceshowthatKB |= \u03b1 andKB |= \u03b1 .\n2 3\n7.2 (Adapted from Barwise and Etchemendy (1993).) Given the following, can you prove\nthattheunicornismythical? Howaboutmagical? Horned?\nIftheunicorn ismythical, thenitisimmortal, but ifitisnotmythical, thenitisa\nmortalmammal. Iftheunicorn iseitherimmortaloramammal, thenitishorned.\nTheunicornismagicalifitishorned.\n7.3 Consider the problem of deciding whether a propositional logic sentence is true in a\ngivenmodel.\na. Write a recursive algorithm PL-TRUE?(s,m) that returns true if and only if the sen-\ntence s is true in the model m (where m assigns a truth value for every symbol in s).\nThealgorithm shouldrunintimelinearinthesizeofthesentence. (Alternatively, usea\nversionofthisfunction fromtheonlinecoderepository.)\nb. Givethree examplesofsentences thatcanbedetermined tobetrueorfalseina partial\nmodelthatdoesnotspecifyatruthvalueforsomeofthesymbols.\nc. Showthatthetruthvalue(ifany)ofasentenceinapartialmodelcannotbedetermined\nefficientlyingeneral.\nd. Modify your PL-TRUE? algorithm so that it can sometimes judge truth from partial\nmodels,whileretainingitsrecursivestructure andlinear runtime. Givethreeexamples\nofsentences whosetruthinapartialmodelisnotdetected byyouralgorithm.\ne. Investigate whetherthemodifiedalgorithm makes TT-ENTAILS? moreefficient.\n7.4 Whichofthefollowingarecorrect?\na. False |= True.\nb. True |= False.\nc. (A\u2227B)|= (A \u21d4 B).\nd. A \u21d4 B |= A\u2228B.\ne. A \u21d4 B |= \u00acA\u2228B.\nf. (A\u2227B) \u21d2 C |= (A \u21d2 C)\u2228(B \u21d2 C).\ng. (C \u2228(\u00acA\u2227\u00acB))\u2261 ((A \u21d2 C)\u2227(B \u21d2 C)).\nh. (A\u2228B)\u2227(\u00acC \u2228\u00acD\u2228E) |= (A\u2228B).\ni. (A\u2228B)\u2227(\u00acC \u2228\u00acD\u2228E) |= (A\u2228B)\u2227(\u00acD\u2228E).\nj. (A\u2228B)\u2227\u00ac(A \u21d2 B)issatisfiable.\nk. (A \u21d4 B)\u2227(\u00acA\u2228B)issatisfiable.\nl. (A \u21d4 B) \u21d4 C has the same number of models as (A \u21d4 B)forany fixed set of\nproposition symbolsthatincludes A,B,C. Exercises 281\n7.5 Proveeachofthefollowingassertions:\na. \u03b1isvalidifandonlyifTrue |= \u03b1.\nb. Forany\u03b1,False |= \u03b1.\nc. \u03b1|= \u03b2 ifandonlyifthesentence (\u03b1 \u21d2 \u03b2)isvalid.\nd. \u03b1\u2261 \u03b2 ifandonlyifthesentence (\u03b1 \u21d4 \u03b2)isvalid.\ne. \u03b1|= \u03b2 ifandonlyifthesentence (\u03b1\u2227\u00ac\u03b2)isunsatisfiable.\n7.6 Prove,orfindacounterexample to,eachofthefollowingassertions:\na. If\u03b1 |= \u03b3 or\u03b2 |= \u03b3 (orboth)then(\u03b1\u2227\u03b2) |= \u03b3\nb. If\u03b1 |= (\u03b2\u2227\u03b3)then\u03b1|= \u03b2 and\u03b1 |= \u03b3.\nc. If\u03b1 |= (\u03b2\u2228\u03b3)then\u03b1|= \u03b2 or\u03b1 |= \u03b3 (orboth).\n7.7 Consideravocabularywithonlyfourpropositions, A,B,C,andD. Howmanymodels\narethereforthefollowingsentences?\na. B\u2228C.\nb. \u00acA\u2228\u00acB\u2228\u00acC \u2228\u00acD.\nc. (A \u21d2 B)\u2227A\u2227\u00acB\u2227C \u2227D.\n7.8 Wehavedefinedfourbinarylogicalconnectives.\na. Arethereanyothersthatmightbeuseful?\nb. Howmanybinaryconnectives cantherebe?\nc. Whyaresomeofthemnotveryuseful?\n7.9 Usingamethodofyourchoice,verifyeachoftheequivalencesinFigure7.11(page249).\n7.10 Decidewhethereachofthefollowingsentencesisvalid,unsatisfiable, orneither. Ver-\nifyyourdecisions usingtruthtablesortheequivalence rulesofFigure7.11(page249).\na. Smoke \u21d2 Smoke\nb. Smoke \u21d2 Fire\nc. (Smoke \u21d2 Fire) \u21d2 (\u00acSmoke \u21d2 \u00acFire)\nd. Smoke \u2228Fire \u2228\u00acFire\ne. ((Smoke \u2227Heat) \u21d2 Fire) \u21d4 ((Smoke \u21d2 Fire)\u2228(Heat \u21d2 Fire))\nf. (Smoke \u21d2 Fire) \u21d2 ((Smoke \u2227Heat) \u21d2 Fire)\ng. Big \u2228Dumb \u2228(Big \u21d2 Dumb)\n7.11 Anypropositional logicsentenceislogically equivalent totheassertion thateachpos-\nsible world in which it would be false is not the case. From this observation, prove that any\nsentence canbewritteninCNF.\n7.12 Useresolutiontoprovethesentence\u00acA\u2227\u00acBfromtheclausesinExercise7.20.\n7.13 Thisexerciselooksintotherelationship betweenclausesandimplication sentences. 282 Chapter 7. LogicalAgents\na. Showthattheclause (\u00acP \u2228\u00b7\u00b7\u00b7\u2228\u00acP \u2228Q)islogically equivalent totheimplication\n1 m\nsentence(P \u2227\u00b7\u00b7\u00b7\u2227P ) \u21d2 Q.\n1 m\nb. Show that every clause (regardless of the number of positive literals) can bewritten in\nthe form (P \u2227\u00b7\u00b7\u00b7\u2227P ) \u21d2 (Q \u2228\u00b7\u00b7\u00b7\u2228Q ), where the Ps and Qs are proposition\n1 m 1 n\nsymbols. A knowledge base consisting of such sentences is in implicative normal\nIMPLICATIVE formorKowalskiform(Kowalski,1979).\nNORMALFORM\nc. Writedownthefullresolution ruleforsentences inimplicativenormalform.\n7.14 According to some political pundits, a person who is radical (R) is electable (E) if\nhe\/sheisconservative (C),butotherwiseisnotelectable.\na. Whichofthefollowingarecorrectrepresentations ofthis assertion?\n(i) (R\u2227E) \u21d0\u21d2 C\n(ii) R \u21d2 (E \u21d0\u21d2 C)\n(iii) R \u21d2 ((C \u21d2 E)\u2228\u00acE)\nb. Whichofthesentences in(a)canbeexpressedinHornform?\n7.15 Thisquestion considers representing satisfiability (SAT)problemsasCSPs.\na. Drawtheconstraint graphcorresponding totheSATproblem\n(\u00acX 1\u2228X 2)\u2227(\u00acX 2\u2228X 3)\u2227...\u2227(\u00acX n\u22121\u2228X n)\nfortheparticularcase n=5.\nb. Howmanysolutions arethereforthisgeneralSATproblem as afunction ofn?\nc. Supposeweapply BACKTRACKING-SEARCH (page215)tofindallsolutions toaSAT\nCSP of the type given in (a). (To find all solutions to a CSP, we simply modify the\nbasic algorithm so it continues searching after each solution is found.) Assume that\nvariables are ordered X ,...,X and false is ordered before true. How much time\n1 n\nwillthealgorithm taketoterminate? (Writean O(\u00b7)expression asafunction ofn.)\nd. We know that SAT problems in Horn form can be solved in linear time by forward\nchaining (unit propagation). We also know that every tree-structured binary CSP with\ndiscrete, finite domains can be solved in time linear in the number of variables (Sec-\ntion6.5). Arethesetwofactsconnected? Discuss.\n7.16 Explain why every nonempty propositional clause, by itself, is satisfiable. Prove rig-\norouslythateverysetoffive3-SATclausesissatisfiable, provided thateachclausementions\nexactly three distinct variables. Whatisthesmallest setofsuch clauses thatisunsatisfiable?\nConstructsuchaset.\n7.17 Apropositional 2-CNFexpressionisaconjunction ofclauses,eachcontaining exactly\n2literals, e.g.,\n(A\u2228B)\u2227(\u00acA\u2228C)\u2227(\u00acB\u2228D)\u2227(\u00acC \u2228G)\u2227(\u00acD\u2228G).\na. Proveusingresolution thattheabovesentenceentails G. Exercises 283\nb. Two clauses are semantically distinct if they are not logically equivalent. How many\nsemantically distinct2-CNFclausescanbeconstructed fromnproposition symbols?\nc. Usingyouranswerto(b),provethatpropositional resolution alwaysterminatesintime\npolynomialinngivena2-CNFsentencecontaining nomorethanndistinctsymbols.\nd. Explainwhyyourargument in(c)doesnotapplyto3-CNF.\n7.18 Considerthefollowingsentence:\n[(Food \u21d2 Party)\u2228(Drinks \u21d2 Party)] \u21d2 [(Food \u2227Drinks) \u21d2 Party].\na. Determine,usingenumeration,whetherthissentenceisvalid,satisfiable(butnotvalid),\norunsatisfiable.\nb. Convert the left-hand and right-hand sides of the main implication into CNF, showing\neachstep,andexplainhowtheresultsconfirmyouranswerto(a).\nc. Proveyouranswerto(a)usingresolution.\nDISJUNCTIVE 7.19 Asentenceisindisjunctivenormalform(DNF)ifitisthedisjunctionofconjunctions\nNORMALFORM\nofliterals. Forexample, thesentence (A\u2227B\u2227\u00acC)\u2228(\u00acA\u2227C)\u2228(B \u2227\u00acC)isinDNF.\na. Anypropositional logic sentence islogically equivalent totheassertion that somepos-\nsible world in which it would be true is in fact the case. From this observation, prove\nthatanysentence canbewritteninDNF.\nb. Construct an algorithm that converts any sentence in propositional logic into DNF.\n(Hint: The algorithm is similar to the algorithm for conversion to CNF given in Sec-\ntion7.5.2.)\nc. Construct asimplealgorithm thattakes asinput asentence inDNFandreturns asatis-\nfyingassignment ifoneexists,orreportsthatnosatisfying assignment exists.\nd. Applythealgorithms in(b)and(c)tothefollowingsetofsentences:\nA \u21d2 B\nB \u21d2 C\nC \u21d2 \u00acA.\ne. Since the algorithm in (b) is very similar to the algorithm for conversion to CNF, and\nsince the algorithm in (c) is much simpler than any algorithm for solving a set of sen-\ntencesinCNF,whyisthistechnique notusedinautomated reasoning?\n7.20 Convertthefollowingsetofsentences toclausalform.\nS1: A \u21d4 (B \u2228E).\nS2: E \u21d2 D.\nS3: C \u2227F \u21d2 \u00acB.\nS4: E \u21d2 B.\nS5: B \u21d2 F.\nS6: B \u21d2 C\nGiveatraceoftheexecutionofDPLLontheconjunction oftheseclauses. 284 Chapter 7. LogicalAgents\n7.21 Is arandomly generated 4-CNF sentence with n symbols and m clauses more orless\nlikely to be solvable than a randomly generated 3-CNF sentence with n symbols and m\nclauses? Explain.\n7.22 Minesweeper,thewell-knowncomputergame,iscloselyrelatedtothewumpusworld.\nA minesweeper world is a rectangular grid of N squares with M invisible mines scattered\namong them. Any square may be probed by the agent; instant death follows if a mine is\nprobed. Minesweeper indicates the presence of mines by revealing, in each probed square,\nthe number of mines that are directly or diagonally adjacent. The goal is to probe every\nunminedsquare.\na. LetX be true iffsquare [i,j] contains amine. Write down the assertion that exactly\ni,j\ntwo mines are adjacent to [1,1] as a sentence involving some logical combination of\nX propositions.\ni,j\nb. Generalize your assertion from (a) by explaining how to construct a CNF sentence\nassertingthat kofnneighbors contain mines.\nc. Explain precisely how an agent can use DPLL to prove that a given square does (or\ndoesnot)containamine,ignoring theglobal constraint thatthereareexactly M mines\ninall.\nd. Supposethattheglobalconstraint isconstructed fromyourmethodfrompart(b). How\ndoes the number of clauses depend on M and N? Suggest a way to modify DPLL so\nthattheglobalconstraint doesnotneedtoberepresented explicitly.\ne. Are any conclusions derived by the method in part (c) invalidated when the global\nconstraint istakenintoaccount?\nf. Give examples of configurations of probe values that induce long-range dependencies\nsuch that the contents of a given unprobed square would give information about the\ncontentsofafar-distant square. (Hint: consideranN\u00d71board.)\n7.23 How long does it take to prove KB |= \u03b1 using DPLL when \u03b1 is a literal already\ncontained inKB? Explain.\n7.24 Trace the behavior of DPLL on the knowledge base in Figure 7.16 when trying to\nproveQ,andcomparethisbehaviorwiththatoftheforward-chaining algorithm.\n7.25 Write a successor-state axiom for the Locked predicate, which applies to doors, as-\nsumingtheonlyactionsavailable areLock andUnlock.\n7.26 Section 7.7.1 provides some of the successor-state axioms required for the wumpus\nworld. Writedownaxiomsforallremainingfluentsymbols.\n7.27 Modify the HYBRID-WUMPUS-AGENT to use the 1-CNF logical state estimation\nmethod described on page 271. We noted on that page that such an agent will not be able\ntoacquire, maintain, andusemorecomplexbeliefs suchasthedisjunction P \u2228P . Sug-\n3,1 2,2\ngest a method for overcoming this problem by defining additional proposition symbols, and\ntryitoutinthewumpusworld. Doesitimprovetheperformance oftheagent? 8\nFIRST-ORDER LOGIC\nInwhichwenoticethattheworldisblessedwithmanyobjects,someofwhichare\nrelatedtootherobjects, andinwhichweendeavor toreasonaboutthem.\nInChapter7,weshowedhowaknowledge-based agentcouldrepresenttheworldinwhichit\noperates and deduce what actions totake. Weused propositional logic as ourrepresentation\nlanguage because it sufficed to illustrate the basic concepts of logic and knowledge-based\nagents. Unfortunately, propositional logic is too puny a language to represent knowledge\nof complex environments in a concise way. In this chapter, we examine first-order logic,1\nFIRST-ORDERLOGIC\nwhich is sufficiently expressive to represent a good deal of our commonsense knowledge.\nIt also either subsumes or forms the foundation of many other representation languages and\nhasbeenstudied intensively formanydecades. WebegininSection8.1withadiscussion of\nrepresentationlanguagesingeneral;Section8.2coversthesyntaxandsemanticsoffirst-order\nlogic;Sections8.3and8.4illustrate theuseoffirst-order logicforsimplerepresentations.\n8.1 REPRESENTATION REVISITED\nIn this section, we discuss the nature of representation languages. Ourdiscussion motivates\nthedevelopmentoffirst-orderlogic,amuchmoreexpressive languagethanthepropositional\nlogicintroduced inChapter7. Welookatpropositional logicandatotherkindsoflanguages\nto understand what works and what fails. Our discussion will be cursory, compressing cen-\nturiesofthought, trial,anderrorintoafewparagraphs.\nProgramming languages (such as C++ or Java or Lisp) are by far the largest class of\nformal languages in common use. Programs themselves represent, in a direct sense, only\ncomputational processes. Data structures within programs can represent facts; for example,\naprogram could usea 4\u00d74array torepresent the contents ofthe wumpusworld. Thus, the\nprogramminglanguagestatementWorld[2,2]\u2190Pit isafairlynaturalwaytoassertthatthere\nis apit in square [2,2]. (Such representations might be considered ad hoc; database systems\nwere developed precisely to provide a more general, domain-independent way to store and\n1 Alsocalledfirst-orderpredicatecalculus,sometimesabbreviatedasFOLorFOPC.\n285 286 Chapter 8. First-OrderLogic\nretrieve facts.) What programming languages lack is any general mechanism for deriving\nfactsfromotherfacts;eachupdatetoadatastructureisdonebyadomain-specificprocedure\nwhosedetails arederived bytheprogrammer fromhisorherownknowledge ofthedomain.\nThisproceduralapproachcanbecontrastedwiththedeclarativenatureofpropositionallogic,\ninwhichknowledgeandinferenceareseparate,andinferenceisentirelydomainindependent.\nA second drawback of data structures in programs (and of databases, for that matter)\nis the lack of any easy way to say, for example, \u201cThere is a pit in [2,2] or [3,1]\u201d or \u201cIf the\nwumpusisin[1,1]thenheisnotin[2,2].\u201d Programscanstoreasinglevalueforeachvariable,\nandsomesystemsallowthevaluetobe\u201cunknown,\u201dbuttheylacktheexpressivenessrequired\ntohandle partialinformation.\nPropositional logic is a declarative language because its semantics is based on a truth\nrelation between sentences and possible worlds. It also has sufficient expressive power to\ndeal withpartial information, using disjunction and negation. Propositional logic hasathird\nproperty that is desirable in representation languages, namely, compositionality. In a com-\nCOMPOSITIONALITY\npositional language, the meaning of asentence is a function of the meaning of its parts. For\nexample, the meaning of \u201cS \u2227 S \u201d is related to the meanings of \u201cS \u201d and \u201cS .\u201d It\n1,4 1,2 1,4 1,2\nwouldbeverystrange if\u201cS \u201dmeantthatthere isastench insquare [1,4]and\u201cS \u201dmeant\n1,4 1,2\nthatthereisastenchinsquare[1,2],but\u201cS \u2227S \u201dmeantthatFranceandPolanddrew1\u20131\n1,4 1,2\nin last week\u2019s ice hockey qualifying match. Clearly, noncompositionality makes life much\nmoredifficultforthereasoning system.\nAs we saw in Chapter 7, however, propositional logic lacks the expressive power to\nconcisely describe anenvironment withmanyobjects. Forexample, wewereforced towrite\naseparate ruleaboutbreezes andpitsforeachsquare, suchas\nB \u21d4 (P \u2228P ).\n1,1 1,2 2,1\nInEnglish,ontheotherhand,itseemseasyenoughtosay,onceandforall,\u201cSquaresadjacent\ntopitsarebreezy.\u201d ThesyntaxandsemanticsofEnglishsomehowmakeitpossibletodescribe\ntheenvironment concisely.\n8.1.1 The languageofthought\nNatural languages (such as English or Spanish) are very expressive indeed. We managed to\nwritealmostthiswholebookinnatural language, withonlyoccasional lapsesintootherlan-\nguages (including logic, mathematics, and the language of diagrams). There is a long tradi-\ntioninlinguisticsandthephilosophyoflanguagethatviewsnaturallanguageasadeclarative\nknowledge representation language. If we could uncover the rules for natural language, we\ncould use it in representation and reasoning systems and gain the benefit of the billions of\npagesthathavebeenwritteninnaturallanguage.\nThemodernviewofnaturallanguageisthatitservesaasamediumforcommunication\nratherthanpurerepresentation. Whenaspeakerpoints andsays, \u201cLook!\u201d thelistenercomes\nto know that, say, Superman has finally appeared over the rooftops. Yet we would not want\nto say that the sentence \u201cLook!\u201d represents that fact. Rather, the meaning of the sentence\ndepends both on the sentence itself and on the context in which the sentence was spoken.\nClearly, one could not store a sentence such as \u201cLook!\u201d in a knowledge base and expect to Section8.1. Representation Revisited 287\nrecover its meaning without also storing a representation of the context\u2014which raises the\nquestion of how the context itself can be represented. Natural languages also suffer from\nambiguity,aproblemforarepresentation language. AsPinker(1995)putsit: \u201cWhenpeople\nAMBIGUITY\nthinkaboutspring,surelytheyarenotconfusedastowhethertheyarethinkingaboutaseason\nor something that goes boing\u2014and if one word can correspond to two thoughts, thoughts\ncan\u2019tbewords.\u201d\nThe famous Sapir\u2013Whorf hypothesis claims that our understanding of the world is\nstrongly influenced bythelanguage wespeak. Whorf(1956)wrote\u201cWecutnature up,orga-\nnizeitintoconcepts, andascribe significances aswedo, largely because weareparties toan\nagreement to organize it this way\u2014an agreement that holds throughout our speech commu-\nnity and is codified in the patterns of our language.\u201d It is certainly true that different speech\ncommunities divide uptheworlddifferently. TheFrenchhavetwowords\u201cchaise\u201d and\u201cfau-\nteuil,\u201d for a concept that English speakers cover with one: \u201cchair.\u201d But English speakers\ncaneasilyrecognizethecategoryfauteuilandgiveitaname\u2014roughly\u201copen-armchair\u201d\u2014so\ndoes language really make a difference? Whorf relied mainly on intuition and speculation,\nbut in the intervening years we actually have real data from anthropological, psychological\nandneurological studies.\nForexample,canyourememberwhichofthefollowingtwophrasesformedtheopening\nofSection8.1?\n\u201cInthissection, wediscussthenatureofrepresentation languages ...\u201d\n\u201cThissectioncoversthetopicofknowledge representation languages ...\u201d\nWanner (1974) did a similar experiment and found that subjects made the right choice at\nchance level\u2014about 50% of the time\u2014but remembered the content of what they read with\nbetterthan90% accuracy. Thissuggests thatpeople process thewordstoform somekindof\nnonverbal representation.\nMore interesting is the case in which a concept is completely absent in a language.\nSpeakers of the Australian aboriginal language Guugu Yimithirr have no words for relative\ndirections, such as front, back, right, or left. Instead they use absolute directions, saying,\nfor example, the equivalent of \u201cI have a pain in my north arm.\u201d This difference in language\nmakes a difference in behavior: Guugu Yimithirr speakers are better at navigating in open\nterrain, whileEnglishspeakers arebetteratplacingtheforktotherightoftheplate.\nLanguage also seems to influence thought through seemingly arbitrary grammatical\nfeatures such as the gender of nouns. For example, \u201cbridge\u201d is masculine in Spanish and\nfeminine in German. Boroditsky (2003) asked subjects to choose English adjectives to de-\nscribe a photograph of a particular bridge. Spanish speakers chose big, dangerous, strong,\nandtowering,whereasGermanspeakerschosebeautiful,elegant,fragile,andslender. Words\ncan serve as anchor points that affect how weperceive the world. Loftus and Palmer(1974)\nshowed experimental subjects a movie of an auto accident. Subjects who were asked \u201cHow\nfast were the cars going when they contacted each other?\u201d reported an average of 32 mph,\nwhilesubjects whowereaskedthequestion withtheword\u201csmashed\u201d insteadof\u201ccontacted\u201d\nreported 41mphforthesamecarsinthesamemovie. 288 Chapter 8. First-OrderLogic\nInafirst-orderlogicreasoningsystemthatusesCNF,wecanseethatthelinguisticform\n\u201c\u00ac(A\u2228B)\u201d and \u201c\u00acA\u2227\u00acB\u201d are the same because we can look inside the system and see\nthat the two sentences are stored as the same canonical CNFform. Can wedo that with the\nhuman brain? Until recently the answer was \u201cno,\u201d but now it is \u201cmaybe.\u201d Mitchell et al.\n(2008) put subjects in an fMRI (functional magnetic resonance imaging) machine, showed\nthemwordssuchas\u201ccelery,\u201dandimagedtheirbrains. Theresearcherswerethenabletotrain\nacomputerprogramtopredict,fromabrainimage,whatwordthesubjecthadbeenpresented\nwith. Given two choices (e.g., \u201ccelery\u201d or \u201cairplane\u201d), the system predicts correctly 77% of\nthe time. The system can even predict at above-chance levels for words it has never seen\nan fMRI image of before (by considering the images of related words) and for people it has\nnever seen before (proving that fMRI reveals some level of common representation across\npeople). This type of work is still in its infancy, but fMRI (and other imaging technology\nsuch as intracranial electrophysiology (Sahin et al., 2009)) promises to give us much more\nconcrete ideasofwhathumanknowledge representations are like.\nFrom the viewpoint of formal logic, representing the same knowledge in two different\nways makes absolutely no difference; the same facts will be derivable from either represen-\ntation. Inpractice, however, onerepresentation mightrequire fewerstepstoderiveaconclu-\nsion, meaning that a reasoner with limited resources could get to the conclusion using one\nrepresentation but not the other. For nondeductive tasks such as learning from experience,\noutcomes are necessarily dependent on the form of the representations used. We show in\nChapter 18 that when a learning program considers two possible theories of the world, both\nofwhichareconsistentwithallthedata,themostcommonwayofbreakingthetieistochoose\nthemostsuccincttheory\u2014andthatdependsonthelanguageusedtorepresenttheories. Thus,\ntheinfluenceoflanguage onthoughtisunavoidable foranyagentthatdoeslearning.\n8.1.2 Combining the bestofformaland natural languages\nWecan adopt the foundation of propositional logic\u2014a declarative, compositional semantics\nthat is context-independent and unambiguous\u2014and build a more expressive logic on that\nfoundation, borrowing representational ideas fromnatural language whileavoiding itsdraw-\nbacks. Whenwelookatthesyntaxofnaturallanguage,themostobviouselementsarenouns\nand noun phrases that referto objects (squares, pits, wumpuses) and verbs and verbphrases\nOBJECT\nthat refer to relations among objects (is breezy, is adjacent to, shoots). Some of these rela-\nRELATION\ntions are functions\u2014relations in which there is only one \u201cvalue\u201d for a given \u201cinput.\u201d It is\nFUNCTION\neasytostartlistingexamplesofobjects, relations, andfunctions:\n\u2022 Objects: people,houses,numbers,theories,RonaldMcDonald,colors,baseballgames,\nwars,centuries ...\n\u2022 Relations: thesecanbeunaryrelations or propertiessuchasred,round, bogus, prime,\nPROPERTY\nmultistoried ...,ormoregeneral n-aryrelations suchasbrotherof,biggerthan,inside,\npartof,hascolor,occurred after,owns,comesbetween, ...\n\u2022 Functions: fatherof,bestfriend,thirdinningof,onemore than,beginning of...\nIndeed, almost any assertion can be thought of as referring toobjects and properties orrela-\ntions. Someexamplesfollow: Section8.1. Representation Revisited 289\n\u2022 \u201cOneplustwoequalsthree.\u201d\nObjects: one, two, three, one plus two; Relation: equals; Function: plus. (\u201cOne plus\ntwo\u201d is a name for the object that is obtained by applying the function \u201cplus\u201d to the\nobjects\u201cone\u201dand\u201ctwo.\u201d \u201cThree\u201disanothernameforthisobject.)\n\u2022 \u201cSquaresneighboring thewumpusaresmelly.\u201d\nObjects: wumpus,squares; Property: smelly;Relation: neighboring.\n\u2022 \u201cEvilKingJohnruledEnglandin1200.\u201d\nObjects: John,England,1200;Relation: ruled;Properties: evil,king.\nThelanguageoffirst-orderlogic,whosesyntaxandsemanticswedefineinthenextsection,\nisbuiltaroundobjectsandrelations. Ithasbeensoimportanttomathematics,philosophy,and\nartificial intelligence precisely because those fields\u2014and indeed, much of everyday human\nexistence\u2014can beusefully thought ofasdealing withobjects andtherelations amongthem.\nFirst-order logic can also express facts about some orall of theobjects inthe universe. This\nenables one to represent general laws or rules, such as the statement \u201cSquares neighboring\nthewumpusaresmelly.\u201d\nTheprimarydifference betweenpropositional andfirst-orderlogicliesinthe ontologi-\nONTOLOGICAL calcommitmentmadebyeachlanguage\u2014thatis,whatitassumesaboutthenatureofreality.\nCOMMITMENT\nMathematically, this commitmentisexpressed through thenature oftheformal modelswith\nrespect to which the truth of sentences is defined. Forexample, propositional logic assumes\nthat there are facts that either hold or do not hold in the world. Each fact can be in one\nof two states: true or false, and each model assigns true or false to each proposition sym-\nbol (see Section 7.4.2).2 First-order logic assumes more; namely, that the world consists of\nobjects with certain relations among them that do or do not hold. The formal models are\ncorrespondingly morecomplicatedthanthoseforpropositional logic. Special-purpose logics\nmake still further ontological commitments; forexample, temporal logic assumes that facts\nTEMPORALLOGIC\nhold at particular times and that those times (which may be points or intervals) are ordered.\nThus, special-purpose logics givecertain kinds ofobjects (andtheaxioms about them)\u201cfirst\nclass\u201d status within the logic, rather than simply defining them within the knowledge base.\nHIGHER-ORDER Higher-order logic views the relations and functions referred to by first-order logic as ob-\nLOGIC\njectsinthemselves. Thisallowsonetomakeassertionsaboutallrelations\u2014forexample,one\ncouldwishtodefinewhatitmeansforarelationtobetransitive. Unlikemostspecial-purpose\nlogics, higher-order logic is strictly more expressive than first-order logic, in the sense that\nsomesentences ofhigher-order logiccannot beexpressed by anyfinitenumberoffirst-order\nlogicsentences.\nEPISTEMOLOGICAL A logic can also be characterized by its epistemological commitments\u2014the possible\nCOMMITMENT\nstates of knowledge that it allows with respect to each fact. In both propositional and first-\norderlogic, asentence represents afactand theagent eitherbelieves thesentence tobetrue,\nbelieves it to be false, or has no opinion. These logics therefore have three possible states\nofknowledge regarding anysentence. Systemsusing probability theory,ontheotherhand,\n2 Incontrast,factsinfuzzylogichaveadegreeoftruthbetween0and1.Forexample,thesentence\u201cViennais\nalargecity\u201dmightbetrueinourworldonlytodegree0.6infuzzylogic. 290 Chapter 8. First-OrderLogic\ncan have any degree of belief, ranging from 0 (total disbelief) to 1 (total belief).3 For ex-\nample, a probabilistic wumpus-world agent might believe that the wumpus is in [1,3] with\nprobability 0.75. The ontological and epistemological commitments of five different logics\naresummarizedinFigure8.1.\nLanguage OntologicalCommitment EpistemologicalCommitment\n(Whatexistsintheworld) (Whatanagentbelievesaboutfacts)\nPropositionallogic facts true\/false\/unknown\nFirst-orderlogic facts,objects,relations true\/false\/unknown\nTemporallogic facts,objects,relations,times true\/false\/unknown\nProbabilitytheory facts degreeofbelief\u2208[0,1]\nFuzzylogic factswithdegreeoftruth\u2208[0,1] knownintervalvalue\nFigure8.1 Formallanguagesandtheirontologicalandepistemologicalcommitments.\nInthenextsection,wewilllaunchintothedetailsoffirst-orderlogic. Justasastudentof\nphysicsrequiressomefamiliaritywithmathematics,astudentofAImustdevelopatalentfor\nworkingwithlogicalnotation. Ontheotherhand,itisalsoimportantnottogettooconcerned\nwith the specifics of logical notation\u2014after all, there are dozens of different versions. The\nmainthingstokeepholdofarehowthelanguage facilitates concise representations andhow\nitssemanticsleadstosoundreasoning procedures.\n8.2 SYNTAX AND SEMANTICS OF FIRST-ORDER LOGIC\nWe begin this section by specifying more precisely the way in which the possible worlds\nof first-order logic reflect the ontological commitment to objects and relations. Then we\nintroduce thevariouselementsofthelanguage, explaining theirsemantics aswegoalong.\n8.2.1 Models forfirst-order logic\nRecall from Chapter 7 that the models of a logical language are the formal structures that\nconstitute the possible worlds under consideration. Each model links the vocabulary of the\nlogical sentences to elements of the possible world, so that the truth of any sentence can\nbe determined. Thus, models for propositional logic link proposition symbols to predefined\ntruth values. Models forfirst-order logic are much moreinteresting. First, they have objects\ninthem! Thedomainofamodelisthesetofobjectsordomainelementsitcontains. Thedo-\nDOMAIN\nmainisrequiredtobenonempty\u2014everypossibleworldmustcontainatleastoneobject. (See\nDOMAINELEMENTS\nExercise 8.7 for a discussion of empty worlds.) Mathematically speaking, it doesn\u2019t matter\nwhattheseobjects are\u2014all thatmatters is howmanythereareineachparticularmodel\u2014but\nfor pedagogical purposes we\u2019ll use a concrete example. Figure 8.2 shows a model with five\n3 Itisimportantnottoconfusethedegreeofbeliefinprobabilitytheorywiththedegreeoftruthinfuzzylogic.\nIndeed,somefuzzysystemsallowuncertainty(degreeofbelief)aboutdegreesoftruth. Section8.2. SyntaxandSemanticsofFirst-OrderLogic 291\nobjects: RichardtheLionheart,KingofEnglandfrom1189to1199;hisyoungerbrother, the\nevilKingJohn,whoruledfrom1199to1215;theleftlegsofRichardandJohn;andacrown.\nThe objects in the model may be related in various ways. In the figure, Richard and\nJohn are brothers. Formally speaking, a relation is just the set of tuples of objects that are\nTUPLE\nrelated. (Atuple isacollection ofobjects arranged inafixedorderandiswrittenwithangle\nbrackets surrounding theobjects.) Thus,thebrotherhood relationinthismodelistheset\n{(cid:16)Richard theLionheart, KingJohn(cid:17), (cid:16)KingJohn, RichardtheLionheart(cid:17)}. (8.1)\n(HerewehavenamedtheobjectsinEnglish,butyoumay,ifyouwish,mentallysubstitutethe\npicturesforthenames.) ThecrownisonKingJohn\u2019shead,sothe\u201conhead\u201drelationcontains\njust one tuple, (cid:16)thecrown, KingJohn(cid:17). The \u201cbrother\u201d and \u201con head\u201d relations are binary\nrelations\u2014that is, they relate pairs of objects. The model also contains unary relations, or\nproperties: the\u201cperson\u201dpropertyistrueofbothRichardandJohn;the\u201cking\u201dpropertyistrue\nonlyofJohn(presumably becauseRichardisdeadatthispoint);andthe\u201ccrown\u201dpropertyis\ntrueonlyofthecrown.\nCertain kinds of relationships are best considered as functions, in that a given object\nmust berelated to exactly one object in this way. Forexample, each person has one left leg,\nsothemodelhasaunary\u201cleftleg\u201dfunction thatincludes thefollowingmappings:\n(cid:16)RichardtheLionheart(cid:17) \u2192 Richard\u2019sleftleg\n(8.2)\n(cid:16)KingJohn(cid:17)\u2192 John\u2019sleftleg.\nStrictly speaking, models in first-order logic require total functions, that is, there must be a\nTOTALFUNCTIONS\nvalueforeveryinputtuple. Thus, thecrownmusthavealeftlegandsomusteachoftheleft\nlegs. Thereisatechnicalsolutiontothisawkwardprobleminvolvinganadditional\u201cinvisible\u201d\ncrown\non head\nbrother\nperson person\nking\nbrother\nR J\n$\nleft leg left leg\nFigure 8.2 A model containing five objects, two binary relations, three unary relations\n(indicatedbylabelsontheobjects),andoneunaryfunction,left-leg. 292 Chapter 8. First-OrderLogic\nobject that is the left leg of everything that has no left leg, including itself. Fortunately, as\nlong as one makes no assertions about the left legs of things that have no left legs, these\ntechnicalities areofnoimport.\nSo far, we have described the elements that populate models for first-order logic. The\nother essential part of a model is the link between those elements and the vocabulary of the\nlogicalsentences, whichweexplainnext.\n8.2.2 Symbols andinterpretations\nWe turn now to the syntax of first-order logic. The impatient reader can obtain a complete\ndescription fromtheformalgrammarinFigure8.3.\nThebasic syntactic elements of first-order logic are the symbols that stand forobjects,\nrelations, and functions. The symbols, therefore, come in three kinds: constant symbols,\nCONSTANTSYMBOL\nwhich stand for objects; predicate symbols, which stand for relations; and function sym-\nPREDICATESYMBOL\nbols, whichstand forfunctions. Weadopt theconvention thatthese symbols willbeginwith\nFUNCTIONSYMBOL\nuppercase letters. For example, we might use the constant symbols Richard and John; the\npredicate symbols Brother, OnHead, Person, King, and Crown; and the function symbol\nLeftLeg. As with proposition symbols, the choice of names is entirely up to the user. Each\npredicate andfunction symbolcomeswithanaritythatfixesthenumberofarguments.\nARITY\nAsin propositional logic, every model must provide the information required to deter-\nmine if any given sentence is true or false. Thus, in addition to its objects, relations, and\nfunctions, each model includes an interpretation that specifies exactly which objects, rela-\nINTERPRETATION\ntions and functions are referred to by the constant, predicate, and function symbols. One\npossibleinterpretation forourexample\u2014whichalogicianwouldcalltheintendedinterpre-\nINTENDED tation\u2014isasfollows:\nINTERPRETATION\n\u2022 Richard referstoRichardtheLionheartand John referstotheevilKingJohn.\n\u2022 Brother refers to the brotherhood relation, that is, the set of tuples of objects given in\nEquation (8.1); OnHead refers to the\u201con head\u201d relation that holds between the crown\nandKingJohn;Person,King,andCrown refertothesetsofobjects thatarepersons,\nkings,andcrowns.\n\u2022 LeftLeg referstothe\u201cleftleg\u201dfunction, thatis,themappinggiven inEquation(8.2).\nThere are many other possible interpretations, of course. For example, one interpretation\nmaps Richard to the crown and John to King John\u2019s left leg. There are five objects in\nthe model, so there are 25 possible interpretations just for the constant symbols Richard\nand John. Notice that not all the objects need have a name\u2014for example, the intended\ninterpretation does not name the crown or the legs. It is also possible for an object to have\nseveral names; there is an interpretation under which both Richard and John refer to the\ncrown.4 If you find this possibility confusing, remember that, in propositional logic, it is\nperfectly possible tohaveamodelinwhich Cloudy andSunny arebothtrue; itisthejobof\ntheknowledgebasetoruleoutmodelsthatareinconsistent withourknowledge.\n4 Later,inSection8.2.8,weexamineasemanticsinwhicheveryobjecthasexactlyonename. Section8.2. SyntaxandSemanticsofFirst-OrderLogic 293\nSentence \u2192 AtomicSentence | ComplexSentence\nAtomicSentence \u2192 Predicate | Predicate(Term,...)| Term =Term\nComplexSentence \u2192 (Sentence )| [Sentence ]\n| \u00acSentence\n| Sentence \u2227Sentence\n| Sentence \u2228Sentence\n| Sentence \u21d2 Sentence\n| Sentence \u21d4 Sentence\n| Quantifier Variable,... Sentence\nTerm \u2192 Function(Term,...)\n| Constant\n| Variable\nQuantifier \u2192 \u2200| \u2203\nConstant \u2192 A| X | John | \u00b7\u00b7\u00b7\n1\nVariable \u2192 a| x| s| \u00b7\u00b7\u00b7\nPredicate \u2192 True | False | After | Loves | Raining | \u00b7\u00b7\u00b7\nFunction \u2192 Mother | LeftLeg | \u00b7\u00b7\u00b7\nOPERATORPRECEDENCE : \u00ac,=,\u2227,\u2228,\u21d2,\u21d4\nFigure8.3 The syntax of first-orderlogic with equality, specified in Backus\u2013Naurform\n(seepage1060ifyouarenotfamiliarwiththisnotation).Operatorprecedencesarespecified,\nfrom highest to lowest. The precedence of quantifiers is such that a quantifier holds over\neverythingtotherightofit.\nR J R J R J R J R J R J\n. . . . . . . . .\nFigure8.4 Somemembersofthesetofallmodelsforalanguagewithtwoconstantsym-\nbols,RandJ,andonebinaryrelationsymbol.Theinterpretationofeachconstantsymbolis\nshownbyagrayarrow.Withineachmodel,therelatedobjectsareconnectedbyarrows. 294 Chapter 8. First-OrderLogic\nInsummary,amodelinfirst-orderlogicconsistsofasetofobjectsandaninterpretation\nthat maps constant symbols to objects, predicate symbols to relations on those objects, and\nfunction symbols to functions on those objects. Just as with propositional logic, entailment,\nvalidity, and so on are defined in terms of all possible models. To get an idea of what the\nset ofallpossible models looks like, see Figure 8.4. Itshows that models vary inhow many\nobjects they contain\u2014from one up to infinity\u2014and in the way the constant symbols map\nto objects. If there are two constant symbols and one object, then both symbols must refer\nto the same object; but this can still happen even with more objects. When there are more\nobjects thanconstant symbols, someoftheobjects willhavenonames. Becausethenumber\nof possible models is unbounded, checking entailment by the enumeration of all possible\nmodelsisnotfeasibleforfirst-orderlogic(unlikepropositional logic). Evenifthenumberof\nobjects is restricted, the number of combinations can be very large. (See Exercise 8.5.) For\ntheexampleinFigure8.4,thereare137,506,194,466 models withsixorfewerobjects.\n8.2.3 Terms\nAtermisalogicalexpression thatreferstoanobject. Constantsymbolsarethereforeterms,\nTERM\nbut itisnotalways convenient tohave adistinct symbol tonameevery object. Forexample,\nin English we might use the expression \u201cKing John\u2019s left leg\u201d rather than giving a name\nto his leg. This is what function symbols are for: instead of using a constant symbol, we\nuse LeftLeg(John). In the general case, a complex term is formed by a function symbol\nfollowedbyaparenthesized listoftermsasargumentstothe functionsymbol. Itisimportant\nto remember that acomplex term is just a complicated kind of name. It is not a \u201csubroutine\ncall\u201d that \u201creturns a value.\u201d There is no LeftLeg subroutine that takes a person as input and\nreturnsaleg. Wecanreasonaboutleftlegs(e.g.,statingthegeneralrulethateveryonehasone\nand then deducing that John must have one) without ever providing a definition of LeftLeg.\nThisissomethingthatcannotbedonewithsubroutines inprogramming languages.5\nThe formal semantics of terms is straightforward. Consider a term f(t ,...,t ). The\n1 n\nfunction symbol f refers to somefunction inthe model(call it F); theargument termsrefer\nto objects in the domain (call them d ,...,d ); and the term as a whole refers to the object\n1 n\nthat is the value of the function F applied to d ,...,d . Forexample, suppose the LeftLeg\n1 n\nfunctionsymbolreferstothefunctionshowninEquation(8.2)andJohn referstoKingJohn,\nthen LeftLeg(John) refers to King John\u2019s left leg. In this way, the interpretation fixes the\nreferentofeveryterm.\n8.2.4 Atomicsentences\nNow that we have both terms for referring to objects and predicate symbols for referring to\nrelations, we can put them together to make atomic sentences that state facts. An atomic\n5 \u03bb-expressions provide a useful notation in which new function symbols are constructed \u201con the fly.\u201d For\nexample, thefunctionthatsquaresitsargumentcanbewrittenas(\u03bbx x\u00d7x)andcanbeappliedtoarguments\njustlikeanyotherfunctionsymbol. A\u03bb-expressioncanalsobedefinedandusedasapredicatesymbol. (See\nChapter22.)ThelambdaoperatorinLispplaysexactlythesamerole.Noticethattheuseof\u03bbinthiswaydoes\nnotincreasetheformalexpressivepoweroffirst-orderlogic,becauseanysentencethatincludesa\u03bb-expression\ncanberewrittenby\u201cpluggingin\u201ditsargumentstoyieldanequivalentsentence. Section8.2. SyntaxandSemanticsofFirst-OrderLogic 295\nsentence (or atom for short) is formed from a predicate symbol optionally followed by a\nATOMICSENTENCE\nparenthesized listofterms,suchas\nATOM\nBrother(Richard,John).\nThis states, under the intended interpretation given earlier, that Richard the Lionheart is the\nbrotherofKingJohn.6 Atomicsentences canhavecomplextermsasarguments. Thus,\nMarried(Father(Richard),Mother(John))\nstates that Richard the Lionheart\u2019s father is married to King John\u2019s mother (again, under a\nsuitable interpretation).\nAnatomic sentence is true in agiven model if the relation referred to by the predicate\nsymbolholdsamongtheobjectsreferredtobythearguments.\n8.2.5 Complex sentences\nWe can use logical connectives to construct more complex sentences, with the same syntax\nandsemanticsasinpropositional calculus. Herearefoursentences thataretrueinthemodel\nofFigure8.2underourintendedinterpretation:\n\u00acBrother(LeftLeg(Richard),John)\nBrother(Richard,John)\u2227Brother(John,Richard)\nKing(Richard)\u2228King(John)\n\u00acKing(Richard) \u21d2 King(John).\n8.2.6 Quantifiers\nOnce we have a logic that allows objects, it is only natural to want to express properties of\nentire collections of objects, instead of enumerating the objects by name. Quantifierslet us\nQUANTIFIER\ndothis. First-orderlogiccontains twostandard quantifiers, calleduniversal andexistential.\nUniversalquantification(\u2200)\nRecall the difficulty we had in Chapter 7 with the expression of general rules in proposi-\ntional logic. Rules such as \u201cSquares neighboring the wumpus are smelly\u201d and \u201cAll kings\nare persons\u201d are the bread and butter of first-order logic. We deal with the first of these in\nSection8.3. Thesecondrule,\u201cAllkingsarepersons,\u201diswritteninfirst-orderlogicas\n\u2200x King(x) \u21d2 Person(x).\n\u2200 is usually pronounced \u201cForall ...\u201d. (Remember that the upside-down A stands for \u201call.\u201d)\nThus,thesentence says, \u201cForallx,ifxisaking, thenxisaperson.\u201d Thesymbolxiscalled\na variable. By convention, variables are lowercase letters. A variable is a term all by itself,\nVARIABLE\nand as such can also serve as the argument of a function\u2014for example, LeftLeg(x). Aterm\nwithnovariables iscalledagroundterm.\nGROUNDTERM\nIntuitively, the sentence \u2200x P, where P is any logical expression, says that P is true\nforevery object x. More precisely, \u2200x P istrue in agiven model if P istrue inall possible\nEXTENDED extendedinterpretationsconstructedfromtheinterpretationgiveninthemodel,whereeach\nINTERPRETATION\n6 Weusuallyfollowtheargument-orderingconventionthatP(x,y)isreadas\u201cxisaP ofy.\u201d 296 Chapter 8. First-OrderLogic\nextended interpretation specifiesadomainelementtowhich xrefers.\nThissoundscomplicated,butitisreallyjustacarefulwayofstatingtheintuitivemean-\ning of universal quantification. Consider the model shown in Figure 8.2 and the intended\ninterpretation thatgoeswithit. Wecanextendtheinterpretation infiveways:\nx\u2192 RichardtheLionheart,\nx\u2192 KingJohn,\nx\u2192 Richard\u2019sleftleg,\nx\u2192 John\u2019sleftleg,\nx\u2192 thecrown.\nTheuniversallyquantifiedsentence \u2200x King(x) \u21d2 Person(x)istrueintheoriginalmodel\nif the sentence King(x) \u21d2 Person(x) is true under each of the five extended interpreta-\ntions. Thatis,theuniversally quantifiedsentence isequivalent toasserting thefollowingfive\nsentences:\nRichardtheLionheartisaking \u21d2 RichardtheLionheartisaperson.\nKingJohnisaking \u21d2 KingJohnisaperson.\nRichard\u2019sleftlegisaking \u21d2 Richard\u2019sleftlegisaperson.\nJohn\u2019sleftlegisaking \u21d2 John\u2019sleftlegisaperson.\nThecrownisaking \u21d2 thecrownisaperson.\nLet us look carefully at this set of assertions. Since, in our model, King John is the only\nking, the second sentence asserts that he is a person, as we would hope. But what about\nthe other four sentences, which appear to make claims about legs and crowns? Is that part\nof the meaning of \u201cAll kings are persons\u201d? In fact, the other four assertions are true in the\nmodel, but make no claim whatsoever about the personhood qualifications of legs, crowns,\norindeed Richard. Thisisbecause noneofthese objects isaking. Lookingatthetruthtable\nfor \u21d2 (Figure 7.8 on page 246), we see that the implication is true whenever its premise is\nfalse\u2014regardless ofthetruthoftheconclusion. Thus,byasserting theuniversally quantified\nsentence, which is equivalent to asserting a whole list of individual implications, we end\nup asserting the conclusion of the rule just for those objects for whom the premise is true\nand saying nothing at all about those individuals for whom the premise is false. Thus, the\ntruth-table definition of \u21d2 turns out to be perfect for writing general rules with universal\nquantifiers.\nAcommonmistake,madefrequently evenbydiligent readers whohavereadthispara-\ngraphseveraltimes,istouseconjunction instead ofimplication. Thesentence\n\u2200x King(x)\u2227Person(x)\nwouldbeequivalent toasserting\nRichardtheLionheartisaking\u2227RichardtheLionheartisaperson,\nKingJohnisaking\u2227KingJohnisaperson,\nRichard\u2019sleftlegisaking\u2227Richard\u2019sleftlegisaperson,\nandsoon. Obviously, thisdoesnotcapturewhatwewant. Section8.2. SyntaxandSemanticsofFirst-OrderLogic 297\nExistentialquantification(\u2203)\nUniversalquantificationmakesstatementsabouteveryobject. Similarly,wecanmakeastate-\nment about some object in the universe without naming it, by using anexistential quantifier.\nTosay,forexample,thatKingJohnhasacrownonhishead,wewrite\n\u2203x Crown(x)\u2227OnHead(x,John).\n\u2203xispronounced \u201cThereexistsan xsuchthat...\u201dor\u201cForsome x...\u201d.\nIntuitively, the sentence \u2203x P says that P is true for at least one object x. More\nprecisely, \u2203x P is true in a given model if P is true in at least one extended interpretation\nthatassignsxtoadomainelement. Thatis,atleastoneofthefollowingistrue:\nRichardtheLionheartisacrown\u2227RichardtheLionheartisonJohn\u2019shead;\nKingJohnisacrown\u2227KingJohnisonJohn\u2019shead;\nRichard\u2019sleftlegisacrown\u2227Richard\u2019sleftlegisonJohn\u2019shead;\nJohn\u2019sleftlegisacrown\u2227John\u2019sleftlegisonJohn\u2019shead;\nThecrownisacrown\u2227thecrownisonJohn\u2019shead.\nThe fifth assertion is true in the model, so the original existentially quantified sentence is\ntrue in the model. Notice that, by ourdefinition, the sentence would also be true in a model\nin which King John was wearing two crowns. This is entirely consistent with the original\nsentence \u201cKingJohnhasacrownonhishead.\u201d 7\nJustas\u21d2appearstobethenaturalconnectivetousewith\u2200,\u2227isthenaturalconnective\nto use with \u2203. Using \u2227 as the main connective with \u2200 led to an overly strong statement in\ntheexample inthe previous section; using \u21d2with\u2203usually leads toavery weakstatement,\nindeed. Considerthefollowingsentence:\n\u2203x Crown(x) \u21d2 OnHead(x,John).\nOn the surface, this might look like a reasonable rendition of our sentence. Applying the\nsemantics, weseethatthesentence saysthatatleastoneofthefollowingassertions istrue:\nRichardtheLionheartisacrown \u21d2 RichardtheLionheart isonJohn\u2019shead;\nKingJohnisacrown \u21d2 KingJohnisonJohn\u2019s head;\nRichard\u2019sleftlegisacrown \u21d2 Richard\u2019sleftlegisonJohn\u2019shead;\nandsoon. Nowanimplicationistrueifbothpremiseandconclusionaretrue,orifitspremise\nis false. So if Richard the Lionheart is not a crown, then the first assertion is true and the\nexistential is satisfied. So, an existentially quantified implication sentence is true whenever\nanyobjectfailstosatisfythepremise;hencesuchsentences reallydonotsaymuchatall.\nNestedquantifiers\nWe will often want to express more complex sentences using multiple quantifiers. The sim-\nplestcaseiswherethequantifiers areofthesametype. Forexample, \u201cBrothersaresiblings\u201d\ncanbewrittenas\n\u2200x \u2200y Brother(x,y) \u21d2 Sibling(x,y).\n7 Thereisavariantoftheexistentialquantifier,usuallywritten\u22031or\u2203!,thatmeans\u201cThereexistsexactlyone.\u201d\nThesamemeaningcanbeexpressedusingequalitystatements. 298 Chapter 8. First-OrderLogic\nConsecutive quantifiers of the same type can be written as one quantifier with several vari-\nables. Forexample,tosaythatsiblinghood isasymmetricrelationship, wecanwrite\n\u2200x,y Sibling(x,y) \u21d4 Sibling(y,x).\nIn other cases we will have mixtures. \u201cEverybody loves somebody\u201d means that for every\nperson, thereissomeonethatpersonloves:\n\u2200x \u2203y Loves(x,y).\nOntheotherhand,tosay\u201cThereissomeonewhoislovedbyeveryone,\u201dwewrite\n\u2203y \u2200x Loves(x,y).\nTheorderofquantification isthereforeveryimportant. Itbecomesclearerifweinsertparen-\ntheses. \u2200x(\u2203y Loves(x,y)) says that everyone has a particular property, namely, the prop-\nerty that they love someone. On the other hand, \u2203y (\u2200x Loves(x,y)) says that someone in\ntheworldhasaparticularproperty, namelythepropertyofbeinglovedbyeverybody.\nSomeconfusion can arise when two quantifiers are used with the same variable name.\nConsiderthesentence\n\u2200x (Crown(x)\u2228(\u2203x Brother(Richard,x))).\nHere the x in Brother(Richard,x) is existentially quantified. The rule is that the variable\nbelongs to the innermost quantifier that mentions it; then it will not be subject to any other\nquantification. Another way to think of it is this: \u2203x Brother(Richard,x) is a sentence\naboutRichard(thathehasabrother), notabout x;soputtinga\u2200xoutsideithasnoeffect. It\ncouldequallywellhavebeenwritten\u2203z Brother(Richard,z). Becausethiscanbeasource\nofconfusion, wewillalwaysusedifferentvariablenameswithnestedquantifiers.\nConnectionsbetween\u2200and\u2203\nThetwoquantifiers areactually intimately connected witheachother, through negation. As-\nserting that everyone dislikes parsnips is the same as asserting there does not exist someone\nwholikesthem,andviceversa:\n\u2200x \u00acLikes(x,Parsnips) isequivalent to \u00ac\u2203x Likes(x,Parsnips).\nWecangoonestepfurther: \u201cEveryonelikesicecream\u201dmeansthatthereisnoonewhodoes\nnotlikeicecream:\n\u2200x Likes(x,IceCream) isequivalent to \u00ac\u2203x \u00acLikes(x,IceCream).\nBecause\u2200isreallyaconjunctionovertheuniverseofobjectsand \u2203isadisjunction, itshould\nnotbesurprising that theyobey DeMorgan\u2019s rules. TheDeMorgan rules forquantified and\nunquantified sentences areasfollows:\n\u2200x \u00acP \u2261 \u00ac\u2203x P \u00ac(P \u2228Q) \u2261 \u00acP \u2227\u00acQ\n\u00ac\u2200x P \u2261 \u2203x \u00acP \u00ac(P \u2227Q) \u2261 \u00acP \u2228\u00acQ\n\u2200x P \u2261 \u00ac\u2203x \u00acP P \u2227Q \u2261 \u00ac(\u00acP \u2228\u00acQ)\n\u2203x P \u2261 \u00ac\u2200x \u00acP P \u2228Q \u2261 \u00ac(\u00acP \u2227\u00acQ).\nThus, we do not really need both \u2200and \u2203, just as wedo not really need both \u2227 and \u2228. Still,\nreadability ismoreimportant thanparsimony, sowewillkeepbothofthequantifiers. Section8.2. SyntaxandSemanticsofFirst-OrderLogic 299\n8.2.7 Equality\nFirst-order logic includes onemorewaytomakeatomic sentences, otherthan using apredi-\ncateandtermsasdescribedearlier. Wecanusethe equalitysymboltosignifythattwoterms\nEQUALITYSYMBOL\nrefertothesameobject. Forexample,\nFather(John)=Henry\nsaysthattheobjectreferred toby Father(John)andtheobjectreferred toby Henry arethe\nsame. Because an interpretation fixes the referent of any term, determining the truth of an\nequalitysentenceissimplyamatterofseeingthatthereferentsofthetwotermsarethesame\nobject.\nTheequalitysymbolcanbeusedtostatefactsaboutagivenfunction,aswejustdidfor\ntheFather symbol. Itcanalsobeusedwithnegationtoinsistthattwotermsarenotthesame\nobject. TosaythatRichardhasatleasttwobrothers, wewouldwrite\n\u2203x,y Brother(x,Richard)\u2227Brother(y,Richard)\u2227\u00ac(x=y).\nThesentence\n\u2203x,y Brother(x,Richard)\u2227Brother(y,Richard)\ndoesnothavetheintendedmeaning. Inparticular, itistrue inthemodelofFigure8.2,where\nRichardhasonlyonebrother. Toseethis,considertheextended interpretation inwhichboth\nx and y are assigned to King John. The addition of \u00ac(x=y) rules out such models. The\nnotation x (cid:7)= y issometimesusedasanabbreviation for\u00ac(x=y).\n8.2.8 Analternativesemantics?\nContinuing the example from the previous section, suppose that webelieve that Richard has\ntwobrothers, JohnandGeoffrey.8 Canwecapturethisstateofaffairsbyasserting\nBrother(John,Richard)\u2227Brother(Geoffrey,Richard)? (8.3)\nNot quite. First, this assertion is true in a model where Richard has only one brother\u2014\nwe need to add John (cid:7)= Geoffrey. Second, the sentence doesn\u2019t rule out models in which\nRichard has manymore brothers besides John and Geoffrey. Thus, thecorrect translation of\n\u201cRichard\u2019s brothersareJohnandGeoffrey\u201disasfollows:\nBrother(John,Richard)\u2227Brother(Geoffrey,Richard)\u2227John (cid:7)= Geoffrey\n\u2227\u2200x Brother(x,Richard) \u21d2 (x=John \u2228x=Geoffrey).\nFor many purposes, this seems much more cumbersome than the corresponding natural-\nlanguage expression. As a consequence, humans may make mistakes in translating their\nknowledge into first-order logic, resulting in unintuitive behaviors from logical reasoning\nsystems that use the knowledge. Can we devise a semantics that allows a more straightfor-\nwardlogicalexpression?\nOneproposalthatisverypopularindatabasesystemsworksasfollows. First,weinsist\nthat every constant symbol refer to a distinct object\u2014the so-called unique-names assump-\nUNIQUE-NAMES tion. Second, we assume that atomic sentences not known to be true are in fact false\u2014the\nASSUMPTION\nCLOSED-WORLD closed-world assumption. Finally, we invoke domain closure, meaning that each model\nASSUMPTION\nDOMAINCLOSURE 8 Actuallyhehadfour,theothersbeingWilliamandHenry. 300 Chapter 8. First-OrderLogic\nR J R J R J R J R J\nR R R R . . . R\nJ J J J J\nFigure8.5 Somemembersofthesetofallmodelsforalanguagewithtwoconstantsym-\nbols,RandJ,andonebinaryrelationsymbol,underdatabasesemantics. Theinterpretation\noftheconstantsymbolsisfixed,andthereisadistinctobjectforeachconstantsymbol.\ncontains no more domain elements than those named by the constant symbols. Under the\nDATABASE resulting semantics, which we call database semantics to distinguish it from the standard\nSEMANTICS\nsemantics of first-order logic, the sentence Equation (8.3) does indeed state that Richard\u2019s\ntwo brothers are John and Geoffrey. Database semantics is also used in logic programming\nsystems,asexplained inSection9.4.5.\nIt is instructive to consider the set of all possible models under database semantics for\nthe same case as shown in Figure 8.4. Figure 8.5 shows some of the models, ranging from\nthe model with no tuples satisfying the relation to the model with all tuples satisfying the\nrelation. With two objects, there are four possible two-element tuples, so there are 24=16\ndifferent subsets oftuples that cansatisfy therelation. Thus, there are16possible modelsin\nall\u2014alotfewerthantheinfinitelymanymodelsforthestandardfirst-ordersemantics. Onthe\notherhand,thedatabasesemantics requiresdefiniteknowledgeofwhattheworldcontains.\nThis example brings up an important point: there is no one \u201ccorrect\u201d semantics for\nlogic. The usefulness of any proposed semantics depends on how concise and intuitive it\nmakes the expression of the kinds of knowledge we want to write down, and on how easy\nandnatural itistodevelop thecorresponding rules ofinference. Database semantics ismost\nuseful when we are certain about the identity of all the objects described in the knowledge\nbase and whenwehave allthe facts athand; inother cases, itisquite awkward. Forthe rest\nofthischapter,weassumethestandardsemanticswhilenotinginstancesinwhichthischoice\nleadstocumbersomeexpressions.\n8.3 USING FIRST-ORDER LOGIC\nNowthatwehavedefinedanexpressivelogicallanguage,itistimetolearnhowtouseit. The\nbestwaytodothisisthroughexamples. Wehaveseensomesimplesentencesillustrating the\nvariousaspectsoflogicalsyntax;inthissection, weprovidemoresystematicrepresentations\nof some simple domains. In knowledge representation, a domain is just some part of the\nDOMAIN\nworldaboutwhichwewishtoexpresssomeknowledge.\nWebegin withabriefdescription ofthe TELL\/ASK interface forfirst-orderknowledge\nbases. Then we look at the domains of family relationships, numbers, sets, and lists, and at Section8.3. UsingFirst-OrderLogic 301\nthewumpusworld. Thenextsectioncontainsamoresubstantialexample(electroniccircuits)\nandChapter12coverseverything intheuniverse.\n8.3.1 Assertions andqueries infirst-order logic\nSentencesareaddedtoaknowledgebaseusingTELL,exactlyasinpropositional logic. Such\nsentences are called assertions. Forexample, wecan assert that John is aking, Richard is a\nASSERTION\nperson, andallkingsarepersons:\nTELL(KB, King(John)).\nTELL(KB, Person(Richard)).\nTELL(KB, \u2200x King(x) \u21d2 Person(x)).\nWecanaskquestions oftheknowledgebaseusing ASK. Forexample,\nASK(KB, King(John))\nQUERY\nreturnstrue. QuestionsaskedwithASKarecalledqueriesorgoals. Generallyspeaking,any\nquery thatislogically entailed bytheknowledge baseshould beanswered affirmatively. For\nGOAL\nexample,giventhetwopreceding assertions, thequery\nASK(KB, Person(John))\nshouldalsoreturn true. Wecanaskquantifiedqueries, suchas\nASK(KB, \u2203x Person(x)).\nThe answer is true, but this is perhaps not as helpful as we would like. It is rather like\nanswering \u201cCan you tell me the time?\u201d with \u201cYes.\u201d If we want to know what value of x\nmakesthesentence true,wewillneedadifferentfunction, ASKVARS,whichwecallwith\nASKVARS(KB,Person(x))\nandwhichyieldsastream ofanswers. Inthiscasetherewillbetwoanswers: {x\/John}and\nSUBSTITUTION\n{x\/Richard}. Suchanansweriscalledasubstitutionorbindinglist. ASKVARS isusually\nreserved forknowledge bases consisting solely of Horn clauses, because in such knowledge\nBINDINGLIST\nbases every way of making the query true will bind the variables to specific values. That is\nnotthecasewithfirst-orderlogic; if KB hasbeentoldKing(John)\u2228King(Richard),then\nthereisnobinding toxforthequery \u2203x King(x),eventhoughthequeryistrue.\n8.3.2 The kinshipdomain\nThefirstexampleweconsideristhedomainoffamilyrelationships, orkinship. Thisdomain\nincludes facts such as \u201cElizabeth is the mother of Charles\u201d and \u201cCharles is the father of\nWilliam\u201dandrulessuchas\u201cOne\u2019sgrandmotheristhemotherofone\u2019sparent.\u201d\nClearly,theobjectsinourdomainarepeople. Wehavetwounarypredicates, Male and\nFemale. Kinship relations\u2014parenthood, brotherhood, marriage, andsoon\u2014arerepresented\nby binary predicates: Parent, Sibling, Brother, Sister, Child, Daughter, Son, Spouse,\nWife, Husband, Grandparent, Grandchild, Cousin, Aunt, and Uncle. We use functions\nfor Mother and Father, because every person has exactly one of each of these (at least\naccording tonature\u2019sdesign). 302 Chapter 8. First-OrderLogic\nWe can go through each function and predicate, writing down what we know in terms\noftheothersymbols. Forexample, one\u2019smotherisone\u2019sfemaleparent:\n\u2200m,c Mother(c)=m \u21d4 Female(m)\u2227Parent(m,c).\nOne\u2019shusband isone\u2019smalespouse:\n\u2200w,h Husband(h,w) \u21d4 Male(h)\u2227Spouse(h,w).\nMaleandfemalearedisjoint categories:\n\u2200x Male(x) \u21d4 \u00acFemale(x).\nParentandchildareinverserelations:\n\u2200p,c Parent(p,c) \u21d4 Child(c,p).\nAgrandparent isaparentofone\u2019sparent:\n\u2200g,c Grandparent(g,c) \u21d4 \u2203p Parent(g,p)\u2227Parent(p,c).\nAsibling isanotherchildofone\u2019sparents:\n\u2200x,y Sibling(x,y) \u21d4 x (cid:7)= y\u2227\u2203p Parent(p,x)\u2227Parent(p,y).\nWecouldgoonforseveralmorepageslikethis,andExercise8.14asksyoutodojustthat.\nEachofthesesentencescanbeviewedasanaxiomofthekinshipdomain,asexplained\nin Section 7.1. Axioms are commonly associated with purely mathematical domains\u2014we\nwillseesomeaxiomsfornumbersshortly\u2014buttheyareneeded inalldomains. Theyprovide\nthe basic factual information from which useful conclusions can be derived. Our kinship\naxioms are also definitions; they have theform \u2200x,y P(x,y) \u21d4 .... Theaxioms define\nDEFINITION\ntheMother functionandtheHusband,Male,Parent,Grandparent,andSibling predicates\nintermsofotherpredicates. Ourdefinitions \u201cbottom out\u201dat abasicsetofpredicates (Child,\nSpouse, and Female) in terms of which the others are ultimately defined. This is a natural\nway in which to build up the representation of a domain, and it is analogous to the way in\nwhichsoftwarepackages arebuiltupbysuccessive definitions ofsubroutines fromprimitive\nlibrary functions. Notice that there is not necessarily a unique set of primitive predicates;\nwecould equally wellhave usedParent,Spouse,andMale. Insomedomains, asweshow,\nthereisnoclearly identifiablebasicset.\nNotalllogicalsentencesaboutadomainareaxioms. Somearetheorems\u2014thatis,they\nTHEOREM\nareentailedbytheaxioms. Forexample,considertheassertionthatsiblinghoodissymmetric:\n\u2200x,y Sibling(x,y) \u21d4 Sibling(y,x).\nIs this an axiom ora theorem? In fact, it is a theorem that follows logically from the axiom\nthatdefinessiblinghood. IfweASK theknowledge basethissentence, itshouldreturn true.\nFrom a purely logical point of view, a knowledge base need contain only axioms and\nno theorems, because the theorems do not increase the set of conclusions that follow from\nthe knowledge base. From a practical point of view, theorems are essential to reduce the\ncomputational costofderiving newsentences. Without them,areasoning system hastostart\nfromfirstprincipleseverytime,ratherlikeaphysicisthavingtorederivetherulesofcalculus\nforeverynewproblem. Section8.3. UsingFirst-OrderLogic 303\nNot all axioms are definitions. Some provide more general information about certain\npredicates without constituting a definition. Indeed, some predicates have no complete defi-\nnition because we do not know enough to characterize them fully. For example, there is no\nobviousdefinitivewaytocompletethesentence\n\u2200x Person(x) \u21d4 ...\nFortunately, first-order logic allows us to make use of the Person predicate without com-\npletelydefiningit. Instead, wecanwritepartialspecifications ofproperties thateveryperson\nhasandproperties thatmakesomething aperson:\n\u2200x Person(x) \u21d2 ...\n\u2200x ... \u21d2 Person(x).\nAxioms can also be \u201cjust plain facts,\u201d such as Male(Jim) and Spouse(Jim,Laura).\nSuch facts form the descriptions of specific problem instances, enabling specific questions\nto be answered. The answers to these questions will then be theorems that follow from\nthe axioms. Often, one finds that the expected answers are not forthcoming\u2014for example,\nfromSpouse(Jim,Laura)oneexpects(underthelawsofmanycountries)tobeabletoinfer\n\u00acSpouse(George,Laura);butthisdoesnotfollowfromtheaxiomsgivenearlier\u2014evenafter\nweaddJim (cid:7)= George assuggested inSection8.2.8. Thisisasignthatanaxiomismissing.\nExercise8.8asksthereadertosupplyit.\n8.3.3 Numbers, sets,and lists\nNumbers are perhaps the most vivid example of how a large theory can be built up from\na tiny kernel of axioms. We describe here the theory of natural numbers or non-negative\nNATURALNUMBERS\nintegers. We need a predicate NatNum that will be true of natural numbers; we need one\nconstant symbol, 0; and we need one function symbol, S (successor). The Peano axioms\nPEANOAXIOMS\ndefinenaturalnumbersandaddition.9 Naturalnumbersaredefinedrecursively:\nNatNum(0).\n\u2200n NatNum(n) \u21d2 NatNum(S(n)).\nThat is, 0 is a natural number, and for every object n, if n is a natural number, then S(n) is\na natural number. So the natural numbers are 0, S(0), S(S(0)), and so on. (After reading\nSection 8.2.8, you will notice that these axioms allow for other natural numbers besides the\nusualones;seeExercise8.12.) Wealsoneedaxiomstoconstrain thesuccessorfunction:\n\u2200n 0 (cid:7)= S(n).\n\u2200m,n m (cid:7)= n \u21d2 S(m)(cid:7)= S(n).\nNowwecandefineadditionintermsofthesuccessorfunction:\n\u2200m NatNum(m) \u21d2 +(0,m) = m.\n\u2200m,n NatNum(m)\u2227NatNum(n) \u21d2 +(S(m),n) = S(+(m,n)).\nThe first of these axioms says that adding 0 to any natural number m gives m itself. Notice\ntheuseofthebinaryfunctionsymbol\u201c+\u201dintheterm+(m,0);inordinary mathematics, the\ntermwouldbewritten m+0usinginfixnotation. (Thenotationwehaveusedforfirst-order\nINFIX\n9 ThePeano axiomsalso include theprinciple of induction, whichisa sentence of second-order logic rather\nthanoffirst-orderlogic.TheimportanceofthisdistinctionisexplainedinChapter9. 304 Chapter 8. First-OrderLogic\nlogiciscalledprefix.) Tomakeoursentencesaboutnumberseasiertoread,weallowtheuse\nPREFIX\nofinfixnotation. WecanalsowriteS(n)asn+1,sothesecond axiombecomes\n\u2200m,n NatNum(m)\u2227NatNum(n) \u21d2 (m+1)+n = (m+n)+1.\nThisaxiomreduces additiontorepeated application ofthesuccessorfunction.\nThe use of infix notation is an example of syntactic sugar, that is, an extension to or\nSYNTACTICSUGAR\nabbreviation of the standard syntax that does not change the semantics. Any sentence that\nusessugarcanbe\u201cdesugared\u201dtoproduceanequivalentsentenceinordinaryfirst-orderlogic.\nOnce we have addition, it is straightforward to define multiplication as repeated addi-\ntion, exponentiation asrepeated multiplication, integer division and remainders, primenum-\nbers, and so on. Thus, the whole of numbertheory (including cryptography) can be built up\nfromoneconstant, onefunction, onepredicate andfouraxioms.\nThe domain of sets is also fundamental to mathematics as well as to commonsense\nSET\nreasoning. (Infact, itispossible todefinenumbertheory in termsofsettheory.) Wewantto\nbeable torepresent individual sets, including the empty set. Weneed awaytobuild up sets\nby adding an element to a set or taking the union or intersection of two sets. We will want\nto know whether an element is a member of a set and we will want to distinguish sets from\nobjectsthatarenotsets.\nWewilluse the normal vocabulary of set theory as syntactic sugar. The empty set is a\nconstant written as {}. There is one unary predicate, Set, which is true of sets. The binary\npredicates are x\u2208s (x is a memberof set s)and s \u2286 s (set s is a subset, not necessarily\n1 2 1\nproper, of set s ). The binary functions are s \u2229s (the intersection of two sets), s \u222as\n2 1 2 1 2\n(the union of two sets), and {x|s} (the set resulting from adjoining element x to set s). One\npossible setofaxiomsisasfollows:\n1. Theonlysetsaretheemptysetandthosemadebyadjoining something toaset:\n\u2200s Set(s) \u21d4 (s={})\u2228(\u2203x,s Set(s )\u2227s={x|s }).\n2 2 2\n2. The empty set has no elements adjoined into it. In other words, there is no way to\ndecompose{}intoasmallersetandanelement:\n\u00ac\u2203x,s {x|s}={}.\n3. Adjoininganelementalreadyinthesethasnoeffect:\n\u2200x,s x\u2208s \u21d4 s={x|s}.\n4. The only members of a set are the elements that were adjoined into it. We express\nthis recursively, saying that x is a member of s if and only if s is equal to some set s\n2\nadjoinedwithsomeelementy,whereeither y isthesameasxorxisamemberofs :\n2\n\u2200x,s x\u2208s \u21d4 \u2203y,s (s={y|s }\u2227(x=y\u2228x\u2208s )).\n2 2 2\n5. Asetisasubset ofanothersetifandonly ifallofthefirstset\u2019s membersaremembers\nofthesecondset:\n\u2200s ,s s \u2286 s \u21d4 (\u2200x x\u2208s \u21d2 x\u2208s ).\n1 2 1 2 1 2\n6. Twosetsareequalifandonlyifeachisasubsetoftheother:\n\u2200s ,s (s =s ) \u21d4 (s \u2286 s \u2227s \u2286 s ).\n1 2 1 2 1 2 2 1 Section8.3. UsingFirst-OrderLogic 305\n7. Anobjectisintheintersection oftwosetsifandonlyifitisamemberofbothsets:\n\u2200x,s ,s x\u2208(s \u2229s ) \u21d4 (x\u2208s \u2227x\u2208s ).\n1 2 1 2 1 2\n8. Anobjectisintheunionoftwosetsifandonlyifitisamemberofeitherset:\n\u2200x,s ,s x\u2208(s \u222as ) \u21d4 (x\u2208s \u2228x\u2208s ).\n1 2 1 2 1 2\nLists are similar to sets. The differences are that lists are ordered and the same element can\nLIST\nappearmorethanonceinalist. WecanusethevocabularyofLispforlists: Nil istheconstant\nlist with no elements; Cons, Append, First, and Rest are functions; and Find is the pred-\nicate that does for lists what Member does for sets. List? is a predicate that is true only of\nlists. Aswithsets,itiscommontousesyntacticsugarinlogicalsentencesinvolvinglists. The\nempty list is[]. Theterm Cons(x,y), where y isanonempty list, iswritten [x|y]. Theterm\nCons(x,Nil) (i.e., the list containing the element x) is written as [x]. A list of several ele-\nments,suchas[A,B,C],correspondstothenestedtermCons(A,Cons(B,Cons(C,Nil))).\nExercise8.16asksyoutowriteouttheaxiomsforlists.\n8.3.4 The wumpus world\nSome propositional logic axioms for the wumpus world were given in Chapter 7. The first-\norderaxioms inthis section aremuch moreconcise, capturing inanatural wayexactly what\nwewanttosay.\nRecall that the wumpus agent receives a percept vector with fiveelements. The corre-\nspondingfirst-ordersentencestoredintheknowledgebasemustincludeboththeperceptand\nthetimeatwhichitoccurred; otherwise,theagentwillgetconfused aboutwhenitsawwhat.\nWeuseintegersfortimesteps. Atypicalperceptsentence wouldbe\nPercept([Stench,Breeze,Glitter,None,None],5).\nHere,Percept isabinary predicate, and Stench andsoonareconstants placedinalist. The\nactionsinthewumpusworldcanberepresented bylogicalterms:\nTurn(Right), Turn(Left), Forward, Shoot, Grab, Climb .\nTodetermine whichisbest,theagentprogram executesthequery\nASKVARS(\u2203a BestAction(a,5)),\nwhich returns a binding list such as {a\/Grab}. The agent program can then return Grab as\nthe action to take. The raw percept data implies certain facts about the current state. For\nexample:\n\u2200t,s,g,m,c Percept([s,Breeze,g,m,c],t) \u21d2 Breeze(t),\n\u2200t,s,b,m,c Percept([s,b,Glitter,m,c],t) \u21d2 Glitter(t),\nandsoon. Theserulesexhibitatrivialformofthereasoning processcalledperception,which\nwestudyindepthinChapter24. Noticethequantificationovertimet. Inpropositionallogic,\nwewouldneedcopiesofeachsentence foreachtimestep.\nSimple\u201creflex\u201dbehaviorcanalsobeimplemented byquantifiedimplication sentences.\nForexample,wehave\n\u2200t Glitter(t) \u21d2 BestAction(Grab,t). 306 Chapter 8. First-OrderLogic\nGiventheperceptandrulesfromthepreceding paragraphs, thiswouldyieldthedesiredcon-\nclusionBestAction(Grab,5)\u2014that is,Grab istherightthingtodo.\nWe have represented the agent\u2019s inputs and outputs; now it is time to represent the\nenvironment itself. Let us begin with objects. Obvious candidates are squares, pits, and the\nwumpus. Wecouldnameeachsquare\u2014Square andsoon\u2014butthenthefactthatSquare\n1,2 1,2\nand Square are adjacent would have to be an \u201cextra\u201d fact, and we would need one such\n1,3\nfactforeach pairofsquares. Itisbettertouseacomplex term inwhich therowandcolumn\nappearasintegers; forexample,wecansimplyusethelistterm [1,2]. Adjacencyofanytwo\nsquarescanbedefinedas\n\u2200x,y,a,b Adjacent([x,y],[a,b]) \u21d4\n(x = a\u2227(y = b\u22121\u2228y = b+1))\u2228(y = b\u2227(x = a\u22121\u2228x = a+1)).\nWe could name each pit, but this would be inappropriate for a different reason: there is no\nreason to distinguish among pits.10 It is simpler to use a unary predicate Pit that is true of\nsquares containing pits. Finally, since there is exactly one wumpus, a constant Wumpus is\njustasgoodasaunarypredicate(andperhapsmoredignifiedfromthewumpus\u2019sviewpoint).\nThe agent\u2019s location changes over time, so we write At(Agent,s,t) to mean that the\nagentisatsquaresattimet. Wecanfixthewumpus\u2019slocationwith\u2200tAt(Wumpus,[2,2],t).\nWecanthensaythatobjectscanonlybeatonelocation atatime:\n\u2200x,s ,s ,t At(x,s ,t)\u2227At(x,s ,t) \u21d2 s =s .\n1 2 1 2 1 2\nGiven its current location, the agent can infer properties of the square from properties of its\ncurrent percept. For example, if the agent is at a square and perceives a breeze, then that\nsquareisbreezy:\n\u2200s,t At(Agent,s,t)\u2227Breeze(t) \u21d2 Breezy(s).\nItisusefultoknowthatasquareisbreezybecauseweknowthatthepitscannotmoveabout.\nNoticethatBreezy hasnotimeargument.\nHavingdiscovered whichplacesarebreezy(orsmelly)and,veryimportant, notbreezy\n(ornotsmelly),theagentcandeducewherethepitsare(andwherethewumpusis). Whereas\npropositionallogicnecessitatesaseparateaxiomforeachsquare(seeR andR onpage247)\n2 3\nandwouldneedadifferentsetofaxiomsforeachgeographicallayoutoftheworld,first-order\nlogicjustneedsoneaxiom:\n\u2200s Breezy(s) \u21d4 \u2203r Adjacent(r,s)\u2227Pit(r). (8.4)\nSimilarly, in first-order logic wecan quantify overtime, so weneed just one successor-state\naxiom for each predicate, rather than a different copy for each time step. For example, the\naxiomforthearrow(Equation(7.2)onpage267)becomes\n\u2200t HaveArrow(t+1) \u21d4 (HaveArrow(t)\u2227\u00acAction(Shoot,t)).\nFrom these two example sentences, we can see that the first-order logic formulation is no\nless concise than the original English-language description given in Chapter 7. The reader\n10 Similarly,mostofusdonotnameeachbirdthatfliesoverheadasitmigratestowarmerregionsinwinter. An\nornithologistwishingtostudymigrationpatterns,survivalrates,andsoondoesnameeachbird,bymeansofa\nringonitsleg,becauseindividualbirdsmustbetracked. Section8.4. KnowledgeEngineering inFirst-OrderLogic 307\nis invited to construct analogous axioms for the agent\u2019s location and orientation; in these\ncases, the axioms quantify over both space and time. As in the case of propositional state\nestimation,anagentcanuselogicalinferencewithaxiomsofthiskindtokeeptrackofaspects\noftheworldthatarenotdirectlyobserved. Chapter10goesintomoredepthonthesubjectof\nfirst-ordersuccessor-state axiomsandtheirusesforconstructing plans.\n8.4 KNOWLEDGE ENGINEERING IN FIRST-ORDER LOGIC\nThe preceding section illustrated the use of first-order logic to represent knowledge in three\nsimpledomains. Thissectiondescribesthegeneralprocessofknowledge-baseconstruction\u2014\nKNOWLEDGE aprocesscalledknowledgeengineering. Aknowledgeengineerissomeonewhoinvestigates\nENGINEERING\naparticular domain, learns whatconcepts areimportant inthatdomain, andcreates aformal\nrepresentation of the objects and relations in the domain. We illustrate the knowledge engi-\nneering process in anelectronic circuit domain that should already befairly familiar, so that\nwecanconcentrate ontherepresentational issues involved. Theapproach wetakeissuitable\nfor developing special-purpose knowledge bases whose domain is carefully circumscribed\nand whose range of queries is known in advance. General-purpose knowledge bases, which\ncover a broad range of human knowledge and are intended to support tasks such as natural\nlanguage understanding, arediscussed inChapter12.\n8.4.1 The knowledge-engineering process\nKnowledge engineering projects vary widely in content, scope, and difficulty, but all such\nprojects includethefollowingsteps:\n1. Identify the task. The knowledge engineer must delineate the range of questions that\nthe knowledge base will support and the kinds of facts that will be available for each\nspecific problem instance. Forexample, does the wumpus knowledge base need to be\nable to choose actions or is it required to answer questions only about the contents\nof the environment? Will the sensor facts include the current location? The task will\ndeterminewhatknowledgemustberepresentedinordertoconnectprobleminstancesto\nanswers. ThisstepisanalogoustothePEASprocessfordesigning agentsinChapter2.\n2. Assembletherelevant knowledge. Theknowledge engineer might already beanexpert\nin the domain, or might need to work with real experts to extract what they know\u2014a\nKNOWLEDGE process called knowledgeacquisition. Atthisstage, theknowledge isnotrepresented\nACQUISITION\nformally. Theideaistounderstand thescope oftheknowledge base, asdetermined by\nthetask,andtounderstand howthedomainactuallyworks.\nFor the wumpus world, which is defined by an artificial set of rules, the relevant\nknowledge is easy to identify. (Notice, however, that the definition of adjacency was\nnot supplied explicitly in the wumpus-world rules.) For real domains, the issue of\nrelevance can be quite difficult\u2014for example, a system for simulating VLSI designs\nmightormightnotneedtotakeintoaccountstraycapacitances andskineffects. 308 Chapter 8. First-OrderLogic\n3. Decide on a vocabulary of predicates, functions, and constants. That is, translate the\nimportant domain-level concepts intologic-level names. Thisinvolves manyquestions\nof knowledge-engineering style. Like programming style, this can have a significant\nimpact on the eventual success of the project. Forexample, should pits be represented\nby objects or by a unary predicate on squares? Should the agent\u2019s orientation be a\nfunction or a predicate? Should the wumpus\u2019s location depend on time? Once the\nchoices have been made, the result is a vocabulary that is known as the ontology of\nONTOLOGY\nthe domain. The word ontology means a particular theory of the nature of being or\nexistence. Theontology determines what kinds of things exist, but does not determine\ntheirspecificproperties andinterrelationships.\n4. Encode general knowledge about the domain. The knowledge engineer writes down\nthe axioms for all the vocabulary terms. This pins down (to the extent possible) the\nmeaningoftheterms, enabling theexpert tocheck thecontent. Often,thisstepreveals\nmisconceptions orgaps in the vocabulary that mustbe fixedby returning tostep 3 and\niteratingthrough theprocess.\n5. Encode a description of the specific problem instance. If the ontology is well thought\nout, this step will be easy. It will involve writing simple atomic sentences about in-\nstances of concepts that are already part of the ontology. Fora logical agent, problem\ninstancesaresuppliedbythesensors,whereasa\u201cdisembodied\u201d knowledgebaseissup-\nplied with additional sentences in the same way that traditional programs are supplied\nwithinputdata.\n6. Pose queries to the inference procedure and get answers. This is where the reward is:\nwecanlettheinference procedure operate ontheaxiomsandproblem-specific factsto\nderive the facts we are interested in knowing. Thus, we avoid the need for writing an\napplication-specific solution algorithm.\n7. Debug the knowledge base. Alas, the answers to queries will seldom be correct on\nthe first try. More precisely, the answers will be correct for the knowledge base as\nwritten, assuming that the inference procedure is sound, but they will not be the ones\nthattheuserisexpecting. Forexample,ifanaxiomismissing,somequerieswillnotbe\nanswerable from the knowledge base. A considerable debugging process could ensue.\nMissingaxiomsoraxiomsthataretooweakcanbeeasily identifiedbynoticing places\nwhere the chain of reasoning stops unexpectedly. For example, if the knowledge base\nincludesadiagnostic rule(seeExercise8.13)forfindingthewumpus,\n\u2200s Smelly(s) \u21d2 Adjacent(Home(Wumpus),s),\ninstead of the biconditional, then the agent will never be able to prove the absence of\nwumpuses. Incorrect axioms can be identified because they are false statements about\ntheworld. Forexample,thesentence\n\u2200x NumOfLegs(x,4) \u21d2 Mammal(x)\nis false for reptiles, amphibians, and, more importantly, tables. The falsehood of this\nsentencecanbedeterminedindependentlyoftherestoftheknowledgebase. Incontrast, Section8.4. KnowledgeEngineering inFirst-OrderLogic 309\natypicalerrorinaprogramlookslikethis:\noffset = position + 1.\nItisimpossibletotellwhetherthisstatementiscorrectwithoutlookingattherestofthe\nprogram to see whether, for example, offsetis used to refer to the current position,\nor to one beyond the current position, or whether the value of positionis changed\nbyanotherstatementandsooffsetshouldalsobechanged again.\nTo understand this seven-step process better, we now apply it to an extended example\u2014the\ndomainofelectronic circuits.\n8.4.2 The electronic circuits domain\nWewilldevelopanontologyandknowledgebasethatallowustoreasonaboutdigitalcircuits\nofthekindshowninFigure8.6. Wefollowtheseven-stepprocessforknowledgeengineering.\nIdentifythetask\nThere are many reasoning tasks associated with digital circuits. At the highest level, one\nanalyzes the circuit\u2019s functionality. For example, does the circuit in Figure 8.6 actually add\nproperly? If all the inputs are high, what is the output of gate A2? Questions about the\ncircuit\u2019s structure are also interesting. For example, what are all the gates connected to the\nfirstinput terminal? Doesthe circuit contain feedback loops? Thesewillbeourtasks inthis\nsection. There are more detailed levels of analysis, including those related to timing delays,\ncircuit area, power consumption, production cost, and so on. Each of these levels would\nrequireadditional knowledge.\nAssembletherelevantknowledge\nWhat do we know about digital circuits? Forour purposes, they are composed of wires and\ngates. Signals flow along wires to the input terminals of gates, and each gate produces a\nC\n1\n1\n2 X 1 X 2 1\n3 A\n2\nA O 1 2\n1\nFigure8.6 AdigitalcircuitC1,purportingtobeaone-bitfulladder. Thefirsttwoinputs\narethetwobitstobeadded,andthethirdinputisacarrybit. Thefirstoutputisthesum,and\nthesecondoutputisacarrybitforthenextadder. ThecircuitcontainstwoXORgates,two\nANDgates,andoneORgate. 310 Chapter 8. First-OrderLogic\nsignal on theoutput terminal thatflowsalong another wire. Todetermine whatthese signals\nwill be, we need to know how the gates transform their input signals. There are four types\nofgates: AND,OR,and XORgateshave twoinput terminals, and NOTgates have one. All\ngateshaveoneoutputterminal. Circuits, likegates,haveinputandoutput terminals.\nToreason about functionality and connectivity, we do not need to talk about the wires\nthemselves, the paths they take, or the junctions where they come together. All that matters\nis the connections between terminals\u2014we can say that one output terminal is connected to\nanotherinputterminalwithouthavingtosaywhatactuallyconnectsthem. Otherfactorssuch\nasthesize,shape, color,orcostofthevariouscomponents areirrelevant toouranalysis.\nIfourpurpose weresomething otherthan verifying designs atthegatelevel, theontol-\nogywouldbedifferent. Forexample,ifwewereinterested in debugging faultycircuits, then\nit would probably be a good idea to include the wires in the ontology, because a faulty wire\ncancorruptthesignalflowingalongit. Forresolvingtiming faults,wewouldneedtoinclude\ngate delays. If we were interested in designing a product that would be profitable, then the\ncostofthecircuitanditsspeedrelativetootherproductsonthemarketwouldbeimportant.\nDecideonavocabulary\nWenowknowthatwewanttotalkaboutcircuits,terminals,signals,andgates. Thenextstep\nistochoose functions, predicates, and constants torepresent them. First, weneed tobeable\nto distinguish gates from each other and from other objects. Each gate is represented as an\nobjectnamedbyaconstant, aboutwhichweassertthatitisagatewith,say,Gate(X ). The\n1\nbehavior of each gate is determined by its type: one of the constants AND,OR, XOR, or\nNOT. Because a gate has exactly one type, a function is appropriate: Type(X )=XOR.\n1\nCircuits,likegates,areidentifiedbyapredicate: Circuit(C ).\n1\nNextweconsiderterminals,whichareidentifiedbythepredicateTerminal(x). Agate\norcircuitcanhaveoneormoreinputterminalsandoneormore outputterminals. Weusethe\nfunction In(1,X ) to denote the first input terminal for gate X . A similar function Out is\n1 1\nusedforoutputterminals. Thefunction Arity(c,i,j)saysthatcircuitchasiinputandj out-\nputterminals. Theconnectivitybetweengatescanberepresentedbyapredicate, Connected,\nwhichtakestwoterminalsasarguments, asin Connected(Out(1,X ),In(1,X )).\n1 2\nFinally,weneedtoknowwhetherasignalisonoroff. Onepossibility istouseaunary\npredicate, On(t), which is true when the signal at a terminal is on. This makes it a little\ndifficult, however, to pose questions such as \u201cWhat are all the possible values of the signals\nattheoutputterminalsofcircuit C ?\u201d Wethereforeintroduce asobjectstwosignalvalues, 1\n1\nand0,andafunction Signal(t)thatdenotes thesignalvaluefortheterminal t.\nEncodegeneralknowledgeofthedomain\nOnesignthatwehaveagoodontology isthatwerequireonlyafewgeneralrules,whichcan\nbestatedclearlyandconcisely. Thesearealltheaxiomswewillneed:\n1. Iftwoterminalsareconnected, thentheyhavethesamesignal:\n\u2200t ,t Terminal(t )\u2227Terminal(t )\u2227Connected(t ,t ) \u21d2\n1 2 1 2 1 2\nSignal(t )=Signal(t ).\n1 2 Section8.4. KnowledgeEngineering inFirst-OrderLogic 311\n2. Thesignalateveryterminaliseither1or0:\n\u2200t Terminal(t) \u21d2 Signal(t)=1\u2228Signal(t)=0 .\n3. Connectediscommutative:\n\u2200t ,t Connected(t ,t ) \u21d4 Connected(t ,t ).\n1 2 1 2 2 1\n4. Therearefourtypesofgates:\n\u2200g Gate(g)\u2227k = Type(g) \u21d2 k = AND \u2228k = OR\u2228k = XOR\u2228k = NOT .\n5. AnANDgate\u2019soutputis0ifandonlyifanyofitsinputsis0:\n\u2200g Gate(g)\u2227Type(g)=AND \u21d2\nSignal(Out(1,g))=0 \u21d4 \u2203n Signal(In(n,g))=0.\n6. AnORgate\u2019soutputis1ifandonlyifanyofitsinputsis1:\n\u2200g Gate(g)\u2227Type(g)=OR \u21d2\nSignal(Out(1,g))=1 \u21d4 \u2203n Signal(In(n,g))=1.\n7. AnXORgate\u2019soutputis1ifandonlyifitsinputsaredifferent:\n\u2200g Gate(g)\u2227Type(g)=XOR \u21d2\nSignal(Out(1,g))=1 \u21d4 Signal(In(1,g)) (cid:7)= Signal(In(2,g)).\n8. ANOTgate\u2019soutputisdifferentfromitsinput:\n\u2200g Gate(g)\u2227(Type(g)=NOT) \u21d2\nSignal(Out(1,g)) (cid:7)= Signal(In(1,g)) .\n9. Thegates(exceptforNOT)havetwoinputsandoneoutput.\n\u2200g Gate(g)\u2227Type(g) = NOT \u21d2 Arity(g,1,1) .\n\u2200g Gate(g)\u2227k = Type(g)\u2227(k = AND \u2228k = OR\u2228k = XOR) \u21d2\nArity(g,2,1)\n10. Acircuithasterminals, uptoitsinputandoutputarity, andnothing beyonditsarity:\n\u2200c,i,j Circuit(c)\u2227Arity(c,i,j) \u21d2\n\u2200n (n \u2264 i \u21d2 Terminal(In(c,n)))\u2227(n > i \u21d2 In(c,n) = Nothing)\u2227\n\u2200n (n \u2264 j \u21d2 Terminal(Out(c,n)))\u2227(n > j \u21d2 Out(c,n) = Nothing)\n11. Gates,terminals, signals, gatetypes,andNothing arealldistinct.\n\u2200g,t Gate(g)\u2227Terminal(t) \u21d2\ng (cid:7)= t (cid:7)= 1 (cid:7)= 0 (cid:7)=OR (cid:7)= AND (cid:7)= XOR (cid:7)= NOT (cid:7)= Nothing .\n12. Gatesarecircuits.\n\u2200g Gate(g) \u21d2 Circuit(g)\nEncodethespecificprobleminstance\nThecircuitshowninFigure8.6isencodedascircuit C withthefollowingdescription. First,\n1\nwecategorize thecircuitanditscomponent gates:\nCircuit(C )\u2227Arity(C ,3,2)\n1 1\nGate(X )\u2227Type(X )=XOR\n1 1\nGate(X )\u2227Type(X )=XOR\n2 2\nGate(A )\u2227Type(A )=AND\n1 1\nGate(A )\u2227Type(A )=AND\n2 2\nGate(O )\u2227Type(O )=OR .\n1 1 312 Chapter 8. First-OrderLogic\nThen,weshowtheconnections betweenthem:\nConnected(Out(1,X ),In(1,X )) Connected(In(1,C ),In(1,X ))\n1 2 1 1\nConnected(Out(1,X ),In(2,A )) Connected(In(1,C ),In(1,A ))\n1 2 1 1\nConnected(Out(1,A ),In(1,O )) Connected(In(2,C ),In(2,X ))\n2 1 1 1\nConnected(Out(1,A ),In(2,O )) Connected(In(2,C ),In(2,A ))\n1 1 1 1\nConnected(Out(1,X ),Out(1,C )) Connected(In(3,C ),In(2,X ))\n2 1 1 2\nConnected(Out(1,O ),Out(2,C )) Connected(In(3,C ),In(1,A )).\n1 1 1 2\nPosequeriestotheinferenceprocedure\nWhatcombinations ofinputs wouldcause thefirstoutput of C (thesumbit)tobe0andthe\n1\nsecondoutputofC (thecarrybit)tobe1?\n1\n\u2203i ,i ,i Signal(In(1,C ))=i \u2227Signal(In(2,C ))=i \u2227Signal(In(3,C ))=i\n1 2 3 1 1 1 2 1 3\n\u2227Signal(Out(1,C ))=0\u2227Signal(Out(2,C ))=1.\n1 1\nThe answers are substitutions forthe variables i , i , and i such that the resulting sentence\n1 2 3\nisentailed bytheknowledge base. ASKVARS willgiveusthreesuchsubstitutions:\n{i \/1, i \/1, i \/0} {i \/1, i \/0, i \/1} {i \/0, i \/1, i \/1}.\n1 2 3 1 2 3 1 2 3\nWhatarethepossible setsofvaluesofalltheterminalsfortheaddercircuit?\n\u2203i ,i ,i ,o ,o Signal(In(1,C ))=i \u2227Signal(In(2,C ))=i\n1 2 3 1 2 1 1 1 2\n\u2227Signal(In(3,C ))=i \u2227Signal(Out(1,C ))=o \u2227Signal(Out(2,C ))=o .\n1 3 1 1 1 2\nThis final query will return a complete input\u2013output table for the device, which can be used\nto check that it does in fact add its inputs correctly. This is a simple example of circuit\nCIRCUIT verification. We can also use the definition of the circuit to build larger digital systems, for\nVERIFICATION\nwhichthesamekindofverification procedure canbecarriedout. (SeeExercise8.26.) Many\ndomainsareamenabletothesamekindofstructuredknowledge-base development,inwhich\nmorecomplexconcepts aredefinedontopofsimplerconcepts.\nDebugtheknowledgebase\nWecanperturbtheknowledgebaseinvariouswaystoseewhatkindsoferroneousbehaviors\nemerge. For example, suppose we fail to read Section 8.2.8 and hence forget to assert that\n1 (cid:7)= 0. Suddenly, the system will be unable to prove any outputs for the circuit, except for\nthe input cases 000 and 110. Wecan pinpoint the problem byasking forthe outputs ofeach\ngate. Forexample,wecanask\n\u2203i ,i ,o Signal(In(1,C ))=i \u2227Signal(In(2,C ))=i \u2227Signal(Out(1,X )),\n1 2 1 1 1 2 1\nwhich reveals that nooutputs are knownat X forthe input cases 10 and01. Then, welook\n1\nattheaxiomforXORgates,asappliedtoX :\n1\nSignal(Out(1,X ))=1 \u21d4 Signal(In(1,X )) (cid:7)= Signal(In(2,X )).\n1 1 1\nIftheinputsareknowntobe,say,1and0,thenthisreducesto\nSignal(Out(1,X ))=1 \u21d4 1 (cid:7)= 0.\n1\nNowthe problem is apparent: the system isunable to infer that Signal(Out(1,X ))=1, so\n1\nweneedtotellitthat1 (cid:7)=0. Section8.5. Summary 313\n8.5 SUMMARY\nThischapterhasintroduced first-orderlogic,arepresentationlanguagethatisfarmorepow-\nerfulthanpropositional logic. Theimportantpointsareas follows:\n\u2022 Knowledge representation languages should be declarative, compositional, expressive,\ncontextindependent, andunambiguous.\n\u2022 Logics differ in their ontological commitments and epistemological commitments.\nWhilepropositional logiccommitsonlytotheexistence offacts, first-orderlogiccom-\nmitstotheexistence ofobjects andrelationsandthereby gainsexpressivepower.\n\u2022 The syntax of first-order logic builds on that of propositional logic. It adds terms to\nrepresent objects, and has universal and existential quantifiers to construct assertions\naboutallorsomeofthepossible valuesofthequantifiedvariables.\n\u2022 Apossibleworld,ormodel,forfirst-orderlogicincludes asetofobjectsandan inter-\npretationthatmapsconstantsymbolstoobjects, predicate symbolstorelationsamong\nobjects, andfunctionsymbolstofunctions onobjects.\n\u2022 Anatomicsentenceistruejustwhentherelationnamedbythepredicateholdsbetween\ntheobjectsnamedbytheterms. Extendedinterpretations, whichmapquantifiervari-\nablestoobjectsinthemodel,definethetruthofquantified sentences.\n\u2022 Developingaknowledgebaseinfirst-orderlogicrequiresacarefulprocessofanalyzing\nthe domain, choosing a vocabulary, and encoding the axioms required to support the\ndesiredinferences.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nAlthough Aristotle\u2019s logic deals with generalizations over objects, it fell far short of the ex-\npressivepoweroffirst-orderlogic. Amajorbarriertoitsfurtherdevelopmentwasitsconcen-\ntrationonone-place predicates totheexclusionofmany-placerelational predicates. Thefirst\nsystematic treatment of relations was given by Augustus De Morgan (1864), who cited the\nfollowingexampletoshowthesortsofinferencesthatAristotle\u2019slogiccouldnothandle: \u201cAll\nhorses are animals; therefore, the head of a horse is the head of an animal.\u201d This inference\nis inaccessible to Aristotle because any valid rule that can support this inference must first\nanalyzethesentenceusingthetwo-placepredicate\u201cxistheheadofy.\u201d Thelogicofrelations\nwasstudied indepthbyCharlesSandersPeirce(1870, 2004).\nTruefirst-orderlogicdatesfromtheintroductionofquantifiersinGottlobFrege\u2019s(1879)\nBegriffschrift (\u201cConcept Writing\u201d or \u201cConceptual Notation\u201d). Peirce (1883) also developed\nfirst-order logic independently of Frege, although slightly later. Frege\u2019s ability to nest quan-\ntifiers was a big step forward, but he used an awkward notation. The present notation for\nfirst-order logic isduesubstantially toGiuseppe Peano (1889), but thesemantics isvirtually\nidenticaltoFrege\u2019s. Oddlyenough,Peano\u2019saxiomsweredueinlargemeasuretoGrassmann\n(1861)andDedekind(1888). 314 Chapter 8. First-OrderLogic\nLeopoldLo\u00a8wenheim(1915)gaveasystematictreatmentofmodeltheoryforfirst-order\nlogic, including the firstproper treatment ofthe equality symbol. Lo\u00a8wenheim\u2019s results were\nfurther extended by Thoralf Skolem (1920). Alfred Tarski (1935, 1956) gave an explicit\ndefinitionoftruthandmodel-theoretic satisfaction infirst-orderlogic,usingsettheory.\nMcCarthy(1958)wasprimarilyresponsible fortheintroductionoffirst-orderlogicasa\ntoolforbuildingAIsystems. Theprospectsforlogic-based AIwereadvancedsignificantlyby\nRobinson\u2019s (1965) development ofresolution, acomplete procedure forfirst-order inference\ndescribedinChapter9. ThelogicistapproachtookrootatStanfordUniversity. CordellGreen\n(1969a,1969b)developedafirst-orderreasoningsystem,QA3,leadingtothefirstattemptsto\nbuildalogicalrobotatSRI(FikesandNilsson,1971). First-orderlogicwasappliedbyZohar\nManna and Richard Waldinger (1971) for reasoning about programs and later by Michael\nGenesereth (1984) for reasoning about circuits. In Europe, logic programming (a restricted\nformoffirst-orderreasoning) wasdevelopedforlinguistic analysis(Colmerauer etal.,1973)\nand for general declarative systems (Kowalski, 1974). Computational logic was also well\nentrenchedatEdinburghthroughtheLCF(LogicforComputableFunctions)project(Gordon\netal.,1979). Thesedevelopments arechronicled furtherinChapters9and12.\nPractical applications built with first-order logic include a system for evaluating the\nmanufacturing requirementsforelectronicproducts(Mannion,2002),asystemforreasoning\nabout policies for fileaccess and digital rights management (Halpern and Weissman, 2008),\nandasystemfortheautomatedcomposition ofWebservices (McIlraithandZeng,2001).\nReactions to the Whorf hypothesis (Whorf, 1956) and the problem of language and\nthoughtingeneral,appearinseveralrecentbooks(Gumperz andLevinson,1996;Bowerman\nandLevinson,2001;Pinker,2003;GentnerandGoldin-Meadow,2003). The\u201ctheory\u201dtheory\n(Gopnik and Glymour, 2002; Tenenbaum et al., 2007) views children\u2019s learning about the\nworld as analogous to the construction of scientific theories. Just as the predictions of a\nmachine learning algorithm depend strongly on the vocabulary supplied to it, so will the\nchild\u2019sformulationoftheoriesdependonthelinguisticenvironmentinwhichlearningoccurs.\nThere are a number of good introductory texts on first-order logic, including some by\nleading figures in the history of logic: Alfred Tarski (1941), Alonzo Church (1956), and\nW.V.Quine(1982) (whichisoneofthemostreadable). Enderton (1972)givesamoremath-\nematically oriented perspective. A highly formal treatment of first-order logic, along with\nmany more advanced topics in logic, is provided by Bell and Machover (1977). Manna and\nWaldinger (1985) give a readable introduction to logic from a computer science perspec-\ntive, as do Huth and Ryan (2004), who concentrate on program verification. Barwise and\nEtchemendy(2002)takeanapproachsimilartotheoneusedhere. Smullyan(1995)presents\nresults concisely, using the tableau format. Gallier (1986) provides an extremely rigorous\nmathematical exposition of first-order logic, along witha great deal ofmaterial on its use in\nautomatedreasoning. LogicalFoundationsofArtificialIntelligence (GeneserethandNilsson,\n1987)isbothasolid introduction tologic andthefirstsystematic treatment oflogical agents\nwith percepts and actions, and there are two good handbooks: van Bentham and terMeulen\n(1997) andRobinson andVoronkov (2001). Thejournal ofrecord forthefieldofpuremath-\nematical logic is the Journal of Symbolic Logic, whereas the Journal of Applied Logic deals\nwithconcerns closertothoseofartificialintelligence. Exercises 315\nEXERCISES\n8.1 Alogicalknowledgebaserepresents theworldusingasetofsentenceswithnoexplicit\nstructure. An analogical representation, ontheotherhand, hasphysical structure that corre-\nspondsdirectlytothestructureofthethingrepresented. Consideraroadmapofyourcountry\nasananalogicalrepresentation offactsaboutthecountry\u2014itrepresentsfactswithamaplan-\nguage. Thetwo-dimensionalstructureofthemapcorrespondstothetwo-dimensionalsurface\nofthearea.\na. Givefiveexamplesofsymbolsinthemaplanguage.\nb. An explicit sentence is a sentence that the creator of the representation actually writes\ndown. An implicit sentence is a sentence that results from explicit sentences because\nofpropertiesoftheanalogical representation. Givethree exampleseachofimplicitand\nexplicitsentences inthemaplanguage.\nc. Givethreeexamplesoffactsaboutthephysicalstructureofyourcountrythatcannotbe\nrepresented inthemaplanguage.\nd. Givetwoexamplesoffactsthataremucheasiertoexpressin themaplanguage thanin\nfirst-orderlogic.\ne. Givetwootherexamplesofusefulanalogical representations. Whataretheadvantages\nanddisadvantages ofeachoftheselanguages?\n8.2 Consider a knowledge base containing just two sentences: P(a) and P(b). Does this\nknowledgebaseentail\u2200xP(x)? Explainyouranswerintermsofmodels.\n8.3 Isthesentence\u2203x,y x=y valid? Explain.\n8.4 Writedownalogicalsentencesuchthateveryworldinwhichitistruecontainsexactly\noneobject.\n8.5 Consider a symbol vocabulary that contains c constant symbols, p predicate symbols\nk\nofeach arity k, and f function symbols ofeach arity k, where 1 \u2264 k \u2264 A. Letthe domain\nk\nsizebefixedatD. Foranygivenmodel,eachpredicateorfunction symbolismappedontoa\nrelationorfunction, respectively, ofthesamearity. Youmayassumethatthefunctions inthe\nmodelallowsomeinputtuplestohavenovalueforthefunction(i.e.,thevalueistheinvisible\nobject). Derivea formula forthe number of possible models foradomain with D elements.\nDon\u2019tworryabouteliminating redundant combinations.\n8.6 Whichofthefollowingarevalid(necessarily true)sentences?\na. (\u2203xx=x) \u21d2 (\u2200y \u2203z y=z).\nb. \u2200x P(x)\u2228\u00acP(x).\nc. \u2200x Smart(x)\u2228(x=x).\n8.7 Consider a version of the semantics for first-order logic in which models with empty\ndomains areallowed. Giveatleasttwoexamples ofsentences thatarevalidaccording tothe 316 Chapter 8. First-OrderLogic\nstandard semantics but not according to the new semantics. Discuss which outcome makes\nmoreintuitivesenseforyourexamples.\n8.8 Does the fact \u00acSpouse(George,Laura) follow from the facts Jim (cid:7)= George and\nSpouse(Jim,Laura)? If so, give aproof; if not, supply additional axioms as needed. What\nhappens ifweuseSpouse asaunaryfunctionsymbolinsteadofabinarypredicate?\n8.9 Thisexerciseusesthefunction MapColor andpredicates In(x,y),Borders(x,y),and\nCountry(x), whose arguments are geographical regions, along with constant symbols for\nvarious regions. In each of the following wegive an English sentence and a number ofcan-\ndidate logical expressions. Foreach of the logical expressions, state whether it (1) correctly\nexpresses the English sentence; (2) issyntactically invalid and therefore meaningless; or(3)\nissyntactically validbutdoesnotexpressthemeaningoftheEnglishsentence.\na. ParisandMarseillesarebothinFrance.\n(i) In(Paris \u2227Marseilles,France).\n(ii) In(Paris,France)\u2227In(Marseilles,France).\n(iii) In(Paris,France)\u2228In(Marseilles,France).\nb. Thereisacountrythatborders bothIraqandPakistan.\n(i) \u2203c Country(c)\u2227Border(c,Iraq)\u2227Border(c,Pakistan).\n(ii) \u2203c Country(c) \u21d2 [Border(c,Iraq)\u2227Border(c,Pakistan)].\n(iii) [\u2203c Country(c)] \u21d2 [Border(c,Iraq)\u2227Border(c,Pakistan)].\n(iv) \u2203c Border(Country(c),Iraq \u2227Pakistan).\nc. Allcountries thatborderEcuadorareinSouthAmerica.\n(i) \u2200c Country(c)\u2227Border(c,Ecuador) \u21d2 In(c,SouthAmerica).\n(ii) \u2200c Country(c) \u21d2 [Border(c,Ecuador) \u21d2 In(c,SouthAmerica)].\n(iii) \u2200c [Country(c) \u21d2 Border(c,Ecuador)] \u21d2 In(c,SouthAmerica).\n(iv) \u2200c Country(c)\u2227Border(c,Ecuador)\u2227In(c,SouthAmerica).\nd. NoregioninSouthAmericabordersanyregioninEurope.\n(i) \u00ac[\u2203c,d In(c,SouthAmerica)\u2227In(d,Europe)\u2227Borders(c,d)].\n(ii) \u2200c,d [In(c,SouthAmerica)\u2227In(d,Europe)] \u21d2 \u00acBorders(c,d)].\n(iii) \u00ac\u2200c In(c,SouthAmerica) \u21d2 \u2203d In(d,Europe)\u2227\u00acBorders(c,d).\n(iv) \u2200c In(c,SouthAmerica) \u21d2 \u2200d In(d,Europe) \u21d2 \u00acBorders(c,d).\ne. Notwoadjacentcountries havethesamemapcolor.\n(i) \u2200x,y \u00acCountry(x)\u2228\u00acCountry(y)\u2228\u00acBorders(x,y)\u2228\n\u00ac(MapColor(x) = MapColor(y)).\n(ii) \u2200x,y (Country(x)\u2227Country(y)\u2227Borders(x,y)\u2227\u00ac(x= y)) \u21d2\n\u00ac(MapColor(x) = MapColor(y)).\n(iii) \u2200x,y Country(x)\u2227Country(y)\u2227Borders(x,y)\u2227\n\u00ac(MapColor(x) = MapColor(y)).\n(iv) \u2200x,y (Country(x)\u2227Country(y)\u2227Borders(x,y)) \u21d2 MapColor(x(cid:7)= y). Exercises 317\n8.10 Consideravocabulary withthefollowingsymbols:\nOccupation(p,o): Predicate. Person phasoccupation o.\nCustomer(p1,p2): Predicate. Personp1isacustomerofperson p2.\nBoss(p1,p2): Predicate. Personp1isabossofperson p2.\nDoctor,Surgeon,Lawyer,Actor: Constants denoting occupations.\nEmily,Joe: Constantsdenoting people.\nUsethesesymbolstowritethefollowingassertions infirst-orderlogic:\na. Emilyiseitherasurgeon oralawyer.\nb. Joeisanactor, buthealsoholdsanotherjob.\nc. Allsurgeons aredoctors.\nd. Joedoesnothavealawyer(i.e.,isnotacustomerofanylawyer).\ne. Emilyhasabosswhoisalawyer.\nf. Thereexistsalawyerallofwhosecustomersaredoctors.\ng. Everysurgeon hasalawyer.\n8.11 Completethefollowingexercisesaboutlogicalsenntences:\na. Translateinto good,naturalEnglish(noxsorys!):\n\u2200x,y,l SpeaksLanguage(x,l)\u2227SpeaksLanguage(y,l)\n\u21d2 Understands(x,y)\u2227Understands(y,x).\nb. Explainwhythissentence isentailed bythesentence\n\u2200x,y,l SpeaksLanguage(x,l)\u2227SpeaksLanguage(y,l)\n\u21d2 Understands(x,y).\nc. Translateintofirst-orderlogicthefollowingsentences:\n(i) Understanding leadstofriendship.\n(ii) Friendship istransitive.\nRemembertodefineallpredicates, functions, andconstants youuse.\n8.12 Rewrite the first two Peano axioms in Section 8.3.3 as a single axiom that defines\nNatNum(x)soastoexcludethepossibilityofnaturalnumbersexceptforthosegeneratedby\nthesuccessorfunction.\n8.13 Equation(8.4)onpage306definestheconditionsunderwhichasquareisbreezy. Here\nweconsidertwootherwaystodescribe thisaspectofthewumpusworld.\na. Wecanwritediagnosticrulesleadingfromobservedeffectstohiddencauses. Forfind-\nDIAGNOSTICRULE\ningpits,theobviousdiagnosticrulessaythatifasquareisbreezy,someadjacentsquare\nmustcontain apit; andifasquareisnotbreezy, then noadjacent square contains apit.\nWrite these two rules in first-order logic and show that their conjunction is logically\nequivalenttoEquation(8.4).\nb. Wecanwritecausalrulesleadingfromcausetoeffect. Oneobviouscausalruleisthat\nCAUSALRULE\napitcausesalladjacentsquarestobebreezy. Writethisruleinfirst-orderlogic,explain\nwhyitisincomplete comparedtoEquation(8.4),andsupplythemissingaxiom. 318 Chapter 8. First-OrderLogic\nGeorge Mum\nSpencer Kydd Elizabeth Philip Margaret\nDiana Charles Anne Mark Andrew Sarah Edward Sophie\nWilliam Harry Peter Zara Beatrice Eugenie Louise James\nFigure8.7 Atypicalfamilytree. Thesymbol\u201c(cid:10)(cid:11)\u201dconnectsspousesandarrowspointto\nchildren.\n8.14 Write axioms describing the predicates Grandchild, Greatgrandparent, Ancestor,\nBrother, Sister, Daughter, Son, FirstCousin, BrotherInLaw, SisterInLaw, Aunt, and\nUncle. Find out the proper definition of mth cousin n times removed, and write the def-\ninition in first-order logic. Now write down the basic facts depicted in the family tree in\nFigure 8.7. Using a suitable logical reasoning system, TELL it all the sentences you have\nwrittendown,and ASK itwhoareElizabeth\u2019s grandchildren, Diana\u2019sbrothers-in-law, Zara\u2019s\ngreat-grandparents, andEugenie\u2019sancestors.\n8.15 Explain what is wrong with the following proposed definition of the set membership\npredicate \u2208:\n\u2200x,s x\u2208{x|s}\n\u2200x,s x\u2208s \u21d2 \u2200y x\u2208{y|s}.\n8.16 Using the set axioms as examples, write axioms for the list domain, including all the\nconstants, functions, andpredicates mentionedinthechapter.\n8.17 Explain what is wrong with the following proposed definition of adjacent squares in\nthewumpusworld:\n\u2200x,y Adjacent([x,y],[x+1,y])\u2227Adjacent([x,y],[x,y +1]).\n8.18 Write out the axioms required for reasoning about the wumpus\u2019s location, using a\nconstant symbol Wumpus and a binary predicate At(Wumpus,Location). Remember that\nthereisonlyonewumpus.\n8.19 Assuming predicates Parent(p,q) and Female(p) and constants Joan and Kevin,\nwiththeobviousmeanings, expresseachofthefollowingsentences infirst-orderlogic. (You\nmayusetheabbreviation \u22031 tomean\u201cthereexistsexactlyone.\u201d)\na. Joanhasadaughter(possibly morethanone,andpossibly sonsaswell).\nb. Joanhasexactlyonedaughter(butmayhavesonsaswell).\nc. Joanhasexactlyonechild,adaughter.\nd. JoanandKevinhaveexactlyonechildtogether.\ne. JoanhasatleastonechildwithKevin,andnochildren withanyoneelse. Exercises 319\n8.20 Arithmetic assertions can be written in first-order logic with the predicate symbol <,\nthe function symbols +and\u00d7, and theconstant symbols 0and 1. Additional predicates can\nalsobedefinedwithbiconditionals.\na. Representtheproperty\u201cxisanevennumber.\u201d\nb. Representtheproperty\u201cxisprime.\u201d\nc. Goldbach\u2019s conjecture is the conjecture (unproven as yet) that every even number is\nequaltothesumoftwoprimes. Representthisconjecture asa logicalsentence.\n8.21 InChapter6,weusedequalitytoindicatetherelationbetweenavariableanditsvalue.\nFor instance, we wrote WA=red to mean that Western Australia is colored red. Repre-\nsenting this in first-order logic, we must write more verbosely ColorOf(WA)=red. What\nincorrectinferencecouldbedrawnifwewrotesentencessuchasWA=red directlyaslogical\nassertions?\n8.22 Write in first-order logic the assertion that every key and at least one of every pair of\nsockswilleventuallybelostforever,usingonlythefollowingvocabulary: Key(x),xisakey;\nSock(x), x is a sock; Pair(x,y), x and y are a pair; Now, the current time; Before(t ,t ),\n1 2\ntimet comesbeforetimet ;Lost(x,t),objectxislostattimet.\n1 2\n8.23 Foreachofthefollowingsentences inEnglish, decideiftheaccompanying first-order\nlogic sentence is a good translation. If not, explain whynot and correct it. (Some sentences\nmayhavemorethanoneerror!)\na. Notwopeoplehavethesamesocialsecurity number.\n\u00ac\u2203x,y,n Person(x)\u2227Person(y) \u21d2 [HasSS#(x,n)\u2227HasSS#(y,n)].\nb. John\u2019ssocialsecurity numberisthesameasMary\u2019s.\n\u2203n HasSS#(John,n)\u2227HasSS#(Mary,n).\nc. Everyone\u2019ssocialsecuritynumberhasninedigits.\n\u2200x,n Person(x) \u21d2 [HasSS#(x,n)\u2227Digits(n,9)].\nd. Rewriteeachoftheabove(uncorrected)sentencesusingafunctionsymbolSS#instead\nofthepredicate HasSS#.\n8.24 Represent the following sentences in first-order logic, using a consistent vocabulary\n(whichyoumustdefine):\na. Somestudents tookFrenchinspring2001.\nb. EverystudentwhotakesFrenchpassesit.\nc. OnlyonestudenttookGreekinspring2001.\nd. ThebestscoreinGreekisalwayshigherthanthebestscoreinFrench.\ne. Everypersonwhobuysapolicyissmart.\nf. Nopersonbuysanexpensivepolicy.\ng. Thereisanagentwhosellspolicies onlytopeoplewhoarenotinsured. 320 Chapter 8. First-OrderLogic\nX Y 00 Ad 0 Z 0\nX Y 11 Ad 1 Z 1 X 3 X 2 X 1 X 0\n+ Y 3 Y 2 Y 1 Y 0\nX Y 22 Ad 2 Z 2 Z 4 Z 3 Z 2 Z 1 Z 0\nX Z\nY 33 Ad 3 Z3 4\nFigure8.8 Afour-bitadder.EachAdi isaone-bitadder,asinFigure8.6onpage309.\nh. Thereisabarberwhoshavesallmenintownwhodonotshavethemselves.\ni. Aperson born in the UK, each of whose parents is aUK citizen oraUK resident, is a\nUKcitizenbybirth.\nj. A person born outside the UK, one of whose parents is a UK citizen by birth, is a UK\ncitizenbydescent.\nk. Politicianscanfoolsomeofthepeopleallofthetime,andtheycanfoolallofthepeople\nsomeofthetime,buttheycan\u2019tfoolallofthepeople allofthetime.\nl. All Greeks speak the same language. (Use Speaks(x,l) to mean that person x speaks\nlanguagel.)\n8.25 Write a general set of facts and axioms to represent the assertion \u201cWellington heard\nabout Napoleon\u2019s death\u201d and to correctly answer the question \u201cDid Napoleon hear about\nWellington\u2019s death?\u201d\n8.26 Extend the vocabulary from Section 8.4 to define addition for n-bit binary numbers.\nThenencode the description ofthefour-bit adderin Figure8.8, and pose thequeries needed\ntoverifythatitisinfactcorrect.\n8.27 Obtain a passport application for your country, identify the rules determining eligi-\nbility for a passport, and translate them into first-order logic, following the steps outlined in\nSection8.4.\n8.28 Considerafirst-orderlogicalknowledgebasethatdescribesworldscontainingpeople,\nsongs,albums(e.g.,\u201cMeettheBeatles\u201d)anddisks(i.e.,particularphysicalinstancesofCDs).\nThevocabulary contains thefollowingsymbols:\nCopyOf(d,a): Predicate. Diskdisacopyofalbuma.\nOwns(p,d): Predicate. Personpownsdiskd.\nSings(p,s,a): Albumaincludesarecording ofsong ssungbyperson p.\nWrote(p,s): Personpwrotesongs.\nMcCartney,Gershwin,BHoliday,Joe,EleanorRigby,TheManILove,Revolver:\nConstantswiththeobviousmeanings. Exercises 321\nExpressthefollowingstatements infirst-orderlogic:\na. Gershwinwrote\u201cTheManILove.\u201d\nb. Gershwindidnotwrite\u201cEleanorRigby.\u201d\nc. EitherGershwinorMcCartneywrote\u201cTheManILove.\u201d\nd. Joehaswrittenatleastonesong.\ne. JoeownsacopyofRevolver.\nf. EverysongthatMcCartneysingsonRevolverwaswrittenbyMcCartney.\ng. GershwindidnotwriteanyofthesongsonRevolver.\nh. EverysongthatGershwinwrotehasbeenrecorded onsomealbum. (Possiblydifferent\nsongsarerecorded ondifferent albums.)\ni. Thereisasinglealbumthatcontains everysongthatJoehas written.\nj. Joeownsacopyofanalbum thathasBillieHolidaysinging \u201cTheManILove.\u201d\nk. Joe owns a copy of every album that has a song sung by McCartney. (Of course, each\ndifferentalbumisinstantiated inadifferentphysical CD.)\nl. JoeownsacopyofeveryalbumonwhichallthesongsaresungbyBillieHoliday. 9\nINFERENCE IN\nFIRST-ORDER LOGIC\nIn which we define effective procedures for answering questions posed in first-\norderlogic.\nChapter7showedhowsoundandcompleteinferencecanbeachievedforpropositionallogic.\nIn this chapter, we extend those results to obtain algorithms that can answer any answer-\nablequestion statedinfirst-orderlogic. Section9.1introduces inference rulesforquantifiers\nandshowshowtoreducefirst-orderinference topropositional inference, albeitatpotentially\ngreat expense. Section 9.2 describes the idea of unification, showing how it can be used\nto construct inference rules that work directly with first-order sentences. We then discuss\nthree major families of first-order inference algorithms. Forward chaining and its applica-\ntionstodeductivedatabasesandproductionsystemsarecoveredinSection9.3;backward\nchainingandlogic programmingsystems aredeveloped inSection 9.4. Forwardandback-\nward chaining can be very efficient, but are applicable only to knowledge bases that can\nbe expressed as sets of Horn clauses. General first-order sentences require resolution-based\ntheoremproving,whichisdescribed inSection9.5.\n9.1 PROPOSITIONAL VS. FIRST-ORDER INFERENCE\nThis section and the next introduce the ideas underlying modern logical inference systems.\nWebegin with some simple inference rules that can be applied to sentences with quantifiers\ntoobtain sentences without quantifiers. These rules lead naturally tothe idea that first-order\ninference can be done by converting the knowledge base to propositional logic and using\npropositional inference, which we already know how to do. The next section points out an\nobviousshortcut, leadingtoinference methodsthatmanipulate first-ordersentences directly.\n9.1.1 Inference rules forquantifiers\nLet us begin with universal quantifiers. Suppose our knowledge base contains the standard\nfolkloric axiomstatingthatallgreedykingsareevil:\n\u2200x King(x)\u2227Greedy(x) \u21d2 Evil(x).\n322 Section9.1. Propositional vs.First-OrderInference 323\nThenitseemsquitepermissible toinferanyofthefollowing sentences:\nKing(John)\u2227Greedy(John) \u21d2 Evil(John)\nKing(Richard)\u2227Greedy(Richard) \u21d2 Evil(Richard)\nKing(Father(John))\u2227Greedy(Father(John)) \u21d2 Evil(Father(John)).\n.\n.\n.\nUNIVERSAL The rule of Universal Instantiation (UI for short) says that we can infer any sentence ob-\nINSTANTIATION\ntained by substituting a ground term (a term without variables) for the variable.1 To write\nGROUNDTERM\nouttheinference ruleformally, weusethenotion of substitutionsintroduced inSection8.3.\nLet SUBST(\u03b8,\u03b1)denote theresult ofapplying thesubstitution \u03b8 tothesentence \u03b1. Thenthe\nruleiswritten\n\u2200v \u03b1\nSUBST({v\/g},\u03b1)\nfor any variable v and ground term g. For example, the three sentences given earlier are\nobtained withthesubstitutions {x\/John},{x\/Richard},and{x\/Father(John)}.\nEXISTENTIAL In the rule for Existential Instantiation, the variable is replaced by a single new con-\nINSTANTIATION\nstantsymbol. Theformalstatementisasfollows: foranysentence \u03b1,variablev,andconstant\nsymbolk thatdoesnotappearelsewhereintheknowledgebase,\n\u2203v \u03b1\n.\nSUBST({v\/k},\u03b1)\nForexample,fromthesentence\n\u2203x Crown(x)\u2227OnHead(x,John)\nwecaninferthesentence\nCrown(C )\u2227OnHead(C ,John)\n1 1\nas long as C does not appear elsewhere in the knowledge base. Basically, the existential\n1\nsentence saysthereissomeobjectsatisfying acondition, andapplying theexistential instan-\ntiation rule just gives a name to that object. Of course, that name must not already belong\ntoanotherobject. Mathematics provides anice example: suppose wediscoverthatthere isa\nnumberthatisalittlebiggerthan2.71828andthatsatisfiestheequationd(xy)\/dy=xy forx.\nWecan give this number a name, such as e, but it would be a mistake to give itthe name of\nan existing object, such as \u03c0. In logic, the new name is called a Skolem constant. Existen-\nSKOLEMCONSTANT\ntialInstantiation isaspecial caseofamoregeneral process called skolemization, which we\ncoverinSection9.5.\nWhereas Universal Instantiation can be applied many times to produce many different\nconsequences, Existential Instantiation can be applied once, and then the existentially quan-\ntified sentence can be discarded. Forexample, weno longer need \u2203x Kill(x,Victim)once\nwehaveaddedthesentence Kill(Murderer,Victim). Strictlyspeaking, thenewknowledge\nINFERENTIAL baseisnotlogically equivalent totheold,butitcanbeshowntobeinferentially equivalent\nEQUIVALENCE\ninthesensethatitissatisfiableexactlywhentheoriginal knowledgebaseissatisfiable.\n1 Donotconfusethesesubstitutionswiththeextendedinterpretationsusedtodefinethesemanticsofquantifiers.\nThe substitution replaces a variable with a term (a piece of syntax) to produce a new sentence, whereas an\ninterpretationmapsavariabletoanobjectinthedomain. 324 Chapter 9. Inference inFirst-OrderLogic\n9.1.2 Reduction to propositional inference\nOnce we have rules for inferring nonquantified sentences from quantified sentences, it be-\ncomes possible to reduce first-order inference to propositional inference. In this section we\ngivethemainideas;thedetailsaregiveninSection9.5.\nThe first idea is that, just as an existentially quantified sentence can be replaced by\none instantiation, a universally quantified sentence can be replaced by the set of all possible\ninstantiations. Forexample, supposeourknowledgebasecontainsjustthesentences\n\u2200x King(x)\u2227Greedy(x) \u21d2 Evil(x)\nKing(John)\n(9.1)\nGreedy(John)\nBrother(Richard,John).\nThen weapply UI to the first sentence using all possible ground-term substitutions from the\nvocabulary oftheknowledge base\u2014inthiscase, {x\/John}and{x\/Richard}. Weobtain\nKing(John)\u2227Greedy(John) \u21d2 Evil(John)\nKing(Richard)\u2227Greedy(Richard) \u21d2 Evil(Richard),\nand we discard the universally quantified sentence. Now, the knowledge base is essentially\npropositional if we view the ground atomic sentences\u2014King(John), Greedy(John), and\nso on\u2014as proposition symbols. Therefore, we can apply any of the complete propositional\nalgorithms inChapter7toobtainconclusions suchas Evil(John).\nThis technique of propositionalization can be made completely general, as we show\nin Section 9.5; that is, every first-order knowledge base and query can be propositionalized\nin such a way that entailment is preserved. Thus, we have a complete decision procedure\nfor entailment ... or perhaps not. There is a problem: when the knowledge base includes\na function symbol, the set of possible ground-term substitutions is infinite! For example, if\nthe knowledge base mentions the Father symbol, then infinitely many nested terms such as\nFather(Father(Father(John)))canbeconstructed. Ourpropositional algorithms willhave\ndifficultywithaninfinitelylargesetofsentences.\nFortunately, there is a famous theorem due to Jacques Herbrand (1930) to the effect\nthatifasentence isentailed bytheoriginal, first-order knowledge base, thenthere isaproof\ninvolving justafinite subset ofthepropositionalized knowledge base. Sinceanysuch subset\nhas a maximum depth of nesting among its ground terms, we can find the subset by first\ngeneratingalltheinstantiations withconstantsymbols(Richard andJohn),thenalltermsof\ndepth1(Father(Richard)andFather(John)),thenalltermsofdepth2,andsoon,untilwe\nareabletoconstruct apropositional proofoftheentailedsentence.\nWe have sketched an approach to first-order inference via propositionalization that is\ncomplete\u2014that is, any entailed sentence can be proved. This is a major achievement, given\nthat the space of possible models is infinite. On the other hand, we do not know until the\nproof isdone that thesentence isentailed! Whathappens whenthesentence isnot entailed?\nCanwetell? Well, forfirst-order logic, itturns outthat wecannot. Ourproof procedure can\ngoonandon, generating moreandmoredeeply nested terms, butwewillnotknowwhether\nitisstuckinahopeless looporwhethertheproofisjustabout topopout. Thisisverymuch Section9.2. UnificationandLifting 325\nlikethehaltingproblemforTuringmachines. AlanTuring(1936)andAlonzoChurch(1936)\nbothproved, inratherdifferent ways, theinevitability of thisstateofaffairs. Thequestion of\nentailmentforfirst-orderlogicissemidecidable\u2014thatis,algorithmsexistthatsayyestoevery\nentailed sentence, butnoalgorithm existsthatalsosaysnotoeverynonentailed sentence.\n9.2 UNIFICATION AND LIFTING\nThe preceding section described the understanding of first-order inference that existed up\nto the early 1960s. The sharp-eyed reader (and certainly the computational logicians of the\nearly1960s)willhavenoticedthatthepropositionalization approachisratherinefficient. For\nexample, given the query Evil(x) and the knowledge base in Equation (9.1), it seems per-\nversetogenerate sentences suchasKing(Richard)\u2227Greedy(Richard) \u21d2 Evil(Richard).\nIndeed, theinference ofEvil(John)fromthesentences\n\u2200x King(x)\u2227Greedy(x) \u21d2 Evil(x)\nKing(John)\nGreedy(John)\nseems completely obvious to a human being. We now show how to make it completely\nobvioustoacomputer.\n9.2.1 A first-order inference rule\nTheinferencethatJohnisevil\u2014thatis,that{x\/John}solvesthequeryEvil(x)\u2014workslike\nthis: to use the rule that greedy kings are evil, find some x such that x is a king and x is\ngreedy, and then infer that this x is evil. More generally, if there is some substitution \u03b8 that\nmakes each of the conjuncts of the premise of the implication identical to sentences already\nintheknowledgebase,thenwecanasserttheconclusion oftheimplication, afterapplying \u03b8.\nInthiscase,thesubstitution \u03b8={x\/John}achievesthataim.\nWe can actually make the inference step do even more work. Suppose that instead of\nknowingGreedy(John),weknowthateveryoneisgreedy:\n\u2200y Greedy(y). (9.2)\nThen we would still like to be able to conclude that Evil(John), because we know that\nJohn is a king (given) and John is greedy (because everyone is greedy). What we need for\nthis to work is to find a substitution both for the variables in the implication sentence and\nfor the variables in the sentences that are in the knowledge base. In this case, applying the\nsubstitution{x\/John,y\/John}totheimplicationpremisesKing(x)andGreedy(x)andthe\nknowledge-base sentences King(John) and Greedy(y) will make them identical. Thus, we\ncaninfertheconclusion oftheimplication.\nThis inference process can be captured as a single inference rule that we call Gener-\nGENERALIZED alized Modus Ponens:2 For atomic sentences p , p (cid:2) , and q, where there is a substitution \u03b8\nMODUSPONENS i i 326 Chapter 9. Inference inFirst-OrderLogic\n(cid:2)\nsuchthat SUBST(\u03b8,p\ni\n)=SUBST(\u03b8,p i),foralli,\np (cid:2) , p (cid:2) , ..., p (cid:2) , (p \u2227p \u2227...\u2227p \u21d2 q)\n1 2 n 1 2 n\n.\nSUBST(\u03b8,q)\n(cid:2)\nTherearen+1premisestothisrule: thenatomicsentencesp andtheoneimplication. The\ni\nconclusion istheresultofapplying thesubstitution \u03b8 totheconsequent q. Forourexample:\n(cid:2)\np isKing(John) p isKing(x)\n1 1\n(cid:2)\np isGreedy(y) p isGreedy(x)\n2 2\n\u03b8 is{x\/John,y\/John} q isEvil(x)\nSUBST(\u03b8,q)isEvil(John).\nItiseasytoshowthatGeneralizedModusPonensisasoundinferencerule. First,weobserve\nthat, for any sentence p (whose variables are assumed to be universally quantified) and for\nanysubstitution \u03b8,\np |= SUBST(\u03b8,p)\nholds by Universal Instantiation. It holds in particular for a \u03b8 that satisfies the conditions of\n(cid:2) (cid:2)\ntheGeneralized ModusPonensrule. Thus,from p ,...,p wecaninfer\n1 n\nSUBST(\u03b8,p 1(cid:2) )\u2227...\u2227SUBST(\u03b8,p n(cid:2) )\nandfromtheimplication p \u2227...\u2227p \u21d2 q wecaninfer\n1 n\nSUBST(\u03b8,p 1)\u2227...\u2227SUBST(\u03b8,p n) \u21d2 SUBST(\u03b8,q).\n(cid:2)\nNow, \u03b8 in Generalized Modus Ponens is defined so that SUBST(\u03b8,p\ni\n)=SUBST(\u03b8,p i), for\nall i; therefore the first of these two sentences matches the premise of the second exactly.\nHence, SUBST(\u03b8,q)followsbyModusPonens.\nGeneralized Modus Ponens is a lifted version of Modus Ponens\u2014it raises Modus Po-\nLIFTING\nnens from ground (variable-free) propositional logic to first-order logic. We will see in the\nrest of this chapter that we can develop lifted versions of the forward chaining, backward\nchaining, and resolution algorithms introduced in Chapter 7. The key advantage of lifted\ninference rules over propositionalization is that they make only those substitutions that are\nrequired toallowparticularinferences toproceed.\n9.2.2 Unification\nLifted inference rules require finding substitutions that make different logical expressions\nlook identical. This process is called unification and is a key component of all first-order\nUNIFICATION\nUNIFIER\ninference algorithms. The UNIFY algorithm takes two sentences and returns a unifier for\nthemifoneexists:\nUNIFY(p,q)=\u03b8 where SUBST(\u03b8,p)=SUBST(\u03b8,q).\nLet us look at some examples of how UNIFY should behave. Suppose we have a query\nAskVars(Knows(John,x)): whom does John know? Answers to this query can be found\n2 GeneralizedModusPonensismoregeneralthanModusPonens(page249)inthesensethattheknownfacts\nandthepremiseoftheimplicationneedmatchonlyuptoasubstitution,ratherthanexactly. Ontheotherhand,\nModusPonensallowsanysentence\u03b1asthepremise,ratherthanjustaconjunctionofatomicsentences. Section9.2. UnificationandLifting 327\nbyfindingallsentencesintheknowledgebasethatunifywithKnows(John,x). Herearethe\nresultsofunificationwithfourdifferentsentences thatmightbeintheknowledgebase:\nUNIFY(Knows(John,x), Knows(John,Jane)) = {x\/Jane}\nUNIFY(Knows(John,x), Knows(y,Bill)) = {x\/Bill,y\/John}\nUNIFY(Knows(John,x), Knows(y,Mother(y))) = {y\/John,x\/Mother(John)}\nUNIFY(Knows(John,x), Knows(x,Elizabeth)) = fail .\nThe last unification fails because x cannot take on the values John and Elizabeth at the\nsame time. Now, remember that Knows(x,Elizabeth) means \u201cEveryone knows Elizabeth,\u201d\nso we should be able to infer that John knows Elizabeth. The problem arises only because\nthe two sentences happen to use the same variable name, x. The problem can be avoided\nSTANDARDIZING by standardizing apart one of the two sentences being unified, which means renaming its\nAPART\nvariables to avoid name clashes. Forexample, we can rename xin Knows(x,Elizabeth) to\nx (anewvariablename)withoutchanging itsmeaning. Nowtheunification willwork:\n17\nUNIFY(Knows(John,x), Knows(x 17,Elizabeth)) = {x\/Elizabeth,x 17\/John}.\nExercise9.12delvesfurtherintotheneedforstandardizing apart.\nThere is one more complication: we said that UNIFY should return a substitution\nthat makes the two arguments look the same. But there could be more than one such uni-\nfier. For example, UNIFY(Knows(John,x),Knows(y,z)) could return {y\/John,x\/z} or\n{y\/John,x\/John,z\/John}. The first unifier gives Knows(John,z) as the result of unifi-\ncation, whereas thesecond gives Knows(John,John). Thesecond result could beobtained\nfrom the first by an additional substitution {z\/John}; we say that the first unifier is more\ngeneralthanthesecond, because itplaces fewerrestrictions onthe valuesofthevariables. It\nMOSTGENERAL turnsoutthat,foreveryunifiablepairofexpressions,thereisasinglemostgeneralunifier(or\nUNIFIER\nMGU)thatisunique uptorenaming andsubstitution ofvariables. (Forexample, {x\/John}\nand{y\/John}areconsideredequivalent,asare{x\/John,y\/John}and{x\/John,y\/x}.) In\nthiscaseitis{y\/John,x\/z}.\nAnalgorithm forcomputing mostgeneral unifiers isshownin Figure 9.1. Theprocess\nissimple: recursivelyexplorethetwoexpressions simultaneously \u201csidebyside,\u201dbuilding up\naunifieralongtheway,butfailingiftwocorresponding pointsinthestructuresdonotmatch.\nThere is one expensive step: when matching a variable against a complex term, one must\ncheckwhetherthevariableitselfoccursinsidetheterm;if itdoes,thematchfailsbecauseno\nconsistent unifiercanbeconstructed. Forexample, S(x)can\u2019t unify withS(S(x)). Thisso-\ncalled occur checkmakes thecomplexity oftheentire algorithm quadratic inthesize ofthe\nOCCURCHECK\nexpressions being unified. Some systems, including all logic programming systems, simply\nomittheoccurcheck andsometimes makeunsound inferences asaresult; othersystems use\nmorecomplexalgorithms withlinear-time complexity.\n9.2.3 Storageand retrieval\nUnderlying the TELL and ASK functions used to inform and interrogate a knowledge base\nare the more primitive STORE and FETCH functions. STORE(s) stores a sentence s into the\nknowledge base and FETCH(q) returns all unifiers such that the query q unifies with some 328 Chapter 9. Inference inFirst-OrderLogic\nfunctionUNIFY(x,y,\u03b8)returnsasubstitutiontomakex andy identical\ninputs:x,avariable,constant,list,orcompoundexpression\ny,avariable,constant,list,orcompoundexpression\n\u03b8,thesubstitutionbuiltupsofar(optional,defaultstoempty)\nif\u03b8=failurethenreturnfailure\nelseifx =y thenreturn\u03b8\nelseifVARIABLE?(x)thenreturnUNIFY-VAR(x,y,\u03b8)\nelseifVARIABLE?(y)thenreturnUNIFY-VAR(y,x,\u03b8)\nelseifCOMPOUND?(x)andCOMPOUND?(y)then\nreturnUNIFY(x.ARGS,y.ARGS,UNIFY(x.OP,y.OP,\u03b8))\nelseifLIST?(x)andLIST?(y)then\nreturnUNIFY(x.REST,y.REST,UNIFY(x.FIRST,y.FIRST,\u03b8))\nelsereturnfailure\nfunctionUNIFY-VAR(var,x,\u03b8)returnsasubstitution\nif{var\/val} \u2208 \u03b8thenreturnUNIFY(val,x,\u03b8)\nelseif{x\/val} \u2208 \u03b8thenreturnUNIFY(var,val,\u03b8)\nelseifOCCUR-CHECK?(var,x)thenreturnfailure\nelsereturnadd{var\/x}to\u03b8\nFigure9.1 Theunificationalgorithm. Thealgorithmworksbycomparing thestructures\noftheinputs,elementbyelement. Thesubstitution\u03b8 thatistheargumenttoUNIFY isbuilt\nupalongthewayandisusedtomakesurethatlatercomparisonsareconsistentwithbindings\nthatwereestablishedearlier. InacompoundexpressionsuchasF(A,B),theOPfieldpicks\noutthefunctionsymbolF andtheARGSfieldpicksouttheargumentlist(A,B).\nsentence in the knowledge base. The problem we used to illustrate unification\u2014finding all\nfactsthatunifywithKnows(John,x)\u2014isaninstanceof FETCHing.\nThe simplest way to implement STORE and FETCH is to keep all the facts in one long\nlist and unify each query against every element of the list. Such a process is inefficient, but\nit works, and it\u2019s all you need to understand the rest of the chapter. The remainder of this\nsectionoutlines waystomakeretrievalmoreefficient;itcanbeskippedonfirstreading.\nWe can make FETCH more efficient by ensuring that unifications are attempted only\nwith sentences that have some chance of unifying. For example, there is no point in trying\ntounifyKnows(John,x)withBrother(Richard,John). Wecanavoidsuchunificationsby\nindexing the facts in the knowledge base. A simple scheme called predicate indexing puts\nINDEXING\nPREDICATE all the Knows facts in one bucket and all the Brother facts in another. The buckets can be\nINDEXING\nstoredinahashtableforefficientaccess.\nPredicate indexing is useful when there are many predicate symbols but only a few\nclauses for each symbol. Sometimes, however, a predicate has many clauses. For example,\nsuppose that the tax authorities want to keep track of who employs whom, using a predi-\ncate Employs(x,y). This would be a very large bucket with perhaps millions of employers Section9.2. UnificationandLifting 329\nEmploys(x,y) Employs(x,y)\nEmploys(x,Richard) Employs(IBM,y) Employs(x,John) Employs(x,x) Employs(John,y)\nEmploys(IBM,Richard) Employs(John,John)\n(a) (b)\nFigure9.2 (a)ThesubsumptionlatticewhoselowestnodeisEmploys(IBM,Richard).\n(b)ThesubsumptionlatticeforthesentenceEmploys(John,John).\nand tens of millions of employees. Answering a query such as Employs(x,Richard) with\npredicate indexingwouldrequirescanning theentirebucket.\nForthis particular query, it would help if facts were indexed both by predicate and by\nsecondargument, perhaps usingacombined hashtablekey. Thenwecouldsimplyconstruct\nthe key from the query and retrieve exactly those facts that unify with the query. For other\nqueries, such as Employs(IBM,y), we would need to have indexed the facts by combining\nthe predicate with the first argument. Therefore, facts can be stored under multiple index\nkeys,rendering theminstantly accessible tovariousqueriesthattheymightunifywith.\nGivenasentence tobestored, itispossible toconstruct indices forallpossible queries\nthatunifywithit. ForthefactEmploys(IBM,Richard),thequeriesare\nEmploys(IBM,Richard) DoesIBMemployRichard?\nEmploys(x,Richard) WhoemploysRichard?\nEmploys(IBM,y) WhomdoesIBMemploy?\nEmploys(x,y) Whoemployswhom?\nSUBSUMPTION These queries form a subsumption lattice, as shown in Figure 9.2(a). The lattice has some\nLATTICE\ninteresting properties. For example, the child of any node in the lattice is obtained from its\nparent by a single substitution; and the \u201chighest\u201d common descendant of any two nodes is\nthe result of applying their mostgeneral unifier. Theportion ofthe lattice above anyground\nfactcanbeconstructedsystematically (Exercise9.5). Asentencewithrepeatedconstantshas\na slightly different lattice, as shown in Figure 9.2(b). Function symbols and variables in the\nsentences tobestoredintroduce stillmoreinteresting latticestructures.\nThe scheme we have described works very well whenever the lattice contains a small\nnumber of nodes. For a predicate with n arguments, however, the lattice contains O(2n)\nnodes. If function symbols are allowed, the number of nodes is also exponential in the size\nofthetermsinthesentencetobestored. Thiscanleadtoahugenumberofindices. Atsome\npoint, the benefits of indexing are outweighed by the costs of storing and maintaining all\nthe indices. Wecan respond by adopting afixedpolicy, such as maintaining indices only on\nkeyscomposedofapredicatepluseachargument,orbyusinganadaptivepolicythatcreates\nindices to meet the demands of the kinds of queries being asked. For most AI systems, the\nnumber of facts to be stored is small enough that efficient indexing is considered a solved\nproblem. For commercial databases, where facts number in the billions, the problem has\nbeenthesubjectofintensivestudyandtechnology development.. 330 Chapter 9. Inference inFirst-OrderLogic\n9.3 FORWARD CHAINING\nA forward-chaining algorithm for propositional definite clauses was given in Section 7.5.\nThe idea is simple: start with the atomic sentences in the knowledge base and apply Modus\nPonens in the forward direction, adding new atomic sentences, until no further inferences\ncan be made. Here, we explain how the algorithm is applied to first-order definite clauses.\nDefiniteclausessuchasSituation \u21d2 Response areespeciallyusefulforsystemsthatmake\ninferences in response to newlyarrived information. Manysystems can bedefined this way,\nandforwardchainingcanbeimplementedveryefficiently.\n9.3.1 First-order definite clauses\nFirst-order definite clauses closely resemble propositional definite clauses (page 256): they\naredisjunctions ofliterals ofwhich exactly oneispositive. Adefiniteclause eitherisatomic\nor is an implication whose antecedent is a conjunction of positive literals and whose conse-\nquentisasinglepositiveliteral. Thefollowingarefirst-orderdefiniteclauses:\nKing(x)\u2227Greedy(x) \u21d2 Evil(x).\nKing(John).\nGreedy(y).\nUnlike propositional literals, first-order literals can include variables, in which case those\nvariables are assumed to be universally quantified. (Typically, we omit universal quantifiers\nwhen writing definite clauses.) Not every knowledge base can be converted into a set of\ndefinite clauses because of the single-positive-literal restriction, but many can. Consider the\nfollowingproblem:\nThe law says thatit is a crime foran American to sell weaponsto hostile nations. The\ncountryNono,anenemyofAmerica,hassomemissiles,andallofitsmissilesweresold\ntoitbyColonelWest,whoisAmerican.\nWewillprovethatWestisacriminal. First,wewillrepresentthesefactsasfirst-orderdefinite\nclauses. Thenextsection showshowtheforward-chaining algorithm solvestheproblem.\n\u201c...itisacrimeforanAmericantosellweaponstohostilenations\u201d:\nAmerican(x)\u2227Weapon(y)\u2227Sells(x,y,z)\u2227Hostile(z) \u21d2 Criminal(x). (9.3)\n\u201cNono...hassomemissiles.\u201d Thesentence\u2203xOwns(Nono,x)\u2227Missile(x)istransformed\nintotwodefiniteclausesbyExistentialInstantiation, introducing anewconstant M :\n1\nOwns(Nono,M ) (9.4)\n1\nM. issile(M ) (9.5)\n1\n\u201cAllofitsmissilesweresoldtoitbyColonelWest\u201d:\nMissile(x)\u2227Owns(Nono,x) \u21d2 Sells(West,x,Nono). (9.6)\nWewillalsoneedtoknowthatmissilesareweapons:\nMissile(x) \u21d2 Weapon(x) (9.7) Section9.3. ForwardChaining 331\nandwemustknowthatanenemyofAmericacountsas\u201chostile\u201d:\nEnemy(x,America) \u21d2 Hostile(x). (9.8)\n\u201cWest,whoisAmerican...\u201d:\nAmerican(West). (9.9)\n\u201cThecountry Nono,anenemyofAmerica ...\u201d:\nEnemy(Nono,America). (9.10)\nThis knowledge base contains no function symbols and is therefore an instance of the class\nof Datalog knowledge bases. Datalog is a language that is restricted to first-order definite\nDATALOG\nclauses withnofunction symbols. Datalog getsitsnamebecause itcanrepresent thetypeof\nstatements typically made in relational databases. We will see that the absence of function\nsymbolsmakesinference mucheasier.\n9.3.2 A simpleforward-chaining algorithm\nThefirstforward-chaining algorithmweconsiderisasimple one,showninFigure9.3. Start-\ning from the known facts, it triggers all the rules whose premises are satisfied, adding their\nconclusions to the known facts. The process repeats until the query is answered (assuming\nthat just one answer is required) or no new facts are added. Notice that a fact is not \u201cnew\u201d\nif it is just a renaming of a known fact. One sentence is a renaming of another if they\nRENAMING\nare identical except for the names of the variables. For example, Likes(x,IceCream) and\nLikes(y,IceCream) arerenamings ofeach otherbecause theydiffer only in thechoice of x\nory;theirmeaningsareidentical: everyonelikesicecream.\nWe use our crime problem to illustrate how FOL-FC-ASK works. The implication\nsentences are(9.3),(9.6),(9.7),and(9.8). Twoiterations arerequired:\n\u2022 Onthefirstiteration, rule(9.3)hasunsatisfiedpremises.\nRule(9.6)issatisfiedwith{x\/M },andSells(West,M ,Nono)isadded.\n1 1\nRule(9.7)issatisfiedwith{x\/M },andWeapon(M )isadded.\n1 1\nRule(9.8)issatisfiedwith{x\/Nono},andHostile(Nono)isadded.\n\u2022 On the second iteration, rule (9.3) is satisfied with {x\/West,y\/M ,z\/Nono}, and\n1\nCriminal(West)isadded.\nFigure 9.4shows theproof treethat isgenerated. Notice that nonewinferences arepossible\nat this point because every sentence that could be concluded by forward chaining is already\ncontainedexplicitlyintheKB.Suchaknowledgebaseiscalledafixedpointoftheinference\nprocess. Fixedpointsreachedbyforwardchainingwithfirst-orderdefiniteclausesaresimilar\ntothoseforpropositional forwardchaining (page258);the principal difference isthatafirst-\norderfixedpointcaninclude universally quantifiedatomicsentences.\nFOL-FC-ASK iseasy to analyze. First, itis sound,because every inference is just an\napplicationofGeneralizedModusPonens,whichissound. Second,itiscompletefordefinite\nclause knowledge bases; that is, it answers every query whose answers are entailed by any\nknowledgebaseofdefiniteclauses. ForDatalogknowledgebases,whichcontainnofunction\nsymbols, the proof of completeness is fairly easy. We begin by counting the number of 332 Chapter 9. Inference inFirst-OrderLogic\nfunctionFOL-FC-ASK(KB,\u03b1)returnsasubstitutionorfalse\ninputs:KB,theknowledgebase,asetoffirst-orderdefiniteclauses\n\u03b1,thequery,anatomicsentence\nlocalvariables: new,thenewsentencesinferredoneachiteration\nrepeatuntilnew isempty\nnew\u2190{}\nforeachrule inKB do\n(p 1\u2227...\u2227 pn \u21d2 q)\u2190STANDARDIZE-VARIABLES(rule)\nforeach\u03b8suchthatSUBST(\u03b8,p\n1\n\u2227 ... \u2227 pn)=SUBST(\u03b8,p 1(cid:5) \u2227 ... \u2227 p n(cid:5))\nforsomep(cid:5),...,p(cid:5) inKB\n1 n\nq(cid:5)\u2190SUBST(\u03b8,q)\nifq(cid:5) doesnotunifywithsomesentencealreadyinKB ornew then\naddq(cid:5)tonew\n\u03c6\u2190UNIFY(q(cid:5),\u03b1)\nif\u03c6isnotfail thenreturn\u03c6\naddnew toKB\nreturnfalse\nFigure 9.3 A conceptually straightforward, but very inefficient, forward-chainingalgo-\nrithm. Oneachiteration,itaddsto KB alltheatomicsentencesthatcanbeinferredinone\nstep fromthe implicationsentencesandtheatomicsentencesalreadyin KB. Thefunction\nSTANDARDIZE-VARIABLES replacesallvariablesinitsargumentswithnewonesthathave\nnotbeenusedbefore.\nCriminal(West)\nWeapon(M) Sells(West,M,Nono) Hostile(Nono)\n1 1\nAmerican(West) Missile(M) Owns(Nono,M) Enemy(Nono,America)\n1 1\nFigure9.4 Theprooftreegeneratedbyforwardchainingonthecrimeexample.Theinitial\nfactsappearatthe bottomlevel, factsinferredon the firstiterationin the middlelevel, and\nfactsinferredontheseconditerationatthetoplevel.\npossible factsthatcanbeadded, whichdetermines themaximumnumberofiterations. Letk\nbethemaximumarity(numberofarguments)ofanypredicate, pbethenumberofpredicates,\nand n be the number of constant symbols. Clearly, there can be no more than pnk distinct\ngroundfacts,soafterthismanyiterationsthealgorithmmusthavereachedafixedpoint. Then\nwecanmakeanargumentverysimilartotheproofofcompletenessforpropositionalforward Section9.3. ForwardChaining 333\nchaining. (See page 258.) The details of how to make the transition from propositional to\nfirst-ordercompleteness aregivenfortheresolution algorithm inSection9.5.\nFor general definite clauses with function symbols, FOL-FC-ASK can generate in-\nfinitely many new facts, so we need to be more careful. Forthe case in which an answer to\nthe query sentence q is entailed, we must appeal to Herbrand\u2019s theorem to establish that the\nalgorithm will find a proof. (See Section 9.5 for the resolution case.) If the query has no\nanswer, the algorithm could fail to terminate in some cases. For example, if the knowledge\nbaseincludes thePeanoaxioms\nNatNum(0)\n\u2200n NatNum(n) \u21d2 NatNum(S(n)),\nthen forward chaining adds NatNum(S(0)), NatNum(S(S(0))), NatNum(S(S(S(0)))),\nand so on. This problem is unavoidable in general. As with general first-order logic, entail-\nmentwithdefiniteclauses issemidecidable.\n9.3.3 Efficient forwardchaining\nThe forward-chaining algorithm in Figure 9.3 is designed for ease of understanding rather\nthan for efficiency of operation. There are three possible sources of inefficiency. First, the\n\u201cinner loop\u201d of the algorithm involves finding all possible unifiers such that the premise of\narule unifies with asuitable set offacts in the knowledge base. This is often called pattern\nmatching and can be very expensive. Second, the algorithm rechecks every rule on every\nPATTERNMATCHING\niteration to see whether its premises are satisfied, even ifvery few additions are made to the\nknowledge base on each iteration. Finally, the algorithm might generate many facts that are\nirrelevant tothegoal. Weaddresseachoftheseissuesinturn.\nMatchingrulesagainstknownfacts\nTheproblemofmatchingthepremiseofaruleagainstthefactsintheknowledgebasemight\nseemsimpleenough. Forexample,suppose wewanttoapplytherule\nMissile(x) \u21d2 Weapon(x).\nThenweneedtofindallthefactsthatunifywithMissile(x);inasuitablyindexedknowledge\nbase,thiscanbedoneinconstant timeperfact. Nowconsider arulesuchas\nMissile(x)\u2227Owns(Nono,x) \u21d2 Sells(West,x,Nono).\nAgain,wecanfindalltheobjects ownedbyNonoinconstant timeperobject; then, foreach\nobject, wecould check whether it isa missile. If the knowledge base contains manyobjects\nownedbyNonoandveryfewmissiles,however,itwouldbebettertofindallthemissilesfirst\nCONJUNCT and then check whether they are owned by Nono. This is the conjunct ordering problem:\nORDERING\nfindanorderingtosolvetheconjuncts oftherulepremisesothatthetotalcostisminimized.\nIt turns out that finding the optimal ordering is NP-hard, but good heuristics are available.\nForexample, the minimum-remaining-values (MRV)heuristic used forCSPsin Chapter6\nwould suggest ordering the conjuncts to look formissiles first if fewer missiles than objects\nareownedbyNono. 334 Chapter 9. Inference inFirst-OrderLogic\nNT Diff(wa,nt)\u2227Diff(wa,sa)\u2227\nQ\nDiff(nt,q)\u2227Diff(nt,sa)\u2227\nWA\nDiff(q,nsw)\u2227Diff(q,sa)\u2227\nDiff(nsw,v)\u2227Diff(nsw,sa)\u2227\nSA NSW\nDiff(v,sa) \u21d2 Colorable()\nV Diff(Red,Blue) Diff(Red,Green)\nDiff(Green,Red)Diff(Green,Blue)\nDiff(Blue,Red) Diff(Blue,Green)\nT\n(a) (b)\nFigure9.5 (a)ConstraintgraphforcoloringthemapofAustralia. (b)Themap-coloring\nCSPexpressedasasingledefiniteclause.Eachmapregionisrepresentedasavariablewhose\nvaluecanbeoneoftheconstantsRed,Green orBlue.\nThe connection between pattern matching and constraint satisfaction is actually very\nclose. We can view each conjunct as a constraint on the variables that it contains\u2014for ex-\nample, Missile(x) is a unary constraint on x. Extending this idea, we can express every\nfinite-domain CSP as a single definite clause together with some associated ground facts.\nConsiderthemap-coloringproblemfromFigure6.1,shownagaininFigure9.5(a). Anequiv-\nalentformulation asasingledefiniteclauseisgiveninFigure9.5(b). Clearly, theconclusion\nColorable()canbeinferred onlyiftheCSPhasasolution. BecauseCSPsingeneralinclude\n3-SAT problems as special cases, wecan conclude that matching a definite clause against a\nsetoffactsisNP-hard.\nItmightseemratherdepressingthatforwardchaininghasanNP-hardmatchingproblem\ninitsinnerloop. Therearethreewaystocheerourselvesup:\n\u2022 We can remind ourselves that most rules in real-world knowledge bases are small and\nsimple (like the rules in our crime example) rather than large and complex (like the\nCSP formulation in Figure 9.5). It is common in the database world to assume that\nboth the sizes of rules and the arities of predicates are bounded by a constant and to\nworry only about data complexity\u2014that is, the complexity of inference as a function\nDATACOMPLEXITY\nof the number of ground facts in the knowledge base. It is easy to show that the data\ncomplexityofforwardchaining ispolynomial.\n\u2022 We can consider subclasses of rules for which matching is efficient. Essentially every\nDatalog clause can be viewed as defining a CSP, so matching will be tractable just\nwhenthecorresponding CSPistractable. Chapter6describesseveraltractablefamilies\nof CSPs. For example, if the constraint graph (the graph whose nodes are variables\nand whose links are constraints) forms a tree, then the CSP can be solved in linear\ntime. Exactlythesameresultholdsforrulematching. Forinstance,ifweremoveSouth Section9.3. ForwardChaining 335\nAustraliafromthemapinFigure9.5,theresultingclauseis\nDiff(wa,nt)\u2227Diff(nt,q)\u2227Diff(q,nsw)\u2227Diff(nsw,v) \u21d2 Colorable()\nwhichcorresponds tothe reduced CSPshown inFigure 6.12 onpage 224. Algorithms\nforsolvingtree-structuredCSPscanbeapplieddirectlytotheproblemofrulematching.\n\u2022 We can try to to eliminate redundant rule-matching attempts in the forward-chaining\nalgorithm, asdescribed next.\nIncrementalforwardchaining\nWhenweshowedhow forward chaining worksonthecrimeexample, wecheated; inpartic-\nular, we omitted some of the rule matching done by the algorithm shown in Figure 9.3. For\nexample,ontheseconditeration, therule\nMissile(x) \u21d2 Weapon(x)\nmatchesagainstMissile(M )(again), andofcourse theconclusion Weapon(M )isalready\n1 1\nknown so nothing happens. Such redundant rule matching can be avoided if we make the\nfollowing observation: Every new fact inferred on iteration t must be derived from at least\none new fact inferred on iteration t \u2212 1. This is true because any inference that does not\nrequireanewfactfromiteration t\u22121couldhavebeendoneatiteration t\u22121already.\nThis observation leads naturally to an incremental forward-chaining algorithm where,\natiterationt,wecheckaruleonlyifitspremiseincludesaconjunct p thatunifieswithafact\ni\np(cid:2) newlyinferredatiteration t\u22121. Therule-matching stepthenfixesp tomatchwithp(cid:2) ,but\ni i i\nallows the other conjuncts of the rule to match with facts from any previous iteration. This\nalgorithmgeneratesexactlythesamefactsateachiterationasthealgorithminFigure9.3,but\nismuchmoreefficient.\nWith suitable indexing, it is easy to identify all the rules that can be triggered by any\ngivenfact,andindeedmanyrealsystemsoperateinan\u201cupdate\u201dmodewhereinforwardchain-\ning occurs in response to each new fact that is TELLed to the system. Inferences cascade\nthrough thesetofrulesuntilthefixedpointisreached, andthentheprocess beginsagainfor\nthenextnewfact.\nTypically,onlyasmallfractionoftherulesintheknowledgebaseareactuallytriggered\nby the addition of a given fact. This means that a great deal of redundant work is done in\nrepeatedly constructing partial matches that have some unsatisfied premises. Our crime ex-\nampleisrathertoosmalltoshowthiseffectively,butnoticethatapartialmatchisconstructed\nonthefirstiterationbetweentherule\nAmerican(x)\u2227Weapon(y)\u2227Sells(x,y,z)\u2227Hostile(z) \u21d2 Criminal(x)\nandthefactAmerican(West). Thispartialmatchisthendiscardedandrebuiltonthesecond\niteration (when the rule succeeds). It would be better to retain and gradually complete the\npartialmatchesasnewfactsarrive,ratherthandiscarding them.\nThe rete algorithm3 was the first to address this problem. The algorithm preprocesses\nRETE\nthe set of rules in the knowledge base to construct a sort of dataflow network in which each\n3 ReteisLatinfornet.TheEnglishpronunciationrhymeswithtreaty. 336 Chapter 9. Inference inFirst-OrderLogic\nnode is a literal from a rule premise. Variable bindings flow through the network and are\nfiltered out when they fail to match a literal. If two literals in a rule share a variable\u2014for\nexample, Sells(x,y,z) \u2227 Hostile(z) in the crime example\u2014then the bindings from each\nliteral are filtered through an equality node. A variable binding reaching a node for an n-\nary literal such as Sells(x,y,z) might have towait forbindings forthe other variables tobe\nestablished before the process can continue. At any given point, the state of a rete network\ncaptures allthepartialmatchesoftherules,avoiding agreatdealofrecomputation.\nRete networks, and various improvements thereon, have been a key component of so-\nPRODUCTION called production systems, which were among the earliest forward-chaining systems in\nSYSTEM\nwidespread use.4 The XCON system (originally called R1; McDermott, 1982) was built\nwithaproduction-system architecture. XCONcontainedseveralthousand rulesfordesigning\nconfigurationsofcomputercomponentsforcustomersoftheDigitalEquipmentCorporation.\nIt was one of the first clear commercial successes in the emerging field of expert systems.\nManyothersimilarsystemshavebeenbuiltwiththesameunderlying technology, whichhas\nbeenimplemented inthegeneral-purpose language OPS-5.\nCOGNITIVE Productionsystemsarealsopopularincognitivearchitectures\u2014that is,modelsofhu-\nARCHITECTURES\nmanreasoning\u2014such asACT (Anderson, 1983)and SOAR (Lairdetal.,1987). Insuchsys-\ntems, the \u201cworking memory\u201d of the system models human short-term memory, and the pro-\nductionsarepartoflong-termmemory. Oneachcycleofoperation, productions arematched\nagainsttheworkingmemoryoffacts. Aproduction whoseconditions aresatisfiedcanaddor\ndelete facts in working memory. In contrast to the typical situation in databases, production\nsystems often have many rules and relatively few facts. With suitably optimized matching\ntechnology, somemodernsystemscanoperateinrealtimewithtensofmillionsofrules.\nIrrelevant facts\nThe final source of inefficiency in forward chaining appears to be intrinsic to the approach\nandalsoarisesinthepropositional context. Forwardchainingmakesallallowableinferences\nbasedontheknownfacts,eveniftheyareirrelevanttothegoalathand. Inourcrimeexample,\ntherewerenorulescapableofdrawingirrelevantconclusions, sothelackofdirectednesswas\nnotaproblem. Inothercases(e.g.,ifmanyrulesdescribetheeatinghabitsofAmericansand\nthepricesofmissiles), FOL-FC-ASK willgenerate manyirrelevant conclusions.\nOne way to avoid drawing irrelevant conclusions is to use backward chaining, as de-\nscribedinSection9.4. Anothersolutionistorestrictforwardchainingtoaselectedsubsetof\nrules, as in PL-FC-ENTAILS? (page 258). A third approach has emerged in the field of de-\nDEDUCTIVE ductive databases, which are large-scale databases, like relational databases, but which use\nDATABASES\nforwardchainingasthestandardinferencetoolratherthan SQLqueries. Theideaistorewrite\nthe rule set, using information from the goal, so that only relevant variable bindings\u2014those\nbelongingtoaso-calledmagicset\u2014areconsideredduringforwardinference. Forexample,if\nMAGICSET\nthegoalisCriminal(West),therulethatconcludesCriminal(x)willberewrittentoinclude\nanextraconjunctthatconstrains thevalueof x:\nMagic(x)\u2227American(x)\u2227Weapon(y)\u2227Sells(x,y,z)\u2227Hostile(z) \u21d2 Criminal(x).\n4 Thewordproductioninproductionsystemsdenotesacondition\u2013actionrule. Section9.4. BackwardChaining 337\nThe fact Magic(West) is also added to the KB. In this way, even if the knowledge base\ncontains factsaboutmillionsofAmericans, onlyColonelWestwillbeconsidered during the\nforward inference process. The complete process for defining magic sets and rewriting the\nknowledge base is too complex to go into here, but the basic idea is to perform a sort of\n\u201cgeneric\u201d backward inference from the goal in order to work out which variable bindings\nneed to be constrained. The magic sets approach can therefore be thought of as a kind of\nhybridbetweenforwardinference andbackwardpreprocessing.\n9.4 BACKWARD CHAINING\nThe second major family of logical inference algorithms uses the backward chaining ap-\nproach introduced inSection7.5fordefiniteclauses. These algorithms workbackward from\nthe goal, chaining through rules to find known facts that support the proof. We describe\nthebasicalgorithm, andthenwedescribehowitisusedinlogicprogramming,whichisthe\nmostwidelyusedformofautomatedreasoning. Wealsoseethatbackwardchaininghassome\ndisadvantages compared withforward chaining, and welook atwaysto overcome them. Fi-\nnally,welookatthecloseconnectionbetweenlogicprogrammingandconstraintsatisfaction\nproblems.\n9.4.1 A backward-chaining algorithm\nFigure 9.6 shows a backward-chaining algorithm for definite clauses. FOL-BC-ASK(KB,\ngoal)willbe proved iftheknowledge base contains aclause of the form lhs \u21d2 goal, where\nlhs (left-handside)isalistofconjuncts. AnatomicfactlikeAmerican(West)isconsidered\nasaclause whoselhs istheemptylist. Nowaquery thatcontains variables mightbeproved\nin multiple ways. For example, the query Person(x) could be proved with the substitution\nGENERATOR\n{x\/John}aswellaswith{x\/Richard}. SoweimplementFOL-BC-ASK asagenerator\u2014\nafunction thatreturnsmultipletimes,eachtimegivingone possible result.\nBackward chaining is a kind of AND\/OR search\u2014the OR part because the goal query\ncanbeprovedbyanyruleintheknowledgebase,andthe ANDpartbecausealltheconjuncts\ninthelhs ofaclausemustbeproved. FOL-BC-OR worksbyfetchingallclauses thatmight\nunify with the goal, standardizing the variables in the clause to be brand-new variables, and\nthen, if the rhs of the clause does indeed unify with the goal, proving every conjunct in the\nlhs, using FOL-BC-AND. That function in turn works by proving each of the conjuncts in\nturn, keeping track oftheaccumulated substitution aswego. Figure 9.7isthe proof tree for\nderiving Criminal(West)fromsentences (9.3)through(9.10).\nBackward chaining, as we have written it, is clearly a depth-first search algorithm.\nThis means that its space requirements are linear in the size of the proof (neglecting, for\nnow, the space required to accumulate the solutions). It also means that backward chaining\n(unlikeforwardchaining)suffersfromproblemswithrepeatedstatesandincompleteness. We\nwill discuss these problems and some potential solutions, but first we show how backward\nchaining isusedinlogicprogrammingsystems. 338 Chapter 9. Inference inFirst-OrderLogic\nfunctionFOL-BC-ASK(KB,query)returnsageneratorofsubstitutions\nreturnFOL-BC-OR(KB,query,{})\ngeneratorFOL-BC-OR(KB,goal,\u03b8)yieldsasubstitution\nforeachrule(lhs \u21d2 rhs)inFETCH-RULES-FOR-GOAL(KB,goal)do\n(lhs,rhs)\u2190STANDARDIZE-VARIABLES((lhs,rhs))\nforeach\u03b8(cid:5)inFOL-BC-AND(KB,lhs,UNIFY(rhs,goal,\u03b8))do\nyield\u03b8(cid:5)\ngeneratorFOL-BC-AND(KB,goals,\u03b8)yieldsasubstitution\nif\u03b8 = failure thenreturn\nelseifLENGTH(goals)=0thenyield\u03b8\nelsedo\nfirst,rest\u2190FIRST(goals),REST(goals)\nforeach\u03b8(cid:5)inFOL-BC-OR(KB, SUBST(\u03b8,first),\u03b8)do\nforeach\u03b8(cid:5)(cid:5)inFOL-BC-AND(KB,rest,\u03b8(cid:5))do\nyield\u03b8(cid:5)(cid:5)\nFigure9.6 Asimplebackward-chainingalgorithmforfirst-orderknowledgebases.\nCriminal(West)\nAmerican(West) Weapon(y) Sells(West,M ,z) Hostile(Nono)\n1\n{ } {z\/Nono}\nMissile(y) Missile(M ) Owns(Nono,M ) Enemy(Nono,America)\n1 1\n{y\/M 1} { } { } { }\nFigure9.7 ProoftreeconstructedbybackwardchainingtoprovethatWestisacriminal.\nThetreeshouldbereaddepthfirst,lefttoright.ToproveCriminal(West),wehavetoprove\nthe four conjuncts below it. Some of these are in the knowledge base, and others require\nfurtherbackward chaining. Bindings for each successful unification are shown next to the\ncorrespondingsubgoal.Notethatonceonesubgoalinaconjunctionsucceeds,itssubstitution\nisappliedtosubsequentsubgoals.Thus,bythetimeFOL-BC-ASKgetstothelastconjunct,\noriginallyHostile(z),zisalreadyboundtoNono. Section9.4. BackwardChaining 339\n9.4.2 Logicprogramming\nLogic programming is a technology that comes fairly close to embodying the declarative\nidealdescribedinChapter7: thatsystemsshouldbeconstructed byexpressingknowledgein\naformallanguageandthatproblemsshouldbesolvedbyrunninginferenceprocessesonthat\nknowledge. TheidealissummedupinRobertKowalski\u2019sequation,\nAlgorithm = Logic +Control .\nProlog isthemostwidelyused logicprogramming language. Itisused primarily asarapid-\nPROLOG\nprototypinglanguageandforsymbol-manipulationtaskssuchaswritingcompilers(VanRoy,\n1990) and parsing natural language (Pereira and Warren, 1980). Many expert systems have\nbeenwritteninPrologforlegal,medical, financial,andotherdomains.\nProlog programs are sets of definite clauses written in a notation somewhat different\nfrom standard first-order logic. Prologuses uppercase letters forvariables and lowercase for\nconstants\u2014the opposite ofourconvention forlogic. Commasseparate conjuncts inaclause,\nand theclause iswritten\u201cbackwards\u201d from whatweareused to; instead ofA\u2227B \u21d2 C in\nPrologwehaveC :- A, B.Hereisatypicalexample:\ncriminal(X) :- american(X), weapon(Y), sells(X,Y,Z), hostile(Z).\nThe notation [E|L] denotes a list whose first element is E and whose rest is L. Here is a\nProlog program for append(X,Y,Z),which succeeds if list Z is the result of appending\nlistsXandY:\nappend([],Y,Y).\nappend([A|X],Y,[A|Z]) :- append(X,Y,Z).\nIn English, we can read these clauses as (1) appending an empty list with a list Y produces\nthe same list Y and (2) [A|Z]is the result of appending [A|X] onto Y, provided that Z is\nthe result ofappending Xonto Y.Inmost high-level languages wecan writeasimilar recur-\nsive function that describes how to append two lists. The Prolog definition is actually much\nmore powerful, however, because it describes a relation that holds among three arguments,\nrather than a function computed from two arguments. For example, we can ask the query\nappend(X,Y,[1,2]): whattwo lists can be appended to give [1,2]? Weget back the\nsolutions\nX=[] Y=[1,2];\nX=[1] Y=[2];\nX=[1,2] Y=[]\nThe execution of Prolog programs is done through depth-first backward chaining, where\nclauses are tried inthe order inwhich theyare written in the knowledge base. Someaspects\nofPrologfalloutsidestandard logicalinference:\n\u2022 Prolog uses the database semantics of Section 8.2.8 rather than first-order semantics,\nandthisisapparent initstreatmentofequality andnegation(seeSection9.4.5).\n\u2022 Thereisasetofbuilt-in functions forarithmetic. Literalsusingthesefunction symbols\nare \u201cproved\u201d by executing code rather than doing further inference. For example, the 340 Chapter 9. Inference inFirst-OrderLogic\ngoal\u201cXis4+3\u201dsucceedswithXboundto7. Ontheotherhand,thegoal\u201c5isX+Y\u201d\nfails,becausethebuilt-in functions donotdoarbitrary equationsolving.5\n\u2022 Therearebuilt-inpredicatesthathavesideeffectswhenexecuted. Theseincludeinput\u2013\noutput predicates and the assert\/retractpredicates formodifying theknowledge\nbase. Suchpredicateshavenocounterpartinlogicandcanproduceconfusingresults\u2014\nforexample,iffactsareassertedinabranchoftheprooftreethateventually fails.\n\u2022 TheoccurcheckisomittedfromProlog\u2019sunificationalgorithm. Thismeansthatsome\nunsoundinferences canbemade;thesearealmostneveraprobleminpractice.\n\u2022 Prologusesdepth-first backward-chaining search withnochecks forinfiniterecursion.\nThismakes itvery fast when given theright setof axioms, but incomplete when given\nthewrongones.\nProlog\u2019sdesignrepresentsacompromisebetweendeclarativenessandexecutionefficiency\u2014\ninasmuchasefficiencywasunderstood atthetimePrologwasdesigned.\n9.4.3 Efficient implementationoflogicprograms\nThe execution of a Prolog program can happen in two modes: interpreted and compiled.\nInterpretation essentially amounts to running the FOL-BC-ASK algorithm from Figure 9.6,\nwith the program as the knowledge base. We say \u201cessentially\u201d because Prolog interpreters\ncontainavarietyofimprovementsdesigned tomaximizespeed. Hereweconsideronlytwo.\nFirst, our implementation had to explicitly manage the iteration over possible results\ngenerated by each of the subfunctions. Prolog interpreters have a global data structure,\na stack of choice points, to keep track of the multiple possibilities that we considered in\nCHOICEPOINT\nFOL-BC-OR. This global stack is more efficient, and it makes debugging easier, because\nthedebuggercanmoveupanddownthestack.\nSecond,oursimpleimplementationofFOL-BC-ASKspendsagooddealoftimegener-\natingsubstitutions. Insteadofexplicitlyconstructingsubstitutions, Prologhaslogicvariables\nthat remember their current binding. At any point in time, every variable in the program ei-\nther is unbound or is bound to some value. Together, these variables and values implicitly\ndefine the substitution for the current branch of the proof. Extending the path can only add\nnew variable bindings, because an attempt to add a different binding for an already bound\nvariable results in a failure of unification. When a path in the search fails, Prolog will back\nuptoaprevious choice point, andthen itmight have tounbind somevariables. Thisisdone\nbykeepingtrackofallthevariables thathavebeenboundinastackcalledthetrail. Aseach\nTRAIL\nnewvariableisboundbyUNIFY-VAR,thevariableispushedontothetrail. Whenagoalfails\nand it is time to back up to a previous choice point, each of the variables is unbound as it is\nremovedfromthetrail.\nEven the most efficient Prolog interpreters require several thousand machine instruc-\ntions per inference step because of the cost of index lookup, unification, and building the\nrecursive call stack. In effect, the interpreter always behaves as if it has never seen the pro-\ngram before; for example, it has to find clauses that match the goal. A compiled Prolog\n5 NotethatifthePeanoaxiomsareprovided,suchgoalscanbesolvedbyinferencewithinaPrologprogram. Section9.4. BackwardChaining 341\nprocedureAPPEND(ax,y,az,continuation)\ntrail\u2190GLOBAL-TRAIL-POINTER()\nifax =[]andUNIFY(y,az)thenCALL(continuation)\nRESET-TRAIL(trail)\na,x,z\u2190NEW-VARIABLE(),NEW-VARIABLE(),NEW-VARIABLE()\nifUNIFY(ax,[a |x])andUNIFY(az,[a |z])thenAPPEND(x,y,z,continuation)\nFigure9.8 Pseudocoderepresentingtheresultofcompilingthe Appendpredicate. The\nfunctionNEW-VARIABLEreturnsanewvariable,distinctfromallothervariablesusedsofar.\nTheprocedureCALL(continuation)continuesexecutionwiththespecifiedcontinuation.\nprogram,ontheotherhand,isaninferenceprocedureforaspecificsetofclauses,soitknows\nwhatclauses matchthegoal. Prologbasically generates aminiature theorem proverforeach\ndifferentpredicate, therebyeliminatingmuchoftheoverheadofinterpretation. Itisalsopos-\nsible to open-code the unification routine for each different call, thereby avoiding explicit\nOPEN-CODE\nanalysis oftermstructure. (Fordetailsofopen-coded unification, seeWarren etal.(1977).)\nThe instruction sets of today\u2019s computers give a poor match with Prolog\u2019s semantics,\nsomostPrologcompilerscompileintoanintermediatelanguageratherthandirectlyintoma-\nchine language. The most popular intermediate language is the Warren Abstract Machine,\norWAM,named afterDavidH.D.Warren, oneoftheimplementers ofthefirstPrologcom-\npiler. The WAM is an abstract instruction set that is suitable for Prolog and can be either\ninterpretedortranslatedintomachinelanguage. OthercompilerstranslatePrologintoahigh-\nlevellanguagesuchasLisporCandthenusethatlanguage\u2019scompilertotranslatetomachine\nlanguage. Forexample,thedefinitionoftheAppendpredicatecanbecompiledintothecode\nshowninFigure9.8. Severalpointsareworthmentioning:\n\u2022 Rather than having to search the knowledge base for Appendclauses, the clauses be-\ncomeaprocedure andtheinferences arecarriedoutsimplyby callingtheprocedure.\n\u2022 Asdescribedearlier,thecurrentvariablebindingsarekeptonatrail. Thefirststepofthe\nproceduresavesthecurrentstateofthetrail,sothatitcan berestoredbyRESET-TRAIL\nifthefirstclausefails. ThiswillundoanybindingsgeneratedbythefirstcalltoUNIFY.\n\u2022 Thetrickiestpartistheuseofcontinuationstoimplementchoicepoints. Youcanthink\nCONTINUATION\nof a continuation as packaging up a procedure and a list of arguments that together\ndefine what should be done next whenever the current goal succeeds. It would not\ndo just to return from a procedure like APPEND when the goal succeeds, because it\ncould succeed in several ways, and each of them has to be explored. Thecontinuation\nargument solves this problem because it can be called each time the goal succeeds. In\nthe APPEND code, ifthe firstargument is empty and the second argument unifies with\nthe third, then the APPEND predicate has succeeded. We then CALL the continuation,\nwith the appropriate bindings on the trail, to do whatever should be done next. For\nexample, if the call to APPEND were at the top level, the continuation would print the\nbindingsofthevariables. 342 Chapter 9. Inference inFirst-OrderLogic\nBefore Warren\u2019s work on the compilation of inference in Prolog, logic programming was\ntoo slow for general use. Compilers by Warren and others allowed Prolog code to achieve\nspeeds that are competitive with C on a variety of standard benchmarks (Van Roy, 1990).\nOf course, the fact that one can write a planner or natural language parser in a few dozen\nlinesofPrologmakesitsomewhatmoredesirablethanCforprototypingmostsmall-scaleAI\nresearch projects.\nParallelizationcanalsoprovidesubstantialspeedup. Therearetwoprincipalsourcesof\nparallelism. The first, called OR-parallelism, comes from the possibility of agoal unifying\nOR-PARALLELISM\nwithmanydifferentclausesintheknowledgebase. Eachgivesrisetoanindependent branch\nin the search space that can lead to a potential solution, and all such branches can be solved\nin parallel. The second, called AND-parallelism, comes from the possibility of solving\nAND-PARALLELISM\neach conjunct inthebody ofanimplication inparallel. AND-parallelism ismoredifficult to\nachieve, because solutions for the whole conjunction require consistent bindings for all the\nvariables. Each conjunctive branch must communicate with the other branches to ensure a\nglobalsolution.\n9.4.4 Redundant inference andinfinite loops\nWe now turn to the Achilles heel of Prolog: the mismatch between depth-first search and\nsearch trees thatinclude repeated states andinfinite paths. Considerthefollowing logicpro-\ngramthatdecidesifapathexistsbetweentwopointsonadirectedgraph:\npath(X,Z) :- link(X,Z).\npath(X,Z) :- path(X,Y), link(Y,Z).\nAsimple three-node graph, described bythefacts link(a,b)and link(b,c),isshown\nin Figure 9.9(a). With this program, the query path(a,c)generates the proof tree shown\ninFigure9.10(a). Ontheotherhand,ifweputthetwoclauses intheorder\npath(X,Z) :- path(X,Y), link(Y,Z).\npath(X,Z) :- link(X,Z).\nthenPrologfollowstheinfinitepathshowninFigure9.10(b). Prologistherefore incomplete\nasatheoremproverfordefiniteclauses\u2014evenforDatalogprograms,asthisexampleshows\u2014\nbecause, for some knowledge bases, it fails to prove sentences that are entailed. Notice that\nforward chaining does not suffer from this problem: once path(a,b),path(b,c),and\npath(a,c)areinferred, forwardchaining halts.\nDepth-first backward chaining also has problems with redundant computations. For\nexample,whenfindingapathfromA toJ inFigure9.9(b),Prologperforms877inferences,\n1 4\nmostofwhichinvolvefindingallpossiblepathstonodesfromwhichthegoalisunreachable.\nThis is similar to the repeated-state problem discussed in Chapter 3. The total amount of\ninference can be exponential in the number of ground facts that are generated. If we apply\nforward chaining instead, at most n2 path(X,Y)facts can be generated linking n nodes.\nFortheproblem inFigure9.9(b),only62inferences areneeded.\nDYNAMIC Forwardchainingongraphsearchproblemsisanexampleof dynamicprogramming,\nPROGRAMMING\nin which the solutions to subproblems are constructed incrementally from those of smaller Section9.4. BackwardChaining 343\nA\n1\nA B C\nJ\n4\n(a) (b)\nFigure 9.9 (a) Finding a path from A to C can lead Prolog into an infinite loop. (b) A\ngraphinwhicheachnodeisconnectedtotworandomsuccessorsinthenextlayer.Findinga\npathfromA toJ requires877inferences.\n1 4\npath(a,c)\npath(a,c)\npath(a,Y) link(Y,c)\nlink(a,c) path(a,Y) link(b,c)\nfail { }\npath(a,Y\u2019) link(Y\u2019,Y)\nlink(a,Y)\n{Y \/b}\n(a) (b)\nFigure 9.10 (a) Proof that a path exists from A to C. (b) Infinite proof tree generated\nwhentheclausesareinthe\u201cwrong\u201dorder.\nsubproblems and are cached to avoid recomputation. We can obtain a similar effect in a\nbackward chaining system using memoization\u2014that is, caching solutions to subgoals as\nthey are found and then reusing those solutions when the subgoal recurs, rather than repeat-\nTABLEDLOGIC ingthepreviouscomputation. Thisistheapproachtakenbytabledlogicprogrammingsys-\nPROGRAMMING\ntems, which use efficient storage and retrieval mechanisms to perform memoization. Tabled\nlogicprogramming combines thegoal-directedness ofbackward chaining withthedynamic-\nprogramming efficiency of forward chaining. It is also complete for Datalog knowledge\nbases, whichmeansthattheprogrammerneedworrylessabout infiniteloops. (Itisstillpos-\nsible to get an infinite loop with predicates like father(X,Y) that refer to a potentially\nunbounded numberofobjects.)\n9.4.5 Databasesemantics ofProlog\nPrologusesdatabasesemantics,asdiscussedinSection8.2.8. Theuniquenamesassumption\nsays that every Prolog constant and every ground term refers to a distinct object, and the\nclosed worldassumption says that the only sentences thatare true arethose that areentailed 344 Chapter 9. Inference inFirst-OrderLogic\nbytheknowledgebase. ThereisnowaytoassertthatasentenceisfalseinProlog. Thismakes\nProloglessexpressivethanfirst-orderlogic,butitispart ofwhatmakesPrologmoreefficient\nandmoreconcise. ConsiderthefollowingPrologassertions aboutsomecourseofferings:\nCourse(CS,101), Course(CS,102), Course(CS,106), Course(EE,101). (9.11)\nUnder the unique names assumption, CS and EE are different (as are 101, 102, and 106),\nso this means that there are four distinct courses. Under the closed-world assumption there\nare no other courses, so there are exactly four courses. But if these were assertions in FOL\nrather than in Prolog, then all we could say is that there are somewhere between one and\ninfinitycourses. That\u2019sbecause theassertions (inFOL)donotdenythepossibility thatother\nunmentionedcoursesarealsooffered,nordotheysaythatthecoursesmentionedaredifferent\nfromeachother. Ifwewantedtotranslate Equation(9.11)intoFOL,wewouldgetthis:\nCourse(d,n) \u21d4 (d=CS \u2227n = 101)\u2228(d=CS \u2227n = 102)\n\u2228(d=CS \u2227n = 106)\u2228(d=EE \u2227n = 101). (9.12)\nThisis called the completion ofEquation (9.11). Itexpresses inFOLthe idea that there are\nCOMPLETION\natmostfourcourses. Toexpress inFOLtheideathatthereare atleastfourcourses, weneed\ntowritethecompletion oftheequality predicate:\nx= y \u21d4 (x = CS \u2227y = CS)\u2228(x= EE \u2227y = EE)\u2228(x = 101\u2227y = 101)\n\u2228(x= 102\u2227y = 102)\u2228(x = 106\u2227y = 106).\nThecompletion isusefulforunderstanding database semantics, butforpractical purposes, if\nyour problem can be described with database semantics, it is more efficient to reason with\nProlog or some other database semantics system, rather than translating into FOL and rea-\nsoningwithafullFOLtheorem prover.\n9.4.6 Constraintlogicprogramming\nIn our discussion of forward chaining (Section 9.3), we showed how constraint satisfaction\nproblems (CSPs) can be encoded as definite clauses. Standard Prolog solves such problems\ninexactlythesamewayasthebacktracking algorithm giveninFigure6.5.\nBecausebacktracking enumeratesthedomainsofthevariables,itworksonlyforfinite-\ndomain CSPs. In Prolog terms, there must be a finite number of solutions for any goal\nwithunbound variables. (Forexample, thegoal diff(Q,SA),whichsaysthat Queensland\nand South Australia must be different colors, has six solutions if three colors are allowed.)\nInfinite-domainCSPs\u2014forexample,withintegerorreal-valuedvariables\u2014require quitedif-\nferentalgorithms, suchasboundspropagation orlinearprogramming.\nConsider the following example. We define triangle(X,Y,Z)as a predicate that\nholdsifthethreearguments arenumbersthatsatisfythetriangleinequality:\ntriangle(X,Y,Z) :-\nX>0, Y>0, Z>0, X+Y>=Z, Y+Z>=X, X+Z>=Y.\nIf we ask Prolog the query triangle(3,4,5), it succeeds. On the other hand, if we\nask triangle(3,4,Z),no solution will be found, because the subgoal Z>=0 cannot be\nhandled byProlog;wecan\u2019tcompareanunbound valueto0. Section9.5. Resolution 345\nCONSTRAINTLOGIC Constraint logic programming (CLP) allows variables to be constrained rather than\nPROGRAMMING\nbound. ACLPsolution isthemostspecific setofconstraints onthe query variables that can\nbederivedfromtheknowledge base. Forexample,thesolution tothetriangle(3,4,Z)\nquery is the constraint 7 >= Z >= 1. Standard logic programs are just a special case of\nCLPinwhichthesolutionconstraints mustbeequality constraints\u2014that is,bindings.\nCLP systems incorporate various constraint-solving algorithms for the constraints al-\nlowed in the language. For example, a system that allows linear inequalities on real-valued\nvariables might include a linear programming algorithm for solving those constraints. CLP\nsystems also adopt a much more flexible approach to solving standard logic programming\nqueries. Forexample, instead ofdepth-first, left-to-right backtracking, theymightuseanyof\nthe more efficient algorithms discussed in Chapter 6, including heuristic conjunct ordering,\nbackjumping, cutset conditioning, and so on. CLP systems therefore combine elements of\nconstraint satisfaction algorithms, logicprogramming, anddeductive databases.\nSeveral systems that allow the programmer more control over the search order for in-\nferencehavebeendefined. TheMRS language(Genesereth andSmith,1981;Russell,1985)\nallows the programmer to write metarules to determine which conjuncts are tried first. The\nMETARULE\nuser could write a rule saying that the goal with the fewest variables should be tried first or\ncouldwritedomain-specific rulesforparticularpredicates.\n9.5 RESOLUTION\nThelastofourthreefamiliesoflogicalsystemsisbasedonresolution. Wesawonpage250\nthat propositional resolution using refutation is a complete inference procedure for proposi-\ntionallogic. Inthissection, wedescribehowtoextendresolution tofirst-orderlogic.\n9.5.1 Conjunctive normal form forfirst-order logic\nAs in the propositional case, first-order resolution requires that sentences be in conjunctive\nnormalform(CNF)\u2014thatis,aconjunction ofclauses, whereeachclauseisadisjunction of\nliterals.6 Literals can contain variables, which are assumed tobe universally quantified. For\nexample,thesentence\n\u2200x American(x)\u2227Weapon(y)\u2227Sells(x,y,z)\u2227Hostile(z) \u21d2 Criminal(x)\nbecomes, inCNF,\n\u00acAmerican(x)\u2228\u00acWeapon(y)\u2228\u00acSells(x,y,z)\u2228\u00acHostile(z)\u2228Criminal(x).\nEvery sentence of first-order logic can be converted into an inferentially equivalent CNF\nsentence. Inparticular,theCNFsentencewillbeunsatisfiablejustwhentheoriginalsentence\nisunsatisfiable, sowehaveabasisfordoingproofsbycontradiction ontheCNFsentences.\n6 Aclausecanalsoberepresentedasanimplicationwithaconjunctionofatomsinthepremiseandadisjunction\nofatomsintheconclusion(Exercise7.13).ThisiscalledimplicativenormalformorKowalskiform(especially\nwhenwrittenwitharight-to-leftimplicationsymbol(Kowalski,1979))andisoftenmucheasiertoread. 346 Chapter 9. Inference inFirst-OrderLogic\nTheprocedureforconversiontoCNFissimilartothepropositionalcase,whichwesaw\nonpage253. Theprincipaldifferencearisesfromtheneedtoeliminateexistentialquantifiers.\nWe illustrate the procedure by translating the sentence \u201cEveryone who loves all animals is\nlovedbysomeone,\u201dor\n\u2200x [\u2200y Animal(y) \u21d2 Loves(x,y)] \u21d2 [\u2203y Loves(y,x)].\nThestepsareasfollows:\n\u2022 Eliminateimplications:\n\u2200x [\u00ac\u2200y \u00acAnimal(y)\u2228Loves(x,y)]\u2228[\u2203y Loves(y,x)].\n\u2022 Move\u00acinwards: Inaddition totheusual rulesfornegatedconnectives, weneedrules\nfornegatedquantifiers. Thus,wehave\n\u00ac\u2200x p becomes \u2203x \u00acp\n\u00ac\u2203x p becomes \u2200x \u00acp.\nOursentencegoesthroughthefollowingtransformations:\n\u2200x [\u2203y \u00ac(\u00acAnimal(y)\u2228Loves(x,y))]\u2228[\u2203y Loves(y,x)].\n\u2200x [\u2203y \u00ac\u00acAnimal(y)\u2227\u00acLoves(x,y)]\u2228[\u2203y Loves(y,x)].\n\u2200x [\u2203y Animal(y)\u2227\u00acLoves(x,y)]\u2228[\u2203y Loves(y,x)].\nNotice how a universal quantifier (\u2200y) in the premise of the implication has become\nan existential quantifier. The sentence now reads \u201cEither there is some animal that x\ndoesn\u2019t love, or (if this is not the case) someone loves x.\u201d Clearly, the meaning of the\noriginalsentence hasbeenpreserved.\n\u2022 Standardizevariables: Forsentenceslike(\u2203xP(x))\u2228(\u2203xQ(x))whichusethesame\nvariable name twice, change the name of one of the variables. This avoids confusion\nlaterwhenwedropthequantifiers. Thus,wehave\n\u2200x [\u2203y Animal(y)\u2227\u00acLoves(x,y)]\u2228[\u2203z Loves(z,x)].\n\u2022 Skolemize: Skolemization is the process of removing existential quantifiers by elimi-\nSKOLEMIZATION\nnation. Inthesimplecase,itisjustliketheExistentialInstantiation ruleofSection9.1:\ntranslate\u2203x P(x)intoP(A),whereAisanewconstant. However,wecan\u2019tapplyEx-\nistential Instantiation tooursentence abovebecause itdoesn\u2019t matchthepattern \u2203v \u03b1;\nonly parts of the sentence match the pattern. If we blindly apply the rule to the two\nmatchingpartsweget\n\u2200x [Animal(A)\u2227\u00acLoves(x,A)]\u2228Loves(B,x),\nwhich has the wrong meaning entirely: it says that everyone either fails to love a par-\nticular animal Aoris loved by some particular entity B. In fact, our original sentence\nallowseachpersontofailtoloveadifferentanimalortobelovedbyadifferentperson.\nThus,wewanttheSkolementitiestodepend onxandz:\n\u2200x [Animal(F(x))\u2227\u00acLoves(x,F(x))]\u2228Loves(G(z),x).\nHere F and G are Skolem functions. The general rule is that the arguments of the\nSKOLEMFUNCTION\nSkolem function are all the universally quantified variables in whose scope the exis-\ntentialquantifierappears. AswithExistential Instantiation, theSkolemized sentence is\nsatisfiableexactlywhentheoriginal sentence issatisfiable. Section9.5. Resolution 347\n\u2022 Dropuniversal quantifiers: Atthis point, all remaining variables mustbe universally\nquantified. Moreover, thesentence isequivalent tooneinwhichalltheuniversal quan-\ntifiershavebeenmovedtotheleft. Wecantherefore droptheuniversal quantifiers:\n[Animal(F(x))\u2227\u00acLoves(x,F(x))]\u2228Loves(G(z),x).\n\u2022 Distribute\u2228over\u2227:\n[Animal(F(x))\u2228Loves(G(z),x)]\u2227[\u00acLoves(x,F(x))\u2228Loves(G(z),x)] .\nThisstepmayalsorequireflatteningoutnestedconjunctions anddisjunctions.\nThe sentence is now in CNF and consists of two clauses. It is quite unreadable. (It may\nhelp to explain that theSkolem function F(x)refers tothe animal potentially unloved by x,\nwhereas G(z) refers to someone who might love x.) Fortunately, humans seldom need look\natCNFsentences\u2014the translation processiseasilyautomated.\n9.5.2 The resolutioninference rule\nTheresolution rule forfirst-order clauses issimply aliftedversion ofthepropositional reso-\nlution rule given on page 253. Two clauses, which are assumed to be standardized apart so\nthat they share no variables, can be resolved if they contain complementary literals. Propo-\nsitional literals are complementary if one is the negation of the other; first-order literals are\ncomplementary ifoneunifieswiththenegationoftheother. Thus,wehave\n(cid:3) \u2228\u00b7\u00b7\u00b7\u2228(cid:3) , m \u2228\u00b7\u00b7\u00b7\u2228m\n1 k 1 n\nSUBST(\u03b8,(cid:3) 1\u2228\u00b7\u00b7\u00b7\u2228(cid:3) i\u22121\u2228(cid:3) i+1\u2228\u00b7\u00b7\u00b7\u2228(cid:3)\nk\n\u2228m 1\u2228\u00b7\u00b7\u00b7\u2228m j\u22121\u2228m j+1\u2228\u00b7\u00b7\u00b7\u2228m n)\nwhere UNIFY((cid:3) i,\u00acm j)=\u03b8. Forexample, wecanresolvethetwoclauses\n[Animal(F(x))\u2228Loves(G(x),x)] and [\u00acLoves(u,v)\u2228\u00acKills(u,v)]\nby eliminating the complementary literals Loves(G(x),x) and \u00acLoves(u,v), with unifier\n\u03b8={u\/G(x),v\/x}, toproducetheresolvent clause\n[Animal(F(x))\u2228\u00acKills(G(x),x)].\nThis rule is called the binary resolution rule because it resolves exactly two literals. The\nBINARYRESOLUTION\nbinary resolution rulebyitself does notyield acomplete inference procedure. Thefullreso-\nlutionruleresolvessubsetsofliteralsineachclausethatareunifiable. Analternativeapproach\nis to extend factoring\u2014the removal of redundant literals\u2014to the first-order case. Proposi-\ntional factoring reduces two literals to one if they are identical; first-order factoring reduces\ntwoliterals toone iftheyare unifiable. Theunifiermustbeapplied tothe entire clause. The\ncombination ofbinaryresolution andfactoring iscomplete.\n9.5.3 Exampleproofs\nResolution proves that KB |= \u03b1 by proving KB \u2227\u00ac\u03b1 unsatisfiable, that is, by deriving the\nempty clause. The algorithmic approach is identical to the propositional case, described in 348 Chapter 9. Inference inFirst-OrderLogic\n\u00acAmerican(x) ^ \u00acWeapon(y) ^ \u00acSells(x,y,z) ^ \u00acHostile(z) ^ Criminal(x) \u00acCriminal(West)\nAmerican(West) \u00acAmerican(West) ^ \u00acWeapon(y) ^ \u00acSells(West,y,z) ^ \u00acHostile(z)\n\u00acMissile(x) ^ Weapon(x) \u00acWeapon(y) ^ \u00acSells(West,y,z) ^ \u00acHostile(z)\nMissile(M1) \u00acMissile(y) ^ \u00acSells(West,y,z) ^ \u00acHostile(z)\n\u00acMissile(x) ^ \u00acOwns(Nono,x) ^ Sells(West,x,Nono) \u00acSells(West,M1,z) ^ \u00acHostile(z)\nMissile(M1) \u00acMissile(M1) ^ \u00acOwns(Nono,M1) ^ \u00acHostile(Nono)\nOwns(Nono,M1) \u00acOwns(Nono,M1) ^ \u00acHostile(Nono)\n\u00acEnemy(x,America) ^ Hostile(x) \u00acHostile(Nono)\nEnemy(Nono,America) \u00acEnemy(Nono,America)\nFigure9.11 AresolutionproofthatWestisacriminal.Ateachstep,theliteralsthatunify\nareinbold.\nFigure 7.12, soweneed notrepeat ithere. Instead, wegive twoexample proofs. Thefirstis\nthecrimeexamplefromSection9.3. Thesentences inCNFare\n\u00acAmerican(x)\u2228\u00acWeapon(y)\u2228\u00acSells(x,y,z)\u2228\u00acHostile(z)\u2228Criminal(x)\n\u00acMissile(x)\u2228\u00acOwns(Nono,x)\u2228Sells(West,x,Nono)\n\u00acEnemy(x,America)\u2228Hostile(x)\n\u00acMissile(x)\u2228Weapon(x)\nOwns(Nono,M ) Missile(M )\n1 1\nAmerican(West) Enemy(Nono,America).\nWealso include the negated goal \u00acCriminal(West). Theresolution proof is shown in Fig-\nure9.11. Noticethestructure: single\u201cspine\u201dbeginningwiththegoalclause,resolvingagainst\nclauses from the knowledge base until the empty clause is generated. This is characteristic\nof resolution on Horn clause knowledge bases. In fact, the clauses along the main spine\ncorrespond exactly to the consecutive values of the goals variable in the backward-chaining\nalgorithm of Figure 9.6. This is because we always choose to resolve with a clause whose\npositive literal unified with the leftmost literal of the \u201ccurrent\u201d clause on the spine; this is\nexactly what happens in backward chaining. Thus, backward chaining is just a special case\nofresolution withaparticularcontrol strategytodecidewhichresolution toperform next.\nOursecond examplemakesuseofSkolemization andinvolves clauses thatarenotdef-\ninite clauses. This results in a somewhat more complex proof structure. In English, the\nproblem isasfollows:\nEveryonewholovesallanimalsislovedbysomeone.\nAnyonewhokillsananimalislovedbynoone.\nJacklovesallanimals.\nEitherJackorCuriositykilledthecat,whoisnamedTuna.\nDidCuriositykillthecat? Section9.5. Resolution 349\nFirst, we express the original sentences, some background knowledge, and the negated goal\nGinfirst-orderlogic:\nA. \u2200x [\u2200y Animal(y) \u21d2 Loves(x,y)] \u21d2 [\u2203y Loves(y,x)]\nB. \u2200x [\u2203z Animal(z)\u2227Kills(x,z)] \u21d2 [\u2200y \u00acLoves(y,x)]\nC. \u2200x Animal(x) \u21d2 Loves(Jack,x)\nD. Kills(Jack,Tuna)\u2228Kills(Curiosity,Tuna)\nE. Cat(Tuna)\nF. \u2200x Cat(x) \u21d2 Animal(x)\n\u00acG. \u00acKills(Curiosity,Tuna)\nNowweapplytheconversion procedure toconverteachsentence toCNF:\nA1. Animal(F(x))\u2228Loves(G(x),x)\nA2. \u00acLoves(x,F(x))\u2228Loves(G(x),x)\nB. \u00acLoves(y,x)\u2228\u00acAnimal(z)\u2228\u00acKills(x,z)\nC. \u00acAnimal(x)\u2228Loves(Jack,x)\nD. Kills(Jack,Tuna)\u2228Kills(Curiosity,Tuna)\nE. Cat(Tuna)\nF. \u00acCat(x)\u2228Animal(x)\n\u00acG. \u00acKills(Curiosity,Tuna)\nTheresolutionproofthatCuriositykilledthecatisgiveninFigure9.12. InEnglish,theproof\ncouldbeparaphrased asfollows:\nSuppose Curiosity did not kill Tuna. We know that either Jack or Curiosity did; thus\nJackmusthave. Now,Tunaisacatandcatsareanimals,soTunaisananimal. Because\nanyonewhokillsananimalislovedbynoone,weknowthatnoonelovesJack. Onthe\nother hand, Jack loves all animals, so someone loves him; so we have a contradiction.\nTherefore,Curiositykilledthecat.\nCat(Tuna) \u00acCat(x) ^ Animal(x) Kills(Jack, Tuna) ^ Kills(Curiosity, Tuna) \u00acKills(Curiosity, Tuna)\nAnimal(Tuna) \u00acLoves(y, x) ^ \u00acAnimal(z) ^ \u00acKills(x, z) Kills(Jack, Tuna) \u00acLoves(x,F(x)) ^ Loves(G(x), x) \u00acAnimal(x) ^ Loves(Jack, x)\n^ ^ ^\n\u00acLoves(y, x) \u00acKills(x, Tuna) \u00acAnimal(F(Jack)) Loves(G(Jack), Jack) Animal(F(x)) Loves(G(x), x)\n\u00acLoves(y, Jack) Loves(G(Jack),Jack)\nFigure9.12 A resolutionproofthat Curiositykilled the cat. Notice the use of factoring\nin the derivation of the clause Loves(G(Jack),Jack). Notice also in the upper right, the\nunificationofLoves(x,F(x))andLoves(Jack,x)canonlysucceedafterthevariableshave\nbeenstandardizedapart. 350 Chapter 9. Inference inFirst-OrderLogic\nTheproofanswersthequestion \u201cDidCuriosity killthecat?\u201d butoftenwewanttoposemore\ngeneral questions, such as \u201cWho killed the cat?\u201d Resolution can do this, but it takes a little\nmore work to obtain the answer. The goal is \u2203w Kills(w,Tuna), which, when negated,\nbecomes\u00acKills(w,Tuna)inCNF.RepeatingtheproofinFigure9.12withthenewnegated\ngoal, we obtain a similar proof tree, but with the substitution {w\/Curiosity} in one of the\nsteps. So, in this case, finding out who killed the cat is just a matter of keeping track of the\nbindings forthequeryvariables intheproof.\nNONCONSTRUCTIVE Unfortunately, resolution can produce nonconstructive proofs for existential goals.\nPROOF\nForexample, \u00acKills(w,Tuna)resolves withKills(Jack,Tuna)\u2228Kills(Curiosity,Tuna)\nto give Kills(Jack,Tuna), which resolves again with \u00acKills(w,Tuna) to yield the empty\nclause. Notice that w has two different bindings in this proof; resolution is telling us that,\nyes, someone killed Tuna\u2014either Jack or Curiosity. This is no great surprise! One so-\nlution is to restrict the allowed resolution steps so that the query variables can be bound\nonly once in a given proof; then we need to be able to backtrack over the possible bind-\nings. Another solution is to add a special answer literal to the negated goal, which be-\nANSWERLITERAL\ncomes \u00acKills(w,Tuna) \u2228 Answer(w). Now, the resolution process generates an answer\nwhenever a clause is generated containing just a single answer literal. Forthe proof in Fig-\nure 9.12, this is Answer(Curiosity). The nonconstructive proof would generate the clause\nAnswer(Curiosity)\u2228Answer(Jack),whichdoesnotconstitute ananswer.\n9.5.4 Completeness ofresolution\nThissection givesacompleteness proof ofresolution. Itcanbesafely skipped bythosewho\narewillingtotakeitonfaith.\nREFUTATION Weshowthatresolution isrefutation-complete, whichmeansthatifasetofsentences\nCOMPLETENESS\nis unsatisfiable, then resolution will always be able to derive a contradiction. Resolution\ncannot be used to generate all logical consequences of a set of sentences, but it can be used\ntoestablish that agivensentence isentailed bythesetofsentences. Hence, itcanbeusedto\nfindallanswerstoagivenquestion, Q(x),byprovingthatKB \u2227\u00acQ(x)isunsatisfiable.\nWetakeitasgiventhatanysentenceinfirst-orderlogic(withoutequality)canberewrit-\nten as a set of clauses in CNF.This can be proved by induction on the form of the sentence,\nusing atomic sentences as the base case (Davis and Putnam, 1960). Our goal therefore is to\nprove the following: if S is an unsatisfiable set of clauses, then the application of a finite\nnumberofresolution stepstoS willyieldacontradiction.\nOur proof sketch follows Robinson\u2019s original proof with some simplifications from\nGenesereth andNilsson(1987). Thebasicstructure oftheproof(Figure9.13)isasfollows:\n1. First, we observe that if S is unsatisfiable, then there exists a particular set of ground\ninstancesoftheclausesofSsuchthatthissetisalsounsatisfiable(Herbrand\u2019stheorem).\n2. WethenappealtothegroundresolutiontheoremgiveninChapter7,whichstatesthat\npropositional resolution iscompleteforgroundsentences.\n3. Wethen usealiftinglemmatoshowthat, foranypropositional resolution proof using\nthe set of ground sentences, there is a corresponding first-order resolution proof using\nthefirst-ordersentences fromwhichthegroundsentences wereobtained. Section9.5. Resolution 351\nAny set of sentences S is representable in clausal form\nAssume S is unsatisfiable, and in clausal form\nHerbrand\u2019s theorem\nSome set S' of ground instances is unsatisfiable\nGround resolution\ntheorem\nResolution can find a contradiction in S'\nLifting lemma\nThere is a resolution proof for the contradiction in S'\nFigure9.13 Structureofacompletenessproofforresolution.\nTocarryoutthefirststep,weneedthreenewconcepts:\nHERBRAND \u2022 Herbrand universe: If S is a set of clauses, then H , the Herbrand universe of S, is\nUNIVERSE S\nthesetofallgroundtermsconstructable fromthefollowing:\na. Thefunction symbolsinS,ifany.\nb. Theconstant symbolsinS,ifany;ifnone,thentheconstant symbolA.\nForexample,ifS containsjusttheclause\u00acP(x,F(x,A))\u2228\u00acQ(x,A)\u2228R(x,B),then\nH isthefollowinginfinitesetofground terms:\nS\n{A,B,F(A,A),F(A,B),F(B,A),F(B,B),F(A,F(A,A)),...}.\n\u2022 Saturation: If S is a set of clauses and P is a set of ground terms, then P(S), the\nSATURATION\nsaturation of S with respect to P, isthe set of allground clauses obtained by applying\nallpossible consistent substitutions ofgroundtermsin P withvariablesinS.\n\u2022 Herbrand base: The saturation of a set S of clauses with respect to its Herbrand uni-\nHERBRANDBASE\nverse is called the Herbrand base of S, written as H (S). For example, if S contains\nS\nsolelytheclausejustgiven,thenH (S)istheinfinitesetofclauses\nS\n{\u00acP(A,F(A,A))\u2228\u00acQ(A,A)\u2228R(A,B),\n\u00acP(B,F(B,A))\u2228\u00acQ(B,A)\u2228R(B,B),\n\u00acP(F(A,A),F(F(A,A),A)) \u2228\u00acQ(F(A,A),A)\u2228R(F(A,A),B),\n\u00acP(F(A,B),F(F(A,B),A))\u2228\u00acQ(F(A,B),A)\u2228R(F(A,B),B),...}\nHERBRAND\u2019S ThesedefinitionsallowustostateaformofHerbrand\u2019stheorem(Herbrand, 1930):\nTHEOREM\nIfasetS ofclausesisunsatisfiable, thenthereexistsafinitesubsetofH (S)that\nS\nisalsounsatisfiable.\n(cid:2)\nLetS bethisfinitesubsetofgroundsentences. Now,wecanappealtothegroundresolution\n(cid:2)\ntheorem (page 255) to show that the resolution closure RC(S ) contains the empty clause.\n(cid:2)\nThatis,running propositional resolution tocompletion on S willderiveacontradiction.\nNow that we have established that there is always a resolution proof involving some\nfinite subset of the Herbrand base of S, the next step is to show that there is a resolution 352 Chapter 9. Inference inFirst-OrderLogic\nGO\u00a8DEL\u2019S INCOMPLETENESS THEOREM\nByslightly extending thelanguage offirst-orderlogic toallow forthe mathemat-\nical induction schema in arithmetic, Kurt Go\u00a8del was able to show, in his incom-\npletenesstheorem,thattherearetruearithmetic sentences thatcannotbeproved.\nThe proof of the incompleteness theorem is somewhat beyond the scope of\nthisbook, occupying, asitdoes, atleast30pages, butwecangiveahinthere. We\nbeginwiththelogical theory ofnumbers. Inthis theory, there isasingle constant,\n0, and a single function, S (the successor function). In the intended model, S(0)\ndenotes 1, S(S(0)) denotes 2, and soon; the language therefore has names forall\nthenaturalnumbers. Thevocabulary alsoincludesthefunctionsymbols+,\u00d7,and\nExpt (exponentiation) andtheusualsetoflogicalconnectivesandquantifiers. The\nfirststepistonoticethatthesetofsentencesthatwecanwriteinthislanguagecan\nbe enumerated. (Imagine defining an alphabetical order on the symbols and then\narranging, in alphabetical order, each of the sets of sentences of length 1, 2, and\nso on.) We can then number each sentence \u03b1 with a unique natural number #\u03b1\n(the Go\u00a8del number). This is crucial: number theory contains a name for each of\nits own sentences. Similarly, we can number each possible proof P with a Go\u00a8del\nnumberG(P),becauseaproofissimplyafinitesequence ofsentences.\nNow suppose we have a recursively enumerable set A of sentences that are\ntrue statements about the natural numbers. Recalling that A can be named by a\ngivensetofintegers,wecanimaginewritinginourlanguage asentence\u03b1(j,A)of\nthefollowingsort:\n\u2200i i is not the Go\u00a8del number of a proof of the sentence whose Go\u00a8del\nnumberisj,wheretheproofusesonlypremisesin A.\nThenlet\u03c3 bethesentence\u03b1(#\u03c3,A),thatis,asentencethatstatesitsownunprov-\nabilityfromA. (Thatthissentence alwaysexistsistruebutnotentirely obvious.)\nNow we make the following ingenious argument: Suppose that \u03c3 is provable\nfrom A; then \u03c3 is false (because \u03c3 says it cannot be proved). But then we have a\nfalsesentencethatisprovablefromA,soAcannotconsistofonlytruesentences\u2014\naviolationofourpremise. Therefore, \u03c3 isnotprovablefrom A. Butthisisexactly\nwhat\u03c3 itselfclaims;hence\u03c3 isatruesentence.\nSo, we have shown (barring 291 pages) that for any set of true sentences of\n2\nnumber theory, and in particular any set of basic axioms, there are other true sen-\ntences that cannot be proved from those axioms. This establishes, among other\nthings, that we can never prove all the theorems of mathematics within any given\nsystem of axioms. Clearly, this was an important discovery for mathematics. Its\nsignificanceforAIhasbeenwidelydebated,beginningwithspeculationsbyGo\u00a8del\nhimself. WetakeupthedebateinChapter26. Section9.5. Resolution 353\nproof using the clauses of S itself, which are not necessarily ground clauses. We start by\nconsidering asingleapplication oftheresolution rule. Robinsonstatedthislemma:\n(cid:2) (cid:2)\nLet C and C be two clauses with no shared variables, and let C and C be\n1 2 1 2\n(cid:2) (cid:2) (cid:2)\ngroundinstancesofC andC . IfC isaresolventofC andC ,thenthereexists\n1 2 1 2\n(cid:2)\na clause C such that (1) C is a resolvent of C and C and (2) C is a ground\n1 2\ninstanceofC.\nThisiscalledaliftinglemma,becauseitliftsaproofstepfromgroundclausesuptogeneral\nLIFTINGLEMMA\nfirst-order clauses. In order to prove his basic lifting lemma, Robinson had to invent unifi-\ncation and derive all of the properties of most general unifiers. Rather than repeat the proof\nhere,wesimplyillustrate thelemma:\nC = \u00acP(x,F(x,A))\u2228\u00acQ(x,A)\u2228R(x,B)\n1\nC = \u00acN(G(y),z)\u2228P(H(y),z)\n2\nC(cid:2) = \u00acP(H(B),F(H(B),A))\u2228\u00acQ(H(B),A)\u2228R(H(B),B)\n1\nC(cid:2) = \u00acN(G(B),F(H(B),A))\u2228P(H(B),F(H(B),A))\n2\nC(cid:2) = \u00acN(G(B),F(H(B),A))\u2228\u00acQ(H(B),A)\u2228R(H(B),B)\nC = \u00acN(G(y),F(H(y),A))\u2228\u00acQ(H(y),A)\u2228R(H(y),B).\n(cid:2) (cid:2) (cid:2)\nWe see that indeed C is a ground instance of C. In general, for C and C to have any\n1 2\nresolvents, they must be constructed by first applying to C and C the most general unifier\n1 2\nofapairofcomplementary literals in C andC . Fromtheliftinglemma,itiseasytoderive\n1 2\nasimilarstatement aboutanysequence ofapplications oftheresolution rule:\n(cid:2) (cid:2)\nForanyclause C intheresolution closureof S thereisaclause C intheresolu-\n(cid:2)\ntionclosureofS suchthatC isagroundinstanceofC andthederivationofC is\n(cid:2)\nthesamelengthasthederivation ofC .\n(cid:2)\nFrom this fact, it follows that if the empty clause appears in the resolution closure of S , it\nmustalsoappearintheresolution closureof S. Thisisbecausetheemptyclausecannotbea\nground instance ofanyotherclause. Torecap: wehaveshownthatifS isunsatisfiable, then\nthereisafinitederivationoftheemptyclauseusingtheresolution rule.\nTheliftingoftheoremprovingfromgroundclausestofirst-orderclausesprovidesavast\nincreaseinpower. Thisincreasecomesfromthefactthatthefirst-orderproofneedinstantiate\nvariables only as far as necessary for the proof, whereas the ground-clause methods were\nrequired toexamineahugenumberofarbitrary instantiations.\n9.5.5 Equality\nNoneoftheinferencemethodsdescribedsofarinthischapterhandleanassertionoftheform\nx = y. Threedistinctapproachescanbetaken. Thefirstapproachistoaxiomatizeequality\u2014\ntowritedownsentencesabouttheequalityrelationintheknowledgebase. Weneedtosaythat\nequalityisreflexive,symmetric,andtransitive,andwealsohavetosaythatwecansubstitute\nequals for equals in any predicate orfunction. So weneed three basic axioms, and then one 354 Chapter 9. Inference inFirst-OrderLogic\nforeachpredicate andfunction:\n\u2200x x=x\n\u2200x,y x=y \u21d2 y=x\n\u2200x,y,z x=y\u2227y=z \u21d2 x=z\n\u2200x,y x=y \u21d2 (P (x) \u21d4 P (y))\n1 1\n\u2200x,y x=y \u21d2 (P (x) \u21d4 P (y))\n2 2\n.\n.\n.\n\u2200w,x,y,z w=y\u2227x=z \u21d2 (F (w,x)=F (y,z))\n1 1\n\u2200w,x,y,z w=y\u2227x=z \u21d2 (F (w,x)=F (y,z))\n2 2\n.\n.\n.\nGiven these sentences, a standard inference procedure such as resolution can perform tasks\nrequiringequalityreasoning,suchassolvingmathematicalequations. However,theseaxioms\nwill generate a lot of conclusions, most of them not helpful to a proof. So there has been a\nsearch formoreefficientwaysofhandling equality. Onealternative istoadd inference rules\nrather than axioms. The simplest rule, demodulation, takes a unit clause x=y and some\nclause \u03b1 that contains the term x, and yields a new clause formed by substituting y for x\nwithin \u03b1. It works if the term within \u03b1 unifies with x; it need not be exactly equal to x.\nNotethat demodulation isdirectional; given x = y,thexalways getsreplaced withy,never\nvice versa. That means that demodulation can be used for simplifying expressions using\ndemodulators suchasx+0=xorx1=x. Asanotherexample, given\nFather(Father(x)) = PaternalGrandfather(x)\nBirthdate(Father(Father(Bella)),1926)\nwecanconclude bydemodulation\nBirthdate(PaternalGrandfather(Bella),1926).\nMoreformally,wehave\n\u2022 Demodulation: For any terms x, y, and z, where z appears somewhere in literal m\nDEMODULATION i\nandwhereUNIFY(x,z) = \u03b8,\nx=y, m \u2228\u00b7\u00b7\u00b7\u2228m\n1 n\n.\nSUB(SUBST(\u03b8,x),SUBST(\u03b8,y),m 1\u2228\u00b7\u00b7\u00b7\u2228m n)\nwhere SUBST is the usual substitution of a binding list, and SUB(x,y,m) means to\nreplace xwithy everywherethat xoccurswithinm.\nTherulecanalsobeextendedtohandlenon-unitclausesinwhichanequalityliteralappears:\n\u2022 Paramodulation: Foranyterms x,y,andz,wherez appearssomewhereinliteral m ,\nPARAMODULATION i\nandwhereUNIFY(x,z) = \u03b8,\n(cid:3) \u2228\u00b7\u00b7\u00b7\u2228(cid:3) \u2228x=y, m \u2228\u00b7\u00b7\u00b7\u2228m\n1 k 1 n\n.\nSUB(SUBST(\u03b8,x),SUBST(\u03b8,y),SUBST(\u03b8,(cid:3) 1\u2228\u00b7\u00b7\u00b7\u2228(cid:3)\nk\n\u2228m 1\u2228\u00b7\u00b7\u00b7\u2228m n)\nForexample,from\nP(F(x,B),x)\u2228Q(x) and F(A,y)=y\u2228R(y) Section9.5. Resolution 355\nwe have \u03b8=UNIFY(F(A,y),F(x,B))={x\/A,y\/B}, and we can conclude by paramodu-\nlationthesentence\nP(B,A)\u2228Q(A)\u2228R(B).\nParamodulation yieldsacompleteinference procedure forfirst-orderlogicwithequality.\nA third approach handles equality reasoning entirely within an extended unification\nalgorithm. That is, terms are unifiable if they are provably equal under some substitution,\nwhere \u201cprovably\u201d allows for equality reasoning. For example, the terms 1 + 2 and 2 + 1\nnormally are not unifiable, but a unification algorithm that knows that x+y=y +x could\nEQUATIONAL unifythemwiththeemptysubstitution. Equationalunificationofthiskindcanbedonewith\nUNIFICATION\nefficientalgorithmsdesignedfortheparticularaxiomsused(commutativity,associativity,and\nso on) rather than through explicit inference with those axioms. Theorem provers using this\ntechnique arecloselyrelatedtotheCLPsystemsdescribed inSection9.4.\n9.5.6 Resolutionstrategies\nWe know that repeated applications of the resolution inference rule will eventually find a\nproofifoneexists. Inthissubsection, weexaminestrategies thathelpfindproofs efficiently.\nUnitpreference: Thisstrategypreferstodoresolutionswhereoneofthesentencesisasingle\nUNITPREFERENCE\nliteral (also known as a unit clause). The idea behind the strategy is that we are trying to\nproduce anemptyclause, soitmightbeagoodideatopreferinferences thatproduceshorter\nclauses. Resolvingaunitsentence(suchasP)withanyothersentence(suchas\u00acP\u2228\u00acQ\u2228R)\nalways yields a clause (in this case, \u00acQ \u2228 R) that is shorter than the other clause. When\nthe unit preference strategy was first tried for propositional inference in 1964, it led to a\ndramaticspeedup,makingitfeasibletoprovetheoremsthat couldnotbehandledwithoutthe\npreference. Unitresolution is arestricted form of resolution in which every resolution step\nmust involve a unit clause. Unit resolution is incomplete in general, but complete for Horn\nclauses. Unitresolution proofsonHornclauses resembleforwardchaining.\nTheOTTER theoremprover(OrganizedTechniquesforTheorem-proving andEffective\nResearch, McCune, 1992), uses a form of best-first search. Its heuristic function measures\nthe\u201cweight\u201dofeachclause,wherelighterclausesarepreferred. Theexactchoiceofheuristic\nis up to the user, but generally, the weight of a clause should be correlated with its size or\ndifficulty. Unitclauses aretreated aslight; thesearch can thus beseenasageneralization of\ntheunitpreference strategy.\nSet of support: Preferences that try certain resolutions first are helpful, but in general it is\nSETOFSUPPORT\nmoreeffectivetotrytoeliminatesomepotential resolutions altogether. Forexample,wecan\ninsist that every resolution step involve at least one element of a special set of clauses\u2014the\nset of support. The resolvent is then added into the set of support. If the set of support is\nsmallrelativetothewholeknowledgebase,thesearchspace willbereduced dramatically.\nWe have to be careful with this approach because a bad choice for the set of support\nwill make the algorithm incomplete. However, if wechoose the set of support S so that the\nremainder of the sentences are jointly satisfiable, then set-of-support resolution is complete.\nForexample, onecan usethenegated query asthe setofsupport, ontheassumption that the 356 Chapter 9. Inference inFirst-OrderLogic\noriginal knowledge base is consistent. (After all, if it is not consistent, then the fact that the\nqueryfollowsfromitisvacuous.) Theset-of-support strategyhastheadditionaladvantageof\ngenerating goal-directed prooftreesthatareofteneasyforhumanstounderstand.\nInputresolution: Inthisstrategy,everyresolutioncombinesoneoftheinputsentences(from\nINPUTRESOLUTION\nthe KB or the query) with some other sentence. The proof in Figure 9.11 on page 348 uses\nonly input resolutions and has the characteristic shape of a single \u201cspine\u201d with single sen-\ntences combining onto the spine. Clearly, the space of proof trees of this shape is smaller\nthan the space of all proof graphs. In Horn knowledge bases, Modus Ponens is a kind of\ninputresolutionstrategy,becauseitcombinesanimplicationfromtheoriginalKBwithsome\nothersentences. Thus,itisnosurprise thatinputresolution iscomplete forknowledge bases\nthatareinHornform,butincomplete inthegeneral case. The linearresolution strategyisa\nLINEARRESOLUTION\nslightgeneralization thatallowsP andQtoberesolvedtogethereitherif P isintheoriginal\nKB orifP isanancestorofQintheprooftree. Linearresolution iscomplete.\nSubsumption: Thesubsumption methodeliminates allsentences thataresubsumed by(that\nSUBSUMPTION\nis,morespecificthan)anexistingsentenceintheKB.Forexample,ifP(x)isintheKB,then\nthereisnosenseinadding P(A)andevenlesssenseinaddingP(A)\u2228Q(B). Subsumption\nhelpskeeptheKBsmallandthushelpskeepthesearchspacesmall.\nPracticalusesofresolution theoremprovers\nTheorem provers can be applied to the problems involved in the synthesis and verification\nSYNTHESIS\nofbothhardwareandsoftware. Thus,theorem-proving research iscarriedoutinthefieldsof\nVERIFICATION\nhardwaredesign, programminglanguages, andsoftwareengineering\u2014not justinAI.\nIn the case of hardware, the axioms describe the interactions between signals and cir-\ncuit elements. (See Section 8.4.2 on page 309 for an example.) Logical reasoners designed\nspecially for verification have been able to verify entire CPUs, including their timing prop-\nerties (Srivas and Bickford, 1990). The AURA theorem prover has been applied to design\ncircuitsthataremorecompactthananyprevious design(Wojciechowski andWojcik,1983).\nIn the case of software, reasoning about programs is quite similar to reasoning about\nactions, as in Chapter 7: axioms describe the preconditions and effects of each statement.\nThe formal synthesis of algorithms was one of the first uses of theorem provers, as outlined\nby Cordell Green (1969a), who built on earlier ideas by Herbert Simon (1963). The idea\nis to constructively prove a theorem to the effect that \u201cthere exists a program p satisfying a\nDEDUCTIVE certain specification.\u201d Although fully automated deductivesynthesis, asitiscalled, has not\nSYNTHESIS\nyet become feasible forgeneral-purpose programming, hand-guided deductive synthesis has\nbeensuccessfulindesigningseveralnovelandsophisticatedalgorithms. Synthesisofspecial-\npurpose programs,suchasscientificcomputing code,isalso anactiveareaofresearch.\nSimilartechniquesarenowbeingappliedtosoftwareverificationbysystemssuchasthe\nSPIN model checker (Holzmann, 1997). Forexample, the Remote Agent spacecraft control\nprogram was verified before and after flight (Havelund et al., 2000). The RSA public key\nencryptionalgorithmandtheBoyer\u2013Moorestring-matching algorithmhavebeenverifiedthis\nway(BoyerandMoore,1984). Section9.6. Summary 357\n9.6 SUMMARY\nWehave presented ananalysis oflogical inference infirst-orderlogic and anumberofalgo-\nrithmsfordoingit.\n\u2022 A first approach uses inference rules (universal instantiation and existential instan-\ntiation) to propositionalize the inference problem. Typically, this approach is slow,\nunlessthedomainissmall.\n\u2022 Theuseofunificationtoidentify appropriate substitutions forvariables eliminates the\ninstantiationstepinfirst-orderproofs,makingtheprocessmoreefficientinmanycases.\n\u2022 A lifted version of Modus Ponens uses unification to provide a natural and powerful\ninference rule, generalized Modus Ponens. The forward-chaining and backward-\nchainingalgorithmsapplythisruletosetsofdefiniteclauses.\n\u2022 Generalized Modus Ponens is complete for definite clauses, although the entailment\nproblem is semidecidable. For Datalog knowledge bases consisting of function-free\ndefiniteclauses, entailment isdecidable.\n\u2022 Forward chaining is used in deductive databases, where it can be combined with re-\nlational database operations. It is also used in production systems, which perform\nefficient updates with very large rule sets. Forward chaining is complete for Datalog\nandrunsinpolynomial time.\n\u2022 Backward chaining is used in logic programming systems, which employ sophisti-\ncated compiler technology to provide very fast inference. Backward chaining suffers\nfromredundant inferences andinfiniteloops;thesecanbealleviated bymemoization.\n\u2022 Prolog, unlike first-order logic, uses aclosed worldwiththe unique names assumption\nand negation as failure. These make Prolog a more practical programming language,\nbutbringitfurtherfrompurelogic.\n\u2022 The generalized resolution inference rule provides a complete proof system for first-\norderlogic,usingknowledgebasesinconjunctive normalform.\n\u2022 Several strategies exist for reducing the search space of a resolution system without\ncompromisingcompleteness. Oneofthemostimportantissuesisdealingwithequality;\nweshowedhowdemodulationandparamodulationcanbeused.\n\u2022 Efficient resolution-based theorem provers have been used to prove interesting mathe-\nmaticaltheoremsandtoverifyandsynthesize softwareandhardware.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nGottlob Frege, who developed full first-order logic in 1879, based his system of inference\non a collection of valid schemas plus a single inference rule, Modus Ponens. Whitehead\nand Russell (1910) expounded the so-called rules of passage (the actual term is from Her-\nbrand (1930)) that are used to move quantifiers to the front of formulas. Skolem constants 358 Chapter 9. Inference inFirst-OrderLogic\nand Skolem functions were introduced, appropriately enough, by Thoralf Skolem (1920).\nOddlyenough, itwasSkolemwhointroduced theHerbranduniverse(Skolem,1928).\nHerbrand\u2019s theorem (Herbrand, 1930) has played a vital role in the development of\nautomated reasoning. Herbrand is also the inventor of unification. Go\u00a8del (1930) built on\nthe ideas of Skolem and Herbrand to show that first-order logic has a complete proof pro-\ncedure. Alan Turing (1936) and Alonzo Church (1936) simultaneously showed, using very\ndifferent proofs, that validity in first-order logic was not decidable. The excellent text by\nEnderton(1972)explainsalloftheseresultsinarigorousyetunderstandable fashion.\nAbraham Robinson proposed that an automated reasoner could be built using proposi-\ntionalizationandHerbrand\u2019stheorem,andPaulGilmore(1960)wrotethefirstprogram. Davis\nandPutnam(1960)introducedthepropositionalizationmethodofSection9.1. Prawitz(1960)\ndeveloped the key idea of letting the quest for propositional inconsistency drive the search,\nand generating terms from the Herbrand universe only when they were necessary to estab-\nlishpropositional inconsistency. Afterfurtherdevelopmentbyotherresearchers,thisidealed\nJ.A.Robinson(norelation) todevelopresolution (Robinson, 1965).\nIn AI, resolution was adopted for question-answering systems by Cordell Green and\nBertram Raphael(1968). EarlyAIimplementations putagood dealofeffortinto datastruc-\ntures that would allow efficient retrieval of facts; this work is covered in AI programming\ntexts (Charniak etal., 1987; Norvig, 1992; Forbus and de Kleer, 1993). By the early 1970s,\nforward chaining was well established in AI as an easily understandable alternative to res-\nolution. AI applications typically involved large numbers of rules, so it was important to\ndevelop efficient rule-matching technology, particularly for incremental updates. The tech-\nnology forproductionsystems wasdeveloped tosupport suchapplications. Theproduction\nsystem language OPS-5 (Forgy, 1981; Brownston et al., 1985), incorporating the efficient\nretematchprocess(Forgy,1982),wasusedforapplicationssuch astheR1expertsystemfor\nRETE\nminicomputerconfiguration (McDermott,1982).\nTheSOARcognitivearchitecture (Laird etal.,1987;Laird,2008)wasdesignedtohan-\ndle very large rule sets\u2014up to a million rules (Doorenbos, 1994). Example applications of\nSOAR include controlling simulated fighter aircraft (Jones et al., 1998), airspace manage-\nment (Taylor etal., 2007), AI characters forcomputer games(Wintermute etal., 2007), and\ntraining toolsforsoldiers(WrayandJones, 2005).\nThe field of deductive databases began with a workshop in Toulouse in 1977 that\nbrought together experts in logical inference and database systems (Gallaire and Minker,\n1978). Influential workbyChandraandHarel(1980) andUllman(1985) ledtotheadoption\nofDatalogasastandardlanguagefordeductivedatabases. Thedevelopmentofthemagicsets\ntechnique for rule rewriting by Bancilhon et al. (1986) allowed forward chaining to borrow\nthe advantage of goal-directedness from backward chaining. Current work includes theidea\nofintegrating multipledatabases intoaconsistent dataspace (Halevy,2007).\nBackward chaining for logical inference appeared first in Hewitt\u2019s PLANNER lan-\nguage (1969). Meanwhile, in1972, AlainColmerauerhaddeveloped andimplemented Pro-\nlog for the purpose of parsing natural language\u2014Prolog\u2019s clauses were intended initially\nas context-free grammar rules (Roussel, 1975; Colmerauer et al., 1973). Much of the the-\noretical background for logic programming was developed by Robert Kowalski, working Bibliographical andHistorical Notes 359\nwith Colmerauer; see Kowalski (1988) and Colmerauer and Roussel (1993) for a historical\noverview. Efficient Prolog compilers are generally based on the Warren Abstract Machine\n(WAM) model of computation developed by David H. D. Warren (1983). Van Roy (1990)\nshowedthatPrologprograms canbecompetitivewithCprogramsintermsofspeed.\nMethodsforavoiding unnecessary loopinginrecursivelogicprogramsweredeveloped\nindependently by Smith et al. (1986) and Tamaki and Sato (1986). The latter paper also\nincluded memoization for logic programs, a method developed extensively as tabled logic\nprogrammingbyDavidS.Warren. SwiftandWarren(1994) showhowtoextend theWAM\nto handle tabling, enabling Datalog programs to execute an order of magnitude faster than\nforward-chaining deductivedatabase systems.\nEarly work on constraint logic programming was done by Jaffar and Lassez (1987).\nJaffaretal.(1992)developedtheCLP(R)systemforhandlingreal-valuedconstraints. There\narenowcommercialproductsforsolvinglarge-scaleconfigurationandoptimizationproblems\nwith constraint programming; one of the best known is ILOG (Junker, 2003). Answer set\nprogramming (Gelfond,2008)extendsProlog,allowingdisjunction andnegation.\nTexts on logic programming and Prolog, including Shoham (1994), Bratko (2001),\nClocksin (2003), and Clocksin and Mellish (2003). Priorto 2000, the Journal ofLogic Pro-\ngramming was the journal of record; it has now been replaced by Theory and Practice of\nLogic Programming. Logic programming conferences include the International Conference\nonLogicProgramming(ICLP)andtheInternationalLogicProgrammingSymposium(ILPS).\nResearch into mathematical theorem proving began even before the first complete\nfirst-order systemsweredeveloped. Herbert Gelernter\u2019s GeometryTheorem Prover(Gelern-\nter, 1959) used heuristic search methods combined withdiagrams forpruning false subgoals\nand was able to prove some quite intricate results in Euclidean geometry. The demodula-\ntion and paramodulation rules for equality reasoning were introduced by Wos et al. (1967)\nand Wosand Robinson (1968), respectively. Theserules were also developed independently\nin the context of term-rewriting systems (Knuth and Bendix, 1970). The incorporation of\nequalityreasoningintotheunificationalgorithmisduetoGordonPlotkin(1972). Jouannaud\nand Kirchner (1991) survey equational unification from a term-rewriting perspective. An\noverviewofunification isgivenbyBaaderandSnyder(2001).\nA number of control strategies have been proposed for resolution, beginning with the\nunitpreferencestrategy(Wosetal.,1964). Theset-of-support strategywasproposedbyWos\net al. (1965) to provide a degree of goal-directedness in resolution. Linear resolution first\nappeared inLoveland (1970). Genesereth and Nilsson (1987, Chapter5) provide ashort but\nthorough analysis ofawidevarietyofcontrolstrategies.\nA Computational Logic (Boyer and Moore, 1979) is the basic reference on the Boyer-\nMooretheoremprover. Stickel(1992)coversthePrologTechnologyTheoremProver(PTTP),\nwhichcombinestheadvantagesofPrologcompilationwiththecompletenessofmodelelimi-\nnation. SETHEO(Letzetal.,1992)isanotherwidelyusedtheoremproverbasedonthisap-\nproach. LEANTAP (Beckert andPosegga, 1995) isanefficient theorem proverimplemented\ninonly25linesofProlog. Weidenbach (2001) describes SPASS,oneofthestrongest current\ntheoremprovers. Themostsuccessfultheoremproverinrecentannualcompetitionshasbeen\nVAMPIRE (Riazanov and Voronkov, 2002). The COQ system (Bertot et al., 2004) and the E 360 Chapter 9. Inference inFirst-OrderLogic\nequational solver (Schulz, 2004) have also proven to be valuable tools for proving correct-\nness. Theorem provers have been used to automatically synthesize and verify software for\ncontrolling spacecraft (Denney et al., 2006), including NASA\u2019s new Orion capsule (Lowry,\n2008). Thedesign ofthe FM9001 32-bit microprocessor wasproved correct bythe NQTHM\nsystem (Hunt and Brock, 1992). TheConference onAutomated Deduction (CADE)runs an\nannualcontestforautomatedtheoremprovers. From2002through2008,themostsuccessful\nsystem has been VAMPIRE (Riazanov and Voronkov, 2002). Wiedijk (2003) compares the\nstrength of 15 mathematical provers. TPTP (Thousands of Problems for Theorem Provers)\nis a library of theorem-proving problems, useful for comparing the performance of systems\n(SutcliffeandSuttner,1998;Sutcliffe etal.,2006).\nTheorem provers have come up with novel mathematical results that eluded human\nmathematicians for decades, as detailed in the book Automated Reasoning and the Discov-\nery of Missing Elegant Proofs (Wos and Pieper, 2003). The SAM (Semi-Automated Math-\nematics) program was the first, proving a lemma in lattice theory (Guard et al., 1969). The\nAURA program has also answered open questions in several areas of mathematics (Wos and\nWinker, 1983). The Boyer\u2013Moore theorem prover (Boyer and Moore, 1979) was used by\nNatarajan Shankar to give the first fully rigorous formal proof of Go\u00a8del\u2019s Incompleteness\nTheorem (Shankar, 1986). The NUPRL system proved Girard\u2019s paradox (Howe, 1987) and\nHigman\u2019s Lemma(Murthyand Russell, 1990). In1933, Herbert Robbins proposed asimple\nsetofaxioms\u2014theRobbinsalgebra\u2014that appearedtodefineBooleanalgebra, butnoproof\nROBBINSALGEBRA\ncould be found (despite serious work by Alfred Tarski and others). On October 10, 1996,\naftereightdaysofcomputation, EQP (aversionof OTTER)foundaproof(McCune,1997).\nMany early papers in mathematical logic are to be found in From Frege to Go\u00a8del:\nA Source Book in Mathematical Logic (van Heijenoort, 1967). Textbooks geared toward\nautomated deduction include the classic Symbolic Logic and Mechanical Theorem Prov-\ning(ChangandLee,1973),aswellasmorerecentworksbyDuffy(1991),Wosetal.(1992),\nBibel (1993), and Kaufmann et al. (2000). The principal journal for theorem proving is the\nJournal of Automated Reasoning; the main conferences are the annual Conference on Auto-\nmated Deduction (CADE) and the International Joint Conference on Automated Reasoning\n(IJCAR). The Handbook of Automated Reasoning (Robinson and Voronkov, 2001) collects\npapersinthefield. MacKenzie\u2019sMechanizingProof(2004)coversthehistoryandtechnology\noftheorem provingforthepopularaudience.\nEXERCISES\n9.1 Prove that Universal Instantiation is sound and that Existential Instantiation produces\naninferentially equivalent knowledgebase.\n9.2 From Likes(Jerry,IceCream) it seems reasonable to infer \u2203x Likes(x,IceCream).\nEXISTENTIAL Writedownageneralinference rule, ExistentialIntroduction,thatsanctions thisinference.\nINTRODUCTION\nStatecarefullytheconditionsthatmustbesatisfiedbythevariablesandtermsinvolved. Exercises 361\n9.3 Suppose a knowledge base contains just one sentence, \u2203x AsHighAs(x,Everest).\nWhichofthefollowingarelegitimateresultsofapplying ExistentialInstantiation?\na. AsHighAs(Everest,Everest).\nb. AsHighAs(Kilimanjaro,Everest).\nc. AsHighAs(Kilimanjaro,Everest)\u2227AsHighAs(BenNevis,Everest)\n(aftertwoapplications).\n9.4 Foreachpairofatomicsentences, givethemostgeneralunifierifitexists:\na. P(A,B,B),P(x,y,z).\nb. Q(y,G(A,B)),Q(G(x,x),y).\nc. Older(Father(y),y),Older(Father(x),John).\nd. Knows(Father(y),y),Knows(x,x).\n9.5 Considerthesubsumption latticesshowninFigure9.2(page 329).\na. Constructthelatticeforthesentence Employs(Mother(John),Father(Richard)).\nb. Construct thelattice forthesentence Employs(IBM,y)(\u201cEveryone worksforIBM\u201d).\nRemembertoincludeeverykindofquerythatunifieswiththesentence.\nc. AssumethatSTORE indexeseachsentenceundereverynodeinitssubsumption lattice.\nExplain how FETCH should workwhensomeofthesesentences contain variables; use\nasexamplesthesentences in(a)and(b)andthequery Employs(x,Father(x)).\n9.6 Write down logical representations for the following sentences, suitable for use with\nGeneralized ModusPonens:\na. Horses,cows,andpigsaremammals.\nb. Anoffspring ofahorseisahorse.\nc. Bluebeardisahorse.\nd. BluebeardisCharlie\u2019sparent.\ne. Offspringandparentareinverserelations.\nf. Everymammalhasaparent.\n9.7 Thesequestions concernconcernissueswithsubstitution andSkolemization.\na. Giventhe premise \u2200x \u2203y P(x,y), it is not valid to conclude that \u2203q P(q,q). Give\nanexampleofapredicate P wherethefirstistruebutthesecondisfalse.\nb. Suppose that an inference engine is incorrectly written with the occurs check omitted,\nso that it allows a literal like P(x,F(x)) to be unified with P(q,q). (As mentioned,\nmost standard implementations of Prolog actually do allow this.) Show that such an\ninference enginewillallowtheconclusion \u2203y P(q,q)tobeinferred fromthepremise\n\u2200x \u2203y P(x,y). 362 Chapter 9. Inference inFirst-OrderLogic\nc. Suppose that a procedure that converts first-order logic to clausal form incorrectly\nSkolemizes \u2200x \u2203y P(x,y) to P(x,Sk0)\u2014that is, it replaces y by a Skolem con-\nstant rather than by a Skolem function of x. Show that an inference engine that uses\nsuch a procedure will likewise allow \u2203q P(q,q) to be inferred from the premise\n\u2200x \u2203y P(x,y).\nd. A common error among students is to suppose that, in unification, one is allowed to\nsubstituteatermforaSkolemconstantinsteadofforavariable. Forinstance, theywill\nsaythattheformulasP(Sk1)andP(A)canbeunifiedunderthesubstitution{Sk1\/A}.\nGiveanexamplewherethisleadstoaninvalidinference.\n9.8 Explainhowtowriteanygiven3-SATproblemofarbitrarysizeusingasinglefirst-order\ndefiniteclauseandnomorethan30ground facts.\n9.9 Supposeyouaregiventhefollowingaxioms:\n1. 0 \u2264 3.\n2. 7 \u2264 9.\n3. \u2200x x\u2264 x.\n4. \u2200x x\u2264 x+0.\n5. \u2200x x+0\u2264 x.\n6. \u2200x,y x+y \u2264 y+x.\n7. \u2200w,x,y,z w \u2264 y \u2227x\u2264 z \u21d2 w+x \u2264 y+z.\n8. \u2200x,y,z x \u2264 y\u2227y \u2264 z \u21d2 x\u2264 z\na. Giveabackward-chaining proof ofthesentence 7 \u2264 3+9. (Besure, ofcourse, touse\nonly the axioms given here, not anything else you may know about arithmetic.) Show\nonlythestepsthatleadstosuccess, nottheirrelevant steps.\nb. Givea forward-chaining proof of the sentence 7 \u2264 3+9. Again, show only the steps\nthatleadtosuccess.\n9.10 A popular children\u2019s riddle is \u201cBrothers and sisters have I none, but that man\u2019s father\nis myfather\u2019s son.\u201d Use the rules ofthe family domain (Section 8.3.2on page 301) toshow\nwhothatmanis. Youmayapplyanyoftheinferencemethodsdescribedinthischapter. Why\ndoyouthinkthatthisriddleisdifficult?\n9.11 Suppose weput into alogical knowledge base a segment of the U.S.census data list-\ning the age, city of residence, date of birth, and mother of every person, using social se-\ncurity numbers as identifying constants for each person. Thus, George\u2019s age is given by\nAge(443-65-1282,56). Which ofthe following indexing schemes S1\u2013S5enable anefficient\nsolution forwhichofthequeriesQ1\u2013Q4(assuming normalbackwardchaining)?\n\u2022 S1: anindexforeachatomineachposition.\n\u2022 S2: anindexforeachfirstargument.\n\u2022 S3: anindexforeachpredicate atom.\n\u2022 S4: anindexforeachcombination ofpredicate andfirstargument. Exercises 363\n\u2022 S5: an index for each combination of predicate and second argument and an index for\neachfirstargument.\n\u2022 Q1: Age(443-44-4321,x)\n\u2022 Q2: ResidesIn(x,Houston)\n\u2022 Q3: Mother(x,y)\n\u2022 Q4: Age(x,34)\u2227ResidesIn(x,TinyTownUSA)\n9.12 One might suppose that we can avoid the problem of variable conflict in unification\nduring backward chaining by standardizing apart all of the sentences in the knowledge base\nonceandforall. Showthat, forsomesentences, thisapproach cannot work. (Hint: Consider\nasentence inwhichonepartunifieswithanother.)\n9.13 In this exercise, use the sentences you wrote in Exercise 9.6 to answer a question by\nusingabackward-chaining algorithm.\na. Draw the proof tree generated by an exhaustive backward-chaining algorithm for the\nquery\u2203h Horse(h),whereclauses arematchedintheordergiven.\nb. Whatdoyounoticeaboutthisdomain?\nc. Howmanysolutions forhactually followfromyoursentences?\nd. Canyouthinkofawaytofindallofthem? (Hint: SeeSmithetal.(1986).)\n9.14 Tracetheexecutionofthebackward-chainingalgorithminFigure9.6(page338)when\nitisappliedtosolvethecrimeproblem(page330). Showthesequenceofvaluestakenonby\nthegoals variable, andarrangethemintoatree.\n9.15 ThefollowingPrologcodedefinesapredicate P.(Rememberthatuppercasetermsare\nvariables, notconstants, inProlog.)\nP(X,[X|Y]).\nP(X,[Y|Z]) :- P(X,Z).\na. ShowprooftreesandsolutionsforthequeriesP(A,[2,1,3])andP(2,[1,A,3]).\nb. Whatstandard listoperation does Prepresent?\n9.16 Thisexerciselooksatsorting inProlog.\na. WritePrologclausesthatdefinethepredicate sorted(L),whichistrueifandonlyif\nlistLissortedinascending order.\nb. Writea Prolog definition forthe predicate perm(L,M),which is true ifand only if L\nisapermutation ofM.\nc. Definesort(L,M)(Misasortedversionof L)usingpermandsorted.\nd. Runsortonlongerandlongerlistsuntilyoulosepatience. Whatisthetimecomplex-\nityofyourprogram?\ne. Writeafastersortingalgorithm, suchasinsertion sortor quicksort, inProlog. 364 Chapter 9. Inference inFirst-OrderLogic\n9.17 This exercise looks at the recursive application of rewrite rules, using logic program-\nming. Arewriterule(ordemodulatorinOTTER terminology)isanequationwithaspecified\ndirection. For example, the rewrite rule x+0 \u2192 x suggests replacing any expression that\nmatchesx+0withtheexpressionx. Rewriterulesareakeycomponentofequationalreason-\ningsystems. Usethepredicate rewrite(X,Y)torepresentrewriterules. Forexample,the\nearlier rewrite rule is written as rewrite(X+0,X).Some terms are primitive and cannot\nbefurthersimplified;thus,wewrite primitive(0)tosaythat0isaprimitiveterm.\na. Writeadefinitionofapredicate simplify(X,Y),thatistruewhen Yisasimplified\nversionofX\u2014thatis,whennofurtherrewriterulesapplytoanysubexpression ofY.\nb. Write a collection of rules for the simplification of expressions involving arithmetic\noperators, andapplyyoursimplification algorithm tosomesampleexpressions.\nc. Writeacollectionofrewriterulesforsymbolicdifferentiation, andusethemalongwith\nyour simplification rules to differentiate and simplify expressions involving arithmetic\nexpressions, including exponentiation.\n9.18 This exercise considers the implementation of search algorithms in Prolog. Suppose\nthat successor(X,Y)is true when state Y is a successor of state X; and that goal(X)\nistrue when Xisagoal state. Writeadefinition for solve(X,P),which means thatPis a\npath (list of states) beginning with X, ending in a goal state, and consisting of a sequence of\nlegalstepsasdefinedbysuccessor.Youwillfindthatdepth-firstsearchistheeasiestway\ntodothis. Howeasywoulditbetoaddheuristic searchcontrol?\n9.19 Supposeaknowledgebasecontainsjustthefollowingfirst-orderHornclauses:\nAncestor(Mother(x),x)\nAncestor(x,y)\u2227Ancestor(y,z) \u21d2 Ancestor(x,z)\nConsideraforwardchainingalgorithmthat,onthejthiteration,terminatesiftheKBcontains\nasentence that unifies withthe query, else adds tothe KBevery atomic sentence that canbe\ninferredfromthesentences alreadyintheKBafteriterationj \u22121.\na. Foreachofthefollowingqueries, saywhetherthealgorithm will(1)giveananswer(if\nso,writedownthatanswer);or(2)terminatewithnoanswer; or(3)neverterminate.\n(i) Ancestor(Mother(y),John)\n(ii) Ancestor(Mother(Mother(y)),John)\n(iii) Ancestor(Mother(Mother(Mother(y))),Mother(y))\n(iv) Ancestor(Mother(John),Mother(Mother(John)))\nb. Canaresolutionalgorithmprovethesentence \u00acAncestor(John,John)fromtheorig-\ninalknowledgebase? Explainhow,orwhynot.\nc. Suppose we add the assertion that \u00ac(Mother(x)=x) and augment the resolution al-\ngorithmwithinferencerulesforequality. Nowwhatistheanswerto(b)?\n9.20 LetLbethefirst-orderlanguagewithasinglepredicateS(p,q),meaning\u201cpshaves q.\u201d\nAssumeadomainofpeople. Exercises 365\na. Consider the sentence \u201cThere exists a person P who shaves every one who does not\nshavethemselves, andonlypeoplethatdonotshavethemselves.\u201d ExpressthisinL.\nb. Convertthesentence in(a)toclausalform.\nc. Construct aresolution proof to show that the clauses in(b) are inherently inconsistent.\n(Note: youdonotneedanyadditional axioms.)\n9.21 Howcanresolution beusedtoshowthatasentence isvalid? Unsatisfiable?\n9.22 Construct an example of two clauses that can be resolved together in two different\nwaysgivingtwodifferentoutcomes.\n9.23 From \u201cHorses are animals,\u201d it follows that \u201cThe head of a horse is the head of an\nanimal.\u201d Demonstrate thatthisinference isvalidbycarrying outthefollowingsteps:\na. Translatethepremiseandtheconclusionintothelanguageoffirst-orderlogic. Usethree\npredicates: HeadOf(h,x)(meaning\u201chistheheadofx\u201d),Horse(x),andAnimal(x).\nb. Negate the conclusion, and convert the premise and the negated conclusion into con-\njunctivenormalform.\nc. Useresolution toshowthattheconclusion followsfromthe premise.\n9.24 Herearetwosentences inthelanguage offirst-orderlogic:\n(A)\u2200x \u2203y (x\u2265 y)\n(B)\u2203y \u2200x (x \u2265 y)\na. Assumethatthevariablesrangeoverallthenaturalnumbers0,1,2,...,\u221eandthatthe\n\u201c\u2265\u201d predicate means \u201cis greater than or equal to.\u201d Under this interpretation, translate\n(A)and(B)intoEnglish.\nb. Is(A)trueunderthisinterpretation?\nc. Is(B)trueunderthisinterpretation?\nd. Does(A)logically entail(B)?\ne. Does(B)logically entail(A)?\nf. Using resolution, tryto prove that (A)follows from (B). Do this even ifyou think that\n(B)does not logically entail (A);continue until the proof breaks downand you cannot\nproceed(ifitdoesbreakdown). Showtheunifyingsubstitutionforeachresolutionstep.\nIftheprooffails,explainexactlywhere,how,andwhyitbreaksdown.\ng. Nowtrytoprovethat(B)followsfrom(A).\n9.25 Resolution can produce nonconstructive proofs for queries with variables, so we had\ntointroduce special mechanismstoextract definiteanswers. Explainwhythisissuedoesnot\narisewithknowledgebasescontaining onlydefiniteclauses.\n9.26 We said in this chapter that resolution cannot be used to generate all logical conse-\nquences ofasetofsentences. Cananyalgorithm dothis? 10\nCLASSICAL PLANNING\nInwhichweseehowanagentcantakeadvantageofthestructureofaproblemto\nconstruct complexplansofaction.\nWe have defined AI as the study of rational action, which means that planning\u2014devising a\nplan of action to achieve one\u2019s goals\u2014is a critical part of AI. We have seen two examples\nof planning agents so far: the search-based problem-solving agent of Chapter 3 and the hy-\nbrid logical agent of Chapter 7. In this chapter we introduce a representation for planning\nproblemsthatscalesuptoproblemsthatcouldnotbehandled bythoseearlierapproaches.\nSection10.1developsanexpressiveyetcarefullyconstrainedlanguageforrepresenting\nplanning problems. Section 10.2 shows how forward and backward search algorithms can\ntakeadvantageofthisrepresentation,primarilythroughaccurateheuristicsthatcanbederived\nautomaticallyfromthestructureoftherepresentation. (Thisisanalogoustothewayinwhich\neffectivedomain-independentheuristicswereconstructedforconstraintsatisfactionproblems\ninChapter6.) Section10.3showshowadatastructurecalledtheplanninggraphcanmakethe\nsearchforaplanmoreefficient. Wethendescribe afewoftheotherapproaches toplanning,\nandconclude bycomparingthevariousapproaches.\nThis chapter covers fully observable, deterministic, static environments with single\nagents. Chapters 11 and 17 cover partially observable, stochastic, dynamic environments\nwithmultipleagents.\n10.1 DEFINITION OF CLASSICAL PLANNING\nThe problem-solving agent of Chapter 3 can find sequences of actions that result in a goal\nstate. Butitdealswithatomicrepresentations ofstates andthus needsgood domain-specific\nheuristicstoperformwell. Thehybridpropositional logicalagentofChapter7canfindplans\nwithout domain-specific heuristics because it uses domain-independent heuristics based on\nthe logical structure of the problem. But it relies on ground (variable-free) propositional\ninference, whichmeansthatitmaybeswampedwhentherearemanyactionsandstates. For\nexample,inthewumpusworld,thesimpleactionofmovingastepforwardhadtoberepeated\nforallfouragentorientations, T timesteps,andn2 currentlocations.\n366 Section10.1. DefinitionofClassicalPlanning 367\nIn response to this, planning researchers have settled on a factored representation\u2014\noneinwhichastateoftheworldisrepresentedbyacollectionofvariables. Weusealanguage\ncalled PDDL,thePlanning DomainDefinition Language, thatallowsustoexpress all 4Tn2\nPDDL\nactions with one action schema. There have been several versions of PDDL; we select a\nsimple version andalter itssyntax tobeconsistent with the rest ofthe book.1 Wenowshow\nhowPDDLdescribesthefourthingsweneedtodefineasearchproblem: theinitialstate,the\nactionsthatareavailable inastate, theresultofapplying anaction, andthegoaltest.\nEachstateisrepresentedasaconjunctionoffluentsthatareground,functionlessatoms.\nFor example, Poor \u2227 Unknown might represent the state of a hapless agent, and a state\nin a package delivery problem might be At(Truck ,Melbourne) \u2227 At(Truck ,Sydney).\n1 2\nDatabasesemanticsisused: theclosed-worldassumptionmeansthatanyfluentsthatarenot\nmentioned are false, and the unique names assumption means that Truck and Truck are\n1 2\ndistinct. Thefollowingfluentsarenotallowedinastate: At(x,y)(becauseitisnon-ground),\n\u00acPoor (becauseitisanegation),andAt(Father(Fred),Sydney)(becauseitusesafunction\nsymbol). The representation of states is carefully designed so that a state can be treated\neither asaconjunction of fluents, which can be manipulated by logical inference, oras a set\nof fluents, which can be manipulated with set operations. The set semantics is sometimes\nSETSEMANTICS\neasiertodealwith.\nActionsaredescribedbyasetofactionschemasthatimplicitlydefinetheACTIONS(s)\nandRESULT(s,a)functionsneededtodoaproblem-solvingsearch. WesawinChapter7that\nanysystemforactiondescriptionneedstosolvetheframeproblem\u2014tosaywhatchangesand\nwhat stays the same as the result ofthe action. Classical planning concentrates on problems\nwhere mostactions leave mostthings unchanged. Think ofaworld consisting ofabunch of\nobjects onaflatsurface. Theaction ofnudging anobject causes that object tochange itslo-\ncationbyavector\u0394. Aconcisedescriptionoftheactionshouldmentiononly\u0394;itshouldn\u2019t\nhavetomentionalltheobjectsthatstayinplace. PDDL doesthatbyspecifying theresultof\nanactionintermsofwhatchanges;everything thatstaysthe sameisleftunmentioned.\nA set of ground (variable-free) actions can be represented by a single action schema.\nACTIONSCHEMA\nTheschemaisaliftedrepresentation\u2014it liftsthelevelofreasoning frompropositional logic\nto a restricted subset of first-order logic. For example, here is an action schema for flying a\nplanefromonelocation toanother:\nAction(Fly(p,from,to),\nPRECOND:At(p,from)\u2227Plane(p)\u2227Airport(from)\u2227Airport(to)\nEFFECT:\u00acAt(p,from)\u2227At(p,to))\nThe schema consists of the action name, a list of all the variables used in the schema, a\nprecondition and an effect. Although we haven\u2019t said yet how the action schema converts\nPRECONDITION\ninto logical sentences, think of the variables as being universally quantified. We are free to\nEFFECT\nchoosewhatevervalueswewanttoinstantiatethevariables. Forexample,hereisoneground\n1 PDDLwasderivedfromtheoriginal STRIPSplanninglanguage(FikesandNilsson,1971). whichisslightly\nmorerestrictedthanPDDL:STRIPSpreconditionsandgoalscannotcontainnegativeliterals. 368 Chapter 10. ClassicalPlanning\nactionthatresultsfromsubstituting valuesforallthevariables:\nAction(Fly(P ,SFO,JFK),\n1\nPRECOND:At(P 1,SFO)\u2227Plane(P 1)\u2227Airport(SFO)\u2227Airport(JFK)\nEFFECT:\u00acAt(P 1,SFO)\u2227At(P 1,JFK))\nTheprecondition andeffectofanactionareeachconjunctions ofliterals(positiveornegated\natomic sentences). The precondition defines the states in which the action can be executed,\nandtheeffect definestheresult ofexecuting theaction. Anaction acan beexecuted instate\nsifsentails theprecondition of a. Entailment can also beexpressed withthe setsemantics:\ns |= q iff every positive literal in q is in s and every negated literal in q is not. In formal\nnotation wesay\n(a \u2208 ACTIONS(s)) \u21d4 s |= PRECOND(a),\nwhereanyvariables in aareuniversally quantified. Forexample,\n\u2200p,from,to (Fly(p,from,to)\u2208 ACTIONS(s)) \u21d4\ns |= (At(p,from)\u2227Plane(p)\u2227Airport(from)\u2227Airport(to))\nWe say that action a is applicable in state s if the preconditions are satisfied by s. When\nAPPLICABLE\nan action schema a contains variables, it may have multiple applicable instantiations. For\nexample, with the initial state defined in Figure 10.1, the Fly action can be instantiated as\nFly(P ,SFO,JFK) or as Fly(P ,JFK,SFO), both of which are applicable in the initial\n1 2\nstate. Ifanactionahasvvariables,then,inadomainwithkuniquenamesofobjects,ittakes\nO(vk)timeintheworstcasetofindtheapplicable groundactions.\nSometimeswewanttopropositionalizeaPDDLproblem\u2014replaceeachactionschema\nPROPOSITIONALIZE\nwith a set of ground actions and then use apropositional solver such as SATPLAN to find a\nsolution. However,thisisimpractical whenv andk arelarge.\n(cid:2)\nThe result of executing action a in state s is defined as a state s which is represented\nby the set of fluents formed by starting with s, removing the fluents that appear as negative\nDELETELIST\nliteralsintheaction\u2019s effects(whatwecallthedeletelistorDEL(a)),andaddingthefluents\nADDLIST\nthatarepositiveliteralsintheaction\u2019s effects(whatwecalltheaddlistorADD(a)):\nRESULT(s,a) = (s\u2212DEL(a))\u222aADD(a). (10.1)\nForexample, withtheaction Fly(P ,SFO,JFK),wewouldremove At(P ,SFO)andadd\n1 1\nAt(P ,JFK). Itisarequirement ofaction schemas that anyvariable intheeffect mustalso\n1\nappear in the precondition. That way, when the precondition is matched against the state s,\nallthe variables willbebound, and RESULT(s,a)willtherefore have only ground atoms. In\notherwords,ground statesareclosedunderthe RESULT operation.\nAlsonotethatthefluentsdonotexplicitlyrefertotime,astheydidinChapter7. There\nweneededsuperscripts fortime,andsuccessor-state axiomsoftheform\nFt+1 \u21d4 ActionCausesFt\u2228(Ft\u2227\u00acActionCausesNotFt).\nIn PDDL the times and states are implicit in the action schemas: the precondition always\nreferstotimetandtheeffecttotimet+1.\nAsetofactionschemasservesasadefinitionofaplanningdomain. Aspecificproblem\nwithin the domain is defined with the addition of an initial state and a goal. The initial Section10.1. DefinitionofClassicalPlanning 369\nInit(At(C , SFO) \u2227 At(C , JFK) \u2227 At(P , SFO) \u2227 At(P , JFK)\n1 2 1 2\n\u2227Cargo(C ) \u2227 Cargo(C ) \u2227 Plane(P ) \u2227 Plane(P )\n1 2 1 2\n\u2227Airport(JFK) \u2227 Airport(SFO))\nGoal(At(C , JFK) \u2227 At(C , SFO))\n1 2\nAction(Load(c, p, a),\nPRECOND:At(c, a) \u2227 At(p, a) \u2227 Cargo(c) \u2227 Plane(p) \u2227 Airport(a)\nEFFECT:\u00acAt(c, a) \u2227 In(c, p))\nAction(Unload(c, p, a),\nPRECOND:In(c, p) \u2227 At(p, a) \u2227 Cargo(c) \u2227 Plane(p) \u2227 Airport(a)\nEFFECT:At(c, a) \u2227 \u00acIn(c, p))\nAction(Fly(p, from, to),\nPRECOND:At(p, from) \u2227 Plane(p) \u2227 Airport(from) \u2227 Airport(to)\nEFFECT:\u00acAt(p, from) \u2227 At(p, to))\nFigure10.1 APDDLdescriptionofanaircargotransportationplanningproblem.\nstate is a conjunction of ground atoms. (As with all states, the closed-world assumption is\nINITIALSTATE\nused, which means that any atoms that are not mentioned are false.) The goal is just like a\nGOAL\nprecondition: aconjunction ofliterals (positive ornegative) thatmaycontain variables, such\nasAt(p,SFO)\u2227Plane(p). Anyvariables aretreatedasexistentially quantified, sothisgoal\nis to have any plane at SFO. The problem is solved when wecan find a sequence of actions\nthatendinastatesthatentailsthegoal. Forexample,thestateRich\u2227Famous\u2227Miserable\nentails the goal Rich \u2227Famous, and the state Plane(Plane )\u2227At(Plane ,SFO) entails\n1 1\nthegoalAt(p,SFO)\u2227Plane(p).\nNowwehavedefinedplanningasasearchproblem: wehaveaninitialstate,anACTIONS\nfunction, a RESULT function, and a goal test. We\u2019ll look at some example problems before\ninvestigating efficientsearchalgorithms.\n10.1.1 Example: Aircargo transport\nFigure10.1showsanaircargotransport problem involving loading andunloading cargoand\nflyingitfrom placetoplace. Theproblem canbedefined withthree actions: Load,Unload,\nandFly. Theactions affecttwopredicates: In(c,p)meansthatcargo cisinsideplanep,and\nAt(x,a)meansthatobjectx(eitherplaneorcargo)isatairport a. Notethatsomecaremust\nbe taken to make sure the At predicates are maintained properly. When a plane flies from\noneairporttoanother, allthecargo insidetheplane goeswithit. Infirst-orderlogicitwould\nbeeasytoquantify overallobjects thatareinside theplane. Butbasic PDDL doesnothave\na universal quantifier, so we need a different solution. The approach we use is to say that a\npieceofcargoceasestobeAtanywherewhenitisInaplane;thecargoonlybecomesAtthe\nnew airport when it is unloaded. So At really means \u201cavailable for use at a given location.\u201d\nThefollowingplanisasolution totheproblem:\n[Load(C ,P ,SFO),Fly(P ,SFO,JFK),Unload(C ,P ,JFK),\n1 1 1 1 1\nLoad(C ,P ,JFK),Fly(P ,JFK,SFO),Unload(C ,P ,SFO)].\n2 2 2 2 2 370 Chapter 10. ClassicalPlanning\nFinally, there is the problem ofspurious actions such as Fly(P ,JFK,JFK), which should\n1\nbeano-op, butwhich hascontradictory effects (according tothedefinition, theeffect would\ninclude At(P ,JFK) \u2227 \u00acAt(P ,JFK)). It is common to ignore such problems, because\n1 1\nthey seldom cause incorrect plans to beproduced. Thecorrect approach isto add inequality\npreconditions saying thatthe from andto airports mustbedifferent; seeanother exampleof\nthisinFigure10.3.\n10.1.2 Example: The spare tireproblem\nConsider the problem of changing a flat tire (Figure 10.2). The goal is to have a good spare\ntireproperlymountedontothecar\u2019saxle,wheretheinitial statehasaflattireontheaxleand\na good spare tire in the trunk. To keep it simple, our version of the problem is an abstract\none,withnostickylugnutsorothercomplications. Therearejustfouractions: removingthe\nspare from the trunk, removing the flat tire from the axle, putting the spare on the axle, and\nleaving the car unattended overnight. We assume that the car is parked in a particularly bad\nneighborhood, sothat the effect ofleaving itovernight isthat the tires disappear. Asolution\ntotheproblem is[Remove(Flat,Axle),Remove(Spare,Trunk),PutOn(Spare,Axle)].\nInit(Tire(Flat) \u2227 Tire(Spare) \u2227 At(Flat,Axle) \u2227 At(Spare,Trunk))\nGoal(At(Spare,Axle))\nAction(Remove(obj,loc),\nPRECOND:At(obj,loc)\nEFFECT:\u00acAt(obj,loc) \u2227 At(obj,Ground))\nAction(PutOn(t, Axle),\nPRECOND:Tire(t) \u2227 At(t,Ground) \u2227 \u00acAt(Flat,Axle)\nEFFECT:\u00acAt(t,Ground) \u2227 At(t,Axle))\nAction(LeaveOvernight,\nPRECOND:\nEFFECT:\u00acAt(Spare,Ground) \u2227 \u00acAt(Spare,Axle) \u2227 \u00acAt(Spare,Trunk)\n\u2227\u00acAt(Flat,Ground) \u2227 \u00acAt(Flat,Axle) \u2227 \u00acAt(Flat, Trunk))\nFigure10.2 Thesimplesparetireproblem.\n10.1.3 Example: The blocks world\nOne of the most famous planning domains is known as the blocks world. This domain\nBLOCKSWORLD\nconsists of a set of cube-shaped blocks sitting on a table.2 The blocks can be stacked, but\nonly one block can fitdirectly ontop ofanother. Arobot arm canpick up ablock and move\nit to another position, either on the table or on top of another block. The arm can pick up\nonlyoneblockatatime,soitcannotpickupablockthathasanotheroneonit. Thegoalwill\nalwaysbetobuildoneormorestacksofblocks, specified intermsofwhatblocksareontop\n2 TheblocksworldusedinplanningresearchismuchsimplerthanSHRDLU\u2019sversion,shownonpage20. Section10.1. DefinitionofClassicalPlanning 371\nInit(On(A,Table) \u2227 On(B,Table) \u2227 On(C,A)\n\u2227 Block(A) \u2227 Block(B) \u2227 Block(C) \u2227 Clear(B) \u2227 Clear(C))\nGoal(On(A,B) \u2227 On(B,C))\nAction(Move(b,x,y),\nPRECOND:On(b,x) \u2227 Clear(b) \u2227 Clear(y) \u2227 Block(b) \u2227 Block(y) \u2227\n(b(cid:7)=x) \u2227 (b(cid:7)=y) \u2227 (x(cid:7)=y),\nEFFECT:On(b,y) \u2227 Clear(x) \u2227 \u00acOn(b,x) \u2227 \u00acClear(y))\nAction(MoveToTable(b,x),\nPRECOND:On(b,x) \u2227 Clear(b) \u2227 Block(b) \u2227 (b(cid:7)=x),\nEFFECT:On(b,Table) \u2227 Clear(x) \u2227 \u00acOn(b,x))\nFigure10.3 Aplanningproblemintheblocksworld: buildingathree-blocktower. One\nsolutionisthesequence[MoveToTable(C,A),Move(B,Table,C),Move(A,Table,B)].\nA\nC B\nB A C\nStart State Goal State\nFigure10.4 Diagramoftheblocks-worldprobleminFigure10.3.\nof what other blocks. Forexample, a goal might be to get block A on B and block B on C\n(seeFigure10.4).\nWeuseOn(b,x)toindicate thatblock bisonx,where xiseitheranotherblock orthe\ntable. Theaction formoving block bfrom thetopof xtothe topofy willbeMove(b,x,y).\nNow,oneofthepreconditions onmovingbisthatnootherblockbeonit. Infirst-orderlogic,\nthis would be \u00ac\u2203x On(x,b) or, alternatively, \u2200x \u00acOn(x,b). Basic PDDL does not allow\nquantifiers, so instead we introduce a predicate Clear(x) that is true when nothing is on x.\n(Thecompleteproblem description isinFigure10.3.)\nTheactionMove movesablockbfrom xtoy ifbothbandy areclear. Afterthemove\nismade,bisstillclearbuty isnot. AfirstattemptattheMove schemais\nAction(Move(b,x,y),\nPRECOND:On(b,x)\u2227Clear(b)\u2227Clear(y),\nEFFECT:On(b,y)\u2227Clear(x)\u2227\u00acOn(b,x)\u2227\u00acClear(y)).\nUnfortunately, thisdoesnotmaintain Clear properly when xory isthetable. Whenxisthe\nTable, this action has the effect Clear(Table), but the table should not become clear; and\nwheny=Table,ithastheprecondition Clear(Table),butthetabledoesnothavetobeclear 372 Chapter 10. ClassicalPlanning\nfor us to move a block onto it. To fix this, we do two things. First, we introduce another\nactiontomoveablockbfromxtothetable:\nAction(MoveToTable(b,x),\nPRECOND:On(b,x)\u2227Clear(b),\nEFFECT:On(b,Table)\u2227Clear(x)\u2227\u00acOn(b,x)).\nSecond, we take the interpretation of Clear(x) to be \u201cthere is a clear space on x to hold a\nblock.\u201d Underthisinterpretation, Clear(Table)willalwaysbetrue. Theonlyproblemisthat\nnothing prevents the planner from using Move(b,x,Table) instead of MoveToTable(b,x).\nWecouldlivewiththisproblem\u2014itwillleadtoalarger-than-necessary searchspace,butwill\nnotleadtoincorrectanswers\u2014orwecouldintroducethepredicateBlock andaddBlock(b)\u2227\nBlock(y)totheprecondition ofMove.\n10.1.4 The complexityofclassicalplanning\nIn this subsection we consider the theoretical complexity of planning and distinguish two\ndecision problems. PlanSAT is the question of whether there exists any plan that solves a\nPLANSAT\nplanning problem. Bounded PlanSAT asks whether there is a solution of length k or less;\nBOUNDEDPLANSAT\nthiscanbeusedtofindanoptimalplan.\nThefirstresult isthat bothdecision problems aredecidable forclassical planning. The\nprooffollowsfromthefactthatthenumberofstatesisfinite. Butifweaddfunctionsymbols\nto the language, then the number of states becomes infinite, and PlanSAT becomes only\nsemidecidable: analgorithmexiststhatwillterminatewiththecorrectanswerforanysolvable\nproblem, but may not terminate on unsolvable problems. The Bounded PlanSAT problem\nremains decidable even in the presence of function symbols. For proofs of the assertions in\nthissection, seeGhallabetal.(2004).\nBothPlanSATandBounded PlanSATareinthecomplexity classPSPACE,aclassthat\nis larger (and hence more difficult) than NP and refers to problems that can be solved by a\ndeterministic Turing machine with a polynomial amount of space. Even if we make some\nrather severe restrictions, the problems remain quite difficult. For example, if we disallow\nnegative effects, both problems are still NP-hard. However, if we also disallow negative\npreconditions, PlanSATreduces totheclassP.\nThese worst-case results may seem discouraging. We can take solace in the fact that\nagents are usually not asked to find plans for arbitrary worst-case problem instances, but\nratherareaskedforplansinspecificdomains(suchasblocks-worldproblemswithnblocks),\nwhich can be much easier than the theoretical worst case. For many domains (including the\nblocks world and the air cargo world), Bounded PlanSAT is NP-complete while PlanSAT is\ninP;inotherwords,optimalplanningisusuallyhard,butsub-optimalplanningissometimes\neasy. To do well on easier-than-worst-case problems, we will need good search heuristics.\nThat\u2019s the true advantage of the classical planning formalism: it has facilitated the develop-\nment of very accurate domain-independent heuristics, whereas systems based on successor-\nstateaxiomsinfirst-orderlogichavehadlesssuccess incomingupwithgoodheuristics. Section10.2. AlgorithmsforPlanningasState-SpaceSearch 373\n10.2 ALGORITHMS FOR PLANNING AS STATE-SPACE SEARCH\nNowweturnourattentiontoplanningalgorithms. Wesawhowthedescription ofaplanning\nproblem defines a search problem: we can search from the initial state through the space\nof states, looking for a goal. One of the nice advantages of the declarative representation of\nactionschemasisthatwecanalsosearchbackwardfromthegoal,lookingfortheinitialstate.\nFigure10.5comparesforwardandbackwardsearches.\n10.2.1 Forward(progression) state-space search\nNowthatwehaveshownhowaplanning problem mapsintoasearch problem, wecansolve\nplanning problems with any of the heuristic search algorithms from Chapter 3 or a local\nsearch algorithm from Chapter 4 (provided we keep track of the actions used to reach the\ngoal). From the earliest days of planning research (around 1961) until around 1998 it was\nassumed that forward state-space search was too inefficient to be practical. It is not hard to\ncomeupwithreasonswhy.\nFirst, forward search is prone to exploring irrelevant actions. Consider the noble task\nof buying acopy of AI:A Modern Approach from an online bookseller. Suppose there is an\nAt(P , B)\n1\nAt(P , A)\nFly(P 1, A, B) 2\nAt(P , A)\n1\n(a)\nAt(P , A)\n2\nFly(P, A, B) At(P , A)\n2 1\nAt(P , B)\n2\nAt(P , A)\n1\nAt(P , B) Fly(P, A, B)\n2 1\nAt(P , B)\n(b) 1\nAt(P , B)\n2\nAt(P , B) Fly(P, A, B)\n1 2\nAt(P , A)\n2\nFigure 10.5 Two approaches to searching for a plan. (a) Forward (progression) search\nthrough the space of states, starting in the initial state and using the problem\u2019s actions to\nsearch forward for a member of the set of goal states. (b) Backward (regression) search\nthroughsetsofrelevantstates,startingatthesetofstatesrepresentingthegoalandusingthe\ninverseoftheactionstosearchbackwardfortheinitialstate. 374 Chapter 10. ClassicalPlanning\nactionschemaBuy(isbn)witheffectOwn(isbn). ISBNsare10digits,sothisactionschema\nrepresents 10 billion ground actions. An uninformed forward-search algorithm would have\ntostartenumerating these10billionactions tofindonethat leadstothegoal.\nSecond,planningproblemsoftenhavelargestatespaces. Consideranaircargoproblem\nwith10airports, whereeachairporthas5planesand20piecesofcargo. Thegoalistomove\nallthecargo atairport Atoairport B. Thereisasimplesolution totheproblem: loadthe20\npiecesofcargo intooneoftheplanes atA,flytheplanetoB,andunload thecargo. Finding\nthe solution can be difficult because the average branching factor is huge: each of the 50\nplanes can fly to 9 other airports, and each of the 200 packages can be either unloaded (if\nit is loaded) or loaded into any plane at its airport (if it is unloaded). So in any state there\nis a minimum of 450 actions (when all the packages are at airports with no planes) and a\nmaximumof10,450(whenallpackagesandplanesareatthesameairport). Onaverage,let\u2019s\nsaythereareabout2000possible actionsperstate,sothesearchgraphuptothedepthofthe\nobvioussolution hasabout200041 nodes.\nClearly, even this relatively small problem instance is hopeless without an accurate\nheuristic. Althoughmanyreal-worldapplications ofplanninghavereliedondomain-specific\nheuristics,itturnsout(asweseeinSection10.2.3)thatstrongdomain-independent heuristics\ncanbederivedautomatically; thatiswhatmakesforwardsearchfeasible.\n10.2.2 Backward (regression) relevant-states search\nIn regression search we start at the goal and apply the actions backward until we find a\nsequence ofstepsthatreaches theinitialstate. Itiscalled relevant-states search because we\nRELEVANT-STATES\nonly consider actions that arerelevant tothegoal (orcurrent state). Asinbelief-state search\n(Section4.4),thereisasetofrelevantstatestoconsiderateachstep,notjustasingle state.\nWestartwiththegoal,whichisaconjunctionofliteralsformingadescriptionofasetof\nstates\u2014forexample,thegoal\u00acPoor\u2227Famous describesthosestatesinwhichPoor isfalse,\nFamous is true, and any other fluent can have any value. If there are n ground fluents in a\ndomain,thenthereare 2n groundstates(eachfluentcanbetrueorfalse), but 3n descriptions\nofsetsofgoalstates(eachfluentcanbepositive, negative,ornotmentioned).\nIn general, backward search works only when we know how to regress from a state\ndescription to the predecessor state description. Forexample, it is hard to search backwards\nforasolutiontothen-queensproblembecausethereisnoeasywaytodescribethestatesthat\nareonemoveawayfromthe goal. Happily, the PDDL representation wasdesigned tomake\niteasytoregressactions\u2014ifadomaincanbeexpressedinPDDL,thenwecandoregression\nsearch onit. Givenaground goal description g and aground action a,theregression from g\n(cid:2)\noveragivesusastatedescription g definedby\ng(cid:2) = (g\u2212ADD(a))\u222aPrecond(a).\nThat is, the effects that were added by the action need not have been true before, and also\nthe preconditions must have held before, or else the action could not have been executed.\nNote that DEL(a) does not appear in the formula; that\u2019s because while we know the fluents\nin DEL(a) are no longer true after the action, we don\u2019t know whether or not they were true\nbefore, sothere\u2019snothing tobesaidaboutthem. Section10.2. AlgorithmsforPlanningasState-SpaceSearch 375\nTogetthefulladvantage ofbackwardsearch,weneedtodealwithpartiallyuninstanti-\natedactionsandstates,notjustgroundones. Forexample,supposethegoalistodeliveraspe-\n(cid:2)\ncificpieceofcargotoSFO:At(C ,SFO). ThatsuggeststheactionUnload(C ,p,SFO):\n2 2\n(cid:2)\nAction(Unload(C ,p,SFO),\n2\nPRECOND:In(C 2,p(cid:2) )\u2227At(p(cid:2) ,SFO)\u2227Cargo(C 2)\u2227Plane(p(cid:2) )\u2227Airport(SFO)\nEFFECT:At(C 2,SFO)\u2227\u00acIn(C 2,p(cid:2) ).\n(cid:2)\n(Notethatwehave standardized variable names(changing ptop inthiscase) sothatthere\nwill be no confusion between variable names if we happen to use the same action schema\ntwice in a plan. The same approach was used in Chapter 9 for first-order logical inference.)\nThis represents unloading the package from an unspecified plane at SFO;any plane will do,\nbut we need not say which one now. We can take advantage of the power of first-order\nrepresentations: asingledescription summarizesthepossibility ofusinganyoftheplanesby\n(cid:2)\nimplicitlyquantifying overp. Theregressedstatedescription is\ng(cid:2) = In(C ,p(cid:2) )\u2227At(p(cid:2) ,SFO)\u2227Cargo(C )\u2227Plane(p(cid:2) )\u2227Airport(SFO).\n2 2\nThefinalissueisdecidingwhichactionsarecandidates toregressover. Intheforwarddirec-\ntion wechose actions that were applicable\u2014those actions that could be the next step in the\nplan. Inbackward search wewantactions that are relevant\u2014those actions thatcould bethe\nRELEVANCE\nlaststepinaplanleadinguptothecurrent goalstate.\nForan action to be relevant to a goal it obviously must contribute to the goal: at least\noneoftheaction\u2019seffects(eitherpositiveornegative)mustunifywithanelementofthegoal.\nWhat is less obvious is that the action must not have any effect (positive or negative) that\nnegates an element of the goal. Now, if the goal is A\u2227B \u2227C and an action has the effect\nA\u2227B\u2227\u00acCthenthereisacolloquialsenseinwhichthatactionisveryrelevanttothegoal\u2014it\ngets ustwo-thirds ofthe waythere. Butitisnot relevant inthe technical sense defined here,\nbecause this action could not be the final step of a solution\u2014we would always need at least\nonemoresteptoachieve C.\nGiven the goal At(C ,SFO), several instantiations of Unload are relevant: we could\n2\nchose any specific plane to unload from, or we could leave the plane unspecified by using\n(cid:2)\ntheaction Unload(C ,p,SFO). Wecan reduce thebranching factorwithout ruling outany\n2\nsolutions byalways using theaction formed bysubstituting the mostgeneral unifierinto the\n(standardized) actionschema.\nAs another example, consider the goal Own(0136042597), given an initial state with\n10billionISBNs,andthesingleactionschema\nA = Action(Buy(i),PRECOND:ISBN(i),EFFECT:Own(i)).\nAs we mentioned before, forward search without a heuristic would have to start enumer-\nating the 10 billion ground Buy actions. But with backward search, we would unify the\n(cid:2)\ngoal Own(0136042597) with the (standardized) effect Own(i), yielding the substitution\n\u03b8 = {i(cid:2) \/0136042597}. Then we would regress over the action Subst(\u03b8,A(cid:2) ) to yield the\npredecessor state description ISBN(0136042597). This is part of, and thus entailed by, the\ninitialstate,sowearedone. 376 Chapter 10. ClassicalPlanning\nWe can make this more formal. Assume a goal description g which contains a goal\n(cid:2) (cid:2)\nliteralg andanactionschemaAthatisstandardized toproduce A. IfA hasaneffectliteral\ni\n(cid:2) (cid:2) (cid:2) (cid:2)\ne\nj\nwhere Unify(g i,e j)=\u03b8 and wherewedefine a = SUBST(\u03b8,A)andif there isnoeffect\n(cid:2) (cid:2)\nina thatisthenegationofaliteraling,thena isarelevantactiontowards g.\nBackwardsearch keepsthebranching factorlowerthan forwardsearch, formostprob-\nlem domains. However, the fact that backward search uses state sets rather than individual\nstates makes it harder to come up with good heuristics. That is the main reason why the\nmajorityofcurrentsystemsfavorforwardsearch.\n10.2.3 Heuristicsforplanning\nNeither forward nor backward search is efficient without a good heuristic function. Recall\nfrom Chapter 3 that a heuristic function h(s) estimates the distance from a state s to the\ngoal and that if we can derive an admissible heuristic for this distance\u2014one that does not\n\u2217\noverestimate\u2014then we can use A search to find optimal solutions. An admissible heuristic\ncan be derived by defining a relaxed problem that is easier to solve. The exact cost of a\nsolution tothiseasierproblem thenbecomestheheuristicfortheoriginal problem.\nBy definition, there is no way to analyze an atomic state, and thus it it requires some\ningenuity by a human analyst to define good domain-specific heuristics for search problems\nwith atomic states. Planning uses a factored representation for states and action schemas.\nThat makes it possible to define good domain-independent heuristics and for programs to\nautomatically applyagooddomain-independent heuristic foragivenproblem.\nThink of a search problem as a graph where the nodes are states and the edges are\nactions. The problem is to find a path connecting the initial state to a goal state. There are\ntwo ways we can relax this problem to make it easier: by adding more edges to the graph,\nmaking it strictly easier to find a path, or by grouping multiple nodes together, forming an\nabstraction ofthestatespacethathasfewerstates, andthusiseasiertosearch.\nWe look first at heuristics that add edges to the graph. For example, the ignore pre-\nIGNORE\nconditions heuristicdrops allpreconditions from actions. Everyaction becomes applicable\nPRECONDITIONS\nHEURISTIC\nin every state, and any single goal fluent can be achieved in one step (if there is an applica-\nble action\u2014if not, the problem is impossible). This almost implies that the number of steps\nrequired to solve the relaxed problem is the number of unsatisfied goals\u2014almost but not\nquite, because (1) some action may achieve multiple goals and (2) some actions may undo\ntheeffects ofothers. Formanyproblems anaccurate heuristic isobtained byconsidering (1)\nand ignoring (2). First, we relax the actions by removing all preconditions and all effects\nexcept those that are literals in the goal. Then, we count the minimum number of actions\nrequired such that the union of those actions\u2019 effects satisfies the goal. This is an instance\nSET-COVER of the set-cover problem. There is one minor irritation: the set-cover problem is NP-hard.\nPROBLEM\nFortunately a simple greedy algorithm is guaranteed to return a set covering whose size is\nwithin a factor of logn of the true minimum covering, where n is the number of literals in\nthegoal. Unfortunately, thegreedyalgorithm losestheguarantee ofadmissibility.\nItisalsopossibletoignoreonlyselectedpreconditionsofactions. Considerthesliding-\nblock puzzle (8-puzzle or 15-puzzle) from Section 3.2. We could encode this as a planning Section10.2. AlgorithmsforPlanningasState-SpaceSearch 377\nproblem involving tileswithasingleschema Slide:\nAction(Slide(t,s ,s ),\n1 2\nPRECOND:On(t,s 1)\u2227Tile(t)\u2227Blank(s 2)\u2227Adjacent(s 1,s 2)\nEFFECT:On(t,s 2)\u2227Blank(s 1)\u2227\u00acOn(t,s 1)\u2227\u00acBlank(s 2))\nAs we saw in Section 3.6, if we remove the preconditions Blank(s ) \u2227 Adjacent(s ,s )\n2 1 2\nthen any tile can move in one action to any space and we get the number-of-misplaced-tiles\nheuristic. IfweremoveBlank(s )thenwegettheManhattan-distance heuristic. Itiseasyto\n2\nsee how these heuristics could be derived automatically from the action schema description.\nTheeaseofmanipulatingtheschemasisthegreatadvantageofthefactoredrepresentation of\nplanning problems, ascomparedwiththeatomicrepresentation ofsearchproblems.\nIGNOREDELETE Another possibility is the ignore delete lists heuristic. Assume for a moment that all\nLISTS\ngoalsandpreconditions contain onlypositiveliterals3 Wewanttocreatearelaxedversionof\ntheoriginalproblemthatwillbeeasiertosolve,andwherethelengthofthesolutionwillserve\nasagoodheuristic. Wecandothatbyremovingthedeletelistsfromallactions(i.e.,removing\nallnegativeliteralsfromeffects). Thatmakesitpossibletomakemonotonicprogresstowards\nthegoal\u2014noactionwilleverundoprogressmadebyanotheraction. ItturnsoutitisstillNP-\nhardtofindtheoptimalsolution tothisrelaxed problem, but anapproximate solution canbe\nfound in polynomial time by hill-climbing. Figure 10.6 diagrams part of the state space for\ntwo planning problems using the ignore-delete-lists heuristic. The dots represent states and\nthe edges actions, and theheight ofeach dot abovethe bottom plane represents the heuristic\nvalue. States onthe bottom plane are solutions. Inboth these problems, there isawidepath\ntothegoal. Therearenodeadends,sononeedforbacktracking; asimplehillclimbingsearch\nwilleasilyfindasolution totheseproblems(although itmaynotbeanoptimalsolution).\nThe relaxed problems leave us with a simplified\u2014but still expensive\u2014planning prob-\nlemjusttocalculate thevalueoftheheuristic function. Manyplanning problems have 10100\nstates ormore, and relaxing the actions does nothing to reduce the number of states. There-\nfore, we now look at relaxations that decrease the number of states by forming a state ab-\nstraction\u2014a many-to-one mapping from states in the ground representation of the problem\nSTATEABSTRACTION\ntotheabstract representation.\nThe easiest form of state abstraction is to ignore some fluents. For example, consider\nan air cargo problem with 10 airports, 50 planes, and 200 pieces of cargo. Each plane can\nbe at one of 10 airports and each package can be either in one of the planes or unloaded at\none of theairports. Sothere are 5010 \u00d720050+10 \u2248 10155 states. Now consider a particular\nproblem inthat domaininwhichithappens thatallthepackages areatjust 5oftheairports,\nandallpackagesatagivenairporthavethesamedestination. Thenausefulabstractionofthe\nproblemistodropalltheAtfluentsexceptfortheonesinvolving oneplaneandonepackage\nat each of the 5 airports. Now there are only 510 \u00d755+10 \u2248 1017 states. A solution in this\nabstract state space will be shorter than a solution in the original space (and thus will be an\nadmissible heuristic), and the abstract solution is easy to extend to a solution to the original\nproblem (byaddingadditional Load andUnload actions).\n3 Manyproblemsarewrittenwiththisconvention. Forproblemsthataren\u2019t,replaceeverynegativeliteral\u00acP\ninagoalorpreconditionwithanewpositiveliteral,P(cid:3). 378 Chapter 10. ClassicalPlanning\nFigure10.6 Twostatespacesfromplanningproblemswiththeignore-delete-listsheuris-\ntic. Theheightabovethebottomplaneistheheuristicscoreofastate;statesonthebottom\nplanearegoals. Therearenolocalminima,sosearchforthegoalisstraightforward. From\nHoffmann(2005).\nAkeyideaindefiningheuristicsisdecomposition: dividingaproblemintoparts,solv-\nDECOMPOSITION\nSUBGOAL ing each part independently, and then combining the parts. The subgoal independence as-\nINDEPENDENCE\nsumption is that the cost of solving a conjunction of subgoals is approximated by the sum\nof the costs of solving each subgoal independently. The subgoal independence assumption\ncanbeoptimisticorpessimistic. Itisoptimisticwhentherearenegativeinteractions between\nthe subplans for each subgoal\u2014for example, when an action in one subplan deletes a goal\nachieved by another subplan. It is pessimistic, and therefore inadmissible, when subplans\ncontainredundantactions\u2014forinstance,twoactionsthatcouldbereplacedbyasingleaction\ninthemergedplan.\nSupposethegoalisasetoffluentsG,whichwedivideintodisjointsubsetsG ,...,G .\n1 n\nWethen findplans P ,...,P that solve therespective subgoals. Whatisanestimate ofthe\n1 n\ncostoftheplanforachievingallofG? WecanthinkofeachCost(P )asaheuristicestimate,\ni\nandweknowthatifwecombineestimatesbytakingtheirmaximumvalue,wealwaysgetan\nadmissible heuristic. So max iCOST(P i) is admissible, and sometimes it is exactly correct:\nit could be that P serendipitously achieves all the G . But in most cases, in practice the\n1 i\nestimateistoolow. Couldwesumthecostsinstead? Formanyproblemsthatisareasonable\nestimate,butitisnotadmissible. ThebestcaseiswhenwecandeterminethatG andG are\ni j\nindependent. IftheeffectsofP leaveallthepreconditions andgoalsofP unchanged, then\ni j\ntheestimate COST(P i)+COST(P j)isadmissible, andmoreaccuratethanthemaxestimate.\nWeshowinSection10.3.1thatplanning graphscanhelpprovidebetterheuristic estimates.\nItisclearthat thereisgreat potential forcutting downthe search space byforming ab-\nstractions. The trick is choosing the right abstractions and using them in a way that makes\nthetotalcost\u2014defininganabstraction, doinganabstractsearch,andmappingtheabstraction\nback to the original problem\u2014less than the cost of solving the original problem. The tech- Section10.3. PlanningGraphs 379\nniques of pattern databases from Section 3.6.3 can be useful, because the cost of creating\nthepatterndatabase canbeamortizedovermultipleproblem instances.\nAnexampleofasystemthatmakesuseofeffectiveheuristicsisFF,orFASTFORWARD\n(Hoffmann, 2005), a forward state-space searcher that uses the ignore-delete-lists heuristic,\nestimating the heuristic with the help of a planning graph (see Section 10.3). FF then uses\nhill-climbing search (modifiedtokeeptrack oftheplan)withtheheuristic tofindasolution.\nWhenithitsaplateauorlocalmaximum\u2014whennoactionleadstoastatewithbetterheuristic\nscore\u2014then FF uses iterative deepening search until it finds a state that is better, or it gives\nupandrestartshill-climbing.\n10.3 PLANNING GRAPHS\nAll of the heuristics we have suggested can suffer from inaccuracies. This section shows\nhow a special data structure called a planning graph can be used to give better heuristic\nPLANNINGGRAPH\nestimates. These heuristics can be applied to any of the search techniques we have seen so\nfar. Alternatively, wecansearch forasolution overthespace formedbytheplanning graph,\nusinganalgorithm called GRAPHPLAN.\nAplanningproblemasksifwecanreachagoalstatefromtheinitialstate. Supposewe\naregivenatreeofallpossible actions fromtheinitial statetosuccessorstates, andtheirsuc-\ncessors, andsoon. Ifweindexedthistreeappropriately, we couldanswertheplanning ques-\ntion \u201ccan wereach state G from state S \u201d immediately, just by looking it up. Ofcourse, the\n0\ntree is of exponential size, so this approach is impractical. A planning graph is polynomial-\nsize approximation to this tree that can be constructed quickly. The planning graph can\u2019t\nanswer definitively whether G is reachable from S , but it can estimate how many steps it\n0\ntakestoreachG. Theestimateisalwayscorrectwhenitreportsthegoalisnotreachable, and\nitneveroverestimates thenumberofsteps,soitisanadmissible heuristic.\nAplanninggraphisadirectedgraphorganizedinto levels: firstalevelS fortheinitial\nLEVEL 0\nstate,consisting ofnodesrepresenting eachfluentthatholdsinS ;thenalevelA consisting\n0 0\nof nodes for each ground action that might be applicable in S ; then alternating levels S\n0 i\nfollowedbyA ;untilwereachatermination condition (tobediscussed later).\ni\nRoughly speaking, S contains all the literals that could hold at time i, depending on\ni\ntheactions executed atpreceding timesteps. Ifitispossible thateither P or\u00acP could hold,\nthen both will be represented in S . Also roughly speaking, A contains all the actions that\ni i\ncould have their preconditions satisfied at time i. We say \u201croughly speaking\u201d because the\nplanning graph records only a restricted subset of the possible negative interactions among\nactions; therefore, aliteral mightshow upatlevel S whenactually itcould notbetrueuntil\nj\na later level, if at all. (A literal will never show up too late.) Despite the possible error, the\nlevel j at which a literal first appears is a good estimate of how difficult it is to achieve the\nliteralfromtheinitialstate.\nPlanning graphs work only for propositional planning problems\u2014ones with no vari-\nables. As we mentioned on page 368, it is straightforward to propositionalize a set of ac- 380 Chapter 10. ClassicalPlanning\nInit(Have(Cake))\nGoal(Have(Cake) \u2227 Eaten(Cake))\nAction(Eat(Cake)\nPRECOND:Have(Cake)\nEFFECT:\u00acHave(Cake) \u2227 Eaten(Cake))\nAction(Bake(Cake)\nPRECOND:\u00acHave(Cake)\nEFFECT:Have(Cake))\nFigure10.7 The\u201chavecakeandeatcaketoo\u201dproblem.\nS A S A S\n0 0 1 1 2\nBake(Cake)\nHave(Cake) Have(Cake) Have(Cake)\n\u00acHave(Cake) \u00acHave(Cake)\nEat(Cake) Eat(Cake)\nEaten(Cake) Eaten(Cake)\n\u00acEaten(Cake) \u00acEaten(Cake) \u00acEaten(Cake)\nFigure10.8 Theplanninggraphforthe\u201chavecakeandeatcaketoo\u201dproblemuptolevel\nS . Rectangles indicate actions (small squares indicate persistence actions), and straight\n2\nlinesindicatepreconditionsandeffects. Mutexlinksareshownascurvedgraylines. Notall\nmutexlinksareshown,becausethegraphwouldbetoocluttered. Ingeneral,iftwoliterals\nare mutex at Si, then the persistence actions for those literals will be mutex at Ai and we\nneednotdrawthatmutexlink.\ntion schemas. Despite the resulting increase inthe size ofthe problem description, planning\ngraphshaveprovedtobeeffectivetoolsforsolvinghardplanningproblems.\nFigure 10.7 shows a simple planning problem, and Figure 10.8 shows its planning\ngraph. Each action at level A is connected to its preconditions at S and its effects at S .\ni i i+1\nSo a literal appears because an action caused it, but we also want to say that a literal can\nPERSISTENCE persist ifnoaction negates it. Thisisrepresented bya persistence action (sometimes called\nACTION\nano-op). Foreveryliteral C,weaddtotheproblemapersistence actionwithprecondition C\nand effect C. Level A in Figure 10.8 shows one \u201creal\u201d action, Eat(Cake), along with two\n0\npersistence actionsdrawnassmallsquareboxes.\nLevel A contains all the actions that could occur in state S , but just as important it\n0 0\nrecordsconflictsbetweenactionsthatwouldpreventthemfromoccurringtogether. Thegray\nlinesinFigure10.8indicate mutualexclusion(ormutex)links. Forexample, Eat(Cake)is\nMUTUALEXCLUSION\nmutually exclusive with the persistence of either Have(Cake) or \u00acEaten(Cake). We shall\nMUTEX\nseeshortlyhowmutexlinksarecomputed.\nLevelS containsalltheliteralsthatcouldresultfrompickinganysubsetoftheactions\n1\nin A , as well as mutex links (gray lines) indicating literals that could not appear together,\n0\nregardless ofthechoiceofactions. Forexample, Have(Cake)andEaten(Cake)aremutex: Section10.3. PlanningGraphs 381\ndepending on the choice of actions in A , either, but not both, could be the result. In other\n0\nwords, S represents a belief state: a set of possible states. The members of this set are all\n1\nsubsetsoftheliteralssuchthatthereisnomutexlinkbetweenanymembersofthesubset.\nWecontinue inthisway,alternating betweenstatelevelS andaction levelA untilwe\ni i\nreach apoint wheretwoconsecutive levels areidentical. At thispoint, wesaythatthegraph\nhasleveledoff. ThegraphinFigure10.8levelsoffatS .\nLEVELEDOFF 2\nWhatweendupwithisastructurewhereevery A levelcontainsalltheactionsthatare\ni\napplicableinS ,alongwithconstraintssayingthattwoactionscannotbothbeexecutedatthe\ni\nsamelevel. Every S level contains allthe literals that could result from anypossible choice\ni\nof actions in A i\u22121, along with constraints saying which pairs of literals are not possible.\nIt is important to note that the process of constructing the planning graph does not require\nchoosingamongactions,whichwouldentailcombinatorialsearch. Instead,itjustrecordsthe\nimpossibility ofcertainchoices usingmutexlinks.\nWenowdefinemutexlinksforbothactionsandliterals. Amutexrelationholdsbetween\ntwoactions atagivenlevelifanyofthefollowingthreeconditions holds:\n\u2022 Inconsistent effects: oneactionnegatesaneffectoftheother. Forexample, Eat(Cake)\nand the persistence of Have(Cake) have inconsistent effects because they disagree on\ntheeffectHave(Cake).\n\u2022 Interference: one of the effects of one action is the negation of a precondition of the\nother. ForexampleEat(Cake)interfereswiththepersistenceofHave(Cake)bynegat-\ningitsprecondition.\n\u2022 Competing needs: one of the preconditions of one action is mutually exclusive with a\npreconditionoftheother. Forexample,Bake(Cake)andEat(Cake)aremutexbecause\ntheycompeteonthevalueoftheHave(Cake)precondition.\nAmutexrelationholdsbetweentwoliteralsatthesamelevelifoneisthenegationoftheother\nor if each possible pair of actions that could achieve the two literals is mutually exclusive.\nThis condition is called inconsistent support. Forexample, Have(Cake) and Eaten(Cake)\nare mutex in S because the only way of achieving Have(Cake), the persistence action, is\n1\nmutex with the only way of achieving Eaten(Cake), namely Eat(Cake). In S the two\n2\nliterals are not mutex, because there are new ways of achieving them, such as Bake(Cake)\nandthepersistence ofEaten(Cake),thatarenotmutex.\nA planning graph is polynomial in the size of the planning problem. For a planning\nproblem with l literals and a actions, each S has no more than l nodes and l2 mutex links,\ni\nand each A has nomore than a+l nodes (including theno-ops), (a+l)2 mutex links, and\ni\n2(al + l) precondition and effect links. Thus, an entire graph with n levels has a size of\nO(n(a+l)2). Thetimetobuildthegraphhasthesamecomplexity.\n10.3.1 Planning graphs forheuristicestimation\nAplanning graph,onceconstructed, isarichsource ofinformation abouttheproblem. First,\nifanygoalliteralfailstoappearinthefinallevelofthegraph,thentheproblemisunsolvable.\nSecond, we can estimate the cost of achieving any goal literal g from state s as the level at\ni\nwhich g first appears in the planning graph constructed from initial state s. Wecall this the\ni 382 Chapter 10. ClassicalPlanning\nlevelcostofg . InFigure10.8,Have(Cake)haslevelcost0andEaten(Cake)haslevelcost\nLEVELCOST i\n1. It is easy to show (Exercise 10.10) that these estimates are admissible for the individual\ngoals. The estimate might not always be accurate, however, because planning graphs allow\nseveral actions at each level, whereas the heuristic counts just the level and not the number\nSERIALPLANNING of actions. For this reason, it is common to use a serial planning graph for computing\nGRAPH\nheuristics. A serial graph insists that only one action can actually occur at any given time\nstep; thisisdonebyadding mutexlinks between everypairof nonpersistence actions. Level\ncostsextractedfromserialgraphsareoftenquitereasonable estimatesofactualcosts.\nToestimate the cost of a conjunction of goals, there are three simple approaches. The\nmax-levelheuristic simplytakesthemaximumlevelcostofanyofthegoals;thisisadmissi-\nMAX-LEVEL\nble,butnotnecessarily accurate.\nThe level sum heuristic, following the subgoal independence assumption, returns the\nLEVELSUM\nsum of the level costs of the goals; this can be inadmissible but works well in practice\nfor problems that are largely decomposable. It is much more accurate than the number-\nof-unsatisfied-goals heuristic from Section 10.2. For our problem, the level-sum heuristic\nestimate for the conjunctive goal Have(Cake)\u2227Eaten(Cake) will be 0+1 = 1, whereas\nthe correct answeris 2, achieved by the plan [Eat(Cake),Bake(Cake)]. That doesn\u2019t seem\nso bad. A more serious error is that if Bake(Cake) were not in the set of actions, then the\nestimatewouldstillbe1,wheninfacttheconjunctive goalwouldbeimpossible.\nFinally, the set-level heuristic findsthe level at which all the literals inthe conjunctive\nSET-LEVEL\ngoal appear in the planning graph without any pair of them being mutually exclusive. This\nheuristic gives the correct values of 2 for our original problem and infinity for the problem\nwithout Bake(Cake). It is admissible, it dominates the max-level heuristic, and it works\nextremelywellontasksinwhichthereisagooddealofinteraction amongsubplans. Itisnot\nperfect, ofcourse; forexample,itignoresinteractions amongthreeormoreliterals.\nAsatoolforgeneratingaccurateheuristics,wecanviewtheplanninggraphasarelaxed\nproblem that is efficiently solvable. To understand the nature of the relaxed problem, we\nneed to understand exactly what it means for a literal g to appear at level S in the planning\ni\ngraph. Ideally, wewould like itto beaguarantee that there exists aplan with iaction levels\nthat achieves g, and also that if g does not appear, there is no such plan. Unfortunately,\nmakingthatguaranteeisasdifficultassolvingtheoriginalplanningproblem. Sotheplanning\ngraph makes the second half of the guarantee (if g does not appear, there is no plan), but\nif g does appear, then all the planning graph promises is that there is a plan that possibly\nachieves g and has no \u201cobvious\u201d flaws. An obvious flaw is defined as a flaw that can be\ndetected by considering two actions or two literals at a time\u2014in other words, by looking at\nthemutexrelations. Therecould bemoresubtle flawsinvolving three, four, ormoreactions,\nbut experience has shown that it is not worth the computational effort to keep track of these\npossibleflaws. Thisissimilartoalessonlearnedfromconstraintsatisfactionproblems\u2014that\nitisoftenworthwhiletocompute2-consistencybeforesearchingforasolution,butlessoften\nworthwhiletocompute3-consistency orhigher. (Seepage211.)\nOneexampleofanunsolvableproblemthatcannotberecognizedassuchbyaplanning\ngraphistheblocks-world problem wherethegoalistogetblockAonB,B onC,andC on\nA. Thisisanimpossiblegoal;atowerwiththebottomontopofthetop. Butaplanninggraph Section10.3. PlanningGraphs 383\ncannot detect the impossibility, because any twoof the three subgoals are achievable. There\narenomutexesbetweenanypairofliterals,onlybetweenthe threeasawhole. Todetectthat\nthisproblem isimpossible, wewouldhavetosearchovertheplanning graph.\n10.3.2 The GRAPHPLAN algorithm\nThissubsectionshowshowtoextractaplandirectlyfromtheplanninggraph,ratherthanjust\nusing the graph to provide aheuristic. The GRAPHPLAN algorithm (Figure 10.9) repeatedly\nadds alevel to aplanning graph with EXPAND-GRAPH. Once all the goals show up as non-\nmutexinthegraph, GRAPHPLAN calls EXTRACT-SOLUTION tosearchforaplanthatsolves\nthe problem. If that fails, it expands another level and tries again, terminating with failure\nwhenthereisnoreasontogoon.\nfunctionGRAPHPLAN(problem)returnssolutionorfailure\ngraph\u2190INITIAL-PLANNING-GRAPH(problem)\ngoals\u2190CONJUNCTS(problem.GOAL)\nnogoods\u2190anemptyhashtable\nfortl =0to\u221edo\nifgoals allnon-mutexinStofgraph then\nsolution\u2190EXTRACT-SOLUTION(graph,goals,NUMLEVELS(graph),nogoods)\nifsolution (cid:7)=failure thenreturnsolution\nifgraph andnogoods havebothleveledoffthenreturnfailure\ngraph\u2190EXPAND-GRAPH(graph,problem)\nFigure10.9 The GRAPHPLAN algorithm. GRAPHPLAN calls EXPAND-GRAPH toadda\nleveluntileitherasolutionisfoundbyEXTRACT-SOLUTION,ornosolutionispossible.\nLetusnowtracetheoperationofGRAPHPLANonthesparetireproblemfrompage370.\nThe graph is shown in Figure 10.10. The first line of GRAPHPLAN initializes the planning\ngraph to a one-level (S ) graph representing the initial state. The positive fluents from the\n0\nproblem description\u2019s initial state areshown, asaretherelevant negative fluents. Notshown\naretheunchanging positiveliterals(suchasTire(Spare))andtheirrelevantnegativeliterals.\nThegoal At(Spare,Axle)isnotpresent in S 0,soweneed notcall EXTRACT-SOLUTION\u2014\nwearecertain that thereisnosolution yet. Instead, EXPAND-GRAPH adds into A\n0\nthethree\nactionswhosepreconditionsexistatlevelS (i.e.,alltheactionsexceptPutOn(Spare,Axle)),\n0\nalongwithpersistence actionsforalltheliteralsin S . Theeffectsoftheactionsareaddedat\n0\nlevelS 1. EXPAND-GRAPH thenlooksformutexrelations andaddsthemtothegraph.\nAt(Spare,Axle)isstillnotpresentinS 1,soagainwedonotcallEXTRACT-SOLUTION.\nWecall EXPAND-GRAPH again, adding A\n1\nand S\n1\nand giving us the planning graph shown\ninFigure10.10. Nowthatwehavethefullcomplementofactions, itisworthwhiletolookat\nsomeoftheexamplesofmutexrelations andtheircauses:\n\u2022 Inconsistent effects: Remove(Spare,Trunk) is mutex with LeaveOvernight because\nonehastheeffectAt(Spare,Ground)andtheotherhasitsnegation. 384 Chapter 10. ClassicalPlanning\nS A S A S\n0 0 1 1 2\nAt(Spare,Trunk) At(Spare,Trunk) At(Spare,Trunk)\nRemove(Spare,Trunk)\nRemove(Spare,Trunk)\n\u00acAt(Spare,Trunk) \u00acAt(Spare,Trunk)\nRemove(Flat,Axle) Remove(Flat,Axle)\nAt(Flat,Axle) At(Flat,Axle) At(Flat,Axle)\nLeaveOvernight \u00acAt(Flat,Axle) \u00acAt(Flat,Axle)\nLeaveOvernight\n\u00acAt(Spare,Axle) \u00acAt(Spare,Axle) \u00acAt(Spare,Axle)\nPutOn(Spare,Axle) At(Spare,Axle)\n\u00acAt(Flat,Ground) \u00acAt(Flat,Ground) \u00acAt(Flat,Ground)\nAt(Flat,Ground) At(Flat,Ground)\n\u00acAt(Spare,Ground) \u00acAt(Spare,Ground) \u00acAt(Spare,Ground)\nAt(Spare,Ground) At(Spare,Ground)\nFigure10.10 The planninggraphforthe spare tire problemafterexpansionto levelS .\n2\nMutexlinksareshownasgraylines. Notalllinksareshown,becausethegraphwouldbetoo\nclutteredifweshowedthemall. Thesolutionisindicatedbyboldlinesandoutlines.\n\u2022 Interference: Remove(Flat,Axle)ismutexwithLeaveOvernight becauseonehasthe\nprecondition At(Flat,Axle)andtheotherhasitsnegationasaneffect.\n\u2022 Competing needs: PutOn(Spare,Axle) is mutex with Remove(Flat,Axle) because\nonehasAt(Flat,Axle)asaprecondition andtheotherhasitsnegation.\n\u2022 Inconsistentsupport: At(Spare,Axle)ismutexwithAt(Flat,Axle)inS becausethe\n2\nonly way of achieving At(Spare,Axle) is by PutOn(Spare,Axle), and that is mutex\nwiththepersistence actionthatistheonlywayofachieving At(Flat,Axle). Thus,the\nmutexrelations detect theimmediate conflict that arises from trying toput twoobjects\ninthesameplaceatthesametime.\nThis time, when wego back to the start of the loop, all the literals from the goal are present\nin S , and none of them is mutex with any other. That means that a solution might exist,\n2\nand EXTRACT-SOLUTION will try to find it. We can formulate EXTRACT-SOLUTION as a\nBoolean constraint satisfaction problem (CSP) where the variables are the actions at each\nlevel,thevaluesforeachvariableareinoroutoftheplan,andtheconstraintsarethemutexes\nandtheneedtosatisfyeachgoalandprecondition.\nAlternatively,wecandefineEXTRACT-SOLUTIONasabackwardsearchproblem,where\neach stateinthesearch contains apointertoalevelintheplanning graph and asetofunsat-\nisfiedgoals. Wedefinethissearchproblem asfollows:\n\u2022 The initial state is the last level of the planning graph, S , along with the set of goals\nn\nfromtheplanning problem.\n\u2022 The actions available in a state at level S are to select any conflict-free subset of the\ni\nactions inA i\u22121 whose effects coverthe goals in thestate. Theresulting state has level\nS i\u22121 and has as its set of goals the preconditions for the selected set of actions. By\n\u201cconflictfree,\u201dwemeanasetofactionssuchthatnotwoofthemaremutexandnotwo\noftheirpreconditions aremutex. Section10.3. PlanningGraphs 385\n\u2022 ThegoalistoreachastateatlevelS suchthatallthegoalsaresatisfied.\n0\n\u2022 Thecostofeachactionis1.\nForthisparticularproblem,westartatS withthegoalAt(Spare,Axle). Theonlychoicewe\n2\nhaveforachievingthegoalsetisPutOn(Spare,Axle). ThatbringsustoasearchstateatS\n1\nwith goals At(Spare,Ground) and \u00acAt(Flat,Axle). The former can be achieved only by\nRemove(Spare,Trunk), and the latter by either Remove(Flat,Axle) or LeaveOvernight.\nButLeaveOvernight ismutexwithRemove(Spare,Trunk),sotheonlysolutionistochoose\nRemove(Spare,Trunk)andRemove(Flat,Axle). ThatbringsustoasearchstateatS with\n0\nthe goals At(Spare,Trunk) and At(Flat,Axle). Both of these are present in the state, so\nwe have a solution: the actions Remove(Spare, Trunk) and Remove(Flat, Axle) in level\nA ,followedbyPutOn(Spare,Axle)inA .\n0 1\nIn the case where EXTRACT-SOLUTION fails to find a solution for a set of goals at\na given level, we record the (level,goals) pair as a no-good, just as we did in constraint\nlearningforCSPs(page220). WheneverEXTRACT-SOLUTION iscalledagainwiththesame\nlevelandgoals, wecanfindtherecorded no-good andimmediately returnfailure ratherthan\nsearching again. Weseeshortlythatno-goods arealsousedinthetermination test.\nWeknow that planning is PSPACE-complete and that constructing the planning graph\ntakespolynomialtime,soitmustbethecasethatsolutionextractionisintractableintheworst\ncase. Therefore,wewillneedsomeheuristicguidanceforchoosingamongactionsduringthe\nbackward search. One approach that works well in practice is a greedy algorithm based on\nthelevelcostoftheliterals. Foranysetofgoals,weproceedinthefollowingorder:\n1. Pickfirsttheliteralwiththehighestlevelcost.\n2. Toachievethatliteral,preferactionswitheasierpreconditions. Thatis,chooseanaction\nsuchthatthesum(ormaximum)ofthelevelcostsofitspreconditions issmallest.\n10.3.3 Terminationof GRAPHPLAN\nSofar,wehaveskatedoverthequestionoftermination. HereweshowthatGRAPHPLANwill\ninfactterminateandreturnfailure whenthereisnosolution.\nThefirstthingtounderstand iswhywecan\u2019tstopexpanding thegraphassoonasithas\nleveled off. Consider an air cargo domain with one plane and n pieces of cargo at airport\nA, all of which have airport B as their destination. In this version of the problem, only one\npiece of cargo can fitinthe plane at atime. Thegraph willlevel offatlevel 4, reflecting the\nfactthatforanysinglepieceofcargo, wecanloadit,flyit,andunloaditatthedestination in\nthreesteps. Butthatdoesnotmeanthatasolution canbeextractedfromthegraphatlevel4;\ninfact asolution willrequire 4n\u22121steps: foreachpiece ofcargo weload, fly,and unload,\nandforallbutthelastpieceweneedtoflybacktoairport Atogetthenextpiece.\nHowlongdowehavetokeepexpandingafterthegraphhasleveledoff? Ifthefunction\nEXTRACT-SOLUTION fails to find a solution, then there must have been at least one set of\ngoals that were not achievable and were marked as a no-good. So if it is possible that there\nmight be fewer no-goods in the next level, then we should continue. As soon as the graph\nitself and the no-goods have both leveled off, withnosolution found, wecan terminate with\nfailurebecause thereisnopossibility ofasubsequent changethatcouldaddasolution. 386 Chapter 10. ClassicalPlanning\nNowallwehavetodoisprovethatthegraphandtheno-goodswillalwaysleveloff. The\nkeytothisproofisthatcertainproperties ofplanninggraphsaremonotonically increasingor\ndecreasing. \u201cXincreases monotonically\u201d meansthat theset ofXsatleveli+1isasuperset\n(notnecessarily proper)ofthesetatlevel i. Theproperties areasfollows:\n\u2022 Literals increase monotonically: Once a literal appears at a given level, it will appear\natallsubsequent levels. Thisisbecause ofthepersistence actions; oncealiteral shows\nup,persistence actionscauseittostayforever.\n\u2022 Actionsincrease monotonically: Onceanaction appears atagiven level, itwillappear\natall subsequent levels. Thisis aconsequence of the monotonic increase ofliterals; if\nthepreconditionsofanactionappearatonelevel,theywillappearatsubsequentlevels,\nandthussowilltheaction.\n\u2022 Mutexesdecreasemonotonically: IftwoactionsaremutexatagivenlevelA ,thenthey\ni\nwillalsobemutexforallpreviouslevelsatwhichtheybothappear. Thesameholdsfor\nmutexes between literals. It might not always appear that way in the figures, because\nthe figures have a simplification: they display neither literals that cannot hold at level\nS nor actions that cannot be executed at level A . We can see that \u201cmutexes decrease\ni i\nmonotonically\u201d istrueifyouconsiderthattheseinvisible literalsandactionsaremutex\nwitheverything.\nThe proof can be handled by cases: if actions A and B are mutex at level A , it\ni\nmust be because of one of the three types of mutex. The first two, inconsistent effects\nand interference, are properties of the actions themselves, so if the actions are mutex\nat A , they will be mutex at every level. The third case, competing needs, depends on\ni\nconditions at level S : that level must contain a precondition of A that is mutex with\ni\na precondition of B. Now, these two preconditions can be mutex if they are negations\nof each other (in which case they would be mutex in every level) or if all actions for\nachieving one aremutex withallactions forachieving the other. Butwealready know\nthat the available actions are increasing monotonically, so, by induction, the mutexes\nmustbedecreasing.\n\u2022 No-goods decrease monotonically: If a set of goals is not achievable at a given level,\nthentheyarenotachievableinanypreviouslevel. Theproofisbycontradiction: ifthey\nwere achievable at some previous level, then we could just add persistence actions to\nmakethemachievable atasubsequent level.\nBecause the actions and literals increase monotonically and because there are only a finite\nnumber of actions and literals, there must come a level that has the same number of actions\nandliteralsasthepreviouslevel. Becausemutexesandno-goodsdecrease,andbecausethere\ncan never be fewer than zero mutexes or no-goods, there must come a level that has the\nsame number of mutexes and no-goods as the previous level. Once agraph has reached this\nstate, then if one of the goals is missing oris mutex with another goal, then wecan stop the\nGRAPHPLAN algorithm and return failure. That concludes a sketch of the proof; for more\ndetailsseeGhallabetal.(2004). Section10.4. OtherClassicalPlanningApproaches 387\nYear Track WinningSystems(approaches)\n2008 Optimal GAMER(modelchecking,bidirectionalsearch)\n2008 Satisficing LAMA(fastdownwardsearchwithFFheuristic)\n2006 Optimal SATPLAN,MAXPLAN(Booleansatisfiability)\n2006 Satisficing SGPLAN(forwardsearch;partitionsintoindependentsubproblems)\n2004 Optimal SATPLAN(Booleansatisfiability)\n2004 Satisficing FASTDIAGONALLYDOWNWARD(forwardsearchwithcausalgraph)\n2002 Automated LPG(localsearch,planninggraphsconvertedtoCSPs)\n2002 Hand-coded TLPLAN(temporalactionlogicwithcontrolrulesforforwardsearch)\n2000 Automated FF(forwardsearch)\n2000 Hand-coded TALPLANNER(temporalactionlogicwithcontrolrulesforforwardsearch)\n1998 Automated IPP(planninggraphs);HSP(forwardsearch)\nFigure10.11 Someofthetop-performingsystemsintheInternationalPlanningCompe-\ntition. Eachyearthere are varioustracks: \u201cOptimal\u201dmeans the plannersmustproducethe\nshortestpossibleplan,while\u201cSatisficing\u201dmeansnonoptimalsolutionsareaccepted. \u201cHand-\ncoded\u201dmeansdomain-specificheuristicsareallowed;\u201cAutomated\u201dmeanstheyarenot.\n10.4 OTHER CLASSICAL PLANNING APPROACHES\nCurrentlythemostpopularandeffectiveapproaches tofullyautomated planning are:\n\u2022 TranslatingtoaBooleansatisfiability (SAT)problem\n\u2022 Forwardstate-space searchwithcarefully craftedheuristics (Section10.2)\n\u2022 Searchusingaplanning graph(Section10.3)\nThese three approaches are not the only ones tried inthe 40-year history of automated plan-\nning. Figure10.11showssomeofthetopsystemsintheInternationalPlanningCompetitions,\nwhichhavebeenheldeveryevenyearsince1998. Inthissectionwefirstdescribethetransla-\ntiontoasatisfiability problem andthendescribe threeotherinfluential approaches: planning\nasfirst-orderlogicaldeduction; asconstraint satisfaction; andasplanrefinement.\n10.4.1 Classicalplanning as Booleansatisfiability\nInSection7.7.4wesawhowSATPLANsolvesplanningproblemsthatareexpressedinpropo-\nsitional logic. Here we show how to translate a PDDL description into a form that can be\nprocessed by SATPLAN. Thetranslation isaseriesofstraightforward steps:\n\u2022 Propositionalize the actions: replace each action schema with a set of ground actions\nformedbysubstitutingconstantsforeachofthevariables. Thesegroundactionsarenot\npartofthetranslation, butwillbeusedinsubsequent steps.\n\u2022 Define the initial state: assert F0 for every fluent F in the problem\u2019s initial state, and\n\u00acF foreveryfluentnotmentioned intheinitialstate.\n\u2022 Propositionalize thegoal: foreveryvariableinthegoal,replacetheliteralsthatcontain\nthevariablewithadisjunction overconstants. Forexample,thegoalofhavingblockA 388 Chapter 10. ClassicalPlanning\nonanotherblock, On(A,x)\u2227Block(x)inaworldwithobjects A,B andC,wouldbe\nreplacedbythegoal\n(On(A,A)\u2227Block(A))\u2228(On(A,B)\u2227Block(B))\u2228(On(A,C)\u2227Block(C)).\n\u2022 Addsuccessor-state axioms: Foreachfluent F,addanaxiomoftheform\nFt+1 \u21d4 ActionCausesFt\u2228(Ft\u2227\u00acActionCausesNotFt),\nwhere ActionCausesF is a disjunction of all the ground actions that have F in their\naddlist, andActionCausesNotF isadisjunction ofalltheground actions thathave F\nintheirdeletelist.\n\u2022 Addprecondition axioms: Foreach ground action A, add the axiom At \u21d2 PRE(A)t,\nthatis,ifanactionistakenattimet,thenthepreconditions musthavebeentrue.\n\u2022 Addactionexclusion axioms: saythateveryactionisdistinctfromeveryotheraction.\nTheresulting translation isintheformthatwecanhandtoSATPLAN tofindasolution.\n10.4.2 Planning asfirst-order logicaldeduction: Situation calculus\nPDDLisalanguagethatcarefullybalancestheexpressiveness ofthelanguagewiththecom-\nplexity ofthealgorithms thatoperate onit. Butsomeproblemsremaindifficulttoexpress in\nPDDL. Forexample, we can\u2019t express the goal \u201cmove all the cargo from A to B regardless\nofhowmanypiecesofcargothereare\u201dinPDDL,butwecandoitinfirst-orderlogic,usinga\nuniversal quantifier. Likewise, first-orderlogiccanconcisely express global constraints such\nas\u201cnomorethanfourrobotscanbeinthesameplaceatthesame time.\u201d PDDLcanonlysay\nthiswithrepetitious preconditions oneverypossible actionthatinvolves amove.\nThe propositional logic representation of planning problems also has limitations, such\nas the fact that the notion of time is tied directly to fluents. For example, South2 means\n\u201cthe agent is facing south at time 2.\u201d With that representation, there is no way to say \u201cthe\nagentwouldbefacingsouth attime2ifitexecuted aright turnattime1;otherwiseitwould\nbe facing east.\u201d First-order logic lets us get around this limitation by replacing the notion\nof linear time with a notion of branching situations, using a representation called situation\nSITUATION calculusthatworkslikethis:\nCALCULUS\n\u2022 The initial state is called a situation. If s is a situation and a is an action, then\nSITUATION\nRESULT(s,a) is also a situation. There are no other situations. Thus, a situation cor-\nresponds to a sequence, or history, of actions. You can also think of a situation as the\nresultofapplyingtheactions,butnotethattwosituationsarethesameonlyiftheirstart\nand actions are the same: (RESULT(s,a) = RESULT(s(cid:2) ,a(cid:2) )) \u21d4 (s = s(cid:2) \u2227a = a(cid:2) ).\nSomeexamplesofactionsandsituations areshowninFigure10.12.\n\u2022 Afunctionorrelationthatcanvaryfromonesituationtothenextisafluent. Byconven-\ntion,thesituationsisalwaysthelastargumenttothefluent,forexampleAt(x,l,s)isa\nrelationalfluentthatistruewhenobjectxisatlocationlinsituations,andLocation isa\nfunctionalfluentsuchthatLocation(x,s) = lholdsinthesamesituationsasAt(x,l,s).\n\u2022 Each action\u2019s preconditions are described with a possibility axiom that says when the\nPOSSIBILITYAXIOM\naction can be taken. Ithas the form \u03a6(s) \u21d2 Poss(a,s)where \u03a6(s)issome formula Section10.4. OtherClassicalPlanningApproaches 389\nResult(Result(S, Forward),\n0\nTurn(Right))\nTurn(Right)\nResult(S, Forward)\n0\nForward\nS\n0\nFigure10.12 Situationsastheresultsofactionsinthewumpusworld.\ninvolving sthatdescribes thepreconditions. Anexamplefromthewumpusworldsays\nthatitispossible toshootiftheagentisaliveandhasanarrow:\nAlive(Agent,s)\u2227Have(Agent,Arrow,s) \u21d2 Poss(Shoot,s)\n\u2022 Each fluent is described with a successor-state axiom that says what happens to the\nfluent, depending on what action is taken. This is similar to the approach we took for\npropositional logic. Theaxiomhastheform\nActionispossible \u21d2\n(Fluentistrueinresultstate \u21d4 Action\u2019seffectmadeittrue\n\u2228Itwastruebeforeandactionleftitalone).\nForexample, theaxiom forthe relational fluent Holding says that the agent isholding\nsomegold g afterexecuting apossible action ifand only ifthe action wasaGrab ofg\noriftheagentwasalreadyholding g andtheactionwasnotreleasing it:\nPoss(a,s) \u21d2\n(Holding(Agent,g,Result(a,s)) \u21d4\na=Grab(g)\u2228(Holding(Agent,g,s)\u2227a(cid:7)= Release(g))).\nUNIQUEACTION \u2022 We need unique action axioms so that the agent can deduce that, for example, a (cid:7)=\nAXIOMS\nRelease(g). For each distinct pair of action names A and A we have an axiom that\ni j\nsaystheactionsaredifferent:\nA (x,...) (cid:7)= A (y,...)\ni j\nGold PIT\nPIT\nPIT\nGold PIT\nPIT\nPIT\nGold PIT\nPIT\nPIT 390 Chapter 10. ClassicalPlanning\nand for each action name A wehave an axiom that says two uses of that action name\ni\nareequalifandonlyifalltheirarguments areequal:\nA (x ,...,x )=A (y ,...,y ) \u21d4 x =y \u2227...\u2227x =y .\ni 1 n i 1 n 1 1 n n\n\u2022 Asolution isasituation (andhenceasequence ofactions) thatsatisfiesthegoal.\nWork in situation calculus has done a lot to define the formal semantics of planning and to\nopen up new areas of investigation. But so far there have not been any practical large-scale\nplanning programs based on logical deduction over the situation calculus. This is in part\nbecause of the difficulty of doing efficient inference in FOL, but is mainly because the field\nhasnotyetdevelopedeffectiveheuristics forplanning withsituation calculus.\n10.4.3 Planning asconstraint satisfaction\nWehaveseenthatconstraint satisfaction hasalotincommonwithBooleansatisfiability, and\nwehaveseenthatCSPtechniquesareeffectiveforschedulingproblems,soitisnotsurprising\nthatitispossibletoencodeaboundedplanningproblem(i.e.,theproblemoffindingaplanof\nlengthk)asaconstraint satisfaction problem (CSP).Theencoding issimilartotheencoding\nto a SAT problem (Section 10.4.1), with one important simplification: at each time step we\nneed only a single variable, Actiont, whose domain is the set of possible actions. We no\nlongerneed one variable forevery action, and wedon\u2019t need theaction exclusion axioms. It\nisalsopossibletoencodeaplanninggraphintoaCSP.ThisistheapproachtakenbyGP-CSP\n(DoandKambhampati, 2003).\n10.4.4 Planning asrefinement ofpartially ordered plans\nAlltheapproaches wehaveseensofarconstruct totallyorderedplansconsisting ofastrictly\nlinear sequences of actions. This representation ignores the fact that many subproblems are\nindependent. A solution to an air cargo problem consists of a totally ordered sequence of\nactions,yetif30packagesarebeingloadedontooneplaneinoneairportand50packagesare\nbeingloadedontoanotheratanotherairport,itseemspointlesstocomeupwithastrictlinear\nordering of80loadactions; thetwosubsetsofactions shouldbethought ofindependently.\nAn alternative is to represent plans as partially ordered structures: a plan is a set of\nactions and a set of constraints of the form Before(a ,a ) saying that one action occurs\ni j\nbeforeanother. InthebottomofFigure10.13,weseeapartiallyorderedplanthatisasolution\nto the spare tire problem. Actions are boxes and ordering constraints are arrows. Note that\nRemove(Spare,Trunk)andRemove(Flat,Axle)canbedoneineitherorderaslongasthey\narebothcompleted beforethe PutOn(Spare,Axle)action.\nPartially ordered plans are created by a search through the space of plans rather than\nthrough the state space. We start with the empty plan consisting of just the initial state and\nthegoal,withnoactionsinbetween,asinthetopofFigure10.13. Thesearchprocedurethen\nlooks for a flaw in the plan, and makes an addition to the plan to correct the flaw (or if no\nFLAW\ncorrection can be made, the search backtracks and tries something else). A flaw is anything\nthat keeps thepartial plan from being asolution. Forexample, oneflawintheemptyplan is\nthatnoactionachievesAt(Spare,Axle). Onewaytocorrecttheflawistoinsertintotheplan Section10.4. OtherClassicalPlanningApproaches 391\nAt(Spare,Trunk)\nStart At(Spare,Axle) Finish\nAt(Flat,Axle)\n(a)\nAt(Spare,Trunk) Remove(Spare,Trunk)\nAt(Spare,Trunk) At(Spare,Ground)\nStart PutOn(Spare,Axle) At(Spare,Axle) Finish\nAt(Flat,Axle) \u00acAt(Flat,Axle)\n(b)\nAt(Spare,Trunk) Remove(Spare,Trunk)\nAt(Spare,Trunk) At(Spare,Ground)\nStart PutOn(Spare,Axle) At(Spare,Axle) Finish\nAt(Flat,Axle) \u00acAt(Flat,Axle)\nAt(Flat,Axle) Remove(Flat,Axle)\n(c)\nFigure10.13 (a)thetireproblemexpressedasanemptyplan. (b)anincompletepartially\norderedplanforthetireproblem.Boxesrepresentactionsandarrowsindicatethatoneaction\nmustoccurbeforeanother.(c)acompletepartially-orderedsolution.\ntheactionPutOn(Spare,Axle). Ofcoursethatintroducessomenewflaws: thepreconditions\nof the new action are not achieved. The search keeps adding to the plan (backtracking if\nnecessary) until all flaws are resolved, as in the bottom of Figure 10.13. At every step, we\nmake the least commitment possible to fix the flaw. For example, in adding the action\nLEASTCOMMITMENT\nRemove(Spare,Trunk)weneedtocommittohaving itoccurbefore PutOn(Spare,Axle),\nbutwemakenoothercommitmentthatplacesitbeforeorafter otheractions. Iftherewerea\nvariableintheactionschemathatcouldbeleftunbound, wewoulddoso.\nIn the 1980s and 90s, partial-order planning was seen as the best way to handle plan-\nning problems with independent subproblems\u2014after all, it was the only approach that ex-\nplicitlyrepresentsindependent branchesofaplan. Ontheotherhand,ithasthedisadvantage\nof not having an explicit representation of states in the state-transition model. That makes\nsomecomputationscumbersome. By2000,forward-search plannershaddevelopedexcellent\nheuristics thatallowedthemtoefficientlydiscovertheindependent subproblems thatpartial-\norder planning was designed for. As a result, partial-order planners are not competitive on\nfullyautomated classical planning problems.\nHowever, partial-order planning remains an important part of the field. Forsome spe-\ncifictasks,suchasoperationsscheduling, partial-orderplanningwithdomainspecificheuris-\ntics is the technology of choice. Many of these systems use libraries of high-level plans, as\ndescribedinSection11.2. Partial-orderplanningisalsooftenusedindomainswhereitisim-\nportantforhumanstounderstand theplans. OperationalplansforspacecraftandMarsrovers\naregeneratedbypartial-orderplannersandarethencheckedbyhumanoperatorsbeforebeing\nuploaded tothevehicles forexecution. Theplan refinement approach makesiteasierforthe\nhumanstounderstandwhattheplanningalgorithmsaredoingandverifythattheyarecorrect. 392 Chapter 10. ClassicalPlanning\n10.5 ANALYSIS OF PLANNING APPROACHES\nPlanning combines the two major areas of AI we have covered so far: search and logic. A\nplanner can beseen either asaprogram that searches forasolution oras onethat (construc-\ntively) proves the existence ofa solution. Thecross-fertilization of ideas from the twoareas\nhas led both to improvements in performance amounting to several orders of magnitude in\nthe last decade and toan increased use of planners in industrial applications. Unfortunately,\nwe do not yet have a clear understanding of which techniques work best on which kinds of\nproblems. Quitepossibly, newtechniques willemergethatdominateexistingmethods.\nPlanning is foremost an exercise in controlling combinatorial explosion. If there are n\npropositions in a domain, then there are 2n states. As we have seen, planning is PSPACE-\nhard. Against such pessimism, the identification of independent subproblems can be apow-\nerfulweapon. Inthebestcase\u2014fulldecomposability oftheproblem\u2014wegetanexponential\nspeedup. Decomposability is destroyed, however, by negative interactions between actions.\nGRAPHPLANrecordsmutexestopointoutwherethedifficultinteractionsare. SATPLANrep-\nresents asimilarrange ofmutexrelations, but does sobyusing thegeneral CNFform rather\nthan a specific data structure. Forward search addresses the problem heuristically by trying\ntofind patterns (subsets ofpropositions) that covertheindependent subproblems. Since this\napproachisheuristic,itcanworkevenwhenthesubproblems arenotcompletelyindependent.\nSometimes it is possible to solve a problem efficiently by recognizing that negative\nSERIALIZABLE interactions canberuledout. Wesaythataproblem has serializable subgoalsifthereexists\nSUBGOAL\nan order of subgoals such that the planner can achieve them in that order without having to\nundo any of the previously achieved subgoals. Forexample, in the blocks world, if the goal\nis to build a tower(e.g., Aon B, which in turn is on C, which in turn is on the Table, as in\nFigure10.4onpage371),thenthesubgoalsareserializable bottomtotop: ifwefirstachieve\nC on Table, we will never have to undo it while we are achieving the other subgoals. A\nplanner that uses the bottom-to-top trick can solve any problem in the blocks world without\nbacktracking (although itmightnotalwaysfindtheshortest plan).\nAsa more complex example, for the Remote Agent planner that commanded NASA\u2019s\nDeep Space One spacecraft, it was determined that the propositions involved in command-\ning a spacecraft are serializable. This is perhaps not too surprising, because a spacecraft is\ndesigned by its engineers to be as easy as possible to control (subject to other constraints).\nTaking advantage of the serialized ordering of goals, the Remote Agent planner was able to\neliminate most of the search. This meant that it was fast enough to control the spacecraft in\nrealtime,something previously considered impossible.\nPlanners such as GRAPHPLAN, SATPLAN, and FF have moved the field of planning\nforward, by raising the level of performance of planning systems, by clarifying the repre-\nsentational and combinatorial issues involved, and by the development of useful heuristics.\nHowever,thereisaquestionofhowfarthesetechniqueswillscale. Itseemslikelythatfurther\nprogress on larger problems cannot rely only on factored and propositional representations,\nand will require some kind of synthesis of first-order and hierarchical representations with\ntheefficientheuristics currently inuse. Section10.6. Summary 393\n10.6 SUMMARY\nIn this chapter, we defined the problem of planning in deterministic, fully observable, static\nenvironments. We described the PDDL representation for planning problems and several\nalgorithmic approaches forsolving them. Thepointstoremember:\n\u2022 Planningsystemsareproblem-solving algorithmsthatoperateonexplicitpropositional\nor relational representations of states and actions. These representations make possi-\nble the derivation of effective heuristics and the development of powerful and flexible\nalgorithmsforsolvingproblems.\n\u2022 PDDL,thePlanningDomainDefinitionLanguage, describes theinitialandgoalstates\nasconjunctions ofliterals, andactionsintermsoftheirpreconditions andeffects.\n\u2022 State-space search can operate inthe forward direction (progression) orthe backward\ndirection (regression). Effective heuristics can be derived by subgoal independence\nassumptions andbyvarious relaxations oftheplanning problem.\n\u2022 Aplanninggraphcanbeconstructedincrementally,startingfromtheinitialstate. Each\nlayer contains a superset of all the literals or actions that could occur at that time step\nandencodesmutualexclusion(mutex)relationsamongliteralsoractionsthatcannotco-\noccur. Planninggraphsyieldusefulheuristicsforstate-spaceandpartial-orderplanners\nandcanbeuseddirectly inthe GRAPHPLAN algorithm.\n\u2022 Otherapproachesincludefirst-orderdeductionoversituationcalculusaxioms;encoding\na planning problem as a Boolean satisfiability problem or as a constraint satisfaction\nproblem;andexplicitlysearching through thespaceofpartially orderedplans.\n\u2022 Eachofthemajorapproaches toplanning hasits adherents, and there isasyetnocon-\nsensusonwhichisbest. Competitionandcross-fertilization amongtheapproacheshave\nresultedinsignificantgainsinefficiencyforplanning systems.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nAI planning arose from investigations into state-space search, theorem proving, and control\ntheoryandfromthepracticalneedsofrobotics,scheduling,andotherdomains. STRIPS(Fikes\nand Nilsson, 1971), the first majorplanning system, illustrates the interaction of these influ-\nences. STRIPS wasdesignedastheplanning componentofthesoftwarefortheShakeyrobot\nprojectatSRI.ItsoverallcontrolstructurewasmodeledonthatofGPS,theGeneralProblem\nSolver (Newell and Simon, 1961), a state-space search system that used means\u2013ends anal-\nysis. Bylander (1992) shows simple STRIPS planning to be PSPACE-complete. Fikes and\nNilsson (1993) give a historical retrospective on the STRIPS project and its relationship to\nmorerecentplanning efforts.\nThe representation language used by STRIPS has been far more influential than its al-\ngorithmic approach; what we call the \u201cclassical\u201d language is close to what STRIPS used. 394 Chapter 10. ClassicalPlanning\nThe Action Description Language, or ADL (Pednault, 1986), relaxed some of the STRIPS\nrestrictions and made it possible to encode more realistic problems. Nebel (2000) explores\nschemes for compiling ADL into STRIPS. The Problem Domain Description Language, or\nPDDL(Ghallabetal.,1998),wasintroducedasacomputer-parsable, standardizedsyntaxfor\nrepresenting planning problems and has been used as the standard language for the Interna-\ntionalPlanningCompetitionsince1998. Therehavebeenseveralextensions; themostrecent\nversion, PDDL 3.0,includes planconstraints andpreferences (GereviniandLong,2005).\nPlannersintheearly1970sgenerallyconsideredtotallyorderedactionsequences. Prob-\nlemdecompositionwasachievedbycomputingasubplanforeachsubgoalandthenstringing\nthe subplans together in some order. This approach, called linear planning by Sacerdoti\nLINEARPLANNING\n(1975), was soon discovered to be incomplete. It cannot solve some very simple problems,\nsuchastheSussmananomaly(seeExercise10.7), found byAllenBrownduring experimen-\ntation withthe HACKER system (Sussman, 1975). Acomplete plannermustallowfor inter-\nleavingofactionsfromdifferentsubplanswithinasinglesequence. Thenotionofserializable\nINTERLEAVING\nsubgoals (Korf, 1987) corresponds exactly to the set of problems for which noninterleaved\nplanners arecomplete.\nOne solution to the interleaving problem was goal-regression planning, a technique in\nwhich steps in a totally ordered plan are reordered so as to avoid conflict between subgoals.\nThis was introduced by Waldinger (1975) and also used by Warren\u2019s (1974) WARPLAN.\nWARPLAN is also notable in that it was the first planner to be written in a logic program-\nminglanguage (Prolog) andisoneofthebestexamples oftheremarkable economy thatcan\nsometimes be gained withlogic programming: WARPLAN isonly 100 lines ofcode, asmall\nfractionofthesizeofcomparable planners ofthetime.\nThe ideas underlying partial-order planning include the detection of conflicts (Tate,\n1975a) and the protection of achieved conditions from interference (Sussman, 1975). The\nconstruction of partially ordered plans (then called task networks) was pioneered by the\nNOAHplanner(Sacerdoti, 1975,1977)andbyTate\u2019s(1975b, 1977) NONLIN system.\nPartial-order planning dominated the next 20 years of research, yet the first clear for-\nmal exposition was TWEAK (Chapman, 1987), a planner that was simple enough to allow\nproofs of completeness and intractability (NP-hardness and undecidability) of various plan-\nning problems. Chapman\u2019s work led to a straightforward description of a complete partial-\norderplanner (McAllesterand Rosenblitt, 1991), then tothewidely distributed implementa-\ntions SNLP (Soderland andWeld, 1991) and UCPOP (Penberthy and Weld, 1992). Partial-\norder planning fell out of favor in the late 1990s as faster methods emerged. Nguyen and\nKambhampati (2001) suggest that a reconsideration is merited: with accurate heuristics de-\nrived from aplanning graph, their REPOP planner scales up muchbetter than GRAPHPLAN\ninparallelizable domainsandiscompetitivewiththefasteststate-space planners.\nThe resurgence of interest in state-space planning was pioneered by Drew McDer-\nmott\u2019sUNPOPprogram(1996),whichwasthefirsttosuggesttheignore-delete-listheuristic,\nThe name UNPOP was a reaction to the overwhelming concentration on partial-order plan-\nning at the time; McDermott suspected that other approaches were not getting the attention\nthey deserved. Bonet and Geffner\u2019s Heuristic Search Planner (HSP) and its later deriva-\ntives (Bonet and Geffner, 1999; Haslum et al., 2005; Haslum, 2006) were the first to make Bibliographical andHistorical Notes 395\nstate-space search practical for large planning problems. HSP searches in the forward di-\nrection while HSPR (Bonet and Geffner, 1999) searches backward. The most successful\nstate-space searcher todate is FF (Hoffmann, 2001; Hoffmann and Nebel, 2001; Hoffmann,\n2005), winner of the AIPS 2000 planning competition. FASTDOWNWARD (Helmert, 2006)\nis a forward state-space search planner that preprocesses the action schemas into an alter-\nnativerepresentation whichmakessomeoftheconstraints moreexplicit. FASTDOWNWARD\n(HelmertandRichter,2004;Helmert,2006)wonthe2004planningcompetition,andLAMA\n(Richter and Westphal, 2008), a planner based on FASTDOWNWARD with improved heuris-\ntics,wonthe2008competition.\nBylander (1994) and Ghallab et al. (2004) discuss the computational complexity of\nseveralvariantsoftheplanningproblem. Helmert(2003)provescomplexityboundsformany\nofthestandard benchmark problems, andHoffmann (2005) analyzes thesearch space ofthe\nignore-delete-list heuristic. Heuristicsfortheset-covering problem arediscussed byCaprara\net al. (1995) for scheduling operations of the Italian railway. Edelkamp (2009) and Haslum\net al. (2007) describe how to construct pattern databases for planning heuristics. As we\nmentionedinChapter3,Felneretal.(2004)showencouragingresultsusingpatterndatabases\nforslidingblockspuzzles,whichcanbethoughtofasaplanningdomain,butHoffmannetal.\n(2006)showsomelimitations ofabstraction forclassicalplanning problems.\nAvrimBlumandMerrickFurst(1995, 1997)revitalized thefieldofplanningwiththeir\nGRAPHPLANsystem,whichwasordersofmagnitudefasterthanthepartial-orderplannersof\nthe time. Othergraph-planning systems, such as IPP (Koehler etal., 1997), STAN (Fox and\nLong,1998),andSGP(Weldetal.,1998),soonfollowed. Adatastructurecloselyresembling\ntheplanninggraphhadbeendevelopedslightlyearlierbyGhallabandLaruelle(1994),whose\nIXTET partial-order planner used it to derive accurate heuristics to guide searches. Nguyen\net al. (2001) thoroughly analyze heuristics derived from planning graphs. Ourdiscussion of\nplanning graphs is based partly on this work and on lecture notes and articles by Subbarao\nKambhampati (Bryce and Kambhampati, 2007). As mentioned in the chapter, a planning\ngraph can be used in many different ways to guide the search for a solution. The winner\nof the 2002 AIPS planning competition, LPG (Gerevini and Serina, 2002, 2003), searched\nplanning graphsusingalocalsearchtechnique inspired by WALKSAT.\nThesituation calculus approach toplanning wasintroduced byJohnMcCarthy(1963).\nTheversionweshowherewasproposed byRayReiter(1991, 2001).\nKautzet al. (1996) investigated various ways to propositionalize action schemas, find-\ning that the most compact forms did not necessarily lead to the fastest solution times. A\nsystematic analysis was carried out by Ernst et al. (1997), who also developed an auto-\nmatic \u201ccompiler\u201d for generating propositional representations from PDDL problems. The\nBLACKBOX planner, which combines ideas from GRAPHPLAN and SATPLAN, was devel-\noped by Kautz and Selman (1998). CPLAN, a planner based on constraint satisfaction, was\ndescribed byvanBeekandChen(1999).\nMostrecently, therehasbeen interest intherepresentation ofplans asbinarydecision\nBINARYDECISION diagrams, compact data structures for Boolean expressions widely studied in the hardware\nDIAGRAM\nverificationcommunity(ClarkeandGrumberg,1987;McMillan,1993). Therearetechniques\nforprovingpropertiesofbinarydecisiondiagrams,includingthepropertyofbeingasolution 396 Chapter 10. ClassicalPlanning\ntoaplanning problem. Cimattietal. (1998) present aplannerbased onthisapproach. Other\nrepresentations have also been used; for example, Vossen et al. (2001) survey the use of\nintegerprogrammingforplanning.\nThe jury is still out, but there are now some interesting comparisons of the various\napproaches to planning. Helmert (2001) analyzes several classes of planning problems, and\nshowsthatconstraint-basedapproachessuchasGRAPHPLANandSATPLANarebestforNP-\nhard domains, while search-based approaches do better in domains where feasible solutions\ncan be found without backtracking. GRAPHPLAN and SATPLAN have trouble in domains\nwith many objects because that means they must create many actions. In some cases the\nproblem can bedelayed oravoided bygenerating thepropositionalized actions dynamically,\nonlyasneeded, ratherthaninstantiating themallbeforethesearchbegins.\nReadings in Planning (Allen et al., 1990) is a comprehensive anthology of early work\nin the field. Weld (1994, 1999) provides two excellent surveys of planning algorithms of\nthe 1990s. It is interesting to see the change in the five years between the two surveys:\nthe first concentrates on partial-order planning, and the second introduces GRAPHPLAN and\nSATPLAN. AutomatedPlanning(Ghallabetal.,2004)isanexcellenttextbookonallaspects\nof planning. LaValle\u2019s text Planning Algorithms (2006) covers both classical and stochastic\nplanning, withextensivecoverageofrobotmotionplanning.\nPlanningresearchhasbeencentraltoAIsinceitsinception, andpapersonplanningare\na staple of mainstream AI journals and conferences. There are also specialized conferences\nsuchastheInternational ConferenceonAIPlanningSystems,theInternationalWorkshopon\nPlanningandScheduling forSpace,andtheEuropeanConference onPlanning.\nEXERCISES\n10.1 Describethedifferencesandsimilaritiesbetweenproblem solvingandplanning.\n10.2 GiventheactionschemasandinitialstatefromFigure10.1,whatarealltheapplicable\nconcrete instances ofFly(p,from,to)inthestatedescribed by\nAt(P ,JFK)\u2227At(P ,SFO)\u2227Plane(P )\u2227Plane(P )\n1 2 1 2\n\u2227Airport(JFK)\u2227Airport(SFO)?\n10.3 The monkey-and-bananas problem is faced by a monkey in a laboratory with some\nbananashangingoutofreachfromtheceiling. Aboxisavailablethatwillenablethemonkey\ntoreachthebananasifheclimbsonit. Initially,themonkeyisatA,thebananasatB,andthe\nbox at C. The monkey and box have height Low, but if the monkey climbs onto the box he\nwillhaveheightHigh,thesameasthebananas. Theactions available tothemonkeyinclude\nGo from one place to another, Push an object from one place to another, ClimbUp onto or\nClimbDown fromanobject, and Grasp orUngrasp anobject. Theresult ofaGrasp isthat\nthemonkeyholdstheobjectifthemonkeyandobjectareinthesameplaceatthesameheight.\na. Writedowntheinitialstatedescription. Exercises 397\nSwitch 4\nDoor 4\nRoom 4\nSwitch 3\nDoor 3\nRoom 3\nShakey\nCorridor\nSwitch 2\nDoor 2\nRoom 2\nSwitch 1\nBox 3\nBox 2\nDoor 1\nRoom 1\nBox 4\nBox 1\nFigure10.14 Shakey\u2019sworld. Shakeycanmovebetweenlandmarkswithina room,can\npassthroughthedoorbetweenrooms,canclimbclimbableobjectsandpushpushableobjects,\nandcanfliplightswitches.\nb. Writethesixactionschemas.\nc. Suppose the monkey wants to fool the scientists, who are off to tea, by grabbing the\nbananas, but leaving the box inits original place. Writethis as ageneral goal (i.e., not\nassumingthattheboxisnecessarilyatC)inthelanguageofsituationcalculus. Canthis\ngoalbesolvedbyaclassical planningsystem?\nd. Your schema for pushing is probably incorrect, because if the object is too heavy, its\npositionwillremainthesamewhenthePushschemaisapplied. Fixyouractionschema\ntoaccountforheavyobjects.\n10.4 The original STRIPS planner was designed to control Shakey the robot. Figure 10.14\nshowsaversionofShakey\u2019s worldconsisting offourroomslinedupalongacorridor, where\neachroomhasadoorandalightswitch. TheactionsinShakey\u2019sworldincludemovingfrom\nplace toplace, pushing movable objects (such asboxes), climbing ontoanddownfromrigid 398 Chapter 10. ClassicalPlanning\nobjects(suchasboxes),andturninglightswitchesonandoff. Therobotitselfcouldnotclimb\nonaboxortoggleaswitch,buttheplannerwascapableoffindingandprintingoutplansthat\nwerebeyondtherobot\u2019sabilities. Shakey\u2019ssixactionsare thefollowing:\n\u2022 Go(x,y,r), which requires that Shakey be At x and that x and y are locations In the\nsameroom r. Byconvention adoorbetweentworoomsisinbothofthem.\n\u2022 Pushaboxbfromlocation xtolocationywithinthesameroom: Push(b,x,y,r). You\nwillneedthepredicate Box andconstants fortheboxes.\n\u2022 Climbontoaboxfrom position x: ClimbUp(x,b); climbdownfrom aboxtoposition\nx: ClimbDown(b,x). Wewillneedthepredicate On andtheconstant Floor.\n\u2022 Turn a light switch on oroff: TurnOn(s,b); TurnOff(s,b). To turn a light on oroff,\nShakeymustbeontopofaboxatthelightswitch\u2019slocation.\nWritePDDLsentences forShakey\u2019ssixactions andtheinitialstatefromFigure10.14. Con-\nstructaplanforShakeytoget Box intoRoom .\n2 2\n10.5 AfiniteTuringmachinehasafiniteone-dimensionaltapeofcells,eachcellcontaining\none of a finite number of symbols. One cell has a read and write head above it. There is a\nfinitesetofstates themachine canbein, oneofwhichistheaccept state. Ateach timestep,\ndependingonthesymbolonthecellundertheheadandthemachine\u2019scurrentstate,thereare\nasetofactions wecanchoose from. Eachaction involves writingasymbol tothecellunder\nthe head, transitioning the machine to a state, and optionally moving the head left or right.\nThe mapping that determines which actions are allowed is the Turing machine\u2019s program.\nYourgoalistocontrolthemachineintotheacceptstate.\nRepresent the Turing machine acceptance problem as a planning problem. If you can\ndothis,itdemonstratesthatdeterminingwhetheraplanningproblemhasasolutionisatleast\nashardastheTuringacceptance problem,whichisPSPACE-hard.\n10.6 Explain why dropping negative effects from every action schema in a planning prob-\nlemresultsinarelaxedproblem.\n10.7 Figure 10.4 (page 371) shows a blocks-world problem that is known as the Sussman\nanomaly. The problem was considered anomalous because the noninterleaved planners of\nSUSSMANANOMALY\nthe early 1970s could not solve it. Write a definition of the problem and solve it, either by\nhandorwithaplanningprogram. Anoninterleaved planneris aplannerthat,whengiventwo\nsubgoals G and G , produces either aplan for G concatenated with aplan for G , orvice\n1 2 1 2\nversa. Explainwhyanoninterleaved plannercannotsolvethisproblem.\n10.8 ProvethatbackwardsearchwithPDDLproblemsiscomplete.\n10.9 Constructlevels0,1,and2oftheplanninggraphfortheprobleminFigure10.1.\n10.10 Provethefollowingassertions aboutplanning graphs:\na. Aliteralthatdoesnotappearinthefinallevelofthegraphcannotbeachieved. Exercises 399\nb. Thelevelcostofaliteralinaserialgraphisnogreaterthantheactualcostofanoptimal\nplanforachievingit.\n10.11 The set-level heuristic (see page 382) uses a planning graph to estimate the cost of\nachieving a conjunctive goal from the current state. What relaxed problem is the set-level\nheuristic thesolutionto?\n10.12 Examinethedefinitionofbidirectionalsearch inChapter3.\na. Wouldbidirectional state-space searchbeagoodideaforplanning?\nb. Whataboutbidirectional searchinthespaceofpartial-order plans?\nc. Deviseaversionofpartial-orderplanninginwhichanactioncanbeaddedtoaplanifits\npreconditionscanbeachievedbytheeffectsofactionsalreadyintheplan. Explainhow\ntodeal with conflicts and ordering constraints. Is the algorithm essentially identical to\nforwardstate-space search?\n10.13 We contrasted forward and backward state-space searchers with partial-order plan-\nners,sayingthatthelatterisaplan-spacesearcher. Explainhowforwardandbackwardstate-\nspace search can also be considered plan-space searchers, and say what the plan refinement\noperators are.\n10.14 Uptonowwehaveassumedthattheplanswecreatealwaysmakesurethatanaction\u2019s\npreconditions aresatisfied. Letusnowinvestigate whatpropositional successor-state axioms\nsuch as HaveArrowt+1 \u21d4 (HaveArrowt \u2227 \u00acShoott) have to say about actions whose\npreconditions arenotsatisfied.\na. Showthat the axioms predict that nothing will happen when anaction isexecuted in a\nstatewhereitspreconditions arenotsatisfied.\nb. Consider aplan pthat contains the actions required toachieve agoal but also includes\nillegalactions. Isitthecasethat\ninitialstate\u2227successor-state axioms\u2227p |= goal?\nc. Withfirst-ordersuccessor-state axiomsinsituation calculus, isitpossible toprovethat\naplancontaining illegalactionswillachievethegoal?\n10.15 Consider how to translate aset of action schemas into the successor-state axioms of\nsituation calculus.\na. Consider the schema for Fly(p,from,to). Write a logical definition for the predicate\nPoss(Fly(p,from,to),s), which is true if the preconditions for Fly(p,from,to) are\nsatisfiedinsituation s.\nb. Next, assuming that Fly(p,from,to) is the only action schema available to the agent,\nwrite down a successor-state axiom for At(p,x,s) that captures the same information\nastheactionschema. 400 Chapter 10. ClassicalPlanning\nc. Now suppose there is an additional method of travel: Teleport(p,from,to). It has\ntheadditionalprecondition \u00acWarped(p)andtheadditionaleffectWarped(p). Explain\nhowthesituation calculus knowledgebasemustbemodified.\nd. Finally, develop a general and precisely specified procedure forcarrying out the trans-\nlationfromasetofactionschemastoasetofsuccessor-state axioms.\n10.16 In the SATPLAN algorithm in Figure 7.22 (page 272), each call to the satisfiabil-\nity algorithm asserts a goal gT, where T ranges from 0 to T . Suppose instead that the\nmax\nsatisfiability algorithm iscalledonlyonce,withthegoal g0\u2228g1\u2228\u00b7\u00b7\u00b7\u2228gTmax.\na. Willthisalwaysreturnaplanifoneexistswithlengthless thanorequaltoT ?\nmax\nb. Doesthisapproach introduce anynewspurious \u201csolutions\u201d?\nc. Discuss how one might modify a satisfiability algorithm such as WALKSAT so that it\nfindsshortsolutions (iftheyexist)whengivenadisjunctive goalofthisform. 11\nPLANNING AND ACTING\nIN THE REAL WORLD\nInwhichweseehow moreexpressive representations and moreinteractive agent\narchitectures leadtoplanners thatareusefulintherealworld.\nThepreviouschapterintroduced themostbasicconcepts, representations, andalgorithmsfor\nplanning. Planners that are are used in the real world for planning and scheduling the oper-\nations of spacecraft, factories, and military campaigns are more complex; they extend both\nthe representation language and the way the planner interacts with the environment. This\nchapter shows how. Section 11.1 extends the classical language for planning to talk about\nactions with durations and resource constraints. Section 11.2 describes methods for con-\nstructing plans thatareorganized hierarchically. Thisallowshuman experts tocommunicate\ntotheplannerwhattheyknowabouthowtosolvetheproblem. Hierarchyalsolendsitselfto\nefficientplanconstructionbecausetheplannercansolveaproblematanabstractlevelbefore\ndelvingintodetails. Section11.3presentsagentarchitectures thatcanhandleuncertain envi-\nronments and interleave deliberation withexecution, andgivessomeexamples ofreal-world\nsystems. Section11.4showshowtoplanwhentheenvironment containsotheragents.\n11.1 TIME, SCHEDULES, AND RESOURCES\nTheclassicalplanningrepresentationtalksaboutwhattodo,andinwhatorder,buttherepre-\nsentation cannot talkabout time: howlong anaction takes andwhenitoccurs. Forexample,\ntheplanners ofChapter10couldproduceascheduleforanairlinethatsayswhichplanesare\nassignedtowhichflights,butwereallyneedtoknowdepartureandarrivaltimesaswell. This\nisthesubjectmatterofscheduling. Therealworldalsoimposesmanyresourceconstraints;\nforexample, anairlinehasalimitednumberofstaff\u2014and staffwhoareononeflightcannot\nbe on another at the same time. This section covers methods for representing and solving\nplanning problemsthatincludetemporalandresource constraints.\nThe approach we take in this section is \u201cplan first, schedule later\u201d: that is, we divide\nthe overall problem into a planning phase in which actions are selected, with some ordering\nconstraints, to meet the goals of the problem, and a later scheduling phase, in which tempo-\nralinformation isaddedtotheplantoensure thatitmeetsresource anddeadline constraints.\n401 402 Chapter 11. PlanningandActingintheRealWorld\nJobs({AddEngine1 \u227aAddWheels1 \u227aInspect1},\n{AddEngine2 \u227aAddWheels2 \u227aInspect2})\nResources(EngineHoists(1),WheelStations(1),Inspectors(2),LugNuts(500))\nAction(AddEngine1,DURATION:30,\nUSE:EngineHoists(1))\nAction(AddEngine2,DURATION:60,\nUSE:EngineHoists(1))\nAction(AddWheels1,DURATION:30,\nCONSUME:LugNuts(20),USE:WheelStations(1))\nAction(AddWheels2,DURATION:15,\nCONSUME:LugNuts(20),USE:WheelStations(1))\nAction(Inspect i,DURATION:10,\nUSE:Inspectors(1))\nFigure11.1 Ajob-shopschedulingproblemforassemblingtwocars,with resourcecon-\nstraints. ThenotationA\u227aBmeansthatactionAmustprecedeactionB.\nThisapproachiscommoninreal-worldmanufacturingandlogisticalsettings,wheretheplan-\nningphase isoftenperformed byhumanexperts. Theautomated methods ofChapter10can\nalso be used for the planning phase, provided that they produce plans with just the minimal\nordering constraints required forcorrectness. GRAPHPLAN (Section 10.3), SATPLAN (Sec-\ntion 10.4.1), and partial-order planners (Section 10.4.4) can do this; search-based methods\n(Section 10.2) produce totally ordered plans, but these can easily be converted to plans with\nminimalordering constraints.\n11.1.1 Representing temporal andresource constraints\nA typical job-shop scheduling problem, as first introduced in Section 6.1.2, consists of a\nset of jobs, each of which consists a collection of actions with ordering constraints among\nJOB\nthem. Each action has a duration and a set of resource constraints required by the action.\nDURATION\nEach constraint specifies a type of resource (e.g., bolts, wrenches, or pilots), the number\nof that resource required, and whether that resource is consumable (e.g., the bolts are no\nCONSUMABLE\nlonger available foruse) or reusable (e.g., apilot is occupied during a flight but is available\nREUSABLE\nagainwhentheflightisover). Resources canalsobe produced byactionswithnegativecon-\nsumption, including manufacturing, growing, andresupply actions. Asolution toajob-shop\nscheduling problem mustspecify thestarttimesforeachaction andmustsatisfy allthetem-\nporal ordering constraints and resource constraints. As with search and planning problems,\nsolutions can be evaluated according to a cost function; this can be quite complicated, with\nnonlinear resource costs, time-dependent delay costs, and so on. For simplicity, we assume\nthatthecostfunction isjustthetotalduration oftheplan, whichiscalledthemakespan.\nMAKESPAN\nFigure11.1showsasimpleexample: aprobleminvolvingtheassemblyoftwocars. The\nproblemconsistsoftwojobs,eachoftheform[AddEngine,AddWheels,Inspect]. Thenthe Section11.1. Time,Schedules, andResources 403\nResources statement declares that there are four types of resources, and gives the number\nof each type available at the start: 1 engine hoist, 1 wheel station, 2 inspectors, and 500 lug\nnuts. The action schemas give the duration and resource needs of each action. The lug nuts\nare consumed as wheels are added to the car, whereas the other resources are \u201cborrowed\u201d at\nthestartofanactionandreleased attheaction\u2019send.\nThe representation of resources as numerical quantities, such as Inspectors(2), rather\nthan as named entities, such as Inspector(I ) and Inspector(I ), is an example of a very\n1 2\ngeneral technique called aggregation. Thecentral idea ofaggregation istogroupindividual\nAGGREGATION\nobjects into quantities when the objects are all indistinguishable with respect to the purpose\nathand. Inourassemblyproblem,itdoesnotmatterwhichinspectorinspectsthecar,sothere\nis no need to make the distinction. (The same idea works in the missionaries-and-cannibals\nproblem in Exercise 3.9.) Aggregation is essential for reducing complexity. Consider what\nhappens when a proposed schedule has 10 concurrent Inspect actions but only 9 inspectors\nareavailable. Withinspectorsrepresentedasquantities, afailureisdetectedimmediatelyand\nthealgorithm backtracks totryanotherschedule. Withinspectors represented asindividuals,\nthealgorithm backtracks totryall 10!waysofassigning inspectors toactions.\n11.1.2 Solvingscheduling problems\nWebeginbyconsideringjustthetemporalschedulingproblem,ignoringresourceconstraints.\nTominimizemakespan(planduration),wemustfindtheearlieststarttimesforalltheactions\nconsistent withtheorderingconstraints suppliedwiththe problem. Itishelpfultoviewthese\norderingconstraintsasadirectedgraphrelatingtheactions,asshowninFigure11.2. Wecan\nCRITICALPATH apply the critical path method (CPM) to this graph to determine the possible start and end\nMETHOD\ntimes of each action. A path through a graph representing a partial-order plan is a linearly\nordered sequence of actions beginning with Start and ending with Finish. (For example,\ntherearetwopathsinthepartial-order planinFigure11.2.)\nThe critical path is that path whose total duration is longest; the path is \u201ccritical\u201d\nCRITICALPATH\nbecause itdetermines theduration oftheentireplan\u2014shortening otherpathsdoesn\u2019t shorten\nthe plan as a whole, but delaying the start of any action on the critical path slows down the\nwholeplan. Actionsthatareoffthecriticalpathhaveawindowoftimeinwhichtheycanbe\nexecuted. Thewindowisspecifiedintermsofanearliestpossiblestarttime,ES,andalatest\npossible start time, LS. The quantity LS \u2013 ES is known as the slack of an action. We can\nSLACK\nsee in Figure 11.2 that the whole plan will take 85 minutes, that each action in the top job\nhas15minutesofslack, andthateachactiononthecritical pathhasnoslack(bydefinition).\nTogethertheES andLS timesforalltheactions constitute aschedulefortheproblem.\nSCHEDULE\nThefollowingformulasserveasadefinitionforES andLS andalsoastheoutlineofa\ndynamic-programming algorithm to compute them. A and B are actions, and A\u227aB means\nthatAcomesbefore B:\nES(Start) = 0\nES(B)= max A\u227aBES(A)+Duration(A)\nLS(Finish) = ES(Finish)\nLS(A) = min B(cid:9)ALS(B)\u2212Duration(A). 404 Chapter 11. PlanningandActingintheRealWorld\n[0,15] [30,45] [60,75]\nAddEngine1 AddWheels1 Inspect1\n30 30 10\n[0,0] [85,85]\nStart Finish\n[0,0] [60,60] [75,75]\nAddEngine2 AddWheels2 Inspect2\n60 15 10\nAddWheels1\nAddEngine1 Inspect1\nAddEngine2 Inspect2\nAddWheels2\n0 10 20 30 40 50 60 70 80 90\nFigure11.2 Top:arepresentationofthetemporalconstraintsforthejob-shopscheduling\nproblemofFigure11.1.Thedurationofeachactionisgivenatthebottomofeachrectangle.\nIn solvingthe problem,we computethe earliest andlatest start times asthe pair [ES,LS],\ndisplayed in the upper left. The difference between these two numbers is the slack of an\naction;actionswithzeroslackareonthecriticalpath,shownwithboldarrows. Bottom: the\nsamesolutionshownasatimeline. Greyrectanglesrepresenttimeintervalsduringwhichan\nactionmaybeexecuted,providedthattheorderingconstraintsarerespected.Theunoccupied\nportionofagrayrectangleindicatestheslack.\nThe idea is that we start by assigning ES(Start) to be 0. Then, as soon as we get an action\nB such that all the actions that come immediately before B have ES values assigned, we\nset ES(B) to be the maximum of the earliest finish times of those immediately preceding\nactions, wheretheearliestfinishtimeofanactionisdefined astheearlieststarttimeplusthe\nduration. This process repeats until every action has been assigned an ES value. The LS\nvaluesarecomputed inasimilarmanner,workingbackwardfromtheFinish action.\nThecomplexity ofthecritical pathalgorithm isjust O(Nb),whereN isthenumberof\nactionsandbisthemaximumbranchingfactorintooroutofanaction. (Toseethis,notethat\nthe LS and ES computations are done once for each action, and each computation iterates\noveratmostbotheractions.) Therefore,findingaminimum-durationschedule,givenapartial\nordering ontheactionsandnoresource constraints, isquiteeasy.\nMathematically speaking, critical-path problems areeasy tosolvebecause theyarede-\nfined as a conjunction of linear inequalities on the start and end times. When we introduce\nresource constraints, the resulting constraints on start and end times become more compli-\ncated. For example, the AddEngine actions, which begin at the same time in Figure 11.2, Section11.1. Time,Schedules, andResources 405\nEngineHoists(1) AddEngine1 AddEngine2\nWheelStations(1) AddWheels1 AddWheels2\nInspect1\nInspectors(2)\nInspect2\n0 10 20 30 40 50 60 70 80 90 100 110 120\nFigure11.3 Asolutiontothejob-shopschedulingproblemfromFigure11.1,takinginto\naccount resource constraints. The left-hand margin lists the three reusable resources, and\nactions are shown aligned horizontally with the resources they use. There are two possi-\nble schedules, depending on which assembly uses the engine hoist first; we\u2019ve shown the\nshortest-durationsolution,whichtakes115minutes.\nrequire the same EngineHoist and so cannot overlap. The \u201ccannot overlap\u201d constraint is a\ndisjunction of two linear inequalities, one for each possible ordering. The introduction of\ndisjunctions turnsouttomakescheduling withresourceconstraints NP-hard.\nFigure 11.3 shows the solution with the fastest completion time, 115 minutes. This is\n30 minutes longer than the 85 minutes required for a schedule without resource constraints.\nNotice that there is no time at which both inspectors are required, so we can immediately\nmoveoneofourtwoinspectors toamoreproductive position.\nThe complexity of scheduling with resource constraints is often seen in practice as\nwell as in theory. A challenge problem posed in 1963\u2014to find the optimal schedule for a\nproblem involving just 10 machines and 10 jobs of 100 actions each\u2014went unsolved for\n23 years (Lawler et al., 1993). Many approaches have been tried, including branch-and-\nbound, simulated annealing, tabu search, constraint satisfaction, and other techniques from\nChapters 3 and 4. One simple but popular heuristic is the minimum slack algorithm: on\nMINIMUMSLACK\neach iteration, schedule for the earliest possible start whichever unscheduled action has all\nitspredecessors scheduled andhastheleastslack;thenupdatetheES andLS timesforeach\naffected action and repeat. The heuristic resembles the minimum-remaining-values (MRV)\nheuristic in constraint satisfaction. It often works well in practice, but for our assembly\nproblem ityieldsa130\u2013minute solution, notthe115\u2013minute solution ofFigure11.3.\nUp to this point, we have assumed that the set of actions and ordering constraints is\nfixed. Undertheseassumptions,everyschedulingproblemcanbesolvedbyanonoverlapping\nsequence that avoids all resource conflicts, provided that each action is feasible by itself. If\na scheduling problem is proving very difficult, however, it may not be a good idea to solve\nitthis way\u2014itmaybebetter toreconsider theactions and constraints, in casethat leads to a\nmuch easier scheduling problem. Thus, it makes sense to integrate planning and scheduling\nbytakingintoaccount durations andoverlapsduring theconstruction ofapartial-order plan.\nSeveraloftheplanningalgorithmsinChapter10canbeaugmentedtohandlethisinformation.\nFor example, partial-order planners can detect resource constraint violations in much the\nsame way they detect conflicts with causal links. Heuristics can be devised to estimate the\ntotalcompletion timeofaplan. Thisiscurrently anactiveareaofresearch. 406 Chapter 11. PlanningandActingintheRealWorld\n11.2 HIERARCHICAL PLANNING\nTheproblem-solving andplanningmethodsoftheprecedingchaptersalloperatewithafixed\nset of atomic actions. Actions can be strung together into sequences orbranching networks;\nstate-of-the-art algorithms cangenerate solutions containing thousands ofactions.\nForplans executed by the human brain, atomic actions are muscle activations. In very\nround numbers, we have about 103 muscles to activate (639, by some counts, but many of\nthemhavemultiple subunits); wecanmodulatetheiractivation perhaps 10timespersecond;\nand we are alive and awake for about 109 seconds in all. Thus, a human life contains about\n1013 actions, give or take one or two orders of magnitude. Even if we restrict ourselves to\nplanning overmuchshorter timehorizons\u2014for example, atwo-weekvacation inHawaii\u2014a\ndetailed motorplanwouldcontainaround 1010 actions. Thisisalotmorethan1000.\nTobridgethisgap,AIsystemswillprobablyhavetodowhathumansappeartodo: plan\nat higher levels of abstraction. A reasonable plan for the Hawaii vacation might be \u201cGo to\nSanFranciscoairport;takeHawaiianAirlinesflight11toHonolulu;dovacationstufffortwo\nweeks;takeHawaiianAirlinesflight12backtoSanFrancisco;gohome.\u201d Givensuchaplan,\nthe action \u201cGo to San Francisco airport\u201d can be viewed as a planning task in itself, with a\nsolution such as \u201cDrive to the long-term parking lot; park; take the shuttle to the terminal.\u201d\nEachofthese actions, inturn, canbedecomposed further, until wereach thelevelofactions\nthatcanbeexecutedwithoutdeliberation togeneratetherequired motorcontrolsequences.\nIn this example, we see that planning can occur both before and during the execution\nof the plan; for example, one would probably defer the problem of planning a route from a\nparking spot in long-term parking to the shuttle bus stop until a particular parking spot has\nbeen found during execution. Thus, that particular action will remain at an abstract level\nprior to the execution phase. We defer discussion of this topic until Section 11.3. Here, we\nHIERARCHICAL concentrate on the aspect of hierarchical decomposition, an idea that pervades almost all\nDECOMPOSITION\nattempts to manage complexity. Forexample, complex software is created from a hierarchy\nofsubroutines orobjectclasses;armiesoperateasahierarchyofunits;governmentsandcor-\nporations have hierarchies of departments, subsidiaries, and branch offices. The key benefit\nof hierarchical structure is that, at each level of the hierarchy, a computational task, military\nmission,oradministrativefunctionisreducedtoasmallnumberofactivitiesatthenextlower\nlevel, so the computational cost of finding the correct way to arrange those activities for the\ncurrent problem is small. Nonhierarchical methods, on the other hand, reduce a task to a\nlargenumberofindividual actions; forlarge-scale problems,thisiscompletely impractical.\n11.2.1 High-levelactions\nThebasicformalismweadopttounderstandhierarchicaldecompositioncomesfromthearea\nHIERARCHICALTASK of hierarchical task networksorHTNplanning. Asin classical planning (Chapter 10), we\nNETWORK\nassumefullobservability anddeterminism andtheavailability ofasetofactions, nowcalled\nprimitiveactions,withstandardprecondition\u2013effect schemas. Thekeyadditional conceptis\nPRIMITIVEACTION\nthe high-levelaction orHLA\u2014forexample, theaction \u201cGotoSanFrancisco airport\u201d inthe\nHIGH-LEVELACTION Section11.2. Hierarchical Planning 407\nRefinement(Go(Home,SFO),\nSTEPS:[Drive(Home,SFOLongTermParking),\nShuttle(SFOLongTermParking,SFO)])\nRefinement(Go(Home,SFO),\nSTEPS:[Taxi(Home,SFO)])\nRefinement(Navigate([a,b],[x,y]),\nPRECOND:a=x \u2227 b=y\nSTEPS:[])\nRefinement(Navigate([a,b],[x,y]),\nPRECOND:Connected([a,b],[a\u22121,b])\nSTEPS:[Left,Navigate([a\u22121,b],[x,y])])\nRefinement(Navigate([a,b],[x,y]),\nPRECOND:Connected([a,b],[a+1,b])\nSTEPS:[Right,Navigate([a+1,b],[x,y])])\n...\nFigure11.4 Definitionsofpossiblerefinementsfortwohigh-levelactions: goingtoSan\nFranciscoairportandnavigatinginthe vacuumworld. Inthe lattercase, notethe recursive\nnatureoftherefinementsandtheuseofpreconditions.\nexample given earlier. Each HLA has one or more possible refinements, into a sequence1\nREFINEMENT\nof actions, each of which may be an HLA or a primitive action (which has no refinements\nby definition). For example, the action \u201cGo to San Francisco airport,\u201d represented formally\nas Go(Home,SFO), might have two possible refinements, as shown in Figure 11.4. The\nsame figure shows a recursive refinement for navigation in the vacuum world: to get to a\ndestination, takeastep,andthengotothedestination.\nTheseexamplesshow thathigh-level actions andtheirrefinements embody knowledge\nabouthowtodothings. Forinstance, therefinements for Go(Home,SFO)saythattogetto\nthe airport you can drive ortake ataxi; buying milk, sitting down, and moving the knight to\ne4arenottobeconsidered.\nAn HLA refinement that contains only primitive actions is called an implementation\nIMPLEMENTATION\nof the HLA. For example, in the vacuum world, the sequences [Right,Right,Down] and\n[Down,Right,Right]both implement the HLANavigate([1,3],[3,2]). Animplementation\nof a high-level plan (a sequence of HLAs) is the concatenation of implementations of each\nHLAinthesequence. Giventheprecondition\u2013effect definitionsofeachprimitiveaction,itis\nstraightforward todeterminewhetheranygivenimplementationofahigh-levelplanachieves\nthe goal. We can say, then, that a high-level plan achieves the goal from a given state if at\nleast one ofitsimplementations achieves the goal from that state. The\u201catleast one\u201d in this\ndefinitioniscrucial\u2014notallimplementationsneedtoachievethegoal,becausetheagentgets\n1 HTN planners often allow refinement into partially ordered plans, and they allow the refinements of two\ndifferentHLAsinaplantoshareactions.Weomittheseimportantcomplicationsintheinterestofunderstanding\nthebasicconceptsofhierarchicalplanning. 408 Chapter 11. PlanningandActingintheRealWorld\ntodecidewhichimplementation itwillexecute. Thus,thesetofpossible implementations in\nHTN planning\u2014each of which may have a different outcome\u2014is not the same as the set of\npossible outcomes innondeterministic planning. There, we required that aplan work for all\noutcomesbecause theagentdoesn\u2019t gettochoosetheoutcome;naturedoes.\nThe simplest case is an HLA that has exactly one implementation. In that case, we\ncan compute the preconditions and effects of the HLA from those of the implementation\n(see Exercise 11.3) and then treat the HLA exactly as if it were a primitive action itself. It\ncan be shown that the right collection of HLAs can result in the time complexity of blind\nsearch dropping from exponential in the solution depth to linear in the solution depth, al-\nthough devising such a collection of HLAs may be a nontrivial task in itself. When HLAs\nhave multiple possible implementations, there are two options: one is to search among the\nimplementations foronethatworks,asinSection11.2.2;theotheristoreasondirectlyabout\ntheHLAs\u2014despitethemultiplicityofimplementations\u2014as explainedinSection11.2.3. The\nlatter method enables the derivation of provably correct abstract plans, without the need to\nconsidertheirimplementations.\n11.2.2 Searching forprimitivesolutions\nHTNplanningisoftenformulatedwithasingle\u201ctoplevel\u201dactioncalledAct,wheretheaimis\ntofindanimplementationofActthatachievesthegoal. Thisapproachisentirelygeneral. For\nexample,classicalplanning problemscanbedefinedasfollows: foreachprimitiveaction a ,\ni\nprovide one refinement of Act with steps [a ,Act]. That creates a recursive definition of Act\ni\nthatletsusaddactions. Butweneedsomewaytostoptherecursion;wedothatbyproviding\none more refinement for Act, one with an empty list of steps and with a precondition equal\nto the goal of the problem. This says that if the goal is already achieved, then the right\nimplementation istodonothing.\nThe approach leads to a simple algorithm: repeatedly choose an HLA in the current\nplanandreplace itwithoneofitsrefinements, untiltheplan achievesthegoal. Onepossible\nimplementation based onbreadth-first treesearch isshowninFigure11.5. Plansareconsid-\neredinorderofdepth ofnesting oftherefinements, ratherthannumberofprimitivesteps. It\nisstraightforward todesignagraph-search versionofthealgorithm aswellasdepth-firstand\niterativedeepening versions.\nInessence,thisformofhierarchicalsearchexploresthespaceofsequencesthatconform\ntotheknowledgecontained intheHLAlibraryabouthowthingsaretobedone. Agreatdeal\nofknowledgecanbeencoded,notjustintheactionsequencesspecifiedineachrefinementbut\nalso in the preconditions for the refinements. For some domains, HTN planners have been\nable to generate huge plans with very little search. For example, O-PLAN (Bell and Tate,\n1985),whichcombinesHTNplanning withscheduling, hasbeenusedtodevelopproduction\nplans for Hitachi. A typical problem involves a product line of 350 different products, 35\nassembly machines, and over 2000 different operations. The planner generates a 30-day\nschedulewiththree8-hourshiftsaday,involvingtensofmillionsofsteps. Anotherimportant\naspect of HTN plans is that they are, by definition, hierarchically structured; usually this\nmakesthemeasyforhumanstounderstand. Section11.2. Hierarchical Planning 409\nfunctionHIERARCHICAL-SEARCH(problem,hierarchy)returnsasolution,orfailure\nfrontier\u2190aFIFOqueuewith[Act]astheonlyelement\nloopdo\nifEMPTY?(frontier)thenreturnfailure\nplan\u2190POP(frontier) \/*choosestheshallowestplaninfrontier *\/\nhla\u2190thefirstHLAinplan,ornull ifnone\nprefix,suffix\u2190theactionsubsequencesbeforeandafterhla inplan\noutcome\u2190RESULT(problem.INITIAL-STATE,prefix)\nifhla isnullthen \/*soplan isprimitiveandoutcome isitsresult*\/\nifoutcome satisfiesproblem.GOAL thenreturnplan\nelseforeachsequence inREFINEMENTS(hla,outcome,hierarchy)do\nfrontier\u2190INSERT(APPEND(prefix,sequence,suffix),frontier)\nFigure11.5 Abreadth-firstimplementationofhierarchicalforwardplanningsearch. The\ninitial plan suppliedto the algorithmis [Act]. The REFINEMENTS functionreturnsa set of\nactionsequences,oneforeachrefinementoftheHLAwhosepreconditionsaresatisfied by\nthespecifiedstate,outcome.\nThe computational benefits of hierarchical search can be seen by examining an ide-\nalized case. Suppose that a planning problem has a solution with d primitive actions. For\na nonhierarchical, forward state-space planner with b allowable actions at each state, the\ncost is O(bd), as explained in Chapter 3. For an HTN planner, let us suppose a very reg-\nular refinement structure: each nonprimitive action has r possible refinements, each into\nk actions at the next lower level. We want to know how many different refinement trees\nthere are with this structure. Now, if there are d actions at the primitive level, then the\nnumber of levels below the root is log d, so the number of internal refinement nodes is\nk\n1+k +k2 +\u00b7\u00b7\u00b7+klogkd\u22121 = (d\u22121)\/(k \u22121). Each internal node has r possible refine-\nments,sor(d\u22121)\/(k\u22121)\npossibleregulardecompositiontreescouldbeconstructed. Examining\nthis formula, we see that keeping r small and k large can result in huge savings: essentially\nwearetakingthekthrootofthenonhierarchical cost,if bandr arecomparable. Smallrand\nlarge k means a library of HLAs with a small number of refinements each yielding a long\naction sequence (that nonetheless allows us to solve any problem). This is not always pos-\nsible: long action sequences that are usable across a wide range of problems are extremely\nprecious.\nThekeytoHTNplanning, then, isthe construction ofaplan library containing known\nmethods for implementing complex, high-level actions. One method of constructing the li-\nbrary is to learn the methods from problem-solving experience. After the excruciating ex-\nperience of constructing a plan from scratch, the agent can save the plan in the library as a\nmethodforimplementingthehigh-levelactiondefinedbythetask. Inthisway,theagentcan\nbecomemoreandmorecompetentovertimeasnewmethodsarebuiltontopofoldmethods.\nOne important aspect of this learning process is the ability to generalize the methods that\nare constructed, eliminating detail that is specific to the problem instance (e.g., the name of 410 Chapter 11. PlanningandActingintheRealWorld\nthe builder or the address of the plot of land) and keeping just the key elements of the plan.\nMethodsforachievingthiskindofgeneralization aredescribed inChapter19. Itseemstous\ninconceivable thathumanscouldbeascompetentastheyarewithoutsomesuchmechanism.\n11.2.3 Searching forabstractsolutions\nThehierarchicalsearchalgorithmintheprecedingsection refinesHLAsallthewaytoprimi-\ntiveactionsequencestodetermineifaplanisworkable. Thiscontradictscommonsense: one\nshouldbeabletodetermine thatthetwo-HLAhigh-level plan\n[Drive(Home,SFOLongTermParking),Shuttle(SFOLongTermParking,SFO)]\ngets one to the airport without having to determine a precise route, choice of parking spot,\nandsoon. Thesolution seemsobvious: writeprecondition\u2013effect descriptions oftheHLAs,\njust as we write down what the primitive actions do. From the descriptions, it ought to be\neasytoprovethatthehigh-level planachieves thegoal. Thisistheholygrail,sotospeak, of\nhierarchical planning because if wederive ahigh-level plan that provably achieves the goal,\nworking in a small search space of high-level actions, then we can commit to that plan and\nworkontheproblemofrefiningeachstepoftheplan. Thisgivesustheexponentialreduction\nwe seek. For this to work, it has to be the case that every high-level plan that \u201cclaims\u201d to\nachieve the goal (by virtue of the descriptions of its steps) does in fact achieve the goal in\nthesensedefinedearlier: itmusthaveatleastoneimplementation thatdoesachievethegoal.\nDOWNWARD\nThisproperty hasbeencalledthe downwardrefinementpropertyforHLAdescriptions.\nREFINEMENT\nPROPERTY\nWriting HLA descriptions that satisfy the downward refinement property is, in princi-\nple,easy: aslongasthedescriptions are true,thenanyhigh-level planthatclaimstoachieve\nthe goal must in fact do so\u2014otherwise, the descriptions are making some false claim about\nwhatthe HLAsdo. Wehave already seen howto writetrue descriptions forHLAsthathave\nexactly one implementation (Exercise 11.3); a problem arises when the HLA has multiple\nimplementations. How can we describe the effects of an action that can be implemented in\nmanydifferent ways?\nOnesafeanswer(atleastforproblemswhereallpreconditionsandgoalsarepositive)is\ntoincludeonlythepositiveeffectsthatareachievedbyeveryimplementationoftheHLAand\nthe negative effects of any implementation. Then the downward refinement property would\nbesatisfied. Unfortunately,thissemanticsforHLAsismuchtooconservative. Consideragain\nthe HLA Go(Home,SFO), which has two refinements, and suppose, for the sake of argu-\nment,asimpleworldinwhichonecanalwaysdrivetotheairport andpark, buttaking ataxi\nrequires Cash as a precondition. In that case, Go(Home,SFO) doesn\u2019t always get you to\ntheairport. Inparticular, itfailsifCash isfalse,andsowecannotassertAt(Agent,SFO)as\naneffectoftheHLA.Thismakesnosense, however;iftheagentdidn\u2019t haveCash,itwould\ndriveitself. Requiringthataneffectholdfor everyimplementation isequivalent toassuming\nthat someone else\u2014an adversary\u2014will choose the implementation. Ittreats theHLA\u2019smul-\ntiple outcomes exactly asiftheHLAwereanondeterministicaction, asinSection 4.3. For\nourcase,theagentitselfwillchoosetheimplementation.\nThe programming languages community has coined the term demonic nondetermin-\nDEMONIC ismforthecasewhereanadversary makesthechoices, contrasting thiswithangelicnonde-\nNONDETERMINISM Section11.2. Hierarchical Planning 411\n(a) (b)\nFigure11.6 Schematicexamplesofreachablesets. Thesetofgoalstatesisshaded.Black\nandgrayarrowsindicatepossibleimplementationsofh andh ,respectively.(a)Thereach-\n1 2\nablesetofanHLAh inastates. (b)Thereachablesetforthesequence[h ,h ]. Because\n1 1 2\nthisintersectsthegoalset,thesequenceachievesthegoal.\nANGELIC terminism, wherethe agent itself makes the choices. Weborrow this term to defineangelic\nNONDETERMINISM\nsemantics for HLA descriptions. The basic concept required for understanding angelic se-\nANGELICSEMANTICS\nmantics is the reachable set of an HLA: given a state s, the reachable set for an HLA h,\nREACHABLESET\nwritten as REACH(s,h), is the set of states reachable by any of the HLA\u2019simplementations.\nThe key idea is that the agent can choose which element of the reachable set it ends up in\nwhenitexecutes theHLA;thus, anHLAwithmultiple refinements ismore\u201cpowerful\u201d than\nthesameHLAwithfewerrefinements. Wecanalsodefinethereachablesetofasequencesof\nHLAs. Forexample,thereachable setofasequence [h ,h ]istheunionofallthereachable\n1 2\nsetsobtained byapplyingh ineachstateinthereachable setofh :\n2 (cid:15) 1\n(cid:2)\nREACH(s,[h 1,h 2])= REACH(s,h 2).\ns(cid:3)\u2208REACH(s,h1)\nGiven these definitions, a high-level plan\u2014a sequence of HLAs\u2014achieves the goal if its\nreachable set intersects the set of goal states. (Compare this to the much stronger condition\nfor demonic semantics, where every member of the reachable set has to be a goal state.)\nConversely, if the reachable set doesn\u2019t intersect the goal, then the plan definitely doesn\u2019t\nwork. Figure11.6illustrates theseideas.\nThe notion of reachable sets yields a straightforward algorithm: search among high-\nlevel plans, looking for one whose reachable set intersects the goal; once that happens, the\nalgorithm can commit to that abstract plan, knowing that it works, and focus on refining\nthe plan further. We will come back to the algorithmic issues later; first, we consider the\nquestion ofhowtheeffects ofanHLA\u2014thereachable setforeachpossible initialstate\u2014are\nrepresented. As with the classical action schemas of Chapter 10, we represent the changes 412 Chapter 11. PlanningandActingintheRealWorld\nmadetoeachfluent. Thinkofafluentasastatevariable. Aprimitiveactioncanaddordelete\na variable or leave it unchanged. (With conditional effects (see Section 11.3.1) there is a\nfourthpossibility: flippingavariabletoitsopposite.)\nAn HLA under angelic semantics can do more: it can control the value of a variable,\nsetting ittotrueorfalse depending onwhichimplementation ischosen. Infact, anHLAcan\nhave nine different effects on a variable: if the variable starts out true, it can always keep\nit true, always make it false, or have a choice; if the variable starts out false, it can always\nkeep it false, always make it true, or have a choice; and the three choices for each case can\nbecombined arbitrarily, making nine. Notationally, this isabitchallenging. We\u2019llusethe (cid:23)\n(cid:23)\nsymboltomean\u201cpossibly, iftheagentsochooses.\u201d Thus,aneffect+Ameans\u201cpossibly add\nA,\u201d that is, either leave Aunchanged ormake it true. Similarly,\n\u2212(cid:23)\nA means \u201cpossibly delete\nA\u201d and\n\u00b1(cid:23)\nA means \u201cpossibly add or delete A.\u201d For example, the HLA Go(Home,SFO),\nwiththetworefinementsshowninFigure11.4,possiblydeletesCash (iftheagentdecidesto\ntake ataxi), soitshould havethe effect\n\u2212(cid:23)\nCash. Thus, weseethat thedescriptions ofHLAs\narederivable,inprinciple,fromthedescriptionsoftheirrefinements\u2014infact,thisisrequired\nif wewanttrue HLAdescriptions, such that the downward refinement property holds. Now,\nsuppose wehavethefollowingschemasfortheHLAsh andh :\n1 2\nAction(h\n1,PRECOND:\u00acA,EFFECT:A\u2227\u2212(cid:23)\nB),\nAction(h\n2,PRECOND:\u00acB,EFFECT:+(cid:23) A\u2227\u00b1(cid:23)\nC).\nThatis,h addsAandpossibledeletesB,whileh possiblyaddsAandhasfullcontrolover\n1 2\nC. Now,ifonlyB istrueintheinitial stateandthegoalisA\u2227C thenthesequence [h ,h ]\n1 2\nachieves the goal: we choose an implementation of h that makes B false, then choose an\n1\nimplementation ofh thatleavesAtrueandmakesC true.\n2\nThe preceding discussion assumes that the effects of an HLA\u2014the reachable set for\nanygiveninitial state\u2014can bedescribed exactlybydescribing theeffect oneachvariable. It\nwould be nice if this were always true, but in many cases we can only approximate the ef-\nfectsbecauseanHLAmayhaveinfinitelymanyimplementationsandmayproducearbitrarily\nwiggly reachable sets\u2014rather like the wiggly-belief-state problem illustrated in Figure 7.21\non page 271. For example, we said that Go(Home,SFO) possibly deletes Cash; it also\npossibly adds At(Car,SFOLongTermParking); but it cannot do both\u2014in fact, it must do\nexactly one. As with belief states, we may need to write approximate descriptions. Wewill\nOPTIMISTIC usetwokindsofapproximation: anoptimisticdescriptionREACH+(s,h)ofanHLAhmay\nDESCRIPTION\n\u2212\nPESSIMISTIC overstate the reachable set, while a pessimistic description REACH (s,h) may understate\nDESCRIPTION\nthereachable set. Thus,wehave\nREACH\u2212 (s,h) \u2286 REACH(s,h) \u2286 REACH+(s,h).\nForexample,anoptimisticdescriptionofGo(Home,SFO)saysthatitpossibledeletesCash\nand possibly adds At(Car,SFOLongTermParking). Another good example arises in the\n8-puzzle, half of whose states are unreachable from any given state (see Exercise 3.4 on\npage 113): the optimistic description of Act might well include the whole state space, since\ntheexactreachable setisquitewiggly.\nWith approximate descriptions, the test for whether a plan achieves the goal needs to\nbe modified slightly. If the optimistic reachable set for the plan doesn\u2019t intersect the goal, Section11.2. Hierarchical Planning 413\n(a) (b)\nFigure 11.7 Goal achievementfor high-levelplans with approximate descriptions. The\nsetofgoalstatesisshaded.Foreachplan,thepessimistic(solidlines)andoptimistic(dashed\nlines)reachablesetsareshown.(a)Theplanindicatedbytheblackarrowdefinitelyachieves\nthegoal,whiletheplanindicatedbythegrayarrowdefinitelydoesn\u2019t.(b)Aplanthatwould\nneedtoberefinedfurthertodetermineifitreallydoesachievethegoal.\nthen the plan doesn\u2019t work; if the pessimistic reachable set intersects the goal, then the plan\ndoes work (Figure 11.7(a)). With exact descriptions, a plan either works or it doesn\u2019t, but\nwith approximate descriptions, there is a middle ground: if the optimistic set intersects the\ngoal but the pessimistic set doesn\u2019t, then we cannot tell if the plan works (Figure 11.7(b)).\nWhen this circumstance arises, the uncertainty can be resolved by refining the plan. This is\na very common situation in human reasoning. Forexample, in planning the aforementioned\ntwo-week Hawaii vacation, one might propose to spend two days on each of seven islands.\nPrudence would indicate that this ambitious plan needs to be refined by adding details of\ninter-island transportation.\nAnalgorithm forhierarchical planning withapproximate angelicdescriptions isshown\nin Figure 11.8. For simplicity, we have kept to the same overall scheme used previously in\nFigure 11.5, that is, abreadth-first search in the space of refinements. Asjust explained, the\nalgorithmcandetectplansthatwillandwon\u2019tworkbycheckingtheintersections oftheopti-\nmisticandpessimisticreachablesetswiththegoal. (Thedetailsofhowtocomputethereach-\nablesetsofaplan,givenapproximatedescriptionsofeachstep,arecoveredinExercise11.5.)\nWhenaworkableabstractplanisfound, thealgorithm decomposes theoriginal probleminto\nsubproblems, one for each step of the plan. The initial state and goal for each subproblem\nare obtained by regressing a guaranteed-reachable goal state through the action schemas for\neach step of the plan. (See Section 10.2.2 for a discussion of how regression works.) Fig-\nure 11.6(b) illustrates the basic idea: the right-hand circled state is the guaranteed-reachable\ngoal state, and the left-hand circled state is the intermediate goal obtained by regressing the 414 Chapter 11. PlanningandActingintheRealWorld\nfunctionANGELIC-SEARCH(problem,hierarchy,initialPlan)returnssolutionorfail\nfrontier\u2190aFIFOqueuewithinitialPlan astheonlyelement\nloopdo\nifEMPTY?(frontier)thenreturnfail\nplan\u2190POP(frontier) \/*choosestheshallowestnodeinfrontier *\/\nifREACH+(problem.INITIAL-STATE,plan)intersectsproblem.GOALthen\nifplan isprimitivethenreturnplan \/*REACH+ isexactforprimitiveplans*\/\nguaranteed\u2190REACH\u2212 (problem.INITIAL-STATE,plan) \u2229 problem.GOAL\nifguaranteed(cid:7)={}andMAKING-PROGRESS(plan,initialPlan)then\nfinalState\u2190anyelementofguaranteed\nreturnDECOMPOSE(hierarchy,problem.INITIAL-STATE,plan,finalState)\nhla\u2190someHLAinplan\nprefix,suffix\u2190theactionsubsequencesbeforeandafterhla inplan\nforeachsequence inREFINEMENTS(hla,outcome,hierarchy)do\nfrontier\u2190INSERT(APPEND(prefix,sequence,suffix),frontier)\nfunctionDECOMPOSE(hierarchy,s0,plan,sf)returnsasolution\nsolution\u2190anemptyplan\nwhileplan isnotemptydo\naction\u2190REMOVE-LAST(plan)\nsi\u2190astateinREACH\u2212 (s0,plan)suchthatsf\u2208REACH\u2212\n(si,action)\nproblem\u2190aproblemwithINITIAL-STATE=si andGOAL=sf\nsolution\u2190APPEND(ANGELIC-SEARCH(problem,hierarchy,action),solution)\nsf \u2190si\nreturnsolution\nFigure11.8 Ahierarchicalplanningalgorithmthatusesangelicsemanticstoidentifyand\ncommittohigh-levelplansthatworkwhileavoidinghigh-levelplansthatdon\u2019t. Thepredi-\ncateMAKING-PROGRESS checkstomakesurethatwearen\u2019tstuckinaninfiniteregression\nofrefinements.Attoplevel,callANGELIC-SEARCHwith[Act]astheinitialPlan.\ngoalthrough thefinalaction.\nThe ability to commit to or reject high-level plans can give ANGELIC-SEARCH a sig-\nnificant computational advantage over HIERARCHICAL-SEARCH, which in turn may have\na large advantage over plain old BREADTH-FIRST-SEARCH. Consider, for example, clean-\ning up a large vacuum world consisting of rectangular rooms connected by narrow corri-\ndors. It makes sense to have an HLA for Navigate (as shown in Figure 11.4) and one for\nCleanWholeRoom. (Cleaning the room could be implemented with the repeated application\nof another HLA to clean each row.) Since there are five actions in this domain, the cost\nfor BREADTH-FIRST-SEARCH grows as 5d, where d is the length of the shortest solution\n(roughly twice the total number of squares); the algorithm cannot manage even two 2\u00d72\nrooms. HIERARCHICAL-SEARCH ismoreefficient,butstillsuffersfromexponential growth\nbecauseittriesallwaysofcleaningthatareconsistentwiththehierarchy. ANGELIC-SEARCH\nscales approximately linearly inthe numberofsquares\u2014it commits toagood high-level se- Section11.3. PlanningandActinginNondeterministic Domains 415\nquence and prunes away the other options. Notice that cleaning a set of rooms by cleaning\neach room in turn is hardly rocket science: it is easy for humans precisely because of the\nhierarchical structure of the task. When we consider how difficult humans find it to solve\nsmall puzzles such asthe 8-puzzle, itseems likely that the human capacity forsolving com-\nplex problems derives to a great extent from their skill in abstracting and decomposing the\nproblem toeliminatecombinatorics.\nThe angelic approach can be extended to find least-cost solutions by generalizing the\nnotion of reachable set. Instead of a state being reachable or not, it has a cost for the most\nefficient way to get there. (The cost is \u221e for unreachable states.) The optimistic and pes-\nsimisticdescriptions boundthesecosts. Inthisway,angelicsearchcanfindprovablyoptimal\nabstract planswithoutconsidering theirimplementations. Thesameapproach canbeusedto\n\u2217\nHIERARCHICAL obtain effective hierarchical lookahead algorithms for online search, in the style of LRTA\nLOOKAHEAD\n(page152). Insomeways,suchalgorithmsmirroraspectsofhumandeliberationintaskssuch\nasplanningavacationtoHawaii\u2014consideration ofalternativesisdoneinitiallyatanabstract\nleveloverlongtimescales;somepartsoftheplanareleftquiteabstractuntilexecution time,\nsuchashowtospendtwolazydaysonMolokai,whileotherspartsareplannedindetail,such\nas the flights to be taken and lodging to be reserved\u2014without these refinements, there is no\nguarantee thattheplanwouldbefeasible.\n11.3 PLANNING AND ACTING IN NONDETERMINISTIC DOMAINS\nIn this section we extend planning to handle partially observable, nondeterministic, and un-\nknown environments. Chapter 4 extended search in similar ways, and the methods here are\nalso similar: sensorless planning (also known as conformant planning) for environments\nwith no observations; contingency planning for partially observable and nondeterministic\nenvironments; andonlineplanningandreplanningforunknownenvironments.\nWhile the basic concepts are the same as in Chapter 4, there are also significant dif-\nferences. These arise because planners deal withfactored representations ratherthan atomic\nrepresentations. Thisaffectsthewaywerepresenttheagent\u2019scapabilityforactionandobser-\nvation and the way we represent belief states\u2014the sets of possible physical states the agent\nmight be in\u2014for unobservable and partially observable environments. We can also take ad-\nvantage of many of the domain-independent methods given in Chapter 10 for calculating\nsearchheuristics.\nConsiderthisproblem: givenachairandatable,thegoalistohavethemmatch\u2014have\nthe same color. In the initial state wehave two cans of paint, but the colors of the paint and\nthefurniture areunknown. Onlythetableisinitiallyinthe agent\u2019s fieldofview:\nInit(Object(Table)\u2227Object(Chair)\u2227Can(C )\u2227Can(C )\u2227InView(Table))\n1 2\nGoal(Color(Chair,c)\u2227Color(Table,c))\nThere are two actions: removing the lid from a paint can and painting an object using the\npaintfromanopencan. Theactionschemasarestraightforward, withoneexception: wenow\nallow preconditions and effects to contain variables that are not part of the action\u2019s variable 416 Chapter 11. PlanningandActingintheRealWorld\nlist. That is, Paint(x,can) does not mention the variable c, representing the color of the\npaint in the can. In the fully observable case, this is not allowed\u2014we would have to name\nthe action Paint(x,can,c). But in the partially observable case, we might or might not\nknowwhatcolorisinthecan. (Thevariable cisuniversally quantified, justlikealltheother\nvariables inanactionschema.)\nAction(RemoveLid(can),\nPRECOND:Can(can)\nEFFECT:Open(can))\nAction(Paint(x,can),\nPRECOND:Object(x)\u2227Can(can)\u2227Color(can,c)\u2227Open(can)\nEFFECT:Color(x,c))\nTosolveapartiallyobservableproblem,theagentwillhavetoreasonabouttheperceptsitwill\nobtainwhenitisexecutingtheplan. Theperceptwillbesuppliedbytheagent\u2019ssensorswhen\nitisactually acting, butwhenitisplanning itwillneed amodel ofitssensors. InChapter4,\nthis model was given by a function, PERCEPT(s). For planning, we augment PDDL with a\nnewtypeofschema,theperceptschema:\nPERCEPTSCHEMA\nPercept(Color(x,c),\nPRECOND:Object(x)\u2227InView(x)\nPercept(Color(can,c),\nPRECOND:Can(can)\u2227InView(can)\u2227Open(can)\nThe first schema says that whenever an object is in view, the agent will perceive the color\nof the object (that is, for the object x, the agent will learn the truth value of Color(x,c) for\nall c). The second schema says that if an open can is in view, then the agent perceives the\ncolor of the paint in the can. Because there are no exogenous events in this world, the color\nof an object will remain the same, even if it is not being perceived, until the agent performs\nan action to change the object\u2019s color. Of course, the agent will need an action that causes\nobjects(oneatatime)tocomeintoview:\nAction(LookAt(x),\nPRECOND:InView(y)\u2227(x(cid:7)= y)\nEFFECT:InView(x)\u2227\u00acInView(y))\nFora fully observable environment, we would have a Percept axiom with no preconditions\nfor each fluent. A sensorless agent, on the other hand, has no Percept axioms at all. Note\nthat evenasensorless agent can solvethe painting problem. Onesolution istoopen anycan\nof paint and apply it to both chair and table, thus coercing them to be the same color (even\nthoughtheagentdoesn\u2019tknowwhatthecoloris).\nA contingent planning agent with sensors can generate a better plan. First, look at the\ntable and chair to obtain their colors; if they are already the same then the plan is done. If\nnot, look at the paint cans; if the paint in a can is the same color as one piece of furniture,\nthenapplythatpainttotheotherpiece. Otherwise,paintbothpieceswithanycolor.\nFinally,anonlineplanningagentmightgenerateacontingent planwithfewerbranches\nat first\u2014perhaps ignoring the possibility that no cans match any of the furniture\u2014and deal Section11.3. PlanningandActinginNondeterministic Domains 417\nwith problems when they arise by replanning. It could also deal with incorrectness of its\naction schemas. Whereas a contingent planner simply assumes that the effects of an action\nalways succeed\u2014that painting the chair does the job\u2014a replanning agent would check the\nresultandmakeanadditional plantofixanyunexpectedfailure,suchasanunpaintedareaor\ntheoriginalcolorshowingthrough.\nIntherealworld,agentsuseacombinationofapproaches. Carmanufacturerssellspare\ntires and air bags, which are physical embodiments of contingent plan branches designed\nto handle punctures or crashes. On the other hand, most car drivers never consider these\npossibilities; when a problem arises they respond as replanning agents. In general, agents\nplan only for contingencies that have important consequences and a nonnegligible chance\nof happening. Thus, a car driver contemplating a trip across the Sahara desert should make\nexplicit contingency plans for breakdowns, whereas a trip to the supermarket requires less\nadvanceplanning. Wenextlookateachofthethreeapproaches inmoredetail.\n11.3.1 Sensorless planning\nSection 4.4.1 (page 138) introduced the basic idea of searching in belief-state space to find\nasolution forsensorless problems. Conversion ofasensorless planning problem to abelief-\nstate planning problem works muchthe samewayasit didinSection 4.4.1; the maindiffer-\nencesarethattheunderlyingphysicaltransitionmodelisrepresentedbyacollectionofaction\nschemas and the belief state can be represented by a logical formula instead of an explicitly\nenumerated set of states. Forsimplicity, weassume that the underlying planning problem is\ndeterministic.\nThe initial belief state for the sensorless painting problem can ignore InView fluents\nbecause the agent has no sensors. Furthermore, we take as given the unchanging facts\nObject(Table) \u2227 Object(Chair) \u2227 Can(C ) \u2227 Can(C ) because these hold in every be-\n1 2\nlief state. The agent doesn\u2019t know the colors of the cans or the objects, or whether the cans\nareopenorclosed, butitdoesknowthatobjects andcanshave colors: \u2200x \u2203c Color(x,c).\nAfterSkolemizing, (seeSection9.5),weobtaintheinitial beliefstate:\nb = Color(x,C(x)).\n0\nIn classical planning, where the closed-world assumption is made, we would assume that\nany fluentnot mentioned inastate is false, but in sensorless (and partially observable) plan-\nning we have to switch to an open-world assumption in which states contain both positive\nand negative fluents, and if a fluent does not appear, its value is unknown. Thus, the belief\nstate corresponds exactly to the set of possible worlds that satisfy the formula. Given this\ninitialbeliefstate,thefollowingactionsequence isasolution:\n[RemoveLid(Can ),Paint(Chair,Can ),Paint(Table,Can )].\n1 1 1\nWe now show how to progress the belief state through the action sequence to show that the\nfinalbeliefstatesatisfiesthegoal.\nFirst, note that in a given belief state b, the agent can consider any action whose pre-\nconditions aresatisfied by b. (Theotheractions cannot beused because thetransition model\ndoesn\u2019t define the effects of actions whose preconditions might be unsatisfied.) According 418 Chapter 11. PlanningandActingintheRealWorld\nto Equation (4.4) (page 139), the general formula for updating the belief state b given an\napplicable actionainadeterministic worldisasfollows:\nb(cid:2) = RESULT(b,a) = {s(cid:2) : s(cid:2) =RESULTP(s,a)ands \u2208 b}\nwhereRESULTP definesthephysicaltransitionmodel. Forthetimebeing,weassumethatthe\ninitial belief state is always a conjunction of literals, that is, a 1-CNF formula. To construct\n(cid:2)\nthenewbeliefstateb,wemustconsiderwhathappens toeachliteral (cid:3)ineachphysicalstate\nsinbwhenactionaisapplied. Forliteralswhosetruthvalueisalreadyknowninb,thetruth\n(cid:2)\nvalue in b is computed from the current value and the add list and delete list of the action.\n(For example, if (cid:3) is in the delete list of the action, then \u00ac(cid:3) is added to b(cid:2) .) What about a\nliteralwhosetruthvalueisunknowninb? Therearethreecases:\n(cid:2)\n1. Iftheactionadds(cid:3),then(cid:3)willbetrueinb regardless ofitsinitialvalue.\n(cid:2)\n2. Iftheactiondeletes(cid:3),then(cid:3)willbefalseinb regardlessofitsinitialvalue.\n3. Iftheactiondoesnotaffect(cid:3),then(cid:3)willretainitsinitialvalue(whichisunknown)and\n(cid:2)\nwillnotappearinb.\n(cid:2)\nHence,weseethatthecalculation ofb isalmostidentical totheobservable case, whichwas\nspecifiedbyEquation(10.1)onpage368:\nb(cid:2) = RESULT(b,a) = (b\u2212DEL(a))\u222aADD(a).\n(cid:2)\nWe cannot quite use the set semantics because (1) we must make sure that b does not con-\ntain both (cid:3) and \u00ac(cid:3), and (2) atoms may contain unbound variables. But it is still the case\nthat RESULT(b,a) is computed by starting with b, setting any atom that appears in DEL(a)\nto false, and setting any atom that appears in ADD(a) to true. For example, if we apply\nRemoveLid(Can )totheinitialbeliefstateb ,weget\n1 0\nb = Color(x,C(x))\u2227Open(Can ).\n1 1\nWhenweapplytheactionPaint(Chair,Can ),thepreconditionColor(Can ,c)issatisfied\n1 1\nbytheknownliteralColor(x,C(x))withbinding{x\/Can ,c\/C(Can )}andthenewbelief\n1 1\nstateis\nb = Color(x,C(x))\u2227Open(Can )\u2227Color(Chair,C(Can )).\n2 1 1\nFinally,weapplytheactionPaint(Table,Can )toobtain\n1\nb = Color(x,C(x))\u2227Open(Can )\u2227Color(Chair,C(Can ))\n3 1 1\n\u2227Color(Table,C(Can )).\n1\nThefinalbeliefstatesatisfiesthegoal,Color(Table,c)\u2227Color(Chair,c),withthevariable\ncboundtoC(Can ).\n1\nThe preceding analysis of the update rule has shown a very important fact: the family\nof belief states defined as conjunctions of literals is closed under updates defined by PDDL\nactionschemas. Thatis,ifthebelief statestartsasaconjunction ofliterals, thenanyupdate\nwill yield a conjunction of literals. That means that in a world with n fluents, any belief\nstate can be represented by a conjunction of size O(n). This is a very comforting result,\nconsidering that there are 2n states in the world. It says we can compactly represent all the\nsubsetsofthose2n statesthatwewilleverneed. Moreover,theprocessofcheckingforbelief Section11.3. PlanningandActinginNondeterministic Domains 419\nstates that are subsets or supersets of previously visited belief states is also easy, at least in\nthepropositional case.\nTheflyinthe ointment of this pleasant picture isthat itonly works foraction schemas\nthat have the same effects for all states in which their preconditions are satisfied. It is this\npropertythatenablesthepreservationofthe1-CNFbelief-staterepresentation. Assoonasthe\neffect can depend on the state, dependencies are introduced between fluents and the 1-CNF\nproperty is lost. Consider, for example, the simple vacuum world defined in Section 3.2.1.\nLet the fluents be AtL and AtR for the location of the robot and CleanL and CleanR for\nthe state of the squares. According to the definition of the problem, the Suck action has no\nprecondition\u2014itcanalwaysbedone. Thedifficultyisthatitseffectdependsontherobot\u2019slo-\ncation: whentherobotisAtL,theresultisCleanL,butwhenitisAtR,theresultisCleanR.\nCONDITIONAL Forsuch actions, ouraction schemas will need something new: a conditional effect. These\nEFFECT\nhave the syntax \u201cwhen condition: effect,\u201d where condition is a logical formula to be com-\npared against the current state, and effect is a formula describing the resulting state. Forthe\nvacuumworld,wehave\nAction(Suck,\nEFFECT:whenAtL:CleanL\u2227whenAtR:CleanR).\nWhen applied to the initial belief state True, the resulting belief state is (AtL\u2227CleanL)\u2228\n(AtR \u2227CleanR), which is no longer in 1-CNF. (This transition can be seen in Figure 4.14\non page 141.) In general, conditional effects can induce arbitrary dependencies among the\nfluentsinabeliefstate, leadingtobeliefstatesofexponential sizeintheworstcase.\nIt is important to understand the difference between preconditions and conditional ef-\nfects. Allconditionaleffectswhoseconditionsaresatisfiedhavetheireffectsappliedtogener-\natetheresultingstate;ifnonearesatisfied,thentheresultingstateisunchanged. Ontheother\nhand, if a precondition is unsatisfied, then the action is inapplicable and the resulting state\nis undefined. From the point of view of sensorless planning, it is better to have conditional\neffects than an inapplicable action. Forexample, we could split Suck into two actions with\nunconditional effectsasfollows:\nAction(SuckL,\nPRECOND:AtL; EFFECT:CleanL)\nAction(SuckR,\nPRECOND:AtR; EFFECT:CleanR).\nNowwehaveonly unconditional schemas, sothebelief states allremainin1-CNF;unfortu-\nnately, wecannot determinetheapplicability ofSuckLandSuckR intheinitialbeliefstate.\nItseemsinevitable, then,thatnontrivial problemswillinvolve wigglybeliefstates, just\nlike those encountered when we considered the problem of state estimation for the wumpus\nworld(seeFigure7.21onpage271). Thesolution suggested thenwastouseaconservative\napproximation to the exact belief state; for example, the belief state can remain in 1-CNF\nif it contains all literals whose truth values can be determined and treats all other literals as\nunknown. While this approach is sound, in that it never generates an incorrect plan, it is\nincomplete because it may be unable to find solutions to problems that necessarily involve\ninteractions among literals. To give a trivial example, if the goal is for the robot to be on 420 Chapter 11. PlanningandActingintheRealWorld\na clean square, then [Suck] is a solution but a sensorless agent that insists on 1-CNF belief\nstateswillnotfindit.\nPerhaps a better solution is to look for action sequences that keep the belief state\nas simple as possible. For example, in the sensorless vacuum world, the action sequence\n[Right,Suck,Left,Suck]generates thefollowingsequence ofbeliefstates:\nb = True\n0\nb = AtR\n1\nb = AtR\u2227CleanR\n2\nb = AtL\u2227CleanR\n3\nb = AtL\u2227CleanR\u2227CleanL\n4\nThat is, the agent can solve the problem while retaining a 1-CNF belief state, even though\nsome sequences (e.g., those beginning with Suck) go outside 1-CNF. The general lesson is\nnot lost on humans: we are always performing little actions (checking the time, patting our\npocketstomakesurewehavethecarkeys,readingstreetsignsaswenavigatethroughacity)\ntoeliminate uncertainty andkeepourbeliefstatemanageable.\nThere is another, quite different approach to the problem of unmanageably wiggly be-\nlief states: don\u2019t bother computing them at all. Suppose the initial belief state is b and we\n0\nwould like to know the belief state resulting from the action sequence [a ,...,a ]. Instead\n1 m\nof computing it explicitly, just represent it as \u201cb then [a ,...,a ].\u201d This is a lazy but un-\n0 1 m\nambiguous representation of the belief state, and it\u2019s quite concise\u2014O(n +m) where n is\nthe size of the initial belief state (assumed to be in 1-CNF) and m is the maximum length\nof an action sequence. As a belief-state representation, it suffers from one drawback, how-\never: determining whether the goal is satisfied, or an action is applicable, may require a lot\nofcomputation.\nThecomputationcanbeimplementedasanentailmenttest: ifA representsthecollec-\nm\ntion of successor-state axioms required to define occurrences of the actions a ,...,a \u2014as\n1 m\nexplainedforSATPLANinSection10.4.1\u2014andG massertsthatthegoalistrueaftermsteps,\nthentheplanachievesthegoalifb \u2227A |= G ,thatis,ifb \u2227A \u2227\u00acG isunsatisfiable.\n0 m m 0 m m\nGivenamodernSATsolver,itmaybepossibletodothismuchmorequicklythancomputing\nthefull belief state. Forexample, ifnoneofthe actions inthe sequence hasaparticular goal\nfluent in its add list, the solver will detect this immediately. It also helps if partial results\naboutthebeliefstate\u2014forexample,fluentsknowntobetrueorfalse\u2014arecachedtosimplify\nsubsequent computations.\nThe final piece of the sensorless planning puzzle is a heuristic function to guide the\nsearch. The meaning of the heuristic function is the same as for classical planning: an esti-\nmate(perhaps admissible) ofthecostofachieving thegoal from thegivenbelief state. With\nbelief states, we have one additional fact: solving any subset of a belief state is necessarily\neasierthansolvingthebeliefstate:\nifb \u2286 b thenh\u2217 (b ) \u2264 h\u2217 (b ).\n1 2 1 2\nHence,anyadmissibleheuristiccomputedforasubsetisadmissibleforthebeliefstateitself.\nThemostobvious candidates arethesingleton subsets, thatis,individual physicalstates. We Section11.3. PlanningandActinginNondeterministic Domains 421\ncan take any random collection of states s ,...,s that are in the belief state b, apply any\n1 N\nadmissible heuristic hfromChapter10,andreturn\nH(b) = max{h(s ),...,h(s )}\n1 N\nastheheuristicestimateforsolving b. Wecouldalsouseaplanninggraphdirectlyonbitself:\nif it is a conjunction of literals (1-CNF), simply set those literals to be the initial state layer\nofthegraph. Ifbisnotin1-CNF,itmaybepossibletofindsetsofliteralsthattogetherentail\nb. For example, if b is in disjunctive normal form (DNF), each term of the DNF formula is\na conjunction of literals that entails b and can form the initial layer of a planning graph. As\nbefore,wecantakethemaximumoftheheuristics obtainedfromeachsetofliterals. Wecan\nalso use inadmissible heuristics such as the ignore-delete-lists heuristic (page 377), which\nseemstoworkquitewellinpractice.\n11.3.2 Contingent planning\nWe saw in Chapter 4 that contingent planning\u2014the generation of plans with conditional\nbranchingbasedonpercepts\u2014isappropriateforenvironmentswithpartialobservability,non-\ndeterminism, orboth. Forthepartially observable painting problem withthepercept axioms\ngivenearlier, onepossible contingent solution isasfollows:\n[LookAt(Table),LookAt(Chair),\nifColor(Table,c)\u2227Color(Chair,c)thenNoOp\nelse[RemoveLid(Can ),LookAt(Can ),RemoveLid(Can ),LookAt(Can ),\n1 1 2 2\nifColor(Table,c)\u2227Color(can,c)thenPaint(Chair,can)\nelseifColor(Chair,c)\u2227Color(can,c)thenPaint(Table,can)\nelse[Paint(Chair,Can ),Paint(Table,Can )]]]\n1 1\nVariables in this plan should be considered existentially quantified; the second line says\nthat if there exists some color c that is the color of the table and the chair, then the agent\nneed not do anything to achieve the goal. When executing this plan, a contingent-planning\nagent can maintain its belief state as a logical formula and evaluate each branch condition\nby determining if the belief state entails the condition formula or its negation. (It is up to\nthe contingent-planning algorithm to make sure that the agent will never end up in a be-\nlief state where the condition formula\u2019s truth value is unknown.) Note that with first-order\nconditions, the formula may be satisfied in more than one way; for example, the condition\nColor(Table,c)\u2227Color(can,c)mightbesatisfiedby{can\/Can }andby{can\/Can }if\n1 2\nboth cans are the same color as the table. In that case, the agent can choose any satisfying\nsubstitution toapplytotherestoftheplan.\nAs shown in Section 4.4.2, calculating the new belief state after an action and subse-\nquentperceptisdoneintwostages. Thefirststagecalculates thebeliefstateaftertheaction,\njustasforthesensorless agent:\n\u02c6b =(b\u2212DEL(a))\u222aADD(a)\nwhere,asbefore,wehaveassumedabeliefstaterepresented asaconjunction ofliterals. The\nsecond stage is a little trickier. Suppose that percept literals p ,...,p are received. One\n1 k\nmight think that we simply need to add these into the belief state; in fact, we can also infer 422 Chapter 11. PlanningandActingintheRealWorld\nthat the preconditions for sensing are satisfied. Now, if a percept p has exactly one percept\naxiom, Percept(p,PRECOND:c), where c is a conjunction of literals, then those literals can\nbethrownintothebeliefstatealongwithp. Ontheotherhand,ifphasmorethanonepercept\naxiomwhosepreconditions\nmightholdaccordingtothepredictedbeliefstate\u02c6b,thenwehave\nto add in the disjunction of the preconditions. Obviously, this takes the belief state outside\n1-CNF and brings up the same complications as conditional effects, with much the same\nclassesofsolutions.\nGivenamechanism forcomputing exact orapproximate belief states, wecan generate\ncontingent plans with an extension of the AND\u2013OR forward search over belief states used\nin Section 4.4. Actions with nondeterministic effects\u2014which are defined simply by using a\ndisjunction inthe EFFECT oftheactionschema\u2014can beaccommodated withminorchanges\ntothebelief-stateupdatecalculationandnochangetothesearchalgorithm.2 Fortheheuristic\nfunction, many of the methods suggested for sensorless planning are also applicable in the\npartially observable, nondeterministic case.\n11.3.3 Onlinereplanning\nImagine watching aspot-welding robot inacarplant. Therobot\u2019s fast, accurate motions are\nrepeated over and over again as each car passes down the line. Although technically im-\npressive, the robot probably does not seem at all intelligent because the motion is a fixed,\npreprogrammed sequence; the robot obviously doesn\u2019t \u201cknow what it\u2019s doing\u201d inany mean-\ningful sense. Now suppose that a poorly attached door falls off the car just as the robot is\nabout to apply a spot-weld. The robot quickly replaces its welding actuator with a gripper,\npicks up the door, checks it forscratches, reattaches it to the car, sends an email to the floor\nsupervisor, switches back to the welding actuator, and resumes its work. All of a sudden,\nthe robot\u2019s behavior seems purposive rather than rote; we assume it results not from a vast,\nprecomputed contingent plan but from an online replanning process\u2014which means that the\nrobot doesneedtoknowwhatit\u2019stryingtodo.\nEXECUTION Replanningpresupposessomeformofexecutionmonitoringtodeterminetheneedfor\nMONITORING\na new plan. One such need arises when a contingent planning agent gets tired of planning\nforevery little contingency, such as whether the sky might fall on its head.3 Some branches\nofapartiallyconstructed contingent plancansimplysayReplan;ifsuchabranchisreached\nduring execution, the agent reverts to planning mode. Aswementioned earlier, the decision\nas to how much of the problem to solve in advance and how much to leave to replanning\nis one that involves tradeoffs among possible events with different costs and probabilities of\noccurring. NobodywantstohavetheircarbreakdowninthemiddleoftheSaharadesertand\nonlythenthinkabouthavingenough water.\n2 Ifcyclicsolutionsarerequiredforanondeterministicproblem,AND\u2013ORsearchmustbegeneralizedtoaloopy\nversionsuchasLAO\u2217(HansenandZilberstein,2001).\n3 In1954,aMrs.HodgesofAlabamawashitbymeteoritethatcrashedthroughherroof. In1992, apieceof\ntheMbalemeteoritehitasmallboyonthehead;fortunately,itsdescentwasslowedbybananaleaves(Jenniskens\netal.,1994).Andin2009,aGermanboyclaimedtohavebeenhitinthehandbyapea-sizedmeteorite.Noserious\ninjuriesresultedfromanyoftheseincidents,suggestingthattheneedforpreplanningagainstsuchcontingencies\nissometimesoverstated. Section11.3. PlanningandActinginNondeterministic Domains 423\nwhole plan\nplan\nS P E G\ncontinuation\nrepair\nO\nFigure11.9 Beforeexecution,theplannercomesupwithaplan,herecalledwhole plan,\ntogetfromS toG. TheagentexecutesstepsoftheplanuntilitexpectstobeinstateE,but\nobservesitisactuallyinO. Theagentthenreplansfortheminimalrepairpluscontinuation\ntoreachG.\nReplanningmayalsobeneedediftheagent\u2019smodeloftheworldisincorrect. Themodel\nMISSING foran action may have a missing precondition\u2014for example, the agent may not know that\nPRECONDITION\nremoving the lid of a paint can often requires a screwdriver; the model may have a missing\neffect\u2014forexample, painting anobjectmaygetpaintontheflooraswell;orthemodelmay\nMISSINGEFFECT\nMISSINGSTATE have a missing state variable\u2014for example, the model given earlier has no notion of the\nVARIABLE\namountofpaint inacan,ofhowitsactions affectthisamount, oroftheneedfortheamount\nto be nonzero. The model may also lack provision for exogenous events such as someone\nEXOGENOUSEVENT\nknocking over the paint can. Exogenous events can also include changes in the goal, such\nas the addition of the requirement that the table and chair not be painted black. Without the\nability to monitor and replan, an agent\u2019s behavior is likely to be extremely fragile if it relies\nonabsolutecorrectness ofitsmodel.\nTheonline agent hasachoice ofhow carefully tomonitortheenvironment. Wedistin-\nguishthreelevels:\n\u2022 Actionmonitoring: beforeexecutinganaction,theagentverifiesthatalltheprecondi-\nACTIONMONITORING\ntionsstillhold.\n\u2022 Planmonitoring: beforeexecutinganaction,theagentverifiesthattheremainingplan\nPLANMONITORING\nwillstillsucceed.\n\u2022 Goalmonitoring: beforeexecutinganaction,theagentcheckstoseeifthereisabetter\nGOALMONITORING\nsetofgoalsitcouldbetryingtoachieve.\nIn Figure 11.9 we see a schematic of action monitoring. The agent keeps track of both its\noriginal plan, wholeplan, and the part of the plan that has not been executed yet, which is\ndenoted by plan. After executing the first few steps of the plan, the agent expects to be in\nstate E. But the agent observes it is actually in state O. It then needs to repair the plan by\nfindingsomepointP ontheoriginalplanthatitcangetbackto. (ItmaybethatP isthegoal\nstate,G.) Theagenttriestominimizethetotalcostoftheplan: therepairpart(from OtoP)\nplusthecontinuation (from P toG). 424 Chapter 11. PlanningandActingintheRealWorld\nNow let\u2019s return to the example problem of achieving a chair and table of matching\ncolor. Supposetheagentcomesupwiththisplan:\n[LookAt(Table),LookAt(Chair),\nifColor(Table,c)\u2227Color(Chair,c)thenNoOp\nelse[RemoveLid(Can ),LookAt(Can ),\n1 1\nifColor(Table,c)\u2227Color(Can ,c)thenPaint(Chair,Can )\n1 1\nelse REPLAN]].\nNow the agent is ready to execute the plan. Suppose the agent observes that the table and\ncan of paint are white and the chair is black. It then executes Paint(Chair,Can ). At this\n1\npoint a classical planner would declare victory; the plan has been executed. But an online\nexecution monitoring agent needs tocheck thepreconditions ofthe remaining emptyplan\u2014\nthat the table and chair are the same color. Suppose the agent perceives that they do not\nhave the same color\u2014in fact, the chair is now a mottled gray because the black paint is\nshowing through. The agent then needs to figure out a position in whole plan to aim for\nand arepair action sequence toget there. Theagent notices thatthe current state isidentical\nto the precondition before the Paint(Chair,Can ) action, so the agent chooses the empty\n1\nsequence for repair and makes its plan be thesame [Paint]sequence that it just attempted.\nWith this new plan in place, execution monitoring resumes, and the Paint action is retried.\nThisbehavior willloop until thechairisperceived tobecompletely painted. Butnotice that\nthe loop is created by a process of plan\u2013execute\u2013replan, rather than by an explicit loop in a\nplan. Notealso that the original plan need notcoverevery contingency. Ifthe agent reaches\nthestepmarked REPLAN,itcanthengenerate anewplan(perhaps involving Can 2).\nAction monitoring is a simple method of execution monitoring, but it can sometimes\nlead to less than intelligent behavior. Forexample, suppose there is no black orwhite paint,\nand the agent constructs a plan to solve the painting problem by painting both the chair and\ntable red. Suppose that there isonly enough red paint forthe chair. With action monitoring,\ntheagentwouldgoaheadandpaintthechairred,thennoticethatitisoutofpaintandcannot\npaintthetable,atwhichpointitwouldreplanarepair\u2014perhaps paintingbothchairandtable\ngreen. Aplan-monitoring agent candetect failure whenever the current state issuch that the\nremaining plan no longer works. Thus, it would not waste time painting the chair red. Plan\nmonitoring achieves this by checking the preconditions for success of the entire remaining\nplan\u2014that is, the preconditions of each step in the plan, except those preconditions that are\nachieved by another step in the remaining plan. Plan monitoring cuts off execution of a\ndoomed plan as soon as possible, rather than continuing until the failure actually occurs.4\nPlan monitoring also allows for serendipity\u2014accidental success. If someone comes along\nandpaintsthetableredatthesametimethattheagentispaintingthechairred,thenthefinal\nplanpreconditionsaresatisfied(thegoalhasbeenachieved),andtheagentcangohomeearly.\nIt is straightforward to modify a planning algorithm so that each action in the plan\nis annotated with the action\u2019s preconditions, thus enabling action monitoring. It is slightly\n4 Planmonitoringmeansthatfinally,after424pages,wehaveanagentthatissmarterthanadungbeetle(see\npage39). Aplan-monitoringagentwouldnoticethatthedungballwasmissingfromitsgraspandwouldreplan\ntogetanotherballandplugitshole. Section11.4. Multiagent Planning 425\nmore complex to enable plan monitoring. Partial-order and planning-graph planners have\nthe advantage that they have already built up structures that contain the relations necessary\nforplanmonitoring. Augmentingstate-space planners with thenecessary annotations canbe\ndonebycarefulbookkeeping asthegoalfluentsareregressed throughtheplan.\nNow that we have described a method for monitoring and replanning, we need to ask,\n\u201cDoes it work?\u201d This is a surprisingly tricky question. If we mean, \u201cCan we guarantee that\nthe agent will always achieve the goal?\u201d then the answer is no, because the agent could\ninadvertently arrive at a dead end from which there is no repair. For example, the vacuum\nagent might have a faulty model of itself and not know that its batteries can run out. Once\ntheydo,itcannot repairanyplans. Ifweruleoutdeadends\u2014assume thatthereexists aplan\nto reach the goal from any state in the environment\u2014and assume that the environment is\nreally nondeterministic, in the sense that such a plan always has some chance of success on\nanygivenexecutionattempt,thentheagentwilleventually reachthegoal.\nTrouble occurs when an action is actually not nondeterministic, but rather depends on\nsome precondition that the agent does not know about. For example, sometimes a paint\ncanmaybeempty,sopaintingfromthatcanhasnoeffect. Noamountofretrying isgoingto\nchangethis.5 Onesolutionistochooserandomlyfromamongthesetofpossiblerepairplans,\nratherthantotrythesameoneeachtime. Inthiscase,therepairplanofopening anothercan\nmight work. A better approach is to learn a better model. Every prediction failure is an\nopportunity forlearning; anagent should be able tomodify itsmodel ofthe worldto accord\nwithitspercepts. Fromthen on,thereplanner willbeable to comeupwitharepairthatgets\nattherootproblem,ratherthanrelyingonlucktochooseagoodrepair. Thiskindoflearning\nisdescribed inChapters18and19.\n11.4 MULTIAGENT PLANNING\nSo far, we have assumed that only one agent is doing the sensing, planning, and acting.\nWhentherearemultiple agentsintheenvironment, eachagent facesamultiagentplanning\nMULTIAGENT probleminwhichittriestoachieveitsowngoalswiththehelporhindrance ofothers.\nPLANNINGPROBLEM\nBetweenthepurelysingle-agent andtrulymultiagentcases isawidespectrumofprob-\nlems that exhibit various degrees of decomposition of the monolithic agent. An agent with\nmultiple effectors that can operate concurrently\u2014for example, a human who can type and\nMULTIEFFECTOR speak atthe same time\u2014needs to do multieffector planningto manage each effector while\nPLANNING\nhandling positive and negative interactions among the effectors. When the effectors are\nphysically decoupled into detached units\u2014as in a fleet of delivery robots in a factory\u2014\nMULTIBODY multieffector planning becomes multibody planning. A multibody problem is still a \u201cstan-\nPLANNING\ndard\u201dsingle-agent problemaslongastherelevantsensorinformationcollectedbyeachbody\ncan be pooled\u2014either centrally or within each body\u2014to form a common estimate of the\nworldstatethattheninformstheexecution oftheoverallplan;inthiscase, themultiplebod-\nies act as a single body. When communication constraints make this impossible, we have\n5 Futilerepetitionofaplanrepairisexactlythebehaviorexhibitedbythesphexwasp(page39). 426 Chapter 11. PlanningandActingintheRealWorld\nDECENTRALIZED whatissometimescalledadecentralizedplanningproblem;thisisperhapsamisnomer,be-\nPLANNING\ncausetheplanningphaseiscentralizedbuttheexecutionphaseisatleastpartiallydecoupled.\nInthiscase,thesubplanconstructed foreachbodymayneedtoincludeexplicitcommunica-\ntiveactions withotherbodies. Forexample, multiple reconnaissance robots covering awide\nareamayoftenbeoutofradiocontactwitheachotherandshouldsharetheirfindingsduring\ntimeswhencommunication isfeasible.\nWhen a single entity is doing the planning, there is really only one goal, which all the\nbodiesnecessarilyshare. Whenthebodiesaredistinctagentsthatdotheirownplanning,they\nmay still share identical goals; for example, two human tennis players who form a doubles\nteam share the goal of winning the match. Even with shared goals, however, the multibody\nand multiagent cases are quite different. In a multibody robotic doubles team, a single plan\ndictates which body willgo where on the court and which body willhit the ball. Ina multi-\nagent doubles team,ontheotherhand, eachagent decides whattodo;without somemethod\nfor coordination, both agents may decide to cover the same part of the court and each may\nCOORDINATION\nleavetheballfortheothertohit.\nTheclearest caseofamultiagent problem, ofcourse, iswhen the agents havedifferent\ngoals. In tennis, the goals of two opposing teams are in direct conflict, leading to the zero-\nsum situation of Chapter 5. Spectators could be viewed as agents if their support ordisdain\nis a significant factor and can be influenced by the players\u2019 conduct; otherwise, they can be\ntreated asanaspect ofnature\u2014just liketheweather\u2014that is assumedtobeindifferent tothe\nplayers\u2019 intentions.6\nFinally, some systems are a mixture of centralized and multiagent planning. For ex-\nample, a delivery company may do centralized, offline planning for the routes of its trucks\nand planes each day, but leave some aspects open for autonomous decisions by drivers and\npilots who can respond individually to traffic and weather situations. Also, the goals of the\ncompany and its employees are brought into alignment, to some extent, by the payment of\nincentives(salariesandbonuses)\u2014a suresignthatthisisatruemultiagentsystem.\nINCENTIVE\nThe issues involved in multiagent planning can be divided roughly into two sets. The\nfirst, covered in Section 11.4.1, involves issues of representing and planning for multiple\nsimultaneous actions;theseissuesoccurinallsettingsfrommultieffectortomultiagentplan-\nning. The second, covered in Section 11.4.2, involves issues of cooperation, coordination,\nandcompetition arisingintruemultiagentsettings.\n11.4.1 Planning withmultiplesimultaneous actions\nForthe time being, wewill treat the multieffector, multibody, and multiagent settings in the\nsame way, labeling them generically as multiactor settings, using the generic term actor to\nMULTIACTOR\ncover effectors, bodies, and agents. The goal of this section is to work out how to define\nACTOR\ntransition models, correct plans, and efficient planning algorithms forthe multiactor setting.\nAcorrectplanisonethat,ifexecutedbytheactors,achievesthegoal. (Inthetruemultiagent\nsetting, of course, the agents may not agree to execute any particular plan, but at least they\n6 We apologize to residents of the United Kingdom, where the mere act of contemplating a game of tennis\nguaranteesrain. Section11.4. Multiagent Planning 427\nActors(A,B)\nInit(At(A,LeftBaseline) \u2227 At(B,RightNet) \u2227\nApproaching(Ball,RightBaseline)) \u2227 Partner(A,B) \u2227 Partner(B,A)\nGoal(Returned(Ball) \u2227 (At(a,RightNet) \u2228 At(a,LeftNet))\nAction(Hit(actor,Ball),\nPRECOND:Approaching(Ball,loc) \u2227 At(actor,loc)\nEFFECT:Returned(Ball))\nAction(Go(actor,to),\nPRECOND:At(actor,loc) \u2227 to (cid:7)= loc,\nEFFECT:At(actor,to) \u2227 \u00acAt(actor,loc))\nFigure11.10 Thedoublestennisproblem. TwoactorsAandB areplayingtogetherand\ncanbeinoneoffourlocations: LeftBaseline,RightBaseline,LeftNet,andRightNet. The\nballcanbereturnedonlyifaplayerisintherightplace. Notethateachactionmustinclude\ntheactorasanargument.\nwill know what plans would work if they did agree to execute them.) For simplicity, we\nassume perfect synchronization: each action takes the same amount of time and actions at\nSYNCHRONIZATION\neachpointinthejointplanaresimultaneous.\nWe begin with the transition model; for the deterministic case, this is the function\nRESULT(s,a). In the single-agent setting, there might be b different choices for the action;\nb can be quite large, especially for first-order representations with many objects to act on,\nbut action schemas provide a concise representation nonetheless. In the multiactor setting\nwith n actors, the single action a is replaced by a joint action (cid:16)a ,...,a (cid:17), where a is the\nJOINTACTION 1 n i\naction taken by the ith actor. Immediately, we see two problems: first, we have to describe\nthe transition model for bn different joint actions; second, we have a joint planning problem\nwithabranching factorofbn.\nHaving put the actors together into a multiactor system with a huge branching factor,\nthe principal focus of research on multiactor planning has been to decouple the actors to\nthe extent possible, so that the complexity of the problem grows linearly with n rather than\nexponentially. Iftheactorshavenointeractionwithoneanother\u2014forexample,nactorseach\nplaying agameofsolitaire\u2014then wecansimplysolve nseparate problems. Iftheactors are\nloosely coupled,canweattainsomething closetothisexponential improvement? Thisis,of\nLOOSELYCOUPLED\ncourse, a central question in many areas of AI. We have seen it explicitly in the context of\nCSPs,where \u201ctree like\u201d constraint graphs yielded efficient solution methods (see page 225),\nas well as in the context of disjoint pattern databases (page 106) and additive heuristics for\nplanning (page378).\nThestandardapproachtolooselycoupledproblemsistopretendtheproblemsarecom-\npletelydecoupledandthenfixuptheinteractions. Forthetransitionmodel,thismeanswriting\nactionschemasasiftheactorsactedindependently. Let\u2019sseehowthisworksforthedoubles\ntennisproblem. Let\u2019ssupposethatatonepointinthegame,theteamhasthegoalofreturning\nthe ball that has been hit to them and ensuring that at least one of them is covering the net. 428 Chapter 11. PlanningandActingintheRealWorld\nA firstpass at amultiactor definition might look like Figure 11.10. With this definition, itis\neasytoseethatthefollowingjointplanplanworks:\nJOINTPLAN\nPLAN 1:\nA: [Go(A,RightBaseline),Hit(A,Ball)]\nB : [NoOp(B),NoOp(B)].\nProblemsarise,however,whenaplanhasbothagentshitting theballatthesametime. Inthe\nreal world, this won\u2019t work, but the action schema for Hit says that the ball will be returned\nsuccessfully. Technically, the difficulty is that preconditions constrain the state in which an\naction canbeexecuted successfully, butdonotconstrain otheractions thatmightmessitup.\nCONCURRENT Wesolvethisbyaugmenting action schemaswithonenewfeature: a concurrentaction list\nACTIONLIST\nstatingwhichactionsmustormustnotbeexecutedconcurrently. Forexample,theHit action\ncouldbedescribed asfollows:\nAction(Hit(a,Ball),\nCONCURRENT:b(cid:7)= a \u21d2 \u00acHit(b,Ball)\nPRECOND:Approaching(Ball,loc)\u2227At(a,loc)\nEFFECT:Returned(Ball)).\nIn other words, the Hit action has its stated effect only if no other Hit action by another\nagent occurs at the same time. (In the SATPLAN approach, this would be handled by a\npartial action exclusion axiom.) Forsome actions, the desired effect is achieved only when\nanotheractionoccursconcurrently. Forexample,twoagentsareneededtocarryacoolerfull\nofbeverages tothetenniscourt:\nAction(Carry(a,cooler,here,there),\nCONCURRENT:b (cid:7)= a\u2227Carry(b,cooler,here,there)\nPRECOND:At(a,here)\u2227At(cooler,here)\u2227Cooler(cooler)\nEFFECT:At(a,there)\u2227At(cooler,there)\u2227\u00acAt(a,here)\u2227\u00acAt(cooler,here)).\nWith these kinds of action schemas, any of the planning algorithms described in Chapter 10\ncanbeadaptedwithonlyminormodificationstogeneratemultiactorplans. Totheextentthat\nthecouplingamongsubplansisloose\u2014meaning thatconcurrency constraints comeintoplay\nonly rarely during plan search\u2014one would expect the various heuristics derived for single-\nagent planning to also be effective in the multiactor context. Wecould extend this approach\nwiththerefinementsofthelasttwochapters\u2014HTNs,partialobservability, conditionals, exe-\ncutionmonitoring, andreplanning\u2014but thatisbeyondthescopeofthisbook.\n11.4.2 Planning withmultipleagents: Cooperationand coordination\nNow let us consider the true multiagent setting in which each agent makes its own plan. To\nstart with, let us assume that the goals and knowledge base are shared. One might think\nthat this reduces to the multibody case\u2014each agent simply computes the joint solution and\nexecutes its own part of that solution. Alas, the \u201cthe\u201d in \u201cthe joint solution\u201d is misleading.\nForourdoublesteam,morethanonejointsolutionexists:\nPLAN 2:\nA: [Go(A,LeftNet),NoOp(A)]\nB : [Go(B,RightBaseline),Hit(B,Ball)]. Section11.4. Multiagent Planning 429\nIfbothagentscanagreeoneitherplan1orplan2,thegoalwillbeachieved. ButifAchooses\nplan2andBchoosesplan1,thennobodywillreturntheball. Conversely,ifAchooses1and\nB chooses 2,then theywillboth trytohittheball. Theagents mayrealize this, but howcan\ntheycoordinate tomakesuretheyagreeontheplan?\nOneoption is to adopt aconvention before engaging in joint activity. A convention is\nCONVENTION\nanyconstraintontheselectionofjointplans. Forexample, theconvention \u201csticktoyourside\nofthecourt\u201d wouldrule outplan 1,causing thedoubles partners toselect plan 2. Driverson\naroadfacetheproblemofnotcollidingwitheachother;this is(partially)solvedbyadopting\nthe convention \u201cstay on the right side of the road\u201d in most countries; the alternative, \u201cstay\non the left side,\u201d works equally well as long as all agents in an environment agree. Similar\nconsiderations applytothedevelopmentofhumanlanguage, wheretheimportantthingisnot\nwhich language each individual should speak, but the fact that a community all speaks the\nsamelanguage. Whenconventions arewidespread, theyarecalled sociallaws.\nSOCIALLAWS\nIn the absence of a convention, agents can use communication to achieve common\nknowledge of a feasible joint plan. For example, a tennis player could shout \u201cMine!\u201d or\n\u201cYours!\u201d toindicateapreferredjointplan. Wecovermechanismsforcommunicationinmore\ndepth in Chapter 22, where we observe that communication does not necessarily involve a\nverbalexchange. Forexample,oneplayercancommunicateapreferredjointplantotheother\nsimply by executing the first part of it. If agent Aheads forthe net, then agent B is obliged\ntogobacktothebaselinetohittheball,becauseplan2istheonlyjointplanthatbeginswith\nA\u2019s heading forthe net. This approach to coordination, sometimes called planrecognition,\nPLANRECOGNITION\nworkswhenasingleaction(orshortsequence ofactions)isenoughtodetermineajointplan\nunambiguously. Note that communication can workas wellwith competitive agents aswith\ncooperative ones.\nConventions can also arise through evolutionary processes. For example, seed-eating\nharvester ants are social creatures that evolved from the less social wasps. Colonies of ants\nexecute very elaborate joint plans without any centralized control\u2014the queen\u2019s job is to re-\nproduce, not to do centralized planning\u2014and with very limited computation, communica-\ntion,andmemorycapabilities ineachant(Gordon, 2000,2007). Thecolonyhasmanyroles,\nincluding interior workers, patrollers, and foragers. Each ant chooses to perform a role ac-\ncording tothe local conditions it observes. Forexample, foragers travel awayfrom the nest,\nsearchforaseed,andwhentheyfindone, bringitbackimmediately. Thus,therateatwhich\nforagers return to the nest is an approximation of the availability of food today. When the\nrateishigh, otherantsabandon theircurrent roleandtakeontheroleofscavenger. Theants\nappeartohaveaconventionontheimportanceofroles\u2014foragingisthemostimportant\u2014and\nantswilleasilyswitchintothemoreimportantroles, butnotintothelessimportant. Thereis\nsomelearningmechanism: acolonylearnstomakemoresuccessfulandprudentactionsover\nthecourseofitsdecades-long life,eventhoughindividual antsliveonlyaboutayear.\nOnefinalexampleofcooperative multiagent behavior appears intheflocking behavior\nof birds. We can obtain a reasonable simulation of a flock if each bird agent (sometimes\nBOID called a boid) observes the positions of its nearest neighbors and then chooses the heading\nandacceleration thatmaximizestheweightedsumofthesethreecomponents: 430 Chapter 11. PlanningandActingintheRealWorld\n(a) (b) (c)\nFigure11.11 (a)Asimulatedflockofbirds,usingReynold\u2019sboidsmodel.Imagecourtesy\nGiuseppeRandazzo,novastructura.net. (b)An actualflock ofstarlings. ImagebyEduardo\n(pastaboy sleeps on flickr). (c) Two competitiveteams of agents attempting to capture the\ntowersintheNEROgame.ImagecourtesyRistoMiikkulainen.\n1. Cohesion: apositivescoreforgettingclosertotheaverageposition oftheneighbors\n2. Separation: anegativescoreforgetting tooclosetoanyoneneighbor\n3. Alignment: apositivescoreforgetting closertotheaverageheadingoftheneighbors\nEMERGENT If all the boids execute this policy, the flock exhibits the emergent behavior of flying as a\nBEHAVIOR\npseudorigid body with roughly constant density that does not disperse over time, and that\noccasionally makessudden swooping motions. Youcanseeastillimages inFigure11.11(a)\nand compare it to an actual flock in (b). As with ants, there is no need for each agent to\npossessajointplanthatmodelstheactionsofotheragents.\nThemostdifficultmultiagentproblemsinvolvebothcooperationwithmembersofone\u2019s\nown team and competition against members of opposing teams, all without centralized con-\ntrol. WeseethisingamessuchasroboticsoccerortheNEROgameshowninFigure11.11(c),\ninwhich two teamsof software agents compete to capture the control towers. Asyet, meth-\nods for efficient planning in these kinds of environments\u2014for example, taking advantage of\nloosecoupling\u2014are intheirinfancy.\n11.5 SUMMARY\nThischapterhasaddressedsomeofthecomplicationsofplanningandactingintherealworld.\nThemainpoints:\n\u2022 Manyactionsconsumeresources,suchasmoney,gas,orrawmaterials. Itisconvenient\nto treat these resources as numeric measures in a pool rather than try to reason about,\nsay, each individual coin and bill in the world. Actions can generate and consume\nresources, anditisusually cheapandeffective tocheck partialplans forsatisfaction of\nresourceconstraints beforeattemptingfurtherrefinements.\n\u2022 Timeisoneofthemostimportant resources. Itcanbehandled byspecialized schedul-\ningalgorithms, orscheduling canbeintegrated withplanning. Bibliographical andHistorical Notes 431\n\u2022 Hierarchical task network (HTN) planning allows the agent to take advice from the\ndomain designer inthe form of high-level actions (HLAs)that can beimplemented in\nvariouswaysbylower-levelactionsequences. TheeffectsofHLAscanbedefinedwith\nangelic semantics, allowing provably correct high-level plans to be derived without\nconsideration of lower-level implementations. HTN methods can create the very large\nplansrequired bymanyreal-worldapplications.\n\u2022 Standard planning algorithms assume complete and correct information and determin-\nistic,fullyobservable environments. Manydomainsviolatethisassumption.\n\u2022 Contingent plans allow the agent to sense the world during execution to decide what\nbranchoftheplantofollow. Insomecases,sensorlessorconformantplanningcanbe\nused to construct a plan that works without the need for perception. Both conformant\nandcontingentplanscanbeconstructedbysearchinthespaceofbeliefstates. Efficient\nrepresentation orcomputation ofbeliefstatesisakeyproblem.\n\u2022 An online planning agent uses execution monitoring and splices in repairs as needed\nto recover from unexpected situations, which can be due to nondeterministic actions,\nexogenous events,orincorrect modelsoftheenvironment.\n\u2022 Multiagentplanning isnecessary whenthere areotheragents intheenvironment with\nwhichtocooperate orcompete. Jointplanscanbeconstructed, butmustbeaugmented\nwithsomeformofcoordinationiftwoagentsaretoagreeonwhichjointplantoexecute.\n\u2022 This chapter extends classic planning to cover nondeterministic environments (where\noutcomes of actions are uncertain), but it is not the last word on planning. Chapter 17\ndescribes techniques for stochastic environments (in which outcomes of actions have\nprobabilities associated with them): Markov decision processes, partially observable\nMarkovdecisionprocesses,andgametheory. InChapter21weshowthatreinforcement\nlearningallowsanagenttolearnhowtobehavefrompastsuccesses andfailures.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nPlanning with time constraints was first dealt with by DEVISER (Vere, 1983). The repre-\nsentation of time in plans was addressed by Allen (1984) and by Dean et al. (1990) in the\nFORBIN system. NONLIN+ (Tate and Whiter, 1984) and SIPE (Wilkins, 1988, 1990) could\nreason about the allocation of limited resources to various plan steps. O-PLAN (Bell and\nTate, 1985), an HTN planner, had a uniform, general representation for constraints on time\nand resources. In addition to the Hitachi application mentioned in the text, O-PLAN has\nbeen applied tosoftware procurement planning atPriceWaterhouse andback-axle assembly\nplanning atJaguarCars.\nThe two planners SAPA (Do and Kambhampati, 2001) and T4 (Haslum and Geffner,\n2001) both used forward state-space search with sophisticated heuristics to handle actions\nwith durations and resources. An alternative is to use very expressive action languages, but\nguide them by human-written domain-specific heuristics, as is done by ASPEN (Fukunaga\netal.,1997), HSTS(Jonssonetal.,2000), andIxTeT(GhallabandLaruelle,1994). 432 Chapter 11. PlanningandActingintheRealWorld\nA number of hybrid planning-and-scheduling systems have been deployed: ISIS (Fox\net al., 1982; Fox, 1990) has been used forjob shop scheduling at Westinghouse, GARI (De-\nscotte and Latombe, 1985) planned the machining and construction of mechanical parts,\nFORBIN was used for factory control, and NONLIN+ was used for naval logistics planning.\nWechosetopresentplanningandschedulingastwoseparateproblems;(Cushingetal.,2007)\nshow that this can lead to incompleteness on certain problems. There is a long history of\nscheduling inaerospace. T-SCHED (Drabble, 1990)wasusedtoschedule mission-command\nsequencesfortheUOSAT-II satellite. OPTIMUM-AIV (Aarupetal.,1994)andPLAN-ERS1\n(Fuchs et al., 1990), both based on O-PLAN, were used for spacecraft assembly and obser-\nvation planning, respectively, at the European Space Agency. SPIKE (Johnston and Adorf,\n1992) was used for observation planning at NASA for the Hubble Space Telescope, while\nthe Space Shuttle Ground Processing Scheduling System (Deale et al., 1994) does job-shop\nscheduling of up to 16,000 worker-shifts. Remote Agent (Muscettola et al., 1998) became\nthefirstautonomousplanner\u2013scheduler tocontrolaspacecraftwhenitflewonboardtheDeep\nSpaceOneprobein1999. Spaceapplications havedriventhedevelopment ofalgorithms for\nresourceallocations; seeLaborie(2003)andMuscettola(2002). Theliteratureonscheduling\nis presented in a classic survey article (Lawler et al., 1993), a recent book (Pinedo, 2008),\nandaneditedhandbook (Blazewiczetal.,2007).\nMACROPS\nThefacility inthe STRIPS program forlearning macrops\u2014\u201cmacro-operators\u201d consist-\ningofasequence ofprimitive steps\u2014could beconsidered the firstmechanism forhierarchi-\ncal planning (Fikes et al., 1972). Hierarchy was also used in the LAWALY system (Siklossy\nand Dreussi, 1973). The ABSTRIPS system (Sacerdoti, 1974) introduced the idea of an ab-\nABSTRACTION straction hierarchy, whereby planning at higher levels was permitted to ignore lower-level\nHIERARCHY\npreconditions of actions in order to derive the general structure of a working plan. Austin\nTate\u2019s Ph.D. thesis (1975b) and work by Earl Sacerdoti (1977) developed the basic ideas of\nHTN planning in its modern form. Many practical planners, including O-PLAN and SIPE,\nare HTNplanners. Yang (1990) discusses properties of actions that make HTNplanning ef-\nficient. Erol, Hendler, and Nau (1994, 1996) present a complete hierarchical decomposition\nplanner aswellasarange ofcomplexity results forpure HTNplanners. Ourpresentation of\nHLAsandangelicsemanticsisduetoMarthietal.(2007,2008). Kambhampatietal.(1998)\nhaveproposedanapproachinwhichdecompositionsarejustanotherformofplanrefinement,\nsimilartotherefinementsfornon-hierarchical partial-order planning.\nBeginningwiththeworkonmacro-operatorsinSTRIPS,oneofthegoalsofhierarchical\nplanninghasbeenthereuseofpreviousplanningexperience intheformofgeneralizedplans.\nThe technique of explanation-based learning, described in depth in Chapter 19, has been\napplied in several systems as a means of generalizing previously computed plans, including\nSOAR (Laird etal.,1986) and PRODIGY (Carbonell etal.,1989). Analternative approach is\nto store previously computed plans in their original form and then reuse them to solve new,\nsimilarproblems by analogy to the original problem. Thisis theapproach taken by thefield\nCASE-BASED called case-based planning (Carbonell, 1983; Alterman, 1988; Hammond, 1989). Kamb-\nPLANNING\nhampati (1994) argues that case-based planning should be analyzed as a form of refinement\nplanning andprovidesaformalfoundation forcase-based partial-order planning. Bibliographical andHistorical Notes 433\nEarly planners lacked conditionals and loops, but some could use coercion to form\nconformant plans. Sacerdoti\u2019s NOAH solved the\u201ckeysandboxes\u201dproblem, aplanning chal-\nlenge problem in which the planner knows little about the initial state, using coercion. Ma-\nson (1993) argued that sensing often can and should be dispensed with in robotic planning,\nand described a sensorless plan that can move a tool into a specific position on a table by a\nsequence oftiltingactions, regardless oftheinitialposition.\nGoldmanandBoddy(1996)introducedthetermconformantplanning,notingthatsen-\nsorless plans are often effective even if the agent has sensors. The first moderately efficient\nconformant planner was Smith and Weld\u2019s (1998) Conformant Graphplan or CGP. Ferraris\nandGiunchiglia(2000)andRintanen(1999)independently developedSATPLAN-basedcon-\nformantplanners. BonetandGeffner(2000)describeaconformantplannerbasedonheuristic\nsearchinthespaceofbeliefstates,drawingonideasfirstdevelopedinthe1960sforpartially\nobservable Markovdecisionprocesses, orPOMDPs(seeChapter17).\nCurrently, there are three main approaches to conformant planning. The first two use\nheuristic search in belief-state space: HSCP (Bertoli et al., 2001a) uses binary decision\ndiagrams (BDDs) to represent belief states, whereas Hoffmann and Brafman (2006) adopt\nthe lazy approach of computing precondition and goal tests on demand using a SAT solver.\nThe third approach, championed primarily by Jussi Rintanen (2007), formulates the entire\nsensorless planning problem as a quantified Boolean formula (QBF) and solves it using a\ngeneral-purpose QBFsolver. Currentconformantplannersarefiveordersofmagnitudefaster\nthan CGP. The winner of the 2006 conformant-planning track at the International Planning\nCompetition wasT (PalaciosandGeffner, 2007), whichusesheuristic search inbelief-state\n0\nspace while keeping the belief-state representation simple by defining derived literals that\ncoverconditional effects. BryceandKambhampati(2007)discusshowaplanning graphcan\nbegeneralized togenerategoodheuristics forconformant andcontingent planning.\nThere has been some confusion in the literature between the terms \u201cconditional\u201d and\n\u201ccontingent\u201d planning. Following Majercik and Littman (2003), we use \u201cconditional\u201d to\nmean a plan (or action) that has different effects depending on the actual state of the world,\nand \u201ccontingent\u201d to mean a plan in which the agent can choose different actions depending\non the results of sensing. The problem of contingent planning received more attention after\nthepublication ofDrewMcDermott\u2019s(1978a)influential article, PlanningandActing.\nThe contingent-planning approach described in the chapter is based on Hoffmann and\nBrafman (2005), and was influenced by the efficient search algorithms for cyclic AND\u2013OR\ngraphs developed by Jimenez and Torras (2000) and Hansen and Zilberstein (2001). Bertoli\net al. (2001b) describe MBP (Model-Based Planner), which uses binary decision diagrams\ntodoconformant andcontingent planning.\nInretrospect, itisnow possible to see how the majorclassical planning algorithms led\ntoextendedversionsforuncertaindomains. Fast-forwardheuristicsearchthroughstatespace\nled to forward search in belief space (Bonet and Geffner, 2000; Hoffmann and Brafman,\n2005); SATPLAN ledtostochastic SATPLAN (MajercikandLittman,2003)andtoplanning\nwithquantified Boolean logic (Rintanen, 2007); partial order planning led to UWL (Etzioni\net al., 1992) and CNLP (Peot and Smith, 1992); GRAPHPLAN led to Sensory Graphplan or\nSGP (Weldetal.,1998). 434 Chapter 11. PlanningandActingintheRealWorld\nThe first online planner with execution monitoring was PLANEX (Fikes et al., 1972),\nwhich worked with the STRIPS planner to control the robot Shakey. The NASL planner\n(McDermott, 1978a) treated a planning problem simply as a specification for carrying out a\ncomplex action, so that execution and planning were completely unified. SIPE (System for\nInteractive Planning and Execution monitoring) (Wilkins, 1988, 1990) was the first planner\nto deal systematically with the problem of replanning. It has been used in demonstration\nprojects in several domains, including planning operations on the flight deck of an aircraft\ncarrier, job-shop scheduling foran Australian beerfactory, and planning the construction of\nmultistory buildings (KartamandLevitt,1990).\nIn the mid-1980s, pessimism about the slow run times of planning systems led to the\nproposal of reflex agents called reactive planning systems (Brooks, 1986; Agre and Chap-\nREACTIVEPLANNING\nman, 1987). PENGI (Agre and Chapman, 1987) could play a (fully observable) video game\nby using Boolean circuits combined with a \u201cvisual\u201d representation of current goals and the\nagent\u2019sinternalstate. \u201cUniversalplans\u201d(Schoppers,1987,1989)weredevelopedasalookup-\ntable method forreactive planning, but turned out to be arediscovery ofthe idea of policies\nPOLICY\nthathadlongbeenusedinMarkovdecision processes (seeChapter17). Auniversal plan(or\na policy) contains a mapping from any state to the action that should be taken in that state.\nKoenig(2001)surveysonlineplanning techniques, underthenameAgent-Centered Search.\nMultiagent planning has leaped in popularity in recent years, although it does have\na long history. Konolige (1982) formalizes multiagent planning in first-order logic, while\nPednault (1986) givesa STRIPS-style description. Thenotion ofjoint intention, whichis es-\nsentialifagentsaretoexecuteajointplan, comesfromwork oncommunicative acts(Cohen\nand Levesque, 1990; Cohen et al., 1990). Boutilier and Brafman (2001) show how to adapt\npartial-order planning to a multiactor setting. Brafman and Domshlak (2008) devise a mul-\ntiactor planning algorithm whose complexity growsonly linearly withthe number ofactors,\nprovidedthatthedegreeofcoupling(measuredpartlybythe treewidthofthegraphofinter-\nactionsamongagents)isbounded. PetrikandZilberstein(2009)showthatanapproachbased\nonbilinearprogramming outperforms thecover-setapproach weoutlinedinthechapter.\nWe have barely skimmed the surface of work on negotiation in multiagent planning.\nDurfeeand Lesser(1989) discuss how tasks canbeshared out amongagents bynegotiation.\nKrausetal.(1991)describe asystemforplaying Diplomacy,aboardgame requiring negoti-\nation, coalition formation, and dishonesty. Stone (2000) shows how agents can cooperate as\nteammatesinthecompetitive,dynamic,partiallyobservableenvironmentofroboticsoccer. In\na later article, Stone (2003) analyzes two competitive multiagent environments\u2014RoboCup,\na robotic soccer competition, and TAC, the auction-based Trading Agents Competition\u2014\nand finds that the computational intractability of our current theoretically well-founded ap-\nproaches hasledtomanymultiagent systemsbeingdesigned byadhocmethods.\nInhishighly influential Society ofMindtheory, Marvin Minsky(1986, 2007) proposes\nthathumanmindsareconstructed fromanensemble ofagents. LivnatandPippenger (2006)\nprovethat,fortheproblemofoptimalpath-finding,andgivenalimitationonthetotalamount\nofcomputing resources, the best architecture foranagent isanensemble ofsubagents, each\nofwhichtriestooptimizeitsownobjective,andallofwhichareinconflictwithoneanother. Exercises 435\nTheboid model on page 429 is due to Reynolds (1987), whowon an Academy Award\nforitsapplication toswarmsofpenguins inBatmanReturns. TheNERO gameandthemeth-\nodsforlearningstrategies aredescribed byBryantandMiikkulainen (2007).\nRecent book on multiagent systems include those by Weiss (2000a), Young (2004),\nVlassis (2008), and Shoham and Leyton-Brown (2009). There is an annual conference on\nautonomous agentsandmultiagent systems(AAMAS).\nEXERCISES\n11.1 Thegoals wehave considered sofarall ask the planner to make the world satisfy the\ngoal at just one time step. Not all goals can be expressed this way: you do not achieve the\ngoal of suspending a chandelier above the ground by throwing it in the air. More seriously,\nyou wouldn\u2019t want your spacecraft life-support system to supply oxygen one day but not\nthe next. A maintenance goal is achieved when the agent\u2019s plan causes a condition to hold\ncontinuouslyfromagivenstateonward. Describehowtoextendtheformalismofthischapter\ntosupport maintenance goals.\n11.2 You have a number of trucks with which to deliver a set of packages. Each package\nstartsatsomelocationonagridmap,andhasadestinationsomewhereelse. Eachtruckisdi-\nrectlycontrolled bymovingforwardandturning. Construct ahierarchy ofhigh-level actions\nforthisproblem. Whatknowledge aboutthesolution doesyourhierarchyencode?\n11.3 Suppose that a high-level action has exactly one implementation as a sequence of\nprimitive actions. Give an algorithm for computing its preconditions and effects, given the\ncompleterefinementhierarchyandschemasfortheprimitive actions.\n11.4 Suppose thatthe optimistic reachable set ofahigh-level plan isasuperset ofthegoal\nset; can anything be concluded about whether the plan achieves the goal? What if the pes-\nsimisticreachable setdoesn\u2019t intersect thegoalset? Explain.\n11.5 Writeanalgorithmthattakesaninitialstate(specifiedbyasetofpropositionalliterals)\nand a sequence of HLAs (each defined by preconditions and angelic specifications of opti-\nmistic and pessimistic reachable sets) and computes optimistic and pessimistic descriptions\nofthereachable setofthesequence.\n11.6 In Figure 11.2 we showed how to describe actions in a scheduling problem by using\nseparate fields for DURATION, USE, and CONSUME. Now suppose we wanted to combine\nscheduling withnondeterministic planning, whichrequires nondeterministic andconditional\neffects. Considereachofthethreefieldsandexplainifthey shouldremainseparate fields,or\niftheyshouldbecomeeffectsoftheaction. Giveanexampleforeachofthethree.\n11.7 Someoftheoperationsinstandardprogramminglanguagescanbemodeledasactions\nthat change the state of the world. For example, the assignment operation changes the con-\ntentsofamemorylocation, andtheprintoperation changes thestate oftheoutput stream. A\nprogram consisting oftheseoperations canalsobeconsidered asaplan, whosegoalisgiven 436 Chapter 11. PlanningandActingintheRealWorld\nbythespecification oftheprogram. Therefore, planning algorithms canbeusedtoconstruct\nprogramsthatachieveagivenspecification.\na. Writeanactionschemafortheassignmentoperator(assigningthevalueofonevariable\ntoanother). Rememberthattheoriginal valuewillbeoverwritten!\nb. Show how object creation can be used by a planner to produce a plan for exchanging\nthevaluesoftwovariablesbyusingatemporaryvariable.\n11.8 Suppose the Flip action always changes the truth value of variable L. Show how\nto define its effects by using an action schema with conditional effects. Show that, despite\nthe use of conditional effects, a 1-CNF belief state representation remains in 1-CNF after a\nFlip.\n11.9 In the blocks world we were forced to introduce two action schemas, Move and\nMoveToTable, in order to maintain the Clear predicate properly. Show how conditional\neffectscanbeusedtorepresent bothofthesecaseswithasingleaction.\n11.10 ConditionaleffectswereillustratedfortheSuck actioninthevacuumworld\u2014which\nsquarebecomescleandependsonwhichsquaretherobotisin. Canyouthinkofanewsetof\npropositional variables todefinestatesofthevacuum world, suchthatSuck hasanuncondi-\ntional description? Writeout thedescriptions of Suck,Left,andRight,using yourproposi-\ntions,anddemonstrate thattheysufficetodescribeallpossible statesoftheworld.\n11.11 Find a suitably dirty carpet, free of obstacles, and vacuum it. Draw the path taken\nby the vacuum cleaner as accurately as you can. Explain it, with reference to the forms of\nplanning discussed inthischapter.\n11.12 To the medication problem in the previous exercise, add a Test action that has the\nconditional effect CultureGrowth when Disease is true and in any case has the perceptual\neffect Known(CultureGrowth). Diagram a conditional plan that solves the problem and\nminimizestheuseoftheMedicate action. 12\nKNOWLEDGE\nREPRESENTATION\nIn which we show how to use first-order logic to represent the most important\naspectsoftherealworld,suchasaction, space,time,thoughts, andshopping.\nThe previous chapters described the technology for knowledge-based agents: the syntax,\nsemantics, andproof theoryofpropositional andfirst-orderlogic, andtheimplementation of\nagents that use these logics. In this chapter we address the question of what content to put\nintosuchanagent\u2019sknowledge base\u2014howtorepresent factsabouttheworld.\nSection 12.1 introduces the idea of a general ontology, which organizes everything in\nthe world into a hierarchy of categories. Section 12.2 covers the basic categories of objects,\nsubstances, andmeasures;Section12.3coversevents,andSection12.4discussesknowledge\nabout beliefs. We then return to consider the technology for reasoning with this content:\nSection 12.5 discusses reasoning systems designed for efficient inference with categories,\nand Section 12.6 discusses reasoning with default information. Section 12.7 brings all the\nknowledgetogetherinthecontextofanInternet shopping environment.\n12.1 ONTOLOGICAL ENGINEERING\nIn\u201ctoy\u201ddomains,thechoiceofrepresentation isnotthatimportant;manychoiceswillwork.\nComplex domains such as shopping on the Internet or driving a car in traffic require more\ngeneral andflexiblerepresentations. Thischaptershowshowtocreatethese representations,\nconcentrating on general concepts\u2014such as Events, Time, Physical Objects, and Beliefs\u2014\nthat occur in many different domains. Representing these abstract concepts is sometimes\nONTOLOGICAL calledontological engineering.\nENGINEERING\nTheprospect of representing everything in the world is daunting. Ofcourse, wewon\u2019t\nactually write a complete description of everything\u2014that would be far too much for even a\n1000-page textbook\u2014but we will leave placeholders where new knowledge for any domain\ncanfitin. Forexample,wewilldefinewhatitmeanstobeaphysicalobject,andthedetailsof\ndifferenttypesofobjects\u2014robots,televisions,books,orwhatever\u2014canbefilledinlater. This\nisanalogoustothewaythatdesignersofanobject-oriented programmingframework(suchas\ntheJavaSwinggraphicalframework)definegeneralconcepts likeWindow,expectingusersto\n437 438 Chapter 12. KnowledgeRepresentation\nAnything\nAbstractObjects GeneralizedEvents\nSets Numbers RepresentationalObjects Interval Places PhysicalObjects Processes\nCategories Sentences Measurements Moments Things Stuff\nTimes Weights Animals Agents Solid Liquid Gas\nHumans\nFigure 12.1 The upperontologyof the world, showingthe topics to be coveredlater in\nthechapter. Each linkindicatesthatthe lowerconceptisa specializationof the upperone.\nSpecializations are not necessarily disjoint; a human is both an animal and an agent, for\nexample.WewillseeinSection12.3.3whyphysicalobjectscomeundergeneralizedevents.\nuse these to define more specific concepts like SpreadsheetWindow. The general framework\nof concepts is called an upper ontology because of the convention of drawing graphs with\nUPPERONTOLOGY\nthegeneralconceptsatthetopandthemorespecificconcepts belowthem,asinFigure12.1.\nBefore considering the ontology further, we should state one important caveat. We\nhave elected to use first-order logic to discuss the content and organization of knowledge,\nalthoughcertainaspectsoftherealworldarehardtocaptureinFOL.Theprincipaldifficulty\nisthatmostgeneralizations haveexceptions orholdonly to adegree. Forexample, although\n\u201ctomatoes are red\u201d is a useful rule, some tomatoes are green, yellow, or orange. Similar\nexceptionscanbefoundtoalmostalltherulesinthischapter. Theabilitytohandleexceptions\nand uncertainty is extremely important, but is orthogonal to the task of understanding the\ngeneralontology. Forthisreason,wedelaythediscussionofexceptionsuntilSection12.5of\nthischapter, andthemoregeneraltopicofreasoning withuncertainty untilChapter13.\nOfwhat use is an upper ontology? Consider the ontology for circuits inSection 8.4.2.\nItmakesmanysimplifyingassumptions: timeisomittedcompletely; signalsarefixedanddo\nnot propagate; the structure of the circuit remains constant. Amore general ontology would\nconsider signals at particular times, and would include the wire lengths and propagation de-\nlays. This would allow us to simulate the timing properties of the circuit, and indeed such\nsimulations are often carried out by circuit designers. We could also introduce more inter-\nesting classes of gates, for example, by describing the technology (TTL,CMOS,and so on)\naswellastheinput\u2013output specification. Ifwewantedtodiscuss reliability ordiagnosis, we\nwould include the possibility that the structure of the circuit or the properties of the gates\nmight change spontaneously. Toaccount forstray capacitances, wewould need to represent\nwherethewiresareontheboard. Section12.1. Ontological Engineering 439\nIfwelookatthewumpusworld,similarconsiderationsapply. Althoughwedorepresent\ntime, ithasasimplestructure: Nothing happens except when theagent acts, andallchanges\nareinstantaneous. Amoregeneral ontology, bettersuitedfortherealworld,wouldallowfor\nsimultaneouschangesextendedovertime. WealsousedaPit predicatetosaywhichsquares\nhave pits. We could have allowed for different kinds of pits by having several individuals\nbelonging to the class of pits, each having different properties. Similarly, we might want to\nallow for other animals besides wumpuses. It might not be possible to pin down the exact\nspecies from the available percepts, so we would need to build up a biological taxonomy to\nhelptheagentpredictthebehaviorofcave-dwellers fromscantyclues.\nFor any special-purpose ontology, it is possible to make changes like these to move\ntoward greater generality. Anobvious question then arises: doall these ontologies converge\non a general-purpose ontology? After centuries of philosophical and computational inves-\ntigation, the answer is \u201cMaybe.\u201d In this section, we present one general-purpose ontology\nthat synthesizes ideas from those centuries. Two major characteristics of general-purpose\nontologies distinguish themfromcollections ofspecial-purpose ontologies:\n\u2022 A general-purpose ontology should be applicable in more or less any special-purpose\ndomain (with the addition of domain-specific axioms). This means that no representa-\ntionalissuecanbefinessedorbrushed underthecarpet.\n\u2022 In any sufficiently demanding domain, different areas of knowledge must be unified,\nbecause reasoning and problem solving could involve several areas simultaneously. A\nrobotcircuit-repairsystem,forinstance,needstoreason aboutcircuitsintermsofelec-\ntrical connectivity and physical layout, and about time, both forcircuit timing analysis\nand estimating labor costs. The sentences describing time therefore must be capable\nofbeing combinedwiththosedescribing spatial layoutandmustworkequally wellfor\nnanoseconds andminutesandforangstroms andmeters.\nWe should say up front that the enterprise of general ontological engineering has so far had\nonly limited success. None of the top AI applications (as listed in Chapter 1) make use\nof a shared ontology\u2014they all use special-purpose knowledge engineering. Social\/political\nconsiderations can make it difficult for competing parties to agree on an ontology. As Tom\nGruber (2004) says, \u201cEvery ontology is a treaty\u2014a social agreement\u2014among people with\nsome common motive in sharing.\u201d When competing concerns outweigh the motivation for\nsharing, therecanbenocommonontology. Thoseontologies thatdoexisthavebeencreated\nalongfourroutes:\n1. Byateamoftrainedontologist\/logicians, whoarchitect theontologyandwriteaxioms.\nTheCYC systemwasmostlybuiltthisway(LenatandGuha,1990).\n2. Byimporting categories, attributes, and values from an existing database ordatabases.\nDBPEDIA wasbuiltbyimportingstructured factsfromWikipedia (Bizeretal.,2007).\n3. Byparsing text documents and extracting information from them. TEXTRUNNER was\nbuiltbyreadingalargecorpusofWebpages(BankoandEtzioni,2008).\n4. By enticing unskilled amateurs to enter commonsense knowledge. The OPENMIND\nsystem was built by volunteers who proposed facts in English (Singh et al., 2002;\nChklovskiandGil,2005). 440 Chapter 12. KnowledgeRepresentation\n12.2 CATEGORIES AND OBJECTS\nThe organization of objects into categories is a vital part of knowledge representation. Al-\nCATEGORY\nthoughinteraction withtheworldtakesplaceatthelevelof individual objects, muchreason-\ning takes place at the level of categories. For example, a shopper would normally have the\ngoalofbuying abasketball, ratherthanaparticular basketball suchasBB . Categoriesalso\n9\nserve to make predictions about objects once they are classified. One infers the presence of\ncertainobjectsfromperceptualinput,inferscategorymembershipfromtheperceivedproper-\ntiesoftheobjects, andthenusescategory information tomakepredictions abouttheobjects.\nFor example, from its green and yellow mottled skin, one-foot diameter, ovoid shape, red\nflesh,blackseeds,andpresenceinthefruitaisle,onecaninferthatanobjectisawatermelon;\nfromthis,oneinfersthatitwouldbeusefulforfruitsalad.\nThere are two choices for representing categories in first-order logic: predicates and\nobjects. That is, we can use the predicate Basketball(b), or we can reify1 the category as\nREIFICATION\nan object, Basketballs. We could then say Member(b,Basketballs), which we will abbre-\nviate as b\u2208Basketballs, to say that b is a member of the category of basketballs. We say\nSubset(Basketballs,Balls), abbreviated as Basketballs \u2282 Balls, tosay that Basketballs is\nasubcategoryofBalls. Wewillusesubcategory, subclass, andsubsetinterchangeably.\nSUBCATEGORY\nCategories serve to organize and simplify the knowledge base through inheritance. If\nINHERITANCE\nwe say that all instances of the category Food are edible, and if we assert that Fruit is a\nsubclass of Food and Apples is a subclass of Fruit, then we can infer that every apple is\nedible. We say that the individual apples inherit the property of edibility, in this case from\ntheirmembership inthe Food category.\nSubclassrelationsorganizecategoriesintoataxonomy,ortaxonomichierarchy. Tax-\nTAXONOMY\nonomieshavebeenusedexplicitlyforcenturiesintechnicalfields. Thelargestsuchtaxonomy\norganizesabout10millionlivingandextinctspecies,manyofthembeetles,2 intoasinglehi-\nerarchy;librarysciencehasdeveloped ataxonomyofallfieldsofknowledge, encoded asthe\nDewey Decimal system; and tax authorities and other government departments have devel-\nopedextensivetaxonomiesofoccupationsandcommercialproducts. Taxonomiesarealsoan\nimportantaspectofgeneralcommonsense knowledge.\nFirst-order logic makes it easy to state facts about categories, either by relating ob-\njects to categories orby quantifying overtheir members. Here are some types of facts, with\nexamplesofeach:\n\u2022 Anobjectisamemberofacategory.\nBB \u2208Basketballs\n9\n\u2022 Acategoryisasubclassofanothercategory.\nBasketballs \u2282 Balls\n\u2022 Allmembersofacategoryhavesomeproperties.\n(x\u2208Basketballs) \u21d2 Spherical(x)\n1 Turningapropositionintoanobjectiscalledreification,fromtheLatinwordres,orthing. JohnMcCarthy\nproposedtheterm\u201cthingification,\u201dbutitnevercaughton.\n2 ThefamousbiologistJ.B.S.Haldanededuced\u201cAninordinatefondnessforbeetles\u201donthepartoftheCreator. Section12.2. CategoriesandObjects 441\n\u2022 Membersofacategory canberecognized bysomeproperties.\nOrange(x)\u2227Round(x)\u2227Diameter(x)=9.5(cid:2)(cid:2)\u2227x\u2208Balls \u21d2 x\u2208Basketballs\n\u2022 Acategoryasawholehassomeproperties.\nDogs\u2208DomesticatedSpecies\nNotice that because Dogs is acategory and is amemberof DomesticatedSpecies, the latter\nmust be a category of categories. Of course there are exceptions to many of the above rules\n(punctured basketballs arenotspherical); wedealwiththeseexceptions later.\nAlthough subclass and member relations are the most important ones for categories,\nwe also want to be able to state relations between categories that are not subclasses of each\nother. Forexample, if wejust say that Males and Females are subclasses of Animals, then\nwe have not said that a male cannot be a female. We say that two or more categories are\ndisjoint if they have no members in common. Andeven if weknow that males and females\nDISJOINT\nare disjoint, we will not know that an animal that is not a male must be a female, unless\nEXHAUSTIVE we say that males and females constitute an exhaustive decomposition of the animals. A\nDECOMPOSITION\ndisjointexhaustivedecompositionisknownasapartition. Thefollowingexamplesillustrate\nPARTITION\nthesethreeconcepts:\nDisjoint({Animals,Vegetables})\nExhaustiveDecomposition({Americans,Canadians,Mexicans},\nNorthAmericans)\nPartition({Males,Females},Animals).\n(Note that the ExhaustiveDecomposition of NorthAmericans is not a Partition, because\nsomepeoplehavedualcitizenship.) Thethreepredicates aredefinedasfollows:\nDisjoint(s) \u21d4 (\u2200c ,c c \u2208s\u2227c \u2208s\u2227c (cid:7)= c \u21d2 Intersection(c ,c )={ })\n1 2 1 2 1 2 1 2\nExhaustiveDecomposition(s,c) \u21d4 (\u2200i i\u2208c \u21d4 \u2203c c \u2208s\u2227i\u2208c )\n2 2 2\nPartition(s,c) \u21d4 Disjoint(s)\u2227ExhaustiveDecomposition(s,c).\nCategories can also be defined by providing necessary and sufficient conditions for\nmembership. Forexample,abachelorisanunmarriedadultmale:\nx\u2208Bachelors \u21d4 Unmarried(x)\u2227x\u2208Adults \u2227x\u2208Males .\nAswediscuss in thesidebar onnatural kinds onpage 443, strict logical definitions forcate-\ngoriesareneitheralwayspossible noralwaysnecessary.\n12.2.1 Physicalcomposition\nThe idea that one object can be part of another is a familiar one. One\u2019s nose is part of one\u2019s\nhead, Romania is part of Europe, and this chapter is part of this book. We use the general\nPartOf relationtosaythatonethingispartofanother. Objectscan begroupedintoPartOf\nhierarchies, reminiscent ofthe Subset hierarchy:\nPartOf(Bucharest,Romania)\nPartOf(Romania,EasternEurope)\nPartOf(EasternEurope,Europe)\nPartOf(Europe,Earth). 442 Chapter 12. KnowledgeRepresentation\nThePartOf relation istransitive andreflexive;thatis,\nPartOf(x,y)\u2227PartOf(y,z) \u21d2 PartOf(x,z).\nPartOf(x,x).\nTherefore, wecanconclude PartOf(Bucharest,Earth).\nCategories of composite objects are often characterized by structural relations among\nCOMPOSITEOBJECT\nparts. Forexample,abipedhastwolegsattached toabody:\nBiped(a) \u21d2 \u2203l ,l ,b Leg(l )\u2227Leg(l )\u2227Body(b) \u2227\n1 2 1 2\nPartOf(l ,a)\u2227PartOf(l ,a)\u2227PartOf(b,a) \u2227\n1 2\nAttached(l ,b)\u2227Attached(l ,b) \u2227\n1 2\nl (cid:7)=l \u2227[\u2200l Leg(l )\u2227PartOf(l ,a) \u21d2 (l =l \u2228l =l )].\n1 2 3 3 3 3 1 3 2\nThe notation for \u201cexactly two\u201d is a little awkward; we are forced to say that there are two\nlegs, that they are not the same, and that if anyone proposes a third leg, it must be the same\nas one of the other two. In Section 12.5.2, we describe a formalism called description logic\nmakesiteasiertorepresentconstraints like\u201cexactly two.\u201d\nWe can define a PartPartition relation analogous to the Partition relation for cate-\ngories. (SeeExercise12.8.) AnobjectiscomposedofthepartsinitsPartPartition andcan\nbeviewedasderiving someproperties fromthoseparts. Forexample,themassofacompos-\niteobjectisthesumofthemassesoftheparts. Noticethatthisisnotthecasewithcategories,\nwhichhavenomass,eventhoughtheirelementsmight.\nIt is also useful to define composite objects with definite parts but no particular struc-\nture. For example, we might want to say \u201cThe apples in this bag weigh two pounds.\u201d The\ntemptation would be to ascribe this weight to the set of apples in the bag, but this would be\namistake because thesetisan abstract mathematical concept thathas elements but does not\nhave weight. Instead, we need a new concept, which we will call a bunch. Forexample, if\nBUNCH\ntheapplesareApple ,Apple ,andApple ,then\n1 2 3\nBunchOf({Apple ,Apple ,Apple })\n1 2 3\ndenotesthecompositeobjectwiththethreeapplesasparts(notelements). Wecanthenusethe\nbunchasanormal,albeitunstructured,object. NoticethatBunchOf({x})=x. Furthermore,\nBunchOf(Apples)isthecompositeobjectconsisting ofallapples\u2014not tobeconfusedwith\nApples,thecategory orsetofallapples.\nWecan define BunchOf in terms of the PartOf relation. Obviously, each element of\nsispartofBunchOf(s):\n\u2200x x\u2208s \u21d2 PartOf(x,BunchOf(s)).\nFurthermore, BunchOf(s) is the smallest object satisfying this condition. In other words,\nBunchOf(s)mustbepartofanyobjectthathasalltheelementsofsasparts:\n\u2200y [\u2200x x\u2208s \u21d2 PartOf(x,y)] \u21d2 PartOf(BunchOf(s),y).\nLOGICAL These axioms are an example of a general technique called logical minimization, which\nMINIMIZATION\nmeansdefininganobjectasthesmallestonesatisfying certain conditions. Section12.2. CategoriesandObjects 443\nNATURAL KINDS\nSome categories have strict definitions: an object is a triangle if and only if it is\na polygon with three sides. On the other hand, most categories in the real world\nhavenoclear-cutdefinition;thesearecallednaturalkindcategories. Forexample,\ntomatoestendtobeadullscarlet; roughly spherical; withanindentation atthetop\nwhere the stem was; about two to four inches in diameter; with a thin but tough\nskin; and with flesh, seeds, and juice inside. There is, however, variation: some\ntomatoes are yellow or orange, unripe tomatoes are green, some are smaller or\nlarger than average, and cherry tomatoes are uniformly small. Rather than having\nacomplete definition of tomatoes, wehave aset offeatures that serves toidentify\nobjects that are clearly typical tomatoes, but might not be able to decide forother\nobjects. (Couldtherebeatomatothatisfuzzylikeapeach?)\nThis poses a problem for a logical agent. The agent cannot be sure that an\nobject it has perceived is a tomato, and even if it were sure, it could not be cer-\ntain which of the properties of typical tomatoes this one has. This problem is an\ninevitable consequence ofoperating inpartially observable environments.\nOne useful approach is to separate what is true of all instances of a cate-\ngory from what is true only of typical instances. So in addition to the category\nTomatoes,wewillalsohavethecategoryTypical(Tomatoes). Here,theTypical\nfunctionmapsacategory tothesubclass thatcontainsonlytypicalinstances:\nTypical(c) \u2286 c.\nMostknowledge aboutnatural kindswillactuallybeabouttheirtypicalinstances:\nx\u2208Typical(Tomatoes) \u21d2 Red(x)\u2227Round(x).\nThus, we can write down useful facts about categories without exact defini-\ntions. Thedifficulty ofproviding exact definitions formost natural categories was\nexplainedindepthbyWittgenstein (1953). Heusedtheexampleofgamestoshow\nthat members of a category shared \u201cfamily resemblances\u201d rather than necessary\nand sufficient characteristics: what strict definition encompasses chess, tag, soli-\ntaire,anddodgeball?\nThe utility of the notion of strict definition was also challenged by\nQuine (1953). He pointed out that even the definition of \u201cbachelor\u201d as an un-\nmarried adult male is suspect; one might, for example, question a statement such\nas \u201cthe Pope is a bachelor.\u201d While not strictly false, this usage is certainly infe-\nlicitous because it induces unintended inferences on the part of the listener. The\ntension could perhaps be resolved by distinguishing between logical definitions\nsuitable for internal knowledge representation and the more nuanced criteria for\nfelicitous linguistic usage. Thelattermaybeachieved by\u201cfiltering\u201d theassertions\nderivedfromtheformer. Itisalsopossiblethatfailuresof linguistic usageserveas\nfeedbackformodifyinginternaldefinitions,sothatfilteringbecomesunnecessary. 444 Chapter 12. KnowledgeRepresentation\n12.2.2 Measurements\nIn both scientific and commonsense theories of the world, objects have height, mass, cost,\nand so on. The values that we assign for these properties are called measures. Ordi-\nMEASURE\nnary quantitative measures are quite easy to represent. We imagine that the universe in-\ncludes abstract \u201cmeasure objects,\u201d such as the length that is the length of this line seg-\nment: . Wecancallthislength1.5inchesor3.81centimeters. Thus,\nthe same length has different names in our language.We represent the length with a units\nfunction that takes a number as argument. (An alternative scheme is explored in Exer-\nUNITSFUNCTION\ncise12.9.) IfthelinesegmentiscalledL ,wecanwrite\n1\nLength(L )=Inches(1.5)=Centimeters(3.81).\n1\nConversion betweenunitsisdonebyequatingmultiples ofoneunittoanother:\nCentimeters(2.54\u00d7d)=Inches(d).\nSimilaraxioms can be written for pounds and kilograms, seconds and days, and dollars and\ncents. Measurescanbeusedtodescribe objectsasfollows:\nDiameter(Basketball )=Inches(9.5).\n12\nListPrice(Basketball )=$(19).\n12\nd\u2208Days \u21d2 Duration(d)=Hours(24).\nNotethat$(1) isnotadollarbill! Onecanhavetwodollarbills, butthere isonly one object\nnamed $(1). Note also that, while Inches(0) and Centimeters(0) refer to the same zero\nlength, theyarenotidentical tootherzeromeasures, suchasSeconds(0).\nSimple, quantitative measures areeasy torepresent. Other measures present moreof a\nproblem,becausetheyhavenoagreedscaleofvalues. Exerciseshavedifficulty,dessertshave\ndeliciousness,andpoemshavebeauty,yetnumberscannotbeassignedtothesequalities. One\nmight,inamomentofpureaccountancy,dismisssuchpropertiesasuselessforthepurposeof\nlogical reasoning; or, stillworse, attempttoimpose anumerical scaleonbeauty. Thiswould\nbe agrave mistake, because it isunnecessary. Themost important aspect of measures is not\ntheparticularnumericalvalues, butthefactthatmeasures canbeordered.\nAlthough measures are not numbers, we can still compare them, using an ordering\nsymbol such as >. For example, we might well believe that Norvig\u2019s exercises are tougher\nthanRussell\u2019s, andthatonescoreslessontougherexercises:\ne \u2208Exercises \u2227e \u2208Exercises \u2227Wrote(Norvig,e )\u2227Wrote(Russell,e ) \u21d2\n1 2 1 2\nDifficulty(e )> Difficulty(e ).\n1 2\ne \u2208Exercises \u2227e \u2208Exercises \u2227Difficulty(e ) > Difficulty(e ) \u21d2\n1 2 1 2\nExpectedScore(e )< ExpectedScore(e ).\n1 2\nThisisenoughtoallowonetodecidewhichexercisestodo,eventhoughnonumericalvalues\nfor difficulty were ever used. (One does, however, have to discover who wrote which exer-\ncises.) Thesesortsofmonotonicrelationships amongmeasuresformthebasisforthefieldof\nqualitative physics, a subfield of AI that investigates how to reason about physical systems\nwithout plunging into detailed equations and numerical simulations. Qualitative physics is\ndiscussed inthehistorical notessection. Section12.2. CategoriesandObjects 445\n12.2.3 Objects: Things andstuff\nThe real world can be seen as consisting of primitive objects (e.g., atomic particles) and\ncomposite objects built from them. Byreasoning at the level of large objects such as apples\nandcars,wecanovercomethecomplexityinvolvedindealingwithvastnumbersofprimitive\nobjectsindividually. Thereis,however,asignificantportionofrealitythatseemstodefyany\nobviousindividuation\u2014division intodistinctobjects. Wegivethisportionthegenericname\nINDIVIDUATION\nstuff. For example, suppose I have some butter and an aardvark in front of me. I can say\nSTUFF\nthereisoneaardvark, butthereisnoobviousnumberof\u201cbutter-objects,\u201d becauseanypartof\nabutter-object isalso abutter-object, atleast until wegettovery smallparts indeed. Thisis\nthe major distinction between stuff and things. If we cut an aardvark in half, we do not get\ntwoaardvarks (unfortunately).\nTheEnglish language distinguishes clearly between stuff andthings. Wesay \u201canaard-\nvark,\u201d but, except in pretentious California restaurants, one cannot say \u201ca butter.\u201d Linguists\ndistinguish between countnouns,such asaardvarks, holes, and theorems, and mass nouns,\nCOUNTNOUNS\nsuch as butter, water, and energy. Several competing ontologies claim tohandle this distinc-\nMASSNOUN\ntion. Herewedescribe justone;theothersarecoveredinthe historical notessection.\nTo represent stuff properly, we begin with the obvious. We need to have as objects in\nour ontology at least the gross \u201clumps\u201d of stuff we interact with. For example, we might\nrecognize a lump of butter as the one left on the table the night before; wemight pick it up,\nweigh it, sell it, or whatever. In these senses, it is an object just like the aardvark. Let us\ncallitButter . WealsodefinethecategoryButter. Informally,itselementswillbeallthose\n3\nthingsofwhichonemightsay\u201cIt\u2019sbutter,\u201dincludingButter . Withsomecaveatsaboutvery\n3\nsmallpartsthatwewomitfornow,anypartofabutter-object isalsoabutter-object:\nb\u2208Butter \u2227PartOf(p,b) \u21d2 p\u2208Butter .\nWecannowsaythatbuttermeltsataround30degreescentigrade:\nb\u2208Butter \u21d2 MeltingPoint(b,Centigrade(30)).\nWecould goontosaythatbutterisyellow,islessdensethanwater, issoftatroomtempera-\nture,hasahighfatcontent,andsoon. Ontheotherhand,butterhasnoparticularsize,shape,\nor weight. We can define more specialized categories of butter such as UnsaltedButter,\nwhich is also a kind of stuff. Note that the category PoundOfButter, which includes as\nmembers all butter-objects weighing one pound, is not a kind of stuff. If we cut a pound of\nbutterinhalf,wedonot,alas,gettwopoundsofbutter.\nWhatisactuallygoingonisthis: somepropertiesare intrinsic: theybelongtothevery\nINTRINSIC\nsubstance of the object, rather than to the object as a whole. When you cut an instance of\nstuff in half, the twopieces retain the intrinsic properties\u2014things like density, boiling point,\nflavor, color, ownership, and so on. On the other hand, their extrinsic properties\u2014weight,\nEXTRINSIC\nlength, shape, and so on\u2014are not retained under subdivision. A category of objects that\nincludes in its definition only intrinsic properties is then a substance, or mass noun; a class\nthat includes any extrinsic properties in its definition is acount noun. Thecategory Stuff is\nthe mostgeneral substance category, specifying nointrinsic properties. Thecategory Thing\nisthemostgeneraldiscreteobjectcategory, specifying no extrinsicproperties. 446 Chapter 12. KnowledgeRepresentation\n12.3 EVENTS\nIn Section 10.4.2, we showed how situation calculus represents actions and their effects.\nSituationcalculus islimitedinitsapplicability: itwasdesigned todescribe aworldinwhich\nactions are discrete, instantaneous, and happen one at a time. Consider a continuous action,\nsuchasfillingabathtub. Situationcalculuscansaythatthetubisemptybeforetheactionand\nfull when the action is done, but it can\u2019t talk about what happens during the action. It also\ncan\u2019t describe two actions happening at the same time\u2014such as brushing one\u2019s teeth while\nwaitingforthetubtofill. Tohandlesuchcasesweintroduce analternativeformalismknown\naseventcalculus,whichisbasedonpointsoftimeratherthanonsituations.3\nEVENTCALCULUS\nEvent calculus reifies fluents and events. The fluent At(Shankar,Berkeley) is an ob-\nject that refers to the fact of Shankar being in Berkeley, but does not by itself say anything\nabout whether it is true. To assert that a fluent is actually true at some point in time we use\nthepredicate T,asinT(At(Shankar,Berkeley),t).\nEventsaredescribed asinstances ofeventcategories.4 TheeventE ofShankarflying\n1\nfromSanFranciscotoWashington, D.C.isdescribed as\nE \u2208 Flyings\u2227Flyer(E ,Shankar)\u2227Origin(E ,SF)\u2227Destination(E ,DC).\n1 1 1 1\nIf this is too verbose, we can define an alternative three-argument version of the category of\nflyingeventsandsay\nE \u2208 Flyings(Shankar,SF,DC).\n1\nWethenuseHappens(E ,i)tosaythattheeventE tookplaceoverthetimeinterval i,and\n1 1\nwe say the same thing in functional form with Extent(E )=i. We represent time intervals\n1\nbya(start,end)pairoftimes;thatis, i = (t ,t )isthetimeintervalthatstartsatt andends\n1 2 1\natt . Thecompletesetofpredicates foroneversionoftheeventcalculus is\n2\nT(f,t) Fluentf istrueattimet\nHappens(e,i) Eventehappens overthetimeinterval i\nInitiates(e,f,t) Eventecausesfluentf tostarttoholdattimet\nTerminates(e,f,t) Eventecausesfluentf toceasetoholdattimet\nClipped(f,i) Fluentf ceasestobetrueatsomepointduringtimeinterval i\nRestored(f,i) Fluentf becomestruesometimeduringtimeinterval i\nWeassumeadistinguishedevent,Start,thatdescribestheinitialstatebysayingwhichfluents\nareinitiatedorterminatedatthestarttime. WedefineT bysayingthatafluentholdsatapoint\nintimeifthefluentwasinitiatedbyaneventatsometimeinthepastandwasnotmadefalse\n(clipped)byaninterveningevent. Afluentdoesnotholdifitwasterminatedbyaneventand\n3 Theterms\u201cevent\u201dand\u201caction\u201dmaybeusedinterchangeably. Informally,\u201caction\u201dconnotesanagentwhile\n\u201cevent\u201dconnotesthepossibilityofagentlessactions.\n4 Someversionsofeventcalculusdonotdistinguisheventcategoriesfrominstancesofthecategories. Section12.3. Events 447\nnotmadetrue(restored) byanotherevent. Formally,theaxiomsare:\nHappens(e,(t ,t ))\u2227Initiates(e,f,t )\u2227\u00acClipped(f,(t ,t))\u2227t < t \u21d2\n1 2 1 1 1\nT(f,t)\nHappens(e,(t ,t ))\u2227Terminates(e,f,t )\u2227\u00acRestored(f,(t ,t))\u2227t < t \u21d2\n1 2 1 1 1\n\u00acT(f,t)\nwhereClipped andRestored aredefinedby\nClipped(f,(t ,t )) \u21d4\n1 2\n\u2203e,t,t Happens(e,(t,t ))\u2227t \u2264t < t \u2227Terminates(e,f,t)\n3 3 1 2\nRestored(f,(t ,t )) \u21d4\n1 2\n\u2203e,t,t Happens(e,(t,t ))\u2227t \u2264t < t \u2227Initiates(e,f,t)\n3 3 1 2\nItisconvenient toextendT toworkoverintervals aswellastimepoints; afluentholds over\nanintervalifitholdsoneverypointwithintheinterval:\nT(f,(t ,t )) \u21d4 [\u2200t (t \u2264 t < t ) \u21d2 T(f,t)]\n1 2 1 2\nFluents and actions are defined with domain-specific axioms that are similar to successor-\nstate axioms. For example, we can say that the only way a wumpus-world agent gets an\narrowisatthestart,andtheonlywaytouseupanarrowistoshootit:\nInitiates(e,HaveArrow(a),t) \u21d4 e= Start\nTerminates(e,HaveArrow(a),t) \u21d4 e \u2208 Shootings(a)\nBy reifying events we make it possible to add any amount of arbitrary information about\nthem. For example, we can say that Shankar\u2019s flight was bumpy with Bumpy(E ). In an\n1\nontology whereevents are n-arypredicates, there would benowaytoadd extra information\nlikethis;movingtoann+1-arypredicate isn\u2019tascalablesolution.\nWecanextendeventcalculustomakeitpossibletorepresentsimultaneousevents(such\nastwopeoplebeingnecessarytorideaseesaw),exogenousevents(suchasthewindblowing\nand changing the location of an object), continuous events (such as the level of water in the\nbathtubcontinuously rising)andothercomplications.\n12.3.1 Processes\nTheeventswehaveseensofararewhatwecall discrete events\u2014theyhave adefinitestruc-\nDISCRETEEVENTS\nture. Shankar\u2019striphasabeginning, middle,andend. Ifinterruptedhalfway,theeventwould\nbesomethingdifferent\u2014itwouldnotbeatripfromSanFranciscotoWashington,butinstead\na trip from San Francisco to somewhere over Kansas. On the other hand, the category of\nevents denoted by Flyings has a different quality. If we take a small interval of Shankar\u2019s\nflight, say, the third 20-minute segment (while he waitsanxiously forabag of peanuts), that\neventisstillamemberofFlyings. Infact,thisistrueforanysubinterval.\nCategories of events with this property are called process categories or liquid event\nPROCESS\ncategories. Anyprocess ethathappens overanintervalalsohappensoveranysubinterval:\nLIQUIDEVENT\n(e\u2208Processes)\u2227Happens(e,(t ,t ))\u2227(t <t <t <t ) \u21d2 Happens(e,(t ,t )).\n1 4 1 2 3 4 2 3\nThe distinction between liquid and nonliquid events is exactly analogous to the difference\nbetween substances, or stuff, and individual objects, or things. In fact, some have called\nTEMPORAL liquideventstemporalsubstances,whereassubstances likebutterare spatialsubstances.\nSUBSTANCE\nSPATIALSUBSTANCE 448 Chapter 12. KnowledgeRepresentation\n12.3.2 Timeintervals\nEvent calculus opens us up to the possibility of talking about time, and time intervals. We\nwillconsidertwokindsoftimeintervals: momentsandextendedintervals. Thedistinction is\nthatonlymomentshavezeroduration:\nPartition({Moments,ExtendedIntervals},Intervals)\ni\u2208Moments \u21d4 Duration(i)=Seconds(0).\nNext we invent a time scale and associate points on that scale with moments, giving us ab-\nsolute times. The time scale is arbitrary; we measure it in seconds and say that the moment\nat midnight (GMT) on January 1, 1900, has time 0. The functions Begin and End pick out\ntheearliestandlatestmomentsinaninterval,andthefunctionTime deliversthepointonthe\ntime scale for a moment. The function Duration gives the difference between the end time\nandthestarttime.\nInterval(i) \u21d2 Duration(i)=(Time(End(i))\u2212Time(Begin(i))).\nTime(Begin(AD1900))=Seconds(0).\nTime(Begin(AD2001))=Seconds(3187324800).\nTime(End(AD2001))=Seconds(3218860800).\nDuration(AD2001)=Seconds(31536000).\nTo make these numbers easier to read, we also introduce a function Date, which takes six\narguments (hours, minutes,seconds, day,month,andyear)andreturnsatimepoint:\nTime(Begin(AD2001))=Date(0,0,0,1,Jan,2001)\nDate(0,20,21,24,1,1995)=Seconds(3000000000).\nTwointervals Meet if the end timeof the firstequals the start timeof the second. Thecom-\npletesetofintervalrelations,asproposedbyAllen(1983),isshowngraphicallyinFigure12.2\nandlogically below:\nMeet(i,j) \u21d4 End(i)=Begin(j)\nBefore(i,j) \u21d4 End(i) < Begin(j)\nAfter(j,i) \u21d4 Before(i,j)\nDuring(i,j) \u21d4 Begin(j) < Begin(i) < End(i) < End(j)\nOverlap(i,j) \u21d4 Begin(i) < Begin(j) < End(i) < End(j)\nBegins(i,j) \u21d4 Begin(i) = Begin(j)\nFinishes(i,j) \u21d4 End(i) = End(j)\nEquals(i,j) \u21d4 Begin(i) = Begin(j)\u2227End(i) = End(j)\nThese all have their intuitive meaning, with the exception of Overlap: we tend to think of\noverlap as symmetric (if i overlaps j then j overlaps i), but in this definition, Overlap(i,j)\nonlyholdsifibeginsbeforej. TosaythatthereignofElizabethIIimmediatelyfollowedthat\nofGeorgeVI,andthereignofElvisoverlapped withthe1950s, wecanwritethefollowing:\nMeets(ReignOf(GeorgeVI),ReignOf(ElizabethII)).\nOverlap(Fifties,ReignOf(Elvis)).\nBegin(Fifties)=Begin(AD1950).\nEnd(Fifties)=End(AD1959). Section12.3. Events 449\nFigure12.2 Predicatesontimeintervals.\n1801\n1797 time\n1789\nFigure12.3 AschematicviewoftheobjectPresident(USA)forthefirst15yearsofits\nexistence.\n12.3.3 Fluents and objects\nPhysical objects can be viewed as generalized events, in the sense that a physical object is\na chunk of space\u2013time. For example, USA can be thought of as an event that began in,\nsay, 1776 as a union of 13 states and is still in progress today as a union of 50. We can\ndescribe the changing properties of USA using state fluents, such as Population(USA). A\nproperty oftheUSAthatchanges everyfouroreightyears, barring mishaps, isitspresident.\nOne might propose that President(USA) is a logical term that denotes a different object\nat different times. Unfortunately, this is not possible, because a term denotes exactly one\nobjectinagivenmodelstructure. (Theterm President(USA,t)candenotedifferentobjects,\ndepending onthevalueoft,butourontology keeps timeindices separate from fluents.) The\nW a sh in g to n\nA d a m s\nJe ffe\nrso n 450 Chapter 12. KnowledgeRepresentation\nonly possibility is that President(USA) denotes a single object that consists of different\npeople atdifferent times. Itistheobject thatisGeorgeWashington from1789to1797, John\nAdamsfrom1797to1801,andsoon,asinFigure12.3. TosaythatGeorgeWashingtonwas\npresident throughout 1790,wecanwrite\nT(Equals(President(USA),GeorgeWashington),AD1790).\nWe use the function symbol Equals rather than the standard logical predicate =, because\nwe cannot have a predicate as an argument to T, and because the interpretation is not that\nGeorgeWashington and President(USA)are logically identical in1790; logical identity is\nnotsomethingthatcanchangeovertime. Theidentityisbetweenthesubeventsofeachobject\nthataredefinedbytheperiod1790.\n12.4 MENTAL EVENTS AND MENTAL OBJECTS\nThe agents we have constructed so far have beliefs and can deduce new beliefs. Yet none\nof them has any knowledge about beliefs or about deduction. Knowledge about one\u2019s own\nknowledgeandreasoningprocessesisusefulforcontrollinginference. Forexample,suppose\nAliceasks \u201cwhatisthesquare root of1764\u201d and Bobreplies \u201cI don\u2019t know.\u201d IfAlice insists\n\u201cthink harder,\u201d Bob should realize that with some more thought, this question can in fact\nbe answered. On the other hand, if the question were \u201cIs your mother sitting down right\nnow?\u201d then Bob should realize that thinking harder is unlikely to help. Knowledge about\nthe knowledge of other agents is also important; Bob should realize that his mother knows\nwhethersheissittingornot,andthataskingherwouldbeawaytofindout.\nWhat we need is a model of the mental objects that are in someone\u2019s head (or some-\nthing\u2019s knowledge base) and of the mental processes that manipulate those mental objects.\nThe model does not have to be detailed. We do not have to be able to predict how many\nmilliseconds itwilltake foraparticular agent tomakeadeduction. Wewillbehappy justto\nbeabletoconclude thatmotherknowswhetherornotsheissitting.\nPROPOSITIONAL We begin with the propositional attitudes that an agent can have toward mental ob-\nATTITUDE\njects: attitudes such as Believes, Knows, Wants, Intends, and Informs. The difficulty is\nthat these attitudes do not behave like \u201cnormal\u201d predicates. Forexample, suppose we try to\nassertthatLoisknowsthatSupermancanfly:\nKnows(Lois,CanFly(Superman)).\nOneminorissuewiththisisthatwenormallythinkofCanFly(Superman)asasentence,but\nhereitappearsasaterm. ThatissuecanbepatchedupjustbereifyingCanFly(Superman);\nmaking it a fluent. A more serious problem isthat, ifit is true that Superman is Clark Kent,\nthenwemustconcludethatLoisknowsthatClarkcanfly:\n(Superman = Clark)\u2227Knows(Lois,CanFly(Superman))\n|= Knows(Lois,CanFly(Clark)).\nThis is a consequence of the fact that equality reasoning is built into logic. Normally that is\nagood thing; ifouragent knowsthat 2+2 = 4and4 < 5, then wewantouragent toknow Section12.4. MentalEventsandMentalObjects 451\nREFERENTIAL that 2 + 2 < 5. This property is called referential transparency\u2014it doesn\u2019t matter what\nTRANSPARENCY\ntermalogicusestorefertoanobject,whatmattersistheobjectthatthetermnames. Butfor\npropositionalattitudeslikebelievesandknows,wewouldliketohavereferentialopacity\u2014the\ntermsuseddomatter,becausenotallagentsknowwhichtermsareco-referential.\nModallogicisdesignedtoaddressthisproblem. Regularlogicisconcernedwithasin-\nMODALLOGIC\ngle modality, the modality of truth, allowing us to express \u201cP is true.\u201d Modal logic includes\nspecial modal operators that take sentences (rather than terms) as arguments. For example,\n\u201cAknowsP\u201disrepresentedwiththenotationK P,whereKisthemodaloperatorforknowl-\nA\nedge. Ittakes twoarguments, anagent (written asthe subscript) and asentence. The syntax\nofmodallogicisthesameasfirst-orderlogic, except thatsentences canalsobeformedwith\nmodaloperators.\nThe semantics of modal logic is more complicated. In first-order logic a model con-\ntains a set of objects and an interpretation that maps each name to the appropriate object,\nrelation, orfunction. In modal logic wewant to be able to consider both the possibility that\nSuperman\u2019s secret identity is Clark and that it isn\u2019t. Therefore, we will need a more com-\nplicated model, one that consists of a collection of possible worlds rather than just one true\nPOSSIBLEWORLD\nACCESSIBILITY world. Theworlds areconnected inagraph by accessibility relations, one relation foreach\nRELATIONS\nmodaloperator. Wesaythatworld w isaccessible fromworld w withrespect tothemodal\n1 0\noperator K if everything in w is consistent with what A knows in w , and we write this\nA 1 0\nasAcc(K ,w ,w ). Indiagrams such asFigure 12.4 weshow accessibility asan arrow be-\nA 0 1\ntweenpossibleworlds. Asanexample,intherealworld,BucharestisthecapitalofRomania,\nbut for an agent that did not know that, other possible worlds are accessible, including ones\nwherethecapital ofRomaniaisSibiuorSofia. Presumably aworldwhere 2+2 = 5would\nnotbeaccessible toanyagent.\nIngeneral, a knowledge atom K P is true in world w if and only if P istrue in every\nA\nworldaccessible from w. Thetruthofmorecomplexsentences isderivedbyrecursive appli-\ncation ofthis rule and the normal rules offirst-order logic. Thatmeans that modal logic can\nbe used to reason about nested knowledge sentences: what one agent knows about another\nagent\u2019s knowledge. For example, we can say that, even though Lois doesn\u2019t know whether\nSuperman\u2019ssecretidentityisClarkKent,shedoesknowthatClarkknows:\nK [K Identity(Superman,Clark)\u2228K \u00acIdentity(Superman,Clark)]\nLois Clark Clark\nFigure12.4showssomepossibleworldsforthisdomain,withaccessibility relationsforLois\nandSuperman.\nIntheTOP-LEFT diagram,itiscommonknowledgethatSupermanknowshisowniden-\ntity, and neither henorLoishasseen the weatherreport. Soinw theworlds w and w are\n0 0 2\naccessible toSuperman;mayberainispredicted, maybenot. ForLoisallfourworldsareac-\ncessiblefromeachother;shedoesn\u2019tknowanythingaboutthereportorifClarkisSuperman.\nButshedoesknowthatSuperman knowswhetherheisClark, because ineveryworldthatis\naccessible to Lois, eitherSuperman knows I, orhe knows \u00acI. Loisdoes not know which is\nthecase,buteitherwaysheknowsSupermanknows.\nIn the TOP-RIGHT diagram it is common knowledge that Lois has seen the weather\nreport. So in w she knows rain is predicted and in w she knows rain is not predicted.\n4 6 452 Chapter 12. KnowledgeRepresentation\nw: I,R w: \u00acI,R w: I,R w: \u00acI,R\n0 1 4 5\nw: I,\u00acR w: \u00acI,\u00acR w: I,\u00acR w: \u00acI,\u00acR\n2 3 6 7\n(a) (b)\nw: I,R w: \u00acI,R\n4 5\nw: I,R w: \u00acI,R\n0 1\nw: I,\u00acR w: \u00acI,\u00acR\n2 3\nw: I,\u00acR w: \u00acI,\u00acR\n6 7\n(c)\nFigure 12.4 Possible worlds with accessibility relations KSuperman (solid arrows) and\nKLois (dottedarrows). Theproposition Rmeans\u201ctheweatherreportfortomorrowisrain\u201d\nandI means\u201cSuperman\u2019ssecretidentityisClarkKent.\u201d Allworldsareaccessibletothem-\nselves;thearrowsfromaworldtoitselfarenotshown.\nSuperman does not know the report, but he knows that Lois knows, because in every world\nthatisaccessible tohim,eithersheknowsRorsheknows\u00acR.\nInthe BOTTOM diagram werepresent thescenario whereitiscommonknowledge that\nSupermanknowshisidentity, andLoismightormightnothave seentheweatherreport. We\nrepresentthisbycombiningthetwotopscenarios, andaddingarrowstoshowthatSuperman\ndoes not know which scenario actually holds. Lois does know, so we don\u2019t need to add any\narrowsforher. In w Superman still knows I but notR,and nowhedoes notknow whether\n0\nLois knows R. From what Superman knows, he might be in w or w , in which case Lois\n0 2\ndoesnotknowwhetherRistrue,orhecouldbeinw ,inwhichcasesheknowsR,orw ,in\n4 6\nwhichcasesheknows\u00acR.\nThereareaninfinitenumberofpossibleworlds,sothetrickistointroducejusttheones\nyou need to represent what you are trying to model. A new possible world is needed to talk\nabout different possible facts (e.g., rain is predicted or not), or to talk about different states\nofknowledge (e.g., does Loisknow that rain ispredicted). Thatmeanstwopossible worlds,\nsuchasw andw inFigure12.4,mighthavethesamebasefactsabout theworld, butdiffer\n4 0\nintheiraccessibility relations, andthereforeinfactsaboutknowledge.\nModallogic solvessometricky issues withtheinterplay ofquantifiers and knowledge.\nTheEnglishsentence\u201cBondknowsthatsomeoneisaspy\u201disambiguous. Thefirstreadingis Section12.5. Reasoning SystemsforCategories 453\nthatthereisaparticularsomeonewhoBondknowsisaspy;wecanwritethisas\n\u2203x K Spy(x),\nBond\nwhich in modal logic means that there is an x that, in all accessible worlds, Bond knows to\nbeaspy. Thesecondreading isthatBondjustknowsthatthere isatleastonespy:\nK \u2203x Spy(x).\nBond\nThemodallogic interpretation isthat ineach accessible worldthere isan xthat isaspy, but\nitneednotbethesamexineachworld.\nNow that we have a modal operator for knowledge, we can write axioms for it. First,\nwe can say that agents are able to draw deductions; if an agent knows P and knows that P\nimpliesQ,thentheagentknowsQ:\n(K P \u2227K (P \u21d2 Q)) \u21d2 K Q.\na a a\nFromthis(and afewotherrules about logical identities) we canestablish thatK (P \u2228\u00acP)\nA\nis a tautology; every agent knows every proposition P is either true or false. On the other\nhand,(K P)\u2228(K \u00acP)isnotatautology; ingeneral, therewillbelotsofpropositions that\nA A\nanagentdoesnotknowtobetrueanddoesnotknowtobefalse.\nIt is said (going back to Plato) that knowledge is justified true belief. That is, if it is\ntrue, if you believe it, and if you have an unassailably good reason, then you know it. That\nmeansthatifyouknowsomething, itmustbetrue,andwehavetheaxiom:\nK P \u21d2 P .\na\nFurthermore, logical agents should be able to introspect on their own knowledge. If they\nknowsomething, thentheyknowthattheyknowit:\nK P \u21d2 K (K P).\na a a\nWecandefinesimilaraxiomsforbelief(oftendenotedbyB)andothermodalities. However,\nLOGICAL one problem with the modal logic approach is that it assumes logical omniscience on the\nOMNISCIENCE\npart of agents. Thatis, ifan agent knows aset of axioms, then it knows allconsequences of\nthose axioms. Thisisonshaky ground evenforthesomewhatabstract notion ofknowledge,\nbutitseemsevenworseforbelief, becausebeliefhasmoreconnotation ofreferringtothings\nthat are physically represented in the agent, not just potentially derivable. There have been\nattempts to define a form of limited rationality for agents; to say that agents believe those\nassertions that can be derived with the application of no more than k reasoning steps, or no\nmorethansseconds ofcomputation. Theseattemptshavebeengenerally unsatisfactory.\n12.5 REASONING SYSTEMS FOR CATEGORIES\nCategoriesaretheprimarybuilding blocksoflarge-scale knowledgerepresentation schemes.\nThis section describes systems specially designed for organizing and reasoning with cate-\ngories. Therearetwocloselyrelatedfamiliesofsystems: semanticnetworksprovidegraph-\nical aids for visualizing a knowledge base and efficient algorithms for inferring properties 454 Chapter 12. KnowledgeRepresentation\nof an object on the basis of its category membership; and description logics provide a for-\nmal language for constructing and combining category definitions and efficient algorithms\nfordeciding subsetandsupersetrelationships betweencategories.\n12.5.1 Semantic networks\nIn1909,CharlesS.Peirceproposedagraphicalnotationofnodesandedgescalledexistential\nEXISTENTIAL graphs that he called \u201cthe logic of the future.\u201d Thus began a long-running debate between\nGRAPHS\nadvocates of \u201clogic\u201d and advocates of \u201csemantic networks.\u201d Unfortunately, the debate ob-\nscured the fact that semantics networks\u2014at least those with well-defined semantics\u2014are a\nform of logic. The notation that semantic networks provide for certain kinds of sentences\nis often more convenient, but if we strip away the \u201chuman interface\u201d issues, the underlying\nconcepts\u2014objects, relations, quantification, andsoon\u2014arethesame.\nThere are many variants of semantic networks, but all are capable of representing in-\ndividual objects, categories of objects, and relations among objects. A typical graphical no-\ntation displays object or category names in ovals or boxes, and connects them with labeled\nlinks. Forexample, Figure 12.5 hasa MemberOf link between Mary and FemalePersons,\ncorresponding tothelogical assertion Mary\u2208FemalePersons;similarly, the SisterOf link\nbetween Mary andJohn corresponds totheassertion SisterOf(Mary,John). Wecancon-\nnect categories using SubsetOf links, and so on. It is such fun drawing bubbles and arrows\nthat one can get carried away. For example, we know that persons have female persons as\nmothers, socanwedrawa HasMother link from Persons toFemalePersons? Theanswer\nisno,becauseHasMother isarelationbetweenapersonandhisorhermother,andcategories\ndonothavemothers.5\nForthisreason,wehaveusedaspecialnotation\u2014thedouble-boxedlink\u2014inFigure12.5.\nThislinkassertsthat\n\u2200x x\u2208Persons \u21d2 [\u2200y HasMother(x,y) \u21d2 y\u2208FemalePersons].\nWemightalsowanttoassertthatpersonshavetwolegs\u2014thatis,\n\u2200x x\u2208Persons \u21d2 Legs(x,2).\nAsbefore, weneed tobe careful not to assert that acategory has legs; the single-boxed link\ninFigure12.5isusedtoassertproperties ofeverymemberof acategory.\nThe semantic network notation makes it convenient to perform inheritance reasoning\nofthekindintroducedinSection12.2. Forexample,byvirtueofbeingaperson,Maryinherits\nthe property of having two legs. Thus, to find out how many legs Mary has, the inheritance\nalgorithm follows the MemberOf link from Mary to the category she belongs to, and then\nfollows SubsetOf links up the hierarchy until it finds a category for which there is a boxed\nLegs link\u2014inthiscase,thePersons category. Thesimplicityandefficiencyofthisinference\n5 Severalearlysystemsfailedtodistinguishbetweenpropertiesofmembersofacategoryandpropertiesofthe\ncategoryasawhole. Thiscanleaddirectlytoinconsistencies,aspointedoutbyDrewMcDermott(1976)inhis\narticle\u201cArtificialIntelligenceMeetsNaturalStupidity.\u201d AnothercommonproblemwastheuseofIsAlinksfor\nbothsubsetandmembershiprelations,incorrespondencewithEnglishusage: \u201cacatisamammal\u201dand\u201cFifiisa\ncat.\u201dSeeExercise12.22formoreontheseissues. Section12.5. Reasoning SystemsforCategories 455\nMammals\nSubsetOf\nHasMother Legs\nPersons 2\nSubsetOf SubsetOf\nFemale Male\nPersons Persons\nMemberOf MemberOf\nSisterOf Legs\nMary John 1\nFigure12.5 Asemanticnetworkwithfourobjects(John,Mary,1, and2)andfourcate-\ngories.Relationsaredenotedbylabeledlinks.\nFlyEvents\nMemberOf\nFly\n17\nAgent During\nOrigin Destination\nShankar NewYork NewDelhi Yesterday\nFigure12.6 Afragmentofasemanticnetworkshowingtherepresentation ofthelogical\nassertionFly(Shankar,NewYork,NewDelhi,Yesterday).\nmechanism, compared withlogical theorem proving, hasbeen oneofthemainattractions of\nsemanticnetworks.\nInheritancebecomescomplicatedwhenanobjectcanbelongtomorethanonecategory\norwhenacategorycanbeasubsetofmorethanoneothercategory;thisiscalledmultiplein-\nMULTIPLE heritance. Insuchcases,theinheritance algorithmmightfindtwoormoreconflictingvalues\nINHERITANCE\nansweringthequery. Forthisreason,multipleinheritance isbannedinsomeobject-oriented\nprogramming (OOP)languages, such asJava, that useinheritance inaclass hierarchy. Itis\nusuallyallowedinsemanticnetworks, butwedeferdiscussion ofthatuntilSection12.6.\nThereadermighthavenoticedanobviousdrawbackofsemanticnetworknotation,com-\nparedtofirst-orderlogic: thefactthatlinks betweenbubbles represent only binary relations.\nFor example, the sentence Fly(Shankar,NewYork,NewDelhi,Yesterday) cannot be as-\nserted directly in a semantic network. Nonetheless, we can obtain the effect of n-ary asser-\ntionsbyreifyingthepropositionitselfasaneventbelongingtoanappropriateeventcategory.\nFigure 12.6 shows the semantic network structure for this particular event. Notice that the\nrestriction tobinaryrelations forcesthecreation ofarichontology ofreifiedconcepts.\nReification of propositions makes it possible to represent every ground, function-free\natomicsentenceoffirst-orderlogicinthesemanticnetworknotation. Certainkindsofuniver- 456 Chapter 12. KnowledgeRepresentation\nsallyquantifiedsentencescanbeassertedusinginverselinksandthesinglyboxedanddoubly\nboxed arrowsapplied tocategories, butthatstillleaves us alongwayshort offullfirst-order\nlogic. Negation, disjunction, nested function symbols, and existential quantification are all\nmissing. Nowitispossibletoextendthenotationtomakeitequivalenttofirst-orderlogic\u2014as\nin Peirce\u2019s existential graphs\u2014but doing so negates one ofthe main advantages of semantic\nnetworks,whichisthesimplicityandtransparency oftheinference processes. Designerscan\nbuild alarge networkandstillhaveagood ideaabout whatqueries willbeefficient, because\n(a)itiseasytovisualizethestepsthattheinferenceprocedurewillgothroughand(b)insome\ncases the query language is so simple that difficult queries cannot be posed. In cases where\nthe expressive powerproves to be too limiting, many semantic network systems provide for\nprocedural attachment to fill in the gaps. Procedural attachment is a technique whereby\na query about (or sometimes an assertion of) a certain relation results in a call to a special\nprocedure designed forthatrelationratherthanageneral inference algorithm.\nOne of the most important aspects of semantic networks is their ability to represent\ndefaultvaluesforcategories. ExaminingFigure12.5carefully,onenoticesthatJohnhasone\nDEFAULTVALUE\nleg,despitethefactthatheisapersonandallpersonshavetwolegs. InastrictlylogicalKB,\nthis would be a contradiction, but in a semantic network, the assertion that all persons have\ntwo legs has only default status; that is, a person is assumed to have two legs unless this is\ncontradictedbymorespecificinformation. Thedefaultsemanticsisenforcednaturallybythe\ninheritance algorithm, because it follows links upwards from the object itself (John in this\ncase)andstopsassoonasitfindsavalue. Wesaythatthedefaultisoverriddenbythemore\nOVERRIDING\nspecific value. Notice that we could also override the default number of legs by creating a\ncategoryofOneLeggedPersons,asubsetofPersons ofwhichJohn isamember.\nWecanretainastrictly logical semantics forthenetworkif wesaythattheLegs asser-\ntionforPersons includes anexception forJohn:\n\u2200x x\u2208Persons \u2227x (cid:7)= John \u21d2 Legs(x,2).\nFor a fixed network, this is semantically adequate but will be much less concise than the\nnetworknotationitselfiftherearelotsofexceptions. Foranetworkthatwillbeupdatedwith\nmoreassertions, however, such anapproach fails\u2014wereally wanttosay thatanypersons as\nyetunknownwithonelegareexceptionstoo. Section12.6goesintomoredepthonthisissue\nandondefaultreasoning ingeneral.\n12.5.2 Descriptionlogics\nThe syntax of first-order logic is designed to make it easy to say things about objects. De-\nscription logics are notations that are designed to make it easier to describe definitions and\nDESCRIPTIONLOGIC\nproperties of categories. Description logic systems evolved from semantic networks in re-\nsponse to pressure to formalize what the networks mean while retaining the emphasis on\ntaxonomic structureasanorganizing principle.\nThe principal inference tasks for description logics are subsumption (checking if one\nSUBSUMPTION\ncategory is a subset of another by comparing their definitions) and classification (checking\nCLASSIFICATION\nwhetheranobject belongs toacategory).. Somesystems also include consistency ofacate-\ngorydefinition\u2014whether themembershipcriteria arelogically satisfiable. Section12.5. Reasoning SystemsforCategories 457\nConcept \u2192 Thing| ConceptName\n| And(Concept,...)\n| All(RoleName,Concept)\n| AtLeast(Integer,RoleName)\n| AtMost(Integer,RoleName)\n| Fills(RoleName,IndividualName,...)\n| SameAs(Path,Path)\n| OneOf(IndividualName,...)\nPath \u2192 [RoleName,...]\nFigure12.7 ThesyntaxofdescriptionsinasubsetoftheCLASSIClanguage.\nTheCLASSIC language(Borgida etal.,1989)isatypicaldescription logic. Thesyntax\nof CLASSIC descriptions is shown in Figure 12.7.6 For example, to say that bachelors are\nunmarriedadultmaleswewouldwrite\nBachelor = And(Unmarried,Adult,Male).\nTheequivalent infirst-orderlogicwouldbe\nBachelor(x) \u21d4 Unmarried(x)\u2227Adult(x)\u2227Male(x).\nNotice that the description logic has an an algebra of operations on predicates, which of\ncoursewecan\u2019tdoinfirst-orderlogic. Anydescription in CLASSIC canbetranslated intoan\nequivalent first-order sentence, but some descriptions are more straightforward in CLASSIC.\nFor example, to describe the set of men with at least three sons who are all unemployed\nand married to doctors, and at mosttwodaughters who areall professors in physics ormath\ndepartments, wewoulduse\nAnd(Man,AtLeast(3,Son),AtMost(2,Daughter),\nAll(Son,And(Unemployed,Married,All(Spouse,Doctor))),\nAll(Daughter,And(Professor,Fills(Department,Physics,Math)))).\nWeleaveitasanexercisetotranslatethisintofirst-orderlogic.\nPerhapsthemostimportantaspectofdescriptionlogicsistheiremphasisontractability\nofinference. Aproblem instance issolved bydescribing itandthen asking ifitissubsumed\nbyoneofseveralpossiblesolutioncategories. Instandardfirst-orderlogicsystems,predicting\nthesolutiontimeisoftenimpossible. Itisfrequently lefttotheusertoengineertherepresen-\ntation to detour around sets of sentences that seem to be causing the system to take several\nweekstosolveaproblem. Thethrustindescriptionlogics,ontheotherhand,istoensurethat\nsubsumption-testing canbesolvedintimepolynomial inthesizeofthedescriptions.7\n6 Notice that the language does not allow one to simply state that one concept, or category, is a subset of\nanother. Thisisadeliberatepolicy:subsumptionbetweencategoriesmustbederivablefromsomeaspectsofthe\ndescriptionsofthecategories.Ifnot,thensomethingismissingfromthedescriptions.\n7 CLASSICprovidesefficientsubsumptiontestinginpractice,buttheworst-caseruntimeisexponential. 458 Chapter 12. KnowledgeRepresentation\nThis sounds wonderful in principle, until one realizes that it can only have one of two\nconsequences: either hard problems cannot be stated at all, or they require exponentially\nlarge descriptions! However,thetractability results doshed lightonwhatsorts ofconstructs\ncause problems and thus help the user to understand how different representations behave.\nFor example, description logics usually lack negation and disjunction. Each forces first-\norderlogicalsystemstogothrough apotentially exponential caseanalysis inordertoensure\ncompleteness. CLASSIC allows only a limited form of disjunction in the Fills and OneOf\nconstructs, which permit disjunction overexplicitly enumerated individuals but not overde-\nscriptions. Withdisjunctive descriptions, nested definitions canleadeasily toanexponential\nnumberofalternative routesbywhichonecategory cansubsumeanother.\n12.6 REASONING WITH DEFAULT INFORMATION\nInthepreceding section,wesawasimpleexampleofanassertionwithdefaultstatus: people\nhave two legs. This default can be overridden by more specific information, such as that\nLongJohn Silverhasone leg. Wesaw thatthe inheritance mechanism insemantic networks\nimplements the overriding of defaults in a simple and natural way. In this section, we study\ndefaults more generally, with a view toward understanding the semantics of defaults rather\nthanjustproviding aprocedural mechanism.\n12.6.1 Circumscription anddefault logic\nWehaveseentwoexamplesofreasoningprocessesthatviolatethemonotonicitypropertyof\nlogic that was proved in Chapter 7.8 In this chapter we saw that a property inherited by all\nmembersofacategory inasemanticnetworkcouldbeoverridden bymorespecificinforma-\ntionforasubcategory. InSection 9.4.5,wesawthat undertheclosed-world assumption, ifa\nproposition \u03b1isnotmentioned inKB thenKB |= \u00ac\u03b1,butKB \u2227\u03b1 |= \u03b1.\nSimple introspection suggests that these failures of monotonicity are widespread in\ncommonsense reasoning. It seems that humans often \u201cjump to conclusions.\u201d For example,\nwhen one sees a car parked on the street, one is normally willing to believe that it has four\nwheels even though only three are visible. Now, probability theory can certainly provide a\nconclusion thatthefourth wheelexists withhighprobability, yet,formostpeople, thepossi-\nbility of the car\u2019s not having four wheels does not arise unless some new evidence presents\nitself. Thus, it seems that the four-wheel conclusion is reached by default, in the absence of\nanyreason todoubt it. Ifnewevidence arrives\u2014forexample, ifonesees theownercarrying\nawheelandnoticesthatthecarisjackedup\u2014thentheconclusioncanberetracted. Thiskind\nof reasoning is said to exhibit nonmonotonicity, because the set of beliefs does not grow\nNONMONOTONICITY\nNONMONOTONIC monotonically over time as new evidence arrives. Nonmonotonic logics have been devised\nLOGIC\nwithmodifiednotionsoftruthandentailmentinordertocapturesuchbehavior. Wewilllook\nattwosuchlogicsthathavebeenstudied extensively: circumscription anddefaultlogic.\n8 Recallthatmonotonicityrequiresallentailedsentencestoremainentailedafternewsentencesareaddedtothe\nKB.Thatis,ifKB |=\u03b1thenKB \u2227\u03b2 |=\u03b1. Section12.6. Reasoning withDefaultInformation 459\nCircumscription can be seen as a more powerful and precise version of the closed-\nCIRCUMSCRIPTION\nworldassumption. Theideaistospecifyparticularpredicatesthatareassumedtobe\u201casfalse\naspossible\u201d\u2014that is,falseforeveryobjectexceptthoseforwhichtheyareknowntobetrue.\nForexample, suppose wewanttoassert thedefault rule thatbirds fly. Wewouldintroduce a\npredicate, sayAbnormal (x),andwrite\n1\nBird(x)\u2227\u00acAbnormal (x) \u21d2 Flies(x).\n1\nIf we say that Abnormal is to be circumscribed, a circumscriptive reasoner is entitled to\n1\nassume \u00acAbnormal (x) unless Abnormal (x) is known to be true. This allows the con-\n1 1\nclusion Flies(Tweety) to be drawn from the premise Bird(Tweety), but the conclusion no\nlongerholdsifAbnormal (Tweety)isasserted.\n1\nMODEL Circumscription can be viewed as an example of a model preference logic. In such\nPREFERENCE\nlogics,asentenceisentailed(withdefaultstatus)ifitistrueinallpreferredmodelsoftheKB,\nas opposed to the requirement of truth in all models in classical logic. For circumscription,\none modelispreferred toanother ifithas fewerabnormal objects.9 Letusseehow thisidea\nworksinthecontextofmultipleinheritance insemanticnetworks. Thestandardexamplefor\nwhich multiple inheritance is problematic is called the \u201cNixon diamond.\u201d It arises from the\nobservation that Richard Nixon was both a Quaker (and hence by default a pacifist) and a\nRepublican (andhencebydefault notapacifist). Wecanwritethisasfollows:\nRepublican(Nixon)\u2227Quaker(Nixon).\nRepublican(x)\u2227\u00acAbnormal (x) \u21d2 \u00acPacifist(x).\n2\nQuaker(x)\u2227\u00acAbnormal (x) \u21d2 Pacifist(x).\n3\nIf we circumscribe Abnormal and Abnormal , there are two preferred models: one in\n2 3\nwhichAbnormal (Nixon)andPacifist(Nixon)holdandoneinwhichAbnormal (Nixon)\n2 3\nand\u00acPacifist(Nixon)hold. Thus,thecircumscriptivereasonerremainsproperly agnosticas\nto whether Nixon was a pacifist. If we wish, in addition, to assert that religious beliefs take\nPRIORITIZED precedenceoverpoliticalbeliefs,wecanuseaformalismcalledprioritizedcircumscription\nCIRCUMSCRIPTION\ntogivepreference tomodelswhere Abnormal isminimized.\n3\nDefaultlogic is aformalism in which default rules can be written togenerate contin-\nDEFAULTLOGIC\ngent,nonmonotonic conclusions. Adefaultrulelookslikethis:\nDEFAULTRULES\nBird(x) :Flies(x)\/Flies(x).\nThisrulemeansthatifBird(x)istrue,andifFlies(x)isconsistentwiththeknowledgebase,\nthenFlies(x)maybeconcluded bydefault. Ingeneral, adefault rulehastheform\nP :J ,...,J \/C\n1 n\nwhere P is called the prerequisite, C is the conclusion, and J are the justifications\u2014if any\ni\none of them can be proven false, then the conclusion cannot be drawn. Any variable that\n9 Fortheclosed-worldassumption,onemodelispreferredtoanotherifithasfewertrueatoms\u2014thatis,preferred\nmodelsareminimalmodels. Thereisanaturalconnectionbetweentheclosed-worldassumptionanddefinite-\nclauseKBs,becausethefixedpointreachedbyforwardchainingondefinite-clauseKBsistheuniqueminimal\nmodel.Seepage258formoreonthispoint. 460 Chapter 12. KnowledgeRepresentation\nappears in J or C must also appear in P. The Nixon-diamond example can be represented\ni\nindefault logicwithonefactandtwodefaultrules:\nRepublican(Nixon)\u2227Quaker(Nixon).\nRepublican(x) :\u00acPacifist(x)\/\u00acPacifist(x).\nQuaker(x) :Pacifist(x)\/Pacifist(x).\nTo interpret what the default rules mean, we define the notion of an extension of a default\nEXTENSION\ntheory to be a maximal set of consequences of the theory. That is, an extension S consists\nof the original known facts and a set of conclusions from the default rules, such that no\nadditionalconclusions canbedrawnfromS andthejustificationsofeverydefaultconclusion\ninS areconsistentwithS. Asinthecaseofthepreferredmodelsincircumscription, wehave\ntwopossibleextensionsfortheNixondiamond: onewhereinheisapacifistandonewherein\nheisnot. Prioritizedschemesexistinwhichsomedefaultrulescanbegivenprecedenceover\nothers, allowingsomeambiguities toberesolved.\nSince 1980, when nonmonotonic logics were first proposed, a great deal of progress\nhas been made in understanding their mathematical properties. There are still unresolved\nquestions, however. For example, if \u201cCars have four wheels\u201d is false, what does it mean\nto have it in one\u2019s knowledge base? What is a good set of default rules to have? If we\ncannot decide, for each rule separately, whether it belongs in our knowledge base, then we\nhaveaseriousproblem ofnonmodularity. Finally, howcanbeliefsthathavedefaultstatusbe\nused to make decisions? This is probably the hardest issue for default reasoning. Decisions\noften involve tradeoffs, and one therefore needs to compare the strengths of belief in the\noutcomes ofdifferent actions, and the costs of making awrong decision. In cases where the\nsame kinds of decisions are being made repeatedly, it is possible to interpret default rules\nas \u201cthreshold probability\u201d statements. For example, the default rule \u201cMy brakes are always\nOK\u201d really means \u201cThe probability that my brakes are OK, given no other information, is\nsufficiently high that the optimal decision is for me to drive without checking them.\u201d When\nthedecisioncontextchanges\u2014forexample,whenoneisdrivingaheavilyladentruckdowna\nsteep mountain road\u2014the default rulesuddenly becomes inappropriate, eventhough thereis\nnonewevidenceoffaultybrakes. Theseconsiderationshaveledsomeresearcherstoconsider\nhowtoembeddefaultreasoning withinprobability theoryor utilitytheory.\n12.6.2 Truth maintenance systems\nWehave seen that many of the inferences drawn by a knowledge representation system will\nhave only default status, rather than being absolutely certain. Inevitably, some of these in-\nferredfactswillturnouttobewrongandwillhavetoberetractedinthefaceofnewinforma-\ntion. This process is called belief revision.10 Suppose that a knowledge base KB contains\nBELIEFREVISION\na sentence P\u2014perhaps a default conclusion recorded by a forward-chaining algorithm, or\nperhaps just an incorrect assertion\u2014and we want to execute TELL(KB, \u00acP). Toavoid cre-\nating a contradiction, we must first execute RETRACT(KB, P). This sounds easy enough.\n10 Beliefrevisionisoftencontrastedwithbeliefupdate,whichoccurswhenaknowledgebaseisrevisedtoreflect\nachangeintheworldratherthannewinformationaboutafixed world. Beliefupdatecombinesbeliefrevision\nwithreasoningabouttimeandchange;itisalsorelatedtotheprocessoffilteringdescribedinChapter15. Section12.6. Reasoning withDefaultInformation 461\nProblems arise, however, if any additional sentences were inferred from P and asserted in\ntheKB.Forexample,theimplicationP \u21d2 QmighthavebeenusedtoaddQ. Theobvious\n\u201csolution\u201d\u2014retracting allsentencesinferredfrom P\u2014failsbecausesuchsentencesmayhave\nother justifications besides P. For example, if R and R \u21d2 Q are also in the KB, then Q\nTRUTH\ndoes not have to be removed after all. Truth maintenance systems, orTMSs, are designed\nMAINTENANCE\nSYSTEM\ntohandle exactlythesekindsofcomplications.\nOne simple approach to truth maintenance is to keep track of the order in which sen-\ntences are told to the knowledge base by numbering them from P to P . When the call\n1 n\nRETRACT(KB,P i)ismade,thesystem revertstothestatejustbefore P\ni\nwasadded,thereby\nremovingbothP andanyinferences thatwerederivedfrom P . Thesentences P through\ni i i+1\nP can then be added again. This is simple, and it guarantees that the knowledge base will\nn\nbe consistent, but retracting P requires retracting and reasserting n\u2212isentences as wellas\ni\nundoing and redoing all the inferences drawn from those sentences. For systems to which\nmanyfactsarebeingadded\u2014such aslargecommercialdatabases\u2014this isimpractical.\nAmoreefficientapproachisthejustification-basedtruthmaintenancesystem,orJTMS.\nJTMS\nInaJTMS,each sentence inthe knowledge base isannotated withajustification consisting\nJUSTIFICATION\nof the set of sentences from which it was inferred. For example, if the knowledge base\nalready contains P \u21d2 Q, then TELL(P) will cause Q to be added with the justification\n{P, P \u21d2 Q}. In general, a sentence can have any number of justifications. Justifica-\ntions make retraction efficient. Given the call RETRACT(P), the JTMS will delete exactly\nthose sentences for which P is a member of every justification. So, if a sentence Q had\nthe single justification {P, P \u21d2 Q}, it would be removed; if it had the additional justi-\nfication {P, P \u2228 R \u21d2 Q}, it would still be removed; but if it also had the justification\n{R, P \u2228R \u21d2 Q},thenitwouldbespared. Inthisway,thetimerequiredforretractionofP\ndepends onlyonthenumberofsentences derived from P ratherthanonthenumberofother\nsentences addedsinceP enteredtheknowledge base.\nTheJTMSassumesthatsentencesthatareconsideredoncewillprobablybeconsidered\nagain, so rather than deleting a sentence from the knowledge base entirely when it loses\nall justifications, we merely mark the sentence as being out of the knowledge base. If a\nsubsequent assertion restores one of the justifications, then we mark the sentence as being\nback in. In this way, the JTMS retains all the inference chains that it uses and need not\nrederivesentences whenajustification becomesvalidagain.\nIn addition to handling the retraction of incorrect information, TMSs can be used to\nspeed up the analysis of multiple hypothetical situations. Suppose, for example, that the\nRomanian Olympic Committee is choosing sites for the swimming, athletics, and eques-\ntrian events at the 2048 Games to be held in Romania. For example, let the first hypothe-\nsisbeSite(Swimming,Pitesti),Site(Athletics,Bucharest),andSite(Equestrian,Arad).\nA great deal of reasoning must then be done to work out the logistical consequences and\nhence the desirability of this selection. If we want to consider Site(Athletics,Sibiu) in-\nstead, the TMS avoids the need to start again from scratch. Instead, we simply retract\nSite(Athletics,Bucharest)andassert Site(Athletics,Sibiu)andtheTMStakescareofthe\nnecessary revisions. Inference chains generated from the choice of Bucharest can be reused\nwithSibiu,providedthattheconclusions arethesame. 462 Chapter 12. KnowledgeRepresentation\nAnassumption-based truthmaintenancesystem,orATMS,makesthistypeofcontext-\nATMS\nswitching between hypothetical worldsparticularly efficient. InaJTMS,themaintenance of\njustifications allows you to move quickly from one state to another by making a few retrac-\ntionsandassertions,butatanytimeonlyonestateisrepresented. AnATMSrepresentsallthe\nstates that have everbeen considered at the same time. Whereas aJTMS simply labels each\nsentence as being in or out, an ATMS keeps track, for each sentence, of which assumptions\nwouldcausethesentencetobetrue. Inotherwords,eachsentencehasalabelthatconsistsof\naset ofassumption sets. Thesentence holds justinthose cases inwhichalltheassumptions\ninoneoftheassumption setshold.\nTruth maintenance systems also provide a mechanism for generating explanations.\nEXPLANATION\nTechnically, an explanation of a sentence P is a set of sentences E such that E entails P.\nIf the sentences in E are already known to be true, then E simply provides a sufficient ba-\nsis for proving that P must be the case. But explanations can also include assumptions\u2014\nASSUMPTION\nsentences that are not known to be true, but would suffice to prove P if they were true. For\nexample, one might not have enough information to prove that one\u2019s car won\u2019t start, but a\nreasonableexplanationmightincludetheassumptionthatthebatteryisdead. This,combined\nwith knowledge of how cars operate, explains the observed nonbehavior. In most cases, we\nwillpreferanexplanation E thatisminimal,meaningthatthereisnopropersubsetofE that\nisalsoanexplanation. AnATMScangenerateexplanations forthe\u201ccarwon\u2019tstart\u201dproblem\nbymakingassumptions (such as\u201cgasincar\u201dor\u201cbattery dead\u201d) inanyorderwelike, evenif\nsome assumptions are contradictory. Then we look at the label for the sentence \u201ccar won\u2019t\nstart\u201dtoreadoffthesetsofassumptions thatwouldjustify thesentence.\nTheexact algorithms used toimplement truth maintenance systems arealittle compli-\ncated,andwedonotcoverthemhere. Thecomputationalcomplexityofthetruthmaintenance\nproblem is at least as great as that of propositional inference\u2014that is, NP-hard. Therefore,\nyou should not expect truth maintenance to be a panacea. When used carefully, however, a\nTMS can provide a substantial increase in the ability of a logical system to handle complex\nenvironments andhypotheses.\n12.7 THE INTERNET SHOPPING WORLD\nInthisfinalsection weputtogether allwehave learned toencode knowledge forashopping\nresearch agent that helps a buyer find product offers on the Internet. The shopping agent is\ngiven a product description by the buyer and has the task of producing a list of Web pages\nthat offer such a product for sale, and ranking which offers are best. In some cases the\nbuyer\u2019s product description will be precise, as in Canon Rebel XTi digital camera, and the\ntaskisthentofindthestore(s) withthebestoffer. Inothercases thedescription willbeonly\npartially specified, as in digital camera for under $300, and the agent will have to compare\ndifferentproducts.\nTheshoppingagent\u2019senvironmentistheentireWorldWideWebinitsfullcomplexity\u2014\nnot atoy simulated environment. Theagent\u2019s percepts are Webpages, but whereas ahuman Section12.7. TheInternet ShoppingWorld 463\nExample Online Store\nSelectfromourfinelineofproducts:\n\u2022Computers\n\u2022Cameras\n\u2022Books\n\u2022Videos\n\u2022Music\n<h1>Example Online Store<\/h1>\n<i>Select<\/i> from our fine line of products:\n<ul>\n<li> <a href=\"http:\/\/example.com\/compu\">Computers<\/a>\n<li> <a href=\"http:\/\/example.com\/camer\">Cameras<\/a>\n<li> <a href=\"http:\/\/example.com\/books\">Books<\/a>\n<li> <a href=\"http:\/\/example.com\/video\">Videos<\/a>\n<li> <a href=\"http:\/\/example.com\/music\">Music<\/a>\n<\/ul>\nFigure12.8 AWebpagefromagenericonlinestoreintheformperceivedby thehuman\nuserofabrowser(top),andthecorrespondingHTMLstringas perceivedbythebrowseror\ntheshoppingagent(bottom). InHTML,charactersbetween<and>aremarkupdirectives\nthat specify how the page is displayed. For example, the string <i>Select<\/i> means\ntoswitch to italic font, displaytheword Select, andthenendtheuse ofitalic font. A page\nidentifiersuchashttp:\/\/example.com\/booksiscalledauniformresourcelocator\n(URL).Themarkup<a href=\"url\">Books<\/a>meanstocreateahypertextlinkto url\nwiththeanchortextBooks.\nWeb user would see pages displayed as an array of pixels on a screen, the shopping agent\nwillperceive apage asacharacter string consisting ofordinary words interspersed withfor-\nmatting commands in the HTML markup language. Figure 12.8 shows a Web page and a\ncorresponding HTML character string. The perception problem for the shopping agent in-\nvolvesextracting usefulinformation frompercepts ofthis kind.\nClearly, perception on Webpages iseasier than, say, perception while driving ataxi in\nCairo. Nonetheless, therearecomplications totheInternetperception task. TheWebpagein\nFigure12.8issimplecomparedtorealshoppingsites,whichmayincludeCSS,cookies,Java,\nJavascript, Flash,robotexclusionprotocols, malformedHTML,soundfiles,movies,andtext\nthat appears only as part of a JPEG image. An agent that can deal with all of the Internet is\nalmost as complex as a robot that can move in the real world. We concentrate on a simple\nagentthatignoresmostofthesecomplications.\nTheagent\u2019sfirsttaskistocollectproductoffersthatarerelevanttoaquery. Ifthequery\nis \u201claptops,\u201d then a Web page with a review of the latest high-end laptop would be relevant,\nbutifitdoesn\u2019t provideawaytobuy, itisn\u2019t anoffer. Fornow,wecansayapageisanoffer\nifitcontainsthewords\u201cbuy\u201dor\u201cprice\u201dor\u201caddtocart\u201dwithinanHTMLlinkorformonthe 464 Chapter 12. KnowledgeRepresentation\npage. Forexample,ifthepagecontainsastringoftheform\u201c<a...add to cart...<\/a\u201d\nthenitisanoffer. Thiscouldberepresentedinfirst-orderlogic,butitismorestraightforward\ntoencodeitintoprogramcode. Weshowhowtodomoresophisticatedinformationextraction\ninSection22.4.\n12.7.1 Followinglinks\nThestrategy istostartatthehomepage ofanonline store and considerallpagesthatcanbe\nreached byfollowing relevantlinks.11 Theagent willhaveknowledge ofanumberofstores,\nforexample:\nAmazon\u2208OnlineStores \u2227Homepage(Amazon,\u201camazon.com\u201d).\nEbay\u2208OnlineStores \u2227Homepage(Ebay,\u201cebay.com\u201d).\nExampleStore \u2208OnlineStores \u2227Homepage(ExampleStore,\u201cexample.com\u201d).\nThese stores classify their goods into product categories, and provide links tothe major cat-\negories from their home page. Minor categories can be reached through a chain of relevant\nlinks, andeventually wewillreachoffers. Inotherwords,a pageisrelevanttothequeryifit\ncan be reached by achain of zero ormorerelevant category links from astore\u2019s home page,\nandthenfromonemorelinktotheproductoffer. Wecandefinerelevance:\nRelevant(page,query) \u21d4\n\u2203store,home store\u2208OnlineStores \u2227Homepage(store,home)\n\u2227\u2203url,url RelevantChain(home,url ,query)\u2227Link(url ,url)\n2 2 2\n\u2227page = Contents(url).\nHere the predicate Link(from,to) means that there is a hyperlink from the from URL to\nthe to URL. To define what counts as a RelevantChain, we need to follow not just any old\nhyperlinks,butonlythoselinkswhoseassociatedanchortextindicatesthatthelinkisrelevant\nto the product query. Forthis, we use LinkText(from,to,text) to mean that there is a link\nbetween from and to with text as the anchor text. A chain of links between two URLs, start\nand end, is relevant to a description d if the anchor text of each link is a relevant category\nnameford. Theexistenceofthechainitselfisdetermined byarecursive definition, withthe\nemptychain(start=end)asthebasecase:\nRelevantChain(start,end,query) \u21d4 (start = end)\n\u2228(\u2203u,text LinkText(start,u,text)\u2227RelevantCategoryName(query,text)\n\u2227RelevantChain(u,end,query)).\nNow we must define what it means for text to be a RelevantCategoryName for query.\nFirst, weneed to relate strings to the categories they name. This is done using the predicate\nName(s,c),whichsaysthatstring sisanameforcategory c\u2014forexample,wemightassert\nthat Name(\u201claptops\u201d,LaptopComputers). Some more examples of the Name predicate\nappear in Figure 12.9(b). Next, we define relevance. Suppose that query is \u201claptops.\u201d Then\nRelevantCategoryName(query,text)istruewhenoneofthefollowingholds:\n\u2022 Thetextandquerynamethesamecategory\u2014e.g., \u201cnotebooks\u201d and\u201claptops.\u201d\n11 Analternativetothelink-followingstrategyistouseanInternetsearchengine;thetechnologybehindInternet\nsearch,informationretrieval,willbecoveredinSection22.3. Section12.7. TheInternet ShoppingWorld 465\nName(\u201cbooks\u201d,Books)\nBooks \u2282Products\nName(\u201cmusic\u201d,MusicRecordings)\nMusicRecordings \u2282Products\nName(\u201cCDs\u201d,MusicCDs)\nMusicCDs \u2282MusicRecordings\nName(\u201celectronics\u201d,Electronics)\nElectronics \u2282Products\nName(\u201cdigitalcameras\u201d,DigitalCameras)\nDigitalCameras \u2282Electronics\nName(\u201cstereos\u201d,StereoEquipment)\nStereoEquipment \u2282Electronics\nName(\u201ccomputers\u201d,Computers)\nComputers \u2282Electronics\nName(\u201cdesktops\u201d,DesktopComputers)\nDesktopComputers \u2282Computers\nName(\u201claptops\u201d,LaptopComputers)\nLaptopComputers \u2282Computers\nName(\u201cnotebooks\u201d,LaptopComputers)\n...\n...\n(a) (b)\nFigure12.9 (a)Taxonomyofproductcategories.(b)Namesforthosecategories.\n\u2022 Thetextnamesasupercategory suchas\u201ccomputers.\u201d\n\u2022 Thetextnamesasubcategory suchas\u201cultralight notebooks.\u201d\nThelogicaldefinition ofRelevantCategoryName isasfollows:\nRelevantCategoryName(query,text) \u21d4\n\u2203c ,c Name(query,c )\u2227Name(text,c )\u2227(c \u2286 c \u2228c \u2286 c ). (12.1)\n1 2 1 2 1 2 2 1\nOtherwise, theanchortextisirrelevant because itnamesacategory outside thisline, suchas\n\u201cclothes\u201d or\u201clawn&garden.\u201d\nTo follow relevant links, then, it is essential to have a rich hierarchy of product cate-\ngories. ThetoppartofthishierarchymightlooklikeFigure 12.9(a). Itwillnotbefeasibleto\nlist all possible shopping categories, because a buyer could always come up with some new\ndesire and manufacturers will always come out with new products to satisfy them (electric\nkneecap warmers?). Nonetheless, anontology ofabout athousand categories willserve asa\nveryusefultoolformostbuyers.\nIn addition to the product hierarchy itself, we also need to have a rich vocabulary of\nnames for categories. Life would be much easier if there were a one-to-one correspon-\ndence between categories and the character strings that name them. We have already seen\nthe problem of synonymy\u2014two names for the same category, such as \u201claptop computers\u201d\nand\u201claptops.\u201d Thereisalso theproblem of ambiguity\u2014one namefortwoormoredifferent\ncategories. Forexample,ifweaddthesentence\nName(\u201cCDs\u201d,CertificatesOfDeposit)\ntotheknowledge baseinFigure12.9(b), then\u201cCDs\u201dwillnametwodifferent categories.\nSynonymy and ambiguity can cause a significant increase in the number of paths that\nthe agent has to follow, and can sometimes make it difficult to determine whether a given\npageisindeedrelevant. Amuchmoreseriousproblemistheverybroadrangeofdescriptions\nthatausercantypeandcategorynamesthatastorecanuse. Forexample, thelinkmightsay\n\u201claptop\u201d whenthe knowledge basehas only \u201claptops\u201d orthe usermight askfor\u201cacomputer 466 Chapter 12. KnowledgeRepresentation\nI can fit on the tray table of an economy-class airline seat.\u201d It is impossible to enumerate in\nadvance all the ways a category can be named, so the agent will have to be able to do addi-\ntionalreasoninginsomecasestodetermineiftheNamerelationholds. Intheworstcase,this\nrequiresfullnaturallanguageunderstanding, atopicthatwewilldefertoChapter22. Inprac-\ntice,afewsimplerules\u2014suchasallowing\u201claptop\u201dtomatchacategorynamed\u201claptops\u201d\u2014go\nalongway. Exercise12.10asksyoutodevelopasetofsuchrules afterdoing someresearch\nintoonlinestores.\nGiven the logical definitions from the preceding paragraphs and suitable knowledge\nbases of product categories and naming conventions, are we ready to apply an inference\nalgorithm to obtain a set of relevant offers for our query? Not quite! The missing element\nis the Contents(url) function, which refers to the HTML page at a given URL. The agent\ndoesn\u2019t havethepagecontents ofeveryURLinitsknowledge base;nordoesithaveexplicit\nrulesfordeducing whatthosecontents mightbe. Instead, wecanarrange fortherightHTTP\nprocedure to be executed whenever a subgoal involves the Contents function. In this way, it\nappears tothe inference engine as ifthe entire Webis inside the knowledge base. This isan\nPROCEDURAL exampleofageneraltechniquecalledproceduralattachment,wherebyparticularpredicates\nATTACHMENT\nandfunctions canbehandledbyspecial-purpose methods.\n12.7.2 Comparing offers\nLet us assume that the reasoning processes of the preceding section have produced a set of\nofferpagesforour\u201claptops\u201d query. Tocomparethoseoffers, theagentmustextracttherele-\nvantinformation\u2014price, speed,disksize,weight,andsoon\u2014fromtheofferpages. Thiscan\nbeadifficult taskwithrealWebpages, forallthereasons mentioned previously. Acommon\nwayof dealing withthis problem is touse programs called wrappersto extract information\nWRAPPER\nfrom a page. The technology of information extraction is discussed in Section 22.4. For\nnow weassume that wrappers exist, and when given apage and a knowledge base, they add\nassertions to the knowledge base. Typically, a hierarchy of wrappers would be applied to a\npage: a very general one to extract dates and prices, a morespecific one to extract attributes\nforcomputer-related products, andifnecessary asite-specific onethatknowstheformatofa\nparticularstore. Givenapageontheexample.com sitewiththetext\nIBM ThinkBook 970. Our price: $399.00\nfollowed byvarious technical specifications, wewouldlike awrapper toextract information\nsuchasthefollowing:\n\u2203c,offer c\u2208LaptopComputers \u2227offer\u2208ProductOffers \u2227\nManufacturer(c,IBM)\u2227Model(c,ThinkBook970)\u2227\nScreenSize(c,Inches(14))\u2227ScreenType(c,ColorLCD)\u2227\nMemorySize(c,Gigabytes(2))\u2227CPUSpeed(c,GHz(1.2))\u2227\nOfferedProduct(offer,c)\u2227Store(offer,GenStore)\u2227\nURL(offer,\u201cexample.com\/computers\/34356.html\u201d)\u2227\nPrice(offer,$(399))\u2227Date(offer,Today).\nThisexampleillustratesseveralissuesthatarisewhenwetakeseriouslythetaskofknowledge\nengineering forcommercialtransactions. Forexample, notice thatthepriceisanattribute of Section12.8. Summary 467\nthe offer, not the product itself. This is important because the offer at a given store may\nchange from day to day even for the same individual laptop; for some categories\u2014such as\nhouses and paintings\u2014the same individual object may even be offered simultaneously by\ndifferent intermediaries at different prices. There are still more complications that we have\nnot handled, such asthe possibility that the price depends onthe method ofpayment and on\nthe buyer\u2019s qualifications for certain discounts. The final task is to compare the offers that\nhavebeenextracted. Forexample,considerthesethreeoffers:\nA :1.4GHzCPU,2GBRAM,250GBdisk,$299.\nB : 1.2GHzCPU,4GBRAM,350GBdisk,$500.\nC : 1.2GHzCPU,2GBRAM,250GBdisk,$399.\nC is dominated by A; that is, A is cheaper and faster, and they are otherwise the same. In\ngeneral, X dominates Y ifX hasabettervalueonatleast oneattribute, andisnotworseon\nany attribute. But neither A nor B dominates the other. To decide which is better we need\nto know how the buyer weighs CPU speed and price against memory and disk space. The\ngeneral topic ofpreferences among multiple attributes isaddressed inSection16.4; fornow,\nour shopping agent will simply return a list of all undominated offers that meet the buyer\u2019s\ndescription. Inthisexample, bothAandB areundominated. Noticethatthisoutcomerelies\non the assumption that everyone prefers cheaper prices, fasterprocessors, and more storage.\nSomeattributes,suchasscreensizeonanotebook,dependontheuser\u2019sparticularpreference\n(portability versusvisibility); forthese,theshopping agentwilljusthavetoasktheuser.\nThe shopping agent we have described here is a simple one; many refinements are\npossible. Still, it has enough capability that with the right domain-specific knowledge it can\nactually be of use to a shopper. Because of its declarative construction, it extends easily to\nmore complex applications. The main point of this section is to show that some knowledge\nrepresentation\u2014in particular, theproducthierarchy\u2014isnecessaryforsuchanagent,andthat\noncewehavesomeknowledgeinthisform,therestfollowsnaturally.\n12.8 SUMMARY\nBy delving into the details of how one represents a variety of knowledge, we hope we have\ngiven the reader a sense of how real knowledge bases are constructed and a feeling for the\ninteresting philosophical issuesthatarise. Themajorpointsareasfollows:\n\u2022 Large-scale knowledge representation requires ageneral-purpose ontology to organize\nandtietogetherthevariousspecificdomainsofknowledge.\n\u2022 Ageneral-purpose ontology needs tocoverawidevariety ofknowledge andshould be\ncapable, inprinciple, ofhandling anydomain.\n\u2022 Building a large, general-purpose ontology is a significant challenge that has yet to be\nfullyrealized, although currentframeworksseemtobequiterobust.\n\u2022 We presented an upper ontology based on categories and the event calculus. We\ncovered categories, subcategories, parts, structured objects, measurements, substances,\nevents,timeandspace,change, andbeliefs. 468 Chapter 12. KnowledgeRepresentation\n\u2022 Naturalkindscannotbedefinedcompletelyinlogic,butpropertiesofnaturalkindscan\nberepresented.\n\u2022 Actions, events, and time can be represented either in situation calculus or in more\nexpressiverepresentations suchaseventcalculus. Suchrepresentations enableanagent\ntoconstruct plansbylogicalinference.\n\u2022 WepresentedadetailedanalysisoftheInternetshoppingdomain,exercisingthegeneral\nontology andshowinghowthedomainknowledge canbeusedbyashopping agent.\n\u2022 Special-purpose representation systems, such as semantic networks and description\nlogics, have been devised to help in organizing a hierarchy of categories. Inheritance\nisanimportantformofinference,allowingthepropertiesofobjectstobededucedfrom\ntheirmembershipincategories.\n\u2022 The closed-world assumption, as implemented in logic programs, provides a simple\nway to avoid having to specify lots of negative information. It is best interpreted as a\ndefaultthatcanbeoverriddenbyadditional information.\n\u2022 Nonmonotoniclogics,suchascircumscriptionanddefaultlogic,areintendedtocap-\nturedefaultreasoning ingeneral.\n\u2022 Truthmaintenancesystemshandleknowledge updatesandrevisions efficiently.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nBriggs(1985) claimsthatformalknowledge representation research beganwithclassical In-\ndian theorizing about thegrammarofShastric Sanskrit, which dates back to thefirstmillen-\nnium B.C. In the West, the use of definitions of terms in ancient Greek mathematics can be\nregardedastheearliestinstance: Aristotle\u2019s Metaphysics(literally,whatcomesafterthebook\nonphysics) isanear-synonym for Ontology. Indeed, thedevelopment oftechnical terminol-\nogyinanyfieldcanberegardedasaformofknowledge representation.\nEarly discussions of representation in AI tended to focus on \u201cproblem representation\u201d\nratherthan\u201cknowledgerepresentation.\u201d (See,forexample,Amarel\u2019s(1968)discussionofthe\nMissionaries andCannibals problem.) Inthe1970s, AIemphasized thedevelopment of\u201cex-\npert systems\u201d (also called \u201cknowledge-based systems\u201d) that could, if given the appropriate\ndomain knowledge, matchorexceed theperformance ofhumanexperts onnarrowlydefined\ntasks. For example, the first expert system, DENDRAL (Feigenbaum et al., 1971; Lindsay\netal.,1980),interpreted theoutputofamassspectrometer(atypeofinstrumentusedtoana-\nlyzethestructureoforganicchemicalcompounds)asaccuratelyasexpertchemists. Although\nthe success of DENDRAL was instrumental in convincing the AI research community of the\nimportance ofknowledge representation, therepresentational formalisms used in DENDRAL\nare highly specific to the domain of chemistry. Over time, researchers became interested in\nstandardized knowledge representation formalisms and ontologies that could streamline the\nprocess of creating new expert systems. In so doing, they ventured into territory previously\nexplored by philosophers of science and of language. The discipline imposed in AI by the\nneedforone\u2019stheoriesto\u201cwork\u201dhasledtomorerapidanddeeperprogressthanwasthecase Bibliographical andHistorical Notes 469\nwhenthese problems weretheexclusive domain ofphilosophy (although ithasattimesalso\nledtotherepeated reinvention ofthewheel).\nThecreationofcomprehensivetaxonomiesorclassificationsdatesbacktoancienttimes.\nAristotle(384\u2013322 B.C.) stronglyemphasizedclassificationandcategorization schemes. His\nOrganon,acollection ofworksonlogicassembledbyhisstudentsafterhisdeath,included a\ntreatisecalledCategoriesinwhichheattemptedtoconstructwhatwewouldnowcallanupper\nontology. Healso introduced thenotions of genusand species forlower-level classification.\nOurpresentsystemofbiologicalclassification,includingtheuseof\u201cbinomialnomenclature\u201d\n(classification via genus and species in the technical sense), was invented by the Swedish\nbiologist Carolus Linnaeus, or Carl von Linne (1707\u20131778). The problems associated with\nnatural kinds and inexact category boundaries have been addressed by Wittgenstein (1953),\nQuine(1953), Lakoff(1987),andSchwartz(1977),amongothers.\nInterest in larger-scale ontologies is increasing, as documented by the Handbook on\nOntologies (Staab, 2004). The OPENCYC project (Lenat and Guha, 1990; Matuszek et al.,\n2006) hasreleased a150,000-concept ontology, withanupperontology similartotheonein\nFigure 12.1aswellasspecificconcepts like\u201cOLEDDisplay\u201d and\u201ciPhone,\u201d whichisatype\nof \u201ccellular phone,\u201d which in turn is a type of \u201cconsumer electronics,\u201d \u201cphone,\u201d \u201cwireless\ncommunication device,\u201d and other concepts. The DBPEDIA project extracts structured data\nfrom Wikipedia; specifically from Infoboxes: the boxes of attribute\/value pairs that accom-\npany many Wikipedia articles (Wu and Weld, 2008; Bizer et al., 2007). As of mid-2009,\nDBPEDIA contains 2.6million concepts, withabout 100 facts perconcept. TheIEEEwork-\ninggroupP1600.1createdtheSuggestedUpperMergedOntology(SUMO)(NilesandPease,\n2001; Pease and Niles, 2002), which contains about 1000 terms in the upper ontology and\nlinks to over 20,000 domain-specific terms. Stoffel et al. (1997) describe algorithms for ef-\nficiently managing a very large ontology. A survey of techniques for extracting knowledge\nfromWebpagesisgivenbyEtzioni etal.(2008).\nOn the Web, representation languages are emerging. RDF (Brickley and Guha, 2004)\nallows for assertions to be made in the form of relational triples, and provides some means\nforevolvingthemeaningofnamesovertime. OWL(Smithetal.,2004)isadescriptionlogic\nthatsupportsinferencesoverthesetriples. Sofar,usageseemstobeinverselyproportionalto\nrepresentational complexity: thetraditionalHTMLandCSSformatsaccountforover99%of\nWebcontent,followedbythesimplestrepresentation schemes,suchasmicroformats(Khare,\n2006) and RDFa (Adida and Birbeck, 2008), which use HTML and XHTML markup to\nadd attributes to literal text. Usage of sophisticated RDF and OWL ontologies is not yet\nwidespread, and the full vision of the Semantic Web (Berners-Lee et al., 2001) has not yet\nbeen realized. The conferences on Formal Ontology in Information Systems (FOIS)contain\nmanyinteresting papersonbothgeneralanddomain-specific ontologies.\nThe taxonomy used in this chapter was developed by the authors and is based in part\non their experience in the CYC project and in part on work by Hwang and Schubert (1993)\nand Davis (1990, 2005). An inspirational discussion of the general project of commonsense\nknowledgerepresentation appearsinHayes\u2019s(1978, 1985b) \u201cNaivePhysicsManifesto.\u201d\nSuccessful deep ontologies within a specific field include the Gene Ontology project\n(Consortium, 2008)andCML,theChemicalMarkupLanguage(Murray-Rust etal.,2003). 470 Chapter 12. KnowledgeRepresentation\nDoubts about the feasibility of a single ontology for all knowledge are expressed by\nDoctorow (2001), Gruber (2004), Halevy et al. (2009), and Smith (2004), who states, \u201cthe\ninitialprojectofbuilding onesingleontology ...has...largelybeenabandoned.\u201d\nTheeventcalculuswasintroducedbyKowalskiandSergot(1986)tohandlecontinuous\ntime,andtherehavebeenseveralvariations(SadriandKowalski,1995;Shanahan,1997)and\noverviews (Shanahan, 1999; Mueller, 2006). van Lambalgen and Hamm (2005) show how\nthelogic ofeventsmapsonto thelanguage weusetotalkabout events. Analternative tothe\nevent and situation calculi is the fluent calculus (Thielscher, 1999). James Allen introduced\ntimeintervalsforthesamereason(Allen,1984),arguingthatintervalsweremuchmorenatu-\nralthansituations forreasoning aboutextended andconcurrent events. PeterLadkin(1986a,\n1986b) introduced \u201cconcave\u201d time intervals (intervals with gaps; essentially, unions of ordi-\nnary\u201cconvex\u201d timeintervals) andapplied thetechniques of mathematical abstract algebra to\ntime representation. Allen (1991) systematically investigates the wide variety of techniques\navailablefortimerepresentation; vanBeekandManchak(1996)analyzealgorithmsfortem-\nporalreasoning. Therearesignificantcommonalitiesbetweentheevent-basedontologygiven\nin this chapter and an analysis of events due to the philosopher Donald Davidson (1980).\nThehistoriesinPatHayes\u2019s(1985a) ontology ofliquids andthechronicles inMcDermott\u2019s\n(1985)theoryofplanswerealsoimportantinfluencesonthefieldandthischapter.\nThequestion ofthe ontological status of substances has along history. Plato proposed\nthat substances were abstract entities entirely distinct from physical objects; he would say\nMadeOf(Butter ,Butter) rather than Butter \u2208Butter. This leads to a substance hierar-\n3 3\nchyinwhich,forexample,UnsaltedButter isamorespecificsubstancethanButter. Thepo-\nsitionadoptedinthischapter, inwhichsubstances arecategoriesofobjects,waschampioned\nbyRichardMontague(1973). IthasalsobeenadoptedintheCYC project. Copeland(1993)\nmountsaserious, butnotinvincible, attack. Thealternative approach mentioned inthechap-\nter,inwhichbutterisoneobjectconsistingofallbutteryobjectsintheuniverse,wasproposed\noriginallybythePolishlogicianLes\u00b4niewski(1916). Hismereology(thenameisderivedfrom\nMEREOLOGY\nthe Greek word for \u201cpart\u201d) used the part\u2013whole relation as a substitute for mathematical set\ntheory, withtheaimofeliminating abstract entities suchassets. Amorereadable exposition\nof these ideas is given by Leonard and Goodman (1940), and Goodman\u2019s The Structure of\nAppearance(1977)appliestheideastovariousproblemsinknowledgerepresentation. While\nsome aspects of the mereological approach are awkward\u2014for example, the need for asepa-\nrateinheritance mechanism based onpart\u2013whole relations\u2014the approach gainedthesupport\nof Quine (1960). Harry Bunt (1985) has provided an extensive analysis of its use in knowl-\nedgerepresentation. CasatiandVarzi(1999)coverparts,wholes,andthespatiallocations.\nMental objects have been the subject of intensive study in philosophy and AI. There\narethreemainapproaches. Theonetaken inthis chapter, based onmodallogic andpossible\nworlds, is the classical approach from philosophy (Hintikka, 1962; Kripke, 1963; Hughes\nandCresswell, 1996). Thebook Reasoning about Knowledge(Faginetal.,1995)provides a\nthorough introduction. The second approach is a first-order theory in which mental objects\narefluents. Davis(2005) andDavisandMorgenstern (2005)describe thisapproach. Itrelies\non the possible-worlds formalism, and builds on work by Robert Moore (1980, 1985). The\nthird approach is a syntactic theory, in which mental objects are represented by character\nSYNTACTICTHEORY Bibliographical andHistorical Notes 471\nstrings. A string is just a complex term denoting a list of symbols, so CanFly(Clark) can\nbe represented by the list of symbols [C,a,n,F,l,y,(,C,l,a,r,k,)]. The syntactic theory\nof mental objects was first studied in depth by Kaplan and Montague (1960), who showed\nthat it led to paradoxes if not handled carefully. Ernie Davis (1990) provides an excellent\ncomparison ofthesyntactic andmodaltheories ofknowledge.\nThe Greek philosopher Porphyry (c. 234\u2013305 A.D.), commenting on Aristotle\u2019s Cat-\negories, drew what might qualify as the first semantic network. Charles S. Peirce (1909)\ndeveloped existential graphs as the first semantic network formalism using modern logic.\nRoss Quillian (1961), driven by an interest in human memory and language processing, ini-\ntiated workon semantic networks within AI.Aninfluential paperby Marvin Minsky (1975)\npresented a version of semantic networks called frames; a frame was a representation of\nan object or category, with attributes and relations to other objects or categories. The ques-\ntionofsemanticsarosequiteacutely withrespecttoQuillian\u2019ssemanticnetworks (andthose\nof others who followed his approach), with their ubiquitous and very vague \u201cIS-A links\u201d\nWoods\u2019s(1975)famousarticle\u201cWhat\u2019sInaLink?\u201d drewtheattentionofAIresearcherstothe\nneed forprecise semantics in knowledge representation formalisms. Brachman (1979) elab-\norated on this point and proposed solutions. Patrick Hayes\u2019s (1979) \u201cThe Logic of Frames\u201d\ncut even deeper, claiming that \u201cMost of \u2018frames\u2019 is just a new syntax for parts of first-order\nlogic.\u201d Drew McDermott\u2019s (1978b) \u201cTarskian Semantics, or, No Notation without Denota-\ntion!\u201d argued that themodel-theoretic approach tosemantics used infirst-order logicshould\nbe applied to all knowledge representation formalisms. This remains a controversial idea;\nnotably, McDermott himself has reversed his position in \u201cA Critique of Pure Reason\u201d (Mc-\nDermott, 1987). Selman and Levesque (1993) discuss the complexity of inheritance with\nexceptions, showingthatinmostformulations itisNP-complete.\nThe development of description logics is the most recent stage in a long line of re-\nsearch aimed at finding useful subsets of first-order logic for which inference is computa-\ntionally tractable. Hector Levesque and Ron Brachman (1987) showed that certain logical\nconstructs\u2014notably, certain uses of disjunction and negation\u2014were primarily responsible\nfor the intractability of logical inference. Building on the KL-ONE system (Schmolze and\nLipkis, 1983), several researchers developed systems that incorporate theoretical complex-\nity analysis, most notably KRYPTON (Brachman et al., 1983) and Classic (Borgida et al.,\n1989). The result has been a marked increase in the speed of inference and a much better\nunderstanding of the interaction between complexity and expressiveness in reasoning sys-\ntems. Calvaneseetal.(1999)summarizethestateoftheart,andBaader etal.(2007)present\na comprehensive handbook of description logic. Against this trend, Doyle and Patil (1991)\nhave argued that restricting the expressiveness of a language either makes it impossible to\nsolvecertainproblemsorencouragestheusertocircumvent thelanguagerestrictionsthrough\nnonlogical means.\nThethreemainformalismsfordealingwithnonmonotonic inference\u2014circumscription\n(McCarthy, 1980), default logic (Reiter, 1980), and modal nonmonotonic logic (McDermott\nandDoyle,1980)\u2014wereallintroduced inonespecialissueoftheAIJournal. Delgrandeand\nSchaub (2003) discuss the merits of the variants, given 25 years of hindsight. Answer set\nprogrammingcanbeseenasanextensionofnegationasfailureorasarefinementofcircum- 472 Chapter 12. KnowledgeRepresentation\nscription; the underlying theory of stable model semantics was introduced by Gelfond and\nLifschitz(1988),andtheleadinganswersetprogrammingsystemsareDLV(Eiteretal.,1998)\nandSMODELS(Niemela\u00a8etal.,2000). ThediskdriveexamplecomesfromtheSMODELSuser\nmanual (Syrja\u00a8nen, 2000). Lifschitz (2001) discusses the use of answer set programming for\nplanning. Brewkaetal.(1997)giveagoodoverviewofthevariousapproaches tononmono-\ntonic logic. Clark (1978) covers the negation-as-failure approach to logic programming and\nClarkcompletion. VanEmdenandKowalski(1976)showthateveryPrologprogramwithout\nnegation has a unique minimal model. Recent years have seen renewed interest in applica-\ntionsofnonmonotonic logicstolarge-scale knowledgerepresentation systems. TheBENINQ\nsystemsforhandling insurance-benefit inquiries wasperhaps thefirstcommercially success-\nfulapplication ofanonmonotonic inheritance system (Morgenstern, 1998). Lifschitz (2001)\ndiscussestheapplicationofanswersetprogrammingtoplanning. Avarietyofnonmonotonic\nreasoning systems based on logic programming are documented in the proceedings of the\nconferences onLogicProgrammingandNonmonotonic Reasoning(LPNMR).\nThe study of truth maintenance systems began with the TMS (Doyle, 1979) and RUP\n(McAllester, 1980) systems, both of which were essentially JTMSs. Forbus and de Kleer\n(1993) explain in depth how TMSs can be used in AI applications. Nayak and Williams\n(1997) show howanefficientincremental TMScalled anITMSmakes itfeasible toplan the\noperations ofaNASAspacecraft inrealtime.\nThischaptercouldnotcovereveryareaofknowledgerepresentationindepth. Thethree\nprincipal topicsomittedarethefollowing:\nQUALITATIVE Qualitativephysics: Qualitativephysicsisasubfieldofknowledgerepresentationconcerned\nPHYSICS\nspecificallywithconstructingalogical,nonnumerictheoryofphysicalobjectsandprocesses.\nThe term was coined by Johan de Kleer (1975), although the enterprise could be said to\nhave started in Fahlman\u2019s (1974) BUILD, a sophisticated planner for constructing complex\ntowers of blocks. Fahlman discovered in the process of designing it that most of the effort\n(80%, by his estimate) went into modeling the physics of the blocks world to calculate the\nstability of various subassemblies of blocks, rather than into planning per se. He sketches a\nhypothetical naive-physics-like processtoexplainwhyyoungchildrencansolve BUILD-like\nproblemswithoutaccesstothehigh-speedfloating-pointarithmeticusedinBUILD\u2019sphysical\nmodeling. Hayes (1985a) uses \u201chistories\u201d\u2014four-dimensional slices of space-time similarto\nDavidson\u2019s events\u2014to construct a fairly complex naive physics of liquids. Hayes was the\nfirsttoprovethatabathwiththepluginwilleventuallyoverflowifthetapkeepsrunningand\nthat a person who falls into a lake will get wet all over. Davis (2008) gives an update to the\nontology ofliquids thatdescribes thepouringofliquidsintocontainers.\nDe Kleer and Brown (1985), Ken Forbus (1985), and Benjamin Kuipers (1985) inde-\npendently and almost simultaneously developed systems that can reason about a physical\nsystem based on qualitative abstractions of the underlying equations. Qualitative physics\nsoon developed to the point where it became possible to analyze an impressive variety of\ncomplex physical systems (Yip, 1991). Qualitative techniques have been used to construct\nnoveldesignsforclocks,windshieldwipers,andsix-leggedwalkers(SubramanianandWang,\n1994). Thecollection Readings inQualitative Reasoning about Physical Systems (Weldand Exercises 473\nde Kleer, 1990) an encyclopedia article by Kuipers (2001), and ahandbook article by Davis\n(2007)introduce tothefield.\nSpatialreasoning: Thereasoning necessary tonavigate inthe wumpusworld and shopping\nSPATIALREASONING\nworld is trivial in comparison to the rich spatial structure of the real world. The earliest\nseriousattempttocapturecommonsensereasoningaboutspaceappearsintheworkofErnest\nDavis(1986,1990). Theregionconnection calculus ofCohnetal.(1997)supportsaformof\nqualitative spatial reasoning and has led to new kinds of geographical information systems;\nseealso (Davis, 2006). Aswithqualitative physics, anagent cangoalong way, sotospeak,\nwithout resorting to a full metric representation. When such a representation is necessary,\ntechniques developedinrobotics(Chapter25)canbeused.\nPSYCHOLOGICAL Psychological reasoning: Psychological reasoning involves the development of a working\nREASONING\npsychology for artificial agents to use in reasoning about themselves and other agents. This\nis often based on so-called folk psychology, the theory that humans in general are believed\nto use in reasoning about themselves and other humans. When AI researchers provide their\nartificialagentswithpsychological theoriesforreasoning aboutotheragents,thetheoriesare\nfrequentlybasedontheresearchers\u2019descriptionofthelogicalagents\u2019owndesign. Psycholog-\nicalreasoning iscurrently mostuseful within thecontext ofnatural language understanding,\nwherediviningthespeaker\u2019s intentions isofparamount importance.\nMinker(2001)collectspapersbyleadingresearchersinknowledgerepresentation,sum-\nmarizing 40 years of work in the field. The proceedings of the international conferences on\nPrinciples ofKnowledgeRepresentation andReasoningprovide themostup-to-date sources\nfor work in this area. Readings in Knowledge Representation (Brachman and Levesque,\n1985) and Formal Theories of the Commonsense World (Hobbs and Moore, 1985) are ex-\ncellent anthologies on knowledge representation; the former focuses more on historically\nimportant papers in representation languages and formalisms, the latter onthe accumulation\nof the knowledge itself. Davis (1990), Stefik (1995), and Sowa (1999) provide textbook in-\ntroductions toknowledgerepresentation, vanHarmelen etal.(2007)contributes ahandbook,\nandaspecialissueofAIJournal coversrecentprogress(DavisandMorgenstern, 2004). The\nbiennial conference on Theoretical Aspects of Reasoning About Knowledge (TARK) covers\napplications ofthetheoryofknowledgeinAI,economics, anddistributed systems.\nEXERCISES\n12.1 Define an ontology in first-order logic for tic-tac-toe. The ontology should contain\nsituations,actions,squares,players,marks(X,O,orblank),andthenotionofwinning,losing,\nordrawing agame. Also define the notion ofaforced win(ordraw): aposition from which\na player can force a win (or draw) with the right sequence of actions. Write axioms for the\ndomain. (Note: The axioms that enumerate the different squares and that characterize the\nwinning positions are rather long. You need not write these out in full, but indicate clearly\nwhattheylooklike.) 474 Chapter 12. KnowledgeRepresentation\n12.2 Figure 12.1 shows the top levels of a hierarchy for everything. Extend it to include\nas many real categories as possible. A good way to do this is to cover all the things in your\neveryday life. This includes objects and events. Start with waking up, and proceed in an\norderly fashion noting everything that you see, touch, do, and think about. For example,\na random sampling produces music, news, milk, walking, driving, gas, Soda Hall, carpet,\ntalking, ProfessorFateman,chicken curry,tongue, $7,sun,thedailynewspaper, andsoon.\nYou should produce both a single hierarchy chart (on a large sheet of paper) and a\nlisting of objects and categories with the relations satisfied by members of each category.\nEveryobjectshould beinacategory, andeverycategoryshouldbeinthehierarchy.\n12.3 Develop a representational system for reasoning about windows in a window-based\ncomputerinterface. Inparticular, yourrepresentation shouldbeabletodescribe:\n\u2022 Thestateofawindow: minimized,displayed, ornonexistent.\n\u2022 Whichwindow(ifany)istheactivewindow.\n\u2022 Thepositionofeverywindowatagiventime.\n\u2022 Theorder(fronttoback)ofoverlapping windows.\n\u2022 Theactions of creating, destroying, resizing, and moving windows; changing the state\nofawindow;andbringing awindowtothefront. Treattheseactions asatomic;thatis,\ndo not deal with the issue of relating them to mouse actions. Give axioms describing\ntheeffectsofactionsonfluents. Youmayuseeithereventorsituation calculus.\nAssume an ontology containing situations, actions, integers (for x and y coordinates) and\nwindows. Definealanguage overthisontology; thatis, alistofconstants, function symbols,\nandpredicates withanEnglishdescription ofeach. Ifyouneedtoaddmorecategories tothe\nontology(e.g.,pixels),youmaydoso,butbesuretospecifytheseinyourwrite-up. Youmay\n(andshould) usesymbolsdefinedinthetext,butbesuretolisttheseexplicitly.\n12.4 Statethefollowinginthelanguage youdevelopedforthepreviousexercise:\na. In situation S , window W is behind W but sticks out on the left and right. Do not\n0 1 2\nstateexactcoordinates forthese;describe the generalsituation.\nb. Ifawindowisdisplayed, thenitstopedgeishigherthanitsbottomedge.\nc. Afteryoucreateawindow w,itisdisplayed.\nd. Awindowcanbeminimizedifitisdisplayed.\n12.5 (Adapted from an example by Doug Lenat.) Your mission is to capture, in logical\nform,enoughknowledgetoansweraseriesofquestionsabout thefollowingsimplescenario:\nYesterdayJohnwenttotheNorthBerkeleySafewaysupermarketandboughttwo\npoundsoftomatoesandapoundofgroundbeef.\nStart by trying to represent the content of the sentence as a series of assertions. You should\nwritesentences that have straightforward logical structure (e.g., statements that objects have\ncertainproperties,thatobjectsarerelatedincertainways,thatallobjectssatisfyingoneprop-\nertysatisfyanother). Thefollowingmighthelpyougetstarted: Exercises 475\n\u2022 Whichclasses, objects, and relations would youneed? Whatare theirparents, siblings\nandsoon? (Youwillneedeventsandtemporalordering, among otherthings.)\n\u2022 Wherewouldtheyfitinamoregeneralhierarchy?\n\u2022 Whataretheconstraints andinterrelationships amongthem?\n\u2022 Howdetailedmustyoubeabouteachofthevariousconcepts?\nTo answer the questions below, your knowledge base must include background knowledge.\nYou\u2019ll have to deal with what kind of things are at a supermarket, what is involved with\npurchasingthethingsoneselects,whatthepurchaseswillbeusedfor,andsoon. Trytomake\nyour representation as general as possible. Togive a trivial example: don\u2019t say \u201cPeople buy\nfoodfromSafeway,\u201dbecausethatwon\u2019thelpyouwiththosewhoshopatanothersupermarket.\nAlso,don\u2019tturnthequestionsintoanswers;forexample,question(c)asks\u201cDidJohnbuyany\nmeat?\u201d\u2014not\u201cDidJohnbuyapoundofgroundbeef?\u201d\nSketch the chains of reasoning that would answer the questions. If possible, use a\nlogicalreasoningsystemtodemonstratethesufficiencyofyourknowledgebase. Manyofthe\nthings you write might be only approximately correct in reality, but don\u2019t worry too much;\nthe idea is to extract the common sense that lets you answer these questions at all. A truly\ncompleteanswertothisquestionisextremelydifficult,probablybeyondthestateoftheartof\ncurrent knowledge representation. But you should beable to put together aconsistent set of\naxiomsforthelimitedquestions posedhere.\na. IsJohnachildoranadult? [Adult]\nb. DoesJohnnowhaveatleasttwotomatoes? [Yes]\nc. DidJohnbuyanymeat? [Yes]\nd. IfMarywasbuyingtomatoesatthesametimeasJohn,didheseeher? [Yes]\ne. Arethetomatoesmadeinthesupermarket? [No]\nf. WhatisJohngoingtodowiththetomatoes? [Eatthem]\ng. DoesSafewayselldeodorant? [Yes]\nh. DidJohnbringsomemoneyoracreditcardtothesupermarket? [Yes]\ni. DoesJohnhavelessmoneyaftergoingtothesupermarket? [Yes]\n12.6 Make the necessary additions or changes to your knowledge base from the previous\nexercisesothatthequestionsthatfollowcanbeanswered. Includeinyourreportadiscussion\nof your changes, explaining why they were needed, whether they were minor or major, and\nwhatkindsofquestions wouldnecessitate furtherchanges.\na. ArethereotherpeopleinSafewaywhileJohnisthere? [Yes\u2014staff!]\nb. IsJohnavegetarian? [No]\nc. Whoownsthedeodorant inSafeway? [SafewayCorporation]\nd. DidJohnhaveanounceofgroundbeef? [Yes]\ne. DoestheShellstationnextdoorhaveanygas? [Yes]\nf. DothetomatoesfitinJohn\u2019scartrunk? [Yes] 476 Chapter 12. KnowledgeRepresentation\n12.7 Represent the following seven sentences using and extending the representations de-\nvelopedinthechapter:\na. Waterisaliquidbetween0and100degrees.\nb. Waterboilsat100degrees.\nc. ThewaterinJohn\u2019swaterbottleisfrozen.\nd. Perrierisakindofwater.\ne. JohnhasPerrierinhiswaterbottle.\nf. Allliquids haveafreezing point.\ng. Aliterofwaterweighsmorethanaliterofalcohol.\n12.8 Writedefinitionsforthefollowing:\na. ExhaustivePartDecomposition\nb. PartPartition\nc. PartwiseDisjoint\nTheseshould beanalogous tothedefinitions forExhaustiveDecomposition,Partition,and\nDisjoint. Is it the case that PartPartition(s,BunchOf(s))? If so, prove it; if not, give a\ncounterexample anddefinesufficientconditions underwhich itdoeshold.\n12.9 Analternative scheme forrepresenting measures involves applying the units function\nto an abstract length object. In such a scheme, one would write Inches(Length(L )) =\n1\n1.5. How does this scheme compare with the one in the chapter? Issues include conversion\naxioms, names for abstract quantities (such as \u201c50 dollars\u201d), and comparisons of abstract\nmeasuresindifferent units(50inchesismorethan50centimeters).\n12.10 Add sentences to extend the definition of the predicate Name(s,c) so that a string\nsuch as \u201claptop computer\u201d matches the appropriate category names from a variety of stores.\nTrytomakeyourdefinitiongeneral. Testitbylookingattenonlinestores,andatthecategory\nnames they give for three different categories. Forexample, for the category of laptops, we\nfound the names \u201cNotebooks,\u201d \u201cLaptops,\u201d \u201cNotebook Computers,\u201d \u201cNotebook,\u201d \u201cLaptops\nandNotebooks,\u201dand\u201cNotebookPCs.\u201d SomeofthesecanbecoveredbyexplicitName facts,\nwhileotherscouldbecoveredbysentences forhandling plurals, conjunctions, etc.\n12.11 Writeeventcalculus axiomstodescribetheactionsinthewumpusworld.\n12.12 Statetheinterval-algebra relationthatholdsbetweeneverypairofthefollowingreal-\nworldevents:\nLK: ThelifeofPresidentKennedy.\nIK: TheinfancyofPresident Kennedy.\nPK: Thepresidency ofPresident Kennedy.\nLJ: ThelifeofPresident Johnson.\nPJ: Thepresidency ofPresidentJohnson.\nLO: ThelifeofPresidentObama. Exercises 477\n12.13 Investigate ways to extend the event calculus to handle simultaneous events. Is it\npossible toavoidacombinatorial explosion ofaxioms?\n12.14 Constructarepresentationforexchangeratesbetweencurrenciesthatallowsfordaily\nfluctuations.\n12.15 Define the predicate Fixed, where Fixed(Location(x)) means that the location of\nobjectxisfixedovertime.\n12.16 Describe the event of trading something for something else. Describe buying as a\nkindoftradinginwhichoneoftheobjectstradedisasumofmoney.\n12.17 The two preceding exercises assume a fairly primitive notion of ownership. Forex-\nample, the buyer starts by owning the dollar bills. This picture begins to break down when,\nfor example, one\u2019s money is in the bank, because there is no longer any specific collection\nof dollar bills that one owns. The picture is complicated still further by borrowing, leasing,\nrenting,andbailment. Investigatethevariouscommonsenseandlegalconceptsofownership,\nandpropose aschemebywhichtheycanberepresented formally.\n12.18 (Adapted from Fagin et al. (1995).) Consider a game played with a deck of just 8\ncards,4acesand4kings. Thethreeplayers,Alice,Bob,andCarlos,aredealttwocardseach.\nWithoutlookingatthem,theyplacethecardsontheirforeheads sothattheotherplayerscan\nsee them. Then the players take turns either announcing that they know what cards are on\ntheir own forehead, thereby winning the game, or saying \u201cI don\u2019t know.\u201d Everyone knows\ntheplayersaretruthfulandareperfectatreasoning aboutbeliefs.\na. Game1. Alice and Bob have both said \u201cI don\u2019t know.\u201d Carlos sees that Alice has two\naces(A-A)andBobhastwokings (K-K).Whatshould Carlossay? (Hint: consider all\nthreepossible casesforCarlos: A-A,K-K,A-K.)\nb. DescribeeachstepofGame1usingthenotation ofmodallogic.\nc. Game2. Carlos, Alice, and Boballsaid \u201cI don\u2019t know\u201d ontheirfirstturn. Aliceholds\nK-KandBobholdsA-K.Whatshould Carlossayonhissecondturn?\nd. Game3. Alice,Carlos,andBoballsay\u201cIdon\u2019tknow\u201dontheirfirstturn,asdoesAlice\nonhersecondturn. AliceandBobbothholdA-K.WhatshouldCarlossay?\ne. Provethattherewillalwaysbeawinnertothisgame.\n12.19 Theassumption oflogical omniscience, discussed on page453, isofcourse not true\nof any actual reasoners. Rather, it is an idealization of the reasoning process that may be\nmore or less acceptable depending on the applications. Discuss the reasonableness of the\nassumption foreachofthefollowingapplications ofreasoning aboutknowledge:\na. Partial knowledge adversary games, such as card games. Here one player wants to\nreasonaboutwhathisopponent knowsaboutthestateofthegame.\nb. Chess with a clock. Here the player may wish to reason about the limits of his oppo-\nnent\u2019s or his own ability to find the best move in the time available. For instance, if\nplayer A has much more time left than player B, then A will sometimes make a move\nthat greatly complicates thesituation, in thehopes ofgaining an advantage because he\nhasmoretimetoworkouttheproperstrategy. 478 Chapter 12. KnowledgeRepresentation\nc. Ashopping agentinanenvironment inwhichtherearecostsofgatheringinformation.\nd. Reasoning about public key cryptography, which rests on the intractability of certain\ncomputational problems.\n12.20 Translate the following description logic expression (from page 457) into first-order\nlogic,andcommentontheresult:\nAnd(Man,AtLeast(3,Son),AtMost(2,Daughter),\nAll(Son,And(Unemployed,Married,All(Spouse,Doctor))),\nAll(Daughter,And(Professor,Fills(Department,Physics,Math)))).\n12.21 Recall that inheritance information in semantic networks can be captured logically\nby suitable implication sentences. This exercise investigates the efficiency of using such\nsentences forinheritance.\na. Consider the information in a used-car catalog such as Kelly\u2019s Blue Book\u2014for exam-\nple, that 1973 Dodge vans are (or perhaps were once) worth $575. Suppose all this\ninformation (for 11,000 models) is encoded as logical sentences, as suggested in the\nchapter. Write down three such sentences, including that for 1973 Dodge vans. How\nwould you use the sentences to find the value of a particular car, given a backward-\nchainingtheorem proversuchasProlog?\nb. Comparethetimeefficiencyofthebackward-chaining methodforsolvingthisproblem\nwiththeinheritance methodusedinsemanticnets.\nc. Explain how forward chaining allows a logic-based system to solve the same problem\nefficiently,assuming thattheKBcontainsonlythe11,000sentences aboutprices.\nd. Describe a situation in which neither forward nor backward chaining on the sentences\nwillallowthepricequeryforanindividual cartobehandled efficiently.\ne. Can you suggest a solution enabling this type of query to be solved efficiently in all\ncases in logic systems? (Hint: Remember that two cars of the same year and model\nhavethesameprice.)\n12.22 One might suppose that the syntactic distinction between unboxed links and singly\nboxed links in semantic networks is unnecessary, because singly boxed links are always at-\ntached to categories; an inheritance algorithm could simply assume that an unboxed link\nattached to a category is intended to apply to all members of that category. Show that this\nargumentisfallacious, givingexamplesoferrorsthatwouldarise.\n12.23 One part of the shopping process that was not covered in this chapter is checking\nforcompatibility between items. Forexample, ifadigital cameraisordered, whataccessory\nbatteries, memorycards,andcasesarecompatiblewiththecamera? Writeaknowledgebase\nthatcandetermine thecompatibility ofasetofitemsandsuggest replacements oradditional\nitemsiftheshoppermakesachoicethatisnotcompatible. Theknowledgebaseshouldworks\nwithatleastonelineofproductsandextendeasilytootherlines.\n12.24 A complete solution to the problem of inexact matches to the buyer\u2019s description\nin shopping is very difficult and requires a full array of natural language processing and Exercises 479\ninformation retrieval techniques. (See Chapters 22 and 23.) One small step is to allow the\nusertospecifyminimumandmaximumvaluesforvariousattributes. Thebuyermustusethe\nfollowinggrammarforproductdescriptions:\nDescription \u2192 Category [Connector Modifier]\u2217\nConnector \u2192 \u201cwith\u201d | \u201cand\u201d | \u201c,\u201d\nModifier \u2192 Attribute | Attribute Op Value\nOp \u2192 \u201c=\u201d | \u201c>\u201d| \u201c<\u201d\nHere, Category names a product category, Attribute is some feature such as \u201cCPU\u201d or\n\u201cprice,\u201dandValue isthetargetvaluefortheattribute. Sothequery\u201ccomputer withatleasta\n2.5GHzCPUforunder$500\u201dmustbere-expressed as\u201ccomputer withCPU>2.5GHzand\nprice<$500.\u201d Implementashopping agentthatacceptsdescriptions inthislanguage.\n12.25 OurdescriptionofInternetshoppingomittedtheall-importantstepofactuallybuying\nthe product. Provide a formal logical description of buying, using event calculus. That is,\ndefine the sequence of events that occurs when a buyer submits a credit-card purchase and\ntheneventually getsbilledandreceivestheproduct. 13\nQUANTIFYING\nUNCERTAINTY\nInwhichweseehowanagentcantameuncertainty withdegrees ofbelief.\n13.1 ACTING UNDER UNCERTAINTY\nAgents may need to handle uncertainty, whether due to partial observability, nondetermin-\nUNCERTAINTY\nism, ora combination of the two. An agent may never know for certain what state it\u2019s in or\nwhereitwillendupafterasequence ofactions.\nWehaveseenproblem-solvingagents(Chapter4)andlogicalagents(Chapters7and11)\ndesigned tohandleuncertainty bykeeping trackofabeliefstate\u2014arepresentation oftheset\nof all possible world states that it might be in\u2014and generating a contingency plan that han-\ndleseverypossibleeventualitythatitssensorsmayreport duringexecution. Despiteitsmany\nvirtues, however,thisapproach hassignificantdrawbacks whentakenliterally asarecipefor\ncreating agentprograms:\n\u2022 When interpreting partial sensor information, a logical agent must consider every log-\nically possible explanation for the observations, no matter how unlikely. This leads to\nimpossiblelargeandcomplexbelief-state representations.\n\u2022 Acorrect contingent plan that handles every eventuality can grow arbitrarily large and\nmustconsiderarbitrarily unlikely contingencies.\n\u2022 Sometimes there is no plan that is guaranteed to achieve the goal\u2014yet the agent must\nact. Itmusthavesomewaytocomparethemeritsofplansthatarenotguaranteed.\nSuppose, for example, that an automated taxi!automated has the goal of delivering a pas-\nsenger to the airport on time. The agent forms a plan, A , that involves leaving home 90\n90\nminutes before the flight departs and driving at a reasonable speed. Even though the airport\nis only about 5 miles away, a logical taxi agent will not be able to conclude with certainty\nthat \u201cPlan A will get us to the airport in time.\u201d Instead, it reaches the weaker conclusion\n90\n\u201cPlan A willget usto theairport in time, aslong asthe cardoesn\u2019t break down orrun out\n90\nofgas,andIdon\u2019tgetintoanaccident,andtherearenoaccidentsonthebridge,andtheplane\ndoesn\u2019t leave early, and nometeorite hits the car, and ....\u201d None ofthese conditions can be\n480 Section13.1. ActingunderUncertainty 481\ndeducedforsure,sotheplan\u2019ssuccesscannotbeinferred. Thisisthequalificationproblem\n(page268),forwhichwesofarhaveseennorealsolution.\nNonetheless, in some sense A is in fact the right thing to do. What do we mean by\n90\nthis? Aswediscussed inChapter2,wemeanthatoutofalltheplansthatcouldbeexecuted,\nA isexpected tomaximize theagent\u2019s performance measure (wheretheexpectation isrel-\n90\native to the agent\u2019s knowledge about the environment). The performance measure includes\ngetting to the airport in time for the flight, avoiding a long, unproductive wait at the airport,\nandavoidingspeedingticketsalongtheway. Theagent\u2019sknowledgecannotguaranteeanyof\nthese outcomes for A , but it can provide some degree of belief that they will be achieved.\n90\nOther plans, such as A , might increase the agent\u2019s belief that it will get to the airport on\n180\ntime, but also increase the likelihood of a long wait. The right thing to do\u2014the rational\ndecision\u2014therefore depends on both the relative importance of various goals and the likeli-\nhood that, and degree to which, they will be achieved. The remainder of this section hones\ntheseideas,inpreparation forthedevelopmentofthegeneraltheories ofuncertainreasoning\nandrationaldecisions thatwepresent inthisandsubsequent chapters.\n13.1.1 Summarizing uncertainty\nLet\u2019s consider an example of uncertain reasoning: diagnosing a dental patient\u2019s toothache.\nDiagnosis\u2014whether for medicine, automobile repair, or whatever\u2014almost always involves\nuncertainty. Letustrytowriterulesfordentaldiagnosisusingpropositional logic,sothatwe\ncanseehowthelogicalapproach breaksdown. Considerthefollowingsimplerule:\nToothache \u21d2 Cavity .\nThe problem is that this rule is wrong. Not all patients with toothaches have cavities; some\nofthemhavegumdisease, anabscess, oroneofseveralotherproblems:\nToothache \u21d2 Cavity \u2228GumProblem \u2228Abscess...\nUnfortunately, in order to make the rule true, we have to add an almost unlimited list of\npossible problems. Wecouldtryturningtheruleintoacausalrule:\nCavity \u21d2 Toothache .\nBut this rule is not right either; not all cavities cause pain. The only way to fix the rule\nis to make it logically exhaustive: to augment the left-hand side with all the qualifications\nrequired for a cavity to cause a toothache. Trying to use logic to cope with a domain like\nmedicaldiagnosis thusfailsforthreemainreasons:\n\u2022 Laziness: It is too much work to list the complete set of antecedents or consequents\nLAZINESS\nneededtoensureanexceptionless ruleandtoohardtousesuchrules.\nTHEORETICAL \u2022 Theoreticalignorance: Medicalsciencehasnocompletetheoryforthedomain.\nIGNORANCE\nPRACTICAL \u2022 Practical ignorance: Even if we know all the rules, we might be uncertain about a\nIGNORANCE\nparticularpatientbecausenotallthenecessary testshave beenorcanberun.\nThe connection between toothaches and cavities is just not a logical consequence in either\ndirection. This is typical of the medical domain, as well as most other judgmental domains:\nlaw,business,design,automobilerepair,gardening,dating,andsoon. Theagent\u2019sknowledge 482 Chapter 13. Quantifying Uncertainty\ncan at best provide only a degree of belief in the relevant sentences. Our main tool for\nDEGREEOFBELIEF\nPROBABILITY dealing with degrees of belief is probability theory. In the terminology of Section 8.1, the\nTHEORY\nontological commitments of logic and probability theory are the same\u2014that the world is\ncomposed of facts that do or do not hold in any particular case\u2014but the epistemological\ncommitments are different: a logical agent believes each sentence to be true or false or has\nno opinion, whereas a probabilistic agent may have a numerical degree of belief between 0\n(forsentences thatarecertainly false)and1(certainly true).\nProbability provides a way of summarizing the uncertainty that comes from our lazi-\nness and ignorance, thereby solving the qualification problem. We might not know forsure\nwhat afflicts a particular patient, but we believe that there is, say, an 80% chance\u2014that is,\na probability of 0.8\u2014that the patient who has a toothache has a cavity. That is, we expect\nthat outofallthesituations that areindistinguishable from the current situation asfarasour\nknowledge goes, the patient will have acavity in 80% of them. This belief could be derived\nfrom statistical data\u201480% of the toothache patients seen so far have had cavities\u2014or from\nsomegeneraldentalknowledge, orfromacombination ofevidencesources.\nOne confusing point is that at the time of our diagnosis, there is no uncertainty in the\nactual world: the patient either has a cavity or doesn\u2019t. So what does it mean to say the\nprobability of a cavity is 0.8? Shouldn\u2019t it be either 0 or 1? The answer is that probability\nstatementsaremadewithrespecttoaknowledgestate,notwithrespecttotherealworld. We\nsay\u201cTheprobabilitythatthepatienthasacavity,giventhatshehasatoothache,is0.8.\u201d Ifwe\nlater learn that the patient has a history of gum disease, we can make a different statement:\n\u201cTheprobability that the patient has acavity, given that she has atoothache and ahistory of\ngum disease, is 0.4.\u201d If we gather further conclusive evidence against a cavity, we can say\n\u201cTheprobability thatthepatient hasacavity, givenallwenowknow,isalmost0.\u201d Notethat\nthese statements do not contradict each other; each is a separate assertion about a different\nknowledgestate.\n13.1.2 Uncertainty andrational decisions\nConsider again the A plan for getting to the airport. Suppose it gives us a 97% chance\n90\nof catching our flight. Does this mean it is a rational choice? Not necessarily: there might\nbe other plans, such as A , with higher probabilities. If it is vital not to miss the flight,\n180\nthen itisworth risking the longer waitattheairport. What about A ,aplan that involves\n1440\nleavinghome24hoursinadvance? Inmostcircumstances, thisisnotagoodchoice,because\nalthough it almost guarantees getting there on time, it involves an intolerable wait\u2014not to\nmentionapossibly unpleasant dietofairportfood.\nTomakesuchchoices, anagent mustfirsthave preferences between thedifferent pos-\nPREFERENCE\nsible outcomes of the various plans. An outcome is a completely specified state, including\nOUTCOME\nsuchfactorsaswhethertheagentarrivesontimeandthelengthofthewaitattheairport. We\nuseutilitytheorytorepresent andreason withpreferences. (Theterm utilityisusedherein\nUTILITYTHEORY\nthe sense of \u201cthe quality of being useful,\u201d not in the sense of the electric company or water\nworks.) Utility theory says that every state has a degree of usefulness, orutility, to an agent\nandthattheagentwillpreferstateswithhigherutility. Section13.2. BasicProbability Notation 483\nTheutility of astate isrelative toan agent. Forexample, the utility ofastate in which\nWhitehascheckmatedBlackinagameofchessisobviouslyhighfortheagentplayingWhite,\nbutlowfortheagentplayingBlack. Butwecan\u2019tgostrictlybythescoresof1,1\/2,and0that\naredictatedbytherulesoftournamentchess\u2014someplayers(includingtheauthors)mightbe\nthrilledwithadrawagainsttheworldchampion,whereasotherplayers(includingtheformer\nworldchampion)mightnot. Thereisnoaccounting fortasteorpreferences: youmightthink\nthat anagent whoprefers jalapen\u02dco bubble-gum ice cream to chocolate chocolate chip is odd\norevenmisguided,butyoucouldnotsaytheagentisirrational. Autilityfunctioncanaccount\nforanysetofpreferences\u2014quirkyortypical,nobleorperverse. Notethatutilitiescanaccount\nforaltruism,simplybyincluding thewelfareofothersasoneofthefactors.\nPreferences, as expressed by utilities, are combined with probabilities in the general\ntheoryofrational decisions called decisiontheory:\nDECISIONTHEORY\nDecisiontheory = probability theory+utilitytheory.\nThe fundamental idea of decision theory is that an agent is rational if and only if it chooses\nthe action that yields the highest expected utility, averaged over all the possible outcomes\nMAXIMUMEXPECTED of the action. This is called the principle of maximum expected utility (MEU). Note that\nUTILITY\n\u201cexpected\u201d might seem like avague, hypothetical term, but asitisused here ithas aprecise\nmeaning: it means the \u201caverage,\u201d or \u201cstatistical mean\u201d of the outcomes, weighted by the\nprobability of the outcome. We saw this principle in action in Chapter 5 when we touched\nbrieflyonoptimaldecisions inbackgammon;itisinfactacompletely generalprinciple.\nFigure13.1sketchesthestructureofanagentthatusesdecisiontheorytoselectactions.\nThe agent is identical, at an abstract level, to the agents described in Chapters 4 and 7 that\nmaintain a belief state reflecting the history of percepts to date. The primary difference is\nthat the decision-theoretic agent\u2019s belief state represents not just the possibilities for world\nstates but also their probabilities. Given the belief state, the agent can make probabilistic\npredictions ofactionoutcomesandhenceselecttheactionwithhighestexpectedutility. This\nchapterandthenextconcentrateonthetaskofrepresenting andcomputingwithprobabilistic\ninformation ingeneral. Chapter 15 deals with methods forthe specific tasks of representing\nand updating the belief state over time and predicting the environment. Chapter 16 covers\nutility theory in more depth, and Chapter 17 develops algorithms for planning sequences of\nactionsinuncertain environments.\n13.2 BASIC PROBABILITY NOTATION\nFor our agent to represent and use probabilistic information, we need a formal language.\nThe language of probability theory has traditionally been informal, written by human math-\nematicians to other human mathematicians. Appendix Aincludes astandard introduction to\nelementary probability theory; here, wetake anapproach moresuited totheneeds ofAIand\nmoreconsistent withtheconcepts offormallogic. 484 Chapter 13. Quantifying Uncertainty\nfunctionDT-AGENT(percept)returnsanaction\npersistent: belief state,probabilisticbeliefsaboutthecurrentstateoftheworld\naction,theagent\u2019saction\nupdatebelief state basedonaction andpercept\ncalculateoutcomeprobabilitiesforactions,\ngivenactiondescriptionsandcurrentbelief state\nselectaction withhighestexpectedutility\ngivenprobabilitiesofoutcomesandutilityinformation\nreturnaction\nFigure13.1 Adecision-theoreticagentthatselectsrationalactions.\n13.2.1 Whatprobabilities areabout\nLike logical assertions, probabilistic assertions are about possible worlds. Whereas logical\nassertions saywhichpossible worldsarestrictly ruled out (allthoseinwhichtheassertion is\nfalse), probabilistic assertions talkabouthowprobable thevariousworldsare. Inprobability\ntheory, the set of all possible worlds is called the sample space. The possible worlds are\nSAMPLESPACE\nmutually exclusive and exhaustive\u2014two possible worlds cannot both be the case, and one\npossible world must be the case. For example, if we are about to roll two (distinguishable)\ndice, there are 36 possible worlds to consider: (1,1), (1,2), ..., (6,6). The Greek letter \u03a9\n(uppercase omega) is used to refer to the sample space, and \u03c9 (lowercase omega) refers to\nelementsofthespace, thatis,particularpossible worlds.\nAfullyspecifiedprobabilitymodelassociatesanumericalprobability P(\u03c9)witheach\nPROBABILITYMODEL\npossible world.1 The basic axioms of probability theory say that every possible world has a\nprobability between0and1andthatthetotalprobability ofthesetofpossible worldsis1:\n(cid:12)\n0 \u2264 P(\u03c9) \u2264 1forevery \u03c9 and P(\u03c9) = 1. (13.1)\n\u03c9\u2208\u03a9\nForexample, if we assume that each die is fair and the rolls don\u2019t interfere with each other,\nthen each of the possible worlds (1,1), (1,2), ..., (6,6) has probability 1\/36. On the other\nhand,ifthediceconspiretoproducethesamenumber,thentheworlds(1,1),(2,2),(3,3),etc.,\nmighthavehigherprobabilities, leaving theotherswithlowerprobabilities.\nProbabilisticassertionsandqueriesarenotusuallyabout particularpossibleworlds,but\nabout setsofthem. Forexample, wemightbeinterested inthe cases wherethetwodice add\nup to 11, the cases where doubles are rolled, and so on. In probability theory, these sets are\ncalled events\u2014a term already used extensively in Chapter 12for adifferent concept. In AI,\nEVENT\nthe sets are always described by propositions in a formal language. (One such language is\ndescribed in Section 13.2.2.) Foreach proposition, the corresponding set contains just those\npossibleworldsinwhichthepropositionholds. Theprobabilityassociatedwithaproposition\n1 Fornow,weassumeadiscrete,countablesetofworlds.Thepropertreatmentofthecontinuouscasebringsin\ncertaincomplicationsthatarelessrelevantformostpurposesinAI. Section13.2. BasicProbability Notation 485\nisdefinedtobethesumoftheprobabilities oftheworldsinwhichitholds:\n(cid:12)\nForanyproposition \u03c6, P(\u03c6) = P(\u03c9). (13.2)\n\u03c9\u2208\u03c6\nFor example, when rolling fair dice, we have P(Total =11) = P((5,6)) + P((6,5)) =\n1\/36 + 1\/36 = 1\/18. Note that probability theory does not require complete knowledge\nof the probabilities of each possible world. For example, if we believe the dice conspire to\nproducethesamenumber,wemightassertthatP(doubles) = 1\/4withoutknowingwhether\nthedicepreferdouble 6todouble 2. Just aswithlogical assertions, thisassertion constrains\ntheunderlying probability modelwithoutfullydetermining it.\nUNCONDITIONAL ProbabilitiessuchasP(Total =11)andP(doubles)arecalledunconditionalorprior\nPROBABILITY\nprobabilities(andsometimesjust\u201cpriors\u201dforshort);theyrefertodegreesofbeliefinpropo-\nPRIORPROBABILITY\nsitions in the absence of any other information. Most of the time, however, we have some\ninformation, usually called evidence, that has already been revealed. For example, the first\nEVIDENCE\ndie may already be showing a 5 and we are waiting with bated breath for the other one to\nstop spinning. In that case, we are interested not in the unconditional probability of rolling\nCONDITIONAL doubles, buttheconditionalorposteriorprobability (orjust\u201cposterior\u201d forshort)ofrolling\nPROBABILITY\nPOSTERIOR doublesgiventhatthefirstdieisa5. ThisprobabilityiswrittenP(doubles|Die =5),where\nPROBABILITY 1\nthe \u201c|\u201d is pronounced \u201cgiven.\u201d Similarly, if I am going to the dentist for a regular checkup,\nthe probability P(cavity)=0.2might beofinterest; but ifIgo tothe dentist because Ihave\natoothache, it\u2019sP(cavity|toothache)=0.6thatmatters. Notethattheprecedence of\u201c|\u201dis\nsuchthatanyexpression oftheform P(...|...)alwaysmeansP((...)|(...)).\nIt is important to understand that P(cavity)=0.2 is still valid after toothache is ob-\nserved; it just isn\u2019t especially useful. When making decisions, an agent needs to condition\non all the evidence it has observed. It is also important to understand the difference be-\ntween conditioning and logical implication. The assertion that P(cavity|toothache)=0.6\ndoes not mean \u201cWhenever toothache is true, conclude that cavity is true with probabil-\nity 0.6\u201d rather it means \u201cWhenever toothache is true and we have no further information,\nconclude that cavity is true with probability 0.6.\u201d The extra condition is important; for ex-\nample, if we had the further information that the dentist found no cavities, we definitely\nwould not want to conclude that cavity is true with probability 0.6; instead we need to use\nP(cavity|toothache \u2227\u00accavity)=0.\nMathematically speaking, conditional probabilities are defined in terms of uncondi-\ntionalprobabilities asfollows: foranypropositions aandb,wehave\nP(a\u2227b)\nP(a|b) = , (13.3)\nP(b)\nwhichholdswheneverP(b) > 0. Forexample,\nP(doubles \u2227Die =5)\nP(doubles|Die =5) = 1 .\n1\nP(Die =5)\n1\nThe definition makes sense if you remember that observing b rules out all those possible\nworldswhere bisfalse,leaving asetwhosetotalprobability isjustP(b). Withinthatset,the\na-worldssatisfy a\u2227bandconstitute afraction P(a\u2227b)\/P(b). 486 Chapter 13. Quantifying Uncertainty\nThedefinition of conditional probability, Equation (13.3), can be written in a different\nformcalledtheproductrule:\nPRODUCTRULE\nP(a\u2227b)= P(a|b)P(b),\nTheproduct ruleisperhaps easiertoremember: itcomesfrom thefactthat,foraandbtobe\ntrue,weneedbtobetrue,andwealsoneed atobetruegivenb.\n13.2.2 The languageofpropositions inprobability assertions\nIn this chapter and the next, propositions describing sets of possible worlds are written in a\nnotationthatcombineselementsofpropositionallogicandconstraintsatisfactionnotation. In\nthe terminology of Section 2.4.7, it is a factored representation, in which apossible world\nisrepresented byasetofvariable\/value pairs.\nVariablesinprobabilitytheoryarecalledrandomvariablesandtheirnamesbeginwith\nRANDOMVARIABLE\nanuppercase letter. Thus, inthe dice example, Total and Die are random variables. Every\n1\nrandom variable has a domain\u2014the set of possible values it can take on. The domain of\nDOMAIN\nTotal for two dice is the set {2,...,12} and the domain of Die is {1,...,6}. A Boolean\n1\nrandom variable has the domain {true,false} (notice that values are always lowercase); for\nexample, the proposition that doubles are rolled can be written as Doubles=true. By con-\nvention, propositions of the form A=true are abbreviated simply as a, while A=false is\nabbreviated as\u00aca. (Theusesofdoubles,cavity,andtoothache inthepreceding section are\nabbreviations of this kind.) As in CSPs, domains can be sets of arbitrary tokens; we might\nchoose the domain of Age to be {juvenile,teen,adult} and the domain of Weather might\nbe{sunny,rain,cloudy,snow}. Whennoambiguityispossible,itiscommontouseavalue\nbyitselftostandfortheproposition thataparticularvariablehasthatvalue;thus, sunny can\nstandforWeather =sunny.\nThe preceding examples all have finite domains. Variables can have infinite domains,\ntoo\u2014eitherdiscrete(liketheintegers)orcontinuous(likethereals). Foranyvariablewithan\nordereddomain,inequalitiesarealsoallowed,suchasNumberOfAtomsInUniverse \u2265 1070.\nFinally, we can combine these sorts of elementary propositions (including the abbre-\nviated forms for Boolean variables) by using the connectives of propositional logic. For\nexample, we can express \u201cThe probability that the patient has a cavity, given that she is a\nteenagerwithnotoothache, is0.1\u201dasfollows:\nP(cavity|\u00actoothache \u2227teen) = 0.1.\nSometimeswewillwanttotalkabouttheprobabilities ofallthepossible valuesofarandom\nvariable. Wecouldwrite:\nP(Weather =sunny) = 0.6\nP(Weather =rain)= 0.1\nP(Weather =cloudy)= 0.29\nP(Weather =snow)= 0.01,\nbutasanabbreviation wewillallow\nP(Weather)=(cid:16)0.6,0.1,0.29,0.01(cid:17), Section13.2. BasicProbability Notation 487\nwheretheboldPindicatesthattheresultisavectorofnumbers,andwherewe assumeapre-\ndefined ordering (cid:16)sunny,rain,cloudy,snow(cid:17) on the domain of Weather. We say that the\nPROBABILITY PstatementdefinesaprobabilitydistributionfortherandomvariableWeather. ThePnota-\nDISTRIBUTION\ntionisalsousedforconditionaldistributions: P(X|Y)givesthevaluesofP(X=x |Y =y )\ni j\nforeachpossible i,j pair.\nForcontinuousvariables,itisnotpossibletowriteouttheentiredistributionasavector,\nbecausethereareinfinitelymanyvalues. Instead,wecandefinetheprobabilitythatarandom\nvariabletakesonsomevalue xasaparameterized function of x. Forexample,thesentence\nP(NoonTemp=x) = Uniform (x)\n[18C,26C]\nexpresses thebelief that the temperature at noon isdistributed uniformly between 18and 26\nPROBABILITY degreesCelsius. Wecallthisaprobabilitydensityfunction.\nDENSITYFUNCTION\nProbability density functions (sometimes called pdfs) differ in meaning from discrete\ndistributions. Saying that the probability density is uniform from 18C to 26C means that\nthere is a 100% chance that the temperature will fall somewhere in that 8C-wide region\nanda50%chancethatitwillfallinany4C-wideregion,andsoon. Wewritetheprobability\ndensityforacontinuousrandomvariableX atvaluexasP(X=x)orjustP(x);theintuitive\ndefinition ofP(x)istheprobability that X fallswithinanarbitrarily smallregion beginning\natx,dividedbythewidthoftheregion:\nP(x) = lim P(x \u2264 X \u2264 x+dx)\/dx .\ndx\u21920\nForNoonTemp wehave\n(cid:24)\n1 if18C \u2264 x \u2264 26C\nP(NoonTemp=x) = Uniform (x) = 8C ,\n[18C,26C] 0otherwise\nwhere C stands forcentigrade (not for aconstant). In P(NoonTemp=20.18C)= 1 , note\n8C\nthat 1 is not a probability, it is a probability density. The probability that NoonTemp is\n8C\nexactly 20.18C is zero, because 20.18C is a region of width 0. Some authors use different\nsymbolsfordiscretedistributions anddensityfunctions; weuseP inbothcases,sinceconfu-\nsionseldomarisesandtheequationsareusuallyidentical. Notethatprobabilities areunitless\nnumbers,whereasdensityfunctionsaremeasuredwithaunit,inthiscasereciprocaldegrees.\nIn addition to distributions on single variables, we need notation for distributions on\nmultiple variables. Commas are used for this. For example, P(Weather,Cavity) denotes\nthe probabilities of all combinations of the values of Weather and Cavity. This is a 4\u00d72\nJOINTPROBABILITY table of probabilities called the joint probability distribution ofWeather and Cavity. We\nDISTRIBUTION\ncan also mix variables with and without values; P(sunny,Cavity) would be a two-element\nvector giving the probabilities of a sunny day with a cavity and a sunny day with no cavity.\nThePnotation makes certain expressions much moreconcise than they might otherwise be.\nForexample,theproduct rulesforallpossible valuesof Weather andCavity canbewritten\nasasingleequation:\nP(Weather,Cavity) = P(Weather |Cavity)P(Cavity), 488 Chapter 13. Quantifying Uncertainty\ninsteadofasthese4\u00d72=8equations (usingabbreviations W andC):\nP(W =sunny\u2227C=true)=P(W =sunny|C=true)P(C=true)\nP(W =rain \u2227C=true)=P(W =rain|C=true)P(C=true)\nP(W =cloudy \u2227C=true)=P(W =cloudy|C=true)P(C=true)\nP(W =snow \u2227C=true)=P(W =snow|C=true)P(C=true)\nP(W =sunny\u2227C=false)=P(W =sunny|C=false)P(C=false)\nP(W =rain \u2227C=false)=P(W=rain|C=false)P(C=false)\nP(W =cloudy \u2227C=false)=P(W =cloudy|C=false)P(C=false)\nP(W =snow \u2227C=false)=P(W =snow|C=false)P(C=false).\nAs a degenerate case, P(sunny,cavity) has no variables and thus is a one-element vec-\ntor that is the probability of a sunny day with a cavity, which could also be written as\nP(sunny,cavity)orP(sunny\u2227cavity). WewillsometimesusePnotationtoderiveresults\naboutindividualP values,andwhenwesay\u201cP(sunny)=0.6\u201ditisreallyanabbreviationfor\n\u201cP(sunny)istheone-element vector(cid:16)0.6(cid:17),whichmeansthatP(sunny)=0.6.\u201d\nNowwehave defined asyntax forpropositions andprobability assertions andwehave\ngivenpartofthesemantics: Equation(13.2)definestheprobabilityofapropositionasthesum\nof the probabilities of worlds in which it holds. To complete the semantics, we need to say\nwhattheworldsareandhowtodeterminewhetherapropositionholdsinaworld. Weborrow\nthis part directly from the semantics of propositional logic, as follows. A possible world is\ndefinedtobeanassignmentofvaluestoalloftherandomvariablesunderconsideration. Itis\neasytoseethatthisdefinitionsatisfiesthebasicrequirementthatpossibleworldsbemutually\nexclusive and exhaustive (Exercise 13.5). For example, if the random variables are Cavity,\nToothache, and Weather, then there are 2\u00d72\u00d74=16 possible worlds. Furthermore, the\ntruth of any given proposition, no matter how complex, can be determined easily in such\nworldsusingthesamerecursive definitionoftruthasforformulasinpropositional logic.\nFromthepreceding definition ofpossible worlds, itfollows that aprobability modelis\ncompletelydeterminedbythejointdistributionforalloftherandomvariables\u2014theso-called\nFULLJOINT full joint probability distribution. For example, if the variables are Cavity, Toothache,\nPROBABILITY\nDISTRIBUTION\nand Weather, then the full joint distribution is given by P(Cavity,Toothache,Weather).\nThisjoint distribution canberepresented asa 2\u00d72\u00d74tablewith16entries. Because every\nproposition\u2019s probability is a sum over possible worlds, a full joint distribution suffices, in\nprinciple, forcalculating theprobability ofanyproposition.\n13.2.3 Probabilityaxiomsand theirreasonableness\nThe basic axioms of probability (Equations (13.1) and (13.2)) imply certain relationships\namongthedegreesofbeliefthatcanbeaccordedtologically relatedpropositions. Forexam-\nple, wecan derive the familiar relationship between the probability of a proposition and the\nprobability ofitsnegation:\n(cid:2)\nP(\u00aca) = P(\u03c9) byEquation(13.2)\n(cid:2)\u03c9\u2208\u00aca (cid:2) (cid:2)\n= P(\u03c9)+ P(\u03c9)\u2212 P(\u03c9)\n(cid:2)\u03c9\u2208\u00aca (cid:2) \u03c9\u2208a \u03c9\u2208a\n= P(\u03c9)\u2212 P(\u03c9) grouping thefirsttwoterms\n\u03c9\u2208\u03a9 \u03c9\u2208a\n= 1\u2212P(a) by(13.1)and(13.2). Section13.2. BasicProbability Notation 489\nWe can also derive the well-known formula for the probability of a disjunction, sometimes\nINCLUSION\u2013\ncalledtheinclusion\u2013exclusion principle:\nEXCLUSION\nPRINCIPLE\nP(a\u2228b)= P(a)+P(b)\u2212P(a\u2227b). (13.4)\nThisruleiseasilyrememberedbynotingthatthecaseswhereaholds,togetherwiththecases\nwhere b holds, certainly cover all the cases where a\u2228b holds; but summing the two sets of\ncasescountstheirintersection twice,soweneedtosubtract P(a\u2227b). Theproofisleftasan\nexercise(Exercise13.6).\nKOLMOGOROV\u2019S Equations(13.1)and(13.4)areoftencalledKolmogorov\u2019saxiomsinhonoroftheRus-\nAXIOMS\nsianmathematician AndreiKolmogorov, whoshowedhowtobuilduptherestofprobability\ntheory from this simple foundation and how to handle the difficulties caused by continuous\nvariables.2 While Equation (13.2) has a definitional flavor, Equation (13.4) reveals that the\naxioms really do constrain the degrees of belief an agent can have concerning logically re-\nlated propositions. This is analogous to the fact that a logical agent cannot simultaneously\nbelieve A, B, and \u00ac(A\u2227B), because there is no possible world in which all three are true.\nWithprobabilities, however,statementsrefernottotheworlddirectly, buttotheagent\u2019s own\nstateofknowledge. Why,then,cananagentnotholdthefollowingsetofbeliefs(eventhough\ntheyviolateKolmogorov\u2019s axioms)?\nP(a) = 0.4 P(a\u2227b) = 0.0\n(13.5)\nP(b) = 0.3 P(a\u2228b) = 0.8.\nThis kind of question has been the subject of decades of intense debate between those who\nadvocate the use of probabilities as the only legitimate form for degrees of belief and those\nwhoadvocate alternative approaches.\nOne argument for the axioms of probability, first stated in 1931 by Bruno de Finetti\n(andtranslatedintoEnglishindeFinetti(1993)),isasfollows: Ifanagenthassomedegreeof\nbelief inaproposition a,then theagentshould beable tostate oddsatwhichitisindifferent\nto a bet for or against a.3 Think of it as a game between two agents: Agent 1 states, \u201cmy\ndegree of belief in event a is 0.4.\u201d Agent 2 is then free to choose whether to wager for or\nagainst aatstakes thatareconsistent withthestated degreeofbelief. Thatis, Agent2could\nchoose toaccept Agent 1\u2019sbet that awilloccur, offering $6against Agent 1\u2019s$4. OrAgent\n2 could accept Agent 1\u2019s bet that \u00aca will occur, offering $4 against Agent 1\u2019s $6. Then we\nobserve the outcome of a, and whoever is right collects the money. If an agent\u2019s degrees of\nbelief do not accurately reflect the world, then you would expect that it would tend to lose\nmoneyoverthelongruntoanopposing agentwhosebeliefsmoreaccurately reflectthestate\noftheworld.\nBut de Finetti proved something much stronger: If Agent 1 expresses a set of degrees\nof belief that violate the axioms of probability theory then there is a combination of bets by\nAgent 2that guarantees that Agent 1 will lose money every time. Forexample, suppose that\nAgent1hasthesetofdegreesofbelieffromEquation(13.5). Figure13.2showsthatifAgent\n2 ThedifficultiesincludetheVitaliset,awell-definedsubsetoftheinterval[0,1]withnowell-definedsize.\n3 Onemightarguethattheagent\u2019spreferencesfordifferentbankbalancesaresuchthatthepossibilityoflosing\n$1isnotcounterbalancedbyanequalpossibilityofwinning$1.Onepossibleresponseistomakethebetamounts\nsmallenoughtoavoidthisproblem.Savage\u2019sanalysis(1954)circumventstheissuealtogether. 490 Chapter 13. Quantifying Uncertainty\n2 chooses to bet $4 on a, $3 on b, and $2 on \u00ac(a\u2228 b), then Agent 1 always loses money,\nregardless of the outcomes for a and b. De Finetti\u2019s theorem implies that no rational agent\ncanhavebeliefsthatviolatetheaxiomsofprobability.\nAgent1 Agent2 Outcomesandpayoffs toAgent1\nProposition Belief Bet Stakes a,b a,\u00acb \u00aca,b \u00aca,\u00acb\na 0.4 a 4to6 \u20136 \u20136 4 4\nb 0.3 b 3to7 \u20137 3 \u20137 3\na\u2228b 0.8 \u00ac(a\u2228b) 2to8 2 2 2 \u20138\n\u201311 \u20131 \u20131 \u20131\nFigure13.2 Because Agent1 hasinconsistentbeliefs, Agent2is able to devisea set of\nbetsthatguaranteesalossforAgent1,nomatterwhattheoutcomeofaandb.\nOne common objection to de Finetti\u2019s theorem is that this betting game is rather con-\ntrived. Forexample, what ifone refuses to bet? Does that end the argument? Theansweris\nthat the betting game is an abstract model for the decision-making situation in which every\nagent is unavoidably involved at every moment. Every action (including inaction) is a kind\nofbet, andeveryoutcome canbeseenasapayoff ofthebet. Refusing tobetislikerefusing\ntoallowtimetopass.\nOtherstrongphilosophicalargumentshavebeenputforwardfortheuseofprobabilities,\nmostnotably those ofCox(1946), Carnap (1950), andJaynes (2003). Theyeach construct a\nset of axioms for reasoning with degrees of beliefs: no contradictions, correspondence with\nordinary logic (forexample, ifbelief in Agoes up, thenbelief in\u00acAmustgodown), andso\non. The only controversial axiom is that degrees of belief must be numbers, or at least act\nlikenumbersinthattheymustbetransitive(ifbeliefinAisgreaterthanbeliefinB,whichis\ngreaterthan beliefin C,then belief inAmustbegreater than C)andcomparable (thebelief\ninAmustbeoneofequalto,greaterthan,orlessthanbeliefinB). Itcanthenbeprovedthat\nprobability istheonlyapproach thatsatisfiestheseaxioms.\nThe world being the way it is, however, practical demonstrations sometimes speak\nlouder than proofs. The success of reasoning systems based on probability theory has been\nmuchmoreeffectiveinmakingconverts. Wenowlookathowtheaxiomscanbedeployedto\nmakeinferences.\n13.3 INFERENCE USING FULL JOINT DISTRIBUTIONS\nPROBABILISTIC Inthissectionwedescribeasimplemethodforprobabilisticinference\u2014that is,thecompu-\nINFERENCE\ntation of posterior probabilities for query propositions given observed evidence. We use the\nfulljointdistributionasthe\u201cknowledgebase\u201dfromwhichanswerstoallquestionsmaybede-\nrived. Alongthewaywealsointroduce severalusefultechniques formanipulating equations\ninvolving probabilities. Section13.3. Inference UsingFullJointDistributions 491\nWHERE DO PROBABILITIES COME FROM?\nThere has been endless debate over the source and status of probability numbers.\nThe frequentist position is that the numbers can come only from experiments: if\nwe test 100 people and find that 10 of them have a cavity, then we can say that\nthe probability of a cavity is approximately 0.1. In this view, the assertion \u201cthe\nprobability ofacavityis0.1\u201dmeansthat0.1isthefraction thatwouldbeobserved\nin the limit of infinitely many samples. From any finite sample, we can estimate\nthetruefractionandalsocalculate howaccurate ourestimateislikelytobe.\nThe objectivist view is that probabilities are real aspects of the universe\u2014\npropensities of objects to behave in certain ways\u2014rather than being just descrip-\ntionsofanobserver\u2019s degreeofbelief. Forexample,thefactthatafaircoincomes\nup heads with probability 0.5 is a propensity of the coin itself. In this view, fre-\nquentist measurements areattempts toobserve these propensities. Mostphysicists\nagreethatquantum phenomena areobjectively probabilistic, butuncertainty atthe\nmacroscopic scale\u2014e.g., in coin tossing\u2014usually arises from ignorance of initial\nconditions anddoesnotseemconsistent withthepropensity view.\nThe subjectivist view describes probabilities as a way of characterizing an\nagent\u2019s beliefs, rather than as having any external physical significance. The sub-\njectiveBayesianviewallowsanyself-consistent ascriptionofpriorprobabilitiesto\npropositions, buttheninsistsonproperBayesianupdating asevidence arrives.\nIn the end, even a strict frequentist position involves subjective analysis be-\ncauseofthereferenceclassproblem: intryingtodeterminetheoutcomeprobabil-\nityofaparticular experiment, thefrequentist hastoplaceitinareference classof\n\u201csimilar\u201d experiments with known outcome frequencies. I. J. Good (1983, p. 27)\nwrote, \u201cevery event in life is unique, and every real-life probability that we esti-\nmate in practice is that of an event that has never occurred before.\u201d For example,\ngiven a particular patient, a frequentist who wants to estimate the probability of a\ncavitywillconsiderareferenceclassofotherpatientswho aresimilarinimportant\nways\u2014age, symptoms, diet\u2014and seewhatproportion ofthem hadacavity. Ifthe\ndentistconsiderseverythingthatisknownaboutthepatient\u2014weight tothenearest\ngram,haircolor,mother\u2019smaidenname\u2014thenthereferenceclassbecomesempty.\nThishasbeenavexingproblem inthephilosophy ofscience.\nThe principle of indifference attributed to Laplace (1816) states that propo-\nsitions that are syntactically \u201csymmetric\u201d with respect to the evidence should be\naccorded equal probability. Various refinements have been proposed, culminating\nin the attempt by Carnap and others to develop a rigorous inductive logic, capa-\nbleofcomputingthecorrectprobabilityforanyproposition fromanycollectionof\nobservations. Currently, itisbelieved thatno unique inductive logic exists; rather,\nany such logic rests on a subjective prior probability distribution whose effect is\ndiminished asmoreobservations arecollected. 492 Chapter 13. Quantifying Uncertainty\ntoothache \u00actoothache\ncatch \u00accatch catch \u00accatch\ncavity 0.108 0.012 0.072 0.008\n\u00accavity 0.016 0.064 0.144 0.576\nFigure13.3 AfulljointdistributionfortheToothache,Cavity,Catch world.\nWebeginwithasimpleexample: adomainconsistingofjustthethreeBooleanvariables\nToothache,Cavity,andCatch (thedentist\u2019snastysteelprobecatchesinmytooth). Thefull\njointdistribution isa2\u00d72\u00d72tableasshowninFigure13.3.\nNoticethattheprobabilities inthejointdistributionsumto1,asrequiredbytheaxioms\nofprobability. NoticealsothatEquation(13.2)givesusadirectwaytocalculatetheprobabil-\nityofanyproposition, simpleorcomplex: simplyidentifythosepossibleworldsinwhichthe\nproposition istrue and add uptheir probabilities. Forexample, there are six possible worlds\ninwhichcavity \u2228toothache holds:\nP(cavity \u2228toothache)= 0.108+0.012+0.072+0.008+0.016+0.064 = 0.28.\nOne particularly common task is to extract the distribution over some subset of variables or\na single variable. Forexample, adding the entries in the first row gives the unconditional or\nMARGINAL marginalprobability4 ofcavity:\nPROBABILITY\nP(cavity) = 0.108+0.012+0.072+0.008 = 0.2.\nThisprocess iscalled marginalization, orsummingout\u2014because wesumuptheprobabil-\nMARGINALIZATION\nities for each possible value of the other variables, thereby taking them out of the equation.\nWecanwritethefollowinggeneralmarginalization rulefor anysetsofvariables YandZ:\n(cid:12)\nP(Y)= P(Y,z), (13.6)\n(cid:2)\nz\u2208Z\nwhere z\u2208Zmeanstosumoverallt(cid:2)hepossiblecombinationsofvaluesofthesetofvariables\nZ. Wesometimesabbreviate thisas ,leaving Zimplicit. Wejustusedtheruleas\n(cid:12) z\nP(Cavity)= P(Cavity,z). (13.7)\nz\u2208{Catch,Toothache}\nA variant of this rule involves conditional probabilities instead of joint probabilities, using\ntheproductrule:\n(cid:12)\nP(Y)= P(Y|z)P(z). (13.8)\nz\nThisruleiscalledconditioning. Marginalization andconditioning turnouttobeusefulrules\nCONDITIONING\nforallkindsofderivations involving probability expressions.\nIn most cases, we are interested in computing conditional probabilities of some vari-\nables, given evidence about others. Conditional probabilities can be found by first using\n4 Socalledbecauseofacommonpracticeamongactuariesofwritingthesumsofobservedfrequenciesinthe\nmarginsofinsurancetables. Section13.3. Inference UsingFullJointDistributions 493\nEquation(13.3)toobtainanexpressionintermsofunconditional probabilities andtheneval-\nuating the expression from the full joint distribution. For example, we can compute the\nprobability ofacavity, givenevidence ofatoothache, asfollows:\nP(cavity \u2227toothache)\nP(cavity|toothache) =\nP(toothache)\n0.108+0.012\n= = 0.6.\n0.108+0.012+0.016+0.064\nJusttocheck,wecanalsocomputetheprobability thatthere isnocavity, givenatoothache:\nP(\u00accavity \u2227toothache)\nP(\u00accavity|toothache) =\nP(toothache)\n0.016+0.064\n= =0.4.\n0.108+0.012+0.016+0.064\nThe two values sum to 1.0, as they should. Notice that in these two calculations the term\n1\/P(toothache) remains constant, no matter which value of Cavity we calculate. In fact,\nit can be viewed as a normalization constant for the distribution P(Cavity|toothache),\nNORMALIZATION\nensuring that it adds up to 1. Throughout the chapters dealing with probability, we use \u03b1 to\ndenotesuchconstants. Withthisnotation, wecanwritethetwopreceding equations inone:\nP(Cavity|toothache)= \u03b1P(Cavity,toothache)\n= \u03b1[P(Cavity,toothache,catch)+P(Cavity,toothache,\u00accatch)]\n= \u03b1[(cid:16)0.108,0.016(cid:17)+(cid:16)0.012,0.064(cid:17)] = \u03b1(cid:16)0.12,0.08(cid:17) = (cid:16)0.6,0.4(cid:17).\nIn other words, we can calculate P(Cavity|toothache) even if we don\u2019t know the value of\nP(toothache)! Wetemporarilyforgetaboutthefactor1\/P(toothache)andaddupthevalues\nforcavity and\u00accavity,getting0.12and0.08. Thosearethecorrect relativeproportions, but\nthey don\u2019t sum to 1, so we normalize them by dividing each one by 0.12 + 0.08, getting\nthetrue probabilities of0.6and 0.4. Normalization turns outtobeauseful shortcut inmany\nprobabilitycalculations,bothtomakethecomputationeasierandtoallowustoproceedwhen\nsomeprobability assessment (suchasP(toothache))isnotavailable.\nFrom the example, we can extract a general inference procedure. We begin with the\ncase in whichthe query involves asingle variable, X (Cavity in theexample). LetEbe the\nlistofevidencevariables(justToothache intheexample),letebethelistofobservedvalues\nforthem, and let Ybe theremaining unobserved variables (just Catch intheexample). The\nqueryisP(X|e)andcanbeevaluatedas\n(cid:12)\nP(X|e)= \u03b1P(X,e)= \u03b1 P(X,e,y), (13.9)\ny\nwhere the summation is over all possible ys (i.e., all possible combinations of values of the\nunobserved variables Y). Notice that together the variables X, E, andYconstitute the com-\npletesetofvariablesforthedomain,soP(X,e,y)issimplyasubsetofprobabilities fromthe\nfulljointdistribution.\nGiventhe fulljoint distribution toworkwith, Equation (13.9) can answerprobabilistic\nqueries for discrete variables. It does not scale well, however: for a domain described by n\nBooleanvariables,itrequiresaninputtableofsizeO(2n)andtakesO(2n)timetoprocessthe 494 Chapter 13. Quantifying Uncertainty\ntable. Ina realistic problem wecould easily have n > 100, making O(2n) impractical. The\nfulljointdistributionintabularformisjustnotapracticaltoolforbuildingreasoningsystems.\nInstead,itshouldbeviewedasthetheoreticalfoundationonwhichmoreeffectiveapproaches\nmaybebuilt,justastruthtablesformedatheoreticalfoundationformorepracticalalgorithms\nlike DPLL. The remainder of this chapter introduces some of the basic ideas required in\npreparation forthedevelopment ofrealistic systemsinChapter14.\n13.4 INDEPENDENCE\nLetusexpandthefulljointdistribution inFigure13.3byaddingafourthvariable, Weather.\nThefulljointdistribution thenbecomes P(Toothache,Catch,Cavity,Weather),whichhas\n2\u00d72\u00d72\u00d74 = 32 entries. It contains four \u201ceditions\u201d of the table shown in Figure 13.3,\none for each kind of weather. What relationship do these editions have to each other and to\ntheoriginalthree-variable table? Forexample,howare P(toothache,catch,cavity,cloudy)\nandP(toothache,catch,cavity)related? Wecanusetheproductrule:\nP(toothache,catch,cavity,cloudy)\n= P(cloudy|toothache,catch,cavity)P(toothache,catch,cavity).\nNow, unless one is in the deity business, one should not imagine that one\u2019s dental problems\ninfluence theweather. Andforindoordentistry, atleast, it seemssafetosaythattheweather\ndoesnotinfluencethedentalvariables. Therefore, thefollowingassertion seemsreasonable:\nP(cloudy|toothache,catch,cavity) = P(cloudy). (13.10)\nFromthis,wecandeduce\nP(toothache,catch,cavity,cloudy) = P(cloudy)P(toothache,catch,cavity).\nAsimilarequationexistsforeveryentryinP(Toothache,Catch,Cavity,Weather). Infact,\nwecanwritethegeneralequation\nP(Toothache,Catch,Cavity,Weather)=P(Toothache,Catch,Cavity)P(Weather).\nThus, the 32-element table for four variables can be constructed from one 8-element table\nandone4-elementtable. Thisdecomposition isillustrated schematically inFigure13.4(a).\nThe property we used in Equation (13.10) is called independence (also marginal in-\nINDEPENDENCE\ndependenceandabsoluteindependence). Inparticular, theweatherisindependent ofone\u2019s\ndentalproblems. Independence betweenpropositions aandbcanbewrittenas\nP(a|b)=P(a) or P(b|a)=P(b) or P(a\u2227b)=P(a)P(b). (13.11)\nAll these forms are equivalent (Exercise 13.12). Independence between variables X and Y\ncanbewrittenasfollows(again, theseareallequivalent):\nP(X|Y)=P(X) or P(Y |X)=P(Y) or P(X,Y)=P(X)P(Y).\nIndependence assertions are usually based on knowledge of the domain. As the toothache\u2013\nweather example illustrates, they can dramatically reduce the amount of information nec-\nessary to specify the full joint distribution. If the complete set of variables can be divided Section13.5. Bayes\u2019RuleandItsUse 495\nCavity Coin Coin\n1 n\nToothache Catch\nWeather\ndecomposes\ndecomposes\ninto\ninto\nCavity\nToothache Catch Weather\nCoin Coin\n1 n\n(a) (b)\nFigure13.4 Twoexamplesoffactoringalargejointdistributionintosmallerdistributions,\nusing absolute independence. (a) Weather and dental problems are independent. (b) Coin\nflipsareindependent.\ninto independent subsets, then the full joint distribution can be factored into separate joint\ndistributions on those subsets. For example, the full joint distribution on the outcome of n\nindependent coin flips, P(C ,...,C ), has 2n entries, but it can be represented as the prod-\n1 n\nuct of n single-variable distributions P(C ). In a more practical vein, the independence of\ni\ndentistry andmeteorology isagood thing, because otherwise the practice ofdentistry might\nrequireintimateknowledgeofmeteorology, andviceversa.\nWhentheyareavailable, then,independence assertions can helpinreducingthesizeof\nthedomainrepresentation andthecomplexityoftheinference problem. Unfortunately, clean\nseparation of entire sets of variables by independence is quite rare. Whenever a connection,\nhowever indirect, exists between two variables, independence will fail to hold. Moreover,\nevenindependent subsetscanbequitelarge\u2014forexample,dentistrymightinvolve dozensof\ndiseases and hundreds ofsymptoms, allof which areinterrelated. Tohandle such problems,\nweneedmoresubtlemethodsthanthestraightforward conceptofindependence.\n13.5 BAYES\u2019 RULE AND ITS USE\nOnpage486,wedefinedtheproductrule. Itcanactuallybewrittenintwoforms:\nP(a\u2227b)= P(a|b)P(b) and P(a\u2227b)= P(b|a)P(a).\nEquatingthetworight-hand sidesanddividing byP(a),weget\nP(a|b)P(b)\nP(b|a) = . (13.12)\nP(a)\nThis equation is known as Bayes\u2019 rule (also Bayes\u2019 law or Bayes\u2019 theorem). This simple\nBAYES\u2019RULE\nequation underlies mostmodernAIsystemsforprobabilistic inference. 496 Chapter 13. Quantifying Uncertainty\nThemore general case ofBayes\u2019 rule formultivalued variables can bewritten in the P\nnotation asfollows:\nP(X|Y)P(Y)\nP(Y |X) = ,\nP(X)\nAsbefore,thisistobetakenasrepresentingasetofequations,eachdealingwithspecificval-\nuesofthevariables. Wewillalsohaveoccasiontouseamoregeneralversionconditionalized\nonsomebackground evidence e:\nP(X|Y,e)P(Y |e)\nP(Y |X,e)= . (13.13)\nP(X|e)\n13.5.1 ApplyingBayes\u2019rule: The simplecase\nOn the surface, Bayes\u2019 rule does not seem very useful. It allows us to compute the single\nterm P(b|a) in terms of three terms: P(a|b), P(b), and P(a). That seems like two steps\nbackwards, but Bayes\u2019 rule is useful in practice because there are many cases where we do\nhave good probability estimates for these three numbers and need to compute the fourth.\nOften, we perceive as evidence the effect of some unknown cause and we would like to\ndeterminethatcause. Inthatcase,Bayes\u2019rulebecomes\nP(effect|cause)P(cause)\nP(cause|effect)= .\nP(effect)\nThe conditional probability P(effect|cause) quantifies the relationship in the causal direc-\nCAUSAL\ntion, whereas P(cause|effect)describes the diagnostic direction. In atask such as medical\nDIAGNOSTIC\ndiagnosis, weoften have conditional probabilities on causal relationships (that is, the doctor\nknowsP(symptoms|disease))andwanttoderiveadiagnosis, P(disease|symptoms). For\nexample, a doctor knows that the disease meningitis causes the patient to have a stiff neck,\nsay, 70% of the time. The doctor also knows some unconditional facts: the prior probabil-\nity that a patient has meningitis is 1\/50,000, and the prior probability that any patient has a\nstiff neck is 1%. Letting s be the proposition that the patient has a stiff neck and m be the\nproposition thatthepatienthasmeningitis, wehave\nP(s|m) = 0.7\nP(m) = 1\/50000\nP(s) = 0.01\nP(s|m)P(m) 0.7\u00d71\/50000\nP(m|s) = = = 0.0014. (13.14)\nP(s) 0.01\nThatis,weexpectlessthan1in700patientswithastiffnecktohavemeningitis. Noticethat\neven though a stiff neck is quite strongly indicated by meningitis (with probability 0.7), the\nprobabilityofmeningitisinthepatientremainssmall. Thisisbecausethepriorprobabilityof\nstiffnecksismuchhigherthanthatofmeningitis.\nSection13.3illustratedaprocessbywhichonecanavoidassessingthepriorprobability\nof the evidence (here, P(s)) by instead computing a posterior probability for each value of Section13.5. Bayes\u2019RuleandItsUse 497\nthequeryvariable(here, mand\u00acm)andthennormalizingtheresults. Thesameprocesscan\nbeappliedwhenusingBayes\u2019rule. Wehave\nP(M|s)= \u03b1(cid:16)P(s|m)P(m),P(s|\u00acm)P(\u00acm)(cid:17).\nThus, to use this approach we need to estimate P(s|\u00acm) instead of P(s). There is no free\nlunch\u2014sometimesthisiseasier,sometimesitisharder. ThegeneralformofBayes\u2019rulewith\nnormalization is\nP(Y |X) = \u03b1P(X|Y)P(Y), (13.15)\nwhere\u03b1isthenormalization constant neededtomaketheentriesinP(Y |X)sumto1.\nOne obvious question to ask about Bayes\u2019 rule is why one might have available the\nconditional probability inonedirection, butnottheother. Inthemeningitis domain, perhaps\nthedoctorknowsthatastiffneckimpliesmeningitisin1outof5000cases;thatis,thedoctor\nhas quantitative information in the diagnostic direction from symptoms to causes. Such a\ndoctor has no need to use Bayes\u2019 rule. Unfortunately, diagnostic knowledge is often more\nfragilethancausalknowledge. Ifthereisasuddenepidemicofmeningitis, theunconditional\nprobability of meningitis, P(m), will go up. The doctor who derived the diagnostic proba-\nbility P(m|s)directly from statistical observation ofpatients before theepidemic willhave\nnoidea howto update the value, butthe doctorwhocomputes P(m|s)from theotherthree\nvalues will see that P(m|s) should go up proportionately with P(m). Most important, the\ncausalinformation P(s|m)isunaffected bytheepidemic,becauseitsimplyreflectstheway\nmeningitis works. The use of this kind of direct causal ormodel-based knowledge provides\nthecrucialrobustness neededtomakeprobabilistic systemsfeasibleintherealworld.\n13.5.2 UsingBayes\u2019rule: Combining evidence\nWehave seen that Bayes\u2019 rule can be useful for answering probabilistic queries conditioned\non one piece of evidence\u2014for example, the stiff neck. In particular, we have argued that\nprobabilisticinformationisoftenavailableintheformP(effect|cause). Whathappenswhen\nwe have two or more pieces of evidence? For example, what can a dentist conclude if her\nnastysteelprobecatchesintheachingtoothofapatient? Ifweknowthefulljointdistribution\n(Figure13.3),wecanreadofftheanswer:\nP(Cavity|toothache \u2227catch) = \u03b1(cid:16)0.108,0.016(cid:17) \u2248 (cid:16)0.871,0.129(cid:17).\nWe know, however, that such an approach does not scale up to larger numbers of variables.\nWecantryusingBayes\u2019ruletoreformulate theproblem:\nP(Cavity|toothache \u2227catch)\n= \u03b1P(toothache \u2227catch|Cavity)P(Cavity). (13.16)\nForthisreformulation towork,weneedtoknowtheconditional probabilities oftheconjunc-\ntiontoothache\u2227catch foreachvalueofCavity. Thatmightbefeasibleforjusttwoevidence\nvariables, but again it does not scale up. If there are n possible evidence variables (X rays,\ndiet,oralhygiene,etc.),thenthereare2n possiblecombinationsofobservedvaluesforwhich\nwe would need to know conditional probabilities. We might as well go back to using the\nfull joint distribution. Thisiswhatfirstledresearchers awayfrom probability theory toward 498 Chapter 13. Quantifying Uncertainty\napproximate methodsforevidence combination that, whilegiving incorrect answers, require\nfewernumberstogiveanyansweratall.\nRather than taking this route, we need to find some additional assertions about the\ndomain that will enable ustosimplify the expressions. The notion of independenceinSec-\ntion13.4provides aclue, butneeds refining. Itwouldbenice ifToothache andCatch were\nindependent, buttheyarenot: iftheprobe catches inthetooth, thenitislikely thatthetooth\nhas a cavity and that the cavity causes a toothache. These variables are independent, how-\never, given thepresence ortheabsence ofacavity. Eachisdirectly caused bythecavity, but\nneither has a direct effect on the other: toothache depends on the state of the nerves in the\ntooth, whereas the probe\u2019s accuracy depends on the dentist\u2019s skill, to which the toothache is\nirrelevant.5 Mathematically, thisproperty iswrittenas\nP(toothache \u2227catch|Cavity) = P(toothache |Cavity)P(catch|Cavity). (13.17)\nCONDITIONAL Thisequationexpressestheconditionalindependenceoftoothache andcatchgivenCavity.\nINDEPENDENCE\nWecanplugitintoEquation(13.16)toobtaintheprobability ofacavity:\nP(Cavity|toothache \u2227catch)\n= \u03b1P(toothache |Cavity)P(catch|Cavity)P(Cavity). (13.18)\nNow the information requirements are the same as for inference, using each piece of evi-\ndence separately: the prior probability P(Cavity) for the query variable and the conditional\nprobability ofeacheffect, givenitscause.\nThegeneraldefinitionofconditionalindependenceoftwovariablesX andY,givena\nthirdvariable Z,is\nP(X,Y |Z)= P(X|Z)P(Y |Z).\nInthedentistdomain,forexample,itseemsreasonabletoassertconditional independence of\nthevariables Toothache andCatch,givenCavity:\nP(Toothache,Catch|Cavity) = P(Toothache |Cavity)P(Catch|Cavity). (13.19)\nNoticethatthisassertionissomewhatstrongerthanEquation(13.17),whichassertsindepen-\ndence only for specific values of Toothache and Catch. As with absolute independence in\nEquation(13.11), theequivalent forms\nP(X|Y,Z)=P(X|Z) and P(Y |X,Z)=P(Y |Z)\ncan also be used (see Exercise 13.17). Section 13.4 showed that absolute independence as-\nsertionsallowadecomposition ofthefulljointdistribution intomuchsmallerpieces. Itturns\nout that the same is true for conditional independence assertions. For example, given the\nassertion inEquation(13.19), wecanderiveadecomposition asfollows:\nP(Toothache,Catch,Cavity)\n= P(Toothache,Catch|Cavity)P(Cavity) (productrule)\n= P(Toothache |Cavity)P(Catch|Cavity)P(Cavity) (using13.19).\n(Thereadercaneasilycheck thatthisequation doesinfacthold inFigure13.3.) Inthisway,\ntheoriginal large table isdecomposed intothree smallertables. Theoriginal table has seven\n5 Weassumethatthepatientanddentistaredistinctindividuals. Section13.6. TheWumpusWorldRevisited 499\nindependent numbers (23=8 entries in the table, but they must sum to 1, so 7 are indepen-\ndent). The smaller tables contain five independent numbers (for a conditional probability\ndistributions such as P(T|C there are tworowsof twonumbers, and each row sums to1, so\nthat\u2019stwoindependent numbers; forapriordistribution likeP(C)thereisonlyoneindepen-\ndent number). Going from seven to five might not seem like a major triumph, but the point\nis that, for n symptoms that are all conditionally independent given Cavity, the size of the\nrepresentation grows as O(n) instead of O(2n). That means that conditional independence\nassertions can allow probabilistic systems to scale up; moreover, they are much more com-\nmonly available than absolute independence assertions. Conceptually, Cavity separates\nSEPARATION\nToothache and Catch because it is a direct cause of both of them. The decomposition of\nlargeprobabilistic domainsintoweaklyconnectedsubsets throughconditional independence\nisoneofthemostimportantdevelopments intherecenthistoryofAI.\nThedentistryexampleillustratesacommonlyoccurringpatterninwhichasinglecause\ndirectly influences anumberofeffects, allofwhichareconditionally independent, giventhe\ncause. Thefulljointdistribution canbewrittenas\n(cid:25)\nP(Cause,Effect ,...,Effect ) = P(Cause) P(Effect |Cause).\n1 n i\ni\nSuch a probability distribution is called a naive Bayes model\u2014\u201cnaive\u201d because it is often\nNAIVEBAYES\nused (as a simplifying assumption) in cases where the \u201ceffect\u201d variables are not actually\nconditionally independent given the cause variable. (The naive Bayes model is sometimes\ncalled a Bayesian classifier, a somewhat careless usage that has prompted true Bayesians\nto call it the idiot Bayes model.) In practice, naive Bayes systems can work surprisingly\nwell, even when the conditional independence assumption is not true. Chapter 20 describes\nmethodsforlearningnaiveBayesdistributions fromobservations.\n13.6 THE WUMPUS WORLD REVISITED\nWecan combine of the ideas in this chapter to solve probabilistic reasoning problems in the\nwumpusworld. (SeeChapter7foracompletedescriptionofthewumpusworld.) Uncertainty\narises in the wumpus world because the agent\u2019s sensors give only partial information about\nthe world. For example, Figure 13.5 shows a situation in which each of the three reachable\nsquares\u2014[1,3], [2,2], and [3,1]\u2014might contain a pit. Pure logical inference can conclude\nnothing aboutwhichsquare ismostlikelytobesafe,soalogicalagentmighthavetochoose\nrandomly. Wewillseethataprobabilistic agentcandomuchbetterthanthelogicalagent.\nOuraimistocalculatetheprobabilitythateachofthethree squarescontainsapit. (For\nthis example we ignore the wumpus and the gold.) The relevant properties of the wumpus\nworld are that (1) a pit causes breezes in all neighboring squares, and (2) each square other\nthan [1,1] contains a pit with probability 0.2. The first step is to identify the set of random\nvariables weneed:\n\u2022 As in the propositional logic case, we want one Boolean variable P for each square,\nij\nwhichistrueiffsquare[i,j]actuallycontains apit. 500 Chapter 13. Quantifying Uncertainty\n1,4 2,4 3,4 4,4 1,4 2,4 3,4 4,4\n1,3 2,3 3,3 4,3 1,3 2,3 3,3 4,3\nOTHER\nQUERY\n1,2 2,2 3,2 4,2 1,2 2,2 3,2 4,2\nB\nOK\nFRONTIER\n1,1 2,1 3,1 4,1 1,1 2,1 3,1 4,1\nKNOWN\nB\nOK OK\n(a) (b)\nFigure13.5 (a)Afterfindingabreezeinboth[1,2]and[2,1],theagentisstuck\u2014thereis\nnosafeplacetoexplore. (b)DivisionofthesquaresintoKnown,Frontier,andOther,for\naqueryabout[1,3].\n\u2022 Wealso have Boolean variables B that are true iffsquare [i,j] is breezy; weinclude\nij\nthesevariables onlyfortheobservedsquares\u2014in thiscase, [1,1],[1,2],and[2,1].\nThenextstep istospecify thefulljoint distribution, P(P ,...,P ,B ,B ,B ). Ap-\n1,1 4,4 1,1 1,2 2,1\nplyingtheproductrule,wehave\nP(P ,...,P ,B ,B ,B )=\n1,1 4,4 1,1 1,2 2,1\nP(B ,B ,B | P ,...,P )P(P ,...,P ).\n1,1 1,2 2,1 1,1 4,4 1,1 4,4\nThis decomposition makes it easy to see what the joint probability values should be. The\nfirst term is the conditional probability distribution of a breeze configuration, given a pit\nconfiguration; its values are 1 if the breezes are adjacent to the pits and 0 otherwise. The\nsecond term is the prior probability of a pit configuration. Each square contains a pit with\nprobability 0.2,independently oftheothersquares; hence,\n(cid:25)4,4\nP(P ,...,P )= P(P ). (13.20)\n1,1 4,4 i,j\ni,j=1,1\nForaparticularconfiguration withexactly npits,P(P ,...,P\n)=0.2n\u00d70.816\u2212n.\n1,1 4,4\nIn the situation in Figure 13.5(a), the evidence consists of the observed breeze (or its\nabsence)ineachsquarethatisvisited, combinedwiththefactthateachsuchsquarecontains\nnopit. Weabbreviatethesefactsasb=\u00acb \u2227b \u2227b andknown=\u00acp \u2227\u00acp \u2227\u00acp .\n1,1 1,2 2,1 1,1 1,2 2,1\nWeareinterested inanswering queries such as P(P |known,b): how likely isit that [1,3]\n1,3\ncontains apit,giventheobservations sofar?\nToanswerthisquery, wecanfollow thestandard approach ofEquation (13.9),namely,\nsumming over entries from the full joint distribution. Let Unknown be the set of P vari-\ni,j Section13.6. TheWumpusWorldRevisited 501\nables forsquares otherthan the Known squares and the query square [1,3]. Then, by Equa-\ntion(13.9),wehave\n(cid:12)\nP(P |known,b) = \u03b1 P(P ,unknown,known,b).\n1,3 1,3\nunknown\nThe full joint probabilities have already been specified, so we are done\u2014that is, unless we\ncare about computation. There are 12 unknown squares; hence the summation contains\n212=4096terms. Ingeneral,thesummationgrowsexponentiallywiththenumberofsquares.\nSurely, one might ask, aren\u2019t the other squares irrelevant? How could [4,4] affect\nwhether [1,3] has a pit? Indeed, this intuition is correct. Let Frontier be the pit variables\n(other than the query variable) that are adjacent to visited squares, in this case just [2,2] and\n[3,1]. Also,letOther bethepitvariablesfortheotherunknownsquares;inthiscase,thereare\n10othersquares,asshowninFigure13.5(b). Thekeyinsight isthattheobservedbreezesare\nconditionally independent of the other variables, given the known, frontier, and query vari-\nables. Touse the insight, wemanipulate the query formula into aform inwhich the breezes\nareconditioned onalltheothervariables, andthenweapply conditional independence:\nP(P |known,b)\n1,3 (cid:12)\n= \u03b1 P(P ,known,b,unknown) (byEquation(13.9))\n1,3\nun(cid:12)known\n= \u03b1 P(b|P ,known,unknown)P(P ,known,unknown)\n1,3 1,3\nunknown\n(bytheproductrule)\n(cid:12) (cid:12)\n= \u03b1 P(b|known,P ,frontier,other)P(P ,known,frontier,other)\n1,3 1,3\nfrontierother\n(cid:12) (cid:12)\n= \u03b1 P(b|known,P ,frontier)P(P ,known,frontier,other),\n1,3 1,3\nfrontierother\nwherethe finalstepuses conditional independence: bisindependent ofother givenknown,\nP , and frontier. Now, the first term in this expression does not depend on the Other\n1,3\nvariables, sowecanmovethesummationinward:\nP(P |known,b)\n1,3 (cid:12) (cid:12)\n= \u03b1 P(b|known,P ,frontier) P(P ,known,frontier,other).\n1,3 1,3\nfrontier other\nBy independence, as in Equation (13.20), the prior term can be factored, and then the terms\ncanbereordered:\nP(P |known,b)\n1,3 (cid:12) (cid:12)\n= \u03b1 P(b|known,P ,frontier) P(P )P(known)P(frontier)P(other)\n1,3 1,3\nfrontier other\n(cid:12) (cid:12)\n= \u03b1P(known)P(P ) P(b|known,P ,frontier)P(frontier) P(other)\n1,3 1,3\nfrontier other\n(cid:12)\n= \u03b1(cid:2) P(P ) P(b|known,P ,frontier)P(frontier),\n1,3 1,3\nfrontier 502 Chapter 13. Quantifying Uncertainty\n1,3 1,3 1,3 1,3 1,3\n1,2 2,2 1,2 2,2 1,2 2,2 1,2 2,2 1,2 2,2\nB B B B B\nOK OK OK OK OK\n1,1 2,1 3,1 1,1 2,1 3,1 1,1 2,1 3,1 1,1 2,1 3,1 1,1 2,1 3,1\nB B B B B\nOK OK OK OK OK OK OK OK OK OK\n0.2 x 0.2 = 0.04 0.2 x 0.8 = 0.16 0.8 x 0.2 = 0.16 0.2 x 0.2 = 0.04 0.2 x 0.8 = 0.16\n(a) (b)\nFigure 13.6 Consistent models for the frontier variables P 2,2 and P 3,1, showing\nP(frontier) for each model: (a) three models with P 1,3=true showing two or three pits,\nand(b)twomodelswithP 1,3=false showingoneortwopits.\nwhere the last step folds P(known) into the normalizing constant and uses the fact that\n(cid:2)\nP(other)equals1.\nother\nNow, there are just four terms in the summation over the frontier variables P and\n2,2\nP . The use of independence and conditional independence has completely eliminated the\n3,1\nothersquaresfromconsideration.\nNoticethattheexpression P(b|known,P ,frontier)is1whenthefrontierisconsis-\n1,3\ntentwiththebreezeobservations, and0otherwise. Thus,foreachvalueofP ,wesumover\n1,3\nthe logical models for the frontier variables that are consistent with the known facts. (Com-\npare with the enumeration over models in Figure 7.5 on page 241.) The models and their\nassociated priorprobabilities\u2014P(frontier)\u2014areshowninFigure13.6. Wehave\nP(P |known,b) = \u03b1(cid:2)(cid:16)0.2(0.04+0.16+0.16), 0.8(0.04+0.16)(cid:17)\u2248 (cid:16)0.31,0.69(cid:17).\n1,3\nThatis,[1,3](and[3,1]bysymmetry)containsapitwithroughly31%probability. Asimilar\ncalculation, which the reader might wish to perform, shows that [2,2] contains a pit with\nroughly 86% probability. The wumpus agent should definitely avoid [2,2]! Note that our\nlogicalagentfromChapter7didnotknowthat[2,2]wasworse thantheothersquares. Logic\ncantellusthatitisunknownwhetherthereisapitin[2,2],butweneedprobability totellus\nhowlikelyitis.\nWhat this section has shown is that even seemingly complicated problems can be for-\nmulated precisely in probability theory and solved with simple algorithms. To get efficient\nsolutions, independence and conditional independence relationships can be used to simplify\nthe summations required. These relationships often correspond to ournatural understanding\nofhowtheproblemshouldbedecomposed. Inthenextchapter, wedevelopformalrepresen-\ntations for such relationships as well as algorithms that operate on those representations to\nperform probabilistic inference efficiently. Section13.7. Summary 503\n13.7 SUMMARY\nThischapterhassuggestedprobabilitytheoryasasuitablefoundationforuncertainreasoning\nandprovided agentleintroduction toitsuse.\n\u2022 Uncertaintyarisesbecauseofbothlazinessandignorance. Itisinescapableincomplex,\nnondeterministic, orpartiallyobservable environments.\n\u2022 Probabilitiesexpresstheagent\u2019sinabilitytoreachadefinitedecisionregardingthetruth\nofasentence. Probabilities summarizetheagent\u2019sbeliefs relativetotheevidence.\n\u2022 Decisiontheorycombinestheagent\u2019sbeliefsanddesires,definingthebestactionasthe\nonethatmaximizesexpected utility.\n\u2022 Basicprobabilitystatementsincludepriorprobabilitiesandconditionalprobabilities\noversimpleandcomplexpropositions.\n\u2022 Theaxiomsofprobability constrain thepossibleassignments ofprobabilities topropo-\nsitions. Anagentthatviolates theaxiomsmustbehaveirrationally insomecases.\n\u2022 The full joint probability distribution specifies the probability of each complete as-\nsignment of values to random variables. It is usually too large to create or use in its\nexplicitform,butwhenitisavailableitcanbeusedtoanswerqueriessimplybyadding\nupentriesforthepossible worldscorresponding tothequerypropositions.\n\u2022 Absoluteindependencebetween subsets ofrandom variables allowsthe fulljoint dis-\ntribution tobefactored intosmallerjointdistributions, greatly reducing itscomplexity.\nAbsoluteindependence seldomoccursinpractice.\n\u2022 Bayes\u2019 rule allows unknown probabilities to be computed from known conditional\nprobabilities, usuallyinthecausaldirection. ApplyingBayes\u2019rulewithmanypiecesof\nevidencerunsintothesamescalingproblemsasdoesthefull jointdistribution.\n\u2022 Conditionalindependencebrought about bydirect causal relationships inthedomain\nmight allow the full joint distribution to be factored into smaller, conditional distri-\nbutions. The naive Bayes model assumes the conditional independence of all effect\nvariables, givenasinglecausevariable, andgrowslinearly withthenumberofeffects.\n\u2022 Awumpus-worldagentcancalculateprobabilities forunobservedaspectsoftheworld,\ntherebyimprovingonthedecisionsofapurelylogicalagent. Conditionalindependence\nmakesthesecalculations tractable.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nProbability theory was invented as a way of analyzing games of chance. In about 850 A.D.\ntheIndianmathematicianMahaviracaryadescribedhowtoarrangeasetofbetsthatcan\u2019tlose\n(what we now call a Dutch book). In Europe, the first significant systematic analyses were\nproduced by Girolamo Cardano around 1565, although publication wasposthumous (1663).\nBythattime,probability hadbeenestablished asamathematical discipline duetoaseriesof 504 Chapter 13. Quantifying Uncertainty\nresults established in a famous correspondence between Blaise Pascal and Pierre de Fermat\nin1654. Aswithprobabilityitself,theresultswereinitiallymotivatedbygamblingproblems\n(see Exercise 13.9). The first published textbook on probability was De Ratiociniis in Ludo\nAleae (Huygens, 1657). The \u201claziness and ignorance\u201d view of uncertainty was described\nby John Arbuthnot in the preface of his translation of Huygens (Arbuthnot, 1692): \u201cIt is\nimpossibleforaDie,withsuchdetermin\u2019dforceanddirection,nottofallonsuchdetermin\u2019d\nside, only I don\u2019t know the force and direction which makes it fall on such determin\u2019d side,\nandtherefore IcallitChance,whichisnothing butthewantofart...\u201d\nLaplace(1816) gaveanexceptionally accurate andmodernoverview ofprobability; he\nwas the first to use the example \u201ctake two urns, A and B,the first containing fourwhite and\ntwoblackballs,...\u201d TheRev.ThomasBayes(1702\u20131761) introduced theruleforreasoning\nabout conditional probabilities that was named after him (Bayes, 1763). Bayes only con-\nsidered the case of uniform priors; it was Laplace who independently developed the general\ncase. Kolmogorov (1950, firstpublished inGermanin1933) presented probability theory in\na rigorously axiomatic framework for the first time. Re\u00b4nyi (1970) later gave an axiomatic\npresentation thattookconditional probability, ratherthanabsolute probability, asprimitive.\nPascalusedprobabilityinwaysthatrequiredboththeobjectiveinterpretation,asaprop-\nerty ofthe world based onsymmetry orrelative frequency, and the subjective interpretation,\nbasedondegreeofbelief\u2014theformerinhisanalysesofprobabilitiesingamesofchance,the\nlatter in the famous \u201cPascal\u2019s wager\u201d argument about the possible existence of God. How-\never, Pascal did not clearly realize the distinction between these two interpretations. The\ndistinction wasfirstdrawnclearlybyJamesBernoulli(1654\u20131705).\nLeibnizintroduced the\u201cclassical\u201d notion ofprobability asaproportion ofenumerated,\nequally probable cases,whichwasalsousedbyBernoulli, although itwasbrought topromi-\nnence byLaplace (1749\u20131827). Thisnotion isambiguous between thefrequency interpreta-\ntionandthesubjective interpretation. Thecasescanbethought tobeequally probable either\nbecause of a natural, physical symmetry between them, or simply because we do not have\nany knowledge that would lead us to consider one more probable than another. The use of\nthis latter, subjective consideration to justify assigning equal probabilities is known as the\nPRINCIPLEOF principle ofindifference. Theprinciple is often attributed to Laplace, but he neverisolated\nINDIFFERENCE\nthe principle explicitly. George Boole and John Venn both referred to it as the principle of\nPRINCIPLEOF\ninsufficientreason;themodernnameisduetoKeynes(1921).\nINSUFFICIENT\nREASON\nThe debate between objectivists and subjectivists became sharper in the 20th century.\nKolmogorov (1963), R. A. Fisher (1922), and Richard von Mises (1928) were advocates of\ntherelativefrequencyinterpretation. KarlPopper\u2019s(1959,firstpublishedinGermanin1934)\n\u201cpropensity\u201d interpretation traces relative frequencies to an underlying physical symmetry.\nFrank Ramsey (1931), Bruno de Finetti (1937), R. T. Cox (1946), Leonard Savage (1954),\nRichard Jeffrey (1983), and E. T. Jaynes (2003) interpreted probabilities as the degrees of\nbelief of specific individuals. Their analyses of degree of belief were closely tied to utili-\ntiesandtobehavior\u2014specifically, tothewillingness toplacebets. RudolfCarnap, following\nLeibniz and Laplace, offered a different kind of subjective interpretation of probability\u2014\nnot as any actual individual\u2019s degree of belief, but as the degree of belief that an idealized\nindividual should have in a particular proposition a, given a particular body of evidence e. Bibliographical andHistorical Notes 505\nCarnap attempted to go further than Leibniz or Laplace by making this notion of degree of\nconfirmationmathematicallyprecise,asalogicalrelationbetweenaande. Thestudyofthis\nCONFIRMATION\nrelation was intended to constitute a mathematical discipline called inductive logic, analo-\nINDUCTIVELOGIC\ngous to ordinary deductive logic (Carnap, 1948, 1950). Carnap was not able to extend his\ninductivelogicmuchbeyondthepropositional case,andPutnam(1963)showedbyadversar-\nialargumentsthatsomefundamentaldifficultieswouldpreventastrictextensiontolanguages\ncapableofexpressing arithmetic.\nCox\u2019stheorem (1946) showsthatanysystem foruncertain reasoning thatmeetshisset\nof assumptions is equivalent to probability theory. This gave renewed confidence to those\nwhoalready favored probability, but others werenot convinced, pointing to theassumptions\n(primarily thatbeliefmustberepresented byasinglenumber, andthusthebelief in\u00acpmust\nbe a function of the belief in p). Halpern (1999) describes the assumptions and shows some\ngaps in Cox\u2019s original formulation. Horn (2003) shows how to patch up the difficulties.\nJaynes(2003)hasasimilarargument thatiseasiertoread.\nThequestionofreferenceclassesiscloselytiedtotheattempttofindaninductivelogic.\nTheapproach ofchoosing the \u201cmostspecific\u201d reference class ofsufficient size wasformally\nproposed by Reichenbach (1949). Various attempts have been made, notably by Henry Ky-\nburg (1977, 1983), to formulate more sophisticated policies in order to avoid some obvious\nfallacies that arise with Reichenbach\u2019s rule, but such approaches remain somewhat ad hoc.\nMorerecentworkbyBacchus,Grove,Halpern,andKoller(1992)extendsCarnap\u2019smethods\nto first-order theories, thereby avoiding many of the difficulties associated with the straight-\nforward reference-class method. Kyburg and Teng (2006) contrast probabilistic inference\nwithnonmonotonic logic.\nBayesian probabilistic reasoning has been used in AI since the 1960s, especially in\nmedicaldiagnosis. Itwasusednotonlytomakeadiagnosisfromavailableevidence,butalso\nto select further questions and tests by using the theory of information value (Section 16.6)\nwhen available evidence was inconclusive (Gorry, 1968; Gorry et al., 1973). One system\noutperformed humanexpertsinthediagnosisofacuteabdominalillnesses(deDombaletal.,\n1974). Lucas etal.(2004) givesanoverview. Theseearly Bayesian systems suffered from a\nnumber of problems, however. Because they lacked any theoretical model of the conditions\nthey were diagnosing, they were vulnerable to unrepresentative data occurring in situations\nforwhichonlyasmallsamplewasavailable(deDombaletal.,1981). Evenmorefundamen-\ntally,becausetheylackedaconciseformalism(suchastheonetobedescribedinChapter14)\nfor representing and using conditional independence information, they depended on the ac-\nquisition, storage, and processing ofenormous tables ofprobabilistic data. Because ofthese\ndifficulties, probabilistic methodsforcopingwithuncertainty felloutoffavorinAIfromthe\n1970stothemid-1980s. Developmentssincethelate1980saredescribedinthenextchapter.\nThe naive Bayes model for joint distributions has been studied extensively in the pat-\nternrecognitionliteraturesincethe1950s(DudaandHart, 1973). Ithasalsobeenused,often\nunwittingly, in information retrieval, beginning with the work of Maron (1961). The proba-\nbilisticfoundations ofthistechnique, describedfurther inExercise13.22,wereelucidatedby\nRobertson and Sparck Jones (1976). Domingos and Pazzani (1997) provide an explanation 506 Chapter 13. Quantifying Uncertainty\nfor the surprising success of naive Bayesian reasoning even in domains where the indepen-\ndenceassumptions areclearlyviolated.\nThere are many good introductory textbooks on probability theory, including those by\nBertsekas and Tsitsiklis (2008) and Grinstead and Snell (1997). DeGroot and Schervish\n(2001) offer a combined introduction to probability and statistics from a Bayesian stand-\npoint. Richard Hamming\u2019s (1991) textbook gives a mathematically sophisticated introduc-\ntiontoprobabilitytheoryfromthestandpointofapropensityinterpretation basedonphysical\nsymmetry. Hacking (1975) and Hald (1990) coverthe early history of theconcept of proba-\nbility. Bernstein(1996)givesanentertaining popularaccount ofthestoryofrisk.\nEXERCISES\n13.1 Showfromfirstprinciples that P(a|b\u2227a)= 1.\n13.2 Using the axioms of probability, prove that any probability distribution on a discrete\nrandomvariable mustsumto1.\n13.3 Foreachofthefollowingstatements, eitherproveitistrue orgiveacounterexample.\na. IfP(a|b,c) = P(b|a,c),thenP(a|c) = P(b|c)\nb. IfP(a|b,c) = P(a),thenP(b|c)= P(b)\nc. IfP(a|b)= P(a),thenP(a|b,c) = P(a|c)\n13.4 WoulditberationalforanagenttoholdthethreebeliefsP(A)=0.4,P(B)=0.3,and\nP(A\u2228B)=0.5? Ifso,whatrangeofprobabilities wouldberationalforthe agenttoholdfor\nA\u2227B? MakeupatableliketheoneinFigure13.2,andshowhowitsupportsyourargument\nabout rationality. Then draw another version of the table where P(A \u2228B)=0.7. Explain\nwhyitisrational tohave thisprobability, eventhough thetable showsonecase thatisaloss\nand three that just break even. (Hint: what is Agent 1committed to about the probability of\neachofthefourcases,especially thecasethatisaloss?)\n13.5 This question deals with the properties of possible worlds, defined on page 488 as\nassignments to all random variables. We will work with propositions that correspond to\nexactly one possible world because they pin down the assignments of all the variables. In\nprobability theory, such propositions are called atomic events. For example, with Boolean\nATOMICEVENT\nvariables X ,X ,X ,theproposition x \u2227\u00acx \u2227\u00acx fixestheassignment ofthevariables;\n1 2 3 1 2 3\ninthelanguage ofpropositional logic,wewouldsayithasexactlyonemodel.\na. Prove, for the case of n Boolean variables, that any two distinct atomic events are\nmutuallyexclusive;thatis,theirconjunction isequivalent tofalse.\nb. Provethatthedisjunction ofallpossible atomiceventsislogically equivalent totrue.\nc. Provethatanypropositionislogicallyequivalenttothedisjunctionoftheatomicevents\nthatentailitstruth. Exercises 507\n13.6 ProveEquation(13.4)fromEquations(13.1)and(13.2).\n13.7 Considerthesetofallpossiblefive-cardpokerhandsdealtfairlyfromastandarddeck\noffifty-twocards.\na. Howmany atomic events are there in the joint probability distribution (i.e., how many\nfive-cardhandsarethere)?\nb. Whatistheprobability ofeachatomicevent?\nc. Whatistheprobability ofbeingdealtaroyalstraight flush? Fourofakind?\n13.8 Giventhefulljointdistribution showninFigure13.3,calculatethefollowing:\na. P(toothache).\nb. P(Cavity).\nc. P(Toothache |cavity).\nd. P(Cavity|toothache \u2228catch).\n13.9 InhisletterofAugust24,1654,Pascalwastryingtoshowhowapotofmoneyshould\nbeallocated whenagamblinggamemustendprematurely. Imagine agamewhereeachturn\nconsists of the roll of a die, player E gets a point when the die is even, and player O gets a\npoint when thedie isodd. Thefirstplayer toget 7points winsthe pot. Suppose thegameis\ninterrupted with E leading 4\u20132. How should the money be fairly split in this case? What is\nthegeneralformula? (FermatandPascalmadeseveralerrors beforesolvingtheproblem, but\nyoushouldbeabletogetitrightthefirsttime.)\n13.10 Deciding to put probability theory to good use, we encounter a slot machine with\nthree independent wheels, each producing one of the four symbols BAR, BELL, LEMON, or\nCHERRY withequal probability. Theslotmachinehasthefollowingpayout schemeforabet\nof1coin(where\u201c?\u201d denotes thatwedon\u2019tcarewhatcomesupforthatwheel):\nBAR\/BAR\/BARpays20coins\nBELL\/BELL\/BELL pays15coins\nLEMON\/LEMON\/LEMON pays5coins\nCHERRY\/CHERRY\/CHERRY pays3coins\nCHERRY\/CHERRY\/? pays2coins\nCHERRY\/?\/? pays1coin\na. Compute the expected \u201cpayback\u201d percentage of the machine. In other words, foreach\ncoinplayed, whatistheexpected coinreturn?\nb. Computetheprobability thatplaying theslotmachineonce willresultinawin.\nc. Estimate the mean and median number of plays you can expect to make until you go\nbroke, ifyou start with10coins. Youcanrun asimulation toestimate this, ratherthan\ntryingtocomputeanexactanswer.\n13.11 Wewishtotransmitann-bitmessagetoareceivingagent. Thebitsinthemessageare\nindependently corrupted (flipped)during transmission with(cid:2)probability each. Withanextra\nparitybitsentalongwiththeoriginalinformation,amessagecanbecorrectedbythereceiver 508 Chapter 13. Quantifying Uncertainty\nifatmostonebitintheentiremessage(includingtheparity bit)hasbeencorrupted. Suppose\nwewanttoensurethatthecorrectmessageisreceivedwithprobabilityatleast1\u2212\u03b4. Whatis\nthemaximumfeasiblevalueofn? Calculatethisvalueforthecase(cid:2)=0.001,\u03b4=0.01.\n13.12 Showthatthethreeformsofindependence inEquation(13.11)areequivalent.\n13.13 Consider two medical tests, A and B, for a virus. Test A is 95% effective at recog-\nnizing theviruswhenitispresent, buthasa10%falsepositiverate(indicating thatthevirus\nispresent,whenitisnot). TestBis90%effectiveatrecognizing thevirus,buthasa5%false\npositive rate. The two tests use independent methods of identifying the virus. The virus is\ncarriedby1%ofallpeople. Saythatapersonistestedforthevirususingonlyoneofthetests,\nandthattestcomesbackpositiveforcarryingthevirus. Whichtestreturningpositiveismore\nindicative ofsomeonereallycarryingthevirus? Justifyyouranswermathematically.\n13.14 Suppose you are given a coin that lands heads with probability x and tails with\nprobability 1 \u2212 x. Are the outcomes of successive flips of the coin independent of each\nother given that you know the value of x? Are the outcomes of successive flips of the coin\nindependent ofeachotherifyoudonotknowthevalueofx? Justifyyouranswer.\n13.15 After your yearly checkup, the doctor has bad news and good news. The bad news\nis that you tested positive for a serious disease and that the test is 99% accurate (i.e., the\nprobability of testing positive when you do have the disease is 0.99, as is the probability of\ntestingnegativewhenyoudon\u2019thavethedisease). Thegoodnewsisthatthisisararedisease,\nstriking only 1 in 10,000 people of your age. Why is it good news that the disease is rare?\nWhatarethechances thatyouactuallyhavethedisease?\n13.16 It is quite often useful to consider the effect of some specific propositions in the\ncontext ofsomegeneral background evidence thatremainsfixed,ratherthaninthecomplete\nabsence of information. The following questions ask you to prove more general versions of\ntheproductruleandBayes\u2019rule,withrespecttosomebackground evidencee:\na. Provetheconditionalized versionofthegeneralproduct rule:\nP(X,Y |e) = P(X|Y,e)P(Y |e).\nb. Provetheconditionalized versionofBayes\u2019ruleinEquation(13.13).\n13.17 Showthatthestatementofconditional independence\nP(X,Y |Z)= P(X|Z)P(Y |Z)\nisequivalent toeachofthestatements\nP(X|Y,Z) = P(X|Z) and P(B|X,Z) = P(Y |Z).\n13.18 Supposeyouaregivenabagcontaining nunbiased coins. Youaretoldthatn\u22121of\nthese coins are normal, with heads on one side and tails on the other, whereas one coin is a\nfake,withheadsonbothsides.\na. Supposeyoureachintothebag,pickoutacoinatrandom,flipit,andgetahead. What\nisthe(conditional) probability thatthecoinyouchoseisthefakecoin? Exercises 509\nb. Suppose you continue flipping the coin for atotal of k times after picking it and see k\nheads. Nowwhatistheconditional probability thatyoupickedthefakecoin?\nc. Supposeyouwantedtodecidewhetherthechosencoinwasfakebyflippingitk times.\nThe decision procedure returns fake if all k flips come up heads; otherwise it returns\nnormal. Whatisthe(unconditional) probability thatthisprocedure makesanerror?\n13.19 In this exercise, you will complete the normalization calculation for the meningitis\nexample. First,makeupasuitable valuefor P(s|\u00acm),anduseittocalculate unnormalized\nvaluesforP(m|s)andP(\u00acm|s)(i.e.,ignoringtheP(s)termintheBayes\u2019ruleexpression,\nEquation(13.14)). Nownormalize thesevaluessothattheyaddto1.\n13.20 Let X, Y, Z be Boolean random variables. Label the eight entries in the joint dis-\ntribution P(X,Y,Z) as a through h. Express the statement that X and Y are conditionally\nindependent given Z, as a set of equations relating a through h. How many nonredundant\nequations arethere?\n13.21 (Adapted from Pearl (1988).) Suppose you are a witness to a nighttime hit-and-run\naccident involving a taxi in Athens. Alltaxis in Athens are blue orgreen. You swear, under\noath, that the taxi wasblue. Extensive testing showsthat, under thedim lighting conditions,\ndiscrimination betweenblueandgreenis75%reliable.\na. Isitpossible tocalculate themostlikelycolorforthetaxi? (Hint: distinguish carefully\nbetweentheproposition thatthetaxiisblueandtheproposition thatitappearsblue.)\nb. Whatifyouknowthat9outof10Atheniantaxisaregreen?\n13.22 Textcategorization isthe taskofassigning agivendocument toone ofafixedsetof\ncategories on the basis of the text it contains. Naive Bayes models are often used for this\ntask. Inthesemodels,thequeryvariable isthedocument category, andthe\u201ceffect\u201dvariables\narethepresenceorabsenceofeachwordinthelanguage; theassumption isthatwordsoccur\nindependently indocuments, withfrequencies determinedbythedocumentcategory.\na. Explain precisely how such a model can be constructed, given as \u201ctraining data\u201d a set\nofdocuments thathavebeenassigned tocategories.\nb. Explainprecisely howtocategorize anewdocument.\nc. Istheconditional independence assumption reasonable? Discuss.\n13.23 In our analysis of the wumpus world, we used the fact that each square contains a\npit withprobability 0.2, independently ofthe contents of the othersquares. Suppose instead\nthat exactly N\/5 pits are scattered at random among the N squares other than [1,1]. Are\nthe variables P and P still independent? What isthe joint distribution P(P ,...,P )\ni,j k,l 1,1 4,4\nnow? Redothecalculation fortheprobabilities ofpitsin[1,3]and[2,2].\n13.24 Redotheprobabilitycalculationforpitsin[1,3]and[2,2],assumingthateachsquare\ncontains a pit with probability 0.01, independent of the other squares. What can you say\nabouttherelativeperformance ofalogicalversusaprobabilistic agentinthiscase?\n13.25 Implement a hybrid probabilistic agent for the wumpus world, based on the hybrid\nagentinFigure7.20andtheprobabilistic inference procedure outlined inthischapter. 14\nPROBABILISTIC\nREASONING\nIn which we explain how to build network models to reason under uncertainty\naccording tothelawsofprobability theory.\nChapter 13 introduced the basic elements of probability theory and noted the importance of\nindependence and conditional independence relationships in simplifying probabilistic repre-\nsentations ofthe world. Thischapter introduces asystematic waytorepresent such relation-\nships explicitly in the form of Bayesian networks. We define the syntax and semantics of\nthese networks and show how they can be used to capture uncertain knowledge in a natu-\nral and efficient way. We then show how probabilistic inference, although computationally\nintractable in the worst case, can be done efficiently in many practical situations. We also\ndescribe a variety of approximate inference algorithms that are often applicable when exact\ninferenceisinfeasible. Weexplorewaysinwhichprobabilitytheorycanbeappliedtoworlds\nwithobjectsandrelations\u2014thatis,tofirst-order,asopposedtopropositional,representations.\nFinally,wesurveyalternative approaches touncertain reasoning.\n14.1 REPRESENTING KNOWLEDGE IN AN UNCERTAIN DOMAIN\nInChapter13,wesawthatthefulljointprobabilitydistributioncanansweranyquestionabout\nthedomain,butcanbecomeintractablylargeasthenumberofvariablesgrows. Furthermore,\nspecifying probabilities forpossible worldsonebyoneisunnatural andtedious.\nWealsosawthatindependenceandconditionalindependencerelationshipsamongvari-\nablescangreatlyreducethenumberofprobabilitiesthatneedtobespecifiedinordertodefine\nthefulljointdistribution. ThissectionintroducesadatastructurecalledaBayesiannetwork1\nBAYESIANNETWORK\ntorepresent the dependencies among variables. Bayesian networks can represent essentially\nanyfulljointprobability distribution andinmanycasescandosoveryconcisely.\n1 Thisisthemostcommonname,buttherearemanysynonyms,includingbeliefnetwork,probabilisticnet-\nwork, causal network, and knowledge map. In statistics, the term graphical model refers to a somewhat\nbroaderclassthatincludesBayesiannetworks.AnextensionofBayesiannetworkscalledadecisionnetworkor\ninfluencediagramiscoveredinChapter16.\n510 Section14.1. Representing KnowledgeinanUncertainDomain 511\nABayesiannetworkisadirected graph inwhicheachnodeisannotated withquantita-\ntiveprobability information. Thefullspecification isasfollows:\n1. Eachnodecorresponds toarandomvariable, whichmaybediscrete orcontinuous.\n2. Asetofdirectedlinksorarrowsconnectspairsofnodes. Ifthereisanarrowfromnode\nX tonodeY,X issaidtobeaparentofY. Thegraphhasnodirectedcycles(andhence\nisadirected acyclicgraph, orDAG.\n3. EachnodeX hasaconditionalprobabilitydistributionP(X |Parents(X ))thatquan-\ni i i\ntifiestheeffectoftheparentsonthenode.\nThetopology ofthenetwork\u2014the setofnodesandlinks\u2014specifies theconditional indepen-\ndence relationships that hold in the domain, in a way that will be made precise shortly. The\nintuitive meaning ofanarrow istypically that X hasadirect influence onY,whichsuggests\nthatcausesshouldbeparentsofeffects. Itisusuallyeasyforadomainexperttodecidewhat\ndirectinfluencesexistinthedomain\u2014mucheasier,infact,thanactuallyspecifyingtheprob-\nabilities themselves. Once the topology of the Bayesian network is laid out, we need only\nspecify a conditional probability distribution for each variable, given its parents. We will\nsee that the combination of the topology and the conditional distributions suffices tospecify\n(implicitly) thefulljointdistribution forallthevariables.\nRecallthesimpleworlddescribedinChapter13,consisting ofthevariablesToothache,\nCavity, Catch, and Weather. We argued that Weather is independent of the other vari-\nables; furthermore, we argued that Toothache and Catch are conditionally independent,\ngiven Cavity. These relationships are represented by the Bayesian network structure shown\nin Figure 14.1. Formally, the conditional independence of Toothache and Catch, given\nCavity,isindicated bytheabsence ofalinkbetweenToothache andCatch. Intuitively, the\nnetwork represents the fact that Cavity is a direct cause of Toothache and Catch, whereas\nnodirectcausalrelationship existsbetween Toothache andCatch.\nNow consider the following example, which is just a little more complex. You have\na new burglar alarm installed at home. It is fairly reliable at detecting a burglary, but also\nresponds on occasion to minor earthquakes. (This example is due to Judea Pearl, a resident\nofLosAngeles\u2014hencetheacuteinterestinearthquakes.) Youalsohavetwoneighbors,John\nand Mary, who have promised to call you at work when they hear the alarm. John nearly\nalways calls when he hears the alarm, but sometimes confuses the telephone ringing with\nCavity\nWeather\nToothache Catch\nFigure 14.1 A simple Bayesian network in which Weather is independentof the other\nthreevariablesandToothache andCatch areconditionallyindependent,givenCavity. 512 Chapter 14. Probabilistic Reasoning\nP(B) P(E)\nBurglary Earthquake\n.001 .002\nB E P(A)\nt t .95\nAlarm\nt f .94\nf t .29\nf f .001\nA P(J) A P(M)\nJohnCalls MaryCalls\nt .90 t .70\nf .05 f .01\nFigure14.2 AtypicalBayesiannetwork,showingboththetopologyandtheconditional\nprobabilitytables(CPTs). IntheCPTs, theletters B, E, A, J, andM standforBurglary,\nEarthquake,Alarm,JohnCalls,andMaryCalls,respectively.\nthe alarm and calls then, too. Mary, on the other hand, likes rather loud music and often\nmisses thealarm altogether. Giventhe evidence ofwhohasor hasnot called, wewouldlike\ntoestimatetheprobability ofaburglary.\nA Bayesian network for this domain appears in Figure 14.2. The network structure\nshows that burglary and earthquakes directly affect the probability of the alarm\u2019s going off,\nbut whether John and Mary call depends only on the alarm. The network thus represents\nourassumptions thattheydonotperceive burglaries directly, theydonotnotice minorearth-\nquakes, andtheydonotconferbeforecalling.\nThe conditional distributions in Figure 14.2 are shown as a conditional probability\nCONDITIONAL table, or CPT. (This form of table can be used for discrete variables; other representations,\nPROBABILITYTABLE\nincluding those suitable for continuous variables, are described in Section 14.2.) Each row\nin a CPT contains the conditional probability of each node value for a conditioning case.\nCONDITIONINGCASE\nA conditioning case is just a possible combination of values for the parent nodes\u2014a minia-\nture possible world, if you like. Each row must sum to 1, because the entries represent an\nexhaustive setofcasesforthevariable. ForBooleanvariables, onceyouknowthattheprob-\nabilityofatruevalueisp,theprobability offalsemustbe1\u2013p,soweoftenomitthesecond\nnumber, asinFigure 14.2. Ingeneral, atable foraBoolean variable with k Boolean parents\ncontains2k independentlyspecifiableprobabilities. Anodewithnoparentshasonlyonerow,\nrepresenting thepriorprobabilities ofeachpossible valueofthevariable.\nNoticethatthenetworkdoesnothavenodescorrespondingtoMary\u2019scurrentlylistening\ntoloud musicortothetelephone ringing andconfusing John. Thesefactors aresummarized\nintheuncertainty associated withthelinks from Alarm toJohnCalls andMaryCalls. This\nshowsbothlazinessandignoranceinoperation: itwouldbealotofworktofindoutwhythose\nfactorswouldbemoreorlesslikelyinanyparticularcase,andwehavenoreasonablewayto\nobtain the relevant information anyway. The probabilities actually summarize a potentially Section14.2. TheSemanticsofBayesianNetworks 513\ninfinite set of circumstances in which the alarm might fail to go off (high humidity, power\nfailure, dead battery, cut wires, a dead mouse stuck inside the bell, etc.) or John or Mary\nmightfailtocallandreportit(outtolunch,onvacation,temporarilydeaf,passinghelicopter,\netc.). Inthisway,asmallagentcancopewithaverylargeworld,atleastapproximately. The\ndegreeofapproximation canbeimprovedifweintroduce additional relevantinformation.\n14.2 THE SEMANTICS OF BAYESIAN NETWORKS\nThe previous section described what a network is, but not what it means. There are two\nways in which one can understand the semantics of Bayesian networks. The first is to see\nthe network as a representation of the joint probability distribution. The second is to view\nitas an encoding ofacollection ofconditional independence statements. Thetwoviews are\nequivalent, but the first turns out to be helpful in understanding how to construct networks,\nwhereasthesecond ishelpfulindesigning inference procedures.\n14.2.1 Representing the full jointdistribution\nViewed as a piece of \u201csyntax,\u201d a Bayesian network is a directed acyclic graph with some\nnumeric parameters attached to each node. One way to define what the network means\u2014its\nsemantics\u2014istodefinethewayinwhichitrepresentsaspecificjointdistribution overallthe\nvariables. Todothis, wefirstneedtoretract (temporarily) whatwesaidearlieraboutthepa-\nrametersassociated witheachnode. Wesaidthatthoseparameterscorrespond toconditional\nprobabilities P(X |Parents(X )); this is atrue statement, but until weassign semantics to\ni i\nthenetworkasawhole,weshould thinkofthemjustasnumbers \u03b8(X |Parents(X )).\ni i\nAgeneric entry inthejointdistribution istheprobability ofaconjunction ofparticular\nassignments to each variable, such as P(X =x \u2227 ... \u2227 X =x ). We use the notation\n1 1 n n\nP(x ,...,x )asanabbreviation forthis. Thevalueofthisentryisgivenbytheformula\n1 n\n(cid:25)n\nP(x ,...,x ) = \u03b8(x |parents(X )), (14.1)\n1 n i i\ni=1\nwhere parents(X ) denotes the values of Parents(X ) that appear in x ,...,x . Thus,\ni i 1 n\neach entry in the joint distribution is represented by the product of the appropriate elements\noftheconditional probability tables(CPTs)intheBayesiannetwork.\nFrom this definition, it is easy to prove that the parameters \u03b8(X |Parents(X )) are\ni i\nexactly the conditional probabilities P(X |Parents(X )) implied by the joint distribution\ni i\n(seeExercise14.2). Hence,wecanrewriteEquation(14.1)as\n(cid:25)n\nP(x ,...,x ) = P(x |parents(X )). (14.2)\n1 n i i\ni=1\nInotherwords, thetables wehavebeen calling conditional probability tables really arecon-\nditional probability tablesaccording tothesemanticsdefinedinEquation(14.1).\nToillustratethis,wecancalculatetheprobabilitythatthealarmhassounded,butneither\naburglary noranearthquake hasoccurred, andbothJohnandMarycall. Wemultiplyentries 514 Chapter 14. Probabilistic Reasoning\nfromthejointdistribution (usingsingle-letter namesfor thevariables):\nP(j,m,a,\u00acb,\u00ace) = P(j|a)P(m|a)P(a|\u00acb\u2227\u00ace)P(\u00acb)P(\u00ace)\n= 0.90\u00d70.70\u00d70.001\u00d70.999\u00d70.998 = 0.000628 .\nSection 13.3 explained that the full joint distribution can be used to answer any query about\nthedomain. IfaBayesiannetworkisarepresentation ofthejointdistribution, thenittoocan\nbeusedtoansweranyquery, bysummingalltherelevantjoint entries. Section14.4explains\nhowtodothis,butalsodescribes methodsthataremuchmoreefficient.\nAmethodforconstructingBayesiannetworks\nEquation (14.2) defines what a given Bayesian network means. The next step is to explain\nhow to construct a Bayesian network in such a way that the resulting joint distribution is a\ngoodrepresentationofagivendomain. WewillnowshowthatEquation(14.2)impliescertain\nconditional independence relationships that can be used to guide the knowledge engineer in\nconstructing thetopologyofthenetwork. First,werewrite theentriesinthejointdistribution\nintermsofconditional probability, usingtheproduct rule (seepage486):\nP(x 1,...,x n) = P(x n|x n\u22121,...,x 1)P(x n\u22121,...,x 1).\nThenwerepeattheprocess,reducingeachconjunctiveprobabilitytoaconditionalprobability\nandasmallerconjunction. Weendupwithonebigproduct:\nP(x 1,...,x n) = P(x n|x n\u22121,...,x 1)P(x n\u22121|x n\u22122,...,x 1) \u00b7\u00b7\u00b7 P(x 2|x 1)P(x 1)\n(cid:25)n\n= P(x i|x i\u22121,...,x 1).\ni=1\nThisidentity iscalled thechainrule. Itholds foranysetofrandom variables. Comparingit\nCHAINRULE\nwithEquation(14.2),weseethatthespecificationofthejointdistribution isequivalenttothe\ngeneralassertion that,foreveryvariable X inthenetwork,\ni\nP(X i|X i\u22121,...,X 1)= P(X i|Parents(X i)), (14.3)\nprovidedthatParents(X i)\u2286 {X i\u22121,...,X 1}. Thislastconditionissatisfiedbynumbering\nthenodesinawaythatisconsistent withthepartialorderimplicitinthegraphstructure.\nWhat Equation (14.3) says is that the Bayesian network is a correct representation of\nthe domain only if each node is conditionally independent of its other predecessors in the\nnodeordering, givenitsparents. Wecansatisfythiscondition withthismethodology:\n1. Nodes: Firstdeterminethesetofvariables thatarerequired tomodelthedomain. Now\norderthem,{X ,...,X }. Anyorderwillwork,buttheresultingnetworkwillbemore\n1 n\ncompactifthevariables areorderedsuchthatcausesprecedeeffects.\n2. Links: Fori=1tondo:\n\u2022 Choose, from X 1,...,X i\u22121, a minimal set of parents for X i, such that Equa-\ntion(14.3)issatisfied.\n\u2022 Foreachparentinsertalinkfromtheparentto X .\ni\n\u2022 CPTs: Writedowntheconditional probability table, P(X |Parents(X )).\ni i Section14.2. TheSemanticsofBayesianNetworks 515\nIntuitively, the parents of node X i should contain all those nodes in X 1, ..., X i\u22121 that\ndirectly influence X . For example, suppose we have completed the network in Figure 14.2\ni\nexceptforthechoiceofparentsforMaryCalls. MaryCalls iscertainlyinfluencedbywhether\nthereisaBurglary oranEarthquake,butnotdirectlyinfluenced. Intuitively, ourknowledge\nof the domain tells us that these events influence Mary\u2019s calling behavior only through their\neffectonthealarm. Also,giventhestateofthealarm,whetherJohncallshasnoinfluenceon\nMary\u2019s calling. Formally speaking, we believe that the following conditional independence\nstatementholds:\nP(MaryCalls|JohnCalls,Alarm,Earthquake,Burglary)=P(MaryCalls|Alarm).\nThus,Alarm willbetheonlyparentnodeforMaryCalls.\nBecauseeachnodeisconnectedonlytoearliernodes,thisconstruction methodguaran-\nteesthatthenetworkisacyclic. AnotherimportantpropertyofBayesiannetworksisthatthey\ncontain no redundant probability values. If there is no redundancy, then there is no chance\nfor inconsistency: it is impossible for the knowledge engineer or domain expert to create a\nBayesiannetworkthatviolatestheaxiomsofprobability.\nCompactnessandnodeordering\nAswellasbeingacompleteandnonredundant representation ofthedomain,aBayesiannet-\nwork can often be far more compact than the full joint distribution. This property is what\nmakesitfeasible tohandle domains withmanyvariables. Thecompactness ofBayesian net-\nLOCALLY worksisanexampleofageneralpropertyoflocallystructured(alsocalledsparse)systems.\nSTRUCTURED\nIn a locally structured system, each subcomponent interacts directly with only a bounded\nSPARSE\nnumber ofother components, regardless ofthe total number of components. Local structure\nisusually associated withlinearratherthanexponential growthincomplexity. Inthecaseof\nBayesian networks, it is reasonable to suppose that in most domains each random variable\nis directly influenced by at most k others, for some constant k. If we assume n Boolean\nvariables for simplicity, then the amount of information needed to specify each conditional\nprobability table will be at most 2k numbers, and the complete network can be specified by\nn2k numbers. Incontrast, thejointdistribution contains 2n numbers. Tomakethisconcrete,\nsuppose we have n=30 nodes, each with five parents (k=5). Then the Bayesian network\nrequires 960numbers,butthefulljointdistribution requiresoverabillion.\nThere are domains in which each variable can be influenced directly by all the others,\nsothatthenetwork isfullyconnected. Thenspecifying theconditional probability tables re-\nquiresthesameamountofinformationasspecifying thejointdistribution. Insomedomains,\nthere will be slight dependencies that should strictly be included by adding a new link. But\nif these dependencies are tenuous, then it maynot be worth the additional complexity in the\nnetwork for the small gain in accuracy. For example, one might object to our burglary net-\nwork on the grounds that if there is an earthquake, then John and Mary would not call even\nif they heard the alarm, because they assume that the earthquake is the cause. Whether to\nadd the link from Earthquake to JohnCalls and MaryCalls (and thus enlarge the tables)\ndepends oncomparing theimportance ofgetting moreaccurate probabilities withthecostof\nspecifying theextrainformation. 516 Chapter 14. Probabilistic Reasoning\nMaryCalls MaryCalls\nJohnCalls JohnCalls\nAlarm Earthquake\nBurglary Burglary\nEarthquake Alarm\n(a) (b)\nFigure 14.3 Network structure depends on order of introduction. In each network, we\nhaveintroducednodesintop-to-bottomorder.\nEven in a locally structured domain, we will get a compact Bayesian network only if\nwe choose the node ordering well. What happens if we happen to choose the wrong or-\nder? Consider theburglary example again. Suppose wedecide toadd the nodes inthe order\nMaryCalls, JohnCalls, Alarm, Burglary, Earthquake. We then get the somewhat more\ncomplicated networkshowninFigure14.3(a). Theprocess goesasfollows:\n\u2022 AddingMaryCalls: Noparents.\n\u2022 Adding JohnCalls: If Mary calls, that probably means the alarm has gone off, which\nof course would make it more likely that John calls. Therefore, JohnCalls needs\nMaryCalls asaparent.\n\u2022 AddingAlarm: Clearly,ifbothcall,itismorelikelythatthealarmhasgoneoffthanif\njustoneorneithercalls,soweneedboth MaryCalls andJohnCalls asparents.\n\u2022 Adding Burglary: If we know the alarm state, then the call from John or Mary might\ngiveusinformation aboutourphoneringing orMary\u2019smusic, butnotaboutburglary:\nP(Burglary|Alarm,JohnCalls,MaryCalls) =P(Burglary|Alarm).\nHenceweneedjustAlarm asparent.\n\u2022 Adding Earthquake: If the alarm is on, it is more likely that there has been an earth-\nquake. (The alarm is an earthquake detector of sorts.) But if we know that there has\nbeenaburglary,thenthatexplainsthealarm,andtheprobabilityofanearthquakewould\nbeonlyslightly abovenormal. Hence,weneedbothAlarm andBurglary asparents.\nThe resulting network has two more links than the original network in Figure 14.2 and re-\nquires three more probabilities to be specified. What\u2019s worse, some of the links represent\ntenuous relationships that require difficult and unnatural probability judgments, such as as- Section14.2. TheSemanticsofBayesianNetworks 517\nsessing the probability of Earthquake, given Burglary and Alarm. This phenomenon is\nquite general and is related to the distinction between causal and diagnostic models intro-\nduced in Section 13.5.1 (see also Exercise 8.13). If we try to build a diagnostic model with\nlinks from symptoms to causes (as from MaryCalls to Alarm or Alarm to Burglary), we\nenduphavingtospecifyadditionaldependenciesbetweenotherwiseindependentcauses(and\noftenbetweenseparately occurring symptomsaswell). Ifwesticktoacausalmodel,weend\nuphavingtospecifyfewernumbers,andthenumberswilloftenbeeasiertocomeupwith. In\nthe domain of medicine, for example, it has been shown by Tversky and Kahneman (1982)\nthat expert physicians prefer to give probability judgments for causal rules rather than for\ndiagnostic ones.\nFigure14.3(b) shows averybad node ordering: MaryCalls, JohnCalls, Earthquake,\nBurglary,Alarm. Thisnetworkrequires31distinctprobabilitiestobespecified\u2014exactlythe\nsamenumberasthefull joint distribution. Itisimportant torealize, however, that anyofthe\nthreenetworkscanrepresent exactlythesamejointdistribution. Thelasttwoversionssimply\nfailtorepresentalltheconditionalindependence relationships andhenceendupspecifying a\nlotofunnecessary numbersinstead.\n14.2.2 Conditionalindependence relationsinBayesiannetworks\nWe have provided a \u201cnumerical\u201d semantics for Bayesian networks in terms of the represen-\ntation of the full joint distribution, as in Equation (14.2). Using this semantics to derive a\nmethod for constructing Bayesian networks, we were led to the consequence that a node is\nconditionally independent of its other predecessors, given its parents. It turns out that we\ncanalso gointheotherdirection. Wecanstart from a\u201ctopological\u201d semantics that specifies\ntheconditional independence relationships encoded bythe graphstructure, andfromthiswe\ncan derive the \u201cnumerical\u201d semantics. The topological semantics2 specifies that each vari-\nable is conditionally independent of its non-descendants, given its parents. Forexample, in\nDESCENDANT\nFigure14.2, JohnCalls isindependent ofBurglary,Earthquake,andMaryCalls giventhe\nvalue ofAlarm. Thedefinition isillustrated inFigure 14.4(a). From these conditional inde-\npendence assertions and the interpretation of the network parameters \u03b8(X |Parents(X ))\ni i\nasspecifications ofconditional probabilities P(X |Parents(X )), thefull jointdistribution\ni i\ngiven in Equation (14.2) can be reconstructed. In this sense, the \u201cnumerical\u201d semantics and\nthe\u201ctopological\u201d semantics areequivalent.\nAnother important independence property is implied by the topological semantics: a\nnodeisconditionallyindependentofallothernodesinthenetwork,givenitsparents,children,\nand children\u2019s parents\u2014that is, given its Markovblanket. (Exercise 14.7asks you toprove\nMARKOVBLANKET\nthis.) Forexample,Burglary isindependentofJohnCalls andMaryCalls,givenAlarm and\nEarthquake. Thisproperty isillustrated inFigure14.4(b).\n2 There is also a general topological criterion called d-separation for deciding whether a set of nodes X is\nconditionally independent of another set Y, givenathirdset Z. Thecriterionisrathercomplicated and isnot\nneededforderivingthealgorithmsinthischapter,soweomitit.DetailsmaybefoundinPearl(1988)orDarwiche\n(2009).Shachter(1998)givesamoreintuitivemethodofascertainingd-separation. 518 Chapter 14. Probabilistic Reasoning\n. . .\nU U\n1 m U 1 . . . U m\nX X\nZ Z Z Z\n1j nj 1j nj\nY Y\nY Y 1 . . . n\n1 . . . n\n(a) (b)\nFigure14.4 (a) A nodeX is conditionallyindependentofits non-descendants(e.g.,the\nZijs) given its parents (the Uis shown in the gray area). (b) A node X is conditionally\nindependentofallothernodesinthenetworkgivenitsMarkovblanket(thegrayarea).\n14.3 EFFICIENT REPRESENTATION OF CONDITIONAL DISTRIBUTIONS\nEvenifthe maximumnumberofparents k issmallish, filling inthe CPTforanode requires\nuptoO(2k)numbersandperhapsagreatdealofexperiencewithallthepossibleconditioning\ncases. Infact,thisisaworst-case scenario inwhichtherelationship betweentheparents and\nthe child is completely arbitrary. Usually, such relationships are describable by a canonical\nCANONICAL distributionthatfitssomestandardpattern. Insuchcases,thecompletetablecanbespecified\nDISTRIBUTION\nbynamingthepatternandperhaps supplying afewparameters\u2014much easierthansupplying\nanexponential numberofparameters.\nDETERMINISTIC The simplest example is provided by deterministic nodes. A deterministic node has\nNODES\nits value specified exactly by the values of its parents, with no uncertainty. The relationship\ncanbealogicalone: forexample,therelationship betweentheparentnodes Canadian,US,\nMexican and the child node NorthAmerican is simply that the child is the disjunction of\nthe parents. The relationship can also be numerical: for example, if the parent nodes are\nthe prices of a particular model of car at several dealers and the child node is the price that\na bargain hunter ends up paying, then the child node is the minimum of the parent values;\nor if the parent nodes are a lake\u2019s inflows (rivers, runoff, precipitation) and outflows (rivers,\nevaporation, seepage)andthechildisthechangeinthewaterlevelofthelake,thenthevalue\nofthechildisthesumoftheinflowparents minusthesumoftheoutflowparents.\nUncertain relationships can often be characterized by so-called noisy logical relation-\nships. The standard example is the noisy-OR relation, which is a generalization of the log-\nNOISY-OR\nical OR. In propositional logic, we might say that Fever is true if and only if Cold, Flu, or\nMalaria is true. The noisy-OR model allows for uncertainty about the ability of each par-\nent to cause the child to be true\u2014the causal relationship between parent and child may be Section14.3. EfficientRepresentation ofConditional Distributions 519\ninhibited, and so a patient could have a cold, but not exhibit a fever. The model makes two\nassumptions. First, it assumes that all the possible causes are listed. (If some are missing,\nwe can always add a so-called leak node that covers \u201cmiscellaneous causes.\u201d) Second, it\nLEAKNODE\nassumes that inhibition of each parent is independent of inhibition of any other parents: for\nexample,whateverinhibitsMalaria fromcausingafeverisindependentofwhateverinhibits\nFlu from causing a fever. Given these assumptions, Fever is false if and only if all its true\nparents are inhibited, and the probability of this is the product of the inhibition probabilities\nq foreachparent. Letussuppose theseindividual inhibition probabilities areasfollows:\nq = P(\u00acfever|cold,\u00acflu,\u00acmalaria)= 0.6,\ncold\nq = P(\u00acfever|\u00accold,flu,\u00acmalaria)= 0.2,\nflu\nq = P(\u00acfever|\u00accold,\u00acflu,malaria)= 0.1.\nmalaria\nThen,fromthisinformation andthenoisy-OR assumptions, theentireCPTcanbebuilt. The\ngeneralruleisthat\n(cid:25)\nP(x |parents(X )) = 1\u2212 q ,\ni i j\n{j:Xj=true}\nwhere the product is taken over the parents that are set to true for that row of the CPT. The\nfollowingtableillustrates thiscalculation:\nCold Flu Malaria P(Fever) P(\u00acFever)\nF F F 0.0 1.0\nF F T 0.9 0.1\nF T F 0.8 0.2\nF T T 0.98 0.02 = 0.2\u00d70.1\nT F F 0.4 0.6\nT F T 0.94 0.06 = 0.6\u00d70.1\nT T F 0.88 0.12 = 0.6\u00d70.2\nT T T 0.988 0.012 = 0.6\u00d70.2\u00d70.1\nIn general, noisy logical relationships in which a variable depends on k parents can be de-\nscribed using O(k) parameters instead of O(2k) for the full conditional probability table.\nThis makes assessment and learning much easier. For example, the CPCS network (Prad-\nhan etal., 1994) uses noisy-OR and noisy-MAX distributions tomodel relationships among\ndiseases and symptoms ininternal medicine. With 448 nodes and 906 links, it requires only\n8,254valuesinsteadof133,931,430 foranetworkwithfullCPTs.\nBayesiannetswithcontinuousvariables\nManyreal-world problems involve continuous quantities, such asheight, mass, temperature,\nandmoney;infact,muchofstatisticsdealswithrandomvariableswhosedomainsarecontin-\nuous. By definition, continuous variables have an infinite number of possible values, so itis\nimpossibletospecifyconditional probabilities explicitlyforeachvalue. Onepossiblewayto\nhandlecontinuousvariablesistoavoidthembyusingdiscretization\u2014thatis,dividingupthe\nDISCRETIZATION 520 Chapter 14. Probabilistic Reasoning\nSubsidy Harvest\nCost\nBuys\nFigure14.5 Asimplenetworkwithdiscretevariables(SubsidyandBuys)andcontinuous\nvariables(Harvest andCost).\npossible values intoafixedsetofintervals. Forexample, temperatures could bedivided into\n(<0oC), (0oC\u2212100oC), and (>100oC). Discretization is sometimes an adequate solution,\nbut often results in a considerable loss of accuracy and very large CPTs. The most com-\nmonsolutionistodefinestandardfamiliesofprobability densityfunctions(seeAppendixA)\nthat are specified by a finite number of parameters. For example, a Gaussian (or normal)\nPARAMETER\ndistribution N(\u03bc,\u03c32)(x) has the mean \u03bc and the variance \u03c32 as parameters. Yet another\nsolution\u2014sometimes called a nonparametric representation\u2014is to define the conditional\nNONPARAMETRIC\ndistribution implicitly with a collection of instances, each containing specific values of the\nparentandchildvariables. Weexplorethisapproach furtherinChapter18.\nA network with both discrete and continuous variables is called a hybrid Bayesian\nHYBRIDBAYESIAN network. To specify a hybrid network, we have to specify two new kinds of distributions:\nNETWORK\nthe conditional distribution for a continuous variable given discrete or continuous parents;\nandtheconditionaldistributionforadiscretevariablegivencontinuousparents. Considerthe\nsimple example in Figure 14.5, in which a customer buys some fruit depending on its cost,\nwhichdependsinturnonthesizeoftheharvestandwhetherthegovernment\u2019ssubsidyscheme\nis operating. The variable Cost is continuous and has continuous and discrete parents; the\nvariable Buys isdiscreteandhasacontinuous parent.\nFor the Cost variable, we need to specify P(Cost|Harvest,Subsidy). The discrete\nparent is handled by enumeration\u2014that is, by specifying both P(Cost|Harvest,subsidy)\nandP(Cost|Harvest,\u00acsubsidy). Tohandle Harvest,wespecify howthedistribution over\nthe cost c depends on the continuous value h of Harvest. In other words, we specify the\nparametersofthecostdistribution asafunctionofh. Themostcommonchoiceisthelinear\nGaussian distribution, in which the child has a Gaussian distribution whose mean \u03bc varies\nLINEARGAUSSIAN\nlinearly with the value of the parent and whose standard deviation \u03c3 is fixed. We need two\ndistributions, oneforsubsidy andonefor\u00acsubsidy,withdifferentparameters:\n\u201c \u201d\n1 \u22121 c\u2212(ath+bt) 2\nP(c|h,subsidy) = N(a th+b t,\u03c3 t2)(c) = \u221a e 2 \u03c3t\n\u03c3 2\u03c0\nt\n\u201e \u00ab\n2\n1 \u22121 c\u2212(afh+bf)\nP(c|h,\u00acsubsidy) = N(a h+b ,\u03c32)(c) = \u221a e 2 \u03c3f .\nf f f\n\u03c3 2\u03c0\nf\nForthisexample,then,theconditional distribution forCost isspecifiedbynamingthelinear\nGaussiandistribution andprovidingtheparameters a ,b ,\u03c3 ,a ,b ,and\u03c3 . Figures14.6(a)\nt t t f f f Section14.3. EfficientRepresentation ofConditional Distributions 521\nP(c | h,subsidy) P(c | h,\u00acsubsidy) P(c | h)\n0.4 0.4 0.4\n0.3 0.3 0.3\n0.2 0.2 0.2\n0.1 0.1 0.1\n12 12 12\n0 810 0 810 0 810\n0 2 C4 ost6 c810 02 H4 a6\nrvesth\n0 2 C4 ost6 c810 02 H4 a6\nrvesth\n0 2 C4 ost6 c810 02 H4 a6\nrvesth\n(a) (b) (c)\nFigure 14.6 The graphs in (a) and (b) show the probability distribution over Cost as a\nfunction of Harvest size, with Subsidy true and false, respectively. Graph (c) shows the\ndistributionP(Cost|Harvest),obtainedbysummingoverthetwosubsidycases.\nand (b) show these tworelationships. Notice that in each case the slope is negative, because\ncost decreases as supply increases. (Of course, the assumption of linearity implies that the\ncostbecomesnegativeatsomepoint;thelinearmodelisreasonableonlyiftheharvestsizeis\nlimitedtoanarrowrange.) Figure14.6(c)showsthedistribution P(c|h),averagingoverthe\ntwopossible valuesofSubsidy andassumingthateachhaspriorprobability 0.5. Thisshows\nthatevenwithverysimplemodels,quiteinteresting distributions canberepresented.\nThe linear Gaussian conditional distribution has some special properties. A network\ncontaining only continuous variables with linear Gaussian distributions has a joint distribu-\ntionthatisamultivariateGaussiandistribution(seeAppendixA)overallthevariables(Exer-\ncise14.9). Furthermore,theposteriordistribution given anyevidencealsohasthisproperty.3\nWhen discrete variables are added as parents (not as children) of continuous variables, the\nCONDITIONAL network defines a conditional Gaussian, or CG, distribution: given any assignment to the\nGAUSSIAN\ndiscretevariables, thedistribution overthecontinuous variables isamultivariate Gaussian.\nNow we turn to the distributions for discrete variables with continuous parents. Con-\nsider, for example, the Buys node in Figure 14.5. It seems reasonable to assume that the\ncustomer will buy if the cost is low and will not buy if it is high and that the probability of\nbuyingvariessmoothlyinsomeintermediateregion. Inotherwords,theconditionaldistribu-\ntionislikea\u201csoft\u201dthreshold function. Onewaytomakesoftthresholds istousetheintegral\nofthestandard normaldistribution:\n(cid:26)\nx\n\u03a6(x) = N(0,1)(x)dx .\n\u2212\u221e\nThentheprobability ofBuys givenCost mightbe\nP(buys|Cost=c) = \u03a6((\u2212c+\u03bc)\/\u03c3),\nwhichmeansthatthecostthresholdoccursaround \u03bc,thewidthofthethresholdregionispro-\nportional to \u03c3, and the probability ofbuying decreases as cost increases. Thisprobit distri-\n3 ItfollowsthatinferenceinlinearGaussiannetworkstakesonlyO(n3)timeintheworstcase,regardlessofthe\nnetworktopology.InSection14.4,weseethatinferencefornetworksofdiscretevariablesisNP-hard. 522 Chapter 14. Probabilistic Reasoning\n1 1\nLogit\nProbit\n0.8 0.8\n0.6 0.6\n0.4 0.4\n0.2 0.2\n0 0\n0 2 4 6 8 10 12 0 2 4 6 8 10 12\nCostc Costc\n(a) (b)\nFigure 14.7 (a) A normal (Gaussian) distribution for the cost threshold, centered on\n\u03bc=6.0withstandarddeviation\u03c3=1.0. (b)Logitandprobitdistributionsfortheprobability\nofbuys givencost,fortheparameters\u03bc=6.0and\u03c3=1.0.\nPROBIT bution(pronounced\u201cpro-bit\u201dandshortfor\u201cprobabilityunit\u201d)is illustratedinFigure14.7(a).\nDISTRIBUTION\nTheformcanbejustifiedbyproposingthattheunderlyingdecisionprocesshasahardthresh-\nold,butthatthepreciselocationofthethreshold issubjecttorandomGaussiannoise.\nAn alternative to the probit model is the logit distribution (pronounced \u201clow-jit\u201d). It\nLOGITDISTRIBUTION\nusesthelogistic\nfunction1\/(1+e\u2212x)toproduce\nasoftthreshold:\nLOGISTICFUNCTION\n1\nP(buys|Cost=c) = .\n1+exp(\u22122\u2212c+\u03bc)\n\u03c3\nThisisillustrated inFigure14.7(b). Thetwodistributions look similar, butthelogitactually\nhasmuchlonger\u201ctails.\u201d Theprobitisoftenabetterfittorealsituations,butthelogitissome-\ntimes easier to deal with mathematically. It is used widely in neural networks (Chapter 20).\nBoth probit and logit can be generalized to handle multiple continuous parents by taking a\nlinearcombination oftheparentvalues.\n14.4 EXACT INFERENCE IN BAYESIAN NETWORKS\nThe basic task for any probabilistic inference system is to compute the posterior probability\ndistribution for a set of query variables, given some observed event\u2014that is, some assign-\nEVENT\nmentofvaluestoasetofevidencevariables. Tosimplifythepresentation, wewillconsider\nonlyonequeryvariable atatime;thealgorithms caneasilybeextended toqueries withmul-\ntiple variables. We will use the notation from Chapter 13: X denotes the query variable; E\ndenotesthesetofevidencevariablesE ,...,E ,andeisaparticularobservedevent; Ywill\n1 m\ndenotesthenonevidence, nonqueryvariables Y ,...,Y (calledthehiddenvariables). Thus,\nHIDDENVARIABLE 1 l\nthe complete set of variables is X={X}\u222aE\u222aY. A typical query asks for the posterior\nprobability distribution P(X|e).\n)c(P\n)c\n| syub(P Section14.4. ExactInference inBayesianNetworks 523\nIn the burglary network, we might observe the event in which JohnCalls=true and\nMaryCalls=true. Wecouldthenaskfor,say,theprobability thataburglary hasoccurred:\nP(Burglary|JohnCalls=true,MaryCalls=true) = (cid:16)0.284,0.716(cid:17).\nIn this section we discuss exact algorithms for computing posterior probabilities and will\nconsider the complexity of this task. It turns out that the general case is intractable, so Sec-\ntion14.5coversmethodsforapproximate inference.\n14.4.1 Inference by enumeration\nChapter 13 explained that any conditional probability can be computed by summing terms\nfrom the full joint distribution. More specifically, a query P(X|e) can be answered using\nEquation(13.9),whichwerepeathereforconvenience:\n(cid:12)\nP(X|e)= \u03b1P(X,e)= \u03b1 P(X,e,y).\ny\nNow,aBayesian networkgivesacompleterepresentation ofthefulljointdistribution. More\nspecifically, Equation (14.2) on page 513 shows that the terms P(x,e,y) in the joint distri-\nbutioncanbewrittenasproducts ofconditional probabilities fromthenetwork. Therefore, a\nquery can be answered using a Bayesian network by computing sums of products of condi-\ntionalprobabilities fromthenetwork.\nConsider the query P(Burglary|JohnCalls=true,MaryCalls=true). The hidden\nvariables for this query are Earthquake and Alarm. From Equation (13.9), using initial\nlettersforthevariables toshortentheexpressions, wehave4\n(cid:12)(cid:12)\nP(B|j,m) = \u03b1P(B,j,m) =\u03b1 P(B,j,m,e,a,).\ne a\nThe semantics of Bayesian networks (Equation (14.2)) then gives us an expression in terms\nofCPTentries. Forsimplicity, wedothisjustforBurglary=true:\n(cid:12)(cid:12)\nP(b|j,m) = \u03b1 P(b)P(e)P(a|b,e)P(j|a)P(m|a).\ne a\nTo compute this expression, we have to add four terms, each computed by multiplying five\nnumbers. Intheworstcase,wherewehavetosumoutalmostallthevariables,thecomplexity\nofthealgorithm foranetworkwith nBooleanvariables isO(n2n).\nAn improvement can be obtained from the following simple observations: the P(b)\ntermisaconstantandcanbemovedoutsidethesummationsoveraande,andtheP(e)term\ncanbemovedoutsidethesummationovera. Hence,wehave\n(cid:12) (cid:12)\nP(b|j,m) = \u03b1P(b) P(e) P(a|b,e)P(j|a)P(m|a). (14.4)\ne a\nThisexpression canbeevaluated bylooping through thevariables inorder, multiplying CPT\nentries as we go. For each summation, we also need to loop over the variable\u2019s possible\nP\n4 Anexpressionsuchas P(a,e)meanstosumP(A = a,E = e)forallpossiblevaluesofe. WhenE is\ne\nBoolean,thereisanambiguityinthatP(e)isusedtomeanbothP(E =true)andP(E =e),butitshouldbe\nclearfromcontextwhichisintended;inparticular,inthecontextofasumthelatterisintended. 524 Chapter 14. Probabilistic Reasoning\nvalues. The structure of this computation is shown in Figure 14.8. Using the numbers from\nFigure 14.2, we obtain P(b|j,m) = \u03b1\u00d70.00059224. The corresponding computation for\n\u00acbyields\u03b1\u00d70.0014919;hence,\nP(B|j,m) = \u03b1(cid:16)0.00059224,0.0014919(cid:17) \u2248 (cid:16)0.284,0.716(cid:17).\nThatis,thechanceofaburglary, givencallsfrombothneighbors, isabout28%.\nTheevaluation processfortheexpression inEquation(14.4)isshownasanexpression\ntree in Figure 14.8. The ENUMERATION-ASK algorithm in Figure 14.9 evaluates such trees\nusing depth-first recursion. Thealgorithm is verysimilar instructure tothe backtracking al-\ngorithmforsolvingCSPs(Figure6.5)andtheDPLLalgorithmforsatisfiability(Figure7.17).\nThespacecomplexityofENUMERATION-ASK isonlylinearinthenumberofvariables:\nthealgorithm sumsoverthefulljointdistribution without everconstructing itexplicitly. Un-\nfortunately, its time complexity for a network with n Boolean variables is always O(2n)\u2014\nbetterthantheO(n2n)forthesimpleapproach described earlier, butstillrather grim.\nNote that the tree in Figure 14.8 makes explicit the repeated subexpressions evalu-\natedbythealgorithm. Theproducts P(j|a)P(m|a)andP(j|\u00aca)P(m|\u00aca)arecomputed\ntwice,onceforeachvalueofe. Thenextsectiondescribesageneralmethodthatavoidssuch\nwastedcomputations.\n14.4.2 The variableeliminationalgorithm\nThe enumeration algorithm can be improved substantially by eliminating repeated calcula-\ntions of the kind illustrated in Figure 14.8. The idea is simple: do the calculation once and\nsavetheresultsforlateruse. Thisisaformofdynamicprogramming. Thereareseveralver-\nVARIABLE sionsofthisapproach;wepresentthevariableeliminationalgorithm,whichisthesimplest.\nELIMINATION\nVariableelimination worksbyevaluating expressions such asEquation(14.4)inright-to-left\norder(thatis,bottomupinFigure14.8). Intermediateresultsarestored,andsummationsover\neachvariable aredoneonlyforthoseportions oftheexpression thatdependonthevariable.\nLetusillustrate thisprocessfortheburglary network. Weevaluatetheexpression\n(cid:12) (cid:12)\nP(B|j,m) = \u03b1 P(B) P(e) P(a|B,e)P(j|a)P(m|a) .\n(cid:27)(cid:28)(cid:29)(cid:30) (cid:27)(cid:28)(cid:29)(cid:30) (cid:27) (cid:28)(cid:29) (cid:30)(cid:27) (cid:28)(cid:29) (cid:30)(cid:27) (cid:28)(cid:29) (cid:30)\ne a\nf1(B) f2(E) f3(A,B,E) f4(A) f5(A)\nNoticethatwehaveannotatedeachpartoftheexpressionwiththenameofthecorresponding\nfactor; each factorisamatrixindexed bythevalues ofitsargument variables. Forexample,\nFACTOR\nthefactorsf (A)andf (A)corresponding toP(j|a)andP(m|a)dependjustonAbecause\n4 5\nJ andM arefixedbythequery. Theyaretherefore two-elementvectors:\n(cid:13) (cid:14) (cid:13) (cid:14) (cid:13) (cid:14) (cid:13) (cid:14)\nP(j|a) 0.90 P(m|a) 0.70\nf (A) = = f (A) = = .\n4 P(j|\u00aca) 0.05 5 P(m|\u00aca) 0.01\nf (A,B,E)willbea2\u00d72\u00d72matrix,whichishardtoshowontheprintedpage. (The\u201cfirst\u201d\n3\nelement isgiven byP(a|b,e)=0.95 andthe\u201clast\u201d byP(\u00aca|\u00acb,\u00ace)=0.999.) Intermsof\nfactors, thequeryexpression iswrittenas\n(cid:12) (cid:12)\nP(B|j,m) = \u03b1f (B)\u00d7 f (E)\u00d7 f (A,B,E)\u00d7f (A)\u00d7f (A)\n1 2 3 4 5\ne a Section14.4. ExactInference inBayesianNetworks 525\nP(b)\n.001\nP(e) P(\u00ace)\n.002 .998\nP(a|b,e) P(\u00aca|b,e) P(a|b,\u00ace) P(\u00aca|b,\u00ace)\n.95 .05 .94 .06\nP(j|a) P( j|\u00aca) P( j|a) P( j|\u00aca)\n.90 .05 .90 .05\nP(m|a) P(m|\u00aca) P(m|a) P(m|\u00aca)\n.70 .01 .70 .01\nFigure 14.8 The structure of the expression shown in Equation (14.4). The evaluation\nproceedstopdown,multiplyingvaluesalongeachpathandsummingatthe\u201c+\u201dnodes.Notice\ntherepetitionofthepathsforjandm.\nfunctionENUMERATION-ASK(X,e,bn)returnsadistributionoverX\ninputs:X, thequeryvariable\ne,observedvaluesforvariablesE\nbn,aBayesnetwithvariables{X} \u222a E \u222a Y \/*Y=hiddenvariables*\/\nQ(X)\u2190adistributionoverX,initiallyempty\nforeachvaluexi ofX do\nQ(xi)\u2190ENUMERATE-ALL(bn.VARS,exi)\nwhereexi iseextendedwithX= xi\nreturnNORMALIZE(Q(X))\nfunctionENUMERATE-ALL(vars,e)returnsarealnumber\nifEMPTY?(vars)thenreturn1.0\nY \u2190FIRST(vars)\nifY hasvaluey ine\nthenreturn(cid:2)P(y|parents(Y)) \u00d7 ENUMERATE-ALL(REST(vars),e)\nelsereturn\ny\nP(y|parents(Y)) \u00d7 ENUMERATE-ALL(REST(vars),ey)\nwhereey iseextendedwithY = y\nFigure14.9 TheenumerationalgorithmforansweringqueriesonBayesiannetworks. 526 Chapter 14. Probabilistic Reasoning\nwherethe\u201c\u00d7\u201doperatorisnotordinarymatrixmultiplication butinsteadthepointwiseprod-\nPOINTWISE uctoperation, tobedescribed shortly.\nPRODUCT\nThe process of evaluation is a process of summing out variables (right to left) from\npointwise products of factors to produce new factors, eventually yielding a factor that is the\nsolution, i.e.,theposteriordistribution overthequeryvariable. Thestepsareasfollows:\n\u2022 First,wesumoutAfromtheproduct off ,f ,andf . Thisgivesusanew2\u00d72factor\n3 4 5\nf (B,E)whoseindicesrangeoverjust B andE:\n6\n(cid:12)\nf (B,E) = f (A,B,E)\u00d7f (A)\u00d7f (A)\n6 3 4 5\na\n= (f (a,B,E)\u00d7f (a)\u00d7f (a))+(f (\u00aca,B,E)\u00d7f (\u00aca)\u00d7f (\u00aca)).\n3 4 5 3 4 5\nNowweareleftwiththeexpression\n(cid:12)\nP(B|j,m) = \u03b1f (B)\u00d7 f (E)\u00d7f (B,E).\n1 2 6\ne\n\u2022 Next,wesumoutE fromtheproductof f andf :\n2 6\n(cid:12)\nf (B) = f (E)\u00d7f (B,E)\n7 2 6\ne\n= f (e)\u00d7f (B,e)+f (\u00ace)\u00d7f (B,\u00ace).\n2 6 2 6\nThisleavestheexpression\nP(B|j,m) = \u03b1f (B)\u00d7f (B)\n1 7\nwhichcanbeevaluatedbytakingthepointwiseproduct andnormalizing theresult.\nExaminingthissequence,weseethattwobasiccomputationaloperationsarerequired: point-\nwiseproduct ofapairoffactors, andsumming out avariable from aproduct offactors. The\nnextsectiondescribes eachoftheseoperations.\nOperationsonfactors\nThe pointwise product of two factors f and f yields a new factor f whose variables are\n1 2\nthe union of the variables in f and f and whose elements are given by the product of the\n1 2\ncorrespondingelementsinthetwofactors. SupposethetwofactorshavevariablesY ,...,Y\n1 k\nincommon. Thenwehave\nf(X ...X ,Y ...Y ,Z ...Z )= f (X ...X ,Y ...Y )f (Y ...Y ,Z ...Z ).\n1 j 1 k 1 l 1 1 j 1 k 2 1 k , l\nIf all the variables are binary, then f and f have 2j+k and 2k+l entries, respectively, and\n1 2\nthe pointwise product has 2j+k+l entries. For example, given two factors f (A,B) and\n1\nf (B,C), the pointwise product f \u00d7f =f (A,B,C) has 21+1+1=8 entries, as illustrated\n2 1 2 3\nin Figure 14.10. Notice that the factor resulting from a pointwise product can contain more\nvariablesthananyofthefactorsbeingmultipliedandthatthesizeofafactorisexponentialin\nthe number of variables. This is where both space and time complexity arise in the variable\nelimination algorithm. Section14.4. ExactInference inBayesianNetworks 527\nA B f (A,B) B C f (B,C) A B C f (A,B,C)\n1 2 3\nT T .3 T T .2 T T T .3\u00d7.2=.06\nT F .7 T F .8 T T F .3\u00d7.8=.24\nF T .9 F T .6 T F T .7\u00d7.6=.42\nF F .1 F F .4 T F F .7\u00d7.4=.28\nF T T .9\u00d7.2=.18\nF T F .9\u00d7.8=.72\nF F T .1\u00d7.6=.06\nF F F .1\u00d7.4=.04\nFigure14.10 Illustratingpointwisemultiplication:f (A,B)\u00d7f (B,C)=f (A,B,C).\n1 2 3\nSummingoutavariablefromaproductoffactorsisdonebyaddingupthesubmatrices\nformed by fixing the variable to each of its values in turn. For example, to sum out A from\nf (A,B,C),wewrite\n3 (cid:12)\nf(B,C) = f (A,B,C) = f (a,B,C)+f (\u00aca,B,C)\n3 3 3\n(cid:13)a (cid:14) (cid:13) (cid:14) (cid:13) (cid:14)\n.06 .24 .18 .72 .24 .96\n= + = .\n.42 .28 .06 .04 .48 .32\nTheonlytrick istonotice thatanyfactorthatdoes notdepend onthevariable tobesummed\nout can be moved outside the summation. Forexample, if wewere to sum out E first in the\nburglary network, therelevantpartoftheexpression would be\n(cid:12) (cid:12)\nf (E)\u00d7f (A,B,E)\u00d7f (A)\u00d7f (A) = f (A)\u00d7f (A)\u00d7 f (E)\u00d7f (A,B,E).\n2 3 4 5 4 5 2 3\ne e\nNow the pointwise product inside the summation is computed, and the variable is summed\noutoftheresulting matrix.\nNotice that matrices are not multiplied until we need to sum out a variable from the\naccumulated product. Atthatpoint, wemultiply justthose matrices thatinclude thevariable\nto be summed out. Given functions for pointwise product and summing out, the variable\nelimination algorithm itselfcanbewrittenquitesimply,asshowninFigure14.11.\nVariableorderingandvariablerelevance\nThealgorithminFigure14.11includesanunspecifiedORDERfunctiontochooseanordering\nfor the variables. Every choice of ordering yields a valid algorithm, but different orderings\ncause different intermediate factors to be generated during the calculation. For example, in\nthe calculation shown previously, we eliminated A before E; if we do it the other way, the\ncalculation becomes\n(cid:12) (cid:12)\nP(B|j,m) = \u03b1f (B)\u00d7 f (A)\u00d7f (A)\u00d7 f (E)\u00d7f (A,B,E),\n1 4 5 2 3\na e\nduringwhichanewfactor f (A,B)willbegenerated.\n6\nIn general, the time and space requirements of variable elimination are dominated by\nthe size of the largest factor constructed during the operation of the algorithm. This in turn 528 Chapter 14. Probabilistic Reasoning\nfunctionELIMINATION-ASK(X,e,bn)returnsadistributionoverX\ninputs:X,thequeryvariable\ne,observedvaluesforvariablesE\nbn,aBayesiannetworkspecifyingjointdistributionP(X 1,...,Xn)\nfactors\u2190[]\nforeachvar inORDER(bn.VARS)do\nfactors\u2190[MAKE-FACTOR(var,e)|factors]\nifvar isahiddenvariablethenfactors\u2190SUM-OUT(var,factors)\nreturnNORMALIZE(POINTWISE-PRODUCT(factors))\nFigure14.11 ThevariableeliminationalgorithmforinferenceinBayesiannetworks.\nis determined by the order of elimination of variables and by the structure of the network.\nIt turns out to be intractable to determine the optimal ordering, but several good heuristics\nare available. One fairly effective method is a greedy one: eliminate whichever variable\nminimizesthesizeofthenextfactortobeconstructed.\nLet us consider one more query: P(JohnCalls|Burglary=true). As usual, the first\nstepistowriteoutthenestedsummation:\n(cid:12) (cid:12) (cid:12)\nP(J|b) =\u03b1P(b) P(e) P(a|b,e)P(J|a) P(m|a).\ne a m (cid:2)\nEvaluating thisexpression from right toleft, wenotice something interesting: P(m|a)\nm\nisequal to1by definition! Hence, there wasnoneed toinclude itin thefirstplace; the vari-\nable M is irrelevant to this query. Another way of saying this is that the result of the query\nP(JohnCalls|Burglary=true) is unchanged if we remove MaryCalls from the network\naltogether. Ingeneral,wecanremoveanyleafnodethatisnotaqueryvariableoranevidence\nvariable. Afteritsremoval, theremaybesomemoreleafnodes, andthesetoomaybeirrele-\nvant. Continuing this process, we eventually find that every variable that is not an ancestor\nof a query variable or evidence variable is irrelevant to the query. A variable elimination\nalgorithm cantherefore removeallthesevariablesbeforeevaluating thequery.\n14.4.3 The complexityofexact inference\nThecomplexityofexactinferenceinBayesiannetworksdependsstronglyonthestructureof\nthenetwork. TheburglarynetworkofFigure14.2belongstothefamilyofnetworksinwhich\nthereisatmostoneundirected pathbetweenanytwonodes inthenetwork. Thesearecalled\nsinglyconnectednetworksorpolytrees,andtheyhaveaparticularlyniceproperty: Thetime\nSINGLYCONNECTED\nandspacecomplexityofexactinferenceinpolytreesislinearinthesizeofthenetwork. Here,\nPOLYTREE\nthe size is defined as the number of CPT entries; if the number of parents of each node is\nbounded byaconstant, thenthecomplexity willalsobelinearinthenumberofnodes.\nMULTIPLY Formultiplyconnectednetworks,suchasthatofFigure14.12(a),variableelimination\nCONNECTED\ncan have exponential time and space complexity in the worst case, even when the number\nof parents per node is bounded. This is not surprising when one considers that because it Section14.4. ExactInference inBayesianNetworks 529\nP(C)=.5\nCloudy\nP(C)=.5\nC P(S) C P(R)\nt .10 Sprinkler Rain t .80 Cloudy\nf .50 f .20 P(S+R=x)\nC t t t f f t f f\nWet\nGrass Spr+Rain t .08 .02.72 .18\nf .10 .40.10 .40\nS R P(W) S+R P(W)\nt t .99 t t .99 Wet\nt f .90 t f .90 Grass\nf t .90 f t .90\nf f .00 f f .00\n(a) (b)\nFigure14.12 (a)Amultiplyconnectednetworkwithconditionalprobabilitytables. (b)A\nclusteredequivalentofthemultiplyconnectednetwork.\nincludes inferenceinpropositional logicasaspecialcase,inference inBayesiannetworksis\nNP-hard. Infact,itcanbeshown(Exercise14.16)thattheproblemisashardasthatofcom-\nputing the number of satisfying assignments for a propositional logic formula. This means\nthatitis#P-hard(\u201cnumber-P hard\u201d)\u2014that is,strictlyharderthanNP-completeproblems.\nThereisacloseconnection betweenthecomplexityofBayesiannetworkinferenceand\nthe complexity of constraint satisfaction problems (CSPs). As we discussed in Chapter 6,\nthe difficulty of solving a discrete CSP is related to how \u201ctreelike\u201d its constraint graph is.\nMeasures such as tree width, which bound the complexity of solving a CSP, can also be\napplied directly to Bayesian networks. Moreover, the variable elimination algorithm can be\ngeneralized tosolveCSPsaswellasBayesiannetworks.\n14.4.4 Clustering algorithms\nThevariableeliminationalgorithmissimpleandefficientforansweringindividualqueries. If\nwewanttocompute posterior probabilities forallthevariables inanetwork, however, itcan\nbe less efficient. Forexample, in a polytree network, one would need to issue O(n) queries\ncosting O(n) each, for a total of O(n2) time. Using clustering algorithms (also known as\nCLUSTERING\njointreealgorithms), thetimecanbereduced to O(n). Forthisreason, thesealgorithms are\nJOINTREE\nwidelyusedincommercialBayesiannetworktools.\nThe basic idea of clustering is to join individual nodes of the network to form clus-\nter nodes in such a way that the resulting network is a polytree. For example, the multiply\nconnected network shown in Figure 14.12(a) can be converted into a polytree by combin-\ning the Sprinkler and Rain node into a cluster node called Sprinkler+Rain, as shown in\nFigure 14.12(b). The two Boolean nodes are replaced by a \u201cmeganode\u201d that takes on four\npossible values: tt,tf,ft,andff. Themeganodehasonlyoneparent, theBooleanvariable\nCloudy, so there are two conditioning cases. Although this example doesn\u2019t show it, the\nprocessofclustering oftenproduces meganodesthatsharesomevariables. 530 Chapter 14. Probabilistic Reasoning\nOncethenetworkisinpolytreeform,aspecial-purpose inferencealgorithmisrequired,\nbecauseordinary inference methodscannothandlemeganodesthatsharevariables witheach\nother. Essentially,thealgorithmisaformofconstraintpropagation(seeChapter6)wherethe\nconstraintsensurethatneighboringmeganodesagreeonthe posteriorprobabilityofanyvari-\nablesthattheyhaveincommon. Withcarefulbookkeeping, thisalgorithmisabletocompute\nposterior probabilities forallthe nonevidence nodes inthenetwork intime linear inthesize\noftheclustered network. However,theNP-hardnessoftheproblem hasnotdisappeared: ifa\nnetwork requires exponential timeand space withvariable elimination, then theCPTsinthe\nclustered networkwillnecessarily beexponentially large.\n14.5 APPROXIMATE INFERENCE IN BAYESIAN NETWORKS\nGiventhe intractability ofexact inference in large, multiply connected networks, itis essen-\ntialtoconsiderapproximate inferencemethods. Thissectiondescribesrandomized sampling\nalgorithms, also called Monte Carlo algorithms, that provide approximate answers whose\nMONTECARLO\naccuracy depends on the number of samples generated. Monte Carlo algorithms, of which\nsimulated annealing (page 126) is an example, are used in many branches of science to es-\ntimate quantities that are difficult to calculate exactly. In this section, we are interested in\nsampling applied to the computation of posterior probabilities. We describe two families of\nalgorithms: directsamplingandMarkovchainsampling. Twootherapproaches\u2014variational\nmethodsandloopypropagation\u2014are mentioned inthenotesattheendofthechapter.\n14.5.1 Directsampling methods\nTheprimitive elementinanysampling algorithm isthegeneration ofsamples from aknown\nprobabilitydistribution. Forexample,anunbiasedcoincanbethoughtofasarandomvariable\nCoin with values (cid:16)heads,tails(cid:17) and a prior distribution P(Coin) = (cid:16)0.5,0.5(cid:17). Sampling\nfromthisdistributionisexactlylikeflippingthecoin: withprobability0.5itwillreturnheads,\nand with probability 0.5 it will return tails. Given a source of random numbers uniformly\ndistributed in the range [0,1], it is a simple matter to sample any distribution on a single\nvariable, whetherdiscreteorcontinuous. (SeeExercise14.17.)\nThesimplestkindofrandomsamplingprocess forBayesiannetworksgenerates events\nfrom a network that has no evidence associated with it. The idea is to sample each variable\ninturn, intopological order. Theprobability distribution from which thevalue issampled is\nconditioned onthevaluesalreadyassignedtothevariable\u2019sparents. Thisalgorithm isshown\nin Figure 14.13. Wecan illustrate its operation on the network in Figure 14.12(a), assuming\nanordering [Cloudy,Sprinkler,Rain,WetGrass]:\n1. SamplefromP(Cloudy)= (cid:16)0.5,0.5(cid:17),valueistrue.\n2. SamplefromP(Sprinkler |Cloudy=true) = (cid:16)0.1,0.9(cid:17),valueisfalse.\n3. SamplefromP(Rain|Cloudy=true) = (cid:16)0.8,0.2(cid:17),valueistrue.\n4. SamplefromP(WetGrass|Sprinkler =false,Rain=true)= (cid:16)0.9,0.1(cid:17),valueistrue.\nInthiscase, PRIOR-SAMPLE returnstheevent [true,false,true,true]. Section14.5. Approximate InferenceinBayesianNetworks 531\nfunctionPRIOR-SAMPLE(bn)returnsaneventsampledfromthepriorspecifiedbybn\ninputs:bn,aBayesiannetworkspecifyingjointdistributionP(X 1,...,Xn)\nx\u2190aneventwithnelements\nforeachvariableXiinX 1,...,Xndo\nx[i]\u2190arandomsamplefromP(Xi |parents(Xi))\nreturnx\nFigure14.13 AsamplingalgorithmthatgenerateseventsfromaBayesiannetwork.Each\nvariableissampledaccordingtotheconditionaldistributiongiventhevaluesalreadysampled\nforthevariable\u2019sparents.\nItiseasytoseethatPRIOR-SAMPLE generatessamplesfromthepriorjointdistribution\nspecifiedbythenetwork. First,let S (x ,...,x )betheprobability thataspecificeventis\nPS 1 n\ngenerated bythe PRIOR-SAMPLE algorithm. Justlooking atthesamplingprocess, wehave\n(cid:25)n\nS (x ...x ) = P(x |parents(X ))\nPS 1 n i i\ni=1\nbecause each sampling step depends only on the parent values. This expression should look\nfamiliar, because itisalsothe probability ofthe eventaccording tothe Bayesian net\u2019s repre-\nsentation ofthejointdistribution, asstatedinEquation(14.2). Thatis,wehave\nS (x ...x ) = P(x ...x ).\nPS 1 n 1 n\nThissimplefactmakesiteasytoanswerquestions byusingsamples.\nIn any sampling algorithm, the answers are computed by counting the actual samples\ngenerated. Suppose there are N total samples, and let N (x ,...,x ) be the number of\nPS 1 n\ntimesthespecificeventx ,...,x occurs inthesetofsamples. Weexpect thisnumber, asa\n1 n\nfraction of the total, to converge in the limit to its expected value according to the sampling\nprobability:\nN (x ,...,x )\nPS 1 n\nlim = S (x ,...,x )= P(x ,...,x ). (14.5)\nPS 1 n 1 n\nN\u2192\u221e N\nFor example, consider the event produced earlier: [true,false,true,true]. The sampling\nprobability forthiseventis\nS (true,false,true,true) = 0.5\u00d70.9\u00d70.8\u00d70.9 = 0.324.\nPS\nHence,inthelimitoflargeN,weexpect32.4%ofthesamplestobeofthisevent.\nWheneverweuseanapproximateequality(\u201c\u2248\u201d)inwhatfollows,wemeanitinexactly\nthis sense\u2014that the estimated probability becomes exact in the large-sample limit. Such an\nestimate is called consistent. For example, one can produce a consistent estimate of the\nCONSISTENT\nprobability ofanypartially specifiedevent x ,...,x ,wherem \u2264 n,asfollows:\n1 m\nP(x ,...,x )\u2248 N (x ,...,x )\/N . (14.6)\n1 m PS 1 m\nThat is, the probability of the event can be estimated as the fraction of all complete events\ngenerated by the sampling process that match the partially specified event. For example, if 532 Chapter 14. Probabilistic Reasoning\nwe generate 1000 samples from the sprinkler network, and 511 of them have Rain=true,\nthentheestimatedprobability ofrain,writtenas\nP\u02c6(Rain=true),is0.511.\nRejectionsamplinginBayesian networks\nREJECTION Rejectionsamplingisageneralmethodforproducingsamplesfromahard-to-sampledistri-\nSAMPLING\nbution given an easy-to-sample distribution. In its simplest form, it can be used to compute\nconditional probabilities\u2014that is,todetermine P(X|e). The REJECTION-SAMPLING algo-\nrithmisshowninFigure14.14. First,itgeneratessamplesfromthepriordistributionspecified\nbythenetwork. Then,itrejectsallthosethatdonotmatchtheevidence. Finally,theestimate\nP\u02c6(X=x|e)isobtained bycountinghowoftenX=xoccursintheremaining samples.\nLetP\u02c6(X|e)betheestimateddistributionthatthealgorithmreturns.\nFromthedefinition\nofthealgorithm, wehave\nN (X,e)\nP\u02c6(X|e)= \u03b1N (X,e) = PS .\nPS\nN (e)\nPS\nFromEquation(14.6),thisbecomes\nP(X,e)\nP\u02c6(X|e)\u2248 = P(X|e).\nP(e)\nThatis,rejection samplingproduces aconsistent estimate ofthetrueprobability.\nContinuing withourexamplefrom Figure14.12(a), letusassumethat wewishtoesti-\nmate P(Rain|Sprinkler =true), using 100 samples. Ofthe 100 that we generate, suppose\nthat 73 have Sprinkler =false and are rejected, while 27 have Sprinkler =true; of the 27,\n8haveRain=true and19haveRain=false. Hence,\nP(Rain|Sprinkler =true)\u2248 NORMALIZE((cid:16)8,19(cid:17))= (cid:16)0.296,0.704(cid:17).\nThe true answer is (cid:16)0.3,0.7(cid:17). As more samples are collected, the estimate will converge to\nthe true answer. The standard deviation of the error in each probability will be proportional\n\u221a\nto1\/ n,wherenisthenumberofsamplesusedintheestimate.\nThe biggest problem with rejection sampling is that it rejects so many samples! The\nfraction ofsamples consistent withtheevidence edrops exponentially asthe numberofevi-\ndencevariablesgrows,sotheprocedure issimplyunusable forcomplexproblems.\nNoticethatrejectionsamplingisverysimilartotheestimationofconditionalprobabili-\ntiesdirectlyfromtherealworld. Forexample,toestimate P(Rain|RedSkyAtNight=true),\none can simply count how often it rains after a red sky is observed the previous evening\u2014\nignoring those evenings when the sky is not red. (Here, the world itself plays the role of\nthe sample-generation algorithm.) Obviously, this could take a long time if the sky is very\nseldomred,andthatistheweaknessofrejection sampling.\nLikelihoodweighting\nLIKELIHOOD Likelihoodweightingavoidstheinefficiencyofrejectionsamplingbygeneratingonlyevents\nWEIGHTING\nthat are consistent with the evidence e. It is a particular instance of the general statistical\nIMPORTANCE techniqueofimportancesampling,tailoredforinferenceinBayesiannetworks. Webeginby\nSAMPLING Section14.5. Approximate InferenceinBayesianNetworks 533\nfunctionREJECTION-SAMPLING(X,e,bn,N)returnsanestimateofP(X|e)\ninputs:X,thequeryvariable\ne,observedvaluesforvariablesE\nbn,aBayesiannetwork\nN,thetotalnumberofsamplestobegenerated\nlocalvariables: N,avectorofcountsforeachvalueofX,initiallyzero\nforj =1toN do\nx\u2190PRIOR-SAMPLE(bn)\nifxisconsistentwithethen\nN[x]\u2190N[x]+1wherex isthevalueofX inx\nreturnNORMALIZE(N)\nFigure14.14 Therejection-samplingalgorithmforansweringqueriesgivenevidenceina\nBayesiannetwork.\ndescribing howthealgorithmworks;thenweshowthatitworkscorrectly\u2014that is,generates\nconsistent probability estimates.\nLIKELIHOOD-WEIGHTING (see Figure 14.15) fixes the values for the evidence vari-\nables E and samples only the nonevidence variables. This guarantees that each event gener-\nated is consistent with the evidence. Not all events are equal, however. Before tallying the\ncountsinthedistribution forthequeryvariable, eacheventisweightedbythelikelihood that\ntheeventaccordstotheevidence, asmeasuredbytheproduct oftheconditionalprobabilities\nforeach evidence variable, given itsparents. Intuitively, events inwhich the actual evidence\nappearsunlikely shouldbegivenlessweight.\nLet us apply the algorithm to the network shown in Figure 14.12(a), with the query\nP(Rain|Cloudy=true,WetGrass=true) and the ordering Cloudy, Sprinkler, Rain, Wet-\nGrass. (Anytopological ordering willdo.) Theprocess goes asfollows: First, theweight w\nissetto1.0. Thenaneventisgenerated:\n1. Cloudy isanevidence variablewithvaluetrue. Therefore, weset\nw \u2190 w\u00d7P(Cloudy=true)= 0.5.\n2. Sprinkler isnotanevidencevariable,sosamplefromP(Sprinkler |Cloudy=true)=\n(cid:16)0.1,0.9(cid:17);suppose thisreturns false.\n3. Similarly, sample from P(Rain|Cloudy=true) = (cid:16)0.8,0.2(cid:17); suppose this returns\ntrue.\n4. WetGrass isanevidence variablewithvalue true. Therefore, weset\nw \u2190 w\u00d7P(WetGrass=true|Sprinkler =false,Rain=true) = 0.45.\nHere WEIGHTED-SAMPLE returns the event [true,false,true,true] with weight 0.45, and\nthisistalliedunderRain=true.\nTo understand why likelihood weighting works, we start by examining the sampling\nprobabilityS\nWS\nforWEIGHTED-SAMPLE. RememberthattheevidencevariablesEarefixed 534 Chapter 14. Probabilistic Reasoning\nfunctionLIKELIHOOD-WEIGHTING(X,e,bn,N)returnsanestimateofP(X|e)\ninputs:X,thequeryvariable\ne,observedvaluesforvariablesE\nbn,aBayesiannetworkspecifyingjointdistributionP(X 1,...,Xn)\nN,thetotalnumberofsamplestobegenerated\nlocalvariables: W,avectorofweightedcountsforeachvalueofX,initiallyzero\nforj =1toN do\nx,w\u2190WEIGHTED-SAMPLE(bn,e)\nW[x]\u2190W[x]+w wherex isthevalueofX inx\nreturnNORMALIZE(W)\nfunctionWEIGHTED-SAMPLE(bn,e)returnsaneventandaweight\nw\u21901;x\u2190aneventwithnelementsinitializedfrome\nforeachvariableXiinX 1,...,Xndo\nifXiisanevidencevariablewithvaluexi ine\nthenw\u2190w \u00d7 P(Xi= xi |parents(Xi))\nelsex[i]\u2190arandomsamplefromP(Xi|parents(Xi))\nreturnx,w\nFigure14.15 Thelikelihood-weightingalgorithmforinferenceinBayesiannetworks. In\nWEIGHTED-SAMPLE, each nonevidence variable is sampled according to the conditional\ndistribution given the values already sampled for the variable\u2019s parents, while a weight is\naccumulatedbasedonthelikelihoodforeachevidencevariable.\nwith values e. We call the nonevidence variables Z (including the query variable X). The\nalgorithm sampleseachvariablein Zgivenitsparentvalues:\n(cid:25)l\nS (z,e)= P(z |parents(Z )). (14.7)\nWS i i\ni=1\nNoticethatParents(Z )canincludebothnonevidencevariablesandevidencevariables. Un-\ni\nlikethepriordistribution P(z),thedistribution S payssomeattentiontotheevidence: the\nWS\nsampled values foreach Z willbeinfluenced byevidence among Z \u2019sancestors. Forexam-\ni i\nple,whensamplingSprinkler thealgorithmpaysattentiontotheevidenceCloudy=true in\nitsparent variable. Ontheotherhand, S pays lessattention tothe evidence than does the\nWS\ntrue posterior distribution P(z|e), because the sampled values for each Z ignore evidence\ni\namongZ \u2019snon-ancestors.5 Forexample,whensampling Sprinkler andRain thealgorithm\ni\nignorestheevidenceinthechildvariableWetGrass=true;thismeansitwillgeneratemany\nsamples with Sprinkler =false and Rain=false despite the fact that the evidence actually\nrulesoutthiscase.\n5 Ideally,wewouldliketouseasamplingdistributionequaltothetrueposteriorP(z|e),totakealltheevidence\ninto account. This cannot be done efficiently, however. If it could, then we could approximate the desired\nprobabilitytoarbitraryaccuracywithapolynomialnumberofsamples.Itcanbeshownthatnosuchpolynomial-\ntimeapproximationschemecanexist. Section14.5. Approximate InferenceinBayesianNetworks 535\nThe likelihood weight w makes up for the difference between the actual and desired\nsampling distributions. The weight for a given sample x, composed from z and e, is the\nproduct of the likelihoods for each evidence variable given its parents (some orall of which\nmaybeamongtheZ s):\ni\n(cid:25)m\nw(z,e) = P(e |parents(E )). (14.8)\ni i\ni=1\nMultiplyingEquations(14.7)and(14.8),weseethattheweightedprobabilityofasamplehas\ntheparticularly convenient form\n(cid:25)l (cid:25)m\nS (z,e)w(z,e) = P(z |parents(Z )) P(e |parents(E ))\nWS i i i i\ni=1 i=1\n= P(z,e) (14.9)\nbecause the two products cover all the variables in the network, allowing us to use Equa-\ntion(14.2)forthejointprobability.\nNow it is easy to show that likelihood weighting estimates are consistent. For any\nparticularvalue xofX,theestimatedposteriorprobability canbecalculated asfollows:\n(cid:12)\nP\u02c6(x|e) = \u03b1 N WS(x,y,e)w(x,y,e) from LIKELIHOOD-WEIGHTING\ny\n(cid:12)\n\u2248 \u03b1(cid:2) S (x,y,e)w(x,y,e) forlarge N\nWS\ny\n(cid:12)\n(cid:2)\n= \u03b1 P(x,y,e) byEquation(14.9)\ny\n= \u03b1(cid:2) P(x,e) = P(x|e).\nHence,likelihood weighting returnsconsistent estimates.\nBecause likelihood weighting uses all the samples generated, it can be much more ef-\nficient than rejection sampling. It will, however, suffer a degradation in performance as the\nnumber of evidence variables increases. This is because most samples will have very low\nweights and hence the weighted estimate will be dominated by the tiny fraction of samples\nthataccordmorethananinfinitesimallikelihoodtotheevidence. Theproblemisexacerbated\nif the evidence variables occur late in the variable ordering, because then the nonevidence\nvariableswillhavenoevidenceintheirparentsandancestorstoguidethegeneration ofsam-\nples. This means the samples will be simulations that bear little resemblance to the reality\nsuggested bytheevidence.\n14.5.2 Inference by Markovchain simulation\nMARKOVCHAIN MarkovchainMonteCarlo(MCMC)algorithmsworkquitedifferentlyfromrejectionsam-\nMONTECARLO\npling and likelihood weighting. Instead of generating each sample from scratch, MCMCal-\ngorithms generate each sample by making a random change to the preceding sample. It is\nthereforehelpfultothinkofanMCMCalgorithm asbeinginaparticular currentstatespeci-\nfyingavalueforeveryvariableandgeneratinga nextstatebymakingrandomchangestothe 536 Chapter 14. Probabilistic Reasoning\ncurrentstate. (IfthisremindsyouofsimulatedannealingfromChapter4orWALKSAT from\nChapter7, thatisbecause both aremembersofthe MCMCfamily.) Herewedescribe apar-\nticularform ofMCMCcalled Gibbssampling, whichisespecially wellsuited forBayesian\nGIBBSSAMPLING\nnetworks. (Otherforms,someofthemsignificantlymorepowerful,arediscussedinthenotes\nattheendofthechapter.) Wewillfirstdescribewhatthealgorithmdoes,thenwewillexplain\nwhyitworks.\nGibbssamplinginBayesian networks\nThe Gibbs sampling algorithm forBayesian networks starts with an arbitrary state (with the\nevidence variables fixed at their observed values) and generates a next state by randomly\nsampling a value for one of the nonevidence variables X . The sampling for X is done\ni i\nconditioned onthecurrent valuesofthevariables intheMarkovblanketofX . (Recallfrom\ni\npage517thattheMarkovblanketofavariableconsistsofitsparents,children,andchildren\u2019s\nparents.) The algorithm therefore wanders randomly around the state space\u2014the space of\npossible complete assignments\u2014flipping one variable at a time, but keeping the evidence\nvariables fixed.\nConsider the query P(Rain|Sprinkler =true,WetGrass=true) applied to the net-\nwork inFigure 14.12(a). Theevidence variables Sprinkler and WetGrass are fixedtotheir\nobserved valuesand thenonevidence variables Cloudy andRain areinitialized randomly\u2014\nlet us say to true and false respectively. Thus, the initial state is [true,true,false,true].\nNowthenonevidence variables aresampledrepeatedly inanarbitrary order. Forexample:\n1. Cloudy is sampled, given the current values of its Markov blanket variables: in this\ncase, we sample from P(Cloudy|Sprinkler =true,Rain=false). (Shortly, we will\nshow how to calculate this distribution.) Suppose the result is Cloudy=false. Then\nthenewcurrentstateis[false,true,false,true].\n2. Rain issampled, giventhecurrent valuesofitsMarkovblanket variables: inthiscase,\nwesamplefrom P(Rain|Cloudy=false,Sprinkler =true,WetGrass=true). Sup-\nposethisyieldsRain=true. Thenewcurrentstateis [false,true,true,true].\nEachstatevisitedduringthisprocessisasamplethatcontributestotheestimateforthequery\nvariable Rain. Iftheprocess visits20stateswhere Rain istrueand60states where Rain is\nfalse, then the answer to the query is NORMALIZE((cid:16)20,60(cid:17)) = (cid:16)0.25,0.75(cid:17). The complete\nalgorithm isshowninFigure14.16.\nWhyGibbssamplingworks\nWe will now show that Gibbs sampling returns consistent estimates for posterior probabil-\nities. The material in this section is quite technical, but the basic claim is straightforward:\nthe sampling process settles into a\u201cdynamic equilibrium\u201d in which the long-run fraction of\ntime spent in each state is exactly proportional to its posterior probability. This remarkable\nTRANSITION propertyfollowsfromthespecifictransitionprobabilitywithwhichtheprocessmovesfrom\nPROBABILITY\none state to another, as defined by the conditional distribution given the Markov blanket of\nthevariablebeingsampled. Section14.5. Approximate InferenceinBayesianNetworks 537\nfunctionGIBBS-ASK(X,e,bn,N)returnsanestimateofP(X|e)\nlocalvariables: N,avectorofcountsforeachvalueofX,initiallyzero\nZ,thenonevidencevariablesinbn\nx,thecurrentstateofthenetwork,initiallycopiedfrome\ninitializexwithrandomvaluesforthevariablesinZ\nforj =1toN do\nforeachZiinZdo\nsetthevalueofZiinxbysamplingfromP(Zi|mb(Zi))\nN[x]\u2190N[x]+1wherex isthevalueofX inx\nreturnNORMALIZE(N)\nFigure14.16 TheGibbssamplingalgorithmforapproximateinferenceinBayesian net-\nworks;thisversioncyclesthroughthevariables,butchoosingvariablesatrandomalsoworks.\nLet q(x \u2192 x(cid:2) ) be the probability that the process makes a transition from state x to\n(cid:2)\nstatex. Thistransitionprobability defineswhatiscalledaMarkovchainonthestatespace.\nMARKOVCHAIN\n(Markov chains also figure prominently in Chapters 15 and 17.) Now suppose that we run\nthe Markov chain for t steps, and let \u03c0 (x) be the probability that the system is in state x at\nt\n(cid:2) (cid:2)\ntime t. Similarly, let \u03c0 (x) be the probability of being in state x at time t + 1. Given\nt+1\n(cid:2)\n\u03c0 (x), wecan calculate \u03c0 (x)by summing, forallstates the system could be inat time t,\nt t+1\n(cid:2)\ntheprobability ofbeinginthatstatetimestheprobability ofmakingthetransition tox:\n(cid:12)\n\u03c0 (x(cid:2) ) = \u03c0 (x)q(x \u2192 x(cid:2) ).\nt+1 t\nx\nSTATIONARY We say that the chain has reached its stationary distribution if \u03c0 =\u03c0 . Let us call this\nDISTRIBUTION t t+1\nstationary distribution \u03c0;itsdefiningequation istherefore\n(cid:12)\n\u03c0(x(cid:2) ) = \u03c0(x)q(x \u2192 x(cid:2) ) forallx(cid:2) . (14.10)\nx\nProvided the transition probability distribution q isergodic\u2014that is, every state isreachable\nERGODIC\nfromeveryotherandtherearenostrictlyperiodiccycles\u2014there isexactlyonedistribution \u03c0\nsatisfying thisequation foranygivenq.\nEquation(14.10)canbereadassayingthattheexpected\u201coutflow\u201dfromeachstate(i.e.,\nits current \u201cpopulation\u201d) is equal to the expected \u201cinflow\u201d from all the states. One obvious\nway to satisfy this relationship is if the expected flowbetween any pair of states is the same\ninbothdirections; thatis,\n\u03c0(x)q(x \u2192 x(cid:2) ) = \u03c0(x(cid:2) )q(x(cid:2) \u2192 x) forallx, x(cid:2) . (14.11)\nWhentheseequations hold,wesaythatq(x \u2192 x(cid:2) )isindetailedbalancewith\u03c0(x).\nDETAILEDBALANCE\nWe can show that detailed balance implies stationarity simply by summing over x in\nEquation(14.11). Wehave\n(cid:12) (cid:12) (cid:12)\n\u03c0(x)q(x \u2192 x(cid:2) )= \u03c0(x(cid:2) )q(x(cid:2) \u2192 x) = \u03c0(x(cid:2) ) q(x(cid:2) \u2192 x)= \u03c0(x(cid:2) )\nx x x 538 Chapter 14. Probabilistic Reasoning\n(cid:2)\nwherethelaststepfollowsbecauseatransition from x isguaranteed tooccur.\nThe transition probability q(x \u2192 x(cid:2) ) defined by the sampling step in GIBBS-ASK is\nactually aspecial case ofthemoregeneral definition ofGibbs sampling, according towhich\neach variable is sampled conditionally on the current values of all the other variables. We\nstart by showing that this general definition of Gibbs sampling satisfies the detailed balance\nequation with a stationary distribution equal to P(x|e), (the true posterior distribution on\nthe nonevidence variables). Then, wesimply observe that, for Bayesian networks, sampling\nconditionallyonallvariablesisequivalenttosamplingconditionallyonthevariable\u2019sMarkov\nblanket(seepage517).\nToanalyze thegeneral Gibbssampler, whichsamples each X inturnwithatransition\ni\nprobability q that conditions on all the other variables, we define X to be these other vari-\ni i\nables (except the evidence variables); their values in the current state are x . If we sample a\ni\n(cid:2)\nnewvaluex forX conditionally onalltheothervariables, including theevidence, wehave\ni i\nq (x \u2192 x(cid:2) ) = q ((x ,x ) \u2192 (x(cid:2) ,x )) = P(x(cid:2)|x ,e).\ni i i i i i i i\nNowweshow thatthetransition probability foreachstepoftheGibbssamplerisindetailed\nbalancewiththetrueposterior:\n\u03c0(x)q (x \u2192 x(cid:2) ) = P(x|e)P(x(cid:2)|x ,e) = P(x ,x |e)P(x(cid:2)|x ,e)\ni i i i i i i\n= P(x |x ,e)P(x |e)P(x(cid:2)|x ,e) (usingthechainruleonthefirstterm)\ni i i i i\n= P(x |x ,e)P(x(cid:2) ,x |e) (usingthechainrulebackward)\ni i i i\n= \u03c0(x(cid:2) )q (x(cid:2) \u2192 x).\ni\nWecanthinkoftheloop\u201cforeachZ inZdo\u201dinFigure14.16asdefiningonelargetransition\ni\nprobability qthatisthesequentialcompositionq \u25e6q \u25e6\u00b7\u00b7\u00b7\u25e6q ofthetransitionprobabilities\n1 2 n\nfor the individual variables. It is easy to show (Exercise 14.19) that if each of q and q has\ni j\n\u03c0 as its stationary distribution, then the sequential composition q \u25e6 q does too; hence the\ni j\ntransition probability q for the whole loop has P(x|e)as its stationary distribution. Finally,\nunless the CPTscontain probabilities of 0 or 1\u2014which can cause the state space to become\ndisconnected\u2014it is easy to see that q is ergodic. Hence, the samples generated by Gibbs\nsamplingwilleventually bedrawnfromthetrueposteriordistribution.\nThe final step is to show how to perform the general Gibbs sampling step\u2014sampling\nX from P(X |x ,e)\u2014in a Bayesian network. Recall from page 517 that avariable isinde-\ni i i\npendent ofallothervariables givenitsMarkovblanket; hence,\nP(x(cid:2)|x\n,e)=\nP(x(cid:2)|mb(X\n)),\ni i i i\nwhere mb(X ) denotes the values of the variables in X \u2019s Markov blanket, MB(X ). As\ni i i\nshowninExercise14.7,theprobabilityofavariablegivenitsMarkovblanketisproportional\ntotheprobability ofthevariablegivenitsparentstimestheprobabilityofeachchildgivenits\nrespective parents:\n(cid:25)\nP(x(cid:2)|mb(X )) = \u03b1P(x(cid:2)|parents(X ))\u00d7 P(y |parents(Y )). (14.12)\ni i i i j j\nYj\u2208Children(Xi)\nHence,toflipeachvariableX conditioned onitsMarkovblanket, thenumberofmultiplica-\ni\ntionsrequiredisequaltothenumberof X \u2019schildren.\ni Section14.6. Relational andFirst-OrderProbability Models 539\nQuality(B ) Quality(B )\n1 2\nHonesty(C ) Kindness(C ) Honesty(C ) Kindness(C )\nQuality(B ) 1 1 2 2\n1\nHonesty(C ) Kindness(C )\n1 1\nRecommendation(C , B ) Recommendation(C , B )\n1 1 2 1\nRecommendation(C , B ) Recommendation(C , B ) Recommendation(C , B )\n1 1 1 2 2 2\n(a) (b)\nFigure 14.17 (a) Bayes net for a single customer C recommending a single book B .\n1 1\nHonest(C )isBoolean,whiletheothervariableshaveintegervaluesfrom1to5. (b)Bayes\n1\nnetwithtwocustomersandtwobooks.\n14.6 RELATIONAL AND FIRST-ORDER PROBABILITY MODELS\nIn Chapter 8, we explained the representational advantages possessed by first-order logic in\ncomparison to propositional logic. First-order logic commits to the existence of objects and\nrelationsamongthemandcanexpressfactsaboutsomeoralloftheobjectsinadomain. This\noftenresultsinrepresentations thatarevastlymoreconcise thantheequivalent propositional\ndescriptions. Now, Bayesian networks are essentially propositional: the set of random vari-\nables is fixed and finite, and each has a fixed domain of possible values. This fact limits the\napplicability of Bayesian networks. If wecan find a wayto combine probability theory with\ntheexpressive poweroffirst-order representations, weexpecttobeabletoincreasedramati-\ncallytherangeofproblemsthatcanbehandled.\nForexample, suppose that an online book retailer would like to provide overall evalu-\nations of products based on recommendations received from its customers. The evaluation\nwill take the form of a posterior distribution over the quality of the book, given the avail-\nable evidence. Thesimplest solution tobase theevaluation ontheaverage recommendation,\nperhaps withavariance determinedbythenumberofrecommendations, butthisfailstotake\nintoaccountthefactthatsomecustomersarekinderthanothersandsomearelesshonestthan\nothers. Kind customers tend to give high recommendations even to fairly mediocre books,\nwhile dishonest customers give very high or very low recommendations for reasons other\nthanquality\u2014forexample, theymightworkforapublisher.6\nFor a single customer C , recommending a single book B , the Bayes net might look\n1 1\nlike the one shown in Figure 14.17(a). (Just as in Section 9.1, expressions with parentheses\nsuchasHonest(C )arejustfancysymbols\u2014inthiscase,fancynamesforrandomvariables.)\n1\n6 Agametheoristwouldadviseadishonestcustomertoavoiddetectionbyoccasionallyrecommendingagood\nbookfromacompetitor.SeeChapter17. 540 Chapter 14. Probabilistic Reasoning\nWithtwocustomers andtwobooks, theBayesnetlooks like the oneinFigure14.17(b). For\nlarger numbers of books and customers, it becomes completely impractical to specify the\nnetworkbyhand.\nFortunately, the network has a lot of repeated structure. Each Recommendation(c,b)\nvariablehasasitsparentsthevariables Honest(c),Kindness(c),andQuality(b). Moreover,\nthe CPTs for all the Recommendation(c,b) variables are identical, as are those for all the\nHonest(c) variables, and so on. The situation seems tailor-made for a first-order language.\nWewouldliketosaysomething like\nRecommendation(c,b) \u223c RecCPT(Honest(c),Kindness(c),Quality(b))\nwith the intended meaning that a customer\u2019s recommendation for a book depends on the\ncustomer\u2019s honesty and kindness and the book\u2019s quality according to some fixed CPT. This\nsectiondevelops alanguage thatletsussayexactlythis,andalotmorebesides.\n14.6.1 Possibleworlds\nRecall from Chapter 13 that a probability model defines a set \u03a9 of possible worlds with\na probability P(\u03c9) for each world \u03c9. For Bayesian networks, the possible worlds are as-\nsignments of values to variables; for the Boolean case in particular, the possible worlds are\nidentical to those of propositional logic. For a first-order probability model, then, it seems\nwe need the possible worlds to be those of first-order logic\u2014that is, a set of objects with\nrelations among them and aninterpretation that maps constant symbols toobjects, predicate\nsymbols to relations, and function symbols to functions on those objects. (See Section 8.2.)\nThemodelalsoneedstodefineaprobability foreachsuchpossibleworld,justasaBayesian\nnetworkdefinesaprobability foreachassignment ofvaluestovariables.\nLetus suppose, fora moment, that wehave figured out how to do this. Then, as usual\n(see page 485), we can obtain the probability of any first-order logical sentence \u03c6 as a sum\noverthepossible worldswhereitistrue:\n(cid:12)\nP(\u03c6) = P(\u03c9). (14.13)\n\u03c9:\u03c6istruein\u03c9\nConditional probabilities P(\u03c6|e)canbe obtained similarly, sowecan, inprinciple, ask any\nquestion we want of our model\u2014e.g., \u201cWhich books are most likely to be recommended\nhighlybydishonest customers?\u201d\u2014and getananswer. Sofar,sogood.\nThere is, however, a problem: the set of first-order models is infinite. We saw this\nexplicitlyinFigure8.4onpage293,whichweshowagaininFigure14.18(top). Thismeans\nthat(1)thesummationinEquation(14.13)couldbeinfeasible,and(2)specifyingacomplete,\nconsistent distribution overaninfinitesetofworldscould beverydifficult.\nSection 14.6.2 explores one approach to dealing with this problem. The idea is to\nborrow not from the standard semantics of first-order logic but from the database seman-\ntics defined in Section 8.2.8 (page 299). The database semantics makes the unique names\nassumption\u2014here, weadoptitfortheconstant symbols. Italsoassumes domainclosure\u2014\nthere are no more objects than those that are named. We can then guarantee a finite set of\npossible worlds by making the set of objects in each world be exactly the set of constant Section14.6. Relational andFirst-OrderProbability Models 541\nR J R J R J R J R J R J\n. . . . . .\n. . .\nR J R J R J R J R J\nR R R R . . . R\nJ J J J J\nFigure14.18 Top:Somemembersofthesetofallpossibleworldsforalanguagewithtwo\nconstantsymbols,RandJ,andonebinaryrelationsymbol,underthestandardsemanticsfor\nfirst-orderlogic. Bottom: thepossibleworldsunderdatabasesemantics. Theinterpretation\noftheconstantsymbolsisfixed,andthereisadistinctobjectforeachconstantsymbol.\nsymbols that are used; as shown in Figure 14.18 (bottom), there is no uncertainty about the\nmappingfromsymbolstoobjectsorabouttheobjectsthatexist. Wewillcallmodelsdefined\nRELATIONAL in this way relational probability models, or RPMs.7 The most significant difference be-\nPROBABILITYMODEL\ntween the semantics of RPMsand the database semantics introduced in Section 8.2.8 is that\nRPMsdo not make the closed-world assumption\u2014obviously, assuming that every unknown\nfactisfalsedoesn\u2019t makesenseinaprobabilistic reasoning system!\nWhentheunderlyingassumptionsofdatabasesemanticsfailtohold,RPMswon\u2019twork\nwell. Forexample,abookretailermightuseanISBN(International StandardBookNumber)\nas a constant symbol to name each book, even though a given \u201clogical\u201d book (e.g., \u201cGone\nWith the Wind\u201d) may have several ISBNs. It would make sense to aggregate recommenda-\ntions across multiple ISBNs, but the retailer may not know for sure which ISBNs are really\nthesamebook. (Notethatwearenotreifying theindividual copiesofthebook,whichmight\nbe necessary for used-book sales, car sales, and so on.) Worse still, each customer is iden-\ntified by a login ID, but a dishonest customer may have thousands of IDs! In the computer\nsecurityfield,thesemultipleIDsarecalledsibylsandtheirusetoconfound areputation sys-\nSIBYL\ntem is called a sibyl attack. Thus, even a simple application in a relatively well-defined,\nSIBYLATTACK\nEXISTENCE online domain involves both existence uncertainty (what are the real books and customers\nUNCERTAINTY\nIDENTITY underlying the observed data) and identity uncertainty (which symbol really refer to the\nUNCERTAINTY\nsameobject). Weneedtobitethebullet anddefineprobability modelsbasedonthestandard\nsemantics of first-order logic, for which the possible worlds vary in the objects they contain\nandinthemappings fromsymbolstoobjects. Section14.6.3showshowtodothis.\n7 ThenamerelationalprobabilitymodelwasgivenbyPfeffer(2000)toaslightlydifferentrepresentation,but\ntheunderlyingideasarethesame. 542 Chapter 14. Probabilistic Reasoning\n14.6.2 Relationalprobability models\nLike first-order logic, RPMshave constant, function, and predicate symbols. (It turns out to\nbe easier to view predicates as functions that return true or false.) We will also assume a\ntypesignatureforeachfunction,thatis,aspecificationofthetypeofeachargumentandthe\nTYPESIGNATURE\nfunction\u2019svalue. Ifthetypeofeachobjectisknown,manyspuriouspossibleworldsareelim-\ninated by this mechanism. For the book-recommendation domain, the types are Customer\nandBook,andthetypesignatures forthefunctions andpredicates areasfollows:\nHonest : Customer \u2192 {true,false}Kindness : Customer \u2192 {1,2,3,4,5}\nQuality :Book \u2192 {1,2,3,4,5}\nRecommendation : Customer \u00d7Book \u2192 {1,2,3,4,5}\nTheconstantsymbolswillbewhatevercustomerandbooknamesappearintheretailer\u2019sdata\nset. Intheexamplegivenearlier(Figure14.17(b)), thesewereC , C andB , B .\n1 2 1 2\nGiven the constants and their types, together with the functions and their type signa-\ntures,therandomvariablesoftheRPMareobtainedbyinstantiating eachfunction witheach\npossible combination of objects: Honest(C ), Quality(B ), Recommendation(C ,B ),\n1 2 1 2\nand so on. These are exactly the variables appearing in Figure 14.17(b). Because each type\nhasonlyfinitelymanyinstances, thenumberofbasicrandomvariables isalsofinite.\nTo complete the RPM, we have to write the dependencies that govern these random\nvariables. Thereisonedependency statement foreachfunction, whereeachargument ofthe\nfunction isalogicalvariable (i.e.,avariablethatranges overobjects, asinfirst-orderlogic):\nHonest(c) \u223c (cid:16)0.99,0.01(cid:17)\nKindness(c) \u223c (cid:16)0.1,0.1,0.2,0.3,0.3(cid:17)\nQuality(b) \u223c (cid:16)0.05,0.2,0.4,0.2,0.15(cid:17)\nRecommendation(c,b) \u223c RecCPT(Honest(c),Kindness(c),Quality(b))\nwhere RecCPT is a separately defined conditional distribution with 2\u00d75\u00d75=50 rows,\neach with 5 entries. The semantics of the RPM can be obtained by instantiating these de-\npendencies for all known constants, giving a Bayesian network (as in Figure 14.17(b)) that\ndefinesajointdistribution overtheRPM\u2019srandomvariables.8\nCONTEXT-SPECIFIC Wecanrefinethemodelbyintroducing a context-specific independencetoreflectthe\nINDEPENDENCE\nfactthatdishonestcustomersignorequalitywhengivingarecommendation; moreover,kind-\nnessplaysnoroleintheirdecisions. Acontext-specific independence allowsavariable tobe\nindependentofsomeofitsparentsgivencertainvaluesofothers;thus,Recommendation(c,b)\nisindependent ofKindness(c)andQuality(b)whenHonest(c)=false:\nRecommendation(c,b) \u223c ifHonest(c) then\nHonestRecCPT(Kindness(c),Quality(b))\nelse(cid:16)0.4,0.1,0.0,0.1,0.4(cid:17) .\n8 Sometechnicalconditions mustbeobserved toguaranteethattheRPMdefinesaproperdistribution. First,\nthedependenciesmustbeacyclic,otherwisetheresultingBayesiannetworkwillhavecyclesandwillnotdefine\naproperdistribution. Second,thedependenciesmustbe well-founded,thatis,therecanbenoinfiniteancestor\nchains,suchasmightarisefromrecursivedependencies. Undersomecircumstances(seeExercise14.6),afixed-\npointcalculationyieldsawell-definedprobabilitymodelforarecursiveRPM. Section14.6. Relational andFirst-OrderProbability Models 543\nFan(C , A ) Fan(C , A ) Author(B )\n1 1 1 2 2\nQuality(B ) Honesty(C ) Kindness(C ) Quality(B )\n1 1 1 2\nRecommendation(C , B ) Recommendation(C , B )\n1 1 2 1\nFigure14.19 FragmentoftheequivalentBayesnetwhenAuthor(B )isunknown.\n2\nThiskindofdependencymaylooklikeanordinaryif\u2013then\u2013elsestatementonaprogramming\nlanguage, but there is a key difference: the inference engine doesn\u2019t necessarily know the\nvalueoftheconditional test!\nWe can elaborate this model in endless ways to make it more realistic. For example,\nsuppose that an honest customer who is a fan of a book\u2019s author always gives the book a 5,\nregardlessofquality:\nRecommendation(c,b) \u223c ifHonest(c)then\nifFan(c,Author(b))thenExactly(5)\nelseHonestRecCPT(Kindness(c),Quality(b))\nelse(cid:16)0.4,0.1,0.0,0.1,0.4(cid:17)\nAgain,theconditional testFan(c,Author(b))isunknown,butifacustomergivesonly5sto\naparticularauthor\u2019sbooksandisnototherwiseespecially kind,thentheposteriorprobability\nthat the customer is afan of that author willbe high. Furthermore, the posterior distribution\nwilltendtodiscount thecustomer\u2019s5sinevaluating thequalityofthatauthor\u2019sbooks.\nInthepreceding example,weimplicitlyassumedthatthevalueofAuthor(b)isknown\nforevery b,but this maynotbethe case. Howcanthesystem reason about whether, say, C\n1\nis a fan of Author(B ) when Author(B ) is unknown? The answer is that the system may\n2 2\nhavetoreason about allpossible authors. Suppose (tokeepthings simple)thattherearejust\ntwo authors, A and A . Then Author(B ) is a random variable with two possible values,\n1 2 2\nA andA ,anditisaparentofRecommendation(C ,B ). Thevariables Fan(C ,A )and\n1 2 1 2 1 1\nFan(C ,A )are parents too. Theconditional distribution for Recommendation(C ,B )is\n1 2 1 2\nthen essentially a multiplexer in which the Author(B ) parent acts as a selector to choose\nMULTIPLEXER 2\nwhich of Fan(C ,A ) and Fan(C ,A ) actually gets to influence the recommendation. A\n1 1 1 2\nfragment of the equivalent Bayes net is shown in Figure 14.19. Uncertainty in the value\nof Author(B ), which affects the dependency structure of the network, is an instance of\n2\nRELATIONAL relational uncertainty.\nUNCERTAINTY\nIn case you are wondering how the system can possibly work out who the author of\nB is: consider the possibility that three other customers are fans of A (and have no other\n2 1\nfavorite authors in common) and all three have given B a 5, even though most other cus-\n2\ntomers find it quite dismal. In that case, it is extremely likely that A is the author of B .\n1 2 544 Chapter 14. Probabilistic Reasoning\nThe emergence of sophisticated reasoning like this from an RPM model of just a few lines\nisanintriguing exampleofhow probabilistic influences spread through thewebofintercon-\nnectionsamongobjectsinthemodel. Asmoredependencies andmoreobjectsareadded,the\npictureconveyed bytheposteriordistribution oftenbecomesclearerandclearer.\nThe next question is how to do inference in RPMs. One approach is to collect the\nevidence and query and the constant symbols therein, construct the equivalent Bayes net,\nand apply any of the inference methods discussed in this chapter. This technique is called\nunrolling. Theobvious drawbackisthattheresulting Bayesnetmaybeverylarge. Further-\nUNROLLING\nmore,iftherearemanycandidate objectsforanunknownrelationorfunction\u2014for example,\ntheunknownauthorofB \u2014thensomevariables inthenetworkmayhavemanyparents.\n2\nFortunately, much can be done to improve on generic inference algorithms. First, the\npresence of repeated substructure in the unrolled Bayes net means that many of the factors\nconstructed during variable elimination (and similar kinds of tables constructed by cluster-\ning algorithms) will be identical; effective caching schemes have yielded speedups of three\nordersofmagnitudeforlargenetworks. Second,inferencemethodsdevelopedtotakeadvan-\ntage ofcontext-specific independence inBayesnets findmanyapplications inRPMs. Third,\nMCMC inference algorithms have some interesting properties when applied to RPMs with\nrelational uncertainty. MCMCworksbysamplingcomplete possible worlds,soineachstate\ntherelational structure iscompletely known. Intheexamplegivenearlier, eachMCMCstate\nwouldspecifythevalueofAuthor(B ),andsotheotherpotential authorsarenolongerpar-\n2\nentsoftherecommendationnodesforB . ForMCMC,then,relationaluncertainty causesno\n2\nincreaseinnetworkcomplexity; instead,theMCMCprocessincludestransitions thatchange\ntherelational structure, andhencethedependency structure, oftheunrolled network.\nAllofthemethodsjustdescribedassumethattheRPMhastobepartiallyorcompletely\nunrolled into a Bayesian network. This is exactly analogous to the method of proposition-\nalization for first-order logical inference. (See page 322.) Resolution theorem-provers and\nlogic programming systems avoid propositionalizing by instantiating the logical variables\nonlyasneededtomaketheinferencegothrough;thatis,theylifttheinferenceprocessabove\nthe level of ground propositional sentences and make each lifted step do the work of many\nground steps. Thesame idea applied inprobabilistic inference. Forexample, inthe variable\nelimination algorithm, alifted factorcanrepresent anentire setofground factors that assign\nprobabilitiestorandomvariablesintheRPM,wherethoserandomvariablesdifferonlyinthe\nconstant symbols usedtoconstruct them. Thedetailsofthis methodarebeyond thescopeof\nthisbook,butreferences aregivenattheendofthechapter.\n14.6.3 Open-universe probability models\nWe argued earlier that database semantics was appropriate for situations in which we know\nexactlythesetofrelevantobjectsthatexistandcanidentifythemunambiguously. (Inpartic-\nular, all observations about an object are correctly associated with the constant symbol that\nnamesit.) Inmanyreal-worldsettings,however,theseassumptionsaresimplyuntenable. We\ngavetheexamplesofmultiple ISBNsandsibyl attacks inthebook-recommendation domain\n(towhichwewillreturninamoment),butthephenomenon isfarmorepervasive: Section14.6. Relational andFirst-OrderProbability Models 545\n\u2022 Avisionsystemdoesn\u2019tknowwhatexists,ifanything,aroundthenextcorner,andmay\nnotknowiftheobjectitseesnowisthesameoneitsawafewminutesago.\n\u2022 Atext-understanding systemdoesnotknowinadvancetheentitiesthatwillbefeatured\nin a text, and must reason about whether phrases such as \u201cMary,\u201d \u201cDr. Smith,\u201d \u201cshe,\u201d\n\u201chiscardiologist,\u201d \u201chismother,\u201dandsoonrefertothesame object.\n\u2022 Anintelligence analyst hunting forspies never knows how many spies there really are\nandcanonlyguesswhethervariouspseudonyms, phonenumbers,andsightingsbelong\ntothesameindividual.\nIn fact, a major part of human cognition seems to require learning what objects exist and\nbeingabletoconnectobservations\u2014which almostnevercomewithuniqueIDsattached\u2014to\nhypothesized objectsintheworld.\nFor these reasons, we need to be able to write so-called open-universe probability\nOPENUNIVERSE\nmodels or OUPMs based on the standard semantics of first-order logic, as illustrated at the\ntop of Figure 14.18. A language for OUPMs provides a way of writing such models easily\nwhile guaranteeing a unique, consistent probability distribution over the infinite space of\npossible worlds.\nThe basic idea is to understand how ordinary Bayesian networks and RPMs manage\nto define a unique probability model and to transfer that insight to the first-order setting. In\nessence, aBayes net generates each possible world, event by event, in the topological order\ndefined bythenetwork structure, whereeach eventisanassignment ofavaluetoavariable.\nAn RPM extends this to entire sets of events, defined by the possible instantiations of the\nlogical variables inagiven predicate orfunction. OUPMsgo further byallowing generative\nsteps that add objects to the possible world under construction, where the number and type\nof objects may depend on the objects that are already in that world. That is, the event being\ngenerated isnottheassignment ofavaluetoavariable, buttheveryexistence ofobjects.\nOnewaytodothisinOUPMsistoaddstatements thatdefineconditional distributions\nover the numbers of objects of various kinds. For example, in the book-recommendation\ndomain, we might want to distinguish between customers (real people) and their login IDs.\nSupposeweexpectsomewherebetween100and10,000distinctcustomers(whomwecannot\nobservedirectly). Wecanexpressthisasapriorlog-normal distribution9 asfollows:\n#Customer \u223c LogNormal[6.9,2.32]().\nWe expect honest customers to have just one ID, whereas dishonest customers might have\nanywherebetween10and1000IDs:\n#LoginID(Owner =c) \u223c ifHonest(c)thenExactly(1)\nelseLogNormal[6.9,2.32]().\nThis statement defines the number of login IDs for a given owner, who is a customer. The\nOwner function is called an origin function because it says where each generated object\nORIGINFUNCTION\ncamefrom. Intheformal semantics of BLOG (asdistinct from first-order logic), thedomain\nelementsineachpossibleworldareactuallygenerationhistories(e.g.,\u201cthefourthloginIDof\ntheseventhcustomer\u201d)ratherthansimpletokens.\n9 AdistributionLogNormal[\u03bc,\u03c32](x)isequivalenttoadistributionN[\u03bc,\u03c32](x)overlog (x).\ne 546 Chapter 14. Probabilistic Reasoning\nSubject to technical conditions of acyclicity and well-foundedness similar to those for\nRPMs, open-universe models of this kind define a unique distribution over possible worlds.\nFurthermore, there exist inference algorithms such that, for every such well-defined model\nand every first-order query, the answer returned approaches the true posterior arbitrarily\nclosely in the limit. There are some tricky issues involved in designing these algorithms.\nFor example, an MCMC algorithm cannot sample directly in the space of possible worlds\nwhen the size of those worlds is unbounded; instead, it samples finite, partial worlds, rely-\ning on the fact that only finitely many objects can be relevant to the query in distinct ways.\nMoreover, transitions must allow formerging two objects into one orsplitting one into two.\n(Details are given in the references at the end of the chapter.) Despite these complications,\nthebasicprincipleestablished inEquation(14.13)stillholds: theprobability ofanysentence\niswelldefinedandcanbecalculated.\nResearchinthisareaisstillatanearlystage, butalready itisbecoming clearthatfirst-\norderprobabilistic reasoning yieldsatremendous increase intheeffectiveness ofAIsystems\nat handling uncertain information. Potential applications include those mentioned above\u2014\ncomputer vision, text understanding, and intelligence analysis\u2014as well asmanyother kinds\nofsensorinterpretation.\n14.7 OTHER APPROACHES TO UNCERTAIN REASONING\nOther sciences (e.g., physics, genetics, and economics) have long favored probability as a\nmodel foruncertainty. In 1819, Pierre Laplace said, \u201cProbability theory is nothing but com-\nmon sense reduced to calculation.\u201d In 1850, James Maxwell said, \u201cThe true logic for this\nworld isthe calculus of Probabilities, which takes account ofthe magnitude ofthe probabil-\nitywhichis,oroughttobe,inareasonable man\u2019smind.\u201d\nGiven this long tradition, it is perhaps surprising that AI has considered many alterna-\ntives to probability. The earliest expert systems of the 1970s ignored uncertainty and used\nstrictlogicalreasoning,butitsoonbecameclearthatthis wasimpracticalformostreal-world\ndomains. The next generation ofexpert systems (especially in medical domains) used prob-\nabilistic techniques. Initial results were promising, but they did not scale up because of the\nexponentialnumberofprobabilities requiredinthefulljointdistribution. (EfficientBayesian\nnetwork algorithms were unknown then.) As a result, probabilistic approaches fell out of\nfavor from roughly 1975 to1988, and avariety ofalternatives toprobability weretried fora\nvarietyofreasons:\n\u2022 One common view is that probability theory is essentially numerical, whereas human\njudgmental reasoning is more \u201cqualitative.\u201d Certainly, we are not consciously aware\nof doing numerical calculations of degrees of belief. (Neither are we aware of doing\nunification, yet we seem to be capable of some kind of logical reasoning.) It might be\nthat we have some kind of numerical degrees of belief encoded directly in strengths\nof connections and activations in our neurons. In that case, the difficulty of conscious\naccesstothosestrengthsisnotsurprising. Oneshouldalso notethatqualitativereason- Section14.7. OtherApproaches toUncertainReasoning 547\ningmechanisms canbebuiltdirectly ontopofprobability theory, sothe\u201cnonumbers\u201d\nargument against probability has little force. Nonetheless, some qualitative schemes\nhave a good deal of appeal in their own right. One of the best studied is default rea-\nsoning,whichtreats conclusions notas\u201cbelieved toacertaindegree,\u201d butas\u201cbelieved\nuntil a better reason is found to believe something else.\u201d Default reasoning is covered\ninChapter12.\n\u2022 Rule-based approaches to uncertainty have also been tried. Such approaches hope to\nbuild on the success of logical rule-based systems, but add a sort of \u201cfudge factor\u201d to\neachruletoaccommodateuncertainty. Thesemethodsweredevelopedinthemid-1970s\nandformedthebasisforalargenumberofexpertsystemsinmedicineandotherareas.\n\u2022 One area that we have not addressed so far is the question of ignorance, as opposed\nto uncertainty. Consider the flipping of a coin. If we know that the coin is fair, then\na probability of 0.5 for heads is reasonable. If we know that the coin is biased, but\nwe do not know which way, then 0.5 for heads is again reasonable. Obviously, the\ntwocasesaredifferent, yettheoutcomeprobability seemsnottodistinguish them. The\nDempster\u2013Shafertheoryusesinterval-valueddegreesofbelieftorepresentanagent\u2019s\nknowledgeoftheprobability ofaproposition.\n\u2022 Probabilitymakesthesameontologicalcommitmentaslogic: thatpropositions aretrue\norfalseintheworld, eveniftheagentisuncertain astowhichisthecase. Researchers\ninfuzzylogichaveproposedanontologythatallowsvagueness: thataproposition can\nbe\u201csortof\u201dtrue. Vaguenessanduncertainty areinfactorthogonal issues.\nThenextthreesubsectionstreatsomeoftheseapproachesinslightlymoredepth. Wewillnot\nprovidedetailed technical material,butwecitereferences forfurtherstudy.\n14.7.1 Rule-based methods foruncertain reasoning\nRule-based systems emerged from early work on practical and intuitive systems for logical\ninference. Logicalsystemsingeneral,andlogicalrule-basedsystemsinparticular,havethree\ndesirable properties:\n\u2022 Locality: In logical systems, whenever we have a rule of the form A \u21d2 B, we can\nLOCALITY\nconcludeB,givenevidenceA,withoutworryingaboutanyotherrules. Inprobabilistic\nsystems,weneedtoconsider alltheevidence.\n\u2022 Detachment: Oncealogicalproofisfoundforaproposition B,theproposition canbe\nDETACHMENT\nusedregardlessofhowitwasderived. Thatis,itcanbe detachedfromitsjustification.\nIndealing with probabilities, on the other hand, the source of the evidence fora belief\nisimportantforsubsequent reasoning.\nTRUTH- \u2022 Truth-functionality: In logic, the truth of complex sentences can be computed from\nFUNCTIONALITY\nthe truth of the components. Probability combination does not work this way, except\nunderstrongglobalindependence assumptions.\nThere have been several attempts to devise uncertain reasoning schemes that retain these\nadvantages. The idea is to attach degrees of belief to propositions and rules and to devise\npurely local schemes for combining and propagating those degrees of belief. The schemes 548 Chapter 14. Probabilistic Reasoning\narealsotruth-functional; forexample,thedegreeofbeliefinA\u2228B isafunctionofthebelief\ninAandthebeliefinB.\nThebadnewsforrule-based systems isthattheproperties of locality, detachment, and\ntruth-functionality are simply not appropriate for uncertain reasoning. Let us look at truth-\nfunctionality first. LetH betheeventthatafaircoinflipcomesupheads,letT betheevent\n1 1\nthat the coin comes up tails on that same flip, and let H be the event that the coin comes\n2\nup heads on a second flip. Clearly, all three events have the same probability, 0.5, and so a\ntruth-functional system must assign the same belief to the disjunction of any two of them.\nBut we can see that the probability of the disjunction depends on the events themselves and\nnotjustontheirprobabilities:\nP(A) P(B) P(A\u2228B)\nP(H ) = 0.5 P(H \u2228H )= 0.50\n1 1 1\nP(H ) = 0.5 P(T ) = 0.5 P(H \u2228T )= 1.00\n1 1 1 1\nP(H ) = 0.5 P(H \u2228H )= 0.75\n2 1 2\nIt gets worse when we chain evidence together. Truth-functional systems have rules of the\nform A %\u2192 B that allow us to compute the belief in B as a function of the belief in the rule\nandthebeliefinA. Bothforward-andbackward-chaining systemscanbedevised. Thebelief\nintheruleisassumedtobeconstantandisusuallyspecifiedbytheknowledgeengineer\u2014for\nexample,asA%\u2192 B.\n0.9\nConsider the wet-grass situation from Figure 14.12(a) (page 529). If we wanted to be\nabletodobothcausalanddiagnostic reasoning, wewouldneedthetworules\nRain %\u2192 WetGrass and WetGrass %\u2192 Rain .\nThesetworulesform afeedback loop: evidence for Rain increases thebelief in WetGrass,\nwhich in turn increases the belief in Rain even more. Clearly, uncertain reasoning systems\nneedtokeeptrackofthepathsalongwhichevidence ispropagated.\nIntercausal reasoning (orexplaining away)isalsotricky. Considerwhathappens when\nwehavethetworules\nSprinkler %\u2192 WetGrass and WetGrass %\u2192 Rain .\nSupposeweseethatthesprinklerison. Chainingforwardthroughourrules,thisincreasesthe\nbelief that the grass willbe wet, which in turn increases the belief that it is raining. But this\nis ridiculous: the fact that the sprinkler is on explains awaythe wetgrass and should reduce\nthebeliefinrain. Atruth-functional systemactsasifitalsobelievesSprinkler %\u2192 Rain.\nGiven these difficulties, how can truth-functional systems be made useful in practice?\nThe answer lies in restricting the task and in carefully engineering the rule base so that un-\ndesirable interactions do not occur. The most famous example of a truth-functional system\nCERTAINTYFACTOR\nforuncertain reasoning isthe certaintyfactors model,whichwasdeveloped forthe MYCIN\nmedical diagnosis program and was widely used in expert systems of the late 1970s and\n1980s. Almostallusesofcertainty factorsinvolved rulesetsthatwereeitherpurelydiagnos-\ntic (as in MYCIN) or purely causal. Furthermore, evidence was entered only at the \u201croots\u201d\noftheruleset, andmostrulesetsweresingly connected. Heckerman(1986) hasshownthat, Section14.7. OtherApproaches toUncertainReasoning 549\nunderthesecircumstances, aminorvariationoncertainty-factor inferencewasexactlyequiv-\nalenttoBayesianinferenceonpolytrees. Inothercircumstances, certaintyfactorscouldyield\ndisastrously incorrect degrees of belief through overcounting of evidence. As rule sets be-\ncamelarger, undesirable interactions betweenrulesbecamemorecommon,andpractitioners\nfoundthatthecertaintyfactorsofmanyotherruleshadtobe \u201ctweaked\u201dwhennewruleswere\nadded. Forthesereasons, Bayesiannetworkshavelargelysupplanted rule-based methodsfor\nuncertain reasoning.\n14.7.2 Representing ignorance: Dempster\u2013Shafer theory\nDEMPSTER\u2013SHAFER The Dempster\u2013Shafer theory is designed to deal with the distinction between uncertainty\nTHEORY\nand ignorance. Rather than computing the probability of a proposition, it computes the\nprobability that the evidence supports the proposition. This measure of belief is called a\nbelieffunction,writtenBel(X).\nBELIEFFUNCTION\nWe return to coin flipping for an example of belief functions. Suppose you pick a\ncoin from a magician\u2019s pocket. Given that the coin might or might not be fair, what belief\nshould you ascribe to the event that it comes up heads? Dempster\u2013Shafer theory says that\nbecause you have no evidence either way, you have to say that the belief Bel(Heads) = 0\nand also that Bel(\u00acHeads) = 0. This makes Dempster\u2013Shafer reasoning systems skeptical\nin a way that has some intuitive appeal. Now suppose you have an expert at your disposal\nwho testifies with 90% certainty that the coin is fair (i.e., he is 90% sure that P(Heads) =\n0.5). Then Dempster\u2013Shafer theory gives Bel(Heads) = 0.9 \u00d7 0.5 = 0.45 and likewise\nBel(\u00acHeads) = 0.45. Thereisstilla10percentage point\u201cgap\u201dthatisnotaccounted forby\ntheevidence.\nThe mathematical underpinnings of Dempster\u2013Shafer theory have a similar flavor to\nthose of probability theory; the main difference is that, instead of assigning probabilities\nto possible worlds, the theory assigns masses to sets of possible world, that is, to events.\nMASS\nThe masses still must add to 1 over all possible events. Bel(A) is defined to be the sum of\nmasses for all events that are subsets of (i.e., that entail) A, including A itself. With this\ndefinition,Bel(A)andBel(\u00acA)sumtoatmost1,andthegap\u2014theintervalbetweenBel(A)\nand1\u2212Bel(\u00acA)\u2014isofteninterpreted asbounding theprobability of A.\nAswithdefaultreasoning,thereisaprobleminconnectingbeliefstoactions. Whenever\nthere is a gap in the beliefs, then a decision problem can be defined such that a Dempster\u2013\nShafer system is unable to make a decision. In fact, the notion of utility in the Dempster\u2013\nShafer model is not yet well understood because the meanings of masses and beliefs them-\nselves have yet tobeunderstood. Pearl(1988) hasargued thatBel(A)should beinterpreted\nnot as adegree of belief in Abut as the probability assigned to all the possible worlds (now\ninterpreted as logical theories) in which A is provable. While there are cases in which this\nquantity mightbeofinterest, itisnotthesameastheprobability thatAistrue.\nABayesiananalysisofthecoin-flippingexamplewouldsuggestthatnonewformalism\nisnecessary tohandlesuchcases. Themodelwouldhavetwovariables: theBias ofthecoin\n(anumberbetween0and1,where0isacointhatalwaysshowstailsand1acointhatalways\nshows heads) and the outcome of the next Flip. The prior probability distribution for Bias 550 Chapter 14. Probabilistic Reasoning\nwouldreflectourbeliefsbasedonthesourceofthecoin(themagician\u2019spocket): somesmall\nprobability that it is fair and some probability that it is heavily biased toward heads or tails.\nTheconditional distribution P(Flip|Bias)simplydefineshowthebiasoperates. IfP(Bias)\nissymmetricabout0.5,thenourpriorprobability fortheflipis\n(cid:26)\n1\nP(Flip=heads) = P(Bias=x)P(Flip=heads|Bias=x)dx = 0.5.\n0\nThis is the same prediction as if we believe strongly that the coin is fair, but that does not\nmean that probability theory treats the two situations identically. The difference arises after\ntheflipsincomputing theposteriordistribution for Bias. Ifthecoincamefrom abank, then\nseeing itcomeupheads threetimesrunning wouldhavealmost noeffectonourstrong prior\nbelief in its fairness; but if the coin comes from the magician\u2019s pocket, the same evidence\nwillleadtoastrongerposterior beliefthatthecoinisbiased towardheads. Thus,aBayesian\napproach expresses our\u201cignorance\u201d in termsofhow ourbeliefs would change in theface of\nfutureinformation gathering.\n14.7.3 Representing vagueness: Fuzzy sets andfuzzy logic\nFuzzy set theory is a means of specifying how well an object satisfies a vague description.\nFUZZYSETTHEORY\n(cid:2) (cid:2)(cid:2)\nFor example, consider the proposition \u201cNate is tall.\u201d Is this true if Nate is 5 10 ? Most\npeople would hesitate to answer \u201ctrue\u201d or \u201cfalse,\u201d preferring to say, \u201csort of.\u201d Note that this\nisnot a question of uncertainty about theexternal world\u2014we aresure ofNate\u2019s height. The\nissueisthatthelinguisticterm\u201ctall\u201ddoesnotrefertoasharpdemarcationofobjectsintotwo\nclasses\u2014there are degrees of tallness. For this reason, fuzzy set theory is not a method for\nuncertain reasoning atall. Rather, fuzzy settheory treats Tall asafuzzy predicate andsays\nthat the truth value of Tall(Nate) is a number between 0 and 1, rather than being just true\nor false. The name \u201cfuzzy set\u201d derives from the interpretation of the predicate as implicitly\ndefiningasetofitsmembers\u2014asetthatdoesnothavesharpboundaries.\nFuzzylogicisamethodforreasoning withlogicalexpressions describing membership\nFUZZYLOGIC\nin fuzzy sets. For example, the complex sentence Tall(Nate)\u2227Heavy(Nate) has a fuzzy\ntruth value that is a function of the truth values of its components. The standard rules for\nevaluating thefuzzytruth, T,ofacomplexsentenceare\nT(A\u2227B)= min(T(A),T(B))\nT(A\u2228B)= max(T(A),T(B))\nT(\u00acA) = 1\u2212T(A).\nFuzzy logic is therefore a truth-functional system\u2014a fact that causes serious difficulties.\nForexample, suppose that T(Tall(Nate))=0.6andT(Heavy(Nate))=0.4. Thenwehave\nT(Tall(Nate) \u2227 Heavy(Nate))=0.4, which seems reasonable, but we also get the result\nT(Tall(Nate)\u2227\u00acTall(Nate))=0.4, which does not. Clearly, the problem arises from the\ninabilityofatruth-functionalapproachtotakeintoaccountthecorrelationsoranticorrelations\namongthecomponent propositions.\nFuzzycontrolisamethodologyforconstructingcontrolsystemsinwhichthemapping\nFUZZYCONTROL\nbetween real-valued input and output parameters is represented by fuzzy rules. Fuzzy con-\ntrolhasbeenverysuccessful incommercial products suchas automatic transmissions, video Section14.8. Summary 551\ncameras, and electric shavers. Critics (see, e.g., Elkan, 1993) argue that these applications\nare successful because they have small rule bases, no chaining of inferences, and tunable\nparameters that can be adjusted to improve the system\u2019s performance. Thefact that they are\nimplemented with fuzzy operators might be incidental to their success; the key is simply to\nprovideaconcise andintuitivewaytospecify asmoothlyinterpolated, real-valued function.\nTherehavebeenattemptstoprovideanexplanation offuzzylogicintermsofprobabil-\nitytheory. Oneideaistoviewassertionssuchas\u201cNateisTall\u201dasdiscreteobservations made\nconcerningacontinuoushiddenvariable,Nate\u2019sactual Height. Theprobabilitymodelspeci-\nfiesP(ObserversaysNateistall | Height),perhapsusingaprobitdistributionasdescribed\non page 522. A posterior distribution over Nate\u2019s height can then be calculated in the usual\nway,forexample,ifthemodelispartofahybridBayesiannetwork. Suchanapproach isnot\ntruth-functional, ofcourse. Forexample,theconditional distribution\nP(ObserversaysNateistallandheavy |Height,Weight)\nallows for interactions between height and weight in the causing of the observation. Thus,\nsomeone who iseight feet tall and weighs 190 pounds is very unlikely tobe called \u201ctall and\nheavy,\u201deventhough\u201ceightfeet\u201dcountsas\u201ctall\u201dand\u201c190pounds\u201dcounts as\u201cheavy.\u201d\nFuzzy predicates can also be given a probabilistic interpretation in terms of random\nsets\u2014that is, random variables whosepossible values are sets of objects. Forexample, Tall\nRANDOMSET\nis a random set whose possible values are sets of people. The probability P(Tall =S ),\n1\nwhere S is some particular set of people, is the probability that exactly that set would be\n1\nidentified as \u201ctall\u201d by an observer. Then the probability that \u201cNate is tall\u201d is the sum of the\nprobabilities ofallthesetsofwhichNateisamember.\nBoth the hybrid Bayesian network approach and the random sets approach appear to\ncapture aspects offuzziness without introducing degrees oftruth. Nonetheless, there remain\nmanyopenissuesconcerning theproperrepresentation oflinguisticobservations andcontin-\nuousquantities\u2014issues thathavebeenneglected bymostoutside thefuzzycommunity.\n14.8 SUMMARY\nThischapterhasdescribedBayesiannetworks,awell-developedrepresentationforuncertain\nknowledge. Bayesian networks play a role roughly analogous to that of propositional logic\nfordefiniteknowledge.\n\u2022 A Bayesian network is a directed acyclic graph whose nodes correspond to random\nvariables;eachnodehasaconditional distribution forthe node,givenitsparents.\n\u2022 Bayesiannetworksprovideaconcisewaytorepresent conditionalindependencerela-\ntionships inthedomain.\n\u2022 ABayesian network specifies afull joint distribution; each joint entry isdefined asthe\nproduct of the corresponding entries in the local conditional distributions. A Bayesian\nnetworkisoftenexponentially smallerthananexplicitlyenumerated jointdistribution.\n\u2022 Many conditional distributions can be represented compactly by canonical families of 552 Chapter 14. Probabilistic Reasoning\ndistributions. HybridBayesian networks,whichincludebothdiscreteandcontinuous\nvariables, useavarietyofcanonical distributions.\n\u2022 Inference in Bayesian networks means computing the probability distribution of a set\nof query variables, given a set of evidence variables. Exact inference algorithms, such\nasvariable elimination, evaluate sumsofproducts ofconditional probabilities as effi-\ncientlyaspossible.\n\u2022 In polytrees (singly connected networks), exact inference takes time linear in the size\nofthenetwork. Inthegeneralcase,theproblem isintractable.\n\u2022 Stochasticapproximation techniques suchaslikelihoodweightingandMarkovchain\nMonteCarlocan give reasonable estimates ofthe true posterior probabilities in anet-\nworkandcancopewithmuchlargernetworksthancanexactalgorithms.\n\u2022 Probabilitytheorycanbecombinedwithrepresentational ideasfromfirst-orderlogicto\nproduce very powerful systems forreasoning under uncertainty. Relational probabil-\nity models (RPMs) include representational restrictions that guarantee a well-defined\nprobabilitydistributionthatcanbeexpressedasanequivalentBayesiannetwork. Open-\nuniverseprobabilitymodelshandleexistenceandidentityuncertainty,definingprob-\nabiltydistributions overtheinfinitespaceoffirst-orderpossible worlds.\n\u2022 Variousalternative systemsforreasoning underuncertainty havebeensuggested. Gen-\nerallyspeaking, truth-functionalsystemsarenotwellsuitedforsuchreasoning.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nThe use of networks to represent probabilistic information began early in the 20th century,\nwith the work of Sewall Wright on the probabilistic analysis of genetic inheritance and an-\nimal growth factors (Wright, 1921, 1934). I. J. Good (1961), in collaboration with Alan\nTuring, developed probabilistic representations and Bayesian inference methods that could\nbe regarded as a forerunner of modern Bayesian networks\u2014although the paper is not often\ncitedinthiscontext.10 Thesamepaperistheoriginal sourceforthenoisy-OR model.\nThe influence diagram representation for decision problems, which incorporated a\nDAG representation for random variables, was used in decision analysis in the late 1970s\n(see Chapter 16), but only enumeration was used for evaluation. Judea Pearl developed the\nmessage-passing methodforcarryingoutinferenceintreenetworks(Pearl,1982a)andpoly-\ntree networks (Kim and Pearl, 1983) and explained the importance of causal rather than di-\nagnostic probability models, incontrast tothecertainty-factor systemstheninvogue.\nThe first expert system using Bayesian networks was CONVINCE (Kim, 1983). Early\napplicationsinmedicineincludedtheMUNINsystemfordiagnosingneuromusculardisorders\n(Andersen etal.,1989) andthe PATHFINDER system forpathology (Heckerman, 1991). The\nCPCS system (Pradhan et al., 1994) is a Bayesian network for internal medicine consisting\n10 I.J.GoodwaschiefstatisticianforTuring\u2019scode-breakingteaminWorldWarII.In2001: ASpaceOdyssey\n(Clarke,1968a),GoodandMinskyarecreditedwithmakingthebreakthroughthatledtothedevelopmentofthe\nHAL9000computer. Bibliographical andHistorical Notes 553\nof 448 nodes, 906 links and 8,254 conditional probability values. (The front cover shows a\nportionofthenetwork.)\nApplications in engineering include the Electric Power Research Institute\u2019s work on\nmonitoring power generators (Morjaria et al., 1995), NASA\u2019s work on displaying time-\ncriticalinformationatMissionControlinHouston(HorvitzandBarry,1995),andthegeneral\nfieldofnetworktomography,whichaimstoinferunobserved localproperties ofnodesand\nlinks in the Internet from observations of end-to-end message performance (Castro et al.,\n2004). Perhaps the most widely used Bayesian network systems have been the diagnosis-\nand-repairmodules(e.g.,thePrinterWizard)inMicrosoft Windows(BreeseandHeckerman,\n1996) and the Office Assistant in Microsoft Office (Horvitz et al., 1998). Another impor-\ntant application area is biology: Bayesian networks have been used for identifying human\ngenesbyreferencetomousegenes(Zhangetal.,2003),inferringcellularnetworksFriedman\n(2004), and manyother tasks in bioinformatics. Wecould go on, but instead we\u2019ll refer you\ntoPourret etal.(2008), a400-page guidetoapplications ofBayesiannetworks.\nRossShachter(1986),workingintheinfluencediagramcommunity,developedthefirst\ncomplete algorithm for general Bayesian networks. His method was based on goal-directed\nreduction ofthenetworkusing posterior-preserving transformations. Pearl(1986) developed\naclusteringalgorithmforexactinferenceingeneralBayesiannetworks,utilizingaconversion\nto a directed polytree of clusters in which message passing was used to achieve consistency\nover variables shared between clusters. A similar approach, developed by the statisticians\nDavid Spiegelhalter and Steffen Lauritzen (Lauritzen and Spiegelhalter, 1988), is based on\nconversion to an undirected form of graphical model called a Markov network. This ap-\nMARKOVNETWORK\nproach isimplemented in the HUGIN system, anefficient and widely used tool foruncertain\nreasoning(Andersenetal.,1989). Boutilieretal.(1996)showhowtoexploitcontext-specific\nindependence inclustering algorithms.\nThe basic idea of variable elimination\u2014that repeated computations within the overall\nsum-of-productsexpressioncanbeavoidedbycaching\u2014appearedinthesymbolicprobabilis-\ntic inference (SPI)algorithm (Shachter et al., 1990). Theelimination algorithm wedescribe\nis closest to that developed by Zhang and Poole (1994). Criteria for pruning irrelevant vari-\nablesweredeveloped byGeigeretal.(1990)andbyLauritzen etal.(1990); thecriterion we\ngiveisasimplespecialcaseofthese. Dechter(1999)showshowthevariableeliminationidea\nNONSERIALDYNAMIC isessentiallyidenticaltononserialdynamicprogramming(BerteleandBrioschi,1972),an\nPROGRAMMING\nalgorithmic approach thatcanbeapplied tosolve arange ofinference problems inBayesian\nnetworks\u2014for example, finding the most likely explanation for a set of observations. This\nconnectsBayesiannetworkalgorithmstorelatedmethodsforsolvingCSPsandgivesadirect\nmeasureofthecomplexityofexactinferenceintermsofthetreewidthofthenetwork. Wexler\nand Meek (2009) describe a method of preventing exponential growth in the size of factors\ncomputed invariable elimination; theiralgorithm breaks downlarge factors into products of\nsmallerfactorsandsimultaneously computesanerrorbound fortheresultingapproximation.\nThe inclusion of continuous random variables in Bayesian networks was considered\nby Pearl (1988) and Shachter and Kenley (1989); these papers discussed networks contain-\ning only continuous variables with linear Gaussian distributions. The inclusion of discrete\nvariables has been investigated by Lauritzen and Wermuth (1989) and implemented in the 554 Chapter 14. Probabilistic Reasoning\ncHUGIN system (Olesen, 1993). Further analysis of linear Gaussian models, with connec-\ntionstomanyothermodelsusedinstatistics, appearsinRoweisandGhahramani(1999)The\nprobit distribution is usually attributed to Gaddum (1933) and Bliss (1934), although it had\nbeen discovered several times in the 19th century. Bliss\u2019s work was expanded considerably\nbyFinney(1947). Theprobit hasbeenusedwidelyformodeling discrete choicephenomena\nandcanbeextended tohandlemorethantwochoices(Daganzo,1979). Thelogitmodelwas\nintroduced by Berkson (1944); initially much derided, it eventually became more popular\nthantheprobitmodel. Bishop(1995)givesasimplejustification foritsuse.\nCooper(1990)showedthatthegeneralproblemofinferenceinunconstrained Bayesian\nnetworks is NP-hard, and Paul Dagum and Mike Luby (1993) showed the corresponding\napproximation problem to be NP-hard. Space complexity is also a serious problem in both\nclustering andvariableelimination methods. Themethodofcutsetconditioning,whichwas\ndeveloped forCSPsin Chapter 6, avoids the construction of exponentially large tables. In a\nBayesian network, a cutset is a set of nodes that, when instantiated, reduces the remaining\nnodes to a polytree that can be solved in linear time and space. The query is answered by\nsummingoveralltheinstantiations ofthecutset, sotheoverallspace requirement isstilllin-\near(Pearl, 1988). Darwiche (2001) describes a recursive conditioning algorithm that allows\nacompleterangeofspace\/time tradeoffs.\nThe development of fast approximation algorithms for Bayesian network inference is\na very active area, with contributions from statistics, computer science, and physics. The\nrejection sampling method is a general technique that is long known to statisticians; it was\nfirst applied to Bayesian networks by Max Henrion (1988), who called it logic sampling.\nLikelihood weighting, which was developed by Fung and Chang (1989) and Shachter and\nPeot (1989), is an example of the well-known statistical method of importance sampling.\nCheng and Druzdzel (2000) describe an adaptive version oflikelihood weighting that works\nwellevenwhentheevidence hasverylowpriorlikelihood.\nMarkovchain Monte Carlo(MCMC)algorithms began withthe Metropolis algorithm,\ndue to Metropolis et al. (1953), which was also the source of the simulated annealing algo-\nrithm described inChapter4. TheGibbssamplerwasdevised byGemanandGeman(1984)\nfor inference in undirected Markov networks. The application of MCMC to Bayesian net-\nworksisduetoPearl(1987). ThepaperscollectedbyGilks etal.(1996)coverawidevariety\nof applications of MCMC, several of which were developed in the well-known BUGS pack-\nage(Gilksetal.,1994).\nTherearetwoveryimportant familiesofapproximation methods thatwedidnotcover\nVARIATIONAL in the chapter. The first is the family of variational approximation methods, which can be\nAPPROXIMATION\nused to simplify complex calculations of all kinds. The basic idea is to propose a reduced\nversion of the original problem that is simple to work with, but that resembles the original\nproblem as closely as possible. The reduced problem is described by some variational pa-\nVARIATIONAL rameters \u03bb that are adjusted to minimize a distance function D between the original and\nPARAMETER\nthe reduced problem, often by solving the system of equations \u2202D\/\u2202\u03bb=0. In many cases,\nstrict upper and lower bounds can be obtained. Variational methods have long been used in\nstatistics (Rustagi, 1976). In statistical physics, the mean-field method is a particular vari-\nMEANFIELD\national approximation in which the individual variables making up the model are assumed Bibliographical andHistorical Notes 555\nto be completely independent. This idea was applied to solve large undirected Markov net-\nworks (Peterson and Anderson, 1987; Parisi, 1988). Saul et al. (1996) developed the math-\nematical foundations for applying variational methods to Bayesian networks and obtained\naccuratelower-boundapproximations forsigmoidnetworks withtheuseofmean-fieldmeth-\nods. Jaakkola and Jordan (1996) extended the methodology to obtain both lower and upper\nbounds. Since these early papers, variational methods have been applied to many specific\nfamilies ofmodels. Theremarkable paper byWainwright andJordan (2008) provides auni-\nfyingtheoretical analysis oftheliterature onvariational methods.\nA second important family of approximation algorithms is based on Pearl\u2019s polytree\nmessage-passing algorithm (1982a). This algorithm can be applied to general networks, as\nsuggested byPearl(1988). Theresults mightbeincorrect, orthealgorithm might failto ter-\nminate, but in many cases, the values obtained are close to the true values. Little attention\nBELIEF was paid to this so-called belief propagation (orBP)approach until McEliece et al. (1998)\nPROPAGATION\nobserved that message passing in a multiply connected Bayesian network was exactly the\ncomputation performed by the turbo decoding algorithm (Berrou et al., 1993), which pro-\nTURBODECODING\nvidedamajorbreakthrough inthedesignofefficienterror-correcting codes. Theimplication\nisthatBPisbothfastandaccurateontheverylargeandveryhighlyconnectednetworksused\nfordecoding andmighttherefore beusefulmoregenerally. Murphyetal.(1999)presented a\npromising empirical study of BP\u2019s performance, and Weiss and Freeman (2001) established\nstrongconvergence resultsforBPonlinearGaussiannetworks. Weiss(2000b)showshowan\napproximationcalledloopybeliefpropagationworks,andwhentheapproximationiscorrect.\nYedidia et al. (2005) made further connections between loopy propagation and ideas from\nstatistical physics.\nTheconnection betweenprobability andfirst-order languages wasfirststudied byCar-\nnap(1950). Gaifman(1964)andScottandKrauss(1966)definedalanguageinwhichproba-\nbilities could beassociated withfirst-ordersentences and forwhichmodelswereprobability\nmeasures on possible worlds. Within AI, this idea was developed for propositional logic\nby Nilsson (1986) and for first-order logic by Halpern (1990). The first extensive inves-\ntigation of knowledge representation issues in such languages was carried out by Bacchus\n(1990). Thebasicideaisthateachsentence intheknowledgebaseexpressed aconstraint on\nthe distribution over possible worlds; one sentence entails another if it expresses a stronger\nconstraint. For example, the sentence \u2200x P(Hungry(x)) > 0.2 rules out distributions\nin which any object is hungry with probability less than 0.2; thus, it entails the sentence\n\u2200x P(Hungry(x)) > 0.1. It turns out that writing a consistent set of sentences in these\nlanguages is quite difficult and constructing a unique probability model nearly impossible\nunless oneadopts therepresentation approach ofBayesian networks bywritingsuitable sen-\ntencesaboutconditional probabilities.\nBeginning in the early 1990s, researchers working on complex applications noticed\ntheexpressivelimitationsofBayesiannetworksanddevelopedvariouslanguages forwriting\n\u201ctemplates\u201dwithlogicalvariables,fromwhichlargenetworkscouldbeconstructed automat-\nically for each problem instance (Breese, 1992; Wellman et al., 1992). The most important\nsuch language was BUGS (Bayesian inference Using Gibbs Sampling) (Gilks et al., 1994),\nINDEXEDRANDOM whichcombinedBayesiannetworkswiththeindexedrandomvariablenotationcommonin\nVARIABLE 556 Chapter 14. Probabilistic Reasoning\nstatistics. (InBUGS,anindexedrandomvariablelookslikeX[i],whereihasadefinedinteger\nrange.) TheselanguagesinheritedthekeypropertyofBayesiannetworks: everywell-formed\nknowledgebasedefinesaunique,consistentprobabilitymodel. Languageswithwell-defined\nsemantics based on unique names and domain closure drew on the representational capa-\nbilities of logic programming (Poole, 1993; Sato and Kameya, 1997; Kersting et al., 2000)\nand semantic networks (Koller and Pfeffer, 1998; Pfeffer, 2000). Pfeffer (2007) went on to\ndevelop IBAL,whichrepresents first-order probability models asprobabilistic programs ina\nprogramming language extended with a randomization primitive. Another important thread\nwas the combination of relational and first-order notations with (undirected) Markov net-\nworks (Taskar et al., 2002; Domingos and Richardson, 2004), where the emphasis has been\nlessonknowledgerepresentation andmoreonlearning fromlargedatasets.\nInitially,inferenceinthesemodelswasperformedbygeneratinganequivalentBayesian\nnetwork. Pfeffer et al. (1999) introduced a variable elimination algorithm that cached each\ncomputed factor for reuse by later computations involving the same relations but different\nobjects, thereby realizing some of the computational gains of lifting. The first truly lifted\ninference algorithm wasalifted form of variable elimination described by Poole (2003) and\nsubsequently improved by de Salvo Braz et al. (2007). Further advances, including cases\nwherecertainaggregateprobabilitiescanbecomputedinclosedform,aredescribedbyMilch\netal.(2008)andKisynskiandPoole(2009). PasulaandRussell(2001)studiedtheapplication\nof MCMC to avoid building the complete equivalent Bayes net in cases of relational and\nidentity uncertainty. Getoor and Taskar (2007) collect many important papers on first-order\nprobability modelsandtheiruseinmachinelearning.\nProbabilistic reasoning about identity uncertainty has two distinct origins. In statis-\ntics, theproblem of record linkagearises whendata records donotcontain standard unique\nRECORDLINKAGE\nidentifiers\u2014for example, various citations of this book might name its first author \u201cStuart\nRussell\u201d or\u201cS. J. Russell\u201d oreven \u201cStewart Russle,\u201d and other authors may use the some of\nthe same names. Literally hundreds of companies exist solely to solve record linkage prob-\nlems in financial, medical, census, and other data. Probabilistic analysis goes back to work\nby Dunn (1946); the Fellegi\u2013Sunter model (1969), which is essentially naive Bayes applied\nto matching, still dominates current practice. The second origin for work on identity uncer-\ntainty is multitarget tracking (Sittler, 1964), which we cover in Chapter 15. For most of its\nhistory, work in symbolic AI assumed erroneously that sensors could supply sentences with\nuniqueidentifiersforobjects. Theissuewasstudiedinthecontextoflanguageunderstanding\nby Charniak and Goldman (1992) and in the context of surveillance by (Huang and Russell,\n1998) and Pasula et al. (1999). Pasula et al. (2003) developed a complex generative model\nfor authors, papers, and citation strings, involving both relational and identity uncertainty,\nand demonstrated high accuracy for citation information extraction. The first formally de-\nfined language for open-universe probability models was BLOG (Milch et al., 2005), which\ncamewith acomplete (albeit slow) MCMCinference algorithm forallwell-defined mdoels.\n(The program code faintly visible on the front cover of this book is part of a BLOG model\nfordetecting nuclear explosions from seismic signals aspart ofthe UNComprehensive Test\nBan Treaty verification regime.) Laskey (2008) describes another open-universe modeling\nlanguage calledmulti-entityBayesiannetworks. Bibliographical andHistorical Notes 557\nAs explained in Chapter 13, early probabilistic systems fell out of favor in the early\n1970s, leaving a partial vacuum to be filled by alternative methods. Certainty factors were\ninvented foruseinthemedicalexpertsystem MYCIN (Shortliffe, 1976),whichwasintended\nboth as an engineering solution and as a model of human judgment under uncertainty. The\ncollection Rule-Based Expert Systems (Buchanan and Shortliffe, 1984) provides a complete\noverview of MYCIN and its descendants (see also Stefik, 1995). David Heckerman (1986)\nshowed that a slightly modified version of certainty factor calculations gives correct proba-\nbilistic results in some cases, but results in serious overcounting of evidence in other cases.\nThePROSPECTOR expertsystem(Dudaetal.,1979)usedarule-basedapproachinwhichthe\nruleswerejustifiedbya(seldom tenable)globalindependence assumption.\nDempster\u2013Shafer theoryoriginates withapaperbyArthurDempster(1968) proposing\nageneralizationofprobabilitytointervalvaluesandacombinationruleforusingthem. Later\nworkbyGlennShafer(1976)ledtotheDempster-Shafertheory\u2019sbeingviewedasacompet-\ning approach to probability. Pearl (1988) and Ruspini et al. (1992) analyze the relationship\nbetweentheDempster\u2013Shafer theoryandstandardprobability theory.\nFuzzysetsweredevelopedbyLotfiZadeh(1965)inresponsetotheperceiveddifficulty\nof providing exact inputs to intelligent systems. The text by Zimmermann (2001) provides\na thorough introduction to fuzzy set theory; papers on fuzzy applications are collected in\nZimmermann (1999). As we mentioned in the text, fuzzy logic has often been perceived\nincorrectlyasadirectcompetitortoprobabilitytheory,whereasinfactitaddressesadifferent\nsetofissues. Possibilitytheory(Zadeh,1978)wasintroducedtohandleuncertaintyinfuzzy\nPOSSIBILITYTHEORY\nsystems and has much in common with probability. Dubois and Prade (1994) survey the\nconnections betweenpossibility theoryandprobability theory.\nThe resurgence of probability depended mainly on Pearl\u2019s development of Bayesian\nnetworksasamethodforrepresentingandusingconditional independence information. This\nresurgence did not comewithout afight; PeterCheeseman\u2019s (1985) pugnacious \u201cIn Defense\nofProbability\u201d andhislaterarticle\u201cAnInquiry intoComputerUnderstanding\u201d (Cheeseman,\n1988, with commentaries) give something of the flavor of the debate. Eugene Charniak\nhelped present the ideas to AI researchers with a popular article, \u201cBayesian networks with-\nout tears\u201d11 (1991), and book (1993). The book by Dean and Wellman (1991) also helped\nintroduceBayesiannetworkstoAIresearchers. Oneoftheprincipalphilosophicalobjections\nofthelogicists wasthat thenumerical calculations thatprobability theory wasthought tore-\nquirewerenotapparenttointrospection andpresumedanunrealistic levelofprecisioninour\nuncertain knowledge. The development of qualitative probabilistic networks (Wellman,\n1990a) provided a purely qualitative abstraction of Bayesian networks, using the notion of\npositive and negative influences between variables. Wellman shows that in manycases such\ninformation is sufficient for optimal decision making without the need for the precise spec-\nification of probability values. Goldszmidt and Pearl (1996) take a similar approach. Work\nby Adnan Darwiche and Matt Ginsberg (1992) extracts the basic properties of conditioning\nandevidencecombinationfromprobability theoryandshows thattheycanalsobeappliedin\nlogicalanddefaultreasoning. Often,programsspeaklouderthanwords,andthereadyavail-\n11 Thetitleoftheoriginalversionofthearticlewas\u201cPearlforswine.\u201d 558 Chapter 14. Probabilistic Reasoning\nabilityofhigh-quality softwaresuchastheBayesNettoolkit(Murphy,2001)accelerated the\nadoption ofthetechnology.\nThemostimportantsinglepublicationinthegrowthofBayesiannetworkswasundoubt-\nedly the text Probabilistic Reasoning in Intelligent Systems (Pearl, 1988). Several excellent\ntexts (Lauritzen, 1996; Jensen, 2001; Korb and Nicholson, 2003; Jensen, 2007; Darwiche,\n2009; Koller and Friedman, 2009) provide thorough treatments of the topics we have cov-\nered in this chapter. New research on probabilistic reasoning appears both in mainstream\nAI journals, such as Artificial Intelligence and the Journal of AI Research, and in more spe-\ncialized journals, suchastheInternational JournalofApproximate Reasoning. Manypapers\non graphical models, which include Bayesian networks, appear in statistical journals. The\nproceedings oftheconferences onUncertainty inArtificial Intelligence (UAI),NeuralInfor-\nmation Processing Systems (NIPS), and Artificial Intelligence and Statistics (AISTATS)are\nexcellent sourcesforcurrentresearch.\nEXERCISES\n14.1 Wehaveabagofthree biased coins a,b,and cwithprobabilities ofcoming upheads\nof 20%, 60%, and 80%, respectively. Onecoin isdrawn randomly from the bag (with equal\nlikelihood of drawing each of the three coins), and then the coin is flipped three times to\ngenerate theoutcomes X ,X ,andX .\n1 2 3\na. DrawtheBayesiannetworkcorresponding tothissetupanddefinethenecessaryCPTs.\nb. Calculate which coinwasmostlikely tohavebeen drawnfrom thebag iftheobserved\nflipscomeoutheadstwiceandtailsonce.\n14.2 Equation (14.1) on page 513 defines the joint distribution represented by a Bayesian\nnetworkintermsoftheparameters\u03b8(X |Parents(X )). Thisexerciseasksyoutoderivethe\ni i\nequivalence between the parameters and the conditional probabilities P(X |Parents(X ))\ni i\nfromthisdefinition.\na. Consider a simple network X \u2192 Y \u2192 Z with three Boolean variables. Use Equa-\ntions (13.3) and (13.6) (pages 485 and 492) to express the conditional probability\nP(z|y)astheratiooftwosums,eachoverentriesinthejointdistribution P(X,Y,Z).\nb. Now use Equation (14.1) to write this expression in terms of the network parameters\n\u03b8(X),\u03b8(Y |X),and\u03b8(Z|Y).\nc. Next,expandoutthesummationsinyourexpressionfrompart(b),writingoutexplicitly\nthe terms for the true and false values of each summed variable. Assuming that all\n(cid:2)\nnetwork parameters satisfy the constraint \u03b8(x |parents(X ))=1, show that the\nxi i i\nresultingexpression reducesto \u03b8(x|y).\nd. Generalize this derivation to show that \u03b8(X |Parents(X )) = P(X |Parents(X ))\ni i i i\nforanyBayesiannetwork. Exercises 559\n14.3 TheoperationofarcreversalinaBayesiannetworkallowsustochangethedirection\nARCREVERSAL\nof an arc X \u2192 Y while preserving the joint probability distribution that the network repre-\nsents (Shachter, 1986). Arc reversal may require introducing new arcs: all the parents of X\nalsobecomeparentsofY,andallparents ofY alsobecomeparents ofX.\na. Assume that X and Y start with m and n parents, respectively, and that all variables\nhavekvalues. BycalculatingthechangeinsizefortheCPTsofX andY,showthatthe\ntotal number of parameters in the network cannot decrease during arc reversal. (Hint:\ntheparents ofX andY neednotbedisjoint.)\nb. Underwhatcircumstances canthetotalnumberremainconstant?\nc. Let the parents of X be U\u222a V and the parents of Y be V \u222a W, where U and W are\ndisjoint. TheformulasforthenewCPTsafterarcreversalareasfollows:\n(cid:12)\nP(Y |U,V,W) = P(Y |V,W,x)P(x|U,V)\nx\nP(X|U,V,W,Y) = P(Y |X,V,W)P(X|U,V)\/P(Y |U,V,W).\nProve that the new network expresses the same joint distribution over all variables as\ntheoriginal network.\n14.4 ConsidertheBayesiannetworkinFigure14.2.\na. Ifnoevidenceisobserved,areBurglary andEarthquake independent? Provethisfrom\nthenumerical semanticsandfromthetopological semantics.\nb. Ifweobserve Alarm=true,areBurglary andEarthquake independent? Justifyyour\nanswerbycalculatingwhethertheprobabilities involvedsatisfythedefinitionofcondi-\ntionalindependence.\n14.5 SupposethatinaBayesiannetworkcontaining anunobserved variable Y,allthevari-\nablesintheMarkovblanket MB(Y)havebeenobserved.\na. Provethatremovingthenode Y fromthenetworkwillnotaffecttheposteriordistribu-\ntionforanyotherunobserved variableinthenetwork.\nb. Discusswhetherwecanremove Y ifweareplanning touse(i)rejection sampling and\n(ii)likelihood weighting.\n14.6 LetH bearandomvariabledenotingthehandednessofanindividualx,withpossible\nx\nvalues l orr. Acommonhypothesis isthatleft-orright-handedness isinherited byasimple\nmechanism; that is, perhaps there is a gene G , also with values l or r, and perhaps actual\nx\nhandedness turns out mostly the same (with some probability s) as the gene an individual\npossesses. Furthermore, perhaps the gene itself is equally likely to be inherited from either\nofanindividual\u2019s parents, withasmallnonzeroprobability mofarandom mutationflipping\nthehandedness.\na. Which of the three networks in Figure 14.20 claim that P(G ,G ,G ) =\nfather mother child\nP(G )P(G )P(G )?\nfather mother child\nb. Which of the three networks make independence claims that are consistent with the\nhypothesis abouttheinheritance ofhandedness? 560 Chapter 14. Probabilistic Reasoning\nG G G G G G\nmother father mother father mother father\nH H H H H H\nmother father mother father mother father\nG G G\nchild child child\nH H H\nchild child child\n(a) (b) (c)\nFigure14.20 ThreepossiblestructuresforaBayesiannetworkdescribinggeneticinheri-\ntanceofhandedness.\nc. Whichofthethreenetworksisthebestdescription ofthehypothesis?\nd. WritedowntheCPTforthe G nodeinnetwork(a),intermsof sandm.\nchild\ne. Suppose that P(G =l) = P(G =l) = q. In network (a), derive an expres-\nfather mother\nsionforP(G =l)intermsofmandq only,byconditioning onitsparentnodes.\nchild\nf. Under conditions of genetic equilibrium, we expect the distribution of genes to be the\nsameacrossgenerations. Usethistocalculatethevalueofq,and,givenwhatyouknow\nabouthandedness inhumans, explainwhythehypothesis described atthebeginning of\nthisquestionmustbewrong.\n14.7 The Markov blanket of a variable is defined on page 517. Prove that a variable\nis independent of all other variables in the network, given its Markov blanket and derive\nEquation(14.12)(page538).\nBattery\nRadio Ignition Gas\nStarts\nMoves\nFigure 14.21 A Bayesian network describing some features of a car\u2019s electrical system\nand engine. Each variable is Boolean, and the true value indicates that the corresponding\naspectofthevehicleisinworkingorder. Exercises 561\n14.8 Considerthenetworkforcardiagnosis showninFigure14.21.\na. ExtendthenetworkwiththeBooleanvariables IcyWeather andStarterMotor.\nb. Givereasonable conditional probability tablesforallthenodes.\nc. How many independent values are contained in the joint probability distribution for\neight Boolean nodes, assuming that no conditional independence relations are known\ntoholdamongthem?\nd. Howmanyindependent probability valuesdoyournetworktablescontain?\ne. TheconditionaldistributionforStarts couldbedescribedasanoisy-ANDdistribution.\nDefinethisfamilyingeneral andrelateittothenoisy-ORdistribution.\n14.9 ConsiderthefamilyoflinearGaussiannetworks, asdefinedonpage520.\na. In a two-variable network, let X be the parent of X , let X have a Gaussian prior,\n1 2 1\nand let P(X |X ) be a linear Gaussian distribution. Show that the joint distribution\n2 1\nP(X ,X )isamultivariate Gaussian, andcalculateitscovariance matrix.\n1 2\nb. Prove by induction that the joint distribution for a general linear Gaussian network on\nX ,...,X isalsoamultivariate Gaussian.\n1 n\n14.10 Theprobit distribution definedonpage522describes theprobability distribution for\naBooleanchild,givenasinglecontinuous parent.\na. Howmightthedefinition beextended tocovermultiplecontinuous parents?\nb. Howmight itbe extended to handle amultivalued child variable? Consider both cases\nwhere the child\u2019s values are ordered (as in selecting a gear while driving, depending\non speed, slope, desired acceleration, etc.) and cases where they are unordered (as in\nselecting bus, train, orcarto gettowork). (Hint: Considerways todivide the possible\nvaluesintotwosets,tomimicaBooleanvariable.)\n14.11 Inyourlocalnuclearpowerstation, thereisanalarmthatsenseswhenatemperature\ngauge exceeds agiven threshold. Thegaugemeasures thetemperature ofthecore. Consider\nthe Boolean variables A (alarm sounds), F (alarm is faulty), and F (gauge is faulty) and\nA G\nthemultivaluednodesG(gaugereading) andT (actualcoretemperature).\na. Draw a Bayesian network for this domain, given that the gauge is more likely to fail\nwhenthecoretemperature getstoohigh.\nb. Isyournetworkapolytree? Whyorwhynot?\nc. Supposetherearejusttwopossibleactualandmeasuredtemperatures,normalandhigh;\ntheprobability thatthegaugegivesthecorrecttemperature isxwhenitisworking, but\ny whenitisfaulty. Givetheconditional probability tableassociated withG.\nd. Suppose the alarm works correctly unless it is faulty, in which case it never sounds.\nGivetheconditional probability tableassociated withA.\ne. Suppose the alarm and gauge are working and the alarm sounds. Calculate an expres-\nsion for the probability that the temperature of the core is too high, in terms of the\nvariousconditional probabilities inthenetwork. 562 Chapter 14. Probabilistic Reasoning\nF F F N F M M\n1 2 1 2 1 2\nM M N\n1 2\nM M\n1 2\nN F F\n1 2\n(i) (ii) (iii)\nFigure14.22 Threepossiblenetworksforthetelescopeproblem.\n14.12 Two astronomers in different parts of the world make measurements M and M of\n1 2\nthenumberofstarsN insomesmallregionofthesky,usingtheirtelescopes. Normally,there\nis a small possibility e of error by up to one star in each direction. Each telescope can also\n(withamuch smallerprobability f)bebadly outoffocus (events F and F ), inwhichcase\n1 2\nthe scientist will undercount by three or more stars (or if N is less than 3, fail to detect any\nstarsatall). ConsiderthethreenetworksshowninFigure14.22.\na. Which of these Bayesian networks are correct (but not necessarily efficient) represen-\ntationsofthepreceding information?\nb. Whichisthebestnetwork? Explain.\nc. WriteoutaconditionaldistributionforP(M |N),forthecasewhereN \u2208{1,2,3}and\n1\nM \u2208{0,1,2,3,4}. Eachentryintheconditional distribution should beexpressed asa\n1\nfunctionoftheparameters eand\/orf.\nd. SupposeM =1andM =3. Whatarethepossible numbersofstarsifyouassumeno\n1 2\npriorconstraint onthevaluesof N?\ne. What is the most likely number of stars, given these observations? Explain how to\ncomputethis, orifitisnotpossible tocompute, explain whatadditional information is\nneededandhowitwouldaffecttheresult.\n14.13 Consider the network shown inFigure 14.22(ii), and assume that the twotelescopes\nwork identically. N \u2208{1,2,3} and M ,M \u2208{0,1,2,3,4}, with the symbolic CPTs as de-\n1 2\nscribed in Exercise 14.12. Using the enumeration algorithm (Figure 14.9 on page 525), cal-\nculatetheprobability distribution P(N |M =2,M =2).\n1 2\n14.14 ConsidertheBayesnetshowninFigure14.23.\na. Whichofthefollowingareassertedbythenetwork structure?\n(i) P(B,I,M) = P(B)P(I)P(M).\n(ii) P(J|G) = P(J|G,I).\n(iii) P(M|G,B,I) = P(M|G,B,I,J). Exercises 563\nB M P(I)\nt t .9\nt f .5\nf t .5\nP(B) f f .1 P(M)\n.9 .1\nB I M\nB I M P(G)\nG\nt t t .9\nt t f .8\nt f t .0\nt f f .0 G P(J)\nf t t .2 J t .9\nf t f .1 f .0\nf f t .0\nf f f .0\nFigure 14.23 A simple Bayes net with Boolean variables B=BrokeElectionLaw,\nI=Indicted,M=PoliticallyMotivatedProsecutor,G=FoundGuilty,J=Jailed.\nb. CalculatethevalueofP(b,i,\u00acm,g,j).\nc. Calculate the probability that someone goes to jail given that they broke the law, have\nbeenindicted, andfaceapolitically motivatedprosecutor.\nd. A context-specific independence (see page 542) allows a variable to be independent\nofsomeofitsparentsgivencertainvaluesofothers. Inadditiontotheusualconditional\nindependences given bythegraph structure, whatcontext-specific independences exist\nintheBayesnetinFigure14.23?\ne. SupposewewanttoaddthevariableP =PresidentialPardon tothenetwork;drawthe\nnewnetworkandbrieflyexplainanylinksyouadd.\n14.15 Considerthevariable eliminationalgorithm inFigure14.11(page528).\na. Section14.4appliesvariableelimination tothequery\nP(Burglary|JohnCalls=true,MaryCalls=true).\nPerformthecalculations indicated andcheckthattheansweriscorrect.\nb. Countthenumberofarithmetic operations performed, andcompare itwiththenumber\nperformedbytheenumeration algorithm.\nc. Supposeanetworkhastheformofachain: asequenceofBooleanvariablesX ,...,X\n1 n\nwhere Parents(X i)={X i\u22121} for i=2,...,n. What is the complexity of computing\nP(X |X =true)usingenumeration? Usingvariableelimination?\n1 n\nd. Provethatthecomplexityofrunningvariableeliminationonapolytreenetworkislinear\ninthesizeofthetreeforanyvariableordering consistent withthenetworkstructure.\n14.16 Investigate thecomplexity ofexactinferenceingeneralBayesiannetworks:\na. Provethatany3-SATproblemcanbereducedtoexactinferenceinaBayesiannetwork\nconstructed to represent the particular problem and hence that exact inference is NP- 564 Chapter 14. Probabilistic Reasoning\nhard. (Hint: Consideranetworkwithonevariableforeachproposition symbol,onefor\neachclause,andonefortheconjunction ofclauses.)\nb. Theproblem ofcounting thenumberofsatisfying assignments fora3-SATproblem is\n#P-complete. Showthatexactinference isatleastashardasthis.\n14.17 Consider the problem of generating a random sample from a specified distribution\non a single variable. Assume you have a random number generator that returns a random\nnumberuniformly distributed between0and1.\na. Let X be a discrete variable with P(X=x )=p for i\u2208{1,...,k}. The cumulative\ni i\nCUMULATIVE distributionofX givestheprobability thatX\u2208{x ,...,x }foreachpossiblej. (See\nDISTRIBUTION 1 j\nalso Appendix A.) Explain how to calculate the cumulative distribution in O(k) time\nand how to generate a single sample of X from it. Can the latter be done in less than\nO(k)time?\nb. Nowsuppose wewanttogenerate N samplesofX,whereN & k. Explainhowtodo\nthiswithanexpectedruntimepersamplethatis constant (i.e.,independent ofk).\nc. Now consider a continuous-valued variable with a parameterized distribution (e.g.,\nGaussian). Howcansamplesbegenerated fromsuchadistribution?\nd. Suppose youwanttoquery acontinuous-valued variable and youareusing asampling\nalgorithmsuchas LIKELIHOODWEIGHTING todotheinference. Howwouldyouhave\ntomodifythequery-answering process?\n14.18 ConsiderthequeryP(Rain|Sprinkler =true,WetGrass=true)inFigure14.12(a)\n(page529)andhowGibbssamplingcananswerit.\na. HowmanystatesdoestheMarkovchainhave?\nb. Calculatethetransition matrixQcontaining q(y \u2192 y(cid:2) )forally,y(cid:2) .\nc. WhatdoesQ2,thesquareofthetransition matrix,represent?\nd. WhataboutQn asn \u2192 \u221e?\ne. Explain how to do probabilistic inference in Bayesian networks, assuming that Qn is\navailable. Isthisapractical waytodoinference?\n14.19 Thisexerciseexploresthestationary distribution forGibbssamplingmethods.\na. Theconvexcomposition [\u03b1,q ;1\u2212\u03b1,q ]ofq andq isatransition probability distri-\n1 2 1 2\nbution that first chooses one of q and q with probabilities \u03b1 and 1\u2212\u03b1, respectively,\n1 2\nand then applies whichever is chosen. Prove that if q and q are in detailed balance\n1 2\nwith \u03c0, then their convex composition is also in detailed balance with \u03c0. (Note: this\nresult justifies avariant of GIBBS-ASK inwhichvariables arechosen atrandom rather\nthansampledinafixedsequence.)\nb. Prove that if each of q and q has \u03c0 as its stationary distribution, then the sequential\n1 2\ncomposition q=q \u25e6q alsohas\u03c0 asitsstationary distribution.\n1 2\nMETROPOLIS\u2013 14.20 TheMetropolis\u2013HastingsalgorithmisamemberoftheMCMCfamily;assuch,itis\nHASTINGS\ndesignedtogeneratesamplesx(eventually)accordingtotargetprobabilities \u03c0(x). (Typically Exercises 565\nweare interested in sampling from \u03c0(x)=P(x|e).) Like simulated annealing, Metropolis\u2013\n(cid:2)\nPROPOSAL Hastingsoperatesintwostages. First,itsamplesanewstatex fromaproposaldistribution\nDISTRIBUTION\nq(x(cid:2)|x),giventhecurrent\nstate x. Then,itprobabilistically acceptsorrejects\nx(cid:2)\naccording to\nACCEPTANCE theacceptanceprobability\nPROBABILITY (cid:13) (cid:14)\n\u03c0(x(cid:2) )q(x|x(cid:2)\n)\n\u03b1(x(cid:2)|x)=\nmin 1, .\n\u03c0(x)q(x(cid:2)|x)\nIftheproposal isrejected, thestateremainsat x.\na. Consider an ordinary Gibbs sampling step for a specific variable X . Show that this\ni\nstep, considered as a proposal, is guaranteed to be accepted by Metropolis\u2013Hastings.\n(Hence,GibbssamplingisaspecialcaseofMetropolis\u2013Hastings.)\nb. Showthatthetwo-stepprocessabove,viewedasatransition probability distribution, is\nindetailed balancewith\u03c0.\n14.21 Three soccer teams A, B, and C, play each other once. Each match is between two\nteams, and can be won, drawn, orlost. Each team has afixed, unknown degree ofquality\u2014\nanintegerrangingfrom0to3\u2014andtheoutcomeofamatchdependsprobabilistically onthe\ndifference inquality betweenthetwoteams.\na. Constructarelationalprobabilitymodeltodescribethis domain,andsuggestnumerical\nvaluesforallthenecessary probability distributions.\nb. Constructtheequivalent Bayesiannetworkforthethreematches.\nc. Suppose that in the first two matches A beats B and draws with C. Using an exact\ninference algorithm ofyourchoice, compute theposterior distribution forthe outcome\nofthethirdmatch.\nd. Suppose there are n teams in the league and we have the results for all but the last\nmatch. Howdoesthecomplexityofpredicting thelastgamevarywithn?\ne. Investigate theapplication ofMCMCtothisproblem. Howquicklydoesitconverge in\npracticeandhowwelldoesitscale? 15\nPROBABILISTIC\nREASONING OVER TIME\nInwhichwetrytointerpret thepresent, understand thepast,andperhaps predict\nthefuture, evenwhenverylittleiscrystalclear.\nAgentsinpartiallyobservableenvironmentsmustbeabletokeeptrackofthecurrentstate,to\ntheextentthattheirsensorsallow. InSection4.4weshowedamethodologyfordoingthat: an\nagentmaintainsabeliefstatethatrepresentswhichstatesoftheworldarecurrentlypossible.\nFrom the belief state and a transition model, the agent can predict how the world might\nevolve in the next timestep. From the percepts observed and a sensor model, the agent can\nupdatethebeliefstate. Thisisapervasiveidea: inChapter4beliefstateswererepresentedby\nexplicitly enumerated sets of states, whereas in Chapters 7 and 11 they were represented by\nlogicalformulas. Thoseapproaches definedbeliefstatesintermsofwhichworldstateswere\npossible, butcouldsaynothing aboutwhichstateswerelikelyorunlikely. Inthischapter, we\nuseprobability theorytoquantify thedegreeofbeliefinelementsofthebeliefstate.\nAs we show in Section 15.1, time itself is handled in the same way as in Chapter 7: a\nchangingworldismodeledusingavariableforeachaspectoftheworldstateateachpointin\ntime. The transition and sensor models maybe uncertain: the transition model describes the\nprobability distribution of the variables at time t, given the state of the world at past times,\nwhile the sensor model describes the probability of each percept at time t, given the current\nstate of the world. Section 15.2 defines the basic inference tasks and describes the gen-\neral structure of inference algorithms for temporal models. Then we describe three specific\nkinds of models: hidden Markov models, Kalman filters, and dynamic Bayesian net-\nworks (which include hidden Markov models and Kalman filters as special cases). Finally,\nSection15.6examinestheproblemsfacedwhenkeeping track ofmorethanonething.\n15.1 TIME AND UNCERTAINTY\nWehavedevelopedourtechniques forprobabilistic reasoning inthecontextofstaticworlds,\nin which each random variable has a single fixed value. Forexample, when repairing a car,\nwe assume that whatever is broken remains broken during the process of diagnosis; our job\nistoinferthestateofthecarfromobservedevidence, which alsoremainsfixed.\n566 Section15.1. TimeandUncertainty 567\nNowconsideraslightly different problem: treating adiabetic patient. Asinthecaseof\ncarrepair, wehave evidence such asrecent insulin doses, food intake, blood sugarmeasure-\nments,andotherphysicalsigns. Thetaskistoassessthecurrentstateofthepatient,including\nthe actual blood sugar level and insulin level. Given this information, we can make a deci-\nsion about the patient\u2019s food intake and insulin dose. Unlike the case of car repair, here the\ndynamic aspects of the problem are essential. Blood sugar levels and measurements thereof\ncan change rapidly over time, depending on recent food intake and insulin doses, metabolic\nactivity, the time of day, and so on. To assess the current state from the history of evidence\nandtopredict theoutcomes oftreatmentactions, wemustmodelthesechanges.\nThesame considerations arise in many other contexts, such as tracking the location of\na robot, tracking the economic activity of a nation, and making sense of a spoken orwritten\nsequence ofwords. Howcandynamicsituations likethesebemodeled?\n15.1.1 States and observations\nWe view the world as a series of snapshots, or time slices, each of which contains a set of\nTIMESLICE\nrandom variables, some observable and some not.1 For simplicity, we will assume that the\nsamesubsetofvariablesisobservableineachtimeslice(althoughthisisnotstrictlynecessary\ninanything thatfollows). WewilluseX todenote thesetofstate variables attime t,which\nt\nare assumed to be unobservable, and E to denote the set of observable evidence variables.\nt\nTheobservation attimetisE =e forsomesetofvalues e .\nt t t\nConsiderthefollowingexample: Youarethesecurityguardstationedatasecretunder-\nground installation. Youwanttoknowwhetherit\u2019srainingtoday, butyouronlyaccesstothe\noutside worldoccurseachmorningwhenyouseethedirectorcominginwith,orwithout, an\numbrella. Foreachdayt,thesetE thuscontainsasingleevidencevariableUmbrella orU\nt t t\nforshort(whethertheumbrellaappears),andtheset X containsasinglestatevariableRain\nt t\norR forshort(whetheritisraining). Otherproblemscaninvolve largersetsofvariables. In\nt\nthediabetes example,wemighthaveevidencevariables, suchasMeasuredBloodSugar and\nt\nPulseRate , andstate variables, such as BloodSugar andStomachContents . (Notice that\nt t t\nBloodSugar andMeasuredBloodSugar arenotthesamevariable; thisishowwedealwith\nt t\nnoisymeasurements ofactualquantities.)\nTheintervalbetweentimeslicesalsodependsontheproblem. Fordiabetesmonitoring,\nasuitable interval might beanhourrather than aday. Inthis chapter weassume theinterval\nbetween slices is fixed, so we can label times by integers. We will assume that the state\nsequencestartsatt=0;forvariousuninterestingreasons,wewillassumethatevidencestarts\narrivingatt=1ratherthant=0. Hence,ourumbrellaworldisrepresentedbystatevariables\nR , R , R ,... and evidence variables U , U ,.... We will use the notation a:b to denote\n0 1 2 1 2\nthe sequence of integers from a to b (inclusive), and the notation X to denote the set of\na:b\nvariables from X toX . Forexample, U corresponds tothevariables U ,U ,U .\na b 1:3 1 2 3\n1 Uncertaintyovercontinuoustimecanbemodeledbystochasticdifferentialequations(SDEs). Themodels\nstudiedinthischaptercanbeviewedasdiscrete-timeapproximationstoSDEs. 568 Chapter 15. Probabilistic Reasoning overTime\n(a) X X X X X\nt\u20132 t\u20131 t t+1 t+2\n(b) X X X X X\nt\u20132 t\u20131 t t+1 t+2\nFigure15.1 (a)Bayesiannetworkstructurecorrespondingtoafirst-orderMarkovprocess\nwithstatedefinedbythevariablesXt. (b)Asecond-orderMarkovprocess.\n15.1.2 Transitionandsensormodels\nWith the set of state and evidence variables for a given problem decided on, the next step is\nto specify how the world evolves (the transition model) and how the evidence variables get\ntheirvalues(thesensormodel).\nThetransitionmodelspecifiestheprobabilitydistribution overthelateststatevariables,\ngiven the previous values, that is, P(X t|X 0:t\u22121). Now we face a problem: the set X 0:t\u22121 is\nMARKOV unbounded insizeastincreases. WesolvetheproblembymakingaMarkovassumption\u2014\nASSUMPTION\nthat thecurrent state depends ononly a finite fixed number ofprevious states. Processes sat-\nisfyingthisassumption werefirststudiedindepthbytheRussianstatistician AndreiMarkov\n(1856\u20131922)andarecalledMarkovprocessesorMarkovchains. Theycomeinvariousfla-\nMARKOVPROCESS\nFIRST-ORDER vors;thesimplestisthefirst-orderMarkovprocess,inwhichthecurrentstatedependsonly\nMARKOVPROCESS\non the previous state and not on any earlier states. In other words, a state provides enough\ninformation tomakethefutureconditionally independent ofthepast,andwehave\nP(X t|X 0:t\u22121) = P(X t|X t\u22121). (15.1)\nHence, in a first-order Markov process, the transition model is the conditional distribution\nP(X t|X t\u22121). The transition model for a second-order Markov process is the conditional\ndistribution P(X t|X t\u22122,X t\u22121). Figure 15.1 shows the Bayesian network structures corre-\nsponding tofirst-orderandsecond-order Markovprocesses.\nEven with the Markov assumption there is still a problem: there are infinitely many\npossible values of t. Do we need to specify a different distribution for each time step? We\navoid this problem by assuming that changes in the world state are caused by a stationary\nSTATIONARY process\u2014thatis,aprocessofchangethatisgovernedbylawsthatdonotthemselveschange\nPROCESS\novertime. (Don\u2019t confuse stationary with static: in a static process, the state itself does not\nchange.) Intheumbrellaworld,then, theconditional probability ofrain, P(R t|R t\u22121),isthe\nsameforallt,andweonlyhavetospecifyoneconditional probability table.\nNow for the sensor model. The evidence variables E could depend on previous vari-\nt\nablesaswellasthecurrent state variables, butanystatethat\u2019s worthitssaltshould sufficeto\nSENSORMARKOV generatethecurrentsensorvalues. Thus,wemakea sensorMarkovassumptionasfollows:\nASSUMPTION\nP(E t|X 0:t,E 0:t\u22121)= P(E t|X t). (15.2)\nThus,P(E |X )isoursensormodel(sometimescalledtheobservation model). Figure15.2\nt t\nshows both the transition model and the sensor model for the umbrella example. Notice the Section15.1. TimeandUncertainty 569\nR P(R )\nt-1 t\nt 0.7\nf 0.3\nRain Rain Rain\nt\u20131 t t+1\nR P(U )\nt t\nt 0.9\nf 0.2\nUmbrella Umbrella Umbrella\nt\u20131 t t+1\nFigure 15.2 Bayesian network structure and conditional distributions describing the\numbrella world. The transition model is P(Raint|Raint\u22121) and the sensor model is\nP(Umbrellat|Raint).\ndirection of the dependence between state and sensors: the arrows go from the actual state\nof the world to sensor values because the state of the world causes the sensors to take on\nparticular values: the rain causes the umbrella to appear. (The inference process, of course,\ngoes in the other direction; the distinction between the direction of modeled dependencies\nandthedirection ofinference isoneoftheprincipal advantages ofBayesiannetworks.)\nIn addition to specifying the transition and sensor models, we need to say how every-\nthing gets started\u2014the prior probability distribution at time 0, P(X ). With that, we have a\n0\nspecification of the complete joint distribution over all the variables, using Equation (14.2).\nForanyt,\n(cid:25)t\nP(X 0:t,E 1:t)= P(X 0) P(X i|X i\u22121)P(E i|X i). (15.3)\ni=1\nThethreetermsontheright-hand sidearetheinitial statemodelP(X ),thetransition model\n0\nP(X i|X i\u22121),andthesensormodelP(E i|X i).\nThestructure in Figure 15.2 is a first-order Markov process\u2014the probability of rain is\nassumed todepend only onwhetheritrained theprevious day. Whethersuch anassumption\nisreasonable depends on the domain itself. Thefirst-order Markov assumption says that the\nstate variables contain all the information needed tocharacterize the probability distribution\nforthenext time slice. Sometimes the assumption is exactly true\u2014forexample, ifaparticle\nis executing a random walk along the x-axis, changing its position by \u00b11 at each time step,\nthen using the x-coordinate as the state gives a first-order Markov process. Sometimes the\nassumptionisonlyapproximate, asinthecaseofpredicting rainonlyonthebasisofwhether\nitrainedthepreviousday. Therearetwowaystoimprovetheaccuracyoftheapproximation:\n1. Increasing the order of the Markov process model. For example, we could make a\nsecond-ordermodelbyaddingRain t\u22122 asaparentofRain t,whichmightgiveslightly\nmore accurate predictions. For example, in Palo Alto, California, it very rarely rains\nmorethantwodaysinarow.\n2. Increasing the set of state variables. For example, we could add Season to allow\nt 570 Chapter 15. Probabilistic Reasoning overTime\nus to incorporate historical records of rainy seasons, or we could add Temperature ,\nt\nHumidity andPressure (perhapsatarangeoflocations)toallowustouseaphysical\nt t\nmodelofrainyconditions.\nExercise 15.1 asks you to show that the first solution\u2014increasing the order\u2014can always be\nreformulated as an increase in the set of state variables, keeping the order fixed. Notice that\nadding state variables might improve the system\u2019s predictive power but also increases the\nprediction requirements: we now have to predict the new variables as well. Thus, we are\nlookingfora\u201cself-sufficient\u201dsetofvariables,whichreallymeansthatwehavetounderstand\nthe \u201cphysics\u201d of the process being modeled. The requirement for accurate modeling of the\nprocess isobviously lessened ifwecan add newsensors (e.g., measurements oftemperature\nandpressure) thatprovideinformation directly aboutthenewstatevariables.\nConsider,forexample,theproblemoftrackingarobotwanderingrandomlyontheX\u2013Y\nplane. Onemightpropose thattheposition andvelocityarea sufficientsetofstatevariables:\nonecansimplyuseNewton\u2019slawstocalculatethenewposition,andthevelocitymaychange\nunpredictably. Iftherobotisbattery-powered,however,thenbatteryexhaustionwouldtendto\nhaveasystematiceffectonthechangeinvelocity. Becausethisinturndependsonhowmuch\npowerwasused byallprevious maneuvers, the Markov property isviolated. Wecan restore\ntheMarkovproperty byincluding thechargelevel Battery asoneofthestatevariables that\nt\nmake up X . This helps in predicting the motion of the robot, but in turn requires a model\nt\nfor predicting Battery t from Battery t\u22121 and the velocity. In some cases, that can be done\nreliably, but more often we findthat error accumulates overtime. In that case, accuracy can\nbeimprovedbyaddinganewsensorforthebatterylevel.\n15.2 INFERENCE IN TEMPORAL MODELS\nHavingsetupthestructureofagenerictemporalmodel,wecanformulatethebasicinference\ntasksthatmustbesolved:\n\u2022 Filtering: This is the task of computing the belief state\u2014the posterior distribution\nFILTERING\nover the most recent state\u2014given all evidence to date. Filtering2 is also called state\nBELIEFSTATE\nestimation. Inourexample,wewishtocompute P(X |e ). Intheumbrellaexample,\nSTATEESTIMATION t 1:t\nthis would mean computing the probability of rain today, given all the observations of\nthe umbrella carrier made so far. Filtering is what a rational agent does to keep track\nof the current state so that rational decisions can be made. It turns out that an almost\nidenticalcalculation providesthelikelihood oftheevidence sequence, P(e ).\n1:t\n\u2022 Prediction: Thisisthetaskofcomputingtheposteriordistributionoverthefuturestate,\nPREDICTION\ngivenall evidence todate. Thatis, wewishtocompute P(X |e )forsome k > 0.\nt+k 1:t\nInthe umbrella example, thismight meancomputing theprobability ofrainthree days\nfromnow,givenalltheobservationstodate. Predictionisusefulforevaluatingpossible\ncoursesofactionbasedontheirexpectedoutcomes.\n2 Theterm\u201cfiltering\u201dreferstotherootsofthisprobleminearlyworkonsignalprocessing,wheretheproblem\nistofilteroutthenoiseinasignalbyestimatingitsunderlyingproperties. Section15.2. Inference inTemporalModels 571\n\u2022 Smoothing: This is the task of computing the posterior distribution over a past state,\nSMOOTHING\ngivenallevidenceuptothepresent. Thatis,wewishtocomputeP(X |e )forsomek\nk 1:t\nsuchthat0 \u2264 k <t. Intheumbrellaexample,itmightmeancomputingtheprobability\nthat it rained last Wednesday, given all the observations of the umbrella carrier made\nuptotoday. Smoothing provides abetterestimate ofthestate thanwasavailable atthe\ntime,becauseitincorporates moreevidence.3\n\u2022 Mostlikelyexplanation: Givenasequence ofobservations, wemightwishtofindthe\nsequence ofstates thatismostlikely tohavegenerated those observations. Thatis, we\nwishtocomputeargmax P(x |e ). Forexample,iftheumbrellaappearsoneach\nx1:t 1:t 1:t\nofthefirstthreedaysandisabsentonthefourth,thenthemostlikelyexplanationisthat\nitrained on the first three days and did not rain on the fourth. Algorithms forthis task\nareusefulinmanyapplications, includingspeechrecognition\u2014where theaimistofind\nthemostlikely sequence ofwords, givenaseries ofsounds\u2014and thereconstruction of\nbitstringstransmitted overanoisychannel.\nInaddition totheseinference tasks,wealsohave\n\u2022 Learning: The transition and sensor models, if not yet known, can be learned from\nobservations. JustaswithstaticBayesiannetworks,dynamicBayesnetlearningcanbe\ndone as a by-product of inference. Inference provides an estimate of what transitions\nactually occurred andofwhatstatesgenerated thesensorreadings, andthese estimates\ncanbeused toupdate themodels. Theupdated models provide newestimates, and the\nprocess iterates to convergence. The overall process is an instance of the expectation-\nmaximizationorEMalgorithm. (SeeSection20.3.)\nNotethatlearning requiressmoothing, ratherthanfiltering, becausesmoothing provides bet-\nterestimatesofthestatesoftheprocess. Learningwithfilteringcanfailtoconvergecorrectly;\nconsider, for example, the problem of learning to solve murders: unless you are an eyewit-\nness, smoothing is always required to infer what happened at the murder scene from the\nobservable variables.\nTheremainderofthissection describes generic algorithms forthefourinference tasks,\nindependent oftheparticularkindofmodelemployed. Improvementsspecifictoeachmodel\naredescribed insubsequent sections.\n15.2.1 Filteringand prediction\nAs we pointed out in Section 7.7.3, a useful filtering algorithm needs to maintain a current\nstateestimateandupdateit,ratherthangoingbackovertheentirehistoryofperceptsforeach\nupdate. (Otherwise, thecostofeachupdateincreases astimegoesby.) Inotherwords,given\nthe result of filtering up to time t, the agent needs to compute the result for t+1 from the\nnewevidencee ,\nt+1\nP(X |e ) = f(e ,P(X |e )),\nt+1 1:t+1 t+1 t 1:t\nRECURSIVE forsomefunctionf. Thisprocessiscalledrecursiveestimation. Wecanviewthecalculation\nESTIMATION\n3 Inparticular,whentrackingamovingobjectwithinaccuratepositionobservations,smoothinggivesasmoother\nestimatedtrajectorythanfiltering\u2014hencethename. 572 Chapter 15. Probabilistic Reasoning overTime\nasbeingcomposedoftwoparts: first,thecurrent statedistribution isprojected forwardfrom\nttot+1;thenitisupdatedusingthenewevidencee . Thistwo-partprocessemergesquite\nt+1\nsimplywhentheformulaisrearranged:\nP(X |e ) = P(X |e ,e ) (dividing uptheevidence)\nt+1 1:t+1 t+1 1:t t+1\n= \u03b1P(e |X ,e )P(X |e ) (usingBayes\u2019rule)\nt+1 t+1 1:t t+1 1:t\n= \u03b1P(e |X )P(X |e ) (bythesensorMarkovassumption). (15.4)\nt+1 t+1 t+1 1:t\nHereandthroughoutthischapter, \u03b1isanormalizingconstantusedtomakeprobabilities sum\nup to 1. The second term, P(X |e ) represents a one-step prediction of the next state,\nt+1 1:t\nandthefirsttermupdatesthiswiththenewevidence;noticethatP(e |X )isobtainable\nt+1 t+1\ndirectly from the sensor model. Now weobtain the one-step prediction for the next state by\nconditioning onthecurrentstate X :\nt (cid:12)\nP(X |e ) = \u03b1P(e |X ) P(X |x ,e )P(x |e )\nt+1 1:t+1 t+1 t+1 t+1 t 1:t t 1:t\n(cid:12) xt\n= \u03b1P(e |X ) P(X |x )P(x |e ) (Markovassumption). (15.5)\nt+1 t+1 t+1 t t 1:t\nxt\nWithinthesummation,thefirstfactorcomesfromthetransitionmodelandthesecondcomes\nfromthecurrentstatedistribution. Hence,wehavethedesiredrecursiveformulation. Wecan\nthinkofthefilteredestimate P(X |e )asa\u201cmessage\u201df thatispropagated forwardalong\nt 1:t 1:t\nthesequence, modifiedbyeachtransition andupdatedbyeachnewobservation. Theprocess\nisgivenby\nf\n1:t+1\n= \u03b1FORWARD(f 1:t,e t+1),\nwhereFORWARDimplementstheupdatedescribedinEquation(15.5)andtheprocessbegins\nwith f = P(X ). When all the state variables are discrete, the time for each update is\n1:0 0\nconstant (i.e., independent of t), and the space required is also constant. (The constants\ndepend, of course, on the size of the state space and the specific type of the temporal model\ninquestion.) Thetimeandspacerequirementsforupdatingmustbeconstantifanagentwith\nlimitedmemoryistokeeptrackofthecurrentstatedistribution overanunbounded sequence\nofobservations.\nLet us illustrate the filtering process for two steps in the basic umbrella example (Fig-\nure15.2.) Thatis,wewillcompute P(R |u )asfollows:\n2 1:2\n\u2022 Onday0,wehavenoobservations, onlythesecurityguard\u2019spriorbeliefs;let\u2019sassume\nthatconsistsofP(R ) = (cid:16)0.5,0.5(cid:17).\n0\n\u2022 Onday1,theumbrellaappears, so U =true. Theprediction from t=0tot=1is\n(cid:12) 1\nP(R ) = P(R |r )P(r )\n1 1 0 0\nr0\n= (cid:16)0.7,0.3(cid:17)\u00d70.5+(cid:16)0.3,0.7(cid:17)\u00d70.5 = (cid:16)0.5,0.5(cid:17).\nThentheupdate stepsimply multiplies bytheprobability of theevidence fort=1and\nnormalizes, asshowninEquation(15.4):\nP(R |u ) = \u03b1P(u |R )P(R )= \u03b1(cid:16)0.9,0.2(cid:17)(cid:16)0.5,0.5(cid:17)\n1 1 1 1 1\n= \u03b1(cid:16)0.45,0.1(cid:17) \u2248 (cid:16)0.818,0.182(cid:17). Section15.2. Inference inTemporalModels 573\n\u2022 Onday2,theumbrellaappears, so U =true. Theprediction from t=1tot=2is\n(cid:12) 2\nP(R |u ) = P(R |r )P(r |u )\n2 1 2 1 1 1\nr1\n= (cid:16)0.7,0.3(cid:17)\u00d70.818+(cid:16)0.3,0.7(cid:17)\u00d70.182 \u2248 (cid:16)0.627,0.373(cid:17),\nandupdatingitwiththeevidencefort=2gives\nP(R |u ,u ) = \u03b1P(u |R )P(R |u )= \u03b1(cid:16)0.9,0.2(cid:17)(cid:16)0.627,0.373(cid:17)\n2 1 2 2 2 2 1\n= \u03b1(cid:16)0.565,0.075(cid:17) \u2248 (cid:16)0.883,0.117(cid:17).\nIntuitively, the probability of rain increases from day 1to day 2 because rain persists. Exer-\ncise15.2(a)asksyoutoinvestigate thistendencyfurther.\nThe task of prediction can be seen simply as filtering without the addition of new\nevidence. In fact, the filtering process already incorporates a one-step prediction, and it is\neasy toderivethe following recursive computation forpredicting the stateatt+k+1from\naprediction fort+k:\n(cid:12)\nP(X |e ) = P(X |x )P(x |e ). (15.6)\nt+k+1 1:t t+k+1 t+k t+k 1:t\nxt+k\nNaturally, thiscomputation involves onlythetransition modelandnotthesensormodel.\nIt is interesting to consider what happens as we try to predict further and further into\nthe future. As Exercise 15.2(b) shows, the predicted distribution for rain converges to a\nfixed point (cid:16)0.5,0.5(cid:17), after which it remains constant for all time. This is the stationary\ndistribution ofthe Markov process defined by thetransition model. (Seealso page 537.) A\ngreat deal is known about the properties of such distributions and about the mixing time\u2014\nMIXINGTIME\nroughly, thetime taken to reach the fixedpoint. Inpractical terms, this dooms tofailure any\nattempt topredict the actual state foranumber ofsteps that is morethan asmall fraction of\nthemixing time, unless the stationary distribution itself isstrongly peaked inasmallarea of\nthe state space. Themore uncertainty there is inthe transition model, the shorter willbe the\nmixingtimeandthemorethefutureisobscured.\nIn addition to filtering and prediction, we can use a forward recursion to compute the\nlikelihoodoftheevidencesequence,P(e ). Thisisausefulquantityifwewanttocompare\n1:t\ndifferent temporal models that might have produced the same evidence sequence (e.g., two\ndifferent modelsforthepersistence ofrain). Forthisrecursion, weusealikelihood message\n(cid:3) (X )=P(X ,e ). Itisasimpleexercisetoshowthatthemessagecalculation isidentical\n1:t t t 1:t\ntothatforfiltering:\n(cid:3)\n1:t+1\n= FORWARD((cid:3) 1:t,e t+1).\nHavingcomputed(cid:3) ,weobtaintheactuallikelihood bysummingoutX :\n1:t (cid:12) t\nL = P(e ) = (cid:3) (x ). (15.7)\n1:t 1:t 1:t t\nxt\nNoticethatthelikelihood messagerepresents theprobabilities oflongerandlongerevidence\nsequencesastimegoesbyandsobecomesnumericallysmallerandsmaller,leadingtounder-\nflow problems with floating-point arithmetic. This is an important problem in practice, but\nweshallnotgointosolutions here. 574 Chapter 15. Probabilistic Reasoning overTime\nX X X X\n0 1 k t\nE E E\n1 k t\nFigure 15.3 Smoothing computes P(Xk|e 1:t), the posterior distribution of the state at\nsomepasttimekgivenacompletesequenceofobservationsfrom1tot.\n15.2.2 Smoothing\nAs we said earlier, smoothing is the process of computing the distribution over past states\ngiven evidence up to the present; that is, P(X |e ) for 0 \u2264 k < t. (See Figure 15.3.)\nk 1:t\nInanticipation ofanotherrecursive message-passing approach, wecansplit thecomputation\nintotwoparts\u2014the evidenceuptok andtheevidence fromk+1tot,\nP(X |e ) = P(X |e ,e )\nk 1:t k 1:k k+1:t\n= \u03b1P(X |e )P(e |X ,e ) (usingBayes\u2019rule)\nk 1:k k+1:t k 1:k\n= \u03b1P(X |e )P(e |X ) (usingconditional independence)\nk 1:k k+1:t k\n= \u03b1f \u00d7b . (15.8)\n1:k k+1:t\nwhere \u201c\u00d7\u201d represents pointwise multiplication of vectors. Here we have defined a \u201cback-\nward\u201dmessage b =P(e |X ), analogous tothe forward message f . Theforward\nk+1:t k+1:t k 1:k\nmessage f can becomputed byfiltering forward from 1to k, as givenby Equation (15.5).\n1:k\nIt turns out that the backward message b can be computed by a recursive process that\nk+1:t\nrunsbackwardfrom t:\n(cid:12)\nP(e |X ) = P(e |X ,x )P(x |X ) (conditioning onX )\nk+1:t k k+1:t k k+1 k+1 k k+1\nx (cid:12)k+1\n= P(e |x )P(x |X ) (byconditional independence)\nk+1:t k+1 k+1 k\nx (cid:12)k+1\n= P(e ,e |x )P(x |X )\nk+1 k+2:t k+1 k+1 k\nx (cid:12)k+1\n= P(e |x )P(e |x )P(x |X ), (15.9)\nk+1 k+1 k+2:t k+1 k+1 k\nxk+1\nwherethelast stepfollows bytheconditional independence ofe and e ,given X .\nk+1 k+2:t k+1\nOfthethreefactorsinthissummation,thefirstandthirdareobtaineddirectlyfromthemodel,\nandthesecondisthe\u201crecursive call.\u201d Usingthemessagenotation, wehave\nb\nk+1:t\n= BACKWARD(b k+2:t,e k+1),\nwhereBACKWARDimplementstheupdatedescribedinEquation(15.9). Aswiththeforward\nrecursion, thetimeandspaceneededforeachupdateareconstantandthusindependent oft.\nWecan now see that the twoterms in Equation (15.8) can both be computed by recur-\nsions through time, one running forward from 1 to k and using the filtering equation (15.5) Section15.2. Inference inTemporalModels 575\nand the other running backward from t to k + 1 and using Equation (15.9). Note that the\nbackward phase is initialized with b =P(e |X )=P( |X )1, where 1 is a vector of\nt+1:t t+1:t t t\n1s. (Becausee isanemptysequence, theprobability ofobserving itis1.)\nt+1:t\nLet us now apply this algorithm to the umbrella example, computing the smoothed\nestimate for the probability of rain at time k=1, given the umbrella observations on days 1\nand2. FromEquation(15.8),thisisgivenby\nP(R |u ,u )= \u03b1P(R |u )P(u |R ). (15.10)\n1 1 2 1 1 2 1\nThe first term we already know to be (cid:16).818,.182(cid:17), from the forward filtering process de-\nscribed earlier. The second term can be computed by applying the backward recursion in\nEquation(15.9):\n(cid:12)\nP(u |R ) = P(u |r )P( |r )P(r |R )\n2 1 2 2 2 2 1\nr2\n= (0.9\u00d71\u00d7(cid:16)0.7,0.3(cid:17))+(0.2\u00d71\u00d7(cid:16)0.3,0.7(cid:17)) = (cid:16)0.69,0.41(cid:17).\nPluggingthisintoEquation(15.10), wefindthatthesmoothedestimateforrainonday1is\nP(R |u ,u )= \u03b1(cid:16)0.818,0.182(cid:17)\u00d7(cid:16)0.69,0.41(cid:17) \u2248 (cid:16)0.883,0.117(cid:17).\n1 1 2\nThus, the smoothed estimate for rain on day 1 is higher than the filtered estimate (0.818) in\nthis case. This is because the umbrella on day 2 makes it more likely to have rained on day\n2;inturn,becauseraintendstopersist, thatmakesitmorelikelytohaverainedonday1.\nBoth the forward and backward recursions take a constant amount of time per step;\nhence, the time complexity of smoothing with respect to evidence e is O(t). This is the\n1:t\ncomplexity for smoothing at a particular time step k. If we want to smooth the whole se-\nquence, one obvious method is simply to run the whole smoothing process once for each\ntime step to be smoothed. This results in a time complexity of O(t2). A better approach\nusesasimpleapplication ofdynamicprogramming toreducethecomplexitytoO(t). Aclue\nappears in the preceding analysis of the umbrella example, where wewere able to reuse the\nresults of the forward-filtering phase. The key to the linear-time algorithm is to record the\nresults of forward filtering over the whole sequence. Then we run the backward recursion\nfrom tdownto1,computing thesmoothed estimate ateachstep k fromthecomputed back-\nward message b and the stored forward message f . The algorithm, aptly called the\nk+1:t 1:k\nFORWARD\u2013\nforward\u2013backwardalgorithm,isshowninFigure15.4.\nBACKWARD\nALGORITHM\nThe alert reader will have spotted that the Bayesian network structure shown in Fig-\nure 15.3 is a polytree as defined on page 528. This means that a straightforward application\nof the clustering algorithm also yields a linear-time algorithm that computes smoothed es-\ntimates for the entire sequence. It is now understood that the forward\u2013backward algorithm\nis in fact a special case of the polytree propagation algorithm used with clustering methods\n(although thetwoweredevelopedindependently).\nTheforward\u2013backwardalgorithmformsthecomputationalbackboneformanyapplica-\ntionsthatdealwithsequences ofnoisyobservations. Asdescribed sofar,ithastwopractical\ndrawbacks. Thefirstisthatitsspacecomplexitycanbetoohighwhenthestatespaceislarge\nandthesequences arelong. Ituses O(|f|t)spacewhere|f|isthesizeoftherepresentation of\nthe forward message. Thespace requirement can be reduced to O(|f|logt) witha concomi- 576 Chapter 15. Probabilistic Reasoning overTime\ntant increase inthe time complexity by afactor of logt, asshown in Exercise 15.3. In some\ncases(seeSection15.3),aconstant-space algorithm canbeused.\nThe second drawback of the basic algorithm is that it needs to be modified to work\nin an online setting where smoothed estimates must be computed for earlier time slices as\nnew observations are continuously added to the end of the sequence. The most common\nFIXED-LAG requirement is for fixed-lag smoothing, which requires computing the smoothed estimate\nSMOOTHING\nP(X t\u2212d|e 1:t) for fixed d. That is, smoothing is done for the time slice d steps behind the\ncurrent time t; as t increases, the smoothing has to keep up. Obviously, we can run the\nforward\u2013backward algorithm over the d-step \u201cwindow\u201d as each new observation is added,\nbutthisseemsinefficient. InSection15.3, wewillseethatfixed-lagsmoothing can, insome\ncases,bedoneinconstanttimeperupdate,independent ofthelagd.\n15.2.3 Finding the mostlikely sequence\nSuppose that [true,true,false,true,true] is the umbrella sequence for the security guard\u2019s\nfirst five days on the job. What is the weather sequence most likely to explain this? Does\nthe absence of the umbrella on day 3 mean that it wasn\u2019t raining, or did the director forget\nto bring it? If it didn\u2019t rain on day 3, perhaps (because weather tends to persist) it didn\u2019t\nrain on day 4 either, but the director brought the umbrella just in case. In all, there are 25\npossibleweathersequenceswecouldpick. Isthereawaytofindthemostlikelyone,shortof\nenumerating allofthem?\nWecouldtrythislinear-timeprocedure: usesmoothingtofindtheposteriordistribution\nfortheweatherateachtimestep;thenconstruct thesequence, usingateachsteptheweather\nthat is most likely according to the posterior. Such an approach should set off alarm bells\nin the reader\u2019s head, because the posterior distributions computed by smoothing are distri-\nfunctionFORWARD-BACKWARD(ev,prior)returnsavectorofprobabilitydistributions\ninputs:ev,avectorofevidencevaluesforsteps1,...,t\nprior,thepriordistributionontheinitialstate,P(X )\n0\nlocalvariables: fv,avectorofforwardmessagesforsteps0,...,t\nb,arepresentationofthebackwardmessage,initiallyall1s\nsv,avectorofsmoothedestimatesforsteps1,...,t\nfv[0]\u2190prior\nfori= 1totdo\nfv[i]\u2190FORWARD(fv[i\u22121],ev[i])\nfori= tdownto1do\nsv[i]\u2190NORMALIZE(fv[i]\u00d7b)\nb\u2190BACKWARD(b,ev[i])\nreturnsv\nFigure15.4 Theforward\u2013backwardalgorithmforsmoothing: computingposteriorprob-\nabilities of a sequence of states given a sequence of observations. The FORWARD and\nBACKWARDoperatorsaredefinedbyEquations(15.5)and(15.9),respectively. Section15.2. Inference inTemporalModels 577\nRain Rain Rain Rain Rain\n1 2 3 4 5\ntrue true true true true\n(a)\nfalse false false false false\nUmbrellat true true false true true\n.8182 .5155 .0361 .0334 .0210\n(b)\n.1818 .0491 .1237 .0173 .0024\nm m m m m\n1:1 1:2 1:3 1:4 1:5\nFigure15.5 (a)PossiblestatesequencesforRaintcanbeviewedaspathsthroughagraph\nof the possible states ateach time step. (States are shownas rectanglesto avoid confusion\nwith nodes in a Bayes net.) (b) Operationof the Viterbi algorithm for the umbrella obser-\nvationsequence[true,true,false,true,true]. Foreacht, wehaveshownthevaluesofthe\nmessagem 1:t,whichgivestheprobabilityofthebestsequencereachingeachstateattimet.\nAlso,foreachstate,theboldarrowleadingintoitindicatesitsbestpredecessorasmeasured\nbytheproductoftheprecedingsequenceprobabilityandthetransitionprobability.Following\ntheboldarrowsbackfromthemostlikelystateinm givesthemostlikelysequence.\n1:5\nbutions over single time steps, whereas to find the most likely sequence we must consider\njoint probabilities over all the time steps. The results can in fact be quite different. (See\nExercise15.4.)\nThere is a linear-time algorithm for finding the most likely sequence, but it requires a\nlittlemorethought. ItreliesonthesameMarkovpropertythatyieldedefficientalgorithmsfor\nfilteringandsmoothing. Theeasiestwaytothinkabouttheproblemistovieweachsequence\nas a path through a graph whose nodes are the possible states at each time step. Such a\ngraph is shown for the umbrella world in Figure 15.5(a). Now consider the task of finding\nthe most likely path through this graph, where the likelihood of any path is the product of\nthe transition probabilities along the path and the probabilities of the given observations at\neach state. Let\u2019s focus in particular on paths that reach the state Rain =true. Because of\n5\ntheMarkovproperty,itfollowsthatthemostlikelypathtothestateRain =true consistsof\n5\nthemostlikelypathtosomestateattime4followedbyatransition toRain =true;andthe\n5\nstateattime4thatwillbecomepartofthepathtoRain =true iswhichevermaximizesthe\n5\nlikelihood of that path. In other words, there is a recursive relationship between most likely\npathstoeachstatex andmostlikelypathstoeachstatex . Wecanwritethisrelationship\nt+1 t\nasanequation connecting theprobabilities ofthepaths:\nmaxP(x ,...,x ,X |e )\n1 t t+1 1:t+1\nx1...xt (cid:13) (cid:14)\n= \u03b1P(e t+1|X t+1)max P(X t+1|x t) max P(x 1,...,x t\u22121,x t|e 1:t) . (15.11)\nxt x1...xt\u22121\nEquation(15.11)isidentical tothefilteringequation (15.5)exceptthat 578 Chapter 15. Probabilistic Reasoning overTime\n1. Theforwardmessage f =P(X |e )isreplacedbythemessage\n1:t t 1:t\nm\n1:t\n= max P(x 1,...,x t\u22121,X t|e 1:t),\nx1...xt\u22121\nthatis,theprobabilities ofthemostlikelypathtoeachstatex ;and\nt\n2. the summation over x in Equation (15.5) is replaced by the maximization over x in\nt t\nEquation(15.11).\nThus,thealgorithmforcomputingthemostlikelysequenceissimilartofiltering: itrunsfor-\nwardalongthesequence,computingthemmessageateachtimestep,usingEquation(15.11).\nThe progress of this computation is shown in Figure 15.5(b). At the end, it will have the\nprobability forthemostlikelysequencereaching eachofthefinalstates. Onecanthuseasily\nselect the most likely sequence overall (the states outlined in bold). In order to identify the\nactualsequence, asopposed tojustcomputing itsprobability, thealgorithm willalsoneedto\nrecord, foreach state, the best state that leads toit; these areindicated by thebold arrowsin\nFigure15.5(b). Theoptimalsequenceisidentifiedbyfollowingtheseboldarrowsbackwards\nfromthebestfinalstate.\nThealgorithmwehavejustdescribediscalledtheViterbialgorithm,afteritsinventor.\nVITERBIALGORITHM\nLike the filtering algorithm, its time complexity is linear in t, the length of the sequence.\nUnlike filtering, which uses constant space, its space requirement is also linear in t. This\nis because the Viterbi algorithm needs to keep the pointers that identify the best sequence\nleadingtoeachstate.\n15.3 HIDDEN MARKOV MODELS\nTheprecedingsectiondevelopedalgorithmsfortemporalprobabilisticreasoningusingagen-\neralframeworkthatwasindependent ofthespecificformofthetransitionandsensormodels.\nIn this and the next two sections, we discuss more concrete models and applications that\nillustrate thepowerofthebasicalgorithms andinsomecasesallowfurtherimprovements.\nHIDDENMARKOV We begin with the hidden Markov model, or HMM. An HMM is a temporal proba-\nMODEL\nbilistic modelinwhichthestateoftheprocess isdescribed byasingle discrete random vari-\nable. The possible values of the variable are the possible states of the world. The umbrella\nexample described in the preceding section is therefore an HMM, since it has just one state\nvariable: Rain . Whathappensifyouhaveamodelwithtwoormorestatevariables? Youcan\nt\nstill fititinto theHMMframework by combining the variables into asingle \u201cmegavariable\u201d\nwhose values are all possible tuples of values of the individual state variables. We will see\nthattherestricted structure ofHMMsallowsforasimpleand elegantmatriximplementation\nofallthebasicalgorithms.4\n4 ThereaderunfamiliarwithbasicoperationsonvectorsandmatricesmightwishtoconsultAppendixAbefore\nproceedingwiththissection. Section15.3. HiddenMarkovModels 579\n15.3.1 Simplifiedmatrix algorithms\nWith a single, discrete state variable X , we can give concrete form to the representations\nt\nof the transition model, the sensor model, and the forward and backward messages. Let the\nstatevariableX havevaluesdenotedbyintegers1,...,S,whereS isthenumberofpossible\nt\nstates. Thetransition modelP(X t|X t\u22121)becomesanS\u00d7S matrixT,where\nT\nij\n= P(X t=j|X t\u22121=i).\nThatis,T istheprobabilityofatransitionfromstate itostatej. Forexample,thetransition\nij\nmatrixfortheumbrellaworldis\n(cid:13) (cid:14)\n0.7 0.3\nT = P(X t|X t\u22121)=\n0.3 0.7\n.\nWealso putthesensor modelinmatrixform. Inthis case, because thevalue oftheevidence\nvariable E is known at time t (call it e ), weneed only specify, for each state, how likely it\nt t\nisthatthestatecausese toappear: weneedP(e |X =i)foreachstatei. Formathematical\nt t t\nconvenience we place these values into an S \u00d7 S diagonal matrix, O whose ith diagonal\nt\nentry is P(e |X =i)and whose other entries are 0. Forexample, on day 1 in the umbrella\nt t\nworldofFigure15.5, U =true,andonday3,U =false,so,fromFigure15.2,wehave\n(cid:13) (cid:14)1 (cid:13) 3(cid:14)\n0.9 0 0.1 0\nO = ; O = .\n1 0 0.2 3 0 0.8\nNow,ifweusecolumnvectorstorepresenttheforwardandbackwardmessages,allthecom-\nputations becomesimplematrix\u2013vectoroperations. Theforwardequation (15.5)becomes\n(cid:12)\nf = \u03b1O T f (15.12)\n1:t+1 t+1 1:t\nandthebackwardequation (15.9)becomes\nb = TO b . (15.13)\nk+1:t k+1 k+2:t\nFrom these equations, we can see that the time complexity of the forward\u2013backward algo-\nrithm (Figure 15.4) applied to a sequence of length t is O(S2t), because each step requires\nmultiplying an S-element vector by an S\u00d7S matrix. The space requirement is O(St), be-\ncausetheforwardpassstores tvectorsofsizeS.\nBesides providing an elegant description of the filtering and smoothing algorithms for\nHMMs, the matrix formulation reveals opportunities for improved algorithms. The first is\na simple variation on the forward\u2013backward algorithm that allows smoothing to be carried\nout in constant space, independently ofthe length of the sequence. Theidea isthat smooth-\ningforanyparticulartimeslice krequiresthesimultaneouspresenceofboththeforwardand\nbackwardmessages,f andb ,accordingtoEquation(15.8). Theforward\u2013backwardal-\n1:k k+1:t\ngorithmachievesthisbystoringthefscomputedontheforwardpasssothattheyareavailable\nduring the backward pass. Another way to achieve this is with a single pass that propagates\nboth fandbinthesamedirection. Forexample, the\u201cforward\u201d message fcanbepropagated\nbackwardifwemanipulate Equation(15.12)toworkintheotherdirection:\nf =\n\u03b1(cid:2) (T(cid:12) )\u22121O\u22121\nf .\n1:t t+1 1:t+1\nThemodifiedsmoothing algorithm worksbyfirstrunning thestandard forward passtocom-\nputef (forgetting alltheintermediate results) andthen running thebackward passforboth\nt:t 580 Chapter 15. Probabilistic Reasoning overTime\nfunctionFIXED-LAG-SMOOTHING(et,hmm,d)returnsadistributionoverXt\u2212d\ninputs:et,thecurrentevidencefortimestept\nhmm,ahiddenMarkovmodelwithS\u00d7 S transitionmatrixT\nd,thelengthofthelagforsmoothing\npersistent: t,thecurrenttime,initially1\nf,theforwardmessageP(Xt|e 1:t),initiallyhmm.PRIOR\nB,thed-stepbackwardtransformationmatrix,initiallytheidentitymatrix\net\u2212d:t,double-endedlistofevidencefromt\u2212dtot,initiallyempty\nlocalvariables: Ot\u2212d,Ot,diagonalmatricescontainingthesensormodelinformation\naddettotheendofet\u2212d:t\nOt\u2190diagonalmatrixcontainingP(et|Xt)\nift>dthen\nf\u2190FORWARD(f,et)\nremoveet\u2212d\u22121fromthebeginningofet\u2212d:t\nOt\u2212d\u2190diagonalmatrixcontainingP(et\u2212d|Xt\u2212d)\nB\u2190O\u2212 t\u22121 dT\u22121BTOt\nelseB\u2190BTOt\nt\u2190t +1\nift>dthenreturnNORMALIZE(f \u00d7 B1)elsereturnnull\nFigure 15.6 An algorithm for smoothing with a fixed time lag of d steps, implemented\nas an online algorithm that outputs the new smoothed estimate given the observation for a\nnew time step. Notice that the final output NORMALIZE(f\u00d7B1) is just \u03b1f\u00d7b, by Equa-\ntion(15.14).\nb and ftogether, using them to compute the smoothed estimate ateach step. Since only one\ncopy of each message is needed, the storage requirements are constant (i.e., independent of\nt, the length of the sequence). There are two significant restrictions on this algorithm: it re-\nquires thatthetransition matrixbeinvertible andthatthe sensormodelhavenozeroes\u2014that\nis,thateveryobservation bepossible ineverystate.\nA second area in which the matrix formulation reveals an improvement is in online\nsmoothing with a fixed lag. The fact that smoothing can be done in constant space suggests\nthat there should exist an efficient recursive algorithm for online smoothing\u2014that is, an al-\ngorithm whose time complexity is independent of the length of the lag. Let us suppose that\nthe lag is d; that is, we are smoothing at time slice t\u2212d, where the current time is t. By\nEquation(15.8),weneedtocompute\n\u03b1f 1:t\u2212d\u00d7b t\u2212d+1:t\nforslicet\u2212d. Then,whenanewobservation arrives,weneedtocompute\n\u03b1f 1:t\u2212d+1\u00d7b t\u2212d+2:t+1\nforslicet\u2212d+1. Howcanthisbedoneincrementally? First,wecancomputef 1:t\u2212d+1 from\nf 1:t\u2212d,usingthestandard filteringprocess, Equation(15.5). Section15.3. HiddenMarkovModels 581\nComputing the backward message incrementally istrickier, because there isnosimple\nrelationship between the old backward message b t\u2212d+1:t and the new backward message\nb t\u2212d+2:t+1. Instead, we will examine the relationship between the old backward message\nb t\u2212d+1:t andthebackward messageatthefrontofthesequence, b t+1:t. Todothis,weapply\nEquation(15.13)dtimestoget\n(cid:31)\n(cid:25)t\nb t\u2212d+1:t = TO i b t+1:t = B t\u2212d+1:t1, (15.14)\ni=t\u2212d+1\nwhere the matrix B t\u2212d+1:t is the product of the sequence of T and O matrices. B can be\nthought of as a \u201ctransformation operator\u201d that transforms a later backward message into an\nearlierone. Asimilarequation holds forthenewbackward messages afterthenextobserva-\ntionarrives:\n(cid:31)\nt(cid:25)+1\nb t\u2212d+2:t+1 = TO i b t+2:t+1 = B t\u2212d+2:t+11. (15.15)\ni=t\u2212d+2\nExaminingtheproductexpressions inEquations(15.14)and (15.15),weseethattheyhavea\nsimple relationship: to get the second product, \u201cdivide\u201d the first product by the firstelement\nTO t\u2212d+1, and multiply by the new last element TO t+1. In matrix language, then, there is a\nsimplerelationship betweentheoldandnewBmatrices:\nB t\u2212d+2:t+1 = O\u2212 t\u22121 d+1T\u22121B t\u2212d+1:tTO t+1 . (15.16)\nThisequation providesanincremental updateforthe Bmatrix,whichinturn(through Equa-\ntion (15.15)) allows us to compute the new backward message b t\u2212d+2:t+1. The complete\nalgorithm, whichrequiresstoring andupdating fandB,isshowninFigure15.6.\n15.3.2 HiddenMarkov model example: Localization\nOnpage145,weintroducedasimpleformofthelocalizationproblemforthevacuumworld.\nIn that version, the robot had a single nondeterministic Move action and its sensors reported\nperfectly whether or not obstacles lay immediately to the north, south, east, and west; the\nrobot\u2019sbeliefstatewasthesetofpossiblelocations itcouldbein.\nHere we make the problem slightly more realistic by including a simple probability\nmodel forthe robot\u2019s motion andby allowing fornoise inthesensors. Thestate variable X\nt\nrepresents the location of the robot on the discrete grid; the domain of this variable is the\nset of empty squares {s 1,...,s n}. Let NEIGHBORS(s) be the set of empty squares that are\nadjacent to sand let N(s)be the size ofthat set. Then the transition model for Move action\nsaysthattherobotisequally likelytoendupatanyneighboring square:\nP(X t+1=j|X t=i) = T\nij\n= (1\/N(i)ifj \u2208 NEIGHBORS(i)else0).\nWedon\u2019t know where the robot starts, so wewill assume auniform distribution overall the\nsquares; thatis,P(X =i)=1\/n. Fortheparticularenvironment weconsider(Figure15.7),\n0\nn=42andthetransition matrix Thas42\u00d742=1764entries.\nThesensorvariable E has16possiblevalues,eachafour-bitsequencegivingthepres-\nt\nence or absence of an obstacle in a particular compass direction. We will use the notation 582 Chapter 15. Probabilistic Reasoning overTime\n(a)Posteriordistribution overrobotlocation after E = NSW\n1\n(b)Posteriordistribution overrobotlocation after E = NSW,E = NS\n1 2\nFigure15.7 Posteriordistributionoverrobotlocation: (a)one observationE =NSW;\n1\n(b)afterasecondobservationE =NS.Thesizeofeachdiskcorrespondstotheprobability\n2\nthattherobotisatthatlocation.Thesensorerrorrateis (cid:2)=0.2.\nNS,forexample,tomeanthatthenorthandsouthsensorsreport anobstacleandtheeastand\nwestdonot. Supposethateachsensor\u2019serrorrateis (cid:2)andthaterrorsoccurindependently for\nthefoursensordirections. Inthatcase,theprobability of gettingallfourbitsrightis(1\u2212(cid:2))4\nandtheprobabilityofgettingthemallwrongis(cid:2)4. Furthermore,ifd isthediscrepancy\u2014the\nit\nnumberofbitsthataredifferent\u2014between thetruevaluesforsquare iandtheactualreading\ne ,thentheprobability thatarobotinsquare iwouldreceiveasensorreading e is\nt t\nP(E =e |X =i) = O = (1\u2212(cid:2))4\u2212dit(cid:2)dit .\nt t t tii\nForexample,theprobabilitythatasquarewithobstaclestothenorthandsouthwouldproduce\nasensorreading NSE is(1\u2212(cid:2))3(cid:2)1.\nGiven the matrices T and O , the robot can use Equation (15.12) to compute the pos-\nt\nterior distribution over locations\u2014that is, to work out where it is. Figure 15.7 shows the\ndistributions P(X |E =NSW)andP(X |E =NSW,E =NS). Thisisthesamemaze\n1 1 2 1 2\nwesawbefore in Figure 4.18(page 146), but there weused logical filtering tofindthe loca-\ntions that were possible, assuming perfect sensing. Those same locations are still the most\nlikelywithnoisysensing, butnoweverylocation hassomenonzero probability.\nIn addition to filtering to estimate its current location, the robot can use smoothing\n(Equation (15.13)) to work out where it was at any given past time\u2014for example, where it\nbegan attime 0\u2014and it can use theViterbi algorithm towork out the mostlikely path it has Section15.3. HiddenMarkovModels 583\n6 1\n5.5 \u03b5 = 0.20 0.9\n5 \u03b5 = 0.10\n\u03b5 = 0.05 0.8 4.5\n\u03b5 = 0.02 0.7\n4 \u03b5 = 0.00\n3.5 0.6 \u03b5 = 0.00\n3 0.5 \u03b5 = 0.02\n2.5 0.4 \u03b5 = 0.05\n2 \u03b5 = 0.10\n1.5 0.3 \u03b5 = 0.20\n1 0.2\n0.5 0.1\n0 5 10 15 20 25 30 35 40 0 5 10 15 20 25 30 35 40\nNumber of observations Number of observations\n(a) (b)\nFigure15.8 PerformanceofHMMlocalizationasafunctionofthelengthoftheobserva-\ntionsequenceforvariousdifferentvaluesofthesensorerrorprobability(cid:2);dataaveragedover\n400runs.(a)Thelocalizationerror,definedastheManhattandistancefromthetruelocation.\n(b)TheViterbipathaccuracy,definedasthefractionofcorrectstatesontheViterbipath.\ntakentogetwhereitisnow. Figure15.8showsthelocalizationerrorandViterbipathaccuracy\nforvarious values of the per-bit sensor errorrate (cid:2). Evenwhen(cid:2) is20%\u2014which means that\ntheoverallsensorreadingiswrong59%ofthetime\u2014therobot isusuallyabletoworkoutits\nlocation within two squares after 25 observations. This is because of the algorithm\u2019s ability\ntointegrate evidenceovertimeandtotakeintoaccount theprobabilistic constraints imposed\non the location sequence by the transition model. When (cid:2) is 10%, the performance after\na half-dozen observations is hard to distinguish from the performance with perfect sensing.\nExercise15.7asksyoutoexplore howrobust theHMMlocalization algorithm istoerrorsin\nthepriordistribution P(X )andinthetransition modelitself. Broadly speaking, highlevels\n0\nof localization and path accuracy are maintained even in the face of substantial errors in the\nmodelsused.\nThe state variable for the example we have considered in this section is a physical\nlocation in the world. Other problems can, of course, include other aspects of the world.\nExercise15.8asksyoutoconsideraversionofthevacuumrobotthathasthepolicyofgoing\nstraight for as long as it can; only when it encounters an obstacle does it change to a new\n(randomly selected) heading. To model this robot, each state in the model consists of a\n(location, heading) pair. For the environment in Figure 15.7, which has 42 empty squares,\nthisleadsto168statesandatransitionmatrixwith1682=28,224entries\u2014stillamanageable\nnumber. Ifweaddthepossibility ofdirtinthesquares, thenumberofstates ismultiplied by\n242 and the transition matrix ends up with more than 1029 entries\u2014no longer a manageable\nnumber; Section 15.5showshowtousedynamic Bayesian networks tomodeldomains with\nmany state variables. If we allow the robot to move continuously rather than in a discrete\ngrid,thenumberofstatesbecomesinfinite;thenextsection showshowtohandlethiscase.\nrorre\nnoitazilacoL\nycarucca\nhtaP 584 Chapter 15. Probabilistic Reasoning overTime\n15.4 KALMAN FILTERS\nImagine watching a small bird flying through dense jungle foliage at dusk: you glimpse\nbrief,intermittent flashesofmotion;youtryhardtoguesswherethebirdisandwhereitwill\nappear next so that you don\u2019t lose it. Orimagine that you are a World WarII radar operator\npeeringatafaint,wanderingblipthatappearsonceevery10secondsonthescreen. Or,going\nback further still, imagine you are Kepler trying to reconstruct the motions of the planets\nfromacollectionofhighlyinaccurateangularobservationstakenatirregularandimprecisely\nmeasuredintervals. Inallthesecases,youaredoingfiltering: estimatingstatevariables(here,\nposition and velocity) from noisy observations over time. If the variables were discrete, we\ncould model the system with a hidden Markov model. This section examines methods for\nhandling continuous variables, using an algorithm called Kalman filtering, after one of its\nKALMANFILTERING\ninventors, RudolfE.Kalman.\nThebird\u2019sflightmightbespecifiedbysixcontinuousvariablesateachtimepoint;three\nforposition(X ,Y ,Z )andthreeforvelocity(X\u02d9 ,Y\u02d9 ,Z\u02d9 ). Wewillneedsuitableconditional\nt t t t t t\ndensities to represent the transition and sensor models; as in Chapter 14, we will use linear\nGaussian distributions. Thismeans that the next state X mustbe alinear function of the\nt+1\ncurrentstateX ,plussomeGaussiannoise,aconditionthatturnsouttobequitereasonablein\nt\npractice. Consider, forexample,the X-coordinate ofthebird,ignoring theothercoordinates\nfor now. Let the time interval between observations be \u0394, and assume constant velocity\nduringtheinterval;thenthepositionupdateisgivenbyX = X +X\u02d9 \u0394. AddingGaussian\nt+\u0394 t\nnoise(toaccount forwindvariation, etc.),weobtainalinearGaussiantransition model:\nP(X =x |X =x ,X\u02d9 =x\u02d9 )= N(x +x\u02d9 \u0394,\u03c32)(x ).\nt+\u0394 t+\u0394 t t t t t t t+\u0394\nTheBayesiannetworkstructureforasystemwithpositionvectorX andvelocityX\u02d9 isshown\nt t\nin Figure 15.9. Note that this is a very specific form of linear Gaussian model; the general\nformwillbedescribed laterinthissectionandcoversavast arrayofapplications beyondthe\nsimplemotionexamplesofthefirstparagraph. ThereadermightwishtoconsultAppendixA\nfor some of the mathematical properties of Gaussian distributions; for our immediate pur-\nMULTIVARIATE poses, the most important is that a multivariate Gaussian distribution for d variables is\nGAUSSIAN\nspecifiedbyad-elementmean\u03bcandad\u00d7dcovariance matrix \u03a3.\n15.4.1 Updating Gaussiandistributions\nInChapter14onpage521,wealludedtoakeypropertyofthelinearGaussianfamilyofdis-\ntributions: itremainsclosedunderthestandardBayesiannetworkoperations. Here,wemake\nthis claim precise in the context of filtering in a temporal probability model. The required\nproperties correspond tothetwo-stepfilteringcalculation inEquation(15.5):\n1. Ifthecurrent distribution P(X |e )isGaussian andthetransition model P(X |x )\nt 1:t t+1 t\nislinearGaussian,thentheone-step predicted distribution givenby\n(cid:26)\nP(X |e )= P(X |x )P(x |e )dx (15.17)\nt+1 1:t t+1 t t 1:t t\nxt\nisalsoaGaussiandistribution. Section15.4. KalmanFilters 585\nX t X t+1\nX t X t+1\nZ t Z t+1\nFigure15.9 BayesiannetworkstructureforalineardynamicalsystemwithpositionXt,\nvelocityX\u02d9 t,andpositionmeasurementZt.\n2. IfthepredictionP(X |e )isGaussianandthesensormodelP(e |X )islinear\nt+1 1:t t+1 t+1\nGaussian,then,afterconditioning onthenewevidence, theupdated distribution\nP(X |e )= \u03b1P(e |X )P(X |e ) (15.18)\nt+1 1:t+1 t+1 t+1 t+1 1:t\nisalsoaGaussiandistribution.\nThus, the FORWARD operator for Kalman filtering takes a Gaussian forward message f 1:t,\nspecifiedbyamean\u03bc andcovariance matrix \u03a3 ,andproducesanewmultivariate Gaussian\nt t\nforward message f , specified by a mean \u03bc and covariance matrix \u03a3 . So, if we\n1:t+1 t+1 t+1\nstartwithaGaussianprior f =P(X )=N(\u03bc ,\u03a3 ),filteringwithalinearGaussianmodel\n1:0 0 0 0\nproduces aGaussianstatedistribution foralltime.\nThis seems to be a nice, elegant result, but why is it so important? The reason is that,\nexcept for a few special cases such as this, filtering with continuous or hybrid (discrete and\ncontinuous)networksgeneratesstatedistributionswhoserepresentationgrowswithoutbound\nover time. This statement is not easy to prove in general, but Exercise 15.10 shows what\nhappens forasimpleexample.\n15.4.2 A simpleone-dimensional example\nWe have said that the FORWARD operator for the Kalman filtermaps a Gaussian into a new\nGaussian. Thistranslates into computing anew meanand covariance matrixfrom theprevi-\nous mean and covariance matrix. Deriving the update rule in the general (multivariate) case\nrequiresratheralotoflinearalgebra,sowewillsticktoaverysimpleunivariatecasefornow;\nand later give the results for the general case. Even for the univariate case, the calculations\nare somewhat tedious, but we feel that they are worth seeing because the usefulness of the\nKalmanfilteristiedsointimatelytothemathematicalproperties ofGaussiandistributions.\nThetemporalmodelweconsiderdescribesarandomwalkofasinglecontinuousstate\nvariableX withanoisyobservationZ . Anexamplemightbethe\u201cconsumerconfidence\u201din-\nt t\ndex,whichcanbemodeledasundergoing arandomGaussian-distributed changeeachmonth\nandismeasuredbyarandomconsumersurveythatalsointroducesGaussiansamplingnoise. 586 Chapter 15. Probabilistic Reasoning overTime\nThepriordistribution isassumedtobeGaussianwithvariance \u03c32:\n0\n\u201e \u00ab\n\u22121\n(x0\u2212\u03bc0)2\nP(x 0) = \u03b1e 2 \u03c302 .\n(Forsimplicity, weusethesamesymbol\u03b1forallnormalizing constants inthissection.) The\ntransition modeladdsaGaussianperturbation ofconstant variance \u03c32 tothecurrentstate:\nx\n\u201e \u00ab\n\u22121\n(xt+1\u2212xt)2\nP(x |x )= \u03b1e 2 \u03c3x2 .\nt+1 t\nThesensormodelassumesGaussiannoisewithvariance \u03c32:\nz\n\u201e \u00ab\n\u22121\n(zt\u2212xt)2\nP(z |x )= \u03b1e 2 \u03c3z2 .\nt t\nNow,giventhepriorP(X ),theone-steppredicteddistributioncomesfromEquation(15.17):\n0\n(cid:26) (cid:26) \u201e \u00ab \u201e \u00ab\n\u221e \u221e \u22121 (x1\u2212x0)2 \u22121 (x0\u2212\u03bc0)2\nP(x 1) = P(x 1|x 0)P(x 0)dx\n0\n= \u03b1 e 2 \u03c3x2 e 2 \u03c302 dx\n0\n\u2212\u221e \u2212\u221e\n(cid:26) \u201e \u00ab\n\u221e \u22121 \u03c302(x1\u2212x0)2+\u03c3x2(x0\u2212\u03bc0)2\n= \u03b1 e 2 \u03c302\u03c3x2 dx\n0\n.\n\u2212\u221e\nThisintegrallooksrathercomplicated. Thekeytoprogress istonoticethattheexponentisthe\nsumoftwoexpressionsthatarequadraticinx andhenceisitselfaquadraticinx . Asimple\n0 0\nCOMPLETINGTHE trick known as completing the square allows the rewriting of any quadratic ax2 +bx +c\nSQUARE 0 0\nas the sum of asquared term a(x \u2212 \u2212b)2 and a residual term c\u2212 b2 that is independent of\n0 2a 4a\nx . Theresidual termcanbetakenoutside theintegral,giving us\n0\n\u201c \u201d(cid:26)\nP(x 1) =\n\u03b1e\u2212 21 c\u2212 4b2\na\n\u221e e\u22121 2(a(x0\u2212\u2212 2ab)2)\ndx\n0\n.\n\u2212\u221e\nNowtheintegralisjusttheintegralofaGaussianoveritsfullrange,whichissimply1. Thus,\nweare left with only the residual term from the quadratic. Then, wenotice that the residual\ntermisaquadratic inx ;infact,aftersimplification, weobtain\n1\n\u201e \u00ab\n\u22121\n(x1\u2212\u03bc0)2\nP(x 1) = \u03b1e 2 \u03c302+\u03c3x2 .\nThatis,theone-steppredicteddistributionisaGaussianwiththesamemean\u03bc andavariance\n0\nequaltothesumoftheoriginal variance \u03c32 andthetransition variance \u03c32.\n0 x\nTo complete the update step, we need to condition on the observation at the first time\nstep,namely,z . FromEquation(15.18), thisisgivenby\n1\nP(x |z ) = \u03b1P(z |x )P(x )\n1 1 1 1 1\n\u201e \u00ab \u201e \u00ab\n\u22121\n(z1\u2212x1)2\n\u22121\n(x1\u2212\u03bc0)2\n= \u03b1e 2 \u03c3z2 e 2 \u03c302+\u03c3x2 .\nOnceagain,wecombinetheexponents andcompletethesquare (Exercise15.11), obtaining\n0 1\n\u22121B\nB(x1\u2212(\u03c302 \u03c3+ 02\u03c3 +x2 \u03c3)z x21 ++ \u03c3\u03c3 z2z2\u03bc0)2\nC\nC\n2@ (\u03c302+\u03c3x2)\u03c3z2\/(\u03c302+\u03c3x2+\u03c3z2)A\nP(x |z ) = \u03b1e . (15.19)\n1 1 Section15.4. KalmanFilters 587\n0.45\n0.4\n0.35\nP(x | z = 2.5)\n0.3 1 1\n0.25 P(x) 0\n0.2\n0.15\n0.1 P(x)\n1\n0.05\n0 z*\n-10 -5 0 1 5 10\nx position\nFigure 15.10 Stages in the Kalman filter update cycle for a random walk with a prior\ngiven by \u03bc 0=0.0 and \u03c3 0=1.0, transition noise given by \u03c3x=2.0, sensor noise given by\n\u03c3z=1.0,andafirstobservationz 1=2.5(markedonthex-axis). Noticehowtheprediction\nP(x )isflattenedout,relativeto P(x ), bythetransitionnoise. Noticealso thatthemean\n1 0\noftheposteriordistributionP(x |z )isslightlytotheleftoftheobservationz becausethe\n1 1 1\nmeanisaweightedaverageofthepredictionandtheobservation.\nThus,afteroneupdatecycle,wehaveanewGaussiandistribution forthestatevariable.\nFromtheGaussianformulainEquation(15.19),weseethatthenewmeanandstandard\ndeviation canbecalculated fromtheoldmeanandstandard deviation asfollows:\n(\u03c32+\u03c32)z +\u03c32\u03bc (\u03c32+\u03c32)\u03c32\n\u03bc = t x t+1 z t and \u03c32 = t x z . (15.20)\nt+1 \u03c32+\u03c32 +\u03c32 t+1 \u03c32+\u03c32 +\u03c32\nt x z t x z\nFigure15.10showsoneupdatecycleforparticularvaluesofthetransitionandsensormodels.\nEquation (15.20) plays exactly the samerole asthe general filtering equation (15.5) or\ntheHMMfilteringequation (15.12). Becauseofthespecial natureofGaussiandistributions,\nhowever, the equations have some interesting additional properties. First, we can interpret\nthe calculation for the new mean \u03bc as simply a weighted mean of the new observation\nt+1\nz and theold mean \u03bc . Ifthe observation is unreliable, then \u03c32 islarge and wepay more\nt+1 t z\nattention to the old mean; if the old mean is unreliable (\u03c32 is large) or the process is highly\nt\nunpredictable (\u03c32 is large), then we pay more attention to the observation. Second, notice\nx\nthat the update for the variance \u03c32 is independent of the observation. We can therefore\nt+1\ncompute in advance what the sequence of variance values will be. Third, the sequence of\nvariance values converges quickly to a fixed value that depends only on \u03c32 and \u03c32, thereby\nx z\nsubstantially simplifying thesubsequent calculations. (SeeExercise15.12.)\n15.4.3 The general case\nThe preceding derivation illustrates the key property of Gaussian distributions that allows\nKalmanfiltering towork: thefact that the exponent isaquadratic form. Thisistrue not just\nfortheunivariatecase;thefullmultivariateGaussiandistribution hastheform\n\u201c \u201d\nN(\u03bc,\u03a3)(x) = \u03b1e\u2212 21 (x\u2212\u03bc )(cid:4)\u03a3\u22121 (x\u2212\u03bc ) .\n)x(P 588 Chapter 15. Probabilistic Reasoning overTime\nMultiplying outthetermsintheexponent makesitclearthat theexponent isalsoaquadratic\nfunction of the values x in x. As in the univariate case, the filtering update preserves the\ni\nGaussiannatureofthestatedistribution.\nLetusfirstdefinethegeneraltemporalmodelusedwithKalmanfiltering. Boththetran-\nsition model and the sensor model allow for a linear transformation with additive Gaussian\nnoise. Thus,wehave\nP(x |x ) = N(Fx ,\u03a3 )(x )\nt+1 t t x t+1\n(15.21)\nP(z |x ) = N(Hx ,\u03a3 )(z ),\nt t t z t\nwhere F and \u03a3 are matrices describing the linear transition model and transition noise co-\nx\nvariance,andHand\u03a3 arethecorresponding matricesforthesensormodel. Nowthe update\nz\nequations forthemeanandcovariance, intheirfull,hairyhorribleness, are\n\u03bc = F\u03bc +K (z \u2212HF\u03bc )\nt+1 t t+1 t+1 t (15.22)\n\u03a3 = (I\u2212K H)(F\u03a3 F(cid:12) +\u03a3 ),\nt+1 t+1 t x\nwhereK =(F\u03a3 F(cid:12) +\u03a3 )H(cid:12) (H(F\u03a3 F(cid:12) +\u03a3 )H(cid:12) +\u03a3 )\u22121 iscalledtheKalmangain\nt+1 t x t x z\nKALMANGAIN matrix. Believeitornot, these equations make someintuitive sense. Forexample, consider\nMATRIX\nthe update for the mean state estimate \u03bc. The term F\u03bc is the predicted state at t + 1, so\nt\nHF\u03bc is the predicted observation. Therefore, the term z \u2212HF\u03bc represents the errorin\nt t+1 t\nthe predicted observation. This is multiplied by K to correct the predicted state; hence,\nt+1\nK isameasureofhowseriously totakethenewobservation relativetotheprediction. As\nt+1\ninEquation (15.20), wealsohavetheproperty thatthevariance update isindependent ofthe\nobservations. The sequence of values for \u03a3 and K can therefore be computed offline, and\nt t\ntheactualcalculations requiredduringonlinetracking arequitemodest.\nToillustrate these equations at work, we have applied them to the problem of tracking\nanobject moving on theX\u2013Y plane. Thestate variables are X = (X,Y,X\u02d9,Y\u02d9)(cid:12) , soF, \u03a3 ,\nx\nH, and \u03a3 are 4\u00d74 matrices. Figure 15.11(a) shows the true trajectory, a series of noisy\nz\nobservations, and the trajectory estimated by Kalman filtering, along with the covariances\nindicated by the one-standard-deviation contours. The filtering process does a good job of\ntracking theactualmotion,and,asexpected, thevariancequicklyreaches afixedpoint.\nWe can also derive equations for smoothing as well as filtering with linear Gaussian\nmodels. Thesmoothing resultsareshowninFigure15.11(b). Noticehowthevariance inthe\nposition estimate issharply reduced, except attheends ofthetrajectory (why?),and thatthe\nestimatedtrajectory ismuchsmoother.\n15.4.4 ApplicabilityofKalmanfiltering\nTheKalmanfilteranditselaborationsareusedinavastarrayofapplications. The\u201cclassical\u201d\napplication isinradartracking ofaircraftandmissiles. Relatedapplications includeacoustic\ntracking of submarines and ground vehicles and visual tracking of vehicles and people. In a\nslightly more esoteric vein, Kalman filters are used to reconstruct particle trajectories from\nbubble-chamber photographs and ocean currents from satellite surface measurements. The\nrangeofapplicationismuchlargerthanjustthetrackingofmotion: anysystemcharacterized\nby continuous state variables and noisy measurements will do. Such systems include pulp\nmills,chemicalplants, nuclearreactors, plantecosystems, andnational economies. Section15.4. KalmanFilters 589\n2D filtering 2D smoothing\n12 12\ntrue true\nobserved observed\nsmoothed smoothed\n11 11\n10 10\nY 9 Y 9\n8 8\n7 7\n6 6\n8 10 12 14 16 X18 20 22 24 26 8 10 12 14 16 X18 20 22 24 26\n(a) (b)\nFigure 15.11 (a) Results of Kalman filtering for an object moving on the X\u2013Y plane,\nshowing the true trajectory (left to right), a series of noisy observations, and the trajectory\nestimatedbyKalmanfiltering.Varianceinthepositionestimateisindicatedbytheovals.(b)\nTheresultsofKalmansmoothingforthesameobservationsequence.\nThe fact that Kalman filtering can be applied to a system does not mean that the re-\nsultswillbevalidoruseful. Theassumptions made\u2014alinear Gaussiantransition andsensor\nEXTENDEDKALMAN models\u2014areverystrong. The extendedKalmanfilter(EKF)attemptstoovercomenonlin-\nFILTER(EKF)\nearities in the system being modeled. A system is nonlinear if the transition model cannot\nNONLINEAR\nbe described as a matrix multiplication of the state vector, as in Equation (15.21). The EKF\nworksbymodelingthesystemaslocallylinearinx intheregionofx =\u03bc ,themeanofthe\nt t t\ncurrent state distribution. This works well forsmooth, well-behaved systems and allows the\ntrackertomaintainandupdateaGaussianstatedistributionthatisareasonableapproximation\ntothetrueposterior. AdetailedexampleisgiveninChapter 25.\nWhat does it mean for a system to be \u201cunsmooth\u201d or \u201cpoorly behaved\u201d? Technically,\nit means that there is significant nonlinearity in system response within the region that is\n\u201cclose\u201d (according to the covariance \u03a3 ) to the current mean \u03bc . To understand this idea\nt t\nin nontechnical terms, consider the example of trying to track a bird as it flies through the\njungle. The bird appears to be heading at high speed straight for a tree trunk. The Kalman\nfilter,whetherregularorextended,canmakeonlyaGaussian predictionofthelocationofthe\nbird,andthemeanofthisGaussianwillbecenteredonthetrunk,asshowninFigure15.12(a).\nAreasonablemodelofthebird,ontheotherhand,wouldpredictevasiveactiontoonesideor\nthe other, asshown in Figure 15.12(b). Sucha model is highly nonlinear, because the bird\u2019s\ndecision variessharply depending onitsprecise locationrelativetothetrunk.\nTo handle examples like these, we clearly need a more expressive language for repre-\nsenting thebehaviorofthesystem being modeled. Withinthe control theorycommunity, for\nwhich problems such asevasive maneuvering by aircraft raise the same kinds of difficulties,\nSWITCHINGKALMAN the standard solution isthe switching Kalmanfilter. Inthis approach, multiple Kalman fil-\nFILTER 590 Chapter 15. Probabilistic Reasoning overTime\n(a) (b)\nFigure15.12 Abirdflyingtowardatree(topviews). (a)AKalmanfilterwillpredictthe\nlocation of the bird using a single Gaussian centered on the obstacle. (b) A more realistic\nmodelallowsforthebird\u2019sevasiveaction,predictingthatitwillflytoonesideortheother.\ntersruninparallel, eachusingadifferentmodelofthesystem\u2014forexample,oneforstraight\nflight, one forsharp left turns, and one for sharp right turns. A weighted sum of predictions\nis used, where the weight depends on how well each filter fits the current data. We will see\nin the next section that this is simply a special case of the general dynamic Bayesian net-\nwork model, obtained by adding a discrete \u201cmaneuver\u201d state variable to the network shown\ninFigure15.9. SwitchingKalmanfiltersarediscussed furtherinExercise15.10.\n15.5 DYNAMIC BAYESIAN NETWORKS\nDYNAMICBAYESIAN A dynamic Bayesian network, or DBN, is a Bayesian network that represents a temporal\nNETWORK\nprobability model of the kind described in Section 15.1. We have already seen examples of\nDBNs: theumbrellanetworkinFigure15.2andtheKalmanfilternetworkinFigure15.9. In\ngeneral,eachsliceofaDBNcanhaveanynumberofstatevariablesX andevidencevariables\nt\nE . For simplicity, we assume that the variables and their links are exactly replicated from\nt\nslice toslice andthat theDBNrepresents afirst-orderMarkov process, sothat eachvariable\ncanhaveparentsonlyinitsownsliceortheimmediately preceding slice.\nIt should be clear that every hidden Markov model can be represented as a DBN with\na single state variable and a single evidence variable. It is also the case that every discrete-\nvariableDBNcanberepresented asanHMM;asexplained inSection15.3,wecancombine\nall the state variables in the DBN into a single state variable whose values are all possible\ntuples of values of the individual state variables. Now, if every HMM is a DBN and every\nDBN can be translated into an HMM, what\u2019s the difference? The difference is that, by de- Section15.5. DynamicBayesianNetworks 591\ncomposingthestateofacomplexsystemintoitsconstituentvariables,thecantakeadvantage\nof sparseness in the temporal probability model. Suppose, for example, that a DBN has 20\nBoolean state variables, each of which has three parents in the preceding slice. Then the\nDBNtransitionmodelhas20\u00d723=160probabilities, whereasthecorresponding HMMhas\n220 states and therefore 240, orroughly a trillion, probabilities in the transition matrix. This\nis bad for at least three reasons: first, the HMM itself requires much more space; second,\nthehugetransition matrixmakesHMMinference muchmoreexpensive;andthird,theprob-\nlem of learning such a huge number of parameters makes the pure HMM model unsuitable\nforlarge problems. Therelationship between DBNsand HMMsis roughly analogous to the\nrelationship betweenordinaryBayesiannetworksandfulltabulated jointdistributions.\nWe have already explained that every Kalman filter model can be represented in a\nDBN with continuous variables and linear Gaussian conditional distributions (Figure 15.9).\nItshouldbeclearfromthediscussion attheendofthepreceding sectionthatnoteveryDBN\ncanberepresented byaKalmanfiltermodel. InaKalmanfilter,thecurrentstatedistribution\nisalwaysasinglemultivariate Gaussiandistribution\u2014that is,asingle\u201cbump\u201dinaparticular\nlocation. DBNs, on the other hand, can model arbitrary distributions. For many real-world\napplications, this flexibility is essential. Consider, for example, the current location of my\nkeys. They might be in my pocket, on the bedside table, on the kitchen counter, dangling\nfrom the front door, or locked in the car. A single Gaussian bump that included all these\nplaces would have toallocate significant probability tothe keysbeing inmid-airin thefront\nhall. Aspects of the real world such as purposive agents, obstacles, and pockets introduce\n\u201cnonlinearities\u201d thatrequirecombinationsofdiscreteandcontinuousvariablesinordertoget\nreasonable models.\n15.5.1 Constructing DBNs\nToconstruct aDBN,onemustspecify three kinds ofinformation: thepriordistribution over\nthestatevariables,P(X );thetransitionmodelP(X |X );andthesensormodelP(E |X ).\n0 t+1 t t t\nTo specify the transition and sensor models, one must also specify the topology of the con-\nnections between successive slices and between the state and evidence variables. Because\nthetransition andsensormodelsareassumedtobestationary\u2014the sameforallt\u2014itismost\nconvenient simply tospecify them forthe firstslice. Forexample, thecomplete DBNspeci-\nficationfortheumbrellaworldisgivenbythethree-node networkshowninFigure15.13(a).\nFromthis specification, the complete DBNwithanunbounded numberoftimeslices canbe\nconstructed asneededbycopying thefirstslice.\nLet us now consider a more interesting example: monitoring a battery-powered robot\nmoving in the X\u2013Y plane, as introduced at the end of Section 15.1. First, we need state\nvariables, which will include both X =(X ,Y ) for position and X\u02d9 =(X\u02d9 ,Y\u02d9 ) for velocity.\nt t t t t t\nWe assume some method of measuring position\u2014perhaps a fixed camera or onboard GPS\n(Global Positioning System)\u2014yielding measurements Z . Theposition atthenext timestep\nt\ndepends on the current position and velocity, as in the standard Kalman filter model. The\nvelocity at the next step depends on the current velocity and the state of the battery. We\nadd Battery to represent the actual battery charge level, which has as parents the previous\nt 592 Chapter 15. Probabilistic Reasoning overTime\nBMeter\n1\nR P(R )\nP(R ) 0 1\n0 t 0.7 Battery Battery\n0.7 f 0.3 0 1\nRain Rain\n0 1 X X\n0 1\nR P(U )\n1 1\nt 0.9\nXXt\n0\nX\n1\nf 0.2\nUmbrella 1 Z 1\n(a) (b)\nFigure 15.13 (a) Specification of the prior, transition model, and sensor model for the\numbrellaDBN.Allsubsequentslicesareassumedtobecopiesofslice1. (b)AsimpleDBN\nforrobotmotionintheX\u2013Yplane.\nbatterylevelandthevelocity,andweaddBMeter ,whichmeasuresthebatterychargelevel.\nt\nThisgivesusthebasicmodelshowninFigure15.13(b).\nIt is worth looking in more depth at the nature of the sensor model for BMeter . Let\nt\nus suppose, for simplicity, that both Battery and BMeter can take on discrete values 0\nt t\nthrough 5. Ifthemeterisalwaysaccurate, thentheCPTP(BMeter |Battery )shouldhave\nt t\nprobabilities of 1.0 \u201calong the diagonal\u201d and probabilities of 0.0elsewhere. In reality, noise\nalways creeps into measurements. For continuous measurements, a Gaussian distribution\nwith a small variance might be used.5 For our discrete variables, we can approximate a\nGaussian using a distribution in which the probability of error drops off in the appropriate\nway, so that the probability of a large error is very small. We use the term Gaussian error\nGAUSSIANERROR modeltocoverboththecontinuous anddiscrete versions.\nMODEL\nAnyone with hands-on experience of robotics, computerized process control, or other\nformsofautomaticsensingwillreadilytestifytothefactthatsmallamountsofmeasurement\nnoise are often the least of one\u2019s problems. Real sensors fail. When a sensor fails, it does\nnot necessarily send a signal saying, \u201cOh, by the way, the data I\u2019m about to send you is a\nload of nonsense.\u201d Instead, it simply sends the nonsense. The simplest kind of failure is\ncalledatransientfailure,wherethesensoroccasionallydecidestosendsomenonsense. For\nTRANSIENTFAILURE\nexample, thebattery levelsensormighthaveahabitofsending azerowhensomeonebumps\ntherobot,evenifthebatteryisfullycharged.\nLet\u2019sseewhathappenswhenatransientfailureoccurswithaGaussianerrormodelthat\ndoesn\u2019taccommodatesuchfailures. Suppose,forexample,thattherobotissittingquietlyand\nobserves20consecutivebatteryreadingsof5. Thenthebatterymeterhasatemporaryseizure\n5 Strictlyspeaking,aGaussiandistributionisproblematicbecauseitassignsnonzeroprobabilitytolargenega-\ntivechargelevels.Thebetadistributionissometimesabetterchoiceforavariablewhoserangeisrestricted. Section15.5. DynamicBayesianNetworks 593\nandthenextreadingisBMeter =0. WhatwillthesimpleGaussianerrormodelleadusto\n21\nbelieve about Battery ? According to Bayes\u2019 rule, the answer depends on both the sensor\n21\nmodel P(BMeter =0|Battery ) and the prediction P(Battery |BMeter ). If the\n21 21 21 1:20\nprobabilityofalargesensorerrorissignificantlylesslikelythantheprobabilityofatransition\ntoBattery =0,evenifthelatterisveryunlikely, thentheposteriordistribution willassign\n21\na high probability to the battery\u2019s being empty. A second reading of 0 at t=22 will make\nthisconclusion almostcertain. Ifthetransientfailurethendisappears andthereadingreturns\nto 5 from t=23 onwards, the estimate for the battery level will quickly return to 5, as if by\nmagic. ThiscourseofeventsisillustratedintheuppercurveofFigure15.14(a),whichshows\ntheexpectedvalueofBattery overtime,usingadiscrete Gaussianerrormodel.\nt\nDespitetherecovery,thereisatime(t=22)whentherobotisconvincedthatitsbattery\nis empty; presumably, then, it should send out a mayday signal and shut down. Alas, its\noversimplified sensor model has led it astray. How can this be fixed? Consider a familiar\nexamplefromeverydayhumandriving: onsharpcurvesorsteephills,one\u2019s\u201cfueltankempty\u201d\nwarninglightsometimesturnson. Ratherthanlookingfortheemergency phone, onesimply\nrecallsthatthefuelgaugesometimesgivesaverylargeerrorwhenthefuelissloshingaround\nin the tank. The moral of the story is the following: for the system to handle sensor failure\nproperly, thesensormodelmustincludethepossibility offailure.\nThe simplest kind of failure model for a sensor allows a certain probability that the\nsensor will return some completely incorrect value, regardless of the true state of the world.\nForexample,ifthebatterymeterfailsbyreturning 0,wemightsaythat\nP(BMeter =0|Battery =5)=0.03,\nt t\nwhichispresumably muchlarger thantheprobability assigned bythesimple Gaussian error\nTRANSIENTFAILURE model. Let\u2019s call this the transient failure model. How does it help when we are faced\nMODEL\nwith a reading of 0? Provided that the predicted probability of an empty battery, according\nto the readings so far, is much less than 0.03, then the best explanation of the observation\nBMeter =0isthatthesensorhastemporarilyfailed. Intuitively, wecanthinkofthebelief\n21\naboutthebatterylevelashaving acertainamountof\u201cinertia\u201d thathelpstoovercometempo-\nrary blips in the meter reading. The upper curve in Figure 15.14(b) shows that the transient\nfailuremodelcanhandletransient failureswithoutacatastrophic changeinbeliefs.\nSomuchfortemporary blips. Whataboutapersistent sensorfailure? Sadly,failures of\nthis kind are alltoo common. Ifthesensor returns 20 readings of5followed by20 readings\nof 0, then the transient sensor failure model described in the preceding paragraph willresult\nin the robot gradually coming to believe that its battery is empty when in fact it may be that\nthe meter has failed. The lower curve in Figure 15.14(b) shows the belief \u201ctrajectory\u201d for\nthis case. By t=25\u2014five readings of 0\u2014the robot is convinced that its battery is empty.\nObviously, we would prefer the robot to believe that its battery meter is broken\u2014if indeed\nthisisthemorelikelyevent.\nPERSISTENT Unsurprisingly, to handle persistent failure, we need a persistent failure model that\nFAILUREMODEL\ndescribes how the sensor behaves under normal conditions and after failure. To do this, we\nneed to augment the state of the system with an additional variable, say, BMBroken, that\ndescribes the status of the battery meter. The persistence of failure must be modeled by an 594 Chapter 15. Probabilistic Reasoning overTime\nE(Battery |...5555005555...) E(Battery |...5555005555...)\nt t\n5 5\n4 4\n3 3\n2 2\n1 1\n0 0\nE(Battery |...5555000000...) E(Battery |...5555000000...)\nt t\n-1 -1\n15 20 25 30 15 20 25 30\nTime step t Time step\n(a) (b)\nFigure15.14 (a)Uppercurve:trajectoryoftheexpectedvalueofBatterytforanobserva-\ntionsequenceconsistingofall5sexceptfor0satt=21andt=22,usingasimpleGaussian\nerrormodel.Lowercurve:trajectorywhentheobservationremainsat0fromt=21onwards.\n(b)Thesameexperimentrunwiththetransientfailuremodel. Noticethatthetransientfail-\nureishandledwell,butthepersistentfailureresultsinexcessivepessimismaboutthebattery\ncharge.\nE(Battery |...5555005555...)\nB\n0\nP(B\n1\n)\n5\nt\nt 1.000\nf 0.001 4\nE(Battery |...5555000000...)\nt 3\nBMBroken BMBroken\n0 1\n2\nP(BMBroken |...5555000000...)\nt\n1\nBMeter\n1\n0\nP(BMBroken |...5555005555...)\nt\n-1\nBattery Battery 15 20 25 30\n0 1\nTime step\n(a) (b)\nFigure15.15 (a)ADBN fragmentshowingthesensorstatusvariablerequiredformod-\neling persistentfailureof the battery sensor. (b)Uppercurves: trajectoriesof the expected\nvalueofBatterytforthe\u201ctransientfailure\u201dand\u201cpermanentfailure\u201dobservationssequences.\nLowercurves:probabilitytrajectoriesfor BMBroken giventhetwoobservationsequences.\narc linking BMBroken toBMBroken . This persistence arc has aCPTthat gives asmall\nPERSISTENCEARC 0 1\nprobability of failure in any given time step, say, 0.001, but specifies that the sensor stays\nbroken once it breaks. When the sensor is OK, the sensor model for BMeter is identical to\nthetransientfailuremodel;whenthesensorisbroken,itsaysBMeter isalways0,regardless\noftheactualbatterycharge.\n)yrettaB(E t\n)yrettaB(E t\n)yrettaB(E t Section15.5. DynamicBayesianNetworks 595\nP(R 0) R t0 P 0(R .71 ) P(R 0) R t0 P 0(R .71 ) R t1 P 0(R .72 ) R t2 P 0(R .73 ) R t3 P 0(R .74 )\n0.7 f 0.3 0.7 f 0.3 f 0.3 f 0.3 f 0.3\nRain Rain Rain Rain Rain Rain Rain\n0 1 0 1 2 3 4\nUmbrella 1 Umbrella 1 Umbrella 2 Umbrella 3 Umbrella 4\nR 1 P(U 1 ) R 1 P(U 1 ) R 2 P(U 2 ) R 3 P(U 3 ) R 4 P(U 4 )\nt 0.9 t 0.9 t 0.9 t 0.9 t 0.9\nf 0.2 f 0.2 f 0.2 f 0.2 f 0.2\nFigure15.16 UnrollingadynamicBayesiannetwork: slicesarereplicatedtoaccommo-\ndatetheobservationsequenceUmbrella .Furthersliceshavenoeffectoninferenceswithin\n1:3\ntheobservationperiod.\nThe persistent failure model for the battery sensor is shown in Figure 15.15(a). Its\nperformance on the two data sequences (temporary blip and persistent failure) is shown in\nFigure 15.15(b). There are several things to notice about these curves. First, in the case\nof the temporary blip, the probability that the sensor is broken rises significantly after the\nsecond 0 reading, but immediately drops back to zero once a 5 is observed. Second, in the\ncase of persistent failure, the probability that the sensor is broken rises quickly to almost 1\nand stays there. Finally, once the sensor is known to be broken, the robot can only assume\nthatitsbatterydischargesatthe\u201cnormal\u201drate,asshownby thegraduallydescending levelof\nE(Battery |...).\nt\nSo far, we have merely scratched the surface of the problem of representing complex\nprocesses. The variety of transition models is huge, encompassing topics as disparate as\nmodeling thehumanendocrine system andmodelingmultiple vehicles driving onafreeway.\nSensor modeling is also a vast subfield in itself, but even subtle phenomena, such as sensor\ndrift, sudden decalibration, and the effects of exogenous conditions (such as weather) on\nsensorreadings,canbehandledbyexplicitrepresentation withindynamicBayesiannetworks.\n15.5.2 Exactinference inDBNs\nHaving sketched some ideas for representing complex processes as DBNs, we now turn to\nthe question of inference. In a sense, this question has already been answered: dynamic\nBayesian networks are Bayesian networks, and we already have algorithms for inference in\nBayesian networks. Given a sequence of observations, one can construct the full Bayesian\nnetwork representation of a DBN by replicating slices until the network is large enough to\naccommodate theobservations, asinFigure15.16. Thistechnique, mentioned inChapter14\ninthecontext ofrelational probability models, iscalled unrolling. (Technically, theDBNis\nequivalent to the semi-infinite network obtained by unrolling forever. Slices added beyond\nthe last observation have no effect on inferences within the observation period and can be\nomitted.) Once the DBN is unrolled, one can use any of the inference algorithms\u2014variable\nelimination, clustering methods,andsoon\u2014described inChapter14.\nUnfortunately, a naive application of unrolling would not be particularly efficient. If\nwe want to perform filtering or smoothing with a long sequence of observations e , the\n1:t 596 Chapter 15. Probabilistic Reasoning overTime\nunrolled network would require O(t) space and would thus grow without bound as more\nobservations were added. Moreover, if we simply run the inference algorithm anew each\ntimeanobservation isadded,theinference timeperupdatewillalsoincreaseasO(t).\nLookingbacktoSection15.2.1,weseethatconstanttimeandspaceperfilteringupdate\ncan be achieved if the computation can be done recursively. Essentially, the filtering update\nin Equation (15.5) works by summing out the state variables ofthe previous time step to get\nthe distribution for the new time step. Summing out variables is exactly what the variable\nelimination (Figure14.11)algorithm does,anditturnsoutthatrunning variable elimination\nwith the variables in temporal order exactly mimics the operation of the recursive filtering\nupdate in Equation (15.5). The modified algorithm keeps at most two slices in memory at\nanyonetime: startingwithslice0,weaddslice1,thensumoutslice0,thenaddslice2,then\nsum out slice 1, and soon. In this way, wecan achieve constant space and timeperfiltering\nupdate. (The same performance can be achieved by suitable modifications to the clustering\nalgorithm.) Exercise15.17asksyoutoverifythisfactfortheumbrellanetwork.\nSomuch for the good news; now forthe bad news: It turns out that the \u201cconstant\u201d for\ntheper-updatetimeandspacecomplexityis,inalmostallcases,exponentialinthenumberof\nstate variables. What happens is that, as the variable elimination proceeds, the factors grow\ntoincludeallthestatevariables(or,moreprecisely, allthosestatevariablesthathaveparents\nintheprevioustimeslice). ThemaximumfactorsizeisO(dn+k)andthetotalupdatecostper\nstepisO(ndn+k),wheredisthedomainsizeofthevariablesandk isthemaximumnumber\nofparents ofanystatevariable.\nOf course, this is much less than the cost of HMM updating, which is O(d2n), but it\nis still infeasible for large numbers of variables. This grim fact is somewhat hard to accept.\nWhat it means is that even though we can use DBNs to represent very complex temporal\nprocesses with many sparsely connected variables, we cannot reason efficiently and exactly\nabout those processes. The DBN model itself, which represents the prior joint distribution\noverall the variables, is factorable into its constituent CPTs, but the posterior joint distribu-\ntionconditioned onanobservation sequence\u2014that is,theforwardmessage\u2014isgenerally not\nfactorable. So far, no one has found a way around this problem, despite the fact that many\nimportantareasofscienceandengineeringwouldbenefitenormouslyfromitssolution. Thus,\nwemustfallbackonapproximate methods.\n15.5.3 Approximateinference inDBNs\nSection 14.5 described two approximation algorithms: likelihood weighting (Figure 14.15)\nandMarkovchainMonteCarlo(MCMC,Figure14.16). Ofthetwo,theformerismosteasily\nadapted totheDBNcontext. (AnMCMCfiltering algorithm isdescribed brieflyinthenotes\nattheendofthechapter.) Wewillsee,however,thatseveral improvementsarerequired over\nthestandard likelihood weightingalgorithm beforeapracticalmethodemerges.\nRecall that likelihood weighting works by sampling the nonevidence nodes of the net-\nworkintopologicalorder,weightingeachsamplebythelikelihooditaccordstotheobserved\nevidence variables. As with the exact algorithms, we could apply likelihood weighting di-\nrectly to an unrolled DBN,but this would suffer from the same problems of increasing time Section15.5. DynamicBayesianNetworks 597\nand space requirements per update as the observation sequence grows. The problem is that\nthe standard algorithm runs each sample in turn, all the way through the network. Instead,\nwe can simply run all N samples together through the DBN, one slice at a time. The mod-\nified algorithm fits the general pattern of filtering algorithms, with the set of N samples as\nthe forward message. The first key innovation, then, is to use the samples themselves as an\napproximaterepresentation ofthecurrentstatedistribution. Thismeetstherequirementofa\n\u201cconstant\u201dtimeperupdate,althoughtheconstantdependsonthenumberofsamplesrequired\ntomaintainanaccurateapproximation. Thereisalsononeed tounrolltheDBN,because we\nneedtohaveinmemoryonlythecurrent sliceandthenextslice.\nIn our discussion of likelihood weighting in Chapter 14, we pointed out that the al-\ngorithm\u2019s accuracy suffers if the evidence variables are \u201cdownstream\u201d from the variables\nbeing sampled, because in that case the samples are generated without any influence from\nthe evidence. Looking at the typical structure of a DBN\u2014say, the umbrella DBN in Fig-\nure15.16\u2014weseethatindeedtheearlystatevariableswillbesampledwithoutthebenefitof\nthelaterevidence. Infact, looking morecarefully, weseethatnoneofthestatevariables has\nanyevidence variables amongitsancestors! Hence, although theweightofeachsamplewill\ndepend on the evidence, the actual set of samples generated will be completely independent\nof the evidence. For example, even if the boss brings in the umbrella every day, the sam-\npling process could stillhallucinate endless days ofsunshine. Whatthismeansinpractice is\nthat the fraction of samples that remain reasonably close to the actual series of events (and\ntherefore have nonnegligible weights) drops exponentially with t, the length of the observa-\ntionsequence. Inotherwords, tomaintain agivenlevelofaccuracy, weneed toincrease the\nnumber of samples exponentially with t. Given that a filtering algorithm that works in real\ntimecanuseonlyafixednumberofsamples,whathappensinpracticeisthattheerrorblows\nupafteraverysmallnumberofupdatesteps.\nClearly, we need a better solution. The second key innovation is to focus the set of\nsamples on the high-probability regions of the state space. This can be done by throwing\naway samples that have very low weight, according to the observations, while replicating\nthosethathavehighweight. Inthatway,thepopulationofsampleswillstayreasonablyclose\ntoreality. Ifwethinkofsamplesasaresource formodelingtheposteriordistribution, thenit\nmakessensetousemoresamplesinregionsofthestatespacewheretheposteriorishigher.\nA family of algorithms called particle filtering is designed to do just that. Particle\nPARTICLEFILTERING\nfilteringworksasfollows: First,apopulationofN initial-statesamplesiscreatedbysampling\nfromthepriordistribution P(X ). Thentheupdate cycleisrepeated foreachtimestep:\n0\n1. Each sample is propagated forward by sampling the next state value x given the\nt+1\ncurrentvalue x forthesample,basedonthetransition model P(X |x ).\nt t+1 t\n2. Eachsampleisweightedbythelikelihooditassignstothenewevidence,P(e |x ).\nt+1 t+1\n3. The population is resampled to generate a new population of N samples. Each new\nsample isselected from the current population; theprobability that aparticular sample\nisselectedisproportional toitsweight. Thenewsamplesareunweighted.\nThe algorithm is shown in detail in Figure 15.17, and its operation for the umbrella DBNis\nillustrated inFigure15.18. 598 Chapter 15. Probabilistic Reasoning overTime\nfunctionPARTICLE-FILTERING(e,N,dbn)returnsasetofsamplesforthenexttimestep\ninputs:e,thenewincomingevidence\nN,thenumberofsamplestobemaintained\ndbn,aDBNwithpriorP(X ),transitionmodelP(X |X ),sensormodelP(E |X )\n0 1 0 1 1\npersistent: S,avectorofsamplesofsizeN,initiallygeneratedfromP(X )\n0\nlocalvariables: W,avectorofweightsofsizeN\nfori =1toN do\nS[i]\u2190samplefromP(X | X = S[i]) \/*step1*\/\n1 0\nW[i]\u2190P(e| X = S[i]) \/*step2*\/\n1\nS\u2190WEIGHTED-SAMPLE-WITH-REPLACEMENT(N,S,W) \/*step3*\/\nreturnS\nFigure 15.17 The particle filtering algorithm implemented as a recursive update op-\neration with state (the set of samples). Each of the sampling operations involves sam-\npling the relevant slice variables in topological order, much as in PRIOR-SAMPLE. The\nWEIGHTED-SAMPLE-WITH-REPLACEMENToperationcanbeimplementedtoruninO(N)\nexpectedtime. Thestepnumbersrefertothedescriptioninthetext.\nRain Rain Rain Rain\nt t+1 t+1 t+1\ntrue\nfalse\n(a) Propagate (b) Weight (c) Resample\nFigure15.18 TheparticlefilteringupdatecyclefortheumbrellaDBNwithN=10,show-\ningthesamplepopulationsofeachstate. (a)Attimet,8samplesindicaterain and2indicate\n\u00acrain. Eachispropagatedforwardbysamplingthenextstatethroughthetransitionmodel.\nAttimet+1, 6samplesindicaterain and4indicate\u00acrain. (b)\u00acumbrella isobservedat\nt+1. Eachsampleisweightedbyitslikelihoodfortheobservation,asindicatedbythesize\nofthecircles. (c)Anewsetof10samplesisgeneratedbyweightedrandomselectionfrom\nthecurrentset,resultingin2samplesthatindicaterain and8thatindicate\u00acrain.\nWecanshowthatthisalgorithmisconsistent\u2014givesthecorrectprobabilitiesasN tends\ntoinfinity\u2014byconsideringwhathappensduringoneupdatecycle. Weassumethatthesample\npopulation starts with a correct representation of the forward message f =P(X |e ) at\n1:t t 1:t\ntime t. Writing N(x |e ) for the number of samples occupying state x after observations\nt 1:t t\ne havebeenprocessed, wetherefore have\n1:t\nN(x |e )\/N = P(x |e ) (15.23)\nt 1:t t 1:t\nforlarge N. Nowwepropagateeachsampleforwardbysamplingthestatevariablesatt+1,\ngiven the values for the sample at t. The number of samples reaching state x from each\nt+1 Section15.6. KeepingTrackofManyObjects 599\nx isthetransition probability timesthepopulation of x ;hence, thetotalnumberofsamples\nt t\nreaching x is\nt+1 (cid:12)\nN(x |e ) = P(x |x )N(x |e ).\nt+1 1:t t+1 t t 1:t\nxt\nNowweweighteachsamplebyitslikelihoodfortheevidenceatt+1. Asampleinstatex\nt+1\nreceives weight P(e |x ). The total weight of the samples in x after seeing e is\nt+1 t+1 t+1 t+1\ntherefore\nW(x |e ) = P(e |x )N(x |e ).\nt+1 1:t+1 t+1 t+1 t+1 1:t\nNow for the resampling step. Since each sample is replicated with probability proportional\ntoitsweight,thenumberofsamplesinstatex afterresampling isproportional tothetotal\nt+1\nweightinx beforeresampling:\nt+1\nN(x |e )\/N = \u03b1W(x |e )\nt+1 1:t+1 t+1 1:t+1\n= \u03b1P(e |x )N(x |e )\nt+1 t+1 (cid:12)t+1 1:t\n= \u03b1P(e |x ) P(x |x )N(x |e )\nt+1 t+1 t+1 t t 1:t\nxt(cid:12)\n= \u03b1NP(e |x ) P(x |x )P(x |e ) (by15.23)\nt+1 t+1 t+1 t t 1:t\n(cid:12)xt\n= \u03b1(cid:2) P(e |x ) P(x |x )P(x |e )\nt+1 t+1 t+1 t t 1:t\nxt\n= P(x |e ) (by15.5).\nt+1 1:t+1\nThereforethesamplepopulationafteroneupdatecyclecorrectlyrepresentstheforwardmes-\nsageattimet+1.\nParticlefiltering is consistent, therefore, butisit efficient? Inpractice, itseemsthatthe\nanswerisyes: particle filteringseemstomaintain agoodapproximation tothetrueposterior\nusingaconstantnumberofsamples. Undercertainassumptions\u2014inparticular, thattheprob-\nabilities in the transition and sensor models are strictly greater than 0 and less than 1\u2014it is\npossible to prove that the approximation maintains bounded error with high probability. On\nthe practical side, the range of applications has grown to include many fields of science and\nengineering; somereferences aregivenattheendofthechapter.\n15.6 KEEPING TRACK OF MANY OBJECTS\nThe preceding sections have considered\u2014without mentioning it\u2014state estimation problems\ninvolving a single object. In this section, we see what happens when two or more objects\ngenerate the observations. What makes this case different from plain old state estimation is\nthat there is now thepossibility of uncertainty about which object generated which observa-\ntion. ThisistheidentityuncertaintyproblemofSection14.6.3(page544),nowviewedina\ntemporal context. Inthecontrol theory literature, this is the dataassociation problem\u2014that\nDATAASSOCIATION\nis,theproblem ofassociating observation datawiththeobjectsthatgenerated them. 600 Chapter 15. Probabilistic Reasoning overTime\n1 5 1 5\n2 4 2 4\n3 3\n2 2\n4 4\n1 3 1 3\n5 5\n(a) (b)\ntrack termination\n1 5 1 5\n2 4 2 4\n3 3\ndetection\nfailure\n2 2\n4 4\n1 3 1 3\ntrack\n5 false alarm initiation 5\n(c) (d)\nFigure15.19 (a)Observationsmadeofobjectlocationsin2Dspaceoverfivetimesteps.\nEachobservationislabeledwiththetimestepbutdoesnotidentifytheobjectthatproduced\nit. (b\u2013c) Possible hypothesesabout the underlyingobject tracks. (d) A hypothesisfor the\ncaseinwhichfalsealarms,detectionfailures,andtrackinitiation\/terminationarepossible.\nThe data association problem was studied originally in the context of radar tracking,\nwherereflectedpulsesaredetectedatfixedtimeintervalsbyarotatingradarantenna. Ateach\ntimestep,multipleblipsmayappearonthescreen,butthereisnodirectobservationofwhich\nblips at timet belong to which blips at timet\u22121. Figure 15.19(a) shows a simple example\nwith two blips pertime step forfive steps. Let the two blip locations at time t be e1 and e2.\nt t\n(Thelabelingofblipswithinatimestepas\u201c1\u201dand\u201c2\u201discompletelyarbitraryandcarriesno\ninformation.) Letusassume,forthetimebeing,thatexactlytwoaircraft, AandB,generated\ntheblips; theirtrue positions are XA andXB. Just tokeep things simple, we\u2019ll alsoassume\nt t\nthat the each aircraft moves independently according to a known transition model\u2014e.g., a\nlinearGaussianmodelasusedintheKalmanfilter(Section15.4).\nSuppose we try to write down the overall probability model for this scenario, just as\nwe did for general temporal processes in Equation (15.3) on page 569. As usual, the joint\ndistribution factorsintocontributions foreachtimestep asfollows:\nP(xA ,xB ,e1 ,e2 ) =\n0:t 0:t 1:t 1:t\n(cid:25)t\nP(xA)P(xB) P(xA|xA )P(xB|xB )P(e1,e2|xA,xB). (15.24)\n0 0 i i\u22121 i i\u22121 i i i i\ni=1\nWewould like tofactor the observation term P(e1,e2|xA,xB)into aproduct oftwoterms,\ni i i i\none for each object, but this would require knowing which observation was generated by\nwhichobject. Instead, wehavetosumoverallpossible waysofassociating theobservations Section15.6. KeepingTrackofManyObjects 601\nwith the objects. Some of those ways are shown in Figure 15.19(b\u2013c); in general, for n\nobjectsandT timesteps,thereare (n!)T waysofdoingit\u2014anawfullylargenumber.\nMathematically speaking, the \u201cway of associating the observations with the objects\u201d\nis a collection of unobserved random variable that identify the source of each observation.\nWe\u2019llwrite\u03c9 todenote theone-to-one mappingfromobjects toobservations attimet,with\nt\n\u03c9 (A) and \u03c9 (B) denoting the specific observations (1 or 2) that \u03c9 assigns to A and B.\nt t t\n(For n objects, \u03c9 will have n! possible values; here, n!=2.) Because the labels \u201c1\u201d ad\nt\n\u201c2\u201d on the observations are assigned arbitrarily, the prior on \u03c9 is uniform and \u03c9 is inde-\nt t\npendent of the states of the objects, xA and xB). So we can condition the observation term\nt t\nP(e1,e2|xA,xB)on\u03c9 andthensimplify:\ni i i i t (cid:12)\nP(e1,e2|xA,xB) = P(e1,e2|xA,xB,\u03c9 )P(\u03c9 |xA,xB)\ni i i i i i i i i i i i\n(cid:12)\u03c9i\n= P(e\u03c9i(A)|xA)P(e\u03c9i(B)|xB)P(\u03c9 |xA,xB)\ni i i i i i i\n\u03c9i\n(cid:12)\n1\n=\nP(e\u03c9i(A)|xA)P(e\u03c9i(B)|xB).\n2 i i i i\n\u03c9i\nPlugging this into Equation (15.24), we get an expression that is only in terms of transition\nandsensormodelsforindividual objects andobservations.\nAs for all probability models, inference means summing out the variables other than\nthe query and the evidence. Forfiltering in HMMs and DBNs, wewere able to sum out the\nstatevariablesfrom1tot\u22121byasimpledynamicprogrammingtrick;forKalmanfilters,we\ntookadvantageofspecialpropertiesofGaussians. Fordataassociation, wearelessfortunate.\nThere is no (known) efficient exact algorithm, for the same reason that there is none for the\nswitching Kalman filter (page 589): the filtering distribution P(xA|e1 ,e2 ) for object A\nt 1:t 1:t\nends up as a mixture of exponentially many distributions, one for each way of picking a\nsequence ofobservations toassigntoA.\nAs a result of the complexity of exact inference, many different approximate methods\nhave been used. The simplest approach is to choose a single \u201cbest\u201d assignment at each time\nstep, given the predicted positions of the objects at the current time step. This assignment\nassociates observations with objects and enables the track of each object to be updated and\na prediction made for the next time step. Forchoosing the \u201cbest\u201d assignment, it is common\nNEAREST-NEIGHBOR to use the so-called nearest-neighbor filter, which repeatedly chooses the closest pairing\nFILTER\nof predicted position and observation and adds that pairing to the assignment. The nearest-\nneighborfilterworkswellwhentheobjectsarewellseparatedinstatespaceandtheprediction\nuncertainty and observation error are small\u2014in other words, when there is no possibility of\nconfusion. When there is more uncertainty as to the correct assignment, a better approach\nis to choose the assignment that maximizes the joint probability of the current observations\ngiven the predicted positions. This can be done very efficiently using the Hungarian algo-\nHUNGARIAN rithm(Kuhn,1955), eventhoughtherearen!assignments tochoosefrom.\nALGORITHM\nAnymethod that commits toasingle best assignment ateach timestep fails miserably\nunder more difficult conditions. In particular, if the algorithm commits to an incorrect as-\nsignment, the prediction at the next time step may be significantly wrong, leading to more 602 Chapter 15. Probabilistic Reasoning overTime\n(a) (b)\nFigure15.20 Imagesfrom(a)upstreamand(b)downstreamsurveillancecamerasroughly\ntwo miles apart on Highway 99 in Sacramento, California. The boxed vehicle has been\nidentifiedatbothcameras.\nincorrect assignments, and so on. Two modern approaches turn out to be much more effec-\ntive. Aparticlefilteringalgorithm (seepage598)fordataassociation worksbymaintaining\nalarge collection ofpossible current assignments. An MCMCalgorithm explores thespace\nofassignment histories\u2014for example, Figure15.19(b\u2013c) mightbestatesintheMCMCstate\nspace\u2014and can change its mind about previous assignment decisions. Current MCMCdata\nassociation methods can handle many hundreds of objects in real time while giving a good\napproximation tothetrueposteriordistributions.\nThe scenario described so far involved n known objects generating n observations at\neach time step. Real application of data association are typically much more complicated.\nOften,thereported observations include falsealarms(alsoknownasclutter),whicharenot\nFALSEALARM\ncausedbyrealobjects. Detectionfailurescanoccur,meaningthatnoobservationisreported\nCLUTTER\nforarealobject. Finally,newobjectsarriveandoldonesdisappear. Thesephenomena,which\nDETECTIONFAILURE\ncreateevenmorepossible worldstoworryabout, areillustrated inFigure15.19(d).\nFigure15.20showstwoimagesfromwidelyseparatedcamerasonaCaliforniafreeway.\nInthis application, weareinterested intwogoals: estimating thetimeittakes, undercurrent\ntraffic conditions, to go from one place to another in the freeway system; and measuring\ndemand, i.e., how many vehicles travel between any two points in the system at particular\ntimes of the day and on particular days of the week. Both goals require solving the data\nassociation problem over a wide area with many cameras and tens of thousands of vehicles\nper hour. With visual surveillance, false alarms are caused by moving shadows, articulated\nvehicles,reflectionsinpuddles,etc.;detectionfailuresarecausedbyocclusion,fog,darkness,\nand lack of visual contrast; and vehicles are constantly entering and leaving the freeway\nsystem. Furthermore, the appearance of any given vehicle can change dramatically between\ncameras depending on lighting conditions and vehicle pose in the image, and the transition\nmodelchangesastrafficjamscomeandgo. Despitetheseproblems,moderndataassociation\nalgorithms havebeensuccessful inestimating trafficparameters inreal-world settings. Section15.7. Summary 603\nData association is an essential foundation for keeping track of a complex world, be-\ncausewithoutitthereisnowaytocombinemultipleobservations ofanygivenobject. When\nobjects in the world interact with each other in complex activities, understanding the world\nrequirescombiningdataassociationwiththerelationalandopen-universeprobabilitymodels\nofSection14.6.3. Thisiscurrently anactiveareaofresearch.\n15.7 SUMMARY\nThis chapter has addressed the general problem of representing and reasoning about proba-\nbilistictemporalprocesses. Themainpointsareasfollows:\n\u2022 Thechanging stateoftheworldishandled byusingasetofrandom variables torepre-\nsentthestateateachpointintime.\n\u2022 Representations can be designed to satisfy the Markov property, so that the future\nis independent of the past given the present. Combined with the assumption that the\nprocess is stationary\u2014that is, the dynamics do not change over time\u2014this greatly\nsimplifiestherepresentation.\n\u2022 A temporal probability model can be thought of as containing a transition model de-\nscribingthestateevolution andasensormodeldescribing theobservation process.\n\u2022 The principal inference tasks in temporal models are filtering, prediction, smooth-\ning, and computing the most likely explanation. Each of these can be achieved using\nsimple,recursive algorithmswhoseruntimeislinearinthe lengthofthesequence.\n\u2022 Threefamilies oftemporalmodels werestudied inmoredepth: hiddenMarkovmod-\nels,Kalmanfilters,anddynamicBayesiannetworks(whichinclude theothertwoas\nspecialcases).\n\u2022 Unless special assumptions are made, as in Kalman filters, exact inference with many\nstatevariablesisintractable. Inpractice,theparticlefilteringalgorithmseemstobean\neffectiveapproximation algorithm.\n\u2022 Whentryingtokeeptrackofmanyobjects, uncertainty arisesastowhichobservations\nbelong to which objects\u2014the data association problem. The number of association\nhypotheses is typically intractably large, but MCMC and particle filtering algorithms\nfordataassociation workwellinpractice.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nManyofthebasic ideasforestimating thestateofdynamical systemscamefrom themathe-\nmatician C. F. Gauss (1809), who formulated adeterministic least-squares algorithm for the\nproblem of estimating orbits from astronomical observations. A. A. Markov (1913) devel-\noped what was later called the Markov assumption in his analysis of stochastic processes; 604 Chapter 15. Probabilistic Reasoning overTime\nheestimated afirst-order Markov chain onletters from thetextofEugene Onegin. Thegen-\neraltheoryofMarkovchainsandtheirmixingtimesiscoveredbyLevinetal.(2008).\nSignificantclassifiedworkonfilteringwasdoneduringWorld WarIIbyWiener(1942)\nfor continuous-time processes and by Kolmogorov (1941) for discrete-time processes. Al-\nthough this work led to important technological developments over the next 20 years, its\nuse of a frequency-domain representation made many calculations quite cumbersome. Di-\nrect state-space modeling of the stochastic process turned out to be simpler, as shown by\nPeter Swerling (1959) and Rudolf Kalman (1960). The latter paper described what is now\nknown as the Kalman filter for forward inference in linear systems with Gaussian noise;\nKalman\u2019sresultshad,however,beenobtainedpreviously bytheDanishstatistician Thorvold\nThiele(1880)andbytheRussianmathematicianRuslanStratonovich(1959),whomKalman\nmet in Moscow in 1960. After a visit to NASA Ames Research Center in 1960, Kalman\nsaw the applicability of the method to the tracking of rocket trajectories, and the filter was\nlater implemented forthe Apollo missions. Important results on smoothing were derived by\nRauch et al. (1965), and the impressively named Rauch\u2013Tung\u2013Striebel smoother is still a\nstandard technique today. Many early results are gathered in Gelb (1974). Bar-Shalom and\nFortmann (1988) give amoremodern treatment withaBayesian flavor, aswellas manyref-\nerences tothevastliterature onthesubject. Chatfield(1989) andBoxetal.(1994) coverthe\ncontroltheoryapproach totimeseriesanalysis.\nThe hidden Markov model and associated algorithms for inference and learning, in-\ncluding the forward\u2013backward algorithm, were developed by Baum and Petrie (1966). The\nViterbialgorithmfirstappearedin(Viterbi,1967). Similarideasalsoappearedindependently\nin the Kalman filtering community (Rauch et al., 1965). The forward\u2013backward algorithm\nwas one of the main precursors of the general formulation of the EM algorithm (Dempster\netal.,1977);seealsoChapter20. Constant-spacesmoothingappearsinBinderetal.(1997b),\nas does the divide-and-conquer algorithm developed in Exercise 15.3. Constant-time fixed-\nlag smoothing for HMMs first appeared in Russell and Norvig (2003). HMMs have found\nmanyapplicationsinlanguageprocessing(Charniak,1993),speechrecognition(Rabinerand\nJuang,1993),machinetranslation(OchandNey,2003),computationalbiology(Kroghetal.,\n1994;Baldietal.,1994),financialeconomicsBharandHamori(2004)andotherfields. There\nhave been several extensions to the basic HMM model, for example the Hierarchical HMM\n(Fine et al., 1998) and Layered HMM (Oliver et al., 2004) introduce structure back into the\nmodel,replacing thesinglestatevariableofHMMs.\nDynamic Bayesian networks (DBNs)can be viewedas asparse encoding ofaMarkov\nprocess and were first used in AI by Dean and Kanazawa (1989b), Nicholson and Brady\n(1992), and Kjaerulff (1992). The last work extends the HUGIN Bayes net system to ac-\ncommodate dynamic Bayesian networks. The book by Dean and Wellman (1991) helped\npopularize DBNsand the probabilistic approach to planning and control within AI.Murphy\n(2002)provides athorough analysisofDBNs.\nDynamic Bayesian networks have become popular for modeling a variety of com-\nplex motion processes in computer vision (Huang et al., 1994; Intille and Bobick, 1999).\nLike HMMs, they have found applications in speech recognition (Zweig and Russell, 1998;\nRichardsonetal.,2000;Stephensonetal.,2000;Nefianetal.,2002;Livescuetal.,2003),ge- Bibliographical andHistorical Notes 605\nnomics(MurphyandMian,1999;Perrinetal.,2003;Husmeier,2003)androbotlocalization\n(Theocharous et al., 2004). Thelink between HMMsand DBNs, and between the forward\u2013\nbackward algorithm and Bayesian network propagation, was made explicitly by Smyth et\nal.(1997). Afurtherunification withKalmanfilters(andotherstatistical models) appears in\nRoweisand Ghahramani (1999). Procedures exist forlearning theparameters (Binder etal.,\n1997a;Ghahramani, 1998)andstructures (Friedman etal.,1998)ofDBNs.\nThe particle filtering algorithm described in Section 15.5 has a particularly interesting\nhistory. Thefirstsamplingalgorithmsforparticlefiltering(alsocalledsequentialMonteCarlo\nmethods)weredeveloped inthecontroltheorycommunitybyHandschinandMayne(1969),\nand the resampling idea that is the core of particle filtering appeared in a Russian control\njournal(Zaritskii etal.,1975). Itwaslaterreinvented instatisticsassequentialimportance-\nsamplingresampling,orSIR(Rubin,1988;LiuandChen,1998),incontroltheoryasparti-\nclefiltering (Gordon etal., 1993; Gordon, 1994), inAI as survival of thefittest(Kanazawa\netal.,1995),andincomputervisionascondensation(IsardandBlake,1996). Thepaperby\nEVIDENCE Kanazawaetal.(1995)includes animprovementcalled evidencereversalwherebythestate\nREVERSAL\nattimet+1issampledconditional onboththestateattimetandtheevidenceattimet+1.\nThis allows the evidence to influence sample generation directly and was proved by Doucet\n(1997)andLiuandChen(1998)toreducetheapproximationerror. Particlefilteringhasbeen\nappliedinmanyareas,includingtrackingcomplexmotionpatternsinvideo(IsardandBlake,\n1996), predicting the stock market (de Freitas et al., 2000), and diagnosing faults on plane-\nRAO-\ntary rovers (Verma et al., 2004). A variant called the Rao-Blackwellized particle filter or\nBLACKWELLIZED\nPARTICLEFILTER\nRBPF (Doucet et al., 2000; Murphy and Russell, 2001) applies particle filtering to a subset\nofstate variables and, foreach particle, performs exact inference on the remaining variables\nconditionedonthevaluesequenceintheparticle. InsomecasesRBPFworkswellwiththou-\nsands of state variables. An application of RBPF to localization and mapping in robotics is\ndescribedinChapter25. ThebookbyDoucetetal.(2001)collectsmanyimportantpaperson\nSEQUENTIALMONTE sequentialMonteCarlo(SMC)algorithms, ofwhichparticlefilteringisthemostimportant\nCARLO\ninstance. Pierre Del Moral and colleagues have performed extensive theoretical analyses of\nSMCalgorithms(DelMoral,2004;DelMoraletal.,2006).\nMCMC methods (see Section 14.5.2) can be applied to the filtering problem; for ex-\nample, Gibbssampling canbeapplied directly toanunrolled DBN.Toavoid theproblem of\nincreasing update times as the unrolled network grows, the decayed MCMC filter (Marthi\nDECAYEDMCMC\net al., 2002) prefers to sample more recent state variables, with a probability that decays as\n1\/k2 for a variable k steps into the past. Decayed MCMC is a provably nondivergent filter.\nASSUMED-DENSITY Nondivergence theorems can also be obtained for certain types of assumed-density filter.\nFILTER\nAnassumed-density filterassumesthattheposteriordistribution overstatesattimetbelongs\ntoaparticular finitelyparameterized family;iftheprojection andupdatestepstakeitoutside\nthis family, the distribution is projected back to give the best approximation within the fam-\nFACTORED ily. For DBNs, the Boyen\u2013Koller algorithm (Boyen et al., 1999) and the factored frontier\nFRONTIER\nalgorithm (Murphy and Weiss, 2001) assume that the posterior distribution can be approxi-\nmated well by a product of small factors. Variational techniques (see Chapter 14) have also\nbeen developed fortemporal models. Ghahramani andJordan (1997) discuss anapproxima-\ntionalgorithm forthefactorialHMM,aDBNinwhichtwoormoreindependently evolving\nFACTORIALHMM 606 Chapter 15. Probabilistic Reasoning overTime\nMarkovchainsarelinkedbyasharedobservationstream. Jordanetal.(1998)coveranumber\nofotherapplications.\nData association for multitarget tracking was first described in a probabilistic setting\nby Sittler (1964). The first practical algorithm for large-scale problems was the \u201cmultiple\nhypothesistracker\u201dorMHTalgorithm(Reid,1979). Manyimportantpapersarecollectedby\nBar-Shalom and Fortmann (1988) and Bar-Shalom (1992). The development of an MCMC\nalgorithm fordata association isdue toPasula etal. (1999), whoapplied ittotraffic surveil-\nlance problems. Ohetal. (2009) provide aformal analysis and extensive experimental com-\nparisons toother methods. Schulz et al. (2003) describe adata association method based on\nparticle filtering. IngemarCoxanalyzed thecomplexity ofdataassociation (Cox,1993; Cox\nandHingorani,1994)andbroughtthetopictotheattentionofthevisioncommunity. Healso\nnoted the applicability of the polynomial-time Hungarian algorithm to the problem of find-\ning most-likely assignments, which had long been considered an intractable problem in the\ntracking community. The algorithm itself was published by Kuhn (1955), based on transla-\ntions ofpapers published in1931 bytwoHungarian mathematicians, De\u00b4nesKo\u00a8nig andJeno\u00a8\nEgerva\u00b4ry. Thebasic theorem hadbeen derived previously, however, inanunpublished Latin\nmanuscript bythefamousPrussianmathematician CarlGustavJacobi(1804\u20131851).\nEXERCISES\n15.1 Show that any second-order Markov process can be rewritten as a first-order Markov\nprocess with an augmented set of state variables. Can this always be done parsimoniously,\ni.e.,withoutincreasingthenumberofparametersneededtospecifythetransitionmodel?\n15.2 In this exercise, we examine what happens to the probabilities in the umbrella world\ninthelimitoflongtimesequences.\na. Suppose we observe an unending sequence of days on which the umbrella appears.\nShowthat,asthedaysgoby,theprobability ofrainonthecurrentdayincreases mono-\ntonicallytowardafixedpoint. Calculatethisfixedpoint.\nb. Now consider forecasting further and further into the future, given just the first two\numbrella observations. First, compute the probability P(r |u ,u ) for k=1...20\n2+k 1 2\nandplottheresults. Youshouldseethattheprobabilityconvergestowardsafixedpoint.\nProvethattheexactvalueofthisfixedpointis0.5.\n15.3 This exercise develops a space-efficient variant of the forward\u2013backward algorithm\ndescribed in Figure 15.4 (page 576). We wish to compute P(X |e ) for k=1,...,t. This\nk 1:t\nwillbedonewithadivide-and-conquer approach.\na. Suppose,forsimplicity, thattisodd,andletthehalfwaypointbeh=(t+1)\/2. Show\nthatP(X |e )canbecomputedfork=1,...,hgivenjusttheinitialforwardmessage\nk 1:t\nf ,thebackward messageb ,andtheevidence e .\n1:0 h+1:t 1:h\nb. Showasimilarresultforthesecondhalfofthesequence. Exercises 607\nc. Given the results of (a) and (b), a recursive divide-and-conquer algorithm can be con-\nstructed by first running forward along the sequence and then backward from the end,\nstoring just the required messages at the middle and the ends. Then the algorithm is\ncalledoneachhalf. Writeoutthealgorithm indetail.\nd. Computethetimeandspacecomplexityofthealgorithmasafunctionoft,thelengthof\nthesequence. Howdoesthischangeifwedividetheinputintomorethantwopieces?\n15.4 Onpage577,weoutlinedaflawedprocedureforfindingthemostlikelystatesequence,\ngiven an observation sequence. The procedure involves finding the most likely state at each\ntimestep, usingsmoothing, andreturning thesequence composed ofthesestates. Showthat,\nfor some temporal probability models and observation sequences, this procedure returns an\nimpossible statesequence (i.e.,theposteriorprobability ofthesequenceiszero).\n15.5 Equation (15.12) describes thefiltering process forthematrix formulation of HMMs.\nGiveasimilarequationforthecalculation oflikelihoods, whichwasdescribedgenerically in\nEquation(15.7).\n15.6 Consider the vacuum worlds of Figure 4.18 (perfect sensing) and Figure 15.7 (noisy\nsensing). Suppose that the robot receives an observation sequence such that, with perfect\nsensing, there is exactly one possible location it could be in. Is this location necessarily the\nmost probable location under noisy sensing for sufficiently small noise probability (cid:2)? Prove\nyourclaimorfindacounterexample.\n15.7 In Section 15.3.2, the prior distribution over locations is uniform and the transition\nmodel assumes an equal probability of moving to any neighboring square. What if those\nassumptions are wrong? Suppose that the initial location is actually chosen uniformly from\nthe northwest quadrant of the room and the Move action actually tends to move southeast.\nKeeping the HMM model fixed, explore the effect on localization and path accuracy as the\nsoutheasterly tendency increases, fordifferent valuesof (cid:2).\n15.8 Consideraversionofthevacuumrobot(page582)thathasthepolicyofgoingstraight\nforaslong as itcan; only when itencounters an obstacle does itchange toanew (randomly\nselected) heading. Tomodelthisrobot, eachstate inthemodelconsists ofa(location, head-\ning)pair. ImplementthismodelandseehowwelltheViterbialgorithmcantrackarobotwith\nthis model. The robot\u2019s policy is more constrained than the random-walk robot; does that\nmeanthatpredictions ofthemostlikelypatharemoreaccurate?\n15.9 This exercise is concerned with filtering in an environment with no landmarks. Con-\nsideravacuumrobotinanemptyroom,representedbyann\u00d7mrectangulargrid. Therobot\u2019s\nlocation is hidden; theonly evidence available totheobserver isanoisy location sensor that\ngives an approximation to the robot\u2019s location. If the robot is at location (x,y) then with\nprobability .1 the sensor gives the correct location, with probability .05 each it reports one\nof the 8 locations immediately surrounding (x,y), with probability .025 each it reports one\nof the 16 locations that surround those 8, and with the remaining probability of .1 it reports\n\u201cno reading.\u201d The robot\u2019s policy is to pick a direction and follow it with probability .8 on\neachstep;therobotswitchestoarandomlyselectednewheadingwithprobability .2(orwith 608 Chapter 15. Probabilistic Reasoning overTime\nS S\nt t+1\nX X\nt t+1\nZ Z\nt t+1\nFigure 15.21 A Bayesian network representation of a switching Kalman filter. The\nswitching variable St is a discrete state variable whose value determines the transition\nmodel for the continuous state variables Xt. For any discrete state i, the transition model\nP(Xt+1|Xt,St=i) isa linearGaussian model,just asin a regularKalmanfilter. Thetran-\nsitionmodelforthediscretestate,P(St+1|St),canbethoughtofasamatrix,asinahidden\nMarkovmodel.\nprobability 1ifitencounters awall). ImplementthisasanHMManddofilteringtotrackthe\nrobot. Howaccurately canwetracktherobot\u2019spath?\n15.10 Often,wewishtomonitoracontinuous-state systemwhosebehaviorswitchesunpre-\ndictablyamongasetofkdistinct\u201cmodes.\u201d Forexample,anaircrafttryingtoevadea missile\ncanexecute aseries ofdistinct maneuvers that the missilemayattempt totrack. ABayesian\nnetworkrepresentation ofsucha switchingKalmanfiltermodelisshowninFigure15.21.\na. Suppose that the discrete state S has k possible values and that the prior continuous\nt\nstate estimate P(X ) is a multivariate Gaussian distribution. Show that the prediction\n0\nP(X )is a mixture of Gaussians\u2014that is, a weighted sum of Gaussians such that the\n1\nweightssumto1.\nb. Show that if the current continuous state estimate P(X |e ) is a mixture of m Gaus-\nt 1:t\nsians, theninthegeneral casetheupdated stateestimate P(X |e )willbeamix-\nt+1 1:t+1\ntureofkmGaussians.\nc. Whataspectofthetemporalprocess dotheweightsintheGaussianmixturerepresent?\nTheresultsin(a)and(b)showthattherepresentationoftheposteriorgrowswithoutlimiteven\nforswitchingKalmanfilters,whichareamongthesimplesthybriddynamicmodels.\n15.11 CompletethemissingstepinthederivationofEquation(15.19)onpage586,thefirst\nupdatestepfortheone-dimensional Kalmanfilter.\n15.12 Letusexaminethebehaviorofthevariance updateinEquation(15.20)(page587).\na. Plotthevalueof\u03c32 asafunction oft,givenvariousvaluesfor\u03c32 and\u03c32.\nt x z\nb. Showthat the update has afixedpoint \u03c32 such that \u03c32 \u2192 \u03c32 as t \u2192 \u221e, and calculate\nt\nthevalueof\u03c32.\nc. Giveaqualitative explanation forwhathappens as\u03c32 \u2192 0andas\u03c32 \u2192 0.\nx z Exercises 609\n15.13 A professor wants to know if students are getting enough sleep. Each day, the pro-\nfessor observes whether the students sleep in class, and whether they have red eyes. The\nprofessorhasthefollowingdomaintheory:\n\u2022 Thepriorprobability ofgetting enoughsleep,withnoobservations, is0.7.\n\u2022 The probability of getting enough sleep on night t is 0.8 given that the student got\nenoughsleeptheprevious night,and0.3ifnot.\n\u2022 Theprobability ofhaving redeyesis0.2ifthestudent gotenough sleep,and0.7ifnot.\n\u2022 Theprobabilityofsleepinginclassis0.1ifthestudentgotenoughsleep,and0.3ifnot.\nFormulate this information as a dynamic Bayesian network that the professor could use to\nfilter or predict from a sequence of observations. Then reformulate it as a hidden Markov\nmodel that has only a single observation variable. Give the complete probability tables for\nthemodel.\n15.14 FortheDBNspecifiedinExercise15.13andfortheevidence values\ne = notredeyes,notsleepinginclass\n1\ne = redeyes,notsleepinginclass\n2\ne = redeyes,sleepinginclass\n3\nperform thefollowingcomputations:\na. Stateestimation: ComputeP(EnoughSleep |e )foreachoft = 1,2,3.\nt 1:t\nb. Smoothing: ComputeP(EnoughSleep |e )foreachoft = 1,2,3.\nt 1:3\nc. Comparethefilteredandsmoothedprobabilities for t = 1andt = 2.\n15.15 Supposethataparticularstudentshowsupwithredeyesandsleepsinclasseveryday.\nGiventhemodeldescribedinExercise15.13,explainwhytheprobabilitythatthestudenthad\nenough sleeptheprevious nightconverges toafixedpointratherthancontinuing togodown\nas wegather more days of evidence. What is the fixed point? Answerthis both numerically\n(bycomputation) andanalytically.\n15.16 Thisexerciseanalyzesinmoredetailthepersistent-failure modelforthebatterysen-\nsorinFigure15.15(a) (page594).\na. Figure 15.15(b) stops at t=32. Describe qualitatively what should happen as t \u2192 \u221e\nifthesensorcontinues toread0.\nb. Supposethattheexternaltemperatureaffectsthebattery sensorinsuchawaythattran-\nsient failures become more likely as temperature increases. Show how to augment the\nDBNstructure inFigure15.15(a),andexplainanyrequired changestotheCPTs.\nc. Giventhenewnetwork structure, canbattery readings beusedbytherobottoinferthe\ncurrenttemperature?\n15.17 Consider applying the variable elimination algorithm to the umbrella DBNunrolled\nforthree slices, where the query is P(R |u ,u ,u ). Show that the space complexity of the\n3 1 2 3\nalgorithm\u2014thesizeofthelargestfactor\u2014isthesame,regardlessofwhethertherainvariables\nareeliminatedinforwardorbackwardorder. 16\nMAKING SIMPLE\nDECISIONS\nInwhichweseehowanagentshouldmakedecisionssothatitgetswhatitwants\u2014\nonaverage, atleast.\nInthischapter, wefillinthedetailsofhowutilitytheorycombineswithprobability theoryto\nyield adecision-theoretic agent\u2014an agent that can make rational decisions based on whatit\nbelievesandwhatitwants. Suchanagentcanmakedecisionsincontextsinwhichuncertainty\nand conflicting goals leave a logical agent with no way to decide: a goal-based agent has a\nbinary distinction between good (goal) and bad (non-goal) states, while a decision-theoretic\nagenthasacontinuous measureofoutcomequality.\nSection 16.1 introduces the basic principle of decision theory: the maximization of\nexpected utility. Section 16.2 shows that the behavior of any rational agent can be captured\nbysupposing autility function thatisbeingmaximized. Section 16.3discusses thenatureof\nutilityfunctionsinmoredetail,andinparticulartheirrelationtoindividualquantitiessuchas\nmoney. Section16.4showshowtohandleutilityfunctions thatdependonseveralquantities.\nIn Section 16.5, we describe the implementation of decision-making systems. In particular,\nweintroduce aformalism called a decision network (also known asaninfluencediagram)\nthat extends Bayesian networks by incorporating actions and utilities. The remainder of the\nchapterdiscusses issuesthatariseinapplications ofdecisiontheorytoexpertsystems.\n16.1 COMBINING BELIEFS AND DESIRES UNDER UNCERTAINTY\nDecision theory, initssimplest form, deals withchoosing among actions based on thedesir-\nabilityoftheirimmediateoutcomes; thatis,theenvironment isassumedtobeepisodic inthe\nsense defined on page 43. (Thisassumption isrelaxed inChapter 17.) InChapter3weused\nthe notation RESULT(s 0,a) for the state that is the deterministic outcome of taking action a\nin state s . In this chapter wedeal withnondeterministic partially observable environments.\n0\nSincetheagentmaynotknowthecurrentstate,weomititanddefineRESULT(a)asarandom\n(cid:2)\nvariable whose values arethepossible outcome states. Theprobability of outcome s, given\nevidence observations e,iswritten\nP(RESULT(a)=s(cid:2)|a,e),\n610 Section16.2. TheBasisofUtilityTheory 611\nwheretheaontheright-handsideoftheconditioning barstandsforthe eventthatactionais\nexecuted.1\nTheagent\u2019spreferencesarecapturedbyautilityfunction,U(s),whichassignsasingle\nUTILITYFUNCTION\nnumbertoexpressthedesirability ofastate. The expected utilityofanactiongiventheevi-\nEXPECTEDUTILITY\ndence,EU(a|e),isjusttheaverageutilityvalueoftheoutcomes,weightedbytheprobability\nthattheoutcomeoccurs:\n(cid:12)\nEU(a|e) = P(RESULT(a)=s(cid:2)|a,e)U(s(cid:2) ). (16.1)\ns(cid:3)\nMAXIMUMEXPECTED Theprinciple ofmaximumexpectedutility(MEU)saysthatarational agentshouldchoose\nUTILITY\ntheactionthatmaximizestheagent\u2019s expectedutility:\naction = argmaxEU(a|e)\na\nInasense,theMEUprinciplecouldbeseenasdefiningallofAI.Allanintelligent agenthas\nto do is calculate the various quantities, maximize utility over its actions, and away it goes.\nButthisdoesnotmeanthattheAIproblemissolvedbythedefinition!\nThe MEU principle formalizes the general notion that the agent should \u201cdo the right\nthing,\u201d but goes only a small distance toward a full operationalization of that advice. Es-\ntimating the state of the world requires perception, learning, knowledge representation, and\ninference. Computing P(RESULT(a)|a,e) requires a complete causal model of the world\nand,aswesawinChapter14,NP-hardinferencein(verylarge)Bayesiannetworks. Comput-\n(cid:2)\ning the outcome utilities U(s) often requires searching or planning, because an agent may\nnot know how good astate is until itknows where it can get to from that state. So, decision\ntheoryisnotapanacea thatsolvestheAIproblem\u2014but itdoes provideausefulframework.\nTheMEUprinciplehasaclearrelationtotheideaofperformancemeasuresintroduced\nin Chapter 2. The basic idea is simple. Consider the environments that could lead to an\nagent having a given percept history, and consider the different agents that wecould design.\nIf an agent acts so as to maximize a utility function that correctly reflects the performance\nmeasure, then the agent will achieve the highest possible performance score (averaged over\nall the possible environments). This is the central justification for the MEU principle itself.\nWhile the claim may seem tautological, it does in fact embody a very important transition\nfrom a global, external criterion of rationality\u2014the performance measure over environment\nhistories\u2014toalocal,internalcriterioninvolvingthemaximizationofautilityfunctionapplied\ntothenextstate.\n16.2 THE BASIS OF UTILITY THEORY\nIntuitively, the principle of Maximum Expected Utility (MEU) seems like a reasonable way\nto make decisions, but it is by no means obvious that it is the only rational way. After all,\nwhy should maximizing the average utility be so special? What\u2019s wrong with an agent that\n1 ClassicaldecisiontheorylPeavesthecurrentstateS0implicit,butwecouldmakeitexplicitbywriting\nP(RESULT(a)=s(cid:3)|a,e)= sP(RESULT(s,a)=s(cid:3)|a)P(S0=s|e). 612 Chapter 16. MakingSimpleDecisions\nmaximizes the weighted sum of the cubes of the possible utilities, or tries to minimize the\nworst possible loss? Could an agent act rationally just by expressing preferences between\nstates, without giving them numeric values? Finally, why should a utility function with the\nrequired properties existatall? Weshallsee.\n16.2.1 Constraints onrationalpreferences\nThesequestions canbeansweredbywritingdownsomeconstraints onthepreferences thata\nrational agentshouldhaveandthenshowingthattheMEUprinciple canbederivedfromthe\nconstraints. Weusethefollowingnotation todescribe anagent\u2019spreferences:\nA \u2019 B theagentprefers AoverB.\nA \u223c B theagentisindifferent betweenAandB.\n\u2019\nA \u223c B theagentprefers AoverB orisindifferent betweenthem.\nNow the obvious question is, what sorts of things are A and B? They could be states of the\nworld, but more often than not there is uncertainty about what is really being offered. For\nexample, an airline passenger who is offered \u201cthe pasta dish or the chicken\u201d does not know\nwhatlurksbeneath thetinfoilcover.2 Thepastacouldbedelicious orcongealed, thechicken\njuicyorovercooked beyondrecognition. Wecanthinkofthesetofoutcomes foreachaction\nasalottery\u2014think ofeachactionasaticket. Alottery LwithpossibleoutcomesS ,...,S\nLOTTERY 1 n\nthatoccurwithprobabilities p ,...,p iswritten\n1 n\nL = [p ,S ; p ,S ; ... p ,S ].\n1 1 2 2 n n\nIngeneral, each outcome S ofalottery canbeeither anatomicstate oranother lottery. The\ni\nprimary issue for utility theory is to understand how preferences between complex lotteries\nare related to preferences between the underlying states in those lotteries. To address this\nissuewelistsixconstraints thatwerequire anyreasonable preference relationtoobey:\n\u2022 Orderability: Given any two lotteries, a rational agent must either prefer one to the\nORDERABILITY\notherorelseratethetwoasequallypreferable. Thatis,the agentcannotavoiddeciding.\nAswesaidonpage490,refusing tobetislikerefusing toallowtimetopass.\nExactlyoneof(A \u2019B), (B \u2019 A), or(A \u223c B)holds.\n\u2022 Transitivity: Givenany three lotteries, ifan agent prefers Ato B and prefers B to C,\nTRANSITIVITY\nthentheagentmustprefer AtoC.\n(A \u2019 B)\u2227(B \u2019 C) \u21d2 (A \u2019 C).\n\u2022 Continuity: If some lottery B is between A and C in preference, then there is some\nCONTINUITY\nprobability pforwhichtherationalagentwillbeindifferentbetweengettingB forsure\nandthelotterythatyieldsAwithprobability pandC withprobability 1\u2212p.\nA\u2019 B \u2019 C \u21d2 \u2203p [p,A; 1\u2212p,C] \u223c B .\n\u2022 Substitutability: If an agent is indifferent between two lotteries A and B, then the\nSUBSTITUTABILITY\nagentisindifferentbetweentwomorecomplexlotteriesthatarethesameexceptthatB\n2 Weapologizetoreaderswhoselocalairlinesnolongerofferfoodonlongflights. Section16.2. TheBasisofUtilityTheory 613\nis substituted for A in one of them. This holds regardless of the probabilities and the\notheroutcome(s) inthelotteries.\nA\u223c B \u21d2 [p,A; 1\u2212p,C]\u223c [p,B;1\u2212p,C].\nThisalsoholdsifwesubstitute \u2019for\u223cinthisaxiom.\n\u2022 Monotonicity: Suppose two lotteries have the sametwo possible outcomes, Aand B.\nMONOTONICITY\nIf an agent prefers A to B, then the agent must prefer the lottery that has a higher\nprobability forA(andviceversa).\nA\u2019 B \u21d2 (p > q \u21d4 [p,A; 1\u2212p,B]\u2019 [q,A; 1\u2212q,B]).\n\u2022 Decomposability: Compound lotteries can be reduced to simpler ones using the laws\nDECOMPOSABILITY\nof probability. This has been called the \u201cno fun in gambling\u201d rule because it says that\ntwo consecutive lotteries can be compressed into a single equivalent lottery, as shown\ninFigure16.1(b).3\n[p,A; 1\u2212p,[q,B; 1\u2212q,C]]\u223c [p,A; (1\u2212p)q,B; (1\u2212p)(1\u2212q),C].\nThese constraints are known as the axioms of utility theory. Each axiom can be motivated\nby showing that an agent that violates it will exhibit patently irrational behavior in some\nsituations. Forexample, we can motivate transitivity by making an agent with nontransitive\npreferences give us all its money. Suppose that the agent has the nontransitive preferences\nA \u2019 B \u2019 C \u2019 A, where A, B, and C are goods that can be freely exchanged. If the agent\ncurrently has A, then we could offer to trade C for A plus one cent. The agent prefers C,\nandsowould bewilling tomakethistrade. Wecould then offer totrade B forC,extracting\nanother cent, and finally trade A for B. This brings us back where we started from, except\nthat theagent hasgiven usthree cents (Figure16.1(a)). Wecankeep going around thecycle\nuntiltheagenthasnomoneyatall. Clearly, theagenthasactedirrationally inthiscase.\n16.2.2 Preferences leadto utility\nNoticethattheaxiomsofutilitytheoryarereallyaxiomsaboutpreferences\u2014theysaynothing\nabout a utility function. But in fact from the axioms of utility we can derive the following\nconsequences (fortheproof, seevonNeumannandMorgenstern, 1944):\n\u2022 ExistenceofUtilityFunction: Ifanagent\u2019spreferencesobeytheaxiomsofutility,then\nthere exists a function U such that U(A) > U(B) if and only if A is preferred to B,\nandU(A) = U(B)ifandonlyiftheagentisindifferent between AandB.\nU(A) > U(B) \u21d4 A\u2019 B\nU(A) = U(B) \u21d4 A\u223c B\n\u2022 Expected Utility of a Lottery: Theutility of a lottery is the sum of the probability of\neachoutcometimestheutilityofthatoutcome.\n(cid:12)\nU([p ,S ;...;p ,S ]) = p U(S ).\n1 1 n n i i\ni\n3 Wecan account forthe enjoyment of gambling by encoding gambling events intothestatedescription; for\nexample,\u201cHave$10andgambled\u201dcouldbepreferredto\u201cHave$10anddidn\u2019tgamble.\u201d 614 Chapter 16. MakingSimpleDecisions\nA\np\nA B\nq\n1\u00a2 1\u00a2\n(1\u2013p)\n(1\u2013q)\nC\nis equivalent to\nA\nB C p\n(1\u2013p)q\nB\n1\u00a2\n(1\u2013p)(1\u2013q) C\n(a) (b)\nFigure 16.1 (a) A cycle of exchanges showing that the nontransitive preferences A \u2019\nB \u2019C \u2019Aresultinirrationalbehavior.(b)Thedecomposabilityaxiom.\nInotherwords,oncetheprobabilitiesandutilitiesofthepossibleoutcomestatesarespecified,\ntheutilityofacompoundlotteryinvolvingthosestatesiscompletelydetermined. Becausethe\noutcomeofanondeterministic actionisalottery,itfollowsthatanagentcanactrationally\u2014\nthatis,consistentlywithitspreferences\u2014onlybychoosinganactionthatmaximizesexpected\nutilityaccording toEquation(16.1).\nTheprecedingtheoremsestablishthatautilityfunctionexistsforanyrationalagent,but\ntheydonotestablish thatitisunique. Itiseasytosee,infact,thatanagent\u2019sbehaviorwould\nnotchangeifitsutilityfunction U(S)weretransformed according to\n(cid:2)\nU (S)= aU(S)+b, (16.2)\nwhere a and b are constants and a > 0; an affine transformation.4 This fact was noted in\nChapter5fortwo-playergamesofchance;here,weseethatit iscompletelygeneral.\nAs in game-playing, in a deterministic environment an agent just needs a preference\nranking on states\u2014the numbers don\u2019t matter. This is called a value function or ordinal\nVALUEFUNCTION\nORDINALUTILITY utilityfunction.\nFUNCTION\nIt is important to remember that the existence of a utility function that describes an\nagent\u2019spreferencebehaviordoesnotnecessarilymeanthat theagentisexplicitlymaximizing\nthatutilityfunctioninitsowndeliberations. AsweshowedinChapter2,rationalbehaviorcan\nbe generated in any number of ways. By observing a rational agent\u2019s preferences, however,\nanobservercanconstruct theutilityfunction thatrepresents whattheagentisactually trying\ntoachieve(eveniftheagentdoesn\u2019t knowit).\n4 Inthissense,utilitiesresembletemperatures:atemperatureinFahrenheitis1.8timestheCelsiustemperature\nplus32.Yougetthesameresultsineithermeasurementsystem. Section16.3. UtilityFunctions 615\n16.3 UTILITY FUNCTIONS\nUtilityisafunctionthatmapsfromlotteriestorealnumbers. Weknowtherearesomeaxioms\non utilities that all rational agents must obey. Is that all we can say about utility functions?\nStrictlyspeaking, thatisit: anagentcanhaveanypreferences itlikes. Forexample,anagent\nmightprefertohaveaprimenumberofdollarsinitsbankaccount;inwhichcase,ifithad$16\nitwouldgiveaway$3. Thismightbeunusual, butwecan\u2019t callitirrational. Anagentmight\nprefer adented 1973 FordPinto toa shiny new Mercedes. Preferences can also interact: for\nexample, the agent might prefer prime numbers of dollars only when it owns the Pinto, but\nwhenitownstheMercedes,itmightprefermoredollarstofewer. Fortunately,thepreferences\nofrealagentsareusually moresystematic, andthuseasiertodealwith.\n16.3.1 Utilityassessmentandutility scales\nIf we want to build a decision-theoretic system that helps the agent make decisions or acts\nonhisorherbehalf, wemustfirstworkoutwhattheagent\u2019sutilityfunction is. Thisprocess,\nPREFERENCE often called preference elicitation, involves presenting choices to the agent and using the\nELICITATION\nobserved preferences topindowntheunderlying utilityfunction.\nEquation(16.2)saysthatthereisnoabsolutescaleforutilities,butitishelpful,nonethe-\nless,toestablishsomescaleonwhichutilitiescanberecordedandcomparedforany particu-\nlarproblem. Ascalecanbeestablished byfixingtheutilitiesofanytwoparticularoutcomes,\njust as we fix a temperature scale by fixing the freezing point and boiling point of water.\nTypically, we fix the utility of a \u201cbest possible prize\u201d at U(S) = u(cid:12) and a \u201cworst possible\nNORMALIZED catastrophe\u201d atU(S) = u\u22a5. Normalizedutilitiesuseascalewithu\u22a5 = 0andu(cid:12) = 1.\nUTILITIES\nGiven a utility scale between u(cid:12) and u\u22a5, we can assess the utility of any particular\nSTANDARDLOTTERY\nprizeS byaskingtheagenttochoosebetweenS andastandardlottery[p,u(cid:12); (1\u2212p),u\u22a5].\nTheprobability pisadjusteduntiltheagentisindifferentbetweenS andthestandardlottery.\nAssumingnormalizedutilities,theutilityofS isgivenbyp. Oncethisisdoneforeachprize,\ntheutilitiesforalllotteriesinvolving thoseprizesaredetermined.\nIn medical, transportation, and environmental decision problems, among others, peo-\nple\u2019slivesareatstake. Insuchcases,u\u22a5 isthevalueassignedtoimmediatedeath(orperhaps\nmany deaths). Although nobody feels comfortable withputting avalue on human life, itisa\nfact that tradeoffs are made all the time. Aircraft are given a complete overhaul at intervals\ndetermined by trips and miles flown, rather than after every trip. Cars are manufactured in\na way that trades off costs against accident survival rates. Paradoxically, a refusal to \u201cput a\nmonetary value on life\u201d means that life is often undervalued. Ross Shachter relates an ex-\nperience with a government agency that commissioned a study on removing asbestos from\nschools. Thedecisionanalystsperformingthestudyassumedaparticulardollarvalueforthe\nlife of a school-age child, and argued that the rational choice under that assumption was to\nremove the asbestos. The agency, morally outraged at the idea of setting the value of a life,\nrejectedthereportoutofhand. Itthendecidedagainstasbestosremoval\u2014implicitlyasserting\nalowervalueforthelifeofachildthanthatassigned bytheanalysts. 616 Chapter 16. MakingSimpleDecisions\nSome attempts have been made to find out the value that people place on their own\nlives. One common \u201ccurrency\u201d used in medical and safety analysis is the micromort, a\nMICROMORT\none in a million chance of death. If you ask people how much they would pay to avoid a\nrisk\u2014for example, to avoid playing Russian roulette with a million-barreled revolver\u2014they\nwill respond with very large numbers, perhaps tens of thousands of dollars, but their actual\nbehaviorreflectsamuchlowermonetaryvalueforamicromort. Forexample,drivinginacar\nfor230 miles incurs arisk of one micromort; overthe life of yourcar\u2014say, 92,000 miles\u2014\nthat\u2019s 400 micromorts. People appear to be willing to pay about $10,000 (at 2009 prices)\nmore for a safer car that halves the risk of death, or about $50 per micromort. A number\nof studies have confirmed a figure in this range across many individuals and risk types. Of\ncourse, this argument holds only forsmallrisks. Mostpeople won\u2019t agree tokillthemselves\nfor$50million.\nAnother measure is the QALY,or quality-adjusted life year. Patients with a disability\nQALY\nare willing to accept a shorter life expectancy to be restored to full health. For example,\nkidneypatientsonaverageareindifferentbetweenlivingtwoyearsonadialysismachineand\noneyearatfullhealth.\n16.3.2 The utilityofmoney\nUtility theory has its roots in economics, and economics provides one obvious candidate\nfor a utility measure: money (or more specifically, an agent\u2019s total net assets). The almost\nuniversal exchangeability of money for all kinds of goods and services suggests that money\nplaysasignificantroleinhumanutilityfunctions.\nItwillusuallybethecasethatanagentprefersmoremoneytoless,allotherthingsbeing\nMONOTONIC equal. We say that the agent exhibits a monotonic preference for more money. This does\nPREFERENCE\nnotmeanthatmoneybehaves asautility function, because itsaysnothing aboutpreferences\nbetweenlotteriesinvolving money.\nSupposeyouhavetriumphedovertheothercompetitorsinatelevisiongameshow. The\nhost now offers you achoice: either you can take the $1,000,000 prize oryou can gamble it\non the flip of a coin. If the coin comes up heads, you end up with nothing, but if it comes\nup tails, you get $2,500,000. If you\u2019re like most people, you would decline the gamble and\npocketthemillion. Areyoubeingirrational?\nEXPECTED Assumingthecoinisfair,theexpectedmonetaryvalue(EMV)ofthegambleis 1($0)\nMONETARYVALUE 2\n+ 1($2,500,000) = $1,250,000, which is more than the original $1,000,000. But that does\n2\nnot necessarily mean that accepting the gamble is a better decision. Suppose we use S to\nn\ndenote the state of possessing total wealth $n, and that your current wealth is $k. Then the\nexpectedutilities ofthetwoactions ofaccepting anddeclining thegambleare\nEU(Accept) = 1U(S )+ 1U(S ),\n2 k 2 k+2,500,000\nEU(Decline) = U(S ).\nk+1,000,000\nTo determine what to do, we need to assign utilities to the outcome states. Utility is not\ndirectly proportional tomonetary value, because theutilityforyourfirstmillionisveryhigh\n(orso they say), whereas the utility foran additional million is smaller. Suppose you assign\nautilityof5toyourcurrentfinancialstatus(S ),a9tothestateS ,andan8tothe\nk k+2,500,000 Section16.3. UtilityFunctions 617\nU U\no o o o o o\no o\no\no $ $\no\no\n-150,000 o 800,000\no\no\n(a) (b)\nFigure16.2 Theutilityofmoney. (a)EmpiricaldataforMr.Beardoveralimitedrange.\n(b)Atypicalcurveforthefullrange.\nstate S . Then the rational action would beto decline, because the expected utility\nk+1,000,000\nof accepting is only 7 (less than the 8 for declining). On the other hand, a billionaire would\nmostlikely haveautility function that islocally linearoverthe range ofafewmillionmore,\nandthuswouldacceptthegamble.\nInapioneeringstudyofactualutilityfunctions,Grayson(1960)foundthattheutilityof\nmoney was almost exactly proportional to the logarithm of the amount. (This idea was first\nsuggested by Bernoulli (1738); see Exercise 16.3.) Oneparticular utility curve, foracertain\nMr. Beard, is shown in Figure 16.2(a). The data obtained for Mr. Beard\u2019s preferences are\nconsistent withautilityfunction\nU(S )= \u2212263.31+22.09log(n+150,000)\nk+n\nfortherangebetween n = \u2212$150,000andn= $800,000.\nWeshould notassume thatthisisthedefinitive utility function formonetary value, but\nitislikely thatmostpeople haveautility function thatisconcave forpositive wealth. Going\ninto debt is bad, but preferences between different levels of debt can display a reversal of\ntheconcavity associatedwithpositivewealth. Forexample,someonealready$10,000,000in\ndebt might well accept a gamble on a fair coin with a gain of $10,000,000 for heads and a\nlossof$20,000,000 fortails.5 ThisyieldstheS-shapedcurveshowninFigure16.2(b).\nIfwerestrictourattention tothepositivepartofthecurves,wheretheslopeisdecreas-\ning,thenforanylottery L,theutilityofbeingfacedwiththatlotteryislessthantheutilityof\nbeinghandedtheexpectedmonetaryvalueofthelotteryasasurething:\nU(L) < U(S ).\nEMV(L)\nThat is, agents with curves of this shape are risk-averse: they prefer a sure thing with a\nRISK-AVERSE\npayoff that is less than the expected monetary value of a gamble. On the other hand, in the\n\u201cdesperate\u201d region at large negative wealth in Figure 16.2(b), the behavior is risk-seeking.\nRISK-SEEKING\n5 Suchbehaviormightbecalleddesperate,butitisrationalifoneisalreadyinadesperatesituation. 618 Chapter 16. MakingSimpleDecisions\nCERTAINTY The value an agent will accept in lieu of a lottery is called the certainty equivalent of the\nEQUIVALENT\nlottery. Studies have shownthat most people willaccept about $400 inlieu ofagamble that\ngives$1000halfthetimeand$0theotherhalf\u2014thatis,thecertaintyequivalentofthelottery\nis$400,whiletheEMVis$500. ThedifferencebetweentheEMVofalotteryanditscertainty\nINSURANCE equivalent is called the insurance premium. Risk aversion is the basis for the insurance\nPREMIUM\nindustry, because it means that insurance premiums are positive. People would rather pay a\nsmall insurance premium than gamble the price of their house against the chance of a fire.\nFrom the insurance company\u2019s point of view, the price of the house is very small compared\nwith the firm\u2019s total reserves. This means that the insurer\u2019s utility curve is approximately\nlinearoversuchasmallregion, andthegamblecoststhecompanyalmostnothing.\nNoticethatforsmallchanges inwealthrelative tothecurrentwealth, almostany curve\nwillbeapproximately linear. Anagent thathasalinearcurveissaidtoberisk-neutral. For\nRISK-NEUTRAL\ngambles with small sums, therefore, we expect risk neutrality. In a sense, this justifies the\nsimplified procedure that proposed small gambles to assess probabilities and to justify the\naxiomsofprobability inSection13.2.3.\n16.3.3 Expected utilityand post-decisiondisappointment\n\u2217\nTherational waytochoosethebestaction, a ,istomaximizeexpected utility:\na\u2217 = argmaxEU(a|e).\na\nIfwehavecalculatedtheexpectedutilitycorrectlyaccordingtoourprobability model,andif\nthe probability model correctly reflects the underlying stochastic processes that generate the\noutcomes, then,onaverage,wewillgettheutilityweexpectifthewholeprocess isrepeated\nmanytimes.\nIn reality, however, our model usually oversimplifies the real situation, either because\nwe don\u2019t know enough (e.g., when making a complex investment decision) or because the\ncomputation of the true expected utility is too difficult (e.g., when estimating the utility of\nsuccessor states of the root node in backgammon). In that case, we are really working with\nestimates E! U(a|e) of the true expected utility. We will assume, kindly perhaps, that the\nestimates are unbiased,thatis,theexpected valueoftheerror, E(E! U(a|e)\u2212EU(a|e))),is\nUNBIASED\nzero. In that case, it still seems reasonable to choose the action with the highest estimated\nutilityandtoexpecttoreceivethatutility, onaverage, whentheactionisexecuted.\nUnfortunately, the real outcome will usually be significantly worse than we estimated,\neven though the estimate was unbiased! To see why, consider a decision problem in which\nthere are k choices, each of which has true estimated utility of 0. Suppose that the error in\neach utility estimate has zero mean and standard deviation of 1, shown as the bold curve in\nFigure 16.3. Now, as we actually start to generate the estimates, some of the errors will be\nnegative (pessimistic) and some will be positive (optimistic). Because we select the action\nwith the highest utility estimate, we are obviously favoring the overly optimistic estimates,\nand that is the source of the bias. It is a straightforward matter to calculate the distribution\nof the maximum of the k estimates (see Exercise 16.11) and hence quantify the extent of\nour disappointment. The curve in Figure 16.3 for k=3 has a mean around 0.85, so the\naverage disappointment will be about 85% of the standard deviation in the utility estimates. Section16.3. UtilityFunctions 619\n0.9\nk=30\n0.8\n0.7\nk=10\n0.6\nk=3\n0.5\n0.4\n0.3\n0.2\n0.1\n0\n-5 -4 -3 -2 -1 0 1 2 3 4 5\nError in utility estimate\nFigure16.3 Plot of the errorin each of k utility estimates andof the distributionof the\nmaximumofkestimatesfork=3,10,and30.\nWith more choices, extremely optimistic estimates are more likely to arise: for k=30, the\ndisappointment willbearoundtwicethestandarddeviation intheestimates.\nThis tendency for the estimated expected utility of the best choice to be too high is\ncalled the optimizer\u2019s curse (Smith and Winkler, 2006). It afflicts even the most seasoned\nOPTIMIZER\u2019SCURSE\ndecision analysts and statisticians. Serious manifestations include believing that an exciting\nnew drug that has cured 80% patients in a trial will cure 80% of patients (it\u2019s been chosen\nfrom k= thousands of candidate drugs) or that a mutual fund advertised as having above-\naverage returns will continue to have them (it\u2019s been chosen to appear in the advertisement\nout of k= dozens of funds in the company\u2019s overall portfolio). It can even be the case that\nwhat appears to be the best choice may not be, if the variance in the utility estimate is high:\nadrug, selected from thousands tried, that has cured 9of 10 patients is probably worse than\nonethathascured800of1000.\nTheoptimizer\u2019scursecropsupeverywherebecauseoftheubiquityofutility-maximizing\nselectionprocesses,sotakingtheutilityestimatesatfacevalueisabadidea. Wecanavoidthe\ncursebyusinganexplicitprobability model P(E! U |EU)oftheerrorintheutilityestimates.\nGiventhis model andaprior P(EU)onwhat wemight reasonably expect the utilities tobe,\nwetreattheutilityestimate,onceobtained,asevidenceandcomputetheposteriordistribution\nforthetrueutilityusingBayes\u2019rule.\n16.3.4 Humanjudgment and irrationality\nDecision theory is a normative theory: it describes how a rational agent should act. A\nNORMATIVETHEORY\nDESCRIPTIVE descriptivetheory,ontheotherhand,describeshowactualagents\u2014forexample,humans\u2014\nTHEORY\nreally do act. The application of economic theory would be greatly enhanced if the two\ncoincided, butthereappears tobesomeexperimental evidence tothecontrary. Theevidence\nsuggests thathumansare\u201cpredictably irrational\u201d (Ariely,2009). 620 Chapter 16. MakingSimpleDecisions\nThebest-knownproblemistheAllaisparadox(Allais,1953). Peoplearegivenachoice\nbetweenlotteries AandB andthenbetweenC andD,whichhavethefollowingprizes:\nA: 80%chanceof$4000 C : 20%chanceof$4000\nB : 100%chanceof$3000 D : 25%chanceof$3000\nMost people consistently prefer B over A (taking the sure thing), and C over D (taking the\nhigher EMV). The normative analysis disagrees! We can see this most easily if we use the\nfreedom implied by Equation (16.2) to set U($0) = 0. In that case, then B \u2019 A implies\nthat U($3000) > 0.8U($4000), whereas C \u2019 D implies exactly the reverse. In other\nwords, there is no utility function that is consistent with these choices. One explanation for\nthe apparently irrational preferences is the certainty effect (Kahneman and Tversky, 1979):\nCERTAINTYEFFECT\npeoplearestronglyattractedtogainsthatarecertain. Thereareseveralreasonswhythismay\nbe so. First, people may prefer to reduce their computational burden; by choosing certain\noutcomes, they don\u2019t have to compute with probabilities. But the effect persists even when\nthecomputations involved areveryeasyones. Second, peoplemaydistrust thelegitimacyof\nthestatedprobabilities. Itrustthatacoinflipisroughly 50\/50 ifIhavecontrol overthecoin\nandthe flip,butI maydistrust theresult iftheflipisdone bysomeone withavested interest\nintheoutcome.6 Inthepresenceofdistrust,itmightbebettertogoforthesurething.7 Third,\npeople may be accounting for their emotional state as well as their financial state. People\nknowtheywouldexperienceregretiftheygaveupacertainreward(B)foran80%chanceat\nREGRET\nahigherrewardandthenlost. Inotherwords,if Aischosen,thereisa20%chanceofgetting\nno money and feeling like a complete idiot, which is worse than just getting no money. So\nperhaps people who choose B over A and C over D are not being irrational; they are just\nsayingthattheyarewillingtogiveup$200ofEMVtoavoida20%chanceoffeelinglikean\nidiot.\nArelatedproblemistheEllsbergparadox. Heretheprizesarefixed,buttheprobabilities\nareunderconstrained. Yourpayoffwilldependonthecolorofaballchosenfromanurn. You\naretoldthattheurncontains 1\/3redballs, and2\/3eitherblackoryellowballs,butyoudon\u2019t\nknowhowmanyblackandhowmanyyellow. Again,youareaskedwhetheryoupreferlottery\nAorB;andthenC orD:\nA: $100foraredball C : $100foraredoryellowball\nB : $100forablackball D : $100forablackoryellowball .\nItshouldbeclearthatifyouthinktherearemoreredthanblackballsthenyoushould prefer\nA over B and C over D; if you think there are fewer red than black you should prefer the\nopposite. But it turns out that most people prefer A over B and also prefer D over C, even\nthough there is no state of the world for which this is rational. It seems that people have\nAMBIGUITY ambiguity aversion: A gives you a 1\/3 chance of winning, while B could be anywhere\nAVERSION\nbetween0and2\/3. Similarly,Dgivesyoua2\/3chance,whileC couldbeanywherebetween\n1\/3and3\/3. Mostpeopleelecttheknownprobability ratherthantheunknownunknowns.\n6 Forexample, the mathematician\/magician Persi Diaconis can make a coin flip come out the way he wants\neverytime(Landhuis,2004).\n7 Eventhesurethingmaynotbecertain.Despitecast-ironpromises,wehavenotyetreceivedthat$27,000,000\nfromtheNigerianbankaccountofapreviouslyunknowndeceasedrelative. Section16.3. UtilityFunctions 621\nYet another problem is that the exact wording of a decision problem can have a big\nimpactontheagent\u2019schoices;thisiscalledtheframingeffect. Experimentsshowthatpeople\nFRAMINGEFFECT\nlike a medical procedure that it is described as having a \u201c90% survival rate\u201d about twice as\nmuchasonedescribedashavinga\u201c10%deathrate,\u201deventhoughthesetwostatementsmean\nexactlythesamething. Thisdiscrepancyinjudgmenthasbeenfoundinmultipleexperiments\nandisaboutthesamewhetherthesubjectswerepatientsinaclinic,statisticallysophisticated\nbusiness schoolstudents, orexperienced doctors.\nPeople feel more comfortable making relative utility judgments rather than absolute\nones. ImayhavelittleideahowmuchImightenjoythevariouswinesofferedbyarestaurant.\nTherestauranttakesadvantageofthisbyofferinga$200bottlethatitknowsnobodywillbuy,\nbutwhichserves toskewupward thecustomer\u2019s estimate ofthevalueofallwinesandmake\nthe$55bottleseemlikeabargain. Thisiscalledtheanchoringeffect.\nANCHORINGEFFECT\nIfhumaninformants insistoncontradictory preference judgments, thereisnothing that\nautomatedagentscandotobeconsistentwiththem. Fortunately,preferencejudgmentsmade\nby humans are often open to revision in the light of further consideration. Paradoxes like\nthe Allais paradox are greatly reduced (but not eliminated) if the choices are explained bet-\nter. In work at the Harvard Business School on assessing the utility of money, Keeney and\nRaiffa(1976,p.210)foundthefollowing:\nSubjectstendtobetoorisk-averseinthesmallandtherefore...thefittedutilityfunctions\nexhibitunacceptablylargeriskpremiumsforlotterieswithalargespread. ...Mostofthe\nsubjects, however,canreconciletheirinconsistenciesandfeelthattheyhavelearnedan\nimportantlessonabouthowtheywanttobehave.Asaconsequence,somesubjectscancel\ntheirautomobilecollisioninsuranceandtakeoutmoreterminsuranceontheirlives.\nThe evidence for human irrationality is also questioned by researchers in the field of evo-\nEVOLUTIONARY lutionary psychology, who point to the fact that our brain\u2019s decision-making mechanisms\nPSYCHOLOGY\ndid not evolve to solve word problems with probabilities and prizes stated as decimal num-\nbers. Let us grant, for the sake of argument, that the brain has built-in neural mechanism\nforcomputingwithprobabilities andutilities, orsomethingfunctionally equivalent; ifso,the\nrequiredinputswouldbeobtainedthroughaccumulatedexperienceofoutcomesandrewards\nratherthanthroughlinguisticpresentationsofnumerical values. Itisfarfromobviousthatwe\ncandirectlyaccessthebrain\u2019sbuilt-inneuralmechanisms bypresentingdecisionproblemsin\nlinguistic\/numerical form. The very fact that different wordings of the same decision prob-\nlem elicit different choices suggests that the decision problem itself is not getting through.\nSpurred by this observation, psychologists have tried presenting problems in uncertain rea-\nsoning and decision making in \u201cevolutionarily appropriate\u201d forms; for example, instead of\nsaying \u201c90% survival rate,\u201d the experimenter might show 100 stick-figure animations of the\noperation, wherethepatient diesin10ofthemandsurvives in90. (Boredom isacomplicat-\ning factor in these experiments!) With decision problems posed in this way, people seem to\nbemuchclosertorationalbehaviorthanpreviously suspected. 622 Chapter 16. MakingSimpleDecisions\n16.4 MULTIATTRIBUTE UTILITY FUNCTIONS\nDecision making in the field of public policy involves high stakes, in both money and lives.\nForexample, indeciding whatlevelsofharmfulemissions to allow fromapowerplant, pol-\nicymakersmustweighthepreventionofdeathanddisability againstthebenefitofthepower\nand the economic burden of mitigating the emissions. Siting a new airport requires consid-\neration of the disruption caused by construction; the cost of land; the distance from centers\nofpopulation; the noise offlight operations; safety issues arising from local topography and\nweatherconditions; and so on. Problems like these, inwhich outcomes are characterized by\nMULTIATTRIBUTE twoormoreattributes, arehandledby multiattributeutilitytheory.\nUTILITYTHEORY\nWe will call the attributes X=X ,...,X ; a complete vector of assignments will be\n1 n\nx=(cid:16)x ,...,x (cid:17),whereeachx iseitheranumericvalueoradiscretevaluewithanassumed\n1 n i\nordering on values. We will assume that higher values of an attribute correspond to higher\nutilities, all other things being equal. For example, if we choose AbsenceOfNoise as an\nattributeintheairportproblem,thenthegreateritsvalue,thebetterthesolution.8 Webeginby\nexamining casesinwhichdecisions canbemadewithoutcombining theattribute valuesinto\na single utility value. Then we look at cases in which the utilities of attribute combinations\ncanbespecifiedveryconcisely.\n16.4.1 Dominance\nSupposethatairportsiteS costsless,generateslessnoisepollution,andissaferthansiteS .\n1 2\nOne would not hesitate to reject S . We then say that there is strict dominance of S over\nSTRICTDOMINANCE 2 1\nS . Ingeneral, ifanoption isoflowervalueonallattributes thansomeotheroption, itneed\n2\nnot be considered further. Strict dominance is often very useful in narrowing down the field\nof choices to the real contenders, although it seldom yields a unique choice. Figure 16.4(a)\nshowsaschematicdiagram forthetwo-attribute case.\nThatisfineforthedeterministic case, inwhichtheattribute valuesareknownforsure.\nWhat about the general case, where the outcomes are uncertain? A direct analog of strict\ndominancecanbeconstructed, where,despitetheuncertainty, allpossibleconcreteoutcomes\nfor S strictly dominate all possible outcomes for S . (See Figure 16.4(b).) Of course, this\n1 2\nwillprobably occurevenlessoftenthaninthedeterministic case.\nSTOCHASTIC Fortunately, there isa more useful generalization called stochastic dominance, which\nDOMINANCE\noccurs very frequently in real problems. Stochastic dominance is easiest to understand in\nthecontextofasingleattribute. Supposewebelievethatthecostofsitingtheairportat S is\n1\nuniformlydistributedbetween$2.8billionand$4.8billionandthatthecostatS isuniformly\n2\ndistributedbetween$3billionand$5.2billion. Figure16.5(a)showsthesedistributions, with\ncost plotted asanegative value. Then, given only the information that utility decreases with\n8 Insomecases,itmaybenecessarytosubdividetherangeofvaluessothatutilityvariesmonotonicallywithin\neachrange.Forexample,iftheRoomTemperatureattributehasautilitypeakat70\u25e6F,wewouldsplititintotwo\nattributesmeasuringthedifferencefromtheideal,onecolderandonehotter.Utilitywouldthenbemonotonically\nincreasingineachattribute. Section16.4. Multiattribute UtilityFunctions 623\nX X\n2 2\nThis region\ndominates A\nB\nC B\nC\nA A\nD\nX X\n1 1\n(a) (b)\nFigure16.4 Strictdominance.(a)Deterministic:OptionAisstrictlydominatedbyBbut\nnotbyCorD.(b)Uncertain:AisstrictlydominatedbyBbutnotbyC.\n0.6 1.2\n0.5 1\n0.4 0.8\nS\n2\n0.3 S S 0.6\n2 1\nS\n1\n0.2 0.4\n0.1 0.2\n0 0\n-6 -5.5 -5 -4.5 -4 -3.5 -3 -2.5 -2 -6 -5.5 -5 -4.5 -4 -3.5 -3 -2.5 -2\nNegative cost Negative cost\n(a) (b)\nFigure16.5 Stochastic dominance. (a) S stochastically dominatesS on cost. (b) Cu-\n1 2\nmulativedistributionsforthenegativecostofS andS .\n1 2\ncost,wecansaythatS stochasticallydominatesS (i.e.,S canbediscarded). Itisimportant\n1 2 2\ntonotethatthisdoesnotfollowfromcomparingtheexpectedcosts. Forexample,ifweknew\nthecostofS tobeexactly$3.8billion,thenwewouldbeunabletomakeadecisionwithout\n1\nadditional information on theutility of money. (It might seem odd that moreinformation on\nthe cost of S could make the agent less able to decide. The paradox is resolved by noting\n1\nthatintheabsenceofexactcostinformation, thedecisioniseasiertomakebutismorelikely\ntobewrong.)\nTheexactrelationship betweentheattributedistributions neededtoestablishstochastic\ndominanceisbestseenbyexaminingthecumulativedistributions,showninFigure16.5(b).\n(Seealso Appendix A.) Thecumulative distribution measures theprobability thatthecostis\nless than or equal to any given amount\u2014that is, it integrates the original distribution. If the\ncumulative distribution for S is always to the right of the cumulative distribution for S ,\n1 2\nytilibaborP ytilibaborP 624 Chapter 16. MakingSimpleDecisions\nthen,stochastically speaking, S ischeaperthanS . Formally,iftwoactions A andA lead\n1 2 1 2\ntoprobability distributions p (x)andp (x)onattribute X,thenA stochastically dominates\n1 2 1\nA onX if\n2\n(cid:26)x (cid:26)x\n\u2200x p (x(cid:2) ) dx(cid:2) \u2264 p (x(cid:2) )dx(cid:2) .\n1 2\n\u2212\u221e \u2212\u221e\nTherelevanceofthisdefinitiontotheselectionofoptimaldecisionscomesfromthefollowing\nproperty: ifA stochasticallydominatesA ,thenforanymonotonicallynondecreasingutility\n1 2\nfunction U(x), the expected utility of A is at least as high as the expected utility of A .\n1 2\nHence,ifanactionisstochastically dominated byanotheraction onallattributes, thenitcan\nbediscarded.\nThe stochastic dominance condition might seem rather technical and perhaps not so\neasy to evaluate without extensive probability calculations. In fact, it can be decided very\neasilyinmanycases. Suppose,forexample,thattheconstructiontransportation costdepends\non the distance to the supplier. The cost itself is uncertain, but the greater the distance, the\ngreater the cost. If S is closer than S , then S will dominate S on cost. Although we\n1 2 1 2\nwill not present them here, there exist algorithms for propagating this kind of qualitative\nQUALITATIVE\ninformation among uncertain variables in qualitative probabilistic networks, enabling a\nPROBABILISTIC\nNETWORKS\nsystemtomakerationaldecisionsbasedonstochasticdominance,withoutusinganynumeric\nvalues.\n16.4.2 Preference structure and multiattribute utility\nSuppose we have n attributes, each of which has d distinct possible values. To specify the\ncompleteutilityfunctionU(x ,...,x ),weneeddnvaluesintheworstcase. Now,theworst\n1 n\ncasecorrespondstoasituationinwhichtheagent\u2019spreferenceshavenoregularityatall. Mul-\ntiattributeutilitytheoryisbasedonthesuppositionthatthepreferencesoftypicalagentshave\nmuchmorestructurethanthat. Thebasicapproachistoidentifyregularitiesinthepreference\nREPRESENTATION behaviorwewouldexpecttoseeandtousewhatarecalledrepresentationtheoremstoshow\nTHEOREM\nthatanagentwithacertainkindofpreference structure has autilityfunction\nU(x ,...,x ) = F[f (x ),...,f (x )],\n1 n 1 1 n n\nwhere F is, we hope, a simple function such as addition. Notice the similarity to the use of\nBayesiannetworkstodecomposethejointprobability ofseveralrandom variables.\nPreferences withoutuncertainty\nLet us begin with the deterministic case. Rememberthat for deterministic environments the\nagent has a value function V(x ,...,x ); the aim is to represent this function concisely.\n1 n\nThe basic regularity that arises in deterministic preference structures is called preference\nPREFERENCE independence. Twoattributes X and X are preferentially independent of a third attribute\nINDEPENDENCE 1 2\nX ifthe preference between outcomes (cid:16)x ,x ,x (cid:17)and (cid:16)x(cid:2) ,x(cid:2) ,x (cid:17)does not depend on the\n3 1 2 3 1 2 3\nparticularvalue x forattribute X .\n3 3\nGoing back to the airport example, where we have (among other attributes) Noise,\nCost,andDeaths toconsider, onemayproposethatNoise andCost arepreferentially inde- Section16.4. Multiattribute UtilityFunctions 625\npendentofDeaths. Forexample,ifwepreferastatewith20,000peopleresidingintheflight\npathandaconstructioncostof$4billionoverastatewith70,000peopleresidingintheflight\npathandacostof$3.7billionwhenthesafetylevelis0.06deathspermillionpassengermiles\ninboth cases, thenwewouldhavethesamepreference whenthe safetylevelis0.12or0.03;\nand the sameindependence would hold forpreferences between any other pairof values for\nNoise and Cost. It is also apparent that Cost and Deaths are preferentially independent of\nNoise and that Noise and Deaths are preferentially independent of Cost. We say that the\nMUTUAL set ofattributes {Noise,Cost,Deaths}exhibits mutualpreferential independence(MPI).\nPREFERENTIAL\nINDEPENDENCE\nMPI says that, whereas each attribute may be important, it does not affect the way in which\nonetradesofftheotherattributes againsteachother.\nMutual preferential independence is something of a mouthful, but thanks to a remark-\nabletheoremduetotheeconomistGe\u00b4rardDebreu(1960),wecanderivefromitaverysimple\nform fortheagent\u2019s value function: Ifattributes X , ..., X aremutually preferentially in-\n1 n\ndependent, thentheagent\u2019spreferencebehaviorcanbedescribedasmaximizingthefunction\n(cid:12)\nV(x ,...,x ) = V (x ),\n1 n i i\ni\nwhere each V is a value function referring only to the attribute X . For example, it might\ni i\nwellbethecasethattheairportdecision canbemadeusingavaluefunction\nV(noise,cost,deaths) = \u2212noise \u00d7104\u2212cost \u2212deaths \u00d71012 .\nADDITIVEVALUE Avalue function ofthistypeiscalled anadditivevaluefunction. Additivefunctions arean\nFUNCTION\nextremely natural way to describe an agent\u2019s preferences and are valid in many real-world\nsituations. Fornattributes, assessinganadditivevaluefunctionrequires assessingnseparate\none-dimensionalvaluefunctionsratherthanonen-dimensionalfunction;typically,thisrepre-\nsentsanexponentialreductioninthenumberofpreferenceexperimentsthatareneeded. Even\nwhen MPI does not strictly hold, as might be the case at extreme values of the attributes, an\nadditive value function might still provide a good approximation to the agent\u2019s preferences.\nThis is especially true when the violations of MPI occur in portions of the attribute ranges\nthatareunlikely tooccurinpractice.\nTounderstand MPI better, ithelps tolook at cases where it doesn\u2019t hold. Suppose you\nare at a medieval market, considering the purchase of some hunting dogs, some chickens,\nand some wicker cages for the chickens. The hunting dogs are very valuable, but if you\ndon\u2019t haveenough cages forthechickens, thedogs willeatthechickens; hence, thetradeoff\nbetween dogs and chickens depends strongly on the number of cages, and MPI is violated.\nTheexistenceofthesekindsofinteractionsamongvariousattributesmakesitmuchharderto\nassesstheoverallvaluefunction.\nPreferences withuncertainty\nWhen uncertainty is present in the domain, we also need to consider the structure of prefer-\nences between lotteries and to understand the resulting properties of utility functions, rather\nthan just value functions. The mathematics of this problem can become quite complicated,\nsowepresent justoneofthemainresults togiveaflavorofwhatcanbedone. Thereaderis\nreferredtoKeeneyandRaiffa(1976)forathorough surveyof thefield. 626 Chapter 16. MakingSimpleDecisions\nUTILITY The basic notion of utility independence extends preference independence to cover\nINDEPENDENCE\nlotteries: aset ofattributes Xisutility independent ofaset ofattributes Yifpreferences be-\ntweenlotteriesontheattributes inXareindependent oftheparticularvaluesoftheattributes\nMUTUALLYUTILITY in Y. A set of attributes is mutually utility independent (MUI) if each of its subsets is\nINDEPENDENT\nutility-independent of the remaining attributes. Again, it seems reasonable to propose that\ntheairportattributes areMUI.\nMUI implies that the agent\u2019s behavior can be described using a multiplicative utility\nMULTIPLICATIVE function(Keeney,1974). Thegeneralformofamultiplicativeutilityfunctionisbestseenby\nUTILITYFUNCTION\nlooking atthecaseforthreeattributes. Forconciseness, weuseU tomeanU (x ):\ni i i\nU =k U +k U +k U +k k U U +k k U U +k k U U\n1 1 2 2 3 3 1 2 1 2 2 3 2 3 3 1 3 1\n+k k k U U U .\n1 2 3 1 2 3\nAlthoughthisdoesnotlookverysimple,itcontainsjustthreesingle-attributeutilityfunctions\nandthreeconstants. Ingeneral,ann-attributeproblemexhibitingMUIcanbemodeledusing\nn single-attribute utilities and n constants. Each of the single-attribute utility functions can\nbe developed independently of the other attributes, and this combination will be guaranteed\nto generate the correct overall preferences. Additional assumptions are required to obtain a\npurelyadditiveutilityfunction.\n16.5 DECISION NETWORKS\nIn this section, welook at a general mechanism formaking rational decisions. The notation\nis often called an influence diagram (Howard and Matheson, 1984), but we will use the\nINFLUENCEDIAGRAM\nmore descriptive term decision network. Decision networks combine Bayesian networks\nDECISIONNETWORK\nwithadditional nodetypesforactionsandutilities. Weuseairportsitingasanexample.\n16.5.1 Representing a decisionproblem witha decisionnetwork\nInitsmostgeneral form,adecisionnetworkrepresents information abouttheagent\u2019scurrent\nstate, its possible actions, the state that will result from the agent\u2019s action, and the utility of\nthat state. It therefore provides a substrate forimplementing utility-based agents of the type\nfirst introduced in Section 2.4.5. Figure 16.6 shows a decision network forthe airport siting\nproblem. Itillustrates thethreetypesofnodesused:\n\u2022 Chancenodes(ovals)representrandomvariables,justastheydoinBayesiannetworks.\nCHANCENODES\nTheagentcould beuncertain about theconstruction cost,thelevelofairtrafficandthe\npotential forlitigation, andtheDeaths,Noise,andtotalCost variables, eachofwhich\nalsodepends onthesitechosen. Eachchance node hasassociated withitaconditional\ndistribution that is indexed by the state of the parent nodes. In decision networks, the\nparent nodes can include decision nodes as well as chance nodes. Note that each of\nthe current-state chance nodes could be part of alarge Bayesian network for assessing\nconstruction costs,airtrafficlevels,orlitigationpotentials.\n\u2022 Decision nodes(rectangles) represent points wherethedecision makerhas achoice of\nDECISIONNODES Section16.5. DecisionNetworks 627\nAirport Site\nAir Traffic Deaths\nLitigation Noise U\nConstruction Cost\nFigure16.6 Asimpledecisionnetworkfortheairport-sitingproblem.\nactions. In this case, the AirportSite action can take on a different value for each site\nunder consideration. The choice influences the cost, safety, and noise that will result.\nInthis chapter, weassume thatwearedealing withasingle decision node. Chapter17\ndealswithcasesinwhichmorethanonedecision mustbemade.\n\u2022 Utility nodes (diamonds) represent the agent\u2019s utility function.9 The utility node has\nUTILITYNODES\nas parents all variables describing the outcome that directly affect utility. Associated\nwith the utility node is a description of the agent\u2019s utility as a function of the parent\nattributes. The description could be just a tabulation of the function, or it might be a\nparameterized additiveorlinearfunction oftheattribute values.\nA simplified form is also used in many cases. The notation remains identical, but the\nchancenodesdescribing theoutcomestateareomitted. Instead, theutilitynodeisconnected\ndirectlytothecurrent-statenodesandthedecisionnode. Inthiscase,ratherthanrepresenting\nautilityfunctiononoutcomestates,theutilitynoderepresentstheexpectedutilityassociated\nwith each action, as defined in Equation (16.1) on page 611; that is, the node is associated\nACTION-UTILITY with an action-utility function (also known as a Q-function in reinforcement learning, as\nFUNCTION\ndescribed in Chapter 21). Figure 16.7 shows the action-utility representation of the airport\nsitingproblem.\nNoticethat,becausetheNoise,Deaths,andCost chance nodesinFigure16.6referto\nfuture states, they can neverhave theirvalues set asevidence variables. Thus, thesimplified\nversion that omits these nodes can be used whenever the more general form can be used.\nAlthough the simplified form contains fewer nodes, the omission of an explicit description\nof the outcome of the siting decision means that it is less flexible with respect to changes in\ncircumstances. Forexample,inFigure16.6,achange inaircraft noiselevelscanbereflected\nby a change in the conditional probability table associated with the Noise node, whereas a\nchange in the weight accorded to noise pollution in the utility function can be reflected by\n9 Thesenodesarealsocalledvaluenodesintheliterature. 628 Chapter 16. MakingSimpleDecisions\nAirport Site\nAir Traffic\nLitigation U\nConstruction\nFigure16.7 Asimplifiedrepresentationoftheairport-sitingproblem. Chancenodescor-\nrespondingtooutcomestateshavebeenfactoredout.\na change in the utility table. In the action-utility diagram, Figure 16.7, on the other hand,\nall such changes have to be reflected by changes to the action-utility table. Essentially, the\naction-utility formulation isacompiledversionoftheoriginal formulation.\n16.5.2 Evaluating decisionnetworks\nActionsareselected byevaluating thedecision networkfor eachpossible settingofthedeci-\nsionnode. Oncethedecision nodeisset, itbehaves exactly likeachance nodethathasbeen\nsetasanevidencevariable. Thealgorithm forevaluating decisionnetworks isthefollowing:\n1. Settheevidence variablesforthecurrentstate.\n2. Foreachpossible valueofthedecision node:\n(a) Setthedecision nodetothatvalue.\n(b) Calculate theposterior probabilities fortheparent nodes oftheutility node, using\nastandard probabilistic inference algorithm.\n(c) Calculatetheresulting utilityfortheaction.\n3. Returntheactionwiththehighestutility.\nThis is a straightforward extension of the Bayesian network algorithm and can be incorpo-\nrated directly into the agent design given in Figure 13.1 on page 484. We will see in Chap-\nter 17 that the possibility of executing several actions in sequence makes the problem much\nmoreinteresting.\n16.6 THE VALUE OF INFORMATION\nInthepreceding analysis, wehaveassumedthatallrelevant information, oratleastallavail-\nable information, is provided to the agent before it makes its decision. In practice, this is Section16.6. TheValueofInformation 629\nhardly ever the case. One of the most important parts of decision making is knowing what\nquestions to ask. Forexample, a doctor cannot expect to be provided with the results of all\npossiblediagnostictestsandquestionsatthetimeapatientfirstenterstheconsultingroom.10\nTestsare often expensive and sometimes hazardous (both directly and because ofassociated\ndelays). Their importance depends on two factors: whether the test results would lead to a\nsignificantly bettertreatmentplan,andhowlikelythevarioustestresultsare.\nINFORMATIONVALUE This section describes information value theory, which enables an agent to choose\nTHEORY\nwhat information to acquire. We assume that, prior to selecting a \u201creal\u201d action represented\nby the decision node, the agent can acquire the value of any of the potentially observable\nchance variables in the model. Thus, information value theory involves a simplified form\nof sequential decision making\u2014simplified because the observation actions affect only the\nagent\u2019s belief state, not the external physical state. The value of any particular observation\nmustderivefromthepotentialtoaffecttheagent\u2019seventualphysicalaction;andthispotential\ncanbeestimated directlyfromthedecision modelitself.\n16.6.1 A simpleexample\nSupposeanoilcompanyishopingtobuyoneofnindistinguishable blocksofocean-drilling\nrights. LetusassumefurtherthatexactlyoneoftheblockscontainsoilworthC dollars,while\nthe others are worthless. The asking price of each block is C\/n dollars. If the company is\nrisk-neutral, thenitwillbeindifferent betweenbuyingablockandnotbuyingone.\nNow suppose that a seismologist offers the company the results of a survey of block\nnumber 3, which indicates definitively whether the block contains oil. How much should\nthe company be willing to pay for the information? The way to answer this question is to\nexaminewhatthecompanywoulddoifithadtheinformation:\n\u2022 Withprobability 1\/n,thesurveywillindicate oilinblock 3. Inthis case, thecompany\nwillbuyblock3forC\/ndollarsandmakeaprofitofC\u2212C\/n = (n\u22121)C\/ndollars.\n\u2022 Withprobability(n\u22121)\/n,thesurveywillshowthattheblockcontainsnooil,inwhich\ncase the company will buy a different block. Now the probability of finding oil in one\noftheotherblockschangesfrom1\/nto1\/(n\u22121),sothecompanymakesanexpected\nprofitofC\/(n\u22121)\u2212C\/n = C\/n(n\u22121)dollars.\nNowwecancalculate theexpected profit,giventhesurveyinformation:\n1 (n\u22121)C n\u22121 C\n\u00d7 + \u00d7 = C\/n.\nn n n n(n\u22121)\nTherefore, the company should be willing to pay the seismologist up to C\/n dollars for the\ninformation: theinformation isworthasmuchastheblockitself.\nThe value of information derives from the fact that with the information, one\u2019s course\nof action can be changed to suit the actual situation. One can discriminate according to the\nsituation, whereas without the information, one has to do what\u2019s best on average over the\npossible situations. In general, the value of a given piece of information is defined to be the\ndifference inexpected valuebetweenbestactionsbeforeandafterinformation isobtained.\n10 IntheUnitedStates,theonlyquestionthatisalwaysaskedbeforehandiswhetherthepatienthasinsurance. 630 Chapter 16. MakingSimpleDecisions\n16.6.2 A general formulaforperfect information\nItissimpletoderiveageneralmathematicalformulaforthevalueofinformation. Weassume\nthat exact evidence canbeobtained about the value ofsomerandom variable E (that is, we\nj\nVALUEOFPERFECT learnE = e ),sothephrase valueofperfectinformation(VPI)isused.11\nINFORMATION j j\nLet the agent\u2019s initial evidence be e. Then the value of the current best action \u03b1 is\ndefinedby\n(cid:12)\nEU(\u03b1|e)= max P(RESULT(a)=s(cid:2)|a,e)U(s(cid:2) ),\na\ns(cid:3)\nandthevalueofthenewbestaction(afterthenewevidence E = e isobtained) willbe\nj j\n(cid:12)\nEU(\u03b1 ej|e,e j) = max P(RESULT(a)=s(cid:2)|a,e,e j)U(s(cid:2) ).\na\ns(cid:3)\nButE isarandom variable whosevalue is currently unknown, sotodetermine thevalue of\nj\ndiscovering E ,givencurrentinformation ewemustaverageoverallpossiblevaluese that\nj jk\nwemightdiscoverforE ,usingourcurrentbeliefsaboutitsvalue:\nj\n(cid:31)\n(cid:12)\nVPI (E )= P(E =e |e)EU(\u03b1 |e,E =e ) \u2212EU(\u03b1|e).\ne j j jk ejk j jk\nk\nTo get some intuition for this formula, consider the simple case where there are only two\nactions,a anda ,fromwhichtochoose. Theircurrentexpectedutilitiesare U andU . The\n1 2 1 2\n(cid:2) (cid:2)\ninformation E = e will yield some new expected utilities U and U for the actions, but\nj jk 1 2\nbefore weobtain E ,wewillhavesome probability distributions overthe possible values of\nj\n(cid:2) (cid:2)\nU andU (whichweassumeareindependent).\n1 2\nSuppose that a and a represent two different routes through a mountain range in\n1 2\nwinter. a is anice, straight highway through a low pass, and a is awinding dirt road over\n1 2\nthe top. Just given this information, a is clearly preferable, because it is quite possible that\n1\na is blocked by avalanches, whereas it is unlikely that anything blocks a . U is therefore\n2 1 1\nclearly higher than U . Itispossible toobtain satellite reports E on theactual state ofeach\n2 j\n(cid:2) (cid:2)\nroad that would give new expectations, U and U , for the two crossings. The distributions\n1 2\nfortheseexpectations areshowninFigure16.8(a). Obviously,inthiscase,itisnotworththe\nexpenseofobtainingsatellitereports,becauseitisunlikelythattheinformationderivedfrom\nthemwillchangetheplan. Withnochange, information hasnovalue.\nNowsuppose thatwearechoosing betweentwodifferentwindingdirtroadsofslightly\ndifferent lengths and we are carrying a seriously injured passenger. Then, even when U\n1\n(cid:2) (cid:2)\nand U are quite close, the distributions of U and U are very broad. There is a significant\n2 1 2\npossibility thatthesecondroutewillturnouttobeclearwhilethefirstisblocked, andinthis\n11 There isno loss of expressiveness in requiring perfect information. Suppose we wanted to model the case\ninwhichwebecomesomewhat morecertainabout avariable. We candothatbyintroducing anothervariable\naboutwhichwelearnperfectinformation. Forexample, supposeweinitiallyhavebroaduncertaintyabout the\nvariable Temperature. Then we gain the perfect knowledge Thermometer = 37; this gives us imperfect\ninformationaboutthetrueTemperature,andtheuncertaintyduetomeasurementerrorisencodedinthesensor\nmodelP(Thermometer|Temperature).SeeExercise16.17foranotherexample. Section16.6. TheValueofInformation 631\nP(U | E) P(U | E) P(U | E)\nj j j\nU U U\nU U U U U U\n2 1 2 1 2 1\n(a) (b) (c)\nFigure16.8 Threegenericcasesforthevalueofinformation. In(a), a willalmostcer-\n1\ntainlyremainsuperiortoa ,sotheinformationisnotneeded.In(b),thechoiceisunclearand\n2\ntheinformationiscrucial. In(c),thechoiceisunclear,butbecauseitmakeslittledifference,\ntheinformationislessvaluable.(Note: ThefactthatU hasahighpeakin(c)meansthatits\n2\nexpectedvalueisknownwithhighercertaintythanU .)\n1\ncase the difference in utilities will be very high. The VPI formula indicates that it might be\nworthwhilegettingthesatellite reports. Suchasituation isshowninFigure16.8(b).\nFinally,supposethatwearechoosingbetweenthetwodirtroadsinsummertime,when\nblockage byavalanches isunlikely. Inthis case, satellite reports might show one route tobe\nmore scenic than the other because of flowering alpine meadows, orperhaps wetterbecause\nof errant streams. It is therefore quite likely that we would change our plan if we had the\ninformation. In this case, however, the difference in value between the two routes is still\nlikelytobeverysmall,sowewillnotbothertoobtainthereports. Thissituation isshownin\nFigure16.8(c).\nIn sum, information has value to the extent that it is likely to cause a change of plan\nandtotheextentthatthenewplanwillbesignificantly betterthantheoldplan.\n16.6.3 Properties ofthe valueofinformation\nOne might ask whether it is possible for information to be deleterious: can it actually have\nnegative expected value? Intuitively, one should expect this to be impossible. After all, one\ncouldintheworstcasejustignoretheinformationandpretendthatonehasneverreceivedit.\nThisisconfirmedbythefollowingtheorem,whichappliestoanydecision-theoretic agent:\nTheexpected valueofinformation isnonnegative:\n\u2200e,E VPI (E ) \u2265 0.\nj e j\nThetheoremfollowsdirectlyfromthedefinitionofVPI,andweleavetheproofasanexercise\n(Exercise16.18). Itis,ofcourse,atheoremaboutexpectedvalue,notactualvalue. Additional\ninformation can easily lead to a plan that turns out to be worse than the original plan if the\ninformation happens tobemisleading. Forexample, amedical testthatgivesafalsepositive\nresultmayleadtounnecessarysurgery;butthatdoesnotmeanthatthetestshouldn\u2019tbedone. 632 Chapter 16. MakingSimpleDecisions\nItisimportanttorememberthatVPIdependsonthecurrentstateofinformation, which\nis why it is subscripted. It can change as more information is acquired. Forany given piece\nof evidence E , the value of acquiring it can go down (e.g., if another variable strongly\nj\nconstrains the posterior for E ) or up (e.g., if another variable provides a clue on which E\nj j\nbuilds, enabling anewandbetterplantobedevised). Thus,VPIisnotadditive. Thatis,\nVPI (E ,E )(cid:7)= VPI (E )+VPI (E ) (ingeneral) .\ne j k e j e k\nVPIis,however,orderindependent. Thatis,\nVPI (E ,E )= VPI (E )+VPI (E ) = VPI (E )+VPI (E ).\ne j k e j e,ej k e k e,ek j\nOrder independence distinguishes sensing actions from ordinary actions and simplifies the\nproblem ofcalculating thevalueofasequenceofsensingactions.\n16.6.4 Implementationofaninformation-gathering agent\nA sensible agent should ask questions in a reasonable order, should avoid asking questions\nthat are irrelevant, should take into account the importance of each piece of information in\nrelation to its cost, and should stop asking questions when that is appropriate. All of these\ncapabilities canbeachievedbyusingthevalueofinformation asaguide.\nFigure 16.9 shows the overall design of an agent that can gather information intel-\nligently before acting. For now, we assume that with each observable evidence variable\nE , there is an associated cost, Cost(E ), which reflects the cost of obtaining the evidence\nj j\nthrough tests, consultants, questions, orwhatever. Theagentrequests whatappears tobethe\nmost efficient observation interms of utility gain perunit cost. Weassume that the result of\ntheactionRequest(E )isthatthenextperceptprovides thevalueof E . Ifnoobservation is\nj j\nworthitscost,theagentselectsa\u201creal\u201daction.\nThe agent algorithm we have described implements a form of information gathering\nthatiscalled myopic. Thisisbecause itusestheVPIformula shortsightedly, calculating the\nMYOPIC\nvalue of information as if only a single evidence variable will be acquired. Myopic control\nis based on the same heuristic idea as greedy search and often works well in practice. (For\nexample, it has been shown to outperform expert physicians in selecting diagnostic tests.)\nfunctionINFORMATION-GATHERING-AGENT(percept)returnsanaction\npersistent: D,adecisionnetwork\nintegratepercept intoD\nj \u2190thevaluethatmaximizesVPI(Ej)\/Cost(Ej)\nifVPI(Ej) > Cost(Ej)\nreturnREQUEST(Ej)\nelsereturnthebestactionfromD\nFigure16.9 Designofasimpleinformation-gatheringagent. Theagentworksbyrepeat-\nedlyselecting the observationwith the highestinformationvalue, untilthe cost of the next\nobservationisgreaterthanitsexpectedbenefit. Section16.7. Decision-Theoretic ExpertSystems 633\nHowever, if there is no single evidence variable that will help a lot, a myopic agent might\nhastily take an action when it would have been better to request two or more variables first\nand then take action. A better approach in this situation would be to construct a conditional\nplan (as described in Section 11.3.2) that asks for variable values and takes different next\nstepsdepending ontheanswer.\nOnefinalconsideration istheeffectaseries ofquestions willhaveonahumanrespon-\ndent. Peoplemayrespondbettertoaseriesofquestionsifthey\u201cmakesense,\u201dsosomeexpert\nsystems are built to take this into account, asking questions in an order that maximizes the\ntotalutilityofthesystemandhumanratherthananorderthatmaximizesvalueofinformation.\n16.7 DECISION-THEORETIC EXPERT SYSTEMS\nThefieldofdecisionanalysis,whichevolvedinthe1950sand1960s,studiestheapplication\nDECISIONANALYSIS\nof decision theory to actual decision problems. It is used to help make rational decisions in\nimportant domains where the stakes are high, such as business, government, law, military\nstrategy,medicaldiagnosisandpublichealth,engineering design,andresourcemanagement.\nThe process involves a careful study of the possible actions and outcomes, as well as the\npreferences placed on each outcome. It is traditional in decision analysis to talk about two\nroles: the decision maker states preferences between outcomes, and the decision analyst\nDECISIONMAKER\nenumeratesthepossibleactionsandoutcomesandelicitspreferencesfromthedecisionmaker\nDECISIONANALYST\nto determine the best course of action. Until the early 1980s, the main purpose of decision\nanalysis was to help humans make decisions that actually reflect their own preferences. As\nmoreandmoredecision processes become automated, decision analysis isincreasingly used\ntoensurethattheautomated processes arebehaving asdesired.\nEarlyexpertsystemresearchconcentrated onansweringquestions, ratherthanonmak-\ning decisions. Those systems that did recommend actions rather than providing opinions on\nmatters of fact generally did so using condition-action rules, rather than with explicit rep-\nresentations of outcomes and preferences. The emergence of Bayesian networks in the late\n1980s made it possible to build large-scale systems that generated sound probabilistic infer-\nences from evidence. The addition of decision networks means that expert systems can be\ndeveloped that recommend optimal decisions, reflecting the preferences of the agent aswell\nastheavailable evidence.\nA system that incorporates utilities can avoid one of the most common pitfalls associ-\natedwiththeconsultationprocess: confusinglikelihoodandimportance. Acommonstrategy\ninearlymedicalexpertsystems,forexample,wastorankpossiblediagnosesinorderoflike-\nlihood and report the most likely. Unfortunately, this can be disastrous! Forthe majority of\npatientsingeneralpractice,thetwomostlikelydiagnosesareusually\u201cThere\u2019snothingwrong\nwithyou\u201dand\u201cYouhaveabadcold,\u201dbutifthethirdmostlikelydiagnosisforagivenpatient\nis lung cancer, that\u2019s a serious matter. Obviously, a testing or treatment plan should depend\nboth on probabilities and utilities. Current medical expert systems can take into account the\nvalueofinformation torecommendtests,andthendescribea differential diagnosis. 634 Chapter 16. MakingSimpleDecisions\nWenowdescribe theknowledge engineering process fordecision-theoretic expert sys-\ntems. Asanexample weconsider theproblem ofselecting amedical treatment forakind of\ncongenital heartdiseaseinchildren(seeLucas,1996).\nAbout0.8% ofchildren are born withaheart anomaly, the most commonbeing aortic\nAORTIC coarctation (aconstriction oftheaorta). Itcanbetreatedwithsurgery, angioplasty (expand-\nCOARCTATION\ningtheaortawithaballoonplacedinsidetheartery),ormedication. Theproblemistodecide\nwhattreatmenttouseandwhentodoit: theyoungertheinfant,thegreatertherisksofcertain\ntreatments,butonemustn\u2019twaittoolong. Adecision-theoreticexpertsystemforthisproblem\ncan be created by a team consisting of at least one domain expert (a pediatric cardiologist)\nandoneknowledgeengineer. Theprocess canbebrokendownintothefollowingsteps:\nCreate a causal model. Determine the possible symptoms, disorders, treatments, and\noutcomes. Then draw arcs between them, indicating what disorders cause what symptoms,\nandwhattreatmentsalleviatewhatdisorders. Someofthiswillbewellknowntothedomain\nexpert, and some will come from the literature. Often the model will match well with the\ninformalgraphical descriptions giveninmedicaltextbooks.\nSimplify to a qualitative decision model. Since we are using the model to make\ntreatment decisions and not for other purposes (such as determining the joint probability of\ncertain symptom\/disorder combinations), we can often simplify by removing variables that\nare not involved in treatment decisions. Sometimes variables will have to be split or joined\nto match the expert\u2019s intuitions. For example, the original aortic coarctation model had a\nTreatmentvariable withvalues surgery, angioplasty, andmedication, andaseparate variable\nfor Timing of the treatment. But the expert had a hard time thinking of these separately, so\ntheywerecombined, withTreatmenttakingonvaluessuchassurgery in1month. Thisgives\nusthemodelofFigure16.10.\nAssignprobabilities. Probabilities cancomefrompatientdatabases, literature studies,\northeexpert\u2019ssubjective assessments. Notethatadiagnostic system willreasonfromsymp-\ntomsandotherobservations tothedisease orothercause oftheproblems. Thus, intheearly\nyears of building these systems, experts were asked for the probability of a cause given an\neffect. Ingeneraltheyfoundthisdifficulttodo,andwerebetterabletoassesstheprobability\nofaneffectgivenacause. Somodernsystemsusuallyassesscausalknowledgeandencodeit\ndirectly in the Bayesian network structure of the model, leaving the diagnostic reasoning to\ntheBayesiannetworkinferencealgorithms (ShachterandHeckerman, 1987).\nAssign utilities. When there are a small number of possible outcomes, they can be\nenumeratedandevaluatedindividuallyusingthemethodsofSection16.3.1. Wewouldcreate\na scale from best to worst outcome and give each a numeric value, for example 0 for death\nand 1 for complete recovery. We would then place the other outcomes on this scale. This\ncanbedonebytheexpert, butitisbetterifthepatient (orin thecaseofinfants, thepatient\u2019s\nparents)canbeinvolved, becausedifferentpeoplehavedifferentpreferences. Ifthereareex-\nponentially manyoutcomes, weneed some waytocombine them using multiattribute utility\nfunctions. Forexample,wemaysaythatthecostsofvariouscomplications areadditive.\nVerify and refine the model. To evaluate the system we need a set of correct (input,\noutput) pairs; a so-called gold standard to compare against. For medical expert systems\nGOLDSTANDARD\nthis usually means assembling the best available doctors, presenting them with a few cases, Section16.7. Decision-Theoretic ExpertSystems 635\nSex\nPostcoarctectomy\nSyndrome\nTachypnea Tachycardia\nParadoxical\nFailure Hypertension\nTo Thrive\nAortic\nIntercostal Aneurysm\nDyspnea\nRecession\nParaplegia\nHeart Intermediate Late\nAge Treatment\nFailure Result Result\nHepato-\nCVA\nmegaly\nPulmonary Aortic\nCrepitations Dissection\nMyocardial\nCardiomegaly\nInfarction\nU\nFigure16.10 Influencediagramforaorticcoarctation(courtesyofPeterLucas).\nand asking them for their diagnosis and recommended treatment plan. We then see how\nwellthe system matches theirrecommendations. Ifitdoes poorly, wetryto isolate theparts\nthat are going wrong and fix them. It can be useful to run the system \u201cbackward.\u201d Instead\nof presenting the system with symptoms and asking for a diagnosis, we can present it with\na diagnosis such as \u201cheart failure,\u201d examine the predicted probability of symptoms such as\ntachycardia, andcomparewiththemedicalliterature.\nSENSITIVITY Perform sensitivity analysis. This important step checks whether the best decision is\nANALYSIS\nsensitivetosmallchangesintheassignedprobabilities andutilitiesbysystematicallyvarying\nthose parameters and running the evaluation again. If small changes lead to significantly\ndifferent decisions, then it could be worthwhile to spend more resources to collect better\ndata. Ifallvariationsleadtothesamedecision, thentheagentwillhavemoreconfidencethat\nitistherightdecision. Sensitivityanalysis isparticularly important, becauseoneofthemain 636 Chapter 16. MakingSimpleDecisions\ncriticisms ofprobabilistic approaches toexpert systems is that it istoo difficult to assess the\nnumerical probabilities required. Sensitivity analysis oftenrevealsthatmanyofthenumbers\nneed be specified only very approximately. For example, we might be uncertain about the\nconditional probability P(tachycardia|dyspnea), but if the optimal decision is reasonably\nrobusttosmallvariations intheprobability, thenourignorance islessofaconcern.\n16.8 SUMMARY\nThischaptershowshowtocombineutilitytheorywithprobabilitytoenableanagenttoselect\nactionsthatwillmaximizeitsexpected performance.\n\u2022 Probability theory describes what an agent should believe on the basis of evidence,\nutilitytheorydescribeswhatanagentwants,anddecisiontheoryputsthetwotogether\ntodescribe whatanagentshoulddo.\n\u2022 We can use decision theory to build a system that makes decisions by considering all\npossible actions and choosing the one that leads to the best expected outcome. Such a\nsystemisknownasarationalagent.\n\u2022 Utility theory shows that an agent whose preferences between lotteries are consistent\nwith a set of simple axioms can be described as possessing a utility function; further-\nmore,theagentselectsactions asifmaximizingitsexpected utility.\n\u2022 Multiattribute utility theory deals with utilities that depend on several distinct at-\ntributes of states. Stochastic dominance is a particularly useful technique for making\nunambiguous decisions, evenwithoutpreciseutilityvaluesforattributes.\n\u2022 Decision networks provide a simple formalism for expressing and solving decision\nproblems. Theyare anatural extension ofBayesian networks, containing decision and\nutilitynodesinaddition tochancenodes.\n\u2022 Sometimes, solving a problem involves finding more information before making a de-\ncision. The value of information is defined as the expected improvement in utility\ncomparedwithmakingadecisionwithouttheinformation.\n\u2022 Expert systems that incorporate utility information have additional capabilities com-\npared with pure inference systems. In addition to being able to make decisions, they\ncanusethevalueofinformation todecide whichquestions toask, ifany;theycanrec-\nommend contingency plans; and they can calculate the sensitivity of their decisions to\nsmallchanges inprobability andutilityassessments.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nThebookL\u2019artdePenser,alsoknownasthePort-RoyalLogic(Arnauld,1662)states:\nTojudgewhatonemustdotoobtainagoodoravoidanevil,itisnecessarytoconsider\nnotonlythegoodandtheevilinitself,butalsotheprobabilitythatithappensordoesnot\nhappen;andtoviewgeometricallytheproportionthatallthesethingshavetogether. Bibliographical andHistorical Notes 637\nModern texts talk of utility rather than good and evil, but this statement correctly notes that\none should multiply utility by probability (\u201cview geometrically\u201d) to give expected utility,\nand maximize that over all outcomes (\u201call these things\u201d) to \u201cjudge what one must do.\u201d It\nis remarkable how much this got right, 350 years ago, and only 8 years after Pascal and\nFermatshowed howtouse probability correctly. ThePort-Royal Logicalso marked thefirst\npublication ofPascal\u2019swager.\nDaniel Bernoulli (1738), investigating the St. Petersburg paradox (see Exercise 16.3),\nwas the first to realize the importance of preference measurement for lotteries, writing \u201cthe\nvalue of an item must not be based on its price, but rather on the utility that it yields\u201d (ital-\nics his). Utilitarian philosopher Jeremy Bentham (1823) proposed the hedonic calculus for\nweighing \u201cpleasures\u201d and \u201cpains,\u201d arguing that all decisions (not just monetary ones) could\nbereducedtoutilitycomparisons.\nThe derivation of numerical utilities from preferences was first carried out by Ram-\nsey(1931); theaxiomsforpreference inthepresenttextare closerinformtothoserediscov-\nered in Theory of Games and Economic Behavior (von Neumann and Morgenstern, 1944).\nAgoodpresentationoftheseaxioms,inthecourseofadiscussiononriskpreference,isgiven\nby Howard (1977). Ramsey had derived subjective probabilities (not just utilities) from an\nagent\u2019s preferences; Savage (1954) and Jeffrey (1983) carry out more recent constructions\nofthis kind. VonWinterfeldt andEdwards (1986) provide amodern perspective ondecision\nanalysis and its relationship to human preference structures. The micromort utility measure\nis discussed by Howard (1989). A 1994 survey by the Economist set the value of a life at\nbetween $750,000 and $2.6 million. However, Richard Thaler (1992) found irrational fram-\ning effects on the price one is willing to pay to avoid a risk of death versus the price one is\nwilling to be paid to accept a risk. For a 1\/1000 chance, a respondent wouldn\u2019t pay more\nthan$200toremovetherisk,butwouldn\u2019taccept$50,000totakeontherisk. Howmuchare\npeople willing topay foraQALY?When itcomes downtoaspecific case ofsaving oneself\nor a family member, the number is approximately \u201cwhatever I\u2019ve got.\u201d But we can ask at a\nsocietallevel: supposethereisavaccinethatwouldyieldX QALYsbutcostsY dollars;isit\nworthit? Inthiscasepeople reportawiderangeofvaluesfromaround $10,000 to$150,000\nper QALY (Prades et al., 2008). QALYs are much more widely used in medical and social\npolicy decision making thanare micromorts; see (Russell, 1990) foratypical example ofan\nargument foramajorchangeinpublichealthpolicyongrounds ofincreased expected utility\nmeasuredinQALYs.\nThe optimizer\u2019s curse was brought to the attention of decision analysts in a forceful\nway by Smith and Winkler (2006), who pointed out that the financial benefits to the client\nprojected by analysts for their proposed course of action almost never materialized. They\ntracethisdirectly tothebiasintroduced byselecting anoptimal action andshowthatamore\ncomplete Bayesian analysis eliminates the problem. The same underlying concept has been\nPOST-DECISION called post-decision disappointment by Harrison and March (1984) and was noted in the\nDISAPPOINTMENT\ncontext of analyzing capital investment projects by Brown (1974). The optimizer\u2019s curse is\nalso closely related to the winner\u2019s curse (Capen et al., 1971; Thaler, 1992), which applies\nWINNER\u2019SCURSE\nto competitive bidding in auctions: whoever wins the auction is very likely to have overes-\ntimated the value of the object in question. Capen et al. quote a petroleum engineer on the 638 Chapter 16. MakingSimpleDecisions\ntopicofbiddingforoil-drilling rights: \u201cIfonewinsatractagainsttwoorthreeothershemay\nfeel fine about his good fortune. But how should he feel if he won against 50 others? Ill.\u201d\nREGRESSIONTOTHE Finally, behind both curses is the general phenomenon of regression to themean, whereby\nMEAN\nindividualsselectedonthebasisofexceptionalcharacteristics previouslyexhibitedwill,with\nhighprobability, becomelessexceptional infuture.\nTheAllais paradox, due toNobelPrize-winning economist Maurice Allais (1953) was\ntested experimentally (Tversky and Kahneman, 1982; Conlisk, 1989) to show that people\nare consistently inconsistent in their judgments. The Ellsberg paradox on ambiguity aver-\nsion was introduced in the Ph.D.thesis of Daniel Ellsberg (Ellsberg, 1962), who went on to\nbecome a military analyst at the RAND Corporation and to leak documents known as The\nPentagon Papers, which contributed to the end of the Vietnam war and the resignation of\nPresident Nixon. Fox and Tversky (1995) describe a further study of ambiguity aversion.\nMark Machina (2005) gives an overview of choice under uncertainty and how it can vary\nfromexpected utilitytheory.\nTherehasbeenarecentoutpouring ofmore-or-less popularbooksonhumanirrational-\nity. The best known is Predictably Irrational (Ariely, 2009); others include Sway (Brafman\nand Brafman, 2009), Nudge (Thaler and Sunstein, 2009), Kluge (Marcus, 2009), How We\nDecide (Lehrer, 2009) and On Being Certain (Burton, 2009). They complement the classic\n(Kahneman et al., 1982) and the article that started it all (Kahneman and Tversky, 1979).\nThefieldofevolutionary psychology (Buss,2005), ontheotherhand, hasruncountertothis\nliterature, arguing that humans are quite rational in evolutionarily appropriate contexts. Its\nadherents pointoutthatirrationality ispenalized bydefinitioninanevolutionary contextand\nshowthatinsomecasesitisanartifactoftheexperimentalsetup(CumminsandAllen,1998).\nThere has been arecent resurgence of interest in Bayesian models of cognition, overturning\ndecadesofpessimism (OaksfordandChater,1998;Elio,2002;ChaterandOaksford,2008).\nKeeney and Raiffa (1976) give a thorough introduction to multiattribute utility the-\nory. They describe early computer implementations of methods for eliciting the necessary\nparameters for a multiattribute utility function and include extensive accounts of real appli-\ncations of the theory. In AI, the principal reference for MAUT is Wellman\u2019s (1985) paper,\nwhich includes a system called URP (Utility Reasoning Package) that can use a collection\nof statements about preference independence and conditional independence to analyze the\nstructure of decision problems. The use of stochastic dominance together with qualitative\nprobability models was investigated extensively by Wellman (1988, 1990a). Wellman and\nDoyle (1992) provide apreliminary sketch of how acomplex set of utility-independence re-\nlationships might be used to provide a structured model of a utility function, in much the\nsame way that Bayesian networks provide a structured model of joint probability distribu-\ntions. BacchusandGrove(1995, 1996)andLaMuraandShoham(1999)givefurtherresults\nalongtheselines.\nDecision theory has been a standard tool in economics, finance, and management sci-\nencesincethe1950s. Untilthe1980s,decisiontreeswerethemaintoolusedforrepresenting\nsimple decision problems. Smith (1988) gives an overview of the methodology of deci-\nsion analysis. Influence diagrams were introduced by Howard and Matheson (1984), based\non earlier work at SRI (Miller et al., 1976). Howard and Matheson\u2019s method involved the Bibliographical andHistorical Notes 639\nderivationofadecisiontreefromadecisionnetwork,butin generalthetreeisofexponential\nsize. Shachter(1986) developed amethod formaking decisions based directly onadecision\nnetwork, without the creation of an intermediate decision tree. This algorithm was also one\nof the first to provide complete inference formultiply connected Bayesian networks. Zhang\netal.(1994)showedhowtotakeadvantageofconditionalindependenceofinformationtore-\nducethesizeoftreesinpractice;theyusethetermdecisionnetworkfornetworksthatusethis\napproach(although othersuseitasasynonymforinfluencediagram). NilssonandLauritzen\n(2000) link algorithms for decision networks to ongoing developments in clustering algo-\nrithmsforBayesiannetworks. KollerandMilch(2003)showhowinfluencediagramscanbe\nusedtosolvegamesthatinvolvegathering information byopposing players, andDetwarasiti\nand Shachter(2005) show how influence diagrams can beused as anaid todecision making\nfor a team that shares goals but is unable to share all information perfectly. The collection\nbyOliverandSmith(1990)hasanumberofusefularticlesondecisionnetworks,asdoesthe\n1990specialissueofthejournalNetworks. Papersondecisionnetworksandutilitymodeling\nalsoappearregularly inthejournals ManagementScience andDecisionAnalysis.\nThe theory of information value was explored first in the context of statistical experi-\nments,whereaquasi-utility (entropy reduction) wasused(Lindley,1956). TheRussiancon-\ntroltheoristRuslanStratonovich(1965)developedthemoregeneraltheorypresentedhere,in\nwhich information has value by virtue of its ability to affect decisions. Stratonovich\u2019s work\nwas not known in the West, where Ron Howard (1966) pioneered the same idea. His paper\nendswiththeremark\u201cIfinformationvaluetheoryandassociateddecisiontheoreticstructures\ndo not in the future occupy a large part of the education of engineers, then the engineering\nprofessionwillfindthatitstraditionalroleofmanagingscientificandeconomicresourcesfor\nthe benefit of man has been forfeited to another profession.\u201d Todate, the implied revolution\ninmanagerial methodshasnotoccurred.\nRecent work by Krause and Guestrin (2009) shows that computing the exact non-\nmyopicvalueofinformationisintractableeveninpolytree networks. Thereareothercases\u2014\nmorerestricted thangeneral valueofinformation\u2014in which themyopicalgorithm doespro-\nvide a provably good approximation to the optimal sequence of observations (Krause et al.,\n2008). Insomecases\u2014forexample, looking fortreasure buried inoneofnplaces\u2014ranking\nexperimentsinorderofsuccessprobabilitydividedbycostgivesanoptimalsolution(Kadane\nandSimon,1977).\nSurprisingly few early AI researchers adopted decision-theoretic tools after the early\napplications inmedicaldecision makingdescribed inChapter13. Oneofthefewexceptions\nwas Jerry Feldman, who applied decision theory to problems in vision (Feldman and Yaki-\nmovsky,1974)andplanning(FeldmanandSproull,1977). Aftertheresurgenceofinterestin\nprobabilisticmethodsinAIinthe1980s,decision-theoreticexpertsystemsgainedwidespread\nacceptance (Horvitz et al., 1988; Cowell et al., 2002). In fact, from 1991 onward, the cover\ndesign of the journal Artificial Intelligence has depicted a decision network, although some\nartisticlicenseappears tohavebeentakenwiththedirection ofthearrows. 640 Chapter 16. MakingSimpleDecisions\nEXERCISES\n16.1 (AdaptedfromDavidHeckerman.) ThisexerciseconcernstheAlmanacGame,which\nis used by decision analysts to calibrate numeric estimation. For each of the questions that\nfollow, give your best guess of the answer, that is, a number that you think is as likely to be\ntoo high as it is to be too low. Also give your guess at a 25th percentile estimate, that is, a\nnumber that you think has a 25% chance of being too high, and a 75% chance of being too\nlow. Dothesameforthe75thpercentile. (Thus,youshould givethreeestimates inall\u2014low,\nmedian,andhigh\u2014foreachquestion.)\na. Numberofpassengers whoflewbetweenNewYorkandLosAngelesin1989.\nb. PopulationofWarsawin1992.\nc. YearinwhichCoronadodiscovered theMississippi River.\nd. NumberofvotesreceivedbyJimmyCarterinthe1976presidential election.\ne. Ageoftheoldestlivingtree,asof2002.\nf. HeightoftheHooverDaminfeet.\ng. Numberofeggsproduced inOregonin1985.\nh. NumberofBuddhistsintheworldin1992.\ni. NumberofdeathsduetoAIDSintheUnitedStatesin1981.\nj. NumberofU.S.patentsgranted in1901.\nThe correct answers appear after the last exercise of this chapter. From the point of view of\ndecisionanalysis,theinteresting thingisnothowcloseyourmedianguessescametothereal\nanswers, but rather how often the real answer came within your 25% and 75% bounds. If it\nwas about half the time, then your bounds are accurate. But if you\u2019re like most people, you\nwill be more sure of yourself than you should be, and fewer than half the answers will fall\nwithinthebounds. Withpractice,youcancalibrateyourselftogiverealisticbounds,andthus\nbemoreusefulinsupplyinginformationfordecisionmaking. Trythissecondsetofquestions\nandseeifthereisanyimprovement:\na. YearofbirthofZsaZsaGabor.\nb. Maximumdistance fromMarstothesuninmiles.\nc. ValueindollarsofexportsofwheatfromtheUnitedStatesin1992.\nd. TonshandledbytheportofHonolulu in1991.\ne. AnnualsalaryindollarsofthegovernorofCalifornia in1993.\nf. PopulationofSanDiegoin1990.\ng. YearinwhichRogerWilliamsfoundedProvidence, RhodeIsland.\nh. HeightofMt.Kilimanjaroinfeet.\ni. LengthoftheBrooklynBridgeinfeet.\nj. Numberofdeathsduetoautomobile accidents intheUnitedStatesin1992. Exercises 641\n16.2 Chris considers four used cars before buying the one withmaximum expected utility.\nPat considers ten cars and does the same. All other things being equal, which one is more\nlikelytohavethebettercar? Whichismorelikelytobedisappointedwiththeircar\u2019squality?\nByhowmuch(intermsofstandard deviations ofexpectedquality)?\n16.3 In 1713, Nicolas Bernoulli stated a puzzle, now called the St. Petersburg paradox,\nwhich works as follows. You have the opportunity to play a game in which a fair coin is\ntossed repeatedly until it comesup heads. Ifthefirstheads appears on thenthtoss, you win\n2n dollars.\na. Showthattheexpectedmonetary valueofthisgameisinfinite.\nb. Howmuchwouldyou,personally, paytoplaythegame?\nc. Nicolas\u2019scousinDanielBernoulliresolvedtheapparentparadoxin1738bysuggesting\nthattheutilityofmoneyismeasuredonalogarithmicscale(i.e.,U(S ) = alog n+b,\nn 2\nwhereS isthestateofhaving$n). Whatistheexpectedutilityofthegameunderthis\nn\nassumption?\nd. Whatisthemaximumamountthatitwouldberationaltopaytoplaythegame,assum-\ningthatone\u2019sinitialwealthis$k?\n16.4 Write a computer program to automate the process in Exercise 16.9. Try your pro-\ngram out on several people of different net worth and political outlook. Comment on the\nconsistency ofyourresults, bothforanindividual andacrossindividuals.\n16.5 The Surprise Candy Company makes candy in two flavors: 70% are strawberry fla-\nvor and 30% are anchovy flavor. Each new piece of candy starts out with a round shape;\nas it moves along the production line, a machine randomly selects a certain percentage to\nbe trimmed into a square; then, each piece is wrapped in a wrapper whose color is chosen\nrandomly to be red or brown. 80% of the strawberry candies are round and 80% have a red\nwrapper, while 90% of the anchovy candies are square and 90% have a brown wrapper. All\ncandiesaresoldindividually insealed,identical, blackboxes.\nNowyou, the customer, have justbought aSurprise candy atthestore buthave notyet\nopenedthebox. ConsiderthethreeBayesnetsinFigure16.11.\nWrapper Shape Wrapper Shape Flavor\nFlavor Flavor Wrapper Shape\n(i) (ii) (iii)\nFigure16.11 ThreeproposedBayesnetsfortheSurpriseCandyproblem,Exercise16.5.\na. Whichnetwork(s)cancorrectly represent P(Flavor,Wrapper,Shape)?\nb. Whichnetworkisthebestrepresentation forthisproblem? 642 Chapter 16. MakingSimpleDecisions\nc. Doesnetwork(i)assertthat P(Wrapper|Shape)=P(Wrapper)?\nd. Whatistheprobability thatyourcandyhasaredwrapper?\ne. Intheboxisaroundcandywitharedwrapper. Whatistheprobability thatitsflavoris\nstrawberry?\nf. A unwrapped strawberry candy is worth s on the open market and an unwrapped an-\nchovycandyiswortha. Writeanexpression forthevalueofanunopened candybox.\ng. Anewlawprohibits trading ofunwrapped candies, butitisstill legaltotradewrapped\ncandies (out ofthe box). Isanunopened candy box now worth more than less than, or\nthesameasbefore?\n16.6 Provethatthejudgments B \u2019 AandC \u2019 D intheAllaisparadox (page 620)violate\ntheaxiomofsubstitutability.\n16.7 Consider the Allais paradox described on page 620: an agent who prefers B over\nA (taking the sure thing), and C over D (taking the higher EMV) is not acting rationally,\naccordingtoutilitytheory. Doyouthinkthisindicatesaproblemfortheagent,aproblemfor\nthetheory, ornoproblematall? Explain.\n16.8 Ticketstoalotterycost$1. Therearetwopossibleprizes: a $10payoffwithprobabil-\nity 1\/50, and a$1,000,000 payoff withprobability 1\/2,000,000. Whatis theexpected mone-\ntaryvalueofalotteryticket? When(ifever)isitrationaltobuyaticket? Beprecise\u2014showan\nequation involving utilities. Youmayassumecurrent wealthof$k andthatU(S ) = 0. You\nk\nmay also assume that U(S ) = 10\u00d7U(S ), but you may not make any assumptions\nk+10 k+1\nabout U(S ). Sociological studies show that people with lower income buy a dis-\nk+1,000,000\nproportionate numberoflotterytickets. Doyouthinkthisisbecause theyareworsedecision\nmakersorbecausetheyhaveadifferentutilityfunction? Considerthevalueofcontemplating\nthe possibility of winning the lottery versus the value of contemplating becoming an action\nherowhilewatchinganadventuremovie.\n16.9 Assessyourownutilityfordifferentincrementalamountsofmoneybyrunningaseries\nofpreferencetestsbetweensomedefiniteamountM andalottery[p,M ;(1\u2212p),0]. Choose\n1 2\ndifferentvaluesofM andM ,andvarypuntilyouareindifferent betweenthetwochoices.\n1 2\nPlottheresulting utilityfunction.\n16.10 How much is a micromort worth to you? Devise a protocol to determine this. Ask\nquestions basedbothonpayingtoavoidriskandbeingpaidtoacceptrisk.\n16.11 Let continuous variables X ,...,X be independently distributed according to the\n1 k\nsameprobabilitydensityfunctionf(x). Provethatthedensityfunctionformax{X ,...,X }\n1 k\nisgivenbykf(x)(F(x))k\u22121,whereF\nisthecumulativedistribution forf.\n16.12 Economists often make use of an exponential utility function for money: U(x) =\n\u2212ex\/R,whereRisapositiveconstant representing anindividual\u2019s risktolerance. Risktoler-\nancereflectshowlikelyanindividualistoacceptalotterywithaparticularexpectedmonetary\nvalue (EMV) versus some certain payoff. As R (which is measured in the same units as x)\nbecomeslarger, theindividual becomeslessrisk-averse. Exercises 643\na. Assume Mary has an exponential utility function with R = $500. Mary is given the\nchoice between receiving $500 with certainty (probability 1) or participating in a lot-\ntery which has a 60% probability of winning $5000 and a40% probability of winning\nnothing. Assuming Marry acts rationally, which option would she choose? Show how\nyouderivedyouranswer.\nb. Considerthechoicebetweenreceiving$100withcertainty (probability1)orparticipat-\ninginalottery whichhasa50%probability ofwinning $500anda50%probability of\nwinningnothing. ApproximatethevalueofR(to3significant digits)inanexponential\nutilityfunctionthatwouldcauseanindividualtobeindifferenttothesetwoalternatives.\n(Youmightfindithelpful towriteashortprogram tohelpyousolvethisproblem.)\n16.13 Repeat Exercise 16.16, using the action-utility representation shown in Figure 16.7.\n16.14 For either of the airport-siting diagrams from Exercises 16.16 and 16.13, to which\nconditional probability table entry is theutility most sensitive, given the available evidence?\n16.15 Considerastudentwhohasthechoicetobuyornotbuyatextbookforacourse. We\u2019ll\nmodelthisasadecision problem withoneBooleandecision node, B,indicating whetherthe\nagent chooses to buy the book, and two Boolean chance nodes, M, indicating whether the\nstudent has mastered the material in the book, and P, indicating whether the student passes\nthecourse. Ofcourse, thereisalso autility node, U. Acertain student, Sam,hasanadditive\nutilityfunction: 0fornotbuyingthebookand-$100forbuyingit;and$2000forpassingthe\ncourseand0fornotpassing. Sam\u2019sconditional probability estimatesareasfollows:\nP(p|b,m) = 0.9 P(m|b) = 0.9\nP(p|b,\u00acm) = 0.5 P(m|\u00acb)= 0.7\nP(p|\u00acb,m) = 0.8\nP(p|\u00acb,\u00acm) = 0.3\nYou might think that P would be independent of B given M, But this course has an open-\nbookfinal\u2014sohavingthebookhelps.\na. Drawthedecisionnetworkforthisproblem.\nb. Computetheexpected utilityofbuying thebookandofnotbuyingit.\nc. WhatshouldSamdo?\n16.16 Thisexercisecompletestheanalysisoftheairport-siting problem inFigure16.6.\na. Providereasonablevariabledomains,probabilities,andutilitiesforthenetwork,assum-\ningthattherearethreepossible sites.\nb. Solvethedecision problem.\nc. Whathappensifchangesintechnologymeanthateachaircraftgenerateshalfthenoise?\nd. Whatifnoiseavoidance becomesthreetimesmoreimportant?\ne. CalculatetheVPIforAirTraffic,Litigation,andConstruction inyourmodel. 644 Chapter 16. MakingSimpleDecisions\n16.17 (Adapted from Pearl(1988).) Aused-car buyer can decide to carry out various tests\nwithvariouscosts(e.g.,kickthetires,takethecartoaqualifiedmechanic)andthen,depend-\ning on the outcome of the tests, decide which car to buy. We will assume that the buyer is\ndeciding whether to buy car c , that there is time tocarry out at most one test, and that t is\n1 1\nthetestofc andcosts$50.\n1\nAcarcan be in good shape (quality q+) orbad shape (quality q\u2212 ), and the tests might\nhelpindicatewhatshapethecarisin. Carc costs$1,500,anditsmarketvalueis$2,000ifit\n1\nisingoodshape;ifnot,$700inrepairswillbeneededtomakeitingoodshape. Thebuyer\u2019s\nestimateisthatc hasa70%chanceofbeingingoodshape.\n1\na. Drawthedecisionnetworkthatrepresents thisproblem.\nb. Calculatetheexpected netgainfrombuying c ,givennotest.\n1\nc. Testscanbedescribed bytheprobability thatthecarwillpassorfailthetestgiventhat\nthecarisingoodorbadshape. Wehavethefollowinginformation:\nP(pass(c ,t )|q+(c )) = 0.8\n1 1 1\nP(pass(c ,t\n)|q\u2212\n(c )) = 0.35\n1 1 1\nUseBayes\u2019theoremtocalculatetheprobabilitythatthecarwillpass(orfail)itstestand\nhencetheprobability thatitisingood(orbad)shapegiveneachpossibletestoutcome.\nd. Calculatetheoptimaldecisions giveneitherapassorafail,andtheirexpectedutilities.\ne. Calculate the value of information of the test, and derive an optimal conditional plan\nforthebuyer.\n16.18 Recallthedefinition ofvalueofinformation inSection16.6.\na. Provethatthevalueofinformation isnonnegativeandorderindependent.\nb. Explain why it is that some people would prefer not to get some information\u2014for ex-\nample,notwantingtoknowthesexoftheirbabywhenanultrasound isdone.\nc. A function f on sets is submodular if, for any element x and any sets A and B such\nSUBMODULARITY\nthatA\u2286 B,addingxtoAgivesagreaterincrease in f thanaddingxtoB:\nA\u2286 B \u21d2 (f(A\u222a{x})\u2212f(A)) \u2265 (f(B\u222a{x})\u2212f(B)).\nSubmodularity captures the intuitive notion of diminishing returns. Is the value of in-\nformation, viewedasafunction f onsetsofpossible observations, submodular? Prove\nthisorfindacounterexample.\nTheanswerstoExercise16.1(whereMstandsformillion): Firstset: 3M,1.6M,1541,41M,\n4768, 221, 649M, 295M, 132, 25,546. Second set: 1917, 155M, 4,500M, 11M, 120,000,\n1.1M,1636,19,340, 1,595,41,710. 17\nMAKING COMPLEX\nDECISIONS\nIn which we examine methods for deciding what to do today, given that we may\ndecideagaintomorrow.\nInthischapter,weaddressthecomputationalissuesinvolvedinmakingdecisionsinastochas-\ntic environment. Whereas Chapter 16 was concerned with one-shot or episodic decision\nproblems, in which the utility of each action\u2019s outcome was well known, we are concerned\nSEQUENTIAL here withsequential decision problems, inwhich theagent\u2019s utility depends onasequence\nDECISIONPROBLEM\nof decisions. Sequential decision problems incorporate utilities, uncertainty, and sensing,\nand include search and planning problems as special cases. Section 17.1 explains how se-\nquential decision problems are defined, and Sections 17.2 and 17.3 explain how they can\nbe solved to produce optimal behavior that balances the risks and rewards of acting in an\nuncertain environment. Section 17.4 extends these ideas to the case of partially observable\nenvironments, andSection17.4.3developsacompletedesignfordecision-theoretic agentsin\npartially observable environments, combining dynamic Bayesian networks from Chapter 15\nwithdecision networksfromChapter16.\nThe second part of the chapter covers environments with multiple agents. In such en-\nvironments, the notion of optimal behavior is complicated by the interactions among the\nagents. Section 17.5 introduces the main ideas of game theory, including the idea that ra-\ntional agents might need tobehave randomly. Section 17.6looks athow multiagent systems\ncanbedesigned sothatmultipleagentscanachieveacommongoal.\n17.1 SEQUENTIAL DECISION PROBLEMS\nSupposethatanagentissituatedinthe4\u00d73environmentshowninFigure17.1(a). Beginning\ninthestartstate,itmustchooseanactionateachtimestep. Theinteraction withtheenviron-\nment terminates when the agent reaches one of the goal states, marked +1 or\u20131. Just as for\nsearch problems, the actions available to the agent in each state are given by ACTIONS(s),\nsometimes abbreviated to A(s); in the 4\u00d73 environment, the actions in every state are Up,\nDown,Left,andRight. Weassume fornowthattheenvironment is fullyobservable, sothat\ntheagentalwaysknowswhereitis.\n645 646 Chapter 17. MakingComplexDecisions\n3 + 1 0.8\n0.1 0.1\n2 \u20131\n1 START\n1 2 3 4\n(a) (b)\nFigure 17.1 (a) A simple 4\u00d73 environment that presents the agent with a sequential\ndecisionproblem. (b)Illustrationofthetransitionmodeloftheenvironment:the\u201cintended\u201d\noutcomeoccurswithprobability0.8,butwithprobability0.2theagentmovesatrightangles\ntotheintendeddirection. Acollisionwithawallresultsinnomovement. Thetwoterminal\nstateshavereward+1and\u20131,respectively,andallotherstateshavearewardof\u20130.04.\nIftheenvironment weredeterministic, asolutionwouldbeeasy: [Up,Up,Right,Right,\nRight]. Unfortunately, theenvironmentwon\u2019talwaysgoalongwiththissolution,becausethe\nactions are unreliable. The particular model of stochastic motion that weadopt isillustrated\nin Figure 17.1(b). Each action achieves the intended effect with probability 0.8, but the rest\nofthetime,theactionmovestheagentatrightanglestotheintendeddirection. Furthermore,\niftheagentbumpsintoawall,itstaysinthesamesquare. Forexample,fromthestartsquare\n(1,1), theaction Upmovestheagentto(1,2)withprobability 0.8,butwithprobability 0.1,it\nmovesrightto(2,1),andwithprobability 0.1,itmovesleft,bumpsintothewall,andstaysin\n(1,1). In such an environment, the sequence [Up,Up,Right,Right,Right] goes up around\nthebarrierandreachesthegoalstateat(4,3)withprobability 0.85=0.32768. Thereisalsoa\nsmallchanceofaccidentallyreachingthegoalbygoingtheotherwayaroundwithprobability\n0.14\u00d70.8,foragrandtotalof0.32776. (SeealsoExercise17.1.)\nAs in Chapter 3, the transition model (or just \u201cmodel,\u201d whenever no confusion can\narise) describes the outcome of each action in each state. Here, the outcome is stochastic,\nso we write\nP(s(cid:2)|s,a)\nto denote the probability of reaching state\ns(cid:2)\nif action a is done in\nstates. WewillassumethattransitionsareMarkovianinthesenseofChapter15,thatis,the\n(cid:2)\nprobabilityofreachings fromsdependsonlyonsandnotonthehistoryofearlierstates. For\nnow, you can think of\nP(s(cid:2)|s,a)\nas a big three-dimensional table containing probabilities.\nLater,inSection17.4.3,wewillseethatthetransitionmodelcanberepresentedasadynamic\nBayesiannetwork,justasinChapter15.\nTocompletethedefinitionofthetaskenvironment, wemustspecifytheutilityfunction\nfor the agent. Because the decision problem is sequential, the utility function will depend\non a sequence of states\u2014an environment history\u2014rather than on a single state. Later in\nthis section, we investigate how such utility functions can be specified in general; for now,\nwe simply stipulate that in each state s, the agent receives a reward R(s), which may be\nREWARD\npositive or negative, but must be bounded. For our particular example, the reward is \u22120.04\nin all states except the terminal states (which have rewards +1 and \u20131). The utility of an Section17.1. Sequential DecisionProblems 647\nenvironment history is just (for now) the sum of the rewards received. For example, if the\nagent reaches the +1 state after 10 steps, its total utility will be 0.6. The negative reward of\n\u20130.04 gives the agent an incentive to reach (4,3) quickly, so our environment is a stochastic\ngeneralization of the search problems of Chapter 3. Another way of saying this is that the\nagentdoesnotenjoylivinginthisenvironment andsowantstoleaveassoonaspossible.\nTosumup: asequentialdecisionproblemforafullyobservable,stochasticenvironment\nMARKOVDECISION withaMarkoviantransitionmodelandadditiverewardsiscalledaMarkovdecisionprocess,\nPROCESS\norMDP,andconsistsofasetofstates(withaninitialstates 0);aset ACTIONS(s)ofactions\nineachstate;atransition modelP(s(cid:2)|s,a);andarewardfunction R(s).1\nThenextquestion is,whatdoesasolution totheproblem look like? Wehaveseen that\nanyfixedactionsequence won\u2019tsolvetheproblem,becausetheagentmightendupinastate\notherthanthegoal. Therefore,asolutionmustspecifywhattheagentshoulddoforanystate\nthattheagentmightreach. Asolutionofthiskindiscalledapolicy. Itistraditionaltodenote\nPOLICY\na policy by \u03c0, and \u03c0(s) is the action recommended by the policy \u03c0 for state s. If the agent\nhas acomplete policy, then no matterwhat theoutcome of anyaction, theagent willalways\nknowwhattodonext.\nEachtimeagivenpolicyisexecutedstartingfromtheinitialstate,thestochastic nature\nof the environment may lead to a different environment history. The quality of a policy is\ntherefore measured by the expected utility of the possible environment histories generated\nby that policy. An optimal policy is a policy that yields the highest expected utility. We\nOPTIMALPOLICY\n\u2217 \u2217\nuse \u03c0 to denote an optimal policy. Given \u03c0 , the agent decides what to do by consulting\n\u2217\nits current percept, which tells it the current state s, and then executing the action \u03c0 (s). A\npolicyrepresentstheagentfunctionexplicitlyandisthereforeadescriptionofasimplereflex\nagent,computed fromtheinformation usedforautility-based agent.\nAn optimal policy for the world of Figure 17.1 is shown in Figure 17.2(a). Notice\nthat, because the cost of taking a step is fairly small compared with the penalty for ending\nup in (4,2) by accident, the optimal policy for the state (3,1) is conservative. The policy\nrecommends taking the long way round, rather than taking the shortcut and thereby risking\nentering (4,2).\nThebalanceofriskandrewardchanges depending onthevalue ofR(s)forthenonter-\nminal states. Figure 17.2(b) shows optimal policies forfourdifferent ranges of R(s). When\nR(s) \u2264 \u22121.6284, life is so painful that the agent heads straight for the nearest exit, even if\nthe exit is worth \u20131. When \u22120.4278 \u2264 R(s) \u2264 \u22120.0850, life is quite unpleasant; the agent\ntakes the shortest route to the +1 state and is willing to risk falling into the \u20131 state by acci-\ndent. In particular, the agent takes the shortcut from (3,1). When life is only slightly dreary\n(\u22120.0221 < R(s) < 0), theoptimal policy takes norisks atall. In(4,1)and(3,2), theagent\nheads directly away from the \u20131 state so that it cannot fall in by accident, even though this\nmeans banging its head against the wall quite a few times. Finally, if R(s) > 0, then life is\npositively enjoyable and the agent avoids both exits. As long as the actions in (4,1), (3,2),\n1 SomedefinitionsofMDPsallowtherewardtodependontheactionandoutcometoo,sotherewardfunction\nis R(s,a,s(cid:3)). This simplifies the description of some environments but does not change the problem in any\nfundamentalway,asshowninExercise17.4. 648 Chapter 17. MakingComplexDecisions\n+1 +1\n\u20131 \u20131\n3 + 1\nR(s) < \u20131.6284 \u2013 0.4278 < R(s) < \u2013 0.0850\n2 \u20131\n+1 +1\n1\n\u20131 \u20131\n1 2 3 4\n\u2013 0.0221 < R(s) < 0 R(s) > 0\n(a) (b)\nFigure17.2 (a)AnoptimalpolicyforthestochasticenvironmentwithR(s)= \u22120.04in\nthenonterminalstates. (b)OptimalpoliciesforfourdifferentrangesofR(s).\nand(3,3)areasshown,everypolicyisoptimal,andtheagent obtainsinfinitetotalrewardbe-\ncauseitneverentersaterminalstate. Surprisingly, itturnsoutthattherearesixotheroptimal\npolicies forvariousrangesof R(s);Exercise17.5asksyoutofindthem.\nThe careful balancing of risk and reward is a characteristic of MDPs that does not\narise in deterministic search problems; moreover, it is a characteristic of many real-world\ndecision problems. For this reason, MDPs have been studied in several fields, including\nAI, operations research, economics, and control theory. Dozens of algorithms have been\nproposed for calculating optimal policies. In sections 17.2 and 17.3 we describe two of the\nmost important algorithm families. First, however, we must complete our investigation of\nutilities andpoliciesforsequential decision problems.\n17.1.1 Utilitiesovertime\nIntheMDPexampleinFigure17.1,theperformance oftheagentwasmeasuredbyasumof\nrewards for the states visited. This choice of performance measure is not arbitrary, but it is\nnot the only possibility for the utility function on environment histories, which we write as\nU ([s ,s ,...,s ]). Ouranalysisdrawsonmultiattributeutilitytheory(Section16.4)and\nh 0 1 n\nissomewhattechnical; theimpatient readermaywishtoskip tothenextsection.\nThefirstquestion toanswer is whetherthere is a finitehorizon oran infinitehorizon\nFINITEHORIZON\nfordecision making. Afinitehorizon means thatthere isa fixed timeN afterwhichnothing\nINFINITEHORIZON\nmatters\u2014the gameisover, sotospeak. Thus, U ([s ,s ,...,s ])=U ([s ,s ,...,s ])\nh 0 1 N+k h 0 1 N\nforallk > 0. Forexample,supposeanagentstartsat(3,1)inthe 4\u00d73worldofFigure17.1,\nand suppose that N =3. Then, to have any chance of reaching the +1 state, the agent must\nhead directly for it, and the optimal action is to go Up. On the other hand, if N=100,\nthen there is plenty of time to take the safe route by going Left. So, with a finite horizon, Section17.1. Sequential DecisionProblems 649\nthe optimal action in a given state could change over time. We say that the optimal policy\nNONSTATIONARY for a finite horizon is nonstationary. With no fixed time limit, on the other hand, there is\nPOLICY\nno reason to behave differently in the same state at different times. Hence, the optimal ac-\ntion depends only on the current state, and the optimal policy is stationary. Policies forthe\nSTATIONARYPOLICY\ninfinite-horizon casearetherefore simplerthanthoseforthefinite-horizon case,andwedeal\nmainly with the infinite-horizon case in this chapter. (We will see later that for partially ob-\nservableenvironments,theinfinite-horizoncaseisnotsosimple.) Notethat\u201cinfinitehorizon\u201d\ndoes not necessarily mean that all state sequences are infinite; it just means that there is no\nfixed deadline. In particular, there can be finite state sequences in an infinite-horizon MDP\ncontaining aterminalstate.\nThenext question wemust decide is how to calculate the utility of state sequences. In\ntheterminologyofmultiattribute utilitytheory,eachstates canbeviewedasanattributeof\ni\nthestatesequence [s ,s ,s ...]. Toobtainasimpleexpression intermsoftheattributes, we\n0 1 2\nwill need to make some sort of preference-independence assumption. The most natural as-\nSTATIONARY sumptionisthattheagent\u2019s preferences betweenstatesequences arestationary. Stationarity\nPREFERENCE\n(cid:2) (cid:2) (cid:2)\nforpreferencesmeansthefollowing: iftwostatesequences [s ,s ,s ,...]and[s ,s ,s ,...]\n0 1 2 0 1 2\n(cid:2)\nbeginwiththesamestate(i.e.,s =s ),thenthetwosequencesshouldbepreference-ordered\n0 0\n(cid:2) (cid:2)\nthesamewayasthesequences[s ,s ,...]and[s ,s ,...]. InEnglish,thismeansthatifyou\n1 2 1 2\nprefer one future to another starting tomorrow, then you should still prefer that future if it\nwere to start today instead. Stationarity is a fairly innocuous-looking assumption with very\nstrong consequences: it turns out that under stationarity there are just two coherent ways to\nassignutilities tosequences:\n1. Additiverewards: Theutilityofastatesequence is\nADDITIVEREWARD\nU ([s ,s ,s ,...]) = R(s )+R(s )+R(s )+\u00b7\u00b7\u00b7 .\nh 0 1 2 0 1 2\nThe 4\u00d73 world in Figure 17.1 uses additive rewards. Notice that additivity was used\nimplicitlyinouruseofpathcostfunctions inheuristicsearchalgorithms (Chapter3).\nDISCOUNTED 2. Discountedrewards: Theutilityofastatesequenceis\nREWARD\nU ([s ,s ,s ,...]) = R(s )+\u03b3R(s )+\u03b32R(s )+\u00b7\u00b7\u00b7 ,\nh 0 1 2 0 1 2\nwherethediscountfactor\u03b3isanumberbetween0and1. Thediscountfactordescribes\nDISCOUNTFACTOR\nthe preference of an agent for current rewards over future rewards. When \u03b3 is close\nto0, rewards inthe distant future are viewed as insignificant. When \u03b3 is 1, discounted\nrewards are exactly equivalent to additive rewards, so additive rewards are a special\ncase of discounted rewards. Discounting appears to be a good model of both animal\nandhumanpreferencesovertime. Adiscountfactorof\u03b3isequivalenttoaninterestrate\nof(1\/\u03b3)\u22121.\nFor reasons that will shortly become clear, we assume discounted rewards in the remainder\nofthechapter, although sometimesweallow\u03b3=1.\nLurking beneath our choice of infinite horizons is a problem: if the environment does\nnot contain aterminal state, orifthe agent neverreaches one, then all environment histories\nwill be infinitely long, and utilities with additive, undiscounted rewards will generally be 650 Chapter 17. MakingComplexDecisions\ninfinite. Whilewecanagreethat+\u221eisbetterthan\u2212\u221e,comparingtwostatesequenceswith\n+\u221eutilityismoredifficult. Therearethreesolutions, twoofwhichwehaveseenalready:\n1. With discounted rewards, the utility of an infinite sequence is finite. In fact, if \u03b3 < 1\nandrewardsareboundedby \u00b1R ,wehave\nmax\n(cid:12)\u221e (cid:12)\u221e\nU ([s ,s ,s ,...]) = \u03b3tR(s ) \u2264 \u03b3tR = R \/(1\u2212\u03b3), (17.1)\nh 0 1 2 t max max\nt=0 t=0\nusingthestandard formulaforthesumofaninfinitegeometricseries.\n2. Iftheenvironment contains terminal states and ifthe agent isguaranteed to getto one\neventually, then we will never need to compare infinite sequences. A policy that is\nguaranteed toreachaterminalstateiscalledaproperpolicy. Withproperpolicies, we\nPROPERPOLICY\ncan use \u03b3=1 (i.e., additive rewards). The first three policies shown in Figure 17.2(b)\nareproper,butthefourthisimproper. Itgainsinfinitetotalrewardbystayingawayfrom\ntheterminalstateswhentherewardforthenonterminalstatesispositive. Theexistence\nof improper policies can cause the standard algorithms for solving MDPs to fail with\nadditiverewards,andsoprovides agoodreasonforusingdiscounted rewards.\n3. Infinite sequences can be compared in terms of the average reward obtained pertime\nAVERAGEREWARD\nstep. Suppose that square (1,1) in the 4\u00d73 world has a reward of 0.1 while the other\nnonterminal states have a reward of 0.01. Then a policy that does its best to stay in\n(1,1)willhavehigheraverage rewardthanonethatstayselsewhere. Average rewardis\na useful criterion for some problems, but the analysis of average-reward algorithms is\nbeyondthescopeofthisbook.\nInsum,discounted rewardspresent thefewestdifficulties inevaluating statesequences.\n17.1.2 Optimalpoliciesand theutilities ofstates\nHaving decided that the utility of a given state sequence is the sum of discounted rewards\nobtained during the sequence, we can compare policies by comparing the expected utilities\nobtained when executing them. We assume the agent is in some initial state s and define S\nt\n(a random variable) to be the state the agent reaches at time t when executing a particular\npolicy\u03c0. (Obviously, S =s,thestatetheagent isinnow.) Theprobability distribution over\n0\nstatesequencesS ,S ,...,isdeterminedbytheinitialstates,thepolicy\u03c0,andthetransition\n1 2\nmodelfortheenvironment.\nTheexpectedutilityobtained byexecuting\u03c0 startinginsisgivenby\n\" #\n(cid:12)\u221e\nU\u03c0(s) = E \u03b3tR(S ) , (17.2)\nt\nt=0\nwhere the expectation is with respect tothe probability distribution overstate sequences de-\nterminedbysand\u03c0. Now,outofallthepoliciestheagentcouldchoosetoexecutestartingin\n\u2217\ns,one(ormore)willhavehigherexpectedutilitiesthanalltheothers. We\u2019lluse\u03c0 todenote\ns\noneofthesepolicies:\n\u03c0\u2217 = argmaxU\u03c0(s). (17.3)\ns\n\u03c0 Section17.1. Sequential DecisionProblems 651\n\u2217\nRemember that \u03c0 is a policy, so it recommends an action for every state; its connection\ns\nwith s in particular is that it\u2019s an optimal policy when s is the starting state. A remarkable\nconsequence of using discounted utilities with infinite horizons is that the optimal policy is\nindependent of the starting state. (Of course, the action sequence won\u2019t be independent;\nremember that a policy is a function specifying an action for each state.) This fact seems\nintuitivelyobvious: ifpolicy\u03c0\u2217 isoptimalstartinginaandpolicy\u03c0\u2217 isoptimalstartinginb,\na b\nthen, when they reach a third state c, there\u2019s no good reason for them to disagree with each\nother, orwith\u03c0\u2217 ,aboutwhattodonext.2 Sowecansimplywrite\u03c0\u2217 foranoptimalpolicy.\nc\nGiven this definition, the true utility of a state is just\nU\u03c0\u2217\n(s)\u2014that is, the expected\nsum of discounted rewards if the agent executes an optimal policy. We write this as U(s),\nmatchingthenotation usedinChapter16fortheutility ofan outcome. NoticethatU(s)and\nR(s) are quite different quantities; R(s) is the \u201cshort term\u201d reward for being in s, whereas\nU(s) is the \u201clong term\u201d total reward from s onward. Figure 17.3 shows the utilities for the\n4\u00d73world. Noticethattheutilities arehigherforstatesclosertothe+1exit,because fewer\nstepsarerequired toreachtheexit.\n3 0.812 0.868 0.918 + 1\n2 0.762 0.660 \u20131\n1 0.705 0.655 0.611 0.388\n1 2 3 4\nFigure 17.3 The utilities of the states in the 4\u00d73 world, calculated with \u03b3=1 and\nR(s)= \u22120.04fornonterminalstates.\nThe utility function U(s) allows the agent to select actions by using the principle of\nmaximum expected utility from Chapter 16\u2014that is, choose the action that maximizes the\nexpectedutilityofthesubsequent state:\n(cid:12)\n\u03c0\u2217\n(s)= argmax\nP(s(cid:2)|s,a)U(s(cid:2)\n). (17.4)\na\u2208A(s)\ns(cid:3)\nThenexttwosections describealgorithms forfindingoptimalpolicies.\n2 Although this seems obvious, it does not hold for finite-horizon policies or for other ways of combining\nrewardsovertime. Theprooffollowsdirectlyfromtheuniquenessoftheutilityfunctiononstates,asshownin\nSection17.2. 652 Chapter 17. MakingComplexDecisions\n17.2 VALUE ITERATION\nIn this section, we present an algorithm, called value iteration, for calculating an optimal\nVALUEITERATION\npolicy. Thebasicideaistocalculate theutilityofeachstateandthenusethestateutilities to\nselectanoptimalactionineachstate.\n17.2.1 The Bellmanequation forutilities\nSection17.1.2definedtheutilityofbeinginastateastheexpectedsumofdiscountedrewards\nfrom that point onwards. From this, it follows that there is a direct relationship between the\nutility ofastate and theutility ofitsneighbors: the utility ofastate istheimmediate reward\nfor that state plus the expected discounted utility of the next state, assuming that the agent\nchoosestheoptimalaction. Thatis,theutilityofastateisgivenby\n(cid:12)\nU(s) = R(s)+\u03b3 max\nP(s(cid:2)|s,a)U(s(cid:2)\n). (17.5)\na\u2208A(s)\ns(cid:3)\nThis is called the Bellman equation, after Richard Bellman (1957). The utilities of the\nBELLMANEQUATION\nstates\u2014definedbyEquation(17.2)astheexpectedutilityofsubsequentstatesequences\u2014are\nsolutions ofthe set of Bellman equations. In fact, they are the unique solutions, as weshow\ninSection17.2.3.\nLet us look at one of the Bellman equations for the 4\u00d73 world. The equation for the\nstate(1,1)is\nU(1,1) = \u22120.04+\u03b3 max[ 0.8U(1,2)+0.1U(2,1)+0.1U(1,1), (Up)\n0.9U(1,1)+0.1U(1,2), (Left)\n0.9U(1,1)+0.1U(2,1), (Down)\n0.8U(2,1)+0.1U(1,2)+0.1U(1,1)]. (Right)\nWhenwepluginthenumbersfromFigure17.3,wefindthat Upisthebestaction.\n17.2.2 The valueiterationalgorithm\nTheBellmanequation isthebasisofthevalueiterationalgorithm forsolvingMDPs. Ifthere\narenpossiblestates,thentherearenBellmanequations, oneforeachstate. Thenequations\ncontainnunknowns\u2014theutilitiesofthestates. Sowewouldliketosolvethesesimultaneous\nequations tofindtheutilities. Thereisoneproblem: theequationsarenonlinear, becausethe\n\u201cmax\u201d operator is not a linear operator. Whereas systems of linear equations can be solved\nquicklyusinglinearalgebratechniques,systemsofnonlinearequationsaremoreproblematic.\nOnethingtotryisaniterativeapproach. Westartwitharbitraryinitialvaluesfortheutilities,\ncalculate the right-hand side of the equation, and plug it into the left-hand side\u2014thereby\nupdating the utility of each state from the utilities of its neighbors. We repeat this until we\nreachanequilibrium. LetU (s)betheutilityvalueforstatesattheithiteration. Theiteration\ni\nstep,calledaBellmanupdate,lookslikethis:\nBELLMANUPDATE\n(cid:12)\nU (s)\u2190 R(s)+\u03b3 max P(s(cid:2)|s,a)U (s(cid:2) ), (17.6)\ni+1 i\na\u2208A(s)\ns(cid:3) Section17.2. ValueIteration 653\nfunctionVALUE-ITERATION(mdp,(cid:2))returnsautilityfunction\ninputs:mdp,anMDPwithstatesS,actionsA(s),transitionmodelP(s(cid:5)|s,a),\nrewardsR(s),discount\u03b3\n(cid:2),themaximumerrorallowedintheutilityofanystate\nlocalvariables: U,U(cid:5),vectorsofutilitiesforstatesinS,initiallyzero\n\u03b4,themaximumchangeintheutilityofanystateinaniteration\nrepeat\nU \u2190U(cid:5);\u03b4\u21900\nforeachstates inS do (cid:12)\nU(cid:5)[s]\u2190R(s) + \u03b3 max P(s(cid:5)|s,a)U[s(cid:5)]\na\u2208A(s)\ns(cid:3)\nif|U(cid:5)[s] \u2212 U[s]| > \u03b4then\u03b4\u2190|U(cid:5)[s] \u2212 U[s]|\nuntil\u03b4 < (cid:2)(1\u2212\u03b3)\/\u03b3\nreturnU\nFigure17.4 Thevalueiterationalgorithmforcalculatingutilitiesof states. Thetermina-\ntionconditionisfromEquation(17.8).\n1e+07\n1 (4,3) c = 0.0001\n(3,3) 1e+06 c = 0.001\n0.8 c = 0.01\n(1,1) 100000 c = 0.1\n0.6 (3,1)\n10000\n0.4 (4,1)\n1000\n0.2\n100\n0\n10\n-0.2\n1\n0 5 10 15 20 25 30 0.50.550.60.650.70.750.80.850.90.95 1\nNumber of iterations Discount factor \u03b3\n(a) (b)\nFigure17.5 (a)Graphshowingtheevolutionoftheutilitiesofselectedstatesusingvalue\niteration. (b) The number of value iterations k required to guarantee an error of at most\n(cid:2)=c\u00b7R ,fordifferentvaluesofc,asafunctionofthediscountfactor\u03b3.\nmax\nwhere the update is assumed to be applied simultaneously to all the states at each iteration.\nIf we apply the Bellman update infinitely often, we are guaranteed to reach an equilibrium\n(see Section 17.2.3), in which case the final utility values must be solutions to the Bellman\nequations. Infact, theyarealsotheuniquesolutions, andthecorresponding policy(obtained\nusing Equation (17.4)) is optimal. The algorithm, called VALUE-ITERATION, is shown in\nFigure17.4.\nWecan apply value iteration to the 4\u00d73 world in Figure 17.1(a). Starting with initial\nvaluesofzero,theutilitiesevolveasshowninFigure17.5(a). Noticehowthestatesatdiffer-\nsetamitse\nytilitU\nderiuqer\nsnoitaretI 654 Chapter 17. MakingComplexDecisions\nentdistancesfrom(4,3)accumulatenegativerewarduntilapathisfoundto(4,3),whereupon\nthe utilities start to increase. We can think of the value iteration algorithm as propagating\ninformation through thestatespacebymeansoflocalupdates.\n17.2.3 Convergence ofvalueiteration\nWesaid that value iteration eventually converges toaunique set ofsolutions ofthe Bellman\nequations. In this section, we explain why this happens. We introduce some useful mathe-\nmaticalideasalongtheway,andweobtainsomemethodsforassessingtheerrorintheutility\nfunction returned whenthealgorithm isterminatedearly;thisisusefulbecause itmeansthat\nwedon\u2019thavetorunforever. Thissection isquitetechnical.\nThebasicconceptusedinshowingthatvalueiterationconvergesisthenotionofacon-\ntraction. Roughlyspeaking,acontractionisafunctionofoneargumentthat,whenappliedto\nCONTRACTION\ntwodifferent inputs inturn, produces twooutputvalues thatare\u201cclosertogether,\u201d byatleast\nsome constant factor, than the original inputs. Forexample, the function \u201cdivide by two\u201d is\na contraction, because, after we divide any two numbers by two, their difference is halved.\nNoticethatthe\u201cdividebytwo\u201dfunction hasafixedpoint, namelyzero,thatisunchanged by\nthe application ofthe function. Fromthis example, wecan discern twoimportant properties\nofcontractions:\n\u2022 A contraction has only one fixed point; if there were two fixed points they would not\ngetclosertogetherwhenthefunction wasapplied, soitwouldnotbeacontraction.\n\u2022 When the function is applied to any argument, the value must get closer to the fixed\npoint (because the fixedpoint does not move), so repeated application of acontraction\nalwaysreachesthefixedpointinthelimit.\nNow,supposeweviewtheBellmanupdate(Equation(17.6))asanoperator B thatisapplied\nsimultaneously toupdate theutilityofeverystate. LetU denotethevectorofutilities forall\ni\nthestatesattheithiteration. ThentheBellmanupdateequationcanbewrittenas\nU \u2190 BU .\ni+1 i\nNext,weneedawaytomeasuredistancesbetweenutilityvectors. Wewillusethemaxnorm,\nMAXNORM\nwhichmeasuresthe\u201clength\u201dofavectorbytheabsolute value ofitsbiggest component:\n||U|| =max|U(s)|.\ns\nWith this definition, the \u201cdistance\u201d between two vectors, ||U \u2212 U(cid:2)||, is the maximum dif-\nference between any two corresponding elements. The main result of this section is the\n(cid:2)\nfollowing: LetU andU beanytwoutilityvectors. Thenwehave\ni i\n||BU \u2212BU(cid:2)|| \u2264\u03b3||U \u2212U(cid:2)||. (17.7)\ni i i i\nThat is, the Bellman update is a contraction by a factor of \u03b3 on the space of utility vectors.\n(Exercise17.6providessomeguidanceonprovingthisclaim.) Hence,fromthepropertiesof\ncontractions in general, it follows that value iteration always converges to a unique solution\noftheBellmanequations whenever\u03b3 < 1. Section17.2. ValueIteration 655\nWe can also use the contraction property to analyze the rate of convergence to a solu-\n(cid:2)\ntion. In particular, we can replace U in Equation (17.7) with the true utilities U, for which\ni\nBU=U. Thenweobtaintheinequality\n||BU \u2212U|| \u2264 \u03b3||U \u2212U||.\ni i\nSo,ifweview||U \u2212U||astheerrorintheestimateU ,weseethattheerrorisreducedbya\ni i\nfactorofatleast \u03b3 oneachiteration. Thismeansthatvalueiteration converges exponentially\nfast. We can calculate the number of iterations required to reach a specified error bound (cid:2)\nas follows: First, recall from Equation (17.1) that the utilities of all states are bounded by\n\u00b1R \/(1\u2212\u03b3). This means that the maximum initial error ||U \u2212U|| \u2264 2R \/(1\u2212\u03b3).\nmax 0 max\nSuppose we run for N iterations to reach an error of at most (cid:2). Then, because the error is\nreducedbyatleast \u03b3 eachtime,werequire \u03b3N \u00b72R \/(1\u2212\u03b3) \u2264 (cid:2). Takinglogs,wefind\nmax\nN=(log(2R \/(cid:2)(1\u2212\u03b3))\/log(1\/\u03b3))\nmax\niterations suffice. Figure17.5(b)showshow N varieswith\u03b3,fordifferent valuesoftheratio\n(cid:2)\/R . The good news is that, because of the exponentially fast convergence, N does not\nmax\ndependmuchontheratio(cid:2)\/R . ThebadnewsisthatN growsrapidlyas\u03b3 becomesclose\nmax\nto 1. We can get fast convergence if we make \u03b3 small, but this effectively gives the agent a\nshorthorizonandcouldmissthelong-term effectsoftheagent\u2019s actions.\nThe error bound in the preceding paragraph gives some idea of the factors influencing\nthe run time of the algorithm, but is sometimes overly conservative as a method of deciding\nwhen to stop the iteration. For the latter purpose, we can use a bound relating the error\nto the size of the Bellman update on any given iteration. From the contraction property\n(Equation(17.7)),itcanbeshownthatiftheupdateissmall(i.e.,nostate\u2019sutilitychangesby\nmuch),thentheerror,comparedwiththetrueutilityfunction, alsoissmall. Moreprecisely,\nif ||U \u2212U || < (cid:2)(1\u2212\u03b3)\/\u03b3 then ||U \u2212U|| < (cid:2). (17.8)\ni+1 i i+1\nThisistheterminationcondition usedinthe VALUE-ITERATION algorithm ofFigure17.4.\nSofar, wehaveanalyzed theerrorintheutility function returned bythevalueiteration\nalgorithm. What the agent really cares about, however, is how well it will do if it makes its\ndecisionsonthebasisofthisutilityfunction. Supposethatafteriiterationsofvalueiteration,\nthe agent has an estimate U of the true utility U and obtains the MEU policy \u03c0 based on\ni i\none-step look-ahead using U (as in Equation (17.4)). Will the resulting behavior be nearly\ni\nasgoodastheoptimalbehavior? Thisisacrucialquestionforanyrealagent,anditturnsout\nthat the answer is yes. U\u03c0i(s) is the utility obtained if \u03c0 is executed starting in s, and the\ni\npolicy loss ||U\u03c0i \u2212U||isthe mosttheagent canlose byexecuting \u03c0 instead oftheoptimal\nPOLICYLOSS i\n\u2217\npolicy\u03c0 . Thepolicylossof\u03c0 isconnected totheerrorin U bythefollowinginequality:\ni i\nif ||U \u2212U||< (cid:2) then ||U\u03c0i \u2212U|| < 2(cid:2)\u03b3\/(1\u2212\u03b3). (17.9)\ni\nInpractice,itoftenoccursthat\u03c0 becomesoptimallongbeforeU hasconverged. Figure17.6\ni i\nshowshowthe maximumerrorin U and thepolicy lossapproach zeroasthevalue iteration\ni\nprocessproceedsforthe4\u00d73environmentwith\u03b3=0.9. Thepolicy\u03c0 isoptimalwheni=4,\ni\neventhoughthemaximumerrorin U isstill0.46.\ni\nNow we have everything we need to use value iteration in practice. We know that\nit converges to the correct utilities, we can bound the error in the utility estimates if we 656 Chapter 17. MakingComplexDecisions\nstop after a finite number of iterations, and we can bound the policy loss that results from\nexecuting the corresponding MEU policy. As a final note, all of the results in this section\ndepend on discounting with \u03b3 < 1. If \u03b3=1 and the environment contains terminal states,\nthen a similar set of convergence results and error bounds can be derived whenever certain\ntechnical conditions aresatisfied.\n17.3 POLICY ITERATION\nIn the previous section, we observed that it is possible to get an optimal policy even when\nthe utility function estimate is inaccurate. If one action is clearly better than all others, then\nthe exact magnitude of the utilities on the states involved need not be precise. This insight\nsuggestsanalternativewaytofindoptimalpolicies. Thepolicyiterationalgorithmalternates\nPOLICYITERATION\nthefollowingtwosteps,beginning fromsomeinitialpolicy \u03c0 :\n0\n\u2022 Policy evaluation: given a policy \u03c0 , calculate U =U\u03c0i, the utility of each state if \u03c0\nPOLICYEVALUATION i i i\nweretobeexecuted.\nPOLICY \u2022 Policy improvement: Calculate a new MEU policy \u03c0 , using one-step look-ahead\nIMPROVEMENT i+1\nbasedonU (asinEquation(17.4)).\ni\nThealgorithm terminateswhenthepolicyimprovementstepyieldsnochangeintheutilities.\nAtthis point, we know that the utility function U is a fixed point of the Bellman update, so\ni\nitisasolution totheBellmanequations, and\u03c0 mustbeanoptimalpolicy. Becausethereare\ni\nonlyfinitelymanypolicies forafinitestatespace, andeachiteration canbeshowntoyield a\nbetterpolicy,policyiteration mustterminate. Thealgorithm isshowninFigure17.7.\nThe policy improvement step is obviously straightforward, but how do we implement\nthe POLICY-EVALUATION routine? It turns out that doing so is much simpler than solving\nthe standard Bellman equations (which is what value iteration does), because the action in\neachstateisfixedbythepolicy. Attheithiteration, thepolicy\u03c0 specifiestheaction\u03c0 (s)in\ni i\n1\nMax error\nPolicy loss\n0.8\n0.6\n0.4\n0.2\n0\n0 2 4 6 8 10 12 14\nNumber of iterations\nFigure 17.6 The maximum error ||Ui \u2212U|| of the utility estimates and the policy loss\n||U\u03c0i \u2212U||,asafunctionofthenumberofiterationsofvalueiteration.\nssol\nyciloP\/rorre\nxaM Section17.3. PolicyIteration 657\nstates. Thismeansthatwehaveasimplifiedversion oftheBellmanequation (17.5)relating\ntheutilityofs(under\u03c0 )totheutilitiesofitsneighbors:\ni\n(cid:12)\nU (s)= R(s)+\u03b3\nP(s(cid:2)|s,\u03c0\n(s))U\n(s(cid:2)\n). (17.10)\ni i i\ns(cid:3)\nForexample,suppose \u03c0 isthepolicyshowninFigure17.2(a). Thenwehave\u03c0 (1,1)=Up,\ni i\n\u03c0 (1,2)=Up,andsoon,andthesimplifiedBellmanequations are\ni\nU (1,1) = \u22120.04+0.8U (1,2)+0.1U (1,1)+0.1U (2,1),\ni i i i\nU (1,2) = \u22120.04+0.8U (1,3)+0.2U (1,2),\ni i i\n.\n.\n.\nThe important point is that these equations are linear, because the \u201cmax\u201doperator has been\nremoved. For n states, we have n linear equations with n unknowns, which can be solved\nexactlyintimeO(n3)bystandardlinearalgebramethods.\nForsmallstatespaces,policyevaluationusingexactsolutionmethodsisoftenthemost\nefficient approach. For large state spaces, O(n3) time might be prohibitive. Fortunately, it\nis not necessary to do exact policy evaluation. Instead, we can perform some number of\nsimplified value iteration steps (simplified because the policy is fixed) to give a reasonably\ngoodapproximation oftheutilities. ThesimplifiedBellmanupdateforthisprocessis\n(cid:12)\nU (s)\u2190 R(s)+\u03b3 P(s(cid:2)|s,\u03c0 (s))U (s(cid:2) ),\ni+1 i i\ns(cid:3)\nand this is repeated k times to produce the next utility estimate. The resulting algorithm is\nMODIFIEDPOLICY calledmodifiedpolicyiteration. Itisoftenmuchmoreefficientthanstandardpolicyiteration\nITERATION\norvalueiteration.\nfunctionPOLICY-ITERATION(mdp)returnsapolicy\ninputs:mdp,anMDPwithstatesS,actionsA(s),transitionmodelP(s(cid:5)|s,a)\nlocalvariables: U,avectorofutilitiesforstatesinS,initiallyzero\n\u03c0,apolicyvectorindexedbystate,initiallyrandom\nrepeat\nU \u2190POLICY-EVALUATION(\u03c0,U,mdp)\nunchanged?\u2190true\nforeachstates(cid:12)inS do (cid:12)\nif max\nP(s(cid:5)|s,a)U[s(cid:5)\n] >\nP(s(cid:5)|s,\u03c0[s])U[s(cid:5)\n]thendo\na\u2208A(s)\ns(cid:3) (cid:12) s(cid:3)\n\u03c0[s]\u2190argmax P(s(cid:5)|s,a)U[s(cid:5) ]\na\u2208A(s)\ns(cid:3)\nunchanged?\u2190false\nuntilunchanged?\nreturn\u03c0\nFigure17.7 Thepolicyiterationalgorithmforcalculatinganoptimalpolicy. 658 Chapter 17. MakingComplexDecisions\nThe algorithms we have described so far require updating the utility or policy for all\nstates at once. It turns out that this is not strictly necessary. In fact, on each iteration, we\ncan pick any subset ofstates and apply either kind ofupdating (policy improvement orsim-\nplified value iteration) to that subset. This very general algorithm is called asynchronous\nASYNCHRONOUS policy iteration. Given certain conditions on the initial policy and initial utility function,\nPOLICYITERATION\nasynchronous policy iteration is guaranteed to converge to an optimal policy. The freedom\nto choose any states to work on means that we can design much more efficient heuristic\nalgorithms\u2014for example, algorithms that concentrate on updating the values of states that\narelikely tobereachedbyagoodpolicy. Thismakesalotofsenseinreallife: ifonehasno\nintention ofthrowing oneself offacliff, oneshould notspend timeworrying about theexact\nvalueoftheresulting states.\n17.4 PARTIALLY OBSERVABLE MDPS\nThedescription ofMarkov decision processes inSection 17.1assumed that theenvironment\nwas fully observable. With this assumption, the agent always knows which state it is in.\nThis,combinedwiththeMarkovassumptionforthetransitionmodel,meansthattheoptimal\npolicydependsonlyonthecurrentstate. Whentheenvironmentisonlypartiallyobservable,\nthe situation is, one might say, much less clear. The agent does not necessarily know which\nstateitisin,soitcannotexecutetheaction\u03c0(s)recommendedforthatstate. Furthermore,the\nutility ofastatesandtheoptimalactioninsdepend notjustons,butalsoonhowmuchthe\nPARTIALLY agent knows whenitisins. Forthese reasons, partially observable MDPs(orPOMDPs\u2014\nOBSERVABLEMDP\npronounced\u201cpom-dee-pees\u201d)areusuallyviewedasmuchmoredifficultthanordinaryMDPs.\nWecannotavoidPOMDPs,however,becausetherealworldisone.\n17.4.1 DefinitionofPOMDPs\nTo get a handle on POMDPs, we must first define them properly. A POMDP has the same\nelements as an MDP\u2014the transition model P(s(cid:2)|s,a), actions A(s), and reward function\nR(s)\u2014but, like the partially observable search problems of Section 4.4, italso has asensor\nmodelP(e|s). Here,asinChapter15,thesensormodelspecifiestheprobability ofperceiv-\ning evidence e in state s.3 For example, we can convert the 4\u00d73 world of Figure 17.1 into\na POMDP by adding a noisy or partial sensor instead of assuming that the agent knows its\nlocation exactly. Suchasensormightmeasure the number ofadjacent walls, whichhappens\nto be 2 in all the nonterminal squares except for those in the third column, where the value\nis1;anoisyversionmightgivethewrongvaluewithprobability 0.1.\nIn Chapters 4 and 11, we studied nondeterministic and partially observable planning\nproblems andidentified the beliefstate\u2014the setofactual states the agent might bein\u2014as a\nkeyconceptfordescribingandcalculatingsolutions. InPOMDPs,thebeliefstatebbecomesa\nprobability distribution overallpossiblestates,justasinChapter15. Forexample, theinitial\n3 AswiththerewardfunctionforMDPs,thesensormodelcanalsodependontheactionandoutcomestate,but\nagainthischangeisnotfundamental. Section17.4. PartiallyObservableMDPs 659\nbeliefstateforthe4\u00d73POMDPcouldbetheuniformdistribution overtheninenonterminal\nstates, i.e., (cid:16)1,1, 1,1, 1,1,1, 1,1,0,0(cid:17). We write b(s) for the probability assigned to the\n9 9 9 9 9 9 9 9 9\nactualstatesbybeliefstateb. Theagentcancalculateitscurrentbeliefstateastheconditional\nprobability distribution over the actual states given the sequence of percepts and actions so\nfar. ThisisessentiallythefilteringtaskdescribedinChapter15. Thebasicrecursivefiltering\nequation (15.5 on page 572) shows how to calculate the new belief state from the previous\nbelief state and thenew evidence. ForPOMDPs,wealso have anaction to consider, but the\nresult isessentially thesame. If b(s)wasthe previous belief state, and theagent does action\naandthenperceivesevidence e,thenthenewbeliefstateisgivenby\n(cid:12)\nb(cid:2) (s(cid:2)\n)=\n\u03b1P(e|s(cid:2)\n)\nP(s(cid:2)|s,a)b(s),\ns\nwhere \u03b1is anormalizing constant that makes the belief state sum to 1. Byanalogy with the\nupdateoperatorforfiltering(page572),wecanwritethisas\n(cid:2)\nb = FORWARD(b,a,e). (17.11)\nInthe4\u00d73POMDP,supposetheagentmovesLeftanditssensorreports1adjacentwall;then\nit\u2019s quite likely (although not guaranteed, because both the motion and the sensor are noisy)\nthattheagentisnowin(3,1). Exercise17.13asksyoutocalculatetheexactprobabilityvalues\nforthenewbeliefstate.\nThe fundamental insight required to understand POMDPs is this: the optimal action\ndepends onlyontheagent\u2019s current belief state. Thatis,theoptimal policycanbedescribed\n\u2217\nby a mapping \u03c0 (b) from belief states to actions. It does not depend on the actual state the\nagentisin. Thisisagoodthing,becausetheagentdoesnotknowitsactualstate;allitknows\nisthebeliefstate. Hence,thedecision cycleofaPOMDPagentcanbebroken downintothe\nfollowingthreesteps:\n\u2217\n1. Giventhecurrentbeliefstate b,executetheactiona=\u03c0 (b).\n2. Receivepercept e.\n3. SetthecurrentbeliefstatetoFORWARD(b,a,e)andrepeat.\nNowwecanthink ofPOMDPsasrequiring asearch inbelief-state space, justlikethemeth-\nods for sensorless and contingency problems in Chapter 4. The main difference is that the\nPOMDPbelief-state space iscontinuous, because aPOMDPbelief stateisaprobability dis-\ntribution. For example, a belief state for the 4\u00d73 world is a point in an 11-dimensional\ncontinuous space. An action changes the belief state, not just the physical state. Hence, the\naction isevaluated atleastinpartaccording totheinformation theagentacquires asaresult.\nPOMDPstherefore include the value of information (Section 16.6) as one component of the\ndecision problem.\nLet\u2019s look more carefully at the outcome of actions. In particular, let\u2019s calculate the\n(cid:2)\nprobability thatanagentinbeliefstatebreachesbeliefstateb afterexecutingactiona. Now,\nif we knew the action and the subsequent percept, then Equation (17.11) would provide a\n(cid:2)\ndeterministic update to the belief state: b = FORWARD(b,a,e). Of course, the subsequent\n(cid:2)\npercept isnot yetknown, sotheagent mightarrive inoneofseveral possible belief states b,\ndepending on the percept that is received. The probability of perceiving e, given that a was 660 Chapter 17. MakingComplexDecisions\n(cid:2)\nperformed starting inbelief state b, is given by summing overall the actual states s that the\nagentmightreach:\n(cid:12)\nP(e|a,b) = P(e|a,s(cid:2) ,b)P(s(cid:2)|a,b)\n(cid:12)s(cid:3)\n=\nP(e|s(cid:2) )P(s(cid:2)|a,b)\n(cid:12)s(cid:3)\n(cid:12)\n=\nP(e|s(cid:2)\n)\nP(s(cid:2)|s,a)b(s).\ns(cid:3) s\nLet us write the probability of reaching\nb(cid:2)\nfrom b, given action a, as\nP(b(cid:2)|b,a)).\nThen that\ngivesus\n(cid:12)\nP(b(cid:2)|b,a)\n=\nP(b(cid:2)|a,b)\n=\nP(b(cid:2)|e,a,b)P(e|a,b)\n(cid:12) e (cid:12) (cid:12)\n=\nP(b(cid:2)|e,a,b) P(e|s(cid:2)\n)\nP(s(cid:2)|s,a)b(s),\n(17.12)\ne s(cid:3) s\nwhereP(b(cid:2)|e,a,b)is1ifb(cid:2)\n=FORWARD(b,a,e)and0otherwise.\nEquation(17.12)canbeviewedasdefiningatransitionmodelforthebelief-statespace.\nWecanalsodefinearewardfunction forbeliefstates(i.e.,theexpectedrewardfortheactual\nstatestheagentmightbein):\n(cid:12)\n\u03c1(b) = b(s)R(s).\ns\nTogether,\nP(b(cid:2)|b,a)\nand \u03c1(b) define an observable MDP on the space of belief states. Fur-\n\u2217\nthermore,itcanbeshownthatanoptimalpolicyforthisMDP,\u03c0 (b),isalsoanoptimalpolicy\nfortheoriginal POMDP.Inotherwords, solving aPOMDPonaphysical statespace canbe\nreducedtosolvinganMDPonthecorresponding belief-statespace. Thisfactisperhapsless\nsurprisingifwerememberthatthebeliefstateisalwaysobservabletotheagent,bydefinition.\nNotice that, although we have reduced POMDPs to MDPs, the MDPwe obtain has a\ncontinuous (and usually high-dimensional) state space. None of the MDP algorithms de-\nscribed in Sections 17.2 and 17.3 applies directly to such MDPs. The next two subsec-\ntions describe a value iteration algorithm designed specifically for POMDPs and an online\ndecision-making algorithm, similartothosedeveloped for gamesinChapter5.\n17.4.2 ValueiterationforPOMDPs\nSection 17.2 described a value iteration algorithm that computed one utility value for each\nstate. With infinitely many belief states, we need to be more creative. Consider an optimal\n\u2217\npolicy \u03c0 and itsapplication in aspecific belief state b: the policy generates an action, then,\nforeachsubsequent percept, thebeliefstateisupdated and anewaction isgenerated, andso\non. Forthisspecificb,therefore,thepolicyisexactlyequivalenttoaconditionalplan,asde-\nfinedinChapter4fornondeterministicandpartiallyobservableproblems. Insteadofthinking\nabout policies, letusthink about conditional plans andhowtheexpected utility ofexecuting\nafixedconditional planvarieswiththeinitialbeliefstate. Wemaketwoobservations: Section17.4. PartiallyObservableMDPs 661\n1. Lettheutilityofexecutingafixedconditionalplanpstartinginphysicalstatesbe\u03b1 (s).\n(cid:2) p\nThentheexpected utility ofexecuting pinbelief state bisjust b(s)\u03b1 (s),orb\u00b7\u03b1\ns p p\nif we think of them both as vectors. Hence, the expected utility of a fixed conditional\nplanvarieslinearly withb;thatis,itcorresponds toahyperplane inbeliefspace.\n2. At any given belief state b, the optimal policy will choose to execute the conditional\nplanwithhighestexpectedutility;andtheexpectedutilityofbundertheoptimalpolicy\nisjusttheutilityofthatconditional plan:\nU(b) = U\u03c0\u2217 (b) = maxb\u00b7\u03b1 .\np\np\n\u2217\nIftheoptimalpolicy\u03c0 choosestoexecutepstartingatb,thenitisreasonabletoexpect\nthat it might choose to execute p in belief states that are very close to b; in fact, if we\nbound the depth of the conditional plans, then there are only finitely many such plans\nand the continuous space of belief states will generally be divided into regions, each\ncorresponding toaparticularconditional planthatisoptimalinthatregion.\nFromthese twoobservations, weseethattheutility function U(b)onbelief states, being the\nmaximumofacollection ofhyperplanes, willbepiecewiselinearandconvex.\nToillustrate this, weuseasimple two-state world. Thestates arelabeled 0and1,with\nR(0)=0 and R(1)=1. There are two actions: Stay stays put with probability 0.9 and Go\nswitches to the other state with probability 0.9. Fornow we will assume the discount factor\n\u03b3=1. Thesensor reports the correct state with probability 0.6. Obviously, the agent should\nStaywhenitthinksit\u2019sinstate1andGowhenitthinksit\u2019sinstate0.\nThe advantage of a two-state world is that the belief space can be viewed as one-\ndimensional, because the two probabilities must sum to 1. In Figure 17.8(a), the x-axis\nrepresentsthebeliefstate,definedbyb(1),theprobabilityofbeinginstate1. Nowletuscon-\nsider the one-step plans [Stay] and [Go], each of which receives the reward for the current\nstatefollowedbythe(discounted) rewardforthestatereached aftertheaction:\n\u03b1 (0) = R(0)+\u03b3(0.9R(0)+0.1R(1)) = 0.1\n[Stay]\n\u03b1 (1) = R(1)+\u03b3(0.9R(1)+0.1R(0)) = 1.9\n[Stay]\n\u03b1 (0) = R(0)+\u03b3(0.9R(1)+0.1R(0)) = 0.9\n[Go]\n\u03b1 (1) = R(1)+\u03b3(0.9R(0)+0.1R(1)) = 1.1\n[Go]\nThehyperplanes(lines,inthiscase)forb\u00b7\u03b1 andb\u00b7\u03b1 areshowninFigure17.8(a)and\n[Stay] [Go]\ntheir maximum is shown in bold. The bold line therefore represents the utility function for\nthe finite-horizon problem that allows just one action, and in each \u201cpiece\u201d of the piecewise\nlinear utility function the optimal action is the first action of the corresponding conditional\nplan. Inthiscase,theoptimalone-step policyistoStaywhenb(1) > 0.5andGootherwise.\nOncewehave utilities \u03b1 (s)forallthe conditional plans pofdepth 1ineach physical\np\nstate s, we can compute the utilities for conditional plans of depth 2 by considering each\npossible first action, each possible subsequent percept, and then each way of choosing a\ndepth-1plantoexecuteforeachpercept:\n[Stay; ifPercept=0thenStay elseStay]\n[Stay; ifPercept=0thenStay elseGo]... 662 Chapter 17. MakingComplexDecisions\n3 3\n2.5 2.5\n2 2\n[Stay]\n1.5 1.5\n[Go]\n1 1\n0.5 0.5\n0 0\n0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1\nProbability of state 1 Probability of state 1\n(a) (b)\n3 7.5\n2.5 7\n2 6.5\n1.5 6\n1 5.5\n0.5 5\n0 4.5\n0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1\nProbability of state 1 Probability of state 1\n(c) (d)\nFigure17.8 (a)Utility oftwo one-stepplansasa functionoftheinitialbeliefstate b(1)\nforthetwo-stateworld, with thecorrespondingutility functionshowninbold. (b)Utilities\nfor8 distinct two-step plans. (c) Utilities forfourundominatedtwo-step plans. (d) Utility\nfunctionforoptimaleight-stepplans.\nThere are eight distinct depth-2 plans in all, and their utilities are shown in Figure 17.8(b).\nNotice that four of the plans, shown as dashed lines, are suboptimal across the entire belief\nspace\u2014we say these plans are dominated, and they need not be considered further. There\nDOMINATEDPLAN\nare four undominated plans, each of which is optimal in a specific region, as shown in Fig-\nure17.8(c). Theregionspartition thebelief-state space.\nWerepeat theprocess fordepth 3,andsoon. Ingeneral, let pbeadepth-d conditional\nplanwhoseinitialactionisaandwhosedepth-d\u22121subplan forpercept eisp.e;then\n(cid:31)\n(cid:12) (cid:12)\n\u03b1 (s) = R(s)+\u03b3\nP(s(cid:2)|s,a) P(e|s(cid:2)\n)\u03b1\n(s(cid:2)\n) . (17.13)\np p.e\ns(cid:3) e\nThisrecursionnaturallygivesusavalueiterationalgorithm,whichissketchedinFigure17.9.\nThestructureofthealgorithmanditserroranalysisaresimilartothoseofthebasicvalueiter-\nation algorithm inFigure 17.4onpage653; themaindifference isthat instead ofcomputing\none utility number for each state, POMDP-VALUE-ITERATION maintains a collection of\nytilitU\nytilitU\nytilitU\nytilitU Section17.4. PartiallyObservableMDPs 663\nfunctionPOMDP-VALUE-ITERATION(pomdp,(cid:2))returnsautilityfunction\ninputs:pomdp,aPOMDPwithstatesS,actionsA(s),transitionmodelP(s(cid:5)|s,a),\nsensormodelP(e|s),rewardsR(s),discount\u03b3\n(cid:2),themaximumerrorallowedintheutilityofanystate\nlocalvariables: U,U(cid:5),setsofplanspwithassociatedutilityvectors\u03b1p\nU(cid:5)\u2190asetcontainingjusttheemptyplan[],with\u03b1 (s)= R(s)\n[]\nrepeat\nU \u2190U(cid:5)\nU(cid:5)\u2190thesetofallplansconsistingofanactionand,foreachpossiblenextpercept,\naplaninU withutilityvectorscomputedaccordingtoEquation(17.13)\nU(cid:5)\u2190REMOVE-DOMINATED-PLANS(U(cid:5))\nuntilMAX-DIFFERENCE(U,U(cid:5)) < (cid:2)(1\u2212\u03b3)\/\u03b3\nreturnU\nFigure 17.9 A high-level sketch of the value iteration algorithm for POMDPs. The\nREMOVE-DOMINATED-PLANSstepandMAX-DIFFERENCEtestaretypicallyimplemented\naslinearprograms.\nundominated plans with their utility hyperplanes. The algorithm\u2019s complexity depends pri-\nmarilyonhowmanyplansgetgenerated. Given |A|actions and|E|possibleobservations, it\niseasytoshowthatthereare|A|O(|E|d\u22121)distinctdepth-dplans.\nEvenforthelowlytwo-state\nworld with d=8, the exact number is 2255. The elimination of dominated plans is essential\nforreducing thisdoubly exponential growth: thenumberofundominated planswithd=8is\njust144. Theutilityfunction forthese144plansisshowninFigure17.8(d).\nNotice that even though state 0 has lower utility than state 1, the intermediate belief\nstates have even lower utility because the agent lacks the information needed to choose a\ngood action. This is why information has value in the sense defined in Section 16.6 and\noptimalpoliciesinPOMDPsofteninclude information-gathering actions.\nGivensuchautilityfunction,anexecutablepolicycanbeextractedbylookingatwhich\nhyperplane is optimal at any given belief state b and executing the first action of the corre-\nsponding plan. In Figure 17.8(d), the corresponding optimal policy is still the same as for\ndepth-1plans: Staywhenb(1) > 0.5andGootherwise.\nIn practice, the value iteration algorithm in Figure 17.9 is hopelessly inefficient for\nlarger problems\u2014even the 4\u00d73 POMDPis too hard. The main reason is that, given ncon-\nditional plans at level d, the algorithm constructs\n|A|\u00b7n|E|\nconditional plans at level d+1\nbeforeeliminatingthedominatedones. Sincethe1970s,whenthisalgorithmwasdeveloped,\ntherehavebeenseveraladvancesincludingmoreefficientformsofvalueiterationandvarious\nkindsofpolicyiterationalgorithms. Someofthesearediscussedinthenotesattheendofthe\nchapter. Forgeneral POMDPs,however, finding optimal policies isvery difficult (PSPACE-\nhard, in fact\u2014i.e., very hard indeed). Problems witha few dozen states are often infeasible.\nThenext section describes adifferent, approximate method forsolving POMDPs,onebased\nonlook-ahead search. 664 Chapter 17. MakingComplexDecisions\nA A A A A\nt\u20132 t\u20131 t t+1 t+2\nX X X X X U\nt\u20131 t t+1 t+2 t+3 t+3\nR R R R\nt\u20131 t t+1 t+2\nE E E E E\nt\u20131 t t+1 t+2 t+3\nFigure17.10 Thegenericstructureofadynamicdecisionnetwork.Variableswithknown\nvaluesareshaded.Thecurrenttimeistandtheagentmustdecidewhattodo\u2014thatis,choose\navalueforAt. Thenetworkhasbeenunrolledintothefutureforthreestepsandrepresents\nfuturerewards,aswellastheutilityofthestateatthelook-aheadhorizon.\n17.4.3 Onlineagents forPOMDPs\nInthissection,weoutlineasimpleapproachtoagentdesignforpartiallyobservable,stochas-\nticenvironments. Thebasicelementsofthedesignarealready familiar:\n\u2022 The transition and sensor models are represented by a dynamic Bayesian network\n(DBN),asdescribed inChapter15.\n\u2022 Thedynamic Bayesian network is extended withdecision and utility nodes, as used in\ndecision networks in Chapter 16. The resulting model is called a dynamic decision\nDYNAMICDECISION network,orDDN.\nNETWORK\n\u2022 A filtering algorithm is used to incorporate each new percept and action and to update\nthebeliefstaterepresentation.\n\u2022 Decisions are made by projecting forward possible action sequences and choosing the\nbestone.\nDBNs are factored representations in the terminology of Chapter 2; they typically have\nan exponential complexity advantage over atomic representations and can model quite sub-\nstantial real-world problems. Theagentdesign istherefore apractical implementation ofthe\nutility-based agentsketched inChapter2.\nIn the DBN, the single state S becomes a set of state variables X , and there may be\nt t\nmultipleevidencevariablesE . WewilluseA torefertotheactionattimet,sothetransition\nt t\nmodelbecomes P(X |X ,A )andthesensormodelbecomes P(E |X ). Wewilluse R to\nt+1 t t t t t\nrefertotherewardreceivedattime tandU torefertotheutility ofthestateattime t. (Both\nt\nofthesearerandomvariables.) Withthisnotation,adynamicdecisionnetworklookslikethe\noneshowninFigure17.10.\nDynamicdecisionnetworkscanbeusedasinputsforanyPOMDPalgorithm,including\nthoseforvalueandpolicyiterationmethods. Inthissection,wefocusonlook-aheadmethods\nthatprojectactionsequencesforwardfromthecurrentbeliefstateinmuchthesamewayasdo\nthe game-playing algorithms of Chapter 5. The network in Figure 17.10 has been projected\nthree steps into the future; the current and future decisions A and the future observations Section17.4. PartiallyObservableMDPs 665\nA in P(X | E )\nt t 1:t\nE . . .\nt+1\n... ... ... ...\nA t+1 in P(X t+1 | E 1:t+1) . . .\n... ... ...\nE . . .\nt+2\n... ... ...\nA t+2 in P(X t+2 | E 1:t+2) . . .\n... ... ...\nE t+3 . . .\n... ... ...\nU(X ) . . .\nt+3\n10 4 6 3\nFigure17.11 Partofthelook-aheadsolutionoftheDDNinFigure17.10. Eachdecision\nwillbetakeninthebeliefstateindicated.\nE and rewards R are all unknown. Notice that the network includes nodes for the rewards\nfor X and X , but the utility for X . This is because the agent must maximize the\nt+1 t+2 t+3\n(discounted) sum of all future rewards, and U(X ) represents the reward for X and all\nt+3 t+3\nsubsequentrewards. AsinChapter5,weassumethatU isavailableonlyinsomeapproximate\nform: ifexactutilityvalueswereavailable,look-aheadbeyonddepth1wouldbeunnecessary.\nFigure 17.11 shows part of the search tree corresponding to the three-step look-ahead\nDDNinFigure17.10. Eachofthetriangularnodesisabeliefstateinwhichtheagentmakes\na decision A for i=0,1,2,.... The round (chance) nodes correspond to choices by the\nt+i\nenvironment, namely, what evidence E arrives. Notice that there are no chance nodes\nt+i\ncorresponding to the action outcomes; this is because the belief-state update for an action is\ndeterministic regardless oftheactualoutcome.\nThe belief state at each triangular node can be computed by applying a filtering al-\ngorithm to the sequence of percepts and actions leading to it. In this way, the algorithm\ntakes into account the fact that, for decision A , the agent will have available percepts\nt+i\nE , ..., E , even though at time t it does not know what those percepts willbe. In this\nt+1 t+i\nway,adecision-theoretic agentautomatically takesintoaccountthevalueofinformation and\nwillexecuteinformation-gathering actionswhereappropriate.\nA decision can be extracted from the search tree by backing up the utility values from\nthe leaves, taking an average at the chance nodes and taking the maximum at the decision\nnodes. ThisissimilartotheEXPECTIMINIMAX algorithmforgametreeswithchancenodes,\nexcept that (1) there can also be rewards at non-leaf states and (2) the decision nodes corre-\nspond to belief states rather than actual states. The time complexity of an exhaustive search\ntodepthdisO(|A|d \u00b7|E|d),where|A|isthenumberofavailable actionsand|E|isthenum-\nber of possible percepts. (Notice that this is far less than the number of depth-d conditional 666 Chapter 17. MakingComplexDecisions\nplans generated by value iteration.) For problems in which the discount factor \u03b3 is not too\nclose to 1, a shallow search is often good enough to give near-optimal decisions. It is also\npossible to approximate the averaging step atthe chance nodes, by sampling from the set of\npossibleperceptsinsteadofsummingoverallpossiblepercepts. Therearevariousotherways\noffindinggoodapproximate solutions quickly, butwedeferthemtoChapter21.\nDecision-theoreticagentsbasedondynamicdecisionnetworkshaveanumberofadvan-\ntagescompared withother, simpleragentdesigns presented inearlierchapters. Inparticular,\ntheyhandlepartiallyobservable,uncertainenvironments andcaneasilyrevisetheir\u201cplans\u201dto\nhandle unexpected evidence. Withappropriate sensormodels, theycanhandle sensorfailure\nandcanplantogatherinformation. Theyexhibit\u201cgraceful degradation\u201d undertimepressure\nand incomplex environments, using various approximation techniques. Sowhatismissing?\nOnedefectofourDDN-basedalgorithmisitsrelianceonforwardsearchthroughstatespace,\nratherthanusingthehierarchicalandotheradvancedplanningtechniquesdescribedinChap-\nter11. Therehavebeenattemptstoextendthesetechniquesintotheprobabilisticdomain,but\nsofarthey haveproved tobeinefficient. Asecond, related problem isthebasically proposi-\ntionalnature oftheDDNlanguage. Wewouldliketobeabletoextendsomeoftheideasfor\nfirst-order probabilistic languages to the problem of decision making. Current research has\nshownthatthisextension ispossible andhassignificant benefits, asdiscussed inthenotesat\ntheendofthechapter.\n17.5 DECISIONS WITH MULTIPLE AGENTS: GAME THEORY\nThis chapter has concentrated on making decisions in uncertain environments. But what if\ntheuncertainty isduetootheragentsandthedecisionstheymake? Andwhatifthedecisions\nof those agents are in turn influenced by our decisions? We addressed this question once\nbefore, when westudied gamesinChapter 5. There, however, wewereprimarily concerned\nwith turn-taking games in fully observable environments, for which minimax search can be\nusedtofindoptimalmoves. Inthissectionwestudytheaspectsofgametheorythatanalyze\nGAMETHEORY\ngames with simultaneous moves and other sources of partial observability. (Game theorists\nusethetermsperfectinformationandimperfectinformationratherthanfullyandpartially\nobservable.) Gametheorycanbeusedinatleasttwoways:\n1. Agentdesign: Gametheorycananalyzetheagent\u2019sdecisionsandcomputetheexpected\nutility for each decision (under the assumption that other agents are acting optimally\naccording to game theory). Forexample, in the game two-finger Morra, two players,\nO and E, simultaneously display one or two fingers. Let the total number of fingers\nbe f. If f is odd, O collects f dollars from E; and if f is even, E collects f dollars\nfrom O. Gametheory can determine the best strategy against a rational player and the\nexpectedreturnforeachplayer.4\n4 Morraisarecreationalversionofaninspectiongame.Insuchgames,aninspectorchoosesadaytoinspecta\nfacility(suchasarestaurantorabiologicalweaponsplant),andthefacilityoperatorchoosesadaytohideallthe\nnastystuff.Theinspectorwinsifthedaysaredifferent,andthefacilityoperatorwinsiftheyarethesame. Section17.5. DecisionswithMultipleAgents: GameTheory 667\n2. Mechanism design: When an environment is inhabited by many agents, it might be\npossible to define the rules of the environment (i.e., the game that the agents must\nplay)sothatthecollective goodofallagentsismaximizedwheneachagentadoptsthe\ngame-theoretic solution that maximizes its own utility. For example, game theory can\nhelp design the protocols for a collection of Internet traffic routers so that each router\nhasanincentive toactinsuch awaythat global throughput ismaximized. Mechanism\ndesigncanalsobeusedtoconstruct intelligent multiagentsystemsthatsolvecomplex\nproblemsinadistributed fashion.\n17.5.1 Single-movegames\nWestartbyconsidering arestricted setofgames: oneswhere allplayerstakeactionsimulta-\nneously and the result of the game is based on this single set of actions. (Actually, it is not\ncrucialthattheactionstakeplaceatexactlythesametime;whatmattersisthatnoplayerhas\nknowledge ofthe otherplayers\u2019 choices.) Therestriction to asingle move (and the very use\nof the word \u201cgame\u201d) might make this seem trivial, but in fact, game theory is serious busi-\nness. It is used in decision-making situations including the auctioning of oil drilling rights\nand wireless frequency spectrum rights, bankruptcy proceedings, product development and\npricingdecisions,andnationaldefense\u2014situations involvingbillionsofdollarsandhundreds\nofthousands oflives. Asingle-movegameisdefinedbythreecomponents:\n\u2022 Players oragents who will be making decisions. Two-player games have received the\nPLAYER\nmostattention, although n-player games for n > 2are also common. Wegive players\ncapitalized names,likeAliceandBoborO andE.\n\u2022 Actionsthattheplayerscanchoose. Wewillgiveactionslowercasenames,likeone or\nACTION\ntestify. Theplayersmayormaynothavethesamesetofactionsavailable.\n\u2022 Apayoff function that gives the utility toeach player foreach combination of actions\nPAYOFFFUNCTION\nby all the players. Forsingle-move games the payoff function can be represented by a\nmatrix, a representation known as the strategic form (also called normal form). The\nSTRATEGICFORM\npayoffmatrixfortwo-fingerMorraisasfollows:\nO:one O:two\nE:one E = +2,O = \u22122 E = \u22123,O = +3\nE:two E = \u22123,O = +3 E = +4,O = \u22124\nForexample, the lower-right corner shows that when player O chooses action two and\nE alsochoosestwo,thepayoffis+4forE and\u22124forO.\nEach player in a game must adopt and then execute a strategy (which is the name used in\nSTRATEGY\ngametheoryforapolicy). Apurestrategyisadeterministicpolicy;forasingle-movegame,\nPURESTRATEGY\na pure strategy is just a single action. Formany games an agent can do better with a mixed\nstrategy, which is a randomized policy that selects actions according to aprobability distri-\nMIXEDSTRATEGY\nbution. The mixed strategy that chooses action a with probability p and action b otherwise\nis written [p:a;(1 \u2212 p):b]. For example, a mixed strategy for two-finger Morra might be\n[0.5:one;0.5:two]. A strategy profile is an assignment of a strategy to each player; given\nSTRATEGYPROFILE\nthestrategyprofile,thegame\u2019soutcomeisanumericvalueforeachplayer.\nOUTCOME 668 Chapter 17. MakingComplexDecisions\nAsolutiontoagameisastrategyprofileinwhicheachplayeradoptsarationalstrategy.\nSOLUTION\nWe will see that the most important issue in game theory is to define what \u201crational\u201d means\nwhen each agent chooses only part of the strategy profile that determines the outcome. It is\nimportant to realize that outcomes are actual results of playing a game, while solutions are\ntheoretical constructs used to analyze a game. We will see that some games have a solution\nonly in mixed strategies. But that does not mean that a player must literally be adopting a\nmixedstrategytoberational.\nConsider the following story: Two alleged burglars, Alice and Bob, are caught red-\nhandednearthesceneofaburglary andareinterrogated separately. Aprosecutorofferseach\na deal: if you testify against your partner as the leader of a burglary ring, you\u2019ll go free for\nbeing thecooperative one, while yourpartner willserve 10 years inprison. However, if you\nboth testify against each other, you\u2019ll both get5years. Alice andBobalsoknow that ifboth\nrefuse to testify they will serve only 1 year each for the lesser charge of possessing stolen\nPRISONER\u2019S property. Now Alice and Bob face the so-called prisoner\u2019s dilemma: should they testify\nDILEMMA\nor refuse? Being rational agents, Alice and Bob each want to maximize their own expected\nutility. Let\u2019sassumethatAliceiscallouslyunconcernedaboutherpartner\u2019sfate,soherutility\ndecreases in proportion to the number of years she will spend in prison, regardless of what\nhappenstoBob. Bobfeelsexactlythesameway. Tohelpreacharational decision, theyboth\nconstruct thefollowingpayoffmatrix:\nAlice:testify Alice:refuse\nBob:testify A= \u22125,B = \u22125 A= \u221210,B = 0\nBob:refuse A = 0,B = \u221210 A = \u22121,B = \u22121\nAlice analyzes the payoff matrix as follows: \u201cSuppose Bob testifies. Then I get 5 years if I\ntestify and 10 years if I don\u2019t, so in that case testifying is better. On the other hand, if Bob\nrefuses, thenIget0yearsifItestifyand1yearifIrefuse,sointhatcaseaswelltestifying is\nbetter. Soineithercase,it\u2019sbetterformetotestify, sothat\u2019swhatImustdo.\u201d\nDOMINANT Alice has discovered that testify is a dominant strategy for the game. We say that a\nSTRATEGY\n(cid:2)\nSTRONG strategysforplayerpstronglydominatesstrategys iftheoutcomeforsisbetterforpthan\nDOMINATION\n(cid:2)\nthe outcome for s, for every choice of strategies by the other player(s). Strategy s weakly\n(cid:2) (cid:2)\ndominates s if sis betterthan s on atleast one strategy profile and no worse on anyother.\nWEAKDOMINATION\nAdominantstrategyisastrategythatdominatesallothers. Itisirrationaltoplayadominated\nstrategy, and irrational not to play a dominant strategy if one exists. Being rational, Alice\nchoosesthedominantstrategy. Weneedjustabitmoreterminology: wesaythatanoutcome\nisParetooptimal5 ifthere isnootheroutcome thatallplayers would prefer. An outcome is\nPARETOOPTIMAL\nParetodominatedbyanotheroutcomeifallplayers wouldprefertheotheroutcome.\nPARETODOMINATED\nIf Alice is clever as well as rational, she will continue to reason as follows: Bob\u2019s\ndominant strategy isalsototestify. Therefore, hewilltestifyandwewillbothgetfiveyears.\nWhen each player has a dominant strategy, the combination of those strategies is called a\nDOMINANT\nSTRATEGY dominant strategy equilibrium. In general, a strategy profile forms an equilibrium if no\nEQUILIBRIUM\nplayer canbenefit byswitching strategies, given that every otherplayer sticks withthesame\nEQUILIBRIUM\n5 ParetooptimalityisnamedaftertheeconomistVilfredoPareto(1848\u20131923). Section17.5. DecisionswithMultipleAgents: GameTheory 669\nstrategy. Anequilibrium isessentially a local optimumin thespace of policies; itisthe top\nofapeak thatslopes downward along every dimension, wherea dimension corresponds toa\nplayer\u2019s strategychoices.\nThe mathematician John Nash (1928\u2013) proved that every game has at least one equi-\nlibrium. The general concept of equilibrium is now called Nash equilibrium in his honor.\nClearly, a dominant strategy equilibrium is a Nash equilibrium (Exercise 17.16), but some\nNASHEQUILIBRIUM\ngameshaveNashequilibria butnodominantstrategies.\nThe dilemma in the prisoner\u2019s dilemma is that the equilibrium outcome is worse for\nboth players than the outcome they would get if they both refused totestify. Inother words,\n(testify,testify)isParetodominatedbythe(-1,-1)outcomeof(refuse,refuse). Isthereany\nway for Alice and Bob to arrive at the (-1, -1) outcome? It is certainly an allowable option\nfor both of them to refuse to testify, but is is hard to see how rational agents can get there,\ngiventhedefinitionofthegame. Eitherplayercontemplating playing refuse willrealizethat\nhe or she would do better by playing testify. That is the attractive power of an equilibrium\npoint. Gametheorists agree thatbeingaNashequilibrium is anecessary condition forbeing\nasolution\u2014although theydisagreewhetheritisasufficient condition.\nIt is easy enough to get to the (refuse,refuse) solution if we modify the game. For\nexample,wecouldchangetoarepeatedgameinwhichtheplayersknowthattheywillmeet\nagain. Ortheagents might havemoral beliefs that encourage cooperation andfairness. That\nmeans they have a different utility function, necessitating a different payoff matrix, making\nit a different game. We will see later that agents with limited computational powers, rather\nthantheabilitytoreasonabsolutelyrationally,canreachnon-equilibriumoutcomes,ascanan\nagentthatknowsthattheotheragenthaslimitedrationality. Ineachcase,weareconsidering\nadifferent gamethantheonedescribed bythepayoffmatrixabove.\nNow let\u2019s look at a game that has no dominant strategy. Acme, a video game console\nmanufacturer, has todecide whether itsnext gamemachine willuse Blu-ray discs orDVDs.\nMeanwhile, the video game software producer Best needs to decide whether to produce its\nnextgameonBlu-rayorDVD.Theprofitsforbothwillbepositiveiftheyagreeandnegative\niftheydisagree, asshowninthefollowingpayoffmatrix:\nAcme:bluray Acme:dvd\nBest:bluray A = +9,B = +9 A= \u22124,B = \u22121\nBest:dvd A = \u22123,B = \u22121 A= +5,B = +5\nThere is no dominant strategy equilibrium for this game, but there are two Nash equilibria:\n(bluray, bluray) and (dvd, dvd). We know these are Nash equilibria because if either player\nunilaterally movestoadifferent strategy, thatplayer willbeworse off. Nowtheagents have\na problem: there are multiple acceptable solutions, but if each agent aims for a different\nsolution, then both agents will suffer. How can they agree on a solution? One answer is\nthat both should choose the Pareto-optimal solution (bluray, bluray); that is, we can restrict\nthedefinition of\u201csolution\u201d totheunique Pareto-optimal Nashequilibrium provided that one\nexists. Every game has at least one Pareto-optimal solution, but a game might have several,\nor they might not be equilibrium points. For example, if (bluray, bluray) had payoff (5,\n5), then there would be two equal Pareto-optimal equilibrium points. To choose between 670 Chapter 17. MakingComplexDecisions\nthem the agents can either guess or communicate, which can be done either by establishing\na convention that orders the solutions before the game begins or by negotiating to reach a\nmutually beneficial solution during the game (which would mean including communicative\nactions aspartofasequential game). Communication thus arises ingametheory forexactly\nthesamereasonsthatitaroseinmultiagentplanninginSection11.4. Gamesinwhichplayers\nCOORDINATION needtocommunicatelikethisarecalled coordination games.\nGAME\nA game can have more than one Nash equilibrium; how do we know that every game\nmust have at least one? Some games have no pure-strategy Nash equilibria. Consider, for\nexample, any pure-strategy profile for two-finger Morra (page 666). If the total number of\nfingersiseven,thenOwillwanttoswitch;ontheotherhand(sotospeak),ifthetotalisodd,\nthenE willwanttoswitch. Therefore, nopurestrategy profilecanbeanequilibrium andwe\nmustlooktomixedstrategies instead.\nButwhichmixed strategy? In 1928, von Neumann developed amethod forfinding the\noptimal mixed strategy for two-player, zero-sum games\u2014games in which the sum of the\nZERO-SUMGAME\npayoffsisalwayszero.6 Clearly,Morraissuchagame. Fortwo-player, zero-sum games,we\nknow that the payoffs are equal and opposite, so we need consider the payoffs of only one\nplayer, whowillbethemaximizer(justasinChapter5). ForMorra,wepicktheevenplayer\nEtobethemaximizer,sowecandefinethepayoffmatrixbythevaluesU (e,o)\u2014thepayoff\nE\ntoE ifE doeseandO doeso. (Forconvenience wecallplayer E \u201cher\u201dand O \u201chim.\u201d) Von\nNeumann\u2019smethodiscalledthethemaximintechnique, anditworksasfollows:\nMAXIMIN\n\u2022 Suppose we change the rules as follows: first E picks her strategy and reveals it to\nO. Then O picks his strategy, with knowledge of E\u2019s strategy. Finally, we evaluate\nthe expected payoff of the game based on the chosen strategies. This gives us a turn-\ntaking game to which we can apply the standard minimax algorithm from Chapter 5.\nLet\u2019s suppose this gives an outcome U . Clearly, this game favors O, so the true\nE,O\nutility U of the original game (from E\u2019s point of view) is at least U . Forexample,\nE,O\nif we just look at pure strategies, the minimax game tree has a root value of \u22123 (see\nFigure17.12(a)), soweknowthatU \u2265 \u22123.\n\u2022 Nowsuppose wechangetherulestoforce O torevealhisstrategyfirst,followedby E.\nThentheminimaxvalueofthisgameisU ,andbecausethisgamefavorsEweknow\nO,E\nthat U is at mostU . With pure strategies, the value is +2(see Figure 17.12(b)), so\nO,E\nweknowU \u2264 +2.\nCombining these twoarguments, wesee that thetrue utility U ofthesolution totheoriginal\ngamemustsatisfy\nU \u2264 U \u2264 U orinthiscase, \u22123 \u2264 U \u2264 2.\nE,O O,E\nTopinpointthevalueofU,weneedtoturnouranalysistomixedstrategies. First,observethe\nfollowing: once the first player has revealed his or her strategy, the second player might as\nwellchooseapurestrategy. Thereasonissimple: ifthesecondplayerplaysamixedstrategy,\n[p:one;(1\u2212p):two],itsexpectedutilityisalinearcombination (p\u00b7u +(1\u2212p)\u00b7u )of\none two\n6 oraconstant\u2014seepage162. Section17.5. DecisionswithMultipleAgents: GameTheory 671\n(a) E -3 (b) O 2\none two one two\nO -3 -3 E 2 4\none two one two one two one two\n2 -3 -3 4 2 -3 -3 4\n(c) E (d) O\n[p: one; (1 \u2013 p): two] [q: one; (1 \u2013 q): two]\nO E\none two one two\n2p \u2013 3(1 \u2013 p) 3p + 4(1 \u2013 p) 2q \u2013 3(1 \u2013 q) 3q + 4(1 \u2013 q)\n(e) U (f) U\n+4 +4\n+3 +3\ntwo two\n+2 +2\none one\n+1 +1\n0 p 0 q\n1 1\n\u20131 \u20131\n\u20132 \u20132\n\u20133 \u20133\nFigure 17.12 (a) and (b): Minimax game trees for two-finger Morra if the players take\nturns playing pure strategies. (c) and (d): Parameterized game trees where the first player\nplays a mixed strategy. The payoffs depend on the probability parameter (p or q) in the\nmixedstrategy. (e)and(f): Foranyparticularvalueoftheprobabilityparameter,thesecond\nplayer will choose the \u201cbetter\u201d of the two actions, so the value of the first player\u2019s mixed\nstrategyisgivenbytheheavylines. Thefirstplayerwillchoosetheprobabilityparameterfor\nthemixedstrategyattheintersectionpoint.\ntheutilitiesofthepurestrategies, u andu . Thislinearcombination canneverbebetter\none two\nthanthebetterofu andu ,sothesecondplayercanjustchoosethebetterone.\none two\nWiththisobservation inmind,theminimaxtreescanbethought ofashaving infinitely\nmany branches at the root, corresponding to the infinitely many mixed strategies the first 672 Chapter 17. MakingComplexDecisions\nplayer can choose. Each of these leads to a node with two branches corresponding to the\npurestrategiesforthesecondplayer. Wecandepicttheseinfinitetreesfinitelybyhavingone\n\u201cparameterized\u201d choiceattheroot:\n\u2022 IfE chooses first, the situation isas shown inFigure 17.12(c). E chooses the strategy\n[p:one;(1\u2212p):two]attheroot,andthenOchoosesapurestrategy(andhenceamove)\ngiventhevalueofp. IfOchoosesone,theexpectedpayoff(toE)is2p\u22123(1\u2212p)=5p\u2212\n3; if O chooses two, the expected payoff is \u22123p+4(1 \u2212p)=4\u22127p. We can draw\nthesetwopayoffsasstraightlinesonagraph,where prangesfrom0to1onthex-axis,\nasshowninFigure17.12(e). O,theminimizer,willalwayschoosethelowerofthetwo\nlines,asshownbytheheavylinesinthefigure. Therefore,thebestthatE candoatthe\nrootistochoose ptobeattheintersection point, whichiswhere\n5p\u22123 = 4\u22127p \u21d2 p = 7\/12.\nTheutilityforE atthispointisU = \u22121\/12.\nE,O\n\u2022 If O moves first, the situation is as shown in Figure 17.12(d). O chooses the strategy\n[q:one;(1\u2212q):two]attheroot, andthen E chooses amovegiventhevalueofq. The\npayoffsare2q\u22123(1\u2212q)=5q\u22123and\u22123q+4(1\u2212q)=4\u22127q.7 Again,Figure17.12(f)\nshowsthatthebestO candoattherootistochoosetheintersection point:\n5q\u22123 = 4\u22127q \u21d2 q = 7\/12.\nTheutilityforE atthispointisU = \u22121\/12.\nO,E\nNowweknow that thetrue utility ofthe original gameliesbetween \u22121\/12 and \u22121\/12, that\nis, it is exactly \u22121\/12! (The moral is that it is better to be O than E if you are playing this\ngame.) Furthermore, thetrueutility isattained bythemixedstrategy [7\/12:one;5\/12:two],\nMAXIMIN whichshouldbeplayedbybothplayers. Thisstrategyiscalledthemaximinequilibriumof\nEQUILIBRIUM\nthe game, and is a Nash equilibrium. Note that each component strategy in an equilibrium\nmixed strategy has the same expected utility. In this case, both one and two have the same\nexpectedutility, \u22121\/12,asthemixedstrategyitself.\nOur result for two-finger Morra is an example of the general result by von Neumann:\neverytwo-playerzero-sumgamehasamaximinequilibriumwhenyouallowmixedstrategies.\nFurthermore, every Nash equilibrium in a zero-sum game is a maximin for both players. A\nplayer who adopts the maximin strategy has two guarantees: First, no other strategy can do\nbetteragainst anopponent whoplayswell(although someotherstrategies mightbebetterat\nexploiting an opponent who makes irrational mistakes). Second, the player continues to do\njustaswellevenifthestrategyisrevealedtotheopponent.\nThe general algorithm for finding maximin equilibria in zero-sum games is somewhat\nmoreinvolvedthanFigures17.12(e)and(f)mightsuggest. Whentherearenpossibleactions,\na mixed strategy is a point in n-dimensional space and the lines become hyperplanes. It\u2019s\nalso possible for some pure strategies for the second player to be dominated by others, so\nthat they are not optimal against any strategy for the first player. After removing all such\nstrategies (which might have to be done repeatedly), the optimal choice at the root is the\n7 It is a coincidence that these equations are the same as those for p; the coincidence arises because\nUE(one,two)=UE(two,one)= \u22123.Thisalsoexplainswhytheoptimalstrategyisthesameforbothplayers. Section17.5. DecisionswithMultipleAgents: GameTheory 673\nhighest (or lowest) intersection point of the remaining hyperplanes. Finding this choice is\nanexampleofalinearprogrammingproblem: maximizinganobjective function subjectto\nlinear constraints. Such problems can be solved by standard techniques in time polynomial\ninthenumberofactions(andinthenumberofbitsusedtospecifytherewardfunction,ifyou\nwanttogettechnical).\nThequestionremains,whatshouldarationalagentactually doinplayingasinglegame\nof Morra? The rational agent will have derived the fact that [7\/12:one;5\/12:two] is the\nmaximin equilibrium strategy, and willassume that thisismutual knowledge witharational\nopponent. Theagentcouldusea12-sideddieorarandomnumbergeneratortopickrandomly\naccordingtothismixedstrategy, inwhichcasetheexpected payoffwouldbe-1\/12forE. Or\nthe agent could just decide to play one, or two. In either case, the expected payoff remains\n-1\/12forE. Curiously,unilaterallychoosingaparticularactiondoesnotharmone\u2019sexpected\npayoff,butallowingtheotheragenttoknowthatonehasmadesuchaunilateraldecisiondoes\naffecttheexpected payoff,because thentheopponent canadjusthisstrategy accordingly.\nFindingequilibria innon-zero-sum gamesissomewhatmorecomplicated. Thegeneral\napproach hastwosteps: (1)Enumerateallpossible subsets ofactions thatmightformmixed\nstrategies. Forexample, first try all strategy profiles where each player uses a single action,\nthen those where each player uses either one or two actions, and so on. This is exponential\ninthenumberofactions, andsoonlyappliestorelatively smallgames. (2)Foreachstrategy\nprofileenumeratedin(1),checktoseeifitisanequilibrium. Thisisdonebysolvingasetof\nequationsandinequalitiesthataresimilartotheonesusedinthezero-sumcase. Fortwoplay-\ners these equations are linear and can be solved with basic linear programming techniques,\nbutforthreeormoreplayers theyarenonlinearandmaybeverydifficulttosolve.\n17.5.2 Repeated games\nSofarwehave looked only atgames that last a single move. The simplest kind of multiple-\nmovegameistherepeatedgame,inwhichplayersfacethesamechoicerepeatedly, buteach\nREPEATEDGAME\ntime with knowledge of the history of all players\u2019 previous choices. A strategy profile for a\nrepeated gamespecifies anaction choice foreach playerateachtimestep foreverypossible\nhistoryofpreviouschoices. AswithMDPs,payoffsareadditiveovertime.\nLet\u2019sconsidertherepeatedversionoftheprisoner\u2019sdilemma. WillAliceandBobwork\ntogether and refuse to testify, knowing they will meet again? The answer depends on the\ndetails of the engagement. For example, suppose Alice and Bob know that they must play\nexactly100roundsofprisoner\u2019sdilemma. Thentheybothknowthatthe100throundwillnot\nbearepeated game\u2014thatis,itsoutcomecanhavenoeffect onfuture rounds\u2014and therefore\ntheywillbothchoose thedominant strategy, testify,inthatround. Butoncethe100thround\nis determined, the 99th round can have no effect on subsequent rounds, so it too will have\nadominant strategy equilibrium at (testify,testify). By induction, both players willchoose\ntestify oneveryround,earning atotaljailsentence of500yearseach.\nWe can get different solutions by changing the rules of the interaction. For example,\nsuppose that after each round there is a 99% chance that the players will meet again. Then\nthe expected number of rounds is still 100, but neither player knows for sure which round 674 Chapter 17. MakingComplexDecisions\nwillbethelast. Undertheseconditions, morecooperative behaviorispossible. Forexample,\none equilibrium strategy is foreach player to refuse unless the other player has ever played\nPERPETUAL testify. This strategy could be called perpetual punishment. Suppose both players have\nPUNISHMENT\nadoptedthisstrategy,andthisismutualknowledge. Thenaslongasneitherplayerhasplayed\ntestify,thenatanypointintimetheexpected futuretotalpayoffforeachplayeris\n(cid:12)\u221e\n0.99t\u00b7(\u22121) = \u2212100.\nt=0\nAplayerwhodeviatesfromthestrategyandchooses testify willgainascoreof0ratherthan\n\u22121 on the very next move, but from then on both players will play testify and the player\u2019s\ntotalexpected futurepayoffbecomes\n(cid:12)\u221e\n0+ 0.99t\u00b7(\u22125) = \u2212495.\nt=1\nTherefore, at every step, there is no incentive to deviate from (refuse,refuse). Perpetual\npunishment is the \u201cmutually assured destruction\u201d strategy of the prisoner\u2019s dilemma: once\neither player decides to testify, it ensures that both players suffer a great deal. But it works\nasadeterrentonlyiftheotherplayerbelievesyouhaveadoptedthisstrategy\u2014oratleastthat\nyoumighthaveadopted it.\nOtherstrategies aremoreforgiving. Themostfamous, calledtit-for-tat, callsforstart-\nTIT-FOR-TAT\ningwithrefuse andthenechoing theotherplayer\u2019s previous moveonallsubsequent moves.\nSoAlicewouldrefuse aslong asBobrefuses andwouldtestify themoveafterBobtestified,\nbut would go back to refusing if Bob did. Although very simple, this strategy has proven to\nbehighlyrobustandeffectiveagainstawidevarietyofstrategies.\nWe can also get different solutions by changing the agents, rather than changing the\nrules of engagement. Suppose the agents are finite-state machines with n states and they\nare playing a game with m > n total steps. The agents are thus incapable of representing\nthe number of remaining steps, and must treat it as an unknown. Therefore, they cannot do\nthe induction, and are free to arrive at the more favorable (refuse, refuse) equilibrium. In\nthiscase,ignorance isbliss\u2014orrather, havingyouropponent believethatyouareignorantis\nbliss. Yoursuccess in these repeated games depends onthe other player\u2019s perception of you\nasabullyorasimpleton, andnotonyouractualcharacteristics.\n17.5.3 Sequential games\nInthegeneralcase,agameconsistsofasequenceofturnsthatneednotbeallthesame. Such\ngamesarebestrepresentedbyagametree,whichgametheoristscalltheextensiveform. The\nEXTENSIVEFORM\ntree includes all the same information we saw in Section 5.1: an initial state S , a function\n0\nPLAYER(s) that tells which player has the move, a function ACTIONS(s) enumerating the\npossible actions, a function RESULT(s,a) that defines the transition to a new state, and a\npartial function UTILITY(s,p), which is defined only on terminal states, to give the payoff\nforeachplayer.\nTo represent stochastic games, such as backgammon, we add a distinguished player,\nchance, that can take random actions. Chance\u2019s \u201cstrategy\u201d is part of the definition of the Section17.5. DecisionswithMultipleAgents: GameTheory 675\ngame, specified as a probability distribution over actions (the other players get to choose\ntheir own strategy). To represent games with nondeterministic actions, such as billiards, we\nbreaktheactionintotwopieces: theplayer\u2019s actionitself hasadeterministic result, andthen\nchance hasaturntoreacttotheaction initsowncapricious way. Torepresent simultaneous\nmoves,asintheprisoner\u2019sdilemmaortwo-fingerMorra,weimposeanarbitraryorderonthe\nplayers,butwehavetheoptionofassertingthattheearlierplayer\u2019sactionsarenotobservable\nto the subsequent players: e.g., Alice must choose refuse or testify first, then Bob chooses,\nbut Bob does not know what choice Alice made at that time (we can also represent the fact\nthat the moveis revealed later). However, we assume the players always remember all their\nownprevious actions;thisassumption iscalled perfectrecall.\nThe key idea of extensive form that sets it apart from the game trees of Chapter 5 is\nthe representation of partial observability. We saw in Section 5.6 that a player in a partially\nobservable game such as Kriegspiel can create a game tree over the space of belief states.\nWith that tree, wesaw that in somecases aplayer can findasequence of moves (astrategy)\nthatleadstoaforcedcheckmateregardlessofwhatactualstatewestartedin,andregardlessof\nwhatstrategytheopponentuses. However,thetechniquesofChapter5couldnottellaplayer\nwhat to do when there is no guaranteed checkmate. If the player\u2019s best strategy depends\non the opponent\u2019s strategy and vice versa, then minimax (or alpha\u2013beta) by itself cannot\nfind a solution. The extensive form does allow us to find solutions because it represents the\nbelief states (game theorists call them information sets) of all players at once. From that\nINFORMATIONSETS\nrepresentation wecanfindequilibrium solutions, justaswe didwithnormal-form games.\nAsasimple example ofasequential game, place twoagents inthe4\u00d73world ofFig-\nure17.1andhavethemmovesimultaneously untiloneagentreachesanexitsquare,andgets\nthe payoff for that square. If we specify that no movement occurs when the two agents try\nto move into the same square simultaneously (a common problem at many traffic intersec-\ntions), then certain pure strategies can get stuck forever. Thus, agents need a mixedstrategy\ntoperform wellinthisgame: randomly choosebetweenmoving aheadandstaying put. This\nisexactlywhatisdonetoresolvepacketcollisions inEthernetnetworks.\nNext we\u2019ll consider a very simple variant of poker. The deck has only four cards, two\naces and two kings. One card is dealt to each player. The first player then has the option\nto raise the stakes of the game from 1 point to 2, or to check. If player 1 checks, the game\nis over. If he raises, then player 2 has the option to call, accepting that the game is worth 2\npoints, or fold, conceding the 1 point. If the game does not end with a fold, then the payoff\ndepends on the cards: it is zero for both players if they have the same card; otherwise the\nplayerwiththekingpaysthestakestotheplayerwiththeace.\nTheextensive-form treeforthisgameisshowninFigure17.13. Nonterminalstatesare\nshownascircles, withtheplayertomoveinsidethecircle;player0ischance. Eachactionis\ndepictedasanarrowwithalabel,correspondingtoaraise,check,call,orfold,or,forchance,\nthefourpossible deals(\u201cAK\u201dmeansthatplayer1getsanaceandplayer2aking). Terminal\nstates are rectangles labeled by their payoff to player 1 and player 2. Information sets are\nshown as labeled dashed boxes; for example, I is the information set where it is player\n1,1\n1\u2019s turn, and he knows he has an ace (but does not know what player 2 has). In information\nset I , it is player 2\u2019s turn and she knows that she has an ace and that player 1 has raised,\n2,1 676 Chapter 17. MakingComplexDecisions\nr c\n1 2 0,0!\nf\nk\nI I +1,-1!\n1,1 0,0! 2,1\n1\/6: AA\nr c\n1 2 +2,-2\nf\nk\n1\/3: AK\nI +1,-1!\n0 +1,-1! 2,2\n1\/6: KK\nr c\n1 2 0,0\nf\nk\n1\/3: KA\nI\n1,2\n0,0!\nI\n+1,-1!\n2,1\nr c\n1 22 -2,+2\nf\nk\n-1,+1! +1,-1!\nFigure17.13 Extensiveformofasimplifiedversionofpoker.\nbut does not know what card player 1 has. (Due to the limits of two-dimensional paper, this\ninformation setisshownastwoboxesratherthanone.)\nOnewaytosolveanextensivegameistoconvertittoanormal-form game. Recallthat\nthenormalformisamatrix,eachrowofwhichislabeledwithapurestrategyforplayer1,and\neach column byapure strategy forplayer 2. Inan extensive gameapure strategy forplayer\nicorresponds toanaction foreachinformation setinvolving thatplayer. SoinFigure17.13,\none pure strategy forplayer 1 is \u201craise when in I (that is, when I have an ace), and check\n1,1\nwhen in I (when I have a king).\u201d In the payoff matrix below, this strategy is called rk.\n1,2\nSimilarly, strategy cf for player 2 means \u201ccall when I have an ace and fold when I have a\nking.\u201d Since this is a zero-sum game, the matrix below gives only the payoff for player 1;\nplayer2alwayshastheopposite payoff:\n2:cc 2:cf 2:ff 2:fc\n1:rr 0 -1\/6 1 7\/6\n1:kr -1\/3 -1\/6 5\/6 2\/3\n1:rk 1\/3 0 1\/6 1\/2\n1:kk 0 0 0 0\nThis game is so simple that it has two pure-strategy equilibria, shown in bold: cf for player\n2 and rk or kk for player 1. But in general we can solve extensive games by converting\nto normal form and then finding a solution (usually a mixed strategy) using standard linear\nprogramming methods. That works in theory. But if a player has I information sets and\na actions per set, then that player will have aI pure strategies. In other words, the size of\nthe normal-form matrix is exponential in the number of information sets, so in practice the Section17.5. DecisionswithMultipleAgents: GameTheory 677\napproach works only forvery small game trees, on the order of a dozen states. A game like\nTexashold\u2019em pokerhasabout1018 states,makingthisapproach completelyinfeasible.\nWhat are the alternatives? In Chapter 5 we saw how alpha\u2013beta search could handle\ngames of perfect information with huge game trees by generating the tree incrementally, by\npruningsomebranches,andbyheuristicallyevaluatingnonterminalnodes. Butthatapproach\ndoes not work well forgames with imperfect information, for two reasons: first, it is harder\ntoprune,becauseweneedtoconsidermixedstrategiesthatcombinemultiplebranches,nota\npurestrategythatalwayschoosesthebestbranch. Second,itishardertoheuristicallyevaluate\nanonterminal node,because wearedealing withinformation sets,notindividual states.\nKoller et al. (1996) come to the rescue with an alternative representation of extensive\ngames, called the sequence form, that is only linear in the size of the tree, rather than ex-\nSEQUENCEFORM\nponential. Rather than represent strategies, it represents paths through the tree; the number\nof paths is equal to the number of terminal nodes. Standard linear programming methods\ncan again be applied to this representation. The resulting system can solve poker variants\nwith 25,000 states in a minute ortwo. This is an exponential speedup overthe normal-form\napproach, butstillfallsfarshortofhandling fullpoker, with1018 states.\nIf we can\u2019t handle 1018 states, perhaps we can simplify the problem by changing the\ngametoasimplerform. Forexample,ifIholdanaceandamconsidering thepossibility that\nthenextcardwillgivemeapairofaces,thenIdon\u2019tcareaboutthesuitofthenextcard;any\nsuit will do equally well. This suggests forming an abstraction of the game, one in which\nABSTRACTION\nsuits are ignored. The resulting game tree will be smaller by a factor of 4!=24. Suppose I\ncan solve this smaller game; how will the solution to that game relate to the original game?\nIfnoplayerisgoingforaflush(orbluffingso),thenthesuits don\u2019t mattertoanyplayer, and\nthesolution fortheabstraction willalsobeasolution fortheoriginal game. However, ifany\nplayeriscontemplatingaflush,thentheabstractionwillbeonlyanapproximatesolution(but\nitispossibletocomputeboundsontheerror).\nTherearemanyopportunitiesforabstraction. Forexample, atthepointinagamewhere\neach player has two cards, if I hold a pair of queens, then the other players\u2019 hands could be\nabstracted into three classes: better (only a pair of kings or a pair of aces), same (pair of\nqueens) or worse (everything else). However, this abstraction might be too coarse. A better\nabstraction would divide worse into, say, medium pair (nines through jacks), low pair, and\nnopair. Theseexamples areabstractions ofstates; itisalso possible toabstract actions. For\nexample,instead ofhavingabetactionforeachintegerfrom 1to1000,wecouldrestrict the\nbets to 100, 101, 102 and 103. Or we could cut out one of the rounds of betting altogether.\nWe can also abstract over chance nodes, by considering only a subset of the possible deals.\nThisisequivalenttotherollouttechniqueusedinGoprograms. Puttingalltheseabstractions\ntogether, we can reduce the 1018 states of poker to 107 states, a size that can be solved with\ncurrenttechniques.\nPokerprograms basedonthisapproach caneasily defeatnoviceandsomeexperienced\nhuman players, but are not yet at the level of master players. Part of the problem is that\nthesolution theseprograms approximate\u2014the equilibrium solution\u2014is optimalonlyagainst\nan opponent who also plays the equilibrium strategy. Against fallible human players it is\nimportant to be able to exploit an opponent\u2019s deviation from the equilibrium strategy. As 678 Chapter 17. MakingComplexDecisions\nGautamRao(aka\u201cTheCount\u201d),theworld\u2019sleadingonlinepokerplayer,said(Billings etal.,\n2003), \u201cYou have a very strong program. Once you add opponent modeling to it, it will kill\neveryone.\u201d However,goodmodelsofhumanfallability remainelusive.\nInasense,extensivegameformistheoneofthemostcompleterepresentationswehave\nseen so far: it can handle partially observable, multiagent, stochastic, sequential, dynamic\nenvironments\u2014most of the hard cases from the list of environment properties on page 42.\nHowever,therearetwolimitationsofgametheory. First,itdoesnotdealwellwithcontinuous\nstates and actions (although there have been some extensions to the continuous case; for\nCOURNOT example,thetheoryofCournotcompetitionusesgametheorytosolveproblemswheretwo\nCOMPETITION\ncompanies choose prices for their products from a continuous space). Second, game theory\nassumes the gameis known. Partsof the gamemay be specified asunobservable tosome of\ntheplayers, butitmustbeknown whatparts areunobservable. Incases inwhichtheplayers\nlearn the unknown structure of the game over time, the model begins to break down. Let\u2019s\nexamineeachsourceofuncertainty, andwhethereachcanberepresented ingametheory.\nActions: There is no easy way to represent a game where the players have to discover\nwhat actions are available. Consider the game between computer virus writers and security\nexperts. Partoftheproblem isanticipating whatactionthe viruswriterswilltrynext.\nStrategies: Game theory is very good at representing the idea that the other players\u2019\nstrategies are initially unknown\u2014as long as we assume all agents are rational. The theory\nitself does not say whatto do when the other players are less than fully rational. The notion\nBAYES\u2013NASH ofaBayes\u2013Nashequilibriumpartiallyaddressesthispoint: itisanequilibrium withrespect\nEQUILIBRIUM\ntoaplayer\u2019s priorprobability distribution overtheother players\u2019 strategies\u2014in otherwords,\nitexpresses aplayer\u2019s beliefsabouttheotherplayers\u2019likelystrategies.\nChance: Ifagamedependsontherollofadie,itiseasyenoughtomodelachancenode\nwithuniform distribution overtheoutcomes. Butwhatifitis possible that thedie isunfair?\nWecanrepresent thatwithanotherchance node, higherupinthetree,withtwobranches for\n\u201cdie is fair\u201d and \u201cdie is unfair,\u201d such that the corresponding nodes in each branch are in the\nsameinformation set(thatis,theplayersdon\u2019tknowifthedieisfairornot). Andwhatifwe\nsuspect the other opponent does know? Thenweadd another chance node, with one branch\nrepresenting thecasewheretheopponent doesknow,andonewherehedoesn\u2019t.\nUtilities: What if wedon\u2019t know ouropponent\u2019s utilities? Again, that can be modeled\nwith a chance node, such that the other agent knows its own utilities in each branch, but we\ndon\u2019t. But what if we don\u2019t know our own utilities? For example, how do I know if it is\nrational toordertheChef\u2019ssaladifIdon\u2019tknowhowmuchIwilllikeit? Wecanmodelthat\nwithyetanotherchancenodespecifying anunobservable \u201cintrinsic quality\u201d ofthesalad.\nThus,weseethatgametheoryisgoodatrepresentingmostsourcesofuncertainty\u2014but\nat the cost of doubling the size of the tree every time we add another node; a habit which\nquickly leads to intractably large trees. Because of these and other problems, game theory\nhasbeenusedprimarilytoanalyzeenvironmentsthatareatequilibrium,ratherthantocontrol\nagentswithinanenvironment. Nextweshallseehowitcanhelpdesignenvironments. Section17.6. Mechanism Design 679\n17.6 MECHANISM DESIGN\nIn the previous section, we asked, \u201cGiven a game, what is a rational strategy?\u201d In this sec-\ntion,weask,\u201cGiventhatagentspickrationalstrategies,whatgameshouldwedesign?\u201d More\nspecifically,wewouldliketodesignagamewhosesolutions, consistingofeachagentpursu-\ningitsownrational strategy, result inthemaximization of someglobal utility function. This\nproblem is called mechanism design, or sometimes inverse game theory. Mechanism de-\nMECHANISMDESIGN\nsignisastapleofeconomics andpolitical science. Capitalism 101saysthatifeveryonetries\nto get rich, the total wealth of society will increase. But the examples we will discuss show\nthatpropermechanismdesignisnecessarytokeeptheinvisiblehandontrack. Forcollections\nofagents,mechanismdesignallowsustoconstructsmartsystemsoutofacollectionofmore\nlimitedsystems\u2014evenuncooperative systems\u2014inmuchthesamewaythatteamsofhumans\ncanachievegoalsbeyondthereachofanyindividual.\nExamples of mechanism design include auctioning off cheap airline tickets, routing\nTCPpacketsbetweencomputers, decidinghowmedicalinternswillbeassignedtohospitals,\nand deciding how robotic soccer players will cooperate with their teammates. Mechanism\ndesign becamemorethananacademic subject inthe1990swhen severalnations, facedwith\ntheproblemofauctioning offlicensestobroadcastinvariousfrequencybands,losthundreds\nof millions of dollars in potential revenue as a result of poor mechanism design. Formally,\na mechanism consists of (1) a language for describing the set of allowable strategies that\nMECHANISM\nagentsmayadopt,(2)adistinguishedagent,calledthecenter,thatcollectsreportsofstrategy\nCENTER\nchoices from the agents in the game, and (3) an outcome rule, known to all agents, that the\ncenterusestodeterminethepayoffstoeachagent, giventheirstrategychoices.\n17.6.1 Auctions\nLet\u2019sconsider auctionsfirst. Anauction isamechanism forselling somegoods tomembers\nAUCTION\nof a pool of bidders. For simplicity, we concentrate on auctions with a single item for sale.\nEach bidder i has a utility value v for having the item. In some cases, each bidder has a\ni\nprivate value for the item. For example, the first item sold on eBay was a broken laser\npointer, whichsoldfor$14.83toacollectorofbrokenlaser pointers. Thus,weknowthatthe\ncollector has v \u2265 $14.83, but most other people would have v * $14.83. In other cases,\ni j\nsuch as auctioning drilling rights for an oil tract, the item has a common value\u2014the tract\nwillproduce someamount ofmoney, X, and all bidders value adollarequally\u2014but there is\nuncertainty astowhattheactual valueof X is. Different bidders havedifferent information,\nandhencedifferentestimatesoftheitem\u2019struevalue. Ineithercase,biddersendupwiththeir\nown v . Given v , each bidder gets a chance, at the appropriate time ortimes in the auction,\ni i\nto make a bid b . The highest bid, b wins the item, but the price paid need not be b ;\ni max max\nthat\u2019spartofthemechanism design.\nThe best-known auction mechanism is the ascending-bid,8 or English auction, in\nASCENDING-BID\nwhich the center starts by asking for a minimum (or reserve) bid b . If some bidder is\nENGLISHAUCTION min\n8 Theword\u201cauction\u201dcomesfromtheLatinaugere,toincrease. 680 Chapter 17. MakingComplexDecisions\nwilling to pay that amount, the center then asks for b + d, for some increment d, and\nmin\ncontinues up from there. The auction ends when nobody is willing to bid anymore; then the\nlastbidderwinstheitem,paying thepricehebid.\nHow do we know if this is a good mechanism? One goal is to maximize expected\nrevenue for the seller. Another goal is to maximize a notion of global utility. These goals\noverlap tosome extent, because one aspect ofmaximizing global utility isto ensure that the\nwinner of the auction is the agent who values the item the most (and thus is willing to pay\nthemost). Wesayanauction isefficientifthe goods gototheagent whovalues them most.\nEFFICIENT\nTheascending-bidauctionisusuallybothefficientandrevenuemaximizing,butifthereserve\nprice is set too high, the bidder who values it most may not bid, and if the reserve is set too\nlow,thesellerlosesnetrevenue.\nProbably the most important things that an auction mechanism can do is encourage a\nsufficient numberofbidders toenterthe gameand discourage them from engaging in collu-\nsion. Collusionisanunfairorillegalagreementbytwoormorebidderstomanipulateprices.\nCOLLUSION\nItcanhappeninsecretbackroom dealsortacitly, withinthe rulesofthemechanism.\nFor example, in 1999, Germany auctioned ten blocks of cell-phone spectrum with a\nsimultaneous auction (bidsweretakenonalltenblocksatthesametime),usingtherulethat\nanybidmustbeaminimumofa10%raiseoverthepreviousbidonablock. Therewereonly\ntwo credible bidders, and the first, Mannesman, entered the bid of 20 million deutschmark\nonblocks1-5and18.18milliononblocks6-10. Why18.18M?OneofT-Mobile\u2019smanagers\nsaid they \u201cinterpreted Mannesman\u2019s first bid as an offer.\u201d Both parties could compute that\na 10% raise on 18.18M is 19.99M; thus Mannesman\u2019s bid was interpreted as saying \u201cwe\ncan each get half the blocks for 20M; let\u2019s not spoil it by bidding the prices up higher.\u201d\nAnd in fact T-Mobile bid 20M on blocks 6-10 and that was the end of the bidding. The\nGerman government got less than they expected, because the two competitors were able to\nuse the bidding mechanism to come to a tacit agreement on how not to compete. From\nthe government\u2019s point of view, a better result could have been obtained by any of these\nchanges to the mechanism: a higher reserve price; a sealed-bid first-price auction, so that\nthe competitors could not communicate through their bids; or incentives to bring in a third\nbidder. Perhaps the 10% rule was an error in mechanism design, because it facilitated the\nprecisesignaling fromMannesmantoT-Mobile.\nIn general, both the seller and the global utility function benefit if there are more bid-\nders, although global utility can suffer if you count the cost of wasted time of bidders that\nhave no chance of winning. One way to encourage more bidders is to make the mechanism\neasier for them. After all, if it requires too much research orcomputation on the part of the\nbidders, they may decide to take their money elsewhere. So it is desirable that the bidders\nhaveadominantstrategy. Recallthat \u201cdominant\u201d meansthatthestrategy worksagainst all\nother strategies, which in turn means that an agent can adopt it without regard for the other\nstrategies. Anagentwithadominantstrategy canjustbid,without wastingtimecontemplat-\ning other agents\u2019 possible strategies. A mechanism where agents have a dominant strategy\nis called a strategy-proof mechanism. If, as is usually the case, that strategy involves the\nSTRATEGY-PROOF\nbiddersrevealingtheirtruevalue, v ,thenitiscalledatruth-revealing, ortruthful,auction;\nTRUTH-REVEALING i\nREVELATION theterm incentivecompatibleisalsoused. Therevelation principlestatesthatanymecha-\nPRINCIPLE Section17.6. Mechanism Design 681\nnismcanbetransformedintoanequivalenttruth-revealing mechanism,sopartofmechanism\ndesignisfindingtheseequivalent mechanisms.\nIt turns out that the ascending-bid auction has most of the desirable properties. The\nbidder with the highest value v gets the goods at a price of b +d, where b is the highest\ni o o\nbid among all the other agents and d is the auctioneer\u2019s increment.9 Bidders have a simple\ndominantstrategy: keepbiddingaslongasthecurrentcostisbelowyourv . Themechanism\ni\nisnotquitetruth-revealing, because thewinningbidderrevealsonlythathisv \u2265 b +d;we\ni o\nhavealowerboundonv butnotanexactamount.\ni\nA disadvantage (from the point of view of the seller) of the ascending-bid auction is\nthat it can discourage competition. Suppose that in a bid for cell-phone spectrum there is\none advantaged company that everyone agrees would be able to leverage existing customers\nand infrastructure, and thus can make alarger profit than anyone else. Potential competitors\ncan see that they have no chance in an ascending-bid auction, because the advantaged com-\npany can always bid higher. Thus, the competitors may not enter at all, and the advantaged\ncompanyendsupwinningatthereserveprice.\nAnothernegativepropertyoftheEnglishauctionisitshighcommunicationcosts. Either\ntheauction takesplaceinoneroomorallbiddershavetohave high-speed, securecommuni-\ncationlines;ineithercasetheyhavetohavethetimeavailabletogothroughseveralroundsof\nbidding. Analternative mechanism,whichrequires muchlesscommunication, isthesealed-\nSEALED-BID bidauction. Eachbiddermakes asingle bidandcommunicates ittotheauctioneer, without\nAUCTION\ntheotherbidders seeing it. Withthismechanism, thereisno longer asimple dominant strat-\negy. If your value is v and you believe that the maximum of all the other agents\u2019 bids will\ni\nbe b , then you should bid b + (cid:2), for some small (cid:2), if that is less than v . Thus, your bid\no o i\ndepends on your estimation of the other agents\u2019 bids, requiring you to do more work. Also,\nnote that the agent with the highest v might not win the auction. This is offset by the fact\ni\nthattheauctionismorecompetitive, reducing thebiastowardanadvantaged bidder.\nA small change in the mechanism for sealed-bid auctions produces the sealed-bid\nSEALED-BID second-priceauction,alsoknownasaVickreyauction.10 Insuchauctions,thewinnerpays\nSECOND-PRICE\nAUCTION\nthe price of the second-highest bid, b , rather than paying his own bid. This simple modifi-\nVICKREYAUCTION o\ncation completely eliminates thecomplexdeliberations required forstandard (or first-price)\nsealed-bidauctions, becausethedominantstrategyisnowsimplytobidv ;themechanismis\ni\ntruth-revealing. Notethattheutilityofagentiintermsofhisbidb ,hisvaluev ,andthebest\ni i\nbidamongtheotheragents, b ,is\no\n(cid:24)\n(v \u2212b ) ifb > b\nu = i o i o\ni 0 otherwise.\nTo see that b = v is a dominant strategy, note that when (v \u2212 b ) is positive, any bid\ni i i o\nthat wins the auction is optimal, and bidding v in particular wins the auction. On the other\ni\nhand, when (v \u2212b )isnegative, anybid that loses the auction isoptimal, and bidding v in\ni o i\n9 Thereisactuallyasmallchancethattheagentwithhighestvifailstogetthegoods,inthecaseinwhich\nbo <vi <bo+d.Thechanceofthiscanbemadearbitrarilysmallbydecreasingtheincrementd.\n10 NamedafterWilliamVickrey(1914\u20131996), whowonthe1996NobelPrizeineconomicsforthisworkand\ndiedofaheartattackthreedayslater. 682 Chapter 17. MakingComplexDecisions\nparticularlosestheauction. Sobidding v isoptimalforallpossiblevaluesofb ,andinfact,\ni o\nv istheonlybidthathasthisproperty. Becauseofitssimplicityandtheminimalcomputation\ni\nrequirements for both seller and bidders, the Vickrey auction is widely used in constructing\ndistributed AI systems. Also, Internet search engines conduct over a billion auctions a day\nto sell advertisements along with their search results, and online auction sites handle $100\nbillionayearingoods,allusingvariantsoftheVickreyauction. Notethattheexpectedvalue\nto the seller is b , which is the same expected return as the limit of the English auction as\no\ntheincrement dgoestozero. Thisisactually averygeneral result: the revenueequivalence\nREVENUE\ntheorem states that, with a few minor caveats, any auction mechanism where risk-neutral\nEQUIVALENCE\nTHEOREM\nbidders have values v known only to themselves (but know a probability distribution from\ni\nwhichthosevaluesaresampled),willyieldthesameexpectedrevenue. Thisprinciplemeans\nthatthevariousmechanismsarenotcompetingonthebasisofrevenuegeneration, butrather\nonotherqualities.\nAlthoughthesecond-priceauctionistruth-revealing, itturnsoutthatextendingtheidea\ntomultiplegoods andusinganext-price auction isnottruth-revealing. ManyInternet search\nengines use a mechanism where they auction k slots for ads on a page. The highest bidder\nwins the top spot, the second highest gets the second spot, and so on. Each winner pays the\nprice bid by the next-lower bidder, with the understanding that payment is made only if the\nsearcher actually clicks on the ad. The top slots are considered more valuable because they\nare more likely to be noticed and clicked on. Imagine that three bidders, b ,b and b , have\n1 2 3\nvaluations foraclick of v =200,v =180, and v =100, and thatk = 2slots are available,\n1 2 3\nwhere it is known that the top spot is clicked on 5% of the time and the bottom spot 2%. If\nall bidders bid truthfully, then b wins the top slot and pays 180, and has an expected return\n1\nof(200\u2212180)\u00d70.05=1. Thesecondslotgoestob . Butb canseethatifsheweretobid\n2 1\nanythingintherange101\u2013179,shewouldconcedethetopslottob ,winthesecondslot,and\n2\nyieldanexpectedreturnof(200\u2212100)\u00d7.02=2. Thus,b candoubleherexpectedreturnby\n1\nbiddinglessthanhertruevalueinthiscase. Ingeneral,biddersinthismultislotauctionmust\nspendalotofenergyanalyzing thebidsofotherstodeterminetheirbeststrategy; thereisno\nsimpledominantstrategy. Aggarwal etal.(2006)showthatthereisauniquetruthfulauction\nmechanism for this multislot problem, in which the winner of slot j pays the full price for\nslot j just for those additional clicks that are available at slot j and not at slot j + 1. The\nwinner pays the price for the lower slot for the remaining clicks. In our example, b would\n1\nbid200truthfully, andwouldpay180fortheadditional .05\u2212.02=.03clicksinthetopslot,\nbut would pay only the cost of the bottom slot, 100, for the remaining .02 clicks. Thus, the\ntotalreturnto b wouldbe(200\u2212180)\u00d7.03+(200\u2212100)\u00d7.02=2.6.\n1\nAnotherexample ofwhere auctions can come into play within AI is whenacollection\nof agents are deciding whether to cooperate on a joint plan. Hunsberger and Grosz (2000)\nshow that this can be accomplished efficiently with an auction in which the agents bid for\nrolesinthejointplan. Section17.6. Mechanism Design 683\n17.6.2 Commongoods\nNow let\u2019s consider another type of game, in which countries set their policy for controlling\nairpollution. Eachcountry hasachoice: theycanreduce pollution atacostof-10points for\nimplementing thenecessary changes, ortheycancontinue topollute, whichgivesthemanet\nutility of-5(inadded health costs, etc.) andalsocontributes -1points toeveryothercountry\n(because the air is shared across countries). Clearly, the dominant strategy for each country\nis\u201ccontinue topollute,\u201d butifthereare100countries andeachfollowsthispolicy, theneach\ncountry gets a total utility of -104, whereas if every country reduced pollution, they would\nTRAGEDYOFTHE each have a utility of -10. This situation is called the tragedy of the commons: if nobody\nCOMMONS\nhas to pay for using a common resource, then it tends to be exploited in a way that leads to\na lower total utility for all agents. It is similar to the prisoner\u2019s dilemma: there is another\nsolution to the game that is better for all parties, but there appears to be no way for rational\nagentstoarriveatthatsolution.\nThe standard approach for dealing with the tragedy of the commons is to change the\nmechanism toone that charges each agent forusing the commons. More generally, weneed\nto ensure that all externalities\u2014effects on global utility that are not recognized in the in-\nEXTERNALITIES\ndividual agents\u2019 transactions\u2014are made explicit. Setting the prices correctly is the difficult\npart. In the limit, this approach amounts to creating a mechanism in which each agent is\neffectively requiredtomaximizeglobalutility, butcando sobymakingalocaldecision. For\nthis example, a carbon tax would be an example of a mechanism that charges for use of the\ncommonsinawaythat,ifimplemented well,maximizesglobalutility.\nAsafinalexample,considertheproblemofallocatingsomecommongoods. Supposea\ncitydecidesitwantstoinstallsomefreewirelessInternet transceivers. However,thenumber\noftransceivers theycanaffordislessthanthenumberofneighborhoods thatwantthem. The\ncity wants to allocate the goods efficiently, to the neighborhoods that would value them the\n(cid:2)\nmost. That is, they want to maximize the global utility V = v . The problem is that if\ni i\ntheyjustaskeachneighborhood council\u201chowmuchdoyouvaluethisfreegift?\u201d theywould\nallhaveanincentive tolie,andreportahighvalue. Itturns outthereisamechanism, known\nVICKREY-CLARKE- as the Vickrey-Clarke-Groves, or VCG, mechanism, that makes it a dominant strategy for\nGROVES\neachagent toreport itstrueutility andthatachieves anefficient allocation ofthegoods. The\nVCG\ntrick is that each agent pays a tax equivalent to the loss in global utility that occurs because\noftheagent\u2019s presenceinthegame. Themechanism workslike this:\n1. Thecenteraskseachagenttoreportitsvalueforreceivinganitem. Callthisb .\ni\n2. Thecenterallocatesthegoodstoasubsetofthebidders. WecallthissubsetA,anduse\nthenotation b (A) to meanthe result to iunder this allocation: b if iisin A(that is, i\ni i\nis a winner), and 0 otherwise. The center chooses A to maximize total reported utility\n(cid:2)\nB = b (A).\ni i\n3. The center calculates (for each i) the sum of the reported utilities for all the winners\n(cid:2)\nexcept i. Weusethe notation B\u2212i = j(cid:14)=ib j(A). Thecenteralso computes (foreach\ni)theallocation thatwould maximizetotal global utility ifiwerenotinthegame; call\nthatsumW\u2212i.\n4. EachagentipaysataxequaltoW\u2212i\u2212B\u2212i. 684 Chapter 17. MakingComplexDecisions\nIn this example, the VCG rule means that each winner would pay a tax equal to the highest\nreported value among the losers. That is, if I report my value as5, and that causes someone\nwithvalue2tomissoutonanallocation, thenIpayataxof2. Allwinners should behappy\nbecause theypayataxthatislessthantheirvalue, andalllosersareashappy astheycanbe,\nbecausetheyvaluethegoodslessthantherequiredtax.\nWhy is it that this mechanism is truth-revealing? First, consider the payoff to agent i,\nwhichisthevalueofgettinganitem,minusthetax:\nv i(A)\u2212(W\u2212i\u2212B\u2212i). (17.14)\nHerewedistinguish the agent\u2019s true utility, v , from his reported utility b (but weare trying\ni i\nto show that a dominant strategy is b =v ). Agent i knows that the center will maximize\ni i\nglobalutilityusingthereportedvalues,\n(cid:12) (cid:12)\nb (A) = b (A)+ b (A)\nj i j\nj j(cid:14)=i\nwhereasagentiwantsthecentertomaximize(17.14), whichcanberewritten as\n(cid:12)\nv i(A)+ b j(A)\u2212W\u2212i .\nj(cid:14)=i\nSince agent i cannot affect the value of W\u2212i (it depends only on the other agents), the only\nwayicanmakethecenteroptimizewhatiwantsistoreportthetrueutility, b =v .\ni i\n17.7 SUMMARY\nThischapter showshowtouse knowledge about theworld tomakedecisions even whenthe\noutcomesofanactionareuncertainandtherewardsforactingmightnotbereapeduntilmany\nactionshavepassed. Themainpointsareasfollows:\n\u2022 Sequential decision problems inuncertain environments, also called Markov decision\nprocesses, or MDPs, are defined by a transition model specifying the probabilistic\noutcomesofactionsandarewardfunctionspecifying therewardineachstate.\n\u2022 Theutilityofastatesequence isthesumofalltherewardsoverthesequence, possibly\ndiscounted over time. The solution of an MDP is a policy that associates a decision\nwitheverystate thattheagent mightreach. Anoptimalpolicy maximizes theutility of\nthestatesequences encountered whenitisexecuted.\n\u2022 The utility of a state is the expected utility of the state sequences encountered when\nan optimal policy is executed, starting in that state. The value iteration algorithm for\nsolvingMDPsworksbyiterativelysolvingtheequationsrelatingtheutilityofeachstate\ntothoseofitsneighbors.\n\u2022 Policy iteration alternates between calculating the utilities of states under the current\npolicyandimprovingthecurrentpolicywithrespect tothecurrentutilities.\n\u2022 Partially observable MDPs, or POMDPs, are much more difficult to solve than are\nMDPs. Theycanbesolved byconversion toanMDPinthecontinuous spaceofbelief Bibliographical andHistorical Notes 685\nstates; both value iteration and policy iteration algorithms have been devised. Optimal\nbehavior in POMDPs includes information gathering to reduce uncertainty and there-\nforemakebetterdecisions inthefuture.\n\u2022 A decision-theoretic agent can be constructed for POMDP environments. The agent\nuses a dynamic decision network to represent the transition and sensor models, to\nupdateitsbeliefstate,andtoprojectforwardpossible actionsequences.\n\u2022 Game theory describes rational behavior for agents in situations in which multiple\nagentsinteract simultaneously. Solutions ofgamesare Nashequilibria\u2014strategy pro-\nfilesinwhichnoagenthasanincentivetodeviatefromthespecifiedstrategy.\n\u2022 Mechanism design can be used to set the rules by which agents will interact, in order\nto maximize some global utility through the operation of individually rational agents.\nSometimes, mechanisms exist that achieve this goal without requiring each agent to\nconsiderthechoicesmadebyotheragents.\nWe shall return to the world of MDPs and POMDP in Chapter 21, when we study rein-\nforcement learningmethods that allow anagent toimproveitsbehavior from experience in\nsequential, uncertain environments.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nRichardBellmandevelopedtheideasunderlying themodernapproachtosequential decision\nproblems while working atthe RANDCorporation beginning in 1949. According to his au-\ntobiography (Bellman, 1984), he coined the exciting term \u201cdynamic programming\u201d to hide\nfrom a research-phobic Secretary of Defense, Charles Wilson, the fact that his group was\ndoingmathematics. (Thiscannotbestrictlytrue,becausehisfirstpaperusingtheterm(Bell-\nman,1952)appeared beforeWilsonbecameSecretary ofDefensein1953.) Bellman\u2019sbook,\nDynamicProgramming(1957),gavethenewfieldasolidfoundationandintroducedthebasic\nalgorithmic approaches. Ron Howard\u2019s Ph.D. thesis (1960) introduced policy iteration and\nthe idea of average reward for solving infinite-horizon problems. Several additional results\nwere introduced by Bellman and Dreyfus (1962). Modified policy iteration is due to van\nNunen (1976) and Puterman and Shin (1978). Asynchronous policy iteration was analyzed\nbyWilliamsandBaird(1993),whoalsoprovedthepolicylossboundinEquation(17.9). The\nanalysis of discounting in terms of stationary preferences is due to Koopmans (1972). The\ntexts by Bertsekas (1987), Puterman (1994), and Bertsekas and Tsitsiklis (1996) provide a\nrigorous introduction to sequential decision problems. Papadimitriou and Tsitsiklis (1987)\ndescribe resultsonthecomputational complexity ofMDPs.\nSeminalworkbySutton(1988)andWatkins(1989)onreinforcementlearningmethods\nfor solving MDPs played a significant role in introducing MDPs into the AI community, as\ndid the later survey by Barto et al. (1995). (Earlier work by Werbos (1977) contained many\nsimilar ideas, but was not taken up to the same extent.) The connection between MDPsand\nAIplanning problemswasmadefirstbySvenKoenig(1991), whoshowedhowprobabilistic\nSTRIPS operators provide acompactrepresentation fortransition models(seealsoWellman, 686 Chapter 17. MakingComplexDecisions\n1990b). Work by Dean et al. (1993) and Tash and Russell (1994) attempted to overcome\nthe combinatorics oflarge state spaces by using alimited search horizon and abstract states.\nHeuristics based on the value of information can be used to select areas of the state space\nwherealocalexpansionofthehorizonwillyieldasignificantimprovementindecisionqual-\nity. Agents using this approach can tailor their effort to handle time pressure and generate\nsomeinteresting behaviors suchasusingfamiliar\u201cbeatenpaths\u201dtofindtheirwayaroundthe\nstatespacequicklywithouthavingtorecomputeoptimaldecisions ateachpoint.\nAs one might expect, AI researchers have pushed MDPs in the direction of more ex-\npressive representations that can accommodate much larger problems than the traditional\natomicrepresentations basedontransition matrices. TheuseofadynamicBayesiannetwork\nto represent transition models was an obvious idea, but work on factored MDPs (Boutilier\nFACTOREDMDP\net al., 2000; Koller and Parr, 2000; Guestrin et al., 2003b) extends the idea to structured\nrepresentations ofthevaluefunction withprovable improvements incomplexity. Relational\nMDPs (Boutilier et al., 2001; Guestrin et al., 2003a) go one step further, using structured\nRELATIONALMDP\nrepresentations tohandledomainswithmanyrelatedobjects.\nTheobservationthatapartiallyobservableMDPcanbetransformedintoaregularMDP\noverbelief states isduetoAstrom (1965) andAoki(1965). Thefirstcomplete algorithm for\nthe exact solution of POMDPs\u2014essentially the value iteration algorithm presented in this\nchapter\u2014was proposed by EdwardSondik (1971) in hisPh.D.thesis. (Alater journal paper\nby Smallwood and Sondik (1973) contains some errors, but is more accessible.) Lovejoy\n(1991) surveyed the first twenty-five years of POMDP research, reaching somewhat pes-\nsimistic conclusions about the feasibility of solving large problems. The first significant\ncontribution within AI was the Witness algorithm (Cassandra et al., 1994; Kaelbling et al.,\n1998), an improved version of POMDPvalue iteration. Otheralgorithms soon followed, in-\ncluding anapproach duetoHansen(1998)thatconstructs apolicyincrementally intheform\nofafinite-state automaton. Inthispolicy representation, thebelief statecorresponds directly\nto a particular state in the automaton. More recent work in AI has focused on point-based\nvalue iteration methods that, at each iteration, generate conditional plans and \u03b1-vectors for\na finite set of belief states rather than for the entire belief space. Lovejoy (1991) proposed\nsuch an algorithm for a fixed grid of points, an approach taken also by Bonet (2002). An\ninfluential paper by Pineau et al. (2003) suggested generating reachable points by simulat-\ning trajectories in a somewhat greedy fashion; Spaan and Vlassis (2005) observe that one\nneed generate plans for only a small, randomly selected subset of points to improve on the\nplans from the previous iteration for all points in the set. Current point-based methods\u2014\nsuchaspoint-based policyiteration(Jietal.,2007)\u2014cangeneratenear-optimal solutionsfor\nPOMDPswiththousands ofstates. BecausePOMDPsarePSPACE-hard(Papadimitriou and\nTsitsiklis, 1987), furtherprogress mayrequire takingadvantage ofvariouskinds ofstructure\nwithinafactored representation.\nTheonlineapproach\u2014using look-ahead searchtoselectanactionforthecurrentbelief\nstate\u2014was first examined by Satia and Lave (1973). The use of sampling at chance nodes\nwas explored analytically by Kearns et al. (2000) and Ng and Jordan (2000). The basic\nideas for an agent architecture using dynamic decision networks were proposed by Dean\nand Kanazawa (1989a). ThebookPlanning and Control by Deanand Wellman (1991) goes Bibliographical andHistorical Notes 687\ninto much greater depth, making connections between DBN\/DDN models and the classical\ncontrol literature on filtering. Tatman and Shachter (1990) showed how to apply dynamic\nprogramming algorithms to DDN models. Russell (1998) explains various ways in which\nsuchagentscanbescaledupandidentifiesanumberofopenresearchissues.\nThe roots of game theory can be traced back to proposals made in the 17th century\nby Christiaan Huygens and Gottfried Leibniz to study competitive and cooperative human\ninteractions scientifically and mathematically. Throughout the 19th century, several leading\neconomists created simple mathematical examples to analyze particular examples of com-\npetitive situations. The first formal results in game theory are due to Zermelo (1913) (who\nhad,theyearbefore,suggestedaformofminimaxsearchforgames,albeitanincorrectone).\nEmile Borel (1921) introduced the notion of a mixed strategy. John von Neumann (1928)\nprovedthateverytwo-person, zero-sumgamehasamaximinequilibrium inmixedstrategies\nand a well-defined value. Von Neumann\u2019s collaboration with the economist Oskar Morgen-\nstern led to the publication in 1944 of the Theory of Games and Economic Behavior, the\ndefining book for game theory. Publication of the book was delayed by the wartime paper\nshortage untilamemberoftheRockefellerfamilypersonally subsidized itspublication.\nIn1950,attheageof21,JohnNashpublishedhisideasconcerningequilibriaingeneral\n(non-zero-sum) games. Hisdefinition ofanequilibrium solution, although originating inthe\nwork of Cournot (1838), became known as Nash equilibrium. After a long delay because\noftheschizophrenia hesuffered from 1959 onward, Nashwasawarded theNobelMemorial\nPrizeinEconomics(alongwithReinhartSeltenandJohnHarsanyi)in1994. TheBayes\u2013Nash\nequilibrium is described by Harsanyi (1967) and discussed by Kadane and Larkey (1982).\nSomeissuesintheuseofgametheoryforagentcontrolarecoveredbyBinmore(1982).\nThe prisoner\u2019s dilemma was invented as a classroom exercise by Albert W. Tucker in\n1950(basedonanexamplebyMerrillFloodandMelvinDresher)andiscoveredextensively\nby Axelrod (1985) and Poundstone (1993). Repeated games were introduced by Luce and\nRaiffa(1957), and gamesofpartial information inextensive form byKuhn(1953). Thefirst\npractical algorithm for sequential, partial-information games was developed within AI by\nKolleretal. (1996); the paper by Kollerand Pfeffer(1997) provides a readable introduction\ntothefieldanddescribeaworkingsystemforrepresenting andsolving sequential games.\nThe use of abstraction to reduce a game tree to a size that can be solved with Koller\u2019s\ntechnique is discussed by Billings et al. (2003). Bowling et al. (2008) show how to use\nimportance sampling to get a better estimate of the value of a strategy. Waugh et al. (2009)\nshowthattheabstractionapproachisvulnerabletomakingsystematicerrorsinapproximating\nthe equilibrium solution, meaning that the whole approach is on shaky ground: it works for\nsome games but not others. Korb et al. (1999) experiment with an opponent model in the\nform of a Bayesian network. It plays five-card stud about as well as experienced humans.\n(Zinkevich et al., 2008) show how an approach that minimizes regret can find approximate\nequilibria forabstractions with 1012 states,100timesmorethanpreviousmethods.\nGame theory and MDPs are combined in the theory of Markov games, also called\nstochasticgames(Littman,1994;HuandWellman,1998). Shapley(1953)actuallydescribed\nthe value iteration algorithm independently of Bellman, but his results were not widely ap-\npreciated, perhaps because they were presented in the context of Markov games. Evolu- 688 Chapter 17. MakingComplexDecisions\ntionary game theory (Smith, 1982; Weibull, 1995) looks at strategy drift over time: if your\nopponent\u2019s strategy is changing, how should you react? Textbooks on game theory from\naneconomics point ofview include those byMyerson (1991), Fudenberg and Tirole(1991),\nOsborne(2004),andOsborneandRubinstein(1994);MailathandSamuelson(2006)concen-\ntrateonrepeated games. FromanAIperspective wehaveNisan etal.(2007),Leyton-Brown\nandShoham(2008), andShohamandLeyton-Brown(2009).\nThe2007NobelMemorialPrizeinEconomicswenttoHurwicz,Maskin,andMyerson\n\u201cforhaving laidthefoundations ofmechanism designtheory\u201d (Hurwicz,1973). Thetragedy\nofthecommons,amotivatingproblemforthefield,waspresentedbyHardin(1968). Therev-\nelation principle isdue to Myerson (1986), and the revenue equivalence theorem wasdevel-\noped independently by Myerson (1981) and Riley and Samuelson (1981). Two economists,\nMilgrom(1997)andKlemperer(2002),writeaboutthemultibillion-dollar spectrumauctions\ntheywereinvolved in.\nMechanism designisusedinmultiagent planning (HunsbergerandGrosz,2000;Stone\netal.,2009)andscheduling(Rassentietal.,1982). Varian(1995)givesabriefoverviewwith\nconnectionstothecomputerscienceliterature, andRosenscheinandZlotkin(1994)presenta\nbook-lengthtreatmentwithapplicationstodistributedAI.RelatedworkondistributedAIalso\ngoesunderothernames,includingcollectiveintelligence(TumerandWolpert,2000;Segaran,\n2007) and market-based control (Clearwater, 1996). Since 2001 there has been an annual\nTrading Agents Competition (TAC), in which agents try to make the best profit on a series\nofauctions (Wellmanetal.,2001; Arunachalam andSadeh, 2005). Papersoncomputational\nissuesinauctions oftenappearintheACMConferences onElectronicCommerce.\nEXERCISES\n17.1 For the 4\u00d73 world shown in Figure 17.1, calculate which squares can be reached\nfrom(1,1)bytheactionsequence[Up,Up,Right,Right,Right]andwithwhatprobabilities.\nExplainhowthiscomputationisrelatedtothepredictiontask(seeSection15.2.1)forahidden\nMarkovmodel.\n17.2 SelectaspecificmemberofthesetofpoliciesthatareoptimalforR(s)> 0asshown\ninFigure17.2(b),andcalculatethefractionoftimetheagentspendsineachstate,inthelimit,\nif the policy is executed forever. (Hint: Construct the state-to-state transition probability\nmatrixcorresponding tothepolicyandseeExercise15.2.)\n17.3 Suppose that we define the utility of a state sequence to be the maximum reward ob-\ntainedinanystateinthesequence. Showthatthisutilityfunctiondoesnotresultinstationary\npreferences between state sequences. Is it still possible to define a utility function on states\nsuchthatMEUdecision makinggivesoptimalbehavior?\n17.4 Sometimes MDPsare formulated with a reward function R(s,a) that depends on the\n(cid:2)\nactiontakenorwitharewardfunction R(s,a,s)thatalsodepends ontheoutcomestate.\na. WritetheBellmanequations fortheseformulations. Exercises 689\n(cid:2)\nb. ShowhowanMDPwithrewardfunction R(s,a,s)canbetransformedintoadifferent\nMDP with reward function R(s,a), such that optimal policies in the new MDP corre-\nspondexactlytooptimalpolicies intheoriginal MDP.\nc. Nowdothesametoconvert MDPswithR(s,a)intoMDPswithR(s).\n17.5 Fortheenvironment showninFigure17.1, findallthethreshold values forR(s)such\nthattheoptimalpolicychanges whenthethreshold iscrossed. Youwillneedawaytocalcu-\nlate the optimal policy and its value for fixed R(s). (Hint: Prove that the value of any fixed\npolicyvarieslinearly withR(s).)\n17.6 Equation(17.7)onpage654statesthattheBellmanoperator isacontraction.\na. Showthat,foranyfunctions f andg,\n|maxf(a)\u2212maxg(a)| \u2264 max|f(a)\u2212g(a)|.\na a a\nb. Write out an expression for |(BU \u2212 BU(cid:2) )(s)| and then apply the result from (a) to\ni i\ncompletetheproofthattheBellmanoperatorisacontraction.\n17.7 This exercise considers two-player MDPs that correspond to zero-sum, turn-taking\ngames like those in Chapter 5. Let the players be A and B, and let R(s) be the reward for\nplayerAinstates. (Therewardfor B isalwaysequalandopposite.)\na. LetU (s)betheutilityofstateswhenitisA\u2019sturntomoveins,andletU (s)bethe\nA B\nutilityofstateswhenitisB\u2019sturntomoveins. Allrewardsandutilitiesarecalculated\nfromA\u2019spointofview(justasinaminimaxgametree). WritedownBellmanequations\ndefiningU (s)andU (s).\nA B\nb. Explainhowtodotwo-playervalueiterationwiththeseequations,anddefineasuitable\nterminationcriterion.\nc. Consider the game described in Figure 5.17 on page 197. Draw the state space (rather\nthanthe gametree), showing themovesby Aassolid linesand movesbyB asdashed\nlines. MarkeachstatewithR(s). Youwillfindithelpfultoarrange thestates (s ,s )\nA B\nonatwo-dimensional grid,using s ands as\u201ccoordinates.\u201d\nA B\nd. Nowapplytwo-playervalueiterationtosolvethisgame,andderivetheoptimalpolicy.\n17.8 Considerthe 3\u00d73worldshowninFigure17.14(a). Thetransition modelisthesame\nasinthe4\u00d73Figure17.1: 80%ofthetimetheagentgoesinthedirection itselects; therest\nofthetimeitmovesatrightanglestotheintended direction.\nImplement value iteration for this world for each value of r below. Use discounted\nrewards with a discount factor of 0.99. Show the policy obtained in each case. Explain\nintuitively whythevalueofr leadstoeachpolicy.\na. r = 100\nb. r = \u22123\nc. r = 0\nd. r = +3 690 Chapter 17. MakingComplexDecisions\nr -1 +10 +50 -1 -1 -1 -1 -1 -1 -1\n\u00b7\u00b7\u00b7\n-1 -1 -1 Start\n\u00b7\u00b7\u00b7\n-1 -1 -1 -50 +1 +1 +1 +1 +1 +1 +1\n\u00b7\u00b7\u00b7\n(a) (b)\nFigure 17.14 (a) 3\u00d73 world for Exercise 17.8. The reward for each state is indicated.\nTheupperrightsquareisaterminalstate. (b) 101\u00d73worldforExercise17.9(omitting93\nidenticalcolumnsinthemiddle).Thestartstatehasreward0.\n17.9 Consider the 101\u00d73world shown inFigure 17.14(b). Inthe start state the agent has\na choice of two deterministic actions, Up or Down, but in the other states the agent has one\ndeterministic action, Right. Assuming a discounted reward function, for what values of the\ndiscount \u03b3 should the agent choose Up and for which Down? Compute the utility of each\naction as a function of \u03b3. (Note that this simple example actually reflects many real-world\nsituations in which one must weigh the value of an immediate action versus the potential\ncontinual long-term consequences, suchaschoosing todumppollutants intoalake.)\n17.10 Consider an undiscounted MDPhaving three states, (1, 2, 3), with rewards \u22121, \u22122,\n0, respectively. State 3isaterminal state. In states 1and 2there are twopossible actions: a\nandb. Thetransition modelisasfollows:\n\u2022 Instate 1,actionamovestheagent tostate 2withprobability 0.8and makestheagent\nstayputwithprobability 0.2.\n\u2022 Instate 2,actionamovestheagent tostate 1withprobability 0.8and makestheagent\nstayputwithprobability 0.2.\n\u2022 In either state 1 orstate 2, action b moves the agent to state 3 with probability 0.1 and\nmakestheagentstayputwithprobability 0.9.\nAnswerthefollowingquestions:\na. Whatcanbedetermined qualitatively abouttheoptimalpolicyinstates1and2?\nb. Apply policy iteration, showing each step in full, to determine the optimal policy and\nthevaluesofstates1and2. Assumethattheinitialpolicyhasactionbinbothstates.\nc. What happens to policy iteration if the initial policy has action a in both states? Does\ndiscounting help? Doestheoptimalpolicydependonthediscount factor?\n17.11 Considerthe4\u00d73worldshowninFigure17.1.\na. Implementanenvironment simulator forthisenvironment, such thatthespecific geog-\nraphy of the environment is easily altered. Some code for doing this is already in the\nonlinecoderepository. Exercises 691\nb. Create an agent that uses policy iteration, and measure its performance in the environ-\nment simulator from various starting states. Perform several experiments from each\nstarting state, and compare the average total reward received perrun withtheutility of\nthestate,asdetermined byyouralgorithm.\nc. Experiment with increasing the size of the environment. How does the run time for\npolicyiteration varywiththesizeoftheenvironment?\n17.12 How can the value determination algorithm be used to calculate the expected loss\nexperienced by an agent using a given set of utility estimates U and an estimated model P,\ncomparedwithanagentusingcorrect values?\n17.13 Let the initial belief state b for the 4\u00d73 POMDP on page 658 be the uniform dis-\n0\ntribution over the nonterminal states, i.e., (cid:16)1, 1,1,1, 1,1, 1,1,1,0,0(cid:17). Calculate the exact\n9 9 9 9 9 9 9 9 9\nbeliefstateb aftertheagentmovesLeftanditssensorreports1adjacentwall. Alsocalculate\n1\nb assumingthatthesamethinghappens again.\n2\n17.14 What is the time complexity of d steps of POMDP value iteration for a sensorless\nenvironment?\n17.15 Consider a version of the two-state POMDP on page 661 in which the sensor is\n90% reliable in state 0 but provides no information in state 1 (that is, it reports 0 or 1 with\nequal probability). Analyze, eitherqualitatively orquantitatively, theutility function andthe\noptimalpolicyforthisproblem.\n17.16 Showthatadominant strategy equilibrium isaNashequilibrium, butnotviceversa.\n17.17 In the children\u2019s game of rock\u2013paper\u2013scissors each player reveals at the same time\na choice of rock, paper, or scissors. Paper wraps rock, rock blunts scissors, and scissors cut\npaper. In the extended version rock\u2013paper\u2013scissors\u2013fire\u2013water, fire beats rock, paper, and\nscissors; rock, paper, and scissors beat water; and water beats fire. Write out the payoff\nmatrixandfindamixed-strategy solutiontothisgame.\n17.18 Thefollowingpayoffmatrix,fromBlinder(1983)bywayofBernstein(1996),shows\nagamebetweenpoliticians andtheFederalReserve.\nFed: contract Fed: donothing Fed: expand\nPol: contract F = 7,P = 1 F = 9,P = 4 F = 6,P = 6\nPol: donothing F = 8,P = 2 F = 5,P = 5 F = 4,P = 9\nPol: expand F = 3,P = 3 F = 2,P = 7 F = 1,P = 8\nPoliticians can expand or contract fiscal policy, while the Fed can expand or contract mon-\netary policy. (And of course either side can choose to do nothing.) Each side also has pref-\nerences for who should do what\u2014neither side wants to look like the bad guys. The payoffs\nshown are simply the rank orderings: 9 for first choice through 1 for last choice. Find the\nNashequilibriumofthegameinpurestrategies. IsthisaPareto-optimalsolution? Youmight\nwishtoanalyzethepolicies ofrecentadministrations inthislight. 692 Chapter 17. MakingComplexDecisions\n17.19 ADutchauction issimilarinanEnglishauction, butratherthanstarting thebidding\natalowpriceandincreasing, inaDutchauctionthesellerstartsatahighpriceandgradually\nlowers the price until some buyer is willing to accept that price. (If multiple bidders accept\nthe price, one is arbitrarily chosen as the winner.) More formally, the seller begins with a\nprice p and gradually lowers p by increments of d until at least one buyer accepts the price.\nAssumingallbidders actrationally, isittruethatforarbitrarily small d,aDutchauctionwill\nalwaysresultinthebidderwiththehighestvaluefortheitemobtainingtheitem? Ifso,show\nmathematically why. Ifnot,explainhowitmaybepossible forthebidderwithhighestvalue\nfortheitemnottoobtainit.\n17.20 Imagine anauction mechanism thatisjustlikeanascending-bid auction, except that\nat the end, the winning bidder, the one who bid b , pays only b \/2 rather than b .\nmax max max\nAssuming all agents are rational, what is the expected revenue to the auctioneer for this\nmechanism, comparedwithastandard ascending-bid auction?\n17.21 Teams in the National Hockey League historically received 2 points for winning a\ngame and 0 for losing. If the game is tied, an overtime period is played; if nobody wins in\novertime, the game is a tie and each team gets 1 point. But league officials felt that teams\nwere playing too conservatively inovertime (to avoid aloss), and it would be more exciting\nif overtime produced a winner. So in 1999 the officials experimented in mechanism design:\ntheruleswerechanged, giving ateam thatloses inovertime1 point, not0. Itisstill2points\nforawinand1foratie.\na. Washockeyazero-sum gamebeforetherulechange? After?\nb. Suppose thatatacertain timetinagame,thehometeam hasprobability pofwinning\nin regulation time, probability 0.78 \u2212 p of losing, and probability 0.22 of going into\novertime, where they have probability q of winning, .9\u2212q of losing, and .1 of tying.\nGiveequations fortheexpected valueforthehomeandvisiting teams.\nc. Imagine that it werelegal and ethical forthetwo teamsto enterinto apact where they\nagreethat theywillskate toatieinregulation time,and thenboth tryinearnest towin\nin overtime. Under what conditions, in terms of p and q, would it be rational for both\nteamstoagreetothispact?\nd. LongleyandSankaran(2005)reportthatsincetherulechange,thepercentageofgames\nwith a winner in overtime went up 18.2%, as desired, but the percentage of overtime\ngames also went up 3.6%. What does that suggest about possible collusion or conser-\nvativeplayaftertherulechange? 18\nLEARNING FROM\nEXAMPLES\nIn which we describe agents that can improve their behavior through diligent\nstudyoftheirownexperiences.\nAnagentislearningifitimprovesitsperformanceonfuturetasksaftermakingobservations\nLEARNING\nabout the world. Learning can range from the trivial, as exhibited by jotting down a phone\nnumber, to the profound, as exhibited by Albert Einstein, who inferred a new theory of the\nuniverse. Inthis chapter wewillconcentrate on one class of learning problem, which seems\nrestricted but actually has vast applicability: from a collection of input\u2013output pairs, learn a\nfunction thatpredictstheoutputfornewinputs.\nWhy would we want an agent to learn? If the design of the agent can be improved,\nwhywouldn\u2019tthedesigners justprogram inthatimprovement tobeginwith? Therearethree\nmain reasons. First, the designers cannot anticipate all possible situations that the agent\nmight find itself in. For example, a robot designed to navigate mazes must learn the layout\nof each new maze it encounters. Second, the designers cannot anticipate all changes over\ntime;aprogramdesignedtopredicttomorrow\u2019sstockmarket pricesmustlearntoadaptwhen\nconditions change from boom to bust. Third, sometimes human programmers have no idea\nhowtoprogramasolutionthemselves. Forexample,mostpeoplearegoodatrecognizingthe\nfaces of family members, but even the best programmers are unable to program a computer\nto accomplish that task, except by using learning algorithms. This chapter first gives an\noverview of the various forms of learning, then describes one popular approach, decision-\ntree learning, inSection 18.3, followed byatheoretical analysis oflearning inSections 18.4\nand 18.5. We look at various learning systems used in practice: linear models, nonlinear\nmodels(inparticular, neuralnetworks),nonparametricmodels,andsupportvectormachines.\nFinallyweshowhowensembles ofmodelscanoutperform asinglemodel.\n18.1 FORMS OF LEARNING\nAnycomponent ofanagent canbeimprovedbylearning from data. Theimprovements, and\nthetechniques usedtomakethem,dependonfourmajorfactors:\n\u2022 Whichcomponentistobeimproved.\n693 694 Chapter 18. LearningfromExamples\n\u2022 Whatpriorknowledgetheagentalreadyhas.\n\u2022 Whatrepresentation isusedforthedataandthecomponent.\n\u2022 Whatfeedback isavailabletolearnfrom.\nComponentstobelearned\nChapter2described severalagentdesigns. Thecomponents oftheseagentsinclude:\n1. Adirectmappingfromconditions onthecurrentstatetoactions.\n2. Ameanstoinferrelevantproperties oftheworldfromthepercept sequence.\n3. Information about the way the world evolves and about the results of possible actions\ntheagentcantake.\n4. Utilityinformation indicating thedesirability ofworldstates.\n5. Action-value information indicating thedesirability ofactions.\n6. Goalsthatdescribe classesofstateswhoseachievement maximizestheagent\u2019s utility.\nEachofthesecomponentscanbelearned. Consider,forexample,anagenttrainingtobecome\na taxi driver. Every time the instructor shouts \u201cBrake!\u201d the agent might learn a condition\u2013\naction rule for when to brake (component 1); the agent also learns every time the instructor\ndoes not shout. By seeing many camera images that it is told contain buses, it can learn\nto recognize them (2). By trying actions and observing the results\u2014for example, braking\nhard on a wet road\u2014it can learn the effects of its actions (3). Then, when it receives no tip\nfrom passengers who have been thoroughly shaken up during the trip, it can learn a useful\ncomponent ofitsoverallutilityfunction (4).\nRepresentation andpriorknowledge\nWe have seen several examples of representations for agent components: propositional and\nfirst-order logical sentences for the components in a logical agent; Bayesian networks for\nthe inferential components of adecision-theoretic agent, and so on. Effective learning algo-\nrithms have been devised for all of these representations. This chapter (and most of current\nmachine learning research) covers inputs that form a factored representation\u2014a vector of\nattribute values\u2014and outputs that can be either a continuous numerical value or a discrete\nvalue. Chapter 19 covers functions and prior knowledge composed of first-order logic sen-\ntences, andChapter20concentrates onBayesiannetworks.\nThere is another way to look at the various types of learning. We say that learning\na (possibly incorrect) general function or rule from specific input\u2013output pairs is called in-\nINDUCTIVE ductive learning. We will see in Chapter 19 that we can also do analytical or deductive\nLEARNING\nDEDUCTIVE learning: going from a known general rule to a new rule that is logically entailed, but is\nLEARNING\nusefulbecauseitallowsmoreefficientprocessing.\nFeedbacktolearnfrom\nTherearethree typesoffeedback thatdeterminethethreemaintypesoflearning:\nUNSUPERVISED Inunsupervisedlearningtheagentlearnspatternsintheinputeventhoughnoexplicit\nLEARNING\nfeedback issupplied. Themostcommonunsupervised learning taskisclustering: detecting\nCLUSTERING Section18.2. Supervised Learning 695\npotentially useful clusters of input examples. For example, a taxi agent might gradually\ndevelop a concept of \u201cgood traffic days\u201d and \u201cbad traffic days\u201d without ever being given\nlabeledexamplesofeachbyateacher.\nREINFORCEMENT In reinforcement learning the agent learns from a series of reinforcements\u2014rewards\nLEARNING\norpunishments. Forexample,thelackofatipattheendofthejourneygivesthetaxiagentan\nindication that itdid something wrong. The twopoints fora win at the end of achess game\ntellstheagentitdidsomethingright. Itisuptotheagenttodecidewhichoftheactionsprior\ntothereinforcement weremostresponsible forit.\nSUPERVISED Insupervisedlearningtheagentobservessomeexampleinput\u2013output pairsandlearns\nLEARNING\nafunctionthatmapsfrominputtooutput. Incomponent1above,theinputsarepercepts and\nthe output are provided by ateacher who says \u201cBrake!\u201d or\u201cTurn left.\u201d In component 2, the\ninputsarecameraimagesandtheoutputsagaincomefromateacherwhosays\u201cthat\u2019sabus.\u201d\nIn3, thetheory ofbraking isafunction from states andbraking actions tostopping distance\nin feet. In this case the output value is available directly from the agent\u2019s percepts (after the\nfact);theenvironment istheteacher.\nSEMI-SUPERVISED In practice, these distinction are not always so crisp. In semi-supervised learning we\nLEARNING\nare given a few labeled examples and must make what we can of a large collection of un-\nlabeled examples. Even the labels themselves may not be the oracular truths that we hope\nfor. Imagine thatyou are trying tobuild asystem toguess aperson\u2019s age from aphoto. You\ngather some labeled examples by snapping pictures of people and asking their age. That\u2019s\nsupervised learning. But in reality some of the people lied about their age. It\u2019s not just\nthat there is random noise in the data; rather the inaccuracies are systematic, and to uncover\nthemisanunsupervised learningprobleminvolving images, self-reported ages,andtrue(un-\nknown)ages. Thus,bothnoiseandlackoflabelscreateacontinuum betweensupervised and\nunsupervised learning.\n18.2 SUPERVISED LEARNING\nThetaskofsupervised learning isthis:\nGivenatrainingsetofN exampleinput\u2013output pairs\nTRAININGSET\n(x ,y ),(x ,y ),...(x ,y ),\n1 1 2 2 N N\nwhereeach y wasgenerated byanunknownfunction y =f(x),\nj\ndiscoverafunction hthatapproximates thetruefunction f.\nHere x and y can be any value; they need not be numbers. The function h isa hypothesis.1\nHYPOTHESIS\nLearningisasearchthrough thespaceofpossiblehypotheses foronethatwillperform well,\neven on new examples beyond the training set. Tomeasure the accuracy of a hypothesis we\ngive it a test set of examples that are distinct from the training set. We say a hypothesis\nTESTSET\n1 Anoteonnotation:exceptwherenoted,wewillusejtoindextheNexamples;xjwillalwaysbetheinputand\nyj theoutput. Incaseswheretheinputisspecificallyavectorofattributevalues(beginningwithSection18.3),\nwewillusexj forthejthexampleandwewilluseitoindexthenattributesofeachexample. Theelementsof\nxj arewrittenxj,1,xj,2,...,xj,n. 696 Chapter 18. LearningfromExamples\nf(x) f(x) f(x) f(x)\nx x x x\n(a) (b) (c) (d)\nFigure18.1 (a) Example(x,f(x)) pairsand a consistent, linearhypothesis. (b)A con-\nsistent,degree-7polynomialhypothesisforthesamedataset. (c)Adifferentdataset,which\nadmits an exact degree-6 polynomial fit or an approximate linear fit. (d) A simple, exact\nsinusoidalfittothesamedataset.\ngeneralizes well if it correctly predicts the value of y for novel examples. Sometimes the\nGENERALIZATION\nfunction f is stochastic\u2014it is not strictly a function of x, and what we have to learn is a\nconditional probability distribution, P(Y |x).\nWhen the output y is one of a finite set of values (such as sunny, cloudy or rainy),\nthe learning problem is called classification, and is called Boolean or binary classification\nCLASSIFICATION\nif there are only two values. When y is a number (such as tomorrow\u2019s temperature), the\nlearning problem is called regression. (Technically, solving a regression problem is finding\nREGRESSION\na conditional expectation or average value of y, because the probability that we have found\nexactlytherightreal-valued numberfor y is0.)\nFigure18.1showsafamiliarexample: fittingafunctionofasinglevariabletosomedata\npoints. Theexamplesarepoints inthe (x,y)plane, wherey = f(x). Wedon\u2019t knowwhatf\nis, butwewillapproximate itwithafunction hselected fromahypothesisspace, H,which\nHYPOTHESISSPACE\nforthisexamplewewilltaketobethesetofpolynomials,suchasx5+3x2+2. Figure18.1(a)\nshows some data with an exact fit by a straight line (the polynomial 0.4x +3). The line is\ncalledaconsistenthypothesisbecauseitagreeswithallthedata. Figure18.1(b)showsahigh-\nCONSISTENT\ndegree polynomial that is also consistent with the same data. This illustrates a fundamental\nproblemininductivelearning: howdowechoosefromamongmultipleconsistenthypotheses?\nOne answer is to prefer the simplest hypothesis consistent with the data. This principle is\ncalledOckham\u2019srazor,afterthe14th-centuryEnglishphilosopherWilliamofOckham,who\nOCKHAM\u2019SRAZOR\nusedittoarguesharplyagainstallsortsofcomplications. Definingsimplicityisnoteasy,but\nitseemsclearthatadegree-1polynomial issimplerthanadegree-7polynomial, andthus(a)\nshouldbepreferred to(b). WewillmakethisintuitionmorepreciseinSection18.4.3.\nFigure 18.1(c) shows a second data set. There is no consistent straight line for this\ndata set; in fact, it requires a degree-6 polynomial for an exact fit. There are just 7 data\npoints, so a polynomial with 7 parameters does not seem to be finding any pattern in the\ndata and we do not expect it to generalize well. A straight line that is not consistent with\nany ofthe data points, but mightgeneralize fairly wellforunseen values of x, isalso shown\nin (c). In general, there is a tradeoff between complex hypotheses that fit the training data\nwell and simpler hypotheses that may generalize better. In Figure 18.1(d) we expand the Section18.3. LearningDecisionTrees 697\nhypothesis space H to allow polynomials over both x and sin(x), and find that the data in\n(c) can be fitted exactly by asimple function of the form ax+b+csin(x). This shows the\nimportanceofthechoiceofhypothesisspace. Wesaythatalearningproblemisrealizableif\nREALIZABLE\nthehypothesisspacecontainsthetruefunction. Unfortunately, wecannotalwaystellwhether\nagivenlearning problem isrealizable, becausethetruefunction isnotknown.\nIn some cases, an analyst looking at a problem is willing to make more fine-grained\ndistinctions about thehypothesis space, tosay\u2014even beforeseeing anydata\u2014not justthat a\nhypothesis is possible or impossible, but rather how probable it is. Supervised learning can\n\u2217\nbedonebychoosing thehypothesis h thatismostprobable giventhedata:\nh\u2217 = argmaxP(h|data).\nh\u2208H\nByBayes\u2019rulethisisequivalentto\nh\u2217 = argmaxP(data|h)P(h).\nh\u2208H\nThen we can say that the prior probability P(h) is high for a degree-1 or -2 polynomial,\nlower for a degree-7 polynomial, and especially low for degree-7 polynomials with large,\nsharpspikes asinFigure18.1(b). Weallowunusual-looking functions whenthedatasaywe\nreallyneedthem,butwediscourage thembygivingthemalowpriorprobability.\nWhy not let H be the class of all Java programs, orTuring machines? Afterall, every\ncomputable function can be represented by some Turing machine, and that is the best we\ncan do. One problem with this idea is that it does not take into account the computational\ncomplexity oflearning. Thereisatradeoff between theexpressiveness ofahypothesis space\nand the complexity of finding a good hypothesis within that space. For example, fitting a\nstraight line to data is an easy computation; fitting high-degree polynomials is somewhat\nharder; and fitting Turing machines is in general undecidable. A second reason to prefer\nsimple hypothesis spaces is that presumably we will want to use h after we have learned it,\nand computing h(x) when h is a linear function is guaranteed to be fast, while computing\nanarbitrary Turingmachine program isnotevenguaranteed toterminate. Forthese reasons,\nmostworkonlearninghasfocused onsimplerepresentations.\nWewillseethattheexpressiveness\u2013complexitytradeoffisnotassimpleasitfirstseems:\nitisoftenthecase,aswesawwithfirst-orderlogicinChapter8,thatanexpressive language\nmakesitpossibleforasimplehypothesistofitthedata,whereasrestrictingtheexpressiveness\nof the language means that any consistent hypothesis must be very complex. For example,\ntherulesofchesscanbewritteninapageortwooffirst-order logic,butrequirethousandsof\npageswhenwritteninpropositional logic.\n18.3 LEARNING DECISION TREES\nDecision tree induction is one of the simplest and yet most successful forms of machine\nlearning. Wefirstdescribe therepresentation\u2014the hypothesis space\u2014and then showhowto\nlearnagoodhypothesis. 698 Chapter 18. LearningfromExamples\n18.3.1 The decisiontree representation\nA decision tree represents a function that takes as input a vector of attribute values and\nDECISIONTREE\nreturns a \u201cdecision\u201d\u2014a single output value. The input and output values can be discrete or\ncontinuous. Fornow wewillconcentrate on problems where the inputs have discrete values\nand the output has exactly two possible values; this is Boolean classification, where each\nexampleinputwillbeclassifiedastrue(apositiveexample)orfalse(anegativeexample).\nPOSITIVE\nA decision tree reaches its decision by performing a sequence of tests. Each internal\nNEGATIVE\nnode in the tree corresponds to a test of the value of one of the input attributes, A , and\ni\nthe branches from the node are labeled with the possible values of the attribute, A =v .\ni ik\nEach leaf node in the tree specifies avalue to be returned by the function. The decision tree\nrepresentation is natural for humans; indeed, many \u201cHow To\u201d manuals (e.g., for car repair)\narewrittenentirelyasasingledecision treestretching overhundreds ofpages.\nAs an example, we will build a decision tree to decide whether to wait for a table at a\nrestaurant. The aim here is to learn a definition for the goal predicate WillWait. First we\nGOALPREDICATE\nlisttheattributes thatwewillconsideraspartoftheinput:\n1. Alternate: whetherthereisasuitablealternative restaurant nearby.\n2. Bar: whethertherestaurant hasacomfortable barareatowaitin.\n3. Fri\/Sat: trueonFridaysandSaturdays.\n4. Hungry: whetherwearehungry.\n5. Patrons: howmanypeopleareintherestaurant (valuesare None,Some,andFull).\n6. Price: therestaurant\u2019s pricerange($,$$,$$$).\n7. Raining: whetheritisrainingoutside.\n8. Reservation: whetherwemadeareservation.\n9. Type: thekindofrestaurant (French,Italian,Thai,orburger).\n10. WaitEstimate: thewaitestimatedbythehost(0\u201310minutes,10\u201330, 30\u201360, or>60).\nNote that every variable has a small set of possible values; the value of WaitEstimate, for\nexample, isnot aninteger, rather itisone ofthefourdiscrete values 0\u201310, 10\u201330, 30\u201360, or\n>60. Thedecisiontreeusuallyusedbyoneofus(SR)forthisdomainisshowninFigure18.2.\nNoticethatthetreeignoresthePrice andType attributes. Examplesareprocessedbythetree\nstarting attherootandfollowing theappropriate branch untilaleafisreached. Forinstance,\nan example with Patrons=Full and WaitEstimate=0\u201310 will be classified as positive\n(i.e.,yes,wewillwaitforatable).\n18.3.2 Expressiveness ofdecisiontrees\nA Boolean decision tree is logically equivalent to the assertion that the goal attribute is true\nif and only if the input attributes satisfy one of the paths leading to a leaf with value true.\nWritingthisoutinpropositional logic,wehave\nGoal \u21d4 (Path \u2228Path \u2228\u00b7\u00b7\u00b7),\n1 2\nwhere each Path is aconjunction of attribute-value tests required to follow that path. Thus,\nthe whole expression is equivalent to disjunctive normal form (see page 283), which means Section18.3. LearningDecisionTrees 699\nthat any function in propositional logic can be expressed as a decision tree. As an example,\ntherightmostpathinFigure18.2is\nPath = (Patrons=Full \u2227WaitEstimate=0\u201310).\nFor a wide variety of problems, the decision tree format yields a nice, concise result. But\nsome functions cannot be represented concisely. Forexample, the majority function, which\nreturns true if and only if more than half of the inputs are true, requires an exponentially\nlarge decision tree. In other words, decision trees are good for some kinds of functions and\nbad forothers. Is there any kind ofrepresentation that is efficient for all kinds offunctions?\nUnfortunately, the answer is no. We can show this in a general way. Consider the set of all\nBoolean functions onnattributes. Howmany different functions are inthis set? Thisis just\nthe number of different truth tables that we can write down, because the function is defined\nby its truth table. A truth table over n attributes has 2n rows, one for each combination of\nvaluesoftheattributes. Wecanconsiderthe\u201canswer\u201dcolumnofthetableasa2n-bitnumber\nthatdefinesthefunction.\nThatmeansthereare22n\ndifferentfunctions(andtherewillbemore\nthan that number of trees, since more than one tree can compute the same function). Thisis\na scary number. Forexample, with just the ten Boolean attributes of our restaurant problem\nthere are 21024 or about 10308 different functions to choose from, and for20 attributes there\nareover 10300,000. Wewillneed someingenious algorithms tofindgood hypotheses insuch\nalargespace.\n18.3.3 Inducing decisiontrees from examples\nAnexampleforaBooleandecisiontreeconsistsofan(x,y)pair,wherexisavectorofvalues\nfortheinputattributes, and y isasingleBooleanoutput value. Atraining setof12examples\nPatrons?\nNone Some Full\nNo Yes WaitEstimate?\n>60 30-60 10-30 0-10\nNo Alternate? Hungry? Yes\nNo Yes No Yes\nReservation? Fri\/Sat? Yes Alternate?\nNo Yes No Yes No Yes\nBar? Yes No Yes Yes Raining?\nNo Yes No Yes\nNo Yes No Yes\nFigure18.2 Adecisiontreefordecidingwhethertowaitforatable. 700 Chapter 18. LearningfromExamples\nInputAttributes Goal\nExample\nAlt Bar Fri Hun Pat Price Rain Res Type Est WillWait\nx Yes No No Yes Some $$$ No Yes French 0\u201310 y =Yes\n1 1\nx Yes No No Yes Full $ No No Thai 30\u201360 y =No\n2 2\nx No Yes No No Some $ No No Burger 0\u201310 y =Yes\n3 3\nx Yes No Yes Yes Full $ Yes No Thai 10\u201330 y =Yes\n4 4\nx Yes No Yes No Full $$$ No Yes French >60 y =No\n5 5\nx No Yes No Yes Some $$ Yes Yes Italian 0\u201310 y =Yes\n6 6\nx No Yes No No None $ Yes No Burger 0\u201310 y =No\n7 7\nx No No No Yes Some $$ Yes Yes Thai 0\u201310 y =Yes\n8 8\nx No Yes Yes No Full $ Yes No Burger >60 y =No\n9 9\nx Yes Yes Yes Yes Full $$$ No Yes Italian 10\u201330 y =No\n10 10\nx No No No No None $ No No Thai 0\u201310 y =No\n11 11\nx Yes Yes Yes Yes Full $ No No Burger 30\u201360 y =Yes\n12 12\nFigure18.3 Examplesfortherestaurantdomain.\nis shown in Figure 18.3. The positive examples are the ones in which the goal WillWait is\ntrue(x ,x ,...);thenegativeexamplesaretheonesinwhichitisfalse (x ,x ,...).\n1 3 2 5\nWe want a tree that is consistent with the examples and is as small as possible. Un-\nfortunately, no matter how we measure size, it is an intractable problem to find the smallest\nconsistent tree; thereisnowaytoefficiently search through\nthe22n\ntrees. Withsomesimple\nheuristics, however,wecanfindagoodapproximate solution: asmall(butnotsmallest)con-\nsistenttree. TheDECISION-TREE-LEARNING algorithmadoptsagreedydivide-and-conquer\nstrategy: always test the most important attribute first. This test divides the problem up into\nsmaller subproblems that can then be solved recursively. By \u201cmost important attribute,\u201d we\nmeantheonethatmakesthemostdifferencetotheclassificationofanexample. Thatway,we\nhopetogettothecorrectclassificationwithasmallnumberoftests,meaningthatallpathsin\nthetreewillbeshortandthetreeasawholewillbeshallow.\nFigure18.4(a)showsthatType isapoorattribute,becauseitleavesuswithfourpossible\noutcomes,eachofwhichhasthesamenumberofpositiveasnegativeexamples. Ontheother\nhand,in(b)weseethatPatrons isafairlyimportantattribute,becauseifthevalueisNone or\nSome,thenweareleftwithexamplesetsforwhichwecananswerdefinitively(No andYes,\nrespectively). Ifthe valueis Full,weareleftwithamixedsetofexamples. Ingeneral, after\nthe first attribute test splits up the examples, each outcome is a new decision tree learning\nprobleminitself,withfewerexamplesandonelessattribute. Therearefourcasestoconsider\nfortheserecursive problems:\n1. If the remaining examples are all positive (or all negative), then we are done: we can\nanswer Yes or No. Figure 18.4(b) shows examples ofthis happening in the None and\nSome branches.\n2. Iftherearesomepositiveandsomenegativeexamples,thenchoosethebestattributeto\nsplitthem. Figure18.4(b)showsHungry beingusedtosplittheremainingexamples.\n3. Iftherearenoexamplesleft,itmeansthatnoexamplehasbeenobserved forthiscom- Section18.3. LearningDecisionTrees 701\n1 3 4 6 8 12 1 3 4 6 8 12\n2 5 7 9 10 11 2 5 7 9 10 11\nType? Patrons?\nFrench Italian Thai Burger None Some Full\n1 6 4 8 3 12 1 3 6 8 4 12\n5 10 2 11 7 9 7 11 2 5 9 10\nNo Yes Hungry?\nNo Yes\n4 12\n5 9 2 10\n(a) (b)\nFigure 18.4 Splitting the examples by testing on attributes. At each node we show the\npositive(lightboxes) and negative(darkboxes)examplesremaining. (a) Splitting on Type\nbringsusnonearertodistinguishingbetweenpositiveandnegativeexamples. (b)Splitting\nonPatronsdoesagoodjobofseparatingpositiveandnegativeexamples. Aftersplittingon\nPatrons,Hungryisafairlygoodsecondtest.\nbination ofattribute values, and wereturn adefault value calculated from the plurality\nclassificationofalltheexamplesthatwereusedinconstructingthenode\u2019sparent. These\narepassedalonginthevariable parent examples.\n4. If there are no attributes left, but both positive and negative examples, it means that\ntheseexampleshaveexactlythesamedescription,butdifferentclassifications. Thiscan\nhappen because there is an error or noise in the data; because the domain is nondeter-\nNOISE\nministic; orbecause wecan\u2019t observe anattribute thatwoulddistinguish theexamples.\nThebestwecandoisreturntheplurality classification oftheremainingexamples.\nThe DECISION-TREE-LEARNING algorithm is shown in Figure 18.5. Note that the set of\nexamples iscrucial for constructing thetree, butnowheredotheexamplesappearinthetree\nitself. A tree consists of just tests on attributes in the interior nodes, values of attributes on\nthe branches, and output values on the leaf nodes. Thedetails of the IMPORTANCE function\nare given in Section 18.3.4. The output of the learning algorithm on our sample training\nset is shown in Figure 18.6. The tree is clearly different from the original tree shown in\nFigure 18.2. One might conclude that the learning algorithm is not doing a very good job\noflearning the correct function. Thiswould bethewrong conclusion todraw, however. The\nlearningalgorithmlooksattheexamples,notatthecorrectfunction,andinfact,itshypothesis\n(see Figure 18.6) not only is consistent with all the examples, but is considerably simpler\nthantheoriginal tree! Thelearning algorithm hasnoreason toinclude testsforRaining and\nReservation, because it can classify all the examples without them. It has also detected an\ninteresting and previously unsuspected pattern: the first author will wait for Thai food on\nweekends. It is also bound tomake some mistakes forcases where ithas seen no examples.\nForexample,ithasneverseenacasewherethewaitis0\u201310minutesbuttherestaurantisfull. 702 Chapter 18. LearningfromExamples\nfunction DECISION-TREE-LEARNING(examples,attributes,parent examples) returns\natree\nifexamples isemptythenreturnPLURALITY-VALUE(parent examples)\nelseifallexamples havethesameclassificationthenreturntheclassification\nelseifattributes isemptythenreturnPLURALITY-VALUE(examples)\nelse\nA\u2190argmax\na\u2208attributes\nIMPORTANCE(a,examples)\ntree\u2190anewdecisiontreewithroottestA\nforeachvaluevk ofAdo\nexs\u2190{e : e\u2208examples and e.A = vk}\nsubtree\u2190DECISION-TREE-LEARNING(exs,attributes\u2212A,examples)\naddabranchtotree withlabel(A = vk)andsubtreesubtree\nreturntree\nFigure 18.5 The decision-tree learning algorithm. The function IMPORTANCE is de-\nscribedinSection18.3.4.ThefunctionPLURALITY-VALUEselectsthemostcommonoutput\nvalueamongasetofexamples,breakingtiesrandomly.\nPatrons?\nNone Some Full\nNo Yes Hungry?\nNo Yes\nNo Type?\nFrench Italian Thai Burger\nYes No Fri\/Sat? Yes\nNo Yes\nNo Yes\nFigure18.6 Thedecisiontreeinducedfromthe12-exampletrainingset.\nIn that case it says not to wait when Hungry is false, but I (SR)would certainly wait. With\nmoretrainingexamplesthelearningprogram couldcorrectthismistake.\nWenotethereisadangerofover-interpreting thetreethatthealgorithm selects. When\nthereareseveralvariables ofsimilarimportance, thechoice betweenthemissomewhatarbi-\ntrary: withslightly different input examples, adifferent variable wouldbechosen tosplit on\nfirst,andthewholetreewouldlookcompletely different. Thefunction computed bythetree\nwouldstillbesimilar,butthestructure ofthetreecanvary widely.\nWecanevaluate the accuracy ofalearning algorithm witha learningcurve, asshown\nLEARNINGCURVE\ninFigure 18.7. Wehave 100 examples atourdisposal, which we split into atraining set and Section18.3. LearningDecisionTrees 703\n1\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0 20 40 60 80 100\nTraining set size\nFigure 18.7 A learning curve for the decision tree learning algorithm on 100 randomly\ngeneratedexamplesintherestaurantdomain.Eachdatapointistheaverageof20trials.\natestset. Welearnahypothesis hwiththetrainingsetandmeasureitsaccuracywiththetest\nset. We do this starting with a training set of size 1 and increasing one at a time up to size\n99. Foreach size weactually repeat the process ofrandomly splitting 20times, andaverage\nthe results of the 20 trials. The curve shows that as the training set size grows, the accuracy\nincreases. (Forthis reason, learning curves are also called happygraphs.) Inthis graph we\nreach95%accuracy, anditlookslikethecurvemightcontinue toincreasewithmoredata.\n18.3.4 Choosingattribute tests\nThe greedy search used in decision tree learning is designed to approximately minimize the\ndepth of the final tree. The idea is to pick the attribute that goes as far as possible toward\nproviding an exact classification of the examples. A perfect attribute divides the examples\nintosets,eachofwhichareallpositiveorallnegativeandthuswillbeleavesofthetree. The\nPatrons attribute isnotperfect,butitisfairlygood. Areallyuselessattribute, suchasType,\nleaves the example sets with roughly the same proportion of positive and negative examples\nastheoriginal set.\nAllweneed,then,isaformalmeasureof\u201cfairlygood\u201dand\u201creallyuseless\u201dandwecan\nimplement the IMPORTANCE function ofFigure 18.5. Wewillusethe notion ofinformation\ngain, which is defined in terms of entropy, the fundamental quantity in information theory\nENTROPY\n(ShannonandWeaver,1949).\nEntropyisameasureoftheuncertaintyofarandomvariable; acquisitionofinformation\ncorresponds to a reduction in entropy. A random variable with only one value\u2014a coin that\nalways comesupheads\u2014has nouncertainty andthus itsentropy isdefined aszero; thus, we\ngain no information by observing its value. Aflipof afair coin is equally likely to come up\nheads or tails, 0 or 1, and we will soon show that this counts as \u201c1 bit\u201d of entropy. The roll\nofafairfour-sided diehas2bitsofentropy, because ittakestwobitstodescribe oneoffour\nequallyprobablechoices. Nowconsideranunfaircointhatcomesupheads99%ofthetime.\nIntuitively,thiscoinhaslessuncertaintythanthefaircoin\u2014ifweguessheadswe\u2019llbewrong\nonly1%ofthetime\u2014sowewouldlikeittohaveanentropymeasurethatisclosetozero,but\ntes\ntset\nno\ntcerroc\nnoitroporP 704 Chapter 18. LearningfromExamples\npositive. Ingeneral,theentropyofarandomvariable V withvaluesv ,eachwithprobability\nk\nP(v ),isdefinedas\nk\n(cid:12) (cid:12)\n1\nEntropy: H(V) = P(v )log = \u2212 P(v )log P(v ).\nk 2 P(v ) k 2 k\nk\nk k\nWecancheckthattheentropy ofafaircoinflipisindeed1bit:\nH(Fair) = \u2212(0.5log 0.5+0.5log 0.5) = 1.\n2 2\nIfthecoinisloadedtogive99%heads, weget\nH(Loaded)= \u2212(0.99log 0.99+0.01log 0.01) \u2248 0.08bits.\n2 2\nIt will help to define B(q) as the entropy of a Boolean random variable that is true with\nprobability q:\nB(q)=\u2212(qlog q+(1\u2212q)log (1\u2212q)).\n2 2\nThus, H(Loaded)=B(0.99) \u2248 0.08. Now let\u2019s get back to decision tree learning. If a\ntraining set contains p positive examples and n negative examples, then the entropy of the\ngoalattribute onthewholesetis\n(cid:13) (cid:14)\np\nH(Goal)= B .\np+n\nThe restaurant training set in Figure 18.3 has p = n = 6, so the corresponding entropy is\nB(0.5)orexactly1bit. Atestonasingleattribute Amightgiveusonlypartofthis1bit. We\ncanmeasureexactlyhowmuchbylookingattheentropyremainingaftertheattribute test.\nAnattribute AwithddistinctvaluesdividesthetrainingsetE intosubsetsE ,...,E .\n1 d\nEach subset E has p positive examples and n negative examples, so if we go along that\nk k k\nbranch, wewillneed anadditional B(p \/(p +n ))bitsofinformation toanswertheques-\nk k k\ntion. Arandomlychosenexamplefromthetrainingsethasthekthvaluefortheattributewith\nprobability (p +n )\/(p+n),sotheexpectedentropy remainingaftertestingattribute Ais\nk k\n(cid:12)d\nRemainder(A) = pk+nkB( pk ).\np+n pk+nk\nk=1\nTheinformationgainfromtheattribute testonAistheexpectedreduction inentropy:\nINFORMATIONGAIN\nGain(A) = B( p )\u2212Remainder(A).\np+n\nInfactGain(A)isjustwhatweneedtoimplementthe IMPORTANCE function. Returningto\ntheattributes considered inFigure18.4,wehave\n$ %\nGain(Patrons)= 1\u2212 2 B(0)+ 4 B(4)+ 6 B(2) \u2248 0.541bits,\n$ 12 2 12 4 12 6 %\nGain(Type) = 1\u2212 2 B(1)+ 2 B(1)+ 4 B(2)+ 4 B(2) = 0bits,\n12 2 12 2 12 4 12 4\nconfirming our intuition that Patrons is a better attribute to split on. In fact, Patrons has\nthemaximumgainofanyoftheattributes andwouldbechosenbythedecision-tree learning\nalgorithm astheroot. Section18.3. LearningDecisionTrees 705\n18.3.5 Generalizationandoverfitting\nOn some problems, the DECISION-TREE-LEARNING algorithm will generate a large tree\nwhen there is actually no pattern to be found. Consider the problem of trying to predict\nwhether the roll of a die will come up as 6 ornot. Suppose that experiments are carried out\nwith various dice and that the attributes describing each training example include the color\nof the die, its weight, the time when the roll was done, and whether the experimenters had\ntheir fingers crossed. If the dice are fair, the right thing to learn is a tree with a single node\nthat says \u201cno,\u201d But the DECISION-TREE-LEARNING algorithm will seize on any pattern it\ncan find in the input. If it turns out that there are 2 rolls of a 7-gram blue die with fingers\ncrossed and theyboth come out6, then the algorithm mayconstruct apath that predicts 6in\nthatcase. Thisproblemiscalled overfitting. Ageneral phenomenon, overfittingoccurswith\nOVERFITTING\nalltypesoflearners,evenwhenthetargetfunctionisnotatallrandom. InFigure18.1(b)and\n(c),wesawpolynomialfunctionsoverfittingthedata. Overfittingbecomesmorelikelyasthe\nhypothesis spaceandthenumberofinputattributes grows,andlesslikelyasweincrease the\nnumberoftrainingexamples.\nDECISIONTREE Fordecisiontrees,atechniquecalled decisiontreepruningcombatsoverfitting. Prun-\nPRUNING\ning works by eliminating nodes that are not clearly relevant. We start with a full tree, as\ngenerated by DECISION-TREE-LEARNING. We then look at a test node that has only leaf\nnodes asdescendants. Ifthetestappears tobeirrelevant\u2014detecting onlynoiseinthedata\u2014\nthen weeliminate the test, replacing it witha leaf node. Werepeat this process, considering\neachtestwithonlyleafdescendants, untileachonehaseitherbeenpruned oraccepted asis.\nThequestionis,howdowedetectthatanodeistestinganirrelevantattribute? Suppose\nweareatanodeconsistingofppositiveandnnegativeexamples. Iftheattributeisirrelevant,\nwewouldexpectthatitwouldsplittheexamplesintosubsetsthateachhaveroughlythesame\nproportion ofpositiveexamplesasthewholeset, p\/(p+n),andsotheinformationgainwill\nbeclosetozero.2 Thus,theinformation gainisagoodcluetoirrelevance. Now thequestion\nis,howlargeagainshouldwerequireinordertosplitonaparticular attribute?\nWecan answerthisquestion byusing astatistical significance test. Suchatest begins\nSIGNIFICANCETEST\nbyassumingthatthereisnounderlying pattern (theso-called nullhypothesis). Thentheac-\nNULLHYPOTHESIS\ntualdataareanalyzed tocalculate theextenttowhichtheydeviate fromaperfect absence of\npattern. Ifthe degree ofdeviation isstatistically unlikely (usually taken to meana5% prob-\nability or less), then that is considered to be good evidence for the presence of a significant\npatterninthedata. Theprobabilities arecalculatedfromstandarddistributions oftheamount\nofdeviation onewouldexpecttoseeinrandomsampling.\nIn this case, the null hypothesis is that the attribute is irrelevant and, hence, that the\ninformation gain for an infinitely large sample would be zero. We need to calculate the\nprobability that, under the null hypothesis, a sample of size v=n + p would exhibit the\nobserveddeviationfromtheexpecteddistribution ofpositiveandnegativeexamples. Wecan\nmeasurethedeviation bycomparingtheactualnumbersofpositiveandnegativeexamplesin\n2 Thegainwillbestrictlypositiveexceptfortheunlikelycasewherealltheproportionsare exactlythesame.\n(SeeExercise18.5.) 706 Chapter 18. LearningfromExamples\neachsubset, p andn ,withtheexpectednumbers, p\u02c6 andn\u02c6 ,assumingtrueirrelevance:\nk k k k\np +n p +n\np\u02c6 = p\u00d7 k k n\u02c6 =n\u00d7 k k .\nk k\np+n p+n\nAconvenient measureofthetotaldeviation isgivenby\n(cid:12)d (p \u2212p\u02c6 )2 (n \u2212n\u02c6 )2\nk k k k\n\u0394 = + .\np\u02c6 n\u02c6\nk k\nk=1\nUnder the null hypothesis, the value of \u0394 is distributed according to the \u03c72 (chi-squared)\ndistribution with v \u22121 degrees of freedom. We can use a \u03c72 table or a standard statistical\nlibrary routine to see if a particular \u0394 value confirms or rejects the null hypothesis. For\nexample, consider the restaurant type attribute, with four values and thus three degrees of\nfreedom. Avalueof\u0394=7.82ormorewouldrejectthenullhypothesisatthe5%level(anda\nvalueof\u0394=11.35ormorewouldrejectatthe1%level). Exercise18.8asksyoutoextendthe\nDECISION-TREE-LEARNING algorithm toimplement this form of pruning, which is known\n\u03c72 as\u03c72 pruning.\nPRUNING\nWithpruning,noiseintheexamplescanbetolerated. Errorsintheexample\u2019slabel(e.g.,\nanexample(x,Yes)thatshouldbe(x,No))givealinearincreaseinpredictionerror,whereas\nerrorsinthedescriptions ofexamples(e.g., Price=$whenitwasactuallyPrice=$$)have\nan asymptotic effect that gets worse as the tree shrinks down to smaller sets. Pruned trees\nperform significantly better than unpruned trees when the data contain a large amount of\nnoise. Also,theprunedtreesareoftenmuchsmallerandhenceeasiertounderstand.\nOnefinalwarning: Youmightthink that \u03c72 pruning andinformation gainlooksimilar,\nso why not combine them using an approach called early stopping\u2014have the decision tree\nEARLYSTOPPING\nalgorithm stopgenerating nodeswhenthereisnogoodattribute tospliton,ratherthangoing\nto all the trouble of generating nodes and then pruning them away. The problem with early\nstopping is that it stops us from recognizing situations where there is no one good attribute,\nbuttherearecombinations ofattributes thatareinformative. Forexample,considertheXOR\nfunction of twobinary attributes. If there are roughly equal numberof examples forallfour\ncombinations of input values, then neither attribute willbeinformative, yet thecorrect thing\ntodo istosplit onone ofthe attributes (it doesn\u2019t matterwhich one), and then atthesecond\nlevel we will get splits that are informative. Early stopping would miss this, but generate-\nand-then-prune handles itcorrectly.\n18.3.6 Broadening the applicabilityofdecisiontrees\nInordertoextend decision tree induction toawidervariety ofproblems, anumberofissues\nmust be addressed. We will briefly mention several, suggesting that a full understanding is\nbestobtained bydoingtheassociated exercises:\n\u2022 Missing data: In many domains, not all the attribute values will be known for every\nexample. The values might have gone unrecorded, or they might be too expensive to\nobtain. This gives rise to two problems: First, given a complete decision tree, how\nshould one classify an example that is missing one of the test attributes? Second, how Section18.3. LearningDecisionTrees 707\nshould one modify the information-gain formula when some examples have unknown\nvaluesfortheattribute? Thesequestions areaddressed inExercise18.9.\n\u2022 Multivalued attributes: When an attribute has many possible values, the information\ngain measure gives an inappropriate indication of the attribute\u2019s usefulness. In the ex-\ntreme case, an attribute such as ExactTime has a different value for every example,\nwhich means each subset of examples is a singleton with a unique classification, and\ntheinformationgainmeasurewouldhaveitshighestvalueforthisattribute. Butchoos-\ningthissplitfirstisunlikely toyieldthebesttree. Onesolution istousethegainratio\nGAINRATIO\n(Exercise18.10). AnotherpossibilityistoallowaBooleantestoftheformA=v ,that\nk\nis, picking out just one of the possible values for an attribute, leaving the remaining\nvaluestopossibly betestedlaterinthetree.\n\u2022 Continuous and integer-valued input attributes: Continuous or integer-valued at-\ntributessuchasHeight andWeight,haveaninfinitesetofpossiblevalues. Ratherthan\ngenerate infinitely many branches, decision-tree learning algorithms typically find the\nsplit point that gives the highest information gain. For example, at a given node in\nSPLITPOINT\nthe tree, it might be the case that testing on Weight > 160 gives the most informa-\ntion. Efficient methods exist for finding good split points: start by sorting the values\nof the attribute, and then consider only split points that are between two examples in\nsortedorderthathavedifferentclassifications, whilekeepingtrackoftherunningtotals\nof positive and negative examples on each side of the split point. Splitting is the most\nexpensivepartofreal-world decision treelearningapplications.\n\u2022 Continuous-valued output attributes: If we are trying to predict a numerical output\nvalue, such as the price of an apartment, then we need a regression tree rather than a\nREGRESSIONTREE\nclassification tree. A regression tree has at each leaf a linear function of some subset\nof numerical attributes, rather than a single value. For example, the branch for two-\nbedroom apartments might end with a linear function of square footage, number of\nbathrooms, and average income for the neighborhood. The learning algorithm must\ndecide when to stop splitting and begin applying linear regression (see Section 18.6)\novertheattributes.\nA decision-tree learning system for real-world applications must be able to handle all of\nthese problems. Handling continuous-valued variables isespecially important, because both\nphysical and financial processes provide numerical data. Several commercial packages have\nbeen built that meet these criteria, and they have been used to develop thousands of fielded\nsystems. Inmanyareasofindustryandcommerce,decisiontreesareusuallythefirstmethod\ntriedwhenaclassification methodistobeextracted from adataset. Oneimportant property\nofdecisiontreesisthatitispossibleforahumantounderstandthereasonfortheoutputofthe\nlearningalgorithm. (Indeed,thisisalegalrequirementforfinancialdecisionsthataresubject\nto anti-discrimination laws.) This is a property not shared by some other representations,\nsuchasneuralnetworks. 708 Chapter 18. LearningfromExamples\n18.4 EVALUATING AND CHOOSING THE BEST HYPOTHESIS\nWe want to learn a hypothesis that fits the future data best. To make that precise we need\nSTATIONARITY to define \u201cfuture data\u201d and \u201cbest.\u201d We make the stationarity assumption: that there is a\nASSUMPTION\nprobability distribution overexamples that remains stationary overtime. Each example data\npoint(beforeweseeit)isarandomvariableE whoseobservedvaluee =(x ,y )issampled\nj j j j\nfromthatdistribution, andisindependent oftheprevious examples:\nP(E j|E j\u22121,E j\u22122,...)= P(E j),\nandeachexamplehasanidentical priorprobability distribution:\nP(E j)= P(E j\u22121) = P(E j\u22122) = \u00b7\u00b7\u00b7 .\nExamplesthatsatisfytheseassumptionsarecalledindependent andidenticallydistributed or\ni.i.d.. Ani.i.d.assumption connects thepasttothefuture; withoutsomesuchconnection, all\nI.I.D.\nbets are off\u2014the future could be anything. (We will see later that learning can still occur if\nthereare slowchangesinthedistribution.)\nThe next step is to define \u201cbest fit.\u201d We define the error rate of a hypothesis as the\nERRORRATE\nproportionofmistakesitmakes\u2014theproportionoftimesthath(x) (cid:7)= yforan(x,y)example.\nNow, just because a hypothesis h has a low error rate on the training set does not mean that\nitwillgeneralize well. Aprofessor knowsthatanexamwillnotaccurately evaluate students\nif they have already seen the exam questions. Similarly, to get an accurate evaluation of a\nhypothesis,weneedtotestitonasetofexamplesithasnotseenyet. Thesimplestapproachis\ntheonewehaveseenalready: randomlysplittheavailabledataintoatrainingsetfromwhich\nthelearningalgorithmproduces handatestsetonwhichtheaccuracyofhisevaluated. This\nHOLDOUT method, sometimes called holdoutcross-validation, hasthedisadvantage thatitfails touse\nCROSS-VALIDATION\nalltheavailable data;ifweusehalfthedataforthetestset,thenweareonlytraining onhalf\nthe data, and we may get a poor hypothesis. On the other hand, if we reserve only 10% of\nthe data for the test set, then we may, by statistical chance, get a poor estimate of the actual\naccuracy.\nWecansqueezemoreoutofthedataandstillgetanaccurateestimateusingatechnique\nK-FOLD calledk-foldcross-validation. Theideaisthateachexampleservesdoubleduty\u2014astraining\nCROSS-VALIDATION\ndata and test data. First wesplit the data into k equal subsets. Wethen perform k rounds of\nlearning; on each round 1\/k of the data is held out as a test set and the remaining examples\nare used as training data. The average test set score of the k rounds should then be a better\nestimate than a single score. Popular values for k are 5 and 10\u2014enough to give an estimate\nthat is statistically likely to be accurate, at a cost of 5 to 10 times longer computation time.\nLEAVE-ONE-OUT Theextremeisk = n,alsoknownasleave-one-outcross-validation orLOOCV.\nCROSS-VALIDATION\nDespite the best efforts of statistical methodologists, users frequently invalidate their\nLOOCV\nresults by inadvertently peeking at the test data. Peeking can happen like this: A learning\nPEEKING\nalgorithmhasvarious\u201cknobs\u201dthatcanbetwiddledtotuneitsbehavior\u2014forexample,various\ndifferent criteria for choosing the next attribute in decision tree learning. The researcher\ngenerateshypotheses forvariousdifferentsettingsofthe knobs,measurestheirerrorrateson\nthetestset,andreportstheerrorrateofthebesthypothesis. Alas,peekinghasoccurred! The Section18.4. EvaluatingandChoosing theBestHypothesis 709\nreasonisthatthehypothesiswasselectedonthebasisofitstestseterrorrate,soinformation\naboutthetestsethasleakedintothelearning algorithm.\nPeekingisaconsequenceofusingtest-setperformancetobothchooseahypothesisand\nevaluate it. The way to avoid this is to really hold the test set out\u2014lock it away until you\nare completely done with learning and simply wish to obtain an independent evaluation of\nthe finalhypothesis. (And then, if you don\u2019t like the results ... you have toobtain, and lock\naway, a completely new test set if you want to go back and find a better hypothesis.) If the\ntestsetislockedaway,butyoustillwanttomeasureperformanceonunseendataasawayof\nselectingagoodhypothesis,thendividetheavailabledata(withoutthetestset)intoatraining\nset and a validation set. The next section shows how to use validation sets to find a good\nVALIDATIONSET\ntradeoffbetweenhypothesis complexityandgoodness offit.\n18.4.1 Model selection: Complexity versus goodnessoffit\nInFigure18.1(page696)weshowedthathigher-degree polynomialscanfitthetrainingdata\nbetter,butwhenthedegreeistoohightheywilloverfit,andperformpoorlyonvalidationdata.\nChoosingthedegreeofthepolynomialisaninstanceoftheproblemofmodelselection. You\nMODELSELECTION\ncan think ofthe task offinding the best hypothesis astwotasks: model selection defines the\nhypothesis spaceandthenoptimization findsthebesthypothesis withinthatspace.\nOPTIMIZATION\nInthis section weexplain how to select among models that are parameterized by size.\nForexample,withpolynomialswehavesize=1forlinearfunctions, size=2forquadratics,\nand so on. Fordecision trees, the size could be the number of nodes in the tree. In all cases\nwewanttofindthevalueofthesize parameterthatbestbalancesunderfitting andoverfitting\ntogivethebesttestsetaccuracy.\nAnalgorithm to perform model selection and optimization is shown in Figure 18.8. It\nWRAPPER\nis awrapperthat takes alearning algorithm as an argument (DECISION-TREE-LEARNING,\nforexample). Thewrapperenumerates modelsaccording toaparameter, size. Foreachsize,\nit uses cross validation on Learner to compute the average error rate on the training and\ntestsets. Westartwiththesmallest, simplest models (whichprobably underfitthedata), and\niterate, considering more complex models at each step, until the models start to overfit. In\nFigure 18.9 we see typical curves: the training set error decreases monotonically (although\nthere may in general be slight random variation), while the validation set error decreases at\nfirst, and then increases when the model begins to overfit. The cross-validation procedure\npicksthevalueofsize withthelowestvalidationseterror;thebottomoftheU-shapedcurve.\nWethengenerate ahypothesis ofthat size,using allthedata(without holding outanyofit).\nFinally,ofcourse,weshouldevaluatethereturned hypothesis onaseparatetestset.\nThisapproachrequiresthatthelearningalgorithmaccepta parameter, size,anddeliver\nahypothesisofthatsize. Aswesaid,fordecisiontreelearning,thesizecanbethenumberof\nnodes. We can modify DECISION-TREE-LEARNER so that it takes the number of nodes as\nan input, builds the tree breadth-first rather than depth-first (but at each level it still chooses\nthehighestgainattribute first),andstopswhenitreachesthedesirednumberofnodes. 710 Chapter 18. LearningfromExamples\nfunctionCROSS-VALIDATION-WRAPPER(Learner,k,examples)returnsahypothesis\nlocalvariables: errT,anarray,indexedbysize,storingtraining-seterrorrates\nerrV,anarray,indexedbysize,storingvalidation-seterrorrates\nforsize =1to\u221edo\nerrT[size],errV[size]\u2190CROSS-VALIDATION(Learner,size,k,examples)\niferrT hasconvergedthendo\nbest size\u2190thevalueofsize withminimumerrV[size]\nreturn Learner(best size,examples)\nfunctionCROSS-VALIDATION(Learner,size,k,examples)returnstwovalues:\naveragetrainingseterrorrate,averagevalidationseterrorrate\nfold errT \u21900;fold errV \u21900\nforfold =1to k do\ntraining set,validation set\u2190PARTITION(examples,fold,k)\nh\u2190Learner(size,training set)\nfold errT\u2190fold errT +ERROR-RATE(h,training set)\nfold errV \u2190fold errV +ERROR-RATE(h,validation set)\nreturnfold errT\/k,fold errV\/k\nFigure18.8 Analgorithmtoselectthemodelthathasthelowesterrorrateonvalidation\ndata by building models of increasing complexity, and choosing the one with best empir-\nical error rate on validation data. Here errT means error rate on the training data, and\nerrV means error rate on the validation data. Learner(size,examples) returns a hypoth-\nesis whose complexityis set by the parametersize, and which is trained on the examples.\nPARTITION(examples,fold,k)splitsexamplesintotwosubsets: avalidationsetofsizeN\/k\nandatrainingsetwithalltheotherexamples.Thesplitisdifferentforeachvalueoffold.\n18.4.2 From error rates to loss\nSo far, we have been trying to minimize error rate. This is clearly better than maximizing\nerror rate, but it is not the full story. Consider the problem of classifying email messages\nas spam or non-spam. It is worse to classify non-spam as spam (and thus potentially miss\nan important message) then to classify spam as non-spam (and thus suffer a few seconds of\nannoyance). Soaclassifierwitha1%errorrate, wherealmost alltheerrorswereclassifying\nspam as non-spam, would be better than a classifier with only a 0.5% error rate, if most of\nthoseerrorswereclassifying non-spam asspam. WesawinChapter16thatdecision-makers\nshould maximize expected utility, and utility is what learners should maximize as well. In\nmachine learning it is traditional to express utilities by means of a loss function. The loss\nLOSSFUNCTION\nfunction L(x,y,y\u02c6) is defined as the amount of utility lost by predicting h(x)=y\u02c6when the\ncorrectansweris f(x)=y:\nL(x,y,y\u02c6) = Utility(resultofusing y givenaninputx)\n\u2212 Utility(resultofusing y\u02c6givenaninputx) Section18.4. EvaluatingandChoosing theBestHypothesis 711\n60\nValidation Set Error\nTraining Set Error\n50\n40\n30\n20\n10\n0\n1 2 3 4 5 6 7 8 9 10\nTree size\nFigure18.9 Errorrateson training data (lower, dashed line) and validationdata (upper,\nsolidline)fordifferentsizedecisiontrees. We stopwhenthetrainingseterrorrateasymp-\ntotes,andthenchoosethetreewithminimalerroronthevalidationset;inthiscasethetree\nofsize7nodes.\nThisisthe mostgeneral formulation of theloss function. Oftenasimplified version is used,\nL(y,y\u02c6), that is independent of x. We will use the simplified version for the rest of this\nchapter, which means we can\u2019t say that it is worse to misclassify a letter from Mom than it\nis to misclassify a letter from our annoying cousin, but we can say it is 10 times worse to\nclassifynon-spam asspamthanvice-versa:\nL(spam,nospam) = 1, L(nospam,spam) = 10.\nNotethatL(y,y)isalwayszero; bydefinition thereisnoloss whenyouguess exactly right.\nFor functions with discrete outputs, we can enumerate a loss value for each possible mis-\nclassification, but we can\u2019t enumerate all the possibilities for real-valued data. If f(x) is\n137.035999, wewould be fairly happy with h(x) = 137.036, but just how happy should we\nbe? Ingeneralsmallerrors arebetterthanlargeones;twofunctions thatimplementthatidea\nare the absolute value of the difference (called the L loss), and the square of the difference\n1\n(called the L loss). If we are content with the idea of minimizing error rate, we can use\n2\nthe L loss function, which has a loss of 1 for an incorrect answer and is appropriate for\n0\/1\ndiscrete-valued outputs:\nAbsolutevalueloss: L (y,y\u02c6) = |y\u2212y\u02c6|\n1\nSquarederrorloss: L (y,y\u02c6) = (y\u2212y\u02c6)2\n2\n0\/1loss: L (y,y\u02c6)= 0ify = y\u02c6, else1\n0\/1\nThe learning agent can theoretically maximize its expected utility by choosing the hypoth-\nesis that minimizes expected loss over all input\u2013output pairs it will see. It is meaningless\nto talk about this expectation without defining aprior probability distribution, P(X,Y)over\nexamples. Let E bethe set of all possible input\u2013output examples. Thenthe expected gener-\nGENERALIZATION alization lossforahypothesis h(withrespecttolossfunction L)is\nLOSS\netar\nrorrE 712 Chapter 18. LearningfromExamples\n(cid:12)\nGenLoss (h) = L(y,h(x))P(x,y) ,\nL\n(x,y)\u2208E\n\u2217\nandthebesthypothesis, h ,istheonewiththeminimumexpectedgeneralization loss:\n\u2217\nh = argminGenLoss (h).\nL\nh\u2208H\nBecause P(x,y)is notknown, thelearning agent canonly estimate generalization loss with\nempiricallossonasetofexamples, E:\nEMPIRICALLOSS\n(cid:12)\n1\nEmpLoss (h) = L(y,h(x)).\nL,E N\n(x,y)\u2208E\nTheestimatedbesthypothesis\nh\u02c6\u2217\nisthentheonewithminimumempiricalloss:\nh\u02c6\u2217\n= argminEmpLoss (h).\nL,E\nh\u2208H\nTherearefourreasonswhy\nh\u02c6\u2217\nmaydifferfromthetruefunction, f: unrealizability, variance,\nnoise, and computational complexity. First, f may not be realizable\u2014may not be in H\u2014or\nmay be present in such a way that other hypotheses are preferred. Second, a learning algo-\nrithm will return different hypotheses for different sets of examples, even if those sets are\ndrawn from the same true function f, and those hypotheses will make different predictions\non new examples. Thehigher the variance among the predictions, the higher the probability\nofsignificant error. Notethat evenwhentheproblem isrealizable, therewillstill berandom\nvariance, but that variance decreases towards zero as the number of training examples in-\ncreases. Third, f may benondeterministic or noisy\u2014it mayreturn different values for f(x)\nNOISE\neachtimexoccurs. Bydefinition, noisecannotbepredicted; inmanycases,itarisesbecause\ntheobservedlabelsyaretheresultofattributesoftheenvironmentnotlistedinx. Andfinally,\nwhenHiscomplex, itcanbecomputationally intractable tosystematically searchthewhole\nhypothesis space. The best we can do is a local search (hill climbing or greedy search) that\nexploresonlypartofthespace. Thatgivesusanapproximationerror. Combiningthesources\noferror, we\u2019releftwithanestimationofanapproximation ofthetruefunction f.\nTraditional methods in statistics and the early years of machine learning concentrated\nSMALL-SCALE on small-scale learning, where the number of training examples ranged from dozens to the\nLEARNING\nlow thousands. Here the generalization error mostly comes from the approximation error of\nnothavingthetruef inthehypothesisspace,andfromestimationerrorofnothavingenough\ntraining examples to limit variance. In recent years there has been more emphasis on large-\nLARGE-SCALE scale learning, often with millions of examples. Here the generalization error is dominated\nLEARNING\nbylimitsofcomputation: thereisenoughdataandarichenoughmodelthatwecouldfindan\nh that is very close to the true f, but the computation to find it is too complex, so we settle\nforasub-optimal approximation.\n18.4.3 Regularization\nInSection18.4.1,wesawhowtodomodelselectionwithcross-validation onmodelsize. An\nalternativeapproachistosearchforahypothesisthatdirectlyminimizestheweightedsumof Section18.5. TheTheoryofLearning 713\nempiricallossandthecomplexity ofthehypothesis, whichwewillcallthetotalcost:\nCost(h) = EmpLoss(h)+\u03bbComplexity(h)\nh\u02c6\u2217\n= argminCost(h).\nh\u2208H\nHere \u03bb is a parameter, a positive number that serves as a conversion rate between loss and\nhypothesis complexity (which after all are not measured on the same scale). This approach\ncombines loss and complexity into one metric, allowing us to find the best hypothesis all at\nonce. Unfortunately we still need to do a cross-validation search to find the hypothesis that\ngeneralizes best, but this time it is with different values of \u03bb rather than size. We select the\nvalueof\u03bbthatgivesusthebestvalidation setscore.\nThisprocess ofexplicitly penalizing complex hypotheses iscalled regularization (be-\nREGULARIZATION\ncauseitlooksforafunctionthatismoreregular,orlesscomplex). Notethatthecostfunction\nrequires us to make two choices: the loss function and the complexity measure, which is\ncalled a regularization function. The choice of regularization function depends on the hy-\npothesis space. For example, a good regularization function for polynomials is the sum of\nthesquaresofthecoefficients\u2014keeping thesumsmallwouldguideusawayfromthewiggly\npolynomialsinFigure18.1(b)and(c). Wewillshowanexampleofthistypeofregularization\ninSection18.6.\nAnotherwaytosimplifymodelsistoreducethedimensionsthatthemodelsworkwith.\nAprocess of feature selection can beperformed todiscard attributes thatappear tobeirrel-\nFEATURESELECTION\nevant. \u03c72 pruning isakindoffeatureselection.\nIt is in fact possible to have the empirical loss and the complexity measured on the\nsamescale, without theconversion factor \u03bb: theycan bothbemeasured inbits. Firstencode\nthe hypothesis as a Turing machine program, and count the number of bits. Then count\nthe number of bits required to encode the data, where a correctly predicted example costs\nzero bits and the cost ofan incorrectly predicted example depends on how large the error is.\nMINIMUM\nThe minimum description length or MDL hypothesis minimizes the total number of bits\nDESCRIPTION\nLENGTH\nrequired. This works well in the limit, but for smaller problems there is a difficulty in that\nthe choice of encoding for the program\u2014for example, how best to encode a decision tree\nas a bit string\u2014affects the outcome. In Chapter 20 (page 805), we describe a probabilistic\ninterpretation oftheMDLapproach.\n18.5 THE THEORY OF LEARNING\nThe main unanswered question in learning is this: How can we be sure that our learning\nalgorithm hasproduced ahypothesis thatwillpredictthecorrectvalueforpreviously unseen\ninputs? Informalterms,howdoweknowthatthehypothesis hisclosetothetargetfunction\nf if we don\u2019t know what f is? These questions have been pondered for several centuries.\nIn more recent decades, other questions have emerged: how many examples do we need\nto get a good h? What hypothesis space should we use? If the hypothesis space is very\ncomplex, can we even find the best h, or do we have to settle for a local maximum in the 714 Chapter 18. LearningfromExamples\nspaceofhypotheses? Howcomplexshouldhbe? Howdoweavoidoverfitting? Thissection\nexaminesthesequestions.\nWe\u2019ll start with the question of how many examples are needed for learning. We saw\nfrom the learning curve fordecision tree learning on the restaurant problem (Figure 18.7 on\npage 703) that improves with more training data. Learning curves are useful, but they are\nspecifictoaparticularlearningalgorithm onaparticularproblem. Aretheresomemoregen-\neralprinciples governing thenumberofexamples needed ingeneral? Questions likethis are\nCOMPUTATIONAL addressed bycomputationallearningtheory, whichliesattheintersection ofAI,statistics,\nLEARNINGTHEORY\nandtheoreticalcomputerscience. Theunderlyingprincipleisthatanyhypothesisthatisseri-\nouslywrongwillalmostcertainly be\u201cfoundout\u201dwithhighprobability afterasmallnumber\nofexamples,becauseitwillmakeanincorrectprediction. Thus,anyhypothesisthatisconsis-\ntentwithasufficientlylargesetoftrainingexamplesisunlikelytobeseriouslywrong: thatis,\nPROBABLY\nit must be probably approximately correct. Any learning algorithm that returns hypotheses\nAPPROXIMATELY\nCORRECT\nthat areprobably approximately correct iscalled a PAClearningalgorithm; wecanuse this\nPACLEARNING\napproach toprovidebounds ontheperformance ofvariouslearning algorithms.\nPAC-learning theorems, like all theorems, are logical consequences of axioms. When\na theorem (as opposed to, say, a political pundit) states something about the future based on\nthe past, the axioms have to provide the \u201cjuice\u201d to make that connection. ForPAClearning,\nthe juice isprovided bythe stationarity assumption introduced onpage 708, whichsays that\nfuture examples are going to be drawn from the same fixed distribution P(E)=P(X,Y)\nas past examples. (Note that we do not have to know what distribution that is, just that it\ndoesn\u2019t change.) In addition, to keep things simple, we will assume that the true function f\nisdeterministic andisamemberofthehypothesis class Hthatisbeingconsidered.\nThe simplest PAC theorems deal with Boolean functions, for which the 0\/1 loss is ap-\npropriate. The error rate of a hypothesis h, defined informally earlier, is defined formally\nhereastheexpectedgeneralization errorforexamplesdrawnfromthestationarydistribution:\n(cid:12)\nerror(h) = GenLoss (h) = L (y,h(x))P(x,y) .\nL0\/1 0\/1\nx,y\nIn other words, error(h) is the probability that h misclassifies a new example. This is the\nsamequantitybeingmeasuredexperimentally bythelearning curvesshownearlier.\nA hypothesis h is called approximately correct if error(h) \u2264 (cid:2), where (cid:2) is a small\nconstant. Wewillshow thatwecanfindanN such that, afterseeing N examples, withhigh\nprobability, all consistent hypotheses will be approximately correct. One can think of an\napproximately correcthypothesisasbeing\u201cclose\u201dtothetruefunctioninhypothesisspace: it\n(cid:2) liesinside whatiscalled the(cid:2)-ballaround thetruefunction f. Thehypothesis spaceoutside\n-BALL\nthisballiscalledH .\nbad\nWe can calculate the probability that a \u201cseriously wrong\u201d hypothesis h \u2208 H is\nb bad\nconsistent with the first N examples as follows. We know that error(h ) > (cid:2). Thus, the\nb\nprobability that it agrees with a given example is at most 1 \u2212 (cid:2). Since the examples are\nindependent, theboundforN examplesis\nP(h agreeswithN examples) \u2264 (1\u2212(cid:2))N .\nb Section18.5. TheTheoryofLearning 715\nThe probability that H contains at least one consistent hypothesis is bounded by the sum\nbad\noftheindividual probabilities:\nP(H contains aconsistent hypothesis)\u2264 |H |(1\u2212(cid:2))N \u2264 |H|(1\u2212(cid:2))N ,\nbad bad\nwhere we have used the fact that |H | \u2264 |H|. We would like to reduce the probability of\nbad\nthiseventbelowsomesmallnumber\u03b4:\n|H|(1\u2212(cid:2))N \u2264 \u03b4 .\nGiventhat1\u2212(cid:2) \u2264 e\u2212(cid:2),wecanachievethisifweallowthealgorithm tosee\n(cid:13) (cid:14)\n1 1\nN \u2265 ln +ln|H| (18.1)\n(cid:2) \u03b4\nexamples. Thus,ifalearningalgorithm returnsahypothesis thatisconsistentwiththismany\nexamples, then with probability at least 1 \u2212 \u03b4, it has error at most (cid:2). In other words, it is\nprobably approximately correct. Thenumberofrequired examples, asafunction of(cid:2) and\u03b4,\nSAMPLE iscalledthesamplecomplexityofthehypothesis space.\nCOMPLEXITY\nAs we saw earlier, if H is the set of all Boolean functions on n attributes, then |H| =\n22n . Thus, thesample complexity ofthespace growsas 2n. Becausethenumberofpossible\nexamples is also 2n, this suggests that PAC-learning in the class of all Boolean functions\nrequires seeing all, or nearly all, of the possible examples. A moment\u2019s thought reveals the\nreason for this: H contains enough hypotheses to classify any given set of examples in all\npossibleways. Inparticular, foranysetof N examples,thesetofhypothesesconsistent with\nthose examples contains equal numbers of hypotheses that predict x to be positive and\nN+1\nhypotheses thatpredict x tobenegative.\nN+1\nTo obtain real generalization to unseen examples, then, it seems we need to restrict\nthe hypothesis space H in some way; but of course, if we do restrict the space, we might\neliminatethetruefunctionaltogether. Therearethreewaystoescapethisdilemma. Thefirst,\nwhich wewillcoverin Chapter19, is tobring priorknowledge tobear onthe problem. The\nsecond, which we introduced in Section 18.4.3, is to insist that the algorithm return not just\nanyconsistenthypothesis,butpreferablyasimpleone(asisdoneindecisiontreelearning). In\ncases where finding simple consistent hypotheses istractable, the sample complexity results\nare generally better than for analyses based only on consistency. The third escape, which\nwe pursue next, is to focus on learnable subsets of the entire hypothesis space of Boolean\nfunctions. This approach relies on the assumption that the restricted language contains a\nhypothesis h that is close enough to the true function f; the benefits are that the restricted\nhypothesisspaceallowsforeffectivegeneralization andistypicallyeasiertosearch. Wenow\nexamineonesuchrestricted language inmoredetail.\n18.5.1 PAClearning example: Learning decisionlists\nWe now show how to apply PAC learning to a new hypothesis space: decision lists. A\nDECISIONLISTS\ndecision list consists of a series of tests, each of which is a conjunction of literals. If a\ntest succeeds when applied to an example description, the decision list specifies the value\nto be returned. If the test fails, processing continues with the next test in the list. Decision\nlists resemble decision trees, but their overall structure is simpler: they branch only in one 716 Chapter 18. LearningfromExamples\nNo No\nPatrons(x, Some) Patrons(x, Full) ^ Fri\/Sat(x) No\nYes Yes\nYes Yes\nFigure18.10 Adecisionlistfortherestaurantproblem.\ndirection. In contrast, the individual tests are more complex. Figure 18.10 shows a decision\nlistthatrepresents thefollowinghypothesis:\nWillWait \u21d4 (Patrons = Some)\u2228(Patrons =Full \u2227Fri\/Sat).\nIf we allow tests of arbitrary size, then decision lists can represent any Boolean function\n(Exercise 18.14). On the other hand, if we restrict the size of each test to at most k literals,\nthen itis possible forthe learning algorithm to generalize successfully from asmall number\nk-DL ofexamples. Wecallthislanguagek-DL. TheexampleinFigure18.10isin2-DL.Itiseasyto\nk-DT show(Exercise18.14)thatk-DLincludesasasubsetthelanguagek-DT,thesetofalldecision\ntrees ofdepth at most k. Itis important to remember that the particular language referred to\nby k-DL depends on the attributes used to describe the examples. We will use the notation\nk-DL(n)todenoteak-DLlanguage usingnBooleanattributes.\nThe first task is to show that k-DL is learnable\u2014that is, that any function in k-DL can\nbe approximated accurately after training on a reasonable number of examples. To do this,\nwe need to calculate the number of hypotheses in the language. Let the language of tests\u2014\nconjunctions ofatmostk literals using nattributes\u2014be Conj(n,k). Because adecision list\nisconstructed oftests,andbecauseeachtestcanbeattachedtoeitheraYes oraNo outcome\norcanbeabsentfromthedecisionlist,thereareatmost3|Conj(n,k)|\ndistinctsetsofcomponent\ntests. Eachofthesesetsoftestscanbeinanyorder, so\n|k-DL(n)| \u2264 3|Conj(n,k)||Conj(n,k)|!.\nThenumberofconjunctions ofk literalsfrom nattributes isgivenby\n(cid:13) (cid:14)\n(cid:12)k\n2n\n|Conj(n,k)| = = O(nk).\ni\ni=0\nHence,aftersomework,weobtain\n|k-DL(n)| = 2O(nklog2(nk)) .\nWecanplugthisintoEquation (18.1)toshowthatthenumberofexamplesneeded forPAC-\nlearning ak-DLfunction ispolynomial inn:\n(cid:13) (cid:14)\n1 1\nN \u2265 ln +O(nklog (nk)) .\n(cid:2) \u03b4 2\nTherefore,anyalgorithmthatreturnsaconsistentdecisionlistwillPAC-learnak-DLfunction\ninareasonable numberofexamples,forsmall k.\nThe next task is to find an efficient algorithm that returns a consistent decision list.\nWe will use a greedy algorithm called DECISION-LIST-LEARNING that repeatedly finds a Section18.6. RegressionandClassification withLinearModels 717\nfunctionDECISION-LIST-LEARNING(examples)returnsadecisionlist,orfailure\nifexamples isemptythenreturnthetrivialdecisionlistNo\nt\u2190atestthatmatchesanonemptysubsetexamples ofexamples\nt\nsuchthatthemembersofexamples areallpositiveorallnegative\nt\nifthereisnosucht thenreturnfailure\niftheexamplesinexamples arepositivetheno\u2190Yes elseo\u2190No\nt\nreturnadecisionlistwithinitialtestt andoutcomeo andremainingtestsgivenby\nDECISION-LIST-LEARNING(examples \u2212 examples t)\nFigure18.11 Analgorithmforlearningdecisionlists.\n1\n0.9\n0.8\nDecision tree\n0.7 Decision list\n0.6\n0.5\n0.4\n0 20 40 60 80 100\nTraining set size\nFigure18.12 LearningcurveforDECISION-LIST-LEARNINGalgorithmontherestaurant\ndata. ThecurveforDECISION-TREE-LEARNINGisshownforcomparison.\ntest that agrees exactly with some subset of the training set. Once it finds such a test, it\nadds it to the decision list under construction and removes the corresponding examples. It\nthenconstructs theremainderofthedecision list, using justtheremaining examples. Thisis\nrepeated untiltherearenoexamplesleft. Thealgorithm isshowninFigure18.11.\nThis algorithm does not specify the method for selecting the next test to add to the\ndecisionlist. Althoughtheformalresultsgivenearlierdonotdependontheselectionmethod,\nit would seem reasonable to prefer small tests that match large sets of uniformly classified\nexamples,sothattheoveralldecisionlistwillbeascompactaspossible. Thesimpleststrategy\nistofindthesmallesttesttthatmatchesanyuniformlyclassifiedsubset,regardlessofthesize\nofthesubset. Eventhisapproach worksquitewell,asFigure 18.12suggests.\n18.6 REGRESSION AND CLASSIFICATION WITH LINEAR MODELS\nNow it is time to move on from decision trees and lists to a different hypothesis space, one\nthat has been used for hundred of years: the class of linear functions of continuous-valued\nLINEARFUNCTION\ntes\ntset\nno\ntcerroc\nnoitroporP 718 Chapter 18. LearningfromExamples\n1000\n900\n800\n700\n600\n500 Loss\n400\nw\n300 0\n500 1000 1500 2000 2500 3000 3500\nw\n1\nHouse size in square feet\n(a) (b)\nFigure 18.13 (a) Data points of price versus floor space of houses for sale in Berkeley,\nCA, in July 2009, along with the linear function hypot(cid:2)hesis that minimizes squared error\nloss: y = 0.232x+246. (b) Plot of the loss function j(w 1xj +w\n0\n\u2212yj)2 for various\nvaluesofw ,w . Notethatthelossfunctionisconvex,withasingleglobalminimum.\n0 1\ninputs. We\u2019ll start with the simplest case: regression with a univariate linear function, oth-\nerwise known as \u201cfitting a straight line.\u201d Section 18.6.2 covers the multivariate case. Sec-\ntions 18.6.3 and 18.6.4 show how to turn linear functions into classifiers by applying hard\nandsoftthresholds.\n18.6.1 Univariatelinearregression\nAunivariatelinearfunction(astraightline)withinputxandoutputyhastheformy=w x+\n1\nw ,where w and w arereal-valued coefficients tobelearned. Weusethe letter w because\n0 0 1\nwe think of the coefficients as weights; the value of y is changed by changing the relative\nWEIGHT\nweightofonetermoranother. We\u2019lldefine wtobethevector[w ,w ],anddefine\n0 1\nh (x)=w x+w .\nw 1 0\nFigure 18.13(a) shows an example of a training set of n points in the x,y plane, each point\nrepresenting the size in square feet and the price of a house offered for sale. The task of\nfindingtheh thatbestfitsthesedataiscalled linearregression. Tofitalinetothedata,all\nLINEARREGRESSION w\nwehavetodoisfindthevaluesoftheweights[w ,w ]thatminimizetheempiricalloss. Itis\n0 1\ntraditional (going back to Gauss3)to use the squared loss function, L , summed overall the\n2\ntraining examples:\n(cid:12)N (cid:12)N (cid:12)N\nLoss(h )= L (y ,h (x )) = (y \u2212h (x ))2 = (y \u2212(w x +w ))2 .\nw 2 j w j j w j j 1 j 0\nj=1 j=1 j=1\n3 Gaussshowedthatiftheyj valueshavenormallydistributednoise,thenthemostlikelyvaluesofw1andw0\nareobtainedbyminimizingthesumofthesquaresoftheerrors.\n0001$\nni\necirp\nesuoH Section18.6. RegressionandClassification withLinearModels 719\n(cid:2)\nWe would like to find w\u2217 = argmin Loss(h ). The sum N (y \u2212 (w x + w ))2 is\nw w j=1 j 1 j 0\nminimizedwhenitspartialderivativeswithrespectto w andw arezero:\n0 1\n(cid:12)N (cid:12)N\n\u2202 \u2202\n(y \u2212(w x +w ))2 = 0and (y \u2212(w x +w ))2 = 0. (18.2)\nj 1 j 0 j 1 j 0\n\u2202w \u2202w\n0 1\nj=1 j=1\nTheseequations haveauniquesolution:\n(cid:2) (cid:2) (cid:2)\n(cid:12) (cid:12)\nN( x y )\u2212( x )( y )\nw = (cid:2)j j (cid:2)j j ; w =( y \u2212w ( x ))\/N . (18.3)\n1 N( x2)\u2212( x )2 0 j 1 j\nj j\nForthe example in Figure 18.13(a), the solution is w =0.232, w = 246, and the line with\n1 0\nthoseweightsisshownasadashedlineinthefigure.\nMany forms of learning involve adjusting weights to minimize a loss, so it helps to\nhaveamentalpicture ofwhat\u2019sgoingoninweightspace\u2014the space definedbyallpossible\nWEIGHTSPACE\nsettings of the weights. Forunivariate linear regression, the weight space defined by w and\n0\nw istwo-dimensional, sowecangraphthelossasafunction ofw andw ina3Dplot(see\n1 0 1\nFigure18.13(b)). Weseethatthelossfunction isconvex,asdefinedonpage133;thisistrue\nfor every linear regression problem with an L loss function, and implies that there are no\n2\nlocal minima. In some sense that\u2019s the end of the story for linear models; if we need to fit\nlinestodata,weapplyEquation(18.3).4\nTo go beyond linear models, we will need to face the fact that the equations defining\nminimum loss (as in Equation (18.2)) will often have no closed-form solution. Instead, we\nwill face a general optimization search problem in a continuous weight space. As indicated\ninSection 4.2(page 129), such problems can beaddressed by a hill-climbing algorithm that\nfollows the gradient of the function to be optimized. In this case, because we are trying to\nminimize the loss, we will use gradient descent. We choose any starting point in weight\nGRADIENTDESCENT\nspace\u2014here, a point in the (w , w ) plane\u2014and then move to a neighboring point that is\n0 1\ndownhill,repeating untilweconverge ontheminimumpossibleloss:\nw \u2190 anypointintheparameterspace\nloopuntilconvergence do\nforeachw inwdo\ni\n\u2202\nw \u2190 w \u2212\u03b1 Loss(w) (18.4)\ni i\n\u2202w\ni\nTheparameter \u03b1,whichwecalledthestepsizeinSection4.2,isusually called thelearning\nratewhenwearetryingtominimizelossinalearningproblem. Itcanbeafixedconstant,or\nLEARNINGRATE\nitcandecayovertimeasthelearning processproceeds.\nForunivariateregression, thelossfunctionisaquadratic function, sothepartialderiva-\ntive will be a linear function. (The only calculus you need to know is that \u2202 x2=2x and\n\u2202x\n\u2202 x=1.) Let\u2019s first work out the partial derivatives\u2014the slopes\u2014in the simplified case of\n\u2202x\n4 Withsomecaveats: theL2 lossfunctionisappropriatewhenthereisnormally-distributednoisethatisinde-\npendentofx;allresultsrelyonthestationarityassumption;etc. 720 Chapter 18. LearningfromExamples\nonlyonetraining example, (x,y):\n\u2202 \u2202\nLoss(w) = (y\u2212h (x))2\nw\n\u2202w \u2202w\ni i\n\u2202\n= 2(y\u2212h (x))\u00d7 (y\u2212h (x))\nw w\n\u2202w\ni\n\u2202\n= 2(y\u2212h (x))\u00d7 (y\u2212(w x+w )), (18.5)\nw 1 0\n\u2202w\ni\napplying thistobothw andw weget:\n0 1\n\u2202 \u2202\nLoss(w)= \u22122(y\u2212h (x)); Loss(w) = \u22122(y\u2212h (x))\u00d7x\nw w\n\u2202w \u2202w\n0 1\nThen,pluggingthisbackintoEquation(18.4),andfoldingthe2intotheunspecifiedlearning\nrate\u03b1,wegetthefollowinglearning rulefortheweights:\nw \u2190 w +\u03b1(y\u2212h (x)); w \u2190 w +\u03b1(y\u2212h (x))\u00d7x\n0 0 w 1 1 w\nThese updates make intuitive sense: if h (x) > y, i.e., the output of the hypothesis is too\nw\nlarge, reduce w a bit, and reduce w if x was a positive input but increase w if x was a\n0 1 1\nnegativeinput.\nTheprecedingequationscoveronetrainingexample. ForN trainingexamples,wewant\ntominimizethesumoftheindividuallossesforeachexample. Thederivativeofasumisthe\nsumofthederivatives, sowehave:\n(cid:12) (cid:12)\nw \u2190 w +\u03b1 (y \u2212h (x )); w \u2190 w +\u03b1 (y \u2212h (x ))\u00d7x .\n0 0 j w j 1 1 j w j j\nj j\nBATCHGRADIENT These updates constitute the batch gradient descent learning rule for univariate linear re-\nDESCENT\ngression. Convergence to the unique global minimum is guaranteed (as long as we pick \u03b1\nsmallenough) butmaybeveryslow: wehavetocyclethrough allthetraining dataforevery\nstep,andtheremaybemanysteps.\nSTOCHASTIC There is another possibility, called stochastic gradient descent, where we consider\nGRADIENTDESCENT\nonly a single training point at a time, taking a step after each one using Equation (18.5).\nStochastic gradient descent can be used in an online setting, where new data are coming in\none at a time, or offline, where we cycle through the same data as many times as is neces-\nsary,takingastepafterconsideringeachsingleexample. Itisoftenfasterthanbatchgradient\ndescent. Withafixedlearning rate \u03b1, however, itdoes not guarantee convergence; itcan os-\ncillatearoundtheminimumwithoutsettlingdown. Insomecases,asweseelater,aschedule\nofdecreasing learning rates(asinsimulatedannealing) doesguarantee convergence.\n18.6.2 Multivariatelinearregression\nMULTIVARIATE Wecaneasilyextendtomultivariatelinearregression problems, inwhicheachexamplex\nLINEARREGRESSION j\nisann-elementvector.5 Ourhypothesis spaceisthesetoffunctions oftheform\n(cid:12)\nh (x ) = w +w x +\u00b7\u00b7\u00b7+w x = w + w x .\nsw j 0 1 j,1 n j,n 0 i j,i\ni\n5 ThereadermaywishtoconsultAppendixAforabriefsummaryoflinearalgebra. Section18.6. RegressionandClassification withLinearModels 721\nThew term,theintercept,standsoutasdifferentfromtheothers. Wecanfixthatbyinventing\n0\na dummy input attribute, x , which is defined as always equal to 1. Then h is simply the\nj,0\ndot product of the weights and the input vector (or equivalently, the matrix product of the\ntranspose oftheweightsandtheinputvector):\n(cid:12)\nh (x ) = w\u00b7x = w(cid:12) x = w x .\nsw j j j i j,i\ni\n\u2217\nThebestvectorofweights, w ,minimizessquared-error lossovertheexamples:\n(cid:12)\nw\u2217 = argmin L (y ,w\u00b7x ).\n2 j j\nw\nj\nMultivariatelinearregressionisactuallynotmuchmorecomplicatedthantheunivariatecase\nwejust covered. Gradient descent willreach the(unique) minimum oftheloss function; the\nupdateequation foreachweightw is\n(cid:12) i\nw \u2190 w +\u03b1 x (y \u2212h (x )). (18.6)\ni i j,i j w j\nj\nIt is also possible to solve analytically for the w that minimizes loss. Let y be the vector of\noutputs for the training examples, and X be the data matrix, i.e., the matrix of inputs with\nDATAMATRIX\nonen-dimensional exampleperrow. Thenthesolution\nw\u2217\n=\n(X(cid:12) X)\u22121X(cid:12)\ny\nminimizesthesquared error.\nWith univariate linear regression we didn\u2019t have to worry about overfitting. But with\nmultivariate linear regression in high-dimensional spaces it is possible that some dimension\nthatisactually irrelevant appearsbychancetobeuseful,resulting inoverfitting.\nThus,itiscommontouseregularizationonmultivariatelinearfunctionstoavoidover-\nfitting. Recall that with regularization we minimize the total cost of a hypothesis, counting\nboththeempiricallossandthecomplexity ofthehypothesis:\nCost(h) = EmpLoss(h)+\u03bbComplexity(h).\nFor linear functions the complexity can be specified as a function of the weights. We can\nconsiderafamilyofregularization functions:\n(cid:12)\nComplexity(h ) =L (w) = |w |q .\nw q i\ni\nAs with loss functions,6 with q=1 we have L regularization, which minimizes the sum of\n1\ntheabsolute values; withq=2, L regularization minimizes thesum ofsquares. Whichreg-\n2\nularization function should you pick? Thatdepends onthe specific problem, but L regular-\n1\nization hasanimportant advantage: ittendstoproduce a sparsemodel. Thatis,itoften sets\nSPARSEMODEL\nmanyweightstozero,effectivelydeclaringthecorresponding attributestobeirrelevant\u2014just\nas DECISION-TREE-LEARNING does(although byadifferent mechanism). Hypotheses that\ndiscardattributes canbeeasierforahumantounderstand, andmaybelesslikelytooverfit.\n6 ItisperhapsconfusingthatL1andL2areusedforbothlossfunctionsandregularizationfunctions.Theyneed\nnotbeusedinpairs:youcoulduseL2losswithL1regularization,orviceversa. 722 Chapter 18. LearningfromExamples\nw w\n2 2\nw*\nw*\nw w\n1 1\nFigure18.14 WhyL regularizationtendstoproduceasparsemodel. (a)With L regu-\n1 1\nlarization(box),the minimalachievableloss(concentriccontours)oftenoccurson anaxis,\nmeaninga weightof zero. (b)With L regularization(circle), the minimalloss islikely to\n2\noccuranywhereonthecircle,givingnopreferencetozeroweights.\nFigure18.14givesanintuitiveexplanationofwhyL regularizationleadstoweightsof\n1\nzero, while L regularization does not. Note that minimizing Loss(w)+\u03bbComplexity(w)\n2\nis equivalent to minimizing Loss(w) subject to the constraint that Complexity(w) \u2264 c, for\nsomeconstant cthatisrelated to \u03bb. Now,inFigure 18.14(a) thediamond-shaped box repre-\nsents theset of points win two-dimensional weight space that have L complexity less than\n1\nc; our solution will have to be somewhere inside this box. The concentric ovals represent\ncontours ofthelossfunction, withtheminimumlossatthecenter. Wewanttofindthepoint\nintheboxthatisclosest totheminimum;youcanseefromthediagram that,foranarbitrary\npositionoftheminimumanditscontours, itwillbecommonforthecorneroftheboxtofind\nitswayclosesttotheminimum,justbecausethecornersarepointy. Andofcoursethecorners\nare the points that have a value of zero in some dimension. In Figure 18.14(b), we\u2019ve done\nthe same for the L complexity measure, which represents a circle rather than a diamond.\n2\nHere you can see that, in general, there is no reason for the intersection to appear on one of\ntheaxes; thusL regularization does nottend toproduce zeroweights. Theresult isthat the\n2\nnumberofexamplesrequiredtofindagoodhislinearinthenumberofirrelevantfeaturesfor\nL regularization, but only logarithmic with L regularization. Empirical evidence onmany\n2 1\nproblemssupports thisanalysis.\nAnotherwaytolookatitisthatL regularization takesthedimensionalaxesseriously,\n1\nwhile L treats them as arbitrary. The L function is spherical, which makes it rotationally\n2 2\ninvariant: Imagine a set of points in a plane, measured by their x and y coordinates. Now\nimagine rotating the axes by 45o. You\u2019d get a different set of (x(cid:2) ,y(cid:2) ) values representing\nthe same points. If you apply L regularization before and after rotating, you get exactly\n2\n(cid:2) (cid:2)\nthe same point as the answer (although the point would be described with the new (x,y )\ncoordinates). Thatisappropriate whenthechoiceofaxesreallyisarbitrary\u2014when itdoesn\u2019t\nmatterwhetheryourtwodimensions aredistancesnorthandeast;ordistancesnorth-east and Section18.6. RegressionandClassification withLinearModels 723\nsouth-east. WithL regularizationyou\u2019dgetadifferentanswer,becausetheL functionisnot\n1 1\nrotationally invariant. That is appropriate when the axes are not interchangeable; it doesn\u2019t\nmakesensetorotate\u201cnumberofbathrooms\u201d 45o towards\u201clotsize.\u201d\n18.6.3 Linearclassifiers witha hardthreshold\nLinear functions can be used to do classification as well as regression. For example, Fig-\nure18.15(a)showsdatapointsoftwoclasses: earthquakes(whichareofinteresttoseismolo-\ngists)andunderground explosions (whichareofinterestto armscontrolexperts). Eachpoint\nis defined by two input values, x and x , that refer to body and surface wave magnitudes\n1 2\ncomputed from the seismic signal. Given these training data, the task of classification is to\nlearn ahypothesis hthat willtake new (x ,x )points andreturn either 0forearthquakes or\n1 2\n1forexplosions.\n7.5 7.5\n7 7\n6.5 6.5\n6 6\n5.5 5.5\n5 5\n4.5 4.5\n4 4\n3.5 3.5\n3 3\n2.5 2.5\n4.5 5 5.5 6 6.5 7 4.5 5 5.5 6 6.5 7\nx x\n1 1\n(a) (b)\nFigure18.15 (a)Plotoftwoseismicdataparameters,bodywavemagnitudex andsur-\n1\nface wave magnitudex , forearthquakes(white circles) andnuclearexplosions(blackcir-\n2\ncles)occurringbetween1982and1990inAsiaandtheMiddleEast(Kebeasyetal.,1998).\nAlsoshownisadecisionboundarybetweentheclasses. (b)Thesamedomainwithmoredata\npoints.Theearthquakesandexplosionsarenolongerlinearlyseparable.\nDECISION A decision boundary is a line (or a surface, in higher dimensions) that separates the\nBOUNDARY\ntwo classes. In Figure 18.15(a), the decision boundary is a straight line. A linear decision\nboundaryiscalledalinearseparatoranddatathatadmitsuchaseparatorarecalled linearly\nLINEARSEPARATOR\nLINEAR separable. Thelinearseparatorinthiscaseisdefinedby\nSEPARABILITY\nx = 1.7x \u22124.9 or \u22124.9+1.7x \u2212x = 0.\n2 1 1 2\nTheexplosions,whichwewanttoclassifywithvalue1,aretotherightofthislinewithhigher\nvalues of x and lower values of x , so they are points for which \u22124.9+1.7x \u2212x > 0,\n1 2 1 2\nwhile earthquakes have \u22124.9 + 1.7x \u2212 x < 0. Using the convention of a dummy input\n1 2\nx =1,wecanwritetheclassification hypothesis as\n0\nh (x)= 1ifw\u00b7x \u2265 0and0otherwise.\nw\nx\n2\nx\n2 724 Chapter 18. LearningfromExamples\nAlternatively, we can think of h as the result of passing the linear function w\u00b7x through a\nTHRESHOLD thresholdfunction:\nFUNCTION\nh (x)= Threshold(w\u00b7x)whereThreshold(z)=1ifz \u22650and0otherwise.\nw\nThethreshold function isshowninFigure18.17(a).\nNow that the hypothesis h (x) has a well-defined mathematical form, we can think\nw\nabout choosing the weights w to minimize the loss. In Sections 18.6.1 and 18.6.2, we did\nthis both in closed form (by setting the gradient to zero and solving for the weights) and\nby gradient descent in weight space. Here, we cannot do either of those things because the\ngradient is zero almost everywhere in weight space except at those points where w\u00b7x=0,\nandatthosepointsthegradientisundefined.\nThere is, however, a simple weight update rule that converges to a solution\u2014that is, a\nlinearseparatorthatclassifiesthedataperfectly\u2013provided thedataarelinearlyseparable. For\nasingleexample(x,y),wehave\nw \u2190 w +\u03b1(y\u2212h (x))\u00d7x (18.7)\ni i w i\nwhichisessentiallyidenticaltotheEquation(18.6),theupdateruleforlinearregression! This\nPERCEPTRON ruleiscalledtheperceptronlearningrule,forreasonsthatwillbecomeclearinSection18.7.\nLEARNINGRULE\nBecause weareconsidering a0\/1classification problem, however, thebehavior issomewhat\ndifferent. Boththetruevalue yandthehypothesisoutputh (x)areeither0or1,sothereare\nw\nthreepossibilities:\n\u2022 Iftheoutputiscorrect, i.e.,y=h (x),thentheweightsarenotchanged.\nw\n\u2022 Ifyis1buth (x)is0,thenw isincreasedwhenthecorresponding inputx ispositive\nw i i\nand decreased when x is negative. This makes sense, because we want to make w\u00b7x\ni\nbiggersothath (x)outputsa1.\nw\n\u2022 Ifyis0buth (x)is1,thenw isdecreasedwhenthecorrespondinginputx ispositive\nw i i\nand increased when x is negative. This makes sense, because we want to make w\u00b7x\ni\nsmallersothat h (x)outputsa0.\nw\nTypically the learning rule is applied one example at a time, choosing examples at random\n(as in stochastic gradient descent). Figure 18.16(a) shows atraining curve forthis learning\nTRAININGCURVE\nrule applied to the earthquake\/explosion data shown in Figure 18.15(a). A training curve\nmeasures the classifier performance on a fixed training set as the learning process proceeds\non that same training set. The curve shows the update rule converging to a zero-error linear\nseparator. The\u201cconvergence\u201dprocessisn\u2019texactlypretty,butitalwaysworks. Thisparticular\nruntakes657stepstoconverge,foradatasetwith63examples,soeachexampleispresented\nroughly 10timesonaverage. Typically, thevariation acrossrunsisverylarge.\nWe have said that the perceptron learning rule converges to a perfect linear separator\nwhen the data points are linearly separable, but what if they are not? This situation is all\ntoo common in the real world. For example, Figure 18.15(b) adds back in the data points\nleft out by Kebeasy et al. (1998) when they plotted the data shown in Figure 18.15(a). In\nFigure 18.16(b), we show the perceptron learning rule failing to converge even after 10,000\nsteps: even though it hits the minimum-error solution (three errors) many times, the algo-\nrithm keeps changing the weights. In general, the perceptron rule may not converge to a Section18.6. RegressionandClassification withLinearModels 725\n1 1 1\n0.9 0.9 0.9\n0.8 0.8 0.8\n0.7 0.7 0.7\n0.6 0.6 0.6\n0.5 0.5 0.5\n0.4 0.4 0.4\n0 100 200 300 400 500 600 700 0 20000 40000 60000 80000 100000 0 20000 40000 60000 80000 100000\nNumber of weight updates Number of weight updates Number of weight updates\n(a) (b) (c)\nFigure 18.16 (a) Plot of total training-setaccuracyvs. numberof iterationsthroughthe\ntraining set for the perceptron learning rule, given the earthquake\/explosion data in Fig-\nure 18.15(a). (b) The same plot for the noisy, non-separable data in Figure 18.15(b); note\nthechangeinscale ofthex-axis. (c)Thesame plotasin(b),witha learningrateschedule\n\u03b1(t)=1000\/(1000+t).\nstable solution for fixed learning rate \u03b1, but if \u03b1 decays as O(1\/t) where t is the iteration\nnumber,thentherulecanbeshowntoconverge toaminimum-errorsolution whenexamples\nare presented in a random sequence.7 It can also be shown that finding the minimum-error\nsolution isNP-hard,sooneexpectsthatmanypresentations oftheexampleswillberequired\nfor convergence to be achieved. Figure 18.16(b) shows the training process with a learning\nrate schedule \u03b1(t)=1000\/(1000 + t): convergence is not perfect after 100,000 iterations,\nbutitismuchbetterthanthefixed-\u03b1case.\n18.6.4 Linearclassificationwithlogisticregression\nWe have seen that passing the output of a linear function through the threshold function\ncreates a linear classifier; yet the hard nature of the threshold causes some problems: the\nhypothesis h (x)isnotdifferentiable andisinfactadiscontinuous function ofitsinputsand\nw\nitsweights;thismakeslearningwiththeperceptronruleaveryunpredictable adventure. Fur-\nthermore, the linear classifier always announces acompletely confident prediction of 1or0,\neven for examples that are very close to the boundary; in many situations, we really need\nmoregradatedpredictions.\nAlloftheseissuescanberesolvedtoalargeextentbysofteningthethresholdfunction\u2014\napproximating the hard threshold with a continuous, differentiable function. In Chapter 14\n(page 522), we saw two functions that look like soft thresholds: the integral of the standard\nnormal distribution (used for the probit model) and the logistic function (used for the logit\nmodel). Although thetwofunctions areverysimilarinshape, thelogistic function\n1\nLogistic(z) =\n1+e\u2212z\nP P\n7 Technically, we require that \u221e \u03b1(t)=\u221e and \u221e \u03b12(t) < \u221e. The decay \u03b1(t)=O(1\/t) satisfies\nt=1 t=1\ntheseconditions.\ntcerroc\nnoitroporP\ntcerroc\nnoitroporP\ntcerroc\nnoitroporP 726 Chapter 18. LearningfromExamples\n1 1\n1\n0.8\n0.6\n0.5 0.5 0.4\n0.2 -4\n-2\n0 -2 0 x 2 4 6 10 8 6 4 2 0 x 2\n1\n0 0\n-8 -6 -4 -2 0 2 4 6 8 -6 -4 -2 0 2 4 6\n(a) (b) (c)\nFigure 18.17 (a) The hard threshold function Threshold(z) with 0\/1 output. Note\nthat the function is nondifferentiable at z=0. (b) The logistic function, Logistic(z) =\n1 , also known as the sigmoid function. (c) Plot of a logistic regression hypothesis\n1+e\u2212z\nh w(x)=Logistic(w\u00b7x)forthedatashowninFigure18.15(b).\nhas more convenient mathematical properties. The function is shown in Figure 18.17(b).\nWiththelogisticfunction replacing thethreshold function, wenowhave\n1\nh (x)= Logistic(w\u00b7x) = .\nw 1+e\u2212w\u00b7x\nAnexampleofsuchahypothesisforthetwo-inputearthquake\/explosion problemisshownin\nFigure 18.17(c). Notice that the output, being anumberbetween 0and 1, can beinterpreted\nas a probability of belonging to the class labeled 1. The hypothesis forms a soft boundary\nin the input space and gives a probability of 0.5 for any input at the center of the boundary\nregion,andapproaches 0or1aswemoveawayfromtheboundary.\nTheprocess offittingtheweights ofthis modeltominimize lossonadatasetiscalled\nLOGISTIC logisticregression. Thereisnoeasyclosed-formsolutiontofindtheoptimalvalueofwwith\nREGRESSION\nthismodel,butthegradient descent computation isstraightforward. Becauseourhypotheses\nno longer output just 0 or 1, we will use the L loss function; also, to keep the formulas\n2\n(cid:2)\nreadable, we\u2019lluseg tostandforthelogisticfunction, withg itsderivative.\nFor a single example (x,y), the derivation of the gradient is the same as for linear\nregression (Equation (18.5)) up to the point where the actual form of his inserted. (Forthis\n(cid:2)\nderivation, wewillneedthechainrule: \u2202g(f(x))\/\u2202x=g (f(x))\u2202f(x)\/\u2202x.) Wehave\nCHAINRULE\n\u2202 \u2202\nLoss(w) = (y\u2212h (x))2\nw\n\u2202w \u2202w\ni i\n\u2202\n= 2(y\u2212h (x))\u00d7 (y\u2212h (x))\nw w\n\u2202w\ni\n\u2202\n= \u22122(y\u2212h (x))\u00d7g(cid:2) (w\u00b7x)\u00d7 w\u00b7x\nw\n\u2202w\ni\n= \u22122(y\u2212h (x))\u00d7g(cid:2) (w\u00b7x)\u00d7x .\nw i Section18.7. ArtificialNeuralNetworks 727\n1 1 1\n0.9 0.9 0.9\n0.8 0.8 0.8\n0.7 0.7 0.7\n0.6 0.6 0.6\n0.5 0.5 0.5\n0.4 0.4 0.4\n0 1000 2000 3000 4000 5000 0 20000 40000 60000 80000 100000 0 20000 40000 60000 80000 100000\nNumber of weight updates Number of weight updates Number of weight updates\n(a) (b) (c)\nFigure 18.18 Repeat of the experiments in Figure 18.16 using logistic regression and\nsquarederror. Theplotin(a)covers5000iterationsrather than1000,while(b)and(c)use\nthesamescale.\nThederivative g(cid:2) ofthelogistic functionsatisfiesg(cid:2) (z)=g(z)(1\u2212g(z)), sowehave\ng(cid:2) (w\u00b7x) = g(w\u00b7x)(1\u2212g(w\u00b7x)) =h (x)(1\u2212h (x))\nw w\nsotheweightupdateforminimizingthelossis\nw \u2190 w +\u03b1(y\u2212h (x))\u00d7h (x)(1\u2212h (x))\u00d7x . (18.8)\ni i w w w i\nRepeating the experiments of Figure 18.16 with logistic regression instead of the linear\nthreshold classifier, we obtain the results shown in Figure 18.18. In (a), the linearly sep-\narable case, logistic regression is somewhat slower to converge, but behaves much more\npredictably. In (b) and (c), where the data are noisy and nonseparable, logistic regression\nconverges farmorequickly andreliably. Theseadvantages tendtocarryoverintoreal-world\napplications and logistic regression has become one of the most popular classification tech-\nniquesforproblemsinmedicine,marketingandsurveyanalysis,creditscoring,publichealth,\nandotherapplications.\n18.7 ARTIFICIAL NEURAL NETWORKS\nWe turn now to what seems to be a somewhat unrelated topic: the brain. In fact, as we\nwill see, the technical ideas we have discussed so far in this chapter turn out to be useful in\nbuildingmathematicalmodelsofthebrain\u2019sactivity;conversely, thinkingaboutthebrainhas\nhelpedinextendingthescopeofthetechnical ideas.\nChapter 1 touched briefly on the basic findings of neuroscience\u2014in particular, the hy-\npothesis that mental activity consists primarily of electrochemical activity in networks of\nbrain cells called neurons. (Figure 1.2on page 11 showed a schematic diagram of atypical\nneuron.) Inspired by this hypothesis, some of the earliest AI work aimed to create artificial\nneural networks. (Other names for the field include connectionism, parallel distributed\nNEURALNETWORK\nprocessing, and neural computation.) Figure 18.19 shows a simple mathematical model\nof the neuron devised by McCulloch and Pitts (1943). Roughly speaking, it \u201cfires\u201d when a\nlinearcombinationofitsinputsexceedssome(hardorsoft) threshold\u2014that is,itimplements\nelpmaxe\nrep\nrorre\nderauqS\nelpmaxe\nrep\nrorre\nderauqS\nelpmaxe\nrep\nrorre\nderauqS 728 Chapter 18. LearningfromExamples\nBias Weight\na 0 = 1 w a j= g(in j)\n0,j\ng\nin\nw \u03a3 j\ni,j\na a\ni j\nInput Input Activation Output\nOutput\nLinks Function Function Links\nFigure(cid:2)18.19 Asimplemathematicalmodelforaneuron. Theunit\u2019soutputactivationis\nn\naj=g( i=0wi,jai),whereaiistheoutputactivationofunitiandwi,j istheweightonthe\nlinkfromunititothisunit.\na linear classifier of the kind described in the preceding section. A neural network is just a\ncollection of units connected together; the properties of the network are determined by its\ntopology andtheproperties ofthe\u201cneurons.\u201d\nSince 1943, much more detailed and realistic models have been developed, both for\nneurons and for larger systems in the brain, leading to the modern field of computational\nCOMPUTATIONAL neuroscience. On the other hand, researchers in AI and statistics became interested in the\nNEUROSCIENCE\nmoreabstractproperties ofneuralnetworks, suchastheirabilitytoperformdistributed com-\nputation, totolerate noisyinputs, andtolearn. Although weunderstand nowthatotherkinds\nof systems\u2014including Bayesian networks\u2014have these properties, neural networks remain\none of the most popular and effective forms of learning system and are worthy of study in\ntheirownright.\n18.7.1 Neural network structures\nNeural networks are composed of nodes or units (see Figure 18.19) connected by directed\nUNIT\nlinks. Alinkfromunititounitj servestopropagatetheactivationa fromitoj.8 Eachlink\nLINK i\nalso hasanumeric weightw associated withit, which determines the strength andsign of\nACTIVATION i,j\ntheconnection. Justasinlinearregression models,eachunithasadummyinputa =1with\nWEIGHT 0\nanassociated weightw . Eachunitj firstcomputesaweightedsumofitsinputs:\n0,j\n(cid:12)n\nin = w a .\nj i,j i\ni=0\nACTIVATION Thenitappliesanactivation functiong tothissumtoderivetheoutput:\nFUNCTION (cid:31)\n(cid:12)n\na = g(in )= g w a . (18.9)\nj j i,j i\ni=0\n8 Anoteonnotation: forthissection,weareforcedtosuspendourusualconventions. Inputattributesarestill\nindexed by i , so that an \u201cexternal\u201d activation ai is given by input xi; but index j will refer to internal units\nratherthanexamples. Throughoutthissection,themathematicalderivationsconcernasinglegenericexamplex,\nomittingtheusualsummationsoverexamplestoobtainresultsforthewholedataset. Section18.7. ArtificialNeuralNetworks 729\nTheactivation function g istypically eitherahardthreshold (Figure18.17(a)), inwhichcase\ntheunitiscalledaperceptron,oralogisticfunction(Figure18.17(b)),inwhichcasetheterm\nPERCEPTRON\nSIGMOID sigmoid perceptron is sometimes used. Both of these nonlinear activation function ensure\nPERCEPTRON\ntheimportantpropertythattheentirenetworkofunitscanrepresentanonlinearfunction(see\nExercise18.22). Asmentionedinthediscussionoflogisticregression(page725),thelogistic\nactivation functionhastheaddedadvantage ofbeingdifferentiable.\nHaving decided on the mathematical model for individual \u201cneurons,\u201d the next task is\nto connect them together to form a network. There are two fundamentally distinct ways to\nFEED-FORWARD do this. A feed-forward network has connections only in one direction\u2014that is, it forms a\nNETWORK\ndirectedacyclicgraph. Everynodereceivesinputfrom\u201cupstream\u201dnodesanddeliversoutput\nto\u201cdownstream\u201d nodes; therearenoloops. Afeed-forward networkrepresents afunction of\nitscurrentinput;thus,ithasnointernalstateotherthantheweightsthemselves. Arecurrent\nRECURRENT network, on the other hand, feeds its outputs back into its own inputs. This means that\nNETWORK\ntheactivation levelsofthenetworkformadynamical system thatmayreach astablestateor\nexhibitoscillationsorevenchaoticbehavior. Moreover,theresponseofthenetworktoagiven\ninput depends on its initial state, which may depend on previous inputs. Hence, recurrent\nnetworks (unlike feed-forward networks) can support short-term memory. This makes them\nmore interesting as models of the brain, but also more difficult to understand. This section\nwill concentrate on feed-forward networks; some pointers for further reading on recurrent\nnetworksaregivenattheendofthechapter.\nFeed-forwardnetworksareusuallyarrangedin layers,suchthateachunitreceivesinput\nLAYERS\nonlyfromunitsintheimmediatelyprecedinglayer. Inthenexttwosubsections, wewilllook\nat single-layer networks, in which every unit connects directly from the network\u2019s inputs to\nitsoutputs, andmultilayernetworks, whichhaveoneormore layersofhiddenunitsthatare\nHIDDENUNIT\nnot connected tothe outputs of the network. Sofarinthis chapter, wehave considered only\nlearningproblemswithasingleoutputvariable y,butneuralnetworksareoftenusedincases\nwhere multiple outputs are appropriate. For example, if we want to train a network to add\ntwoinput bits, each a0ora1, wewillneed oneoutput forthesum bit andone forthecarry\nbit. Also, whenthelearning problem involves classification intomorethan twoclasses\u2014for\nexample,whenlearningtocategorizeimagesofhandwritten digits\u2014itiscommontouseone\noutputunitforeachclass.\n18.7.2 Single-layerfeed-forward neural networks (perceptrons)\nAnetworkwithalltheinputsconnecteddirectlytotheoutputsiscalledasingle-layerneural\nPERCEPTRON network, or a perceptron network. Figure 18.20 shows a simple two-input, two-output\nNETWORK\nperceptronnetwork. Withsuchanetwork,wemighthopetolearnthetwo-bitadderfunction,\nforexample. Hereareallthetrainingdatawewillneed:\nx x y (carry) y (sum)\n1 2 3 4\n0 0 0 0\n0 1 0 1\n1 0 0 1\n1 1 1 0 730 Chapter 18. LearningfromExamples\nThefirstthingtonoticeisthataperceptronnetworkwithmoutputsisreallymseparate\nnetworks, because each weight affects only one of the outputs. Thus, there will be m sepa-\nrate training processes. Furthermore, depending on the type of activation function used, the\ntrainingprocesseswillbeeitherthe perceptronlearningrule(Equation(18.7)onpage724)\norgradient descentruleforthe logistic regression (Equation(18.8)onpage727).\nIfyoutryeithermethodonthetwo-bit-adderdata,somethinginteresting happens. Unit\n3 learns the carry function easily, but unit 4 completely fails to learn the sum function. No,\nunit 4 isnot defective! The problem iswith the sum function itself. Wesawin Section 18.6\nthatlinearclassifiers(whetherhardorsoft)canrepresent lineardecisionboundariesinthein-\nputspace. Thisworksfineforthecarryfunction,whichisalogicalAND(seeFigure18.21(a)).\nThesum function, however, is an XOR (exclusive OR)of the two inputs. AsFigure 18.21(c)\nillustrates, thisfunction isnotlinearly separable sothe perceptron cannotlearnit.\nThe linearly separable functions constitute just a small fraction of all Boolean func-\ntions; Exercise 18.20 asksyou toquantify thisfraction. Theinability ofperceptrons tolearn\neven such simple functions as XOR was a significant setback to the nascent neural network\nw w w\n1,3 1,3 3,5\n1 3 1 3 5\nw w w\n1,4 1,4 3,6\nw w w\n2,3 2,3 4,5\n2 4 2 4 6\nw w w\n2,4 2,4 4,6\n(a) (b)\nFigure18.20 (a)Aperceptronnetworkwithtwoinputsandtwooutputunits.(b)Aneural\nnetworkwithtwoinputs,onehiddenlayeroftwounits,andoneoutputunit. Notshownare\nthedummyinputsandtheirassociatedweights.\nx x x\n1 1 1\n1 1 1\n?\n0 0 0\n0 1 x 2 0 1 x 2 0 1 x 2\n(a)x andx (b)x orx (c)x xorx\n1 2 1 2 1 2\nFigure18.21 Linearseparabilityinthresholdperceptrons. Blackdotsindicateapointin\ntheinputspacewherethevalueofthefunctionis1,andwhitedotsindicateapointwherethe\nvalueis0. Theperceptronreturns1ontheregiononthenon-shadedsideoftheline. In(c),\nnosuchlineexiststhatcorrectlyclassifiestheinputs. Section18.7. ArtificialNeuralNetworks 731\n1 1\n0.9 0.9\n0.8 0.8\n0.7 0.7\n0.6 Perceptron 0.6\nDecision tree\n0.5 0.5 Perceptron\nDecision tree\n0.4 0.4\n0 10 20 30 40 50 60 70 80 90 100 0 10 20 30 40 50 60 70 80 90 100\nTraining set size Training set size\n(a) (b)\nFigure18.22 Comparingtheperformanceofperceptronsanddecisiontrees. (a)Percep-\ntronsarebetteratlearningthemajorityfunctionof11inputs. (b)Decisiontreesarebetterat\nlearningtheWillWait predicateintherestaurantexample.\ncommunity in the 1960s. Perceptrons are far from useless, however. Section 18.6.4 noted\nthat logistic regression (i.e., training asigmoid perceptron) is eventoday a very popular and\neffective tool. Moreover, a perceptron can represent some quite \u201ccomplex\u201d Boolean func-\ntions very compactly. For example, the majority function, which outputs a 1 only if more\nthan half ofitsninputs are 1, canberepresented byaperceptron witheach w =1andwith\ni\nw =\u2212n\/2. Adecisiontreewouldneedexponentiallymanynodestorepresentthisfunction.\n0\nFigure 18.22 shows the learning curve fora perceptron on two different problems. On\nthe left, we show the curve for learning the majority function with 11 Boolean inputs (i.e.,\nthefunctionoutputsa1if6ormoreinputsare1). Aswewouldexpect,theperceptronlearns\nthe function quite quickly, because the majority function is linearly separable. On the other\nhand,thedecision-tree learnermakesnoprogress, because themajorityfunctionisveryhard\n(althoughnotimpossible) torepresentasadecisiontree. Ontheright,wehavetherestaurant\nexample. The solution problem is easily represented as a decision tree, but is not linearly\nseparable. Thebestplanethrough thedatacorrectlyclassifiesonly65%.\n18.7.3 Multilayerfeed-forward neural networks\n(McCulloch andPitts,1943)werewellawarethatasinglethreshold unitwouldnotsolveall\ntheir problems. In fact, their paper proves that such a unit can represent the basic Boolean\nfunctions AND, OR,and NOT andthen goes ontoargue that anydesired functionality canbe\nobtainedbyconnecting largenumbersofunitsinto(possibly recurrent)networksofarbitrary\ndepth. Theproblem wasthatnobodyknewhowtotrainsuchnetworks.\nThis turns out to be an easy problem if we think of a network the right way: as a\nfunction h (x)parameterized bytheweights w. Considerthesimplenetwork showninFig-\nw\nure 18.20(b), which hastwoinput units, twohidden units, and twooutput unit. (Inaddition,\neach unit has a dummy input fixed at 1.) Given an input vector x=(x ,x ), the activations\n1 2\ntes\ntset\nno\ntcerroc\nnoitroporP\ntes\ntset\nno\ntcerroc\nnoitroporP 732 Chapter 18. LearningfromExamples\nh (x,x) h (x,x)\nW 1 2 W 1 2\n1 1\n0.8 0.8\n0.6 0.6\n0.4 0.4\n0.2 0.2\n4 4\n0 2 0 2\n-4 -2 x 10 2 4 -4 -2 0 x 2 -4 -2 x 10 2 4 -4 -2 0 x 2\n(a) (b)\nFigure18.23 (a)Theresultofcombiningtwoopposite-facingsoftthresholdfunctionsto\nproducearidge.(b)Theresultofcombiningtworidgestoproduceabump.\noftheinputunitsaresetto(a ,a )=(x ,x ). Theoutputatunit5isgivenby\n1 2 1 2\na = g(w w a +w a )\n5 0,5,+ 3,5 3 4,5 4\n= g(w w g(w +w a +w a )+w g(w 4+w a +w a ))\n0,5,+ 3,5 0,3 1,3 1 2,3 2 4,5 0 1,4 1 2,4 2\n= g(w w g(w +w x +w x )+w g(w 4+w x +w x )).\n0,5,+ 3,5 0,3 1,3 1 2,3 2 4,5 0 1,4 1 2,4 2\nThus, we have the output expressed as a function of the inputs and the weights. A similar\nexpression holds for unit 6. As long as we can calculate the derivatives of such expressions\nwith respect to the weights, we can use the gradient-descent loss-minimization method to\ntrain the network. Section 18.7.4 shows exactly how to do this. And because the function\nrepresentedbyanetworkcanbehighlynonlinear\u2014composed, asitis,ofnestednonlinearsoft\nNONLINEAR threshold functions\u2014we canseeneuralnetworksasatoolfor doingnonlinearregression.\nREGRESSION\nBefore delving into learning rules, let us look at the ways in which networks generate\ncomplicated functions. First,rememberthateachunitinasigmoidnetwork represents asoft\nthreshold in its input space, as shown in Figure 18.17(c) (page 726). With one hidden layer\nand one output layer, as in Figure 18.20(b), each output unit computes a soft-thresholded\nlinear combination of several such functions. For example, by adding two opposite-facing\nsoftthresholdfunctionsandthresholdingtheresult,wecanobtaina\u201cridge\u201dfunctionasshown\ninFigure 18.23(a). Combining twosuch ridges atright angles toeach other(i.e., combining\ntheoutputsfromfourhiddenunits), weobtaina\u201cbump\u201dasshowninFigure18.23(b).\nWithmorehiddenunits, wecanproduce morebumpsofdifferentsizesinmoreplaces.\nInfact,withasingle,sufficientlylargehiddenlayer,itispossibletorepresentanycontinuous\nfunction oftheinputs witharbitrary accuracy; withtwolayers, evendiscontinuous functions\ncanberepresented.9 Unfortunately, forany particular network structure, itishardertochar-\nacterizeexactlywhichfunctions canberepresented andwhichonescannot.\n9 Theproofiscomplex,butthemainpointisthattherequirednumberofhiddenunitsgrowsexponentiallywith\nthenumberofinputs.Forexample,2n\/nhiddenunitsareneededtoencodeallBooleanfunctionsofninputs. Section18.7. ArtificialNeuralNetworks 733\n18.7.4 Learning inmultilayernetworks\nFirst,letusdispensewithoneminorcomplicationarisinginmultilayernetworks: interactions\namong the learning problems when the network has multiple outputs. In such cases, we\nshouldthinkofthenetworkasimplementingavectorfunctionh ratherthanascalarfunction\nw\nh ; for example, the network in Figure 18.20(b) returns a vector [a ,a ]. Similarly, the\nw 5 6\ntarget output willbe avector y. Whereas aperceptron network decomposes into m separate\nlearningproblemsforanm-outputproblem,thisdecompositionfailsinamultilayernetwork.\nForexample, both a and a in Figure 18.20(b) depend on all of the input-layer weights, so\n5 6\nupdatestothoseweightswilldependonerrorsinbotha anda . Fortunately,thisdependency\n5 6\nis very simple in the case of any loss function that is additive across the components of the\nerrorvector y\u2212h (x). FortheL loss,wehave,foranyweightw,\nw 2\n(cid:12) (cid:12)\n\u2202 \u2202 \u2202 \u2202\nLoss(w)= |y\u2212h (x)|2 = (y \u2212a )2 = (y \u2212a )2 (18.10)\nw k k k k\n\u2202w \u2202w \u2202w \u2202w\nk k\nwhere the index k ranges over nodes in the output layer. Each term in the final summation\nis just the gradient of the loss for the kth output, computed as if the other outputs did not\nexist. Hence, we can decompose an m-output learning problem into m learning problems,\nprovidedweremembertoaddupthegradientcontributionsfromeachofthemwhenupdating\ntheweights.\nThe major complication comes from the addition of hidden layers to the network.\nWhereas the error y \u2212 h at the output layer is clear, the error at the hidden layers seems\nw\nmysterious because the training data do not say what value the hidden nodes should have.\nFortunately, it turns out that we can back-propagate the error from the output layer to the\nBACK-PROPAGATION\nhiddenlayers. Theback-propagationprocessemergesdirectlyfromaderivationoftheoverall\nerrorgradient. First,wewilldescribetheprocesswithanintuitivejustification; then,wewill\nshowthederivation.\nAt the output layer, the weight-update rule is identical to Equation (18.8). We have\nmultiple output units, so let Err be the kth component of the error vector y\u2212h . Wewill\nk w\nalso find it convenient to define a modified error \u0394 =Err\n\u00d7g(cid:2)\n(in ), so that the weight-\nk k k\nupdaterulebecomes\nw \u2190 w +\u03b1\u00d7a \u00d7\u0394 . (18.11)\nj,k j,k j k\nToupdate the connections between the input units and the hidden units, weneed to define a\nquantity analogous to the error term for output nodes. Here is where we do the error back-\npropagation. Theideaisthathiddennodej is\u201cresponsible\u201dforsomefractionoftheerror \u0394\nk\nin each of the output nodes to which itconnects. Thus, the \u0394 values are divided according\nk\ntothestrength oftheconnection betweenthehidden nodeand theoutput node andareprop-\nagated back to provide the \u0394 values for the hidden layer. The propagation rule for the \u0394\nj\nvaluesisthefollowing:\n(cid:12)\n(cid:2)\n\u0394 = g (in ) w \u0394 . (18.12)\nj j j,k k\nk 734 Chapter 18. LearningfromExamples\nfunctionBACK-PROP-LEARNING(examples,network)returnsaneuralnetwork\ninputs:examples,asetofexamples,eachwithinputvectorxandoutputvectory\nnetwork,amultilayernetworkwithLlayers,weightswi,j,activationfunctiong\nlocalvariables: \u0394,avectoroferrors,indexedbynetworknode\nrepeat\nforeachweightwi,j innetwork do\nwi,j\u2190asmallrandomnumber\nforeachexample(x,y)inexamples do\n\/*Propagatetheinputsforwardtocomputetheoutputs*\/\nforeachnodeiintheinputlayerdo\nai\u2190xi\nfor(cid:3)=2toLdo\nforeachno(cid:2)dejinlayer(cid:3)do\ninj\u2190\ni\nwi,j ai\naj\u2190g(inj)\n\/*Propagatedeltasbackwardfromoutputlayertoinputlayer*\/\nforeachnodej intheoutputlayerdo\n\u0394[j]\u2190g(cid:5)(inj) \u00d7 (yj \u2212 aj)\nfor(cid:3)=L\u22121to1do\nforeachnodeiinl(cid:2)ayer(cid:3)do\n\u0394[i]\u2190g(cid:5)(ini)\nj\nwi,j \u0394[j]\n\/*Updateeveryweightinnetworkusingdeltas*\/\nforeachweightwi,j innetwork do\nwi,j\u2190wi,j + \u03b1 \u00d7 ai \u00d7 \u0394[j]\nuntilsomestoppingcriterionissatisfied\nreturnnetwork\nFigure18.24 Theback-propagationalgorithmforlearninginmultilayernetworks.\nNowtheweight-updaterulefortheweightsbetweentheinputsandthehiddenlayerisessen-\ntiallyidenticaltotheupdaterulefortheoutput layer:\nw \u2190 w +\u03b1\u00d7a \u00d7\u0394 .\ni,j i,j i j\nTheback-propagation processcanbesummarizedasfollows:\n\u2022 Computethe\u0394valuesfortheoutputunits, usingtheobserved error.\n\u2022 Starting with output layer, repeat the following foreach layer in the network, until the\nearliesthiddenlayerisreached:\n\u2013 Propagatethe\u0394valuesbacktothepreviouslayer.\n\u2013 Updatetheweightsbetweenthetwolayers.\nThedetailed algorithm isshowninFigure18.24.\nFor the mathematically inclined, we will now derive the back-propagation equations\nfrom first principles. The derivation is quite similar to the gradient calculation for logistic Section18.7. ArtificialNeuralNetworks 735\nregression (leading uptoEquation (18.8) onpage 727), except that wehavetouse thechain\nrulemorethanonce.\nFollowing Equation (18.10), we compute just the gradient for Loss = (y \u2212a )2 at\nk k k\nthe kthoutput. Thegradient ofthis loss with respect toweights connecting the hidden layer\nto the output layer will be zero except for weights w that connect to the kth output unit.\nj,k\nForthoseweights, wehave\n\u2202Loss \u2202a \u2202g(in )\nk = \u22122(y \u2212a ) k = \u22122(y \u2212a ) k\nk k k k\n\u2202w \u2202w \u2202w\nj,k j,k j,k \u239b \u239e\n(cid:12)\n\u2202in \u2202\n= \u22122(y \u2212a )g(cid:2) (in ) k = \u22122(y \u2212a )g(cid:2) (in ) \u239d w a \u23a0\nk k k k k k j,k j\n\u2202w \u2202w\nj,k j,k\nj\n= \u22122(y \u2212a )g(cid:2) (in )a = \u2212a \u0394 ,\nk k k j j k\nwith\u0394 definedasbefore. Toobtainthegradientwithrespecttothe w weightsconnecting\nk i,j\ntheinputlayertothehidden \u00a1layer, wehavetoexpandoutthe activations a andreapply the\nj\nchainrule. Wewillshowthederivation ingorydetail because itisinteresting toseehowthe\nderivativeoperatorpropagates backthrough thenetwork:\n\u2202Loss \u2202a \u2202g(in )\nk = \u22122(y \u2212a ) k = \u22122(y \u2212a ) k\nk k k k\n\u2202w \u2202w \u2202w\ni,j i,j i,j\u239b \u239e\n(cid:12)\n\u2202in \u2202\n= \u22122(y \u2212a )g(cid:2) (in ) k = \u22122\u0394 \u239d w a \u23a0\nk k k k j,k j\n\u2202w \u2202w\ni,j i,j\nj\n\u2202a \u2202g(in )\n= \u22122\u0394 w j = \u22122\u0394 w j\nk j,k k j,k\n\u2202w \u2202w\ni,j i,j\n\u2202in\n= \u22122\u0394 w g(cid:2) (in ) j\nk j,k j\n\u2202w\ni,j (cid:31)\n(cid:12)\n\u2202\n= \u22122\u0394 w g(cid:2) (in ) w a\nk j,k j i,j i\n\u2202w\ni,j\ni\n= \u22122\u0394 w g(cid:2) (in )a = \u2212a \u0394 ,\nk j,k j i i j\nwhere\u0394 isdefinedasbefore. Thus,weobtaintheupdaterulesobtainedearlierfromintuitive\nj\nconsiderations. Itisalsoclearthattheprocess canbecontinued fornetworkswithmorethan\nonehiddenlayer, whichjustifiesthegeneralalgorithm giveninFigure18.24.\nHaving made it through (or skipped over) all the mathematics, let\u2019s see how a single-\nhidden-layer network performs on the restaurant problem. First, we need to determine the\nstructure of the network. We have 10 attributes describing each example, so we will need\n10 input units. Should we have one hidden layer or two? How many nodes in each layer?\nShouldtheybefullyconnected? Thereisnogoodtheorythatwilltellustheanswer. (Seethe\nnextsection.) Asalways,wecanusecross-validation: tryseveraldifferentstructures andsee\nwhichoneworksbest. Itturnsoutthatanetworkwithonehiddenlayercontainingfournodes\nis about right for this problem. In Figure 18.25, we show two curves. The first is a training\ncurve showing the mean squared error on a given training set of 100 restaurant examples 736 Chapter 18. LearningfromExamples\n14 1\n12 0.9\n10\n0.8\n8\n0.7\n6\n0.6 Decision tree\n4 Multilayer network\n2 0.5\n0 0.4\n0 50 100 150 200 250 300 350 400 0 10 20 30 40 50 60 70 80 90 100\nNumber of epochs Training set size\n(a) (b)\nFigure 18.25 (a) Training curve showing the gradual reduction in error as weights are\nmodified over several epochs, for a given set of examples in the restaurant domain. (b)\nComparativelearningcurvesshowingthatdecision-treelearningdoesslightlybetteronthe\nrestaurantproblemthanback-propagationinamultilayernetwork.\nduringtheweight-updatingprocess. Thisdemonstratesthatthenetworkdoesindeedconverge\nto a perfect fit to the training data. The second curve is the standard learning curve for the\nrestaurant data. The neural network does learn well, although not quite as fast as decision-\ntree learning; this is perhaps not surprising, because the data were generated from a simple\ndecision treeinthefirstplace.\nNeural networks are capable of farmore complex learning tasks of course, although it\nmust be said that a certain amount of twiddling is needed to get the network structure right\nandtoachieveconvergence tosomethingclosetotheglobaloptimuminweightspace. There\nare literally tens of thousands of published applications of neural networks. Section 18.11.1\nlooksatonesuchapplication inmoredepth.\n18.7.5 Learning neural network structures\nSofar,wehaveconsidered theproblem oflearning weights, givenafixednetwork structure;\njust as with Bayesian networks, we also need to understand how to find the best network\nstructure. Ifwechooseanetworkthatistoobig,itwillbeabletomemorizealltheexamples\nby forming a large lookup table, but will not necessarily generalize well to inputs that have\nnotbeenseenbefore.10 Inotherwords,likeallstatisticalmodels,neuralnetworksaresubject\ntooverfitting whenthere aretoo manyparameters inthe model. Wesawthis in Figure 18.1\n(page 696), where the high-parameter models in (b) and (c) fit all the data, but might not\ngeneralize aswellasthelow-parametermodelsin(a)and(d).\nIfwesticktofullyconnectednetworks,theonlychoicestobemadeconcernthenumber\n10 Ithasbeenobservedthatverylargenetworks dogeneralizewellaslongastheweightsarekeptsmall. This\nrestrictionkeepstheactivationvaluesinthelinearregionofthesigmoidfunctiong(x)wherexisclosetozero.\nThis,inturn,meansthatthenetworkbehaveslikealinearfunction(Exercise18.22)withfarfewerparameters.\ntes\ngniniart\nno\nrorre\nlatoT\ntes\ntset\nno\ntcerroc\nnoitroporP Section18.8. Nonparametric Models 737\nof hidden layers and their sizes. The usual approach is to try several and keep the best. The\ncross-validation techniques of Chapter 18 are needed if we are to avoid peeking at the test\nset. Thatis,wechoosethenetworkarchitecture thatgivesthehighestprediction accuracyon\nthevalidation sets.\nIf we want to consider networks that are not fully connected, then we need to find\nsomeeffectivesearchmethodthroughtheverylargespaceofpossibleconnectiontopologies.\nOPTIMALBRAIN The optimal brain damage algorithm begins with a fully connected network and removes\nDAMAGE\nconnections from it. After the network is trained for the first time, an information-theoretic\napproach identifies an optimal selection of connections that can be dropped. The network\nis then retrained, and if its performance has not decreased then the process is repeated. In\nadditiontoremovingconnections, itisalsopossible toremoveunitsthatarenotcontributing\nmuchtotheresult.\nSeveralalgorithmshavebeenproposedforgrowingalargernetworkfromasmallerone.\nOne, the tiling algorithm, resembles decision-list learning. The idea is to start with a single\nTILING\nunit that does its best to produce the correct output on as many of the training examples as\npossible. Subsequentunitsareaddedtotakecareoftheexamplesthatthefirstunitgotwrong.\nThealgorithm addsonlyasmanyunitsasareneededtocoveralltheexamples.\n18.8 NONPARAMETRIC MODELS\nLinearregression and neural networks use thetraining data toestimate afixedset ofparam-\neters w. Thatdefinesourhypothesis h (x),andatthatpointwecanthrowawaythetraining\nw\ndata, because they are all summarized by w. A learning model that summarizes data with a\nset of parameters of fixed size (independent of the number of training examples) is called a\nparametricmodel.\nPARAMETRICMODEL\nNo matter how much data you throw at a parametric model, it won\u2019t change its mind\nabouthowmanyparametersitneeds. Whendatasetsaresmall,itmakessensetohaveastrong\nrestrictionontheallowablehypotheses, toavoidoverfitting. Butwhentherearethousandsor\nmillionsorbillionsofexamplestolearnfrom,itseemslike abetterideatoletthedataspeak\nfor themselves rather than forcing them to speak through a tiny vector of parameters. If the\ndata say that the correct answer is a very wiggly function, we shouldn\u2019t restrict ourselves to\nlinearorslightly wigglyfunctions.\nNONPARAMETRIC Anonparametricmodelisonethatcannotbecharacterizedbyaboundedsetofparam-\nMODEL\neters. Forexample, suppose that each hypothesis wegenerate simply retains within itself all\nofthetraining examples andusesallofthemtopredict thenextexample. Suchahypothesis\nfamily would benonparametric because the effective number of parameters isunbounded\u2014\nINSTANCE-BASED it grows with the number of examples. This approach is called instance-based learning or\nLEARNING\nmemory-basedlearning. Thesimplestinstance-basedlearningmethodistablelookup: take\nTABLELOOKUP\nallthetrainingexamples,puttheminalookuptable,andthenwhenaskedforh(x),seeifxis\ninthe table; ifitis, return thecorresponding y. Theproblem withthismethod is thatitdoes\nnotgeneralize well: whenxisnotinthetableallitcandoisreturnsomedefault value. 738 Chapter 18. LearningfromExamples\n7.5 7.5\n7 7\n6.5 6.5\n6 6\n5.5 5.5\nx x\n1 5 1 5\n4.5 4.5\n4 4\n3.5 3.5\n3 3\n2.5 2.5\n4.5 5 5.5 6 6.5 7 4.5 5 5.5 6 6.5 7\nx x\n2 2\n(k=1) (k=5)\nFigure18.26 (a)Ak-nearest-neighbormodelshowingtheextentoftheexplosionclassfor\nthedatainFigure18.15,withk=1. Overfittingisapparent. (b)With k=5,theoverfitting\nproblemgoesawayforthisdataset.\n18.8.1 Nearestneighbor models\nWecanimproveontablelookupwithaslightvariation: givenaqueryx ,findthekexamples\nq\nNEAREST that are nearest to x . This is called k-nearest neighbors lookup. We\u2019ll use the notation\nNEIGHBORS q\nNN(k,x )todenotethesetofk nearestneighbors.\nq\nTo do classification, first find NN(k,x ), then take the plurality vote of the neighbors\nq\n(which is the majority vote in the case of binary classification). To avoid ties, k is always\nchosen to be an odd number. To do regression, we can take the mean or median of the k\nneighbors, orwecansolvealinearregression problem onthe neighbors.\nIn Figure 18.26, we show the decision boundary of k-nearest-neighbors classification\nfor k= 1 and 5 on the earthquake data set from Figure 18.15. Nonparametric methods are\nstillsubjecttounderfittingandoverfitting,justlikeparametricmethods. Inthiscase1-nearest\nneighborsisoverfitting;itreactstoomuchtotheblackoutlierintheupperrightandthewhite\noutlier at (5.4, 3.7). The 5-nearest-neighbors decision boundary is good; higher k would\nunderfit. Asusual, cross-validation canbeusedtoselectthebestvalueofk.\nThe very word \u201cnearest\u201d implies a distance metric. How do we measure the distance\nfrom a query point x to an example point x ? Typically, distances are measured with a\nq j\nMINKOWSKI MinkowskidistanceorLp norm,definedas\nDISTANCE (cid:12)\nLp(x ,x ) = ( |x \u2212x |p)1\/p .\nj q j,i q,i\ni\nWith p=2this isEuclidean distance and withp=1itis Manhattan distance. With Boolean\nattribute values, the number of attributes on which the two points differ is called the Ham-\nmingdistance. Often p=2isused ifthedimensions aremeasuring similarproperties, such\nHAMMINGDISTANCE\nasthewidth, height and depth ofparts onaconveyor belt, and Manhattan distance isused if\nthey aredissimilar, such as age, weight, and gender ofapatient. Note that ifweuse the raw\nnumbers from each dimension then the total distance will be affected by a change in scale\nin any dimension. That is, if we change dimension i from measurements in centimeters to Section18.8. Nonparametric Models 739\nmileswhilekeeping theotherdimensions thesame,we\u2019llgetdifferent nearest neighbors. To\navoidthis,itiscommontoapplynormalizationtothemeasurementsineachdimension. One\nNORMALIZATION\nsimple approach is to compute the mean \u03bc and standard deviation \u03c3 of the values in each\ni i\ndimension, and rescale them so that x becomes (x \u2212 \u03bc )\/\u03c3 . A more complex metric\nj,i j,i i i\nMAHALANOBIS knownastheMahalanobisdistancetakesintoaccountthecovariance betweendimensions.\nDISTANCE\nIn low-dimensional spaces with plenty of data, nearest neighbors works very well: we\nare likely to have enough nearby data points to get a good answer. But as the number of\ndimensions rises weencounter aproblem: the nearest neighbors inhigh-dimensional spaces\nareusually notverynear! Consider k-nearest-neighbors onadata setof N points uniformly\ndistributed throughout the interior of an n-dimensional unit hypercube. We\u2019ll define the k-\nneighborhood ofapointasthesmallesthypercube thatcontains thek-nearest neighbors. Let\n(cid:3)betheaveragesidelengthofaneighborhood. Thenthevolumeoftheneighborhood(which\ncontains k points) is (cid:3)n and the volume of the full cube (which contains N points) is 1. So,\nonaverage, (cid:3)n=k\/N. Takingnthrootsofbothsidesweget(cid:3) = (k\/N)1\/n.\nTo be concrete, let k=10 and N=1,000,000. In two dimensions (n=2; a unit\nsquare), the average neighborhood has (cid:3)=0.003, a small fraction of the unit square, and\nin3dimensions (cid:3)isjust2%oftheedgelengthoftheunitcube. Butbythetimewegetto17\ndimensions, (cid:3)ishalftheedgelengthoftheunithypercube, andin200dimensions itis94%.\nCURSEOF Thisproblem hasbeencalledthecurseofdimensionality.\nDIMENSIONALITY\nAnotherwaytolookatit: considerthepointsthatfallwithinathinshellmakingupthe\nouter 1% of the unit hypercube. These are outliers; in general it will be hard to find a good\nvalue forthembecause wewillbeextrapolating ratherthaninterpolating. Inonedimension,\nthese outliers are only 2% of the points on the unit line (those points where x < .01 or\nx > .99), but in 200 dimensions, over 98% of the points fall within this thin shell\u2014almost\nallthepoints areoutliers. Youcanseeanexample ofapoornearest-neighbors fitonoutliers\nifyoulookaheadtoFigure18.28(b).\nTheNN(k,x )function isconceptually trivial: givenasetofN examplesandaquery\nq\nx ,iteratethroughtheexamples,measurethedistanceto x fromeachone,andkeepthebest\nq q\nk. Ifwearesatisfiedwithanimplementation thattakesO(N)executiontime,thenthatisthe\nend of the story. But instance-based methods are designed for large data sets, so we would\nlike an algorithm with sublinear run time. Elementary analysis of algorithms tells us that\nexact table lookup is O(N) with a sequential table, O(logN) with a binary tree, and O(1)\nwith a hash table. We will now see that binary trees and hash tables are also applicable for\nfindingnearest neighbors.\n18.8.2 Finding nearest neighbors with k-dtrees\nAbalancedbinarytreeoverdatawithanarbitrarynumberofdimensionsiscalledak-dtree,\nK-DTREE\nfor k-dimensional tree. (In our notation, the number of dimensions is n, so they would be\nn-d trees. The construction of a k-d tree is similar to the construction of a one-dimensional\nbalancedbinarytree. Westartwithasetofexamplesandattherootnodewesplitthemalong\ntheithdimension bytesting whetherx \u2264 m. Wechose thevaluemtobethemedian ofthe\ni\nexamplesalongtheithdimension;thushalftheexampleswillbeintheleftbranchofthetree 740 Chapter 18. LearningfromExamples\n1 1\n0.9 0.9\n0.8 0.8\n0.7 0.7\n0.6 0.6\n0.5 0.5\n0.4 0.4\n0.3 0.3\n0.2 0.2\n0.1 0.1\n0 0\n25 50 75 100 125 150 175 200 25 50 75 100 125 150 175 200\nNumber of dimensions Number of dimensions\n(a) (b)\nFigure18.27 Thecurseofdimensionality:(a)Thelengthoftheaverageneighborhoodfor\n10-nearest-neighborsinaunithypercubewith1,000,000points,asafunctionofthenumber\nof dimensions. (b) The proportion of points that fall within a thin shell consisting of the\nouter1%ofthehypercube,asafunctionofthenumberofdimensions.Sampledfrom10,000\nrandomlydistributedpoints.\nand half inthe right. Wethen recursively makeatree fortheleft and right sets ofexamples,\nstopping when there are fewerthan two examples left. To choose a dimension to split on at\neach node of the tree, one can simply select dimension i mod n at level i of the tree. (Note\nthatwemayneedtosplitonanygivendimensionseveraltimesasweproceeddownthetree.)\nAnotherstrategyistosplitonthedimension thathasthewidestspreadofvalues.\nExact lookup from a k-d tree is just like lookup from a binary tree (with the slight\ncomplicationthatyouneedtopayattentiontowhichdimensionyouaretestingateachnode).\nBut nearest neighbor lookup is more complicated. As we go down the branches, splitting\nthe examples in half, in some cases we can discard the other half of the examples. But not\nalways. Sometimes the point we are querying for falls very close to the dividing boundary.\nThe query point itself might be on the left hand side of the boundary, but one or more of\nthe k nearest neighbors might actually be on the right-hand side. We have to test for this\npossibility by computing the distance of the query point to the dividing boundary, and then\nsearching bothsides ifwecan\u2019t findk examplesontheleftthatarecloserthanthisdistance.\nBecause ofthisproblem, k-dtreesareappropriate onlywhen therearemanymoreexamples\nthan dimensions, preferably at least 2n examples. Thus, k-d trees work well with up to 10\ndimensionswiththousandsofexamplesorupto20dimensionswithmillionsofexamples. If\nwedon\u2019thaveenoughexamples, lookupisnofasterthanalinearscanoftheentiredataset.\n18.8.3 Locality-sensitivehashing\nHash tables have the potential to provide even faster lookup than binary trees. But how can\nwefindnearestneighbors usingahashtable, whenhashcodesrelyonanexactmatch? Hash\ncodes randomly distribute values among the bins, but we want to have near points grouped\nLOCALITY-SENSITIVE togetherinthesamebin;wewantalocality-sensitive hash(LSH).\nHASH\ndoohrobhgien\nfo\nhtgnel\negdE\nllehs\nroiretxe\nni\nstniop\nfo\nnoitroporP Section18.8. Nonparametric Models 741\nWe can\u2019t use hashes to solve NN(k,x ) exactly, but with a clever use of randomized\nq\nalgorithms, we can find an approximate solution. First we define the approximate near-\nAPPROXIMATE neighborsproblem: given adata setofexample points and aquery point x ,find, withhigh\nNEAR-NEIGHBORS q\nprobability, an example point (orpoints) that isnear x . Tobe moreprecise, werequire that\nq\nif there is a point x that is within a radius r of x , then with high probability the algorithm\nj q\nwillfindapointx j(cid:3) thatiswithindistancecrofq. Ifthereisnopointwithinradiusrthenthe\nalgorithm isallowed toreport failure. Thevalues of cand \u201chigh probability\u201d areparameters\nofthealgorithm.\nTo solve approximate near neighbors, we will need a hash function g(x) that has the\npropertythat,foranytwopoints x\nj\nandx j(cid:3),theprobabilitythattheyhavethesamehashcode\nis small if their distance is more than cr, and is high if their distance is less than r. For\nsimplicity we will treat each point as a bit string. (Any features that are not Boolean can be\nencoded intoasetofBooleanfeatures.)\nThe intuition we rely on is that if two points are close together in an n-dimensional\nspace,thentheywillnecessarilybeclosewhenprojecteddownontoaone-dimensionalspace\n(aline). Infact, wecandiscretize thelineinto bins\u2014hash buckets\u2014so that, withhigh prob-\nability, nearpoints project downto exactly the same bin. Points that are faraway from each\notherwilltendtoproject downinto different binsformostprojections, but therewillalways\nbe a few projections that coincidentally project far-apart points into the same bin. Thus, the\nbin forpoint x contains many(but not all)points thatare nearto x ,as wellassome points\nq q\nthatarefaraway.\nThetrickofLSHistocreatemultiplerandomprojectionsandcombinethem. Arandom\nprojection is just a random subset of the bit-string representation. We choose (cid:3) different\nrandomprojectionsandcreate (cid:3)hashtables,g (x),...,g (x). Wethenenteralltheexamples\n1 (cid:3)\nintoeachhashtable. Thenwhengivenaquerypointx ,wefetchthesetofpointsinbing (q)\nq k\nforeach k,andunion these setstogether intoasetofcandidate points, C. Thenwecompute\ntheactualdistancetox foreachofthepointsinC andreturnthekclosestpoints. Withhigh\nq\nprobability, eachofthepointsthatarenearto x willshowupinatleastoneofthebins, and\nq\nalthough some far-away points will show up as well, we can ignore those. With large real-\nworld problems, such as finding the near neighbors in a data set of 13 million Web images\nusing512dimensions(Torralbaetal.,2008),locality-sensitivehashingneedstoexamineonly\na few thousand images out of 13 million to find nearest neighbors; a thousand-fold speedup\noverexhaustiveork-dtreeapproaches.\n18.8.4 Nonparametric regression\nNow we\u2019ll look at nonparametric approaches to regression rather than classification. Fig-\nure 18.28 shows an example of some different models. In (a), wehave perhaps the simplest\nmethod of all, known informally as \u201cconnect-the-dots,\u201d and superciliously as \u201cpiecewise-\nlinear nonparametric regression.\u201d This model creates a function h(x) that, when given a\nquery x , solves the ordinary linear regression problem with just two points: the training\nq\nexamples immediately to the left and right of x . When noise is low, this trivial method is\nq\nactuallynottoobad,whichiswhyitisastandardfeatureofchartingsoftwareinspreadsheets. 742 Chapter 18. LearningfromExamples\n8 8\n7 7\n6 6\n5 5\n4 4\n3 3\n2 2\n1 1\n0 0\n0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14\n(a) (b)\n8 8\n7 7\n6 6\n5 5\n4 4\n3 3\n2 2\n1 1\n0 0\n0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14\n(c) (d)\nFigure18.28 Nonparametricregressionmodels:(a)connectthedots,(b)3-nearestneigh-\nborsaverage,(c)3-nearest-neighborslinearregression, (d)locallyweightedregressionwith\naquadratickernelofwidthk=10.\nButwhenthedataarenoisy, theresulting functionisspiky, anddoesnotgeneralize well.\nNEAREST- k-nearest-neighbors regression (Figure 18.28(b)) improves on connect-the-dots. In-\nNEIGHBORS\nREGRESSION\nstead of using just the two examples to the left and right of a query point x , we use the\nq\nk nearest neighbors (here 3). A larger value of k tends to smooth out the magnitude of\nthespikes, although theresulting function hasdiscontinuities. In(b), wehave thek-nearest-\n(cid:2)\nneighborsaverage: h(x)isthemeanvalueofthekpoints, y \/k. Noticethatattheoutlying\nj\npoints,nearx=0andx=14,theestimatesarepoorbecausealltheevidencecomesfromone\nside(theinterior),andignoresthetrend. In(c),wehavek-nearest-neighbor linearregression,\nwhichfindsthebestlinethroughthekexamples. Thisdoesabetterjobofcapturingtrendsat\ntheoutliers, butisstilldiscontinuous. Inboth(b)and(c),we\u2019releftwiththequestionofhow\ntochoose agoodvaluefork. Theanswer,asusual, iscross-validation.\nLOCALLYWEIGHTED Locallyweightedregression(Figure18.28(d))givesustheadvantagesofnearestneigh-\nREGRESSION\nbors,withoutthediscontinuities. Toavoiddiscontinuities inh(x),weneedtoavoiddisconti- Section18.8. Nonparametric Models 743\n1\n0.5\n0\n-10 -5 0 5 10\nFigure 18.29 A quadratic kernel, K(d)= max(0,1 \u2212 (2|x|\/k)2), with kernel width\nk=10,centeredonthequerypointx=0.\nnuitiesinthesetofexamplesweusetoestimateh(x). Theideaoflocallyweightedregression\nisthatateachquerypoint x ,theexamplesthatareclosetox areweightedheavily, andthe\nq q\nexamplesthatarefartherawayareweightedlessheavilyornotatall. Thedecreaseinweight\noverdistanceisalwaysgradual, notsudden.\nWe decide how much to weight each example with a function known as a kernel. A\nKERNEL\nkernelfunctionlookslikeabump;inFigure18.29weseethespecifickernelusedtogenerate\nFigure 18.28(d). We can see that the weight provided by this kernel is highest in the center\nandreacheszeroatadistanceof\u00b15. Canwechoosejustanyfunctionforakernel? No. First,\nnotethatweinvokeakernelfunction KwithK(Distance(x ,x )),wherex isaquerypoint\nj q q\nthat is a given distance from x , and we want to know how much to weight that distance.\nj\nSo K should be symmetric around 0 and have a maximum at 0. The area under the kernel\nmustremainbounded aswegoto\u00b1\u221e. Othershapes, suchasGaussians, havebeenusedfor\nkernels, but the latest research suggests that the choice of shape doesn\u2019t matter much. We\ndo have to be careful about the width of the kernel. Again, this is a parameter of the model\nthatisbestchosen bycross-validation. Justasinchoosing thek fornearest neighbors, ifthe\nkernels aretoowidewe\u2019llgetunderfitting andiftheyaretoo narrow we\u2019llgetoverfitting. In\nFigure18.29(d), thevalueofk=10givesasmoothcurvethatlooksaboutright\u2014but maybe\nit does not pay enough attention to the outlier at x=6; a narrower kernel width would be\nmoreresponsivetoindividual points.\nDoing locally weighted regression with kernels is now straightforward. For a given\nquerypointx wesolvethefollowingweightedregression problem usinggradientdescent:\nq (cid:12)\nw\u2217 = argmin K(Distance(x ,x ))(y \u2212w\u00b7x )2 ,\nq j j j\nw\nj\nwhere Distance is any of the distance metrics discussed for nearest neighbors. Then the\nanswerish(x\n)=w\u2217\u00b7x\n.\nq q\nNotethatweneedtosolveanewregressionproblemforeveryquerypoint\u2014that\u2019swhat\nit means to be local. (In ordinary linear regression, we solved the regression problem once,\nglobally, and then used thesameh forany query point.) Mitigating against thisextra work\nw 744 Chapter 18. LearningfromExamples\nis the fact that each regression problem will be easier to solve, because it involves only the\nexamples withnonzero weight\u2014the examples whosekernels overlap thequery point. When\nkernelwidthsaresmall,thismaybejustafewpoints.\nMostnonparametricmodelshavetheadvantagethatitiseasytodoleave-one-outcross-\nvalidation without having to recompute everything. With a k-nearest-neighbors model, for\ninstance, whengivenatestexample(x,y)weretrievetheknearestneighborsonce,compute\nthe per-example loss L(y,h(x)) from them, and record that as the leave-one-out result for\neveryexamplethatisnotoneoftheneighbors. Thenweretrievethek+1nearestneighbors\nand record distinct results for leaving out each of the k neighbors. With N examples the\nwholeprocessisO(k),notO(kN).\n18.9 SUPPORT VECTOR MACHINES\nSUPPORTVECTOR ThesupportvectormachineorSVMframeworkiscurrently themostpopularapproach for\nMACHINE\n\u201coff-the-shelf\u201d supervised learning: ifyoudon\u2019thaveanyspecialized priorknowledge about\na domain, then the SVM is an excellent method to try first. There are three properties that\nmakeSVMsattractive:\n1. SVMsconstructamaximummarginseparator\u2014adecisionboundarywiththelargest\npossibledistance toexamplepoints. Thishelpsthemgeneralize well.\n2. SVMs create a linear separating hyperplane, but they have the ability to embed the\ndataintoahigher-dimensional space, using theso-called kerneltrick. Often,datathat\narenot linearly separable in the original input space are easily separable inthe higher-\ndimensional space. The high-dimensional linear separator is actually nonlinear in the\noriginal space. Thismeansthehypothesis space isgreatly expanded overmethods that\nusestrictly linearrepresentations.\n3. SVMsareanonparametricmethod\u2014theyretaintrainingexamplesandpotentiallyneed\nto store them all. On the other hand, in practice they often end up retaining only a\nsmallfractionofthenumberofexamples\u2014sometimes asfewas asmallconstant times\nthe numberof dimensions. Thus SVMscombine the advantages of nonparametric and\nparametric models: they have the flexibility to represent complex functions, but they\nareresistant tooverfitting.\nYou could say that SVMs are successful because of one key insight and one neat trick. We\nwillcovereachinturn. InFigure18.30(a),wehaveabinaryclassificationproblemwiththree\ncandidate decision boundaries, each a linear separator. Each of them is consistent with all\nthe examples, so from the point of view of 0\/1 loss, each would be equally good. Logistic\nregression would find some separating line; the exact location of the line depends on all the\nexample points. The key insight of SVMs is that some examples are more important than\nothers, andthatpayingattention tothemcanleadtobettergeneralization.\nConsider the lowest of the three separating lines in (a). It comes very close to 5 of the\nblack examples. Although itclassifies allthe examples correctly, andthus minimizes loss, it Section18.9. SupportVectorMachines 745\n1 1\n0.8 0.8\n0.6 0.6\n0.4 0.4\n0.2 0.2\n0 0\n0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1\n(a) (b)\nFigure18.30 Supportvectormachineclassification: (a)Twoclassesofpoints(blackand\nwhite circles) and three candidate linear separators. (b) The maximum margin separator\n(heavy line), is at the midpoint of the margin (area between dashed lines). The support\nvectors(pointswithlargecircles)aretheexamplesclosesttotheseparator.\nshould make you nervous that so many examples are close to the line; it may be that other\nblackexampleswillturnouttofallontheothersideoftheline.\nSVMsaddressthisissue: Insteadofminimizingexpectedempiricallossonthetraining\ndata, SVMs attempt to minimize expected generalization loss. We don\u2019t know where the\nas-yet-unseen points may fall, but under the probabilistic assumption that they are drawn\nfrom the same distribution as the previously seen examples, there are some arguments from\ncomputationallearningtheory(Section18.5)suggestingthatweminimizegeneralizationloss\nby choosing the separator that is farthest away from the examples we have seen so far. We\nMAXIMUMMARGIN callthisseparator, showninFigure18.30(b) the maximummargin separator. Themargin\nSEPARATOR\nis the width of the area bounded by dashed lines in the figure\u2014twice the distance from the\nMARGIN\nseparatortothenearestexamplepoint.\nNow, how do we find this separator? Before showing the equations, some notation:\nTraditionally SVMsuse the convention that class labels are +1and -1, instead ofthe +1 and\n0wehave been using so far. Also, where weput the intercept into the weight vector w(and\na corresponding dummy 1 value into x ), SVMs do not do that; they keep the intercept\nj,0\nas a separate parameter, b. With that in mind, the separator is defined as the set of points\n{x : w\u00b7x+b=0}. We could search the space of w and b with gradient descent to find the\nparameters thatmaximizethemarginwhilecorrectly classifying alltheexamples.\nHowever, it turns out there is another approach to solving this problem. We won\u2019t\nshow the details, but will just say that there is an alternative representation called the dual 746 Chapter 18. LearningfromExamples\nrepresentation, inwhichtheoptimalsolution isfoundbysolving\n(cid:12) (cid:12)\n1\nargmax \u03b1 \u2212 \u03b1 \u03b1 y y (x \u00b7x ) (18.13)\nj j k j k j k\n2\n\u03b1\nj j,k\n(cid:2)\nQUADRATIC subject to the constraints \u03b1 \u2265 0 and \u03b1 y =0. This is a quadratic programming\nPROGRAMMING j j j j\noptimization problem, forwhichthere aregoodsoftware packages. Oncewehave found the\n(cid:2)\nvector \u03b1 we can get back to w with the equation w= \u03b1 x , or we can stay in the dual\nj j j\nrepresentation. TherearethreeimportantpropertiesofEquation(18.13). First,theexpression\nisconvex;ithasasingleglobalmaximumthatcanbefoundefficiently. Second,thedataenter\ntheexpressiononlyintheformofdotproductsofpairsofpoints. Thissecondpropertyisalso\ntrueoftheequation fortheseparatoritself;oncetheoptimal\u03b1 havebeencalculated, itis\nj\n\u239b \u239e\n(cid:12)\nh(x) = sign\u239d \u03b1 y (x\u00b7x )\u2212b\u23a0 . (18.14)\nj j j\nj\nAfinalimportant property isthattheweights \u03b1 associated witheachdatapointarezeroex-\nj\nceptforthe supportvectors\u2014the points closest totheseparator. (Theyarecalled\u201csupport\u201d\nSUPPORTVECTOR\nvectors because they \u201chold up\u201d the separating plane.) Because there are usually manyfewer\nsupportvectorsthanexamples,SVMsgainsomeoftheadvantages ofparametricmodels.\nWhatiftheexamples arenotlinearly separable? Figure18.31(a) showsaninput space\ndefined by attributes x=(x ,x ), with positive examples (y= +1) inside a circular region\n1 2\nandnegativeexamples(y=\u22121)outside. Clearly,thereisnolinearseparatorforthisproblem.\nNow,supposewere-expresstheinputdata\u2014i.e.,wemapeachinputvectorxtoanewvector\noffeature values, F(x). Inparticular, letususethethreefeatures\n\u221a\nf =x2 , f =x2 , f = 2x x . (18.15)\n1 1 2 2 3 1 2\nWe will see shortly where these came from, but for now, just look at what happens. Fig-\nure18.31(b)showsthedatainthenew,three-dimensionalspacedefinedbythethreefeatures;\nthe data are linearly separable in this space! This phenomenon is actually fairly general: if\ndataaremappedintoaspace ofsufficiently high dimension, thentheywillalmostalwaysbe\nlinearlyseparable\u2014ifyoulookatasetofpointsfromenough directions, you\u2019llfindawayto\nmakethemlineup. Here,weusedonlythreedimensions;11 Exercise18.16asksyoutoshow\nthatfourdimensions sufficeforlinearly separating acircle anywhereintheplane(notjustat\ntheorigin),andfivedimensionssufficetolinearlyseparate anyellipse. Ingeneral(withsome\nspecialcasesexcepted)ifwehaveN datapointsthentheywillalwaysbeseparableinspaces\nofN \u22121dimensions ormore(Exercise18.25).\nNow, we would not usually expect to find a linear separator in the input space x, but\nwecanfindlinearseparatorsinthehigh-dimensional featurespaceF(x)simplybyreplacing\nx \u00b7x inEquation(18.13)withF(x )\u00b7F(x ). Thisbyitselfisnotremarkable\u2014replacing xby\nj k j k\nF(x)inanylearningalgorithmhastherequiredeffect\u2014butthedotproducthassomespecial\nproperties. It turns out that F(x )\u00b7F(x )can often becomputed without firstcomputing F\nj k\n11 Thereadermaynoticethatwecouldhaveusedjustf1andf2,butthe3Dmappingillustratestheideabetter. Section18.9. SupportVectorMachines 747\n1.5\n1\n\u221a2xx\n1 2\n0.5 3\n2\n1\n0 0\n-1\n-0.5 -2 2.5\n-3 2\n0\n-1 0.5 1.5\n1 1 x2\n2\n-1.5 x 12 1.5 2 0.5\n-1.5 -1 -0.5 0 0.5 1 1.5\nx\n1\n(a) (b)\nFigure 18.31 (a) A two-dimensional training set with positive examples as black cir-\ncles and negative examples as white circles. The true decision boundary, x2 + x2 \u2264 1,\n1 2\nis also s\u221ahown. (b) The same data after mapping into a three-dimensional input space\n(x2,x2, 2x x ). Thecirculardecisionboundaryin(a)becomesalineardecisionboundary\n1 2 1 2\ninthreedimensions.Figure18.30(b)givesacloseupoftheseparatorin(b).\nforeachpoint. Inourthree-dimensional featurespacedefinedbyEquation(18.15),alittlebit\nofalgebra showsthat\nF(x )\u00b7F(x ) = (x \u00b7x )2 .\nj k j k\n\u221a\n(That\u2019s why the 2 is in f .) The expression (x \u00b7 x )2 is called a kernel function,12 and\nKERNELFUNCTION 3 j k\nis usually written as K(x ,x ). The kernel function can be applied to pairs of input data to\nj k\nevaluate dotproducts insomecorresponding featurespace. So,wecanfindlinearseparators\ninthehigher-dimensional featurespace F(x)simplybyreplacing x \u00b7x inEquation(18.13)\nj k\nwithakernelfunctionK(x ,x ). Thus,wecanlearninthehigher-dimensional space,butwe\nj k\ncomputeonlykernelfunctions ratherthanthefulllistoffeatures foreachdatapoint.\nThenextstepistoseethatthere\u2019snothingspecialaboutthekernelK(x ,x )=(x \u00b7x )2.\nj k j k\nIt corresponds to a particular higher-dimensional feature space, but other kernel functions\ncorrespond to other feature spaces. A venerable result in mathematics, Mercer\u2019s theo-\nrem (1909), tells us that any \u201creasonable\u201d13 kernel function corresponds to some feature\nMERCER\u2019STHEOREM\nspace. These feature spaces can be very large, even for innocuous-looking kernels. For ex-\nPOLYNOMIAL ample, the polynomial kernel, K(x ,x )=(1 + x \u00b7 x )d, corresponds to a feature space\nKERNEL j k j k\nwhosedimension isexponential ind.\n12 Thisusage of \u201ckernel function\u201d isslightly different fromthe kernels in locally weighted regression. Some\nSVMkernelsaredistancemetrics,butnotallare.\n13 Here,\u201creasonable\u201dmeansthatthematrixKjk=K(xj,xk)ispositivedefinite.\nx 2 748 Chapter 18. LearningfromExamples\nThis then is the clever kernel trick: Plugging these kernels into Equation (18.13),\nKERNELTRICK\noptimal linear separators can be found efficiently in feature spaces with billions of (or, in\nsomecases, infinitely many)dimensions. Theresulting linearseparators, whenmappedback\nto the original input space, can correspond to arbitrarily wiggly, nonlinear decision bound-\nariesbetweenthepositiveandnegativeexamples.\nIn the case of inherently noisy data, we may not want a linear separator in some high-\ndimensional space. Rather, we\u2019d like a decision surface in a lower-dimensional space that\ndoes not cleanly separate the classes, but reflects the reality of the noisy data. That is pos-\nsible withthe soft margin classifier, which allows examples to fall on the wrong side of the\nSOFTMARGIN\ndecision boundary, but assigns them a penalty proportional to the distance required to move\nthembackonthecorrectside.\nThe kernel method can be applied not only with learning algorithms that find optimal\nlinear separators, but also with any other algorithm that can be reformulated to work only\nwith dot products of pairs of data points, as in Equations 18.13 and 18.14. Once this is\ndone, the dot product is replaced by a kernel function and we have a kernelized version\nKERNELIZATION\nof the algorithm. This can be done easily for k-nearest-neighbors and perceptron learning\n(Section18.7.2),amongothers.\n18.10 ENSEMBLE LEARNING\nSo far we have looked at learning methods in which a single hypothesis, chosen from a\nENSEMBLE hypothesis space, is used to make predictions. The idea of ensemble learning methods is\nLEARNING\nto select a collection, or ensemble, of hypotheses from the hypothesis space and combine\ntheir predictions. For example, during cross-validation we might generate twenty different\ndecision trees,andhavethemvoteonthebestclassification foranewexample.\nThe motivation for ensemble learning is simple. Consider an ensemble of K=5 hy-\npothesesandsupposethatwecombinetheirpredictionsusingsimplemajorityvoting. Forthe\nensembletomisclassify anewexample,atleastthreeofthefivehypotheses havetomisclas-\nsifyit. Thehopeisthatthisismuchlesslikelythanamisclassificationbyasinglehypothesis.\nSuppose we assume that each hypothesis h in the ensemble has an error of p\u2014that is, the\nk\nprobabilitythatarandomlychosenexampleismisclassifiedbyh isp. Furthermore,suppose\nk\nweassumethattheerrorsmadebyeachhypothesisareindependent. Inthatcase,ifpissmall,\nthen the probability of a large number of misclassifications occurring is minuscule. For ex-\nample,asimplecalculation(Exercise18.18)showsthatusinganensembleoffivehypotheses\nreduces an error rate of 1 in 10 down to an error rate of less than 1 in 100. Now, obviously\nthe assumption of independence isunreasonable, because hypotheses arelikely tobemisled\nin the same way by any misleading aspects of the training data. But ifthe hypotheses are at\nleastalittlebitdifferent,therebyreducingthecorrelationbetweentheirerrors,thenensemble\nlearning canbeveryuseful.\nAnother way to think about the ensemble idea is as a generic way of enlarging the\nhypothesisspace. Thatis,thinkoftheensembleitselfasahypothesisandthenewhypothesis Section18.10. EnsembleLearning 749\n\u2013\n\u2013\n\u2013 \u2013 \u2013 \u2013\n\u2013\n\u2013\n\u2013 \u2013 \u2013 \u2013\n\u2013\n\u2013 + \u2013 \u2013\n\u2013 \u2013\n\u2013 + + \u2013\n+ ++ + \u2013 \u2013\n\u2013 + + \u2013\n+ ++ + +\n\u2013 \u2013 \u2013\n\u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013\n\u2013 \u2013\n\u2013 \u2013\nFigure18.32 Illustrationoftheincreasedexpressivepowerobtainedby ensemblelearn-\ning. We take three linear threshold hypotheses, each of which classifies positively on the\nunshaded side, and classify as positive any example classified positively by all three. The\nresultingtriangularregionisahypothesisnotexpressibleintheoriginalhypothesisspace.\nspaceasthesetofallpossibleensemblesconstructablefromhypothesesintheoriginalspace.\nFigure18.32showshowthiscanresultinamoreexpressivehypothesis space. Iftheoriginal\nhypothesis space allows for a simple and efficient learning algorithm, then the ensemble\nmethodprovidesawaytolearnamuchmoreexpressiveclassofhypotheseswithoutincurring\nmuchadditional computational oralgorithmic complexity.\nThemostwidelyusedensemblemethodiscalledboosting. Tounderstandhowitworks,\nBOOSTING\nWEIGHTEDTRAINING we need first to explain the idea of a weighted training set. In such a training set, each\nSET\nexample has an associated weight w \u2265 0. The higher the weight of an example, the higher\nj\nis the importance attached to it during the learning of a hypothesis. It is straightforward to\nmodifythelearning algorithmswehaveseensofartooperate withweightedtraining sets.14\nBoosting starts with w =1foralltheexamples (i.e., anormal training set). From this\nj\nset,itgeneratesthefirsthypothesis, h . Thishypothesiswillclassifysomeofthetrainingex-\n1\namplescorrectly andsomeincorrectly. Wewouldlikethenexthypothesis todobetteronthe\nmisclassifiedexamples,soweincreasetheirweightswhiledecreasingtheweightsofthecor-\nrectly classified examples. From this new weighted training set, wegenerate hypothesis h .\n2\nTheprocesscontinuesinthiswayuntilwehavegeneratedK hypotheses,whereK isaninput\ntotheboostingalgorithm. Thefinalensemblehypothesisisaweighted-majoritycombination\nofalltheKhypotheses,eachweightedaccordingtohowwellitperformedonthetrainingset.\nFigure18.33showshowthealgorithmworksconceptually. Therearemanyvariantsoftheba-\nsicboostingidea,withdifferentwaysofadjustingtheweightsandcombiningthehypotheses.\nOnespecificalgorithm,calledADABOOST,isshowninFigure18.34. ADABOOSThasavery\nimportant property: if the input learning algorithm L is a weak learning algorithm\u2014which\nWEAKLEARNING\n14 Forlearningalgorithmsinwhichthisisnotpossible,onecaninsteadcreateareplicatedtrainingsetwhere\nthejthexampleappearswj times,usingrandomizationtohandlefractionalweights. 750 Chapter 18. LearningfromExamples\nh = h = h = h =\n1 2 3 4\nh\nFigure18.33 Howtheboostingalgorithmworks. Eachshadedrectanglecorrespondsto\nanexample; the heightof therectanglecorrespondsto theweight. Thechecksandcrosses\nindicatewhethertheexamplewasclassifiedcorrectlybythecurrenthypothesis. Thesizeof\nthedecisiontreeindicatestheweightofthathypothesisinthefinalensemble.\nmeans that L always returns a hypothesis with accuracy on the training set that is slightly\nbetterthanrandomguessing(i.e.,50%+(cid:2)forBooleanclassification)\u2014then ADABOOSTwill\nreturn a hypothesis that classifies the training data perfectly for large enough K. Thus, the\nalgorithm boosts the accuracy of the original learning algorithm on the training data. This\nresult holds no matter how inexpressive the original hypothesis space and no matter how\ncomplexthefunction beinglearned.\nLetusseehowwellboostingdoesontherestaurantdata. Wewillchooseasouroriginal\nhypothesis spacetheclassofdecision stumps,whicharedecision treeswithjustonetest,at\nDECISIONSTUMP\nthe root. The lower curve in Figure 18.35(a) shows that unboosted decision stumps are not\nveryeffectiveforthisdataset,reachingapredictionperformanceofonly81%on100training\nexamples. When boosting is applied (with K=5), the performance is better, reaching 93%\nafter100examples.\nAninteresting thing happens astheensemble size K increases. Figure18.35(b) shows\nthe training set performance (on 100 examples) as a function of K. Notice that the error\nreaches zero when K is 20; that is, a weighted-majority combination of 20 decision stumps\nsufficestofitthe100examplesexactly. Asmorestumpsareaddedtotheensemble, theerror\nremains at zero. The graph also shows that the test set performance continues to increase\nlong after the training set error has reached zero. At K = 20, the test performance is 0.95\n(or 0.05 error), and the performance increases to 0.98 as late as K = 137, before gradually\ndropping to0.95.\nThisfinding,whichisquiterobustacrossdatasetsandhypothesisspaces,cameasquite\na surprise when it was first noticed. Ockham\u2019s razor tells us not to make hypotheses more Section18.10. EnsembleLearning 751\nfunctionADABOOST(examples,L,K)returnsaweighted-majorityhypothesis\ninputs:examples,setofN labeledexamples(x 1,y 1),...,(xN,yN)\nL,alearningalgorithm\nK,thenumberofhypothesesintheensemble\nlocalvariables: w,avectorofN exampleweights,initially1\/N\nh,avectorofK hypotheses\nz,avectorofK hypothesisweights\nfork =1toK do\nh[k]\u2190L(examples,w)\nerror\u21900\nforj =1toN do\nifh[k](xj)(cid:7)= yj thenerror\u2190error + w[j]\nforj =1toN do\nifh[k](xj)=yj thenw[j]\u2190w[j]\u00b7 error\/(1\u2212error)\nw\u2190NORMALIZE(w)\nz[k]\u2190log(1\u2212error)\/error\nreturnWEIGHTED-MAJORITY(h,z)\nFigure18.34 TheADABOOSTvariantoftheboostingmethodforensemblelearning.The\nalgorithmgenerateshypothesesbysuccessivelyreweightingthetrainingexamples.Thefunc-\ntion WEIGHTED-MAJORITY generates a hypothesis that returns the output value with the\nhighestvotefromthehypothesesinh,withvotesweightedbyz.\n1\n0.95 1\n0.9 0.95\n0.85 0.9\n0.8 0.85\nTraining error\n0.75\n0.8 Test error\n0.7\n0.75\n0.65 Boosted decision stumps\nDecision stump 0.7\n0.6\n0.55 0.65\n0.5 0.6\n0 20 40 60 80 100 0 50 100 150 200\nTraining set size Number of hypotheses K\n(a) (b)\nFigure18.35 (a)GraphshowingtheperformanceofboosteddecisionstumpswithK=5\nversusunboosteddecisionstumpsonthe restaurantdata. (b)Theproportioncorrectonthe\ntrainingset and the test set as a functionof K, the numberof hypothesesin the ensemble.\nNoticethatthetestsetaccuracyimprovesslightlyevenafterthetrainingaccuracyreaches1,\ni.e.,aftertheensemblefitsthedataexactly.\ntes\ntset\nno\ntcerroc\nnoitroporP\nycarucca\ntset\/gniniarT 752 Chapter 18. LearningfromExamples\ncomplex than necessary, but the graph tells us that the predictions improve as the ensemble\nhypothesis getsmorecomplex! Various explanations have been proposed forthis. Oneview\nis that boosting approximates Bayesian learning (see Chapter 20), which can be shown to\nbe an optimal learning algorithm, and the approximation improves as more hypotheses are\nadded. Another possible explanation is that the addition of further hypotheses enables the\nensembletobemoredefiniteinitsdistinctionbetweenpositiveandnegativeexamples,which\nhelpsitwhenitcomestoclassifying newexamples.\n18.10.1 OnlineLearning\nSofar,everything wehavedoneinthischapterhasreliedontheassumption thatthedataare\ni.i.d. (independentandidenticallydistributed). Ontheonehand,thatisasensibleassumption:\nifthefuturebearsnoresemblancetothepast,thenhowcanwepredictanything? Ontheother\nhand,itistoostronganassumption: itisrarethatourinputshavecapturedalltheinformation\nthatwouldmakethefuturetrulyindependent ofthepast.\nInthissectionweexaminewhattodowhenthedataarenoti.i.d.;whentheycanchange\novertime. Inthiscase,itmatterswhenwemakeaprediction,sowewilladopttheperspective\ncalledonlinelearning: anagentreceivesaninputx fromnature,predictsthecorresponding\nONLINELEARNING j\ny , and then is told the correct answer. Then the process repeats with x , and so on. One\nj j+1\nmight think this task is hopeless\u2014if nature is adversarial, all the predictions may be wrong.\nItturnsoutthattherearesomeguarantees wecanmake.\nLet us consider the situation where our input consists of predictions from a panel of\nexperts. Forexample, each dayasetof K pundits predicts whetherthestock marketwillgo\nupordown, and ourtask istopool those predictions and makeourown. Onewaytodo this\nis to keep track of how welleach expert performs, and choose to believe them in proportion\nRANDOMIZED\nWEIGHTED totheirpastperformance. Thisiscalledthe randomizedweightedmajorityalgorithm. We\nMAJORITY\nALGORITHM candescribed itmoreformally:\n1. Initializeasetofweights{w ,...,w }allto1.\n1 K\n2. Receivethepredictions {y\u02c6 ,...,y\u02c6 }fromtheexperts.\n1 K (cid:2)\n\u2217\n3. Randomlychooseanexpertk ,inproportion toitsweight: P(k)=w k\/( k(cid:3)w k(cid:3)).\n4. Predicty\u02c6 k\u2217.\n5. Receivethecorrectanswer y.\n6. Foreachexpert k suchthaty\u02c6 (cid:7)= y,updatew \u2190\u03b2w\nk k k\nHere\u03b2 isanumber, 0 < \u03b2 < 1,thattellshowmuchtopenalizeanexpertforeachmistake.\nWe measure the success of this algorithm in terms of regret, which is defined as the\nREGRET\nnumber of additional mistakes we make compared to the expert who, in hindsight, had the\n\u2217\nbestprediction record. Let M bethenumberofmistakesmadebythebestexpert. Thenthe\nnumberofmistakes, M,madebytherandom weightedmajorityalgorithm, isbounded by15\n\u2217\nM ln(1\/\u03b2)+lnK\nM < .\n1\u2212\u03b2\n15 See(Blum,1996)fortheproof. Section18.11. PracticalMachineLearning 753\nThis bound holds for any sequence of examples, even ones chosen by adversaries trying to\ndo their worst. To be specific, when there are K=10 experts, if we choose \u03b2=1\/2 then\n\u2217 \u2217\nour number of mistakes is bounded by 1.39M +4.6, and if \u03b2=3\/4 by 1.15M +9.2. In\ngeneral,if\u03b2iscloseto1thenweareresponsivetochangeoverthelongrun;ifthebestexpert\nchanges, wewillpick up onitbefore too long. However, wepay apenalty atthe beginning,\nwhen we start with all experts trusted equally; we may accept the advice of the bad experts\nfortoolong. When\u03b2 iscloserto0,thesetwofactorsarereversed. Notethatwecanchoose\u03b2\n\u2217\nNO-REGRET togetasymptotically closetoM inthelongrun; thisiscalled no-regret learning(because\nLEARNING\ntheaverageamountofregretpertrialtendsto0asthenumber oftrialsincreases).\nOnline learning is helpful when the data may be changing rapidly over time. It is also\nuseful forapplications thatinvolve alargecollection ofdatathatisconstantly growing, even\nifchangesaregradual. Forexample,withadatabaseofmillionsofWebimages,youwouldn\u2019t\nwanttotrain,say,alinearregressionmodelonallthedata, andthenretrainfromscratchevery\ntimeanewimageisadded. Itwouldbemorepracticaltohaveanonlinealgorithmthatallows\nimages to be added incrementally. For most learning algorithms based on minimizing loss,\nthereisanonlineversionbasedonminimizingregret. Itisa bonusthatmanyoftheseonline\nalgorithms comewithguaranteed bounds onregret.\nTosomeobservers, itissurprising thattherearesuchtight bounds onhowwellwecan\ndo compared to a panel of experts. Toothers, the really surprising thing is that when panels\nof human experts congregate\u2014predicting stock market prices, sports outcomes, or political\ncontests\u2014the viewing public is so willing to listen to them pontificate and so unwilling to\nquantify theirerrorrates.\n18.11 PRACTICAL MACHINE LEARNING\nWehaveintroducedawiderangeofmachinelearningtechniques,eachillustratedwithsimple\nlearningtasks. Inthissection,weconsidertwoaspectsofpracticalmachinelearning. Thefirst\ninvolvesfindingalgorithmscapableoflearningtorecognizehandwrittendigitsandsqueezing\nevery last drop of predictive performance out of them. The second involves anything but\u2014\npointingoutthatobtaining, cleaning,andrepresentingthedatacanbeatleastasimportantas\nalgorithm engineering.\n18.11.1 Casestudy: Handwrittendigitrecognition\nRecognizing handwritten digits is an important problem with many applications, including\nautomated sorting of mail by postal code, automated reading of checks and tax returns, and\ndataentryforhand-held computers. Itisanareawhererapid progresshasbeenmade,inpart\nbecause ofbetterlearning algorithms andinpartbecauseof theavailability ofbettertraining\nsets. TheUnited StatesNational Institute ofScience and Technology (NIST)has archived a\ndatabase of 60,000 labeled digits, each 20\u00d720=400 pixels with 8-bit grayscale values. It\nhasbecomeoneofthestandardbenchmarkproblemsforcomparingnewlearningalgorithms.\nSomeexampledigitsareshowninFigure18.36. 754 Chapter 18. LearningfromExamples\nFigure18.36 ExamplesfromtheNISTdatabaseofhandwrittendigits.Toprow:examples\nofdigits0\u20139thatareeasytoidentify.Bottomrow:moredifficultexamplesofthesamedigits.\nManydifferent learning approaches have been tried. Oneofthe first, and probably the\nsimplest, is the 3-nearest-neighbor classifier, which also has the advantage of requiring no\ntraining time. As a memory-based algorithm, however, it must store all 60,000 images, and\nitsruntimeperformance isslow. Itachievedatesterrorrateof2.4%.\nA single-hidden-layer neural network was designed for this problem with 400 input\nunits(oneperpixel)and10outputunits(oneperclass). Usingcross-validation, itwasfound\nthatroughly300hiddenunitsgavethebestperformance. Withfullinterconnections between\nlayers, therewereatotalof123,300weights. Thisnetworkachieveda1.6%errorrate.\nAseries of specialized neuralnetworks called LeNetwere devised totake advantage\nofthestructure oftheproblem\u2014that theinput consists ofpixelsinatwo\u2013dimensional array,\nand that small changes in the position or slant of an image are unimportant. Each network\nhadaninputlayerof32\u00d732units,ontowhichthe20\u00d720pixelswerecenteredsothateach\ninputunitispresentedwithalocalneighborhood ofpixels. Thiswasfollowedbythreelayers\nof hidden units. Each layer consisted of several planes of n\u00d7n arrays, where n is smaller\nthanthepreviouslayersothatthenetworkisdown-samplingtheinput,andwheretheweights\nofeveryunitinaplaneareconstrained tobeidentical, sothattheplaneisacting asafeature\ndetector: itcanpickoutafeaturesuchasalongverticallineorashortsemi-circulararc. The\noutput layerhad10units. Manyversions ofthisarchitecture weretried; arepresentative one\nhad hidden layers with768, 192, and 30units, respectively. Thetraining setwasaugmented\nbyapplyingaffinetransformations totheactualinputs: shifting,slightlyrotating,andscaling\ntheimages. (Ofcourse, thetransformations havetobesmall, orelse a6willbetransformed\nintoa9!) ThebesterrorrateachievedbyLeNetwas0.9%.\nA boosted neural network combined three copies of the LeNet architecture, with the\nsecond one trained on a mix of patterns that the first one got 50% wrong, and the third one\ntrainedonpatternsforwhichthefirsttwodisagreed. During testing,thethreenetsvotedwith\nthemajorityruling. Thetesterrorratewas0.7%.\nAsupportvectormachine(seeSection18.9)with25,000supportvectorsachievedan\nerror rate of 1.1%. This is remarkable because the SVM technique, like the simple nearest-\nneighbor approach, required almostnothought oriterated experimentation onthepartofthe\ndeveloper, yetitstillcameclosetotheperformance ofLeNet,whichhadhadyearsofdevel-\nopment. Indeed, the support vector machine makes no use of the structure of the problem,\nandwouldperform justaswellifthepixelswerepresented in apermutedorder. Section18.11. PracticalMachineLearning 755\nVIRTUALSUPPORT A virtual support vector machine starts with a regular SVM and then improves it\nVECTORMACHINE\nwithatechniquethatisdesignedtotakeadvantageofthestructureoftheproblem. Insteadof\nallowingproducts ofallpixelpairs,thisapproach concentrates onkernels formedfrompairs\nofnearby pixels. Italsoaugments thetraining setwithtransformations ofthe examples, just\nasLeNetdid. AvirtualSVMachievedthebesterrorraterecorded todate,0.56%.\nShapematchingisatechniquefromcomputervisionusedtoaligncorrespondingparts\nof two different images of objects (Belongie et al., 2002). The idea is to pick out a set\nof points in each of the two images, and then compute, for each point in the first image,\nwhich point in the second image itcorresponds to. From this alignment, wethen compute a\ntransformation between the images. The transformation gives us a measure of the distance\nbetweentheimages. Thisdistancemeasureisbettermotivatedthanjustcountingthenumber\nof differing pixels, and it turns out that a 3\u2013nearest neighbor algorithm using this distance\nmeasure performs very well. Training on only 20,000 of the 60,000 digits, and using 100\nsample points per image extracted from a Canny edge detector, a shape matching classifier\nachieved0.63%testerror.\nHumansareestimatedtohaveanerrorrateofabout0.2%onthisproblem. Thisfigure\nis somewhat suspect because humans have not been tested as extensively as have machine\nlearning algorithms. On a similar data set of digits from the United States Postal Service,\nhumanerrorswereat2.5%.\nThe following figure summarizes the error rates, run time performance, memory re-\nquirements, and amount oftraining timeforthe seven algorithms wehave discussed. Italso\nadds another measure, the percentage of digits that must be rejected to achieve 0.5% error.\nFor example, if the SVM is allowed to reject 1.8% of the inputs\u2014that is, pass them on for\nsomeone else to make the final judgment\u2014then its error rate on the remaining 98.2% of the\ninputsisreduced from1.1%to0.5%.\nThe following table summarizes the error rate and some of the other characteristics of\ntheseventechniques wehavediscussed.\n3 300 Boosted Virtual Shape\nNN Hidden LeNet LeNet SVM SVM Match\nErrorrate(pct.) 2.4 1.6 0.9 0.7 1.1 0.56 0.63\nRuntime(millisec\/digit) 1000 10 30 50 2000 200\nMemoryrequirements (Mbyte) 12 .49 .012 .21 11\nTrainingtime(days) 0 7 14 30 10\n%rejectedtoreach0.5%error 8.1 3.2 1.8 0.5 1.8\n18.11.2 Casestudy: Wordsenses and houseprices\nIn a textbook weneed to deal with simple, toy data to get the ideas across: a small data set,\nusually in two dimensions. But in practical applications of machine learning, the data set\nis usually large, multidimensional, and messy. The data are not handed to the analyst in a\nprepackaged setof(x,y)values;rathertheanalystneedstogooutandacquiretherightdata.\nThere is a task to be accomplished, and most of the engineering problem is deciding what\ndata are necessary to accomplish the task; a smaller part is choosing and implementing an 756 Chapter 18. LearningfromExamples\n1\n0.95\n0.9\n0.85\n0.8\n0.75\n1 10 100 1000\nTraining set size (millions of words)\nFigure18.37 Learningcurvesforfivelearningalgorithmsona commontask. Notethat\nthere appears to be more room for improvementin the horizontal direction (more training\ndata) than in the vertical direction (different machine learning algorithm). Adapted from\nBankoandBrill(2001).\nappropriate machine learning method toprocess the data. Figure18.37 showsatypical real-\nworld example, comparing five learning algorithms on the task of word-sense classification\n(given a sentence such as \u201cThe bank folded,\u201d classify the word \u201cbank\u201d as \u201cmoney-bank\u201d or\n\u201criver-bank\u201d). The point is that machine learning researchers have focused mainly on the\nverticaldirection: CanIinventanewlearningalgorithm thatperformsbetterthanpreviously\npublished algorithms on a standard training set of 1 million words? But the graph shows\nthere is more room for improvement in the horizontal direction: instead of inventing a new\nalgorithm,allIneedtodoisgather10millionwordsoftrainingdata;eventheworstalgorithm\nat 10 million words is performing better than the best algorithm at 1 million. As we gather\nevenmoredata,thecurvescontinuetorise,dwarfingthedifferences betweenalgorithms.\nConsider another problem: the task of estimating the true value of houses that are for\nsale. In Figure 18.13 we showed a toy version of this problem, doing linear regression of\nhouse size to asking price. You probably noticed many limitations of this model. First, it is\nmeasuring the wrong thing: we want to estimate the selling price of a house, not the asking\nprice. To solve this task we\u2019ll need data on actual sales. But that doesn\u2019t mean we should\nthrow away the data about asking price\u2014we can use it as one of the input features. Besides\nthe size of the house, we\u2019ll need more information: the number of rooms, bedrooms and\nbathrooms; whether the kitchen and bathrooms have been recently remodeled; the age of\nthe house; we\u2019ll also need information about the lot, and the neighborhood. But how do\nwe define neighborhood? By zip code? What if part of one zip code is on the \u201cwrong\u201d\nside of the highway or train tracks, and the other part is desirable? What about the school\ndistrict? Should the name of the school district be a feature, or the average test scores? In\nadditiontodecidingwhatfeaturestoinclude,wewillhavetodealwithmissingdata;different\nareas have different customs on what data are reported, and individual cases will always be\nmissing some data. If the data you want are not available, perhaps you can set up a social\nnetworking site to encourage people to share and correct data. In the end, this process of\ntes\ntset\nno\ntcerroc\nnoitroporP Section18.12. Summary 757\ndeciding whatfeatures touse,andhowtousethem,isjustasimportant aschoosing between\nlinearregression, decision trees,orsomeotherformoflearning.\nThat said, one does have to pick a method (or methods) for a problem. There is no\nguaranteed way to pick the best method, but there are some rough guidelines. Decision\ntrees are good when there are a lot of discrete features and you believe that many of them\nmaybeirrelevant. Nonparametricmethodsaregoodwhenyouhavealotofdataandnoprior\nknowledge,andwhenyoudon\u2019twanttoworrytoomuchaboutchoosingjusttherightfeatures\n(aslongastherearefewerthan20orso). However,nonparametric methodsusuallygiveyou\nafunction hthatismoreexpensive torun. Supportvectormachines areoftenconsidered the\nbestmethodtotryfirst,providedthedatasetisnottoolarge.\n18.12 SUMMARY\nThis chapter has concentrated on inductive learning of functions from examples. The main\npointswereasfollows:\n\u2022 Learningtakesmanyforms,depending onthenatureoftheagent, thecomponent tobe\nimproved,andtheavailablefeedback.\n\u2022 Iftheavailablefeedbackprovidesthecorrectanswerforexampleinputs,thenthelearn-\ning problem is called supervised learning. The task is to learn a function y = h(x).\nLearningadiscrete-valued functioniscalledclassification;learningacontinuousfunc-\ntioniscalledregression.\n\u2022 Inductive learning involves finding a hypothesis that agrees well with the examples.\nOckham\u2019s razor suggests choosing the simplest consistent hypothesis. The difficulty\nofthistaskdepends onthechosen representation.\n\u2022 Decision trees can represent all Boolean functions. The information-gain heuristic\nprovidesanefficientmethodforfindingasimple,consistent decision tree.\n\u2022 The performance of a learning algorithm is measured by the learning curve, which\nshowstheprediction accuracy onthe testsetasafunctionofthetraining-setsize.\n\u2022 Whentherearemultiplemodelstochoosefrom, cross-validation canbeusedtoselect\namodelthatwillgeneralize well.\n\u2022 Sometimes not all errors are equal. A loss function tells us how bad each error is; the\ngoalisthentominimizelossoveravalidation set.\n\u2022 Computational learning theory analyzes the sample complexity and computational\ncomplexityofinductive learning. Thereisatradeoff betweentheexpressiveness ofthe\nhypothesis language andtheeaseoflearning.\n\u2022 Linear regression is a widely used model. The optimal parameters of a linear regres-\nsionmodelcanbefoundbygradientdescent search,orcomputedexactly.\n\u2022 Alinearclassifierwithahardthreshold\u2014also knownasa perceptron\u2014can betrained\nby a simple weight update rule to fit data that are linearly separable. In other cases,\ntherulefailstoconverge. 758 Chapter 18. LearningfromExamples\n\u2022 Logistic regression replaces the perceptron\u2019s hard threshold with a soft threshold de-\nfined by a logistic function. Gradient descent works well even for noisy data that are\nnotlinearly separable.\n\u2022 Neural networks represent complex nonlinear functions with a network of linear-\nthreshold units. termMultilayer feed-forward neural networks can represent any func-\ntion, given enough units. The back-propagation algorithm implements a gradient de-\nscentinparameterspacetominimizetheoutputerror.\n\u2022 Nonparametric models use all the data to make each prediction, rather than trying to\nsummarize the data first with a few parameters. Examples include nearest neighbors\nandlocallyweightedregression.\n\u2022 Support vector machines find linear separators with maximum margin to improve\nthe generalization performance of the classifier. Kernel methodsimplicitly transform\ntheinputdataintoahigh-dimensional spacewherealinearseparatormayexist,evenif\ntheoriginal dataarenon-separable.\n\u2022 Ensemble methods such as boosting often perform better than individual methods. In\nonlinelearningwecanaggregatetheopinionsofexpertstocomearbitrarily closetothe\nbestexpert\u2019sperformance, evenwhenthedistribution ofthedataisconstantly shifting.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nChapter1outlinedthehistoryofphilosophicalinvestigationsintoinductivelearning. William\nof Ockham16 (1280\u20131349), the most influential philosopher of his century and a majorcon-\ntributortomedievalepistemology, logic,andmetaphysics, iscreditedwithastatementcalled\n\u201cOckham\u2019sRazor\u201d\u2014inLatin,Entianonsuntmultiplicanda praeternecessitatem, andinEn-\nglish, \u201cEntitiesarenottobemultipliedbeyond necessity.\u201d Unfortunately, thislaudable piece\nof advice is nowhere to be found in his writings in precisely these words (although he did\nsay \u201cPluralitas non est ponenda sine necessitate,\u201d or \u201cplurality shouldn\u2019t be posited without\nnecessity\u201d). A similar sentiment was expressed by Aristotle in 350 B.C. in Physics book I,\nchapterVI:\u201cForthemorelimited,ifadequate, isalwayspreferable.\u201d\nThe first notable use of decision trees was in EPAM, the \u201cElementary Perceiver And\nMemorizer\u201d (Feigenbaum, 1961), which was a simulation of human concept learning. ID3\n(Quinlan, 1979)addedthecrucialideaofchoosing theattribute withmaximumentropy; itis\nthebasisforthedecisiontreealgorithminthischapter. Informationtheorywasdevelopedby\nClaude Shannon to aid in the study of communication (Shannon and Weaver, 1949). (Shan-\nnon also contributed one of the earliest examples of machine learning, a mechanical mouse\nnamed Theseus that learned to navigate through a maze by trial and error.) The \u03c72 method\nof tree pruning was described by Quinlan (1986). C4.5, an industrial-strength decision tree\npackage, canbefound inQuinlan (1993). Anindependent tradition ofdecision treelearning\nexists inthestatistical literature. Classification and Regression Trees(Breiman etal.,1984),\nknownasthe\u201cCARTbook,\u201distheprincipal reference.\n16 Thenameisoftenmisspelledas\u201cOccam,\u201dperhapsfromtheFrenchrendering,\u201cGuillaumed\u2019Occam.\u201d Bibliographical andHistorical Notes 759\nCross-validation was first introduced by Larson (1931), and in a form close to what\nwe show by Stone (1974) and Golub et al. (1979). The regularization procedure is due to\nTikhonov (1963). Guyon and Elisseeff (2003) introduce ajournal issue devoted tothe prob-\nlemoffeature selection. Bankoand Brill(2001) andHalevy etal.(2009) discuss theadvan-\ntages of using large amounts of data. It was Robert Mercer, a speech researcher who said\nin 1985 \u201cThere is no data like more data.\u201d (Lyman and Varian, 2003) estimate that about 5\nexabytes (5 \u00d71018 bytes) of data was produced in 2002, and that the rate of production is\ndoubling every3years.\nTheoretical analysis of learning algorithms began with the work of Gold (1967) on\nidentification in the limit. This approach was motivated in part by models of scientific\ndiscovery fromthephilosophy ofscience (Popper, 1962), buthasbeenapplied mainlytothe\nproblem oflearning grammarsfromexamplesentences (Oshersonetal.,1986).\nWhereastheidentification-in-the-limit approachconcentratesoneventualconvergence,\nKOLMOGOROV the study ofKolmogorov complexityor algorithmic complexity, developed independently\nCOMPLEXITY\nbySolomonoff(1964,2009)andKolmogorov(1965),attemptstoprovideaformaldefinition\nfor the notion of simplicity used in Ockham\u2019s razor. To escape the problem that simplicity\ndepends on the way in which information is represented, it is proposed that simplicity be\nmeasuredbythelengthoftheshortest program forauniversal Turingmachinethatcorrectly\nreproduces theobserved data. Although there aremany possible universal Turing machines,\nand hence many possible \u201cshortest\u201d programs, these programs differ in length by at most a\nconstant that is independent of the amount of data. This beautiful insight, which essentially\nshows that any initial representation bias will eventually be overcome by the data itself, is\nmarredonlybytheundecidability ofcomputing thelength of theshortest program. Approx-\nMINIMUM\nimate measures such as the minimum description length, or MDL (Rissanen, 1984, 2007)\nDESCRIPTION\nLENGTH\ncan be used instead and have produced excellent results in practice. The text by Li and Vi-\ntanyi(1993)isthebestsourceforKolmogorovcomplexity.\nThetheoryofPAC-learningwasinauguratedbyLeslieValiant(1984). Hisworkstressed\ntheimportanceofcomputationalandsamplecomplexity. WithMichaelKearns(1990),Valiant\nshowed that several concept classes cannot be PAC-learned tractably, even though sufficient\ninformationisavailableintheexamples. Somepositiveresultswereobtainedforclassessuch\nasdecision lists(Rivest,1987).\nAnindependent traditionofsample-complexity analysishasexistedinstatistics, begin-\nUNIFORM\nningwiththeworkonuniformconvergence theory(VapnikandChervonenkis, 1971). The\nCONVERGENCE\nTHEORY\nso-calledVCdimensionprovidesameasureroughlyanalogousto,butmoregeneralthan,the\nVCDIMENSION\nln|H|measureobtainedfromPACanalysis. TheVCdimensioncanbeappliedtocontinuous\nfunction classes, to which standard PAC analysis does not apply. PAC-learning theory and\nVCtheory were firstconnected by the \u201cfour Germans\u201d (none ofwhom actually is German):\nBlumer,Ehrenfeucht, Haussler, andWarmuth(1989).\nLinear regression with squared error loss goes back to Legendre (1805) and Gauss\n(1809), who were both working on predicting orbits around the sun. The modern use of\nmultivariate regression for machine learning is covered in texts such as Bishop (2007). Ng\n(2004)analyzed thedifferences between L andL regularization.\n1 2 760 Chapter 18. LearningfromExamples\nTheterm logistic functioncomesfromPierre-Franc\u00b8oisVerhulst (1804\u20131849), astatis-\ntician whoused the curve tomodel population growth withlimited resources, amore realis-\ntic model than the unconstrained geometric growth proposed by Thomas Malthus. Verhulst\ncalled it the courbe logistique, because of its relation tothe logarithmic curve. Theterm re-\ngression is due to Francis Galton, nineteenth century statistician, cousin of Charles Darwin,\nandinitiatorofthefieldsofmeteorology, fingerprintanalysis, andstatistical correlation, who\nuseditinthesenseofregressiontothemean. Thetermcurseofdimensionalitycomesfrom\nRichardBellman(1961).\nLogistic regression can be solved with gradient descent, or with the Newton-Raphson\nmethod (Newton,1671; Raphson, 1690). Avariant oftheNewtonmethodcalled L-BFGSis\nsometimesusedforlarge-dimensional problems;theLstandsfor\u201climitedmemory,\u201dmeaning\nthat it avoids creating the full matrices all at once, and instead creates parts of them on the\nfly. BFGSareauthors\u2019initials (Byrd etal.,1995).\nNearest-neighbors modelsdatebackatleasttoFixandHodges(1951)andhavebeena\nstandardtoolinstatisticsandpatternrecognitioneversince. WithinAI,theywerepopularized\nbyStanfillandWaltz(1986),whoinvestigatedmethodsforadaptingthedistancemetrictothe\ndata. HastieandTibshirani(1996)developedawaytolocalizethemetrictoeachpointinthe\nspace,dependingonthedistributionofdataaroundthatpoint. Gionisetal.(1999)introduced\nlocality-sensitive hashing, which has revolutionized the retrieval of similar objects in high-\ndimensional spaces, particularly in computer vision. Andoni and Indyk (2006) provide a\nrecentsurveyofLSHandrelatedmethods.\nThe ideas behind kernel machines come from Aizerman et al. (1964) (who also in-\ntroduced the kernel trick), but the full development of the theory is due to Vapnik and his\ncolleagues (Boser et al., 1992). SVMs were made practical with the introduction of the\nsoft-margin classifier for handling noisy data in a paper that won the 2008 ACM Theory\nand Practice Award(Cortes and Vapnik, 1995), andof theSequential Minimal Optimization\n(SMO)algorithm forefficientlysolving SVMproblemsusingquadratic programming (Platt,\n1999). SVMshave proven tobevery popular and effective fortasks such astextcategoriza-\ntion(Joachims,2001),computationalgenomics(CristianiniandHahn,2007),andnaturallan-\nguageprocessing,suchasthehandwrittendigitrecognitionofDeCosteandScho\u00a8lkopf(2002).\nAs part of this process, many new kernels have been designed that work with strings, trees,\nand othernonnumerical data types. Arelated technique that also uses the kernel trick toim-\nplicitly represent an exponential feature space isthe voted perceptron (Freund andSchapire,\n1999; Collins and Duffy, 2002). Textbooks on SVMs include Cristianini and Shawe-Taylor\n(2000)andScho\u00a8lkopf andSmola(2002). Afriendlierexposition appearsintheAIMagazine\narticle by Cristianini and Scho\u00a8lkopf (2002). Bengio and LeCun (2007) show some of the\nlimitations ofSVMsandotherlocal,nonparametric methods forlearning functions thathave\naglobalstructure butdonothavelocalsmoothness.\nEnsemblelearningisanincreasingly populartechnique for improvingtheperformance\nof learning algorithms. Bagging (Breiman, 1996), the first effective method, combines hy-\nBAGGING\npotheseslearnedfrommultiple bootstrapdatasets,eachgeneratedbysubsamplingtheorig-\ninaldataset. Theboostingmethoddescribedinthischapteroriginatedwiththeoretical work\nby Schapire (1990). The ADABOOST algorithm was developed by Freund and Schapire Bibliographical andHistorical Notes 761\n(1996) and analyzed theoretically bySchapire (2003). Friedman etal. (2000) explain boost-\ning from a statistician\u2019s viewpoint. Online learning is covered in a survey by Blum (1996)\nand a book by Cesa-Bianchi and Lugosi (2006). Dredze et al. (2008) introduce the idea of\nconfidence-weighted online learning for classification: in addition to keeping a weight for\neachparameter, theyalsomaintain ameasureofconfidence, sothatanewexamplecanhave\na large effect on features that were rarely seen before (and thus had low confidence) and a\nsmalleffectoncommonfeaturesthathavealready beenwell-estimated.\nThe literature on neural networks is rather too large (approximately 150,000 papers to\ndate)tocoverindetail. CowanandSharp(1988b, 1988a)surveytheearlyhistory, beginning\nwith the work of McCulloch and Pitts (1943). (As mentioned in Chapter 1, John McCarthy\nhaspointedtotheworkofNicolasRashevsky(1936,1938)astheearliestmathematicalmodel\nof neural learning.) Norbert Wiener, a pioneer of cybernetics and control theory (Wiener,\n1948), worked with McCulloch and Pitts and influenced a number of young researchers in-\ncludingMarvinMinsky,whomayhavebeenthefirsttodevelopaworkingneuralnetworkin\nhardware in 1951 (see Minsky and Papert, 1988, pp. ix\u2013x). Turing (1948) wrote a research\nreport titled Intelligent Machinery thatbegins withthesentence \u201cIpropose toinvestigate the\nquestionastowhetheritispossibleformachinerytoshowintelligentbehaviour\u201dandgoeson\ntodescribe arecurrent neuralnetworkarchitecture hecalled\u201cB-typeunorganized machines\u201d\nandanapproachtotrainingthem. Unfortunately, thereport wentunpublisheduntil1969,and\nwasallbutignoreduntilrecently.\nFrank Rosenblatt (1957) invented the modern \u201cperceptron\u201d and proved the percep-\ntron convergence theorem (1960), although it had been foreshadowed by purely mathemat-\nical work outside the context of neural networks (Agmon, 1954; Motzkin and Schoenberg,\n1954). Some early work was also done on multilayer networks, including Gamba percep-\ntrons (Gamba et al., 1961) and madalines (Widrow, 1962). Learning Machines (Nilsson,\n1965) covers muchof thisearly workand more. Thesubsequent demise ofearly perceptron\nresearch efforts was hastened (or, the authors later claimed, merely explained) by the book\nPerceptrons (Minsky and Papert, 1969), which lamented the field\u2019s lack of mathematical\nrigor. Thebook pointed outthat single-layer perceptrons could represent only linearly sepa-\nrableconcepts andnotedthelackofeffectivelearning algorithms formultilayernetworks.\nThe papers in (Hinton and Anderson, 1981), based on a conference in San Diego in\n1979, can be regarded as marking a renaissance of connectionism. The two-volume \u201cPDP\u201d\n(Parallel Distributed Processing) anthology (Rumelhart et al., 1986a) and a short article in\nNature (Rumelhart et al., 1986b) attracted a great deal of attention\u2014indeed, the number of\npapers on \u201cneural networks\u201d multiplied by a factor of 200 between 1980\u201384 and 1990\u201394.\nThe analysis of neural networks using the physical theory of magnetic spin glasses (Amit\net al., 1985) tightened the links between statistical mechanics and neural network theory\u2014\nprovidingnotonlyusefulmathematicalinsightsbutalsorespectability. Theback-propagation\ntechniquehadbeeninventedquiteearly(BrysonandHo,1969)butitwasrediscoveredseveral\ntimes(Werbos,1974;Parker,1985).\nTheprobabilisticinterpretationofneuralnetworkshasseveralsources,includingBaum\nand Wilczek (1988) and Bridle (1990). The role of the sigmoid function is discussed by\nJordan (1995). Bayesian parameter learning for neural networks was proposed by MacKay 762 Chapter 18. LearningfromExamples\n(1992) and is explored further by Neal(1996). Thecapacity ofneural networks to represent\nfunctionswasinvestigated byCybenko(1988,1989),whoshowedthattwohiddenlayersare\nenough to represent any function and a single layer is enough to represent any continuous\nfunction. The\u201coptimalbraindamage\u201dmethodforremovinguselessconnectionsisbyLeCun\net al. (1989), and Sietsma and Dow (1988) show how to remove useless units. The tiling\nalgorithm for growing larger structures is due to Me\u00b4zard and Nadal (1989). LeCun et al.\n(1995)surveyanumberofalgorithmsforhandwrittendigitrecognition. Improvederrorrates\nsince then were reported by Belongie et al. (2002) for shape matching and DeCoste and\nScho\u00a8lkopf (2002) for virtual support vectors. At the time of writing, the best test error rate\nreported is0.39%byRanzato etal.(2007)usingaconvolutional neuralnetwork.\nThecomplexityofneuralnetworklearninghasbeeninvestigatedbyresearchersincom-\nputational learning theory. Early computational results were obtained by Judd (1990), who\nshowedthatthegeneralproblemoffindingasetofweightsconsistentwithasetofexamples\nisNP-complete,evenunderveryrestrictiveassumptions. Someofthefirstsamplecomplexity\nresults were obtained by Baum and Haussler (1989), who showed that the number of exam-\nples required for effective learning grows as roughly W logW, where W is the number of\nweights.17 Since then, a much more sophisticated theory has been developed (Anthony and\nBartlett, 1999), including theimportant resultthattherepresentational capacity ofanetwork\ndepends on the size of the weights as well as on their number, a result that should not be\nsurprising inthelightofourdiscussion ofregularization.\nThe most popular kind of neural network that we did not cover is the radial basis\nRADIALBASIS function,orRBF,network. Aradialbasisfunctioncombinesaweightedcollectionofkernels\nFUNCTION\n(usuallyGaussians,ofcourse)todofunctionapproximation. RBFnetworkscanbetrainedin\ntwo phases: first, an unsupervised clustering approach is used to train the parameters of the\nGaussians\u2014the meansandvariances\u2014are trained, asinSection20.3.1. Inthesecondphase,\nthe relative weights of the Gaussians are determined. This is a system of linear equations,\nwhichweknowhowtosolvedirectly. Thus,bothphasesofRBFtraininghaveanicebenefit:\nthefirstphaseisunsupervised, andthusdoesnotrequirelabeledtrainingdata,andthesecond\nphase,although supervised, isefficient. SeeBishop(1995) formoredetails.\nRecurrentnetworks,inwhichunitsarelinked incycles, werementioned inthechap-\nHOPFIELDNETWORK ter but not explored in depth. Hopfield networks (Hopfield, 1982) are probably the best-\nunderstood class of recurrent networks. They use bidirectional connections with symmetric\nweights (i.e., w = w ), all of the units are both input and output units, the activation\ni,j j,i\nfunction g isthesignfunction, andtheactivation levelscanonlybe\u00b11. AHopfieldnetwork\nASSOCIATIVE functions as an associative memory: after the network trains on a set of examples, a new\nMEMORY\nstimulus will cause it to settle into an activation pattern corresponding to the example in the\ntrainingsetthatmostcloselyresemblesthenewstimulus. Forexample,ifthetrainingsetcon-\nsistsofasetofphotographs, andthenewstimulusisasmallpieceofoneofthephotographs,\nthen the network activation levels will reproduce the photograph from which the piece was\ntaken. Notice that the original photographs are not stored separately in the network; each\n17 Thisapproximatelyconfirmed\u201cUncleBernie\u2019srule.\u201d TherulewasnamedafterBernieWidrow,whorecom-\nmendedusingroughlytentimesasmanyexamplesasweights. Exercises 763\nweight is a partial encoding of all the photographs. One of the most interesting theoretical\nresultsisthatHopfieldnetworkscanreliablystoreupto0.138N trainingexamples,whereN\nisthenumberofunitsinthenetwork.\nBOLTZMANN Boltzmannmachines(HintonandSejnowski,1983,1986)alsousesymmetricweights,\nMACHINE\nbut include hidden units. In addition, they use a stochastic activation function, such that\nthe probability of the output being 1 is some function of the total weighted input. Boltz-\nmannmachinesthereforeundergostatetransitionsthatresembleasimulatedannealingsearch\n(see Chapter4)forthe configuration that bestapproximates thetraining set. Itturns out that\nBoltzmannmachinesareverycloselyrelatedtoaspecialcaseofBayesiannetworksevaluated\nwithastochastic simulationalgorithm. (SeeSection14.5.)\nForneuralnets,Bishop(1995),Ripley(1996),andHaykin(2008)aretheleadingtexts.\nThefieldofcomputational neuroscience iscoveredbyDayanandAbbott(2001).\nTheapproachtakeninthischapterwasinfluencedbytheexcellentcoursenotesofDavid\nCohn,TomMitchell,AndrewMoore,andAndrewNg. Thereareseveraltop-notchtextbooks\ninMachineLearning(Mitchell,1997;Bishop,2007)andinthecloselyalliedandoverlapping\nfields of pattern recognition (Ripley, 1996; Duda et al., 2001), statistics (Wasserman, 2004;\nHastieetal.,2001), datamining(Handetal.,2001; WittenandFrank, 2005), computational\nlearning theory(KearnsandVazirani,1994;Vapnik, 1998)andinformation theory(Shannon\nand Weaver, 1949; MacKay, 2002; Cover and Thomas, 2006). Other books concentrate on\nimplementations (Segaran, 2007; Marsland, 2009) and comparisons of algorithms (Michie\net al., 1994). Current research in machine learning is published in the annual proceedings\noftheInternational Conference on Machine Learning (ICML)and theconference on Neural\nInformation Processing Systems (NIPS), in Machine Learning and the Journal of Machine\nLearningResearch,andinmainstreamAIjournals.\nEXERCISES\n18.1 Considertheproblem facedbyaninfantlearningtospeakand understand alanguage.\nExplain how this process fits into the general learning model. Describe the percepts and\nactions oftheinfant, andthetypes oflearning the infant mustdo. Describe thesubfunctions\ntheinfantistryingtolearnintermsofinputsandoutputs, andavailable exampledata.\n18.2 Repeat Exercise 18.1 forthe case oflearning to play tennis (orsome other sport with\nwhichyouarefamiliar). Isthissupervised learningorreinforcement learning?\n18.3 Suppose we generate a training set from a decision tree and then apply decision-tree\nlearning to that training set. Is it the case that the learning algorithm will eventually return\nthecorrecttreeasthetraining-set sizegoestoinfinity? Whyorwhynot?\n18.4 In the recursive construction of decision trees, it sometimes happens that a mixed set\nof positive and negative examples remains at a leaf node, even after all the attributes have\nbeenused. Supposethatwehaveppositiveexamplesandnnegativeexamples. 764 Chapter 18. LearningfromExamples\na. ShowthatthesolutionusedbyDECISION-TREE-LEARNING,whichpicksthemajority\nclassification, minimizestheabsolute erroroverthesetof examplesattheleaf.\nb. Showthattheclassprobabilityp\/(p+n)minimizesthesumofsquared errors.\nCLASSPROBABILITY\n18.5 Suppose that an attribute splits the set of examples E into subsets E and that each\nk\nsubsethasp positiveexamplesandn negativeexamples. Showthattheattributehasstrictly\nk k\npositiveinformation gainunlesstheratio p \/(p +n )isthesameforallk.\nk k k\n18.6 Considerthefollowingdatasetcomprisedofthreebinaryinputattributes(A ,A ,and\n1 2\nA )andonebinaryoutput:\n3\nExample A A A Outputy\n1 2 3\nx 1 0 0 0\n1\nx 1 0 1 0\n2\nx 0 1 0 0\n3\nx 1 1 1 1\n4\nx 1 1 0 1\n5\nUsethealgorithm inFigure18.5(page 702)tolearn adecision treeforthesedata. Showthe\ncomputations madetodeterminetheattribute tosplitateachnode.\n18.7 Adecisiongraphisageneralizationofadecisiontreethatallowsnodes(i.e.,attributes\nused forsplits) tohavemultiple parents, ratherthan justa single parent. Theresulting graph\nmuststillbeacyclic. Now,considertheXORfunction ofthreebinaryinputattributes, which\nproduces thevalue1ifandonlyifanoddnumberofthethreeinputattributes hasvalue1.\na. Drawaminimal-sized decision treeforthethree-input XORfunction.\nb. Drawaminimal-sized decision graphforthethree-input XORfunction.\n18.8 Thisexerciseconsiders \u03c72 pruning ofdecision trees(Section18.3.5).\na. Createadata setwithtwoinput attributes, such that theinformation gain atthe rootof\nthetreeforbothattributesiszero,butthereisadecisiontreeofdepth2thatisconsistent\nwith all the data. What would \u03c72 pruning do on this data set if applied bottom up? If\nappliedtopdown?\nb. Modify DECISION-TREE-LEARNING to include \u03c72-pruning. You might wish to con-\nsultQuinlan(1986)orKearnsandMansour(1998)fordetails.\n18.9 The standard DECISION-TREE-LEARNING algorithm described in the chapter does\nnothandlecasesinwhichsomeexampleshavemissingattribute values.\na. First,weneedtofindawaytoclassifysuchexamples,givenadecisiontreethatincludes\ntests onthe attributes forwhich values can be missing. Suppose that anexample xhas\na missing value for attribute A and that the decision tree tests for A at a node that x\nreaches. One way to handle this case is to pretend that the example has all possible\nvalues for the attribute, but to weight each value according to its frequency among all\nof the examples that reach that node in the decision tree. The classification algorithm\nshouldfollowallbranchesatanynodeforwhichavalueismissingandshouldmultiply Exercises 765\ntheweightsalongeachpath. Writeamodifiedclassificationalgorithmfordecisiontrees\nthathasthisbehavior.\nb. Now modify the information-gain calculation so that in any given collection of exam-\nples C at a given node in the tree during the construction process, the examples with\nmissingvaluesforanyoftheremaining attributes aregiven \u201cas-if\u201dvalues according to\nthefrequencies ofthosevaluesinthesetC.\n18.10 In Section 18.3.6, we noted that attributes with many different possible values can\ncauseproblemswiththegainmeasure. Suchattributes tendtosplittheexamplesintonumer-\noussmallclassesorevensingletonclasses,therebyappearingtobehighlyrelevantaccording\ntothegainmeasure. Thegain-ratiocriterionselectsattributesaccordingtotheratiobetween\ntheir gain and their intrinsic information content\u2014that is, the amount of information con-\ntainedintheanswertothequestion,\u201cWhatisthevalueofthisattribute?\u201d Thegain-ratiocrite-\nrionthereforetriestomeasurehowefficientlyanattribute providesinformationonthecorrect\nclassification ofanexample. Writeamathematical expression fortheinformation content of\nanattribute, andimplementthegainratiocriterion inDECISION-TREE-LEARNING.\n18.11 SupposeyouarerunningalearningexperimentonanewalgorithmforBooleanclas-\nsification. You have a data set consisting of 100 positive and 100 negative examples. You\nplantouseleave-one-outcross-validation andcompareyouralgorithmtoabaselinefunction,\na simple majority classifier. (A majority classifier is given a set of training data and then\nalways outputs the class that is in the majority in the training set, regardless of the input.)\nYou expect the majority classifier to score about 50% on leave-one-out cross-validation, but\ntoyoursurprise, itscoreszeroeverytime. Canyouexplainwhy?\n18.12 Construct a decision list to classify the data below. Select tests to be as small as\npossible(intermsofattributes), breakingtiesamongtestswiththesamenumberofattributes\nbyselectingtheonethatclassifiesthegreatestnumberofexamplescorrectly. Ifmultipletests\nhavethesamenumberofattributesandclassifythesamenumberofexamples,thenbreakthe\ntieusingattributes withlowerindexnumbers(e.g.,select A overA ).\n1 2\nExample A A A A y\n1 2 3 4\nx 1 0 0 0 1\n1\nx 1 0 1 1 1\n2\nx 0 1 0 0 1\n3\nx 0 1 1 0 0\n4\nx 1 1 0 1 1\n5\nx 0 1 0 1 0\n6\nx 0 0 1 1 1\n7\nx 0 0 1 0 0\n8\n18.13 Prove that a decision list can represent the same function as a decision tree while\nusing atmostasmanyrulesasthereareleavesinthedecision treeforthatfunction. Givean\nexampleofafunctionrepresentedbyadecisionlistusingstrictlyfewerrulesthanthenumber\nofleavesinaminimal-sized decisiontreeforthatsamefunction. 766 Chapter 18. LearningfromExamples\n18.14 Thisexerciseconcernstheexpressiveness ofdecisionlists(Section18.5).\na. Show that decision lists can represent any Boolean function, if the size of the tests is\nnotlimited.\nb. Showthatifthetestscancontainatmostkliteralseach,thendecisionlistscanrepresent\nanyfunction thatcanberepresented byadecision treeofdepthk.\n18.15 Supposea7-nearest-neighbors regressionsearchreturns {7,6,8,4,7,11,100} asthe\n7 nearest y values for a given x value. What is the value of y\u02c6 that minimizes the L loss\n1\nfunctiononthisdata? Thereisacommonnameinstatisticsforthisvalueasafunctionofthe\ny values;whatisit? Answerthesametwoquestions forthe L lossfunction.\n2\n18.16 Figure18.31showedhowacircleattheorigincanbelinearly separated bymapping\nfromthefeatures (x ,x )tothetwodimensions(x2,x2). Butwhatifthecircleisnotlocated\n1 2 1 2\nat the origin? What if it is an ellipse, not a circle? The general equation for a circle (and\nhencethedecisionboundary) is(x \u2212a)2+(x \u2212b)2\u2212r2=0,andthegeneralequationfor\n1 2\nanellipseisc(x \u2212a)2+d(x \u2212b)2\u22121=0.\n1 2\na. Expand out the equation for the circle and show what the weights w would be for the\ni\ndecision boundary in the four-dimensional feature space (x ,x ,x2,x2). Explain why\n1 2 1 2\nthismeansthatanycircleislinearly separable inthisspace.\nb. Dothesameforellipses inthefive-dimensional featurespace(x ,x ,x2,x2,x x ).\n1 2 1 2 1 2\n18.17 Construct a support vector machine that computes the XOR function. Use values of\n+1 and \u20131 (instead of 1 and 0) for both inputs and outputs, so that an example looks like\n([\u22121,1],1)or([\u22121,\u22121],\u22121). Maptheinput[x ,x ]intoaspaceconsistingofx andx x .\n1 2 1 1 2\nDraw the four input points in this space, and the maximal margin separator. What is the\nmargin? Nowdrawtheseparating linebackintheoriginalEuclideaninputspace.\n18.18 Consider an ensemble learning algorithm that uses simple majority voting among\nK learned hypotheses. Suppose that each hypothesis has error (cid:2) and that the errors made\nby each hypothesis are independent of the others\u2019. Calculate a formula for the error of the\nensemble algorithm in terms of K and (cid:2), and evaluate it for the cases where K=5, 10, and\n20and(cid:2)=0.1,0.2,and0.4. Iftheindependence assumptionisremoved,isitpossibleforthe\nensembleerrortobe worsethan(cid:2)?\n18.19 Construct by hand a neural network that computes the XOR function of two inputs.\nMakesuretospecify whatsortofunitsyouareusing.\n18.20\nRecallfromChapter18thatthereare22n\ndistinctBooleanfunctionsofninputs. How\nmanyofthesearerepresentable byathreshold perceptron?\n18.21 Section 18.6.4 (page 725) noted that the output of the logistic function could be in-\nterpreted asa probability passigned bythemodeltotheproposition that f(x)=1;theprob-\nability that f(x)=0 is therefore 1 \u2212 p. Write down the probability p as a function of x\nand calculate the derivative of logp with respect to each weight w . Repeat the process for\ni\nlog(1\u2212p). Thesecalculationsgivealearningruleforminimizingthenegative-log-likelihood Exercises 767\nloss function for a probabilistic hypothesis. Comment on any resemblance to other learning\nrulesinthechapter.\n18.22 Suppose youhad aneural network withlinearactivation functions. Thatis, foreach\nunittheoutputissomeconstant ctimestheweightedsumoftheinputs.\na. Assume that the network has one hidden layer. For a given assignment to the weights\nw, write down equations for the value of the units in the output layer as a function of\nwandtheinput layer x,without anyexplicit mention oftheoutput ofthehidden layer.\nShowthatthereisanetworkwithnohiddenunitsthatcomputesthesamefunction.\nb. Repeat the calculation inpart (a), but this timedo itfora network with any numberof\nhiddenlayers.\nc. Suppose a network with one hidden layer and linear activation functions has n input\nand output nodes and h hidden nodes. What effect does the transformation in part (a)\nto a network with no hidden layers have on the total number of weights? Discuss in\nparticularthecase h* n.\n18.23 Suppose that a training set contains only a single example, repeated 100 times. In\n80 of the 100 cases, the single output value is 1; in the other 20, it is 0. What will a back-\npropagation network predict for this example, assuming that it has been trained and reaches\naglobaloptimum? (Hint: tofindtheglobaloptimum, differentiate theerrorfunction andset\nittozero.)\n18.24 TheneuralnetworkwhoselearningperformanceismeasuredinFigure18.25hasfour\nhidden nodes. Thisnumberwaschosen somewhatarbitrarily. Useacross-validation method\ntofindthebestnumberofhiddennodes.\n18.25 ConsidertheproblemofseparatingN datapointsintopositiveandnegativeexamples\nusing a linear separator. Clearly, this can always be done for N =2 points on a line of\ndimension d=1, regardless of how the points are labeled or where they are located (unless\nthepointsareinthesameplace).\na. ShowthatitcanalwaysbedoneforN=3pointsonaplaneofdimensiond=2,unless\ntheyarecollinear.\nb. Showthatitcannot alwaysbedoneforN=4pointsonaplaneofdimensiond=2.\nc. ShowthatitcanalwaysbedoneforN=4pointsinaspaceofdimension d=3,unless\ntheyarecoplanar.\nd. Showthatitcannot alwaysbedoneforN =5pointsinaspaceofdimension d=3.\ne. The ambitious student may wish to prove that N points in general position (but not\nN +1)arelinearlyseparable inaspaceofdimension N \u22121. 19\nKNOWLEDGE IN\nLEARNING\nInwhichweexaminetheproblem oflearning whenyouknowsomething already.\nInalloftheapproaches tolearning described intheprevious chapter, theideaistoconstruct\nafunction thathastheinput\u2013output behaviorobserved inthedata. Ineachcase,thelearning\nmethodscanbeunderstoodassearchingahypothesisspacetofindasuitablefunction,starting\nfrom only a very basic assumption about the form of the function, such as \u201csecond-degree\npolynomial\u201d or\u201cdecision tree\u201d and perhaps a preference for simpler hypotheses. Doing this\namounts to saying that before you can learn something new, you must first forget (almost)\neverything you know. In this chapter, we study learning methods that can take advantage\nof prior knowledge about the world. In most cases, the prior knowledge is represented\nPRIORKNOWLEDGE\nas general first-order logical theories; thus for the first time we bring together the work on\nknowledgerepresentation andlearning.\n19.1 A LOGICAL FORMULATION OF LEARNING\nChapter 18 defined pure inductive learning as a process of finding a hypothesis that agrees\nwiththeobservedexamples. Here,wespecializethisdefinitiontothecasewherethehypoth-\nesisisrepresentedbyasetoflogicalsentences. Exampledescriptionsandclassificationswill\nalso be logical sentences, and a new example can be classified by inferring a classification\nsentence from the hypothesis and the example description. This approach allows for incre-\nmentalconstructionofhypotheses,onesentenceatatime. Italsoallowsforpriorknowledge,\nbecause sentences that are already known can assist in the classification of new examples.\nThelogical formulation oflearning mayseemlikealotofextraworkatfirst,butitturns out\ntoclarify manyoftheissues inlearning. Itenables ustogowellbeyond the simplelearning\nmethodsofChapter18byusingthefullpoweroflogicalinferenceintheserviceoflearning.\n19.1.1 Examples andhypotheses\nRecallfromChapter18therestaurant learningproblem: learningarulefordecidingwhether\ntowaitforatable. ExamplesweredescribedbyattributessuchasAlternate,Bar,Fri\/Sat,\n768 Section19.1. ALogicalFormulationofLearning 769\nand so on. In a logical setting, an example is described by a logical sentence; the attributes\nbecome unary predicates. Let us generically call the ith example X . For instance, the first\ni\nexamplefromFigure18.3(page700)isdescribed bythesentences\nAlternate(X )\u2227\u00acBar(X )\u2227\u00acFri\/Sat(X )\u2227Hungry(X )\u2227...\n1 1 1 1\nWewillusethenotationD (X )torefertothedescriptionofX ,whereD canbeanylogical\ni i i i\nexpression taking a single argument. The classification of the example is given by a literal\nusingthegoalpredicate, inthiscase\nWillWait(X ) or \u00acWillWait(X ).\n1 1\nThecompletetrainingsetcanthusbeexpressedastheconjunctionofalltheexampledescrip-\ntionsandgoalliterals.\nThe aim of inductive learning in general is to find a hypothesis that classifies the ex-\namples well and generalizes well to new examples. Herewe are concerned withhypotheses\nexpressed inlogic;eachhypothesis h willhavetheform\nj\n\u2200x Goal(x) \u21d4 C (x),\nj\nwhere C (x) is a candidate definition\u2014some expression involving the attribute predicates.\nj\nForexample,adecisiontreecanbeinterpretedasalogicalexpressionofthisform. Thus,the\ntree in Figure 18.6 (page 702) expresses the following logical definition (which we will call\nh forfuturereference):\nr\n\u2200r WillWait(r) \u21d4 Patrons(r,Some)\n\u2228 Patrons(r,Full)\u2227Hungry(r)\u2227Type(r,French)\n\u2228 Patrons(r,Full)\u2227Hungry(r)\u2227Type(r,Thai) (19.1)\n\u2227Fri\/Sat(r)\n\u2228 Patrons(r,Full)\u2227Hungry(r)\u2227Type(r,Burger).\nEachhypothesis predicts thatacertain setofexamples\u2014namely, thosethat satisfy itscandi-\ndate definition\u2014will be examples of the goal predicate. This set is called the extension of\nEXTENSION\nthe predicate. Two hypotheses with different extensions are therefore logically inconsistent\nwith each other, because they disagree on their predictions for at least one example. If they\nhavethesameextension, theyarelogically equivalent.\nThehypothesisspaceHisthesetofallhypotheses{h ,...,h }thatthelearningalgo-\n1 n\nrithmisdesigned toentertain. Forexample, the DECISION-TREE-LEARNING algorithm can\nentertain anydecision treehypothesis defined intermsoftheattributes provided; itshypoth-\nesis space therefore consists of all these decision trees. Presumably, the learning algorithm\nbelievesthatoneofthehypotheses iscorrect;thatis,itbelievesthesentence\nh \u2228h \u2228h \u2228...\u2228h . (19.2)\n1 2 3 n\nAs the examples arrive, hypotheses that are not consistent with the examples can be ruled\nout. Letusexaminethisnotion ofconsistency morecarefully. Obviously, ifhypothesis h is\nj\nconsistentwiththeentiretrainingset,ithastobeconsistentwitheachexampleinthetraining\nset. What would it mean for it to be inconsistent with an example? There are two possible\nwaysthatthiscanhappen: 770 Chapter 19. KnowledgeinLearning\n\u2022 Anexample canbeafalsenegative forthehypothesis, ifthehypothesis saysitshould\nFALSENEGATIVE\nbenegativebutinfactitispositive. Forinstance, thenewexampleX described by\n13\nPatrons(X ,Full)\u2227\u00acHungry(X )\u2227...\u2227WillWait(X )\n13 13 13\nwouldbeafalsenegativeforthehypothesis h givenearlier. From h andtheexample\nr r\ndescription, we can deduce both WillWait(X ), which is what the example says,\n13\nand \u00acWillWait(X ), which is what the hypothesis predicts. The hypothesis and the\n13\nexamplearethereforelogically inconsistent.\n\u2022 Anexample can be a false positive forthe hypothesis, if the hypothesis says it should\nFALSEPOSITIVE\nbepositivebutinfactitisnegative.1\nIf an example is afalse positive or false negative fora hypothesis, then the example and the\nhypothesis arelogically inconsistent witheachother. Assumingthattheexampleisacorrect\nobservation offact,thenthehypothesis canberuledout. Logically, thisisexactlyanalogous\nto the resolution rule of inference (see Chapter 9), where the disjunction of hypotheses cor-\nresponds toaclause andtheexample corresponds toaliteral thatresolves against oneofthe\nliteralsintheclause. Anordinarylogicalinferencesystemthereforecould,inprinciple,learn\nfrom the example by eliminating one or more hypotheses. Suppose, for example, that the\nexampleisdenotedbythesentenceI ,andthehypothesisspaceish \u2228h \u2228h \u2228h . Thenif\n1 1 2 3 4\nI isinconsistentwithh andh ,thelogicalinferencesystemcandeducethenewhypothesis\n1 2 3\nspaceh \u2228h .\n1 4\nWe therefore can characterize inductive learning in a logical setting as a process of\ngradually eliminating hypotheses that are inconsistent with the examples, narrowing down\nthe possibilities. Because the hypothesis space is usually vast (oreveninfinite inthe case of\nfirst-order logic), we do not recommend trying to build a learning system using resolution-\nbasedtheoremprovingandacompleteenumerationofthehypothesisspace. Instead,wewill\ndescribe twoapproaches thatfindlogically consistent hypotheses withmuchlesseffort.\n19.1.2 Current-best-hypothesis search\nCURRENT-BEST- The idea behind current-best-hypothesis search is to maintain a single hypothesis, and to\nHYPOTHESIS\nadjust it as new examples arrive in order to maintain consistency. The basic algorithm was\ndescribed byJohnStuartMill(1843), andmaywellhaveappeared evenearlier.\nSuppose we have some hypothesis such as h , of which we have grown quite fond.\nr\nAs long as each new example is consistent, we need do nothing. Then along comes a false\nnegativeexample, X . Whatdowedo? Figure 19.1(a) shows h schematically asaregion:\n13 r\neverythinginsidetherectangleispartoftheextensionofh . Theexamplesthathaveactually\nr\nbeen seen so far are shown as \u201c+\u201d or \u201c\u2013\u201d, and we see that h correctly categorizes all the\nr\nexamples as positive or negative examples of WillWait. In Figure 19.1(b), a new example\n(circled)isafalsenegative: thehypothesissaysitshouldbenegativebutitisactuallypositive.\nTheextensionofthehypothesismustbeincreasedtoincludeit. Thisiscalledgeneralization;\nGENERALIZATION\nonepossiblegeneralization isshowninFigure19.1(c). TheninFigure19.1(d),weseeafalse\npositive: the hypothesis says the new example (circled) should be positive, but it actually is\n1 Theterms\u201cfalse positive\u201dand \u201cfalsenegative\u201d areused inmedicine todescribe erroneous resultsfromlab\ntests.Aresultisafalsepositiveifitindicatesthatthepatienthasthediseasewheninfactnodiseaseispresent. Section19.1. ALogicalFormulationofLearning 771\n\u2013 \u2013 \u2013 \u2013\n\u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013\n\u2013 \u2013 \u2013 \u2013 \u2013\n\u2013 \u2013 \u2013 \u2013 \u2013\n+ + + + +\n+ + + + +\n+ \u2013 + \u2013 + \u2013 + \u2013 + \u2013\n\u2013 \u2013 \u2013 \u2013 \u2013\n+ + + + +\n+ + + + + \u2013 + + \u2013 + +\n++ \u2013 ++ \u2013 ++ \u2013 ++ \u2013 ++ \u2013\n\u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013\n(a) (b) (c) (d) (e)\nFigure19.1 (a)Aconsistenthypothesis. (b)Afalsenegative. (c)Thehypothesisisgen-\neralized.(d)Afalsepositive.(e)Thehypothesisisspecialized.\nfunctionCURRENT-BEST-LEARNING(examples,h)returnsahypothesisorfail\nifexamples isemptythen\nreturnh\ne\u2190FIRST(examples)\nife isconsistentwithh then\nreturnCURRENT-BEST-LEARNING(REST(examples),h)\nelseife isafalsepositiveforh then\nforeachh(cid:5) in specializationsofh consistentwithexamples seensofardo\nh(cid:5)(cid:5)\u2190CURRENT-BEST-LEARNING(REST(examples),h(cid:5))\nifh(cid:5)(cid:5) (cid:7)= fail thenreturnh(cid:5)(cid:5)\nelseife isafalsenegativeforh then\nforeachh(cid:5) ingeneralizationsofh consistentwithexamples seensofardo\nh(cid:5)(cid:5)\u2190CURRENT-BEST-LEARNING(REST(examples),h(cid:5))\nifh(cid:5)(cid:5) (cid:7)= fail thenreturnh(cid:5)(cid:5)\nreturnfail\nFigure 19.2 The current-best-hypothesis learning algorithm. It searches for a consis-\ntent hypothesis that fits all the examples and backtracks when no consistent specializa-\ntion\/generalizationcan be found. To start the algorithm, any hypothesis can be passed in;\nitwillbespecializedorgneralizedasneeded.\nnegative. Theextension ofthehypothesis mustbedecreased toexclude theexample. Thisis\ncalledspecialization; inFigure19.1(e)weseeonepossible specialization ofthe hypothesis.\nSPECIALIZATION\nThe\u201cmoregeneral than\u201d and\u201cmore specific than\u201d relations between hypotheses provide the\nlogicalstructure onthehypothesis spacethatmakesefficientsearchpossible.\nWecannowspecifytheCURRENT-BEST-LEARNING algorithm,showninFigure19.2.\nNoticethateachtimeweconsidergeneralizingorspecializingthehypothesis,wemustcheck\nforconsistency withtheotherexamples, because anarbitrary increase\/decrease intheexten-\nsionmightinclude\/exclude previously seennegative\/positive examples. 772 Chapter 19. KnowledgeinLearning\nWehave defined generalization and specialization as operations that change the exten-\nsion of a hypothesis. Now we need to determine exactly how they can be implemented as\nsyntactic operations that change the candidate definition associated with the hypothesis, so\nthataprogramcancarrythemout. Thisisdonebyfirstnotingthatgeneralizationandspecial-\nization are also logical relationships between hypotheses. If hypothesis h , with definition\n1\nC ,isageneralization ofhypothesis h withdefinitionC ,thenwemusthave\n1 2 2\n\u2200x C (x) \u21d2 C (x).\n2 1\nTherefore in order to construct a generalization of h , we simply need to find a defini-\n2\ntion C that is logically implied by C . This is easily done. For example, if C (x) is\n1 2 2\nAlternate(x) \u2227 Patrons(x,Some), then one possible generalization is given by C (x) \u2261\n1\nDROPPING Patrons(x,Some). This is called dropping conditions. Intuitively, it generates a weaker\nCONDITIONS\ndefinitionandthereforeallowsalargersetofpositiveexamples. Thereareanumberofother\ngeneralization operations, depending on the language being operated on. Similarly, we can\nspecialize ahypothesis byadding extra conditions toitscandidate definition orbyremoving\ndisjuncts from adisjunctive definition. Letusseehowthis worksontherestaurant example,\nusingthedatainFigure18.3.\n\u2022 Thefirstexample, X ,ispositive. Theattribute Alternate(X )istrue,solettheinitial\n1 1\nhypothesis be\nh : \u2200x WillWait(x) \u21d4 Alternate(x).\n1\n\u2022 Thesecondexample,X ,isnegative. h predictsittobepositive,soitisafalsepositive.\n2 1\nTherefore,weneedtospecialize h . Thiscanbedonebyaddinganextraconditionthat\n1\nwillruleoutX ,whilecontinuing toclassifyX aspositive. Onepossibility is\n2 1\nh : \u2200x WillWait(x) \u21d4 Alternate(x)\u2227Patrons(x,Some).\n2\n\u2022 Thethirdexample,X ,ispositive. h predictsittobenegative,soitisafalsenegative.\n3 2\nTherefore,weneedtogeneralize h . WedroptheAlternate condition, yielding\n2\nh : \u2200x WillWait(x) \u21d4 Patrons(x,Some).\n3\n\u2022 Thefourthexample,X ,ispositive. h predictsittobenegative,soitisafalsenegative.\n4 3\nWe therefore need to generalize h . We cannot drop the Patrons condition, because\n3\nthat would yield an all-inclusive hypothesis that would be inconsistent with X . One\n2\npossibility istoaddadisjunct:\nh : \u2200x WillWait(x) \u21d4 Patrons(x,Some)\n4\n\u2228(Patrons(x,Full)\u2227Fri\/Sat(x)).\nAlready,thehypothesis isstartingtolookreasonable. Obviously, thereareotherpossibilities\nconsistent withthefirstfourexamples;herearetwoofthem:\nh(cid:2) : \u2200x WillWait(x) \u21d4 \u00acWaitEstimate(x,30-60).\n4\nh(cid:2)(cid:2) : \u2200x WillWait(x) \u21d4 Patrons(x,Some)\n4\n\u2228(Patrons(x,Full)\u2227WaitEstimate(x,10-30)).\nTheCURRENT-BEST-LEARNING algorithmisdescribednondeterministically, becauseatany\npoint,theremaybeseveralpossiblespecializationsorgeneralizationsthatcanbeapplied. The Section19.1. ALogicalFormulationofLearning 773\nfunctionVERSION-SPACE-LEARNING(examples)returnsaversionspace\nlocalvariables: V,theversionspace:thesetofallhypotheses\nV \u2190thesetofallhypotheses\nforeachexamplee inexamples do\nifV isnotemptythenV \u2190VERSION-SPACE-UPDATE(V,e)\nreturnV\nfunctionVERSION-SPACE-UPDATE(V,e)returnsanupdatedversionspace\nV \u2190{h\u2208V : hisconsistentwithe}\nFigure19.3 Theversionspacelearningalgorithm.ItfindsasubsetofV thatisconsistent\nwithalltheexamples.\nchoicesthataremadewillnotnecessarily leadtothesimplesthypothesis, andmayleadtoan\nunrecoverable situation wherenosimplemodificationofthe hypothesis isconsistent withall\nofthedata. Insuchcases, theprogrammustbacktrack toaprevious choicepoint.\nThe CURRENT-BEST-LEARNING algorithm and its variants have been used in many\nmachine learning systems, starting with Patrick Winston\u2019s (1970) \u201carch-learning\u201d program.\nWithalargenumberofexamplesandalargespace, however,somedifficulties arise:\n1. Checkingallthepreviousexamplesoveragainforeachmodificationisveryexpensive.\n2. Thesearchprocessmayinvolveagreatdealofbacktracking. AswesawinChapter18,\nhypothesis spacecanbeadoublyexponentially largeplace.\n19.1.3 Least-commitment search\nBacktracking arises because the current-best-hypothesis approach has to choose a particular\nhypothesis as its best guess even though it does not have enough data yet to be sure of the\nchoice. What we can do instead is to keep around all and only those hypotheses that are\nconsistent with all the data so far. Each new example will either have no effect or will get\nrid of some of the hypotheses. Recall that the original hypothesis space can be viewed as a\ndisjunctive sentence\nh \u2228h \u2228h ...\u2228h .\n1 2 3 n\nAsvarioushypothesesarefoundtobeinconsistentwiththeexamples,thisdisjunctionshrinks,\nretaining only those hypotheses not ruled out. Assuming that the original hypothesis space\ndoes in fact contain the right answer, the reduced disjunction must still contain the right an-\nswerbecauseonlyincorrecthypotheseshavebeenremoved. Thesetofhypothesesremaining\niscalledtheversionspace,andthelearningalgorithm (sketched inFigure19.3)iscalledthe\nVERSIONSPACE\nCANDIDATE versionspacelearningalgorithm (alsothe candidateelimination algorithm).\nELIMINATION\nOne important property of this approach is that it is incremental: one never has to\ngo back and reexamine the old examples. All remaining hypotheses are guaranteed to be\nconsistent with them already. But there is an obvious problem. We already said that the 774 Chapter 19. KnowledgeinLearning\nThis region all inconsistent\nG 1 G 2 G 3 . . . G m\nMore general\nMore specific\nS 1 S 2 . . . S n\nThis region all inconsistent\nFigure19.4 Theversionspacecontainsallhypothesesconsistentwiththeexamples.\nhypothesisspaceisenormous,sohowcanwepossiblywritedownthisenormousdisjunction?\nThe following simple analogy is very helpful. How do you represent all the real num-\nbers between 1 and 2? After all, there are an infinite number of them! The answer is to use\nanintervalrepresentation thatjustspecifiestheboundaries oftheset: [1,2]. Itworksbecause\nwehaveanordering ontherealnumbers.\nWealsohaveanorderingonthehypothesisspace,namely,generalization\/specialization.\nThis is a partial ordering, which means that each boundary will not be a point but rather a\nset of hypotheses called a boundary set. The great thing is that we can represent the entire\nBOUNDARYSET\nversion space using just twoboundary sets: amostgeneral boundary (the G-set) andamost\nG-SET\nspecific boundary (the S-set). Everything in between isguaranteed to be consistent with the\nS-SET\nexamples. Beforeweprovethis,letusrecap:\n\u2022 The current version space is the set of hypotheses consistent with all the examples so\nfar. Itisrepresented bytheS-setandG-set,eachofwhichis asetofhypotheses.\n\u2022 Every member of the S-set is consistent with all observations so far, and there are no\nconsistent hypotheses thataremorespecific.\n\u2022 Every member of the G-set is consistent with all observations so far, and there are no\nconsistent hypotheses thataremoregeneral.\nWewanttheinitialversionspace(beforeanyexampleshavebeenseen)torepresentallpossi-\nblehypotheses. WedothisbysettingtheG-settocontain True (thehypothesis thatcontains\neverything), andtheS-settocontain False (thehypothesis whoseextension isempty).\nFigure19.4showsthegeneralstructureoftheboundary-set representationoftheversion\nspace. Toshowthattherepresentation issufficient,weneed thefollowingtwoproperties: Section19.1. ALogicalFormulationofLearning 775\n1. Everyconsistenthypothesis(otherthanthoseintheboundarysets)ismorespecificthan\nsomememberoftheG-set,andmoregeneral thansomememberof theS-set. (Thatis,\nthere are no \u201cstragglers\u201d left outside.) This follows directly from the definitions of S\nand G. If there were a straggler h, then it would have to be no more specific than any\nmember of G, in which case it belongs in G; or no more general than any member of\nS,inwhichcaseitbelongsinS.\n2. Everyhypothesis morespecific than somememberoftheG-setand moregeneral than\nsomememberoftheS-setisaconsistent hypothesis. (Thatis, there areno\u201choles\u201d be-\ntween the boundaries.) Any h between S and G must reject all the negative examples\nrejectedbyeachmemberofG(becauseitismorespecific),andmustacceptallthepos-\nitiveexamplesacceptedbyanymemberofS (becauseitismoregeneral). Thus,hmust\nagree with all the examples, and therefore cannot be inconsistent. Figure 19.5 shows\nthe situation: there are no known examples outside S but inside G, so any hypothesis\ninthegapmustbeconsistent.\nWe have therefore shown that if S and G are maintained according to their definitions, then\nthey provide a satisfactory representation ofthe version space. Theonly remaining problem\nis how to update S and G for a new example (the job of the VERSION-SPACE-UPDATE\nfunction). This may appear rather complicated at first, but from the definitions and with the\nhelpofFigure19.4,itisnottoohardtoreconstruct thealgorithm.\n\u2013 \u2013\n\u2013 G\n\u2013 1\n\u2013\n\u2013\n\u2013 \u2013 + + S G 2 \u2013\n+ + + 1 \u2013\n+\n+\n+ + +\n\u2013\n\u2013\n\u2013 \u2013\nFigure 19.5 The extensions of the members of G and S. No known examples lie in\nbetweenthetwosetsofboundaries.\nWeneedtoworryaboutthemembers S andG oftheS-andG-sets. Foreachone,the\ni i\nnewexamplemaybeafalsepositiveorafalsenegative.\n1. Falsepositive for S : Thismeans S istoo general, but there are no consistent special-\ni i\nizationsofS (bydefinition), sowethrowitoutoftheS-set.\ni\n2. FalsenegativeforS : ThismeansS istoospecific,sowereplaceitbyallitsimmediate\ni i\ngeneralizations, provided theyaremorespecificthansomememberofG.\n3. FalsepositiveforG : ThismeansG istoogeneral,sowereplaceitbyallitsimmediate\ni i\nspecializations, provided theyaremoregeneralthansomememberofS. 776 Chapter 19. KnowledgeinLearning\n4. Falsenegative forG : ThismeansG istoospecific, butthere areno consistent gener-\ni i\nalizations ofG (bydefinition) sowethrowitoutoftheG-set.\ni\nWecontinuetheseoperations foreachnewexampleuntiloneofthreethingshappens:\n1. Wehave exactly one hypothesis left inthe version space, in which case wereturn itas\ntheunique hypothesis.\n2. The version space collapses\u2014either S or G becomes empty, indicating that there are\nnoconsistent hypotheses forthetraining set. Thisisthesamecaseasthefailure ofthe\nsimpleversionofthedecision treealgorithm.\n3. We run out of examples and have several hypotheses remaining in the version space.\nThis means the version space represents a disjunction of hypotheses. For any new\nexample,ifallthedisjunctsagree,thenwecanreturntheirclassificationoftheexample.\nIftheydisagree, onepossibility istotakethemajorityvote.\nWeleaveasanexercisetheapplication oftheVERSION-SPACE-LEARNING algorithm tothe\nrestaurant data.\nTherearetwoprincipal drawbackstotheversion-space approach:\n\u2022 Ifthedomaincontainsnoiseorinsufficientattributesforexactclassification,theversion\nspacewillalwayscollapse.\n\u2022 Ifweallowunlimiteddisjunction inthehypothesis space,theS-setwillalwayscontain\na single most-specific hypothesis, namely, the disjunction of the descriptions of the\npositiveexamplesseentodate. Similarly,theG-setwillcontainjustthenegationofthe\ndisjunction ofthedescriptions ofthenegativeexamples.\n\u2022 For some hypothesis spaces, the number of elements in the S-set or G-set may grow\nexponentiallyinthenumberofattributes,eventhoughefficientlearningalgorithmsexist\nforthosehypothesis spaces.\nTo date, no completely successful solution has been found for the problem of noise. The\nproblem ofdisjunction canbeaddressed byallowing onlylimitedformsofdisjunction orby\nGENERALIZATION including a generalization hierarchy of more general predicates. For example, instead of\nHIERARCHY\nusing thedisjunction WaitEstimate(x,30-60)\u2228WaitEstimate(x,>60), wemight use the\nsingle literal LongWait(x). The set of generalization and specialization operations can be\neasilyextendedtohandlethis.\nThe pure version space algorithm was first applied in the Meta-DENDRAL system,\nwhich was designed to learn rules for predicting how molecules would break into pieces in\na mass spectrometer (Buchanan and Mitchell, 1978). Meta-DENDRAL was able to generate\nrulesthatweresufficientlynoveltowarrantpublicationinajournalofanalyticalchemistry\u2014\nthe first real scientific knowledge generated by a computer program. It was also used in the\nelegantLEXsystem(Mitchelletal.,1983),whichwasabletolearntosolvesymbolicintegra-\ntion problems by studying its own successes and failures. Although version space methods\nare probably not practical in most real-world learning problems, mainly because of noise,\ntheyprovideagooddealofinsight intothelogicalstructure ofhypothesis space. Section19.2. KnowledgeinLearning 777\nPrior\nknowledge\nKnowledge-based\nObservations Hypotheses Predictions\ninductive learning\nFigure 19.6 A cumulative learning process uses, and adds to, its stock of background\nknowledgeovertime.\n19.2 KNOWLEDGE IN LEARNING\nTheprecedingsectiondescribedthesimplestsettingforinductivelearning. Tounderstandthe\nrole of prior knowledge, we need to talk about the logical relationships among hypotheses,\nexampledescriptions, andclassifications. LetDescriptions denotetheconjunction ofallthe\nexampledescriptions inthetrainingset,andletClassifications denotetheconjunction ofall\ntheexampleclassifications. ThenaHypothesis that\u201cexplains theobservations\u201d mustsatisfy\nthefollowingproperty (recallthat |=means\u201clogically entails\u201d):\nHypothesis \u2227Descriptions |= Classifications . (19.3)\nENTAILMENT Wecall this kind of relationship an entailment constraint, inwhich Hypothesis is the\u201cun-\nCONSTRAINT\nknown.\u201d Pure inductive learning means solving this constraint, where Hypothesis is drawn\nfrom some predefined hypothesis space. For example, if we consider a decision tree as a\nlogicalformula(seeEquation(19.1)onpage769),thenadecisiontreethatisconsistentwith\nall the examples will satisfy Equation (19.3). If weplace no restrictions on the logical form\nofthehypothesis, ofcourse,then Hypothesis = Classifications alsosatisfiestheconstraint.\nOckham\u2019s razor tells us to prefer small, consistent hypotheses, so we try to do better than\nsimplymemorizingtheexamples.\nThissimpleknowledge-freepictureofinductivelearningpersisteduntiltheearly1980s.\nThemodernapproachistodesignagentsthatalreadyknowsomethingandaretryingtolearn\nsomemore. Thismaynotsoundlikeaterrificallydeepinsight,butitmakesquiteadifference\nto the way we design agents. It might also have some relevance to our theories about how\nscienceitselfworks. Thegeneralideaisshownschematically inFigure19.6.\nAnautonomous learning agent thatusesbackground knowledge mustsomehow obtain\nthe background knowledge in the first place, in order for it to be used in the new learning\nepisodes. This method must itself be a learning process. The agent\u2019s life history will there-\nfore be characterized by cumulative, or incremental, development. Presumably, the agent\ncould start out with nothing, performing inductions in vacuo like a good little pure induc-\ntion program. But once it has eaten from the Tree of Knowledge, it can no longer pursue\nsuch naive speculations and should use its background knowledge to learn more and more\neffectively. Thequestion isthenhowtoactuallydothis. 778 Chapter 19. KnowledgeinLearning\n19.2.1 Somesimpleexamples\nLetusconsidersomecommonsenseexamplesoflearningwithbackgroundknowledge. Many\napparently rational cases of inferential behavior in the face of observations clearly do not\nfollowthesimpleprinciples ofpureinduction.\n\u2022 Sometimes one leaps to general conclusions after only one observation. Gary Larson\nonce drew a cartoon in which a bespectacled caveman, Zog, is roasting his lizard on\nthe end of a pointed stick. He is watched by an amazed crowd of his less intellectual\ncontemporaries,whohavebeenusingtheirbarehandstoholdtheirvictualsoverthefire.\nThisenlightening experience isenough toconvince thewatchers ofageneral principle\nofpainless cooking.\n\u2022 OrconsiderthecaseofthetravelertoBrazilmeetingherfirstBrazilian. Onhearinghim\nspeak Portuguese, she immediately concludes that Brazilians speak Portuguese, yeton\ndiscovering that his name is Fernando, she does not conclude that all Brazilians are\ncalled Fernando. Similar examples appear in science. For example, when a freshman\nphysics student measures the density and conductance of a sample of copper at a par-\nticular temperature, she is quite confident in generalizing those values to all pieces of\ncopper. Yetwhenshemeasuresitsmass,shedoesnotevenconsiderthehypothesisthat\nallpieces ofcopperhave that mass. Ontheotherhand, itwouldbequite reasonable to\nmakesuchageneralization overallpennies.\n\u2022 Finally, consider the case of a pharmacologically ignorant but diagnostically sophisti-\ncated medical student observing a consulting session between a patient and an expert\ninternist. After a series of questions and answers, the expert tells the patient to take a\ncourse of a particular antibiotic. The medical student infers the general rule that that\nparticularantibiotic iseffectiveforaparticulartypeof infection.\nThese are all cases in which the use of background knowledge allows much faster learning\nthanonemightexpectfromapureinduction program.\n19.2.2 Somegeneral schemes\nIn each of the preceding examples, one can appeal to prior knowledge to try to justify the\ngeneralizations chosen. Wewillnowlookatwhatkindsofentailment constraints areoperat-\ningineachcase. Theconstraints willinvolve the Background knowledge, inaddition tothe\nHypothesis andtheobserved Descriptions andClassifications.\nIn the case of lizard toasting, the cavemen generalize by explaining the success of the\npointed stick: it supports the lizard while keeping the hand away from the fire. From this\nexplanation,theycaninferageneralrule: thatanylong,rigid,sharpobjectcanbeusedtotoast\nsmall, soft-bodied edibles. Thiskind ofgeneralization process hasbeencalled explanation-\nEXPLANATION-\nbased learning, or EBL.Notice that the general rule follows logically from the background\nBASED\nLEARNING\nknowledgepossessedbythecavemen. Hence,theentailmentconstraintssatisfiedbyEBLare\nthefollowing:\nHypothesis \u2227Descriptions |= Classifications\nBackground |= Hypothesis . Section19.2. KnowledgeinLearning 779\nBecause EBL uses Equation (19.3), it was initially thought to be a way to learn from ex-\namples. But because it requires that the background knowledge be sufficient to explain the\nHypothesis, which in turn explains the observations, the agent does not actually learn any-\nthing factually new from the example. Theagent could have derived the example from what\nit already knew, although that might have required an unreasonable amount of computation.\nEBL is now viewed as a method for converting first-principles theories into useful, special-\npurpose knowledge. Wedescribealgorithms forEBLinSection19.3.\nThe situation of our traveler in Brazil is quite different, for she cannot necessarily ex-\nplain why Fernando speaks the way he does, unless she knows her papal bulls. Moreover,\nthe same generalization would be forthcoming from a traveler entirely ignorant of colonial\nhistory. The relevant prior knowledge in this case is that, within any given country, most\npeople tend to speak the same language; on the other hand, Fernando is not assumed to be\nthenameofallBrazilians because thiskindofregularity doesnotholdfornames. Similarly,\nthefreshman physics student alsowould behardputtoexplain theparticular values that she\ndiscoversfortheconductance anddensityofcopper. Shedoesknow,however,thatthemate-\nrial of which an object is composed and its temperature together determine its conductance.\nIneachcase,thepriorknowledge Background concernstherelevanceofasetoffeaturesto\nRELEVANCE\nthegoal predicate. Thisknowledge, together withtheobservations, allowstheagent toinfer\nanew,generalrulethatexplains theobservations:\nHypothesis \u2227Descriptions |= Classifications ,\n(19.4)\nBackground \u2227Descriptions \u2227Classifications |= Hypothesis .\nRELEVANCE-BASED Wecallthiskindofgeneralization relevance-based learning,orRBL(although thenameis\nLEARNING\nnot standard). Notice that whereas RBLdoes make use ofthe content ofthe observations, it\ndoesnotproducehypothesesthatgobeyondthelogicalcontentofthebackgroundknowledge\nand the observations. It is a deductive form of learning and cannot by itself account for the\ncreation ofnewknowledgestarting fromscratch.\nIn the case of the medical student watching the expert, we assume that the student\u2019s\nprior knowledge is sufficient to infer the patient\u2019s disease D from the symptoms. This is\nnot, however, enough to explain the fact that the doctor prescribes aparticular medicine M.\nThe student needs to propose another rule, namely, that M generally is effective against D.\nGiventhisruleandthestudent\u2019spriorknowledge,thestudentcannowexplainwhytheexpert\nprescribes M in this particular case. We can generalize this example to come up with the\nentailment constraint\nBackground \u2227Hypothesis \u2227Descriptions |= Classifications . (19.5)\nThatis,thebackground knowledge andthenewhypothesis combinetoexplain theexamples.\nAswithpureinductivelearning,thelearningalgorithmshouldproposehypothesesthatareas\nsimple as possible, consistent with this constraint. Algorithms that satisfy constraint (19.5)\nKNOWLEDGE-BASED\narecalled knowledge-basedinductivelearning,orKBIL,algorithms.\nINDUCTIVE\nLEARNING\nKBIL algorithms, which are described in detail in Section 19.5, have been studied\nINDUCTIVELOGIC mainly in the field of inductive logic programming, or ILP. In ILP systems, prior knowl-\nPROGRAMMING\nedgeplaystwokeyrolesinreducing thecomplexity oflearning: 780 Chapter 19. KnowledgeinLearning\n1. Becauseanyhypothesis generated mustbeconsistent withthepriorknowledge aswell\nas with the new observations, the effective hypothesis space size is reduced to include\nonlythosetheoriesthatareconsistent withwhatisalready known.\n2. For any given set of observations, the size of the hypothesis required to construct an\nexplanation for the observations can be much reduced, because the prior knowledge\nwill be available to help out the new rules in explaining the observations. The smaller\nthehypothesis, theeasieritistofind.\nIn addition to allowing the use of prior knowledge in induction, ILP systems can formulate\nhypotheses in general first-order logic, rather than in the restricted attribute-based language\nofChapter18. Thismeans thatthey canlearn inenvironments that cannot be understood by\nsimplersystems.\n19.3 EXPLANATION-BASED LEARNING\nExplanation-based learning is a method for extracting general rules from individual obser-\nvations. As an example, consider the problem of differentiating and simplifying algebraic\nexpressions (Exercise 9.17). If we differentiate an expression such as X2 with respect to\nX, we obtain 2X. (We use a capital letter for the arithmetic unknown X, to distinguish it\nfrom the logical variable x.) In a logical reasoning system, the goal might be expressed as\nASK(Derivative(X2,X)=d, KB),withsolutiond = 2X.\nAnyonewhoknowsdifferentialcalculuscanseethissolution\u201cbyinspection\u201dasaresult\nofpracticeinsolvingsuchproblems. Astudentencounteringsuchproblemsforthefirsttime,\nor a program with no experience, will have a much more difficult job. Application of the\nstandard rules of differentiation eventually yields the expression 1 \u00d7 (2 \u00d7 (X(2\u22121))), and\neventually this simplifies to 2X. In the authors\u2019 logic programming implementation, this\ntakes 136 proof steps, of which 99 are on dead-end branches in the proof. After such an\nexperience, we would like the program to solve the same problem much more quickly the\nnexttimeitarises.\nThe technique of memoization has long been used in computer science to speed up\nMEMOIZATION\nprograms by saving the results of computation. The basic idea of memo functions is to\naccumulate a database of input\u2013output pairs; when the function is called, it first checks the\ndatabase to see whether it can avoid solving the problem from scratch. Explanation-based\nlearning takes this a good deal further, by creating general rules that cover an entire class\nof cases. In the case of differentiation, memoization would remember that the derivative of\nX2 withrespect toX is2X,butwouldleavetheagenttocalculate thederivativeofZ2 with\nrespect to Z from scratch. We would like to be able to extract the general rule that for any\narithmetic unknown u, the derivative of u2 with respect to u is 2u. (An even more general\nrule for un can also be produced, but the current example suffices to make the point.) In\nlogicalterms,thisisexpressed bytherule\nArithmeticUnknown(u) \u21d2 Derivative(u2,u)=2u. Section19.3. Explanation-Based Learning 781\nIfthe knowledge base contains such arule, then anynew case that isaninstance ofthis rule\ncanbesolvedimmediately.\nThisis,ofcourse,merelyatrivialexampleofaverygeneral phenomenon. Oncesome-\nthing is understood, it can be generalized and reused in other circumstances. It becomes an\n\u201cobvious\u201d stepandcanthenbeusedasabuilding blockinsolving problems still morecom-\nplex. Alfred North Whitehead (1911), co-author with Bertrand Russell of Principia Mathe-\nmatica, wrote \u201cCivilization advances by extending the number of important operations that\nwecandowithoutthinking aboutthem,\u201dperhapshimselfapplyingEBLtohisunderstanding\nof events such as Zog\u2019s discovery. If you have understood the basic idea of the differenti-\nation example, then your brain is already busily trying to extract the general principles of\nexplanation-based learning fromit. Noticethatyouhadn\u2019t already invented EBLbefore you\nsaw the example. Like the cavemen watching Zog, you (and we) needed an example before\nwe could generate the basic principles. This is because explaining why something is a good\nideaismucheasierthancomingupwiththeideainthefirstplace.\n19.3.1 Extracting general rules from examples\nThebasic idea behind EBLisfirst toconstruct anexplanation ofthe observation using prior\nknowledge, and then to establish adefinition of the class of cases forwhich thesame expla-\nnation structure can be used. This definition provides the basis fora rule covering all of the\ncasesintheclass. The\u201cexplanation\u201d canbealogicalproof, butmoregenerally itcanbeany\nreasoning orproblem-solving process whose steps arewell defined. The keyis tobe able to\nidentify thenecessary conditions forthosesamestepstoapplytoanothercase.\nWe will use for our reasoning system the simple backward-chaining theorem prover\ndescribed inChapter9. Theprooftreefor Derivative(X2,X)=2X istoolargetouseasan\nexample, so we will use a simpler problem to illustrate the generalization method. Suppose\nourproblem istosimplify 1\u00d7(0+X). Theknowledgebaseincludes thefollowingrules:\nRewrite(u,v)\u2227Simplify(v,w) \u21d2 Simplify(u,w).\nPrimitive(u) \u21d2 Simplify(u,u).\nArithmeticUnknown(u) \u21d2 Primitive(u).\nNumber(u) \u21d2 Primitive(u).\nRewrite(1\u00d7u,u).\nRewrite(0+u,u).\n.\n.\n.\nThe proof that the answer is X is shown in the top half of Figure 19.7. The EBL method\nactually constructs twoprooftreessimultaneously. Thesecondprooftreeusesa variabilized\ngoal inwhich the constants from the original goal are replaced by variables. Asthe original\nproof proceeds, the variabilized proof proceeds in step, using exactly the same rule applica-\ntions. This could cause some of the variables to become instantiated. Forexample, in order\ntousetheruleRewrite(1\u00d7u,u),thevariablexinthesubgoalRewrite(x\u00d7(y+z),v)must\n(cid:2)\nbebound to1. Similarly, y mustbebound to0inthesubgoal Rewrite(y+z,v )inorderto\nusetherule Rewrite(0+u,u). Oncewehavethe generalized proof tree, wetake theleaves 782 Chapter 19. KnowledgeinLearning\nSimplify(1 \u00d7 (0+X),w)\nRewrite(1\u00d7(0+X),v) Simplify(0+X,w)\nYes,{v \/ 0+X}\nRewrite(0+X,v') Simplify(X,w)\nYes,{v' \/ X} {w \/ X}\nPrimitive(X)\nArithmeticUnknown(X)\nSimplify(x \u00d7(y+z),w) Yes,{ }\nRewrite(x \u00d7(y+z),v) Simplify(y+z,w)\nYes,{x \/ 1, v \/ y+z}\nRewrite(y+z,v') Simplify(z,w)\nYes,{y \/ 0, v'\/ z} {w \/ z}\nPrimitive(z)\nArithmeticUnknown(z)\nYes,{ }\nFigure19.7 Prooftreesforthesimplificationproblem. Thefirsttreeshowstheprooffor\ntheoriginalprobleminstance,fromwhichwecanderive\nArithmeticUnknown(z) \u21d2 Simplify(1\u00d7(0+z),z).\nThesecondtreeshowstheproofforaprobleminstancewithallconstantsreplacedbyvari-\nables,fromwhichwecanderiveavarietyofotherrules.\n(withthenecessary bindings) andformageneral ruleforthe goalpredicate:\nRewrite(1\u00d7(0+z),0+z)\u2227Rewrite(0+z,z)\u2227ArithmeticUnknown(z)\n\u21d2 Simplify(1\u00d7(0+z),z).\nNoticethatthefirsttwoconditions ontheleft-hand sideare true regardless ofthevalue ofz.\nWecanthereforedropthemfromtherule,yielding\nArithmeticUnknown(z) \u21d2 Simplify(1\u00d7(0+z),z).\nIngeneral, conditions canbedropped fromthefinalruleiftheyimposenoconstraints onthe\nvariables on the right-hand side of the rule, because the resulting rule will still be true and\nwill be more efficient. Notice that we cannot drop the condition ArithmeticUnknown(z),\nbecause not all possible values of z are arithmetic unknowns. Values other than arithmetic\nunknownsmightrequiredifferent formsofsimplification: forexample,if z were2\u00d73,then\nthecorrectsimplification of1\u00d7(0+(2\u00d73))wouldbe6andnot2\u00d73.\nTorecap,thebasicEBLprocessworksasfollows:\n1. Givenanexample,constructaproofthatthegoalpredicateappliestotheexampleusing\ntheavailable background knowledge. Section19.3. Explanation-Based Learning 783\n2. In parallel, construct a generalized proof tree for the variabilized goal using the same\ninferencestepsasintheoriginalproof.\n3. Construct a new rule whose left-hand side consists of the leaves of the proof tree and\nwhose right-hand side is the variabilized goal (after applying the necessary bindings\nfromthegeneralized proof).\n4. Dropanyconditions fromtheleft-hand sidethataretrueregardlessofthevaluesofthe\nvariablesinthegoal.\n19.3.2 Improving efficiency\nThegeneralized prooftreeinFigure19.7actuallyyieldsmorethanonegeneralized rule. For\nexample, if we terminate, or prune, the growth of the right-hand branch in the proof tree\nwhenitreachesthePrimitive step,wegettherule\nPrimitive(z) \u21d2 Simplify(1\u00d7(0+z),z).\nThisruleisasvalidas, but moregeneral than, therule using ArithmeticUnknown,because\nitcoverscaseswhere z isanumber. Wecanextractastillmoregeneralrulebypruningafter\nthestepSimplify(y+z,w),yielding therule\nSimplify(y+z,w) \u21d2 Simplify(1\u00d7(y+z),w).\nIngeneral,arulecanbeextractedfromanypartialsubtreeofthegeneralizedprooftree. Now\nwehaveaproblem: whichoftheserulesdowechoose?\nThe choice of which rule to generate comes down to the question of efficiency. There\narethreefactorsinvolved intheanalysisofefficiencygainsfromEBL:\n1. Adding large numbers of rules can slow down the reasoning process, because the in-\nferencemechanism muststillcheckthoserulesevenincases wheretheydonotyielda\nsolution. Inotherwords,itincreases the branchingfactorinthesearchspace.\n2. Tocompensate for the slowdown in reasoning, the derived rules must offer significant\nincreases inspeedforthecasesthattheydocover. Theseincreases comeaboutmainly\nbecause the derived rules avoid dead ends that would otherwise be taken, but also be-\ncausetheyshortentheproofitself.\n3. Derivedrulesshouldbeasgeneralaspossible, sothattheyapplytothelargestpossible\nsetofcases.\nAcommonapproachtoensuringthatderivedrulesareefficientistoinsistontheoperational-\nityofeachsubgoal intherule. Asubgoal isoperational ifitis\u201ceasy\u201dtosolve. Forexample,\nOPERATIONALITY\nthe subgoal Primitive(z) is easy to solve, requiring at most two steps, whereas the subgoal\nSimplify(y +z,w) could lead to an arbitrary amount of inference, depending on the values\nof y and z. If a test for operationality is carried out at each step in the construction of the\ngeneralizedproof,thenwecanprunetherestofabranchassoonasanoperationalsubgoalis\nfound, keepingjusttheoperational subgoalasaconjunct ofthenewrule.\nUnfortunately, there is usually a tradeoff between operationality and generality. More\nspecific subgoals are generally easier to solve but cover fewer cases. Also, operationality\nis a matter of degree: one or two steps is definitely operational, but what about 10 or 100? 784 Chapter 19. KnowledgeinLearning\nFinally, the cost of solving a given subgoal depends on what other rules are available in the\nknowledge base. It can go up or down as more rules are added. Thus, EBL systems really\nface a very complex optimization problem in trying to maximize the efficiency of a given\ninitialknowledge base. Itissometimespossible toderiveamathematicalmodeloftheeffect\non overall efficiency of adding a given rule and to use this model to select the best rule to\nadd. The analysis can become very complicated, however, especially when recursive rules\nare involved. One promising approach is to address the problem of efficiency empirically,\nsimplybyaddingseveralrulesandseeingwhichonesareusefulandactuallyspeedthingsup.\nEmpirical analysis of efficiency is actually at the heart of EBL. What we have been\ncalling loosely the \u201cefficiency of agiven knowledge base\u201d isactually the average-case com-\nplexity on a distribution of problems. By generalizing from past example problems, EBL\nmakes the knowledge base more efficient for the kind of problems that it is reasonable to\nexpect. This works as long as the distribution of past examples is roughly the same as for\nfuture examples\u2014the same assumption used for PAC-learning in Section 18.5. If the EBL\nsystem is carefully engineered, it is possible to obtain significant speedups. For example, a\nvery large Prolog-based natural language system designed for speech-to-speech translation\nbetween Swedish and English was able to achieve real-time performance only by the appli-\ncationofEBLtotheparsingprocess (SamuelssonandRayner, 1991).\n19.4 LEARNING USING RELEVANCE INFORMATION\nOurtravelerinBrazilseemstobeabletomakeaconfidentgeneralizationconcerningthelan-\nguagespokenbyotherBrazilians. Theinferenceissanctionedbyherbackgroundknowledge,\nnamely, that people in a given country (usually) speak the same language. We can express\nthisinfirst-orderlogicasfollows:2\nNationality(x,n)\u2227Nationality(y,n)\u2227Language(x,l) \u21d2 Language(y,l).(19.6)\n(Literal translation: \u201cIf x and y have the samenationality n and x speaks language l, then y\nalsospeaksit.\u201d) Itisnotdifficulttoshowthat,fromthissentence andtheobservation that\nNationality(Fernando,Brazil)\u2227Language(Fernando,Portuguese),\nthefollowingconclusion isentailed(seeExercise19.1):\nNationality(x,Brazil) \u21d2 Language(x,Portuguese).\nSentencessuchas(19.6)express astrict formofrelevance: givennationality, language\nisfullydetermined. (Putanotherway: languageisafunctionofnationality.) Thesesentences\nFUNCTIONAL arecalled functionaldependenciesordeterminations. Theyoccursocommonly incertain\nDEPENDENCY\nkinds of applications (e.g., defining database designs) that a special syntax is used to write\nDETERMINATION\nthem. Weadoptthenotation ofDavies(1985):\nNationality(x,n) \u2019 Language(x,l).\n2 Weassumeforthesakeofsimplicitythatapersonspeaksonlyonelanguage. Clearly,therulewouldhaveto\nbeamendedforcountriessuchasSwitzerlandandIndia. Section19.4. LearningUsingRelevanceInformation 785\nAs usual, this is simply a syntactic sugaring, but it makes it clear that the determination is\nreally a relationship between the predicates: nationality determines language. The relevant\nproperties determining conductance anddensity canbeexpressed similarly:\nMaterial(x,m)\u2227Temperature(x,t) \u2019 Conductance(x,\u03c1);\nMaterial(x,m)\u2227Temperature(x,t) \u2019 Density(x,d).\nThecorrespondinggeneralizationsfollowlogicallyfromthedeterminationsandobservations.\n19.4.1 Determining the hypothesis space\nAlthough the determinations sanction general conclusions concerning all Brazilians, or all\npieces of copper at a given temperature, they cannot, of course, yield a general predictive\ntheory for all nationalities, or for all temperatures and materials, from a single example.\nTheirmaineffectcanbeseenaslimitingthespaceofhypothesesthatthelearningagentneed\nconsider. In predicting conductance, for example, one need consider only material and tem-\nperature and can ignore mass, ownership, day of the week, the current president, and so on.\nHypotheses can certainly include terms that are in turn determined by material and temper-\nature, such as molecular structure, thermal energy, or free-electron density. Determinations\nspecifyasufficientbasisvocabularyfromwhichtoconstructhypothesesconcerningthetarget\npredicate. This statement can be proven by showing that a given determination is logically\nequivalent toastatement thatthecorrect definition ofthetarget predicate isoneofthesetof\nalldefinitionsexpressible usingthepredicates ontheleft-hand sideofthedetermination.\nIntuitively, it is clear that a reduction in the hypothesis space size should make it eas-\nier to learn the target predicate. Using the basic results of computational learning theory\n(Section 18.5), we can quantify the possible gains. First, recall that for Boolean functions,\nlog(|H|) examples are required to converge to a reasonable hypothesis, where |H| is the\nsize of the hypothesis space. If the learner has n Boolean features with which to construct\nhypotheses, then, in the absence of further restrictions, |H| = O(22n ), so the number of ex-\namples isO(2n). Ifthe determination contains dpredicates intheleft-hand side, the learner\nwillrequireonly O(2d)examples,areduction ofO(2n\u2212d).\n19.4.2 Learning and using relevanceinformation\nAs we stated in the introduction to this chapter, prior knowledge is useful in learning; but\nit too has to be learned. In order to provide a complete story of relevance-based learning,\nwe must therefore provide a learning algorithm for determinations. The learning algorithm\nwenowpresentisbasedonastraightforward attempttofindthesimplestdetermination con-\nsistent with the observations. A determination P \u2019 Q says that if any examples match on\nP, then they must also match on Q. A determination is therefore consistent with a set of\nexamples if every pair that matches on the predicates on the left-hand side also matches on\nthe goal predicate. For example, suppose we have the following examples of conductance\nmeasurements onmaterialsamples: 786 Chapter 19. KnowledgeinLearning\nfunctionMINIMAL-CONSISTENT-DET(E,A)returnsasetofattributes\ninputs:E,asetofexamples\nA,asetofattributes,ofsizen\nfori =0ton do\nforeachsubsetAiofAofsizei do\nifCONSISTENT-DET?(Ai,E)thenreturnAi\nfunctionCONSISTENT-DET?(A,E)returnsatruthvalue\ninputs:A,asetofattributes\nE,asetofexamples\nlocalvariables: H,ahashtable\nforeachexamplee inE do\nifsomeexampleinH hasthesamevaluesase fortheattributesA\nbutadifferentclassificationthenreturnfalse\nstoretheclassofe inH,indexedbythevaluesforattributesAoftheexamplee\nreturntrue\nFigure19.8 Analgorithmforfindingaminimalconsistentdetermination.\nSample Mass Temperature Material Size Conductance\nS1 12 26 Copper 3 0.59\nS1 12 100 Copper 3 0.57\nS2 24 26 Copper 6 0.59\nS3 12 26 Lead 2 0.05\nS3 12 100 Lead 2 0.04\nS4 24 26 Lead 4 0.05\nThe minimal consistent determination is Material \u2227Temperature \u2019 Conductance. There\nis a nonminimal but consistent determination, namely, Mass \u2227 Size \u2227 Temperature \u2019\nConductance. Thisisconsistentwiththeexamplesbecausemassandsizedeterminedensity\nand, inourdata set, wedonot have twodifferent materials withthe samedensity. Asusual,\nwewouldneedalargersamplesetinordertoeliminateanearlycorrecthypothesis.\nThere are several possible algorithms for finding minimal consistent determinations.\nThemostobviousapproachistoconductasearchthroughthespaceofdeterminations,check-\ningalldeterminations withonepredicate, twopredicates, andsoon, until aconsistent deter-\nmination isfound. Wewillassumeasimple attribute-based representation, like thatusedfor\ndecision tree learning in Chapter 18. A determination d will be represented by the set of\nattributesontheleft-handside,becausethetargetpredicateisassumedtobefixed. Thebasic\nalgorithm isoutlinedinFigure19.8.\nThe time complexity of this algorithm depends on the size of the smallest consistent\ndetermination. Suppose thisdetermination has pattributes outofthe ntotalattr(cid:20)ib(cid:21)utes. Then\nthealgorithmwillnotfindituntilsearchingthesubsetsofAofsizep. Thereare n = O(np)\np Section19.4. LearningUsingRelevanceInformation 787\n1\n0.9\nRBDTL\nDTL\n0.8\n0.7\n0.6\n0.5\n0.4\n0 20 40 60 80 100 120 140\nTraining set size\nFigure 19.9 A performance comparison between DECISION-TREE-LEARNING and\nRBDTL on randomly generated data for a target function that depends on only 5 of 16\nattributes.\nsuch subsets; hence the algorithm is exponential inthe size ofthe minimal determination. It\nturns out that the problem is NP-complete, so we cannot expect to do better in the general\ncase. Inmostdomains, however, there willbesufficient localstructure (seeChapter14fora\ndefinitionoflocallystructured domains)that pwillbesmall.\nGivenanalgorithm forlearningdeterminations, alearning agenthasawaytoconstruct\naminimalhypothesiswithinwhichtolearnthetargetpredicate. Forexample,wecancombine\nMINIMAL-CONSISTENT-DET withthe DECISION-TREE-LEARNING algorithm. Thisyields\na relevance-based decision-tree learning algorithm RBDTL that first identifies a minimal\nset of relevant attributes and then passes this set to the decision tree algorithm for learning.\nUnlike DECISION-TREE-LEARNING, RBDTL simultaneously learns andusesrelevance in-\nformationinordertominimizeitshypothesisspace. WeexpectthatRBDTLwilllearnfaster\nthanDECISION-TREE-LEARNING,andthisisinfactthecase. Figure19.9showsthelearning\nperformance for the two algorithms on randomly generated data for a function that depends\nononly 5of16 attributes. Obviously, incases whereall the available attributes are relevant,\nRBDTL willshownoadvantage.\nThissectionhasonlyscratched thesurfaceofthefieldof declarative bias,whichaims\nDECLARATIVEBIAS\ntounderstand how priorknowledge can beused to identify the appropriate hypothesis space\nwithinwhichtosearchforthecorrecttargetdefinition. Therearemanyunansweredquestions:\n\u2022 Howcanthealgorithms beextendedtohandlenoise?\n\u2022 Canwehandle continuous-valued variables?\n\u2022 Howcanotherkindsofpriorknowledgebeused,besidesdeterminations?\n\u2022 How can the algorithms be generalized to cover any first-order theory, rather than just\nanattribute-based representation?\nSomeofthesequestions areaddressed inthenextsection.\ntes\ntset\nno\ntcerroc\nnoitroporP 788 Chapter 19. KnowledgeinLearning\n19.5 INDUCTIVE LOGIC PROGRAMMING\nInductivelogicprogramming(ILP)combinesinductivemethodswiththepoweroffirst-order\nrepresentations, concentrating in particular on the representation of hypotheses as logic pro-\ngrams.3 It has gained popularity for three reasons. First, ILP offers a rigorous approach to\nthe general knowledge-based inductive learning problem. Second, it offers complete algo-\nrithms for inducing general, first-order theories from examples, which can therefore learn\nsuccessfully in domains where attribute-based algorithms are hard to apply. An example is\nin learning how protein structures fold (Figure 19.10). The three-dimensional configuration\nof a protein molecule cannot be represented reasonably by a set of attributes, because the\nconfiguration inherently refers to relationships between objects, not to attributes of a single\nobject. First-order logic is an appropriate language for describing the relationships. Third,\ninductive logic programming produces hypotheses that are (relatively) easy for humans to\nread. For example, the English translation in Figure 19.10 can be scrutinized and criticized\nbyworkingbiologists. Thismeansthatinductive logicprogrammingsystemscanparticipate\ninthescientificcycleofexperimentation, hypothesisgeneration,debate,andrefutation. Such\nparticipation wouldnotbepossible forsystemsthatgenerate \u201cblack-box\u201d classifiers, suchas\nneuralnetworks.\n19.5.1 Anexample\nRecallfromEquation(19.5)thatthegeneralknowledge-basedinductionproblemisto\u201csolve\u201d\ntheentailmentconstraint\nBackground \u2227Hypothesis \u2227Descriptions |= Classifications\nfortheunknown Hypothesis, given the Background knowledge and examples described by\nDescriptions and Classifications. To illustrate this, we will use the problem of learning\nfamily relationships from examples. The descriptions will consist of an extended family\ntree, described in terms of Mother, Father, and Married relations and Male and Female\nproperties. As an example, we will use the family tree from Exercise 8.14, shown here in\nFigure19.11. Thecorresponding descriptions areasfollows:\nFather(Philip,Charles) Father(Philip,Anne) ...\nMother(Mum,Margaret) Mother(Mum,Elizabeth) ...\nMarried(Diana,Charles) Married(Elizabeth,Philip) ...\nMale(Philip) Male(Charles) ...\nFemale(Beatrice) Female(Margaret) ...\nThesentencesinClassifications dependonthetargetconceptbeinglearned. Wemightwant\nto learn Grandparent, BrotherInLaw, or Ancestor, for example. For Grandparent, the\n3 ItmightbeappropriateatthispointforthereadertorefertoChapter7forsomeoftheunderlyingconcepts,\nincludingHornclauses,conjunctivenormalform,unification,andresolution. Section19.5. Inductive LogicProgramming 789\ncompletesetofClassifications contains 20\u00d720=400conjuncts oftheform\nGrandparent(Mum,Charles) Grandparent(Elizabeth,Beatrice) ...\n\u00acGrandparent(Mum,Harry) \u00acGrandparent(Spencer,Peter) ...\nWecouldofcourselearnfromasubsetofthiscompleteset.\nThe object of an inductive learning program is to come up with a set of sentences for\ntheHypothesis suchthattheentailmentconstraintissatisfied. Suppose,forthemoment,that\nthe agent hasno background knowledge: Background isempty. Thenonepossible solution\nH:5[111-113]\nH:1[19-37] H:6[79-88]\nH:3[71-84]\nH:4[61-64]\nH:1[8-17]\nH:5[66-70]\nH:2[26-33]\nE:2[96-98]\nE:1[57-59]\nH:4[93-108]\nH:2[41-64]\nH:7[99-106] H:3[40-50]\n2mhr - Four-helical up-and-down bundle 1omd - EF-Hand\n(a) (b)\nFigure 19.10 (a) and (b) show positive and negative examples, respectively, of the\n\u201cfour-helical up-and-down bundle\u201d concept in the domain of protein folding. Each\nexample structure is coded into a logical expression of about 100 conjuncts such as\nTotalLength(D2mhr,118)\u2227NumberHelices(D2mhr,6)\u2227....Fromthesedescriptionsand\nfrom classifications such as Fold(FOUR-HELICAL-UP-AND-DOWN-BUNDLE,D2mhr),\ntheILPsystemPROGOL(Muggleton,1995)learnedthefollowingrule:\nFold(FOUR-HELICAL-UP-AND-DOWN-BUNDLE,p)\u21d0\nHelix(p,h 1)\u2227Length(h 1,HIGH)\u2227Position(p,h 1,n)\n\u2227(1\u2264n\u22643)\u2227Adjacent(p,h ,h )\u2227Helix(p,h ).\n1 2 2\nThiskindofrulecouldnotbelearned,orevenrepresented,byanattribute-basedmechanism\nsuchaswesaw inpreviouschapters. TherulecanbetranslatedintoEnglishas\u201c Proteinp\nhasfoldclass\u201cFour-helicalup-and-down-bundle\u201difitcontainsalonghelixh atasecondary\n1\nstructurepositionbetween1and3andh isnexttoasecondhelix.\u201d\n1 790 Chapter 19. KnowledgeinLearning\nforHypothesis isthefollowing:\nGrandparent(x,y) \u21d4 [\u2203z Mother(x,z)\u2227Mother(z,y)]\n\u2228 [\u2203z Mother(x,z)\u2227Father(z,y)]\n\u2228 [\u2203z Father(x,z)\u2227Mother(z,y)]\n\u2228 [\u2203z Father(x,z)\u2227Father(z,y)].\nNoticethatanattribute-based learningalgorithm,suchas DECISION-TREE-LEARNING,will\ngetnowhere insolving thisproblem. Inordertoexpress Grandparent asanattribute (i.e., a\nunarypredicate), wewouldneedtomake pairsofpeopleintoobjects:\nGrandparent((cid:16)Mum,Charles(cid:17))...\nThenwegetstuckintryingtorepresenttheexampledescriptions. Theonlypossibleattributes\narehorriblethingssuchas\nFirstElementIsMotherOfElizabeth((cid:16)Mum,Charles(cid:17)).\nThe definition of Grandparent in terms of these attributes simply becomes a large disjunc-\ntionofspecificcasesthatdoesnotgeneralizetonewexamplesatall. Attribute-basedlearning\nalgorithmsareincapable oflearningrelational predicates. Thus,oneoftheprincipal advan-\ntages of ILP algorithms is their applicability to a much wider range of problems, including\nrelational problems.\nThereader will certainly have noticed that a little bit ofbackground knowledge would\nhelp in the representation of the Grandparent definition. For example, if Background in-\ncludedthesentence\nParent(x,y) \u21d4 [Mother(x,y)\u2228Father(x,y)],\nthenthedefinitionofGrandparent wouldbereducedto\nGrandparent(x,y) \u21d4 [\u2203z Parent(x,z)\u2227Parent(z,y)].\nThis shows how background knowledge can dramatically reduce the size of hypotheses re-\nquiredtoexplaintheobservations.\nIt is also possible for ILP algorithms to create new predicates in order to facilitate the\nexpression of explanatory hypotheses. Given the example data shown earlier, it is entirely\nreasonable for the ILP program to propose an additional predicate, which we would call\nGeorge Mum\nSpencer Kydd Elizabeth Philip Margaret\nDiana Charles Anne Mark Andrew Sarah Edward Sophie\nWilliam Harry Peter Zara Beatrice Eugenie Louise James\nFigure19.11 Atypicalfamilytree. Section19.5. Inductive LogicProgramming 791\n\u201cParent,\u201d in order to simplify the definitions of the target predicates. Algorithms that can\nCONSTRUCTIVE generate new predicates are called constructive inductionalgorithms. Clearly, constructive\nINDUCTION\ninduction is a necessary part of the picture of cumulative learning. It has been one of the\nhardestproblemsinmachinelearning,butsomeILPtechniquesprovideeffectivemechanisms\nforachieving it.\nIn the rest of this chapter, we will study the two principal approaches to ILP. The first\nuses a generalization of decision tree methods, and the second uses techniques based on\ninverting aresolution proof.\n19.5.2 Top-down inductive learningmethods\nThefirstapproachtoILPworksbystartingwithaverygeneral ruleandgraduallyspecializing\nit so that it fits the data. This is essentially what happens in decision-tree learning, where a\ndecision tree is gradually grown until it is consistent with the observations. To do ILP we\nuse first-order literals instead ofattributes, and the hypothesis isaset ofclauses instead of a\ndecision tree. Thissection describes FOIL (Quinlan, 1990),oneofthefirstILPprograms.\nSuppose we are trying to learn a definition of the Grandfather(x,y) predicate, using\nthe same family data as before. As with decision-tree learning, we can divide the examples\nintopositiveandnegativeexamples. Positiveexamplesare\n(cid:16)George,Anne(cid:17), (cid:16)Philip,Peter(cid:17), (cid:16)Spencer,Harry(cid:17), ...\nandnegativeexamplesare\n(cid:16)George,Elizabeth(cid:17), (cid:16)Harry,Zara(cid:17), (cid:16)Charles,Philip(cid:17), ...\nNoticethateachexample isapair ofobjects, because Grandfather isabinary predicate. In\nall,thereare12positiveexamplesinthefamilytreeand388 negativeexamples(alltheother\npairsofpeople).\nFOILconstructsasetofclauses,eachwithGrandfather(x,y)asthehead. Theclauses\nmust classify the 12 positive examples as instances of the Grandfather(x,y) relationship,\nwhilerulingoutthe388negativeexamples. TheclausesareHornclauses,withtheextension\nthatnegatedliterals areallowedinthebody ofaclause andareinterpreted usingnegation as\nfailure, asinProlog. Theinitialclausehasanemptybody:\n\u21d2 Grandfather(x,y).\nThisclause classifies every example as positive, so itneeds to bespecialized. Wedo this by\naddingliteralsoneatatimetotheleft-hand side. Herearethreepotential additions:\nFather(x,y) \u21d2 Grandfather(x,y).\nParent(x,z) \u21d2 Grandfather(x,y).\nFather(x,z) \u21d2 Grandfather(x,y).\n(Noticethatweareassumingthataclausedefining Parent isalreadypartofthebackground\nknowledge.) Thefirstofthesethreeclausesincorrectly classifiesallofthe12positiveexam-\nples as negative and can thus beignored. Thesecond and third agree withallof the positive\nexamples, butthesecondisincorrect onalargerfraction of thenegativeexamples\u2014twiceas\nmany,because itallowsmothersaswellasfathers. Hence,wepreferthethirdclause. 792 Chapter 19. KnowledgeinLearning\nNow we need to specialize this clause further, to rule out the cases in which x is the\nfatherofsomez,butz isnotaparentofy. Addingthesingleliteral Parent(z,y)gives\nFather(x,z)\u2227Parent(z,y) \u21d2 Grandfather(x,y),\nwhich correctly classifies all the examples. FOIL will find and choose this literal, thereby\nsolving the learning task. In general, the solution is a set of Horn clauses, each of which\nimplies the target predicate. For example, if we didn\u2019t have the Parent predicate in our\nvocabulary, thenthesolution mightbe\nFather(x,z)\u2227Father(z,y) \u21d2 Grandfather(x,y)\nFather(x,z)\u2227Mother(z,y) \u21d2 Grandfather(x,y).\nNotethateachoftheseclausescoverssomeofthepositiveexamples,thattogethertheycover\nall the positive examples, and that NEW-CLAUSE is designed in such a way that no clause\nwillincorrectly coveranegativeexample. Ingeneral FOIL willhavetosearchthrough many\nunsuccessful clauses beforefindingacorrectsolution.\nThisexample isavery simpleillustration ofhow FOIL operates. Asketch of thecom-\nplete algorithm is shown in Figure 19.12. Essentially, the algorithm repeatedly constructs a\nclause,literalbyliteral,untilitagreeswithsomesubset ofthepositiveexamplesandnoneof\nthe negative examples. Then the positive examples covered by the clause are removed from\nthe training set, and the process continues until no positive examples remain. The two main\nsubroutinestobeexplainedareNEW-LITERALS,whichconstructsallpossiblenewliteralsto\naddtotheclause, and CHOOSE-LITERAL,whichselectsaliteraltoadd.\nNEW-LITERALS takes a clause and constructs all possible \u201cuseful\u201d literals that could\nbeaddedtotheclause. Letususeasanexampletheclause\nFather(x,z) \u21d2 Grandfather(x,y).\nTherearethreekindsofliteralsthatcanbeadded:\n1. Literalsusingpredicates: theliteralcanbenegatedorunnegated,anyexistingpredicate\n(includingthegoalpredicate)canbeused,andtheargumentsmustallbevariables. Any\nvariablecanbeusedforanyargumentofthepredicate,withonerestriction: eachliteral\nmustincludeatleastonevariablefromanearlierliteralorfromtheheadoftheclause.\nLiterals such as Mother(z,u), Married(z,z), \u00acMale(y), and Grandfather(v,x) are\nallowed, whereas Married(u,v) is not. Notice that the use of the predicate from the\nheadoftheclauseallows FOIL tolearnrecursive definitions.\n2. Equality and inequality literals: these relate variables already appearing in the clause.\nForexample, we might add z (cid:7)= x. These literals can also include user-specified con-\nstants. Forlearning arithmetic wemightuse0and1,andforlearning listfunctions we\nmightusetheemptylist[].\n3. Arithmetic comparisons: when dealing with functions of continuous variables, literals\nsuch as x > y and y \u2264 z can be added. As in decision-tree learning, a constant\nthreshold valuecanbechosen tomaximizethediscriminatory powerofthetest.\nTheresultingbranchingfactorinthissearchspaceisverylarge(seeExercise19.6),butFOIL\ncan also use type information to reduce it. Forexample, if the domain included numbers as Section19.5. Inductive LogicProgramming 793\nfunctionFOIL(examples,target)returnsasetofHornclauses\ninputs:examples,setofexamples\ntarget,aliteralforthegoalpredicate\nlocalvariables: clauses,setofclauses,initiallyempty\nwhileexamples containspositiveexamplesdo\nclause\u2190NEW-CLAUSE(examples,target)\nremovepositiveexamplescoveredbyclause fromexamples\naddclause toclauses\nreturnclauses\nfunctionNEW-CLAUSE(examples,target)returnsaHornclause\nlocalvariables: clause,aclausewithtarget asheadandanemptybody\nl,aliteraltobeaddedtotheclause\nextended examples,asetofexampleswithvaluesfornewvariables\nextended examples\u2190examples\nwhileextended examples containsnegativeexamplesdo\nl\u2190CHOOSE-LITERAL(NEW-LITERALS(clause),extended examples)\nappendl tothebodyofclause\nextended examples\u2190setofexamplescreatedbyapplyingEXTEND-EXAMPLE\ntoeachexampleinextended examples\nreturnclause\nfunctionEXTEND-EXAMPLE(example,literal)returnsasetofexamples\nifexample satisfiesliteral\nthenreturnthesetofexamplescreatedbyextendingexample with\neachpossibleconstantvalueforeachnewvariableinliteral\nelsereturntheemptyset\nFigure 19.12 Sketch of the FOIL algorithm forlearning sets of first-orderHorn clauses\nfromexamples. NEW-LITERALSandCHOOSE-LITERALareexplainedinthetext.\nwellaspeople,typerestrictionswouldpreventNEW-LITERALS fromgeneratingliteralssuch\nasParent(x,n),wherexisapersonandnisanumber.\nCHOOSE-LITERALusesaheuristicsomewhatsimilartoinformationgain(seepage704)\nto decide which literal to add. The exact details are not important here, and a number of\ndifferent variations have been tried. One interesting additional feature of FOIL is the use of\nOckham\u2019srazortoeliminatesomehypotheses. Ifaclausebecomeslonger(accordingtosome\nmetric) than the total length of the positive examples that the clause explains, that clause is\nnotconsideredasapotentialhypothesis. Thistechniqueprovidesawaytoavoidovercomplex\nclausesthatfitnoiseinthedata.\nFOIL andits relatives havebeen used tolearn awidevariety ofdefinitions. Oneofthe\nmostimpressivedemonstrations (QuinlanandCameron-Jones,1993)involvedsolvingalong\nsequence of exercises on list-processing functions from Bratko\u2019s (1986) Prolog textbook. In 794 Chapter 19. KnowledgeinLearning\neach case, the program wasable tolearn acorrect definition ofthefunction from asmallset\nofexamples, usingthepreviously learnedfunctions asbackground knowledge.\n19.5.3 Inductive learningwithinverse deduction\nThe second major approach to ILP involves inverting the normal deductive proof process.\nINVERSE Inverse resolution is based on the observation that if the example Classifications follow\nRESOLUTION\nfromBackground \u2227Hypothesis \u2227Descriptions,thenonemustbeabletoprovethisfactby\nresolution (becauseresolutioniscomplete). Ifwecan\u201crun theproofbackward,\u201dthenwecan\nfind aHypothesis such that the proof goes through. The key, then, is to find away to invert\ntheresolution process.\nWewillshowabackwardproofprocessforinverseresolution thatconsistsofindividual\nbackward steps. Recall that an ordinary resolution step takes two clauses C and C and\n1 2\nresolves them to produce the resolvent C. An inverse resolution step takes a resolvent C\nand produces two clauses C and C , such that C is the result of resolving C and C .\n1 2 1 2\nAlternatively, it may take a resolvent C and clause C and produce a clause C such that C\n1 2\nistheresultofresolving C andC .\n1 2\nThe early steps in an inverse resolution process are shown in Figure 19.13, where we\nfocus on the positive example Grandparent(George,Anne). Theprocess begins at the end\nof the proof (shown at the bottom of the figure). We take the resolvent C to be empty\nclause(i.e. acontradiction) andC tobe\u00acGrandparent(George,Anne),whichisthenega-\n2\ntion of the goal example. The first inverse step takes C and C and generates the clause\n2\nGrandparent(George,Anne) for C . The next step takes this clause as C and the clause\n1\nParent(Elizabeth,Anne)asC ,andgenerates theclause\n2\n\u00acParent(Elizabeth,y)\u2228Grandparent(George,y)\nasC . Thefinalstep treats this clause as the resolvent. With Parent(George,Elizabeth)as\n1\nC ,onepossible clauseC isthehypothesis\n2 1\nParent(x,z)\u2227Parent(z,y) \u21d2 Grandparent(x,y).\nNowwehavearesolutionproofthatthehypothesis,descriptions,andbackgroundknowledge\nentailtheclassification Grandparent(George,Anne).\nClearly, inverse resolution involves a search. Each inverse resolution step is nonde-\nterministic, because for any C, there can be many or even an infinite number of clauses\nC and C that resolve to C. For example, instead of choosing \u00acParent(Elizabeth,y) \u2228\n1 2\nGrandparent(George,y) for C in the last step of Figure 19.13, the inverse resolution step\n1\nmighthavechosenanyofthefollowingsentences:\n\u00acParent(Elizabeth,Anne)\u2228Grandparent(George,Anne).\n\u00acParent(z,Anne)\u2228Grandparent(George,Anne).\n\u00acParent(z,y)\u2228Grandparent(George,y).\n.\n.\n.\n(See Exercises 19.4 and 19.5.) Furthermore, the clauses that participate in each step can be\nchosenfromtheBackground knowledge,fromtheexampleDescriptions,fromthenegated Section19.5. Inductive LogicProgramming 795\nClassifications,orfromhypothesizedclausesthathavealreadybeengeneratedintheinverse\nresolution tree. Thelarge number ofpossibilities means alarge branching factor (and there-\nforeaninefficientsearch)withoutadditionalcontrols. Anumberofapproaches totamingthe\nsearchhavebeentriedinimplemented ILPsystems:\n1. Redundant choices can be eliminated\u2014for example, by generating only the most spe-\ncifichypothesespossibleandbyrequiringthatallthehypothesizedclausesbeconsistent\nwitheachother, andwiththeobservations. Thislastcriterionwouldruleouttheclause\n\u00acParent(z,y)\u2228Grandparent(George,y),listedbefore.\n2. The proof strategy can be restricted. For example, we saw in Chapter 9 that linear\nresolutionisacomplete,restrictedstrategy. Linearresolutionproducesprooftreesthat\nhave a linear branching structure\u2014the whole tree follows one line, with only single\nclausesbranching offthatline(asinFigure19.13).\n3. Therepresentationlanguagecanberestricted,forexamplebyeliminatingfunctionsym-\nbols or by allowing only Horn clauses. For instance, PROGOL operates with Horn\nINVERSE clausesusinginverseentailment. Theideaistochangetheentailmentconstraint\nENTAILMENT\nBackground \u2227Hypothesis \u2227Descriptions |= Classifications\ntothelogically equivalent form\nBackground \u2227Descriptions \u2227\u00acClassifications |= \u00acHypothesis.\nFrom this, one can use a process similar to the normal Prolog Horn-clause deduction,\nwithnegation-as-failure toderive Hypothesis. Becauseitisrestricted toHornclauses,\nthisisanincomplete method, but itcanbemoreefficientthan full resolution. Itisalso\npossibletoapplycompleteinference withinverseentailment (Inoue,2001).\n4. Inferencecanbedonewithmodelcheckingratherthantheoremproving. ThePROGOL\nsystem (Muggleton, 1995) uses a form of model checking to limit the search. That\n\u00acParent(x,z) \u00acParent(z,y) Grandparent(x,y) Parent(George,Elizabeth)\n{x\/George, z\/Elizabeth}\n\u00acParent(Elizabeth,y) Grandparent(George,y) Parent(Elizabeth,Anne)\n{y\/Anne}\nGrandparent(George,Anne) \u00acGrandparent(George,Anne)\nFigure 19.13 Early steps in an inverse resolution process. The shaded clauses are\ngenerated by inverse resolution steps from the clause to the right and the clause below.\nThe unshaded clauses are from the Descriptions and Classifications (including negated\nClassifications).\n>\n>\n> 796 Chapter 19. KnowledgeinLearning\nis, like answer set programming, it generates possible values for logical variables, and\nchecksforconsistency.\n5. Inferencecanbedonewithgroundpropositional clausesratherthaninfirst-orderlogic.\nTheLINUSsystem(LavraucandDuzeroski,1994)worksbytranslatingfirst-orderthe-\nories into propositional logic, solving them with a propositional learning system, and\nthen translating back. Working with propositional formulas can be more efficient on\nsomeproblems, aswesawwithSATPLAN inChapter10.\n19.5.4 Making discoveries withinductive logicprogramming\nAninverse resolution procedure that inverts a complete resolution strategy is, in principle, a\ncomplete algorithm for learning first-order theories. That is, if some unknown Hypothesis\ngenerates a set of examples, then an inverse resolution procedure can generate Hypothesis\nfrom the examples. This observation suggests an interesting possibility: Suppose that the\navailable examples include avariety oftrajectories offalling bodies. Wouldaninverse reso-\nlutionprogrambetheoretically capableofinferringthelawofgravity? Theanswerisclearly\nyes,becausethelawofgravityallowsonetoexplaintheexamples,givensuitablebackground\nmathematics. Similarly,onecanimaginethatelectromagnetism,quantummechanics,andthe\ntheoryofrelativityarealsowithinthescopeofILPprograms. Ofcourse,theyarealsowithin\nthe scope of a monkey with a typewriter; we still need better heuristics and new ways to\nstructure thesearchspace.\nOnethingthatinverseresolution systems willdoforyouisinventnewpredicates. This\nabilityisoftenseenassomewhatmagical,becausecomputersareoftenthoughtofas\u201cmerely\nworking with what they are given.\u201d In fact, new predicates fall directly out of the inverse\nresolution step. Thesimplestcasearises inhypothesizing twonewclauses C andC ,given\n1 2\naclauseC. TheresolutionofC andC eliminatesaliteralthatthetwoclausesshare;hence,\n1 2\nitisquite possible thattheeliminated literalcontained apredicate thatdoes notappearin C.\nThus, whenworking backward, one possibility is togenerate anewpredicate from which to\nreconstruct themissingliteral.\nFigure19.14showsanexampleinwhichthenewpredicateP isgeneratedintheprocess\noflearning adefinition forAncestor. Oncegenerated, P canbeusedinlaterinverseresolu-\ntionsteps. Forexample,alaterstepmighthypothesizethatMother(x,y) \u21d2 P(x,y). Thus,\nthenewpredicate P hasitsmeaningconstrainedbythegenerationofhypotheses thatinvolve\nit. Another example might lead to the constraint Father(x,y) \u21d2 P(x,y). In other words,\nthe predicate P is what we usually think of as the Parent relationship. As we mentioned\nearlier, the invention of new predicates can significantly reduce the size of the definition of\nthegoalpredicate. Hence,byincludingtheabilitytoinventnewpredicates,inverseresolution\nsystemscanoftensolvelearning problemsthatareinfeasible withothertechniques.\nSomeof the deepest revolutions in science come from the invention of new predicates\nand functions\u2014for example, Galileo\u2019s invention of acceleration orJoule\u2019s invention of ther-\nmal energy. Once these terms are available, the discovery of new laws becomes (relatively)\neasy. The difficult part lies in realizing that some new entity, with a specific relationship\nto existing entities, will allow an entire body of observations to be explained with a much Section19.6. Summary 797\n\u00ac Father(x,y) P(x,y) \u00ac P(George,y) Ancestor(George,y)\n{x\/George}\nFather(George,y) Ancestor(George,y)\nFigure19.14 AninverseresolutionstepthatgeneratesanewpredicateP.\nsimplerandmoreeleganttheorythanpreviously existed.\nAsyet,ILPsystemshavenotmadediscoveriesonthelevelofGalileoorJoule,buttheir\ndiscoveries have been deemed publishable in the scientific literature. For example, in the\nJournalofMolecularBiology,Turcotteetal.(2001)describetheautomateddiscoveryofrules\nforprotein folding by the ILP program PROGOL. Many of the rules discovered by PROGOL\ncould havebeenderivedfrom knownprinciples, butmosthadnotbeenpreviously published\nas part of a standard biological database. (See Figure 19.10 for an example.). In related\nwork, Srinivasan et al. (1994) dealt with the problem of discovering molecular-structure-\nbasedrulesforthemutagenicityofnitroaromaticcompounds. Thesecompoundsarefoundin\nautomobileexhaustfumes. For80%ofthecompoundsinastandarddatabase,itispossibleto\nidentify fourimportantfeatures, andlinearregression on thesefeaturesoutperforms ILP.For\ntheremaining20%,thefeaturesalonearenotpredictive, andILPidentifiesrelationships that\nallow it to outperform linear regression, neural nets, and decision trees. Most impressively,\nKingetal.(2009)endowedarobotwiththeabilitytoperformmolecularbiologyexperiments\nand extended ILP techniques to include experiment design, thereby creating an autonomous\nscientistthatactuallydiscoverednewknowledgeaboutthefunctionalgenomicsofyeast. For\nalltheseexamplesitappearsthattheabilitybothtorepresentrelationsandtousebackground\nknowledgecontribute toILP\u2019shighperformance. ThefactthattherulesfoundbyILPcanbe\ninterpreted by humans contributes to the acceptance of these techniques in biology journals\nratherthanjustcomputersciencejournals.\nILPhas made contributions to other sciences besides biology. One of the most impor-\ntant is natural language processing, where ILP has been used to extract complex relational\ninformation fromtext. TheseresultsaresummarizedinChapter23.\n19.6 SUMMARY\nThis chapter has investigated various ways in which prior knowledge can help an agent to\nlearn from new experiences. Because much prior knowledge is expressed in terms of rela-\ntional models rather than attribute-based models, we have also covered systems that allow\nlearning ofrelational models. Theimportantpointsare:\n\u2022 The use of prior knowledge in learning leads to a picture of cumulative learning, in\nwhichlearning agentsimprovetheirlearning abilityastheyacquiremoreknowledge.\n\u2022 Priorknowledge helps learning byeliminating otherwise consistent hypotheses andby\n>\n>\n> 798 Chapter 19. KnowledgeinLearning\n\u201cfillingin\u201dtheexplanationofexamples,therebyallowingforshorterhypotheses. These\ncontributions oftenresultinfasterlearningfromfewerexamples.\n\u2022 Understanding the different logical roles played by prior knowledge, as expressed by\nentailmentconstraints, helpstodefineavarietyoflearningtechniques.\n\u2022 Explanation-basedlearning(EBL)extractsgeneralrulesfromsingleexamplesby ex-\nplainingtheexamplesandgeneralizingtheexplanation. Itprovides adeductivemethod\nforturning first-principles knowledgeintouseful, efficient,special-purpose expertise.\n\u2022 Relevance-based learning(RBL)uses priorknowledge in the form of determinations\nto identify the relevant attributes, thereby generating a reduced hypothesis space and\nspeedinguplearning. RBLalsoallowsdeductivegeneralizationsfromsingleexamples.\n\u2022 Knowledge-based inductive learning (KBIL)finds inductive hypotheses that explain\nsetsofobservations withthehelpofbackground knowledge.\n\u2022 Inductive logic programming (ILP) techniques perform KBIL on knowledge that is\nexpressed in first-order logic. ILP methods can learn relational knowledge that is not\nexpressible inattribute-based systems.\n\u2022 ILPcanbedone withatop-down approach ofrefining averygeneral rule orthrough a\nbottom-upapproach ofinverting thedeductive process.\n\u2022 ILPmethodsnaturallygeneratenewpredicates withwhichconcisenewtheoriescanbe\nexpressedandshowpromiseasgeneral-purpose scientifictheoryformationsystems.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nAlthoughtheuseofpriorknowledgeinlearningwouldseemtobeanaturaltopicforphiloso-\nphersofscience,littleformalworkwasdoneuntilquiterecently. Fact,Fiction,andForecast,\nby the philosopher Nelson Goodman (1954), refuted the earlier supposition that induction\nwas simply a matter of seeing enough examples of some universally quantified proposition\nandthenadoptingitasahypothesis. Consider, forexample, thehypothesis\u201cAllemeraldsare\ngrue,\u201d where grue means \u201cgreen if observed before time t, but blue if observed thereafter.\u201d\nAt any time up to t, we might have observed millions of instances confirming the rule that\nemeraldsaregrue,andnodisconfirminginstances,andyetweareunwillingtoadopttherule.\nThiscanbeexplained onlybyappealtotheroleofrelevantpriorknowledgeintheinduction\nprocess. Goodmanproposesavarietyofdifferentkindsofpriorknowledgethatmightbeuse-\nful,includingaversionofdeterminationscalledoverhypotheses. Unfortunately, Goodman\u2019s\nideaswereneverpursuedinmachinelearning.\nThecurrent-best-hypothesisapproachisanoldideainphilosophy(Mill,1843). Early\nwork in cognitive psychology also suggested that it is a natural form of concept learning in\nhumans (Bruner et al., 1957). In AI, the approach is most closely associated with the work\nof Patrick Winston, whose Ph.D. thesis (Winston, 1970) addressed the problem of learning\ndescriptions of complex objects. The version space method (Mitchell, 1977, 1982) takes\na different approach, maintaining the set of all consistent hypotheses and eliminating those\nfound tobeinconsistent withnewexamples. Theapproach wasused intheMeta-DENDRAL Bibliographical andHistorical Notes 799\nexpert system for chemistry (Buchanan and Mitchell, 1978), and later in Mitchell\u2019s (1983)\nLEX system, which learns to solve calculus problems. Athird influential thread wasformed\nby the work ofMichalski and colleagues on the AQseries of algorithms, which learned sets\noflogical rules(Michalski, 1969;Michalski etal.,1986).\nEBL had its roots in the techniques used by the STRIPS planner (Fikes et al., 1972).\nWhen a plan was constructed, a generalized version of it was saved in a plan library and\nused in later planning as a macro-operator. Similar ideas appeared in Anderson\u2019s ACT*\narchitecture, under the heading of knowledge compilation (Anderson, 1983), and in the\nSOAR architecture, as chunking (Laird et al., 1986). Schema acquisition (DeJong, 1981),\nanalytical generalization (Mitchell, 1982), and constraint-based generalization (Minton,\n1984) were immediate precursors of the rapid growth of interest in EBL stimulated by the\npapers of Mitchell et al. (1986) and DeJong and Mooney (1986). Hirsh (1987) introduced\ntheEBLalgorithmdescribedinthetext,showinghowitcould beincorporated directlyintoa\nlogicprogrammingsystem. VanHarmelenandBundy(1988)explainEBLasavariantofthe\npartialevaluation methodusedinprogram analysis systems(Jones etal.,1993).\nInitial enthusiasm for EBLwas tempered by Minton\u2019s finding (1988) that, without ex-\ntensiveextrawork,EBLcouldeasilyslowdownaprogramsignificantly. Formalprobabilistic\nanalysisoftheexpectedpayoffofEBLcanbefoundin Greiner(1989)andSubramanianand\nFeldman(1990). Anexcellent surveyofearlyworkonEBLappearsin Dietterich(1990).\nInsteadofusingexamplesasfociforgeneralization, onecanusethemdirectly tosolve\nANALOGICAL new problems, in a process known as analogical reasoning. This form of reasoning ranges\nREASONING\nfrom a form of plausible reasoning based on degree of similarity (Gentner, 1983), through\na form of deductive inference based on determinations but requiring the participation of the\nexample (Davies and Russell, 1987), to a form of \u201clazy\u201d EBL that tailors the direction of\ngeneralization of the old example to fit the needs of the new problem. This latter form of\nanalogical reasoning is found most commonly in case-based reasoning (Kolodner, 1993)\nandderivational analogy(VelosoandCarbonell, 1993).\nRelevance information in the form of functional dependencies was first developed in\nthe database community, where it is used to structure large sets of attributes into manage-\nable subsets. Functional dependencies were used for analogical reasoning by Carbonell\nand Collins (1973) and rediscovered and given a full logical analysis by Davies and Rus-\nsell (Davies, 1985; Davies and Russell, 1987). Their role as prior knowledge in inductive\nlearning was explored by Russell and Grosof (1987). The equivalence of determinations to\narestricted-vocabulary hypothesis space wasproved inRussell (1988). Learning algorithms\nfor determinations and the improved performance obtained by RBDTL were first shown in\nthe FOCUS algorithm, due toAlmuallim andDietterich (1991). Tadepalli (1993) describes a\nveryingenious algorithm forlearning withdeterminations thatshowslarge improvements in\nlearning speed.\nThe idea that inductive learning can be performed by inverse deduction can be traced\nto W. S. Jevons (1874), who wrote, \u201cThe study both of Formal Logic and of the Theory of\nProbabilities has led meto adopt theopinion that there isno such thing as adistinct method\nof induction as contrasted with deduction, but that induction is simply an inverse employ-\nmentofdeduction.\u201d Computational investigations beganwiththeremarkablePh.D.thesisby 800 Chapter 19. KnowledgeinLearning\nGordonPlotkin(1971) atEdinburgh. AlthoughPlotkindeveloped manyofthetheorems and\nmethodsthatareincurrentuseinILP,hewasdiscouragedbysomeundecidability resultsfor\ncertainsubproblems ininduction. MIS(Shapiro, 1981)reintroduced theproblem oflearning\nlogic programs, but was seen mainly as a contribution to the theory of automated debug-\nging. Workonruleinduction, suchasthe ID3 (Quinlan, 1986) and CN2 (ClarkandNiblett,\n1989)systems,ledtoFOIL (Quinlan,1990), whichforthefirsttimeallowedpractical induc-\ntion of relational rules. The field of relational learning was reinvigorated by Muggleton and\nBuntine(1988),whoseCIGOLprogramincorporated aslightlyincompleteversionofinverse\nresolution andwascapableofgenerating newpredicates. Theinverseresolution methodalso\nappears in(Russell, 1986), withasimple algorithm given in afootnote. Thenextmajorsys-\ntem was GOLEM (Muggleton and Feng, 1990), which uses a covering algorithm based on\nPlotkin\u2019s concept of relative least general generalization. ITOU (Rouveirol and Puget, 1989)\nand CLINT (DeRaedt, 1992) wereothersystemsofthatera. Morerecently, PROGOL (Mug-\ngleton, 1995) has taken a hybrid (top-down and bottom-up) approach to inverse entailment\nand has been applied to a number of practical problems, particularly in biology and natural\nlanguage processing. Muggleton (2000) describes anextension of PROGOL tohandle uncer-\ntaintyintheformofstochastic logicprograms.\nA formal analysis of ILP methods appears in Muggleton (1991), a large collection of\npapers in Muggleton (1992), and a collection of techniques and applications in the book\nbyLavraucandDuzeroski(1994). PageandSrinivasan(2002)giveamorerecentoverviewof\nthefield\u2019shistory andchallenges forthefuture. Earlycomplexity results byHaussler(1989)\nsuggestedthatlearningfirst-ordersentenceswasintractible. However,withbetterunderstand-\ningoftheimportance ofsyntactic restrictions onclauses, positiveresults havebeenobtained\neven for clauses with recursion (Duzeroski et al., 1992). Learnability results for ILP are\nsurveyedbyKietzandDuzeroski(1994)andCohenandPage(1995).\nAlthough ILPnowseemstobethedominant approach toconstructive induction, ithas\nnot been the only approach taken. So-called discovery systems aim to model the process\nDISCOVERYSYSTEM\nof scientific discovery of new concepts, usually by a direct search in the space of concept\ndefinitions. Doug Lenat\u2019s Automated Mathematician, or AM (Davis and Lenat, 1982), used\ndiscovery heuristics expressed as expert system rules to guide its search for concepts and\nconjectures in elementary number theory. Unlike most systems designed for mathematical\nreasoning, AM lacked a concept of proof and could only make conjectures. It rediscovered\nGoldbach\u2019s conjecture and the Unique Prime Factorization theorem. AM\u2019sarchitecture was\ngeneralized inthe EURISKOsystem(Lenat,1983)byaddingamechanismcapable ofrewrit-\ningthe system\u2019s owndiscovery heuristics. EURISKO wasapplied inanumberofareas other\nthanmathematicaldiscovery, althoughwithlesssuccessthan AM. Themethodology ofAM\nand EURISKOhasbeencontroversial (RitchieandHanna,1984;LenatandBrown,1984).\nAnotherclassofdiscoverysystemsaimstooperate withreal scientificdatatofindnew\nlaws. The systems DALTON, GLAUBER, and STAHL (Langley et al., 1987) are rule-based\nsystems that look for quantitative relationships in experimental data from physical systems;\nin each case, the system has been able to recapitulate a well-known discovery from the his-\ntory of science. Discovery systems based on probabilistic techniques\u2014especially clustering\nalgorithms thatdiscovernewcategories\u2014are discussed inChapter20. Exercises 801\nEXERCISES\n19.1 Show, by translating into conjunctive normal form and applying resolution, that the\nconclusion drawnonpage784concerning Braziliansissound.\n19.2 For each of the following determinations, write down the logical representation and\nexplainwhythedetermination istrue(ifitis):\na. Designanddenomination determinethemassofacoin.\nb. Foragivenprogram, inputdetermines output.\nc. Climate,foodintake, exercise, andmetabolism determine weightgainandloss.\nd. Baldnessisdetermined bythebaldness (orlackthereof)of one\u2019smaternalgrandfather.\n19.3 Wouldaprobabilistic versionofdeterminations beuseful? Suggestadefinition.\n19.4 Fill in the missing values for the clauses C or C (or both) in the following sets of\n1 2\nclauses, giventhatC istheresolventofC andC :\n1 2\na. C = True \u21d2 P(A,B),C = P(x,y) \u21d2 Q(x,y),C =??.\n1 2\nb. C = True \u21d2 P(A,B),C =??,C =??.\n1 2\nc. C = P(x,y) \u21d2 P(x,f(y)),C =??,C =??.\n1 2\nIfthereismorethanonepossiblesolution,provideoneexampleofeachdifferentkind.\n19.5 Suppose one writes a logic program that carries out a resolution inference step. That\nis, let Resolve(c ,c ,c) succeed if c is the result of resolving c and c . Normally, Resolve\n1 2 1 2\nwould be used as part of a theorem prover by calling it with c and c instantiated to par-\n1 2\nticular clauses, thereby generating the resolvent c. Now suppose instead that we call it with\nc instantiated and c and c uninstantiated. Will this succeed in generating the appropriate\n1 2\nresults of an inverse resolution step? Would you need any special modifications to the logic\nprogramming systemforthistowork?\n19.6 Suppose that FOIL is considering adding a literal to a clause using a binary predicate\nP andthatpreviousliterals(including theheadoftheclause)containfivedifferentvariables.\na. Howmanyfunctionallydifferentliteralscanbegenerated? Twoliteralsarefunctionally\nidenticaliftheydifferonlyinthenamesofthenewvariablesthattheycontain.\nb. Can you find a general formula for the number of different literals with a predicate of\narityrwhenthereare nvariables previously used?\nc. Whydoes FOIL notallowliteralsthatcontainnopreviously usedvariables?\n19.7 UsingthedatafromthefamilytreeinFigure19.11,orasubsetthereof,applytheFOIL\nalgorithm tolearnadefinition forthe Ancestor predicate. 20\nLEARNING\nPROBABILISTIC MODELS\nInwhichweviewlearning asaformofuncertain reasoning fromobservations.\nChapter13pointedouttheprevalenceofuncertaintyinrealenvironments. Agentscanhandle\nuncertainty byusingthemethodsofprobability anddecision theory, butfirsttheymustlearn\ntheir probabilistic theories of the world from experience. This chapter explains how they\ncan do that, by formulating the learning task itself as a process of probabilistic inference\n(Section20.1). WewillseethataBayesianviewoflearningisextremelypowerful,providing\ngeneral solutions to the problems of noise, overfitting, and optimal prediction. It also takes\nintoaccountthefactthataless-than-omniscientagentcanneverbecertainaboutwhichtheory\noftheworldiscorrect,yetmuststillmakedecisions byusingsometheoryoftheworld.\nWedescribemethodsforlearningprobability models\u2014primarily Bayesiannetworks\u2014\nin Sections 20.2 and 20.3. Some of the material in this chapter is fairly mathematical, al-\nthoughthegenerallessonscanbeunderstoodwithoutplungingintothedetails. Itmaybenefit\nthereadertoreviewChapters13and14andpeekatAppendixA.\n20.1 STATISTICAL LEARNING\nThe key concepts in this chapter, just as in Chapter 18, are data and hypotheses. Here, the\ndataareevidence\u2014thatis,instantiationsofsomeoralloftherandomvariablesdescribingthe\ndomain. The hypotheses in this chapter are probabilistic theories of how the domain works,\nincluding logicaltheories asaspecial case.\nConsider a simple example. Our favorite Surprise candy comes in two flavors: cherry\n(yum)andlime(ugh). Themanufacturerhasapeculiarsenseofhumorandwrapseachpiece\nof candy in the same opaque wrapper, regardless of flavor. The candy is sold in very large\nbags,ofwhichthereareknowntobefivekinds\u2014again, indistinguishable fromtheoutside:\nh : 100%cherry,\n1\nh : 75%cherry+25%lime,\n2\nh : 50%cherry+50%lime,\n3\nh : 25%cherry+75%lime,\n4\nh : 100%lime .\n5\n802 Section20.1. StatisticalLearning 803\nGiven a new bag of candy, the random variable H (for hypothesis) denotes the type of the\nbag, with possible values h through h . H is not directly observable, of course. As the\n1 5\npieces of candy are opened and inspected, data are revealed\u2014D , D , ..., D , where each\n1 2 N\nD is a random variable with possible values cherry and lime. The basic task faced by the\ni\nagent is to predict the flavor of the next piece of candy.1 Despite its apparent triviality, this\nscenario serves to introduce many of the major issues. The agent really does need to infer a\ntheoryofitsworld,albeitaverysimpleone.\nBayesianlearningsimplycalculatestheprobability ofeachhypothesis,giventhedata,\nBAYESIANLEARNING\nand makes predictions on that basis. That is, the predictions are made by using all the hy-\npotheses,weightedbytheirprobabilities, ratherthanbyusingjustasingle\u201cbest\u201dhypothesis.\nIn this way, learning is reduced to probabilistic inference. Let D represent all the data, with\nobserved valued;thentheprobability ofeachhypothesis isobtainedbyBayes\u2019rule:\nP(h |d) = \u03b1P(d|h )P(h ). (20.1)\ni i i\nNow,suppose wewanttomakeaprediction aboutanunknownquantityX. Thenwehave\n(cid:12) (cid:12)\nP(X|d)= P(X|d,h )P(h |d) = P(X|h )P(h |d), (20.2)\ni i i i\ni i\nwhere we have assumed that each hypothesis determines a probability distribution over X.\nThis equation shows that predictions are weighted averages overthe predictions of the indi-\nvidual hypotheses. The hypotheses themselves are essentially \u201cintermediaries\u201d between the\nrawdataandthepredictions. ThekeyquantitiesintheBayesianapproacharethe hypothesis\nprior,P(h ),andthelikelihoodofthedataundereachhypothesis, P(d|h ).\nHYPOTHESISPRIOR i i\nFor our candy example, we will assume for the time being that the prior distribution\nLIKELIHOOD\nover h ,...,h is given by (cid:16)0.1,0.2,0.4,0.2,0.1(cid:17), as advertised by the manufacturer. The\n1 5\nlikelihood of the data is calculated under the assumption that the observations are i.i.d. (see\npage708),sothat\n(cid:25)\nP(d|h ) = P(d |h ). (20.3)\ni j i\nj\nFor example, suppose the bag is really an all-lime bag (h ) and the first 10 candies are all\n5\nlime;thenP(d|h )is0.510,because halfthecandies inanh bagarelime.2 Figure20.1(a)\n3 3\nshows how the posterior probabilities of the five hypotheses change as the sequence of 10\nlime candies is observed. Notice that the probabilities start out at their prior values, so h\n3\nis initially the most likely choice and remains so after 1 lime candy is unwrapped. After 2\nlimecandies areunwrapped, h ismostlikely; after3ormore, h (thedreadedall-limebag)\n4 5\nis the most likely. After 10 in a row, we are fairly certain of our fate. Figure 20.1(b) shows\nthepredicted probability thatthenextcandyislime,based onEquation(20.2). Aswewould\nexpect,itincreases monotonically toward1.\n1 Statisticallysophisticatedreaderswillrecognizethisscenarioasavariantoftheurn-and-ballsetup. Wefind\nurnsandballslesscompellingthancandy;furthermore,candylendsitselftoothertasks,suchasdecidingwhether\ntotradethebagwithafriend\u2014seeExercise20.2.\n2 Westatedearlierthatthebagsofcandyareverylarge;otherwise,thei.i.d.assumptionfailstohold.Technically,\nitismorecorrect(butlesshygienic)torewrapeachcandyafterinspectionandreturnittothebag. 804 Chapter 20. LearningProbabilistic Models\n1 1\nP(h | d)\n1\nP(h | d)\n0.8 P(h2 | d) 0.9 3\nP(h | d)\nP(h4 | d) 0.8\n0.6 5\n0.7\n0.4\n0.6\n0.2\n0.5\n0 0.4\n0 2 4 6 8 10 0 2 4 6 8 10\nNumber of observations in d Number of observations in d\n(a) (b)\nFigure 20.1 (a) Posterior probabilities P(hi|d 1,...,dN) from Equation (20.1). The\nnumber of observations N ranges from 1 to 10, and each observation is of a lime candy.\n(b)BayesianpredictionP(dN+1=lime|d 1,...,dN)fromEquation(20.2).\nThe example shows that the Bayesian prediction eventually agrees with the true hy-\npothesis. This is characteristic of Bayesian learning. For any fixed prior that does not rule\nout the true hypothesis, the posterior probability of any false hypothesis will, under certain\ntechnical conditions, eventually vanish. Thishappens simplybecause theprobability ofgen-\nerating \u201cuncharacteristic\u201d data indefinitely is vanishingly small. (This point is analogous to\none made in the discussion of PAC learning in Chapter 18.) More important, the Bayesian\nprediction is optimal, whetherthe data setbesmallorlarge. Giventhe hypothesis prior, any\notherprediction isexpected tobecorrectlessoften.\nThe optimality of Bayesian learning comes at a price, of course. For real learning\nproblems, the hypothesis space is usually very large orinfinite, as wesaw in Chapter 18. In\nsomecases, thesummationinEquation (20.2)(orintegration, inthecontinuous case)canbe\ncarriedouttractably, butinmostcaseswemustresorttoapproximate orsimplifiedmethods.\nAverycommonapproximation\u2014onethatisusuallyadoptedinscience\u2014istomakepre-\ndictionsbasedonasinglemostprobablehypothesis\u2014that is,anh thatmaximizesP(h |d).\ni i\nMAXIMUMA ThisisoftencalledamaximumaposterioriorMAP(pronounced\u201cem-ay-pee\u201d)hypothesis.\nPOSTERIORI\nPredictions madeaccording toanMAPhypothesis h areapproximately Bayesian tothe\nMAP\nextentthatP(X|d)\u2248 P(X|h ). Inourcandyexample,h =h afterthreelimecan-\nMAP MAP 5\ndiesinarow,sotheMAPlearnerthenpredicts thatthefourth candyislimewithprobability\n1.0\u2014a much more dangerous prediction than the Bayesian prediction of 0.8 shown in Fig-\nure20.1(b). Asmoredataarrive,theMAPandBayesian predictions becomecloser, because\nthecompetitors totheMAPhypothesis becomelessandlessprobable.\nAlthough our example doesn\u2019t show it, finding MAP hypotheses is often much easier\nthanBayesianlearning,becauseitrequiressolvinganoptimizationprobleminsteadofalarge\nsummation(orintegration) problem. Wewillseeexamplesof thislaterinthechapter.\nsisehtopyh\nfo\nytilibaborp\nroiretsoP\nemil\nsi ydnac\ntxen\ntaht\nytilibaborP Section20.1. StatisticalLearning 805\nIn both Bayesian learning and MAP learning, the hypothesis prior P(h ) plays an im-\ni\nportant role. We saw in Chapter 18 that overfitting can occur when the hypothesis space\nis too expressive, so that it contains many hypotheses that fit the data set well. Rather than\nplacing an arbitrary limit on the hypotheses to be considered, Bayesian and MAP learning\nmethods use the prior to penalize complexity. Typically, more complex hypotheses have a\nlower prior probability\u2014in part because there are usually many more complex hypotheses\nthan simple hypotheses. Onthe otherhand, more complex hypotheses have agreater capac-\nity to fit the data. (In the extreme case, a lookup table can reproduce the data exactly with\nprobability 1.) Hence, the hypothesis priorembodies atradeoff between thecomplexity ofa\nhypothesis anditsdegreeoffittothedata.\nWecanseetheeffectofthistradeoffmostclearlyinthelogicalcase,whereH contains\nonlydeterministic hypotheses. Inthatcase,P(d|h )is1ifh isconsistent and 0otherwise.\ni i\nLooking at Equation (20.1), we see that h will then be the simplest logical theory that\nMAP\nis consistent with the data. Therefore, maximum a posteriori learning provides a natural\nembodimentofOckham\u2019srazor.\nAnother insight into the tradeoff between complexity and degree of fit is obtained by\ntaking the logarithm of Equation (20.1). Choosing h to maximize P(d|h )P(h ) is\nMAP i i\nequivalent tominimizing\n\u2212log P(d|h )\u2212log P(h ).\n2 i 2 i\nUsing the connection between information encoding and probability that we introduced in\nChapter18.3.4,weseethatthe\u2212 log P(h )termequalsthenumberofbitsrequiredtospec-\n2 i\nifythehypothesish . Furthermore, \u2212 log P(d|h )istheadditionalnumberofbitsrequired\ni 2 i\nto specify the data, given the hypothesis. (To see this, consider that no bits are required\nif the hypothesis predicts the data exactly\u2014as with h and the string of lime candies\u2014and\n5\nlog 1=0.) Hence, MAP learning is choosing the hypothesis that provides maximum com-\n2\npression ofthedata. Thesametaskisaddressed moredirectly bythe minimumdescription\nlength,orMDL,learningmethod. WhereasMAPlearningexpressessimplicitybyassigning\nhigherprobabilities tosimplerhypotheses, MDLexpresses itdirectly bycounting thebitsin\nabinaryencoding ofthehypotheses anddata.\nA final simplification is provided by assuming a uniform prior over the space of hy-\npotheses. In that case, MAP learning reduces to choosing an h that maximizes P(d|h ).\ni i\nMAXIMUM- Thisiscalledamaximum-likelihood(ML)hypothesis, h . Maximum-likelihood learning\nLIKELIHOOD ML\nis very common in statistics, a discipline in which many researchers distrust the subjective\nnatureofhypothesis priors. Itisareasonable approach whenthereisnoreasontopreferone\nhypothesis overanother apriori\u2014for example, whenallhypotheses are equally complex. It\nprovides a good approximation to Bayesian and MAP learning when the data set is large,\nbecause the data swamps the prior distribution over hypotheses, but it has problems (as we\nshallsee)withsmalldatasets. 806 Chapter 20. LearningProbabilistic Models\n20.2 LEARNING WITH COMPLETE DATA\nThegeneraltaskoflearningaprobability model,givendata thatareassumedtobegenerated\nfrom that model, is called density estimation. (The term applied originally to probability\nDENSITYESTIMATION\ndensityfunctions forcontinuous variables, butisusednow fordiscretedistributions too.)\nThis section covers the simplest case, where we have complete data. Data are com-\nCOMPLETEDATA\nplete when each data point contains values forevery variable in the probability model being\nPARAMETER learned. We focus on parameter learning\u2014finding the numerical parameters for a proba-\nLEARNING\nbility model whose structure is fixed. For example, we might be interested in learning the\nconditional probabilities in a Bayesian network with a given structure. We will also look\nbrieflyattheproblem oflearning structure andatnonparametric densityestimation.\n20.2.1 Maximum-likelihoodparameter learning: Discrete models\nSupposewebuyabagoflimeandcherrycandyfromanewmanufacturerwhoselime\u2013cherry\nproportions are completely unknown; the fraction could be anywhere between 0 and 1. In\nthat case, we have a continuum of hypotheses. The parameter in this case, which we call\n\u03b8, is the proportion of cherry candies, and the hypothesis is h . (The proportion of limes is\n\u03b8\njust 1\u2212\u03b8.) If we assume that all proportions are equally likely a priori, then a maximum-\nlikelihood approach is reasonable. If we model the situation with a Bayesian network, we\nneedjustonerandomvariable, Flavor (theflavorofarandomlychosencandyfromthebag).\nIthasvaluescherry andlime,wheretheprobabilityofcherry is\u03b8(seeFigure20.2(a)). Now\nsuppose weunwrap N candies, ofwhich care cherries and (cid:3)=N \u2212care limes. According\ntoEquation(20.3), thelikelihood ofthisparticular datasetis\n(cid:25)N\nP(d|h ) = P(d |h ) = \u03b8c\u00b7(1\u2212\u03b8)(cid:3) .\n\u03b8 j \u03b8\nj=1\nThe maximum-likelihood hypothesis is given by the value of \u03b8 that maximizes this expres-\nsion. Thesamevalueisobtained bymaximizingtheloglikelihood,\nLOGLIKELIHOOD\n(cid:12)N\nL(d|h ) = logP(d|h ) = logP(d |h )= clog\u03b8+(cid:3)log(1\u2212\u03b8).\n\u03b8 \u03b8 j \u03b8\nj=1\n(Bytaking logarithms, wereduce theproduct toasum overthe data, whichisusually easier\ntomaximize.) Tofindthemaximum-likelihood valueof\u03b8,wedifferentiate Lwithrespect to\n\u03b8 andsettheresulting expression tozero:\ndL(d|h ) c (cid:3) c c\n\u03b8 = \u2212 = 0 \u21d2 \u03b8 = = .\nd\u03b8 \u03b8 1\u2212\u03b8 c+(cid:3) N\nIn English, then, the maximum-likelihood hypothesis h asserts that the actual proportion\nML\nofcherries inthebagisequaltotheobserved proportion inthecandiesunwrapped sofar!\nIt appears that we have done a lot of work to discover the obvious. In fact, though,\nwehavelaidoutonestandardmethodformaximum-likelihood parameterlearning,amethod\nwithbroadapplicability: Section20.2. LearningwithCompleteData 807\nP(F=cherry)\n\u03b8\nP(F=cherry) Flavor\n\u03b8\nF P(W=red| F)\n\u03b8\nFlavor cherry 1\n\u03b8\nlime 2\nWrapper\n(a) (b)\nFigure20.2 (a)Bayesiannetworkmodelforthecaseofcandieswithanunknownpropor-\ntionofcherriesandlimes. (b)Modelforthecasewherethewrappercolordepends(proba-\nbilistically)onthecandyflavor.\n1. Writedownanexpressionforthelikelihoodofthedataasafunctionoftheparameter(s).\n2. Writedownthederivativeoftheloglikelihood withrespecttoeachparameter.\n3. Findtheparametervaluessuchthatthederivativesarezero.\nThe trickiest step is usually the last. In our example, it was trivial, but we will see that in\nmanycasesweneedtoresorttoiterativesolutionalgorithmsorothernumericaloptimization\ntechniques, as described in Chapter 4. The example also illustrates a significant problem\nwith maximum-likelihood learning in general: when the data set is small enough that some\neventshavenotyetbeenobserved\u2014forinstance,nocherrycandies\u2014themaximum-likelihood\nhypothesis assigns zero probability to those events. Various tricks are used to avoid this\nproblem,suchasinitializing thecountsforeacheventto1insteadof0.\nLetus look atanother example. Suppose this new candy manufacturer wants togive a\nlittlehinttotheconsumerandusescandywrapperscoloredredandgreen. TheWrapper for\neachcandyisselectedprobabilistically, accordingtosomeunknownconditionaldistribution,\ndepending on the flavor. The corresponding probability model is shown in Figure 20.2(b).\nNotice that it has three parameters: \u03b8, \u03b8 , and \u03b8 . With these parameters, the likelihood of\n1 2\nseeing, say, a cherry candy in a green wrapper can be obtained from the standard semantics\nforBayesiannetworks(page513):\nP(Flavor =cherry,Wrapper =green|h )\n\u03b8,\u03b81,\u03b82\n= P(Flavor =cherry|h )P(Wrapper =green|Flavor =cherry,h )\n\u03b8,\u03b81,\u03b82 \u03b8,\u03b81,\u03b82\n= \u03b8\u00b7(1\u2212\u03b8 ).\n1\nNowweunwrap N candies, ofwhichcarecherries and (cid:3)arelimes. Thewrappercounts are\nasfollows: r ofthecherrieshaveredwrappersand g havegreen,whiler ofthelimeshave\nc c (cid:3)\nredandg havegreen. Thelikelihood ofthedataisgivenby\n(cid:3)\nP(d|h ) = \u03b8c(1\u2212\u03b8)(cid:3)\u00b7\u03b8rc(1\u2212\u03b8 )gc \u00b7\u03b8r(cid:3)(1\u2212\u03b8 )g(cid:3) .\n\u03b8,\u03b81,\u03b82 1 1 2 2 808 Chapter 20. LearningProbabilistic Models\nThislooksprettyhorrible, buttakinglogarithmshelps:\nL=[clog\u03b8+(cid:3)log(1\u2212\u03b8)]+[rclog\u03b8 1+gclog(1\u2212\u03b8 1)]+[r(cid:3)log\u03b8 2+g(cid:3)log(1\u2212\u03b8 2)].\nThebenefitoftakinglogsisclear: theloglikelihoodisthesumofthreeterms,eachofwhich\ncontainsasingleparameter. Whenwetakederivativeswithrespecttoeachparameterandset\nthemtozero,wegetthreeindependent equations, eachcontaining justoneparameter:\n\u2202L = c \u2212 (cid:3) = 0 \u21d2 \u03b8 = c\n\u2202\u03b8 \u03b8 1\u2212\u03b8 c+(cid:3)\n\u2202L = rc \u2212 gc = 0 \u21d2 \u03b8 = rc\n\u2202\u03b81 \u03b81 1\u2212\u03b81 1 rc+gc\n\u2202L = r(cid:3) \u2212 g(cid:3) = 0 \u21d2 \u03b8 = r(cid:3) .\n\u2202\u03b82 \u03b82 1\u2212\u03b82 2 r(cid:3)+g(cid:3)\nThe solution for \u03b8 is the same as before. The solution for \u03b8 , the probability that a cherry\n1\ncandy has a red wrapper, is the observed fraction of cherry candies with red wrappers, and\nsimilarlyfor\u03b8 .\n2\nTheseresultsareverycomforting, anditiseasytoseethattheycanbeextended toany\nBayesiannetworkwhoseconditionalprobabilitiesarerepresentedastables. Themostimpor-\ntant point is that, with complete data, the maximum-likelihood parameter learning problem\nforaBayesiannetworkdecomposesintoseparatelearningproblems,oneforeachparameter.\n(SeeExercise20.6forthenontabulatedcase,whereeachparameteraffectsseveralconditional\nprobabilities.) Thesecond point isthat theparametervalues foravariable, given itsparents,\nare just the observed frequencies of the variable values for each setting of the parent values.\nAsbefore,wemustbecarefultoavoidzeroeswhenthedataset issmall.\n20.2.2 NaiveBayes models\nProbably the most common Bayesian network model used in machine learning is the naive\nBayesmodelfirstintroduced onpage499. Inthis model, the\u201cclass\u201d variable C (which isto\nbepredicted) istherootandthe\u201cattribute\u201d variables X aretheleaves. Themodelis\u201cnaive\u201d\ni\nbecause it assumes that the attributes are conditionally independent of each other, given the\nclass. (The model in Figure 20.2(b) is a naive Bayes model with class Flavor and just one\nattribute, Wrapper.) AssumingBooleanvariables, theparameters are\n\u03b8=P(C=true),\u03b8 =P(X =true|C=true),\u03b8 =P(X =true|C=false).\ni1 i i2 i\nThe maximum-likelihood parameter values are found in exactly the same way as for Fig-\nure20.2(b). Oncethemodelhasbeentrainedinthisway,itcanbeusedtoclassifynewexam-\nplesforwhichtheclassvariable C isunobserved. Withobservedattributevalues x ,...,x ,\n1 n\ntheprobability ofeachclassisgivenby\n(cid:25)\nP(C|x ,...,x )= \u03b1 P(C) P(x |C).\n1 n i\ni\nA deterministic prediction can be obtained by choosing the most likely class. Figure 20.3\nshows the learning curve for this method when it is applied to the restaurant problem from\nChapter 18. The method learns fairly well but not as well as decision-tree learning; this is\npresumably because the true hypothesis\u2014which is a decision tree\u2014is not representable ex-\nactly using anaiveBayesmodel. NaiveBayeslearning turnsout todosurprisingly wellina\nwide range of applications; the boosted version (Exercise 20.4) is one of the most effective Section20.2. LearningwithCompleteData 809\n1\n0.9\n0.8\n0.7\n0.6\nDecision tree\n0.5 Naive Bayes\n0.4\n0 20 40 60 80 100\nTraining set size\nFigure20.3 ThelearningcurvefornaiveBayeslearningappliedtotherestaurantproblem\nfromChapter18;thelearningcurvefordecision-treelearningisshownforcomparison.\ngeneral-purpose learning algorithms. Naive Bayes learning scales well to very large prob-\nlems: withn Boolean attributes, there are just 2n+1parameters, and nosearch is required\ntofindh ,the maximum-likelihood naive Bayeshypothesis. Finally, naiveBayeslearning\nML\nsystems have no difficulty with noisy or missing data and can give probabilistic predictions\nwhenappropriate.\n20.2.3 Maximum-likelihoodparameter learning: Continuous models\nContinuous probability models such as the linear Gaussian model were introduced in Sec-\ntion14.3. Becausecontinuousvariablesareubiquitousinreal-worldapplications, itisimpor-\ntanttoknowhowtolearntheparametersofcontinuous models fromdata. Theprinciples for\nmaximum-likelihood learningareidentical inthecontinuous anddiscretecases.\nLet us begin with a very simple case: learning the parameters of a Gaussian density\nfunction onasinglevariable. Thatis,thedataaregenerated asfollows:\nP(x) =\n\u221a1 e\u2212(x 2\u2212 \u03c3\u03bc 2)2\n.\n2\u03c0\u03c3\nThe parameters of this model are the mean \u03bc and the standard deviation \u03c3. (Notice that the\nnormalizing \u201cconstant\u201d depends on \u03c3, so we cannot ignore it.) Let the observed values be\nx ,...,x . Thentheloglikelihood is\n1 N\nL =\n(cid:12)N\nlog\n\u221a1 e\u2212(xj 2\u2212 \u03c32\u03bc)2\n=\nN(\u2212log\u221a\n2\u03c0\u2212log\u03c3)\u2212\n(cid:12)N (x\nj\n\u2212\u03bc)2\n.\n2\u03c0\u03c3 2\u03c32\nj=1 j=1\nSettingthederivatives tozeroasusual,weobtain\n(cid:2) P\n\u2202L = \u2212 1 N (x \u2212\u03bc) = 0 \u21d2 \u03bc = jxj\n\u2202\u03bc \u03c32 j=1 (cid:2)j (cid:9)NP\n(20.4)\n\u2202L = \u2212N + 1 N (x \u2212\u03bc)2 = 0 \u21d2 \u03c3 =\nj(xj\u2212\u03bc)2\n.\n\u2202\u03c3 \u03c3 \u03c33 j=1 j N\nThatis,themaximum-likelihood valueofthemeanisthesampleaverageandthemaximum-\nlikelihood value of the standard deviation is the square root of the sample variance. Again,\nthesearecomforting resultsthatconfirm\u201ccommonsense\u201d practice.\ntes\ntset\nno\ntcerroc\nnoitroporP 810 Chapter 20. LearningProbabilistic Models\n1\n0.8\nP(y |x)\n4 0.6\n3.5\n3\n2.5\n0.4\n2\n1.5\n1\n0.5 0.81 0.2\n0 0.6\n0 0.2 0 x.4 0.6 0.8 1 0 0.20.4 y 0\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nx\n(a) (b)\nFigure20.4 (a)AlinearGaussianmodeldescribedas y=\u03b8 x+\u03b8 plusGaussiannoise\n1 2\nwithfixedvariance.(b)Asetof50datapointsgeneratedfromthismodel.\nNowconsideralinearGaussianmodelwithonecontinuous parentX andacontinuous\nchild Y. As explained on page 520, Y has a Gaussian distribution whose mean depends\nlinearly on the value of X and whose standard deviation is fixed. To learn the conditional\ndistribution P(Y |X),wecanmaximizetheconditional likelihood\nP(y|x) =\n\u221a1 e\u2212(y\u2212(\u03b81 2\u03c3x 2+\u03b82))2\n. (20.5)\n2\u03c0\u03c3\nHere,theparametersare\u03b8 ,\u03b8 ,and\u03c3. Thedataareacollectionof(x ,y )pairs,asillustrated\n1 2 j j\ninFigure20.4. Usingtheusualmethods(Exercise20.5),wecanfindthemaximum-likelihood\nvalues of the parameters. The point here is different. If we consider just the parameters \u03b8\n1\nand \u03b8 that define thelinear relationship between x and y, it becomes clearthat maximizing\n2\nthe log likelihood with respect to these parameters is the same as minimizing the numerator\n(y \u2212 (\u03b8 x + \u03b8 ))2 in the exponent of Equation (20.5). This is the L loss, the squared er-\n1 2 2\nror between the actual value y and the prediction \u03b8 x+\u03b8 . This is the quantity minimized\n1 2\nby the standard linear regression procedure described in Section 18.6. Now we can under-\nstandwhy: minimizingthesumofsquarederrorsgivesthemaximum-likelihood straight-line\nmodel,provided thatthedataaregenerated withGaussiannoiseoffixedvariance.\n20.2.4 Bayesianparameter learning\nMaximum-likelihood learning gives rise to some very simple procedures, but it has some\nserious deficiencies with small data sets. For example, after seeing one cherry candy, the\nmaximum-likelihood hypothesis is that the bag is 100% cherry (i.e., \u03b8=1.0). Unless one\u2019s\nhypothesis prior is that bags must be either all cherry or all lime, this is not a reasonable\nconclusion. It is more likely that the bag is a mixture of lime and cherry. The Bayesian\napproach to parameter learning starts by defining a prior probability distribution over the\npossible hypotheses. We call this the hypothesis prior. Then, as data arrives, the posterior\nHYPOTHESISPRIOR\nprobability distribution isupdated.\ny Section20.2. LearningwithCompleteData 811\n2.5 6\n[5,5]\n5 [30,10]\n2\n[2,2] 4\n1.5\n3\n[1,1]\n1\n2\n[6,2]\n0.5\n1\n[3,1]\n0 0\n0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1\nParameter\u03b8 Parameter\u03b8\n(a) (b)\nFigure20.5 Examplesofthebeta[a,b]distributionfordifferentvaluesof[a,b].\nThe candy example in Figure 20.2(a) has one parameter, \u03b8: the probability that a ran-\ndomly selected piece of candy is cherry-flavored. In the Bayesian view, \u03b8 is the (unknown)\nvalue of a random variable \u0398 that defines the hypothesis space; the hypothesis prior is just\nthepriordistribution P(\u0398). Thus,P(\u0398=\u03b8)isthepriorprobabilitythatthebaghasafraction\n\u03b8 ofcherrycandies.\nIf the parameter \u03b8 can be any value between 0 and 1, then P(\u0398) must be a continuous\ndistributionthatisnonzeroonlybetween0and1andthatintegratesto1. Theuniformdensity\nP(\u03b8) = Uniform[0,1](\u03b8) is one candidate. (See Chapter 13.) It turns out that the uniform\ndensity isamemberofthefamilyofbetadistributions. Eachbetadistribution isdefinedby\nBETADISTRIBUTION\ntwohyperparameters3 aandbsuchthat\nHYPERPARAMETER\nbeta[a,b](\u03b8) =\n\u03b1\u03b8a\u22121(1\u2212\u03b8)b\u22121\n, (20.6)\nfor\u03b8intherange[0,1]. Thenormalizationconstant\u03b1,whichmakesthedistributionintegrate\nto1,depends onaandb. (SeeExercise 20.7.) Figure20.5showswhatthedistribution looks\nlike forvarious values of aand b. The meanvalue of the distribution is a\/(a+b), so larger\nvalues of a suggest a belief that \u0398 is closer to 1 than to 0. Larger values of a+b make the\ndistribution more peaked, suggesting greater certainty about the value of \u0398. Thus, the beta\nfamilyprovides ausefulrangeofpossibilities forthehypothesis prior.\nBesides its flexibility, the beta family has another wonderful property: if \u0398 has aprior\nbeta[a,b], then, after a data point is observed, the posterior distribution for \u0398 is also a beta\ndistribution. In other words, beta is closed under update. The beta family is called the\nconjugate prior for the family of distributions for a Boolean variable.4 Let\u2019s see how this\nCONJUGATEPRIOR\nworks. Supposeweobserveacherrycandy;thenwehave\n3 Theyarecalledhyperparametersbecausetheyparameterizeadistributionover\u03b8,whichisitselfaparameter.\n4 Otherconjugatepriorsincludethe Dirichletfamilyfortheparametersofadiscretemultivalueddistribution\nandtheNormal\u2013WishartfamilyfortheparametersofaGaussiandistribution.SeeBernardoandSmith(1994).\n)\u03b8\n=\n\u0398(P\n)\u03b8\n=\n\u0398(P 812 Chapter 20. LearningProbabilistic Models\n\u0398\nFlavor Flavor Flavor\n1 2 3\nWrapper Wrapper Wrapper\n1 2 3\n\u0398 \u0398\n1 2\nFigure20.6 ABayesiannetworkthatcorrespondstoaBayesianlearningprocess. Poste-\nriordistributionsfortheparametervariables \u0398,\u0398 ,and\u0398 canbeinferredfromtheirprior\n1 2\ndistributionsandtheevidenceintheFlavoriandWrapperi variables.\nP(\u03b8|D =cherry) = \u03b1P(D =cherry|\u03b8)P(\u03b8)\n1 1\n= \u03b1(cid:2) \u03b8\u00b7beta[a,b](\u03b8) = \u03b1(cid:2) \u03b8\u00b7\u03b8a\u22121(1\u2212\u03b8)b\u22121\n=\n\u03b1(cid:2) \u03b8a(1\u2212\u03b8)b\u22121\n= beta[a+1,b](\u03b8).\nThus, after seeing acherry candy, wesimply increment the aparameter toget the posterior;\nsimilarly, after seeing a lime candy, weincrement the b parameter. Thus, we can view the a\nand bhyperparameters as virtual counts, inthesense thataprior beta[a,b] behaves exactly\nVIRTUALCOUNTS\nasifwehad started outwithauniform prior beta[1,1] andseen a\u22121actual cherry candies\nandb\u22121actuallimecandies.\nByexaminingasequenceofbetadistributions forincreasingvaluesofaandb,keeping\nthe proportions fixed, we can see vividly how the posterior distribution over the parameter\n\u0398 changes as data arrive. Forexample, suppose the actual bag ofcandy is75% cherry. Fig-\nure 20.5(b) shows the sequence beta[3,1], beta[6,2], beta[30,10]. Clearly, the distribution\nisconvergingtoanarrowpeakaroundthetruevalueof \u0398. Forlargedatasets,then,Bayesian\nlearning(atleastinthiscase)convergestothesameanswerasmaximum-likelihoodlearning.\nNowletusconsideramorecomplicated case. Thenetwork inFigure 20.2(b)hasthree\nparameters, \u03b8,\u03b8 ,and\u03b8 ,where\u03b8 istheprobability ofaredwrapperonacherrycandy and\n1 2 1\n\u03b8 is the probability of a red wrapper on a lime candy. The Bayesian hypothesis prior must\n2\ncover all three parameters\u2014that is, we need to specify P(\u0398,\u0398 ,\u0398 ). Usually, we assume\n1 2\nPARAMETER parameterindependence:\nINDEPENDENCE\nP(\u0398,\u0398 ,\u0398 ) = P(\u0398)P(\u0398 )P(\u0398 ).\n1 2 1 2 Section20.2. LearningwithCompleteData 813\nWith this assumption, each parameter can have its ownbeta distribution that is updated sep-\narately as data arrive. Figure 20.6 shows how we can incorporate the hypothesis prior and\nany data into one Bayesian network. The nodes \u0398,\u0398 ,\u0398 have no parents. But each time\n1 2\nwemakeanobservation ofawrapperandcorresponding flavorofapieceofcandy, weadda\nnodeFlavor ,whichisdependent ontheflavorparameter \u0398:\ni\nP(Flavor =cherry|\u0398=\u03b8)= \u03b8 .\ni\nWealsoaddanodeWrapper ,whichisdependent on\u0398 and\u0398 :\ni 1 2\nP(Wrapper =red |Flavor =cherry,\u0398 =\u03b8 ) = \u03b8\ni i 1 1 1\nP(Wrapper =red |Flavor =lime,\u0398 =\u03b8 )= \u03b8 .\ni i 2 2 2\nNow, the entire Bayesian learning process can be formulated as an inference problem. We\nadd new evidence nodes, then query the unknown nodes (in this case, \u0398,\u0398 ,\u0398 ). This for-\n1 2\nmulation of learning and prediction makes it clear that Bayesian learning requires no extra\n\u201cprinciples oflearning.\u201d Furthermore, thereis, inessence, justonelearning algorithm \u2014the\ninference algorithm forBayesian networks. Ofcourse, thenatureofthesenetworks issome-\nwhatdifferent from those of Chapter14because ofthe potentially huge numberofevidence\nvariables representing the training set and the prevalence of continuous-valued parameter\nvariables.\n20.2.5 Learning Bayes net structures\nSofar, wehaveassumed thatthestructure oftheBayesnetisgivenandwearejusttrying to\nlearn the parameters. The structure of the network represents basic causal knowledge about\nthe domain that is often easy for an expert, or even a naive user, to supply. In some cases,\nhowever, the causal model may be unavailable or subject to dispute\u2014for example, certain\ncorporations have long claimed that smoking does not cause cancer\u2014so it is important to\nunderstand how the structure of a Bayes net can be learned from data. This section gives a\nbriefsketchofthemainideas.\nThe most obvious approach is to search for a good model. We can start with a model\ncontaining no links and begin adding parents for each node, fitting the parameters with the\nmethods we have just covered and measuring the accuracy of the resulting model. Alterna-\ntively, we can start with an initial guess at the structure and use hill-climbing or simulated\nannealing search to make modifications, retuning the parameters after each change in the\nstructure. Modifications can include reversing, adding, or deleting links. We must not in-\ntroduce cycles in the process, so many algorithms assume that an ordering is given for the\nvariables, and that a node can have parents only among those nodes that come earlier in the\nordering (just asintheconstruction process inChapter14). Forfullgenerality, wealsoneed\ntosearchoverpossible orderings.\nThere are two alternative methods fordeciding when a good structure has been found.\nThefirstistotestwhethertheconditionalindependenceassertionsimplicitinthestructureare\nactually satisfied inthe data. Forexample, the useof anaive Bayesmodel fortherestaurant\nproblem assumesthat\nP(Fri\/Sat,Bar |WillWait) = P(Fri\/Sat|WillWait)P(Bar |WillWait) 814 Chapter 20. LearningProbabilistic Models\nandwecancheckinthedatathatthesameequation holdsbetweenthecorresponding condi-\ntional frequencies. But even if the structure describes the true causal nature of the domain,\nstatistical fluctuations in the data set mean that the equation will never be satisfied exactly,\nso we need to perform a suitable statistical test to see if there is sufficient evidence that the\nindependence hypothesis is violated. The complexity of the resulting network will depend\non the threshold used forthis test\u2014the stricter the independence test, the more links will be\naddedandthegreaterthedangerofoverfitting.\nAn approach more consistent with the ideas in this chapter is to assess the degree to\nwhich the proposed model explains the data (in a probabilistic sense). We must be careful\nhow we measure this, however. If we just try to find the maximum-likelihood hypothesis,\nwewill end up with afully connected network, because adding more parents to anode can-\nnot decrease the likelihood (Exercise 20.8). We are forced to penalize model complexity in\nsome way. The MAP (or MDL) approach simply subtracts a penalty from the likelihood of\neach structure (after parameter tuning) before comparing different structures. The Bayesian\napproach places ajoint prioroverstructures and parameters. There are usually fartoo many\nstructures to sum over (superexponential in the number of variables), so most practitioners\nuseMCMCtosampleoverstructures.\nPenalizingcomplexity(whetherbyMAPorBayesianmethods)introducesanimportant\nconnection between the optimal structure and the nature of the representation for the condi-\ntional distributions in the network. With tabular distributions, the complexity penalty for a\nnode\u2019s distribution growsexponentially with thenumber of parents, but with, say, noisy-OR\ndistributions, itgrows only linearly. This means that learning with noisy-OR (orother com-\npactlyparameterized)modelstendstoproducelearnedstructureswithmoreparentsthandoes\nlearning withtabulardistributions.\n20.2.6 Density estimationwithnonparametric models\nItispossibletolearnaprobabilitymodelwithoutmakinganyassumptionsaboutitsstructure\nand parameterization by adopting the nonparametric methods of Section 18.8. The task of\nNONPARAMETRIC nonparametric density estimation is typically done in continuous domains, such as that\nDENSITYESTIMATION\nshown inFigure 20.7(a). Thefigure shows aprobability density function on aspace defined\nby two continuous variables. In Figure 20.7(b) we see a sample of data points from this\ndensityfunction. Thequestion is,canwerecoverthemodelfromthesamples?\nFirst we will consider k-nearest-neighbors models. (In Chapter 18 we saw nearest-\nneighbor models for classification and regression; here we see them for density estimation.)\nGivenasampleofdatapoints,toestimatetheunknownprobability densityataquerypointx\nwecansimplymeasurethedensityofthedatapointsintheneighborhoodofx. Figure20.7(b)\nshows two query points (small squares). For each query point we have drawn the smallest\ncircle that encloses 10neighbors\u2014the 10-nearest-neighborhood. Wecansee thatthe central\ncircle is large, meaning there is a low density there, and the circle on the right is small,\nmeaningthereisahighdensitythere. InFigure20.8weshowthreeplotsofdensityestimation\nusing k-nearest-neighbors, for different values of k. It seems clear that (b) is about right,\nwhile(a)istoospiky(k istoosmall)and(c)istoosmooth(k istoobig). Section20.2. LearningwithCompleteData 815\nDensity\n18\n16 1\n14\n0.9\n12\n10 0.8\n8\n6 0.7\n4\n1 0.6\n2 0.8\n0 0 0.6 0.5\n0.2 0.4\n0.4 0.6 0.2 0.4\n0.8 0\n1\n0.3\n0 0.2 0.4 0.6 0.8 1\n(a) (b)\nFigure20.7 (a)A3DplotofthemixtureofGaussiansfromFigure20.11(a). (b)A128-\npointsampleofpointsfromthemixture,togetherwithtwoquerypoints(smallsquares)and\ntheir10-nearest-neighborhoods(mediumandlargecircles).\nDensity Density Density\n1 1 1\n0.8 0.8 0.8\n0.6 0.6 0.6\n0 0.20.40.60.8 00.20.4 0 0.20.40.60.8 00.20.4 0 0.20.40.60.8 00.20.4\n(a) (b) (c)\nFigure 20.8 Density estimation using k-nearest-neighbors, applied to the data in Fig-\nure 20.7(b), for k=3, 10, and 40 respectively. k = 3 is too spiky, 40 is too smooth, and\n10isjustaboutright.Thebestvalueforkcanbechosenbycross-validation.\nDensity Density Density\n1 1 1\n0.8 0.8 0.8\n0.6 0.6 0.6\n0 0.20.40.60.8 00.20.4 0 0.20.40.60.8 00.20.4 0 0.20.40.60.8 00.20.4\n(a) (b) (c)\nFigure20.9 KerneldensityestimationforthedatainFigure20.7(b),usingGaussianker-\nnelswithw=0.02,0.07,and0.20respectively.w=0.07isaboutright. 816 Chapter 20. LearningProbabilistic Models\nAnother possibility is to use kernel functions, as we did for locally weighted regres-\nsion. Toapplyakernelmodeltodensityestimation,assumethateachdatapointgeneratesits\nownlittledensityfunction, usingaGaussiankernel. Theestimateddensityataquerypointx\nisthentheaverage densityasgivenbyeachkernelfunction:\n(cid:12)N\n1\nP(x) = K(x,x ).\nj\nN\nj=1\nWewillassumespherical Gaussianswithstandard deviation w alongeachaxis:\nK(x,x j) =\n\u221a1 e\u2212D( 2x w,x 2j)2\n,\n(w2 2\u03c0)d\nwhere d is the number of dimensions in x and D is the Euclidean distance function. We\nstill have the problem of choosing a suitable value for kernel width w; Figure 20.9 shows\nvaluesthataretoosmall,justright, andtoolarge. Agoodvalueofwcanbechosenbyusing\ncross-validation.\n20.3 LEARNING WITH HIDDEN VARIABLES: THE EM ALGORITHM\nThe preceding section dealt with the fully observable case. Many real-world problems have\nhiddenvariables (sometimes called latent variables), which are not observable in the data\nLATENTVARIABLE\nthat are available for learning. For example, medical records often include the observed\nsymptoms, the physician\u2019s diagnosis, the treatment applied, and perhaps the outcome of the\ntreatment, but they seldom contain a direct observation of the disease itself! (Note that the\ndiagnosisisnotthedisease;itisacausalconsequenceoftheobservedsymptoms,whicharein\nturncausedbythedisease.) Onemightask,\u201cIfthediseaseisnotobserved,whynotconstruct\na model without it?\u201d The answer appears in Figure 20.10, which shows a small, fictitious\ndiagnostic modelforheartdisease. Therearethreeobservablepredisposing factorsandthree\nobservable symptoms (which are too depressing to name). Assume that each variable has\nthree possible values (e.g., none, moderate, and severe). Removing the hidden variable\nfrom the network in (a) yields the network in (b); the total number of parameters increases\nfrom 78 to 708. Thus, latent variables can dramatically reduce the number of parameters\nrequired tospecify aBayesiannetwork. This,inturn,candramatically reduce theamountof\ndataneededtolearntheparameters.\nHidden variables are important, but they do complicate the learning problem. In Fig-\nure 20.10(a), for example, it is not obvious how to learn the conditional distribution for\nHeartDisease,givenitsparents, becausewedonotknowthevalueofHeartDisease ineach\ncase; the same problem arises in learning the distributions for the symptoms. This section\nEXPECTATION\u2013 describes an algorithm called expectation\u2013maximization, or EM, that solves this problem\nMAXIMIZATION\nin avery general way. Wewill show three examples and then provide a general description.\nThe algorithm seems like magic at first, but once the intuition has been developed, one can\nfindapplications forEMinahugerangeoflearning problems. Section20.3. LearningwithHiddenVariables: TheEMAlgorithm 817\n2 2 2 2 2 2\nSmoking Diet Exercise Smoking Diet Exercise\n54 HeartDisease\n6 6 6 54 162 486\nSymptom Symptom Symptom Symptom Symptom Symptom\n1 2 3 1 2 3\n(a) (b)\nFigure20.10 (a)Asimple diagnosticnetworkforheartdisease, whichisassumedtobe\na hidden variable. Each variable has three possible values and is labeled with the number\nof independent parameters in its conditional distribution; the total number is 78. (b) The\nequivalent network with HeartDisease removed. Note that the symptom variables are no\nlongerconditionallyindependentgiventheirparents.Thisnetworkrequires708parameters.\n20.3.1 Unsupervised clustering: Learning mixtures ofGaussians\nUNSUPERVISED Unsupervised clustering is the problem of discerning multiple categories in a collection of\nCLUSTERING\nobjects. Theproblemisunsupervisedbecausethecategorylabelsarenotgiven. Forexample,\nsuppose we record the spectra of a hundred thousand stars; are there different types of stars\nrevealed by the spectra, and, if so, how many types and what are their characteristics? We\nare all familiar with terms such as \u201cred giant\u201d and \u201cwhite dwarf,\u201d but the stars do not carry\nthese labels on their hats\u2014astronomers had to perform unsupervised clustering to identify\nthese categories. Other examples include the identification of species, genera, orders, and\nso on in the Linn\u00e6an taxonomy and the creation of natural kinds for ordinary objects (see\nChapter12).\nUnsupervised clustering beginswithdata. Figure20.11(b) shows500datapoints,each\nofwhichspecifies thevalues oftwocontinuous attributes. Thedatapoints mightcorrespond\nto stars, and the attributes might correspond to spectral intensities at two particular frequen-\ncies. Next,weneedtounderstandwhatkindofprobability distribution mighthavegenerated\nMIXTURE the data. Clustering presumes that the data are generated from a mixture distribution, P.\nDISTRIBUTION\nSuch a distribution has k components, each of which is a distribution in its own right. A\nCOMPONENT\ndatapointisgeneratedbyfirstchoosingacomponentandthengenerating asamplefromthat\ncomponent. Lettherandom variable C denotethecomponent, withvalues1,...,k;thenthe\nmixturedistribution isgivenby\n(cid:12)k\nP(x) = P(C=i) P(x|C=i),\ni=1\nwhere x refers to the values of the attributes for a data point. Forcontinuous data, a natural\nchoiceforthecomponentdistributionsisthemultivariate Gaussian,whichgivestheso-called\nMIXTUREOF mixtureofGaussiansfamilyofdistributions. Theparameters ofamixtureofGaussians are\nGAUSSIANS 818 Chapter 20. LearningProbabilistic Models\n1 1 1\n0.8 0.8 0.8\n0.6 0.6 0.6\n0.4 0.4 0.4\n0.2 0.2 0.2\n0 0 0\n0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1\n(a) (b) (c)\nFigure20.11 (a)AGaussianmixturemodelwiththreecomponents;theweights(left-to-\nright)are0.2,0.3,and0.5.(b)500datapointssampledfromthemodelin(a).(c)Themodel\nreconstructedbyEMfromthedatain(b).\nw =P(C=i) (the weight of each component), \u03bc (the mean of each component), and \u03a3\ni i i\n(the covariance of each component). Figure 20.11(a) shows a mixture of three Gaussians;\nthis mixture is in fact the source of the data in (b) as well as being the model shown in\nFigure20.7(a)onpage815.\nThe unsupervised clustering problem, then, is to recover a mixture model like the one\ninFigure20.11(a)fromrawdatalikethatinFigure20.11(b). Clearly,ifweknewwhichcom-\nponentgenerated eachdatapoint,thenitwouldbeeasytorecoverthecomponentGaussians:\nwecouldjustselectallthedatapointsfromagivencomponentandthenapply(amultivariate\nversionof)Equation(20.4)(page809)forfittingtheparametersofaGaussiantoasetofdata.\nOn the other hand, if we knew the parameters of each component, then we could, at least in\na probabilistic sense, assign each data point to a component. The problem is that we know\nneithertheassignments northeparameters.\nThe basic idea of EM in this context is to pretend that we know the parameters of the\nmodelandthentoinfertheprobabilitythateachdatapointbelongstoeachcomponent. After\nthat,werefitthecomponentstothedata,whereeachcomponentisfittedtotheentiredataset\nwith each point weighted by the probability that it belongs to that component. The process\niterates until convergence. Essentially, weare\u201ccompleting\u201d thedata byinferring probability\ndistributionsoverthehiddenvariables\u2014whichcomponenteachdatapointbelongsto\u2014based\nonthecurrentmodel. ForthemixtureofGaussians, weinitialize themixture-model parame-\ntersarbitrarily andtheniteratethefollowingtwosteps:\n1. E-step: Compute the probabilities p =P(C=i|x ), the probability that datum x\nij j j\nwasgeneratedbycomponenti. ByBayes\u2019rule,wehavep =\u03b1P(x |C=i)P(C=i).\nij j\nThe term P(x |C=i) is just the probability at x of the ith Gaussian, and the term\nj j (cid:2)\nP(C=i) is just the weight parameter for the ith Gaussian. Define n = p , the\ni j ij\neffectivenumberofdatapointscurrently assigned tocomponenti.\n2. M-step: Computethenewmean,covariance, andcomponentweightsusingthefollow-\ningstepsinsequence: Section20.3. LearningwithHiddenVariables: TheEMAlgorithm 819\n(cid:12)\n\u03bc \u2190 p x \/n\ni ij j i\nj\n(cid:12)\n\u03a3 \u2190 p (x \u2212\u03bc )(x \u2212\u03bc )(cid:12) \/n\ni ij j i j i i\nj\nw \u2190 n \/N\ni i\nwhere N is the total number of data points. The E-step, or expectation step, can be viewed\nascomputingtheexpectedvaluesp ofthehiddenindicatorvariablesZ ,whereZ is1if\nINDICATORVARIABLE ij ij ij\ndatumx wasgeneratedbytheithcomponentand0otherwise. TheM-step,ormaximization\nj\nstep, finds the new values of the parameters that maximize the log likelihood of the data,\ngiventheexpectedvaluesofthehidden indicatorvariables.\nThefinalmodelthatEMlearnswhenitisappliedtothedatainFigure20.11(a)isshown\nin Figure 20.11(c); it is virtually indistinguishable from the original model from which the\ndata were generated. Figure 20.12(a) plots the log likelihood of the data according to the\ncurrentmodelasEMprogresses.\nThere are two points to notice. First, the log likelihood for the final learned model\nslightly exceeds that of the original model, from which the data were generated. This might\nseem surprising, but it simply reflects the fact that the data were generated randomly and\nmight not provide an exact reflection of the underlying model. The second point is that EM\nincreases theloglikelihood ofthedataateveryiteration. Thisfactcanbeprovedingeneral.\nFurthermore, under certain conditions (that hold in ost cases), EM can be proven to reach\na local maximum in likelihood. (In rare cases, it could reach a saddle point or even a local\nminimum.) Inthis sense, EMresembles agradient-based hill-climbing algorithm, butnotice\nthatithasno\u201cstepsize\u201dparameter.\n700 -1975\n600 -1980\n500 -1985\n400 -1990\n300 -1995\n200 -2000\n-2005\n100\n-2010\n0\n-2015\n-100\n-2020\n-200\n-2025\n0 5 10 15 20 0 20 40 60 80 100 120\nIteration number Iteration number\n(a) (b)\nFigure20.12 Graphsshowingtheloglikelihoodofthedata, L, asafunctionoftheEM\niteration.Thehorizontallineshowstheloglikelihoodaccordingtothetruemodel.(a)Graph\nfor the Gaussian mixture model in Figure 20.11. (b) Graph for the Bayesian network in\nFigure20.13(a).\nLdoohilekil-goL Ldoohilekil-goL 820 Chapter 20. LearningProbabilistic Models\nP(Bag=1)\n\u03b8\nBag C\nBag P(F=cherry| B)\n\u03b8\n1 F1\n\u03b8\n2 F2\nFlavor Wrapper Hole X\n(a) (b)\nFigure20.13 (a)Amixturemodelforcandy. Theproportionsofdifferentflavors,wrap-\npers,presenceofholesdependonthebag,whichisnotobserved. (b)Bayesiannetworkfor\naGaussianmixture. ThemeanandcovarianceoftheobservablevariablesXdependonthe\ncomponentC.\nThings do not always go as well as Figure 20.12(a) might suggest. It can happen, for\nexample,thatoneGaussiancomponentshrinkssothatitcoversjustasingledatapoint. Then\nits variance will go to zero and its likelihood will go to infinity! Another problem is that\ntwocomponents can\u201cmerge,\u201dacquiringidenticalmeansandvariancesandsharingtheirdata\npoints. These kinds of degenerate local maxima are serious problems, especially in high\ndimensions. One solution is to place priors on the model parameters and to apply the MAP\nversion of EM. Another is to restart a component with new random parameters if it gets too\nsmallortooclosetoanothercomponent. Sensibleinitialization alsohelps.\n20.3.2 Learning Bayesiannetworks withhidden variables\nTo learn a Bayesian network with hidden variables, we apply the same insights that worked\nformixturesofGaussians. Figure20.13represents asituation inwhichtherearetwobagsof\ncandies that have been mixed together. Candies are described by three features: in addition\nto the Flavor and the Wrapper, some candies have a Hole in the middle and some do not.\nThe distribution of candies in each bag is described by a naive Bayes model: the features\nare independent, given the bag, but the conditional probability distribution for each feature\ndepends on the bag. The parameters are as follows: \u03b8 is the prior probability that a candy\ncomes from Bag 1; \u03b8 and \u03b8 are the probabilities that the flavor is cherry, given that the\nF1 F2\ncandy comes from Bag 1orBag 2 respectively; \u03b8 and \u03b8 give the probabilities that the\nW1 W2\nwrapperisred;and \u03b8 and\u03b8 givetheprobabilities thatthecandy hasahole. Noticethat\nH1 H2\nthe overall model is a mixture model. (In fact, we can also model the mixture of Gaussians\nas a Bayesian network, as shown in Figure 20.13(b).) In the figure, the bag is a hidden\nvariable because, once thecandies have been mixedtogether, weno longer know which bag\neach candy came from. In such a case, can we recover the descriptions of the two bags by Section20.3. LearningwithHiddenVariables: TheEMAlgorithm 821\nobservingcandiesfromthemixture? LetusworkthroughaniterationofEMforthisproblem.\nFirst,let\u2019slookatthedata. Wegenerated 1000samplesfrom amodelwhosetrueparameters\nareasfollows:\n\u03b8=0.5, \u03b8 =\u03b8 =\u03b8 =0.8, \u03b8 =\u03b8 =\u03b8 =0.3. (20.7)\nF1 W1 H1 F2 W2 H2\nThat is, the candies are equally likely to come from either bag; the first is mostly cherries\nwith red wrappers and holes; the second is mostly limes with green wrappers and no holes.\nThecountsfortheeightpossible kindsofcandyareasfollows:\nW =red W =green\nH=1 H=0 H=1 H=0\nF =cherry 273 93 104 90\nF =lime 79 100 94 167\nWestartbyinitializing theparameters. Fornumerical simplicity, wearbitrarily choose5\n\u03b8(0)=0.6, \u03b8(0) =\u03b8(0) =\u03b8(0) =0.6, \u03b8(0) =\u03b8(0) =\u03b8(0) =0.4. (20.8)\nF1 W1 H1 F2 W2 H2\nFirst, let us work on the \u03b8 parameter. In the fully observable case, we would estimate this\ndirectly fromthe observed counts ofcandies frombags1and2. Because thebagisahidden\nvariable, we calculate the expected counts instead. The expected count N\u02c6(Bag=1) is the\nsum,overallcandies, oftheprobability thatthecandycame frombag1:\n(cid:12)N\n\u03b8(1) = N\u02c6(Bag=1)\/N = P(Bag=1|flavor ,wrapper ,holes )\/N .\nj j j\nj=1\nTheseprobabilities canbecomputed byanyinference algorithm forBayesian networks. For\na naive Bayes model such as the one in our example, we can do the inference \u201cby hand,\u201d\nusingBayes\u2019ruleandapplying conditional independence:\n(cid:12)N\n\u03b8(1) =\n1 (cid:2)P(flavorj|Bag=1)P(wrapperj|Bag=1)P(holesj|Bag=1)P(Bag=1)\n.\nN\nj=1\niP(flavorj|Bag=i)P(wrapperj|Bag=i)P(holesj|Bag=i)P(Bag=i)\nApplying thisformula to, say, the273 red-wrapped cherry candies withholes, weget acon-\ntribution of\n273 \u03b8(0) \u03b8(0) \u03b8(0) \u03b8(0)\n\u00b7 F1 W1 H1 \u2248 0.22797.\n1000 \u03b8(0) \u03b8(0) \u03b8(0) \u03b8(0)+\u03b8(0) \u03b8(0) \u03b8(0) (1\u2212\u03b8(0))\nF1 W1 H1 F2 W2 H2\nContinuingwiththeothersevenkindsofcandyinthetableofcounts,weobtain\u03b8(1)=0.6124.\nNowletusconsidertheotherparameters, suchas \u03b8 . Inthefullyobservable case,we\nF1\nwouldestimatethisdirectlyfromtheobservedcountsofcherryandlimecandiesfrombag1.\nTheexpected countofcherrycandiesfrombag1isgivenby\n(cid:12)\nP(Bag=1|Flavor =cherry,wrapper ,holes ).\nj j j\nj:Flavorj=cherry\n5 Itisbetterinpracticetochoosethemrandomly,toavoidlocalmaximaduetosymmetry. 822 Chapter 20. LearningProbabilistic Models\nAgain, these probabilities can be calculated by any Bayes net algorithm. Completing this\nprocess, weobtainthenewvaluesofalltheparameters:\n\u03b8(1)=0.6124, \u03b8(1) =0.6684, \u03b8(1) =0.6483, \u03b8(1) =0.6558,\nF1 W1 H1 (20.9)\n(1) (1) (1)\n\u03b8 =0.3887,\u03b8 =0.3817, \u03b8 =0.3827.\nF2 W2 H2\nThe log likelihood of the data increases from about \u22122044 initially to about \u22122021 after\nthe first iteration, as shown in Figure 20.12(b). That is, the update improves the likelihood\nitself by a factor of about e23 \u2248 1010. By the tenth iteration, the learned model is a better\nfitthantheoriginal model(L= \u22121982.214). Thereafter, progress becomes veryslow. This\nis not uncommon with EM, and many practical systems combine EM with a gradient-based\nalgorithm suchasNewton\u2013Raphson (seeChapter4)forthelastphaseoflearning.\nThe general lesson from this example is that the parameter updates for Bayesian net-\nwork learning with hidden variables are directly available from the results of inference on\neach example. Moreover, only local posterior probabilities are needed for each parame-\nter. Here, \u201clocal\u201d means that the CPT for each variable X can be learned from posterior\ni\nprobabilities involving just X and its parents U . Defining \u03b8 to be the CPT parameter\ni i ijk\nP(X =x |U =u ),theupdateisgivenbythenormalizedexpected countsasfollows:\ni ij i ik\n\u03b8 \u2190 N\u02c6(X =x ,U =u )\/N\u02c6(U =u ).\nijk i ij i ik i ik\nTheexpectedcountsareobtainedbysummingovertheexamples,computingtheprobabilities\nP(X =x ,U =u ) for each by using any Bayes net inference algorithm. For the exact\ni ij i ik\nalgorithms\u2014including variableelimination\u2014alltheseprobabilities areobtainabledirectlyas\naby-product ofstandard inference, withnoneed forextracomputations specific tolearning.\nMoreover, theinformation needed forlearning isavailable locallyforeachparameter.\n20.3.3 Learning hidden Markovmodels\nOur final application of EM involves learning the transition probabilities in hidden Markov\nmodels (HMMs). Recall from Section 15.3 that a hidden Markov model can be represented\nby a dynamic Bayes net with a single discrete state variable, as illustrated in Figure 20.14.\nEach data point consists of an observation sequence of finite length, so the problem is to\nlearn the transition probabilities from a set of observation sequences (or from just one long\nsequence).\nWe have already worked out how to learn Bayes nets, but there is one complication:\nin Bayes nets, each parameter is distinct; in a hidden Markov model, on the other hand, the\nindividualtransitionprobabilitiesfromstateitostatejattimet,\u03b8 =P(X =j|X =i),\nijt t+1 t\nare repeated across time\u2014that is, \u03b8 =\u03b8 for all t. To estimate the transition probability\nijt ij\nfrom state i to state j, we simply calculate the expected proportion of times that the system\nundergoes atransition tostate j wheninstatei:\n(cid:12) (cid:12)\n\u03b8 \u2190 N\u02c6(X =j,X =i)\/ N\u02c6(X =i).\nij t+1 t t\nt t\nTheexpectedcountsarecomputedbyanHMMinferencealgorithm. Theforward\u2013backward\nalgorithm shown in Figure 15.4 can be modified very easily to compute the necessary prob-\nabilities. One important point is that the probabilities required are obtained by smoothing Section20.3. LearningwithHiddenVariables: TheEMAlgorithm 823\nP(R 0) R t0 P 0(R .71) P(R 0) R t0 P 0(R .71) R t1 P 0(R .72) R t2 P 0(R .73 ) R t3 P 0(R .74 )\n0.7 f 0.3 0.7 f 0.3 f 0.3 f 0.3 f 0.3\nRain Rain Rain Rain Rain Rain Rain\n0 1 0 1 2 3 4\nUmbrella 1 Umbrella 1 Umbrella 2 Umbrella 3 Umbrella 4\nR 1 P(U 1) R 1 P(U 1) R 2 P(U 2) R 3 P(U 3) R 4 P(U 4)\nt 0.9 t 0.9 t 0.9 t 0.9 t 0.9\nf 0.2 f 0.2 f 0.2 f 0.2 f 0.2\nFigure 20.14 An unrolled dynamic Bayesian network that represents a hidden Markov\nmodel(repeatofFigure15.16).\nrather than filtering; that is, we need to pay attention to subsequent evidence in estimating\ntheprobability thataparticulartransition occurred. The evidence inamurdercaseisusually\nobtained afterthecrime(i.e.,thetransition fromstate itostatej)hastakenplace.\n20.3.4 The general form oftheEM algorithm\nWe have seen several instances of the EM algorithm. Each involves computing expected\nvalues ofhidden variables foreach example and then recomputing the parameters, using the\nexpected values as if they were observed values. Let x be all the observed values in all the\nexamples, let Z denote all the hidden variables for all the examples, and let \u03b8 be all the\nparameters fortheprobability model. ThentheEMalgorithm is\n(cid:12)\n\u03b8(i+1) = argmax P(Z=z|x,\u03b8(i))L(x,Z=z|\u03b8).\n\u03b8\nz\nThisequationistheEMalgorithminanutshell. TheE-stepisthecomputationofthesumma-\ntion,whichistheexpectationoftheloglikelihoodofthe\u201ccompleted\u201ddatawithrespecttothe\ndistributionP(Z=z|x,\u03b8(i)),whichistheposterioroverthehiddenvariables,giventhedata.\nThe M-step is the maximization of this expected log likelihood with respect to the parame-\nters. FormixturesofGaussians,thehiddenvariablesaretheZ s,whereZ is1ifexamplej\nij ij\nwasgeneratedbycomponent i. ForBayesnets,Z isthevalueofunobserved variable X in\nij i\nexamplej. ForHMMs,Z isthestateofthesequence inexamplej attimet. Startingfrom\njt\nthe general form, itis possible toderive anEM algorithm for aspecific application once the\nappropriate hiddenvariables havebeenidentified.\nAs soon as we understand the general idea of EM, it becomes easy to derive all sorts\nof variants and improvements. For example, in many cases the E-step\u2014the computation of\nposteriors over the hidden variables\u2014is intractable, as in large Bayes nets. It turns out that\none can use an approximate E-step and still obtain an effective learning algorithm. With a\nsamplingalgorithm suchasMCMC(seeSection14.5),thelearning processisveryintuitive:\neach state (configuration of hidden and observed variables) visited by MCMC is treated ex-\nactlyasifitwereacompleteobservation. Thus,theparameters canbeupdated directly after\neachMCMCtransition. Otherformsofapproximate inference, suchasvariational andloopy\nmethods, havealsoprovedeffectiveforlearning verylarge networks. 824 Chapter 20. LearningProbabilistic Models\n20.3.5 Learning Bayes net structures withhidden variables\nIn Section 20.2.5, we discussed the problem of learning Bayes net structures with complete\ndata. When unobserved variables may be influencing the data that are observed, things get\nmoredifficult. Inthesimplestcase,ahumanexpertmighttellthelearningalgorithmthatcer-\ntainhidden variables exist, leaving ittothealgorithm tofindaplacefortheminthenetwork\nstructure. Forexample,analgorithmmighttrytolearnthestructureshowninFigure20.10(a)\nonpage817,giventheinformationthatHeartDisease (athree-valuedvariable)shouldbein-\ncludedinthemodel. Asinthecomplete-datacase,theoverallalgorithmhasanouterloopthat\nsearchesoverstructuresandaninnerloopthatfitsthenetworkparametersgiventhestructure.\nIf the learning algorithm is not told which hidden variables exist, then there are two\nchoices: either pretend that the data is really complete\u2014which may force the algorithm to\nlearn aparameter-intensive model such astheoneinFigure 20.10(b)\u2014or invent newhidden\nvariablesinordertosimplifythemodel. Thelatterapproachcanbeimplementedbyincluding\nnewmodificationchoicesinthestructuresearch: inadditiontomodifyinglinks,thealgorithm\ncanaddordeleteahiddenvariableorchangeitsarity. Ofcourse,thealgorithmwillnotknow\nthat the new variable it has invented is called HeartDisease; nor will it have meaningful\nnamesforthevalues. Fortunately, newlyinventedhiddenvariableswillusuallybeconnected\ntopreexistingvariables,soahumanexpertcanofteninspectthelocalconditionaldistributions\ninvolving thenewvariableandascertain itsmeaning.\nAsinthecomplete-datacase,puremaximum-likelihoodstructurelearningwillresultin\nacompletely connected network (moreover, one withno hidden variables), so some form of\ncomplexity penaltyisrequired. WecanalsoapplyMCMCtosamplemanypossible network\nstructures, thereby approximating Bayesian learning. For example, wecanlearnmixtures of\nGaussianswithanunknownnumberofcomponentsbysamplingoverthenumber;theapprox-\nimateposteriordistributionforthenumberofGaussiansis givenbythesamplingfrequencies\noftheMCMCprocess.\nFor the complete-data case, the inner loop to learn the parameters is very fast\u2014just a\nmatter of extracting conditional frequencies from the data set. When there are hidden vari-\nables, the inner loop may involve many iterations of EM ora gradient-based algorithm, and\neachiterationinvolvesthecalculationofposteriorsinaBayesnet,whichisitselfanNP-hard\nproblem. To date, this approach has proved impractical for learning complex models. One\npossibleimprovementistheso-called structuralEMalgorithm, whichoperatesinmuchthe\nSTRUCTURALEM\nsame way as ordinary (parametric) EM except that the algorithm can update the structure\nas well as the parameters. Just as ordinary EM uses the current parameters to compute the\nexpected counts in the E-step and then applies those counts in the M-step to choose new\nparameters,structuralEMusesthecurrentstructuretocomputeexpectedcountsandthenap-\npliesthose counts intheM-steptoevaluate thelikelihood forpotential newstructures. (This\ncontrasts with the outer-loop\/inner-loop method, which computes new expected counts for\neach potential structure.) In this way, structural EM may make several structural alterations\ntothenetworkwithoutoncerecomputingtheexpectedcounts,andiscapableoflearningnon-\ntrivial Bayes net structures. Nonetheless, much work remains to be done before we can say\nthatthestructure-learning problem issolved. Section20.4. Summary 825\n20.4 SUMMARY\nStatistical learning methods rangefromsimple calculation ofaverages totheconstruction of\ncomplex models such as Bayesian networks. They have applications throughout computer\nscience, engineering, computational biology, neuroscience, psychology, and physics. This\nchapter has presented some of the basic ideas and given a flavor of the mathematical under-\npinnings. Themainpointsareasfollows:\n\u2022 Bayesian learning methods formulate learning as a form of probabilistic inference,\nusing the observations to update a prior distribution over hypotheses. This approach\nprovidesagoodwaytoimplementOckham\u2019srazor,butquickly becomesintractablefor\ncomplexhypothesis spaces.\n\u2022 Maximum a posteriori (MAP) learning selects a single most likely hypothesis given\nthe data. Thehypothesis prior isstill used and the method is often moretractable than\nfullBayesianlearning.\n\u2022 Maximum-likelihoodlearningsimplyselectsthehypothesisthatmaximizesthelikeli-\nhoodofthedata;itisequivalent toMAPlearningwithauniformprior. Insimplecases\nsuchaslinearregressionandfullyobservableBayesiannetworks,maximum-likelihood\nsolutions can be found easily in closed form. Naive Bayes learning is a particularly\neffectivetechnique thatscaleswell.\n\u2022 When some variables are hidden, local maximum likelihood solutions can be found\nusing the EM algorithm. Applications include clustering using mixtures of Gaussians,\nlearningBayesiannetworks, andlearning hiddenMarkovmodels.\n\u2022 Learning the structure of Bayesian networks is an example of model selection. This\nusually involves a discrete search in the space of structures. Some method is required\nfortrading offmodelcomplexityagainstdegreeoffit.\n\u2022 Nonparametric models represent a distribution using the collection of data points.\nThus,thenumberofparametersgrowswiththetrainingset. Nearest-neighborsmethods\nlook at the examples nearest to the point in question, whereas kernel methods form a\ndistance-weighted combination ofalltheexamples.\nStatisticallearningcontinuestobeaveryactiveareaofresearch. Enormousstrideshavebeen\nmadeinboth theory andpractice, tothepoint whereitispossible tolearn almost anymodel\nforwhichexactorapproximate inference isfeasible.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nThe application of statistical learning techniques in AI was an active area of research in the\nearly years (see Duda and Hart, 1973) but became separated from mainstream AI as the\nlatterfieldconcentratedonsymbolicmethods. Aresurgence ofinterestoccurredshortlyafter\nthe introduction of Bayesian network models in the late 1980s; at roughly the same time, 826 Chapter 20. LearningProbabilistic Models\na statistical view of neural network learning began to emerge. In the late 1990s, there was\na noticeable convergence of interests in machine learning, statistics, and neural networks,\ncentered onmethodsforcreatinglargeprobabilistic modelsfromdata.\nThe naive Bayes model is one of the oldest and simplest forms of Bayesian network,\ndating backtothe 1950s. Itsorigins werementioned inChapter13. Itssurprising success is\npartially explained by Domingos and Pazzani (1997). Aboosted form ofnaive Bayes learn-\ningwonthefirstKDDCupdataminingcompetition (Elkan,1997). Heckerman(1998)gives\nan excellent introduction to the general problem of Bayes net learning. Bayesian parame-\nterlearningwithDirichletpriorsforBayesiannetworkswasdiscussedbySpiegelhalter etal.\n(1993). TheBUGSsoftwarepackage(Gilksetal.,1994)incorporatesmanyoftheseideasand\nprovides averypowerfultoolforformulating andlearning complexprobability models. The\nfirstalgorithms forlearning Bayesnet structures usedconditional independence tests (Pearl,\n1988; Pearl and Verma, 1991). Spirtes et al. (1993) developed a comprehensive approach\nembodied in the TETRAD package forBayes net learning. Algorithmic improvements since\nthen led to a clear victory in the 2001 KDD Cup data mining competition for a Bayes net\nlearning method (Cheng et al., 2002). (The specific task here was a bioinformatics prob-\nlem with 139,351 features!) A structure-learning approach based on maximizing likelihood\nwasdeveloped by Cooperand Herskovits (1992) and improved byHeckerman et al. (1994).\nSeveral algorithmic advances since that time have led to quite respectable performance in\nthe complete-data case (Moore and Wong, 2003; Teyssier and Koller, 2005). Oneimportant\ncomponent is an efficient data structure, the AD-tree, for caching counts over all possible\ncombinations of variables and values (Moore and Lee, 1997). Friedman and Goldszmidt\n(1996)pointedouttheinfluenceoftherepresentation oflocalconditionaldistributions onthe\nlearnedstructure.\nThe general problem of learning probability models with hidden variables and miss-\ning data wasaddressed by Hartley (1958), who described the general idea of what was later\ncalled EM and gave several examples. Further impetus came from the Baum\u2013Welch algo-\nrithm forHMMlearning (BaumandPetrie, 1966), whichisaspecial caseofEM.Thepaper\nby Dempster, Laird, and Rubin (1977), which presented the EM algorithm in general form\nand analyzed its convergence, is one of the most cited papers in both computer science and\nstatistics. (Dempster himself views EM as a schema rather than an algorithm, since a good\ndeal of mathematical work may be required before it can be applied to a new family of dis-\ntributions.) McLachlan and Krishnan (1997) devote an entire book to the algorithm and its\nproperties. The specific problem of learning mixture models, including mixtures of Gaus-\nsians,iscoveredbyTitterington etal.(1985). WithinAI,thefirstsuccessfulsystemthatused\nEMformixture modeling was AUTOCLASS (Cheeseman etal.,1988; Cheeseman andStutz,\n1996). AUTOCLASShasbeenappliedtoanumberofreal-worldscientificclassificationtasks,\nincludingthediscoveryofnewtypesofstarsfromspectraldata(Goebeletal.,1989)andnew\nclassesofproteinsandintronsinDNA\/proteinsequencedatabases(HunterandStates,1992).\nFor maximum-likelihood parameter learning in Bayes nets with hidden variables, EM\nandgradient-basedmethodswereintroducedaroundthesametimebyLauritzen(1995),Rus-\nsell et al. (1995), and Binder et al. (1997a). The structural EM algorithm was developed by\nFriedman (1998) and applied to maximum-likelihood learning of Bayes net structures with Exercises 827\nlatentvariables. FriedmanandKoller(2003). describeBayesianstructure learning.\nTheabilitytolearnthestructureofBayesiannetworksiscloselyconnected totheissue\nof recovering causal information from data. That is, is it possible to learn Bayes nets in\nsuch a way that the recovered network structure indicates real causal influences? For many\nyears,statisticiansavoidedthisquestion,believingthatobservationaldata(asopposedtodata\ngeneratedfromexperimentaltrials)couldyieldonlycorrelational information\u2014after all,any\ntwo variables that appear related might in fact be influenced by a third, unknown causal\nfactor rather than influencing each other directly. Pearl (2000) has presented convincing\narguments to the contrary, showing that there are in fact many cases where causality can be\nascertained and developing the causal network formalism to express causes and the effects\nCAUSALNETWORK\nofintervention aswellasordinary conditional probabilities.\nNonparametric densityestimation, alsocalled Parzenwindowdensity estimation, was\ninvestigated initially byRosenblatt (1956) and Parzen (1962). Since thattime, ahuge litera-\nturehasdevelopedinvestigating theproperties ofvarious estimators. Devroye(1987)givesa\nthorough introduction. There isalsoarapidly growing literature onnonparametric Bayesian\nmethods, originating with the seminal work of Ferguson (1973) on the Dirichlet process,\nDIRICHLETPROCESS\nwhichcanbethoughtofasadistribution overDirichletdistributions. Thesemethodsarepar-\nticularlyusefulformixtureswithunknownnumbersofcomponents. Ghahramani(2005)and\nJordan (2005) provide useful tutorials on the many applications of these ideas to statistical\nlearning. The text by Rasmussen and Williams (2006) covers the Gaussian process, which\nGAUSSIANPROCESS\ngivesawayofdefiningpriordistributions overthespaceofcontinuous functions.\nThematerialinthischapterbringstogetherworkfromthefieldsofstatisticsandpattern\nrecognition, so the story has been told many times in many ways. Good texts on Bayesian\nstatistics include thosebyDeGroot(1970), Berger(1985), andGelmanetal.(1995). Bishop\n(2007)andHastieetal.(2009)provideanexcellentintroduction tostatistical machinelearn-\ning. Forpatternclassification, theclassictextformanyyearshasbeenDudaandHart(1973),\nnow updated (Duda etal., 2001). Theannual NIPS(Neural Information Processing Confer-\nence)conference,whoseproceedingsarepublishedastheseriesAdvancesinNeuralInforma-\ntionProcessingSystems,isnowdominatedbyBayesianpapers. PapersonlearningBayesian\nnetworksalsoappearintheUncertaintyinAIandMachineLearningconferences andinsev-\neralstatistics conferences. Journalsspecifictoneuralnetworksinclude NeuralComputation,\nNeural Networks, and the IEEE Transactions on Neural Networks. Specifically Bayesian\nvenues include the Valencia International Meetings on Bayesian Statistics and the journal\nBayesianAnalysis.\nEXERCISES\n20.1 The data used for Figure 20.1 on page 804 can be viewed as being generated by h .\n5\nFor each of the other four hypotheses, generate a data set of length 100 and plot the cor-\nresponding graphs for P(h |d ,...,d ) and P(D =lime|d ,...,d ). Comment on\ni 1 N N+1 1 N\nyourresults. 828 Chapter 20. LearningProbabilistic Models\n20.2 SupposethatAnn\u2019sutilitiesforcherryandlimecandiesare c and(cid:3) ,whereasBob\u2019s\nA A\nutilities are c and (cid:3) . (But once Ann has unwrapped a piece of candy, Bob won\u2019t buy\nB B\nit.) Presumably, if Bob likes lime candies much more than Ann, it would be wise for Ann\ntosellherbagofcandies oncesheissufficiently sureofitslimecontent. Ontheotherhand,\nif Ann unwraps too many candies in the process, the bag will be worth less. Discuss the\nproblem of determining the optimal point at which to sell the bag. Determine the expected\nutilityoftheoptimalprocedure, giventhepriordistribution fromSection20.1.\n20.3 Two statisticians go to the doctor and are both given the same prognosis: A 40%\nchance that the problem is the deadly disease A, and a 60% chance of the fatal disease B.\nFortunately, thereareanti-A andanti-B drugs thatareinexpensive, 100% effective, and free\nof side-effects. The statisticians have the choice of taking one drug, both, or neither. What\nwillthefirststatistician(anavidBayesian)do? Howaboutthesecondstatistician,whoalways\nusesthemaximumlikelihood hypothesis?\nThe doctor does some research and discovers that disease B actually comes in two\nversions, dextro-B and levo-B, which are equally likely and equally treatable by the anti-B\ndrug. Nowthattherearethreehypotheses, whatwillthetwostatisticians do?\n20.4 ExplainhowtoapplytheboostingmethodofChapter18tonaiveBayeslearning. Test\ntheperformance oftheresulting algorithm ontherestaurant learning problem.\n20.5 ConsiderN datapoints(x ,y ),wherethey saregeneratedfromthex saccordingto\nj j j j\nthelinearGaussianmodelinEquation(20.5). Findthevaluesof\u03b8 ,\u03b8 ,and\u03c3 thatmaximize\n1 2\ntheconditional loglikelihood ofthedata.\n20.6 Consider the noisy-OR model for fever described in Section 14.3. Explain how to\napplymaximum-likelihoodlearningtofittheparametersofsuchamodeltoasetofcomplete\ndata. (Hint: usethechainruleforpartialderivatives.)\n20.7 ThisexerciseinvestigatespropertiesoftheBetadistributiondefinedinEquation(20.6).\na. By integrating over the range [0,1], show that the normalization constant for the dis-\ntribution beta[a,b] is given by \u03b1 = \u0393(a + b)\/\u0393(a)\u0393(b) where \u0393(x) is the Gamma\nfunction,definedby\u0393(x+1)=x\u00b7\u0393(x)and\u0393(1)=1. (Forinteger x,\u0393(x+1)=x!.)\nGAMMAFUNCTION\nb. Showthatthemeanisa\/(a+b).\nc. Findthemode(s)(themostlikelyvalue(s)of\u03b8).\nd. Describethedistributionbeta[(cid:2),(cid:2)]forverysmall(cid:2). Whathappensassuchadistribution\nisupdated?\n20.8 Consideranarbitrary Bayesian network, acomplete dataset forthatnetwork, andthe\nlikelihood for the data set according to the network. Give a simple proof that the likelihood\nofthedatacannotdecreaseifweaddanewlinktothenetworkandrecomputethemaximum-\nlikelihood parametervalues.\n20.9 Consider a single Boolean random variable Y (the \u201cclassification\u201d). Let the prior\nprobability P(Y =true)be\u03c0. Let\u2019strytofind\u03c0,givenatrainingsetD=(y ,...,y )with\n1 N\nN independent samples ofY. Furthermore, suppose p of the N are positive and n of the N\narenegative. Exercises 829\na. Write down an expression for the likelihood of D (i.e., the probability of seeing this\nparticularsequence ofexamples,givenafixedvalueof \u03c0)intermsof\u03c0,p,andn.\nb. BydifferentiatingtheloglikelihoodL,findthevalueof\u03c0thatmaximizesthelikelihood.\nc. Nowsuppose weaddink Booleanrandomvariables X ,X ,...,X (the\u201cattributes\u201d)\n1 2 k\nthat describe each sample, and suppose weassume that the attributes are conditionally\nindependent ofeach other given the goal Y. Draw theBayes net corresponding to this\nassumption.\nd. Write down the likelihood for the data including the attributes, using the following\nadditional notation:\n\u2022 \u03b1 isP(X =true|Y =true).\ni i\n\u2022 \u03b2 isP(X =true|Y =false).\ni i\n\u2022 p+ isthecountofsamplesforwhichX =trueandY =true.\ni i\n\u2022 n+ isthecountofsamplesforwhichX =falseandY =true.\ni i\n\u2022 p\u2212 isthecountofsamplesforwhichX =trueandY =false.\ni i\n\u2022 n\u2212 isthecountofsamplesforwhichX =falseandY =false.\ni i\n[Hint: considerfirsttheprobability ofseeingasingleexamplewithspecifiedvaluesfor\nX ,X ,...,X andY.]\n1 2 k\ne. Bydifferentiating theloglikelihood L,findthevaluesof\u03b1 and\u03b2 (intermsofthevar-\ni i\niouscounts)thatmaximizethelikelihoodandsayinwordswhatthesevaluesrepresent.\nf. Letk = 2,andconsideradatasetwith4allfourpossibleexamplesoftheXORfunction.\nComputethemaximumlikelihood estimates of\u03c0,\u03b1 ,\u03b1 ,\u03b2 ,and\u03b2 .\n1 2 1 2\ng. Given these estimates of \u03c0, \u03b1 , \u03b1 , \u03b2 , and \u03b2 , what are the posterior probabilities\n1 2 1 2\nP(Y =true|x ,x )foreachexample?\n1 2\n20.10 Consider the application of EM to learn the parameters for the network in Fig-\nure20.13(a), giventhetrueparametersinEquation(20.7).\na. Explain why the EM algorithm would not work if there were just two attributes in the\nmodelratherthanthree.\nb. Showthecalculations forthefirstiterationofEMstarting fromEquation(20.8).\nc. What happens if we start with all the parameters set to the same value p? (Hint: you\nmayfindithelpful toinvestigate thisempirically beforederivingthegeneral result.)\nd. Writeoutanexpressionfortheloglikelihoodofthetabulatedcandydataonpage821in\ntermsoftheparameters,calculatethepartialderivatives withrespecttoeachparameter,\nandinvestigate thenatureofthefixedpointreachedinpart(c). 21\nREINFORCEMENT\nLEARNING\nIn which we examine how an agent can learn from success and failure, from re-\nwardandpunishment.\n21.1 INTRODUCTION\nChapters18,19,and20coveredmethodsthatlearnfunctions,logicaltheories,andprobability\nmodelsfrom examples. Inthischapter, wewillstudyhowagents canlearn whattodointhe\nabsenceoflabeledexamplesofwhattodo.\nConsider, for example, the problem of learning to play chess. A supervised learning\nagent needs tobe told the correct moveforeach position it encounters, but such feedback is\nseldom available. In the absence of feedback from a teacher, an agent can learn a transition\nmodelforitsownmovesandcan perhaps learn topredict theopponent\u2019s moves, butwithout\nsomefeedbackaboutwhatisgoodandwhatisbad,theagentwillhavenogroundsfordecid-\ning which moveto make. Theagent needs toknow that something good has happened when\nit (accidentally) checkmates the opponent, and that something bad has happened when it is\ncheckmated\u2014or vice versa, if the game is suicide chess. This kind of feedback is called a\nreward,orreinforcement. Ingameslikechess,thereinforcementisreceivedonlyattheend\nREINFORCEMENT\nof the game. In other environments, the rewards come more frequently. In ping-pong, each\npoint scored can be considered a reward; when learning to crawl, any forward motion is an\nachievement. Our framework for agents regards the reward as part of the input percept, but\nthe agent must be \u201chardwired\u201d to recognize that part as a reward rather than as just another\nsensory input. Thus, animalsseemtobehardwired torecognize painandhungerasnegative\nrewards and pleasure andfood intake aspositive rewards. Reinforcement has been carefully\nstudiedbyanimalpsychologists forover60years.\nRewards were introduced in Chapter 17, where they served to define optimal policies\nin Markov decision processes (MDPs). An optimal policy is a policy that maximizes the\nexpectedtotalreward. Thetaskofreinforcementlearningistouseobservedrewardstolearn\nan optimal (ornearly optimal) policy for the environment. Whereas in Chapter 17 the agent\nhasacompletemodeloftheenvironment andknowstherewardfunction,hereweassumeno\n830 Section21.1. Introduction 831\npriorknowledge ofeither. Imagineplaying anew gamewhoserules youdon\u2019t know; aftera\nhundred orso moves, your opponent announces, \u201cYou lose.\u201d This is reinforcement learning\ninanutshell.\nIn many complex domains, reinforcement learning is the only feasible way to train a\nprogram toperform athighlevels. Forexample,ingameplaying, itisveryhardforahuman\ntoprovideaccurateandconsistentevaluationsoflargenumbersofpositions, whichwouldbe\nneeded to train an evaluation function directly from examples. Instead, the program can be\ntold when it has won or lost, and it can use this information to learn an evaluation function\nthatgivesreasonablyaccurateestimatesoftheprobabilityofwinningfromanygivenposition.\nSimilarly,itisextremelydifficulttoprogramanagenttoflyahelicopter;yetgivenappropriate\nnegativerewardsforcrashing, wobbling,ordeviatingfrom asetcourse,anagentcanlearnto\nflybyitself.\nReinforcementlearningmightbeconsidered toencompassallofAI:anagentisplaced\nin an environment and must learn to behave successfully therein. To keep the chapter man-\nageable, wewillconcentrate onsimpleenvironments andsimpleagentdesigns. Forthemost\npart, we will assume a fully observable environment, so that the current state is supplied by\neach percept. On the other hand, we will assume that the agent does not know how the en-\nvironment worksorwhatitsactions do,andwewillallow forprobabilistic action outcomes.\nThus, the agent faces an unknown Markov decision process. We will consider three of the\nagentdesignsfirstintroduced inChapter2:\n\u2022 Autility-basedagentlearnsautilityfunctiononstatesandusesittoselectactionsthat\nmaximizetheexpectedoutcomeutility.\n\u2022 A Q-learning agent learns an action-utility function, or Q-function, giving the ex-\nQ-LEARNING\npectedutilityoftakingagivenactioninagivenstate.\nQ-FUNCTION\n\u2022 Areflexagentlearnsapolicythatmapsdirectly fromstatestoactions.\nAutility-based agentmustalsohaveamodel oftheenvironment inordertomakedecisions,\nbecause itmustknowthestates towhichitsactions willlead. Forexample, inordertomake\nuseofabackgammonevaluationfunction, abackgammonprogrammustknowwhatitslegal\nmoves are and how they affect the board position. Only in this way can it apply the utility\nfunction to the outcome states. A Q-learning agent, on the other hand, can compare the\nexpectedutilitiesforitsavailable choiceswithoutneedingtoknowtheiroutcomes, soitdoes\nnot need a model of the environment. On the other hand, because they do not know where\ntheiractionslead,Q-learningagentscannotlookahead;thiscanseriouslyrestricttheirability\ntolearn, asweshallsee.\nWebegin in Section 21.2 with passive learning, where the agent\u2019s policy is fixed and\nPASSIVELEARNING\nthetaskistolearntheutilitiesofstates(orstate\u2013action pairs);thiscouldalsoinvolvelearning\namodeloftheenvironment. Section21.3covers active learning, wheretheagent mustalso\nACTIVELEARNING\nlearn what to do. The principal issue is exploration: an agent must experience as much as\nEXPLORATION\npossible ofitsenvironment inordertolearn howtobehave in it. Section 21.4discusses how\nan agent can use inductive learning to learn much faster from its experiences. Section 21.5\ncovers methods forlearning direct policy representations in reflexagents. Anunderstanding\nofMarkovdecisionprocesses (Chapter17)isessentialforthischapter. 832 Chapter 21. Reinforcement Learning\n21.2 PASSIVE REINFORCEMENT LEARNING\nTokeep things simple, westart with the case of a passive learning agent using a state-based\nrepresentation in a fully observable environment. In passive learning, the agent\u2019s policy \u03c0\nis fixed: in state s, it always executes the action \u03c0(s). Its goal is simply to learn how good\nthe policy is\u2014that is, to learn the utility function U\u03c0(s). We will use as our example the\n4\u00d73 world introduced in Chapter 17. Figure 21.1 shows a policy for that world and the\ncorresponding utilities. Clearly, the passive learning task is similar to the policy evaluation\ntask, part of the policy iteration algorithm described in Section 17.3. The main difference\nis that the passive learning agent does not know the transition model\nP(s(cid:2)|s,a),\nwhich\n(cid:2)\nspecifies the probability of reaching state s from state s after doing action a; nor does it\nknowtherewardfunctionR(s),whichspecifiestherewardforeachstate.\n3 +1\n3 0.812 0.868 0.918 + 1\n2 \u20131 2 0.762 0.660 \u20131\n1 1 0.705 0.655 0.611 0.388\n1 2 3 4 1 2 3 4\n(a) (b)\nFigure 21.1 (a) A policy \u03c0 for the 4\u00d73 world; this policy happens to be optimal with\nrewardsof R(s)= \u22120.04inthenonterminalstatesandnodiscounting. (b)Theutilitiesof\nthestatesinthe4\u00d73world,givenpolicy\u03c0.\nTheagentexecutesasetoftrialsintheenvironmentusingitspolicy\u03c0. Ineachtrial,the\nTRIAL\nagent starts in state (1,1) and experiences a sequence of state transitions until it reaches one\noftheterminal states, (4,2)or(4,3). Itspercepts supply boththecurrent stateandthereward\nreceivedinthatstate. Typicaltrialsmightlooklikethis:\n(1,1) (cid:2)(1,2) (cid:2)(1,3) (cid:2)(1,2) (cid:2)(1,3) (cid:2)(2,3) (cid:2)(3,3) (cid:2)(4,3)\n-.04 -.04 -.04 -.04 -.04 -.04 -.04 +1\n(1,1) (cid:2)(1,2) (cid:2)(1,3) (cid:2)(2,3) (cid:2)(3,3) (cid:2)(3,2) (cid:2)(3,3) (cid:2)(4,3)\n-.04 -.04 -.04 -.04 -.04 -.04 -.04 +1\n(1,1) (cid:2)(2,1) (cid:2)(3,1) (cid:2)(3,2) (cid:2)(4,2) .\n-.04 -.04 -.04 -.04 -1\nNotethat each state percept is subscripted withthe reward received. Theobject istouse the\ninformationaboutrewardstolearntheexpectedutility U\u03c0(s)associatedwitheachnontermi-\nnal state s. Theutility is defined to be the expected sum of (discounted) rewards obtained if Section21.2. PassiveReinforcement Learning 833\npolicy\u03c0 isfollowed. AsinEquation(17.2)onpage650,wewrite\n\" #\n(cid:12)\u221e\nU\u03c0(s) = E \u03b3tR(S ) (21.1)\nt\nt=0\nwhereR(s)istherewardforastate, S (arandomvariable)isthestatereachedattimetwhen\nt\nexecuting policy \u03c0, and S =s. Wewillinclude a discountfactor \u03b3 inall of ourequations,\n0\nbutforthe4\u00d73worldwewillset\u03b3=1.\n21.2.1 Directutility estimation\nDIRECTUTILITY A simple method for direct utility estimation was invented in the late 1950s in the area of\nESTIMATION\nADAPTIVECONTROL adaptive control theory by Widrow and Hoff (1960). The idea is that the utility of a state\nTHEORY\nis the expected total reward from that state onward (called the expected reward-to-go), and\nREWARD-TO-GO\neachtrialprovides a sampleofthisquantity foreachstatevisited. Forexample, thefirsttrial\nin the set of three given earlier provides a sample total reward of 0.72 for state (1,1), two\nsamples of0.76 and 0.84 for(1,2), twosamples of0.80 and 0.88 for(1,3), and so on. Thus,\nattheendofeachsequence,thealgorithmcalculatestheobservedreward-to-goforeachstate\nandupdatestheestimatedutilityforthatstateaccordingly, justbykeeping arunningaverage\nforeachstateinatable. Inthelimitofinfinitelymanytrials,thesampleaveragewillconverge\ntothetrueexpectation inEquation(21.1).\nIt is clear that direct utility estimation is just an instance of supervised learning where\neach example has the state as input and the observed reward-to-go as output. This means\nthat we have reduced reinforcement learning to a standard inductive learning problem, as\ndiscussed inChapter18. Section 21.4discusses theuseofmorepowerful kinds ofrepresen-\ntations for the utility function. Learning techniques for those representations can be applied\ndirectlytotheobserveddata.\nDirect utility estimation succeeds in reducing the reinforcement learning problem to\nan inductive learning problem, about which much is known. Unfortunately, it misses a very\nimportant source of information, namely, the fact that the utilities of states are not indepen-\ndent! Theutility ofeachstateequals itsownrewardplus theexpected utility ofitssuccessor\nstates. That is, the utility values obey the Bellman equations for a fixed policy (see also\nEquation(17.10)):\n(cid:12)\nU\u03c0(s) = R(s)+\u03b3 P(s(cid:2)|s,\u03c0(s))U\u03c0(s(cid:2) ). (21.2)\ns(cid:3)\nByignoringtheconnections betweenstates,directutility estimation missesopportunities for\nlearning. For example, the second of the three trials given earlier reaches the state (3,2),\nwhich has not previously been visited. The next transition reaches (3,3), which is known\nfrom the first trial to have a high utility. The Bellman equation suggests immediately that\n(3,2)isalsolikelytohaveahighutility, because itleadsto(3,3),butdirectutilityestimation\nlearns nothing until the end of the trial. More broadly, we can view direct utility estimation\nas searching for U in a hypothesis space that is much larger than it needs to be, in that it\nincludes many functions that violate the Bellman equations. For this reason, the algorithm\noftenconverges veryslowly. 834 Chapter 21. Reinforcement Learning\nfunctionPASSIVE-ADP-AGENT(percept)returnsanaction\ninputs:percept,aperceptindicatingthecurrentstates(cid:5)andrewardsignalr(cid:5)\npersistent: \u03c0,afixedpolicy\nmdp,anMDPwithmodelP,rewardsR,discount\u03b3\nU,atableofutilities,initiallyempty\nNsa,atableoffrequenciesforstate\u2013actionpairs,initiallyzero\nNs(cid:3)|sa,atableofoutcomefrequenciesgivenstate\u2013actionpairs,initiallyzero\ns,a,thepreviousstateandaction,initiallynull\nifs(cid:5)isnewthenU[s(cid:5)]\u2190r(cid:5);R[s(cid:5)]\u2190r(cid:5)\nifs isnotnullthen\nincrementNsa[s,a]andNs(cid:3)|sa[s(cid:5),s,a]\nforeacht suchthatNs(cid:3)|sa[t,s,a]isnonzerodo\nP(t|s,a)\u2190Ns(cid:3)|sa[t,s,a]\/Nsa[s,a]\nU \u2190POLICY-EVALUATION(\u03c0,U,mdp)\nifs(cid:5).TERMINAL?thens,a\u2190nullelses,a\u2190s(cid:5),\u03c0[s(cid:5)]\nreturna\nFigure21.2 Apassivereinforcementlearningagentbasedonadaptivedynamicprogram-\nming. The POLICY-EVALUATION function solves the fixed-policy Bellman equations, as\ndescribedonpage657.\n21.2.2 Adaptivedynamicprogramming\nADAPTIVEDYNAMIC An adaptive dynamic programming (or ADP) agent takes advantage of the constraints\nPROGRAMMING\namong the utilities of states by learning the transition model that connects them and solv-\ning the corresponding Markov decision process using a dynamic programming method. For\napassivelearningagent,thismeanspluggingthelearnedtransitionmodelP(s(cid:2)|s,\u03c0(s))and\nthe observed rewards R(s) into the Bellman equations (21.2) to calculate the utilities of the\nstates. As we remarked in our discussion of policy iteration in Chapter 17, these equations\nare linear (no maximization involved) so they can be solved using any linear algebra pack-\nage. Alternatively, we can adopt the approach of modified policy iteration (see page 657),\nusing a simplified value iteration process to update the utility estimates after each change to\nthe learned model. Because the model usually changes only slightly with each observation,\nthe value iteration process can use the previous utility estimates as initial values and should\nconverge quitequickly.\nThe process of learning the model itself is easy, because the environment is fully ob-\nservable. Thismeansthatwehaveasupervisedlearningtaskwheretheinputisastate\u2013action\npair and the output is the resulting state. In the simplest case, we can represent the tran-\nsition model as a table of probabilities. We keep track of how often each action outcome\noccurs and estimate the transition probability\nP(s(cid:2)|s,a)\nfrom the frequency with which\ns(cid:2)\nis reached when executing a in s. Forexample, in the three trials given on page 832, Right\nis executed three times in (1,3) and two out of three times the resulting state is (2,3), so\nP((2,3)|(1,3),Right)isestimatedtobe2\/3. Section21.2. PassiveReinforcement Learning 835\n0.6\n1 (4,3)\n(3,3) 0.5\n0.8 (1,3)\n0.4\n(1,1)\n0.6 (3,2)\n0.3\n0.4\n0.2\n0.2 0.1\n0 0\n0 20 40 60 80 100 0 20 40 60 80 100\nNumber of trials Number of trials\n(a) (b)\nFigure21.3 ThepassiveADPlearningcurvesforthe4\u00d73world,giventheoptimalpolicy\nshowninFigure21.1. (a)Theutilityestimatesforaselectedsubsetofstates, asa function\nofthenumberoftrials. Noticethelargechangesoccurringaroundthe78thtrial\u2014thisisthe\nfirst time that the agent falls into the \u22121 terminal state at (4,2). (b) The root-mean-square\nerror(seeAppendixA)intheestimateforU(1,1),averagedover20runsof100trialseach.\nThe full agent program for a passive ADP agent is shown in Figure 21.2. Its perfor-\nmance on the 4\u00d73 world is shown in Figure 21.3. In terms of how quickly its value es-\ntimates improve, the ADP agent is limited only by its ability to learn the transition model.\nIn this sense, it provides a standard against which to measure other reinforcement learning\nalgorithms. Itis,however, intractable forlarge statespaces. Inbackgammon, forexample, it\nwouldinvolve solvingroughly 1050 equations in1050 unknowns.\nAreaderfamiliarwiththeBayesianlearning ideasofChapter20willhavenoticed that\nthe algorithm in Figure 21.2 is using maximum-likelihood estimation to learn the transition\nmodel; moreover, by choosing a policy based solely on the estimated model it is acting as\nif the model were correct. This is not necessarily a good idea! For example, a taxi agent\nthat didn\u2019t know about how traffic lights might ignore a red light once or twice without no\nill effects and then formulate a policy to ignore red lights from then on. Instead, it might\nbe a good idea to choose a policy that, while not optimal for the model estimated by maxi-\nmumlikelihood, worksreasonablywellforthewholerangeof modelsthathaveareasonable\nchanceofbeingthetruemodel. Therearetwomathematicalapproaches thathavethisflavor.\nBAYESIAN\nThe first approach, Bayesian reinforcement learning, assumes a prior probability\nREINFORCEMENT\nLEARNING P(h)foreachhypothesis haboutwhatthetruemodelis;theposteriorprobability P(h|e)is\nobtainedintheusualwaybyBayes\u2019rulegiventheobservationstodate. Then,iftheagenthas\ndecided tostop learning, the optimal policy istheone that gives thehighest expected utility.\nLet u\u03c0 be the expected utility, averaged over all possible start states, obtained by executing\nh\npolicy\u03c0 inmodelh. Thenwehave\n(cid:12)\n\u03c0\u2217 = argmax P(h|e)u\u03c0 .\nh\n\u03c0\nh\nsetamitse\nytilitU\nytilitu\nni\nrorre\nSMR 836 Chapter 21. Reinforcement Learning\nIn somespecial cases, this policy can even be computed! If the agent willcontinue learning\nin the future, however, then finding an optimal policy becomes considerably more difficult,\nbecause the agent must consider the effects of future observations on its beliefs about the\ntransition model. Theproblem becomes aPOMDPwhosebelief states aredistributions over\nmodels. This concept provides an analytical foundation for understanding the exploration\nproblem described inSection21.3.\nROBUSTCONTROL Thesecond approach, derived from robustcontroltheory, allowsforasetofpossible\nTHEORY\nmodelsHanddefinesanoptimalrobustpolicyasonethatgivesthebestoutcomeintheworst\ncaseoverH:\n\u03c0\u2217 = argmaxminu\u03c0 .\nh\n\u03c0 h\nOften, thesetH willbethesetofmodels thatexceedsomelikelihood threshold onP(h|e),\nso the robust and Bayesian approaches are related. Sometimes, the robust solution can be\ncomputed efficiently. There are, moreover, reinforcement learning algorithms that tend to\nproduce robustsolutions, althoughwedonotcoverthemhere.\n21.2.3 Temporal-difference learning\nSolving the underlying MDP as in the preceding section is not the only way to bring the\nBellman equations to bear on the learning problem. Another way is to use the observed\ntransitions to adjust the utilities of the observed states so that they agree with the constraint\nequations. Consider, for example, the transition from (1,3) to (2,3) in the second trial on\npage 832. Suppose that, as a result of the firsttrial, the utility estimates are U\u03c0(1,3)=0.84\nandU\u03c0(2,3)=0.92. Now,ifthis transition occurred allthe time,wewould expect theutili-\ntiestoobeytheequation\nU\u03c0(1,3) = \u22120.04+U\u03c0(2,3),\nsoU\u03c0(1,3)wouldbe0.88. Thus,itscurrentestimateof0.84mightbealittlelowandshould\n(cid:2)\nbe increased. More generally, when a transition occurs from state s to state s, weapply the\nfollowingupdatetoU\u03c0(s):\nU\u03c0(s) \u2190 U\u03c0(s)+\u03b1(R(s)+\u03b3U\u03c0(s(cid:2) )\u2212U\u03c0(s)). (21.3)\nHere,\u03b1isthelearningrateparameter. Becausethisupdateruleusesthedifferenceinutilities\nTEMPORAL- betweensuccessivestates, itisoftencalledthetemporal-difference, orTD,equation.\nDIFFERENCE\nAll temporal-difference methods work by adjusting the utility estimates towards the\nidealequilibrium thatholdslocally whentheutility estimates arecorrect. Inthecaseofpas-\nsivelearning, theequilibrium isgiven byEquation (21.2). NowEquation (21.3) does in fact\ncause theagenttoreachtheequilibrium givenbyEquation (21.2), butthere issomesubtlety\n(cid:2)\ninvolved. First, notice that the update involves only the observed successor s, whereas the\nactualequilibriumconditionsinvolveallpossiblenextstates. Onemightthinkthatthiscauses\nanimproperly largechangein U\u03c0(s)whenaveryraretransition occurs;but,infact,because\nrare transitions occur only rarely, the average value of U\u03c0(s) will converge to the correct\nvalue. Furthermore, if we change \u03b1 from a fixed parameter to a function that decreases as\nthenumberoftimesastate hasbeen visited increases, then U\u03c0(s)itself willconverge tothe Section21.2. PassiveReinforcement Learning 837\nfunctionPASSIVE-TD-AGENT(percept)returnsanaction\ninputs:percept,aperceptindicatingthecurrentstates(cid:5)andrewardsignalr(cid:5)\npersistent: \u03c0,afixedpolicy\nU,atableofutilities,initiallyempty\nNs,atableoffrequenciesforstates,initiallyzero\ns,a,r,thepreviousstate,action,andreward,initiallynull\nifs(cid:5)isnewthenU[s(cid:5)]\u2190r(cid:5)\nifs isnotnullthen\nincrementNs[s]\nU[s]\u2190U[s] + \u03b1(Ns[s])(r + \u03b3U[s(cid:5)] \u2212 U[s])\nifs(cid:5).TERMINAL?thens,a,r\u2190nullelses,a,r\u2190s(cid:5),\u03c0[s(cid:5)],r(cid:5)\nreturna\nFigure21.4 Apassivereinforcementlearningagentthatlearnsutilityestimatesusingtem-\nporaldifferences.Thestep-sizefunction\u03b1(n)ischosentoensureconvergence,asdescribed\ninthetext.\ncorrect value.1 Thisgivesustheagentprogram showninFigure21.4. Figure21.5illustrates\ntheperformance ofthepassiveTDagentonthe 4\u00d73world. Itdoesnotlearnquiteasfastas\nthe ADPagent and shows muchhigher variability, but itismuch simplerand requires much\nlesscomputationperobservation. NoticethatTDdoesnotneedatransitionmodeltoperform\nitsupdates. Theenvironment suppliestheconnection betweenneighboring statesintheform\nofobserved transitions.\nTheADPapproach andtheTDapproach areactually closely related. Bothtrytomake\nlocal adjustments tothe utility estimates inordertomakeeach state \u201cagree\u201d withitssucces-\nsors. One difference is that TD adjusts a state to agree with its observed successor (Equa-\ntion (21.3)), whereas ADP adjusts the state to agree with all of the successors that might\noccur, weighted by their probabilities (Equation (21.2)). This difference disappears when\nthe effects of TD adjustments are averaged over a large number of transitions, because the\nfrequencyofeachsuccessorinthesetoftransitionsisapproximatelyproportionaltoitsprob-\nability. A more important difference is that whereas TD makes a single adjustment per ob-\nserved transition, ADP makes as many as it needs to restore consistency between the utility\nestimates U and the environment model P. Although the observed transition makes only a\nlocal change in P, its effects might need to be propagated throughout U. Thus, TD can be\nviewedasacrudebutefficientfirstapproximation toADP.\nEach adjustment made by ADP could be seen, from the TD point of view, as a re-\nsult of a \u201cpseudoexperience\u201d generated by simulating the current environment model. It\nis possible to extend the TD approach to use an environment model to generate several\npseudoexperiences\u2014transitionsthattheTDagentcanimaginemighthappen,givenitscurrent\nmodel. Foreachobservedtransition, theTDagentcangenerate alargenumberofimaginary\n1 Thetechnical conditionsaregiven onpage725. InFigure21.5wehaveused\u03b1(n)=60\/(59+n), which\nsatisfiestheconditions. 838 Chapter 21. Reinforcement Learning\n0.6\n1 (4,3)\n(3,3) 0.5\n0.8 (1,3)\n(1,1) 0.4\n(2,1) 0.6\n0.3\n0.4\n0.2\n0.2 0.1\n0 0\n0 100 200 300 400 500 0 20 40 60 80 100\nNumber of trials Number of trials\n(a) (b)\nFigure21.5 TheTD learningcurvesforthe 4\u00d73 world. (a) Theutility estimates fora\nselectedsubsetofstates,asafunctionofthenumberoftrials. (b)Theroot-mean-squareerror\nintheestimateforU(1,1),averagedover20runsof500trialseach. Onlythefirst100trials\nareshowntoenablecomparisonwithFigure21.3.\ntransitions. Inthisway,theresultingutilityestimateswillapproximatemoreandmoreclosely\nthoseofADP\u2014ofcourse, attheexpenseofincreased computation time.\nIna similar vein, wecan generate more efficient versions of ADPby directly approxi-\nmating the algorithms forvalue iteration orpolicy iteration. Even though the value iteration\nalgorithm is efficient, it is intractable if we have, say, 10100 states. However, many of the\nnecessary adjustments to the state values on each iteration will be extremely tiny. One pos-\nsible approach to generating reasonably good answers quickly is to bound the number of\nadjustmentsmadeaftereachobservedtransition. Onecanalsouseaheuristictorankthepos-\nPRIORITIZED sibleadjustmentssoastocarryoutonlythemostsignificant ones. Theprioritizedsweeping\nSWEEPING\nheuristic preferstomakeadjustments tostateswhose likelysuccessors havejustundergone a\nlarge adjustment in their own utility estimates. Using heuristics like this, approximate ADP\nalgorithmsusuallycanlearnroughlyasfastasfullADP,intermsofthenumberoftrainingse-\nquences, butcanbeseveralordersofmagnitudemoreefficientintermsofcomputation. (See\nExercise 21.3.) This enables them to handle state spaces that are far too large for full ADP.\nApproximate ADPalgorithms haveanadditional advantage: intheearlystages oflearning a\nnew environment, the environment model P often will be far from correct, so there is little\npointincalculatinganexactutilityfunctiontomatchit. Anapproximationalgorithmcanuse\naminimumadjustmentsizethatdecreasesastheenvironment modelbecomesmoreaccurate.\nThis eliminates the very long value iterations that can occur early in learning due to large\nchanges inthemodel.\nsetamitse\nytilitU\nytilitu\nni\nrorre\nSMR Section21.3. ActiveReinforcement Learning 839\n21.3 ACTIVE REINFORCEMENT LEARNING\nApassivelearningagenthasafixedpolicythatdeterminesitsbehavior. Anactiveagentmust\ndecide whatactions totake. Letusbeginwiththeadaptive dynamicprogramming agent and\nconsiderhowitmustbemodifiedtohandlethisnewfreedom.\nFirst, the agent will need to learn a complete model with outcome probabilities for all\nactions, ratherthanjust themodelforthefixedpolicy. Thesimplelearning mechanism used\nby PASSIVE-ADP-AGENT will do just fine for this. Next, we need to take into account the\nfact thatthe agent hasachoice ofactions. Theutilities itneeds tolearn arethose defined by\ntheoptimalpolicy;theyobeytheBellmanequationsgivenonpage652,whichwerepeathere\nforconvenience:\n(cid:12)\nU(s) = R(s)+\u03b3 max\nP(s(cid:2)|s,a)U(s(cid:2)\n). (21.4)\na\ns(cid:3)\nThese equations can be solved to obtain the utility function U using the value iteration or\npolicyiterationalgorithmsfromChapter17. Thefinalissueiswhattodoateachstep. Having\nobtained a utility function U that is optimal for the learned model, the agent can extract an\noptimal action by one-step look-ahead to maximize the expected utility; alternatively, if it\nuses policy iteration, the optimal policy is already available, so it should simply execute the\nactiontheoptimalpolicyrecommends. Orshouldit?\n21.3.1 Exploration\nFigure 21.6 shows the results of one sequence of trials for an ADP agent that follows the\nrecommendation of the optimal policy for the learned model at each step. The agent does\nnot learn the true utilities or the true optimal policy! What happens instead is that, in the\n39th trial, it finds a policy that reaches the +1 reward along the lower route via (2,1), (3,1),\n(3,2), and (3,3). (See Figure 21.6(b).) After experimenting with minor variations, from the\n276th trial onward it sticks to that policy, never learning the utilities of the other states and\nneverfindingtheoptimalroutevia(1,2),(1,3),and(2,3). Wecallthisagentthegreedyagent.\nGREEDYAGENT\nRepeatedexperimentsshowthatthegreedyagentveryseldomconvergestotheoptimalpolicy\nforthisenvironment andsometimesconverges toreallyhorrendous policies.\nHowcanitbethatchoosingtheoptimalactionleadstosuboptimalresults? Theanswer\nis that the learned model is not the same as the true environment; what is optimal in the\nlearned modelcantherefore besuboptimal inthetrueenvironment. Unfortunately, theagent\ndoes not know what the true environment is, so it cannot compute the optimal action forthe\ntrueenvironment. What,then,istobedone?\nWhat the greedy agent has overlooked is that actions do more than provide rewards\naccording tothecurrentlearnedmodel;theyalsocontribute tolearning thetruemodelbyaf-\nfectingthepercepts thatarereceived. Byimprovingthemodel,theagentwillreceivegreater\nrewards in the future.2 An agent therefore must make a tradeoff between exploitation to\nEXPLOITATION\nmaximize its reward\u2014as reflected inits current utility estimates\u2014and exploration to maxi-\nEXPLORATION\n2 NoticethedirectanalogytothetheoryofinformationvalueinChapter16. 840 Chapter 21. Reinforcement Learning\n2\n3 +1\nRMS error\n1.5 Policy loss\n2 \u20131\n1\n0.5\n1\n0\n0 50 100 150 200 250 300 350 400 450 500\n1 2 3 4\nNumber of trials\n(a) (b)\nFigure21.6 Performanceofa greedyADP agentthatexecutesthe actionrecommended\nbytheoptimalpolicyforthelearnedmodel. (a)RMSerrorintheutilityestimatesaveraged\nover the nine nonterminal squares. (b) The suboptimal policy to which the greedy agent\nconvergesinthisparticularsequenceoftrials.\nmizeitslong-term well-being. Pureexploitation risksgettingstuckinarut. Pureexploration\ntoimproveone\u2019sknowledgeisofnouseifoneneverputsthatknowledgeintopractice. Inthe\nreal world, one constantly has to decide between continuing in a comfortable existence and\nstriking outintotheunknown inthehopes ofdiscovering anewandbetterlife. With greater\nunderstanding, lessexploration isnecessary.\nCanwebe alittle more precise than this? Isthere an optimal exploration policy? This\nquestionhasbeenstudiedindepthinthesubfieldofstatisticaldecisiontheorythatdealswith\nso-called banditproblems. (Seesidebar.)\nBANDITPROBLEM\nAlthough bandit problems are extremely difficult to solve exactly to obtain an optimal\nexploration method, it is nonetheless possible to come up with a reasonable scheme that\nwill eventually lead to optimal behavior by the agent. Technically, any such scheme needs\nto be greedy in the limit of infinite exploration, or GLIE. A GLIE scheme must try each\nGLIE\naction in each state an unbounded number of times to avoid having a finite probability that\nan optimal action ismissed because of an unusually bad series of outcomes. AnADPagent\nusingsuchaschemewilleventuallylearnthetrueenvironment model. AGLIEschememust\nalsoeventuallybecomegreedy,sothattheagent\u2019sactionsbecomeoptimalwithrespecttothe\nlearned(andhencethetrue)model.\nThereareseveralGLIEschemes;oneofthesimplest istohave theagentchoose aran-\ndom action a fraction 1\/t of the time and to follow the greedy policy otherwise. While this\ndoes eventually converge to an optimal policy, it can be extremely slow. A more sensible\napproach would give some weight to actions that the agent has not tried very often, while\ntending to avoid actions that are believed to be of low utility. This can be implemented by\naltering the constraint equation (21.4) so that it assigns a higher utility estimate to relatively\nssol\nycilop\n,rorre\nSMR Section21.3. ActiveReinforcement Learning 841\nEXPLORATION AND BANDITS\nIn Las Vegas, a one-armed bandit is a slot machine. A gambler can insert a coin,\npullthelever, and collect thewinnings (ifany). Ann-armedbandithasnlevers.\nThe gambler must choose which lever to play on each successive coin\u2014the one\nthathaspaidoffbest,ormaybeonethathasnotbeentried?\nThen-armedbandit problem isaformal modelforreal problems inmanyvi-\ntally important areas, such as deciding on the annual budget for AI research and\ndevelopment. Each arm corresponds to an action (such as allocating $20 million\nforthedevelopmentofnewAItextbooks),andthepayofffrompullingthearmcor-\nresponds to the benefits obtained from taking the action (immense). Exploration,\nwhether it is exploration of a new research field orexploration of a new shopping\nmall,isrisky, isexpensive, andhasuncertain payoffs; ontheotherhand, failureto\nexploreatallmeansthatoneneverdiscovers anyactionsthatareworthwhile.\nToformulateabanditproblemproperly,onemustdefineexactlywhatismeant\nby optimal behavior. Most definitions in the literature assume that the aim is to\nmaximizetheexpectedtotalrewardobtained overtheagent\u2019slifetime. Thesedefi-\nnitionsrequirethattheexpectationbetakenoverthepossibleworldsthattheagent\ncouldbein,aswellasoverthepossibleresultsofeachactionsequenceinanygiven\nworld. Here, a\u201cworld\u201d isdefinedbythetransition model\nP(s(cid:2)|s,a).\nThus, inor-\nderto act optimally, the agent needs aprior distribution over the possible models.\nTheresulting optimization problemsareusuallywildlyintractable.\nInsomecases\u2014forexample,whenthepayoffofeachmachineisindependent\nand discounted rewards are used\u2014it is possible to calculate a Gittins index for\neach slot machine (Gittins, 1989). The index is a function only of the number of\ntimestheslotmachinehasbeenplayedandhowmuchithaspaidoff. Theindexfor\neachmachineindicateshowworthwhileitistoinvestmore;generallyspeaking,the\nhigher the expected return and the higher the uncertainty in the utility of a given\nchoice, the better. Choosing the machine with the highest index value gives an\noptimalexplorationpolicy. Unfortunately,nowayhasbeenfoundtoextendGittins\nindicestosequential decision problems.\nOne can use the theory of n-armed bandits to argue for the reasonableness\nof the selection strategy in genetic algorithms. (See Chapter 4.) If you consider\neach arm in an n-armed bandit problem to be a possible string of genes, and the\ninvestment of a coin in one arm to be the reproduction of those genes, then it can\nbeproventhatgeneticalgorithmsallocatecoinsoptimally,givenanappropriateset\nofindependence assumptions. 842 Chapter 21. Reinforcement Learning\nunexplored state\u2013action pairs. Essentially, thisamounts toanoptimisticprioroverthepossi-\nbleenvironments andcauses theagenttobehave initially as ifthere werewonderful rewards\nscattered alloverthe place. Letus use U+(s)todenote the optimistic estimate of the utility\n(i.e.,theexpected reward-to-go) ofthestate s,andletN(s,a)bethenumberoftimesaction\na has been tried in state s. Suppose we are using value iteration in an ADP learning agent;\nthenweneedtorewritetheupdateequation (Equation(17.6) onpage652)toincorporate the\noptimisticestimate. Thefollowingequation doesthis:\n(cid:13) (cid:14)\n(cid:2)\nU+(s) \u2190 R(s)+\u03b3 max f P(s(cid:2)|s,a)U+(s(cid:2) ), N(s,a) . (21.5)\na s(cid:3)\nEXPLORATION Here, f(u,n) is called the exploration function. It determines how greed (preference for\nFUNCTION\nhigh values of u) is traded off against curiosity (preference for actions that have not been\ntried often and have low n). The function f(u,n) should be increasing in u and decreasing\ninn. Obviously, therearemanypossible functions thatfitthese conditions. Oneparticularly\nsimpledefinitionis\n(cid:24)\nR+ ifn< N\nf(u,n) = e\nu otherwise\nwhereR+ isanoptimisticestimateofthebestpossiblerewardobtainableinanystateandN\ne\nis a fixed parameter. This will have the effect of making the agent try each action\u2013state pair\natleastN times.\ne\nThe fact that U+ rather than U appears on the right-hand side of Equation (21.5) is\nveryimportant. Asexploration proceeds,thestatesandactionsnearthestartstatemightwell\nbe tried a large number of times. If we used U, the more pessimistic utility estimate, then\nthe agent would soon become disinclined to explore further afield. The use of U+ means\nthat the benefits of exploration are propagated back from the edges of unexplored regions,\nso that actions that lead toward unexplored regions are weighted more highly, rather than\njust actions that are themselves unfamiliar. Theeffect of this exploration policy can be seen\nclearlyinFigure21.7,whichshowsarapidconvergence towardoptimalperformance, unlike\nthatofthegreedyapproach. Averynearlyoptimalpolicyisfoundafterjust18trials. Notice\nthat the utility estimates themselves do not converge as quickly. This is because the agent\nstops exploring the unrewarding parts of the state space fairly soon, visiting them only \u201cby\naccident\u201dthereafter. However,itmakesperfectsensefortheagentnottocareabouttheexact\nutilities ofstatesthatitknowsareundesirable andcanbeavoided.\n21.3.2 Learning anaction-utility function\nNowthatwehavean activeADPagent, letusconsider how toconstruct anactivetemporal-\ndifference learning agent. The most obvious change from the passive case is that the agent\nis no longer equipped with a fixed policy, so, if it learns a utility function U, it will need to\nlearn a model in order to be able to choose an action based on U via one-step look-ahead.\nThemodelacquisition problemfortheTDagentisidenticaltothatfortheADPagent. What\noftheTDupdateruleitself? Perhapssurprisingly, theupdaterule(21.3)remainsunchanged.\nThismightseem odd, forthefollowing reason: Supposetheagent takesastep thatnormally Section21.3. ActiveReinforcement Learning 843\n2.2\n(1,1) 1.4\n2 (1,2)\n(1,3) 1.2 RMS error\n1.8\n(2,3) Policy loss\n1.6 (3,2) 1\n(3,3)\n1.4 (4,3) 0.8\n1.2 0.6\n1 0.4\n0.8\n0.2\n0.6\n0\n0 20 40 60 80 100 0 20 40 60 80 100\nNumber of trials Number of trials\n(a) (b)\nFigure21.7 PerformanceoftheexploratoryADPagent. usingR+ = 2andNe = 5. (a)\nUtility estimates for selected states overtime. (b) The RMS error in utility values and the\nassociatedpolicyloss.\nleadstoagooddestination,butbecauseofnondeterminism intheenvironmenttheagentends\nupinacatastrophicstate. TheTDupdaterulewilltakethisasseriouslyasiftheoutcomehad\nbeen the normal result of the action, whereas one might suppose that, because the outcome\nwas a fluke, the agent should not worry about it too much. In fact, of course, the unlikely\noutcome will occur only infrequently in a large set of training sequences; hence in the long\nrun its effects will be weighted proportionally to its probability, as we would hope. Once\nagain,itcanbeshownthattheTDalgorithm willconverge tothesamevaluesasADPasthe\nnumberoftrainingsequences tendstoinfinity.\nThere is an alternative TD method, called Q-learning, which learns an action-utility\nrepresentation instead of learning utilities. We will use the notation Q(s,a) to denote the\nvalueofdoingactionainstates. Q-valuesaredirectlyrelated toutilityvaluesasfollows:\nU(s) = maxQ(s,a). (21.6)\na\nQ-functions may seem like just another way of storing utility information, but they have a\nvery important property: a TD agent that learns a Q-function does not need a model of the\nform\nP(s(cid:2)|s,a),\neither for learning or for action selection. For this reason, Q-learning is\ncalled a model-free method. As with utilities, we can write a constraint equation that must\nholdatequilibrium whentheQ-valuesarecorrect:\nMODEL-FREE\n(cid:12)\nQ(s,a) = R(s)+\u03b3\nP(s(cid:2)|s,a)maxQ(s(cid:2) ,a(cid:2)\n). (21.7)\na(cid:3)\ns(cid:3)\nAs in the ADP learning agent, we can use this equation directly as an update equation for\nan iteration process that calculates exact Q-values, given an estimated model. This does,\nhowever, require that a model also be learned, because the equation uses\nP(s(cid:2)|s,a).\nThe\ntemporal-difference approach, on the other hand, requires no model of state transitions\u2014all\nsetamitse\nytilitU\nssol\nycilop\n,rorre\nSMR 844 Chapter 21. Reinforcement Learning\nfunctionQ-LEARNING-AGENT(percept)returnsanaction\ninputs:percept,aperceptindicatingthecurrentstates(cid:5)andrewardsignalr(cid:5)\npersistent: Q,atableofactionvaluesindexedbystateandaction,initiallyzero\nNsa,atableoffrequenciesforstate\u2013actionpairs,initiallyzero\ns,a,r,thepreviousstate,action,andreward,initiallynull\nifTERMINAL?(s)thenQ[s,None]\u2190r(cid:5)\nifs isnotnullthen\nincrementNsa[s,a]\nQ[s,a]\u2190Q[s,a] + \u03b1(Nsa[s,a])(r + \u03b3 maxa(cid:3) Q[s(cid:5),a(cid:5)] \u2212 Q[s,a])\ns,a,r\u2190s(cid:5),argmax\na(cid:3)\nf(Q[s(cid:5),a(cid:5)],Nsa[s(cid:5),a(cid:5)]),r(cid:5)\nreturna\nFigure21.8 AnexploratoryQ-learningagent. Itisanactivelearnerthatlearnsthevalue\nQ(s,a)of each actionin each situation. Ituses the same explorationfunctionf as the ex-\nploratoryADPagent,butavoidshavingtolearnthetransitionmodelbecausetheQ-valueof\nastatecanberelateddirectlytothoseofitsneighbors.\nitneedsaretheQvalues. Theupdateequation forTDQ-learningis\nQ(s,a) \u2190 Q(s,a)+\u03b1(R(s)+\u03b3 maxQ(s(cid:2) ,a(cid:2) )\u2212Q(s,a)), (21.8)\na(cid:3)\n(cid:2)\nwhichiscalculated wheneveraction aisexecutedinstatesleading tostates.\nThe complete agent design for an exploratory Q-learning agent using TD is shown in\nFigure 21.8. Notice that it uses exactly the same exploration function f as that used by the\nexploratory ADP agent\u2014hence the need to keep statistics on actions taken (the table N). If\nasimplerexploration policy isused\u2014say, acting randomly onsome fraction ofsteps, where\nthefractiondecreases overtime\u2014thenwecandispense withthestatistics.\nQ-learninghasacloserelativecalled SARSA(forState-Action-Reward-State-Action).\nSARSA\nTheupdateruleforSARSAisverysimilartoEquation(21.8):\nQ(s,a) \u2190 Q(s,a)+\u03b1(R(s)+\u03b3 Q(s(cid:2) ,a(cid:2) )\u2212Q(s,a)), (21.9)\n(cid:2) (cid:2)\nwhere a is the action actually taken in state s. The rule is applied at the end of each\n(cid:2) (cid:2)\ns, a, r, s, a quintuplet\u2014hence the name. The difference from Q-learning is quite subtle:\nwhereas Q-learning backs up the best Q-value from the state reached in the observed transi-\ntion, SARSAwaitsuntilanaction isactually takenand backsuptheQ-valueforthataction.\nNow, for a greedy agent that always takes the action with best Q-value, the two algorithms\nare identical. When exploration is happening, however, they differ significantly. Because\nQ-learning usesthebestQ-value, itpaysnoattention tothe actual policybeing followed\u2014it\nisanoff-policylearningalgorithm,whereasSARSAisanon-policyalgorithm. Q-learningis\nOFF-POLICY\nmoreflexiblethanSARSA,inthesensethataQ-learningagentcanlearnhowtobehavewell\nON-POLICY\nevenwhenguidedbyarandomoradversarial exploration policy. Ontheotherhand,SARSA\nismorerealistic: forexample,iftheoverallpolicyisevenpartlycontrolledbyotheragents,it\nisbettertolearnaQ-functionforwhatwillactuallyhappen ratherthanwhattheagentwould\nliketohappen. Section21.4. Generalization inReinforcement Learning 845\nBoth Q-learning and SARSA learn the optimal policy for the 4\u00d73 world, but do so\nat a much slower rate than the ADP agent. This is because the local updates do not enforce\nconsistencyamongalltheQ-valuesviathemodel. Thecomparisonraisesageneralquestion:\nis it better to learn a model and a utility function or to learn an action-utility function with\nno model? In other words, what is the best way to represent the agent function? This is\nan issue at the foundations of artificial intelligence. As we stated in Chapter 1, one of the\nkey historical characteristics of much of AI research is its (often unstated) adherence to the\nknowledge-based approach. This amounts to an assumption that the best way to represent\nthe agent function is to build a representation of some aspects of the environment in which\ntheagentissituated.\nSome researchers, both inside and outside AI, have claimed that the availability of\nmodel-free methods such asQ-learning means thatthe knowledge-based approach isunnec-\nessary. Thereis,however,littletogoonbutintuition. Ourintuition,forwhatit\u2019sworth,isthat\nas the environment becomes more complex, the advantages of a knowledge-based approach\nbecome more apparent. This is borne out even in games such as chess, checkers (draughts),\nand backgammon (see next section), where efforts to learn an evaluation function by means\nofamodelhavemetwithmoresuccessthanQ-learning methods.\n21.4 GENERALIZATION IN REINFORCEMENT LEARNING\nSofar, wehave assumed that the utility functions and Q-functions learned by the agents are\nrepresented in tabular form with one output value for each input tuple. Such an approach\nworksreasonably wellforsmall statespaces, but thetimeto convergence and(forADP)the\ntimeperiterationincreaserapidlyasthespacegetslarger. Withcarefullycontrolled, approx-\nimate ADP methods, it might be possible to handle 10,000 states or more. This suffices for\ntwo-dimensional maze-like environments, but more realistic worlds are out of the question.\nBackgammon and chess are tiny subsets of the real world, yet their state spaces contain on\nthe order of 1020 and 1040 states, respectively. It would be absurd to suppose that one must\nvisitallthesestatesmanytimesinordertolearnhowtoplay thegame!\nFUNCTION One way to handle such problems is to use function approximation, which simply\nAPPROXIMATION\nmeans using any sort of representation for the Q-function other than a lookup table. The\nrepresentation isviewed asapproximate because itmight not bethe case that the true utility\nfunction orQ-function canberepresented inthechosen form. Forexample, inChapter5we\ndescribed an evaluation function for chess that is represented as aweighted linear function\nofasetoffeatures(orbasisfunctions)f ,...,f :\nBASISFUNCTION 1 n\nU\u02c6 (s) = \u03b8 f (s)+\u03b8 f (s)+\u00b7\u00b7\u00b7+\u03b8 f (s).\n\u03b8 1 1 2 2 n n\nA reinforcement learning algorithm can learn values for the parameters \u03b8=\u03b8 ,...,\u03b8 such\n1 n\nthat the evaluation function U\u02c6 approximates the true utility function. Instead of, say, 1040\n\u03b8\nvalues in a table, this function approximator is characterized by, say, n=20 parameters\u2014\nan enormous compression. Although no one knows the true utility function for chess, no\none believes that it can be represented exactly in 20 numbers. If the approximation is good 846 Chapter 21. Reinforcement Learning\nenough, however, theagentmightstillplayexcellent chess.3 Function approximation makes\nitpracticaltorepresentutilityfunctionsforverylargestatespaces,butthatisnotitsprincipal\nbenefit. The compression achieved by a function approximator allows the learning agent to\ngeneralize from states it has visited to states it has not visited. That is, the most important\naspectoffunctionapproximationisnotthatitrequireslessspace,butthatitallowsforinduc-\ntive generalization over input states. To give you some idea of the power of this effect: by\nexaminingonlyoneinevery1012 ofthepossiblebackgammonstates,itispossibletolearna\nutilityfunction thatallowsaprogramtoplayaswellasanyhuman(Tesauro,1992).\nOntheflip side, of course, there isthe problem that there could fail tobe any function\nin the chosen hypothesis space that approximates the true utility function sufficiently well.\nAs in all inductive learning, there is a tradeoff between the size of the hypothesis space and\nthetimeittakestolearnthefunction. Alargerhypothesis spaceincreases thelikelihood that\nagoodapproximation canbefound, butalsomeansthatconvergence islikelytobedelayed.\nLetusbeginwiththesimplestcase,whichisdirectutilityestimation. (SeeSection21.2.)\nWith function approximation, this is an instance of supervised learning. Forexample, sup-\nposewerepresenttheutilitiesforthe4\u00d73worldusingasimplelinearfunction. Thefeatures\nofthesquares arejusttheir xandy coordinates, sowehave\nU\u02c6 (x,y) =\u03b8 +\u03b8 x+\u03b8 y . (21.10)\n\u03b8 0 1 2\nThus,if(\u03b8 ,\u03b8 ,\u03b8 )=(0.5,0.2,0.1),thenU\u02c6 (1,1)=0.8. Givenacollection oftrials, weob-\n0 1 2 \u03b8\ntainasetofsamplevaluesofU\u02c6 (x,y),andwecanfindthebestfit,inthesenseofminimizing\n\u03b8\nthesquarederror, usingstandard linearregression. (SeeChapter18.)\nFor reinforcement learning, it makes more sense to use an online learning algorithm\nthat updates the parameters after each trial. Suppose we run a trial and the total reward\nobtained starting at (1,1) is 0.4. This suggests that U\u02c6 (1,1), currently 0.8, is too large and\n\u03b8\nmust be reduced. How should the parameters be adjusted to achieve this? As with neural-\nnetwork learning, we write an error function and compute its gradient with respect to the\nparameters. If u (s) is the observed total reward from state s onward in the jth trial, then\nj\ntheerrorisdefined as(half) thesquared difference ofthepredicted totalandtheactual total:\nE (s) = (U\u02c6 (s)\u2212u (s))2\/2. Therateofchangeoftheerrorwithrespecttoeachparameter\nj \u03b8 j\n\u03b8 is\u2202E \/\u2202\u03b8 ,sotomovetheparameterinthedirection ofdecreasing theerror,wewant\ni j i\n\u2202E (s) \u2202U\u02c6 (s)\n\u03b8 \u2190 \u03b8 \u2212\u03b1 j = \u03b8 +\u03b1(u (s)\u2212U\u02c6 (s)) \u03b8 . (21.11)\ni i i j \u03b8\n\u2202\u03b8 \u2202\u03b8\ni i\nThis is called the Widrow\u2013Hoff rule, or the delta rule, for online least-squares. For the\nWIDROW\u2013HOFFRULE\nlinearfunctionapproximator U\u02c6 (s)inEquation(21.10),wegetthreesimpleupdaterules:\nDELTARULE \u03b8\n\u03b8 \u2190 \u03b8 +\u03b1(u (s)\u2212U\u02c6 (s)),\n0 0 j \u03b8\n\u03b8 \u2190 \u03b8 +\u03b1(u (s)\u2212U\u02c6 (s))x,\n1 1 j \u03b8\n\u03b8 \u2190 \u03b8 +\u03b1(u (s)\u2212U\u02c6 (s))y .\n2 2 j \u03b8\n3 WedoknowthattheexactutilityfunctioncanberepresentedinapageortwoofLisp,Java,orC++. Thatis,\nitcanberepresentedbyaprogramthatsolvesthegameexactlyeverytimeitiscalled. Weareinterestedonlyin\nfunctionapproximatorsthatuseareasonable amountofcomputation. Itmightinfactbebettertolearnavery\nsimplefunctionapproximatorandcombineitwithacertainamountoflook-aheadsearch.Thetradeoffsinvolved\narecurrentlynotwellunderstood. Section21.4. Generalization inReinforcement Learning 847\nWe can apply these rules to the example where U\u02c6 (1,1) is 0.8 and u (1,1) is 0.4. \u03b8 , \u03b8 ,\n\u03b8 j 0 1\nand\u03b8 arealldecreased by 0.4\u03b1,whichreduces theerrorfor(1,1). Noticethat changing the\n2\nparameters\u03b8inresponsetoanobservedtransitionbetweentwostatesalsochangesthevalues\nof U\u02c6 for every other state! This is what we mean by saying that function approximation\n\u03b8\nallowsareinforcement learnertogeneralize fromitsexperiences.\nWe expect that the agent will learn faster if it uses a function approximator, provided\nthat the hypothesis space is not too large, but includes some functions that are a reasonably\ngood fit to the true utility function. Exercise 21.5 asks you to evaluate the performance of\ndirectutilityestimation, bothwithandwithoutfunction approximation. Theimprovementin\nthe4\u00d73worldisnoticeablebutnotdramatic,becausethisisaverysmallstatespacetobegin\nwith. Theimprovementismuchgreaterina 10\u00d710worldwitha+1rewardat(10,10). This\nworld is well suited for a linear utility function because the true utility function is smooth\nand nearly linear. (See Exercise 21.8.) If we put the +1 reward at (5,5), the true utility is\nmore like a pyramid and the function approximator in Equation (21.10) will fail miserably.\nAll is not lost, however! Remember that what matters for linear function approximation\nis that the function be linear in the parameters\u2014the features themselves can be arbitrary\nno(cid:10)nlinearfunctionsofthestatevariables. Hence,wecanincludeatermsuchas\u03b8 3f 3(x,y) =\n\u03b8 (x\u2212x )2+(y\u2212y )2 thatmeasuresthedistancetothegoal.\n3 g g\nWecanapplytheseideasequallywelltotemporal-differencelearners. Allweneeddois\nadjusttheparameters totrytoreduce thetemporaldifference betweensuccessive states. The\nnew versions of the TD and Q-learning equations (21.3 on page 836 and 21.8 on page 844)\naregivenby\n\u2202U\u02c6 (s)\n\u03b8 \u2190 \u03b8 +\u03b1[R(s)+\u03b3U\u02c6 (s(cid:2) )\u2212U\u02c6 (s)] \u03b8 (21.12)\ni i \u03b8 \u03b8\n\u2202\u03b8\ni\nforutilitiesand\n\u2202Q\u02c6 (s,a)\n\u03b8 \u2190 \u03b8 +\u03b1[R(s)+\u03b3 maxQ\u02c6 (s(cid:2) ,a(cid:2) )\u2212Q\u02c6 (s,a)] \u03b8 (21.13)\ni i \u03b8 \u03b8\na(cid:3) \u2202\u03b8\ni\nforQ-values. ForpassiveTDlearning,theupdaterulecanbeshowntoconvergetotheclosest\npossible4 approximation tothetruefunction whenthefunction approximator is linear inthe\nparameters. With active learning and nonlinear functions such as neural networks, all bets\nare off: There are some very simple cases in which the parameters can go off to infinity\neven though there are good solutions in the hypothesis space. There are more sophisticated\nalgorithms thatcan avoid these problems, but atpresent reinforcement learning withgeneral\nfunction approximators remainsadelicate art.\nFunction approximation can also be very helpful for learning a model of the environ-\nment. Rememberthatlearning amodelforan observable environment isasupervised learn-\ningproblem,becausethenextperceptgivestheoutcomestate. Anyofthesupervisedlearning\nmethodsinChapter18canbeused,withsuitableadjustmentsforthefactthatweneedtopre-\ndictacompletestatedescriptionratherthanjustaBooleanclassificationorasinglerealvalue.\nFor a partially observable environment, the learning problem is much more difficult. If we\nknowwhatthehiddenvariablesareandhowtheyarecausallyrelatedtoeachotherandtothe\n4 Thedefinitionofdistancebetweenutilityfunctionsisrathertechnical;seeTsitsiklisandVanRoy(1997). 848 Chapter 21. Reinforcement Learning\nobservablevariables,thenwecanfixthestructureofadynamicBayesiannetworkandusethe\nEMalgorithm tolearn the parameters, as wasdescribed in Chapter20. Inventing thehidden\nvariables and learning the model structure are still open problems. Somepractical examples\naredescribed inSection21.6.\n21.5 POLICY SEARCH\nThe final approach we will consider for reinforcement learning problems is called policy\nsearch. In some ways, policy search is the simplest of all the methods in this chapter: the\nPOLICYSEARCH\nideaistokeeptwiddling thepolicyaslongasitsperformance improves,thenstop.\nLetus begin withthe policies themselves. Rememberthat apolicy \u03c0 isafunction that\nmapsstatestoactions. Weareinterestedprimarilyin parameterized representations of\u03c0 that\nhave far fewer parameters than there are states in the state space (just as in the preceding\nsection). For example, we could represent \u03c0 by a collection of parameterized Q-functions,\noneforeachaction, andtaketheactionwiththehighestpredicted value:\n\u03c0(s) = maxQ\u02c6 (s,a). (21.14)\n\u03b8\na\nEach Q-function could be a linear function of the parameters \u03b8, as in Equation (21.10),\nor it could be a nonlinear function such as a neural network. Policy search will then ad-\njust the parameters \u03b8 to improve the policy. Notice that if the policy is represented by Q-\nfunctions, then policy search results in a process that learns Q-functions. This process is\nnot the same asQ-learning! In Q-learning with function approximation, the algorithm finds\na value of \u03b8 such that Q\u02c6 is \u201cclose\u201d to Q\u2217 , the optimal Q-function. Policy search, on the\n\u03b8\nother hand, finds a value of \u03b8 that results in good performance; the values found by the two\nmethods may differ very substantially. (For example, the approximate Q-function defined\nby Q\u02c6 (s,a)=Q\u2217 (s,a)\/10 gives optimal performance, even though it is not at all close to\n\u03b8\n\u2217\nQ .) Anotherclearinstance ofthe difference isthecase where \u03c0(s)iscalculated using, say,\ndepth-10 look-ahead search with anapproximate utility function U\u02c6 . Avalue of \u03b8 that gives\n\u03b8\ngoodresultsmaybealongwayfrommaking\nU\u02c6\nresemblethetrueutilityfunction.\n\u03b8\nOne problem with policy representations of the kind given in Equation (21.14) is that\nthepolicy isadiscontinuous function oftheparameters whentheactions arediscrete. (Fora\ncontinuousactionspace,thepolicycanbeasmoothfunctionoftheparameters.) Thatis,there\nwillbevaluesof\u03b8suchthataninfinitesimalchangein\u03b8causesthepolicytoswitchfromone\naction to another. This means that the value of the policy may also change discontinuously,\nwhichmakesgradient-basedsearchdifficult. Forthisreason,policysearchmethodsoftenuse\nastochasticpolicyrepresentation \u03c0 (s,a),whichspecifiestheprobabilityofselectingaction\nSTOCHASTICPOLICY \u03b8\nainstates. Onepopularrepresentation isthe softmaxfunction:\nSOFTMAXFUNCTION\n(cid:12)\n\u03c0 (s,a) = eQ\u02c6 \u03b8(s,a)\/ eQ\u02c6 \u03b8(s,a(cid:3)) .\n\u03b8\na(cid:3)\nSoftmax becomes nearly deterministic if one action is much better than the others, but it\nalways gives adifferentiable function of \u03b8; hence, the value of the policy (which depends in Section21.5. PolicySearch 849\na continuous fashion on the action selection probabilities) is a differentiable function of \u03b8.\nSoftmaxisageneralization ofthelogistic function(page725)tomultiplevariables.\nNowletuslookatmethodsforimprovingthepolicy. Westartwiththesimplestcase: a\ndeterministic policy and a deterministic environment. Let \u03c1(\u03b8) be the policy value, i.e., the\nPOLICYVALUE\nexpectedreward-to-gowhen\u03c0 isexecuted. Ifwecanderiveanexpressionfor\u03c1(\u03b8)inclosed\n\u03b8\nform,thenwehaveastandardoptimizationproblem,asdescribedinChapter4. Wecanfollow\nthe policy gradient vector \u2207 \u03c1(\u03b8) provided \u03c1(\u03b8) is differentiable. Alternatively, if \u03c1(\u03b8) is\nPOLICYGRADIENT \u03b8\nnot available in closed form, we can evaluate \u03c0 simply by executing it and observing the\n\u03b8\naccumulatedreward. Wecanfollowtheempiricalgradientbyhillclimbing\u2014i.e.,evaluating\nthe change in policy value for small increments in each parameter. With the usual caveats,\nthisprocesswillconverge toalocaloptimuminpolicyspace.\nWhen the environment (or the policy) is stochastic, things get more difficult. Suppose\nwe are trying to do hill climbing, which requires comparing \u03c1(\u03b8) and \u03c1(\u03b8 +\u0394\u03b8) for some\nsmall \u0394\u03b8. The problem is that the total reward on each trial may vary widely, so estimates\nof the policy value from a small number of trials will be quite unreliable; trying to compare\ntwosuch estimates willbeeven moreunreliable. Onesolution issimply to runlots oftrials,\nmeasuring the sample variance and using it to determine that enough trials have been run\nto get a reliable indication of the direction of improvement for \u03c1(\u03b8). Unfortunately, this is\nimpractical formanyrealproblemswhereeachtrialmaybeexpensive, time-consuming, and\nperhapsevendangerous.\nForthecaseofastochastic policy\u03c0 (s,a),itispossibletoobtainanunbiasedestimate\n\u03b8\nof the gradient at \u03b8, \u2207 \u03c1(\u03b8), directly from the results of trials executed at \u03b8. Forsimplicity,\n\u03b8\nwewillderive thisestimate forthesimple caseofanonsequential environment inwhich the\nreward R(a) is obtained immediately after doing action a in the start state s . In this case,\n0\nthepolicyvalueisjusttheexpected valueofthereward,and wehave\n(cid:12) (cid:12)\n\u2207 \u03c1(\u03b8)= \u2207 \u03c0 (s ,a)R(a) = (\u2207 \u03c0 (s ,a))R(a).\n\u03b8 \u03b8 \u03b8 0 \u03b8 \u03b8 0\na a\nNow we perform a simple trick so that this summation can be approximated by samples\ngenerated from the probability distribution defined by \u03c0 (s ,a). Suppose that we have N\n\u03b8 0\ntrialsinallandtheactiontakenonthejthtrialisa . Then\nj\n(cid:12) (\u2207 \u03c0 (s ,a))R(a) 1 (cid:12)N (\u2207 \u03c0 (s ,a ))R(a )\n\u2207 \u03c1(\u03b8)= \u03c0 (s ,a)\u00b7 \u03b8 \u03b8 0 \u2248 \u03b8 \u03b8 0 j j .\n\u03b8 \u03b8 0\n\u03c0 (s ,a) N \u03c0 (s ,a )\n\u03b8 0 \u03b8 0 j\na j=1\nThus, the true gradient of the policy value is approximated by a sum of terms involving\nthe gradient of the action-selection probability in each trial. For the sequential case, this\ngeneralizes to\n1 (cid:12)N (\u2207 \u03c0 (s,a ))R (s)\n\u2207 \u03c1(\u03b8)\u2248 \u03b8 \u03b8 j j\n\u03b8\nN \u03c0 (s,a )\n\u03b8 j\nj=1\nfor each state s visited, where a is executed in s on the jth trial and R (s) is the total\nj j\nreward received from state s onwards in the jth trial. The resulting algorithm is called\nREINFORCE (Williams, 1992); it is usually much more effective than hill climbing using\nlotsoftrialsateachvalueof\u03b8. Itisstillmuchslowerthannecessary, however. 850 Chapter 21. Reinforcement Learning\nConsider the following task: given two blackjack5 programs, determine which is best.\nOne way to do this is to have each play against a standard \u201cdealer\u201d for a certain number of\nhandsandthentomeasuretheirrespectivewinnings. Theproblemwiththis,aswehaveseen,\nisthatthewinningsofeachprogram fluctuatewidelydepending onwhetheritreceivesgood\nor bad cards. An obvious solution is to generate a certain number of hands in advance and\nhave each program play the same set of hands. In this way, we eliminate the measurement\nCORRELATED error due to differences in the cards received. This idea, called correlated sampling, un-\nSAMPLING\nderlies a policy-search algorithm called PEGASUS (Ng and Jordan, 2000). The algorithm is\napplicable to domains for which a simulator is available so that the \u201crandom\u201d outcomes of\nactions canberepeated. Thealgorithm worksbygenerating inadvance N sequences ofran-\ndomnumbers, eachofwhichcanbeusedtorunatrialofanypolicy. Policysearchiscarried\noutbyevaluatingeachcandidatepolicyusingthesamesetofrandomsequencestodetermine\ntheactionoutcomes. Itcanbeshownthatthenumberofrandomsequencesrequiredtoensure\nthatthevalueofeverypolicyiswellestimated depends onlyonthecomplexity ofthepolicy\nspace,andnotatallonthecomplexityoftheunderlying domain.\n21.6 APPLICATIONS OF REINFORCEMENT LEARNING\nWenowturntoexamplesoflarge-scale applications ofreinforcement learning. Weconsider\napplicationsingameplaying,wherethetransitionmodelisknownandthegoalistolearnthe\nutilityfunction, andinrobotics, wherethemodelisusually unknown.\n21.6.1 Applicationsto gameplaying\nThefirstsignificant application ofreinforcement learning wasalsothefirstsignificant learn-\ning program of any kind\u2014the checkers program written by Arthur Samuel (1959, 1967).\nSamuel first used a weighted linear function for the evaluation of positions, using up to 16\ntermsatanyonetime. HeappliedaversionofEquation(21.12)toupdatetheweights. There\nweresomesignificantdifferences,however,betweenhisprogramandcurrentmethods. First,\nheupdatedtheweightsusingthedifferencebetweenthecurrentstateandthebacked-upvalue\ngenerated byfulllook-ahead inthesearchtree. Thisworksfine,because itamountstoview-\ning the state space at a different granularity. A second difference was that the program did\nnotuseanyobservedrewards! Thatis,thevaluesofterminalstatesreachedinself-playwere\nignored. Thismeansthatitistheoretically possibleforSamuel\u2019sprogramnottoconverge, or\nto converge on a strategy designed to lose rather than to win. Hemanaged to avoid this fate\nby insisting that the weight for material advantage should always be positive. Remarkably,\nthis was sufficient to direct the program into areas of weight space corresponding to good\ncheckers play.\nGerryTesauro\u2019s backgammon program TD-GAMMON (1992) forcefully illustrates the\npotential of reinforcement learning techniques. In earlier work (Tesauro and Sejnowski,\n1989), Tesauro tried learning a neural network representation of Q(s,a) directly from ex-\n5 Alsoknownastwenty-oneorpontoon. Section21.6. Applications ofReinforcement Learning 851\n\u03b8\nx\nFigure21.9 Setupfortheproblemofbalancingalongpoleontopofamovingcart. The\ncartcanbejerkedleftorrightbyacontrollerthatobserves x,\u03b8,x\u02d9,and\u03b8\u02d9.\namples of moves labeled with relative values by a human expert. This approach proved\nextremely tedious forthe expert. Itresulted inaprogram, called NEUROGAMMON,thatwas\nstrong by computer standards, but not competitive with human experts. The TD-GAMMON\nproject was an attempt to learn from self-play alone. The only reward signal was given at\nthe end of each game. The evaluation function was represented by a fully connected neural\nnetwork with a single hidden layer containing 40 nodes. Simply by repeated application of\nEquation (21.12), TD-GAMMON learned toplayconsiderably betterthan NEUROGAMMON,\neventhoughtheinputrepresentation contained justtheraw boardpositionwithnocomputed\nfeatures. Thistookabout200,000traininggamesandtwoweeksofcomputertime. Although\nthat may seem like a lot of games, it is only a vanishingly small fraction of the state space.\nWhenprecomputedfeatureswereaddedtotheinputrepresentation,anetworkwith80hidden\nnodes wasable, after300,000 training games, toreach astandard ofplay comparable tothat\nof the top three human players worldwide. Kit Woolsey, a top player and analyst, said that\n\u201cThereisnoquestion inmymindthatitspositional judgment isfarbetterthanmine.\u201d\n21.6.2 Applicationto robot control\nThe setup for the famous cart\u2013pole balancing problem, also known as the inverted pendu-\nCART\u2013POLE\nINVERTED lum, is shown in Figure 21.9. The problem is to control the position x of the cart so that\nPENDULUM\nthe pole stays roughly upright (\u03b8 \u2248 \u03c0\/2), while staying within the limits of the cart track\nas shown. Several thousand papers in reinforcement learning and control theory have been\npublished on this seemingly simple problem. The cart\u2013pole problem differs from the prob-\nlemsdescribedearlierinthatthestatevariables\nx,\u03b8,x\u02d9,and\u03b8\u02d9\narecontinuous. Theactionsare\nBANG-BANG usuallydiscrete: jerkleftorjerkright,theso-called bang-bangcontrolregime.\nCONTROL\nThe earliest work on learning for this problem was carried out by Michie and Cham-\nbers(1968). TheirBOXES algorithm wasabletobalancethepoleforoveranhourafteronly\nabout30trials. Moreover,unlikemanysubsequentsystems, BOXESwasimplementedwitha 852 Chapter 21. Reinforcement Learning\nrealcartandpole,notasimulation. Thealgorithmfirstdiscretized thefour-dimensional state\nspaceintoboxes\u2014hence thename. Itthenrantrialsuntilthe polefelloverorthecarthitthe\nendofthetrack. Negativereinforcement wasassociated withthefinalactioninthefinalbox\nand then propagated back through the sequence. It was found that the discretization caused\nsome problems when the apparatus was initialized in a position different from those used in\ntraining, suggesting that generalization was not perfect. Improved generalization and faster\nlearning canbeobtained usinganalgorithm that adaptively partitions thestatespaceaccord-\ningtotheobservedvariationinthereward,orbyusingacontinuous-state, nonlinearfunction\napproximatorsuchasaneuralnetwork. Nowadays,balancing atripleinvertedpendulum isa\ncommonexercise\u2014afeatfarbeyondthecapabilities ofmosthumans.\nStill more impressive is the application of reinforcement learning to helicopter flight\n(Figure 21.10). This work has generally used policy search (Bagnell and Schneider, 2001)\nas well as the PEGASUS algorithm with simulation based on a learned transition model (Ng\netal.,2004). FurtherdetailsaregiveninChapter25.\nFigure21.10 Superimposedtime-lapse imagesof an autonomoushelicopterperforming\na very difficult \u201cnose-in circle\u201d maneuver. The helicopter is under the control of a policy\ndevelopedbythe PEGASUS policy-searchalgorithm. A simulatormodelwas developedby\nobservingtheeffectsofvariouscontrolmanipulationsontherealhelicopter;thenthealgo-\nrithmwasrunonthesimulatormodelovernight.Avarietyofcontrollersweredevelopedfor\ndifferentmaneuvers. In all cases, performance far exceeded that of an expert human pilot\nusingremotecontrol.(ImagecourtesyofAndrewNg.) Section21.7. Summary 853\n21.7 SUMMARY\nThis chapter has examined the reinforcement learning problem: how an agent can become\nproficientinanunknownenvironment, givenonlyitsperceptsandoccasional rewards. Rein-\nforcement learning canbeviewedasamicrocosm fortheentireAIproblem, butitisstudied\ninanumberofsimplifiedsettingstofacilitate progress. Themajorpointsare:\n\u2022 The overall agent design dictates the kind of information that must be learned. The\nthree main designs we covered were the model-based design, using a model P and a\nutility function U; the model-free design, using an action-utility function Q; and the\nreflexdesign, usingapolicy\u03c0.\n\u2022 Utilitiescanbelearnedusingthreeapproaches:\n1. Direct utilityestimation usesthetotal observed reward-to-go foragivenstateas\ndirectevidenceforlearning itsutility.\n2. Adaptive dynamic programming (ADP) learns a model and a reward function\nfrom observations and then uses value or policy iteration to obtain the utilities or\nan optimal policy. ADP makes optimal use of the local constraints on utilities of\nstatesimposedthroughtheneighborhood structure oftheenvironment.\n3. Temporal-difference(TD)methodsupdateutilityestimatestomatchthoseofsuc-\ncessorstates. Theycanbeviewedassimpleapproximations totheADPapproach\nthatcanlearnwithoutrequiring atransition model. Usinga learned modeltogen-\neratepseudoexperiences can,however,resultinfasterlearning.\n\u2022 Action-utility functions, or Q-functions, can be learned by an ADP approach or a TD\napproach. With TD, Q-learning requires no model in either the learning or action-\nselectionphase. Thissimplifiesthelearningproblembutpotentiallyrestrictstheability\nto learn in complex environments, because the agent cannot simulate the results of\npossiblecourses ofaction.\n\u2022 When the learning agent is responsible for selecting actions while it learns, it must\ntrade off the estimated value of those actions against the potential for learning useful\nnew information. Anexact solution of the exploration problem is infeasible, but some\nsimpleheuristics doareasonable job.\n\u2022 Inlargestatespaces, reinforcement learning algorithmsmustuseanapproximate func-\ntional representation in order to generalize over states. The temporal-difference signal\ncanbeuseddirectly toupdateparameters inrepresentations suchasneuralnetworks.\n\u2022 Policy-search methods operate directly on a representation of the policy, attempting\nto improve it based on observed performance. The variation in the performance in a\nstochasticdomainisaseriousproblem;forsimulateddomainsthiscanbeovercomeby\nfixingtherandomness inadvance.\nBecauseofitspotentialforeliminatinghandcodingofcontrolstrategies,reinforcementlearn-\ning continues to be one of the most active areas of machine learning research. Applications\nin robotics promise tobe particularly valuable; these will require methods forhandling con- 854 Chapter 21. Reinforcement Learning\ntinuous, high-dimensional, partially observable environments in which successful behaviors\nmayconsist ofthousands orevenmillionsofprimitiveactions.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nTuring(1948,1950)proposedthereinforcement-learning approach,althoughhewasnotcon-\nvincedofitseffectiveness, writing,\u201ctheuseofpunishments andrewardscanatbestbeapart\nof the teaching process.\u201d Arthur Samuel\u2019s work (1959) was probably the earliest successful\nmachine learning research. Although this work was informal and had a number of flaws,\nit contained most of the modern ideas in reinforcement learning, including temporal differ-\nencing and function approximation. Around the same time, researchers in adaptive control\ntheory(WidrowandHoff,1960),buildingonworkbyHebb(1949),weretrainingsimplenet-\nworksusingthedeltarule. (Thisearlyconnectionbetweenneuralnetworksandreinforcement\nlearning may have led to the persistent misperception that the latter is a subfield of the for-\nmer.) Thecart\u2013poleworkofMichieandChambers(1968)canalsobeseenasareinforcement\nlearningmethodwithafunctionapproximator. Thepsychologicalliteratureonreinforcement\nlearningismucholder;HilgardandBower(1975)provideagoodsurvey. Directevidencefor\nthe operation of reinforcement learning in animals has been provided by investigations into\ntheforagingbehaviorofbees;thereisaclearneuralcorrelateoftherewardsignalintheform\nofalarge neuron mapping from thenectar intake sensors directly tothe motorcortex (Mon-\ntague et al., 1995). Research using single-cell recording suggests that the dopamine system\nin primate brains implements something resembling value function learning (Schultz et al.,\n1997). The neuroscience text by Dayan and Abbott (2001) describes possible neural imple-\nmentations of temporal-difference learning, while Dayan and Niv (2008) survey the latest\nevidence fromneuroscientific andbehavioral experiments.\nThe connection between reinforcement learning and Markov decision processes was\nfirst made by Werbos (1977), but the development of reinforcement learning in AI stems\nfrom work at the University of Massachusetts in the early 1980s (Barto et al., 1981). The\npaper by Sutton (1988) provides a good historical overview. Equation (21.3) in this chapter\nis a special case for \u03bb=0 of Sutton\u2019s general TD(\u03bb) algorithm. TD(\u03bb) updates the utility\nvaluesofallstatesinasequence leading uptoeachtransition byanamountthatdropsoffas\n\u03bbt for states t steps in the past. TD(1) is identical to the Widrow\u2013Hoff ordelta rule. Boyan\n(2002), building on work by Bradtke and Barto (1996), argues that TD(\u03bb) and related algo-\nrithmsmake inefficient useofexperiences; essentially, theyareonline regression algorithms\nthat converge much more slowly than offline regression. His LSTD (least-squares temporal\ndifferencing) algorithm is an online algorithm for passive reinforcement learning that gives\nthe same results as offline regression. Least-squares policy iteration, or LSPI (Lagoudakis\nand Parr, 2003), combines this idea with the policy iteration algorithm, yielding a robust,\nstatistically efficient,model-free algorithm forlearningpolicies.\nThe combination of temporal-difference learning with the model-based generation of\nsimulated experiences wasproposed inSutton\u2019s DYNAarchitecture (Sutton, 1990). Theidea\nof prioritized sweeping was introduced independently by Moore and Atkeson (1993) and Bibliographical andHistorical Notes 855\nPengandWilliams(1993). Q-learningwasdevelopedinWatkins\u2019sPh.D.thesis(1989),while\nSARSAappeared inatechnical reportbyRummeryandNiranjan (1994).\nBanditproblems, whichmodeltheproblem ofexploration for nonsequential decisions,\nareanalyzedindepthbyBerryandFristedt(1985). Optimalexplorationstrategiesforseveral\nsettings are obtainable using the technique called Gittins indices (Gittins, 1989). A vari-\nety of exploration methods for sequential decision problems are discussed by Barto et al.\n(1995). Kearns and Singh (1998) and Brafman and Tennenholtz (2000) describe algorithms\nthatexploreunknown environments andareguaranteed toconverge onnear-optimal policies\nin polynomial time. Bayesian reinforcement learning (Dearden et al., 1998, 1999) provides\nanotherangleonbothmodeluncertainty andexploration.\nFunction approximation in reinforcement learning goes back to the work of Samuel,\nwhousedbothlinearandnonlinearevaluationfunctionsandalsousedfeature-selectionmeth-\nodstoreduce thefeature space. Latermethods include the CMAC(Cerebellar ModelArtic-\nCMAC\nulation Controller) (Albus, 1975), which is essentially a sum of overlapping local kernel\nfunctions, and the associative neural networks of Barto et al. (1983). Neural networks are\ncurrently the most popular form of function approximator. The best-known application is\nTD-Gammon (Tesauro, 1992, 1995), which was discussed in the chapter. One significant\nproblem exhibitedbyneural-network-based TDlearners isthattheytendtoforgetearlierex-\nperiences, especially those in parts of the state space that are avoided once competence is\nachieved. Thiscanresultincatastrophic failureifsuchcircumstances reappear. Functionap-\nproximation based on instance-based learning can avoid this problem (Ormoneit and Sen,\n2002;Forbes,2002).\nTheconvergence ofreinforcement learningalgorithmsusingfunction approximation is\nan extremely technical subject. Results for TD learning have been progressively strength-\nenedforthecaseoflinearfunctionapproximators (Sutton, 1988;Dayan,1992;Tsitsiklisand\nVanRoy, 1997), but several examples ofdivergence have been presented fornonlinear func-\ntions (see Tsitsiklis and Van Roy, 1997, for a discussion). Papavassiliou and Russell (1999)\ndescribe a new type of reinforcement learning that converges with any form of function ap-\nproximator, providedthatabest-fitapproximation canbefoundfortheobserveddata.\nPolicysearchmethodswerebroughttotheforebyWilliams(1992),whodevelopedthe\nREINFORCE familyofalgorithms. LaterworkbyMarbachandTsitsiklis(1998),Suttonetal.\n(2000), and Baxterand Bartlett(2000) strengthened and generalized theconvergence results\nforpolicysearch. Themethodofcorrelated sampling forcomparing different configurations\nof a system was described formally by Kahn and Marshall (1953), but seems to have been\nknown long before that. Its use in reinforcement learning is due to Van Roy (1998) and Ng\nand Jordan (2000); the latter paper also introduced the PEGASUS algorithm and proved its\nformalproperties.\nAs we mentioned in the chapter, the performance of a stochastic policy is a continu-\nous function of its parameters, which helps with gradient-based search methods. This is not\nthe only benefit: Jaakkola et al. (1995) argue that stochastic policies actually work better\nthan deterministic policies in partially observable environments, if both are limited to act-\ning based on the current percept. (One reason is that the stochastic policy is less likely to\nget \u201cstuck\u201d because of some unseen hindrance.) Now, in Chapter 17 we pointed out that 856 Chapter 21. Reinforcement Learning\noptimal policies in partially observable MDPs are deterministic functions of the belief state\nratherthanthecurrentpercept, sowewouldexpectstillbetterresultsbykeepingtrackofthe\nbelief state using the filtering methods of Chapter 15. Unfortunately, belief-state space is\nhigh-dimensional and continuous, and effective algorithms have not yet been developed for\nreinforcement learningwithbeliefstates.\nReal-world environments also exhibit enormous complexity in terms of the number\nof primitive actions required to achieve significant reward. For example, a robot playing\nsoccer might make a hundred thousand individual leg motions before scoring a goal. One\ncommonmethod,usedoriginallyinanimaltraining,iscalledrewardshaping. Thisinvolves\nREWARDSHAPING\nsupplying the agent with additional rewards, called pseudorewards, for\u201cmaking progress.\u201d\nPSEUDOREWARD\nFor example, in soccer the real reward is for scoring a goal, but pseudorewards might be\ngiven for making contact with the ball or for kicking it toward the goal. Such rewards can\nspeed up learning enormously and are simple to provide, but there is a risk that the agent\nwilllearntomaximizethepseudorewards ratherthanthetruerewards;forexample,standing\nnext to the ball and \u201cvibrating\u201d causes many contacts with the ball. Ng et al. (1999) show\n(cid:2)\nthat the agent will still learn the optimal policy provided that the pseudoreward F(s,a,s)\nsatisfies F(s,a,s(cid:2) )=\u03b3\u03a6(s(cid:2) )\u2212\u03a6(s),where \u03a6isanarbitrary function ofthe state. \u03a6canbe\nconstructed to reflect any desirable aspects of the state, such as achievement of subgoals or\ndistance toagoalstate.\nThegeneration ofcomplexbehaviorscanalsobefacilitated byhierarchicalreinforce-\nHIERARCHICAL\nmentlearningmethods, whichattempttosolveproblemsatmultiplelevels ofabstraction\u2014\nREINFORCEMENT\nLEARNING\nmuch like the HTNplanningmethods of Chapter11. Forexample, \u201cscoring agoal\u201d can be\nbroken down into \u201cobtain possession,\u201d \u201cdribble towards the goal,\u201d and \u201cshoot;\u201d and each of\nthese can be broken down further into lower-level motor behaviors. The fundamental result\nin this area is due to Forestier and Varaiya (1978), who proved that lower-level behaviors\nof arbitrary complexity can be treated just like primitive actions (albeit ones that can take\nvarying amounts of time) from the point of view of the higher-level behavior that invokes\nthem. Current approaches (Parr and Russell, 1998; Dietterich, 2000; Sutton et al., 2000;\nAndre and Russell, 2002) build on this result to develop methods for supplying an agent\nwithapartial program thatconstrains theagent\u2019s behavior tohaveaparticular hierarchical\nPARTIALPROGRAM\nstructure. The partial-programming language for agent programs extends an ordinary pro-\ngramming language by adding primitives for unspecified choices that must be filled in by\nlearning. Reinforcement learning is then applied to learn the best behavior consistent with\nthe partial program. The combination of function approximation, shaping, and hierarchical\nreinforcement learning hasbeenshowntosolvelarge-scale problems\u2014forexample,policies\nthatexecutefor104stepsinstatespacesof10100stateswithbranchingfactorsof1030(Marthi\net al., 2005). One key result (Dietterich, 2000) is that the hierarchical structure provides a\nnatural additive decomposition oftheoverallutility function intotermsthatdepend onsmall\nsubsetsofthevariablesdefiningthestatespace. Thisissomewhatanalogoustotherepresen-\ntationtheoremsunderlying theconciseness ofBayesnets(Chapter14).\nThetopicofdistributedandmultiagentreinforcementlearningwasnottoucheduponin\nthechapterbutisofgreatcurrent interest. Indistributed RL,theaimistodevisemethodsby\nwhichmultiple,coordinatedagentslearntooptimizeacommonutilityfunction. Forexample, Bibliographical andHistorical Notes 857\ncan wedevise methods whereby separate subagents forrobot navigation and robot obstacle\nSUBAGENT\navoidance could cooperatively achieve a combined control system that is globally optimal?\nSome basic results in this direction have been obtained (Guestrin et al., 2002; Russell and\nZimdars, 2003). The basic idea is that each subagent learns its own Q-function from its\nownstream of rewards. Forexample, arobot-navigation component can receive rewards for\nmakingprogresstowardsthegoal,whiletheobstacle-avoidance componentreceivesnegative\nrewards foreverycollision. Eachglobal decision maximizes thesumofQ-functions andthe\nwholeprocessconverges toglobally optimalsolutions.\nMultiagent RL is distinguished from distributed RL by the presence of agents who\ncannot coordinate their actions (except by explicit communicative acts) and who may not\nshare the same utility function. Thus, multiagent RL deals with sequential game-theoretic\nproblems or Markovgames, asdefined inChapter17. Theconsequent requirement forran-\ndomized policies isnot asignificant complication, aswesawonpage 848. Whatdoes cause\nproblems is the fact that, while an agent is learning to defeat its opponent\u2019s policy, the op-\nponent is changing its policy to defeat the agent. Thus, the environment is nonstationary\n(seepage568). Littman(1994)notedthisdifficultywhenintroducing thefirstRLalgorithms\nfor zero-sum Markov games. Hu and Wellman (2003) present a Q-learning algorithm for\ngeneral-sumgamesthatconvergeswhentheNashequilibrium isunique;whentherearemul-\ntipleequilibria, thenotionofconvergence isnotsoeasyto define(Shohametal.,2004).\nSometimestherewardfunction isnoteasytodefine. Consider thetaskofdrivingacar.\nThere are extreme states (such as crashing the car) that clearly should have a large penalty.\nBut beyond that, it is difficult to be precise about the reward function. However, it is easy\nenough forahumantodriveforawhileandthentellarobot\u201cdo itlikethat.\u201d Therobotthen\nAPPRENTICESHIP has the task of apprenticeship learning; learning from an example of the task done right,\nLEARNING\nwithout explicit rewards. Ng etal. (2004) and Coates etal. (2009) show how this technique\nworks for learning to fly a helicopter; see Figure 25.25 on page 1002 for an example of the\nacrobatics the resulting policy is capable of. Russell (1998) describes the task of inverse\nINVERSE\nreinforcement learning\u2014figuring out what the reward function must be from an example\nREINFORCEMENT\nLEARNING\npath through that state space. Thisis useful as a part of apprenticeship learning, oras a part\nofdoingscience\u2014wecanunderstand ananimalorrobotbyworkingbackwardsfromwhatit\ndoestowhatitsrewardfunction mustbe.\nThischapterhasdealt onlywithatomicstates\u2014all theagent knowsabout astate isthe\nset of available actions and the utilities of the resulting states (or of state-action pairs). But\nit is also possible to apply reinforcement learning to structured representations rather than\nRELATIONAL\natomicones;thisiscalledrelational reinforcementlearning(Tadepallietal.,2004).\nREINFORCEMENT\nLEARNING\nThesurveybyKaelblingetal.(1996)providesagoodentrypointtotheliterature. The\ntextbySuttonandBarto(1998),twoofthefield\u2019spioneers,focusesonarchitecturesandalgo-\nrithms,showinghowreinforcement learning weavestogethertheideasoflearning, planning,\nand acting. The somewhat more technical work by Bertsekas and Tsitsiklis (1996) gives a\nrigorous grounding in the theory of dynamic programming and stochastic convergence. Re-\ninforcement learning papers arepublished frequently in MachineLearning, intheJournalof\nMachineLearning Research, andintheInternational Conferences onMachine Learning and\ntheNeuralInformation Processing Systemsmeetings. 858 Chapter 21. Reinforcement Learning\nEXERCISES\n21.1 Implement apassive learning agent inasimpleenvironment, suchasthe4\u00d73world.\nForthe case of an initially unknown environment model, compare the learning performance\nofthedirect utility estimation, TD,andADPalgorithms. Dothecomparison fortheoptimal\npolicy and for several random policies. For which do the utility estimates converge faster?\nWhat happens when the size of the environment is increased? (Try environments with and\nwithoutobstacles.)\n21.2 Chapter 17 defined a proper policy for an MDP as one that is guaranteed to reach a\nterminal state. Show that it is possible for a passive ADP agent to learn a transition model\nfor which its policy \u03c0 is improper even if \u03c0 is proper for the true MDP; with such models,\nthe POLICY-EVALUATION step may fail if \u03b3=1. Show that this problem cannot arise if\nPOLICY-EVALUATION isapplied tothelearnedmodelonlyattheendofatrial.\n21.3 Starting withthepassive ADPagent, modifyittouseanapproximate ADPalgorithm\nasdiscussed inthetext. Dothisintwosteps:\na. Implementapriorityqueueforadjustments totheutilityestimates. Wheneverastateis\nadjusted, all of its predecessors also become candidates for adjustment and should be\nadded to the queue. Thequeue is initialized with the state from which the most recent\ntransition tookplace. Allowonlyafixednumberofadjustments.\nb. Experiment with various heuristics forordering thepriority queue, examining theiref-\nfectonlearning ratesandcomputation time.\n21.4 Writeouttheparameterupdateequations forTDlearningwith\n(cid:9)\nU\u02c6(x,y) = \u03b8 +\u03b8 x+\u03b8 y+\u03b8 (x\u2212x )2+(y\u2212y )2 .\n0 1 2 3 g g\n21.5 Implement an exploring reinforcement learning agent that uses direct utility estima-\ntion. Make two versions\u2014one with a tabular representation and one using the function ap-\nproximatorinEquation(21.10). Comparetheirperformance inthreeenvironments:\na. The4\u00d73worlddescribed inthechapter.\nb. A10\u00d710worldwithnoobstacles anda+1rewardat(10,10).\nc. A10\u00d710worldwithnoobstacles anda+1rewardat(5,5).\n21.6 Devisesuitable features forreinforcement learning instochastic gridworlds(general-\nizations ofthe 4\u00d73 world) that contain multiple obstacles and multiple terminal states with\nrewardsof +1or\u22121.\n21.7 Extend the standard game-playing environment (Chapter 5) to incorporate a reward\nsignal. Put two reinforcement learning agents into the environment (they may, of course,\nshare the agent program) and have them play against each other. Apply the generalized TD\nupdaterule(Equation(21.12))toupdatetheevaluationfunction. Youmightwishtostartwith\nasimplelinearweightedevaluation function andasimplegame,suchastic-tac-toe. Exercises 859\n21.8 Compute the true utility function and the best linear approximation in x and y (as in\nEquation(21.10))forthefollowingenvironments:\na. A10\u00d710worldwithasingle +1terminalstateat(10,10).\nb. Asin(a),butadda\u22121terminalstateat(10,1).\nc. Asin(b),butaddobstacles in10randomly selectedsquares.\nd. Asin(b),butplaceawallstretching from(5,2)to(5,9).\ne. Asin(a),butwiththeterminalstateat(5,5).\nTheactions aredeterministic movesinthefourdirections. Ineach case, compare the results\nusing three-dimensional plots. Foreach environment, propose additional features (besides x\nandy)thatwouldimprovetheapproximation andshowtheresults.\n21.9 Implement the REINFORCE and PEGASUS algorithms and apply them to the 4\u00d73\nworld,usingapolicyfamilyofyourownchoosing. Commentontheresults.\n21.10 Isreinforcementlearninganappropriateabstractmodelforevolution? Whatconnec-\ntionexists,ifany,betweenhardwiredrewardsignalsandevolutionary fitness? 22\nNATURAL LANGUAGE\nPROCESSING\nIn which we see how to make use of the copious knowledge that is expressed in\nnaturallanguage.\nHomosapiensissetapartfromotherspeciesbythecapacityforlanguage. Somewherearound\n100,000yearsago,humanslearnedhowtospeak,andabout7,000yearsagolearnedtowrite.\nAlthoughchimpanzees, dolphins, andotheranimalshaveshownvocabularies ofhundreds of\nsigns,onlyhumanscanreliablycommunicateanunboundednumberofqualitativelydifferent\nmessagesonanytopicusingdiscretesigns.\nOf course, there are other attributes that are uniquely human: no other species wears\nclothes, creates representational art, or watches three hours of television a day. But when\nAlan Turing proposed his Test (see Section 1.1.1), he based it on language, not art or TV.\nThere are two main reasons why we want our computer agents to be able to process natural\nlanguages: first,tocommunicatewithhumans,atopicwetakeupinChapter23,andsecond,\ntoacquire information fromwrittenlanguage, thefocusofthischapter.\nThere are over a trillion pages of information on the Web, almost all of it in natural\nKNOWLEDGE language. An agent that wants to do knowledge acquisition needs to understand (at least\nACQUISITION\npartially) the ambiguous, messy languages that humans use. We examine the problem from\nthe point of view of specific information-seeking tasks: text classification, information re-\ntrieval,andinformationextraction. Onecommonfactorinaddressingthesetasksistheuseof\nlanguagemodels: modelsthatpredicttheprobability distribution oflanguage expressions.\nLANGUAGEMODEL\n22.1 LANGUAGE MODELS\nFormallanguages,suchastheprogramminglanguagesJavaorPython,havepreciselydefined\nLANGUAGE language models. A language can be defined as a set of strings; \u201cprint(2 + 2)\u201d is a\nlegal program inthe language Python, whereas \u201c2)+(2 print\u201disnot. Since there are an\ninfinitenumberoflegalprograms,theycannotbeenumerated; insteadtheyarespecifiedbya\nset of rules called a grammar. Formallanguages also have rules that define the meaning or\nGRAMMAR\nSEMANTICS semanticsofaprogram;forexample,therulessaythatthe\u201cmeaning\u201d of \u201c2 + 2\u201dis4,and\nthemeaningof\u201c1\/0\u201disthatanerrorissignaled.\n860 Section22.1. LanguageModels 861\nNatural languages, such as English or Spanish, cannot be characterized as a definitive\nset of sentences. Everyone agrees that \u201cNot to be invited is sad\u201d is a sentence of English,\nbutpeople disagree onthegrammaticality of\u201cTobenotinvited issad.\u201d Therefore, itismore\nfruitful todefineanatural language model asaprobability distribution oversentences rather\nthan a definitive set. That is, rather than asking if a string of words is or is not a memberof\nthesetdefiningthelanguage, weinsteadaskforP(S=words)\u2014whatistheprobability that\narandom sentencewouldbewords.\nNaturallanguagesarealsoambiguous. \u201cHesawherduck\u201dcanmeaneitherthathesaw\nAMBIGUITY\na waterfowl belonging to her, or that he saw her move to evade something. Thus, again, we\ncannot speak ofasingle meaning forasentence, but rather of aprobability distribution over\npossible meanings.\nFinally, natural languages are difficult to deal with because they are very large, and\nconstantly changing. Thus, our language models are, at best, an approximation. We start\nwiththesimplestpossible approximations andmoveupfromthere.\n22.1.1 N-gram character models\nUltimately,awrittentextiscomposedofcharacters\u2014letters, digits,punctuation, andspaces\nCHARACTERS\nin English (and more exotic characters in some other languages). Thus, one of the simplest\nlanguagemodelsisaprobability distribution oversequencesofcharacters. AsinChapter15,\nwe write P(c ) for the probability of a sequence of N characters, c through c . In one\n1:N 1 N\nWebcollection,P(\u201cthe\u201d)=0.027andP(\u201czgq\u201d)=0.000000002. Asequenceofwrittensym-\nbolsoflength niscalledann-gram(from theGreekrootforwritingorletters), withspecial\ncase \u201cunigram\u201d for1-gram, \u201cbigram\u201d for2-gram, and \u201ctrigram\u201d for3-gram. A model ofthe\nN probability distribution of n-lettersequences is thus called an n-gram model. (Butbe care-\n-GRAMMODEL\nful: we can have n-gram models over sequences of words, syllables, or other units; not just\novercharacters.)\nAnn-gram modelisdefinedasaMarkovchainoforder n\u22121. Recallfrompage 568\nthat in a Markov chain the probability of character c depends only on the immediately pre-\ni\nceding characters, not on any other characters. So in a trigram model (Markov chain of\norder2)wehave\nP(c i|c 1:i\u22121) = P(c i|c i\u22122:i\u22121).\nWe can define the probability of a sequence of characters P(c ) under the trigram model\n1:N\nbyfirstfactoring withthechainruleandthenusingtheMarkovassumption:\n(cid:25)N (cid:25)N\nP(c 1:N) = P(c i|c 1:i\u22121)= P(c i|c i\u22122:i\u22121).\ni=1 i=1\nForatrigramcharactermodelinalanguagewith100characters,P(C i|C i\u22122:i\u22121)hasamillion\nentries, andcanbeaccurately estimated bycounting charactersequences inabodyoftextof\n10 million characters or more. We call a body of text a corpus (plural corpora), from the\nCORPUS\nLatinwordforbody. 862 Chapter 22. NaturalLanguageProcessing\nWhatcanwedowithn-gramcharactermodels? Onetaskforwhichtheyarewellsuited\nLANGUAGE islanguageidentification: givenatext,determinewhatnaturallanguageitiswrittenin. This\nIDENTIFICATION\nis arelatively easy task; evenwith short texts such as \u201cHello, world\u201d or\u201cWie geht es dir,\u201d it\niseasy toidentify thefirstasEnglish andthesecond asGerman. Computersystems identify\nlanguages with greater than 99% accuracy; occasionally, closely related languages, such as\nSwedishandNorwegian,areconfused.\nOne approach to language identification is to first build a trigram character model of\neach candidate language, P(c i|c i\u22122:i\u22121,(cid:3)), wherethe variable (cid:3)ranges overlanguages. For\neach (cid:3) the model is built by counting trigrams in acorpus of that language. (About 100,000\ncharacters of each language are needed.) That gives us a model of P(Text|Language), but\nwewanttoselectthemostprobablelanguagegiventhetext,soweapplyBayes\u2019rulefollowed\nbytheMarkovassumption togetthemostprobable language:\n(cid:3)\u2217 = argmax P((cid:3)|c )\n1:N\n(cid:3)\n= argmax P((cid:3))P(c |(cid:3))\n1:N\n(cid:3)\n(cid:25)N\n= argmax P((cid:3)) P(c i|c i\u22122:i\u22121,(cid:3))\n(cid:3)\ni=1\nThetrigram model canbe learned from acorpus, but whatabout thepriorprobability P((cid:3))?\nWemayhave someestimate ofthese values; forexample, ifweareselecting arandom Web\npageweknowthatEnglishisthemostlikelylanguageandthattheprobabilityofMacedonian\nwill be less than 1%. The exact number weselect forthese priors is not critical because the\ntrigrammodelusuallyselectsonelanguagethatisseveralordersofmagnitudemoreprobable\nthananyother.\nOther tasks for character models include spelling correction, genre classification, and\nnamed-entity recognition. Genre classification means deciding if a text is a news story, a\nlegal document, a scientific article, etc. While many features help make this classification,\ncounts of punctuation and other character n-gram features go a long way (Kessler et al.,\n1997). Named-entity recognition is the task of finding names of things in a document and\ndeciding whatclasstheybelong to. Forexample, inthetext\u201cMr. Sopersteen wasprescribed\naciphex,\u201dweshouldrecognizethat\u201cMr. Sopersteen\u201disthenameofapersonand\u201caciphex\u201dis\nthenameofadrug. Character-level modelsaregoodforthistaskbecause theycanassociate\nthecharactersequence \u201cex \u201d(\u201cex\u201dfollowedbyaspace)withadrugnameand\u201csteen \u201dwith\napersonname,andtherebyidentifywordsthattheyhaveneverseenbefore.\n22.1.2 Smoothing n-gram models\nThe major complication of n-gram models is that the training corpus provides only an esti-\nmateofthetrueprobability distribution. Forcommoncharacter sequences suchas\u201c th\u201dany\nEnglishcorpuswillgiveagoodestimate: about1.5%ofalltrigrams. Ontheotherhand,\u201c ht\u201d\nis very uncommon\u2014no dictionary words start with ht. It is likely that the sequence would\nhave acount ofzero inatraining corpus ofstandard English. Does that meanweshould as-\nsignP(\u201c th\u201d)=0? Ifwedid,thenthetext\u201cTheprogram issuesanhttprequest\u201d wouldhave Section22.1. LanguageModels 863\nanEnglishprobabilityofzero,whichseemswrong. Wehaveaproblemingeneralization: we\nwant ourlanguage models to generalize well totexts they haven\u2019t seen yet. Just because we\nhaveneverseen\u201c http\u201dbefore doesnotmeanthatourmodelshould claimthatit isimpossi-\nble. Thus, we will adjust our language model so that sequences that have a count of zero in\nthetraining corpuswillbeassignedasmallnonzeroprobability (andtheothercountswillbe\nadjusted downward slightly sothat theprobability still sumsto1). Theprocess od adjusting\ntheprobability oflow-frequency countsiscalled smoothing.\nSMOOTHING\nThesimplesttypeofsmoothingwassuggestedbyPierre-SimonLaplaceinthe18thcen-\ntury: hesaidthat,inthelackoffurtherinformation,ifarandomBooleanvariableX hasbeen\nfalseinallnobservationssofarthentheestimateforP(X=true)shouldbe1\/(n+2). That\nis,heassumesthatwithtwomoretrials, onemightbetrueand onefalse. Laplacesmoothing\n(alsocalledadd-onesmoothing)isastepintherightdirection,butperformsrelativelypoorly.\nAbetterapproachisabackoffmodel,inwhichwestartbyestimatingn-gramcounts,butfor\nBACKOFFMODEL\nanyparticularsequencethathasalow(orzero)count,webackoffto(n\u22121)-grams. Linear\nLINEAR\ninterpolation smoothing is a backoff model that combines trigram, bigram, and unigram\nINTERPOLATION\nSMOOTHING\nmodelsbylinearinterpolation. Itdefinestheprobability estimateas\nP* (c i|c i\u22122:i\u22121)= \u03bb 3P(c i|c i\u22122:i\u22121)+\u03bb 2P(c i|c i\u22121)+\u03bb 1P(c i),\nwhere \u03bb +\u03bb +\u03bb =1. The parameter values \u03bb can be fixed, or they can be trained with\n3 2 1 i\nan expectation\u2013maximization algorithm. It is also possible to have the values of \u03bb depend\ni\non the counts: if we have a high count of trigrams, then we weigh them relatively more; if\nonlyalowcount,thenweputmoreweightonthebigramandunigrammodels. Onecampof\nresearchers has developed evermore sophisticated smoothing models, while the other camp\nsuggestsgatheringalargercorpussothatevensimplesmoothingmodelsworkwell. Bothare\ngettingatthesamegoal: reducing thevarianceinthelanguage model.\nOnecomplication: note that the expression P(c i|c i\u22122:i\u22121) asks for P(c 1|c-1:0) when\ni = 1, but there are no characters before c . We can introduce artificial characters, for\n1\nexample, defining c to be a space character or a special \u201cbegin text\u201d character. Or we can\n0\nfall back on lower-order Markov models, in effect defining c-1:0 to be the empty sequence\nandthusP(c 1|c-1:0)=P(c 1).\n22.1.3 Model evaluation\nWith so many possible n-gram models\u2014unigram, bigram, trigram, interpolated smoothing\nwithdifferent values of \u03bb,etc.\u2014howdoweknow whatmodeltochoose? Wecanevaluate a\nmodel with cross-validation. Split the corpus into a training corpus and a validation corpus.\nDetermine the parameters of the model from the training data. Then evaluate the model on\nthevalidation corpus.\nThe evaluation can be a task-specific metric, such as measuring accuracy on language\nidentification. Alternatively wecanhave atask-independent model oflanguage quality: cal-\nculate the probability assigned to the validation corpus by the model; the higher the proba-\nbility the better. This metric is inconvenient because the probability of a large corpus will\nbe a very small number, and floating-point underflow becomes an issue. A different way of\ndescribing theprobability ofasequenceiswithameasurecalledperplexity,definedas\nPERPLEXITY 864 Chapter 22. NaturalLanguageProcessing\n\u22121\nPerplexity(c 1:N)= P(c 1:N) N .\nPerplexity canbethought ofasthereciprocal ofprobability, normalized bysequence length.\nItcanalsobethoughtofastheweightedaveragebranching factorofamodel. Supposethere\nare 100 characters in our language, and our model says they are all equally likely. Then for\nasequence ofanylength, theperplexity willbe100. Ifsomecharacters aremorelikely than\nothers, andthemodelreflectsthat,thenthemodelwillhaveaperplexity lessthan100.\n22.1.4 N-gram wordmodels\nNow weturn to n-gram models overwords rather than characters. Allthe same mechanism\napplies equally towordandcharacter models. Themaindifference isthatthevocabulary\u2014\nVOCABULARY\nthe set of symbols that make up the corpus and the model\u2014is larger. There are only about\n100 characters in most languages, and sometimes we build character models that are even\nmore restrictive, for example by treating \u201cA\u201d and \u201ca\u201d as the same symbol or by treating all\npunctuation asthesamesymbol. Butwithwordmodelswehaveatleasttensofthousands of\nsymbols,andsometimesmillions. Thewiderangeisbecauseitisnotclearwhatconstitutesa\nword. InEnglishasequenceofletterssurroundedbyspacesisaword,butinsomelanguages,\nlikeChinese,wordsarenotseparatedbyspaces,andeveninEnglishmanydecisionsmustbe\nmadetohaveaclearpolicyonwordboundaries: howmanywordsarein\u201cne\u2019er-do-well\u201d? Or\nin\u201c(Tel:1-800-960-5660x123)\u201d?\nOUTOF Wordn-grammodelsneedtodealwithoutofvocabularywords. Withcharactermod-\nVOCABULARY\nels, we didn\u2019t have to worry about someone inventing a new letter of the alphabet.1 But\nwithwordmodels thereisalwaysthechance ofanewwordthatwasnotseen inthetraining\ncorpus, so we need to model that explicitly in our language model. This can be done by\nadding just one new word to the vocabulary: <UNK>, standing for the unknown word. We\ncan estimate n-gram counts for <UNK> by this trick: go through the training corpus, and\nthe first time any individual word appears it is previously unknown, so replace it with the\nsymbol <UNK>. Allsubsequent appearances of the word remain unchanged. Then compute\nn-gram counts forthe corpus as usual, treating <UNK>just like anyother word. Then when\nanunknown wordappears inatestset, welook upitsprobability under <UNK>. Sometimes\nmultiple unknown-word symbols are used, for different classes. For example, any string of\ndigitsmightbereplaced with<NUM>,oranyemailaddress with<EMAIL>.\nTo get a feeling for what word models can do, we built unigram, bigram, and trigram\nmodelsoverthewordsinthisbookandthenrandomlysampledsequences ofwordsfromthe\nmodels. Theresultsare\nUnigram: logical areasareconfusion amayrighttriesagentgoalthewas...\nBigram: systemsareverysimilarcomputational approach wouldberepresented ...\nTrigram: planning andscheduling areintegrated thesuccess ofnaive bayesmodelis...\nEvenwiththissmallsample,itshouldbeclearthattheunigrammodelisapoorapproximation\nofeitherEnglishorthecontentofanAItextbook,andthatthebigramandtrigrammodelsare\n1 WiththepossibleexceptionofthegroundbreakingworkofT.Geisel(1955). Section22.2. TextClassification 865\nmuchbetter. Themodelsagreewiththisassessment: theperplexity was891fortheunigram\nmodel,142forthebigram modeland91forthetrigrammodel.\nWith the basics of n-gram models\u2014both character- and word-based\u2014established, we\ncanturnnowtosomelanguage tasks.\n22.2 TEXT CLASSIFICATION\nTEXT Wenowconsiderindepththetaskoftextclassification,alsoknownascategorization: given\nCLASSIFICATION\natextofsomekind,decidewhichofapredefinedsetofclassesitbelongsto. Languageiden-\ntification and genre classification areexamples oftextclassification, asissentiment analysis\n(classifying amovieorproductreviewaspositiveornegative)andspamdetection(classify-\nSPAMDETECTION\ninganemailmessageasspamornot-spam). Since\u201cnot-spam\u201d isawkward,researchers have\ncoined the term ham fornot-spam. We can treat spam detection as a problem in supervised\nlearning. A training set is readily available: the positive (spam) examples are in my spam\nfolder, thenegative(ham)examplesareinmyinbox. Hereisanexcerpt:\nSpam:WholesaleFashionWatches-57%today.Designerwatchesforcheap...\nSpam:YoucanbuyViagraFr$1.85AllMedicationsatunbeatableprices! ...\nSpam:WECANTREATANYTHINGYOUSUFFERFROMJUSTTRUSTUS...\nSpam:Sta.rtearn*ingthesalaryyo,ud-eservebyo\u2019btainingtheprope,rcrede\u2019ntials!\nHam:Thepracticalsignificanceofhypertreewidthinidentifyingmore...\nHam:Abstract:Wewillmotivatetheproblemofsocialidentityclustering:...\nHam:Goodtoseeyoumyfriend.HeyPeter,Itwasgoodtohearfromyou....\nHam:PDSimpliesconvexityoftheresultingoptimizationproblem(KernelRidge...\nFrom this excerpt we can start to get an idea of what might be good features to include in\nthe supervised learning model. Word n-grams such as \u201cforcheap\u201d and \u201cYou can buy\u201d seem\nto be indicators of spam (although they would have a nonzero probability in ham as well).\nCharacter-level features also seem important: spam ismore likely tobe alluppercase and to\nhavepunctuationembeddedinwords. Apparentlythespammersthoughtthatthewordbigram\n\u201cyou deserve\u201d would be too indicative of spam, and thus wrote \u201cyo,u d-eserve\u201d instead. A\ncharacter model should detect this. We could either create a full character n-gram model\nof spam and ham, or we could handcraft features such as \u201cnumber of punctuation marks\nembeddedinwords.\u201d\nNote that we have two complementary ways of talking about classification. In the\nlanguage-modeling approach, wedefineonen-gramlanguagemodelforP(Message|spam)\nbytrainingonthespamfolder,andonemodelforP(Message|ham)bytrainingontheinbox.\nThenwecanclassify anewmessagewithanapplication ofBayes\u2019rule:\nargmax P(c|message) = argmax P(message|c)P(c).\nc\u2208{spam,ham} c\u2208{spam,ham}\nwhere P(c)isestimated justby counting thetotal numberofspam and ham messages. This\napproach workswellforspamdetection, justasitdidforlanguage identification. 866 Chapter 22. NaturalLanguageProcessing\nIn the machine-learning approach we represent the message as a set of feature\/value\npairs and apply a classification algorithm h to the feature vector X. We can make the\nlanguage-modeling andmachine-learning approachescompatiblebythinkingofthen-grams\nas features. This is easiest to see with a unigram model. The features are the words in the\nvocabulary: \u201ca,\u201d \u201caardvark,\u201d ..., and the values are the number of times each word appears\ninthemessage. Thatmakesthefeaturevectorlargeandsparse. Ifthereare100,000wordsin\nthelanguagemodel,thenthefeaturevectorhaslength100,000,butforashortemailmessage\nalmostallthefeatures willhavecount zero. Thisunigram representation hasbeen called the\nbagofwordsmodel. Youcanthink ofthemodelasputting thewordsofthetraining corpus\nBAGOFWORDS\nin a bag and then selecting words one at a time. The notion of order of the words is lost; a\nunigrammodelgivesthesameprobability toanypermutation ofatext. Higher-order n-gram\nmodelsmaintainsomelocalnotionofwordorder.\nWithbigramsandtrigramsthenumberoffeatures issquared orcubed, andwecanadd\nin other, non-n-gram features: the time the message was sent, whether a URL or an image\nis part of the message, an ID number for the sender of the message, the sender\u2019s number of\npreviousspamandhammessages,andsoon. Thechoiceoffeaturesisthemostimportantpart\nofcreatingagoodspamdetector\u2014moreimportantthanthechoiceofalgorithmforprocessing\nthe features. In part this is because there is a lot of training data, so if we can propose a\nfeature, the data can accurately determine if it is good or not. It is necessary to constantly\nupdate features, because spam detection is an adversarial task; the spammers modify their\nspaminresponse tothespamdetector\u2019s changes.\nItcan be expensive to run algorithms on a very large feature vector, so often aprocess\noffeatureselectionisusedtokeeponlythefeaturesthatbestdiscriminatebetweenspamand\nFEATURESELECTION\nham. Forexample,thebigram\u201cofthe\u201disfrequentinEnglish, andmaybeequallyfrequentin\nspam and ham, sothere isno sense incounting it. Often the top hundred orsofeatures do a\ngoodjobofdiscriminating betweenclasses.\nOnce we have chosen a set of features, we can apply any of the supervised learning\ntechniques we have seen; popular ones for text categorization include k-nearest-neighbors,\nsupport vector machines, decision trees, naive Bayes, and logistic regression. All of these\nhave been applied to spam detection, usually with accuracy in the 98%\u201399% range. With a\ncarefully designed featureset,accuracy canexceed99.9%.\n22.2.1 Classificationby data compression\nAnother way to think about classification is as a problem in data compression. A lossless\nDATACOMPRESSION\ncompressionalgorithmtakesasequenceofsymbols,detectsrepeatedpatternsinit,andwrites\na description of the sequence that is more compact than the original. For example, the text\n\u201c0.142857142857142857\u201dmightbecompressedto\u201c0.[142857]*3.\u201d Compressionalgorithms\nworkbybuilding dictionaries ofsubsequences ofthetext,andthenreferring toentries inthe\ndictionary. Theexampleherehadonlyonedictionary entry, \u201c142857.\u201d\nIn effect, compression algorithms are creating a language model. The LZWalgorithm\ninparticulardirectlymodelsamaximum-entropyprobabilitydistribution. Todoclassification\nbycompression, wefirstlumptogetherallthespamtrainingmessagesandcompressthemas Section22.3. Information Retrieval 867\naunit. Wedothesamefortheham. Thenwhengiven anewmessage toclassify, weappend\nittothespammessagesandcompresstheresult. Wealsoappend ittothehamandcompress\nthat. Whichever class compresses better\u2014adds the fewernumber of additional bytes forthe\nnew message\u2014is the predicted class. The idea is that a spam message will tend to share\ndictionaryentrieswithotherspammessagesandthuswillcompressbetterwhenappendedto\nacollection thatalready containsthespamdictionary.\nExperimentswithcompression-based classificationonsomeofthestandardcorporafor\ntextclassification\u2014the 20-Newsgroups dataset,theReuters-10Corpora, theIndustry Sector\ncorpora\u2014indicatethatwhereasrunningoff-the-shelfcompressionalgorithmslikegzip,RAR,\nand LZW can be quite slow, their accuracy is comparable to traditional classification algo-\nrithms. This is interesting in its own right, and also serves to point out that there is promise\nforalgorithmsthatusecharacter n-gramsdirectlywithnopreprocessing ofthetextorfeature\nselection: theyseemtobecaptiring somerealpatterns.\n22.3 INFORMATION RETRIEVAL\nINFORMATION Information retrieval is the task of finding documents that are relevant to a user\u2019s need for\nRETRIEVAL\ninformation. The best-known examples of information retrieval systems are search engines\nontheWorldWideWeb. AWebusercantypeaquerysuchas[AIbook]2 intoasearchengine\nand see a list of relevant pages. In this section, we will see how such systems are built. An\ninformation retrieval(henceforth IR)systemcanbecharacterized by\nIR\n1. Acorpusofdocuments. Eachsystemmustdecidewhatitwantstotreatasadocument:\naparagraph, apage,oramultipage text.\n2. Queries posed in a query language. A query specifies what the user wants to know.\nQUERYLANGUAGE\nThe query language can be just a list of words, such as [AI book]; or it can specify\na phrase of words that must be adjacent, as in [\u201cAI book\u201d]; it can contain Boolean\noperatorsasin[AIANDbook];itcanincludenon-Booleanoperatorssuchas[AINEAR\nbook]or[AIbooksite:www.aaai.org].\n3. Aresultset. ThisisthesubsetofdocumentsthattheIRsystemjudgestoberelevantto\nRESULTSET\nthequery. Byrelevant, wemeanlikely tobeofusetotheperson whoposed thequery,\nRELEVANT\nfortheparticularinformation needexpressedinthequery.\n4. A presentation of the result set. This can be as simple as a ranked list of document\nPRESENTATION\ntitles or as complex as a rotating color map of the result set projected onto a three-\ndimensional space,rendered asatwo-dimensional display.\nBOOLEANKEYWORD The earliest IR systems worked on a Boolean keyword model. Each word in the document\nMODEL\ncollection istreated as aBoolean feature that is true ofadocument ifthe word occurs in the\ndocument and false if it does not. So the feature \u201cretrieval\u201d is true for the current chapter\nbut false for Chapter 15. The query language is the language of Boolean expressions over\n2 Wedenote asearch query as[query]. Squarebrackets areused ratherthan quotation markssothat wecan\ndistinguishthequery[\u201ctwowords\u201d]from[twowords]. 868 Chapter 22. NaturalLanguageProcessing\nfeatures. A document is relevant only if the expression evaluates to true. For example, the\nquery[information ANDretrieval]istrueforthecurrentchapterandfalseforChapter15.\nThis model has the advantage of being simple to explain and implement. However,\nit has some disadvantages. First, the degree of relevance of a document is a single bit, so\nthere is no guidance as to how to order the relevant documents for presentation. Second,\nBoolean expressions are unfamiliar to users who are not programmers or logicians. Users\nfind it unintuitive that when they want to know about farming in the states of Kansas and\nNebraska they need to issue the query [farming (Kansas OR Nebraska)]. Third, it can be\nhard to formulate anappropriate query, even foraskilled user. Suppose wetry[information\nAND retrieval AND models AND optimization] and get an empty result set. We could try\n[information ORretrieval ORmodels ORoptimization], butifthat returns too manyresults,\nitisdifficulttoknowwhattotrynext.\n22.3.1 IR scoring functions\nMostIRsystemshaveabandonedtheBooleanmodelandusemodelsbasedonthestatisticsof\nBM25SCORING wordcounts. Wedescribethe BM25scoringfunction,whichcomesfromtheOkapiproject\nFUNCTION\nof Stephen Robertson and Karen Sparck Jones at London\u2019s City College, and has been used\ninsearchengines suchastheopen-source Luceneproject.\nAscoringfunctiontakesadocumentandaqueryandreturnsanumericscore;themost\nrelevant documents have the highest scores. In the BM25 function, the score is a linear\nweighted combination ofscores foreach ofthe words that makeup thequery. Threefactors\naffect the weight of a query term: First, the frequency with which a query term appears in\na document (also known as TF for term frequency). For the query [farming in Kansas],\ndocuments that mention \u201cfarming\u201d frequently will have higher scores. Second, the inverse\ndocument frequency oftheterm, or IDF. Theword\u201cin\u201d appears inalmostevery document,\nsoithasahigh document frequency, andthus alowinverse document frequency, and thusit\nisnotasimportanttothequeryas\u201cfarming\u201dor\u201cKansas.\u201d Third,thelengthofthedocument.\nAmillion-worddocumentwillprobablymentionallthequery words,butmaynotactuallybe\naboutthequery. Ashortdocument thatmentionsallthewords isamuchbettercandidate.\nThe BM25 function takes all three of these into account. We assume we have created\nan index of the N documents in the corpus so that we can look up TF(q ,d ), the count of\ni j\nthe number of times word q appears in document d . We also assume a table of document\ni j\nfrequency counts, DF(q ), that gives the number of documents that contain the word q .\ni i\nThen,givenadocument d andaqueryconsisting ofthewords q ,wehave\nj 1:N\n(cid:12)N TF(q ,d )\u00b7(k+1)\nBM25(d ,q ) = IDF(q )\u00b7 i j ,\nj 1:N i TF(q ,d )+k\u00b7(1\u2212b+b\u00b7 |dj| )\ni=1 i j L\nwhere |d | is the length of document d in words, and L is the average document length\nj (cid:2) j\nin the corpus: L = |d |\/N. We have two parameters, k and b, that can be tuned by\ni i\ncross-validation; typical values are k = 2.0 and b = 0.75. IDF(q )is the inverse document\ni Section22.3. Information Retrieval 869\nfrequency ofword q ,givenby\ni\nN \u2212DF(q )+0.5\ni\nIDF(q )= log .\ni\nDF(q )+0.5\ni\nOf course, it would be impractical to apply the BM25 scoring function to every document\nin the corpus. Instead, systems create an index ahead of time that lists, for each vocabulary\nINDEX\nword,thedocumentsthatcontaintheword. Thisiscalledthehitlistfortheword. Thenwhen\nHITLIST\ngiven a query, we intersect the hit lists of the query words and only score the documents in\ntheintersection.\n22.3.2 IR system evaluation\nHowdoweknow whetheranIRsystem isperforming well? Weundertake anexperiment in\nwhichthesystemisgivenasetofqueriesandtheresultsetsarescoredwithrespecttohuman\nrelevance judgments. Traditionally, therehavebeentwomeasures usedinthescoring: recall\nand precision. Weexplain them withthehelp ofanexample. Imagine thatan IRsystem has\nreturned aresult setforasingle query, forwhich weknow which documents areandare not\nrelevant, outofacorpusof100documents. Thedocument countsineachcategory aregiven\ninthefollowingtable:\nInresultset Notinresultset\nRelevant 30 20\nNotrelevant 10 40\nPrecision measures the proportion of documents in the result set that are actually relevant.\nPRECISION\nInourexample,theprecision is 30\/(30+10)=.75. Thefalsepositive rateis1\u2212.75=.25.\nRecall measures the proportion of all the relevant documents in the collection that are in\nRECALL\nthe result set. In our example, recall is 30\/(30 +20)=.60. The false negative rate is 1\u2212\n.60=.40. Inaverylargedocumentcollection,suchastheWorldWideWeb,recallisdifficult\nto compute, because there is no easy way to examine every page on the Web for relevance.\nAllwecandoiseitherestimaterecallbysamplingorignorerecallcompletelyandjustjudge\nprecision. In the case of a Web search engine, there may be thousands of documents in the\nresult set, so it makes more sense to measure precision for several different sizes, such as\n\u201cP@10\u201d (precision in the top 10 results) or\u201cP@50,\u201d rather than to estimate precision in the\nentireresultset.\nIt is possible to trade off precision against recall by varying the size of the result set\nreturned. Intheextreme, asystem thatreturns everydocument inthedocument collection is\nguaranteed arecall of 100%, but willhave low precision. Alternately, asystem could return\na single document and have low recall, but a decent chance at 100% precision. A summary\nofbothmeasuresistheF score,asinglenumberthatistheharmonicmeanofprecision and\n1\nrecall, 2PR\/(P +R).\n22.3.3 IR refinements\nThere are many possible refinements to the system described here, and indeed Web search\nenginesarecontinually updatingtheiralgorithmsastheydiscovernewapproachesandasthe\nWebgrowsandchanges. 870 Chapter 22. NaturalLanguageProcessing\nOnecommonrefinementisabettermodeloftheeffectofdocumentlengthonrelevance.\nSinghal et al. (1996) observed that simple document length normalization schemes tend to\nfavor short documents too much and long documents not enough. They propose a pivoted\ndocument length normalization scheme; the idea is that the pivot is the document length at\nwhich the old-style normalization is correct; documents shorter than that get a boost and\nlongeronesgetapenalty.\nThe BM25 scoring function uses a word model that treats all words as completely in-\ndependent, but we know that some words are correlated: \u201ccouch\u201d is closely related to both\n\u201ccouches\u201d and\u201csofa.\u201d ManyIRsystemsattempttoaccountforthesecorrelations.\nForexample, ifthequeryis[couch], itwouldbeashametoexcludefromtheresultset\nthose documents that mention \u201cCOUCH\u201d or \u201ccouches\u201d but not \u201ccouch.\u201d Most IR systems\ndo case folding of \u201cCOUCH\u201d to \u201ccouch,\u201d and some use a stemming algorithm to reduce\nCASEFOLDING\n\u201ccouches\u201d to the stem form \u201ccouch,\u201d both in the query and the documents. This typically\nSTEMMING\nyields a small increase in recall (on the order of 2% for English). However, it can harm\nprecision. For example, stemming \u201cstocking\u201d to \u201cstock\u201d will tend to decrease precision for\nqueries about eitherfootcoverings orfinancial instruments, although itcould improverecall\nfor queries about warehousing. Stemming algorithms based on rules (e.g., remove \u201c-ing\u201d)\ncannot avoid this problem, but algorithms based on dictionaries (don\u2019t remove \u201c-ing\u201d if the\nword is already listed in the dictionary) can. While stemming has a small effect in English,\nit is more important in other languages. In German, for example, it is not uncommon to\nsee words like \u201cLebensversicherungsgesellschaftsangestellter\u201d (life insurance company em-\nployee). Languages suchasFinnish, Turkish, Inuit, andYupikhaverecursive morphological\nrulesthatinprinciple generate wordsofunbounded length.\nThenextstepistorecognizesynonyms,suchas\u201csofa\u201dfor\u201ccouch.\u201d Aswithstemming,\nSYNONYM\nthis has the potential for small gains in recall, but can hurt precision. A user who gives the\nquery [Tim Couch] wants to see results about the football player, not sofas. The problem is\nthat\u201clanguagesabhorabsolutesynonymsjustasnatureabhorsavacuum\u201d(Cruse,1986). That\nis, anytime there aretwowords thatmean thesamething, speakers ofthelanguage conspire\nto evolve the meanings to remove the confusion. Related words that are not synonyms also\nplay an important role in ranking\u2014terms like \u201cleather\u201d, \u201cwooden,\u201d or \u201cmodern\u201d can serve\nto confirm that the document really is about \u201ccouch.\u201d Synonyms and related words can be\nfound in dictionaries or by looking for correlations in documents or in queries\u2014if we find\nthat many users who ask the query [new sofa] follow it up with the query [new couch], we\ncaninthefuturealter[newsofa]tobe[newsofaORnewcouch].\nAs a final refinement, IR can be improved by considering metadata\u2014data outside of\nMETADATA\nthetext ofthe document. Examples include human-supplied keywords andpublication data.\nOntheWeb,hypertext linksbetweendocuments areacrucialsourceofinformation.\nLINKS\n22.3.4 The PageRankalgorithm\nPageRank3 wasoneofthetwooriginal ideas thatsetGoogle\u2019s search apart from otherWeb\nPAGERANK\nsearch engines whenitwasintroduced in1997. (Theotherinnovation wastheuseofanchor\n3 ThenamestandsbothforWebpagesandforcoinventorLarryPage(BrinandPage,1998). Section22.3. Information Retrieval 871\nfunctionHITS(query)returnspages withhubandauthoritynumbers\npages\u2190EXPAND-PAGES(RELEVANT-PAGES(query))\nforeachp inpages do\np.AUTHORITY\u21901\np.HUB\u21901\nrepeatuntilconvergencedo\nforeachp inpages d(cid:2)o\np.AUTHOR(cid:2)ITY\u2190\ni\nINLINKi(p).HUB\np.HUB\u2190\ni\nOUTLINKi(p).AUTHORITY\nNORMALIZE(pages)\nreturnpages\nFigure 22.1 The HITS algorithm for computing hubs and authorities with respect to a\nquery. RELEVANT-PAGESfetchesthepagesthatmatchthequery,andEXPAND-PAGESadds\nineverypagethatlinkstoorislinkedfromoneoftherelevantpages. NORMALIZEdivides\neach page\u2019s score by the sum of the squares of all pages\u2019 scores (separately for both the\nauthorityandhubsscores).\ntext\u2014theunderlinedtextinahyperlink\u2014toindexapage,eventhoughtheanchortextwason\nadifferent pagethantheonebeingindexed.) PageRankwasinvented tosolvetheproblemof\nthetyrannyofTF scores: ifthequeryis[IBM],howdowemakesurethatIBM\u2019shomepage,\nibm.com,isthefirstresult,evenifanotherpagementionstheterm\u201cIBM\u201dmorefrequently?\nTheideaisthatibm.comhasmanyin-links(linkstothepage),soitshouldberankedhigher:\neach in-link is a vote for the quality of the linked-to page. But if we only counted in-links,\nthenitwouldbepossible foraWebspammertocreateanetwork ofpages andhavethemall\npoint to a page of his choosing, increasing the score of that page. Therefore, the PageRank\nalgorithm is designed to weight links from high-quality sites more heavily. What is a high-\nquality site? Onethatislinked tobyotherhigh-quality sites. Thedefinition isrecursive, but\nwewillseethattherecursion bottomsoutproperly. ThePageRankforapagepisdefinedas:\n(cid:12)\n1\u2212d PR(in )\ni\nPR(p) = +d ,\nN C(in )\ni\ni\nwhere PR(p) is the PageRank of page p, N is the total number of pages in the corpus, in\ni\nare the pages that link in to p, and C(in ) is the count of the total number of out-links on\ni\npage in . The constant d is a damping factor. It can be understood through the random\ni\nRANDOMSURFER surfer model: imagine a Web surfer who starts at some random page and begins exploring.\nMODEL\nWith probability d (we\u2019ll assume d=0.85) the surfer clicks on one of the links on the page\n(choosing uniformly among them), and with probability 1\u2212d she gets bored with the page\nand restarts on a random page anywhere on the Web. The PageRank of page p is then the\nprobability that the random surfer will be at page p at any point in time. PageRank can be\ncomputed by an iterative procedure: start with all pages having PR(p)=1, and iterate the\nalgorithm, updating ranksuntiltheyconverge. 872 Chapter 22. NaturalLanguageProcessing\n22.3.5 The HITSalgorithm\nThe Hyperlink-Induced Topic Search algorithm, also known as \u201cHubs and Authorities\u201d or\nHITS, is another influential link-analysis algorithm (see Figure 22.1). HITS differs from\nPageRankinseveralways. First,itisaquery-dependent measure: itratespages withrespect\nto a query. That means that it must be computed anew for each query\u2014a computational\nburden that most search engines have elected not totake on. Given aquery, HITS firstfinds\na set of pages that are relevant to the query. It does that by intersecting hit lists of query\nwords, andthen adding pages inthelink neighborhood ofthese pages\u2014pages thatlink toor\narelinkedfromoneofthepagesintheoriginalrelevantset.\nEach page in this set is considered an authority on the query to the degree that other\nAUTHORITY\npages in the relevant set point to it. A page is considered a hub to the degree that it points\nHUB\nto other authoritative pages in the relevant set. Just as with PageRank, we don\u2019t want to\nmerely count the number of links; we want to give more value to the high-quality hubs and\nauthorities. Thus, as with PageRank, weiterate a process that updates the authority score of\na page to be the sum of the hub scores of the pages that point to it, and the hub score to be\nthesum oftheauthority scores ofthe pages itpoints to. Ifwe then normalize thescores and\nrepeat k times,theprocesswillconverge.\nBoth PageRank and HITS played important roles in developing our understanding of\nWebinformationretrieval. Thesealgorithmsandtheirextensionsareusedinrankingbillions\nofqueries dailyassearch engines steadily develop betterwaysofextracting yetfinersignals\nofsearchrelevance.\n22.3.6 Questionanswering\nInformation retrieval is the task of finding documents that are relevant to a query, where the\nQUESTION querymaybeaquestion, orjustatopicareaorconcept. Questionansweringisasomewhat\nANSWERING\ndifferent task, in which the query really is a question, and the answer is not a ranked list\nof documents but rather a short response\u2014a sentence, or even just a phrase. There have\nbeen question-answering NLP (natural language processing) systems since the 1960s, but\nonlysince2001havesuchsystemsusedWebinformation retrievaltoradically increase their\nbreadthofcoverage.\nThe ASKMSR system (Bankoetal., 2002) isatypical Web-based question-answering\nsystem. It is based on the intuition that most questions will be answered many times on the\nWeb, so question answering should be thought of as a problem in precision, not recall. We\ndon\u2019t have to deal with all the different ways that an answer might be phrased\u2014we only\nhave to find one of them. For example, consider the query [Who killed Abraham Lincoln?]\nSuppose a system had to answer that question with access only to a single encyclopedia,\nwhoseentryonLincolnsaid\nJohn Wilkes Booth altered history with a bullet. He will foreverbe known as the man\nwhoendedAbrahamLincoln\u2019slife.\nTousethispassagetoanswerthequestion, thesystem wouldhavetoknowthatendingalife\ncanbeakilling,that\u201cHe\u201dreferstoBooth,andseveralother linguistic andsemanticfacts. Section22.4. Information Extraction 873\nASKMSRdoesnotattemptthiskindofsophistication\u2014itknowsnothingaboutpronoun\nreference,oraboutkilling,oranyotherverb. Itdoesknow15differentkindsofquestions,and\nhowthey canberewritten asqueries toasearch engine. Itknowsthat[Who killed Abraham\nLincoln] canberewritten asthequery[*killed Abraham Lincoln]andas[Abraham Lincoln\nwaskilled by*]. Itissues theserewritten queries andexamines theresults thatcomeback\u2014\nnot the full Web pages, just the short summaries of text that appear near the query terms.\nTheresultsarebrokeninto1-,2-,and3-gramsandtalliedforfrequencyintheresultsetsand\nfor weight: an n-gram that came back from a very specific query rewrite (such as the exact\nphrase match query [\u201cAbraham Lincoln was killed by *\u201d]) would get more weight than one\nfrom a general query rewrite, such as [Abraham OR Lincoln OR killed]. We would expect\nthat\u201cJohnWilkesBooth\u201dwouldbeamongthehighlyrankedn-gramsretrieved,butsowould\n\u201cAbrahamLincoln\u201dand\u201ctheassassination of\u201dand\u201cFord\u2019sTheatre.\u201d\nOnce the n-grams are scored, they are filtered by expected type. If the original query\nstartswith\u201cwho,\u201dthenwefilteronnamesofpeople;for\u201chowmany\u201dwefilteronnumbers,for\n\u201cwhen,\u201donadateortime. Thereisalsoafilterthatsaystheanswershouldnotbepartofthe\nquestion; together these should allow us to return \u201cJohn Wilkes Booth\u201d (and not \u201cAbraham\nLincoln\u201d)asthehighest-scoring response.\nIn some cases the answer will be longer than three words; since the components re-\nsponses only go up to 3-grams, a longer response would have to be pieced together from\nshorter pieces. For example, in a system that used only bigrams, the answer \u201cJohn Wilkes\nBooth\u201dcouldbepiecedtogetherfromhigh-scoringpieces\u201cJohnWilkes\u201dand\u201cWilkesBooth.\u201d\nAt the Text Retrieval Evaluation Conference (TREC), ASKMSR was rated as one of\nthe top systems, beating out competitors with the ability to do far more complex language\nunderstanding. ASKMSR relies upon the breadth of the content on the Web rather than on\nits own depth of understanding. It won\u2019t be able to handle complex inference patterns like\nassociating \u201cwhokilled\u201d with\u201cended thelifeof.\u201d ButitknowsthattheWebissovastthatit\ncanaffordtoignorepassages likethatandwaitforasimplepassage itcanhandle.\n22.4 INFORMATION EXTRACTION\nINFORMATION Informationextractionistheprocessofacquiringknowledgebyskimmingatextandlook-\nEXTRACTION\ning for occurrences of a particular class of object and for relationships among objects. A\ntypical task is to extract instances of addresses from Web pages, with database fields for\nstreet, city, state, and zip code; or instances of storms from weather reports, with fields for\ntemperature, wind speed, and precipitation. In a limited domain, this can be done with high\naccuracy. Asthedomaingetsmoregeneral, morecomplexlinguistic modelsandmorecom-\nplex learning techniques are necessary. We will see in Chapter 23 how to define complex\nlanguage models of the phrase structure (noun phrases and verb phrases) of English. But so\nfar there are no complete models of this kind, so for the limited needs of information ex-\ntraction, we define limited models that approximate the full English model, and concentrate\non just the parts that are needed for the task at hand. The models we describe in this sec- 874 Chapter 22. NaturalLanguageProcessing\ntionareapproximations inthesamewaythatthesimple1-CNFlogical modelinFigure7.21\n(page271)isanapproximations ofthefull,wiggly,logical model.\nIn this section we describe six different approaches to information extraction, in order\nofincreasing complexity onseveraldimensions: deterministic tostochastic, domain-specific\ntogeneral, hand-crafted tolearned, andsmall-scale tolarge-scale.\n22.4.1 Finite-stateautomata forinformationextraction\nATTRIBUTE-BASED Thesimplesttypeofinformation extraction system isan attribute-based extraction system\nEXTRACTION\nthatassumesthattheentiretextreferstoasingleobjectandthetaskistoextractattributes of\nthat object. For example, we mentioned in Section 12.7 the problem of extracting from the\ntext \u201cIBM ThinkBook 970. Our price: $399.00\u201d the set of attributes {Manufacturer=IBM,\nModel=ThinkBook970, Price=$399.00}. We can address this problem by defining a tem-\nplate (also known as a pattern) for each attribute we would like to extract. The template is\nTEMPLATE\nREGULAR definedbyafinitestateautomaton,thesimplestexampleofwhichistheregularexpression,\nEXPRESSION\nor regex. Regular expressions are used in Unix commands such as grep, in programming\nlanguages such as Perl, and in word processors such as Microsoft Word. The details vary\nslightly from one tool to another and so are best learned from the appropriate manual, but\nhereweshowhowtobuilduparegularexpression templatefor pricesindollars:\n[0-9] matchesanydigitfrom0to9\n[0-9]+ matchesoneormoredigits\n[.][0-9][0-9] matchesaperiodfollowedbytwodigits\n([.][0-9][0-9])? matchesaperiodfollowedbytwodigits,ornothing\n[$][0-9]+([.][0-9][0-9])? matches$249.99or$1.23or$1000000 or...\nTemplatesareoftendefinedwiththreeparts: aprefixregex,atargetregex,andapostfixregex.\nForprices,thetargetregexisasabove,theprefixwouldlook forstringssuchas\u201cprice:\u201d and\nthe postfix could be empty. The idea is that some clues about an attribute come from the\nattribute valueitselfandsomecomefromthesurrounding text.\nIf a regular expression for an attribute matches the text exactly once, then we can pull\nouttheportion ofthetextthatisthevalueoftheattribute. Ifthereisnomatch,allwecando\nisgiveadefaultvalueorleavetheattributemissing;butifthereareseveralmatches,weneed\naprocesstochooseamongthem. Onestrategyistohaveseveraltemplatesforeachattribute,\nordered by priority. So, for example, the top-priority template for price might look for the\nprefix\u201courprice:\u201d;ifthatisnotfound,welookfortheprefix \u201cprice:\u201d andifthatisnotfound,\nthe empty prefix. Another strategy is to take all the matches and find some way to choose\namong them. Forexample, we could take the lowest price that is within 50% of the highest\nprice. Thatwillselect$78.00asthetargetfromthetext\u201cListprice$99.00, specialsaleprice\n$78.00,shipping $3.00.\u201d\nRELATIONAL Onestepupfromattribute-based extractionsystemsare relationalextractionsystems,\nEXTRACTION\nwhich deal with multiple objects and the relations among them. Thus, when these systems\nseethetext\u201c$249.99,\u201dtheyneedtodeterminenotjustthatitisaprice,butalsowhichobject\nhasthatprice. Atypicalrelational-based extractionsystemisFASTUS,whichhandlesnews\nstoriesaboutcorporate mergersandacquisitions. Itcanreadthestory Section22.4. Information Extraction 875\nBridgestone Sports Co. said Friday it has set up a joint venture in Taiwan with a local\nconcernandaJapanesetradinghousetoproducegolfclubstobeshippedtoJapan.\nandextracttherelations:\ne\u2208JointVentures \u2227Product(e,\u201cgolf clubs\u201d)\u2227Date(e,\u201cFriday\u201d)\n\u2227Member(e,\u201cBridgestone SportsCo\u201d)\u2227Member(e,\u201ca local concern\u201d)\n\u2227Member(e,\u201ca Japanesetrading house\u201d).\nCASCADED\nA relational extraction system can be built asa series of cascaded finite-state transducers.\nFINITE-STATE\nTRANSDUCERS\nThatis,thesystem consists ofaseries ofsmall,efficientfinite-state automata(FSAs),where\neach automaton receives text as input, transduces the text into adifferent format, and passes\nitalongtothenextautomaton. FASTUS consistsoffivestages:\n1. Tokenization\n2. Complex-wordhandling\n3. Basic-group handling\n4. Complex-phrase handling\n5. Structuremerging\nFASTUS\u2019s first stage is tokenization, which segments the stream of characters into tokens\n(words, numbers, and punctuation). ForEnglish, tokenization can be fairly simple; just sep-\naratingcharacters atwhitespaceorpunctuation doesafairlygoodjob. Sometokenizers also\ndealwithmarkuplanguages suchasHTML,SGML,andXML.\nThesecond stage handles complexwords, including collocations suchas\u201csetup\u201d and\n\u201cjoint venture,\u201d as well as proper names such as \u201cBridgestone Sports Co.\u201d These are rec-\nognized by a combination of lexical entries and finite-state grammar rules. For example, a\ncompanynamemightberecognized bytherule\nCapitalizedWord+(\u201cCompany\u201d|\u201cCo\u201d|\u201cInc\u201d|\u201cLtd\u201d)\nThe third stage handles basic groups, meaning noun groups and verb groups. The idea is\nto chunk these into units that will be managed by the later stages. We will see how to write\na complex description of noun and verb phrases in Chapter 23, but here we have simple\nrules that only approximate the complexity of English, but have the advantage of being rep-\nresentable by finite state automata. The example sentence would emerge from this stage as\nthefollowingsequence oftaggedgroups:\n1 NG: Bridgestone Sports Co. 10 NG: a local concern\n2 VG: said 11 CJ: and\n3 NG: Friday 12 NG: a Japanese trading house\n4 NG: it 13 VG: to produce\n5 VG: had set up 14 NG: golf clubs\n6 NG: a joint venture 15 VG: to be shipped\n7 PR: in 16 PR: to\n8 NG: Taiwan 17 NG: Japan\n9 PR: with\nHereNGmeansnoungroup,VGisverbgroup, PRispreposition, andCJisconjunction. 876 Chapter 22. NaturalLanguageProcessing\nThe fourth stage combines the basic groups into complex phrases. Again, the aim\nis to have rules that are finite-state and thus can be processed quickly, and that result in\nunambiguous (or nearly unambiguous) output phrases. One type of combination rule deals\nwithdomain-specific events. Forexample,therule\nCompany+SetUpJointVenture(\u201cwith\u201dCompany+)?\ncaptures one way to describe the formation of a joint venture. This stage is the first one in\nthecascadewheretheoutputisplacedintoadatabasetemplateaswellasbeingplacedinthe\noutput stream. The final stage merges structures that were built up in the previous step. If\nthenextsentence says\u201cThejointventurewillstartproduction inJanuary,\u201dthenthisstepwill\nnotice that there are two references to a joint venture, and that they should be merged into\none. Thisisaninstance oftheidentityuncertaintyproblemdiscussed inSection14.6.3.\nIngeneral,finite-statetemplate-basedinformationextractionworkswellforarestricted\ndomaininwhichitispossible topredetermine whatsubjects willbediscussed, andhowthey\nwill be mentioned. The cascaded transducer model helps modularize the necessary knowl-\nedge, easing construction of the system. These systems work especially well when they are\nreverse-engineering textthathasbeengenerated byaprogram. Forexample, ashopping site\nontheWebisgenerated byaprogram thattakesdatabase entriesandformatsthemintoWeb\npages; a template-based extractor then recovers the original database. Finite-state informa-\ntionextraction islesssuccessful atrecovering information inhighly variable format, suchas\ntextwrittenbyhumansonavarietyofsubjects.\n22.4.2 Probabilisticmodels forinformationextraction\nWheninformationextractionmustbeattemptedfromnoisyorvariedinput,simplefinite-state\napproaches fare poorly. It is too hard to get all the rules and their priorities right; it is better\ntouseaprobabilistic modelratherthanarule-based model. Thesimplestprobabilistic model\nforsequences withhidden stateisthehiddenMarkovmodel,orHMM.\nRecall from Section 15.3 that an HMM models a progression through a sequence of\nhidden states, x , with an observation e at each step. To apply HMMs to information ex-\nt t\ntraction, we can either build one big HMM for all the attributes or build a separate HMM\nfor each attribute. We\u2019ll do the second. The observations are the words of the text, and the\nhiddenstatesarewhetherweareinthetarget, prefix,orpostfixpartoftheattribute template,\norin the background (not part of a template). Forexample, here is a brief text and the most\nprobable (Viterbi)pathforthattextfortwoHMMs,onetrainedtorecognize thespeakerina\ntalkannouncement, andonetrainedtorecognizedates. The\u201c-\u201dindicatesabackground state:\nText: There will be a seminar by Dr. Andrew McCallum on Friday\nSpeaker: - - - - PRE PRE TARGET TARGET TARGET POST -\nDate: - - - - - - - - - PRE TARGET\nHMMshavetwobigadvantagesoverFSAsforextraction. First,HMMsareprobabilistic,and\nthus tolerant to noise. In a regular expression, if a single expected character is missing, the\nregexfailstomatch;withHMMsthereisgracefuldegradationwithmissingcharacters\/words,\nandwegetaprobabilityindicatingthedegreeofmatch,notjustaBooleanmatch\/fail. Second, Section22.4. Information Extraction 877\ndr\nwho : professor\nspeaker with 0.99 robert\nspeak 1.0 ; michael\n5409 about mr\nappointment how\nwill\n0.99\n(\n0.76 received\nhas\nw 0.56 is\ncavalier\nstevens\nseminar that christel\nreminder 1.0 by l\ntheater speakers 0.24\nartist \/\nadditionally here\n0.44\nPrefix Target Postfix\nFigure 22.2 Hidden Markov model for the speaker of a talk announcement. The two\nsquare states are the target (note the second target state has a self-loop, so the target can\nmatch a string of any length), the fourcircles to the left are the prefix, and the one on the\nrightisthepostfix. Foreachstate,onlyafewofthehigh-probabilitywordsareshown.From\nFreitagandMcCallum(2000).\nHMMscan be trained from data; they don\u2019t require laborious engineering of templates, and\nthustheycanmoreeasilybekeptuptodateastextchangesovertime.\nNotethatwehaveassumed acertain levelofstructure inourHMMtemplates: theyall\nconsist of one or more target states, and any prefix states must precede the targets, postfix\nstates most follow the targets, and other states must be background. This structure makes\nit easier to learn HMMs from examples. With a partially specified structure, the forward\u2013\nbackward algorithm can be used to learn both the transition probabilities P(X t|X t\u22121) be-\ntween states and the observation model, P(E |X ), which says how likely each word is in\nt t\neach state. For example, the word \u201cFriday\u201d would have high probability in one or more of\nthetargetstatesofthedateHMM,andlowerprobability elsewhere.\nWithsufficienttrainingdata,theHMMautomaticallylearnsastructureofdatesthatwe\nfindintuitive: thedateHMMmighthaveonetargetstateinwhichthehigh-probability words\nare \u201cMonday,\u201d \u201cTuesday,\u201d etc., and which has a high-probability transition to a target state\nwith words \u201cJan\u201d, \u201cJanuary,\u201d \u201cFeb,\u201d etc. Figure 22.2 shows the HMM for the speaker of a\ntalk announcement, as learned from data. The prefix covers expressions such as \u201cSpeaker:\u201d\nand \u201cseminar by,\u201d and the target has one state that covers titles and first names and another\nstatethatcoversinitialsandlastnames.\nOnce the HMMs have been learned, we can apply them to a text, using the Viterbi\nalgorithm to find the most likely path through the HMM states. One approach is to apply\neach attribute HMM separately; in this case you would expect most of the HMMs to spend\nmost of their time in background states. This is appropriate when the extraction is sparse\u2014\nwhenthenumberofextracted wordsissmallcomparedtothelengthofthetext. 878 Chapter 22. NaturalLanguageProcessing\nTheotherapproachistocombinealltheindividualattributesintoonebigHMM,which\nwouldthenfindapaththatwandersthrough different target attributes, firstfindingaspeaker\ntarget, then a date target, etc. Separate HMMs are better when we expect just one of each\nattribute in a text and one big HMM is better when the texts are more free-form and dense\nwith attributes. With either approach, in the end we have a collection of target attribute\nobservations, and have to decide what to do with them. If every expected attribute has one\ntarget filler then the decision is easy: we have an instance of the desired relation. If there\naremultiple fillers,weneedtodecide whichtochoose, aswediscussed withtemplate-based\nsystems. HMMs have the advantage of supplying probability numbers that can help make\nthechoice. Ifsometargets aremissing,weneedtodecideifthisisaninstance ofthedesired\nrelationatall,orifthetargetsfoundarefalsepositives. Amachinelearningalgorithmcanbe\ntrainedtomakethischoice.\n22.4.3 Conditionalrandom fields forinformationextraction\nOne issue with HMMs for the information extraction task is that they model a lot of prob-\nabilities that we don\u2019t really need. An HMM is a generative model; it models the full joint\nprobability ofobservations andhiddenstates,andthuscanbeusedtogeneratesamples. That\nis, we can use the HMM model not only to parse a text and recover the speaker and date,\nbutalsotogenerate arandom instance ofatextcontaining aspeakerandadate. Sincewe\u2019re\nnot interested in that task, it is natural to ask whether we might be better off with a model\nthat doesn\u2019t bother modeling that possibility. All we need in order to understand a text is a\ndiscriminative model, one that models the conditional probability of the hidden attributes\ngiven the observations (the text). Given a text e , the conditional model finds the hidden\n1:N\nstatesequence X thatmaximizesP(X |e ).\n1:N 1:N 1:N\nModeling this directly gives us some freedom. We don\u2019t need the independence as-\nsumptions of the Markov model\u2014we can have an x that is dependent on x . A framework\nt 1\nCONDITIONAL forthistype of modelis the conditional random field,orCRF,which models aconditional\nRANDOMFIELD\nprobability distribution of a set of target variables given a set of observed variables. Like\nBayesiannetworks,CRFscanrepresentmanydifferentstructuresofdependenciesamongthe\nLINEAR-CHAIN\nvariables. One common structure is the linear-chain conditional random field for repre-\nCONDITIONAL\nRANDOMFIELD\nsentingMarkovdependencies amongvariables inatemporalsequence. Thus,HMMsarethe\ntemporal version of naive Bayes models, and linear-chain CRFs are the temporal version of\nlogistic regression, where thepredicted target isan entire state sequence ratherthan asingle\nbinaryvariable.\nLete be the observations (e.g., words in a document), and x be the sequence of\n1:N 1:N\nhidden states (e.g., the prefix, target, and postfix states). A linear-chain conditional random\nfielddefinesaconditional probability distribution:\nP\nP(x |e ) = \u03b1e[ N i=1F(x i\u22121,x i,e,i)] ,\n1:N 1:N\nwhere\u03b1isanormalizationfactor(tomakesuretheprobabilitiessumto1),andF isafeature\nfunction definedastheweightedsumofacollection ofk component featurefunctions:\n(cid:12)\nF(x i\u22121,x i,e,i) = \u03bb kf k(x i\u22121,x i,e,i).\nk Section22.4. Information Extraction 879\nThe \u03bb parameter values are learned with aMAP (maximum a posteriori) estimation proce-\nk\ndurethatmaximizestheconditional likelihood ofthetraining data. Thefeaturefunctions are\nthekeycomponentsofaCRF.Thefunctionf\nk\nhasaccesstoapairofadjacentstates,x i\u22121and\nx ,butalsotheentireobservation(word)sequence e,andthecurrentpositioninthetemporal\ni\nsequence, i. This gives us a lot of flexibility in defining features. We can define a simple\nfeature function, forexample one that produces avalue of1ifthe current wordis ANDREW\nandthecurrentstateis SPE(cid:24)AKER:\nf 1(x i\u22121,x i,e,i) = 01 oif thx ei r= wisS ePEAKER ande i = ANDREW\nHowarefeaturesliketheseused? Itdependsontheircorresponding weights. If\u03bb > 0,then\n1\nwhenever f is true, it increases the probability of the hidden state sequence x . This is\n1 1:N\nanother way of saying \u201cthe CRFmodel should prefer the target state SPEAKER for the word\nANDREW.\u201d If on the other hand \u03bb\n1\n< 0, the CRF model will try to avoid this association,\nandif\u03bb = 0,thisfeatureisignored. Parametervaluescanbesetmanuallyorcanbelearned\n1\nfromdata. Nowconsiderasecondfeaturefunction:\n(cid:24)\nf 2(x i\u22121,x i,e,i) = 01 oif thx ei r= wisS ePEAKER ande i+1 = SAID\nThis feature is true if the current state is SPEAKER and the next word is \u201csaid.\u201d One would\ntherefore expect apositive \u03bb valuetogowiththefeature. Moreinterestingly, notethatboth\n2\nf and f can hold at the same time for a sentence like \u201cAndrew said ....\u201d In this case, the\n1 2\ntwo features overlap each other and both boost the belief in x 1= SPEAKER. Because of the\nindependence assumption, HMMscannot useoverlapping features; CRFscan. Furthermore,\na feature in a CRFcan use any part of the sequence e . Features can also be defined over\n1:N\ntransitionsbetweenstates. Thefeatureswedefinedherewerebinary,butingeneral,afeature\nfunctioncanbeanyreal-valuedfunction. Fordomainswherewehavesomeknowledgeabout\nthe types of features we would like to include, the CRF formalism gives us a great deal of\nflexibility in defining them. This flexibility can lead to accuracies that are higher than with\nlessflexiblemodelssuchasHMMs.\n22.4.4 Ontologyextractionfrom largecorpora\nSo far we have thought of information extraction as finding a specific set of relations (e.g.,\nspeaker, time, location) in a specific text (e.g., a talk announcement). A different applica-\ntion of extraction technology is building a large knowledge base or ontology of facts from\na corpus. This is different in three ways: First it is open-ended\u2014we want to acquire facts\nabout all types of domains, not just one specific domain. Second, with a large corpus, this\ntaskisdominatedbyprecision, notrecall\u2014justaswithquestionansweringontheWeb(Sec-\ntion 22.3.6). Third, the results can be statistical aggregates gathered from multiple sources,\nratherthanbeingextractedfromonespecifictext.\nFor example, Hearst (1992) looked at the problem of learning an ontology of concept\ncategories and subcategories from a large corpus. (In 1992, a large corpus was a 1000-page\nencyclopedia; todayitwouldbea100-million-page Webcorpus.) Theworkconcentrated on\ntemplates that are very general (not tied to a specific domain) and have high precision (are 880 Chapter 22. NaturalLanguageProcessing\nalmostalwayscorrectwhentheymatch)butlowrecall(donot alwaysmatch). Hereisoneof\nthemostproductive templates:\nNP suchasNP (,NP)*(,)? ((and|or)NP)? .\nHere the bold words and commas must appear literally in the text, but the parentheses are\nfor grouping, the asterisk means repetition of zero or more, and the question mark means\noptional. NP is avariable standing fora noun phrase; Chapter 23 describes how to identify\nnounphrases;fornowjustassumethatweknowsomewordsarenounsandotherwords(such\nas verbs) that we can reliably assume are not part of a simple noun phrase. This template\nmatches the texts \u201cdiseases such asrabies affect your dog\u201d and \u201csupports network protocols\nsuch as DNS,\u201d concluding that rabies is a disease and DNS is a network protocol. Similar\ntemplatescanbeconstructed withthekeywords\u201cincluding,\u201d\u201cespecially,\u201dand\u201corother.\u201d Of\ncourse these templates will fail to match many relevant passages, like \u201cRabies is a disease.\u201d\nThatisintentional. The\u201cNP isaNP\u201dtemplatedoesindeedsometimesdenoteasubcategory\nrelation, but it often means something else, as in \u201cThere is a God\u201d or \u201cShe is a little tired.\u201d\nWithalargecorpuswecanaffordtobepicky;touseonlythehigh-precision templates. We\u2019ll\nmiss many statements of a subcategory relationship, but most likely we\u2019ll find a paraphrase\nofthestatementsomewhereelseinthecorpusinaformwecanuse.\n22.4.5 Automated templateconstruction\nThesubcategory relationissofundamentalthatisworthwhiletohandcraftafewtemplatesto\nhelpidentify instances ofitoccurring innaturallanguage text. Butwhataboutthethousands\nof other relations in the world? There aren\u2019t enough AI grad students in the world to create\nanddebug templates forallofthem. Fortunately, itispossible tolearntemplates fromafew\nexamples,thenusethetemplatestolearnmoreexamples,fromwhichmoretemplatescanbe\nlearned,andsoon. Inoneofthefirstexperimentsofthiskind,Brin(1999)startedwithadata\nsetofjustfiveexamples:\n(\u201cIsaacAsimov\u201d,\u201cTheRobotsofDawn\u201d)\n(\u201cDavidBrin\u201d,\u201cStartideRising\u201d)\n(\u201cJamesGleick\u201d,\u201cChaos\u2014MakingaNewScience\u201d)\n(\u201cCharlesDickens\u201d,\u201cGreatExpectations\u201d)\n(\u201cWilliamShakespeare\u201d,\u201cTheComedyofErrors\u201d)\nClearlytheseareexamplesoftheauthor\u2013title relation,butthelearningsystemhadnoknowl-\nedge of authors or titles. The words in these examples were used in a search over a Web\ncorpus, resulting in199matches. Eachmatchisdefinedasatupleofsevenstrings,\n(Author, Title,Order,Prefix,Middle,Postfix,URL),\nwhere Order is true if the author came first and false if the title came first, Middle is the\ncharacters between theauthorandtitle, Prefixisthe 10characters before the match, Suffix is\nthe10characters afterthematch,and URListheWebaddresswherethematchwasmade.\nGiven a set of matches, a simple template-generation scheme can find templates to\nexplainthematches. Thelanguage oftemplateswasdesigned tohaveaclosemappingtothe\nmatches themselves, tobeamenable toautomated learning, andtoemphasize high precision Section22.4. Information Extraction 881\n(possibly at the risk of lower recall). Each template has the same seven components as a\nmatch. The Author and Title are regexes consisting of any characters (but beginning and\nending in letters) and constrained to have a length from half the minimum length of the\nexamples to twice the maximum length. The prefix, middle, and postfix are restricted to\nliteral strings, not regexes. The middle is the easiest to learn: each distinct middle string in\nthe set of matches is a distinct candidate template. For each such candidate, the template\u2019s\nPrefixisthendefinedasthelongestcommonsuffixofalltheprefixesinthematches,andthe\nPostfixisdefinedasthelongestcommonprefixofallthepostfixesinthematches. Ifeitherof\nthese is of length zero, then the template is rejected. The URL of the template is defined as\nthelongestprefixoftheURLsinthematches.\nIn the experiment run by Brin, the first 199 matches generated three templates. The\nmostproductivetemplatewas\n<LI><B>Title<\/B> byAuthor(\nURL:www.sff.net\/locus\/c\nThethreetemplateswerethenusedtoretrieve4047more(author,title)examples. Theexam-\nples were then used to generate more templates, and so on, eventually yielding over 15,000\ntitles. Givenagood setoftemplates, thesystem can collect agood setofexamples. Givena\ngoodsetofexamples,thesystemcanbuildagoodsetoftemplates.\nThe biggest weakness in this approach is the sensitivity to noise. If one of the first\nfew templates isincorrect, errors can propagate quickly. One waytolimit this problem isto\nnot accept a new example unless it is verified by multiple templates, and not accept a new\ntemplateunlessitdiscoversmultipleexamplesthatarealsofoundbyothertemplates.\n22.4.6 Machinereading\nAutomatedtemplateconstructionisabigstepupfromhandcraftedtemplateconstruction, but\nitstill requires ahandful oflabeled examples ofeachrelation toget started. Tobuild alarge\nontology withmanythousands ofrelations, eventhatamount ofworkwould beonerous; we\nwouldliketohaveanextractionsystemwithnohumaninputofanykind\u2014asystemthatcould\nreadonitsownandbuildupitsowndatabase. Suchasystemwouldberelation-independent;\nwould work for any relation. In practice, these systems work on all relations in parallel,\nbecauseoftheI\/Odemandsoflargecorpora. Theybehaveless likeatraditionalinformation-\nextraction system thatistargeted atafewrelations andmorelikeahumanreaderwholearns\nfromthetextitself;becauseofthisthefieldhasbeencalled machinereading.\nMACHINEREADING\nArepresentative machine-reading systemisTEXTRUNNER (BankoandEtzioni,2008).\nTEXTRUNNER uses cotraining toboost itsperformance, but itneeds something to bootstrap\nfrom. InthecaseofHearst(1992),specificpatterns(e.g.,suchas)providedthebootstrap,and\nforBrin (1998), itwasaset offiveauthor\u2013title pairs. For TEXTRUNNER, the original inspi-\nration wasataxonomy ofeight verygeneral syntactic templates, asshowninFigure 22.3. It\nwasfeltthatasmallnumberoftemplateslikethiscouldcovermostofthewaysthatrelation-\nshipsareexpressedinEnglish. Theactualbootsrappingstartsfromasetoflabelledexamples\nthat are extracted from the PennTreebank, acorpus of parsed sentences. Forexample, from\ntheparseofthesentence \u201cEinstein received theNobelPrize in1921,\u201d TEXTRUNNER isable 882 Chapter 22. NaturalLanguageProcessing\ntoextracttherelation(\u201cEinstein,\u201d \u201creceived,\u201d\u201cNobelPrize\u201d).\nGiven a set of labeled examples of this type, TEXTRUNNER trains a linear-chain CRF\nto extract further examples from unlabeled text. The features in the CRF include function\nwords like \u201cto\u201d and \u201cof\u201d and \u201cthe,\u201d but not nouns and verbs (and not noun phrases or verb\nphrases). Because TEXTRUNNER is domain-independent, it cannot rely on predefined lists\nofnounsandverbs.\nType Template Example Frequency\nVerb NP Verb NP XestablishedY 38%\n1 2\nNoun\u2013Prep NP NP Prep NP XsettlementwithY 23%\n1 2\nVerb\u2013Prep NP Verb Prep NP XmovedtoY 16%\n1 2\nInfinitive NP toVerb NP XplanstoacquireY 9%\n1 2\nModifier NP Verb NP Noun XisYwinner 5%\n1 2\nNoun-Coordinate NP (,|and|-|:)NP NP X-Ydeal 2%\n1 2\nVerb-Coordinate NP (,|and)NP Verb X,Ymerge 1%\n1 2\nAppositive NP NP (:|,)? NP Xhometown: Y 1%\n1 2\nFigure22.3 Eightgeneraltemplatesthatcoverabout95%ofthewaysthat relationsare\nexpressedinEnglish.\nTEXTRUNNER achieves a precision of 88% and recall of 45% (F\n1\nof 60%) on a large\nWeb corpus. TEXTRUNNER has extracted hundreds of millions of facts from a corpus of a\nhalf-billion Web pages. Forexample, even though it has no predefined medical knowledge,\nithasextracted over2000answerstothequery[whatkillsbacteria]; correct answersinclude\nantibiotics, ozone, chlorine, Cipro, andbroccoli sprouts. Questionable answersinclude \u201cwa-\nter,\u201d whichcamefrom thesentence \u201cBoiling waterforatleast 10minutes willkillbacteria.\u201d\nItwouldbebettertoattribute thisto\u201cboilingwater\u201dratherthanjust\u201cwater.\u201d\nWiththetechniques outlined inthischapterandcontinual newinventions, wearestart-\ningtogetclosertothegoalofmachinereading.\n22.5 SUMMARY\nThemainpointsofthischapterareasfollows:\n\u2022 Probabilistic language models based on n-grams recover a surprising amount of infor-\nmation about a language. They can perform well on such diverse tasks as language\nidentification, spellingcorrection, genreclassification, andnamed-entity recognition.\n\u2022 These language models can have millions of features, so feature selection and prepro-\ncessingofthedatatoreducenoiseisimportant.\n\u2022 Text classification can be done with naive Bayes n-gram models or with any of the\nclassification algorithms wehave previously discussed. Classification canalso beseen\nasaproblem indatacompression. Bibliographical andHistorical Notes 883\n\u2022 Information retrieval systems use a very simple language model based on bags of\nwords, yet still manage to perform well in terms of recall and precision on very large\ncorporaoftext. OnWebcorpora, link-analysis algorithmsimproveperformance.\n\u2022 Questionansweringcanbehandledbyanapproachbasedoninformationretrieval,for\nquestions that have multiple answers in the corpus. When more answers are available\ninthecorpus, wecanusetechniques thatemphasizeprecision ratherthanrecall.\n\u2022 Information-extraction systems use a more complex model that includes limited no-\ntions of syntax and semantics in the form of templates. They can be built from finite-\nstateautomata,HMMs,orconditionalrandomfields,andcanbelearnedfromexamples.\n\u2022 Inbuildingastatisticallanguagesystem,itisbesttodeviseamodelthatcanmakegood\nuseofavailable data,evenifthemodelseemsoverlysimplistic.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nN-gram letter models for language modeling were proposed by Markov (1913). Claude\nShannon (Shannon and Weaver, 1949) was the first to generate n-gram word models of En-\nglish. Chomsky(1956,1957)pointedoutthelimitationsoffinite-statemodelscomparedwith\ncontext-free models, concluding, \u201cProbabilistic models give no particular insight into some\nofthebasicproblemsofsyntacticstructure.\u201d Thisistrue, butprobabilisticmodelsdoprovide\ninsight into some other basic problems\u2014problems that context-free models ignore. Chom-\nsky\u2019sremarkshadtheunfortunateeffectofscaringmanypeopleawayfromstatisticalmodels\nfortwodecades, untilthesemodelsreemerged foruseinspeechrecognition (Jelinek, 1976).\nKessleretal.(1997)showhowtoapplycharactern-grammodelstogenreclassification,\nand Kleinet al. (2003) describe named-entity recognition with character models. Franz and\nBrants(2006) describe theGoogle n-gram corpusof13millionunique wordsfrom atrillion\nwordsofWebtext;itisnowpublicly available. Thebagofwordsmodelgetsitsnamefrom\na passage from linguist Zellig Harris (1954), \u201clanguage is not merely a bag of words but\na tool with particular properties.\u201d Norvig (2009) gives some examples of tasks that can be\naccomplished withn-grammodels.\nAdd-onesmoothing,firstsuggestedbyPierre-SimonLaplace(1816),wasformalizedby\nJeffreys (1948), and interpolation smoothing is due to Jelinek and Mercer (1980), who used\nit for speech recognition. Other techniques include Witten\u2013Bell smoothing (1991), Good\u2013\nTuring smoothing (Church and Gale, 1991) and Kneser\u2013Ney smoothing (1995). Chen and\nGoodman(1996)andGoodman(2001)surveysmoothing techniques.\nSimple n-gram letter and word models are not the only possible probabilistic models.\nBlei et al. (2001) describe a probabilistic text model called latent Dirichlet allocation that\nviewsadocumentasamixtureoftopics,eachwithitsowndistribution ofwords. Thismodel\ncan be seen as an extension and rationalization of the latent semantic indexing model of\n(Deerwester et al., 1990) (see also Papadimitriou et al. (1998)) and is also related to the\nmultiple-cause mixturemodelof(Sahamietal.,1996). 884 Chapter 22. NaturalLanguageProcessing\nManningandSchu\u00a8tze(1999)andSebastiani(2002)surveytext-classificationtechniques.\nJoachims (2001) uses statistical learning theory and support vector machines to give a theo-\nreticalanalysisofwhenclassificationwillbesuccessful. Apte\u00b4etal.(1994)reportanaccuracy\nof96% inclassifying Reutersnewsarticles intothe\u201cEarnings\u201d category. KollerandSahami\n(1997)reportaccuracyupto95%withanaiveBayesclassifier,andupto98.6%withaBayes\nclassifier that accounts for some dependencies among features. Lewis (1998) surveys forty\nyears of application of naive Bayes techniques to text classification and retrieval. Schapire\nand Singer (2000) show that simple linear classifiers can often achieve accuracy almost as\ngood as more complex models and are more efficient to evaluate. Nigamet al. (2000) show\nhow to use the EM algorithm to label unlabeled documents, thus learning a better classifi-\ncation model. Witten et al. (1999) describe compression algorithms for classification, and\nshow the deep connection between the LZW compression algorithm and maximum-entropy\nlanguage models.\nMany of the n-gram model techniques are also used in bioinformatics problems. Bio-\nstatisticsandprobabilisticNLParecomingclosertogether,aseachdealswithlong,structured\nsequences chosenfromanalphabet ofconstituents.\nThe field of information retrieval is experiencing a regrowth in interest, sparked by\nthe wide usage of Internet searching. Robertson (1977) gives an early overview and intro-\nduces the probability ranking principle. Croft et al. (2009) and Manning et al. (2008) are\nthe firsttextbooks to coverWeb-based search as wellas traditional IR.Hearst (2009) covers\nuser interfaces for Web search. The TREC conference, organized by the U.S. government\u2019s\nNational Institute of Standards and Technology (NIST), hosts an annual competition for IR\nsystems and publishes proceedings with results. In the first seven years of the competition,\nperformance roughly doubled.\nThemostpopularmodelforIRisthevectorspacemodel(Saltonetal.,1975). Salton\u2019s\nwork dominated the early years of the field. There are two alternative probabilistic models,\none due to Ponte and Croft (1998) and one by Maron and Kuhns (1960) and Robertson and\nSparck Jones (1976). Lafferty and Zhai (2001) show that the models are based on the same\njoint probability distribution, but that the choice of model has implications for training the\nparameters. Craswelletal.(2005)describetheBM25scoringfunctionandSvoreandBurges\n(2009)describehowBM25canbeimprovedwithamachinelearning approachthatincorpo-\nratesclickdata\u2014examples ofpastsearchqueiesandtheresultsthatwereclickedon.\nBrin and Page (1998) describe the PageRank algorithm and the implementation of a\nWebsearchengine. Kleinberg (1999)describes theHITSalgorithm. Silverstein etal.(1998)\ninvestigate a log of a billion Web searches. The journal Information Retrieval and the pro-\nceedings oftheannualSIGIRconference coverrecentdevelopments inthefield.\nEarlyinformation extraction programs include GUS (Bobrow etal.,1977) and FRUMP\n(DeJong, 1982). Recentinformation extraction hasbeenpushed forwardbytheannual Mes-\nsage Understand Conferences (MUC), sponsored by the U.S. government. The FASTUS\nfinite-state system was done by Hobbs et al. (1997). It was based in part on the idea from\nPereira and Wright (1991) of using FSAs as approximations to phrase-structure grammars.\nSurveys of template-based systems are given by Roche and Schabes (1997), Appelt (1999), Exercises 885\nand Muslea (1999). Large databases of facts were extracted by Craven et al. (2000), Pasca\netal.(2006),Mitchell(2007), andDurmeandPasca(2008).\nFreitag and McCallum (2000) discuss HMMs for Information Extraction. CRFs were\nintroduced by Lafferty et al. (2001); an example of their use for information extraction is\ndescribedin(McCallum,2003)andatutorialwithpractical guidanceisgivenby(Suttonand\nMcCallum,2007). Sarawagi(2007)givesacomprehensive survey.\nBanko et al. (2002) present the ASKMSR question-answering system; a similar sys-\ntem is due to Kwok et al. (2001). Pasca and Harabagiu (2001) discuss a contest-winning\nquestion-answering system. Twoearly influential approaches toautomated knowledge engi-\nneeringwerebyRiloff(1993),whoshowedthatanautomatically constructed dictionaryper-\nformedalmostaswellasacarefullyhandcrafteddomain-specificdictionary,andbyYarowsky\n(1995),whoshowedthatthetaskofwordsenseclassification(seepage756)couldbeaccom-\nplishedthroughunsupervised trainingonacorpusofunlabeledtextwithaccuracyasgoodas\nsupervised methods.\nTheideaofsimultaneouslyextractingtemplatesandexamplesfromahandfuloflabeled\nexamples was developed independently and simultaneously by Blum and Mitchell (1998),\nwho called it cotraining and by Brin (1998), who called it DIPRE (Dual Iterative Pattern\nRelation Extraction). You can see why the term cotraining has stuck. Similar early work,\nunderthenameofbootstrapping, wasdonebyJonesetal.(1999). Themethodwasadvanced\nby the QXTRACT (Agichtein and Gravano, 2003) and KNOWITALL (Etzioni et al., 2005)\nsystems. MachinereadingwasintroducedbyMitchell(2005)andEtzionietal.(2006)andis\nthefocusofthe TEXTRUNNER project(Banko etal.,2007;BankoandEtzioni,2008).\nThischapterhasfocused onnaturallanguage text,butitisalsopossible todoinforma-\ntion extraction based on the physical structure or layout of text rather than on the linguistic\nstructure. HTML lists and tables in both HTML and relational databases are home to data\nthatcanbeextractedandconsolidated(Hurst,2000;Pintoetal.,2003;Cafarellaetal.,2008).\nThe Association for Computational Linguistics (ACL) holds regular conferences and\npublishes the journal Computational Linguistics. There is also an International Conference\nonComputationalLinguistics(COLING).ThetextbookbyManningandSchu\u00a8tze(1999)cov-\ners statistical language processing, while Jurafsky and Martin (2008) give a comprehensive\nintroduction tospeechandnaturallanguage processing.\nEXERCISES\n22.1 This exercise explores the quality of the n-gram model of language. Find or create a\nmonolingual corpus of100,000 wordsormore. Segmentitinto words, andcompute the fre-\nquencyofeachword. Howmanydistinctwordsarethere? Alsocountfrequenciesofbigrams\n(twoconsecutive words)and trigrams(three consecutive words). Nowusethose frequencies\ntogeneratelanguage: fromtheunigram,bigram,andtrigram models,inturn,generatea100-\nword text by making random choices according to the frequency counts. Compare the three\ngenerated textswithactuallanguage. Finally, calculatetheperplexity ofeachmodel. 886 Chapter 22. NaturalLanguageProcessing\n22.2 Write a program to do segmentation of words without spaces. Given a string, such\nastheURL\u201cthelongestlistofthelongeststuffatthelongestdomainnameatlonglast.com,\u201d returna\nlist ofcomponent words: [\u201cthe,\u201d \u201clongest,\u201d \u201clist,\u201d ...]. Thistask isuseful forparsing URLs,\nfor spelling correction when words runtogether, and for languages such as Chinese that do\nnot have spaces between words. It can be solved with aunigram orbigram word model and\nadynamicprogramming algorithm similartotheViterbialgorithm.\n22.3 (AdaptedfromJurafskyandMartin(2000).) Inthisexercise youwilldevelopaclassi-\nfierforauthorship: given atext, the classifier predicts which oftwocandidate authors wrote\nthe text. Obtain samples of text from two different authors. Separate them into training and\ntest sets. Now train a language model on the training set. You can choose what features to\nuse; n-grams of words orletters are the easiest, but you can add additional features that you\nthink may help. Then compute the probability of the text under each language model and\nchose the most probable model. Assess the accuracy of this technique. How does accuracy\nchange as you alter the set of features? This subfield of linguistics is called stylometry; its\nSTYLOMETRY\nsuccessesincludetheidentificationoftheauthorofthedisputedFederalistPapers(Mosteller\nand Wallace, 1964) and some disputed works of Shakespeare (Hope, 1994). Khmelev and\nTweedie(2001) producegoodresultswithasimpleletterbigrammodel.\n22.4 Thisexerciseconcernstheclassificationofspamemail. Createacorpusofspamemail\nandoneofnon-spammail. Examineeachcorpusanddecidewhatfeaturesappeartobeuseful\nfor classification: unigram words? bigrams? message length, sender, time of arrival? Then\ntrainaclassificationalgorithm(decisiontree,naiveBayes,SVM,logisticregression, orsome\notheralgorithmofyourchoosing)onatrainingsetandreportitsaccuracyonatestset.\n22.5 Create a test set of ten queries, and pose them to three major Web search engines.\nEvaluate each one for precision at 1, 3, and 10 documents. Can you explain the differences\nbetweenengines?\n22.6 Trytoascertain whichofthesearchenginesfromthepreviousexerciseareusingcase\nfolding, stemming,synonyms, andspelling correction.\n22.7 Write aregular expression orashort program to extract company names. Testit on a\ncorpusofbusiness newsarticles. Reportyourrecallandprecision.\n22.8 Consider the problem of trying to evaluate the quality of an IR system that returns a\nranked list of answers (like most Web search engines). The appropriate measure of quality\ndepends on the presumed model of what the searcher is trying to achieve, and whatstrategy\nsheemploys. Foreachofthefollowingmodels,propose acorresponding numericmeasure.\na. Thesearcherwilllookatthefirsttwentyanswersreturned, withtheobjectiveofgetting\nasmuchrelevantinformation aspossible.\nb. Thesearcherneedsonlyonerelevantdocument,andwillgodownthelistuntilshefinds\nthefirstone.\nc. Thesearcherhasafairlynarrowqueryandisabletoexamine alltheanswersretrieved.\nShe wants to be sure that she has seen everything in the document collection that is Exercises 887\nrelevant to her query. (E.g., a lawyer wants to be sure that she has found all relevant\nprecedents, andiswillingtospendconsiderable resources onthat.)\nd. The searcher needs just one document relevant to the query, and can afford to pay a\nresearchassistantforanhour\u2019sworklookingthroughtheresults. Theassistantcanlook\nthrough 100 retrieved documents inan hour. Theassistant will charge the searcher for\nthefullhourregardlessofwhetherhefindsitimmediatelyor attheendofthehour.\ne. The searcher will look through all the answers. Examining a document has cost $A;\nfinding arelevant document has value $B; failing to finda relevant document has cost\n$C foreachrelevantdocumentnotfound.\nf. Thesearcherwantstocollectasmanyrelevantdocumentsaspossible, butneedssteady\nencouragement. She looks through the documents in order. If the documents she has\nlookedatsofararemostlygood, shewillcontinue; otherwise, shewillstop. 23\nNATURAL LANGUAGE\nFOR COMMUNICATION\nInwhichweseehowhumanscommunicatewithoneanotherinnaturallanguage,\nandhowcomputeragentsmightjoinintheconversation.\nCommunicationistheintentional exchange ofinformation broughtaboutby theproduction\nCOMMUNICATION\nandperceptionofsignsdrawnfromasharedsystemofconventional signs. Mostanimalsuse\nSIGN\nsigns torepresent important messages: foodhere, predator nearby, approach, withdraw, let\u2019s\nmate. Inapartially observable world, communication canhelpagents besuccessful because\ntheycanlearninformationthatisobservedorinferredbyothers. Humansarethemostchatty\nof all species, and if computer agents are to be helpful, they\u2019ll need to learn to speak the\nlanguage. In this chapter we look at language models for communication. Models aimed at\ndeep understanding of a conversation necessarily need to be more complex than the simple\nmodels aimed at, say, spam classification. We start with grammatical models of the phrase\nstructure of sentences, add semantics to the model, and then apply it to machine translation\nandspeechrecognition.\n23.1 PHRASE STRUCTURE GRAMMARS\nThe n-gram language models of Chapter 22 were based on sequences of words. The big\nissueforthesemodelsisdatasparsity\u2014withavocabulary of,say,105 words,thereare1015\ntrigram probabilities to estimate, and so a corpus of even a trillion words will not be able to\nsupply reliable estimates for all of them. We can address the problem of sparsity through\ngeneralization. Fromthefactthat\u201cblack dog\u201dismorefrequent than\u201cdogblack\u201dandsimilar\nobservations, we can form the generalization that adjectives tend to come before nouns in\nEnglish (whereas they tend to follow nouns in French: \u201cchien noir\u201d is more frequent). Of\ncoursetherearealwaysexceptions;\u201cgalore\u201disanadjectivethatfollowsthenounitmodifies.\nDespitetheexceptions,thenotionofalexicalcategory(alsoknownasapartofspeech)such\nLEXICALCATEGORY\nasnounoradjective isausefulgeneralization\u2014useful initsownright,butmore sowhenwe\nSYNTACTIC string together lexical categories to form syntactic categories such as noun phrase or verb\nCATEGORIES\nphrase, and combine these syntactic categories into trees representing the phrase structure\nPHRASESTRUCTURE\nofsentences: nestedphrases, eachmarkedwithacategory.\n888 Section23.1. PhraseStructureGrammars 889\nGENERATIVE CAPACITY\nGrammaticalformalismscanbeclassifiedbytheirgenerativecapacity: thesetof\nlanguages theycanrepresent. Chomsky(1957)describesfourclassesofgrammat-\nical formalisms that differ only in the form of the rewrite rules. The classes can\nbe arranged in a hierarchy, where each class can be used to describe all the lan-\nguages that can be described by a less powerful class, as well as some additional\nlanguages. Herewelistthehierarchy, mostpowerfulclassfirst:\nRecursively enumerable grammars use unrestricted rules: both sides of the\nrewrite rules can have any number of terminal and nonterminal symbols, asin the\nruleAB C \u2192 D E. Thesegrammarsareequivalent toTuringmachinesintheir\nexpressivepower.\nContext-sensitive grammars are restricted only in that the right-hand side\nmust contain at least as many symbols as the left-hand side. The name \u201ccontext-\nsensitive\u201d comes from the fact that a rule such as A X B \u2192 A Y B says that\nan X can be rewritten as a Y in the context of a preceding A and a following B.\nContext-sensitive grammars can represent languages such as anbncn (a sequence\nofncopiesofafollowedbythesamenumberofbsandthencs).\nIn context-free grammars (or CFGs), the left-hand side consists of a sin-\ngle nonterminal symbol. Thus, each rule licenses rewriting the nonterminal as\nthe right-hand side in any context. CFGs are popular for natural-language and\nprogramming-language grammars, although itisnowwidelyaccepted thatatleast\nsomenaturallanguageshaveconstructionsthatarenotcontext-free(Pullum,1991).\nContext-free grammarscanrepresent anbn,butnotanbncn.\nRegulargrammars are the most restricted class. Every rule has asingle non-\nterminalontheleft-handsideandaterminalsymboloptionallyfollowedbyanon-\nterminalontheright-handside. Regulargrammarsareequivalentinpowertofinite-\nstate machines. They are poorly suited for programming languages, because they\ncannot represent constructs such as balanced opening and closing parentheses (a\nvariation ofthe anbn language). Theclosest theycancomeisrepresenting a\u2217 b\u2217 ,a\nsequence ofanynumberofasfollowedbyanynumberofbs.\nThe grammars higher up in the hierarchy have more expressive power, but\nthe algorithms for dealing with them are less efficient. Up to the 1980s, linguists\nfocusedoncontext-freeandcontext-sensitivelanguages. Sincethen,therehasbeen\nrenewed interest in regular grammars, brought about by the need to process and\nlearn from gigabytes or terabytes of online text very quickly, even at the cost of\na less complete analysis. As Fernando Pereira put it, \u201cThe older I get, the further\ndown the Chomsky hierarchy I go.\u201d To see what he means, compare Pereira and\nWarren (1980) with Mohri, Pereira, and Riley (2002) (and note that these three\nauthorsallnowworkonlargetextcorporaatGoogle). 890 Chapter 23. NaturalLanguageforCommunication\nThere have been many competing language models based on the idea of phrase struc-\nPROBABILISTIC\nture; we will describe a popular model called the probabilistic context-free grammar, or\nCONTEXT-FREE\nGRAMMAR PCFG.1 A grammar is a collection of rules that defines a language as a set of allowable\nGRAMMAR\nstrings ofwords. \u201cContext-free\u201d isdescribed inthesidebar onpage 889, and \u201cprobabilistic\u201d\nLANGUAGE\nmeansthatthegrammarassignsaprobability toeverystring. HereisaPCFGrule:\nVP \u2192 Verb [0.70]\n| VP NP [0.30].\nNON-TERMINAL Here VP (verb phrase) and NP (noun phrase) are non-terminal symbols. The grammar\nSYMBOLS\nalsorefers toactual words, whicharecalled terminalsymbols. Thisruleissaying thatwith\nTERMINALSYMBOL\nprobability 0.70 averb phrase consists solely ofa verb, and with probability 0.30 itis a VP\nfollowedbyanNP. Appendix Bdescribes non-probabilistic context-free grammars.\nWenow defineagrammarforatiny fragment ofEnglish thatissuitable forcommuni-\ncationbetweenagentsexploringthewumpusworld. WecallthislanguageE . Latersections\n0\nimprove on E to make it slightly closer to real English. We are unlikely ever to devise a\n0\ncompletegrammarforEnglish, ifonlybecausenotwopersons wouldagreeentirelyonwhat\nconstitutes validEnglish.\n23.1.1 The lexiconofE\n0\nFirstwedefinethelexicon,orlistofallowablewords. Thewordsaregroupedintothelexical\nLEXICON\ncategories familiar to dictionary users: nouns, pronouns, and names to denote things; verbs\nto denote events; adjectives to modify nouns; adverbs to modify verbs; and function words:\narticles (such as the), prepositions (in), and conjunctions (and). Figure 23.1 shows a small\nlexiconforthelanguage E .\n0\nEachofthecategories endsin...toindicate thatthereareotherwordsinthecategory.\nFor nouns, names, verbs, adjectives, and adverbs, it is infeasible even in principle to list all\nthe words. Not only are there tens of thousands of members in each class, but new ones\u2013\nlike iPod or biodiesel\u2014are being added constantly. These five categories are called open\nclasses. Forthecategoriesofpronoun, relativepronoun, article, preposition, andconjunction\nOPENCLASS\nwe could have listed all the words with a little more work. These are called closed classes;\nCLOSEDCLASS\nthey have a small number of words (a dozen or so). Closed classes change over the course\nofcenturies, notmonths. Forexample, \u201cthee\u201d and \u201cthou\u201d werecommonly used pronouns in\nthe17thcentury, wereonthedeclineinthe19th,andareseen todayonlyinpoetryandsome\nregionaldialects.\n23.1.2 The GrammarofE\n0\nThe next step is to combine the words into phrases. Figure 23.2 shows a grammar for E ,\n0\nwith rules for each of the six syntactic categories and an example for each rewrite rule.2\nFigure 23.3 shows a parse tree for the sentence \u201cEvery wumpus smells.\u201d The parse tree\nPARSETREE\n1 PCFGsarealsoknownasstochasticcontext-freegrammars,orSCFGs.\n2 A relative clause follows and modifies a noun phrase. It consists of a relative pronoun (such as \u201cwho\u201d or\n\u201cthat\u201d)followedbyaverbphrase. Anexampleofarelativeclauseisthatstinksin\u201cThewumpusthatstinksisin\n22.\u201dAnotherkindofrelativeclausehasnorelativepronoun,e.g.,Iknowin\u201cthemanIknow.\u201d Section23.1. PhraseStructureGrammars 891\nNoun \u2192 stench [0.05] | breeze [0.10] | wumpus[0.15]| pits [0.05]| ...\nVerb \u2192 is [0.10] | feel [0.10]| smells [0.10]| stinks [0.05] | ...\nAdjective \u2192 right [0.10]| dead [0.05]| smelly [0.02]| breezy [0.02]...\nAdverb \u2192 here [0.05]| ahead [0.05] | nearby [0.02] | ...\nPronoun \u2192 me [0.10]| you [0.03] | I[0.10] | it [0.10]| ...\nRelPro \u2192 that [0.40] | which [0.15]| who[0.20]| whom [0.02]\u2228...\nName \u2192 John [0.01] | Mary [0.01]| Boston [0.01]| ...\nArticle \u2192 the [0.40]| a [0.30]| an [0.10] | every [0.05] | ...\nPrep \u2192 to [0.20] | in [0.10]| on [0.05]| near [0.10] | ...\nConj \u2192 and [0.50] | or [0.10] | but[0.20] | yet [0.02]\u2228...\nDigit \u2192 0 [0.20] | 1 [0.20]| 2 [0.20]| 3 [0.20]| 4 [0.20] | ...\nFigure23.1 ThelexiconforE .RelProisshortforrelativepronoun,Prepforpreposition,\n0\nandConj forconjunction.Thesumoftheprobabilitiesforeachcategoryis1.\nE : S \u2192 NP VP [0.90] I+feelabreeze\n0\n| S Conj S [0.10] Ifeelabreeze+and+Itstinks\nNP \u2192 Pronoun [0.30] I\n| Name [0.10] John\n| Noun [0.10] pits\n| Article Noun [0.25] the+wumpus\n| Article Adjs Noun [0.05] the+smellydead+wumpus\n| Digit Digit [0.05] 34\n| NP PP [0.10] thewumpus+in13\n| NP RelClause [0.05] thewumpus+thatissmelly\nVP \u2192 Verb [0.40] stinks\n| VP NP [0.35] feel+abreeze\n| VP Adjective [0.05] smells+dead\n| VP PP [0.10] is+in13\n| VP Adverb [0.10] go+ahead\nAdjs \u2192 Adjective [0.80] smelly\n| Adjective Adjs [0.20] smelly+dead\nPP \u2192 Prep NP [1.00] to+theeast\nRelClause \u2192 RelPro VP [1.00] that+issmelly\nFigure23.2 ThegrammarforE ,withexamplephrasesforeachrule. Thesyntacticcat-\n0\negories are sentence (S), noun phrase (NP), verb phrase (VP), list of adjectives (Adjs),\nprepositionalphrase(PP),andrelativeclause(RelClause). 892 Chapter 23. NaturalLanguageforCommunication\nS\n0.90\nNP VP\n0.25 0.40\nArticle Noun Verb\n0.05 0.15 0.10\nEvery wumpus smells\nFigure23.3 Parsetreeforthesentence\u201cEverywumpussmells\u201daccordingtothegrammar\nE . Eachinteriornodeofthetreeislabeledwithitsprobability. Theprobabilityofthetree\n0\nasawholeis0.9\u00d70.25\u00d70.05\u00d70.15\u00d70.40\u00d70.10=0.0000675. Sincethistreeistheonly\nparseofthesentence,thatnumberisalsotheprobabilityof thesentence. Thetreecanalso\nbewritteninlinearformas[S [NP [Article every][Noun wumpus]][VP [Verb smells]]].\ngivesaconstructive proofthatthestring ofwordsisindeed asentence according totherules\nofE . TheE grammargenerates awiderangeofEnglishsentences suchasthefollowing:\n0 0\nJohnisinthepit\nThewumpusthatstinksisin22\nMaryisinBostonandthewumpusisnear32\nUnfortunately, thegrammar overgenerates: thatis,itgenerates sentences that arenotgram-\nOVERGENERATION\nmatical, such as \u201cMe go Boston\u201d and \u201cI smell pits wumpus John.\u201d It also undergenerates:\nUNDERGENERATION\nthere are many sentences of English that it rejects, such as \u201cI think the wumpus is smelly.\u201d\nWewillseehow to learn abetter grammarlater; fornow weconcentrate onwhat wecan do\nwiththegrammarwehave.\n23.2 SYNTACTIC ANALYSIS (PARSING)\nParsingistheprocessofanalyzingastringofwordstouncoveritsphrasestructure,according\nPARSING\ntotherulesofagrammar. Figure23.4showsthatwecanstartwiththeS symbol andsearch\ntopdownforatreethathasthewordsasitsleaves,orwecanstartwiththewordsandsearch\nbottom up for a tree that culminates in an S. Both top-down and bottom-up parsing can be\ninefficient,however,becausetheycanenduprepeatingeffortinareasofthesearchspacethat\nleadtodeadends. Considerthefollowingtwosentences:\nHavethestudentsinsection2ofComputerScience101taketheexam.\nHavethestudentsinsection2ofComputerScience101takentheexam?\nEventhoughtheysharethefirst10words,thesesentenceshaveverydifferentparses,because\nthe first is a command and the second is a question. A left-to-right parsing algorithm would\nhave toguess whetherthe firstwordispart of acommand oraquestion andwillnot beable\nto tell if the guess is correct until at least the eleventh word, take or taken. If the algorithm\nguesseswrong,itwillhavetobacktrackallthewaytothefirstwordandreanalyze thewhole\nsentence undertheotherinterpretation. Section23.2. SyntacticAnalysis(Parsing) 893\nListofitems Rule\nS\nNP VP S \u2192 NP VP\nNP VP Adjective VP \u2192 VP Adjective\nNP Verb Adjective VP \u2192 Verb\nNP Verb dead Adjective \u2192 dead\nNP isdead Verb \u2192 is\nArticle Noun isdead NP \u2192 Article Noun\nArticle wumpusisdead Noun \u2192 wumpus\nthewumpusisdead Article \u2192 the\nFigure23.4 Traceoftheprocessoffindingaparseforthestring\u201cThewumpusisdead\u201d\nasasentence,accordingtothegrammarE . Viewedasatop-downparse,westartwiththe\n0\nlistofitemsbeingS and,oneachstep,matchanitemX witharuleoftheform(X \u2192...)\nandreplaceX inthelistofitemswith(...). Viewedasabottom-upparse,westartwiththe\nlistofitemsbeingthewordsofthesentence,and,oneachstep,matchastringoftokens(...)\ninthelistagainstaruleoftheform(X \u2192...) andreplace(...) withX.\nTo avoid this source of inefficiency we can use dynamic programming: every time we\nanalyze a substring, store the results so we won\u2019t have to reanalyze it later. For example,\noncewediscoverthat\u201cthestudentsinsection2ofComputerScience101\u201disanNP,wecan\nrecordthatresultinadatastructureknownasachart. Algorithmsthatdothisarecalledchart\nCHART\nparsers. Because we are dealing with context-free grammars, any phrase that was found in\nthecontextofonebranchofthesearchspacecanworkjustaswellinanyotherbranchofthe\nsearch space. Therearemanytypes ofchartparsers; wedescribe abottom-up version called\ntheCYKalgorithm,afteritsinventors, JohnCocke,DanielYounger, andTadeo Kasami.\nCYKALGORITHM\nThe CYK algorithm is shown in Figure 23.5. Note that it requires a grammar with all\nrulesinoneoftwoveryspecificformats: lexicalrulesofthe formX \u2192 word,andsyntactic\nCHOMSKYNORMAL rules of the form X \u2192 Y Z. This grammar format, called Chomsky Normal Form, may\nFORM\nseem restrictive, but it is not: any context-free grammar can be automatically transformed\nintoChomskyNormalForm. Exercise23.8leadsyouthroughtheprocess.\nThe CYK algorithm uses space of O(n2m) for the P table, where n is the number of\nwordsinthesentence,andmisthenumberofnonterminalsymbolsinthegrammar,andtakes\ntimeO(n3m). (Sincemisconstant foraparticular grammar, this iscommonly described as\nO(n3).) No algorithm can do better for general context-free grammars, although there are\nfasteralgorithms onmorerestricted grammars. Infact, itisquite atrick forthealgorithm to\ncompleteinO(n3)time,giventhatitispossibleforasentencetohaveanexponentialnumber\nofparsetrees. Considerthesentence\nFallleavesfallandspringleavesspring.\nIt is ambiguous because each word (except \u201cand\u201d) can be either a noun ora verb, and \u201cfall\u201d\nand \u201cspring\u201d can be adjectives as well. (For example, one meaning of \u201cFall leaves fall\u201d is 894 Chapter 23. NaturalLanguageforCommunication\nfunctionCYK-PARSE(words,grammar)returnsP,atableofprobabilities\nN \u2190LENGTH(words)\nM \u2190thenumberofnonterminalsymbolsingrammar\nP\u2190anarrayofsize[M,N,N],initiallyall0\n\/*Insertlexicalrulesforeachword*\/\nfori =1toN do\nforeachruleofform(X \u2192 wordsi[p])do\nP[X,i,1]\u2190p\n\/*Combinefirstandsecondpartsofright-handsidesofrules,fromshorttolong*\/\nforlength =2toN do\nforstart =1toN \u2212length +1 do\nforlen1 =1toN \u22121do\nlen2 \u2190length\u2212len1\nforeachruleoftheform(X \u2192 Y Z [p])do\nP[X,start,length]\u2190MAX(P[X, start, length],\nP[Y, start, len1] \u00d7 P[Z, start +len1, len2] \u00d7 p)\nreturnP\nFigure 23.5 The CYK algorithm for parsing. Given a sequence of words, it finds the\nmost probable derivation for the whole sequence and for each subsequence. It returns the\nwholetable, P, in whichan entry P[X, start, len] is the probabilityofthe mostprobable\nX of lengthlen starting atposition start. If thereis no X of thatsize atthatlocation, the\nprobabilityis0.\nequivalent to\u201cAutumnabandons autumn.) WithE thesentence hasfourparses:\n0\n[S [S [NP Fallleaves]fall]and[S [NP springleaves]spring]\n[S [S [NP Fallleaves]fall]and[S spring[VP leavesspring]]\n[S [S Fall[VP leavesfall]]and[S [NP springleaves]spring]\n[S [S Fall[VP leavesfall]]and[S spring[VP leavesspring]] .\nIfwehadctwo-ways-ambiguous conjoined subsentences, wewouldhave2c waysofchoos-\ning parses forthe subsentences.3 How does the CYK algorithm process these 2c parse trees\nin O(c3) time? The answer is that it doesn\u2019t examine all the parse trees; all it has to do is\ncompute the probability of the most probable tree. The subtrees are all represented in the P\ntable,andwithalittleworkwecouldenumeratethemall(inexponentialtime),butthebeauty\noftheCYKalgorithm isthatwedon\u2019thavetoenumeratethemunlesswewantto.\nInpracticeweareusuallynotinterestedinallparses;justthebestoneorbestfew. Think\nof the CYK algorithm as defining the complete state space defined by the \u201capply grammar\n\u2217\nrule\u201d operator. It is possible to search just part of this space using A search. Each state\nin this space is a list of items (words or categories), as shown in the bottom-up parse table\n(Figure 23.4). The start state is a list of words, and a goal state is the single item S. The\n3 TherealsowouldbeO(c!)ambiguityinthewaythecomponentsconjoin\u2014forexample,(X and(Y andZ))\nversus((XandY)andZ).Butthatisanotherstory,onetoldwellbyChurchandPatil(1982). Section23.2. SyntacticAnalysis(Parsing) 895\n[[S[NP-SBJ-2Hereyes]\n[VPwere\n[VPglazed\n[NP*-2]\n[SBAR-ADVasif\n[S[NP-SBJshe]\n[VPdidn\u2019t\n[VP[VPhear[NP*-1]]\nor\n[VP[ADVPeven]see[NP*-1]]\n[NP-1him]]]]]]]]\n.]\nFigure23.6 Annotatedtree forthe sentence\u201cHereyeswere glazedasif she didn\u2019thear\norevensee him.\u201d fromthePennTreebank. Notethatinthisgrammarthereisadistinction\nbetweenanobjectnounphrase(NP)andasubjectnounphrase(NP-SBJ).Notealsoagram-\nmaticalphenomenonwe havenotcoveredyet: the movementof a phrasefrom onepartof\nthetreetoanother.Thistreeanalyzesthephrase\u201chearorevenseehim\u201dasconsistingoftwo\nconstituent VPs, [VP hear [NP *-1]] and [VP [ADVP even] see [NP *-1]], both of which\nhaveamissingobject,denoted*-1,whichreferstotheNP labeledelsewhereinthetreeas\n[NP-1him].\ncostofastateistheinverse ofitsprobability asdefinedbytherulesapplied sofar, andthere\narevarious heuristics toestimatetheremaining distance tothegoal;thebestheuristics come\n\u2217\nfrommachinelearningappliedtoacorpusofsentences. WiththeA algorithmwedon\u2019thave\nto search the entire state space, and we are guaranteed that the first parse found will be the\nmostprobable.\n23.2.1 Learning probabilities forPCFGs\nA PCFG has many rules, with a probability for each rule. This suggests that learning the\ngrammarfromdatamightbebetterthanaknowledgeengineering approach. Learningiseas-\niestifwearegivenacorpusofcorrectlyparsedsentences,commonlycalledatreebank. The\nTREEBANK\nPennTreebank (Marcus etal., 1993) isthe best known; itconsists of3million words which\nhavebeenannotated withpartofspeech andparse-tree structure, usinghumanlaborassisted\nbysomeautomated tools. Figure23.6showsanannotated tree fromthePennTreebank.\nGivenacorpusoftrees,wecancreateaPCFGjustbycounting(andsmoothing). Inthe\nexampleabove,therearetwonodesoftheform [S[NP...][VP ...]]. Wewouldcountthese,\nand all the other subtrees with root S in the corpus. If there are 100,000 S nodes of which\n60,000areofthisform,thenwecreatetherule:\nS \u2192 NP VP [0.60].\nWhat if a treebank is not available, but we have a corpus of raw unlabeled sentences? It is\nstill possible to learn a grammar from such a corpus, but it is more difficult. First of all,\nweactually have twoproblems: learning the structure ofthe grammarrules and learning the 896 Chapter 23. NaturalLanguageforCommunication\nprobabilitiesassociatedwitheachrule. (WehavethesamedistinctioninlearningBayesnets.)\nWe\u2019ll assume that we\u2019re given the lexical and syntactic category names. (If not, we can just\nassume categories X ,...X and use cross-validation to pick the best value of n.) We can\n1 n\nthen assume that the grammar includes every possible (X \u2192 Y Z) or (X \u2192 word) rule,\nalthough manyoftheseruleswillhaveprobability 0orclose to0.\nWecanthenuseanexpectation\u2013maximization(EM)approach,justaswedidinlearning\nHMMs. Theparameters wearetrying tolearn are therule probabilities; westartthem offat\nrandom oruniform values. Thehidden variables aretheparse trees: wedon\u2019t know whether\nastringofwords w ...w isorisnotgenerated byarule(X \u2192 ...). TheEstepestimates\ni j\nthe probability that each subsequence is generated by each rule. The M step then estimates\ntheprobability ofeachrule. Thewholecomputationcanbedoneinadynamic-programming\nINSIDE\u2013OUTSIDE fashion with an algorithm called the inside\u2013outside algorithm in analogy to the forward\u2013\nALGORITHM\nbackwardalgorithm forHMMs.\nTheinside\u2013outsidealgorithmseemsmagicalinthatitinducesagrammarfromunparsed\ntext. Butithasseveraldrawbacks. First,theparsesthatareassignedbytheinducedgrammars\nareoften difficult tounderstand andunsatisfying tolinguists. Thismakes ithard tocombine\nhandcrafted knowledge with automated induction. Second, itis slow: O(n3m3), where nis\nthe number of words in a sentence and m is the number of grammar categories. Third, the\nspace of probability assignments is very large, and empirically it seems that getting stuck in\nlocalmaximaisasevereproblem. Alternativessuchassimulatedannealing cangetcloserto\nthe global maximum, at a cost of even more computation. Lari and Young (1990) conclude\nthatinside\u2013outside is\u201ccomputationally intractable forrealisticproblems.\u201d\nHowever,progress canbemadeifwearewillingtostepoutsidetheboundsoflearning\nsolelyfromunparsedtext. Oneapproachistolearnfrom prototypes: toseedtheprocesswith\nadozenortworules,similartotherulesinE . Fromthere,morecomplexrulescanbelearned\n1\nmoreeasily,andtheresultinggrammarparsesEnglishwithanoverallrecallandprecisionfor\nsentences of about 80% (Haghighi and Klein, 2006). Another approach is to use treebanks,\nbutinadditiontolearningPCFGrulesdirectlyfromthebracketings,alsolearningdistinctions\nthatarenotinthetreebank. Forexample,notthatthetreeinFigure23.6makesthedistinction\nbetween NP and NP \u2212SBJ. The latter is used for the pronoun \u201cshe,\u201d the former for the\npronoun \u201cher.\u201d We will explore this issue in Section 23.6; for now let us just say that there\naremanywaysinwhich itwouldbe useful to splitacategory like NP\u2014grammarinduction\nsystems that use treebanks but automatically split categories do better than those that stick\nwith the original category set (Petrov and Klein, 2007c). The error rates for automatically\nlearnedgrammarsarestillabout50%higherthanforhand-constructed grammar,butthegap\nisdecreasing.\n23.2.2 Comparing context-free andMarkov models\nTheproblemwithPCFGsisthattheyarecontext-free. Thatmeansthatthedifferencebetween\nP(\u201ceat abanana\u201d) and P(\u201ceat abandanna\u201d) depends only onP(Noun \u2192 \u201cbanana\u201d) versus\nP(Noun \u2192 \u201cbandanna\u201d) and not on the relation between \u201ceat\u201d and the respective objects.\nAMarkov modelofordertwoormore, givenasufficiently large corpus, willknow that \u201ceat Section23.3. AugmentedGrammarsandSemanticInterpretation 897\na banana\u201d is more probable. We can combine a PCFG and Markov model to get the best of\nboth. The simplest approach is to estimate the probability of a sentence with the geometric\nmeanoftheprobabilitiescomputedbybothmodels. Thenwewouldknowthat\u201ceatabanana\u201d\nisprobable fromboththegrammaticalandlexicalpointofview. Butitstillwouldn\u2019tpickup\nthe relation between \u201ceat\u201d and \u201cbanana\u201d in \u201ceat a slightly aging but still palatable banana\u201d\nbecause here the relation is more than two words away. Increasing the order of the Markov\nmodel won\u2019t get at the relation precisely; to do that we can use a lexicalized PCFG, as\ndescribed inthenextsection.\nAnotherproblemwithPCFGsisthattheytendtohavetoostrongapreferenceforshorter\nsentences. In a corpus such as the Wall Street Journal, the average length of a sentence\nis about 25 words. But a PCFG will usually assign fairly high probability to many short\nsentences, suchas\u201cHeslept,\u201dwhereasintheJournalwe\u2019remorelikelytoseesomethinglike\n\u201cIthasbeenreportedbyareliablesourcethattheallegationthathesleptiscredible.\u201d Itseems\nthatthephrases inthe Journal really arenotcontext-free; instead thewriters haveanideaof\ntheexpectedsentencelengthandusethatlengthasasoftglobalconstraintontheirsentences.\nThisishardtoreflectinaPCFG.\n23.3 AUGMENTED GRAMMARS AND SEMANTIC INTERPRETATION\nIn this section we see how to extend context-free grammars\u2014to say that, for example, not\nevery NP isindependent ofcontext, butrather, certain NPsaremorelikely toappearinone\ncontext, andothersinanothercontext.\n23.3.1 LexicalizedPCFGs\nTogetattherelationship betweentheverb\u201ceat\u201dandthenouns\u201cbanana\u201d versus\u201cbandanna,\u201d\nwecanusealexicalized PCFG,inwhichtheprobabilities foraruledepend ontherelation-\nLEXICALIZEDPCFG\nship between words in the parse tree, not just on the adjacency of words in a sentence. Of\ncourse, we can\u2019t have the probability depend on every word in the tree, because we won\u2019t\nhave enough training data to estimate all those probabilities. Itisuseful tointroduce the no-\ntion of the head of a phrase\u2014the most important word. Thus, \u201ceat\u201d is the head of the VP\nHEAD\n\u201ceat a banana\u201d and \u201cbanana\u201d is the head of the NP \u201ca banana.\u201d Weuse the notation VP(v)\nto denote a phrase with category VP whose head word is v. We say that the category VP\nAUGMENTED is augmented with the head variable v. Here is an augmented grammar that describes the\nGRAMMAR\nverb\u2013object relation:\nVP(v) \u2192 Verb(v) NP(n) [P (v,n)]\n1\nVP(v) \u2192 Verb(v) [P (v)]\n2\nNP(n) \u2192 Article(a) Adjs(j) Noun(n) [P (n,a)]\n3\nNoun(banana) \u2192 banana [p ]\nn\n... ...\nHerethe probability P (v,n)depends on thehead words v and n. Wewould set this proba-\n1\nbilitytoberelativelyhighwhenv is\u201ceat\u201dandnis\u201cbanana,\u201dandlowwhennis\u201cbandanna.\u201d 898 Chapter 23. NaturalLanguageforCommunication\nNote that since we are considering only heads, the distinction between \u201ceat a banana\u201d and\n\u201ceat a rancid banana\u201d will not be caught by these probabilities. Another issue with this ap-\nproachisthat,inavocabulary with,say,20,000nounsand5,000verbs,P needs100million\n1\nprobability estimates. Onlyafewpercentofthesecancomefromacorpus;therestwillhave\nto come from smoothing (see Section 22.1.2). For example, we can estimateP (v,n) for a\n1\n(v,n) pair that we have not seen often (or at all) by backing off to a model that depends\nonlyonv. Theseobjectlessprobabilities arestillveryuseful;theycancapturethedistinction\nbetweenatransitiveverblike\u201ceat\u201d\u2014whichwillhaveahighvalueforP andalowvaluefor\n1\nP \u2014and an intransitive verb like \u201csleep,\u201d which will have the reverse. It is quite feasible to\n2\nlearntheseprobabilities fromatreebank.\n23.3.2 Formaldefinition ofaugmented grammarrules\nAugmented rules arecomplicated, sowewillgive them aformaldefinition byshowing how\nanaugmented rulecanbetranslated intoalogical sentence. Thesentence willhavetheform\nDEFINITECLAUSE ofadefiniteclause(seepage256),sotheresultiscalledadefiniteclausegrammar,orDCG.\nGRAMMAR\nWe\u2019ll use as an example a version of a rule from the lexicalized grammar for NP with one\nnewpieceofnotation:\nNP(n) \u2192 Article(a) Adjs(j) Noun(n){Compatible(j,n)}.\nThenewaspecthereisthenotation{constraint}todenotealogicalconstraintonsomeofthe\nvariables;theruleonlyholdswhentheconstraintistrue. HerethepredicateCompatible(j,n)\nismeanttotestwhetheradjectivejandnounnarecompatible;itwouldbedefinedbyaseries\nofassertions such asCompatible(black,dog). Wecanconvert thisgrammarruleintoadef-\niniteclause by(1)reversing theorderofright-andleft-hand sides, (2)makingaconjunction\nofalltheconstituentsandconstraints,(3)addingavariables tothelistofargumentsforeach\ni\nconstituent torepresent thesequence ofwords spanned bytheconstituent, (4)adding aterm\nfor the concatenation of words, Append(s ,...), to the list of arguments for the root of the\n1\ntree. Thatgivesus\nArticle(a,s )\u2227Adjs(j,s )\u2227Noun(n,s )\u2227Compatible(j,n)\n1 2 3\n\u21d2 NP(n,Append(s ,s ,s )).\n1 2 3\nThisdefiniteclausesaysthatifthepredicate Article istrueofaheadwordaandastring s ,\n1\nandAdjs issimilarly trueofahead word j andastring s ,andNoun istrueofaheadword\n2\nn and a string s , and if j and n are compatible, then the predicate NP is true of the head\n3\nwordnandtheresultofappending strings s ,s ,ands .\n1 2 3\nTheDCGtranslationleftouttheprobabilities, butwecould putthembackin: justaug-\nmenteachconstituent withonemorevariable representing theprobability oftheconstituent,\nandaugment therootwithavariable thatistheproduct ofthe constituent probabilities times\ntheruleprobability.\nThe translation from grammar rule to definite clause allows us to talk about parsing\nas logical inference. This makes it possible to reason about languages and strings in many\ndifferentways. Forexample,itmeanswecandobottom-upparsingusingforwardchainingor\ntop-downparsingusingbackwardchaining. Infact,parsing naturallanguagewithDCGswas Section23.3. AugmentedGrammarsandSemanticInterpretation 899\noneofthefirstapplications of(andmotivationsfor)theProloglogicprogramminglanguage.\nLANGUAGE Itissometimespossible toruntheprocessbackwardanddo languagegeneration aswellas\nGENERATION\nparsing. Forexample, skipping ahead to Figure 23.10 (page 903), a logic program could be\ngiventhesemanticformLoves(John,Mary)andapplythedefinite-clause rulestodeduce\nS(Loves(John,Mary),[John,loves,Mary]).\nThisworksfortoyexamples,butseriouslanguage-generationsystemsneedmorecontrolover\ntheprocessthanisaffordedbytheDCGrulesalone.\nE : S \u2192 NP VP | ...\n1 S\nNP \u2192 Pronoun | Name | Noun | ...\nS S\nNP \u2192 Pronoun | Name | Noun | ...\nO O\nVP \u2192 VP NP | ...\nO\nPP \u2192 Prep NP\nO\nPronoun \u2192 I| you|he| she|it | ...\nS\nPronoun \u2192 me| you|him| her|it | ...\nO\n...\nE : S(head) \u2192 NP(Sbj,pn,h) VP(pn,head) | ...\n2\nNP(c,pn,head) \u2192 Pronoun(c,pn,head)| Noun(c,pn,head) | ...\nVP(pn,head) \u2192 VP(pn,head) NP(Obj,p,h) | ...\nPP(head) \u2192 Prep(head) NP(Obj,pn,h)\nPronoun(Sbj,1S,I) \u2192 I\nPronoun(Sbj,1P,we) \u2192 we\nPronoun(Obj,1S,me) \u2192 me\nPronoun(Obj,3P,them) \u2192 them\n...\nFigure 23.7 Top: part of a grammarfor the language E , which handles subjective and\n1\nobjective cases in noun phrases and thus does not overgeneratequite as badly as E . The\n0\nportionsthatareidenticaltoE havebeenomitted. Bottom: partofanaugmentedgrammar\n0\nforE , withthreeaugmentations: caseagreement,subject\u2013verbagreement,andheadword.\n2\nSbj,Obj,1S,1Pand3Pareconstants,andlowercasenamesarevariables.\n23.3.3 Caseagreement andsubject\u2013verb agreement\nWe saw in Section 23.1 that the simple grammar for E overgenerates, producing nonsen-\n0\ntencessuchas\u201cMesmellastench.\u201d Toavoidthisproblem,ourgrammarwouldhavetoknow\nthat\u201cme\u201disnotavalidNP whenitisthesubjectofasentence. Linguistssaythatthepronoun\n\u201cI\u201d is in the subjective case, and \u201cme\u201d is in the objective case.4 We can account for this by\n4 Thesubjectivecaseisalsosometimescalledthenominativecaseandtheobjectivecaseissometimescalled\ntheaccusativecase.Manylanguagesalsohaveadativecaseforwordsintheindirectobjectposition. 900 Chapter 23. NaturalLanguageforCommunication\nsplitting NP intotwocategories, NP andNP ,tostand fornounphrases inthesubjective\nS O\nand objective case, respectively. Wewould also need to split the category Pronoun into the\ntwo categories Pronoun (which includes \u201cI\u201d) and Pronoun (which includes \u201cme\u201d). The\nS O\ntoppartofFigure23.7showsthegrammarforcaseagreement;wecalltheresultinglanguage\nCASEAGREEMENT\nE . NoticethatalltheNP rulesmustbeduplicated, onceforNP andonceforNP .\n1 S O\nSUBJECT\u2013VERB Unfortunately, E still overgenerates. English requires subject\u2013verb agreement for\nAGREEMENT 1\nperson and number of the subject and main verb of a sentence. For example, if \u201cI\u201d is the\nsubject, then \u201cI smell\u201d isgrammatical, but \u201cI smells\u201d isnot. If\u201cit\u201d isthe subject, weget the\nreverse. In English, the agreement distinctions are minimal: most verbs have one form for\nthird-person singular subjects (he, she, or it), and a second form for all other combinations\nofperson and number. Thereisoneexception: the verb\u201ctobe\u201d hasthree forms, \u201cI am\/you\nare \/ he is.\u201d So one distinction (case) splits NP two ways, another distinction (person and\nnumber)splitsNP threeways,andasweuncoverotherdistinctionswewouldendupwithan\nexponential numberofsubscripted NP formsifwetooktheapproach ofE . Augmentations\n1\nareabetterapproach: theycanrepresent anexponential numberofformsasasinglerule.\nIn the bottom of Figure 23.7 wesee (part of) an augmented grammarfor the language\nE , which handles case agreement, subject\u2013verb agreement, and head words. We have just\n2\none NP category, but NP(c,pn,head) has three augmentations: c is a parameter for case,\npn is a parameter for person and number, and head is a parameter for the head word of\nthe phrase. The other categories also are augmented with heads and other arguments. Let\u2019s\nconsideroneruleindetail:\nS(head) \u2192 NP(Sbj,pn,h) VP(pn,head).\nThisruleiseasiesttounderstand right-to-left: whenan NPandaVPareconjoined theyform\nan S, but only if the NPhas the subjective (Sbj) case and the person and number (pn) of the\nNP and VP are identical. If that holds, then we have an S whose head is the same as the\nheadoftheVP.NotetheheadoftheNP,denoted bythedummyvariable h,isnotpartofthe\naugmentationoftheS.ThelexicalrulesforE fillinthevaluesoftheparametersandarealso\n2\nbestreadright-to-left. Forexample,therule\nPronoun(Sbj,1S,I) \u2192 I\nsaysthat\u201cI\u201dcanbeinterpretedasaPronouninthesubjectivecase,first-personsingular,with\nhead \u201cI.\u201d For simplicity we have omitted the probabilities for these rules, but augmentation\ndoes work with probabilities. Augmentation can also work with automated learning mecha-\nnisms. Petrov and Klein (2007c) show how a learning algorithm can automatically split the\nNP category intoNP andNP .\nS O\n23.3.4 Semantic interpretation\nTo show how to add semantics to a grammar, we start with an example that is simpler than\nEnglish: thesemanticsofarithmeticexpressions. Figure23.8showsagrammarforarithmetic\nexpressions, whereeachruleisaugmentedwithavariableindicatingthesemanticinterpreta-\ntionofthephrase. Thesemanticsofadigitsuchas\u201c3\u201disthedigititself. Thesemanticsofan\nexpression suchas\u201c3+4\u201distheoperator\u201c+\u201dapplied tothesemantics ofthephrase\u201c3\u201dand Section23.3. AugmentedGrammarsandSemanticInterpretation 901\nExp(x) \u2192 Exp(x )Operator(op)Exp(x ) {x=Apply(op,x ,x )}\n1 2 1 2\nExp(x) \u2192 (Exp(x) )\nExp(x) \u2192 Number(x)\nNumber(x) \u2192 Digit(x)\nNumber(x) \u2192 Number(x ) Digit(x ){x=10\u00d7x +x }\n1 2 1 2\nDigit(x) \u2192 x {0 \u2264 x \u2264 9}\nOperator(x) \u2192 x{x\u2208{+,\u2212,\u00f7,\u00d7}}\nFigure23.8 Agrammarforarithmeticexpressions,augmentedwithsemantics.Eachvari-\nablexirepresentsthesemanticsofaconstituent.Notetheuseofthe{test}notationtodefine\nlogicalpredicatesthatmustbesatisfied,butthatarenotconstituents.\nExp(5)\nExp(2)\nExp(2)\nExp(3) Exp(4) Exp(2)\nNumber(3) Number(4) Number(2)\nDigit(3) Operator(+) Digit(4) Operator(\u00f7) Digit(2)\n3 + ( 4 \u00f7 2 )\nFigure23.9 Parsetreewithsemanticinterpretationsforthestring\u201c3+(4\u00f72)\u201d.\nCOMPOSITIONAL the phrase \u201c4.\u201d The rules obey the principle of compositional semantics\u2014the semantics of\nSEMANTICS\naphraseisafunctionofthesemanticsofthesubphrases. Figure23.9showstheparsetreefor\n3+(4\u00f72) according to this grammar. The root of the parse tree is Exp(5), an expression\nwhosesemanticinterpretation is5.\nNowlet\u2019smoveontothesemantics ofEnglish, oratleast ofE . Westart bydetermin-\n0\ningwhatsemanticrepresentations wewanttoassociatewithwhatphrases. Weusethesimple\nexamplesentence\u201cJohnlovesMary.\u201d TheNP \u201cJohn\u201dshouldhaveasitssemanticinterpreta-\ntion the logical term John, and the sentence as a whole should have as its interpretation the\nlogical sentence Loves(John,Mary). That much seems clear. The complicated part is the\nVP \u201cloves Mary.\u201d The semantic interpretation of this phrase is neither a logical term nor a\ncomplete logical sentence. Intuitively, \u201cloves Mary\u201d is adescription that might ormight not 902 Chapter 23. NaturalLanguageforCommunication\napply to aparticular person. (In this case, it applies to John.) Thismeans that \u201cloves Mary\u201d\nis a predicate that, when combined with a term that represents a person (the person doing\ntheloving), yields acomplete logical sentence. Using the \u03bb-notation (see page 294), wecan\nrepresent \u201clovesMary\u201dasthepredicate\n\u03bbx Loves(x,Mary).\nNowweneed arule thatsays \u201can NP withsemantics obj followed by aVP withsemantics\npred yieldsasentence whosesemanticsistheresultofapplying pred toobj:\u201d\nS(pred(obj)) \u2192 NP(obj) VP(pred).\nTheruletellsusthatthesemanticinterpretation of\u201cJohnlovesMary\u201dis\n(\u03bbx Loves(x,Mary))(John),\nwhichisequivalent toLoves(John,Mary).\nThe rest of the semantics follows in a straightforward way from the choices we have\nmadesofar. BecauseVPsarerepresentedaspredicates,itisagoodideatobeconsistentand\nrepresent verbsaspredicates aswell. Theverb\u201cloves\u201disrepresented as \u03bby \u03bbx Loves(x,y),\nthepredicatethat,whengiventheargumentMary,returnsthepredicate\u03bbx Loves(x,Mary).\nWeendupwiththegrammarshowninFigure23.10andtheparsetreeshowninFigure23.11.\nWe could just as easily have added semantics to E ; we chose to work with E so that the\n2 0\nreadercanfocusononetypeofaugmentation atatime.\nAdding semantic augmentations to a grammar by hand is laborious and error prone.\nTherefore, there have been several projects to learn semantic augmentations from examples.\nCHILL (Zelle and Mooney, 1996) is an inductive logic programming (ILP) program that\nlearnsagrammarandaspecializedparserforthatgrammarfromexamples. Thetargetdomain\nis natural language database queries. The training examples consist of pairs of word strings\nandcorresponding semanticforms\u2014forexample;\nWhatisthecapitalofthestatewiththelargest population?\nAnswer(c,Capital(s,c)\u2227Largest(p,State(s)\u2227Population(s,p)))\nCHILL\u2019staskistolearnapredicate Parse(words,semantics)thatisconsistent withtheex-\namples and, hopefully, generalizes well to other examples. Applying ILP directly to learn\nthis predicate results inpoor performance: the induced parserhas only about 20% accuracy.\nFortunately, ILPlearners canimprovebyadding knowledge. Inthiscase, mostoftheParse\npredicate was defined as a logic program, and CHILL\u2019s task was reduced to inducing the\ncontrolrulesthatguidetheparsertoselectoneparseoveranother. Withthisadditional back-\nground knowledge, CHILL can learn to achieve 70% to 85% accuracy on various database\nquerytasks.\n23.3.5 Complications\nThegrammarofrealEnglishisendlessly complex. Wewillbrieflymentionsomeexamples.\nTime and tense: Suppose we want to represent the difference between \u201cJohn loves\nTIMEANDTENSE\nMary\u201dand\u201cJohnlovedMary.\u201d Englishusesverbtenses(past, present,andfuture)toindicate Section23.3. AugmentedGrammarsandSemanticInterpretation 903\nS(pred(obj)) \u2192 NP(obj) VP(pred)\nVP(pred(obj)) \u2192 Verb(pred)NP(obj)\nNP(obj) \u2192 Name(obj)\nName(John) \u2192 John\nName(Mary) \u2192 Mary\nVerb(\u03bby \u03bbx Loves(x,y)) \u2192 loves\nFigure23.10 Agrammarthatcanderiveaparsetreeandsemanticinterpretationfor\u201cJohn\nlovesMary\u201d(andthreeothersentences).Eachcategoryisaugmentedwithasingleargument\nrepresentingthesemantics.\nS(Loves(John,Mary))\nVP(\u03bbx Loves(x,Mary))\nNP(John)\nNP(Mary)\nName(John) Verb(\u03bby \u03bbx Loves(x,y)) Name(Mary)\nJohn loves Mary\nFigure23.11 Aparsetreewithsemanticinterpretationsforthestring\u201cJohnlovesMary\u201d.\nthe relative time of an event. One good choice to represent the time of events is the event\ncalculus notationofSection12.3. Ineventcalculuswehave\nJohnlovesmary:E \u2208Loves(John,Mary)\u2227During(Now,Extent(E ))\n1 1\nJohnlovedmary:E \u2208Loves(John,Mary)\u2227After(Now,Extent(E )).\n2 2\nThissuggests thatourtwolexicalrulesforthewords\u201cloves\u201dand\u201cloved\u201dshould bethese:\nVerb(\u03bby \u03bbx e\u2208Loves(x,y)\u2227During(Now,e)) \u2192 loves\nVerb(\u03bby \u03bbx e\u2208Loves(x,y)\u2227After(Now,e)) \u2192 loved.\nOther than this change, everything else about the grammar remains the same, which is en-\ncouraging news; it suggests we are on the right track if wecan so easily add a complication\nlike the tense of verbs (although we have just scratched the surface of a complete grammar\nfortimeandtense). Itisalsoencouraging thatthedistinction betweenprocesses anddiscrete\neventsthatwemadeinourdiscussion ofknowledge representation inSection12.3.1isactu-\nally reflected in language use. We can say \u201cJohn slept a lot last night,\u201d where Sleeping is a\nprocess category, but it isodd to say \u201cJohn found aunicorn a lot last night,\u201d where Finding\nis a discrete event category. A grammar would reflect that fact by having a low probability\nforaddingtheadverbialphrase\u201calot\u201dtodiscrete events.\nQuantification: Consider the sentence \u201cEvery agent feels abreeze.\u201d Thesentence has\nQUANTIFICATION\nonly one syntactic parse under E , but it is actually semantically ambiguous; the preferred\n0 904 Chapter 23. NaturalLanguageforCommunication\nmeaning is \u201cFor every agent there exists a breeze that the agent feels,\u201d but an acceptable\nalternativemeaningis\u201cThereexistsabreezethateveryagentfeels.\u201d5 Thetwointerpretations\ncanberepresented as\n\u2200a a\u2208Agents \u21d2\n\u2203b b\u2208Breezes \u2227\u2203e e\u2208Feel(a,b)\u2227During(Now,e);\n\u2203b b\u2208Breezes \u2200a a\u2208Agents \u21d2\n\u2203e e\u2208Feel(a,b)\u2227During(Now,e).\nThe standard approach to quantification is for the grammar to define not an actual logical\nQUASI-LOGICAL semantic sentence, butrathera quasi-logical form thatisthenturned into alogical sentence\nFORM\nbyalgorithmsoutside oftheparsing process. Thosealgorithms canhavepreference rulesfor\npreferring one quantifier scope overanother\u2014preferences that need not be reflected directly\ninthegrammar.\nPragmatics: We have shown how an agent can perceive a string of words and use a\nPRAGMATICS\ngrammar to derive a set of possible semantic interpretations. Now we address the problem\nof completing the interpretation by adding context-dependent information about the current\nsituation. The most obvious need for pragmatic information is in resolving the meaning of\nindexicals, which arephrases that referdirectly tothe current situation. Forexample, in the\nINDEXICAL\nsentence\u201cIaminBostontoday,\u201dboth\u201cI\u201dand\u201ctoday\u201dareindexicals. Theword\u201cI\u201dwouldbe\nrepresented bythefluent Speaker,anditwouldbeuptothehearertoresolvethemeaningof\nthe fluent\u2014that is not considered part of the grammar but rather an issue of pragmatics; of\nusingthecontextofthecurrentsituation tointerpret fluents.\nAnother part of pragmatics is interpreting the speaker\u2019s intent. The speaker\u2019s action is\nconsidered a speech act, and it is up to the hearer to decipher what type of action it is\u2014a\nSPEECHACT\nquestion, a statement, a promise, a warning, a command, and so on. A command such as\n\u201cgo to22\u201dimplicitly refers tothe hearer. Sofar, ourgrammarfor S covers only declarative\nsentences. We can easily extend it to cover commands. A command can be formed from\na VP, where the subject is implicitly the hearer. We need to distinguish commands from\nstatements, sowealtertherulesfor S toincludethetypeofspeechact:\nS(Statement(Speaker,pred(obj))) \u2192 NP(obj)VP(pred)\nS(Command(Speaker,pred(Hearer))) \u2192 VP(pred).\nLONG-DISTANCE Long-distance dependencies: Questions introduce anew grammatical complexity. In\nDEPENDENCIES\n\u201cWho did the agent tell you to give the gold to?\u201d the final word \u201cto\u201d should be parsed as\n[PP to ], where the \u201c \u201d denotes a gap or trace where an NP is missing; the missing NP\nTRACE\nis licensed by the first word of the sentence, \u201cwho.\u201d A complex system of augmentations is\nused to make sure that the missing NPs match up with the licensing words in just the right\nway,andprohibitgapsinthewrongplaces. Forexample,youcan\u2019thaveagapinonebranch\nof an NP conjunction: \u201cWhat did he play [NP Dungeons and ]?\u201d is ungrammatical. But\nyoucanhave thesamegapinboth branches ofaVP conjunction: \u201cWhatdidyou[VP [VP\nsmell ]and[VP shootanarrowat ]]?\u201d\nAmbiguity: Insomecases,hearersareconsciously awareofambiguityinanutterance.\nAMBIGUITY\nHerearesomeexamplestakenfromnewspaperheadlines:\n5 Ifthisinterpretationseemsunlikely,consider\u201cEveryProtestantbelievesinajustGod.\u201d Section23.3. AugmentedGrammarsandSemanticInterpretation 905\nSquadhelpsdogbitevictim.\nPolicebegincampaigntorundownjaywalkers.\nHelicopterpoweredbyhumanflies.\nOnce-sagging clothdiaperindustrysavedbyfulldumps.\nPortabletoiletbombed;policehavenothingtogoon.\nTeacherstrikesidlekids.\nIncludeyourchildren whenbakingcookies.\nHospitalsaresuedby7footdoctors.\nMilkdrinkers areturningtopowder.\nSafetyexpertssayschoolbuspassengers shouldbebelted.\nButmostofthetimethelanguage wehearseemsunambiguous. Thus,whenresearchers first\nbegan to use computers to analyze language in the 1960s, they were quite surprised to learn\nthat almost every utterance is highly ambiguous, even though the alternative interpretations\nmightnotbeapparenttoanativespeaker. Asystemwithalargegrammarandlexiconmight\nfind thousands of interpretations for a perfectly ordinary sentence. Lexical ambiguity, in\nLEXICALAMBIGUITY\nwhich a word has more than one meaning, is quite common; \u201cback\u201d can be an adverb (go\nback), anadjective (backdoor), anoun(thebackoftheroom) oraverb(back upyourfiles).\n\u201cJack\u201dcanbeaname,anoun(aplayingcard,asix-pointedmetalgamepiece,anauticalflag,\nafish,asocket,oradeviceforraisingheavyobjects), oraverb(tojackupacar,tohuntwith\nSYNTACTIC a light, or to hit a baseball hard). Syntactic ambiguity refers to a phrase that has multiple\nAMBIGUITY\nparses: \u201cI smelled awumpus in 2,2\u201d has twoparses: one where the prepositional phrase \u201cin\n2,2\u201dmodifiesthenounandonewhereitmodifiestheverb. Thesyntacticambiguityleadstoa\nSEMANTIC semanticambiguity,becauseoneparsemeansthatthewumpusisin2,2andtheothermeans\nAMBIGUITY\nthatastenchisin2,2. Inthiscase,gettingthewronginterpretation couldbeadeadlymistake\nfortheagent.\nFinally, there can be ambiguity between literal and figurative meanings. Figures of\nspeech are important in poetry, but are surprisingly common in everyday speech as well. A\nmetonymy is a figure of speech in which one object is used to stand for another. When\nMETONYMY\nwe hear \u201cChrysler announced a new model,\u201d we do not interpret it as saying that compa-\nnies can talk; rather we understand that a spokesperson representing the company made the\nannouncement. Metonymyiscommonandisofteninterpretedunconsciouslybyhumanhear-\ners. Unfortunately, our grammar as it is written is not so facile. To handle the semantics of\nmetonymyproperly,weneedtointroduceawholenewlevelofambiguity. Wedothisbypro-\nvidingtwoobjectsforthesemanticinterpretation ofeveryphraseinthesentence: oneforthe\nobject that the phrase literally refers to (Chrysler) and one for the metonymic reference (the\nspokesperson). We then have to say that there is a relation between the two. In our current\ngrammar,\u201cChryslerannounced\u201d getsinterpreted as\nx = Chrysler \u2227e\u2208 Announce(x)\u2227After(Now,Extent(e)).\nWeneedtochange thatto\nx = Chrysler \u2227e\u2208 Announce(m)\u2227After(Now,Extent(e))\n\u2227Metonymy(m,x). 906 Chapter 23. NaturalLanguageforCommunication\nThis says that there is one entity x that is equal to Chrysler, and another entity m that did\ntheannouncing, andthatthetwoareinametonymyrelation. Thenextstep istodefinewhat\nkinds of metonymy relations can occur. The simplest case is when there is no metonymy at\nall\u2014theliteralobject xandthemetonymicobjectmareidentical:\n\u2200m,x (m = x) \u21d2 Metonymy(m,x).\nForthe Chrysler example, a reasonable generalization is that an organization can be used to\nstandforaspokesperson ofthatorganization:\n\u2200m,x x\u2208Organizations \u2227Spokesperson(m,x) \u21d2 Metonymy(m,x).\nOther metonymies include the author for the works (I read Shakespeare) or more generally\ntheproducer fortheproduct (Idrivea Honda)andthepartforthewhole(TheRedSoxneed\na strong arm). Some examples of metonymy, such as \u201cThe ham sandwich on Table 4 wants\nanotherbeer,\u201daremorenovelandareinterpreted withrespecttoasituation.\nAmetaphor isanother figure ofspeech, inwhich a phrase withone literal meaning is\nMETAPHOR\nused to suggest a different meaning by way of an analogy. Thus, metaphor can be seen as a\nkindofmetonymywheretherelationisoneofsimilarity.\nDisambiguation is the process of recovering the most probable intended meaning of\nDISAMBIGUATION\nan utterance. In one sense wealready have a framework for solving this problem: each rule\nhas a probability associated with it, so the probability of an interpretation is the product of\nthe probabilities of the rules that led to the interpretation. Unfortunately, the probabilities\nreflect how common the phrases are in the corpus from which the grammar was learned,\nand thus reflect general knowledge, not specific knowledge of the current situation. To do\ndisambiguation properly, weneedtocombinefourmodels:\n1. Theworldmodel: thelikelihoodthatapropositionoccursintheworld. Givenwhatwe\nknow about the world, it is more likely that a speaker who says \u201cI\u2019m dead\u201d means \u201cI\naminbigtrouble\u201dratherthan\u201cMylifeended,andyetIcanstilltalk.\u201d\n2. Thementalmodel: thelikelihood thatthespeakerformstheintention ofcommunicat-\ning a certain fact to the hearer. This approach combines models of what the speaker\nbelieves, what the speaker believes the hearer believes, and so on. Forexample, when\na politician says, \u201cI am not a crook,\u201d the world model might assign a probability of\nonly 50% to the proposition that the politician is not a criminal, and 99.999% to the\nproposition thatheisnotahooked shepherd\u2019s staff. Nevertheless, weselecttheformer\ninterpretation because itisamorelikelythingtosay.\n3. Thelanguagemodel: thelikelihoodthatacertainstringofwordswillbechosen,given\nthatthespeakerhastheintention ofcommunicating acertainfact.\n4. The acoustic model: for spoken communication, the likelihood that a particular se-\nquenceofsounds willbegenerated, giventhatthespeaker haschosen agivenstringof\nwords. Section23.5coversspeechrecognition. Section23.4. MachineTranslation 907\n23.4 MACHINE TRANSLATION\nMachinetranslationistheautomatictranslationoftextfromonenaturallanguage(thesource)\nto another (the target). It was one of the first application areas envisioned for computers\n(Weaver, 1949), but it is only in the past decade that the technology has seen widespread\nusage. Hereisapassagefrompage1ofthisbook:\nAI is one of the newestfields in science and engineering. Work started in earnest soon\nafterWorldWarII,andthenameitselfwascoinedin1956. Alongwithmolecularbiol-\nogy, AI is regularlycited as the \u201cfield I wouldmost like to be in\u201d by scientists in other\ndisciplines.\nAndhereitistranslated fromEnglishtoDanishbyanonlinetool,GoogleTranslate:\nAIerenafdenyesteomra\u02daderindenforvidenskabogteknik. Arbejdestartedeforalvor\nlige efter Anden Verdenskrig, og navnet i sig selv var opfundet i 1956. Sammen med\nmolekyl\u00e6rbiologi, erAI j\u00e6vnligtn\u00e6vntsom \u201cfeltet Jeg ville de fleste gernev\u00e6re i\u201d af\nforskereiandrediscipliner.\nFor those who don\u2019t read Danish, here is the Danish translated back to English. The words\nthatcameoutdifferentareinitalics:\nAIisoneofthenewestfieldsofscienceandengineering.Workbeganinearnestjustafter\ntheSecondWorldWar,andthenameitselfwasinventedin1956.Togetherwithmolecular\nbiology,AIisfrequentlymentionedas \u201cfieldIwouldmostliketobein\u201dbyresearchers\ninotherdisciplines.\nThe differences are all reasonable paraphrases, such as frequently mentioned for regularly\ncited. Theonly real erroristhe omission ofthe article the, denoted bythe symbol. Thisis\ntypical accuracy: of the two sentences, one has an error that would not be made by a native\nspeaker, yetthemeaningisclearlyconveyed.\nHistorically, there have been three main applications of machine translation. Rough\ntranslation, as provided by free online services, gives the \u201cgist\u201d of a foreign sentence or\ndocument, but contains errors. Pre-edited translation is used by companies to publish their\ndocumentation and sales materials inmultiple languages. The original source text iswritten\nin a constrained language that is easier to translate automatically, and the results are usually\nedited by ahuman tocorrect anyerrors. Restricted-source translation worksfully automati-\ncally,butonlyonhighlystereotypical language, suchasaweatherreport.\nTranslationisdifficultbecause,inthefullygeneralcase,itrequiresin-depthunderstand-\ning of the text. This is true even for very simple texts\u2014even \u201ctexts\u201d of one word. Consider\ntheword\u201cOpen\u201donthedoorofastore.6 Itcommunicates theideathatthestoreisaccepting\ncustomers at the moment. Now consider the same word \u201cOpen\u201d on a large banner outside a\nnewlyconstructed store. Itmeansthat the store isnowindaily operation, but readers ofthis\nsignwouldnotfeelmisledifthestoreclosed atnight withoutremoving thebanner. Thetwo\nsigns use the identical word to convey different meanings. In German the sign on the door\nwouldbe\u201cOffen\u201dwhilethebannerwouldread\u201cNeuEro\u00a8ffnet.\u201d\n6 ThisexampleisduetoMartinKay. 908 Chapter 23. NaturalLanguageforCommunication\nThe problem is that different languages categorize the world differently. Forexample,\nthe French word \u201cdoux\u201d covers a wide range of meanings corresponding approximately to\nthe English words \u201csoft,\u201d \u201csweet,\u201d and \u201cgentle.\u201d Similarly, the English word \u201chard\u201d covers\nvirtually all uses of the German word \u201chart\u201d (physically recalcitrant, cruel) and some uses\nof the word \u201cschwierig\u201d (difficult). Therefore, representing the meaning of a sentence is\nmoredifficultfortranslationthanitisforsingle-language understanding. AnEnglishparsing\nsystem could use predicates like Open(x), but for translation, the representation language\nwouldhavetomakemoredistinctions,perhapswithOpen (x)representingthe\u201cOffen\u201dsense\n1\nandOpen (x)representing the\u201cNeuEro\u00a8ffnet\u201d sense. Arepresentation language that makes\n2\nallthedistinctions necessary forasetoflanguages iscalledaninterlingua.\nINTERLINGUA\nA translator (human or machine) often needs to understand the actual situation de-\nscribed in the source, not just the individual words. For example, to translate the English\nword \u201chim,\u201d into Korean, a choice must be made between the humble and honorific form, a\nchoicethatdepends onthesocialrelationship betweenthespeakerandthereferent of\u201chim.\u201d\nInJapanese, the honorifics are relative, so thechoice depends onthe social relationships be-\ntweenthespeaker,thereferent,andthelistener. Translators(bothmachineandhuman)some-\ntimesfinditdifficult to makethis choice. Asanother example, totranslate \u201cThe baseball hit\nthe window. It broke.\u201d into French, we must choose the feminine \u201celle\u201d or the masculine\n\u201cil\u201d for\u201cit,\u201d so wemust decide whether \u201cit\u201d refers tothe baseball orthe window. Toget the\ntranslation right,onemustunderstand physicsaswellaslanguage.\nSometimes there is no choice that can yield a completely satisfactory translation. For\nexample, an Italian love poem that uses the masculine \u201cilsole\u201d (sun) and feminine \u201claluna\u201d\n(moon) to symbolize two lovers will necessarily be altered when translated into German,\nwherethegendersarereversed,andfurtheralteredwhentranslatedintoalanguagewherethe\ngendersarethesame.7\n23.4.1 Machinetranslationsystems\nAll translation systems must model the source and target languages, but systems vary in the\ntypeofmodelstheyuse. Somesystemsattempttoanalyzethesourcelanguagetextalltheway\ninto an interlingua knowledge representation and then generate sentences in the target lan-\nguagefromthatrepresentation. Thisisdifficultbecause it involves threeunsolvedproblems:\ncreatingacompleteknowledgerepresentation ofeverything; parsingintothatrepresentation;\nandgenerating sentences fromthatrepresentation.\nOthersystemsarebasedonatransfermodel. Theykeepadatabaseoftranslationrules\nTRANSFERMODEL\n(orexamples), and wheneverthe rule (orexample) matches, they translate directly. Transfer\ncan occur at the lexical, syntactic, or semantic level. For example, a strictly syntactic rule\nmaps English [Adjective Noun] to French [Noun Adjective]. A mixed syntactic and lexical\nrulemapsFrench[S \u201cetpuis\u201dS ]toEnglish[S \u201candthen\u201dS ]. Figure23.12diagramsthe\n1 2 1 2\nvarioustransferpoints.\n7 WarrenWeaver(1949)reportsthatMaxZeldnerpointsoutthatthegreatHebrewpoetH.N.Bialikoncesaid\nthattranslation\u201cislikekissingthebridethroughaveil.\u201d Section23.4. MachineTranslation 909\nInterlingua Semantics\nAttraction(NamedJohn, NamedMary, High)\nEnglish Semantics French Semantics\nLoves(John, Mary) Aime(Jean, Marie)\nEnglish Syntax French Syntax\nS(NP(John), VP(loves, NP(Mary))) S(NP(Jean), VP(aime, NP(Marie)))\nEnglish Words French Words\nJohn loves Mary Jean aime Marie\nFigure 23.12 The Vauquois triangle: schematic diagram of the choices for a machine\ntranslationsystem (Vauquois,1968). We startwith English textatthe top. An interlingua-\nbased system follows the solid lines, parsing English first into a syntactic form, then into\na semanticrepresentationandaninterlinguarepresentation,andthenthroughgenerationto\na semantic, syntactic, andlexicalformin French. A transfer-basedsystem usesthe dashed\nlinesasa shortcut. Differentsystemsmakethetransferatdifferentpoints; somemakeitat\nmultiplepoints.\n23.4.2 Statisticalmachine translation\nNow that we have seen how complex the translation task can be, it should come as no sur-\nprisethatthemostsuccessfulmachinetranslationsystemsarebuiltbytrainingaprobabilistic\nmodel using statistics gathered from a large corpus of text. This approach does not need\na complex ontology of interlingua concepts, nor does it need handcrafted grammars of the\nsourceandtargetlanguages, norahand-labeled treebank. Allitneedsisdata\u2014sampletrans-\nlationsfromwhichatranslationmodelcanbelearned. Totranslateasentencein,say,English\n\u2217\n(e)intoFrench(f),wefindthestringofwords f thatmaximizes\nf\u2217 = argmaxP(f |e) = argmaxP(e|f)P(f).\nf\nHere the factor P(f)is the target language model forFrench; it says how probable a given\nLANGUAGEMODEL\nTRANSLATION sentence is in French. P(e|f) is the translation model; it says how probable an English\nMODEL\nsentence is as a translation for a given French sentence. Similarly, P(f |e) is a translation\nmodelfromEnglishtoFrench.\nShouldweworkdirectly on P(f|e),orapplyBayes\u2019ruleandworkon P(e|f)P(f)?\nIn diagnostic applications like medicine, it is easier to model the domain in the causal di-\nrection: P(symptoms|disease)ratherthan P(disease|symptoms). Butintranslation both\ndirections are equally easy. The earliest work in statistical machine translation did apply\nBayes\u2019 rule\u2014in part because the researchers had agood language model, P(f), andwanted\nto make use of it, and in part because they came from a background in speech recognition,\nwhich is a diagnostic problem. We follow their lead in this chapter, but we note that re-\ncent work in statistical machine translation often optimizes P(f |e) directly, using a more\nsophisticated modelthattakesintoaccount manyofthefeatures fromthelanguage model. 910 Chapter 23. NaturalLanguageforCommunication\nThe language model, P(f), could address any level(s) on the right-hand side of Fig-\nure 23.12, but the easiest and most common approach is to build an n-gram model from a\nFrench corpus, as we have seen before. This captures only a partial, local idea of French\nsentences; however,thatisoftensufficientforroughtranslation.8\nThetranslationmodelislearnedfromabilingualcorpus\u2014acollectionofparalleltexts,\nBILINGUALCORPUS\neach an English\/French pair. Now, if we had an infinitely large corpus, then translating a\nsentence wouldjustbealookuptask: wewouldhaveseentheEnglishsentence beforeinthe\ncorpus, so we could just return the paired French sentence. But of course our resources are\nfinite, and most of the sentences we will be asked to translate will be novel. However, they\nwill be composed of phrases that we have seen before (even if some phrases are as short as\none word). For example, in this book, common phrases include \u201cin this exercise we will,\u201d\n\u201csizeofthestatespace,\u201d\u201casafunction ofthe\u201dand\u201cnotesattheendofthechapter.\u201d Ifasked\ntotranslatethenovelsentence\u201cInthisexercisewewillcomputethesizeofthestatespaceasa\nfunction ofthenumberofactions.\u201d intoFrench,weshouldbeabletobreakthesentenceinto\nphrases, find the phrases in the English corpus (this book), find the corresponding French\nphrases (from the French translation of the book), and then reassemble the French phrases\nintoanorderthatmakessenseinFrench. Inotherwords,givenasourceEnglishsentence, e,\nfindingaFrenchtranslation f isamatterofthreesteps:\n1. BreaktheEnglishsentenceintophrases e ,...,e .\n1 n\n2. For each phrase e , choose a corresponding French phrase f . We use the notation\ni i\nP(f |e )forthephrasal probability that f isatranslation ofe .\ni i i i\n3. Choose a permutation of the phrases f ,...,f . Wewill specify this permutation in a\n1 n\nway that seems a little complicated, but is designed to have a simple probability dis-\ntribution: For each f , we choose a distortion d , which is the number of words that\nDISTORTION i i\nphrase f\ni\nhasmovedwithrespectto f i\u22121;positiveformovingtotheright, negativefor\nmovingtotheleft,andzeroiff\ni\nimmediatelyfollowsf i\u22121.\nFigure 23.13 shows an example of the process. At the top, the sentence \u201cThere is a smelly\nwumpus sleeping in 2 2\u201d is broken into five phrases, e ,...,e . Each of them is translated\n1 5\ninto a corresponding phrase f , and then these are permuted into the order f ,f ,f ,f ,f .\ni 1 3 4 2 5\nWespecifythepermutation intermsofthedistortions d ofeachFrenchphrase, definedas\ni\nd\ni\n= START(f i)\u2212END(f i\u22121)\u22121,\nwhere START(f i)istheordinal numberofthefirstwordofphrase f\ni\nintheFrenchsentence,\nandEND(f i\u22121)istheordinalnumberofthelastwordofphrase f i\u22121. InFigure23.13wesee\nthatf ,\u201ca` 22,\u201dimmediatelyfollowsf ,\u201cquidort,\u201dandthusd =0. Phrasef ,however,has\n5 4 5 2\nmovedonewords totheright of f ,sod =1. Asaspecial case wehave d =0, because f\n1 2 1 1\nstartsatposition 1and END(f 0)isdefinedtobe0(eventhough f\n0\ndoesnotexist).\nNow that we have defined the distortion, d , we can define the probability distribution\ni\nfor distortion, P(d ). Note that for sentences bounded by length n we have |d | \u2264 n , and\ni i\n8 For the finer points of translation, n-grams are clearly not enough. Marcel Proust\u2019s 4000-page novel A la\nre\u00b4cherchedutempsperdubeginsandendswiththesameword(longtemps),sosometranslatorshavedecidedto\ndothesame,thusbasingthetranslationofthefinalwordononethatappearedroughly2millionwordsearlier. Section23.4. MachineTranslation 911\nso the full probability distribution P(d ) has only 2n + 1 elements, far fewer numbers to\ni\nlearn than the number of permutations, n!. That is why we defined the permutation in this\ncircuitous way. Of course, this is a rather impoverished model of distortion. It doesn\u2019t say\nthat adjectives are usually distorted to appear after the noun when we are translating from\nEnglishtoFrench\u2014that factisrepresented intheFrenchlanguage model,P(f). Thedistor-\ntion probability is completely independent of the words in the phrases\u2014it depends only on\nthe integer value d . Theprobability distribution provides a summary of the volatility of the\ni\npermutations; howlikelyadistortion ofP(d=2)is,comparedtoP(d=0),forexample.\nWe\u2019re ready now to put it all together: we can define P(f,d|e), the probability that\nthesequenceofphrases f withdistortions disatranslation ofthesequenceofphrases e. We\nmake the assumption that each phrase translation and each distortion is independent of the\nothers, andthuswecanfactortheexpression as\n(cid:25)\nP(f,d|e) = P(f |e )P(d )\ni i i\ni\ne e e e e\n1 2 3 4 5\nThere is a smelly wumpus sleeping in 2 2\nf f f f f\n1 3 2 4 5\nIl y a un wumpus malodorant qui dort \u00e0 2 2\nd = 0 d = -2 d = +1 d = +1 d = 0\n1 3 2 4 5\nFigure23.13 CandidateFrenchphrasesforeachphraseofanEnglishsentence,withdis-\ntortion(d)valuesforeachFrenchphrase.\nThatgivesusawaytocomputetheprobability P(f,d|e)foracandidate translation f\nanddistortion d. Buttofindthebestf anddwecan\u2019t justenumerate sentences; withmaybe\n100 French phrases for each English phrase in the corpus, there are 1005 different 5-phrase\ntranslations, and5!reorderings foreachofthose. Wewillhavetosearchforagoodsolution.\nA local beam search (see page 125) with a heuristic that estimates probability has proven\neffectiveatfindinganearly-most-probable translation.\nAllthat remains is to learn the phrasal and distortion probabilities. Wesketch the pro-\ncedure;seethenotesattheendofthechapterfordetails.\n1. Findparalleltexts: First,gatheraparallelbilingual corpus. Forexample,a Hansard9\nHANSARD\nis a record of parliamentary debate. Canada, Hong Kong, and other countries pro-\nduce bilingual Hansards, the European Union publishes its official documents in 11\nlanguages, and the United Nations publishes multilingual documents. Bilingual text is\nalso available online; some Web sites publish parallel content with parallel URLs, for\n9 NamedafterWilliamHansard,whofirstpublishedtheBritishparliamentarydebatesin1811. 912 Chapter 23. NaturalLanguageforCommunication\nexample,\/en\/fortheEnglishpageand\/fr\/forthecorrespondingFrenchpage. The\nleadingstatistical translation systemstrainonhundreds ofmillionsofwordsofparallel\ntextandbillions ofwordsofmonolingual text.\n2. Segmentintosentences: Theunitoftranslation isasentence,sowewillhavetobreak\nthe corpus into sentences. Periods are strong indicators of the end of a sentence, but\nconsider \u201cDr. J. R. Smith of Rodeo Dr. paid $29.99 on 9.9.09.\u201d; only the final period\nends a sentence. One way to decide if a period ends a sentence is to train a model\nthat takes as features the surrounding words and their parts of speech. This approach\nachievesabout98%accuracy.\n3. Alignsentences: Foreachsentence intheEnglishversion,determine whatsentence(s)\nit corresponds to in the French version. Usually, the next sentence of English corre-\nsponds tothe nextsentence ofFrench ina1:1 match, but sometimes there isvariation:\nonesentenceinonelanguagewillbesplitintoa2:1match,ortheorderoftwosentences\nwillbeswapped,resultingina2:2match. Bylookingatthesentencelengthsalone(i.e.\nshortsentences shouldalignwithshortsentences), itispossibletoalignthem(1:1,1:2,\nor 2:2, etc.) with accuracy in the 90% to 99% range using a variation on the Viterbi\nalgorithm. Evenbetteralignmentcanbeachievedbyusinglandmarksthatarecommon\ntoboth languages, such asnumbers, dates, proper names, orwords that weknow from\nabilingualdictionaryhaveanunambiguoustranslation. Forexample,ifthe3rdEnglish\nand 4th French sentences contain the string \u201c1989\u201d and neighboring sentences do not,\nthatisgoodevidencethatthesentences shouldbealignedtogether.\n4. Alignphrases: Withinasentence,phrasescanbealignedbyaprocessthatissimilarto\nthat used for sentence alignment, but requiring iterative improvement. When we start,\nwehavenowayofknowingthat\u201cquidort\u201dalignswith\u201csleeping,\u201d butwecanarriveat\nthatalignmentbyaprocessofaggregationofevidence. Overalltheexamplesentences\nwe have seen, we notice that \u201cqui dort\u201d and \u201csleeping\u201d co-occur with high frequency,\nand that in the pair of aligned sentences, no phrase other than \u201cqui dort\u201d co-occurs so\nfrequently in other sentences with \u201csleeping.\u201d A complete phrase alignment over our\ncorpusgivesusthephrasalprobabilities (afterappropriate smoothing).\n5. Extract distortions: Once we have an alignment of phrases we can define distortion\nprobabilities. Simplycount howoften distortion occurs in thecorpus foreachdistance\nd= 0,\u00b11,\u00b12,...,andapplysmoothing.\n6. ImproveestimateswithEM:Useexpectation\u2013maximization toimprovetheestimates\nof P(f |e) and P(d) values. We compute the best alignments with the current values\noftheseparametersintheEstep,thenupdatetheestimatesintheMstepanditeratethe\nprocessuntilconvergence.\n23.5 SPEECH RECOGNITION\nSPEECH Speechrecognitionisthetaskofidentifyingasequenceofwordsutteredbyaspeaker, given\nRECOGNITION\nthe acoustic signal. It has become one of the mainstream applications of AI\u2014millions of Section23.5. SpeechRecognition 913\npeople interact with speech recognition systems every day to navigate voice mail systems,\nsearch the Web from mobile phones, and other applications. Speech is an attractive option\nwhenhands-free operation isnecessary, aswhenoperating machinery.\nSpeech recognition is difficult because the sounds made by a speaker are ambiguous\nand, well, noisy. As a well-known example, the phrase \u201crecognize speech\u201d sounds almost\nthe same as \u201cwreck a nice beach\u201d when spoken quickly. Even this short example shows\nseveral of the issues that make speech problematic. First, segmentation: written words in\nSEGMENTATION\nEnglish have spaces between them, but in fast speech there are no pauses in \u201cwreck a nice\u201d\nthat would distinguish it as a multiword phrase as opposed to the single word \u201crecognize.\u201d\nSecond, coarticulation: when speaking quickly the \u201cs\u201d sound at the end of \u201cnice\u201d merges\nCOARTICULATION\nwith the \u201cb\u201d sound at the beginning of \u201cbeach,\u201d yielding something that is close to a \u201csp.\u201d\nAnother problem that does not show up in this example is homophones\u2014words like \u201cto,\u201d\nHOMOPHONES\n\u201ctoo,\u201dand\u201ctwo\u201dthatsoundthesamebutdifferinmeaning.\nWecanviewspeech recognition asaproblem inmost-likely-sequence explanation. As\nwe saw in Section 15.2, this is the problem of computing the most likely sequence of state\nvariables, x , given a sequence of observations e . In this case the state variables are the\n1:t 1:t\nwords,andtheobservationsaresounds. Moreprecisely,anobservationisavectoroffeatures\nextractedfromtheaudiosignal. Asusual,themostlikelysequencecanbecomputedwiththe\nhelpofBayes\u2019ruletobe:\nargmaxP(word |sound )= argmaxP(sound |word )P(word ).\n1:t 1:t 1:t 1:t 1:t\nword1:t word1:t\nHere P(sound |word ) is the acoustic model. It describes the sounds of words\u2014that\nACOUSTICMODEL 1:t 1:t\n\u201cceiling\u201d begins with a soft \u201cc\u201d and sounds the same as \u201csealing.\u201d P(word ) is known as\n1:t\nthe language model. It specifies the prior probability of each utterance\u2014for example, that\nLANGUAGEMODEL\n\u201cceiling fan\u201disabout500timesmorelikelyasawordsequence than\u201csealingfan.\u201d\nNOISYCHANNEL This approach was named the noisy channel model by Claude Shannon (1948). He\nMODEL\ndescribed asituation inwhichanoriginal message (the wordsinourexample) istransmitted\nover a noisy channel (such as a telephone line) such that a corrupted message (the sounds\nin our example) are received at the other end. Shannon showed that no matter how noisy\nthe channel, it is possible to recover the original message with arbitrarily small error, if we\nencode the original message in a redundant enough way. The noisy channel approach has\nbeenapplied tospeechrecognition, machinetranslation, spellingcorrection, andothertasks.\nOnce we define the acoustic and language models, we can solve for the most likely\nsequence of words using the Viterbi algorithm (Section 15.2.3 on page 576). Most speech\nrecognition systems usealanguage modelthatmakestheMarkovassumption\u2014that thecur-\nrentstateWord dependsonlyonafixednumbernofpreviousstates\u2014andrepresent Word\nt t\nasasinglerandom variable takingonafinitesetofvalues, whichmakesitaHiddenMarkov\nModel(HMM).Thus,speechrecognitionbecomesasimpleapplicationoftheHMMmethod-\nology,asdescribedinSection15.3\u2014simplethatis,oncewedefinetheacousticandlanguage\nmodels. Wecoverthemnext. 914 Chapter 23. NaturalLanguageforCommunication\nVowels Consonants B\u2013N Consonants P\u2013Z\nPhone Example Phone Example Phone Example\n[iy] beat [b] bet [p] pet\n[ih] bit [ch] Chet [r] rat\n[eh] bet [d] debt [s] set\n[\u00e6] bat [f] fat [sh] shoe\n[ah] but [g] get [t] ten\n[ao] bought [hh] hat [th] thick\n[ow] boat [hv] high [dh] that\n[uh] book [jh] jet [dx] butter\n[ey] bait [k] kick [v] vet\n[er] Bert [l] let [w] wet\n[ay] buy [el] bottle [wh] which\n[oy] boy [m] met [y] yet\n[axr] diner [em] bottom [z] zoo\n[aw] down [n] net [zh] measure\n[ax] about [en] button\n[ix] roses [ng] sing\n[aa] cot [eng] washing [-] silence\nFigure 23.14 The ARPA phonetic alphabet, or ARPAbet, listing all the phonesused in\nAmerican English. There are several alternative notations, including an InternationalPho-\nneticAlphabet(IPA),whichcontainsthephonesinallknownlanguages.\n23.5.1 Acousticmodel\nSound waves are periodic changes in pressure that propagate through the air. When these\nwaves strike the diaphragm of a microphone, the back-and-forth movement generates an\nelectric current. An analog-to-digital converter measures the size of the current\u2014which ap-\nproximates theamplitude ofthesound wave\u2014atdiscrete intervals called the samplingrate.\nSAMPLINGRATE\nSpeechsounds, whicharemostlyintherangeof100Hz(100cyclespersecond)to1000Hz,\nare typically sampled at arate of 8 kHz. (CDs and mp3 files are sampled at 44.1 kHz.) The\nQUANTIZATION precisionofeachmeasurementisdeterminedbythequantizationfactor;speechrecognizers\nFACTOR\ntypically keep 8 to 12 bits. That means that a low-end system, sampling at 8 kHzwith 8-bit\nquantization, wouldrequirenearlyhalfamegabyteperminuteofspeech.\nSince we only want to know what words were spoken, not exactly what they sounded\nlike, wedon\u2019t need tokeep allthatinformation. Weonly need todistinguish between differ-\nentspeechsounds. Linguistshaveidentifiedabout100speechsounds,orphones,thatcanbe\nPHONE\ncomposed to form all the words in all known human languages. Roughly speaking, a phone\nis the sound that corresponds to a single vowel or consonant, but there are some complica-\ntions: combinations ofletters,suchas\u201cth\u201dand\u201cng\u201dproducesinglephones, andsomeletters\nproducedifferentphonesindifferentcontexts(e.g.,the\u201ca\u201dinratandrate. Figure23.14lists Section23.5. SpeechRecognition 915\nallthephones thatareused inEnglish, withanexampleofeach. Aphonemeisthesmallest\nPHONEME\nunit of sound that has a distinct meaning to speakers of a particular language. Forexample,\nthe\u201ct\u201din\u201cstick\u201dsounds similarenough tothe\u201ct\u201din\u201ctick\u201dthatspeakers ofEnglishconsider\nthem thesamephoneme. Butthedifference issignificant intheThailanguage, sothere they\naretwophonemes. TorepresentspokenEnglishwewantarepresentationthatcandistinguish\nbetweendifferentphonemes,butonethatneednotdistinguishthenonphonemicvariationsin\nsound: loudorsoft,fastorslow,maleorfemalevoice,etc.\nFirst, we observe that although the sound frequencies in speech may be several kHz,\nthe changes in the content of the signal occur much less often, perhaps at no more than 100\nHz. Therefore, speechsystemssummarizetheproperties ofthesignalovertimeslicescalled\nframes. Aframelength ofabout 10milliseconds (i.e.,80samples at 8kHz)isshort enough\nFRAME\ntoensurethatfewshort-duration phenomena willbemissed. Overlapping framesareusedto\nmakesurethatwedon\u2019tmissasignalbecause ithappens tofallonaframeboundary.\nEachframe issummarized byavectorof features. Picking outfeatures from aspeech\nFEATURE\nsignal is like listening to an orchestra and saying \u201chere the French horns are playing loudly\nand the violins are playing softly.\u201d We\u2019ll give a brief overview of the features in a typical\nsystem. First, a Fourier transform is used to determine the amount of acoustic energy at\nabout a dozen frequencies. Then we compute a measure called the mel frequency cepstral\nMELFREQUENCY\nCEPSTRAL coefficient (MFCC) or MFCC for each frequency. We also compute the total energy in\nCOEFFICIENT(MFCC)\nthe frame. That gives thirteen features; for each one we compute the difference between\nthis frame and the previous frame, and the difference between differences, for a total of 39\nfeatures. These arecontinuous-valued; theeasiest waytofitthem intothe HMMframework\nistodiscretizethevalues. (ItisalsopossibletoextendtheHMMmodeltohandlecontinuous\nmixtures of Gaussians.) Figure 23.15 shows the sequence of transformations from the raw\nsoundtoasequence offrameswithdiscrete features.\nWe have seen how to go from the raw acoustic signal to a series of observations, e .\nt\nNow we have to describe the (unobservable) states of the HMM and define the transition\nmodel, P(X t|X t\u22121), and the sensor model, P(E t|X t). The transition model can be broken\ninto two levels: word and phone. We\u2019ll start from the bottom: the phone model describes\nPHONEMODEL\nAnalog acoustic signal:\nSampled, quantized\ndigital signal:\n10 15 38 22 63 24 10 12 73\nFrames with features:\n52 47 82 89 94 11\nFigure23.15 Translatingthe acoustic signal into a sequence of frames. In this diagram\neach frame is described by the discretized values of three acoustic features; a real system\nwouldhavedozensoffeatures. 916 Chapter 23. NaturalLanguageforCommunication\nPhone HMM for [m]:\n0.3 0.9 0.4\n0.7 0.1 0.6\nOnset Mid End FINAL\nOutput probabilities for the phone HMM:\nOnset: Mid: End:\nC : 0.5 C : 0.2 C : 0.1\n1 3 4\nC : 0.2 C : 0.7 C : 0.5\n2 4 6\nC : 0.3 C : 0.1 C : 0.4\n3 5 7\nFigure 23.16 An HMM for the three-state phone [m]. Each state has several possible\noutputs,eachwithitsownprobability.TheMFCCfeaturelabelsC throughC arearbitrary,\n1 7\nstandingforsomecombinationoffeaturevalues.\n(a) Word model with dialect variation:\n0.5 [ey] 1.0\n1.0 1.0 1.0\n[t] [ow] [m] [t] [ow]\n0.5 1.0\n[aa]\n(b) Word model with coarticulation and dialect var:iations\n0.2 [ow] 1.0 0.5 [ey] 1.0\n1.0\n[t] [m] [t] [ow]\n0.8 1.0 0.5 1.0\n[ah] [aa]\nFigure23.17 Twopronunciationmodelsoftheword\u201ctomato.\u201d Eachmodelis shownas\natransitiondiagramwithstatesascirclesandarrowsshowingallowedtransitionswiththeir\nassociatedprobabilities. (a)Amodelallowingfordialect differences. The0.5numbersare\nestimatesbasedonthetwoauthors\u2019preferredpronunciations.(b)Amodelwithacoarticula-\ntioneffectonthefirstvowel,allowingeitherthe[ow]orthe[ah]phone. Section23.5. SpeechRecognition 917\na phone as three states, the onset, middle, and end. For example, the [t] phone has a silent\nbeginning, asmallexplosive burst ofsound inthemiddle, and(usually) ahissing attheend.\nFigure 23.16 shows an example for the phone [m]. Note that in normal speech, an average\nphone has a duration of 50\u2013100 milliseconds, or 5\u201310 frames. The self-loops in each state\nallowsforvariation inthis duration. Bytaking manyself-loops (especially inthe midstate),\nwe can represent a long \u201cmmmmmmmmmmm\u201d sound. Bypassing the self-loops yields a\nshort\u201cm\u201dsound.\nPRONUNCIATION In Figure 23.17 the phone models are strung together to form a pronunciation model\nMODEL\nfora word. According to Gershwin (1937), you say [t ow m ey t ow] and I say [t ow m aa t\now]. Figure 23.17(a) shows a transition model that provides for this dialect variation. Each\nofthecirclesinthisdiagramrepresents aphonemodelliketheoneinFigure23.16.\nInaddition todialect variation, wordscan have coarticulation variation. Forexample,\nthe [t] phone is produced with the tongue at the top of the mouth, whereas the [ow] has the\ntongue near the bottom. When speaking quickly, the tongue doesn\u2019t have time to get into\nposition for the [ow], and we end up with [t ah] rather than [t ow]. Figure 23.17(b) gives\na model for \u201ctomato\u201d that takes this coarticulation effect into account. More sophisticated\nphonemodelstakeintoaccountthecontextofthesurrounding phones.\nThere can be substantial variation in pronunciation for a word. The most common\npronunciation of \u201cbecause\u201d is [b iy k ah z], but that only accounts for about a quarter of\nuses. Anotherquarter(approximately) substitutes [ix],[ih]or[ax]forthefirstvowel,andthe\nremainder substitute [ax] or [aa] for the second vowel, [zh] or [s] for the final [z], or drop\n\u201cbe\u201dentirely, leaving\u201ccuz.\u201d\n23.5.2 Languagemodel\nFor general-purpose speech recognition, the language model can be an n-gram model of\ntext learned from a corpus of written sentences. However, spoken language has different\ncharacteristics than written language, so it is better to get a corpus of transcripts of spoken\nlanguage. For task-specific speech recognition, the corpus should be task-specific: to build\nyourairlinereservationsystem,gettranscriptsofpriorcalls. Italsohelpstohavetask-specific\nvocabulary, suchasalistofalltheairportsandcitiesserved,andalltheflightnumbers.\nPartofthedesignofavoiceuserinterfaceistocoercetheuserintosayingthingsfroma\nlimitedsetofoptions,sothatthespeechrecognizerwillhaveatighterprobabilitydistribution\ntodeal with. Forexample, asking \u201cWhat city do you wantto go to?\u201d elicits aresponse with\nahighlyconstrained language model,whileasking\u201cHowcanIhelpyou?\u201d doesnot.\n23.5.3 Building a speech recognizer\nThequalityofaspeechrecognition systemdependsonthequalityofallofitscomponents\u2014\nthe language model, the word-pronunciation models, the phone models, and the signal-\nprocessing algorithms used to extract spectral features from the acoustic signal. We have\ndiscussed how the language model can be constructed from a corpus of written text, and we\nleave the details of signal processing to other textbooks. We are left with the pronunciation\nandphonemodels. Thestructureofthepronunciationmodels\u2014suchasthetomatomodelsin 918 Chapter 23. NaturalLanguageforCommunication\nFigure23.17\u2014isusuallydevelopedbyhand. Largepronunciation dictionaries arenowavail-\nable for English and other languages, although their accuracy varies greatly. The structure\nof the three-state phone models is the same for all phones, as shown in Figure 23.16. That\nleavestheprobabilities themselves.\nAsusual, wewillacquire theprobabilities from acorpus, thistimeacorpus ofspeech.\nThe most common type of corpus to obtain is one that includes the speech signal for each\nsentence paired with a transcript of the words. Building a model from this corpus is more\ndifficult than building an n-gram model of text, because we have to build a hidden Markov\nmodel\u2014thephonesequenceforeachwordandthephonestateforeachtimeframearehidden\nvariables. In the early days of speech recognition, the hidden variables were provided by\nlaborious hand-labeling of spectrograms. Recent systems use expectation\u2013maximization to\nautomaticallysupplythemissingdata. Theideaissimple: givenanHMMandanobservation\nsequence, wecan use thesmoothing algorithms from Sections 15.2 and15.3 tocompute the\nprobability ofeachstateateachtimestepand,byasimpleextension, theprobability ofeach\nstate\u2013state pair at consecutive time steps. These probabilities can be viewed as uncertain\nlabels. From the uncertain labels, we can estimate new transition and sensor probabilities,\nand the EM procedure repeats. The method is guaranteed to increase the fit between model\nanddataoneachiteration,anditgenerallyconvergestoamuchbettersetofparametervalues\nthanthoseprovidedbytheinitial, hand-labeled estimates.\nThe systems with the highest accuracy work by training a different model for each\nspeaker, thereby capturing differences indialect aswellasmale\/female andothervariations.\nThis training can require several hours of interaction with the speaker, so the systems with\nthemostwidespread adoption donotcreatespeaker-specific models.\nTheaccuracyofasystemdependsonanumberoffactors. First,thequalityofthesignal\nmatters: ahigh-qualitydirectionalmicrophoneaimedatastationarymouthinapaddedroom\nwill do much better than a cheap microphone transmitting a signal over phone lines from a\ncar in traffic with the radio playing. The vocabulary size matters: when recognizing digit\nstrings withavocabulary of11words(1-9plus\u201coh\u201dand\u201czero\u201d), theworderrorratewillbe\nbelow 0.5%, whereas it rises to about 10% on news stories with a 20,000-word vocabulary,\nand20%onacorpuswitha64,000-wordvocabulary. Thetaskmatterstoo: whenthesystem\nis trying to accomplish a specific task\u2014book a flight or give directions to a restaurant\u2014the\ntaskcanoftenbeaccomplished perfectly evenwithaworderrorrateof10%ormore.\n23.6 SUMMARY\nNatural language understanding is one of the most important subfields of AI. Unlike most\notherareasofAI,naturallanguageunderstandingrequires anempiricalinvestigationofactual\nhumanbehavior\u2014which turnsouttobecomplexandinteresting.\n\u2022 Formal language theory and phrase structure grammars (and in particular, context-\nfree grammar) are useful tools fordealing withsome aspects of natural language. The\nprobabilistic context-free grammar(PCFG)formalismiswidelyused. Bibliographical andHistorical Notes 919\n\u2022 Sentences in a context-free language can be parsed in O(n3) time by a chart parser\nsuchastheCYKalgorithm,whichrequiresgrammarrulestobein ChomskyNormal\nForm.\n\u2022 Atreebankcanbeusedtolearnagrammar. Itisalsopossibletolearnagrammarfrom\nanunparsed corpusofsentences, butthisislesssuccessful.\n\u2022 Alexicalized PCFGallows ustorepresent thatsome relationships between words are\nmorecommonthanothers.\n\u2022 Itisconvenient toaugmentagrammartohandlesuchproblemsassubject\u2013verb agree-\nmentandpronouncase. Definiteclausegrammar(DCG)isaformalismthatallowsfor\naugmentations. With DCG, parsing and semantic interpretation (and even generation)\ncanbedoneusinglogicalinference.\n\u2022 Semanticinterpretation canalsobehandled byanaugmentedgrammar.\n\u2022 Ambiguity is a very important problem in natural language understanding; most sen-\ntences have manypossible interpretations, but usually only one is appropriate. Disam-\nbiguation relies on knowledge about the world, about the current situation, and about\nlanguageuse.\n\u2022 Machine translation systems have been implemented using a range of techniques,\nfrom full syntactic and semantic analysis to statistical techniques based on phrase fre-\nquencies. Currentlythestatistical modelsaremostpopularandmostsuccessful.\n\u2022 Speech recognition systems are also primarily based on statistical principles. Speech\nsystemsarepopularanduseful, albeitimperfect.\n\u2022 Together, machine translation and speech recognition are two of the big successes of\nnatural language technology. One reason that the models perform well is that large\ncorpora areavailable\u2014both translation and speech aretasks that areperformed \u201cinthe\nwild\u201d by people every day. In contrast, tasks like parsing sentences have been less\nsuccessful, in part because no large corpora of parsed sentences are available \u201cin the\nwild\u201dandinpartbecauseparsing isnotusefulinandofitself.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nLike semantic networks, context-free grammars (also known as phrase structure grammars)\nare areinvention of atechnique firstused byancient Indian grammarians (especially Panini,\nca. 350 B.C.) studying Shastric Sanskrit (Ingerman, 1967). They were reinvented by Noam\nChomsky (1956) for the analysis of English syntax and independently by John Backus for\nthe analysis of Algol-58 syntax. PeterNaurextended Backus\u2019s notation and is now credited\n(Backus, 1996) with the \u201cN\u201d in BNF, which originally stood for \u201cBackus Normal Form.\u201d\nATTRIBUTE Knuth (1968) defined a kind of augmented grammar called attribute grammar that is use-\nGRAMMAR\nful for programming languages. Definite clause grammars were introduced by Colmer-\nauer(1975)anddeveloped andpopularized byPereiraandShieber(1987).\nProbabilistic context-free grammars were investigated by Booth (1969) and Salo-\nmaa(1969). Otheralgorithms forPCFGsare presented in theexcellent short monograph by 920 Chapter 23. NaturalLanguageforCommunication\nCharniak(1993)andtheexcellentlongtextbooksbyManningandSchu\u00a8tze(1999)andJuraf-\nsky and Martin (2008). Baker (1979) introduces the inside\u2013outside algorithm for learning a\nPCFG,andLariandYoung(1990)describeitsusesandlimitations. StolckeandOmohundro\n(1994) showhowtolearngrammarruleswithBayesianmodelmerging; HaghighiandKlein\n(2006)describe alearningsystem basedonprototypes.\nLexicalized PCFGs(Charniak, 1997; Hwa,1998) combine thebest aspects ofPCFGs\nand n-gram models. Collins (1999) describes PCFG parsing that is lexicalized with head\nfeatures. Petrovand Klein(2007a) show howtogetthe advantages oflexicalization without\nactuallexicalaugmentationsbylearningspecificsyntacticcategoriesfromatreebankthathas\ngeneralcategories;forexample,thetreebankhasthecategoryNP,fromwhichmorespecific\ncategories suchasNP andNP canbelearned.\nO S\nThere have been many attempts to write formal grammars of natural languages, both\nin \u201cpure\u201d linguistics and in computational linguistics. There are several comprehensive but\ninformalgrammarsofEnglish(Quirk etal.,1985;McCawley,1988;HuddlestonandPullum,\n2002). Since the mid-1980s, there has been a trend toward putting more information in the\nlexicon and less in the grammar. Lexical-functional grammar, or LFG (Bresnan, 1982) was\nthe first major grammar formalism to be highly lexicalized. If we carry lexicalization to an\nextreme, weendupwith categorial grammar (ClarkandCurran, 2004), inwhichthere can\nbe as few as two grammar rules, or with dependency grammar (Smith and Eisner, 2008;\nKu\u00a8bleretal.,2009)inwhichtherearenosyntacticcategories, onlyrelationsbetweenwords.\nSleator and Temperley (1993) describe a dependency parser. Paskin (2001) shows that a\nversionofdependency grammariseasiertolearnthanPCFGs.\nThe first computerized parsing algorithms were demonstrated by Yngve (1955). Ef-\nficient algorithms were developed in the late 1960s, with a few twists since then (Kasami,\n1965;Younger, 1967;Earley,1970;Graham etal.,1980). MaxwellandKaplan(1993)show\nhow chart parsing with augmentations can be made efficient in the average case. Church\nand Patil (1982) address the resolution of syntactic ambiguity. Klein and Manning (2003)\n\u2217 \u2217\ndescribe A parsing, and Pauls and Klein (2009) extend that to K-best A parsing, in which\ntheresultisnotasingleparsebutthe Kbest.\nLeading parsers today include those by Petrov and Klein (2007b), which achieved\n90.6% accuracy on the Wall Street Journal corpus, Charniak and Johnson (2005), which\nachieved 92.0%, and Kooet al. (2008), which achieved 93.2% on the Penn treebank. These\nnumbersarenotdirectlycomparable,andthereissomecriticismofthefieldthatitisfocusing\ntoonarrowlyonafewselectcorpora, andperhapsoverfitting onthem.\nFormal semantic interpretation of natural languages originates within philosophy and\nformallogic, particularly AlfredTarski\u2019s(1935) workonthesemantics offormallanguages.\nBar-Hillel (1954) was the firstto consider the problems ofpragmatics and propose that they\ncould be handled by formal logic. For example, he introduced C. S. Peirce\u2019s (1902) term\nindexical into linguistics. Richard Montague\u2019s essay \u201cEnglish as a formal language\u201d (1970)\nis a kind of manifesto for the logical analysis of language, but the books by Dowty et al.\n(1991)andPortnerandPartee(2002)aremorereadable.\nThe first NLP system to solve an actual task was probably the BASEBALL question\nansweringsystem(Green etal.,1961),whichhandledquestionsaboutadatabaseofbaseball Bibliographical andHistorical Notes 921\nstatistics. CloseafterthatwasWoods\u2019s (1973) LUNAR,whichanswered questions about the\nrocks brought back from the moon by the Apollo program. Roger Schank and his students\nbuilt a series of programs (Schank and Abelson, 1977; Schank and Riesbeck, 1981) that\nall had the task of understanding language. Modern approaches to semantic interpretation\nusually assume that the mapping from syntax to semantics will be learned from examples\n(ZelleandMooney,1996;ZettlemoyerandCollins,2005).\nHobbsetal. (1993) describes aquantitative nonprobabilistic framework forinterpreta-\ntion. More recent work follows an explicitly probabilistic framework (Charniak and Gold-\nman, 1992; Wu, 1993; Franz, 1996). Inlinguistics, optimality theory (Kager, 1999) isbased\non the idea of building soft constraints into the grammar, giving a natural ranking to inter-\npretations (similartoaprobability distribution), ratherthan having thegrammargenerate all\npossibilities with equal rank. Norvig (1988) discusses the problems of considering multiple\nsimultaneous interpretations, rather than settling for a single maximum-likelihood interpre-\ntation. Literary critics (Empson, 1953; Hobbs, 1990) have been ambiguous about whether\nambiguityissomethingtoberesolved orcherished.\nNunberg(1979)outlinesaformalmodelofmetonymy. LakoffandJohnson(1980)give\nanengaginganalysisandcatalogofcommonmetaphorsinEnglish. Martin(1990)andGibbs\n(2006)offercomputational modelsofmetaphorinterpretation.\nThe first important result on grammar induction was a negative one: Gold (1967)\nshowed that it is not possible to reliably learn a correct context-free grammar, given a set of\nstrings fromthatgrammar. Prominentlinguists, suchasChomsky(1957) andPinker(2003),\nUNIVERSAL have used Gold\u2019s result to argue that there must be an innate universal grammar that all\nGRAMMAR\nchildren have from birth. The so-called Poverty of the Stimulus argument says that children\naren\u2019t given enough input tolearn aCFG,so theymust already \u201cknow\u201d the grammarand be\nmerelytuningsomeofitsparameters. Whilethisargumentcontinuestoholdswaythroughout\nmuchofChomskyanlinguistics,ithasbeendismissedbysomeotherlinguists(Pullum,1996;\nElman et al., 1997) and most computer scientists. Asearly as 1969, Horning showed that it\nispossibletolearn,inthesenseofPAClearning, aprobabilistic context-freegrammar. Since\nthen, there have been many convincing empirical demonstrations of learning from positive\nexamplesalone,suchastheILPworkofMooney(1999)andMuggletonandDeRaedt(1994),\nthesequencelearningofNevill-ManningandWitten(1997),andtheremarkablePh.D.theses\nof Schu\u00a8tze (1995) and de Marcken (1996). There is an annual International Conference on\nGrammatical Inference (ICGI). It is possible to learn other grammar formalisms, such as\nregularlanguages(Denis,2001)andfinitestateautomata(ParekhandHonavar,2001). Abney\n(2007)isatextbookintroduction tosemi-supervised learning forlanguage models.\nWordnet(Fellbaum,2001)isapubliclyavailabledictionaryofabout100,000wordsand\nphrases, categorized into parts of speech and linked by semantic relations such as synonym,\nantonym, and part-of. The Penn Treebank (Marcus et al., 1993) provides parse trees for a\n3-million-word corpus of English. Charniak (1996) and Klein and Manning (2001) discuss\nparsing with treebank grammars. The British National Corpus (Leech et al., 2001) contains\n100 million words, and the World Wide Web contains several trillion words; (Brants et al.,\n2007)describe n-grammodelsovera2-trillion-word Webcorpus. 922 Chapter 23. NaturalLanguageforCommunication\nInthe 1930s PetrTroyanskii applied fora patent fora\u201ctranslating machine,\u201d but there\nwerenocomputersavailabletoimplementhisideas. InMarch1947,theRockefellerFounda-\ntion\u2019sWarrenWeaverwrotetoNorbertWiener,suggesting thatmachinetranslation mightbe\npossible. Drawingonworkincryptography andinformation theory, Weaverwrote, \u201cWhen I\nlookatanarticleinRussian,Isay: \u2018ThisisreallywritteninEnglish,butithasbeencodedin\nstrange symbols. I will now proceed to decode.\u201d\u2019 Forthe next decade, the community tried\nto decode in this way. IBM exhibited a rudimentary system in 1954. Bar-Hillel (1960) de-\nscribes the enthusiasm of this period. However, the U.S. government subsequently reported\n(ALPAC,1966) that \u201cthere isno immediate orpredictable prospect ofuseful machine trans-\nlation.\u201d However, limited work continued, and starting in the 1980s, computer power had\nincreased tothepointwheretheALPACfindingswerenolonger correct.\nThebasic statistical approach wedescribe inthe chapter is based on early workby the\nIBM group (Brown et al., 1988, 1993) and the recent work by the ISI and Google research\ngroups (Och and Ney, 2004; Zollmann et al., 2008). A textbook introduction on statistical\nmachinetranslationisgivenbyKoehn(2009),andashorttutorialbyKevinKnight(1999)has\nbeeninfluential. EarlyworkonsentencesegmentationwasdonebyPalmerandHearst(1994).\nOchandNey(2003)andMoore(2005)coverbilingual sentence alignment.\nThe prehistory of speech recognition began in the 1920s with Radio Rex, a voice-\nactivated toy dog. Rex jumped out of his doghouse in response to the word \u201cRex!\u201d (or\nactually almostanysufficientlyloudword). Somewhatmoreserious workbeganafterWorld\nWarII. At AT&TBell Labs, a system was built for recognizing isolated digits (Davis et al.,\n1952)bymeansofsimplepatternmatchingofacousticfeatures. Startingin1971,theDefense\nAdvanced Research Projects Agency (DARPA)of the United States Department of Defense\nfunded four competing five-year projects to develop high-performance speech recognition\nsystems. Thewinner,andtheonlysystemtomeetthegoalof90%accuracywitha1000-word\nvocabulary, was the HARPY system at CMU (Lowerre and Reddy, 1980). The final version\nofHARPYwasderivedfromasystemcalled DRAGONbuiltbyCMUgraduatestudentJames\nBaker (1975); DRAGON was the first to use HMMs for speech. Almost simultaneously, Je-\nlinek (1976) at IBM had developed another HMM-based system. Recent years have been\ncharacterized by steady incremental progress, larger data sets and models, and more rigor-\nouscompetitions onmorerealistic speech tasks. In1997, BillGatespredicted, \u201cThePCfive\nyearsfrom now\u2014youwon\u2019trecognize it,because speech willcomeintotheinterface.\u201d That\ndidn\u2019t quitehappen, butin2008 hepredicted \u201cInfiveyears, Microsoft expects moreInternet\nsearches to be done through speech than through typing on a keyboard.\u201d History will tell if\nheisrightthistimearound.\nSeveral good textbooks on speech recognition are available (Rabiner and Juang, 1993;\nJelinek, 1997; GoldandMorgan, 2000;Huangetal.,2001). Thepresentation inthischapter\ndrewonthesurveybyKay,Gawron,andNorvig(1994)andonthe textbookbyJurafsky and\nMartin(2008). Speechrecognition researchispublishedin ComputerSpeechandLanguage,\nSpeech Communications, and the IEEE Transactions on Acoustics, Speech, and Signal Pro-\ncessing and at the DARPAWorkshops on Speech and Natural Language Processing and the\nEurospeech, ICSLP,andASRUconferences. Exercises 923\nKen Church (2004) shows that natural language research has cycled between concen-\ntrating on the data (empiricism) and concentrating on theories (rationalism). The linguist\nJohnFirth(1957)proclaimed\u201cYoushallknowawordbythecompanyitkeeps,\u201dandlinguis-\ntics of the 1940s and early 1950s was based largely on word frequencies, although without\nthe computational power we have available today. Then Noam (Chomsky, 1956) showed\nthelimitations offinite-state models, andsparked aninterest intheoretical studies ofsyntax,\ndisregarding frequency counts. This approach dominated fortwenty years, until empiricism\nmade a comeback based on the success of work in statistical speech recognition (Jelinek,\n1976). Today,mostworkacceptsthestatisticalframework, butthereisgreatinterestinbuild-\ningstatistical modelsthat consider higher-level models, such assyntactic treesand semantic\nrelations, notjustsequences ofwords.\nWorkonapplications oflanguageprocessingispresentedatthebiennialAppliedNatu-\nralLanguageProcessingconference(ANLP),theconference onEmpiricalMethodsinNatu-\nralLanguageProcessing(EMNLP),andthejournal NaturalLanguageEngineering. Abroad\nrangeofNLPworkappearsinthejournalComputationalLinguisticsanditsconference,ACL,\nandintheComputational Linguistics (COLING)conference.\nEXERCISES\n23.1 Read the following text once for understanding, and remember as much of it as you\ncan. Therewillbeatestlater.\nTheprocedureisactuallyquitesimple. Firstyouarrangethingsintodifferentgroups. Of\ncourse,onepilemaybesufficientdependingonhowmuchthereistodo.Ifyouhavetogo\nsomewhereelseduetolackoffacilitiesthatisthenextstep,otherwiseyouareprettywell\nset. Itisimportantnottooverdothings. Thatis,itisbettertodotoofewthingsatonce\nthantoomany.Intheshortrunthismaynotseemimportantbutcomplicationscaneasily\narise.Amistakeisexpensiveaswell. Atfirstthewholeprocedurewillseemcomplicated.\nSoon,however,itwillbecomejustanotherfacetoflife. Itisdifficulttoforeseeanyend\ntothenecessityforthistaskintheimmediatefuture,butthenonecannevertell. Afterthe\nprocedureiscompletedonearrangesthematerialintodifferentgroupsagain. Thenthey\ncanbeputintotheirappropriateplaces. Eventuallytheywillbeusedoncemoreandthe\nwholecyclewillhavetoberepeated.However,thisispartoflife.\n23.2 AnHMMgrammarisessentially astandard HMMwhosestatevariable isN (nonter-\nminal,withvaluessuchasDet,Adjective,Nounandsoon)andwhoseevidencevariableis\nW (word,withvaluessuchasis,duck,andsoon). TheHMMmodelincludesapriorP(N ),\n0\natransitionmodelP(N |N ),andasensormodelP(W |N ). ShowthateveryHMMgram-\nt+1 t t t\nmar can be written as a PCFG. [Hint: start by thinking about how the HMM prior can be\nrepresented by PCFGrules forthe sentence symbol. Youmay findit helpful to illustrate for\ntheparticularHMMwithvaluesA,B forN andvaluesx,y forW.] 924 Chapter 23. NaturalLanguageforCommunication\n23.3 ConsiderthefollowingPCFGforsimpleverbphrases:\n0.1 :VP \u2192 Verb\n0.2 :VP \u2192 CopulaAdjective\n0.5 :VP \u2192 VerbtheNoun\n0.2 :VP \u2192 VP Adverb\n0.5 :Verb \u2192 is\n0.5 :Verb \u2192 shoots\n0.8 :Copula\u2192 is\n0.2 :Copula\u2192 seems\n0.5 :Adjective \u2192 unwell\n0.5 :Adjective \u2192 well\n0.5 :Adverb \u2192 well\n0.5 :Adverb \u2192 badly\n0.6 :Noun \u2192 duck\n0.4 :Noun \u2192 well\na. Which of the following have a nonzero probability as a VP? (i) shoots the duck well\nwellwell (ii)seemsthewellwell (iii)shootstheunwellwellbadly\nb. Whatistheprobability ofgenerating \u201ciswellwell\u201d?\nc. Whattypesofambiguity areexhibited bythephrasein(b)?\nd. Given any PCFG, is it possible to calculate the probability that the PCFG generates a\nstringofexactly10words?\n23.4 Outline the major differences between Java (or any other computer language with\nwhich you are familiar) and English, commenting on the \u201cunderstanding\u201d problem in each\ncase. Think about such things as grammar, syntax, semantics, pragmatics, compositional-\nity, context-dependence, lexical ambiguity, syntactic ambiguity, reference finding (including\npronouns),backgroundknowledge,andwhatitmeansto\u201cunderstand\u201dinthefirstplace.\n23.5 Thisexerciseconcerns grammarsforverysimplelanguages.\na. Writeacontext-free grammarforthelanguage anbn.\nb. Writeacontext-free grammarforthepalindrome language: thesetofallstrings whose\nsecondhalfisthereverseofthefirsthalf.\nc. Write a context-sensitive grammar for the duplicate language: the set of all strings\nwhosesecondhalfisthesameasthefirsthalf.\n23.6 Consider the sentence \u201cSomeone walked slowly to the supermarket\u201d and a lexicon\nconsisting ofthefollowingwords:\nPronoun \u2192 someone Verb \u2192 walked\nAdv \u2192 slowly Prep \u2192 to\nArticle \u2192 the Noun \u2192 supermarket\nWhichofthefollowingthreegrammars,combinedwiththelexicon,generates thegivensen-\ntence? Showthecorresponding parsetree(s). Exercises 925\n(A): (B): (C):\nS \u2192 NP VP S \u2192 NP VP S \u2192 NP VP\nNP \u2192 Pronoun NP \u2192 Pronoun NP \u2192 Pronoun\nNP \u2192 Article Noun NP \u2192 Noun NP \u2192 Article NP\nVP \u2192 VP PP NP \u2192 Article NP VP \u2192 Verb Adv\nVP \u2192 VP Adv Adv VP \u2192 Verb Vmod Adv \u2192 Adv Adv\nVP \u2192 Verb Vmod \u2192 Adv Vmod Adv \u2192 PP\nPP \u2192 Prep NP Vmod \u2192 Adv PP \u2192 Prep NP\nNP \u2192 Noun Adv \u2192 PP NP \u2192 Noun\nPP \u2192 Prep NP\nFor each of the preceding three grammars, write down three sentences of English and three\nsentences of non-English generated by the grammar. Each sentence should be significantly\ndifferent, should be at least six words long, and should include some new lexical entries\n(which you should define). Suggest ways to improve each grammar to avoid generating the\nnon-English sentences.\n23.7 Collect some examples of time expressions, such as \u201ctwo o\u2019clock,\u201d \u201cmidnight,\u201d and\n\u201c12:46.\u201d Alsothink upsomeexamplesthatareungrammatical, such as\u201cthirteen o\u2019clock\u201d or\n\u201chalfpasttwofifteen.\u201d Writeagrammarforthetimelanguage.\n23.8 In this exercise you will transform E into Chomsky Normal Form (CNF). There are\n0\nfive steps: (a) Add a new start symbol, (b) Eliminate (cid:2) rules, (c) Eliminate multiple words\non right-hand sides, (d) Eliminate rules of the form (X \u2192 Y), (e) Convert long right-hand\nsidesintobinaryrules.\na. Thestartsymbol,S,canoccuronlyontheleft-handsideinCNF.Addanewruleofthe\nformS(cid:2) \u2192 S,usinganewsymbolS(cid:2) .\nb. Theemptystring, (cid:2)cannot appearontheright-hand sideinCNF.E doesnothaveany\n0\nruleswith(cid:2),sothisisnotanissue.\nc. A word can appear on the right-hand side in a rule only of the form (X \u2192 word).\nReplace each rule of the form (X \u2192 ...word...) with (X \u2192 ...W(cid:2) ...) and (W(cid:2)\n\u2192 word),usinganewsymbolW(cid:2) .\nd. A rule (X \u2192 Y) is not allowed in CNF; it must be (X \u2192 Y Z) or (X \u2192 word).\nReplace each rule of the form (X \u2192 Y) with a set of rules of the form (X \u2192 ...),\noneforeachrule(Y \u2192 ...),where(...) indicates oneormoresymbols.\ne. Replace each ruleofthe form (X \u2192 Y Z ...) withtworules, (X \u2192 Y Z(cid:2) )and(Z(cid:2)\n\u2192 Z ...),whereZ(cid:2) isanewsymbol.\nShoweachstepoftheprocess andthefinalsetofrules.\n23.9 Using DCG notation, write a grammar for a language that is just like E , except that\n1\nitenforces agreement between thesubject andverb ofasentence andthus does notgenerate\nungrammatical sentences suchas\u201cIsmellsthewumpus.\u201d 926 Chapter 23. NaturalLanguageforCommunication\n23.10 ConsiderthefollowingPCFG:\nS \u2192NP VP [1.0]\nNP \u2192Noun [0.6]|Pronoun [0.4]\nVP \u2192Verb NP [0.8]|Modal Verb [0.2]\nNoun \u2192can[0.1]|fish[0.3]|...\nPronoun \u2192I[0.4]|...\nVerb \u2192can[0.01]|fish[0.1]|...\nModal \u2192can[0.3]|...\nThe sentence \u201cI can fish\u201d has two parse trees with this grammar. Show the two trees, their\npriorprobabilities, andtheirconditional probabilities, giventhesentence.\n23.11 Anaugmented context-free grammarcan represent languages thataregularcontext-\nfree grammar cannot. Show an augmented context-free grammar for the language anbncn.\nThe allowable values for augmentation variables are 1 and SUCCESSOR(n), where n is a\nvalue. Theruleforasentenceinthislanguage is\nS(n) \u2192 A(n) B(n)C(n).\nShowtherule(s)foreachof A,B,andC.\n23.12 Augment the E grammar so that it handles article\u2013noun agreement. That is, make\n1\nsurethat\u201cagents\u201d and\u201canagent\u201dare NPs,but\u201cagent\u201dand\u201canagents\u201d arenot.\n23.13 Considerthefollowingsentence (from TheNewYorkTimes,July28,2008):\nBanks struggling to recover from multibillion-dollar loans on real estate are cur-\ntailingloanstoAmericanbusinesses,deprivingevenhealthycompaniesofmoney\nforexpansion andhiring.\na. Whichofthewordsinthissentence arelexicallyambiguous?\nb. Findtwocasesofsyntactic ambiguity inthissentence (therearemorethantwo.)\nc. Giveaninstance ofmetaphorinthissentence.\nd. Canyoufindsemanticambiguity?\n23.14 Without lookingbackatExercise23.1,answerthefollowing questions:\na. Whatarethefourstepsthatarementioned?\nb. Whatstepisleftout?\nc. Whatis\u201cthematerial\u201dthatismentioned inthetext?\nd. Whatkindofmistakewouldbeexpensive?\ne. Isitbettertodotoofewthingsortoomany? Why?\n23.15 Select five sentences and submit them to an online translation service. Translate\nthemfromEnglishtoanotherlanguage andbacktoEnglish. Ratetheresulting sentences for\ngrammaticality and preservation of meaning. Repeat the process; does the second round of Exercises 927\niteration give worse results or the same results? Does the choice of intermediate language\nmake a difference to the quality of the results? If you know a foreign language, look at the\ntranslation of one paragraph into that language. Count and describe the errors made, and\nconjecture whytheseerrorsweremade.\n23.16 The D values for the sentence in Figure 23.13 sum to 0. Will that be true of every\ni\ntranslation pair? Proveitorgiveacounterexample.\n23.17 (Adapted from Knight (1999).) Ourtranslation model assumes that, afterthe phrase\ntranslationmodelselectsphrasesandthedistortionmodelpermutesthem,thelanguagemodel\ncan unscramble the permutation. This exercise investigates how sensible that assumption is.\nTrytounscramble theseproposed listsofphrases intothecorrectorder:\na. have,programming, a,seen,never,I,language, better\nb. loves,john,mary\nc. is the, communication, exchange of, intentional, information brought, by, about, the\nproduction, perception of, and signs, from, drawn, a, of, system, signs, conventional,\nshared\nd. created,that,weholdthese, tobe,allmen,truths, are,equal,self-evident\nWhich ones could you do? What type of knowledge did you draw upon? Train a bigram\nmodel from atraining corpus, and use it tofind thehighest-probability permutation of some\nsentences fromatestcorpus. Reportontheaccuracyofthismodel.\n23.18 Calculate the most probable path through the HMM in Figure 23.16 for the output\nsequence [C ,C ,C ,C ,C ,C ,C ]. Alsogiveitsprobability.\n1 2 3 4 4 6 7\n23.19 We forgot to mention that the text in Exercise 23.1 is entitled \u201cWashing Clothes.\u201d\nReread the text and answer the questions in Exercise 23.14. Did you do better this time?\nBransfordandJohnson(1973)usedthistextinacontrolledexperimentandfoundthatthetitle\nhelpedsignificantly. Whatdoesthistellyouabouthowlanguage andmemoryworks? 24\nPERCEPTION\nInwhichweconnect thecomputertotheraw,unwashedworld.\nPerceptionprovidesagentswithinformationabouttheworldtheyinhabitbyinterpreting the\nPERCEPTION\nresponse of sensors. A sensor measures some aspect of the environment in a form that can\nSENSOR\nbeusedasinputbyanagentprogram. Thesensorcouldbeassimpleasaswitch,whichgives\nonebittellingwhetheritisonoroff,orascomplexastheeye. Avarietyofsensorymodalities\nare available to artificial agents. Those they share with humans include vision, hearing, and\ntouch. Modalities that are not available to the unaided human include radio, infrared, GPS,\nandwirelesssignals. Somerobotsdoactivesensing,meaningtheysendoutasignal,suchas\nradarorultrasound, andsensethereflectionofthissignaloffoftheenvironment. Ratherthan\ntryingtocoverallofthese, thischapterwillcoveronemodalityindepth: vision.\nWe saw in our description of POMDPs (Section 17.4, page 658) that a model-based\ndecision-theoretic agent in a partially observable environment has a sensor model\u2014a prob-\nability distribution P(E|S) over the evidence that its sensors provide, given a state of the\nworld. Bayes\u2019rulecanthenbeusedtoupdatetheestimationofthestate.\nFor vision, the sensor model can be broken into two components: An object model\nOBJECTMODEL\ndescribes the objects that inhabit the visual world\u2014people, buildings, trees, cars, etc. The\nobjectmodelcouldincludeaprecise3Dgeometricmodeltakenfromacomputer-aideddesign\n(CAD)system,oritcouldbevagueconstraints,suchasthefactthathumaneyesareusually5\nto7cmapart. Arenderingmodeldescribesthephysical,geometric,andstatisticalprocesses\nRENDERINGMODEL\nthat produce thestimulus from theworld. Rendering models are quite accurate, but they are\nambiguous. For example, a white object under low light may appear as the same color as a\nblack object underintense light. Asmall nearby object maylook thesame asa large distant\nobject. Without additional evidence, we cannot tell if the image that fills the frame is a toy\nGodzillaorarealmonster.\nAmbiguity can be managed with prior knowledge\u2014we know Godzilla is not real, so\nthe image must be a toy\u2014or by selectively choosing to ignore the ambiguity. For example,\nthe vision system for an autonomous car may not be able to interpret objects that are far in\nthe distance, but the agent can choose to ignore the problem, because it is unlikely to crash\nintoanobjectthatismilesaway.\n928 Section24.1. ImageFormation 929\nAdecision-theoretic agent isnot theonlyarchitecture that canmakeuseofvision sen-\nsors. Forexample, fruit flies (Drosophila) are in part reflex agents: they have cervical giant\nfibersthatformadirectpathwayfromtheirvisualsystemtothewingmusclesthatinitiatean\nescape response\u2014an immediate reaction, without deliberation. Flies and many other flying\nanimals make used of a closed-loop control architecture to land on an object. The visual\nsystem extracts an estimate of the distance to the object, and the control system adjusts the\nwingmusclesaccordingly,allowingveryfastchangesofdirection,withnoneedforadetailed\nmodeloftheobject.\nCompared to the data from other sensors (such as the single bit that tells the vacuum\nrobot that it has bumped into a wall), visual observations are extraordinarily rich, both in\nthe detail they can reveal and in the sheer amount of data they produce. A video camera\nfor robotic applications might produce a million 24-bit pixels at 60 Hz; a rate of 10 GB per\nminute. The problem for a vision-capable agent then is: Which aspects of the rich visual\nstimulusshouldbeconsideredtohelptheagentmakegoodactionchoices,andwhichaspects\nshould be ignored? Vision\u2014and all perception\u2014serves to further the agent\u2019s goals, not as\nanendtoitself.\nFEATURE We can characterize three broad approaches to the problem. The feature extraction\nEXTRACTION\napproach, as exhibited by Drosophila, emphasizes simple computations applied directly to\nthe sensor observations. Inthe recognition approach an agent drawsdistinctions among the\nRECOGNITION\nobjectsitencountersbasedonvisualandotherinformation. Recognitioncouldmeanlabeling\neachimagewithayesornoastowhetheritcontainsfoodthatweshouldforage,orcontains\nGrandma\u2019s face. Finally, in the reconstruction approach an agent builds ageometric model\nRECONSTRUCTION\noftheworldfromanimageorasetofimages.\nThe last thirty years of research have produced powerful tools and methods for ad-\ndressing these approaches. Understanding these methods requires an understanding of the\nprocesses bywhich imagesareformed. Therefore, wenowcoverthephysical andstatistical\nphenomena thatoccurintheproduction ofanimage.\n24.1 IMAGE FORMATION\nImaging distorts the appearance of objects. For example, a picture taken looking down a\nlong straight set of railway tracks will suggest that the rails converge and meet. As another\nexample, if you hold your hand in front of your eye, you can block out the moon, which is\nnot smaller than your hand. Asyou moveyour hand back and forth ortilt it, your hand will\nseem toshrink and grow in the image, but itis not doing so inreality (Figure 24.1). Models\noftheseeffectsareessential forbothrecognition andreconstruction.\n24.1.1 Imageswithoutlenses: The pinholecamera\nImage sensors gather light scattered from objects in a scene and create a two-dimensional\nSCENE\nimage. In the eye, the image is formed on the retina, which consists of two types of cells:\nIMAGE\nabout 100 million rods, which are sensitive to light at a wide range of wavelengths, and 5 930 Chapter 24. Perception\nFigure24.1 Imagingdistortsgeometry. Parallel lines appearto meet in the distance, as\nintheimageoftherailwaytracksontheleft. Inthecenter,a smallhandblocksoutmostof\na largemoon. Onthe rightisa foreshorteningeffect: thehandistilted awayfromthe eye,\nmakingitappearshorterthaninthecenterfigure.\nmillion cones. Cones, which are essential for color vision, are of three main types, each of\nwhich is sensitive to a different set of wavelengths. In cameras, the image is formed on an\nimage plane, which can be a piece of film coated with silver halides or a rectangular grid\nof a few million photosensitive pixels, each a complementary metal-oxide semiconductor\nPIXEL\n(CMOS) or charge-coupled device (CCD). Each photon arriving at the sensor produces an\neffect, whose strength depends on the wavelength of the photon. The output of the sensor\nis the sum of all effects due to photons observed in some time window, meaning that image\nsensorsreportaweightedaverageoftheintensity oflightarrivingatthesensor.\nTo see a focused image, we must ensure that all the photons from approximately the\nsamespotinthescenearriveatapproximatelythesamepointintheimageplane. Thesimplest\nway to form a focused image is to view stationary objects with a pinhole camera, which\nPINHOLECAMERA\nconsists ofapinhole opening, O,atthefrontofabox, and animageplane atthebackofthe\nbox (Figure 24.2). Photons from the scene must pass through the pinhole, so if it is small\nenough then nearby photons in the scene will be nearby in the image plane, and the image\nwillbeinfocus.\nThegeometryofsceneandimageiseasiesttounderstand withthepinholecamera. We\nuseathree-dimensional coordinatesystemwiththeoriginatthepinhole,andconsiderapoint\n(cid:2)\nP in the scene, with coordinates (X,Y,Z). P gets projected to the point P in the image\nplanewithcoordinates (x,y,z). Iff isthedistancefromthepinholetotheimageplane,then\nbysimilartriangles, wecanderivethefollowingequations:\n\u2212x X \u2212y Y \u2212fX \u2212fY\n= , = \u21d2 x = , y = .\nf Z f Z Z Z\nPERSPECTIVE These equations define animage-formation process known as perspective projection. Note\nPROJECTION\nthat the Z inthe denominator means that the farther awayan object is, the smallerits image Section24.1. ImageFormation 931\nImage\nplane Y P\nX\nZ\nP\u2032\nPinhole\nf\nFigure24.2 Eachlight-sensitiveelementintheimageplaneatthebackofapinholecam-\nerareceiveslightfromathesmallrangeofdirectionsthatpassesthroughthepinhole. Ifthe\npinholeissmallenough,theresultisafocusedimageatthebackofthepinhole.Theprocess\nofprojectionmeansthatlarge,distantobjectslookthesameassmaller,nearbyobjects.Note\nthattheimageisprojectedupsidedown.\nwill be. Also, note that the minus signs mean that the image is inverted, both left\u2013right and\nup\u2013down, comparedwiththescene.\nUnder perspective projection, distant objects look small. This is what allows you to\ncoverthemoonwithyourhand(Figure24.1). Animportantresultofthiseffectisthatparallel\nlinesconverge toapointonthehorizon. (Thinkofrailwaytracks, Figure24.1.) Alineinthe\nsceneinthedirection (U,V,W)andpassingthroughthepoint(X ,Y ,Z )canbedescribed\n0 0 0\nas the set of points (X +\u03bbU,Y +\u03bbV,Z +\u03bbW), with \u03bbvarying between \u2212\u221e and +\u221e.\n0 0 0\nDifferentchoices of(X ,Y ,Z )yielddifferent linesparallel tooneanother. Theprojection\n0 0 0\nofapointP fromthislineontotheimageplaneisgivenby\n(cid:13) \u03bb (cid:14)\nX +\u03bbU Y +\u03bbV\n0 0\nf ,f .\nZ +\u03bbW Z +\u03bbW\n0 0\nAs \u03bb \u2192 \u221e or \u03bb \u2192 \u2212\u221e, this becomes p\u221e = (fU\/W,fV\/W) if W (cid:7)= 0. This means that\ntwo parallel lines leaving different points in space will converge in the image\u2014for large \u03bb,\ntheimagepointsarenearlythesame,whateverthevalueof(X ,Y ,Z )(again,thinkrailway\n0 0 0\nVANISHINGPOINT\ntracks, Figure 24.1). We call p\u221e the vanishing point associated with the family of straight\nlineswithdirection(U,V,W). Lineswiththesamedirectionsharethesamevanishingpoint.\n24.1.2 Lens systems\nThe drawback of the pinhole camera is that we need a small pinhole to keep the image in\nfocus. Butthesmallerthepinhole,thefewerphotonsgetthrough,meaningtheimagewillbe\ndark. Wecan gather more photons by keeping the pinhole open longer, but then wewill get\nmotion blur\u2014objects inthe scene thatmovewillappearblurred because theysend photons\nMOTIONBLUR\nto multiple locations on the image plane. If we can\u2019t keep the pinhole open longer, we can\ntrytomakeitbigger. Morelightwillenter,butlightfromasmallpatchofobjectinthescene\nwillnowbespreadoverapatchontheimageplane, causingablurred image. 932 Chapter 24. Perception\nImage plane\nLight Source\nIris\nCornea Fovea\nVisual Axis\nOptic Nerve\nLens\nOptical Axis\nLens\nSystem Retina\nFigure24.3 Lensescollectthelightleavingascenepointinarangeofdirections,andsteer\nitalltoarriveatasinglepointontheimageplane. Focusingworksforpointslyingcloseto\na focal plane in space; other points will not be focused properly. In cameras, elements of\nthelenssystemmovetochangethefocalplane,whereasintheeye,theshapeofthelensis\nchangedbyspecializedmuscles.\nVertebrate eyes and modern cameras use a lens system to gather sufficient light while\nLENS\nkeeping the image in focus. A large opening is covered with a lens that focuses light from\nnearby object locations downtonearby locations intheimageplane. However, lenssystems\nhave a limited depth of field: they can focus light only from points that lie within a range\nDEPTHOFFIELD\nof depths (centered around a focal plane). Objects outside this range willbe out of focus in\nFOCALPLANE\nthe image. Tomovethe focal plane, thelens inthe eyecan change shape (Figure 24.3); in a\ncamera,thelensesmovebackandforth.\n24.1.3 Scaledorthographic projection\nPerspective effects aren\u2019t always pronounced. For example, spots on a distant leopard may\nlooksmallbecausetheleopardisfaraway,buttwospotsthatarenexttoeachotherwillhave\naboutthesamesize. Thisisbecausethedifferenceindistancetothespotsissmallcompared\ntothedistance tothem,andsowecansimplify theprojection model. Theappropriate model\nSCALED is scaled orthographic projection. The idea is as follows: If the depth Z of points on the\nORTHOGRAPHIC\nPROJECTION object varies within some range Z \u00b1 \u0394Z, with \u0394Z * Z , then the perspective scaling\n0 0\nfactor f\/Z canbeapproximated byaconstant s = f\/Z . Theequations forprojection from\n0\nthe scene coordinates (X,Y,Z) to the image plane become x = sX and y = sY. Scaled\northographicprojectionisanapproximationthatisvalidonlyforthosepartsofthescenewith\nnotmuchinternaldepthvariation. Forexample,scaledorthographic projectioncanbeagood\nmodelforthefeaturesonthefrontofadistant building.\n24.1.4 Lightand shading\nThe brightness of a pixel in the image is a function of the brightness of the surface patch in\nthescenethatprojectstothepixel. Wewillassumealinearmodel(currentcamerashavenon-\nlinearities attheextremesoflightanddark,butarelinear inthemiddle). Imagebrightness is Section24.1. ImageFormation 933\nDiffuse reflection, bright\nSpecularities\nDiffuse reflection, dark Cast shadow\nFigure24.4 Avarietyofilluminationeffects. Therearespecularities onthemetalspoon\nandonthemilk. Thebrightdiffusesurfaceisbrightbecauseitfacesthelightdirection. The\ndarkdiffusesurfaceisdarkbecauseitistangentialtotheilluminationdirection.Theshadows\nappearatsurfacepointsthatcannotseethelightsource.PhotobyMikeLinksvayer(mlinksva\nonflickr).\nastrong,ifambiguous,cuetotheshapeofanobject,andfromtheretoitsidentity. Peopleare\nusually able to distinguish the three main causes of varying brightness and reverse-engineer\nthe object\u2019s properties. Thefirstcause is overall intensity ofthelight. Eventhough awhite\nOVERALLINTENSITY\nobject inshadow maybelessbright thanablackobject indirectsunlight, theeyecandistin-\nguishrelativebrightnesswell,andperceivethewhiteobjectaswhite. Second,differentpoints\nin the scene may reflect more or less of the light. Usually, the result is that people perceive\nREFLECT\nthese points aslighterordarker, andsoseetexture ormarkings ontheobject. Third, surface\npatches facing thelightarebrighter thansurface patches tilted awayfrom thelight, aneffect\nknown as shading. Typically, people can tell that this shading comes from the geometry of\nSHADING\ntheobject, butsometimesgetshading andmarkingsmixedup. Forexample, astreakofdark\nmakeupunderacheekbonewilloftenlooklikeashadingeffect,makingthefacelookthinner.\nDIFFUSE Most surfaces reflect light by a process of diffuse reflection. Diffuse reflection scat-\nREFLECTION\nterslightevenlyacross thedirections leaving asurface, sothebrightness ofadiffusesurface\ndoesn\u2019t depend ontheviewingdirection. Mostcloth, paints, roughwoodensurfaces, vegeta-\ntion, and rough stone are diffuse. Mirrors are not diffuse, because what you see depends on\nthe direction in which you look at the mirror. The behavior of a perfect mirror is known as\nSPECULAR specularreflection. Somesurfaces\u2014such asbrushed metal, plastic, orawetfloor\u2014display\nREFLECTION\nsmallpatcheswherespecularreflection hasoccurred, calledspecularities. Theseareeasyto\nSPECULARITIES\nidentify,becausetheyaresmallandbright(Figure24.4). Foralmostallpurposes,itisenough\ntomodelallsurfaces asbeingdiffusewithspecularities. 934 Chapter 24. Perception\n\u03b8\n\u03b8\nA B\nFigure24.5 Twosurfacepatchesareilluminatedbyadistantpointsource,whoseraysare\nshown as gray arrowheads. Patch A is tilted away from the source (\u03b8 is close to 900) and\ncollectslessenergy,becauseitcutsfewerlightraysperunitsurfacearea. PatchB,facingthe\nsource(\u03b8iscloseto00),collectsmoreenergy.\nThemainsourceofillumination outsideisthesun,whoseraysalltravelparalleltoone\nDISTANTPOINT another. Wemodel this behavior asa distantpointlight source. Thisisthemostimportant\nLIGHTSOURCE\nmodel of lighting, and is quite effective for indoor scenes as well as outdoor scenes. The\namountoflightcollectedbyasurfacepatchinthismodeldependsontheangle\u03b8betweenthe\nillumination direction andthenormaltothesurface.\nA diffuse surface patch illuminated by a distant point light source will reflect some\nfraction of the light it collects; this fraction is called the diffuse albedo. White paper and\nDIFFUSEALBEDO\nsnowhaveahighalbedo,about0.90,whereasflatblackvelvetandcharcoalhavealowalbedo\nof about 0.05 (which means that 95% of the incoming light is absorbed within the fibers of\nLAMBERT\u2019SCOSINE thevelvetortheporesofthecharcoal). Lambert\u2019scosinelawstatesthatthebrightness ofa\nLAW\ndiffusepatchisgivenby\nI = \u03c1I cos\u03b8 ,\n0\nwhere\u03c1isthediffusealbedo,I istheintensityofthelightsourceand\u03b8istheanglebetween\n0\nthe light source direction and the surface normal (see Figure 24.5). Lampert\u2019s law predicts\nbright image pixels come from surface patches that face the light directly and dark pixels\ncome from patches that see the light only tangentially, so that the shading on a surface pro-\nvides some shape information. We explore this cue in Section 24.4.5. If the surface is not\nreached bythelight source, thenitisin shadow. Shadowsareveryseldom auniform black,\nSHADOW\nbecause the shadowed surface receives some light from other sources. Outdoors, the most\nimportant such source is the sky, which is quite bright. Indoors, light reflected from other\nsurfaces illuminates shadowed patches. These interreflections can have a significant effect\nINTERREFLECTIONS\non the brightness of other surfaces, too. These effects are sometimes modeled by adding a\nAMBIENT constant ambientilluminationtermtothepredicted intensity.\nILLUMINATION Section24.2. EarlyImage-Processing Operations 935\n24.1.5 Color\nFruit is a bribe that a tree offers to animals to carry its seeds around. Trees have evolved to\nhave fruit that turns red oryellow when ripe, and animals have evolved todetect these color\nchanges. Light arriving at the eye has different amounts of energy at different wavelengths;\nthiscanberepresented byaspectralenergydensityfunction. Humaneyesrespondtolightin\nthe 380\u2013750nm wavelength region, with three different types of color receptor cells, which\nhave peakreceptiveness at420mm (blue), 540nm (green), and 570nm (red). Thehumaneye\ncancaptureonlyasmallfractionofthefullspectralenergy densityfunction\u2014butitisenough\ntotellwhenthefruitisripe.\nPRINCIPLEOF Theprincipleoftrichromacystatesthatforanyspectralenergydensity,nomatterhow\nTRICHROMACY\ncomplicated,itispossibletoconstructanotherspectralenergydensityconsistingofamixture\nofjustthreecolors\u2014usually red,green,andblue\u2014suchthat ahumancan\u2019ttellthedifference\nbetween the two. That means that our TVs and computer displays can get by with just the\nthree red\/green\/blue (or R\/G\/B) color elements. It makes our computer vision algorithms\neasier, too. Each surface can be modeled with three different albedos for R\/G\/B. Similarly,\neach light source can be modeled with three R\/G\/B intensities. We then apply Lambert\u2019s\ncosine law to each to get three R\/G\/B pixel values. This model predicts, correctly, that the\nsame surface will produce different colored image patches under different-colored lights. In\nfact,humanobserversarequitegoodatignoringtheeffects ofdifferentcoloredlightsandare\nabletoestimatethecolorofthesurfaceunderwhitelight,aneffectknownascolorconstancy.\nCOLORCONSTANCY\nQuiteaccurate colorconstancy algorithms arenowavailable; simpleversions showupinthe\n\u201cauto white balance\u201d function of your camera. Note that if we wanted to build a camera for\nmantis shrimp, we would need 12 different pixel colors, corresponding to the 12 types of\ncolorreceptorsofthecrustacean.\n24.2 EARLY IMAGE-PROCESSING OPERATIONS\nWehave seen how light reflects off objects in the scene to form an image consisting of, say,\nfivemillion 3-byte pixels. With all sensors there will be noise in the image, and in any case\nthereisalotofdatatodealwith. Sohowdowegetstartedonanalyzing thisdata?\nInthis section wewillstudy threeuseful image-processing operations: edge detection,\ntexture analysis, and computation of optical flow. These are called \u201cearly\u201d or \u201clow-level\u201d\noperations because they are the first in a pipeline of operations. Early vision operations are\ncharacterized by their local nature (they can be carried out in one part of the image without\nregard for anything more than a few pixels away) and by their lack of knowledge: we can\nperform these operations without consideration of the objects that might be present in the\nscene. This makes the low-level operations good candidates for implementation in parallel\nhardware\u2014either in a graphics processor unit (GPU) or an eye. We will then look at one\nmid-leveloperation: segmenting theimageintoregions. 936 Chapter 24. Perception\n1\n2 2\n1\n1\n3A\n4\nFigure 24.6 Different kinds of edges: (1) depth discontinuities; (2) surface orientation\ndiscontinuities;(3)reflectancediscontinuities;(4)illuminationdiscontinuities(shadows).\n24.2.1 Edgedetection\nEdges are straight lines or curves in the image plane across which there is a \u201csignificant\u201d\nEDGE\nchange in image brightness. The goal of edge detection is to abstract away from the messy,\nmultimegabyte imageandtowardamorecompact, abstract representation, asinFigure24.6.\nThe motivation is that edge contours in the image correspond to important scene contours.\nIn the figure we have three examples of depth discontinuity, labeled 1; two surface-normal\ndiscontinuities, labeled 2; a reflectance discontinuity, labeled 3; and an illumination discon-\ntinuity (shadow), labeled 4. Edgedetection isconcerned only withthe image, and thus does\nnotdistinguish betweenthesedifferenttypesofscenediscontinuities; laterprocessing will.\nFigure 24.7(a) shows an image of a scene containing a stapler resting on a desk, and\n(b) shows the output of an edge-detection algorithm on this image. As you can see, there\nis a difference between the output and an ideal line drawing. There are gaps where no edge\nappears, andthereare\u201cnoise\u201dedgesthatdonotcorrespond toanythingofsignificance inthe\nscene. Laterstagesofprocessing willhavetocorrectfortheseerrors.\nHowdowedetect edgesinanimage? Considertheprofileofimagebrightness along a\none-dimensional cross-section perpendicular to an edge\u2014for example, the one between the\nleftedgeofthedeskandthewall. ItlookssomethinglikewhatisshowninFigure24.8(top).\nEdgescorrespondtolocationsinimageswherethebrightnessundergoesasharpchange,\nso anaive idea would be todifferentiate the image and look forplaces where the magnitude\n(cid:2)\nofthederivative I (x)islarge. Thatalmostworks. InFigure24.8(middle),weseethatthere\nisindeedapeakatx=50,buttherearealsosubsidiarypeaksatotherlocations(e.g.,x=75).\nThese arise because of the presence of noise in the image. If wesmooth the image first, the\nspurious peaksarediminished, asweseeinthebottomofthefigure.\nB Section24.2. EarlyImage-Processing Operations 937\n(a) (b)\nFigure24.7 (a)Photographofastapler. (b)Edgescomputedfrom(a).\n2\n1\n0\n\u22121\n0 10 20 30 40 50 60 70 80 90 100\n1\n0\n\u22121\n0 10 20 30 40 50 60 70 80 90 100\n1\n0\n\u22121\n0 10 20 30 40 50 60 70 80 90 100\nFigure24.8 Top:IntensityprofileI(x)alongaone-dimensionalsectionacrossanedgeat\nx=50. Middle: Thederivativeofintensity,I(cid:5)(x). Largevaluesofthisfunctioncorrespond\nto edges, but the function is noisy. Bottom: The derivative of a smoothed version of the\nintensity,(I\u2217G\u03c3)(cid:5),whichcanbecomputedinonestepastheconvolutionI\u2217G(cid:5) \u03c3. Thenoisy\ncandidateedgeatx=75hasdisappeared.\nThe measurement of brightness at a pixel in a CCD camera is based on a physical\nprocess involving the absorption of photons and the release of electrons; inevitably there\nwill be statistical fluctuations of the measurement\u2014noise. The noise can be modeled with 938 Chapter 24. Perception\na Gaussian probability distribution, with each pixel independent of the others. One way to\nsmooth animage istoassign toeach pixel theaverage ofits neighbors. Thistends tocancel\noutextremevalues. Buthowmanyneighborsshouldweconsider\u2014onepixelaway,ortwo,or\nmore? Onegoodanswerisaweighted average thatweights thenearest pixelsthemost,then\ngradually decreases the weight for more distant pixels. The Gaussian filter does just that.\nGAUSSIANFILTER\n(UsersofPhotoshoprecognize thisastheGaussianbluroperation.) RecallthattheGaussian\nfunction withstandard deviation \u03c3 andmean0is\nN (x) = \u221a1 e\u2212x2\/2\u03c32 inonedimension, or\n\u03c3\n2\u03c0\u03c3\nN (x,y) = 1 e\u2212(x2+y2)\/2\u03c32 intwodimensions.\n\u03c3 2\u03c0\u03c32\nThe application of the Gaussian filter replaces the intensity I(x ,y ) with the sum, over all\n0 0\n(x,y) pixels, of I(x,y)N (d), where d is the distance from (x ,y ) to (x,y). This kind of\n\u03c3 0 0\nweighted sum isso common that there is aspecial name and notation forit. Wesay that the\nfunction histheconvolution oftwofunctions f andg (denoted f \u2217g)ifwehave\nCONVOLUTION\n(cid:12)+\u221e\nh(x) = (f \u2217g)(x) = f(u)g(x\u2212u) inonedimension, or\nu=\u2212\u221e\n(cid:12)+\u221e (cid:12)+\u221e\nh(x,y) = (f \u2217g)(x,y) = f(u,v)g(x\u2212u,y\u2212v) intwo.\nu=\u2212\u221ev=\u2212\u221e\nSothesmoothingfunctionisachievedbyconvolving theimagewiththeGaussian,I\u2217N . A\n\u03c3\n\u03c3of1pixelisenoughtosmoothoverasmallamountofnoise,whereas2pixelswillsmootha\nlargeramount,butatthelossofsomedetail. BecausetheGaussian\u2019s influencefadesquickly\natadistance, wecanreplace the\u00b1\u221einthesumswith\u00b13\u03c3.\nWecanoptimizethecomputation bycombiningsmoothingandedgefindingintoasin-\ngleoperation. Itisatheoremthatforanyfunctions f andg,thederivativeoftheconvolution,\n(f\n\u2217g)(cid:2)\n, is equal to the convolution with the derivative, f\n\u2217(g(cid:2)\n). So rather than smoothing\ntheimage and then differentiating, wecan just convolve the imagewith thederivative ofthe\n(cid:2)\nsmoothing function, N . We then mark as edges those peaks in the response that are above\n\u03c3\nsomethreshold.\nThereisanatural generalization ofthisalgorithm fromone-dimensional crosssections\ntogeneraltwo-dimensional images. Intwodimensionsedgesmaybeatanyangle\u03b8. Consid-\neringtheimagebrightness asascalarfunction ofthevariables x,y,itsgradient isavector\n(cid:31)\n(cid:13) (cid:14)\n\u2202I\nI\n\u2207I = \u2202x = x .\n\u2202I I\ny\n\u2202y\nEdgescorrespond tolocationsinimageswherethebrightnessundergoes asharpchange, and\nso the magnitude of the gradient, +\u2207I+, should be large at an edge point. Of independent\ninterestisthedirectionofthegradient\n(cid:13) (cid:14)\n\u2207I cos\u03b8\n= .\n+\u2207I+ sin\u03b8\nThisgivesusa\u03b8 = \u03b8(x,y)ateverypixel,whichdefinestheedgeorientation atthatpixel.\nORIENTATION Section24.2. EarlyImage-Processing Operations 939\nAsinonedimension, toformthegradientwedon\u2019tcompute\u2207I,butrather\u2207(I\u2217N ),\n\u03c3\nthe gradient after smoothing the image by convolving it with a Gaussian. And again, the\nshortcut is that this is equivalent to convolving the image with the partial derivatives of a\nGaussian. Oncewehavecomputed thegradient, wecanobtain edges byfinding edge points\nand linking them together. To tell whether a point is an edge point, we must look at other\npoints a small distance forward and back along the direction of the gradient. If the gradient\nmagnitude at one of these points is larger, then we could get a better edge point by shifting\nthe edge curve very slightly. Furthermore, if the gradient magnitude is too small, the point\ncannot be an edge point. So at an edge point, the gradient magnitude is a local maximum\nalongthedirectionofthegradient, andthegradient magnitudeisaboveasuitablethreshold.\nOncewehavemarkededgepixelsbythisalgorithm,thenextstageistolinkthosepixels\nthatbelongtothesameedgecurves. Thiscanbedonebyassumingthatanytwoneighboring\nedgepixelswithconsistent orientations mustbelongtothesameedgecurve.\n24.2.2 Texture\nIn everyday language, texture is the visual feel of a surface\u2014what you see evokes what\nTEXTURE\nthe surface might feel like if you touched it (\u201ctexture\u201d has the same root as \u201ctextile\u201d). In\ncomputational vision, texture refers to a spatially repeating pattern on a surface that can be\nsensedvisually. Examplesincludethepatternofwindowsonabuilding,stitchesonasweater,\nspots on a leopard, blades of grass on a lawn, pebbles on a beach, and people in a stadium.\nSometimes the arrangement is quite periodic, as in the stitches on a sweater; in other cases,\nsuchaspebblesonabeach,theregularity isonlystatistical.\nWhereasbrightnessisapropertyofindividualpixels,theconceptoftexturemakessense\nonly for a multipixel patch. Given such a patch, we could compute the orientation at each\npixel,andthencharacterize thepatchbyahistogram oforientations. Thetextureofbricksin\nawallwould havetwopeaks inthe histogram (one vertical and onehorizontal), whereas the\ntextureofspotsonaleopard\u2019s skinwouldhaveamoreuniform distribution oforientations.\nFigure24.9showsthatorientationsarelargelyinvarianttochangesinillumination. This\nmakes texture an important clue for object recognition, because other clues, such as edges,\ncanyielddifferent resultsindifferent lighting conditions.\nInimagesoftexturedobjects,edgedetectiondoesnotworkaswellasitdoesforsmooth\nobjects. This is because the most important edges can be lost among the texture elements.\nQuiteliterally,wemaymissthetigerforthestripes. Thesolutionistolookfordifferencesin\ntexture properties, just thewaywelook fordifferences in brightness. Apatch on atigerand\napatchonthegrassybackgroundwillhaveverydifferentorientationhistograms,allowingus\ntofindtheboundary curvebetweenthem.\n24.2.3 Opticalflow\nNext, let us consider what happens when we have a video sequence, instead of just a single\nstatic image. When an object in the video is moving, orwhen the camera is moving relative\nto an object, the resulting apparent motion in the image is called optical flow. Optical flow\nOPTICALFLOW\ndescribes the direction and speed of motion of features in the image\u2014the optical flow of a 940 Chapter 24. Perception\n(a) (b)\nFigure24.9 Twoimagesofthesametextureofcrumpledricepaper,withdifferentillumi-\nnationlevels. Thegradientvectorfield(ateveryeighthpixel)isplottedontopofeachone.\nNoticethat, asthe lightgetsdarker,allthegradientvectorsgetshorter. Thevectorsdonot\nrotate,sothegradientorientationsdonotchange.\nvideo of a race car would be measured in pixels per second, not miles per hour. Theoptical\nflow encodes useful information about scene structure. For example, in a video of scenery\ntaken from a moving train, distant objects have slower apparent motion than close objects;\nthus, the rate of apparent motion can tell us something about distance. Optical flow also\nenablesustorecognizeactions. InFigure24.10(a)and(b),weshowtwoframesfromavideo\nof a tennis player. In (c) we display the optical flow vectors computed from these images,\nshowingthattheracketandfrontlegaremovingfastest.\nThe optical flow vector field can be represented at any point (x,y) by its components\nv (x,y)inthexdirectionandv (x,y)intheydirection. Tomeasureopticalflowweneedto\nx y\nfindcorresponding pointsbetween onetimeframeandthenext. Asimple-minded technique\nis based on the fact that image patches around corresponding points have similar intensity\npatterns. Consider a block of pixels centered at pixel p, (x ,y ), at time t . This block\n0 0 0\nof pixels is to be compared with pixel blocks centered at various candidate pixels at (x +\n0\nD ,y + D ) at time t + D . One possible measure of similarity is the sum of squared\nx 0 y 0 t\nSUMOFSQUARED differences(SSD):\nDIFFERENCES (cid:12)\nSSD(D ,D ) = (I(x,y,t)\u2212I(x+D ,y+D ,t+D ))2 .\nx y x y t\n(x,y)\nHere, (x,y) ranges over pixels in the block centered at (x ,y ). We find the (D ,D ) that\n0 0 x y\nminimizes the SSD.The optical flow at (x ,y ) is then (v ,v ) = (D \/D ,D \/D ). Note\n0 0 x y x t y t\nthatforthistowork,thereneedstobesometextureorvariationinthescene. Ifoneislooking\nat a uniform white wall, then the SSD is going to be nearly the same for the different can- Section24.2. EarlyImage-Processing Operations 941\nFigure24.10 Twoframesofavideosequence. Ontherightistheopticalflowfieldcor-\nresponding to the displacement from one frame to the other. Note how the movement of\nthetennisracketandthe frontleg iscapturedbythe directionsof thearrows. (Courtesyof\nThomasBrox.)\ndidate matches, and the algorithm is reduced to making a blind guess. The best-performing\nalgorithms for measuring optical flow rely on a variety of additional constraints when the\nsceneisonlypartially textured.\n24.2.4 Segmentation ofimages\nSegmentationistheprocessofbreakinganimageintoregionsofsimilarpixels. Eachimage\nSEGMENTATION\npixel can be associated with certain visual properties, such as brightness, color, and texture.\nREGIONS\nWithin an object, or a single part of an object, these attributes vary relatively little, whereas\nacross an inter-object boundary there is typically a large change in one or more of these at-\ntributes. Therearetwoapproachestosegmentation,onefocusingondetectingtheboundaries\noftheseregions, andtheotherondetecting theregionsthemselves(Figure24.11).\nAboundary curvepassing throughapixel (x,y)willhaveanorientation \u03b8,sooneway\ntoformalizetheproblem ofdetecting boundarycurvesisasa machinelearningclassification\nproblem. Based on features from a local neighborhood, wewant to compute the probability\nP (x,y,\u03b8)thatindeedthereisaboundarycurveatthatpixelalongthatorientation. Consider\nb\na circular disk centered at (x,y), subdivided into two half disks by adiameter oriented at \u03b8.\nIf there isa boundary at (x,y,\u03b8) the two half disks might be expected to differsignificantly\nintheirbrightness,color,andtexture. Martin,Fowlkes,andMalik(2004)usedfeaturesbased\non differences in histograms of brightness, color, and texture values measured in these two\nhalfdisks, andthen trained aclassifier. Forthistheyused a datasetofnatural images where\nhumanshadmarkedthe\u201cgroundtruth\u201dboundaries, andthegoaloftheclassifierwastomark\nexactlythoseboundaries markedbyhumansandnoothers.\nBoundariesdetectedbythistechniqueturnouttobesignificantlybetterthanthosefound\nusingthesimpleedge-detectiontechniquedescribedpreviously. Butstilltherearetwolimita-\ntions. (1)Theboundarypixelsformedbythresholding P (x,y,\u03b8)arenotguaranteed toform\nb\nclosed curves, sothisapproach doesn\u2019t deliverregions, and (2)thedecision making exploits\nonlylocalcontextanddoesnotuseglobalconsistency constraints. 942 Chapter 24. Perception\n(a) (b) (c) (d)\nFigure24.11 (a)Originalimage. (b)Boundarycontours,wherethehigher thePb value,\nthe darker the contour. (c) Segmentation into regions, correspondingto a fine partition of\ntheimage. Regionsarerenderedintheirmeancolors. (d)Segmentationintoregions,corre-\nspondingtoa coarserpartitionoftheimage,resultinginfewerregions. (CourtesyofPablo\nArbelaez,MichaelMaire,CharlesFowlkes,andJitendraMalik)\nThealternativeapproachisbasedontryingto\u201ccluster\u201dthe pixelsintoregionsbasedon\ntheir brightness, color, and texture. Shi and Malik (2000) set this up as a graph partitioning\nproblem. The nodes of the graph correspond to pixels, and edges to connections between\npixels. TheweightW ontheedgeconnectingapairofpixelsiandjisbasedonhowsimilar\nij\nthetwopixelsareinbrightness, color, texture, etc. Partitionsthatminimizeanormalized cut\ncriterion are then found. Roughly speaking, the criterion for partitioning the graph is to\nminimize the sum of weights of connections across the groups of pixels and maximize the\nsumofweightsofconnections withinthegroups.\nSegmentation based purely on low-level, local attributes such as brightness and color\ncannot be expected to deliver the final correct boundaries of all the objects in the scene. To\nreliably find object boundaries we need high-level knowledge of the likely kinds of objects\ninthescene. Representing thisknowledge isatopicofactiveresearch. Apopularstrategy is\nto produce an over-segmentation of animage, containing hundreds of homogeneous regions\nknown as superpixels. From there, knowledge-based algorithms can take over; they will\nSUPERPIXELS\nfinditeasier todeal withhundreds ofsuperpixels ratherthan millions ofrawpixels. Howto\nexploithigh-level knowledgeofobjects isthesubjectofthenextsection.\n24.3 OBJECT RECOGNITION BY APPEARANCE\nAppearanceisshorthand forwhatanobject tendstolooklike. Someobject categories\u2014for\nAPPEARANCE\nexample, baseballs\u2014vary rather little in appearance; all of the objects in the category look\nabout the same under most circumstances. In this case, we can compute a set of features\ndescribing eachclassofimageslikelytocontain theobject, thentestitwithaclassifier. Section24.3. ObjectRecognition byAppearance 943\nOtherobjectcategories\u2014for example,houses orballetdancers\u2014vary greatly. Ahouse\ncanhavedifferentsize,color,andshapeandcanlookdifferentfromdifferentangles. Adancer\nlooksdifferentineachpose,orwhenthestagelightschange colors. Ausefulabstractionisto\nsaythatsomeobjectsaremadeupoflocalpatternswhichtendtomovearoundwithrespectto\noneanother. Wecanthenfindtheobjectbylookingatlocalhistogramsofdetectorresponses,\nwhichexposewhethersomepartispresent butsuppress thedetailsofwhereitis.\nTesting each class of images with a learned classifier is an important general recipe.\nIt works extremely well for faces looking directly at the camera, because at low resolution\nand underreasonable lighting, allsuch faces look quite similar. Theface isround, and quite\nbrightcomparedtotheeyesockets;thesearedark,becausetheyaresunken,andthemouthis\nadarkslash,asaretheeyebrows. Majorchangesofilluminationcancausesomevariationsin\nthis pattern, but the range of variation is quite manageable. That makes it possible to detect\nface positions in an image that contains faces. Once a computational challenge, this feature\nisnowcommonplace ineveninexpensive digitalcameras.\nForthe moment, we will consider only faces where the nose is oriented vertically; we\nwill deal with rotated faces below. We sweep a round window of fixed size over the image,\ncompute features for it, and present the features to a classifier. This strategy is sometimes\ncalledtheslidingwindow. Featuresneedtoberobusttoshadowsandtochangesinbrightness\nSLIDINGWINDOW\ncausedbyilluminationchanges. Onestrategyistobuildfeaturesoutofgradientorientations.\nAnother is to estimate and correct the illumination in each image window. To find faces of\ndifferent sizes, repeat the sweep over larger or smaller versions of the image. Finally, we\npostprocess theresponses acrossscalesandlocations toproduce thefinalsetofdetections.\nPostprocessing is important, because it is unlikely that we have chosen a window size\nthat is exactly the right size for a face (even if we use multiple sizes). Thus, we will likely\nhave several overlapping windows that each report a match for a face. However, if we use\naclassifierthat can report strength ofresponse (forexample, logistic regression orasupport\nvector machine) we can combine these partial overlapping matches at nearby locations to\nyieldasinglehigh-quality match. Thatgivesusafacedetectorthatcansearchoverlocations\nand scales. To search rotations as well, we use two steps. We train a regression procedure\nto estimate the best orientation of any face present in a window. Now, for each window, we\nestimatetheorientation, reorientthewindow,thentestwhetheraverticalfaceispresentwith\nourclassifier. Allthisyieldsasystemwhosearchitecture issketched inFigure24.12.\nTraining data is quite easily obtained. There are several data sets of marked-up face\nimages, and rotated face windows are easy to build (just rotate a window from a training\ndata set). One trick that is widely used is to take each example window, then produce new\nexamples by changing the orientation of the window, the center of the window, or the scale\nvery slightly. This is an easy way of getting a bigger data set that reflects real images fairly\nwell; the trick usually improves performance significantly. Face detectors built along these\nlinesnowperform verywellforfrontalfaces(sideviewsare harder). 944 Chapter 24. Perception\nNon-maximal\nsuppresion\nImage Responses Detections\nEstimate\norientation\nCorrect Rotate\nFeatures Classifier\nillumination window\nFigure 24.12 Face finding systems vary, but most follow the architecture illustrated in\ntwo parts here. On the top, we go from images to responses, then apply non-maximum\nsuppressionto findthestrongestlocalresponse. Theresponsesareobtainedbytheprocess\nillustratedonthebottom. Wesweepawindowoffixedsizeoverlargerandsmallerversions\nof the image, so as to find smaller or larger faces, respectively. The illumination in the\nwindow is corrected, and then a regression engine (quite often, a neural net) predicts the\norientationofthe face. Thewindowiscorrectedto thisorientationandthen presentedto a\nclassifier. Classifieroutputsarethenpostprocessedtoensurethatonlyonefaceisplacedat\neachlocationintheimage.\n24.3.1 Complexappearance andpattern elements\nMany objects produce much more complex patterns than faces do. This is because several\neffectscanmovefeaturesaroundinanimageoftheobject. Effectsinclude (Figure24.13)\n\u2022 Foreshortening,whichcausesapatternviewedataslanttobesignificantly distorted.\n\u2022 Aspect, which causes objects to look different when seen from different directions.\nEvenassimpleanobjectasadoughnut hasseveralaspects;seenfromtheside,itlooks\nlikeaflattenedoval,butfromaboveitisanannulus.\n\u2022 Occlusion, where some parts are hidden from some viewing directions. Objects can\nocclude one another, or parts of an object can occlude other parts, an effect known as\nself-occlusion.\n\u2022 Deformation, where internal degrees of freedom of the object change its appearance.\nForexample,peoplecanmovetheirarmsandlegsaround,generatingaverywiderange\nofdifferentbodyconfigurations.\nHowever, our recipe of searching across location and scale can still work. This is because\nsome structure will be present in the images produced by the object. Forexample, apicture\nof a car is likely to show some of headlights, doors, wheels, windows, and hubcaps, though\ntheymaybeinsomewhatdifferentarrangementsindifferentpictures. Thissuggestsmodeling\nobjectswithpatternelements\u2014collectionsofparts. Thesepatternelementsmaymovearound Section24.3. ObjectRecognition byAppearance 945\nForeshortening Aspect\nOcclusion Deformation\nFigure 24.13 Sources of appearancevariation. First, elements can foreshorten, like the\ncircular patch on the top left. This patch is viewed at a slant, and so is elliptical in the\nimage.Second,objectsviewedfromdifferentdirectionscanchangeshapequitedramatically,\naphenomenonknownasaspect. On thetoprightarethreedifferentaspectsofadoughnut.\nOcclusion causes the handle of the mug on the bottom left to disappear when the mug is\nrotated. In this case, because the body and handle belong to the same mug, we have self-\nocclusion.Finally,onthebottomright,someobjectscandeformdramatically.\nwithrespect toone another, but ifmostofthe pattern elements arepresent inabout theright\nplace,thentheobjectispresent. Anobjectrecognizeristhenacollectionoffeaturesthatcan\ntellwhetherthepatternelementsarepresent, andwhethertheyareinabouttherightplace.\nThe most obvious approach is to represent the image window with a histogram of the\npattern elements that appear there. This approach does not work particularly well, because\ntoo many patterns get confused with one another. For example, if the pattern elements are\ncolor pixels, the French, UK, and Netherlands flags will get confused because they have\napproximately the same color histograms, though the colors are arranged in very different\nways. Quite simple modifications of histograms yield very useful features. The trick is to\npreserve some spatial detail in the representation; for example, headlights tend to be at the\nfront of a car and wheels tend to be at the bottom. Histogram-based features have been\nsuccessful inawidevarietyofrecognition applications; wewillsurveypedestrian detection.\n24.3.2 Pedestriandetection withHOGfeatures\nTheWorldBankestimatesthateachyearcaraccidentskillabout1.2millionpeople,ofwhom\nabouttwothirdsarepedestrians. Thismeansthatdetecting pedestrians isanimportantappli-\ncation problem, because cars that can automatically detect and avoid pedestrians might save\nmany lives. Pedestrians wear many different kinds of clothing and appear in many different\nconfigurations, but, at relatively low resolution, pedestrians can have a fairly characteristic\nappearance. The most usual cases are lateral or frontal views of a walk. In these cases, 946 Chapter 24. Perception\nImage Orientation Positive Negative\nhistograms components components\nFigure 24.14 Local orientation histograms are a powerful feature for recognizing even\nquitecomplexobjects. Ontheleft,animageofapedestrian. Onthecenterleft,localorien-\ntation histogramsforpatches. We then applya classifier such as a supportvectormachine\ntofindtheweightsforeachhistogramthatbestseparatethepositiveexamplesofpedestrians\nfromnon-pedestrians. Weseethatthepositivelyweightedcomponentslookliketheoutline\nofa person. Thenegativecomponentsarelessclear;theyrepresentallthepatternsthatare\nnotpedestrians.FigurefromDalalandTriggs(2005)(cid:2)c IEEE.\nwe see either a \u201clollipop\u201d shape \u2014 the torso is wider than the legs, which are together in\nthe stance phase of the walk \u2014 or a \u201cscissor\u201d shape \u2014 where the legs are swinging in the\nwalk. Weexpect to see someevidence ofarms and legs, and the curve around theshoulders\nand head also tends to visible and quite distinctive. This means that, with a careful feature\nconstruction, wecanbuildausefulmoving-window pedestrian detector.\nThere isn\u2019t always a strong contrast between the pedestrian and the background, so it\nisbettertouseorientations thanedgestorepresent theimagewindow. Pedestrians canmove\ntheir arms and legs around, so we should use a histogram to suppress some spatial detail in\nthefeature. Webreakupthewindowintocells,whichcouldoverlap,andbuildanorientation\nhistogram in each cell. Doing so will produce a feature that can tell whether the head-and-\nshoulders curve is at the top of the window orat the bottom, but will not change if the head\nmovesslightly.\nOne further trick is required to make a good feature. Because orientation features are\nnot affected by illumination brightness, we cannot treat high-contrast edges specially. This\nmeansthatthedistinctive curvesontheboundary ofapedestrian aretreatedinthesameway\nas fine texture detail in clothing or in the background, and so the signal may be submerged\ninnoise. Wecanrecovercontrastinformationbycountinggradientorientations withweights\nthat reflect how significant a gradient is compared to other gradients in the same cell. We\nwill write || \u2207Ix || for the gradient magnitude at point x in the image, write C for the cell\nwhosehistogram wewishtocompute, and write wx,C fortheweight thatwewilluseforthe Section24.4. Reconstructing the3DWorld 947\nFigure 24.15 Another example of object recognition, this one using the SIFT feature\n(ScaleInvariantFeatureTransform),anearlierversionof theHOGfeature. Ontheleft,im-\nagesofashoeandatelephonethatserveasobjectmodels.Inthecenter,atestimage.Onthe\nright,theshoeandthetelephonehavebeendetectedby: findingpointsintheimagewhose\nSIFTfeaturedescriptionsmatchamodel;computinganestimateofposeofthemodel;and\nverifyingthatestimate. Astrongmatchisusuallyverifiedwithrarefalsepositives. Images\nfromLowe(1999)(cid:2)c IEEE.\norientation atxforthiscell. Anaturalchoiceofweightis\nwx,C = (cid:2) u| \u2208|\u2207\nC\nI ||x \u2207||\nIu||\n.\nThis compares the gradient magnitude to others in the cell, so gradients that are large com-\npared to their neighbors get a large weight. The resulting feature is usually called a HOG\nfeature(forHistogramOfGradientorientations).\nHOGFEATURE\nThis feature construction is the main way in which pedestrian detection differs from\nfacedetection. Otherwise,buildingapedestrian detector isverylikebuilding afacedetector.\nThe detector sweeps a window across the image, computes features for that window, then\npresents it to a classifier. Non-maximum suppression needs to be applied to the output. In\nmostapplications, thescale andorientation oftypical pedestrians isknown. Forexample, in\ndriving applications in which a camera is fixedto the car, we expect to view mainly vertical\npedestrians, and we are interested only in nearby pedestrians. Several pedestrian data sets\nhavebeenpublished, andthesecanbeusedfortrainingtheclassifier.\nPedestrians are not the only type of object we can detect. In Figure 24.15 we see that\nsimilartechniques canbeusedtofindavarietyofobjectsindifferent contexts.\n24.4 RECONSTRUCTING THE 3D WORLD\nIn this section we show how to go from the two-dimensional image to a three-dimensional\nrepresentation of the scene. The fundamental question is this: Given that all points in the\nscenethatfallalongaraytothepinholeareprojected tothe samepointintheimage,howdo\nwerecoverthree-dimensional information? Twoideascometoourrescue: 948 Chapter 24. Perception\n\u2022 Ifwehavetwo(ormore)imagesfrom different camerapositions, thenwecantriangu-\nlatetofindtheposition ofapointinthescene.\n\u2022 We can exploit background knowledge about the physical scene that gave rise to the\nimage. Givenanobject modelP(Scene)andarendering model P(Image|Scene),we\ncancomputeaposteriordistribution P(Scene|Image).\nThereisasyetnosingle unifiedtheoryforscene reconstruction. Wesurveyeightcommonly\nusedvisualcues: motion,binocularstereopsis, multipleviews,texture,shading,contour,\nandfamiliarobjects.\n24.4.1 Motionparallax\nIfthecameramovesrelativetothethree-dimensional scene, theresulting apparent motionin\ntheimage, optical flow,canbeasource ofinformation forboth themovementofthecamera\nand depth in the scene. To understand this, we state (without proof) an equation that relates\ntheopticalflowtotheviewer\u2019stranslational velocity Tandthedepthinthescene.\nThecomponents oftheopticalflowfieldare\n\u2212T +xT \u2212T +yT\nx z y z\nv (x,y) = , v (x,y) = ,\nx y\nZ(x,y) Z(x,y)\nwhere Z(x,y) is the z-coordinate of the point in the scene corresponding to the point in the\nimageat(x,y).\nNote that both components of the optical flow, v (x,y) and v (x,y), are zero at the\nx y\nFOCUSOF point x = T \/T ,y = T \/T . This point is called the focus of expansion of the flow\nEXPANSION x z y z\nfield. Suppose we change the origin in the x\u2013y plane to lie at the focus of expansion; then\n(cid:2) (cid:2)\nthe expressions for optical flow take on a particularly simple form. Let (x,y ) be the new\ncoordinates definedbyx(cid:2) = x\u2212T \/T ,y(cid:2) =y\u2212T \/T . Then\nx z y z\n(cid:2) (cid:2)\nxT y T\n(cid:2) (cid:2) z (cid:2) (cid:2) z\nv (x,y ) = , v (x,y )= .\nx Z(x(cid:2),y(cid:2)) y Z(x(cid:2),y(cid:2))\nNotethatthere isascale-factor ambiguity here. Ifthecamerawasmoving twiceasfast, and\neveryobjectinthescenewastwiceasbigandattwicethedistance tothecamera,theoptical\nflowfieldwouldbeexactlythesame. Butwecanstillextractquiteusefulinformation.\n1. Suppose you are a fly trying to land on a wall and you want to know the time-to-\ncontact at the current velocity. This time is given by Z\/T . Note that although the\nz\ninstantaneous optical flow field cannot provide either the distance Z or the velocity\ncomponent T , it can provide the ratio of the two and can therefore be used to control\nz\nthelanding approach. Thereisconsiderable experimental evidence thatmanydifferent\nanimalspeciesexploitthiscue.\n2. Consider two points at depths Z , Z , respectively. We may not know the absolute\n1 2\nvalue of either of these, but by considering the inverse of the ratio of the optical flow\nmagnitudes atthese points, wecandetermine thedepthratio Z \/Z . Thisisthecueof\n1 2\nmotion parallax, one we use when we look out of the side window of a moving car or\ntrainandinferthattheslowermovingpartsofthelandscape arefartheraway. Section24.4. Reconstructing the3DWorld 949\nPerceived object\nLeft image Right image\nRight Left\nDisparity\n(a) (b)\nFigure 24.16 Translating a camera parallel to the image plane causes image features to\nmove in the camera plane. The disparity in positions that results is a cue to depth. If we\nsuperimposeleftandrightimage,asin(b),weseethedisparity.\n24.4.2 Binocularstereopsis\nMost vertebrates have two eyes. This is useful for redundancy in case of a lost eye, but it\nhelps in other ways too. Most prey have eyes on the side of the head to enable a widerfield\nBINOCULAR of vision. Predators have the eyes in the front, enabling them to use binocular stereopsis.\nSTEREOPSIS\nTheidea issimilartomotion parallax, except thatinstead ofusing images overtime, weuse\ntwo (or more) images separated in space. Because a given feature in the scene will be in a\ndifferent place relative to the z-axis of each image plane, if we superpose the two images,\nthere willbeadisparity inthe location oftheimage feature inthe twoimages. Youcan see\nDISPARITY\nthis in Figure 24.16, where the nearest point of the pyramid is shifted to the left in the right\nimageandtotherightintheleftimage.\nNote that to measure disparity we need to solve the correspondence problem, that is,\ndetermine for a point in the left image, the point in the right image that results from the\nprojection of the same scene point. This is analogous to what one has to do in measuring\noptical flow, and the most simple-minded approaches are somewhat similar and based on\ncomparingblocksofpixelsaroundcorrespondingpointsusingthesumofsquareddifferences.\nInpractice,weusemuchmoresophisticatedalgorithms,whichexploitadditionalconstraints.\nAssuming that we can measure disparity, how does this yield information about depth\nin the scene? We will need to work out the geometrical relationship between disparity and\ndepth. First, wewill consider the case when both the eyes (or cameras) are looking forward\nwiththeiropticalaxesparallel. Therelationship oftherightcameratotheleftcameraisthen\njust a displacement along the x-axis by an amount b, the baseline. We can use the optical\nflow equations from the previous section, if we think of this as resulting from a translation 950 Chapter 24. Perception\nLeft\neye \u03b4\u03b8\/2\nP\nL\n\u03b8\nb P P\n0\nP\nR\nRight\neye Z \u03b4Z\nFigure24.17 Therelationbetweendisparityanddepthinstereopsis. Thecentersofpro-\njectionofthetwoeyesarebapart,andtheopticalaxesintersectatthefixationpointP . The\n0\npoint P in the scene projects to points PL and PR in the two eyes. In angular terms, the\ndisparitybetweentheseis\u03b4\u03b8. Seetext.\nvector T acting for time \u03b4t, with T = b\/\u03b4t and T = T = 0. The horizontal and vertical\nx y z\ndisparityaregivenbytheopticalflowcomponents,multipliedbythetimestep\u03b4t,H = v \u03b4t,\nx\nV = v \u03b4t. Carryingoutthesubstitutions, wegettheresultthat H = b\/Z,V = 0. Inwords,\ny\nthe horizontal disparity is equal to the ratio of the baseline to the depth, and the vertical\ndisparity iszero. Giventhatweknow b,wecanmeasure H andrecoverthedepth Z.\nUnder normal viewing conditions, humans fixate; that is, there is some point in the\nFIXATE\nsceneatwhichtheopticalaxesofthetwoeyesintersect. Figure24.17showstwoeyesfixated\nat a point P , which is at a distance Z from the midpoint of the eyes. For convenience,\n0\nwe will compute the angular disparity, measured in radians. The disparity at the point of\nfixation P is zero. For some other point P in the scene that is \u03b4Z farther away, we can\n0\ncompute the angular displacements of the left and right images of P, which wewill call P\nL\nand P , respectively. If each of these is displaced by an angle \u03b4\u03b8\/2 relative to P , then the\nR 0\ndisplacement betweenP andP ,whichisthedisparityofP,isjust\u03b4\u03b8. FromFigure24.17,\nL R\ntan\u03b8 = b\/2 andtan(\u03b8\u2212\u03b4\u03b8\/2) = b\/2 ,butforsmallangles, tan\u03b8 \u2248 \u03b8,so\nZ Z+\u03b4Z\nb\/2 b\/2 b\u03b4Z\n\u03b4\u03b8\/2 = \u2212 \u2248\nZ Z +\u03b4Z 2Z2\nand,sincetheactualdisparity is\u03b4\u03b8,wehave\nb\u03b4Z\ndisparity = .\nZ2\nInhumans,b(thebaselinedistancebetweentheeyes)isabout6cm. SupposethatZ isabout\nBASELINE\n100 cm. If the smallest detectable \u03b4\u03b8 (corresponding to the pixel size) is about 5 seconds\nof arc, this gives a \u03b4Z of 0.4 mm. For Z = 30 cm, we get the impressively small value\n\u03b4Z = 0.036 mm. Thatis, atadistance of30cm,humans candiscriminate depths thatdiffer\nbyaslittleas0.036mm,enabling ustothreadneedlesandthelike. Section24.4. Reconstructing the3DWorld 951\nFigure24.18 (a)Fourframesfroma videosequencein whichthe camerais movedand\nrotatedrelativetotheobject.(b)Thefirstframeofthesequence,annotatedwithsmallboxes\nhighlightingthefeaturesfoundbythefeaturedetector.(CourtesyofCarloTomasi.)\n24.4.3 Multipleviews\nShapefromopticalfloworbinoculardisparityaretwoinstancesofamoregeneralframework,\nthatofexploiting multipleviewsforrecovering depth. Incomputervision, thereisnoreason\nforustoberestrictedtodifferentialmotionortoonlyusetwocamerasconvergingatafixation\npoint. Therefore, techniques have been developed that exploit the information available in\nmultiple views, even from hundreds or thousands of cameras. Algorithmically, there are\nthreesubproblems thatneedtobesolved:\n\u2022 The correspondence problem, i.e., identifying features in the different images that are\nprojections ofthesamefeatureinthethree-dimensional world.\n\u2022 The relative orientation problem, i.e., determining the transformation (rotation and\ntranslation) betweenthecoordinate systemsfixedtothedifferent cameras.\n\u2022 Thedepthestimationproblem,i.e.,determiningthedepthsofvariouspointsintheworld\nforwhichimageplaneprojections wereavailable inatleast twoviews\nThe development of robust matching procedures for the correspondence problem, accompa-\nniedbynumericallystablealgorithmsforsolvingforrelativeorientations andscenedepth,is\noneofthesuccessstoriesofcomputervision. ResultsfromonesuchapproachduetoTomasi\nandKanade(1992)areshowninFigures24.18and24.19.\n24.4.4 Texture\nEarlierwesaw how texture wasused forsegmenting objects. Itcan also beused toestimate\ndistances. InFigure 24.20weseethatahomogeneous texture inthescene results invarying\ntextureelements, ortexels, intheimage. Allthepaving tilesin(a)areidentical inthescene.\nTEXEL\nTheyappeardifferent intheimagefortworeasons: 952 Chapter 24. Perception\n(a) (b)\nFigure24.19 (a)Three-dimensionalreconstructionofthelocationsoftheimagefeatures\ninFigure24.18,shownfromabove.(b)Therealhouse,takenfromthesameposition.\n1. Differencesinthedistancesofthetexelsfromthecamera. Distantobjectsappearsmaller\nbyascaling factorof1\/Z.\n2. Differences in the foreshortening of the texels. If all the texels are in the ground plane\nthen distance ones are viewed at an angle that is farther off the perpendicular, and so\nare more foreshortened. The magnitude of the foreshortening effect is proportional to\ncos\u03c3, where \u03c3 is the slant, the angle between the Z-axis and n, the surface normal to\nthetexel.\nResearchers have developed various algorithms that try to exploit the variation in the\nappearance of the projected texels as a basis for determining surface normals. However, the\naccuracy and applicability of these algorithms is not anywhere as general as those based on\nusingmultipleviews.\n24.4.5 Shading\nShading\u2014variation inthe intensity oflight received from different portions ofasurface ina\nscene\u2014is determined by the geometry of the scene and by the reflectance properties of the\nsurfaces. In computer graphics, the objective is to compute the image brightness I(x,y),\ngiven the scene geometry and reflectance properties of the objects in the scene. Computer\nvisionaimstoinverttheprocess\u2014that is,torecoverthegeometryandreflectance properties,\ngiven the image brightness I(x,y). This has proved to be difficult to do in anything but the\nsimplestcases.\nFrom the physical model of section 24.1.4, we know that if a surface normal points\ntoward the light source, the surface is brighter, and if it points away, the surface is darker.\nWe cannot conclude that a dark patch has its normal pointing away from the light; instead,\nit could have low albedo. Generally, albedo changes quite quickly in images, and shading Section24.4. Reconstructing the3DWorld 953\n(a) (b)\nFigure24.20 (a)Atexturedscene.Assumingthattherealtextureisuniformallowsrecov-\neryofthesurfaceorientation. Thecomputedsurfaceorientationisindicatedbyoverlayinga\nblackcircleandpointer,transformedasifthecirclewerepaintedonthesurfaceatthatpoint.\n(b)Recoveryofshapefromtextureforacurvedsurface(whitecircleandpointerthistime).\nImagescourtesyofJitendraMalikandRuthRosenholtz(1994).\nchanges rather slowly, and humans seem to be quite good at using this observation to tell\nwhether low illumination, surface orientation, or albedo caused a surface patch to be dark.\nTo simplify the problem, let us assume that the albedo is known at every surface point. It\nis still difficult to recover the normal, because the image brightness is one measurement but\nthenormalhastwounknownparameters, sowecannot simplysolveforthenormal. Thekey\nto this situation seems to be that nearby normals will be similar, because most surfaces are\nsmooth\u2014they donothavesharpchanges.\nTherealdifficultycomesindealingwithinterreflections. Ifweconsideratypicalindoor\nscene, such as the objects inside an office, surfaces are illuminated not only by the light\nsources, but also by the light reflected from other surfaces in the scene that effectively serve\nassecondary light sources. These mutual illumination effects are quite significant and make\nitquitedifficulttopredicttherelationshipbetweenthenormalandtheimagebrightness. Two\nsurface patches with the same normal might have quite different brightnesses, because one\nreceives light reflected from a large white wall and the other faces only a dark bookcase.\nDespite these difficulties, the problem is important. Humans seem to be able to ignore the\neffects of interreflections and get a useful perception of shape from shading, but we know\nfrustratingly littleaboutalgorithms todothis.\n24.4.6 Contour\nWhen we look at a line drawing, such as Figure 24.21, we get a vivid perception of three-\ndimensional shapeandlayout. How? Itisacombination ofrecognition offamiliarobjectsin\nthesceneandtheapplication ofgeneric constraints suchasthefollowing:\n\u2022 Occluding contours, such as the outlines of the hills. Oneside ofthe contour is nearer\ntotheviewer, theotherside isfarther away. Features suchaslocal convexity andsym- 954 Chapter 24. Perception\nFigure24.21 Anevocativelinedrawing.(CourtesyofIshaMalik.)\nmetryprovidecuestosolvingthefigure-groundproblem\u2014assigning whichsideofthe\nFIGURE-GROUND\ncontour is figure (nearer), and which is ground (farther). At an occluding contour, the\nlineofsightistangential tothesurfaceinthescene.\n\u2022 T-junctions. When one object occludes another, the contour of the farther object is\ninterrupted,assumingthatthenearerobjectisopaque. AT-junctionresultsintheimage.\n\u2022 Position on the ground plane. Humans, like many other terrestrial animals, are very\nofteninascenethatcontainsagroundplane,withvariousobjectsatdifferentlocations\nGROUNDPLANE\nonthis plane. Because ofgravity, typical objects don\u2019t floatinairbut aresupported by\nthisgroundplane,andwecanexploittheveryspecialgeometryofthisviewingscenario.\nLet us work out the projection of objects of different heights and at different loca-\ntions on the ground plane. Suppose that the eye, or camera, is at a height h above\nc\ntheground plane. Consider an object of height \u03b4Y resting on the ground plane, whose\nbottom is at (X,\u2212h ,Z) and top is at (X,\u03b4Y \u2212 h ,Z). The bottom projects to the\nc c\nimagepoint(fX\/Z,\u2212fh \/Z)andthetopto(fX\/Z,f(\u03b4Y \u2212h )\/Z). Thebottomsof\nc c\nnearerobjects(smallZ)projecttopointslowerintheimageplane;fartherobjects have\nbottomsclosertothehorizon.\n24.4.7 Objects and thegeometric structure ofscenes\nA typical adult human head is about 9 inches long. This means that for someone who is 43\nfeetaway,theanglesubtendedbytheheadatthecamerais1degree. Ifweseeapersonwhose\nhead appears to subtend just half a degree, Bayesian inference suggests we are looking at a\nnormal person who is 86 feet away, rather than someone with a half-size head. This line of\nreasoning suppliesuswithamethodtochecktheresultsofapedestrian detector, aswellasa\nmethodtoestimatethedistancetoanobject. Forexample,allpedestrians areaboutthesame\nheight,andtheytendtostandonagroundplane. Ifweknowwherethehorizonisinanimage,\nwecanrankpedestrians bydistancetothecamera. Thisworks becauseweknowwheretheir Section24.4. Reconstructing the3DWorld 955\nImage plane\nHorizon\nGround plane\nC\nC B\nB\nA\nA\nFigure 24.22 In an image of people standing on a ground plane, the people whose feet\nareclosertothehorizonintheimagemustbefartheraway(topdrawing). Thismeansthey\nmustlooksmallerintheimage(leftlowerdrawing).Thismeansthatthesizeandlocationof\nrealpedestriansinanimagedependupononeanotherandonthelocationofthehorizon.To\nexploitthis, we need to identify the groundplane, which is done using shape-from-texture\nmethods.Fromthisinformation,andfromsomelikelypedestrians,wecanrecoverahorizon\nasshowninthecenterimage.Ontheright,acceptablepedestrianboxesgiventhisgeometric\ncontext. Noticethatpedestrianswhoarehigherinthescenemustbesmaller. Iftheyarenot,\nthentheyarefalsepositives.ImagesfromHoiemetal.(2008)(cid:2)c IEEE.\nfeet are, and pedestrians whose feet are closer to the horizon in the image are farther away\nfromthecamera(Figure24.22). Pedestrianswhoarefarther awayfromthecameramustalso\nbesmallerintheimage. Thismeanswecanruleoutsomedetectorresponses\u2014ifadetector\nfinds a pedestrian who is large in the image and whose feet are close to the horizon, it has\nfound an enormous pedestrian; these don\u2019t exist, so the detector is wrong. In fact, many or\nmostimagewindowsarenotacceptable pedestrian windows,andneednotevenbepresented\ntothedetector.\nThere are several strategies for finding the horizon, including searching for a roughly\nhorizontal line with a lot of blue above it, and using surface orientation estimates obtained\nfrom texture deformation. A more elegant strategy exploits the reverse of our geometric\nconstraints. Areasonablyreliablepedestriandetectoris capableofproducingestimatesofthe\nhorizon, if there are several pedestrians in the scene at different distances from the camera.\nThisisbecause therelativescaling ofthepedestrians isacuetowherethehorizon is. Sowe\ncanextractahorizonestimatefromthedetector,thenusethisestimatetoprunethepedestrian\ndetector\u2019s mistakes. 956 Chapter 24. Perception\nIftheobjectisfamiliar,wecanestimatemorethanjustthedistancetoit,becausewhatit\nlookslikeintheimagedependsverystronglyonitspose,i.e.,itspositionandorientationwith\nrespecttotheviewer. Thishasmanyapplications. Forinstance,inanindustrialmanipulation\ntask, the robot arm cannot pick up an object until the pose is known. In the case of rigid\nobjects, whether three-dimensional ortwo-dimensional, this problem hasasimple and well-\ndefinedsolution basedonthealignmentmethod,whichwenowdevelop.\nALIGNMENTMETHOD\nThe object is represented by M features or distinguished points m ,m ,...,m in\n1 2 M\nthree-dimensional space\u2014perhaps the vertices of a polyhedral object. These are measured\nin some coordinate system that is natural for the object. The points are then subjected to\nan unknown three-dimensional rotation R, followed by translation by an unknown amount t\nand then projection to give rise to image feature points p ,p ,...,p on the image plane.\n1 2 N\nIn general, N (cid:7)= M, because some model points may be occluded, and the feature detector\ncouldmisssomefeatures(orinventfalseonesduetonoise). Wecanexpressthisas\np = \u03a0(Rm +t) = Q(m )\ni i i\nfor a three-dimensional model point m and the corresponding image point p . Here, R\ni i\nis a rotation matrix, t is a translation, and \u03a0 denotes perspective projection or one of its\napproximations, suchasscaled orthographic projection. Thenetresultisatransformation Q\nthat will bring the model point m into alignment with the image point p . Although we do\ni i\nnotknowQinitially,wedoknow(forrigidobjects)thatQmustbethesameforallthemodel\npoints.\nWecansolveforQ,giventhethree-dimensional coordinates ofthreemodelpoints and\ntheirtwo-dimensional projections. Theintuition is asfollows: wecanwrite downequations\nrelating the coordinates of p to those of m . In these equations, the unknown quantities\ni i\ncorrespond totheparametersoftherotationmatrix Randthetranslation vectort. Ifwehave\nenough equations, we ought to be able to solve for Q. We will not give a proof here; we\nmerelystatethefollowingresult:\nGiven three noncollinear points m , m , and m in the model, and their scaled\n1 2 3\northographic projections p , p , and p on the image plane, there exist exactly\n1 2 3\ntwotransformationsfromthethree-dimensionalmodelcoordinateframetoatwo-\ndimensional imagecoordinate frame.\nThesetransformationsarerelatedbyareflectionaroundthe imageplaneandcanbecomputed\nby asimple closed-form solution. If wecould identify the corresponding model features for\nthreefeatures intheimage,wecouldcompute Q,theposeoftheobject.\nLetusspecifypositionandorientationinmathematicalterms. ThepositionofapointP\ninthesceneischaracterizedbythreenumbers,the (X,Y,Z)coordinatesofP inacoordinate\nframe with its origin at the pinhole and the Z-axis along the optical axis (Figure 24.2 on\npage 931). What we have available is the perspective projection (x,y) of the point in the\nimage. This specifies the ray from the pinhole along which P lies; what we do not know is\nthedistance. Theterm\u201corientation\u201d couldbeusedintwosenses:\n1. The orientation of the object as a whole. This can be specified in terms of a three-\ndimensional rotationrelating itscoordinate frametothat ofthecamera. Section24.5. ObjectRecognition fromStructuralInformation 957\n2. Theorientation of thesurface oftheobject atP. Thiscan be specified by anormal\nvector,n\u2014whichisavectorspecifyingthedirectionthatisperpendiculartothesurface.\nOften weexpress the surface orientation using the variables slant and tilt. Slant is the\nSLANT\nanglebetweentheZ-axisandn. TiltistheanglebetweentheX-axisandtheprojection\nTILT\nofnontheimageplane.\nWhen the camera moves relative to an object, both the object\u2019s distance and its orientation\nchange. What is preserved is the shape of the object. If the object is a cube, that fact is\nSHAPE\nnotchanged whentheobjectmoves. Geometershavebeenattempting toformalizeshapefor\ncenturies,thebasicconceptbeingthatshapeiswhatremainsunchangedundersomegroupof\ntransformations\u2014for example,combinations ofrotations andtranslations. Thedifficultylies\ninfindingarepresentationofglobalshapethatisgeneralenoughtodealwiththewidevariety\nofobjectsintherealworld\u2014notjustsimpleformslikecylinders,cones,andspheres\u2014andyet\ncanberecovered easily fromthevisual input. Theproblem of characterizing the localshape\nof a surface is much better understood. Essentially, this can be done in terms of curvature:\nhowdoesthesurface normalchangeasonemovesindifferentdirections onthesurface? For\na plane, there is no change at all. For a cylinder, if one moves parallel to the axis, there is\nno change, but in the perpendicular direction, the surface normal rotates at a rate inversely\nproportional to the radius of the cylinder, and so on. All this is studied in the subject called\ndifferential geometry.\nTheshape ofanobject isrelevantforsomemanipulation tasks(e.g.,deciding whereto\ngraspanobject), butitsmostsignificant roleisinobjectrecognition, wheregeometric shape\nalongwithcolorandtextureprovidethemostsignificantcuestoenableustoidentifyobjects,\nclassifywhatisintheimageasanexampleofsomeclassonehasseenbefore,andsoon.\n24.5 OBJECT RECOGNITION FROM STRUCTURAL INFORMATION\nPuttingaboxaroundpedestriansinanimagemaywellbeenoughtoavoiddrivingintothem.\nWehaveseenthatwecanfindaboxbypooling theevidence provided byorientations, using\nhistogram methodstosuppress potentially confusing spatial detail. Ifwewanttoknowmore\naboutwhatsomeoneisdoing,wewillneedtoknowwheretheirarms,legs,body,andheadlie\ninthepicture. Individual bodyparts arequitedifficulttodetect ontheirownusingamoving\nwindowmethod,becausetheircolorandtexturecanvarywidelyandbecausetheyareusually\nsmall in images. Often, forearms and shins are as small as two to three pixels wide. Body\nparts do not usually appear on their own, and representing what is connected to what could\nbequitepowerful,becausepartsthatareeasytofindmighttelluswheretolookforpartsthat\naresmallandhardtodetect.\nInferring thelayoutofhumanbodies inpictures isanimportant taskinvision, because\nthe layout of the body often reveals what people are doing. A model called a deformable\nDEFORMABLE templatecantelluswhichconfigurations areacceptable: theelbowcanbendbuttheheadis\nTEMPLATE\nneverjoinedtothefoot. Thesimplestdeformabletemplatemodelofapersonconnectslower\narmstoupperarms,upperarmstothetorso,andsoon. Therearerichermodels: forexample, 958 Chapter 24. Perception\nwe could represent the fact that left and right upper arms tend to have the same color and\ntexture,asdoleftandrightlegs. Theserichermodelsremaindifficulttoworkwith,however.\n24.5.1 The geometry ofbodies: Finding arms andlegs\nFor the moment, we assume that we know what the person\u2019s body parts look like (e.g., we\nknow the color and texture of the person\u2019s clothing). We can model the geometry of the\nbody asatreeofeleven segments (upperandlowerleftandright armsandlegsrespectively,\na torso, a face, and hair on top of the face) each of which is rectangular. We assume that\ntheposition andorientation (pose)oftheleftlowerarmisindependent ofallothersegments\nPOSE\ngiven the pose of the left upper arm; that the pose of the left upper arm is independent of\nall segments given the pose of the torso; and extend these assumptions in the obvious way\nto include the right arm and the legs, the face, and the hair. Such models are often called\n\u201ccardboardpeople\u201dmodels. Themodelformsatree,whichisusuallyrootedatthetorso. We\nwillsearchtheimageforthebestmatchtothiscardboardpersonusinginferencemethodsfor\natree-structured Bayesnet(seeChapter14).\nThere are two criteria for evaluating a configuration. First, an image rectangle should\nlooklikeitssegment. Forthemoment,wewillremainvagueaboutpreciselywhatthatmeans,\nbutweassumewehaveafunction\u03c6 thatscoreshowwellanimagerectanglematchesabody\ni\nsegment. For each pair of related segments, we have another function \u03c8 that scores how\nwell relations between a pair of image rectangles match those to be expected from the body\nsegments. The dependencies between segments form a tree, so each segment has only one\nparent, and we could write \u03c8 i,pa(i). All the functions will be larger if the match is better,\nso we can think of them as being like a log probability. The cost of a particular match that\nallocates imagerectangle m tobodysegmentiisthen\n(cid:12) i (cid:12)\n\u03c6 i(m i)+ \u03c8 i,pa(i)(m i,mpa(i)).\ni\u2208segments i\u2208segments\nDynamicprogramming canfindthebestmatch,because therelational modelisatree.\nItisinconvenienttosearchacontinuousspace,andwewilldiscretizethespaceofimage\nrectangles. We do so by discretizing the location and orientation of rectangles of fixed size\n(the sizes may be different for different segments). Because ankles and knees are different,\n\u25e6\nwe need to distinguish between a rectangle and the same rectangle rotated by 180 . One\ncouldvisualize theresultasasetofverylargestacks ofsmallrectangles ofimage, cutoutat\ndifferent locations and orientations. There is one stack per segment. We must now find the\nbest allocation of rectangles to segments. This will be slow, because there are many image\nrectangles and,forthemodelwehavegiven,choosing therighttorsowillbeO(M6)ifthere\nare M image rectangles. However, various speedups are available foran appropriate choice\nof \u03c8, and the method is practical (Figure 24.23). Themodel is usually known asa pictorial\nPICTORIAL structuremodel.\nSTRUCTUREMODEL\nRecallourassumptionthatweknowwhatweneedtoknowaboutwhatthepersonlooks\nlike. If weare matching a person in a single image, the most useful feature forscoring seg-\nment matches turns out tobe color. Texture features don\u2019t work wellin mostcases, because\nfoldsonlooseclothingproducestrongshadingpatternsthatoverlaytheimagetexture. These Section24.5. ObjectRecognition fromStructuralInformation 959\nFigure24.23 Apictorialstructuremodelevaluatesamatchbetweenasetofimagerect-\nangles and a cardboard person (shown on the left) by scoring the similarity in appearance\nbetweenbodysegmentsandimagesegmentsandthespatialrelationsbetweentheimageseg-\nments.Generally,amatchisbetteriftheimagesegmentshaveabouttherightappearanceand\nareinabouttherightplacewithrespecttooneanother. Theappearancemodelusesaverage\ncolors forhair, head, torso, and upperand lower arms and legs. The relevantrelations are\nshownasarrows.Ontheright,thebestmatchforaparticularimage,obtainedusingdynamic\nprogramming. The match is a fair estimate of the configuration of the body. Figure from\nFelzenszwalbandHuttenlocher(2000)(cid:2)c IEEE.\npatternsarestrongenoughtodisruptthetruetextureofthe cloth. Incurrentwork,\u03c8typically\nreflects the need for the ends of the segments to be reasonably close together, but there are\nusually no constraints on the angles. Generally, we don\u2019t know what a person looks like,\nand must build a model of segment appearances. We call the description of what a person\nAPPEARANCE looksliketheappearancemodel. Ifwemustreporttheconfiguration ofapersoninasingle\nMODEL\nimage, wecan start with apoorly tuned appearance model, estimate configuration with this,\nthen re-estimate appearance, and so on. In video, wehave many frames of the same person,\nandthiswillrevealtheirappearance.\n24.5.2 Coherent appearance: Tracking people invideo\nTracking people in video is an important practical problem. If we could reliably report the\nlocation of arms, legs, torso, and head in video sequences, we could build much improved\ngame interfaces and surveillance systems. Filtering methods have not had much success\nwiththisproblem, because people canproduce large accelerations andmovequite fast. This\nmeans that for 30 Hz video, the configuration of the body in frame i doesn\u2019t constrain the\nconfigurationofthebodyinframei+1allthatstrongly. Currently,themosteffectivemethods\nexploit thefactthatappearance changes veryslowlyfromframetoframe. Ifwecaninferan\nappearance model of an individual from the video, then we can use this information in a\npictorial structure model to detect that person in each frame of the video. We can then link\ntheselocations acrosstimetomakeatrack. 960 Chapter 24. Perception\ntorso\narm\nLateral walking Appearance Body part Detected figure\ndetector model maps\nmotion blur\n& interlacing\nFigure 24.24 We can track moving people with a pictorial structure model by first ob-\ntaininganappearancemodel,thenapplyingit. Toobtaintheappearancemodel,wescanthe\nimage to find a lateral walking pose. The detector does not need to be very accurate, but\nshouldproducefewfalsepositives. Fromthedetectorresponse,we canreadoffpixelsthat\nlieoneachbodysegment,andothersthatdonotlieonthatsegment.Thismakesitpossibleto\nbuildadiscriminativemodeloftheappearanceofeachbodypart,andthesearetiedtogether\nintoapictorialstructuremodelofthepersonbeingtracked. Finally,wecanreliablytrackby\ndetectingthismodelineachframe.Astheframesinthelowerpartoftheimagesuggest,this\nprocedurecantrackcomplicated,fast-changingbodyconfigurations,despitedegradationof\nthevideosignalduetomotionblur. FigurefromRamananetal.(2007)(cid:2)c IEEE.\nThere are several ways to infer a good appearance model. We regard the video as a\nlarge stack of pictures of the person we wish to track. We can exploit this stack by looking\nfor appearance models that explain many of the pictures. This would work by detecting\nbodysegmentsineachframe,usingthefactthatsegmentshaveroughly paralleledges. Such\ndetectors are not particularly reliable, but the segments we want to find are special. They\nwill appear at least once in most of the frames of video; such segments can be found by\nclustering the detector responses. It is best to start with the torso, because it is big and\nbecause torso detectors tend to be reliable. Once we have a torso appearance model, upper\nleg segments should appear near the torso, and so on. This reasoning yields an appearance\nmodel, but it can be unreliable if people appear against a near-fixed background where the\nsegment detector generates lots of false positives. An alternative is to estimate appearance\nformanyoftheframesofvideobyrepeatedlyreestimatingconfigurationandappearance; we\nthen see if one appearance model explains manyframes. Another alternative, which isquite Section24.6. UsingVision 961\nFigure 24.25 Some complex human actions produce consistent patterns of appearance\nandmotion.Forexample,drinkinginvolvesmovementsofthehandinfrontoftheface. The\nfirstthreeimagesarecorrectdetectionsofdrinking;thefourthisafalse-positive(thecookis\nlookingintothecoffeepot,butnotdrinkingfromit). FigurefromLaptevandPerez(2007)\n(cid:2)c IEEE.\nreliableinpractice,istoapplyadetectorforafixedbodyconfigurationtoalloftheframes. A\ngoodchoiceofconfiguration isonethatiseasytodetectreliably, andwherethereisastrong\nchance theperson willappearinthat configuration even inashort sequence (lateral walking\nisagoodchoice). Wetunethedetector tohavealowfalsepositive rate,soweknowwhenit\nresponds that we have found a real person; and because wehave localized their torso, arms,\nlegs,andhead,weknowwhatthesesegmentslooklike.\n24.6 USING VISION\nIf vision systems could analyze video and understood what people are doing, we would be\nable to: design buildings and public places better by collecting and using data about what\npeopledoinpublic;buildmoreaccurate,moresecure,andlessintrusivesurveillancesystems;\nbuildcomputersportscommentators;andbuildhuman-computerinterfacesthatwatchpeople\nand react to their behavior. Applications for reactive interfaces range from computer games\nthatmakeaplayergetupandmovearoundtosystemsthatsaveenergybymanagingheatand\nlightinabuilding tomatchwheretheoccupants areandwhattheyaredoing.\nSome problems are well understood. If people are relatively small in the video frame,\nandthebackgroundisstable,itiseasytodetectthepeoplebysubtractingabackgroundimage\nfrom the current frame. If the absolute value of the difference is large, this background\nBACKGROUND subtraction declares the pixel to be a foreground pixel; by linking foreground blobs over\nSUBTRACTION\ntime,weobtainatrack.\nStructuredbehaviors likeballet,gymnastics, ortaichihavespecificvocabularies ofac-\ntions. Whenperformed againstasimplebackground, videos oftheseactionsareeasytodeal\nwith. Background subtraction identifies the major moving regions, and we can build HOG\nfeatures(keepingtrackofflowratherthanorientation)topresenttoaclassifier. Wecandetect\nconsistent patterns of action with a variant of our pedestrian detector, where the orientation\nfeaturesarecollected intohistogram bucketsovertimeaswellasspace(Figure24.25).\nMoregeneral problems remain open. Thebig research question isto link observations\nof the body and the objects nearby to the goals and intentions of the moving people. One\nsource ofdifficulty isthat welack asimple vocabulary ofhuman behavior. Behavior isalot 962 Chapter 24. Perception\nlike color, in that people tend to think they know a lot of behavior names but can\u2019t produce\nlonglistsofsuchwordsondemand. Thereisquitealotofevidencethatbehaviorscombine\u2014\nyou can, for example, drink a milkshake while visiting an ATM\u2014but we don\u2019t yet know\nwhat the pieces are, how the composition works, or how many composites there might be.\nAsecond source ofdifficulty isthatwedon\u2019t know whatfeatures expose whatishappening.\nForexample,knowingsomeoneisclosetoanATMmaybeenoughtotellthatthey\u2019revisiting\ntheATM.Athirddifficultyisthattheusualreasoningabouttherelationshipbetweentraining\nand test data is untrustworthy. For example, we cannot argue that a pedestrian detector is\nsafesimply because itperforms wellonalarge dataset, because thatdatasetmaywellomit\nimportant, butrare,phenomena (forexample, people mounting bicycles). Wewouldn\u2019t want\nourautomateddrivertorunoverapedestrian whohappened to dosomething unusual.\n24.6.1 Wordsandpictures\nMany Web sites offer collections of images for viewing. How can we find the images we\nwant? Let\u2019ssuppose theuserenters atextquery, suchas\u201cbicycle race.\u201d Someoftheimages\nwillhavekeywordsorcaptionsattached, orwillcomefromWebpagesthatcontain textnear\nthe image. Forthese, image retrieval can be like text retrieval: ignore the images and match\ntheimage\u2019stextagainstthequery(seeSection22.3onpage867).\nHowever, keywords are usually incomplete. Forexample, a picture of a cat playing in\nthestreetmightbetaggedwithwordslike\u201ccat\u201dand\u201cstreet,\u201dbutitiseasytoforgettomention\nthe\u201cgarbagecan\u201dorthe\u201cfishbones.\u201d Thusaninterestingtaskistoannotateanimage(which\nmayalready haveafewkeywords)withadditional appropriate keywords.\nIn the most straightforward version of this task, we have a set of correctly tagged ex-\nample images, and we wish to tag some test images. This problem is sometimes known as\nauto-annotation. Themostaccurate solutions areobtained using nearest-neighbors methods.\nOne finds the training images that are closest tothe test image in a feature space metric that\nistrained usingexamples, thenreportstheirtags.\nAnother version of the problem involves predicting which tags to attach to which re-\ngionsinatestimage. Herewedonotknowwhichregionsproducedwhichtagsforthetrain-\ningdata. Wecanuseaversionofexpectationmaximizationtoguessaninitialcorrespondence\nbetween textandregions, andfrom thatestimate abetterdecomposition into regions, andso\non.\n24.6.2 Reconstruction from many views\nBinocular stereopsis works because for each point we have four measurements constraining\nthree unknown degrees of freedom. The four measurements are the (x,y) positions of the\npointineachview,andtheunknowndegreesoffreedomarethe(x,y,z)coordinatevaluesof\nthepointinthescene. Thisrathercrudeargumentsuggests, correctly,thattherearegeometric\nconstraints thatpreventmostpairsofpointsfrombeingacceptablematches. Manyimagesof\nasetofpointsshouldrevealtheirpositions unambiguously.\nWe don\u2019t always need a second picture to get a second view of a set of points. If we\nbelieve the original set of points comes from a familiar rigid 3D object, then wemight have Section24.6. UsingVision 963\nanobject modelavailable asasource ofinformation. Ifthis objectmodelconsists ofasetof\n3Dpointsorofasetofpictures oftheobject, andifwecanestablish pointcorrespondences,\nwecandeterminetheparametersofthecamerathatproducedthepointsintheoriginalimage.\nThis is very powerful information. We could use it to evaluate our original hypothesis that\nthe points come from an object model. We do this by using some points to determine the\nparameters of the camera, then projecting model points in this camera and checking to see\nwhetherthereareimagepointsnearby.\nWehavesketchedhereatechnologythatisnowveryhighlydeveloped. Thetechnology\ncan be generalized to deal with views that are not orthographic; to deal with points that are\nobserved in only some views; to deal with unknown camera properties like focal length; to\nexploitvarioussophisticated searchesforappropriatecorrespondences; andtodoreconstruc-\ntion from verylarge numbers ofpoints andofviews. Ifthelocations ofpoints intheimages\nareknownwithsomeaccuracyandtheviewingdirectionsarereasonable, veryhighaccuracy\ncameraandpointinformation canbeobtained. Someapplications are\n\u2022 Model-building: For example, one might build a modeling system that takes a video\nsequence depicting an object and produces a very detailed three-dimensional mesh of\ntexturedpolygonsforuseincomputergraphicsandvirtualrealityapplications. Models\nlike this can now be built from apparently quite unpromising sets of pictures. For ex-\nample, Figure 24.26 shows a model of the Statue of Liberty built from pictures found\nontheInternet.\n\u2022 Matching moves: To place computer graphics characters into real video, we need to\nknow how the camera moved for the real video, so that we can render the character\ncorrectly.\n\u2022 Path reconstruction: Mobile robots need to know where they have been. If they are\nmoving in a world of rigid objects, then performing a reconstruction and keeping the\ncamerainformation isonewaytoobtainapath.\n24.6.3 Usingvisionforcontrolling movement\nOneoftheprincipal usesofvisionistoprovideinformation bothformanipulating objects\u2014\npickingthemup,graspingthem,twirlingthem,andsoon\u2014andfornavigatingwhileavoiding\nobstacles. The ability to use vision for these purposes is present in the most primitive of\nanimal visual systems. In many cases, the visual system is minimal, in the sense that it\nextracts from the available light field just the information the animal needs to inform its\nbehavior. Quite probably, modern vision systems evolved from early, primitive organisms\nthat used a photosensitive spot at one end to orient themselves toward (or away from) the\nlight. We saw in Section 24.4 that flies use a very simple optical flow detection system to\nland on walls. A classic study, What the Frog\u2019s Eye Tells the Frog\u2019s Brain (Lettvin et al.,\n1959),observesofafrogthat,\u201cHewillstarvetodeathsurroundedbyfoodifitisnotmoving.\nHischoiceoffoodisdetermined onlybysizeandmovement.\u201d\nLet us consider a vision system for an automated vehicle driving on a freeway. The\ntasksfacedbythedriverincludethefollowing: 964 Chapter 24. Perception\na b c\n(a) (b) (c)\nFigure24.26 Thestateoftheartinmultiple-viewreconstructionisnowhighlyadvanced.\nThis figureoutlines a system builtby MichaelGoesele and colleaguesfrom the University\nofWashington, TU Darmstadt, and MicrosoftResearch. Froma collectionof picturesof a\nmonumenttakenbyalargecommunityofusersandpostedontheInternet(a),theirsystem\ncandeterminetheviewingdirectionsforthosepictures,shownbythesmallblackpyramids\nin(b)andacomprehensive3Dreconstructionshownin(c).\n1. Lateral control\u2014ensure that the vehicle remains securely within its lane or changes\nlanessmoothlywhenrequired.\n2. Longitudinal control\u2014ensure thatthereisasafedistancetothevehicleinfront.\n3. Obstacleavoidance\u2014monitorvehiclesinneighboringlanesandbepreparedforevasive\nmaneuversifoneofthemdecidestochangelanes.\nThe problem for the driver is to generate appropriate steering, acceleration, and braking ac-\ntionstobestaccomplish thesetasks.\nForlateralcontrol,oneneedstomaintainarepresentation ofthepositionandorientation\nofthecarrelativetothelane. Wecanuseedge-detectionalgorithmstofindedgescorrespond-\ningtothelane-marker segments. Wecanthen fitsmooth curves tothese edgeelements. The\nparameters of these curves carry information about the lateral position of the car, the direc-\ntion it is pointing relative to the lane, and the curvature of the lane. This information, along\nwith information about the dynamics of the car, is all that is needed by the steering-control\nsystem. Ifwehave good detailed mapsofthe road, then the vision system serves toconfirm\nourposition (andtowatchforobstacles thatarenotonthemap).\nForlongitudinal control, oneneeds toknowdistances tothe vehicles infront. Thiscan\nbe accomplished with binocular stereopsis or optical flow. Using these techniques, vision-\ncontrolled carscannowdrivereliably athighwayspeeds.\nThemoregeneralcaseofmobilerobotsnavigating invarious indoorandoutdoorenvi-\nronments has been studied, too. One particular problem, localizing the robot in its environ-\nment, now has pretty good solutions. A group at Sarnoff has developed a system based on\ntwo cameras looking forward that track feature points in 3D and use that to reconstruct the Section24.7. Summary 965\nposition of the robot relative tothe environment. In fact, they have twostereoscopic camera\nsystems, one looking front and one looking back\u2014this gives greater robustness in case the\nrobothastogothroughafeaturelesspatchduetodarkshadows,blankwalls,andthelike. Itis\nunlikelythattherearenofeatureseitherinthefrontorintheback. Nowofcourse,thatcould\nhappen,soabackupisprovidedbyusinganinertialmotionunit(IMU)somewhatakintothe\nmechanisms for sensing acceleration that we humans have in our inner ears. By integrating\nthe sensed acceleration twice, one can keep track of the change in position. Combining the\ndatafromvisionandtheIMUisaproblemofprobabilisticevidencefusionandcanbetackled\nusingtechniques, suchasKalmanfiltering, wehavestudiedelsewhereinthebook.\nIn the use of visual odometry (estimation of change in position), as in other problems\nof odometry, there is the problem of \u201cdrift,\u201d positional errors accumulating over time. The\nsolution for this is to use landmarks to provide absolute position fixes: as soon as the robot\npasses a location in its internal map, it can adjust its estimate of its position appropriately.\nAccuracies ontheorderofcentimeters havebeendemonstrated withthethesetechniques.\nThedriving examplemakesonepoint veryclear: foraspecific task, onedoes notneed\nto recover all the information that, in principle, can be recovered from an image. One does\nnotneedtorecovertheexactshapeofeveryvehicle,solveforshape-from-textureonthegrass\nsurfaceadjacenttothefreeway,andsoon. Instead,avisionsystemshouldcomputejustwhat\nisneeded toaccomplish thetask.\n24.7 SUMMARY\nAlthough perception appears to be an effortless activity for humans, it requires a significant\namountofsophisticated computation. Thegoalofvisionistoextractinformation neededfor\ntaskssuchasmanipulation, navigation, andobjectrecognition.\n\u2022 The process of image formation is well understood in its geometric and physical as-\npects. Givenadescriptionofathree-dimensionalscene,wecaneasilyproduceapicture\nofitfromsomearbitrary cameraposition (thegraphicsproblem). Inverting theprocess\nbygoingfromanimagetoadescription ofthesceneismoredifficult.\n\u2022 To extract the visual information necessary for the tasks of manipulation, navigation,\nand recognition, intermediate representations have to be constructed. Early vision\nimage-processing algorithms extract primitive features from the image, such as edges\nandregions.\n\u2022 There are various cues in the image that enable one to obtain three-dimensional in-\nformation about the scene: motion, stereopsis, texture, shading, and contour analysis.\nEachof these cues relies on background assumptions about physical scenes to provide\nnearlyunambiguous interpretations.\n\u2022 Objectrecognitioninitsfullgeneralityisaveryhardproblem. Wediscussedbrightness-\nbased and feature-based approaches. We also presented a simple algorithm for pose\nestimation. Otherpossibilities exist. 966 Chapter 24. Perception\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nTheeye developed in the Cambrian explosion (530 million years ago), apparently in acom-\nmon ancestor. Since then, endless variations have developed in different creatures, but the\nsame gene, Pax-6, regulates the development of the eye in animals as diverse as humans,\nmice,andDrosophila.\nSystematic attempts to understand human vision can be traced back to ancient times.\nEuclid (ca. 300 B.C.) wrote about natural perspective\u2014the mapping that associates, with\neachpointP inthethree-dimensionalworld,thedirectionoftheray OP joiningthecenterof\nprojection O tothepointP. Hewaswellawareofthenotion ofmotionparallax. Theuseof\nperspective in art was developed in ancient Roman culture, as evidenced by art found in the\nruinsofPompeii(A.D.79),butwasthenlargelylostfor1300years. Themathematicalunder-\nstandingofperspectiveprojection, thistimeinthecontextofprojectionontoplanarsurfaces,\nhaditsnextsignificantadvanceinthe15th-century inRenaissanceItaly. Brunelleschi (1413)\nisusually credited withcreating thefirstpaintings based on geometrically correct projection\nofa three-dimensional scene. In 1435, Alberti codified therules and inspired generations of\nartistswhoseartisticachievementsamazeustothisday. Particularlynotableintheirdevelop-\nmentofthescienceofperspective,asitwascalledinthosedays,wereLeonardodaVinciand\nAlbrecht Du\u00a8rer. Leonardo\u2019s late15th century descriptions oftheinterplay oflightandshade\n(chiaroscuro), umbraandpenumbraregionsofshadows,andaerialperspectivearestillworth\nreading in translation (Kemp, 1989). Stork (2004) analyzes thecreation ofvarious pieces of\nRenaissance artusingcomputervisiontechniques.\nAlthough perspective was known to the ancient Greeks, they were curiously confused\nbytheroleoftheeyesinvision. Aristotlethoughtoftheeyesasdevicesemittingrays,rather\ninthemannerofmodernlaserrangefinders. Thismistakenviewwaslaidtorestbythework\nofArabscientists, suchasAbuAliAlhazen, inthe10thcentury. Alhazenalsodeveloped the\ncameraobscura,aroom(cameraisLatinfor\u201croom\u201dor\u201cchamber\u201d)withapinholethatcasts\nan image on the opposite wall. Of course the image was inverted, which caused no end of\nconfusion. If the eye was to be thought of as such an imaging device, how do we see right-\nside up? This enigma exercised the greatest minds of the era (including Leonardo). Kepler\nfirstproposedthatthelensoftheeyefocusesanimageontheretina,andDescartessurgically\nremovedanoxeyeanddemonstrated thatKeplerwasright. Therewasstillpuzzlement asto\nwhywedonotseeeverythingupsidedown;todaywerealizeitisjustaquestionofaccessing\ntheretinaldatastructure intherightway.\nInthe first half of the 20th century, the most significant research results in vision were\nobtained bytheGestalt school ofpsychology, ledbyMaxWertheimer. Theypointed outthe\nimportance of perceptual organization: for a human observer, the image is not a collection\nof pointillist photoreceptor outputs (pixels in computer vision terminology); rather it is or-\nganized into coherent groups. One could trace the motivation in computer vision of finding\nregions and curves back to this insight. The Gestaltists also drew attention to the \u201cfigure\u2013\nground\u201d phenomenon\u2014a contour separating two image regions that, in the world, are at\ndifferent depths, appears tobelong onlytothenearerregion, the\u201cfigure,\u201dandnotthefarther Bibliographical andHistorical Notes 967\nregion,the\u201cground.\u201d Thecomputervisionproblem ofclassifying imagecurvesaccording to\ntheirsignificance inthescenecanbethoughtofasageneralization ofthisinsight.\nThe period after World War II was marked by renewed activity. Most significant was\ntheworkofJ.J.Gibson(1950,1979),whopointedouttheimportanceofopticalflow,aswell\nastexturegradientsintheestimationofenvironmentalvariablessuchassurfaceslantandtilt.\nHereemphasizedtheimportanceofthestimulusandhowrichitwas. Gibsonemphasizedthe\nroleoftheactiveobserverwhoseself-directedmovementfacilitatesthepickupofinformation\nabouttheexternalenvironment.\nComputer vision was founded in the 1960s. Roberts\u2019s (1963) thesis at MIT was one\nof the earliest publications in the field, introducing key ideas such as edge detection and\nmodel-based matching. There is an urban legend that Marvin Minsky assigned the problem\nof\u201csolving\u201dcomputervisiontoagraduatestudentasasummerproject. AccordingtoMinsky\nthe legend is untrue\u2014it was actually an undergraduate student. But it was an exceptional\nundergraduate, GeraldJaySussman(whoisnowaprofessor at MIT)andthetaskwasnotto\n\u201csolve\u201dvision, buttoinvestigate someaspectsofit.\nInthe1960sand1970s,progresswasslow,hamperedconsiderably bythelackofcom-\nputationalandstorageresources. Low-levelvisualprocessing receivedalotofattention. The\nwidely used Canny edge-detection technique was introduced in Canny (1986). Techniques\nforfindingtextureboundariesbasedonmultiscale,multiorientationfilteringofimagesdateto\nwork such as Malik and Perona (1990). Combining multiple clues\u2014brightness, texture and\ncolor\u2014for finding boundary curves in a learning framework wasshown by Martin, Fowlkes\nandMalik(2004)toconsiderably improveperformance.\nThe closely related problem of finding regions of coherent brightness, color, and tex-\nture, naturally lends itself to formulations in which finding the best partition becomes an\noptimization problem. Three leading examples are the Markov Random Fields approach of\nGeman and Geman (1984), the variational formulation of Mumford and Shah (1989), and\nnormalized cutsbyShiandMalik(2000).\nThrough much of the 1960s, 1970s and 1980s, there were two distinct paradigms in\nwhich visual recognition was pursued, dictated by different perspectives on what was per-\nceivedtobetheprimaryproblem. Computervisionresearchonobjectrecognitionlargelyfo-\ncusedonissuesarisingfromtheprojectionofthree-dimensionalobjectsontotwo-dimensional\nimages. Theideaofalignment,alsofirstintroducedbyRoberts,resurfacedinthe1980sinthe\nwork of Lowe (1987) and Huttenlocher and Ullman (1990). Also popular was an approach\nGENERALIZED based on describing shapes in terms of volumetric primitives, with generalized cylinders,\nCYLINDER\nintroduced byTom Binford(1971),proving particularly popular.\nIncontrast,thepatternrecognitioncommunityviewedthe3D-to-2Daspectsoftheprob-\nlem asnot significant. Theirmotivating examples wereindomains such as optical character\nrecognition andhandwritten zipcoderecognition wherethe primaryconcern isthatoflearn-\ning the typical variations characteristic of a class of objects and separating them from other\nclasses. SeeLeCunetal.(1995)foracomparison ofapproaches.\nIn the late 1990s, these two paradigms started to converge, as both sides adopted the\nprobabilistic modeling and learning techniques that were becoming popular throughout AI.\nTwolinesofworkcontributed significantly. Onewasresearch onfacedetection, suchasthat 968 Chapter 24. Perception\nof Rowley, Baluja and Kanade (1996), and of Viola and Jones (2002b) which demonstrated\nthepowerofpattern recognition techniques onclearly important and useful tasks. Theother\nwasthedevelopmentofpointdescriptors, whichenableonetoconstructfeaturevectorsfrom\nparts of objects. This was pioneered by Schmid and Mohr (1996). Lowe\u2019s (2004) SIFT\ndescriptoriswidelyused. TheHOGdescriptorisduetoDalal andTriggs(2005).\nUllman (1979) and Longuet-Higgins (1981) are influential early works in reconstruc-\ntion from multiple images. Concerns about the stability of structure from motion were sig-\nnificantlyallayedbytheworkofTomasiandKanade(1992)whoshowedthatwiththeuseof\nmultiple frames shape could berecovered quite accurately. Inthe 1990s, withgreat increase\nincomputerspeedandstorage,motionanalysis foundmanynewapplications. Buildinggeo-\nmetrical models of real-world scenes for rendering by computer graphics techniques proved\nparticularlypopular, ledbyreconstruction algorithmssuchastheonedevelopedbyDebevec,\nTaylor, and Malik (1996). The books by Hartley and Zisserman (2000) and Faugeras et al.\n(2001)provideacomprehensive treatmentofthegeometryof multipleviews.\nForsingle images, inferring shape from shading was first studied by Horn (1970), and\nHornand Brooks (1989) present anextensive survey ofthemainpapers from aperiod when\nthis was a much-studied problem. Gibson (1950) was the first to propose texture gradients\nasacuetoshape,thoughacomprehensive analysisforcurved surfacesfirstappears inGard-\ning (1992) and Malik and Rosenholtz (1997). The mathematics of occluding contours, and\nmore generally understanding the visual events in the projection of smooth curved objects,\nowesmuchtotheworkofKoenderink andvanDoorn, whichfindsanextensive treatment in\nKoenderink\u2019s(1990)SolidShape. Inrecentyears,attentionhasturnedtotreatingtheproblem\nofshapeandsurfacerecoveryfromasingleimageasaprobabilisticinferenceproblem,where\ngeometrical cues are not modeled explicitly, but used implicitly in a learning framework. A\ngoodrepresentative istheworkofHoiem,Efros,andHebert(2008).\nForthereaderinterested inhumanvision, Palmer(1999)provides thebestcomprehen-\nsive treatment; Bruce et al. (2003) is a shorter textbook. The books by Hubel (1988) and\nRock (1984) are friendly introductions centered on neurophysiology and perception respec-\ntively. DavidMarr\u2019sbookVision(Marr,1982)playedahistoricalroleinconnectingcomputer\nvision topsychophysics andneurobiology. Whilemanyofhis specific models haven\u2019t stood\nthe test of time, the theoretical perspective from which each task is analyzed at an informa-\ntional, computational, andimplementation levelisstillilluminating.\nFor computer vision, the most comprehensive textbook is Forsyth and Ponce (2002).\nTruccoandVerri(1998)isashorteraccount. Horn(1986)and Faugeras(1993)aretwoolder\nandstillusefultextbooks.\nThemain journals forcomputer vision areIEEE Transactions onPattern Analysis and\nMachine Intelligence and International Journal of Computer Vision. Computer vision con-\nferences include ICCV (International Conference on Computer Vision), CVPR (Computer\nVision and Pattern Recognition), and ECCV (European Conference on Computer Vision).\nResearchwithamachinelearningcomponentisalsopublishedintheNIPS(NeuralInforma-\ntionProcessingSystems)conference,andworkontheinterfacewithcomputergraphicsoften\nappearsattheACMSIGGRAPH(SpecialInterestGroupinGraphics) conference. Exercises 969\nEXERCISES\n24.1 In the shadow of a tree with a dense, leafy canopy, one sees a number of light spots.\nSurprisingly, they all appear to be circular. Why? After all, the gaps between the leaves\nthrough whichthesunshinesarenotlikelytobecircular.\n24.2 Consider a picture of a white sphere floating in front of a black backdrop. The im-\nage curve separating white pixels from black pixels is sometimes called the \u201coutline\u201d of the\nsphere. Showthat the outline ofasphere, viewed inaperspective camera, canbe anellipse.\nWhydospheresnotlooklikeellipses toyou?\n24.3 Consideraninfinitelylongcylinderofradius r oriented withitsaxisalongthey-axis.\nThe cylinder has a Lambertian surface and is viewed by a camera along the positive z-axis.\nWhat will you expect to see in the image if the cylinder is illuminated by a point source\nat infinity located on the positive x-axis? Draw the contours of constant brightness in the\nprojected image. Arethecontours ofequalbrightness uniformly spaced?\n24.4 Edges in an image can correspond to a variety of events in a scene. Consider Fig-\nure24.4(page933),andassumethatitisapictureofarealthree-dimensional scene. Identify\nten different brightness edges in the image, and for each, state whether it corresponds to a\ndiscontinuity in(a)depth,(b)surfaceorientation, (c)reflectance, or(d)illumination.\n24.5 Astereoscopicsystemisbeingcontemplatedforterrainmapping. Itwillconsistoftwo\nCCDcameras, each having 512\u00d7512pixels ona10cm\u00d710cmsquare sensor. Thelenses\nto be used have a focal length of 16 cm, with the focus fixed at infinity. For corresponding\npoints (u ,v ) in the left image and (u ,v ) in the right image, v = v because the x-axes\n1 1 2 2 1 2\nin the two image planes are parallel to the epipolar lines\u2014the lines from the object to the\ncamera. Theoptical axesofthetwocamerasare parallel. The baseline between thecameras\nis1meter.\na. Ifthenearestdistancetobemeasuredis16meters,whatisthelargestdisparitythatwill\noccur(inpixels)?\nb. Whatisthedistance resolution at16meters,duetothepixelspacing?\nc. Whatdistance corresponds toadisparity ofonepixel?\n24.6 Whichofthefollowingaretrue,andwhicharefalse?\na. Finding corresponding points in stereo images is the easiest phase of the stereo depth-\nfindingprocess.\nb. Shape-from-texture canbedonebyprojecting agridoflight-stripes ontothescene.\nc. Lineswithequallengths inthescenealwaysprojecttoequallengths intheimage.\nd. Straightlinesintheimagenecessarily correspond tostraightlinesinthescene. 970 Chapter 24. Perception\nD\nA\nX\nB\nC\nY\nE\nFigure 24.27 Top view of a two-camera vision system observing a bottle with a wall\nbehindit.\n24.7 (Courtesy ofPietroPerona.) Figure24.27showstwocameras atXandYobserving a\nscene. Draw the image seen at each camera, assuming that all named points are in the same\nhorizontal plane. Whatcan beconcluded fromthese twoimages about therelative distances\nofpointsA,B,C,D,andEfromthecamerabaseline, andonwhatbasis? 25\nROBOTICS\nInwhichagentsareendowedwithphysicaleffectors withwhichtodomischief.\n25.1 INTRODUCTION\nRobotsarephysical agentsthatperformtasksbymanipulating thephysicalworld. Todoso,\nROBOT\nthey are equipped with effectors such as legs, wheels, joints, and grippers. Effectors have\nEFFECTOR\na single purpose: to assert physical forces on the environment.1 Robots are also equipped\nwith sensors, which allow them to perceive their environment. Present day robotics em-\nSENSOR\nploys adiverse setofsensors, including camerasandlasers tomeasure theenvironment, and\ngyroscopes andaccelerometers tomeasuretherobot\u2019sownmotion.\nMostoftoday\u2019srobotsfallintooneofthreeprimarycategories. Manipulators,orrobot\nMANIPULATOR\narms (Figure 25.1(a)), are physically anchored to their workplace, for example in a factory\nassembly line or on the International Space Station. Manipulator motion usually involves\na chain of controllable joints, enabling such robots to place their effectors in any position\nwithin the workplace. Manipulators are by far the most common type of industrial robots,\nwith approximately one million units installed worldwide. Some mobile manipulators are\nused in hospitals to assist surgeons. Few car manufacturers could survive without robotic\nmanipulators, andsomemanipulators haveevenbeenusedtogenerate original artwork.\nThesecondcategoryisthemobilerobot. Mobilerobotsmoveabouttheirenvironment\nMOBILEROBOT\nusing wheels, legs, or similar mechanisms. They have been put to use delivering food in\nhospitals, moving containers at loading docks, and similar tasks. Unmanned ground vehi-\ncles,orUGVs,driveautonomously onstreets, highways, andoff-road. Theplanetaryrover\nUGV\nshowninFigure25.2(b)exploredMarsforaperiodof3months in1997. Subsequent NASA\nPLANETARYROVER\nrobots include thetwinMarsExploration Rovers(oneisdepicted onthecoverofthisbook),\nwhich landed in 2003 and were still operating six years later. Other types of mobile robots\nincludeunmannedairvehicles(UAVs),commonlyusedforsurveillance,crop-spraying, and\nUAV\n1 InChapter2wetalkedaboutactuators,noteffectors. Herewedistinguishtheeffector(thephysicaldevice)\nfromtheactuator(thecontrollinethatcommunicatesacommandtotheeffector).\n971 972 Chapter 25. Robotics\n(a) (b)\nFigure 25.1 (a) An industrial robotic manipulator for stacking bags on a pallet. Image\ncourtesyofNachiRoboticSystems. (b)Honda\u2019sP3andAsimohumanoidrobots.\n(a) (b)\nFigure 25.2 (a) Predator, an unmanned aerial vehicle (UAV) used by the U.S. Military.\nImagecourtesyofGeneralAtomicsAeronauticalSystems. (b)NASA\u2019sSojourner,amobile\nrobotthatexploredthesurfaceofMarsinJuly1997.\nmilitary operations. Figure 25.2(a) shows a UAV commonly used by the U.S. military. Au-\ntonomous underwater vehicles (AUVs) are used in deep sea exploration. Mobile robots\nAUV\ndeliverpackagesintheworkplaceandvacuumthefloorsathome.\nThe third type of robot combines mobility with manipulation, and is often called a\nMOBILE mobilemanipulator. Humanoidrobotsmimicthehuman torso. Figure25.1(b) showstwo\nMANIPULATOR\nearly humanoid robots, both manufactured by Honda Corp. in Japan. Mobile manipulators\nHUMANOIDROBOT Section25.2. RobotHardware 973\ncanapplytheireffectorsfurtherafieldthananchoredmanipulators can,buttheirtaskismade\nharderbecausetheydon\u2019thavetherigidity thattheanchorprovides.\nThe field of robotics also includes prosthetic devices (artificial limbs, ears, and eyes\nforhumans), intelligent environments (such as an entire house that is equipped withsensors\nandeffectors), andmultibodysystems,whereinroboticactionisachievedthroughswarmsof\nsmallcooperating robots.\nReal robots must cope with environments that are partially observable, stochastic, dy-\nnamic, and continuous. Many robot environments are sequential and multiagent as well.\nPartial observability and stochasticity are the result of dealing with a large, complex world.\nRobot cameras cannot see around corners, and motion commands are subject to uncertainty\ndue to gears slipping, friction, etc. Also, the real world stubbornly refuses to operate faster\nthanrealtime. Inasimulatedenvironment,itispossibletousesimplealgorithms(suchasthe\nQ-learning algorithm described inChapter21) tolearn inafew CPUhours from millions of\ntrials. Inarealenvironment, itmighttakeyearstorunthesetrials. Furthermore, realcrashes\nreallyhurt,unlikesimulatedones. Practicalroboticsystemsneedtoembodypriorknowledge\nabouttherobot,itsphysicalenvironment, andthetasksthattherobotwillperformsothatthe\nrobotcanlearnquicklyandperform safely.\nRobotics brings together many of the concepts we have seen earlier in the book, in-\ncluding probabilistic state estimation, perception, planning, unsupervised learning, and re-\ninforcement learning. For some of these concepts robotics serves as a challenging example\napplication. Forotherconceptsthischapterbreaksnewgroundinintroducing thecontinuous\nversionoftechniques thatwepreviously sawonlyinthediscrete case.\n25.2 ROBOT HARDWARE\nSofarinthisbook,wehavetakentheagentarchitecture\u2014sensors, effectors,andprocessors\u2014\nasgiven,andwehaveconcentratedontheagentprogram. Thesuccessofrealrobotsdepends\natleastasmuchonthedesignofsensorsandeffectors thatareappropriate forthetask.\n25.2.1 Sensors\nSensors are the perceptual interface between robot and environment. Passive sensors, such\nPASSIVESENSOR\nascameras, aretrueobservers oftheenvironment: theycapture signals thataregenerated by\nother sources in the environment. Active sensors, such as sonar, send energy into the envi-\nACTIVESENSOR\nronment. Theyrelyonthefactthatthisenergy isreflectedbacktothesensor. Activesensors\ntendtoprovidemoreinformationthanpassivesensors,butattheexpenseofincreasedpower\nconsumption and with a danger of interference when multiple active sensors are used at the\nsametime. Whetheractive orpassive, sensors canbe divided into three types, depending on\nwhethertheysensetheenvironment,therobot\u2019slocation,ortherobot\u2019sinternalconfiguration.\nRange finders are sensors that measure the distance to nearby objects. In the early\nRANGEFINDER\ndays of robotics, robots were commonly equipped with sonar sensors. Sonar sensors emit\nSONARSENSORS\ndirectional sound waves, which are reflected by objects, with some of the sound making it 974 Chapter 25. Robotics\n(a) (b)\nFigure25.3 (a) Time offlightcamera; image courtesyof Mesa ImagingGmbH. (b)3D\nrangeimageobtainedwiththiscamera.Therangeimagemakesitpossibletodetectobstacles\nandobjectsinarobot\u2019svicinity.\nback into the sensor. The time and intensity of the returning signal indicates the distance\nto nearby objects. Sonar is the technology of choice for autonomous underwater vehicles.\nStereovision(seeSection24.4.2)reliesonmultiplecamerastoimagethe environment from\nSTEREOVISION\nslightly different viewpoints, analyzing theresulting parallax intheseimagestocomputethe\nrange of surrounding objects. For mobile ground robots, sonar and stereo vision are now\nrarelyused,becausetheyarenotreliablyaccurate.\nMostgroundrobotsarenowequippedwithopticalrangefinders. Justlikesonarsensors,\nopticalrangesensorsemitactivesignals(light)andmeasurethetimeuntilareflectionofthis\nTIMEOFFLIGHT signalarrivesbackatthesensor. Figure25.3(a)showsa timeofflightcamera. Thiscamera\nCAMERA\nacquires range images like the one shown in Figure 25.3(b) at up to 60 frames per second.\nOther range sensors use laser beams and special 1-pixel cameras that can be directed using\ncomplex arrangements of mirrors or rotating elements. These sensors are called scanning\nlidars (short for light detection and ranging). Scanning lidars tend to provide longer ranges\nSCANNINGLIDARS\nthantimeofflightcameras,andtendtoperformbetterinbrightdaylight.\nOther common range sensors include radar, which is often the sensor of choice for\nUAVs. Radar sensors can measure distances of multiple kilometers. On the other extreme\nend of range sensing are tactile sensors such as whiskers, bump panels, and touch-sensitive\nTACTILESENSORS\nskin. These sensors measure range based on physical contact, and can be deployed only for\nsensingobjects veryclosetotherobot.\nA second important class of sensors is location sensors. Most location sensors use\nLOCATIONSENSORS\nrangesensingasaprimarycomponenttodeterminelocation. Outdoors,theGlobalPosition-\nGLOBAL\ning System (GPS)is the most common solution to the localization problem. GPS measures\nPOSITIONING\nSYSTEM\nthe distance to satellites that emit pulsed signals. At present, there are 31 satellites in orbit,\ntransmitting signalsonmultiple frequencies. GPSreceiverscanrecoverthedistance tothese\nsatellites by analyzing phase shifts. By triangulating signals from multiple satellites, GPS Section25.2. RobotHardware 975\nreceivers candetermine theirabsolute location onEarthto withinafewmeters. Differential\nGPSinvolves asecond ground receiver with known location, providing millimeteraccuracy\nDIFFERENTIALGPS\nunder ideal conditions. Unfortunately, GPS does not work indoors or underwater. Indoors,\nlocalization is often achieved by attaching beacons in the environment at known locations.\nMany indoor environments are full of wireless base stations, which can help robots localize\nthrough the analysis of the wireless signal. Underwater, active sonar beacons can provide a\nsenseoflocation, usingsoundtoinformAUVsoftheirrelativedistances tothosebeacons.\nPROPRIOCEPTIVE Thethirdimportantclassisproprioceptive sensors,whichinformtherobotofitsown\nSENSOR\nmotion. To measure the exact configuration of a robotic joint, motors are often equipped\nwithshaftdecodersthatcounttherevolution ofmotorsinsmallincrements. Onrobotarms,\nSHAFTDECODER\nshaft decoders can provide accurate information overany period of time. Onmobile robots,\nshaftdecoders thatreportwheelrevolutions canbeusedfor odometry\u2014themeasurementof\nODOMETRY\ndistance traveled. Unfortunately, wheels tend to drift and slip, so odometry is accurate only\nover short distances. External forces, such as the current for AUVs and the wind for UAVs,\nincrease positional uncertainty. Inertial sensors, such as gyroscopes, rely on the resistance\nINERTIALSENSOR\nofmasstothechangeofvelocity. Theycanhelpreduceuncertainty.\nOtherimportant aspects of robot state are measured by force sensors and torque sen-\nFORCESENSOR\nsors. Theseareindispensablewhenrobotshandlefragileobjectsorobjectswhoseexactshape\nTORQUESENSOR\nand location is unknown. Imagine aone-ton robotic manipulator screwing in alight bulb. It\nwould be all too easy to apply too much force and break the bulb. Force sensors allow the\nrobottosensehowharditisgripping thebulb, andtorquesensors allowittosensehowhard\nit is turning. Good sensors can measure forces in all three translational and three rotational\ndirections. Theydothisatafrequency ofseveralhundred timesasecond, sothatarobotcan\nquicklydetectunexpected forcesandcorrect itsactionsbeforeitbreaksalightbulb.\n25.2.2 Effectors\nEffectors are the means by which robots move and change the shape of their bodies. To\nunderstand thedesign ofeffectors, itwillhelptotalkaboutmotionandshapeintheabstract,\nDEGREEOF using the concept of a degree of freedom (DOF)We count one degree of freedom for each\nFREEDOM\nindependentdirectioninwhicharobot,oroneofitseffectors,canmove. Forexample,arigid\nmobile robot such as an AUV has six degrees of freedom, three for its (x,y,z) location in\nspace and three for its angular orientation, known as yaw, roll, and pitch. These six degrees\ndefinethekinematicstate2 orposeoftherobot. Thedynamicstateofarobotincludesthese\nKINEMATICSTATE\nsixplusanadditionalsixdimensionsfortherateofchangeofeachkinematicdimension,that\nPOSE\nis,theirvelocities.\nDYNAMICSTATE\nFornonrigidbodies,thereareadditional degreesoffreedomwithintherobotitself. For\nexample, the elbow of a human arm possesses two degree of freedom. It can flex the upper\narm towards oraway, and can rotate right orleft. Thewrist has three degrees offreedom. It\ncan move up and down, side to side, and can also rotate. Robot joints also have one, two,\nor three degrees of freedom each. Six degrees of freedom are required to place an object,\nsuch as a hand, at a particular point in a particular orientation. The arm in Figure 25.4(a)\n2 \u201cKinematic\u201disfromtheGreekwordformotion,asis\u201ccinema.\u201d 976 Chapter 25. Robotics\nhas exactly six degrees of freedom, created by five revolute joints that generate rotational\nREVOLUTEJOINT\nmotionandoneprismaticjointthatgeneratesslidingmotion. Youcanverifythatthehuman\nPRISMATICJOINT\narmasawholehasmorethansixdegreesoffreedom byasimpleexperiment: putyourhand\nonthetableandnoticethatyoustillhavethefreedom torotateyourelbowwithoutchanging\ntheconfigurationofyourhand. Manipulators thathaveextra degreesoffreedomareeasierto\ncontrol than robots with only the minimum number of DOFs. Many industrial manipulators\ntherefore havesevenDOFs,notsix.\nP\nR R\nR R\n\u03b8\nR\n(x, y)\n(a) (b)\nFigure25.4 (a)TheStanfordManipulator,anearlyrobotarmwithfiverevolutejoints(R)\nandoneprismaticjoint(P),foratotalofsix degreesoffreedom. (b)Motionofa nonholo-\nnomicfour-wheeledvehiclewithfront-wheelsteering.\nFormobilerobots,theDOFsarenotnecessarilythesameasthenumberofactuatedele-\nments. Consider,forexample,youraveragecar: itcanmoveforwardorbackward,anditcan\nturn, giving it two DOFs. In contrast, a car\u2019s kinematic configuration is three-dimensional:\nonanopenflatsurface, onecaneasily maneuveracartoany (x,y)point, inanyorientation.\n(SeeFigure 25.4(b).) Thus, the carhasthree effective degrees of freedom but twocontrol-\nEFFECTIVEDOF\nlable degrees of freedom. We say a robot is nonholonomic if it has more effective DOFs\nCONTROLLABLEDOF\nthan controllable DOFsand holonomic if the two numbers are the same. Holonomic robots\nNONHOLONOMIC\nareeasiertocontrol\u2014itwouldbemucheasiertoparkacarthatcouldmovesidewaysaswell\nasforwardandbackward\u2014but holonomicrobotsarealsomechanically morecomplex. Most\nrobotarmsareholonomic, andmostmobilerobotsarenonholonomic.\nMobile robots have a range of mechanisms for locomotion, including wheels, tracks,\nand legs. Differential drive robots possess two independently actuated wheels (or tracks),\nDIFFERENTIALDRIVE\none on each side, as on a military tank. If both wheels move at the same velocity, the robot\nmovesonastraight line. Iftheymoveinopposite directions, therobot turnsonthespot. An\nalternativeisthesynchrodrive,inwhicheachwheelcanmoveandturnarounditsownaxis.\nSYNCHRODRIVE\nTo avoid chaos, the wheels are tightly coordinated. When moving straight, for example, all\nwheelspointinthesamedirectionandmoveatthesamespeed. Bothdifferentialandsynchro\ndrives are nonholonomic. Some more expensive robots use holonomic drives, which have\nthreeormorewheelsthatcanbeoriented andmovedindependently.\nSome mobile robots possess arms. Figure 25.5(a) displays a two-armed robot. This\nrobot\u2019s arms use springs to compensate for gravity, and they provide minimal resistance to Section25.2. RobotHardware 977\n(a) (b)\nFigure 25.5 (a) Mobile manipulatorpluggingits charge cable into a wall outlet. Image\ncourtesyofWillowGarage,(cid:2)c 2009.(b)OneofMarcRaibert\u2019sleggedrobotsinmotion.\nexternal forces. Such a design minimizes the physical danger to people who might stumble\nintosucharobot. Thisisakeyconsideration indeploying robotsindomesticenvironments.\nLegs, unlike wheels, can handle rough terrain. However, legs are notoriously slow on\nflatsurfaces, andtheyaremechanically difficulttobuild. Roboticsresearchers havetriedde-\nsignsrangingfromoneleguptodozensoflegs. Leggedrobots havebeenmadetowalk,run,\nandevenhop\u2014asweseewiththeleggedrobotinFigure25.5(b). Thisrobotisdynamically\nDYNAMICALLY stable, meaning that it can remain upright while hopping around. A robot that can remain\nSTABLE\nupright without moving its legs is called statically stable. A robot is statically stable if its\nSTATICALLYSTABLE\ncenterofgravityisabovethepolygonspannedbyitslegs. Thequadruped(four-legged)robot\nshown in Figure 25.6(a) may appear statically stable. However, it walks by lifting multiple\nlegs atthesame time, which renders itdynamically stable. Therobot can walkon snow and\nice, and it will not fall over even if you kick it (as demonstrated in videos available online).\nTwo-leggedrobotssuchasthoseinFigure25.6(b)aredynamically stable.\nOther methods of movement are possible: air vehicles use propellers or turbines; un-\nderwater vehicles use propellers or thrusters, similar to those used on submarines. Robotic\nblimpsrelyonthermaleffectstokeepthemselvesaloft.\nSensorsandeffectorsalonedonotmakearobot. Acomplete robotalsoneedsasource\nof power to drive its effectors. The electric motor is the most popular mechanism for both\nELECTRICMOTOR\nPNEUMATIC manipulator actuation and locomotion, but pneumatic actuation using compressed gas and\nACTUATION\nHYDRAULIC hydraulicactuation usingpressurized fluidsalsohavetheirapplication niches.\nACTUATION 978 Chapter 25. Robotics\n(a) (b)\nFigure25.6 (a)Four-leggeddynamically-stablerobot\u201cBigDog.\u201d ImagecourtesyBoston\nDynamics,(cid:2)c 2009.(b)2009RoboCupStandardPlatformLeaguecompetition,showingthe\nwinningteam,B-Human,fromtheDFKIcenterattheUniversityofBremen.Throughoutthe\nmatch, B-Human outscored their opponents64:1. Their success was built on probabilistic\nstateestimationusingparticlefiltersandKalmanfilters;onmachine-learningmodelsforgait\noptimization;andondynamickickingmoves.ImagecourtesyDFKI,(cid:2)c 2009.\n25.3 ROBOTIC PERCEPTION\nPerceptionistheprocessbywhichrobotsmapsensormeasurementsintointernalrepresenta-\ntions of the environment. Perception is difficult because sensors are noisy, and the environ-\nment is partially observable, unpredictable, and often dynamic. In other words, robots have\nall the problems of state estimation (or filtering) that we discussed in Section 15.2. As a\nrule of thumb, good internal representations for robots have three properties: they contain\nenoughinformationfortherobottomakegooddecisions, theyarestructuredsothattheycan\nbe updated efficiently, and they are natural in the sense that internal variables correspond to\nnaturalstatevariablesinthephysical world.\nInChapter15, wesawthat Kalmanfilters, HMMs,anddynamic Bayesnets canrepre-\nsentthetransitionandsensormodelsofapartiallyobservableenvironment, andwedescribed\nbothexactandapproximate algorithms forupdating the beliefstate\u2014the posteriorprobabil-\nity distribution over the environment state variables. Several dynamic Bayes net models for\nthis process were shown in Chapter 15. For robotics problems, we include the robot\u2019s own\npast actions as observed variables in the model. Figure 25.7 shows the notation used in this\nchapter: X isthestateoftheenvironment(includingtherobot)attimet,Z istheobservation\nt t\nreceivedattimet,andA istheactiontakenaftertheobservation isreceived.\nt Section25.3. RoboticPerception 979\nA t\u22122 A t\u22121 A t\nX t\u22121 X t X t+1\nZ t\u22121 Z t Z t+1\nFigure 25.7 Robot perception can be viewed as temporal inference from sequences of\nactionsandmeasurements,asillustratedbythisdynamicBayesnetwork.\nWewould liketocompute thenew belief state, P(X | z ,a ), fromthecurrent\nt+1 1:t+1 1:t\nbelief state P(X\nt\n| z 1:t,a 1:t\u22121) and the new observation z t+1. We did this in Section 15.2,\nbut here there are two differences: we condition explicitly on the actions as well as the ob-\nservations, and wedeal with continuous rather than discrete variables. Thus, we modify the\nrecursivefilteringequation (15.5onpage572)touseintegration ratherthansummation:\nP(X |z ,a )\nt+1 1:t+1 1:t (cid:26)\n= \u03b1P(z\nt+1\n| X t+1) P(X\nt+1\n|x t,a t) P(x\nt\n|z 1:t,a 1:t\u22121) dx\nt\n. (25.1)\nThis equation states that the posterior over the state variables X at time t +1 is calculated\nrecursively from the corresponding estimate one time step earlier. This calculation involves\nthe previous action a and the current sensor measurement z . For example, if our goal\nt t+1\nis to develop a soccer-playing robot, X might be the location of the soccer ball relative\nt+1\nto the robot. The posterior P(X t|z 1:t,a 1:t\u22121) isa probability distribution overall states that\ncaptureswhatweknowfrompastsensormeasurementsandcontrols. Equation(25.1)tellsus\nhow to recursively estimate this location, by incrementally folding in sensor measurements\n(e.g.,cameraimages)androbotmotioncommands. TheprobabilityP(X |x ,a )iscalled\nt+1 t t\nthetransitionmodelormotionmodel,andP(z | X )isthesensormodel.\nMOTIONMODEL t+1 t+1\n25.3.1 Localizationand mapping\nLocalization is the problem of finding out where things are\u2014including the robot itself.\nLOCALIZATION\nKnowledge about where things are is at the core of any successful physical interaction with\nthe environment. For example, robot manipulators must know the location of objects they\nseektomanipulate; navigating robotsmustknowwheretheyaretofindtheirwayaround.\nTo keep things simple, let us consider a mobile robot that moves slowly in a flat 2D\nworld. Letusalso assume therobot isgiven anexact mapofthe environment. (Anexample\nof such a map appears in Figure 25.10.) The pose of such a mobile robot is defined by its\ntwo Cartesian coordinates with values x and y and its heading with value \u03b8, as illustrated in\nFigure25.8(a). Ifwearrange those threevalues inavector, thenanyparticular stateisgiven\n(cid:12)\nbyX =(x ,y ,\u03b8 ) . Sofarsogood.\nt t t t 980 Chapter 25. Robotics\n\u03c9 \u0394t\nt\nx y\ni, i\n\u03b8\nt+1\nh(x) v \u0394t\nt t\nZ Z Z Z\nx 1 2 3 4\nt+1\n\u03b8\nt\nx\nt\n(a) (b)\nFigure25.8 (a)Asimplifiedkinematicmodelofamobilerobot. Therobotisshownasa\ncirclewithaninteriorlinemarkingtheforwarddirection. Thestatextconsistsofthe(xt,yt)\nposition (shown implicitly) and the orientation \u03b8t. The new state xt+1 is obtained by an\nupdateinpositionofvt\u0394t andinorientationof\u03c9t\u0394t. Alsoshownisalandmarkat(xi,yi)\nobservedattimet. (b)Therange-scansensormodel.Twopossiblerobotposesareshownfor\nagivenrangescan(z ,z ,z ,z ). Itismuchmorelikelythattheposeontheleftgenerated\n1 2 3 4\ntherangescanthantheposeontheright.\nIn the kinematic approximation, each action consists of the \u201cinstantaneous\u201d specifica-\ntionoftwovelocities\u2014a translational velocity v andarotationalvelocity \u03c9 . Forsmalltime\nt t\nintervals \u0394t,acrudedeterministic modelofthemotionofsuchrobotsisgivenby\n\u239b \u239e\nv \u0394tcos\u03b8\nt t\nX\u02c6 = f(X ,v ,\u03c9 )= X +\u239d v \u0394tsin\u03b8 \u23a0 .\nt+1 t (cid:27)t(cid:28)(cid:29)(cid:30)t t t t\n\u03c9 \u0394t\nat t\nThe notation\nX\u02c6\nrefers to a deterministic state prediction. Of course, physical robots are\nsomewhat unpredictable. This is commonly modeled by a Gaussian distribution with mean\nf(X ,v ,\u03c9 )andcovariance \u03a3 . (SeeAppendixAforamathematicaldefinition.)\nt t t x\nP(X |X ,v ,\u03c9 ) = N(X\u02c6 ,\u03a3 ).\nt+1 t t t t+1 x\nThisprobability distribution istherobot\u2019s motionmodel. Itmodels theeffects ofthemotion\na onthelocation oftherobot.\nt\nNext, we need a sensor model. We will consider two kinds of sensor model. The\nfirst assumes that the sensors detect stable, recognizable features of the environment called\nlandmarks. Foreachlandmark,therangeandbearingarereported. Supposetherobot\u2019sstate\nLANDMARK\n(cid:12) (cid:12)\nisx =(x ,y ,\u03b8 ) anditsensesalandmarkwhoselocationisknowntobe(x ,y ) . Without\nt t t t i i\nnoise,therangeandbearingcanbecalculatedbysimplegeometry. (SeeFigure25.8(a).) The\nexactprediction oftheobservedrangeandbearingwouldbe\n(cid:13) (cid:10) (cid:14)\n(x \u2212x )2+(y \u2212y )2\n\u02c6z t =h(x t) = at rctani yi\u2212yt \u2212t \u03b8 i .\nxi\u2212xt t Section25.3. RoboticPerception 981\nAgain, noise distorts ourmeasurements. Tokeep things simple, onemight assume Gaussian\nnoisewithcovariance \u03a3 ,givingusthesensormodel\nz\nP(z |x ) = N(\u02c6z ,\u03a3 ).\nt t t z\nA somewhat different sensor model is used for an array of range sensors, each of which\nhas a fixed bearing relative to the robot. Such sensors produce a vector of range values\n(cid:12)\nz = (z ,...,z ) . Given apose x , let z\u02c6 be the exact range along the jth beam direction\nt 1 M t j\nfromx tothenearestobstacle. Asbefore,thiswillbecorruptedbyGaussiannoise. Typically,\nt\nwe assume that the errors for the different beam directions are independent and identically\ndistributed, sowehave\n(cid:25)M\nP(z |x ) = \u03b1 e\u2212(zj\u2212z\u02c6j)\/2\u03c32 .\nt t\nj=1\nFigure 25.8(b) shows an example of a four-beam range scan and two possible robot poses,\noneofwhichisreasonablylikelytohaveproducedtheobservedscanandoneofwhichisnot.\nComparing the range-scan model to the landmark model, we see that the range-scan model\nhas the advantage that there is no need to identify a landmark before the range scan can be\ninterpreted; indeed, in Figure 25.8(b), the robot faces a featureless wall. On the other hand,\nifthere arevisible, identifiable landmarks, theymayprovideinstant localization.\nChapter 15 described the Kalman filter, which represents the belief state as a single\nmultivariate Gaussian, andtheparticle filter,whichrepresents thebeliefstatebyacollection\nof particles that correspond to states. Most modern localization algorithms use one of two\nrepresentations oftherobot\u2019s belief P(X\nt\n|z 1:t,a 1:t\u22121).\nMONTECARLO Localization using particle filtering is called Monte Carlo localization, or MCL. The\nLOCALIZATION\nMCLalfgorithm isaninstance oftheparticle-filtering algorithm ofFigure15.17(page598).\nAll we need to do is supply the appropriate motion model and sensor model. Figure 25.9\nshowsoneversionusingtherange-scanmodel. Theoperationofthealgorithmisillustratedin\nFigure25.10astherobotfindsoutwhereitisinsideanofficebuilding. Inthefirstimage,the\nparticles areuniformly distributed based ontheprior, indicating globaluncertainty aboutthe\nrobot\u2019s position. In the second image, the first set of measurements arrives and the particles\nform clusters in the areas of high posterior belief. In the third, enough measurements are\navailable topushalltheparticles toasinglelocation.\nThe Kalman filter is the other major way to localize. A Kalman filter represents the\nposteriorP(X\nt\n|z 1:t,a 1:t\u22121)byaGaussian. ThemeanofthisGaussianwillbedenoted\u03bc tand\nitscovariance \u03a3 . ThemainproblemwithGaussianbeliefsisthattheyareonly closedunder\nt\nlinearmotionmodelsf andlinearmeasurementmodels h. Fornonlinear f orh,theresultof\nupdating a filteris in general not Gaussian. Thus, localization algorithms using the Kalman\nfilter linearize the motion and sensor models. Linearization is a local approximation of a\nLINEARIZATION\nnonlinear function by a linear function. Figure 25.11 illustrates the concept of linearization\nfora(one-dimensional) robotmotionmodel. Ontheleft,itdepictsanonlinearmotionmodel\nf(x ,a ) (the control a is omitted in this graph since it plays no role in the linearization).\nt t t\nOntheright,thisfunctionisapproximatedbyalinearfunctionf\u02dc(x ,a ). Thislinearfunction\nt t\nis tangent to f at the point \u03bc , the mean of ourstate estimate at time t. Such alinearization\nt 982 Chapter 25. Robotics\nfunctionMONTE-CARLO-LOCALIZATION(a,z,N,P(X(cid:5)|X, v, \u03c9),P(z|z\u2217),m)returns\nasetofsamplesforthenexttimestep\ninputs:a,robotvelocitiesv and\u03c9\nz,rangescanz 1,...,zM\nP(X(cid:5)|X, v, \u03c9),motionmodel\nP(z|z\u2217),rangesensornoisemodel\nm,2Dmapoftheenvironment\npersistent: S,avectorofsamplesofsizeN\nlocalvariables: W,avectorofweightsofsizeN\nS(cid:5),atemporaryvectorofparticlesofsizeN\nW(cid:5),avectorofweightsofsize N\nifS isemptythen \/*initializationphase*\/\nfori=1toN do\nS[i]\u2190samplefromP(X )\n0\nfori=1toN do \/*updatecycle*\/\nS(cid:5)[i]\u2190samplefromP(X(cid:5)|X =S[i],v,\u03c9)\nW(cid:5)[i]\u21901\nforj =1toM do\nz\u2217\u2190RAYCAST(j,X =S(cid:5)[i],m)\nW(cid:5)[i]\u2190W(cid:5)[i] \u00b7 P(zj|z\u2217)\nS\u2190WEIGHTED-SAMPLE-WITH-REPLACEMENT(N,S(cid:5),W(cid:5))\nreturnS\nFigure25.9 AMonteCarlolocalizationalgorithmusingarange-scansensormodelwith\nindependentnoise.\nis called (firstdegree) Taylor expansion. AKalman filterthat linearizes f and h via Taylor\nTAYLOREXPANSION\nexpansion is called an extended Kalman filter (or EKF). Figure 25.12 shows a sequence\nof estimates of a robot running an extended Kalman filter localization algorithm. As the\nrobotmoves,theuncertainty initslocationestimateincreases, asshownbytheerrorellipses.\nIts error decreases as it senses the range and bearing to a landmark with known location\nand increases again as the robot loses sight of the landmark. EKF algorithms work well if\nlandmarks are easily identified. Otherwise, the posterior distribution may be multimodal, as\ninFigure25.10(b). Theproblem ofneeding toknow theidentity oflandmarks isaninstance\nofthedataassociation problemdiscussed inFigure15.6.\nInsomesituations, nomapoftheenvironment isavailable. Thentherobotwillhaveto\nacquire amap. Thisis abit ofachicken-and-egg problem: the navigating robot willhave to\ndetermine its location relative to a map it doesn\u2019t quite know, at the same time building this\nmapwhileitdoesn\u2019tquiteknowitsactuallocation. Thisproblemisimportantformanyrobot\napplications, and it has been studied extensively under the name simultaneous localization\nSIMULTANEOUS\nandmapping,abbreviated asSLAM.\nLOCALIZATIONAND\nMAPPING\nSLAM problems are solved using many different probabilistic techniques, including\ntheextended Kalmanfilterdiscussed above. UsingtheEKFisstraightforward: justaugment Section25.3. RoboticPerception 983\nRobot position\n(a)\nRobot position\n(b)\nRobot position\n(c)\nFigure25.10 MonteCarlolocalization,aparticlefilteringalgorithmformobilerobotlo-\ncalization. (a)Initial,globaluncertainty. (b)Approximatelybimodaluncertaintyafternavi-\ngatinginthe(symmetric)corridor.(c)Unimodaluncertaintyafterenteringaroomandfinding\nittobedistinctive. 984 Chapter 25. Robotics\n~\nf(X, a) = f(\u03bc, a) + F(X \u2212 \u03bc)\nX X t t t t t t t\nt+1 f(X, a) t+1 f(X, a)\nt t t t\n~\n\u03a3 f(\u03bc, a) \u03a3 f(\u03bc, a)\nt+1 t t t+1 t t \u03a3\nt+1\n\u03bc X \u03bc X\nt t t t\n\u03a3 \u03a3\nt t\n(a) (b)\nFigure25.11 One-dimensionalillustrationofalinearizedmotionmodel:(a)Thefunction\nf,andtheprojectionofamean\u03bc\nt\nandacovarianceinterval(basedon\u03a3 t)intotimet+1.\n(b)Thelinearizedversionisthetangentoff at\u03bc . Theprojectionofthemean\u03bc iscorrect.\nt t\nHowever,theprojectedcovariance\u03a3\u02dc t+1differsfrom\u03a3 t+1.\nrobot\nlandmark\nFigure25.12 ExampleoflocalizationusingtheextendedKalmanfilter. Therobotmoves\nona straightline. As it progresses,its uncertaintyincreasesgradually,asillustrated bythe\nerrorellipses. Whenitobservesalandmarkwithknownposition,theuncertaintyisreduced.\nthe state vector to include the locations of the landmarks in the environment. Luckily, the\nEKFupdatescalesquadratically, soforsmallmaps(e.g.,afewhundredlandmarks)thecom-\nputation is quite feasible. Richer maps are often obtained using graph relaxation methods,\nsimilartotheBayesian network inference techniques discussed inChapter14. Expectation\u2013\nmaximization isalsousedforSLAM.\n25.3.2 Othertypes ofperception\nNot all of robot perception is about localization or mapping. Robots also perceive the tem-\nperature, odors, acoustic signals, andsoon. Manyofthese quantities canbeestimated using\nvariants of dynamic Bayes networks. Allthat is required for such estimators are conditional\nprobability distributions that characterize the evolution ofstate variables overtime, andsen-\nsormodelsthatdescribe therelationofmeasurementstostatevariables.\nIt is also possible to program a robot as a reactive agent, without explicitly reasoning\naboutprobability distributions overstates. Wecoverthat approach inSection25.6.3.\nThe trend in robotics is clearly towards representations with well-defined semantics. Section25.3. RoboticPerception 985\n(a) (b) (c)\nFigure 25.13 Sequence of \u201cdrivable surface\u201d classifier results using adaptive vision. In\n(a)onlytheroadisclassifiedasdrivable(stripedarea). TheV-shapeddarklineshowswhere\nthevehicleisheading. In(b)thevehicleiscommandedtodriveofftheroad,ontoa grassy\nsurface, and the classifier is beginningto classify some of the grass as drivable. In (c) the\nvehiclehasupdateditsmodelofdrivablesurfacetocorrespondtograssaswellasroad.\nProbabilistictechniquesoutperformotherapproachesinmanyhardperceptualproblemssuch\naslocalizationandmapping. However,statisticaltechniquesaresometimestoocumbersome,\nand simplersolutions maybejust as effective inpractice. Tohelp decide whichapproach to\ntake,experience workingwithrealphysical robotsisyourbestteacher.\n25.3.3 Machinelearning inrobot perception\nMachine learning plays an important role in robot perception. This is particularly the case\nwhen the best internal representation is not known. One common approach is to map high-\ndimensionalsensorstreamsintolower-dimensionalspacesusingunsupervisedmachinelearn-\nLOW-DIMENSIONAL ing methods (see Chapter 18). Such an approach is called low-dimensional embedding.\nEMBEDDING\nMachine learning makes it possible to learn sensor and motion models from data, while si-\nmultaneously discovering asuitableinternal representations.\nAnother machine learning technique enables robots to continuously adapt to broad\nchanges in sensor measurements. Picture yourself walking from a sun-lit space into a dark\nneon-litroom. Clearlythingsaredarkerinside. Butthechangeoflightsourcealsoaffectsall\nthe colors: Neon light has a stronger component of green light than sunlight. Yet somehow\nweseem not to notice the change. If wewalk together with people into a neon-lit room, we\ndon\u2019t think that suddenly theirfaces turned green. Ourperception quickly adapts tothe new\nlighting conditions, andourbrainignores thedifferences.\nAdaptive perception techniques enable robots to adjust to such changes. One example\nis shown in Figure 25.13, taken from the autonomous driving domain. Here an unmanned\nground vehicle adapts its classifier of the concept \u201cdrivable surface.\u201d How does this work?\nThe robot uses a laser to provide classification for a small area right in front of the robot.\nWhen this area is found to be flat in the laser range scan, it is used as a positive training\nexamplefortheconcept \u201cdrivable surface.\u201d Amixture-of-Gaussians technique similartothe\nEM algorithm discussed in Chapter 20 is then trained to recognize the specific color and\ntexture coefficients of the small sample patch. The images in Figure 25.13 are the result of\napplying thisclassifiertothefullimage. 986 Chapter 25. Robotics\nMethods that make robots collect theirowntraining data (with labels!) are called self-\nSELF-SUPERVISED supervised. Inthisinstance,therobotusesmachinelearningtoleverageashort-rangesensor\nLEARNING\nthat works wellforterrain classification into asensor that cansee much farther. Thatallows\ntherobot todrive faster, slowingdownonly whenthesensor modelsaysthere isachange in\ntheterrainthatneedstobeexaminedmorecarefully bytheshort-range sensors.\n25.4 PLANNING TO MOVE\nAllof arobot\u2019s deliberations ultimately comedown to deciding how tomoveeffectors. The\nPOINT-TO-POINT point-to-pointmotionproblemistodelivertherobotoritsendeffectortoadesignatedtarget\nMOTION\nlocation. A greater challenge is the compliant motion problem, in which a robot moves\nCOMPLIANTMOTION\nwhilebeinginphysicalcontact withanobstacle. Anexampleofcompliantmotionisarobot\nmanipulatorthatscrewsinalightbulb,orarobotthatpushesaboxacrossatabletop.\nWe begin by finding a suitable representation in which motion-planning problems can\nbedescribed andsolved. Itturnsoutthatthe configuration space\u2014thespaceofrobotstates\ndefined by location, orientation, and joint angles\u2014is abetterplace towork than the original\n3Dspace. Thepathplanningproblem istofindapathfromone configuration toanotherin\nPATHPLANNING\nconfigurationspace. Wehavealreadyencounteredvariousversionsofthepath-planningprob-\nlem throughout this book; the complication added by robotics is that path planning involves\ncontinuousspaces. Therearetwomainapproaches: celldecompositionandskeletonization.\nEach reduces the continuous path-planning problem to a discrete graph-search problem. In\nthissection,weassumethatmotionisdeterministicandthatlocalizationoftherobotisexact.\nSubsequent sections willrelaxtheseassumptions.\n25.4.1 Configurationspace\nWe will start with a simple representation for a simple robot motion problem. Consider the\nrobot arm shown in Figure 25.14(a). It has two joints that move independently. Moving\nthe joints alters the (x,y) coordinates of the elbow and the gripper. (The arm cannot move\nin the z direction.) This suggests that the robot\u2019s configuration can be described by a four-\ndimensionalcoordinate: (x ,y )forthelocationoftheelbowrelativetotheenvironmentand\ne e\n(x ,y ) for the location of the gripper. Clearly, these four coordinates characterize the full\ng g\nWORKSPACE state of the robot. They constitute what is known as workspace representation, since the\nREPRESENTATION\ncoordinates oftherobotarespecifiedinthesamecoordinate system astheobjects itseeksto\nmanipulate (or to avoid). Workspace representations are well-suited for collision checking,\nespecially iftherobotandallobjectsarerepresented bysimplepolygonal models.\nThe problem with the workspace representation is that not all workspace coordinates\nareactually attainable, eveninthe absence ofobstacles. Thisisbecause ofthe linkagecon-\nLINKAGE straints on the space of attainable workspace coordinates. Forexample, the elbow position\nCONSTRAINTS\n(x ,y ) and the gripper position (x ,y ) are always a fixed distance apart, because they are\ne e g g\njoined byarigid forearm. Arobot motion plannerdefined over workspace coordinates faces\nthe challenge of generating paths that adhere to these constraints. This is particularly tricky Section25.4. PlanningtoMove 987\ntable\neelb\ntable\nsshou\nee vertical\nobstacle\nleft wall\nss\n(a) (b)\nFigure25.14 (a)Workspacerepresentationofarobotarmwith2DOFs. Theworkspace\nis a box with a flat obstacle hangingfromthe ceiling. (b)Configurationspace of the same\nrobot. Onlywhiteregionsinthespaceareconfigurationsthatarefreeofcollisions. Thedot\ninthisdiagramcorrespondstotheconfigurationoftherobotshownontheleft.\nbecause thestatespaceiscontinuous andtheconstraints arenonlinear. Itturnsouttobeeas-\nCONFIGURATION iertoplanwithaconfigurationspacerepresentation. Insteadofrepresenting thestateofthe\nSPACE\nrobot by the Cartesian coordinates of its elements, we represent the state by a configuration\nof the robot\u2019s joints. Our example robot possesses two joints. Hence, we can represent its\nstate with the two angles \u03d5 and \u03d5 for the shoulder joint and elbow joint, respectively. In\ns e\ntheabsenceofanyobstacles,arobotcouldfreelytakeonanyvalueinconfigurationspace. In\nparticular, whenplanning apath one could simply connect the present configuration and the\ntarget configuration by a straight line. In following this path, the robot would then move its\njointsataconstant velocity, untilatargetlocationisreached.\nUnfortunately,configurationspaceshavetheirownproblems. Thetaskofarobotisusu-\nally expressed in workspace coordinates, not in configuration space coordinates. This raises\nthe question of how tomapbetween workspace coordinates and configuration space. Trans-\nforming configuration space coordinates into workspace coordinates is simple: it involves\na series of straightforward coordinate transformations. These transformations are linear for\nprismaticjointsandtrigonometricforrevolutejoints. Thischainofcoordinatetransformation\nisknownaskinematics.\nKINEMATICS\nTheinverseproblem ofcalculating theconfiguration ofarobotwhoseeffectorlocation\nINVERSE isspecifiedinworkspacecoordinatesisknownasinversekinematics. Calculatingtheinverse\nKINEMATICS\nkinematicsishard,especiallyforrobotswithmanyDOFs. Inparticular,thesolutionisseldom\nunique. Figure 25.14(a) shows one of twopossible configurations that put the gripper in the\nsamelocation. (Theotherconfiguration wouldhastheelbowbelowtheshoulder.) 988 Chapter 25. Robotics\nconf-2\nconf-1\nconf-3\nconf-3\nconf-1\nconf-2\nee\nss\n(a) (b)\nFigure25.15 Threerobotconfigurations,showninworkspaceandconfigurationspace.\nIn general, this two-link robot arm has between zero and two inverse kinematic solu-\ntions for any set of workspace coordinates. Most industrial robots have sufficient degrees\nof freedom to find infinitely many solutions to motion problems. To see how this is possi-\nble, simply imagine that we added a third revolute joint to our example robot, one whose\nrotational axis is parallel to the ones of the existing joints. In such a case, we can keep the\nlocation(butnottheorientation!) ofthegripperfixedandstillfreelyrotateitsinternaljoints,\nformostconfigurationsoftherobot. Withafewmorejoints(howmany?) wecanachievethe\nsame effect while keeping the orientation of the gripper constant as well. We have already\nseen an example of this in the \u201cexperiment\u201d of placing your hand on the desk and moving\nyour elbow. The kinematic constraint of your hand position is insufficient to determine the\nconfiguration of your elbow. In other words, the inverse kinematics of your shoulder\u2013arm\nassemblypossesses aninfinitenumberofsolutions.\nThe second problem with configuration space representations arises from the obsta-\ncles that may exist in the robot\u2019s workspace. Ourexample in Figure 25.14(a) shows several\nsuchobstacles, including afree-hanging obstacle thatprotrudes intothecenteroftherobot\u2019s\nworkspace. In workspace, such obstacles take on simple geometric forms\u2014especially in\nmost robotics textbooks, which tend to focus on polygonal obstacles. But how do they look\ninconfiguration space?\nFigure25.14(b)showstheconfigurationspaceforourexamplerobot,underthespecific\nobstacleconfigurationshowninFigure25.14(a). Theconfigurationspacecanbedecomposed\ninto twosubspaces: thespace ofallconfigurations that arobot mayattain, commonly called\nfree space, and the space of unattainable configurations, called occupied space. The white\nFREESPACE\narea inFigure 25.14(b) corresponds to the free space. All other regions correspond to occu-\nOCCUPIEDSPACE Section25.4. PlanningtoMove 989\npiedspace. Thedifferent shadings oftheoccupied spacecorresponds tothedifferent objects\nin the robot\u2019s workspace; the black region surrounding the entire free space corresponds to\nconfigurations inwhich the robot collides with itself. Itis easy tosee that extreme values of\nthe shoulder or elbow angles cause such a violation. The two oval-shaped regions on both\nsidesoftherobotcorrespondtothetableonwhichtherobotismounted. Thethirdovalregion\ncorresponds tothe leftwall. Finally, themostinteresting object inconfiguration space isthe\nverticalobstaclethathangsfromtheceilingandimpedestherobot\u2019smotions. Thisobjecthas\nafunnyshapeinconfiguration space: itishighlynonlinearandatplacesevenconcave. With\na little bit of imagination the reader will recognize the shape of the gripper at the upper left\nend. We encourage the reader to pause for a moment and study this diagram. The shape of\nthisobstacle isnot atallobvious! Thedotinside Figure25.14(b) markstheconfiguration of\nthe robot, as shown inFigure 25.14(a). Figure 25.15 depicts three additional configurations,\nboth in workspace and in configuration space. In configuration conf-1, the gripper encloses\ntheverticalobstacle.\nEveniftherobot\u2019sworkspaceisrepresentedbyflatpolygons,theshapeofthefreespace\ncan be very complicated. In practice, therefore, one usually probes a configuration space\ninstead ofconstructing it explicitly. Aplanner may generate aconfiguration and then test to\nsee ifit isin free space by applying the robot kinematics and then checking forcollisions in\nworkspace coordinates.\n25.4.2 Celldecompositionmethods\nCELL The first approach to path planning uses cell decomposition\u2014that is, it decomposes the\nDECOMPOSITION\nfree space into a finite number of contiguous regions, called cells. These regions have the\nimportant property that the path-planning problem within a single region can be solved by\nsimple means (e.g., movingalong astraight line). Thepath-planning problem then becomes\nadiscretegraph-searchproblem,verymuchlikethesearchproblemsintroducedinChapter3.\nThe simplest cell decomposition consists of a regularly spaced grid. Figure 25.16(a)\nshows a square grid decomposition of the space and a solution path that is optimal for this\ngridsize. Grayscale shadingindicates the valueofeachfree-space gridcell\u2014i.e.,thecostof\ntheshortestpathfromthatcelltothegoal. (Thesevaluescanbecomputedbyadeterministic\nformoftheVALUE-ITERATION algorithmgiveninFigure17.4onpage653.) Figure25.16(b)\n\u2217\nshowsthecorresponding workspacetrajectoryforthearm. Ofcourse,wecanalsousetheA\nalgorithm tofindashortest path.\nSuch a decomposition has the advantage that it is extremely simple to implement, but\nit also suffers from three limitations. First, itis workable only forlow-dimensional configu-\nrationspaces,becausethenumberofgridcellsincreasesexponentially withd,thenumberof\ndimensions. Sounds familiar? This is the curse!dimensionality@of dimensionality. Second,\nthereistheproblemofwhattodowithcellsthatare\u201cmixed\u201d\u2014thatis,neitherentirelywithin\nfree space norentirely within occupied space. Asolution path that includes such a cell may\nnot be areal solution, because there may be no wayto cross the cell in the desired direction\ninastraight line. Thiswouldmakethepathplanner unsound. Ontheotherhand, ifweinsist\nthatonlycompletely freecellsmaybeused, theplannerwillbeincomplete, becauseitmight 990 Chapter 25. Robotics\ngoal\nstart\ngoal\nstart\n(a) (b)\nFigure25.16 (a)Valuefunctionandpathfoundforadiscretegridcellapproximationof\ntheconfigurationspace. (b)Thesamepathvisualizedinworkspacecoordinates.Noticehow\ntherobotbendsitselbowtoavoidacollisionwiththeverticalobstacle.\nbethe casethat theonly paths tothegoal gothrough mixedcells\u2014especially ifthecell size\nis comparable to that of the passageways and clearances in the space. And third, any path\nthrough adiscretized statespacewillnotbesmooth. Itisgenerally difficulttoguarantee that\na smooth solution exists near the discrete path. So a robot may not be able to execute the\nsolution foundthroughthisdecomposition.\nCell decomposition methods can be improved in a number of ways, to alleviate some\noftheseproblems. Thefirstapproachallows furthersubdivision ofthemixedcells\u2014perhaps\nusing cells of half the original size. This can be continued recursively until a path is found\nthat lies entirely within free cells. (Of course, the method only works if there is a way to\ndecideifagivencellisamixedcell,whichiseasyonlyiftheconfigurationspaceboundaries\nhaverelativelysimplemathematicaldescriptions.) Thismethodiscompleteprovidedthereis\naboundonthesmallestpassagewaythroughwhichasolutionmustpass. Althoughitfocuses\nmost of the computational effort on the tricky areas within the configuration space, it still\nfails to scale well to high-dimensional problems because each recursive splitting of a cell\ncreates2d smallercells. Asecondwaytoobtainacompletealgorithmistoinsistonanexact\nEXACTCELL cell decomposition ofthe free space. Thismethod must allow cells tobe irregularly shaped\nDECOMPOSITION\nwhere they meet the boundaries of free space, but the shapes must still be \u201csimple\u201d in the\nsense that it should be easy to compute a traversal of any free cell. This technique requires\nsomequiteadvanced geometricideas,soweshallnotpursueitfurtherhere.\nExamining the solution path shown in Figure 25.16(a), we can see an additional diffi-\ncultythatwillhavetoberesolved. Thepathcontainsarbitrarilysharpcorners;arobotmoving\nat any finite speed could not execute such a path. This problem is solved by storing certain\ncontinuous values foreach grid cell. Consider an algorithm which stores, foreach grid cell, Section25.4. PlanningtoMove 991\nthe exact, continuous state that was attained with the cell was first expanded in the search.\nAssumefurther, thatwhenpropagating information tonearbygridcells,weusethiscontinu-\nousstateasabasis,andapplythecontinuousrobotmotionmodelforjumpingtonearbycells.\nIn doing so, we can now guarantee that the resulting trajectory is smooth and can indeed be\nexecutedbytherobot. Onealgorithm thatimplementsthisishybridA*.\nHYBRIDA*\n25.4.3 Modified costfunctions\nNoticethatinFigure25.16, thepathgoesveryclosetotheobstacle. Anyonewhohasdriven\nacarknowsthataparkingspacewithonemillimeterofclearanceoneithersideisnotreallya\nparking spaceatall;forthesamereason, wewouldprefersolution pathsthatarerobustwith\nrespecttosmallmotionerrors.\nThis problem can be solved by introducing a potential field. A potential field is a\nPOTENTIALFIELD\nfunctiondefinedoverstatespace,whosevaluegrowswiththedistancetotheclosestobstacle.\nFigure25.17(a)showssuchapotential field\u2014thedarkeraconfiguration state,thecloseritis\ntoanobstacle.\nThepotentialfieldcanbeusedasanadditionalcosttermintheshortest-pathcalculation.\nThisinducesaninterestingtradeoff. Ontheonehand,therobotseekstominimizepathlength\ntothegoal. Ontheotherhand,ittriestostayawayfromobstaclesbyvirtueofminimizingthe\npotentialfunction. Withtheappropriateweightbalancing thetwoobjectives, aresultingpath\nmaylook liketheone showninFigure 25.17(b). Thisfigurealsodisplays thevalue function\nderived from the combined cost function, again calculated by value iteration. Clearly, the\nresulting pathislonger, butitisalsosafer.\nThere exist many other ways to modify the cost function. For example, it may be\ndesirable to smooth the control parameters over time. For example, when driving a car, a\nsmooth pathisbetterthan ajerky one. Ingeneral, suchhigher-order constraints arenoteasy\ntoaccommodate intheplanning process, unless wemakethemostrecent steering command\napartofthestate. However,itisofteneasytosmooththeresulting trajectory afterplanning,\nusing conjugate gradient methods. Such post-planning smoothing is essential in many real-\nworldapplications.\n25.4.4 Skeletonization methods\nThesecondmajorfamilyofpath-planningalgorithmsisbasedontheideaofskeletonization.\nSKELETONIZATION\nThesealgorithmsreducetherobot\u2019sfreespacetoaone-dimensionalrepresentation, forwhich\ntheplanning problemiseasier. Thislower-dimensional representation iscalledaskeletonof\ntheconfiguration space.\nFigure 25.18 shows an example skeletonization: it is a Voronoi graph of the free\nVORONOIGRAPH\nspace\u2014the set of all points that are equidistant to two or more obstacles. To do path plan-\nning with a Voronoi graph, the robot first changes its present configuration to a point on the\nVoronoi graph. It is easy to show that this can always be achieved by a straight-line motion\ninconfiguration space. Second,therobotfollowstheVoronoigraphuntilitreachesthepoint\nnearest to the target configuration. Finally, the robot leaves the Voronoi graph and moves to\nthetarget. Again,thisfinalstepinvolves straight-line motioninconfiguration space. 992 Chapter 25. Robotics\nstart goal\n(a) (b)\nFigure 25.17 (a) A repelling potential field pushes the robot away from obstacles. (b)\nPathfoundbysimultaneouslyminimizingpathlengthandthepotential.\n(a) (b)\nFigure25.18 (a)TheVoronoigraphisthesetofpointsequidistanttotwoormoreobsta-\nclesinconfigurationspace. (b)Aprobabilisticroadmap,composedof400randomlychosen\npointsinfreespace.\nIn this way, the original path-planning problem is reduced to finding a path on the\nVoronoi graph, which is generally one-dimensional (except in certain nongeneric cases) and\nhasfinitelymanypointswherethreeormoreone-dimensional curvesintersect. Thus,finding Section25.5. PlanningUncertainMovements 993\nthe shortest path along the Voronoi graph is a discrete graph-search problem of the kind\ndiscussed in Chapters 3 and 4. Following the Voronoi graph may not give us the shortest\npath, but the resulting paths tend to maximize clearance. Disadvantages of Voronoi graph\ntechniquesarethattheyaredifficulttoapplytohigher-dimensional configurationspaces,and\nthat they tend to induce unnecessarily large detours when the configuration space is wide\nopen. Furthermore,computingtheVoronoigraphcanbedifficult,especially inconfiguration\nspace,wheretheshapesofobstacles canbecomplex.\nPROBABILISTIC An alternative to the Voronoi graphs is the probabilistic roadmap, a skeletonization\nROADMAP\napproach that offers morepossible routes, andthus deals betterwithwide-open spaces. Fig-\nure25.18(b)showsanexampleofaprobabilisticroadmap. Thegraphiscreatedbyrandomly\ngenerating a large number of configurations, and discarding those that do not fall into free\nspace. Two nodes are joined by an arc if it is \u201ceasy\u201d to reach one node from the other\u2013for\nexample, by a straight line in free space. The result of all this is a randomized graph in the\nrobot\u2019s free space. If we add the robot\u2019s start and goal configurations to this graph, path\nplanning amounts to adiscrete graph search. Theoretically, this approach is incomplete, be-\ncause a bad choice of random points may leave us without any paths from start to goal. It\nis possible to bound the probability of failure in terms of the number of points generated\nand certain geometric properties of the configuration space. It is also possible to direct the\ngeneration of sample points towards the areas where a partial search suggests that a good\npath may be found, working bidirectionally from both the start and the goal positions. With\ntheseimprovements,probabilisticroadmapplanningtendstoscalebettertohigh-dimensional\nconfiguration spacesthanmostalternative path-planning techniques.\n25.5 PLANNING UNCERTAIN MOVEMENTS\nNoneoftherobotmotion-planning algorithmsdiscussedthusfaraddressesakeycharacteris-\nticofroboticsproblems: uncertainty. Inrobotics,uncertaintyarisesfrompartialobservability\noftheenvironment andfromthestochastic (orunmodeled) effectsoftherobot\u2019s actions. Er-\nrorscanalsoarisefrom theuseofapproximation algorithms suchasparticle filtering, which\ndoesnotprovide therobotwithanexactbelief stateevenifthestochastic nature oftheenvi-\nronmentismodeledperfectly.\nMost of today\u2019s robots use deterministic algorithms for decision making, such as the\npath-planning algorithms of the previous section. To do so, it is common practice to extract\nthe most likely state from the probability distribution produced by the state estimation al-\nMOSTLIKELYSTATE\ngorithm. The advantage of this approach is purely computational. Planning paths through\nconfiguration space is already a challenging problem; it would be worse if we had to work\nwith a full probability distribution overstates. Ignoring uncertainty in this way works when\ntheuncertainty issmall. Infact,whentheenvironment modelchangesovertimeastheresult\nofincorporating sensormeasurements, manyrobotsplanpathsonlineduring planexecution.\nThisistheonlinereplanningtechnique ofSection11.3.3.\nONLINEREPLANNING 994 Chapter 25. Robotics\nUnfortunately, ignoring the uncertainty does not always work. In some problems the\nrobot\u2019s uncertainty is simply too massive: How can we use a deterministic path planner to\ncontrol amobile robot that has no clue where itis? Ingeneral, if the robot\u2019s true state is not\nthe one identified by the maximum likelihood rule, the resulting control will be suboptimal.\nDepending on the magnitude of the error this can lead to all sorts of unwanted effects, such\nascollisions withobstacles.\nThefieldofroboticshasadoptedarangeoftechniquesforaccommodatinguncertainty.\nSomearederived from thealgorithms given inChapter17fordecision making underuncer-\ntainty. Iftherobotfacesuncertaintyonlyinitsstatetransition,butitsstateisfullyobservable,\ntheproblemisbestmodeledasaMarkovdecisionprocess(MDP).ThesolutionofanMDPis\nanoptimalpolicy,whichtellstherobotwhattodoineverypossible state. Inthisway,itcan\nhandleallsortsofmotionerrors,whereasasingle-path solution fromadeterministic planner\nNAVIGATION would be muchless robust. Inrobotics, policies arecalled navigation functions. Thevalue\nFUNCTION\nfunction shown in Figure 25.16(a) can be converted into such a navigation function simply\nbyfollowingthegradient.\nJustasinChapter17,partialobservability makestheproblemmuchharder. Theresult-\ning robot control problem is a partially observable MDP,or POMDP.In such situations, the\nrobot maintains aninternal beliefstate, liketheonesdiscussed inSection25.3. Thesolution\nto a POMDP is a policy defined over the robot\u2019s belief state. Put differently, the input to\nthepolicyisanentire probability distribution. Thisenables therobottobase itsdecision not\nonly on what it knows, but also on what it does not know. For example, if it is uncertain\nINFORMATION aboutacriticalstatevariable,itcanrationallyinvokeaninformationgatheringaction. This\nGATHERINGACTION\nis impossible inthe MDPframework, since MDPsassume full observability. Unfortunately,\ntechniquesthatsolvePOMDPsexactlyareinapplicabletorobotics\u2014therearenoknowntech-\nniquesforhigh-dimensionalcontinuousspaces. DiscretizationproducesPOMDPsthatarefar\ntoo large to handle. Oneremedy isto makethe minimization of uncertainty acontrol objec-\nCOASTAL tive. For example, the coastal navigation heuristic requires the robot to stay near known\nNAVIGATION\nlandmarks to decrease its uncertainty. Another approach applies variants of the probabilis-\ntic roadmap planning method to the belief space representation. Such methods tend to scale\nbettertolargediscretePOMDPs.\n25.5.1 Robustmethods\nUncertaintycanalsobehandledusingso-calledrobustcontrolmethods(seepage836)rather\nROBUSTCONTROL\nthan probabilistic methods. A robust method is one that assumes a bounded amount of un-\ncertainty in each aspect of a problem, but does not assign probabilities to values within the\nallowed interval. A robust solution is one that works no matter what actual values occur,\nprovidedtheyarewithintheassumedinterval. Anextremeformofrobustmethodisthecon-\nformant planningapproach given in Chapter11\u2014it produces plans that work with nostate\ninformation atall.\nFINE-MOTION Here, we look at a robust method that is used for fine-motion planning (or FMP) in\nPLANNING\nrobotic assembly tasks. Fine-motion planning involves moving a robot arm in very close\nproximity to a static environment object. The main difficulty with fine-motion planning is Section25.5. PlanningUncertainMovements 995\ninitial Cv\nconfiguration\nmotion v\nenvelope\nFigure25.19 Atwo-dimensionalenvironment,velocityuncertaintycone,andenvelopeof\npossiblerobotmotions. Theintendedvelocityis v, butwithuncertaintytheactualvelocity\ncouldbeanywhereinCv,resultinginafinalconfigurationsomewhereinthemotionenvelope,\nwhichmeanswewouldn\u2019tknowifwehittheholeornot.\ninitial\nCv\nconfiguration\nv\nmotion\nenvelope\nFigure25.20 Thefirstmotioncommandandtheresultingenvelopeofpossiblerobotmo-\ntions. No matter what the error, we know the final configuration will be to the left of the\nhole.\nthattherequiredmotionsandtherelevantfeaturesoftheenvironmentareverysmall. Atsuch\nsmallscales,therobotisunabletomeasureorcontrolitspositionaccuratelyandmayalsobe\nuncertain of the shape of the environment itself; we will assume that these uncertainties are\nall bounded. The solutions to FMP problems will typically be conditional plans or policies\nthatmakeuseofsensorfeedbackduringexecutionandareguaranteedtoworkinallsituations\nconsistent withtheassumeduncertainty bounds.\nA fine-motion plan consists of a series of guarded motions. Each guarded motion\nGUARDEDMOTION\nconsistsof(1)amotioncommandand(2)aterminationcondition,whichisapredicateonthe\nrobot\u2019s sensor values, and returns true to indicate the end of the guarded move. The motion\ncommands are typically compliant motions that allow the effector to slide if the motion\nCOMPLIANTMOTION\ncommandwouldcausecollisionwithanobstacle. Asanexample,Figure25.19showsatwo-\ndimensional configuration space with a narrow vertical hole. It could be the configuration\nspace forinsertion ofarectangular pegintoaholeoracarkeyintotheignition. Themotion\ncommandsareconstant velocities. Thetermination conditions arecontact withasurface. To\nmodeluncertainty incontrol, weassumethatinsteadofmovinginthecommandeddirection,\nthe robot\u2019s actual motion lies in the cone C about it. The figure shows what would happen\nv 996 Chapter 25. Robotics\nCv\nv\nmotion\nenvelope\nFigure25.21 Thesecondmotioncommandandtheenvelopeofpossiblemotions. Even\nwitherror,wewilleventuallygetintothehole.\nif we commanded a velocity straight down from the initial configuration. Because of the\nuncertainty in velocity, the robot could move anywhere in the conical envelope, possibly\ngoing into the hole, but more likely landing to one side of it. Because the robot would not\nthenknowwhichsideoftheholeitwason,itwouldnotknowwhichwaytomove.\nA more sensible strategy is shown in Figures 25.20 and 25.21. In Figure 25.20, the\nrobotdeliberatelymovestoonesideofthehole. Themotioncommandisshowninthefigure,\nand the termination test is contact with any surface. In Figure 25.21, a motion command is\ngiven that causes the robot to slide along the surface and into the hole. Because all possible\nvelocities inthemotionenvelope aretotheright, therobot willslidetotherightwheneverit\nis in contact with a horizontal surface. It will slide down the right-hand vertical edge of the\nholewhenittouches it,becauseallpossiblevelocities aredownrelativetoaverticalsurface.\nIt will keep moving until it reaches the bottom of the hole, because that is its termination\ncondition. In spite of the control uncertainty, all possible trajectories of the robot terminate\nincontactwiththebottomofthehole\u2014thatis,unlesssurfaceirregularitiescausetherobotto\nstickinoneplace.\nAs one might imagine, the problem of constructing fine-motion plans is not trivial; in\nfact, it is a good deal harder than planning with exact motions. One can either choose a\nfixed number of discrete values foreach motion oruse the environment geometry to choose\ndirections that givequalitatively different behavior. Afine-motion planner takes asinput the\nconfiguration-spacedescription,theangleofthevelocityuncertaintycone,andaspecification\nofwhatsensing ispossible fortermination (surface contact inthiscase). Itshould produce a\nmultistepconditional planorpolicythatisguaranteed tosucceed, ifsuchaplanexists.\nOurexample assumes that the planner has an exact model ofthe environment, but itis\npossible to allow forbounded errorinthis modelas follows. Iftheerror canbedescribed in\ntermsofparameters,thoseparameterscanbeaddedasdegreesoffreedomtotheconfiguration\nspace. In the last example, if the depth and width of the hole were uncertain, we could add\nthem as two degrees of freedom to the configuration space. It is impossible to move the\nrobot in these directions in the configuration space or to sense its position directly. But\nboththoserestrictionscanbeincorporated whendescribingthisproblemasanFMPproblem\nby appropriately specifying control and sensor uncertainties. This gives a complex, four-\ndimensional planning problem, but exactly the same planning techniques can be applied. Section25.6. Moving 997\nNoticethatunlikethedecision-theoretic methodsinChapter17,thiskindofrobustapproach\nresults in plans designed for the worst-case outcome, rather than maximizing the expected\nqualityoftheplan. Worst-caseplansareoptimalinthedecision-theoretic senseonlyiffailure\nduringexecution ismuchworsethananyoftheothercostsinvolved inexecution.\n25.6 MOVING\nSo far, we have talked about how to plan motions, but not about how to move. Ourplans\u2014\nparticularlythoseproducedbydeterministicpathplanners\u2014assumethattherobotcansimply\nfollowanypaththatthealgorithm produces. Intherealworld,ofcourse, thisisnotthecase.\nRobots have inertia and cannot execute arbitrary paths except at arbitrarily slow speeds. In\nmostcases,therobotgetstoexertforcesratherthanspecifypositions. Thissectiondiscusses\nmethodsforcalculating theseforces.\n25.6.1 Dynamicsand control\nSection 25.2introduced thenotion of dynamicstate, whichextends thekinematic state ofa\nrobotbyitsvelocity. Forexample,inaddition totheangleofarobotjoint, thedynamicstate\nalso captures the rate of change of the angle, and possibly even its momentary acceleration.\nThe transition model for a dynamic state representation includes the effect of forces on this\nDIFFERENTIAL rate of change. Such models are typically expressed via differential equations, which are\nEQUATION\nequations that relate a quantity (e.g., a kinematic state) to the change of the quantity over\ntime(e.g.,velocity). Inprinciple, wecould havechosen toplanrobot motionusing dynamic\nmodels, instead of ourkinematic models. Such a methodology would lead to superior robot\nperformance, if wecould generate the plans. However, the dynamic state has higher dimen-\nsion than the kinematic space, and the curse of dimensionality would render many motion\nplanningalgorithmsinapplicable forallbutthemostsimplerobots. Forthisreason,practical\nrobotsystemoftenrelyonsimplerkinematicpathplanners.\nA common technique to compensate for the limitations of kinematic plans is to use a\nseparate mechanism, acontroller, forkeeping therobotontrack. Controllers aretechniques\nCONTROLLER\nfor generating robot controls in real time using feedback from the environment, so as to\nachieve a control objective. If the objective is to keep the robot on a preplanned path, it is\nREFERENCE oftenreferredtoasareferencecontrollerandthepathiscalledareferencepath. Controllers\nCONTROLLER\nthat optimize aglobal cost function are known as optimal controllers. Optimal policies for\nREFERENCEPATH\nOPTIMAL continuous MDPsare,ineffect,optimalcontrollers.\nCONTROLLERS\nOn the surface, the problem of keeping a robot on a prespecified path appears to be\nrelatively straightforward. In practice, however, even this seemingly simple problem has its\npitfalls. Figure 25.22(a) illustrates what can go wrong; it shows the path of a robot that\nattempts to follow a kinematic path. Whenever a deviation occurs\u2014whether due to noise or\ntoconstraints ontheforcestherobotcanapply\u2014therobotprovidesanopposingforcewhose\nmagnitude is proportional to this deviation. Intuitively, this might appear plausible, since\ndeviations should be compensated by a counterforce to keep the robot on track. However, 998 Chapter 25. Robotics\n(a) (b) (c)\nFigure 25.22 Robot arm control using (a) proportionalcontrol with gain factor 1.0, (b)\nproportionalcontrolwith gain factor 0.1, and (c) PD (proportionalderivative)controlwith\ngainfactors0.3fortheproportionalcomponentand0.8forthedifferentialcomponent.Inall\ncasestherobotarmtriestofollowthepathshowningray.\nas Figure 25.22(a) illustrates, our controller causes the robot to vibrate rather violently. The\nvibration is the result of a natural inertia of the robot arm: once driven back to its reference\npositiontherobotthenovershoots,whichinducesasymmetricerrorwithoppositesign. Such\novershooting may continue along an entire trajectory, and the resulting robot motion is far\nfromdesirable.\nBefore we can define a better controller, let us formally describe what went wrong.\nControllers that provide force in negative proportion to the observed error are known as P\ncontrollers. The letter \u2018P\u2019 stands for proportional, indicating that the actual control is pro-\nPCONTROLLER\nportional totheerroroftherobotmanipulator. Moreformally, lety(t)bethereference path,\nparameterized bytimeindex t. Thecontrol a generated byaPcontrollerhastheform:\nt\na = K (y(t)\u2212x ).\nt P t\nHerex isthestateoftherobotattimetandK isaconstantknownasthegainparameterof\nGAINPARAMETER t P\nthecontrolleranditsvalueiscalledthegainfactor); K regulateshowstronglythecontroller\np\ncorrects fordeviations between the actual state x and the desired one y(t). Inour example,\nt\nK = 1. At first glance, one might think that choosing a smaller value for K would\nP P\nremedy the problem. Unfortunately, this is not the case. Figure 25.22(b) shows a trajectory\nfor K = .1, still exhibiting oscillatory behavior. Lower values of the gain parameter may\nP\nsimply slow down the oscillation, but do not solve the problem. In fact, in the absence of\nfriction, the P controller is essentially a spring law; so it will oscillate indefinitely around a\nfixedtargetlocation.\nTraditionally, problems of this type fall into the realm of control theory, a field of\nincreasingimportancetoresearchersinAI.Decadesofresearchinthisfieldhaveledtoalarge\nnumberofcontrollers thataresuperiortothesimplecontrollawgivenabove. Inparticular, a\nreferencecontrollerissaidtobestableifsmallperturbationsleadtoaboundederrorbetween\nSTABLE\nthe robot and the reference signal. It is said to be strictly stable if it is able to return to and\nSTRICTLYSTABLE Section25.6. Moving 999\nthenstayonitsreference pathuponsuchperturbations. Our Pcontrollerappears tobestable\nbutnotstrictlystable, sinceitfailstostayanywherenear itsreference trajectory.\nThe simplest controller that achieves strict stability in our domain is a PD controller.\nPDCONTROLLER\nTheletter\u2018P\u2019standsagainforproportional, and\u2018D\u2019standsforderivative. PDcontrollers are\ndescribed bythefollowingequation:\n\u2202(y(t)\u2212x )\na = K (y(t)\u2212x )+K t . (25.2)\nt P t D\n\u2202t\nAs this equation suggests, PD controllers extend P controllers by a differential component,\nwhich adds to the value of a a term that is proportional to the first derivative of the error\nt\ny(t)\u2212x overtime. Whatistheeffectofsuchaterm? Ingeneral, aderivativetermdampens\nt\nthesystemthatisbeingcontrolled. Toseethis,considerasituationwheretheerror(y(t)\u2212x )\nt\nischanging rapidlyovertime,asisthecaseforourPcontroller above. Thederivativeofthis\nerror will then counteract the proportional term, which will reduce the overall response to\ntheperturbation. However,ifthesameerrorpersists anddoesnotchange, thederivativewill\nvanishandtheproportional termdominates thechoiceofcontrol.\nFigure25.22(c) showstheresult ofapplying thisPDcontroller toourrobot arm, using\nasgainparameters K = .3andK = .8. Clearly,theresulting pathismuchsmoother, and\nP D\ndoesnotexhibitanyobviousoscillations.\nPD controllers do have failure modes, however. In particular, PD controllers may fail\nto regulate an error down to zero, even in the absence of external perturbations. Often such\na situation is the result of a systematic external force that is not part of the model. An au-\ntonomouscardrivingonabanked surface, forexample, mayfinditselfsystematically pulled\nto one side. Wearand tear in robot arms cause similar systematic errors. In such situations,\nanover-proportionalfeedbackisrequiredtodrivetheerrorclosertozero. Thesolutiontothis\nproblemliesinaddingathirdtermtothecontrollaw,basedontheintegratederrorovertime:\n(cid:26)\n\u2202(y(t)\u2212x )\na = K (y(t)\u2212x )+K (y(t)\u2212x )dt+K t . (25.3)\nt P t I t D\n\u2202t\n+\nHereK isyetanothergainparameter. Theterm (y(t)\u2212x )dtcalculatestheintegralofthe\nI t\nerror over time. The effect of this term is that long-lasting deviations between the reference\nsignal and the actual state are corrected. If, for example, x is smaller than y(t) for a long\nt\nperiodoftime,thisintegralwillgrowuntiltheresulting control a forcesthiserrortoshrink.\nt\nIntegral terms, then, ensure thatacontroller doesnotexhibitsystematic error, attheexpense\nofincreased danger ofoscillatory behavior. Acontroller withallthree terms iscalled a PID\ncontroller (forproportional integral derivative). PIDcontrollers arewidelyused inindustry,\nPIDCONTROLLER\nforavarietyofcontrolproblems.\n25.6.2 Potential-fieldcontrol\nWe introduced potential fields as an additional cost function in robot motion planning, but\ntheycanalsobeusedforgenerating robotmotiondirectly, dispensing withthepathplanning\nphase altogether. To achieve this, we have to define an attractive force that pulls the robot\ntowardsitsgoalconfiguration andarepellent potential fieldthat pushes therobot awayfrom\nobstacles. Such a potential field is shown in Figure 25.23. Its single global minimum is 1000 Chapter 25. Robotics\ngoal start goal\nstart\n(a) (b)\nFigure 25.23 Potential field control. The robot ascends a potential field composed of\nrepelling forces asserted from the obstacles and an attracting force that correspondsto the\ngoalconfiguration.(a)Successfulpath. (b)Localoptimum.\nthe goal configuration, and the value is the sum of the distance to this goal configuration\nand the proximity to obstacles. No planning was involved in generating the potential field\nshown in the figure. Because of this, potential fields are well suited to real-time control.\nFigure 25.23(a) shows a trajectory of a robot that performs hill climbing in the potential\nfield. In many applications, the potential field can be calculated efficiently for any given\nconfiguration. Moreover, optimizing the potential amounts to calculating the gradient of the\npotential for the present robot configuration. These calculations can be extremely efficient,\nespecially when compared to path-planning algorithms, all of which are exponential in the\ndimensionality oftheconfiguration space(theDOFs)intheworstcase.\nThe fact that the potential field approach manages to find a path to the goal in such\nan efficient manner, even over long distances in configuration space, raises the question as\nto whether there is a need for planning in robotics at all. Are potential field techniques\nsufficient, or were we just lucky in our example? The answer is that we were indeed lucky.\nPotentialfieldshavemanylocalminimathatcantraptherobot. InFigure25.23(b),therobot\napproaches theobstacle bysimplyrotating itsshoulderjoint, untilitgetsstuckonthewrong\nside of the obstacle. The potential field is not rich enough to make the robot bend its elbow\nsothatthearmfitsundertheobstacle. Inotherwords,potentialfieldcontrolisgreatforlocal\nrobotmotionbutsometimeswestillneedglobalplanning. Anotherimportantdrawbackwith\npotentialfieldsisthattheforcestheygeneratedependonlyontheobstacleandrobotpositions,\nnotontherobot\u2019svelocity. Thus,potentialfieldcontrolisreallyakinematicmethodandmay\nfailiftherobotismovingquickly. Section25.6. Moving 1001\nretract, lift higher\nyes\nmove\nforward no\nS 3 stuck? S 4\nlift up set down\npush backward\nS S\n2 1\n(a) (b)\nFigure 25.24 (a) Genghis, a hexapod robot. (b) An augmented finite state machine\n(AFSM) for the control of a single leg. Notice that this AFSM reacts to sensor feedback:\nifalegisstuckduringtheforwardswingingphase,itwillbeliftedincreasinglyhigher.\n25.6.3 Reactivecontrol\nSofarwehave considered control decisions that require some modelof theenvironment for\nconstructing either areference path orapotential field. There are some difficulties with this\napproach. First, models that are sufficiently accurate are often difficult to obtain, especially\nin complex or remote environments, such as the surface of Mars, or for robots that have\nfew sensors. Second, even in cases where we can devise a model with sufficient accuracy,\ncomputational difficulties and localization error might render these techniques impractical.\nInsomecases,areflexagentarchitecture using reactive controlismoreappropriate.\nREACTIVECONTROL\nForexample,picturealeggedrobotthatattemptstoliftalegoveranobstacle. Wecould\ngivethisrobotarulethatsaysliftthelegasmallheight handmoveitforward,andiftheleg\nencounters anobstacle, moveitbackandstart againatahigher height. Youcould saythat h\nismodeling anaspect oftheworld, butwecan alsothink of hasanauxiliary variable ofthe\nrobotcontroller, devoidofdirectphysicalmeaning.\nOne such example is the six-legged (hexapod) robot, shown in Figure 25.24(a), de-\nsigned forwalking through rough terrain. Therobot\u2019s sensors areinadequate toobtain mod-\nelsoftheterrainforpathplanning. Moreover,evenifweaddedsufficientlyaccuratesensors,\nthe twelve degrees of freedom (two for each leg) would render the resulting path planning\nproblem computationally intractable.\nIt is possible, nonetheless, to specify a controller directly without an explicit environ-\nmental model. (We have already seen this with the PD controller, which was able to keep a\ncomplexrobotarmontarget withoutanexplicitmodeloftherobotdynamics;itdid,however,\nrequire a reference path generated from a kinematic model.) Forthe hexapod robot we first\nchoose agait, orpattern ofmovementofthelimbs. Onestatically stable gaitistofirstmove\nGAIT\nthe right front, right rear, and left center legs forward (keeping the other three fixed), and\nthen move the other three. This gait works well on flat terrain. On rugged terrain, obstacles\nmay prevent a leg from swinging forward. This problem can be overcome by a remarkably\nsimple control rule: when a leg\u2019s forward motion is blocked, simply retract it, lift it higher, 1002 Chapter 25. Robotics\nFigure25.25 Multiple exposuresofan RC helicopterexecutinga flip based ona policy\nlearnedwithreinforcementlearning.ImagescourtesyofAndrewNg,StanfordUniversity.\nandtry again. Theresulting controller isshowninFigure25.24(b) asafinitestate machine;\nitconstitutes areflex agent with state, where the internal state isrepresented by the index of\nthecurrentmachinestate(s through s ).\n1 4\nVariantsofthissimplefeedback-driven controllerhavebeenfoundtogenerate remark-\nably robust walking patterns, capable ofmaneuvering therobot overrugged terrain. Clearly,\nsuch a controller is model-free, and it does not deliberate or use search for generating con-\ntrols. Environmental feedbackplaysacrucialroleinthecontroller\u2019sexecution. Thesoftware\nalonedoesnotspecifywhatwillactuallyhappenwhentherobotisplacedinanenvironment.\nBehavior that emerges through the interplay of a (simple) controller and a (complex) envi-\nEMERGENT ronment is often referred to as emergent behavior. Strictly speaking, all robots discussed\nBEHAVIOR\nin this chapter exhibit emergent behavior, due to the fact that no model is perfect. Histori-\ncally, however, the term has been reserved for control techniques that do not utilize explicit\nenvironmental models. Emergentbehaviorisalsocharacteristic ofbiological organisms.\n25.6.4 Reinforcement learning control\nOneparticularlyexcitingformofcontrolisbasedonthepolicysearchformofreinforcement\nlearning (see Section 21.5). This work has been enormously influential in recent years, at\nis has solved challenging robotics problems for which previously no solution existed. An\nexample is acrobatic autonomous helicopter flight. Figure 25.25 shows an autonomous flip\nof a small RC (radio-controlled) helicopter. This maneuver is challenging due to the highly\nnonlinear nature of the aerodynamics involved. Only the most experienced of human pilots\nareable toperform it. Yetapolicy search method(asdescribed inChapter21), using only a\nfewminutesofcomputation, learnedapolicythatcansafely executeaflipeverytime.\nPolicy search needs an accurate model of the domain before it can find a policy. The\ninput to this model is the state of the helicopter at time t, the controls at time t, and the\nresultingstateattimet+\u0394t. Thestateofahelicoptercanbedescribedbythe3Dcoordinates\nof the vehicle, its yaw, pitch, and roll angles, and the rate of change of these six variables.\nThe controls are the manual controls of of the helicopter: throttle, pitch, elevator, aileron,\nand rudder. Allthat remains is the resulting state\u2014how are wegoing to define a model that\naccurately says how the helicopter responds to each control? The answer is simple: Let an\nexpert human pilot fly the helicopter, and record the controls that the expert transmits over\nthe radio and the state variables of the helicopter. About four minutes of human-controlled\nflightsufficestobuildapredictive modelthatissufficiently accuratetosimulatethevehicle. Section25.7. RoboticSoftwareArchitectures 1003\nWhat is remarkable about this example is the ease with which this learning approach\nsolvesachallengingroboticsproblem. Thisisoneofthemanysuccessesofmachinelearning\ninscientificfieldspreviously dominated bycarefulmathematicalanalysis andmodeling.\n25.7 ROBOTIC SOFTWARE ARCHITECTURES\nSOFTWARE Amethodologyforstructuring algorithmsiscalleda softwarearchitecture. Anarchitecture\nARCHITECTURE\nincludes languages and tools forwriting programs, as wellas an overall philosophy forhow\nprogramscanbebroughttogether.\nModern-day software architectures for robotics must decide how to combine reactive\ncontrol and model-based deliberative planning. In many ways, reactive and deliberate tech-\nniques have orthogonal strengths and weaknesses. Reactive control is sensor-driven and ap-\npropriate for making low-level decisions in real time. However, it rarely yields a plausible\nsolutionatthegloballevel,becauseglobalcontroldecisionsdependoninformationthatcan-\nnot be sensed at the time of decision making. For such problems, deliberate planning is a\nmoreappropriate choice.\nConsequently, most robot architectures use reactive techniques at the lower levels of\ncontrol anddeliberative techniques atthe higherlevels. Weencountered such acombination\nin our discussion of PD controllers, where we combined a (reactive) PD controller with a\n(deliberate) path planner. Architectures that combine reactive and deliberate techniques are\nHYBRID calledhybridarchitectures.\nARCHITECTURE\n25.7.1 Subsumption architecture\nSUBSUMPTION Thesubsumptionarchitecture (Brooks, 1986) isaframework forassembling reactive con-\nARCHITECTURE\ntrollers out of finite state machines. Nodes in these machines may contain tests for certain\nsensorvariables, inwhichcasetheexecutiontraceofafinitestatemachineisconditioned on\nthe outcome of such a test. Arcs can be tagged with messages that will be generated when\ntraversingthem,andthataresenttotherobot\u2019smotorsorto otherfinitestatemachines. Addi-\ntionally, finitestatemachines possess internal timers(clocks) thatcontrol thetimeittakesto\nAUGMENTEDFINITE traverse anarc. Theresulting machinesarerefereed toas augmentedfinitestatemachines,\nSTATEMACHINE\norAFSMs,wheretheaugmentation referstotheuseofclocks.\nAn example of a simple AFSM is the four-state machine shown in Figure 25.24(b),\nwhich generates cyclic leg motion for a hexapod walker. This AFSM implements a cyclic\ncontroller, whose execution mostly does not rely on environmental feedback. The forward\nswing phase, however, does rely on sensor feedback. If the leg is stuck, meaning that it has\nfailed to execute the forward swing, the robot retracts the leg, lifts it up a little higher, and\nattempts to execute the forward swing once again. Thus, the controller is able to react to\ncontingencies arisingfromtheinterplay oftherobotanditsenvironment.\nThe subsumption architecture offers additional primitives for synchronizing AFSMs,\nand for combining output values of multiple, possibly conflicting AFSMs. In this way, it\nenablestheprogrammertocomposeincreasinglycomplexcontrollersinabottom-upfashion. 1004 Chapter 25. Robotics\nIn our example, wemight begin with AFSMsforindividual legs, followed by an AFSMfor\ncoordinating multiple legs. On top of this, wemight implement higher-level behaviors such\nascollision avoidance, whichmightinvolvebacking upandturning.\nThe idea of composing robot controllers from AFSMs is quite intriguing. Imagine\nhow difficult it would be to generate the same behavior with any of the configuration-space\npath-planning algorithms described in the previous section. First, we would need an accu-\nrate model of the terrain. The configuration space of a robot with six legs, each of which\nis driven by two independent motors, totals eighteen dimensions (twelve dimensions for the\nconfiguration of the legs, and six for the location and orientation of the robot relative to its\nenvironment). Evenifourcomputerswerefastenoughtofindpathsinsuchhigh-dimensional\nspaces, we would have to worry about nasty effects such as the robot sliding down a slope.\nBecause of such stochastic effects, a single path through configuration space would almost\ncertainly be too brittle, and even a PID controller might not be able to cope with such con-\ntingencies. Inother words, generating motion behavior deliberately issimply too complex a\nproblem forpresent-day robotmotionplanning algorithms.\nUnfortunately, the subsumption architecture has its own problems. First, the AFSMs\nare driven by raw sensor input, an arrangement that works if the sensor data is reliable and\ncontainsallnecessary informationfordecisionmaking,butfailsifsensordatahastobeinte-\ngratedinnontrivialwaysovertime. Subsumption-stylecontrollershavethereforemostlybeen\nappliedtosimpletasks,suchasfollowingawallormovingtowardsvisiblelightsources. Sec-\nond,thelackofdeliberationmakesitdifficulttochangethetaskoftherobot. Asubsumption-\nstyle robot usually does just one task, and it has no notion of how to modify its controls to\naccommodate different goals (just like the dung beetle on page 39). Finally, subsumption-\nstylecontrollers tendtobedifficulttounderstand. Inpractice, theintricateinterplaybetween\ndozens of interacting AFSMs (and the environment) is beyond what most human program-\nmers can comprehend. For all these reasons, the subsumption architecture is rarely used in\nrobotics, despite its great historical importance. However, it has had an influence on other\narchitectures, andonindividual components ofsomearchitectures.\n25.7.2 Three-layer architecture\nHybrid architectures combine reaction with deliberation. The most popular hybrid architec-\nTHREE-LAYER ture is the three-layer architecture, which consists of a reactive layer, an executive layer,\nARCHITECTURE\nandadeliberative layer.\nThereactivelayerprovideslow-levelcontroltotherobot. Itischaracterized byatight\nREACTIVELAYER\nsensor\u2013action loop. Itsdecision cycleisoftenontheorder ofmilliseconds.\nTheexecutivelayer(orsequencing layer)servesasthegluebetweenthereactivelayer\nEXECUTIVELAYER\nandthedeliberative layer. Itaccepts directives bythedeliberative layer, andsequences them\nfor the reactive layer. For example, the executive layer might handle a set of via-points\ngenerated by a deliberative path planner, and make decisions as to which reactive behavior\nto invoke. Decision cycles at the executive layer are usually in the order of a second. The\nexecutive layer is also responsible for integrating sensor information into an internal state\nrepresentation. Forexample,itmayhosttherobot\u2019slocalizationandonlinemappingroutines. Section25.7. RoboticSoftwareArchitectures 1005\nSENSOR INTERFACE PERCEPTION PLANNING&CONTROL USER INTERFACE\nRDDF database corridor Top level control Touch screen UI\npause\/disable command Wireless E-Stop\nLaser 1 interface\nRDDF corridor (smoothed and original) driving mode\nLaser 2 interface\nLaser 3 interface Road finder road center Path planner\nLaser 4 interface laser map\nLaser 5 interface Laser mapper map trajectory VEHICLE\nCamera interface Vision mapper vision map INTERFACE\nRadar interface Radar mapper obstacle list Steering control\nvehicle state (pose, velocity) Touareg interface\nvehicle\nGPS position UKF Pose estimation state Throttle\/brake control\nPower server interface\nGPS compass vehicle state (pose, velocity)\nIMU interface Surface assessment velocity limit\nWheel velocity\nBrake\/steering\nheart beats Linux processes start\/stop emergency stop\nhealth status\nProcess controller Health monitor\npower on\/off\ndata\nGLOBAL\nData logger File system\nSERVICES\nCommunication requests Communication channels clocks\nInter-process communication (IPC) server Time server\nFigure 25.26 Software architecture of a robot car. This software implements a data\npipeline,inwhichallmodulesprocessdatasimultaneously.\nThe deliberative layer generates global solutions to complex tasks using planning.\nDELIBERATIVELAYER\nBecause of the computational complexity involved in generating such solutions, its decision\ncycleisoftenintheorderofminutes. Thedeliberative layer(orplanning layer) usesmodels\nfor decision making. Those models might be either learned from data or supplied and may\nutilizestateinformation gathered attheexecutivelayer.\nVariantsofthethree-layerarchitecturecanbefoundinmostmodern-dayrobotsoftware\nsystems. Thedecomposition intothreelayersisnotverystrict. Somerobotsoftwaresystems\npossessadditionallayers,suchasuserinterfacelayersthatcontroltheinteractionwithpeople,\noramultiagent level forcoordinating arobot\u2019s actions withthat of otherrobots operating in\nthesameenvironment.\n25.7.3 Pipelinearchitecture\nPIPELINE Anotherarchitectureforrobotsisknownasthe pipelinearchitecture. Justlikethesubsump-\nARCHITECTURE\ntionarchitecture,thepipelinearchitectureexecutesmultipleprocessinparallel. However,the\nspecificmodulesinthisarchitecture resemblethoseinthethree-layer architecture.\nFigure 25.26 shows an example pipeline architecture, which is used to control an au-\nSENSORINTERFACE tonomous car. Dataenters thispipeline atthe sensor interface layer. Theperception layer\nLAYER\nPERCEPTIONLAYER 1006 Chapter 25. Robotics\n(a) (b)\nFigure25.27 (a)TheHelpmaterobottransportsfoodandothermedicalitemsindozens\nof hospitals worldwide. (b) Kiva robotsare part of a material-handlingsystem formoving\nshelvesinfulfillmentcenters. ImagecourtesyofKivaSystems.\nthen updates the robot\u2019s internal models of the environment based on this data. Next, these\nPLANNINGAND models are handed to the planning and control layer, which adjusts the robot\u2019s internal\nCONTROLLAYER\nplansturnsthemintoactualcontrols fortherobot. Thosearethencommunicated backtothe\nVEHICLEINTERFACE vehiclethroughthevehicleinterfacelayer.\nLAYER\nThe key to the pipeline architecture is that this all happens in parallel. While the per-\nception layer processes the most recent sensor data, the control layer bases its choices on\nslightly older data. In this way, the pipeline architecture is similar to the human brain. We\ndon\u2019tswitchoffourmotioncontrollerswhenwedigestnewsensordata. Instead,weperceive,\nplan, andactallatthesametime. Processes inthepipeline architecture runasynchronously,\nandallcomputation isdata-driven. Theresulting systemisrobust, anditisfast.\nThearchitectureinFigure25.26alsocontainsother,cross-cuttingmodules,responsible\nforestablishing communication betweenthedifferentelementsofthepipeline.\n25.8 APPLICATION DOMAINS\nHerearesomeoftheprimeapplication domainsforrobotictechnology.\nIndustryandAgriculture. Traditionally, robotshavebeenfieldedinareasthatrequire\ndifficult human labor, yet are structured enough to be amenable to robotic automation. The\nbest example is the assembly line, where manipulators routinely perform tasks such as as-\nsembly, part placement, material handling, welding, and painting. In many of these tasks,\nrobots have become more cost-effective than human workers. Outdoors, many of the heavy\nmachines that we use to harvest, mine, or excavate earth have been turned into robots. For Section25.8. Application Domains 1007\n(a) (b)\nFigure25.28 (a)RoboticcarBOSS,whichwontheDARPAUrbanChallenge. Courtesy\nofCarnegieMellonUniversity.(b)Surgicalrobotsintheoperatingroom.Imagecourtesyof\ndaVinciSurgicalSystems.\nexample,aprojectatCarnegieMellonUniversityhasdemonstratedthatrobotscanstrippaint\nofflargeshipsabout50timesfasterthanpeoplecan,andwithamuchreducedenvironmental\nimpact. Prototypes ofautonomous miningrobotshavebeenfoundtobefasterandmorepre-\ncisethanpeopleintransportingoreinundergroundmines. Robotshavebeenusedtogenerate\nhigh-precision maps of abandoned mines and sewer systems. While many of these systems\nare still in their prototype stages, it is only a matter of time until robots will take overmuch\nofthesemimechanical workthatispresently performedbypeople.\nTransportation. Robotictransportationhasmanyfacets: fromautonomoushelicopters\nthatdeliverpayloads tohard-to-reach locations, toautomatic wheelchairs thattransport peo-\nplewhoareunabletocontrolwheelchairsbythemselves,toautonomousstraddlecarriersthat\noutperform skilledhumandriverswhentransporting containers fromshipstotrucksonload-\ningdocks. Aprimeexampleofindoortransportation robots, orgofers, istheHelpmaterobot\nshown in Figure 25.27(a). This robot has been deployed in dozens of hospitals to transport\nfood and other items. In factory settings, autonomous vehicles are now routinely deployed\nto transport goods in warehouses and between production lines. The Kiva system, shown in\nFigure25.27(b),helpsworkersatfulfillmentcenterspackagegoodsintoshippingcontainers.\nManyoftheserobotsrequireenvironmentalmodificationsfortheiroperation. Themost\ncommon modifications are localization aids such as inductive loops in the floor, active bea-\ncons, or barcode tags. An open challenge in robotics is the design of robots that can use\nnaturalcues,insteadofartificialdevices,tonavigate,particularlyinenvironmentssuchasthe\ndeepoceanwhereGPSisunavailable.\nRobotic cars. Most of use cars every day. Many of us make cell phone calls while\ndriving. Some of us even text. The sad result: more than a million people die every year in\ntrafficaccidents. Robotic carslike BOSS and STANLEY offerhope: Notonlywilltheymake\ndrivingmuchsafer,buttheywillalsofreeusfromtheneedtopayattentiontotheroadduring\nourdailycommute.\nProgress in robotic cars was stimulated by the DARPA Grand Challenge, a race over\n100milesofunrehearseddesertterrain,whichrepresented amuchmorechallengingtaskthan 1008 Chapter 25. Robotics\n(a) (b)\nFigure25.29 (a) A robotmappingan abandonedcoalmine. (b) A 3D mapof the mine\nacquiredbytherobot.\nhadeverbeenaccomplishedbefore. Stanford\u2019sSTANLEYvehiclecompletedthecourseinless\nthansevenhours in2005, winninga$2million prizeandaplaceintheNationalMuseum of\nAmerican History. Figure 25.28(a) depicts BOSS, which in 2007 won the DARPA Urban\nChallenge,acomplicatedroadraceoncitystreetswhererobotsfacedotherrobotsandhadto\nobeytrafficrules.\nHealthcare. Robotsareincreasinglyusedtoassistsurgeonswithinstrumentplacement\nwhenoperatingonorgansasintricateasbrains,eyes,andhearts. Figure25.28(b)showssuch\nasystem. Robotshavebecomeindispensable toolsinarangeofsurgical procedures, suchas\nhip replacements, thanks to their high precision. In pilot studies, robotic devices have been\nfound to reduce the danger of lesions when performing colonoscopy. Outside the operating\nroom, researchers have begun to develop robotic aides for elderly and handicapped people,\nsuchasintelligentroboticwalkersandintelligenttoysthatprovidereminderstotakemedica-\ntion andprovide comfort. Researchers arealso working onrobotic devices forrehabilitation\nthataidpeopleinperforming certainexercises.\nHazardous environments. Robots have assisted people in cleaning up nuclear waste,\nmost notably in Chernobyl and Three Mile Island. Robots were present after the collapse\nof the World Trade Center, where they entered structures deemed too dangerous for human\nsearchandrescuecrews.\nSomecountries haveused robots totransport ammunition and todefuse bombs\u2014a no-\ntoriously dangerous task. A number of research projects are presently developing prototype\nrobots for clearing minefields, on land and at sea. Most existing robots for these tasks are\nteleoperated\u2014a human operates them by remote control. Providing such robots with auton-\nomyisanimportant nextstep.\nExploration. Robots have gone where no one has gone before, including the surface\nof Mars (see Figure 25.2(b) and the cover). Robotic arms assist astronauts in deploying\nand retrieving satellites and in building the International Space Station. Robots also help\nexploreunderthesea. Theyareroutinelyusedtoacquiremapsofsunkenships. Figure25.29\nshowsarobotmappinganabandonedcoalmine,alongwitha3Dmodelofthemineacquired Section25.8. Application Domains 1009\n(a) (b)\nFigure25.30 (a)Roomba,theworld\u2019sbest-sellingmobilerobot,vacuumsfloors. Image\ncourtesyofiRobot,(cid:2)c 2009. (b)Robotichandmodeledafterhumanhand. Imagecourtesy\nofUniversityofWashingtonandCarnegieMellonUniversity.\nusing range sensors. In 1996, a team of researches released a legged robot into the crater\nof an active volcano to acquire data for climate research. Unmanned air vehicles known as\ndronesareusedinmilitaryoperations. Robotsarebecomingveryeffectivetoolsforgathering\nDRONE\ninformation indomainsthataredifficult(ordangerous) for peopletoaccess.\nPersonal Services. Service is an up-and-coming application domain of robotics. Ser-\nvice robots assist individuals in performing daily tasks. Commercially available domestic\nservice robots include autonomous vacuum cleaners, lawn mowers, and golf caddies. The\nworld\u2019s most popular mobile robot is a personal service robot: the robotic vacuum cleaner\nRoomba, shown in Figure 25.30(a). More than three million Roombas have been sold.\nROOMBA\nRoombacannavigateautonomously andperform itstaskswithouthumanhelp.\nOther service robots operate in public places, such as robotic information kiosks that\nhave been deployed in shopping malls and trade fairs, or in museums as tour guides. Ser-\nvice tasks require human interaction, and theability tocoperobustly withunpredictable and\ndynamicenvironments.\nEntertainment. Robots have begun to conquer the entertainment and toy industry.\nIn Figure 25.6(b) we see robotic soccer, a competitive game very much like human soc-\nROBOTICSOCCER\ncer, but played with autonomous mobile robots. Robot soccer provides great opportunities\nfor research in AI, since it raises a range of problems relevant to many other, more serious\nrobot applications. Annual robotic soccer competitions have attracted large numbers of AI\nresearchers andaddedalotofexcitementtothefieldofrobotics.\nHuman augmentation. A final application domain of robotic technology is that of\nhuman augmentation. Researchers have developed legged walking machines that can carry\npeople around, very much like a wheelchair. Several research efforts presently focus on the\ndevelopmentofdevicesthatmakeiteasierforpeopletowalkormovetheirarmsbyproviding\nadditionalforcesthroughextraskeletalattachments. Ifsuchdevicesareattachedpermanently, 1010 Chapter 25. Robotics\nthey can be thought of as artificial robotic limbs. Figure 25.30(b) shows a robotic hand that\nmayserveasaprosthetic deviceinthefuture.\nRobotic teleoperation, or telepresence, is another form of human augmentation. Tele-\noperation involves carrying out tasks over long distances with the aid of robotic devices.\nA popular configuration for robotic teleoperation is the master\u2013slave configuration, where\na robot manipulator emulates the motion of a remote human operator, measured through a\nhaptic interface. Underwater vehicles are often teleoperated; the vehicles can go to a depth\nthatwouldbedangerous forhumansbutcanstillbeguided bythehumanoperator. Allthese\nsystemsaugmentpeople\u2019sabilitytointeractwiththeirenvironments. Someprojectsgoasfar\nasreplicating humans,atleastataverysuperficiallevel. Humanoidrobotsarenowavailable\ncommercially throughseveralcompaniesinJapan.\n25.9 SUMMARY\nRobotics concerns itself with intelligent agents that manipulate the physical world. In this\nchapter, wehavelearned thefollowingbasicsofrobothardwareandsoftware.\n\u2022 Robots are equipped with sensors for perceiving their environment and effectors with\nwhich they can assert physical forces on their environment. Most robots are either\nmanipulators anchored atfixedlocations ormobilerobotsthatcanmove.\n\u2022 Robotic perception concerns itself with estimating decision-relevant quantities from\nsensor data. To do so, we need an internal representation and a method for updating\nthis internal representation overtime. Commonexamples of hard perceptual problems\nincludelocalization, mapping,andobjectrecognition.\n\u2022 ProbabilisticfilteringalgorithmssuchasKalmanfiltersandparticle filtersareuseful\nforrobotperception. Thesetechniquesmaintainthebelief state,aposteriordistribution\noverstatevariables.\n\u2022 Theplanningofrobotmotionisusuallydoneinconfigurationspace,whereeachpoint\nspecifiesthelocation andorientation oftherobotanditsjointangles.\n\u2022 Configuration space search algorithms include cell decomposition techniques, which\ndecomposethespaceofallconfigurations intofinitelymanycells,andskeletonization\ntechniques, whichprojectconfiguration spacesontolower-dimensional manifolds. The\nmotionplanning problem isthensolvedusingsearchinthese simplerstructures.\n\u2022 Apath found byasearch algorithm can be executed byusing the path asthe reference\ntrajectory for a PID controller. Controllers are necessary in robotics to accommodate\nsmallperturbations; pathplanning aloneisusually insufficient.\n\u2022 Potential field techniques navigate robots by potential functions, defined overthe dis-\ntance to obstacles and the goal location. Potential field techniques may get stuck in\nlocalminima,buttheycangeneratemotiondirectlywithouttheneedforpathplanning.\n\u2022 Sometimes itis easier to specify a robot controller directly, rather than deriving apath\nfrom an explicit model of the environment. Such controllers can often be written as\nsimplefinitestatemachines. Bibliographical andHistorical Notes 1011\n\u2022 There exist different architectures for software design. The subsumption architec-\ntureenablesprogrammerstocomposerobotcontrollersfrominterconnectedfinitestate\nmachines. Three-layer architectures are common frameworks for developing robot\nsoftware that integrate deliberation, sequencing of subgoals, and control. The related\npipelinearchitecture processes datainparallel through asequence ofmodules, corre-\nsponding toperception, modeling, planning, control, androbotinterfaces.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nThe word robot waspopularized by Czech playwright Karel Capek in his 1921 play R.U.R.\n(Rossum\u2019s Universal Robots). The robots, which were grown chemically rather than con-\nstructed mechanically, end up resenting their masters and decide to take over. It appears\n(Glanc, 1978) it was Capek\u2019s brother, Josef, who first combined the Czech words \u201crobota\u201d\n(obligatory work)and\u201crobotnik\u201d (serf)toyield\u201crobot\u201d in his1917shortstory Opilec.\nTheterm robotics was firstused by Asimov(1950). Robotics (under other names) has\namuchlongerhistory,however. InancientGreekmythology, amechanicalmannamedTalos\nwas supposedly designed and built by Hephaistos, the Greek god of metallurgy. Wonderful\nautomata were built in the 18th century\u2014Jacques Vaucanson\u2019s mechanical duck from 1738\nbeing one early example\u2014but the complex behaviors they exhibited were entirely fixed in\nadvance. Possiblytheearliestexampleofaprogrammablerobot-likedevicewastheJacquard\nloom(1805),described onpage14.\nThefirstcommercialrobotwasarobotarmcalledUnimate,shortforuniversalautoma-\nUNIMATE\ntion, developed by Joseph Engelberger and George Devol. In 1961, the first Unimate robot\nwas sold to General Motors, where it was used for manufacturing TV picture tubes. 1961\nwasalsotheyearwhenDevolobtained thefirstU.S.patent ona robot. Elevenyears later, in\n1972, Nissan Corp. wasamong thefirst toautomate anentire assembly line withrobots, de-\nvelopedbyKawasakiwithrobotssupplied byEngelberger andDevol\u2019scompanyUnimation.\nThis development initiated a major revolution that took place mostly in Japan and the U.S.,\nandthatisstillongoing. Unimationfollowedupin1978withthedevelopmentofthePUMA\nPUMA\nrobot, short forProgrammable Universal Machine forAssembly. ThePUMArobot, initially\ndevelopedforGeneralMotors,wasthedefactostandardforroboticmanipulationforthetwo\ndecadesthatfollowed. Atpresent,thenumberofoperatingrobotsisestimatedatonemillion\nworldwide,morethanhalfofwhichareinstalled inJapan.\nTheliteratureonroboticsresearchcanbedividedroughlyintotwoparts: mobilerobots\nand stationary manipulators. Grey Walter\u2019s \u201cturtle,\u201d built in 1948, could be considered the\nfirstautonomousmobilerobot,althoughitscontrolsystemwasnotprogrammable. The\u201cHop-\nkins Beast,\u201d built in the early 1960s at Johns Hopkins University, was much more sophisti-\ncated; it had pattern-recognition hardware and could recognize the cover plate of a standard\nACpoweroutlet. Itwascapableofsearchingforoutlets,pluggingitselfin,andthenrecharg-\ning its batteries! Still, the Beast had a limited repertoire of skills. The first general-purpose\nmobilerobotwas\u201cShakey,\u201ddevelopedatwhatwasthentheStanfordResearchInstitute(now 1012 Chapter 25. Robotics\nSRI) in the late 1960s (Fikes and Nilsson, 1971; Nilsson, 1984). Shakey was the first robot\nto integrate perception, planning, and execution, and much subsequent research in AI was\ninfluenced by this remarkable achievement. Shakey appears on the cover of this book with\nproject leader Charlie Rosen (1917\u20132002). Other influential projects include the Stanford\nCart and the CMU Rover(Moravec, 1983). Cox and Wilfong (1990) describes classic work\nonautonomous vehicles.\nThe field of robotic mapping has evolved from two distinct origins. The first thread\nbegan with work by Smith and Cheeseman (1986), who applied Kalman filters to the si-\nmultaneous localization and mapping problem. This algorithm was first implemented by\nMoutarlier and Chatila (1989), and later extended by Leonard and Durrant-Whyte (1992);\nsee Dissanayake et al. (2001) for an overview of early Kalman filtervariations. The second\nthread began with the development of the occupancy grid representation for probabilistic\nOCCUPANCYGRID\nmapping, which specifies the probability that each (x,y) location is occupied by an obsta-\ncle (Moravec and Elfes, 1985). Kuipers and Levitt (1988) were among the first to propose\ntopological rather than metric mapping, motivated by models ofhuman spatial cognition. A\nseminal paperbyLuand Milios(1997) recognized thesparseness ofthesimultaneous local-\nization and mapping problem, which gave rise tothedevelopment ofnonlinear optimization\ntechniques by Konolige (2004) and Montemerlo and Thrun (2004), as well as hierarchical\nmethodsbyBosseetal.(2004). ShatkayandKaelbling(1997)andThrunetal.(1998)intro-\nduced theEMalgorithm into thefieldof robotic mapping fordata association. Anoverview\nofprobabilistic mappingmethodscanbefoundin(Thrun etal.,2005).\nEarly mobile robot localization techniques are surveyed by Borenstein et al. (1996).\nAlthough Kalman filtering was well known as a localization method in control theory for\ndecades, the general probabilistic formulation of the localization problem did not appear\nin the AI literature until much later, through the work of Tom Dean and colleagues (Dean\netal., 1990, 1990) andofSimmonsand Koenig (1995). Thelatterwork introduced theterm\nMARKOV Markovlocalization. Thefirstreal-worldapplicationofthistechniquewasbyBurgardetal.\nLOCALIZATION\n(1999), through a series of robots that were deployed in museums. Monte Carlo localiza-\ntion based on particle filters was developed by Fox et al. (1999) and is now widely used.\nRAO-\nTheRao-Blackwellized particlefiltercombinesparticlefilteringforrobotlocalization with\nBLACKWELLIZED\nPARTICLEFILTER\nexactfilteringformapbuilding (MurphyandRussell,2001;Montemerlo etal.,2002).\nHAND\u2013EYE The study of manipulator robots, originally called hand\u2013eye machines, has evolved\nMACHINES\nalong quite different lines. The first major effort at creating a hand\u2013eye machine was Hein-\nrichErnst\u2019sMH-1,describedinhisMITPh.D.thesis(Ernst,1961). TheMachineIntelligence\nproject at Edinburgh also demonstrated an impressive early system for vision-based assem-\nbly called FREDDY (Michie, 1972). After these pioneering efforts, a great deal of work fo-\ncused on geometric algorithms fordeterministic and fully observable motion planning prob-\nlems. The PSPACE-hardness of robot motion planning was shown in a seminal paper by\nReif(1979). Theconfiguration spacerepresentation isduetoLozano-Perez (1983). Aseries\nof papers by Schwartz and Sharir on what they called piano movers problems (Schwartz\nPIANOMOVERS\netal.,1987)washighlyinfluential.\nRecursivecelldecompositionforconfigurationspaceplanningwasoriginatedbyBrooks\nand Lozano-Perez (1985) and improved significantly by Zhu and Latombe (1991). The ear- Bibliographical andHistorical Notes 1013\nliest skeletonization algorithms were based on Voronoi diagrams (Rowat, 1979) and visi-\nbility graphs (Wesley and Lozano-Perez, 1979). Guibas et al. (1992) developed efficient\nVISIBILITYGRAPH\ntechniques for calculating Voronoi diagrams incrementally, and Choset (1996) generalized\nVoronoi diagrams tobroader motion-planning problems. John Canny (1988) established the\nfirstsingly exponential algorithm formotion planning. The seminal text by Latombe (1991)\ncoversavarietyofapproachestomotion-planning, asdothetextsbyChosetetal.(2004)and\nLaValle(2006). Kavrakietal.(1996) developed probabilistic roadmaps, whicharecurrently\none of the most effective methods. Fine-motion planning with limited sensing was investi-\ngated by Lozano-Perez et al. (1984) and Canny and Reif (1987). Landmark-based naviga-\ntion (Lazanas and Latombe, 1992) uses many of the same ideas in the mobile robot arena.\nKeyworkapplying POMDPmethods(Section17.4)tomotionplanning underuncertainty in\nrobotics isduetoPineau etal.(2003)andRoyetal.(2005).\nThecontrolofrobotsasdynamicalsystems\u2014whetherformanipulationornavigation\u2014\nhas generated a huge literature that is barely touched on by this chapter. Important works\ninclude a trilogy on impedance control by Hogan (1985) and a general study of robot dy-\nnamics by Featherstone (1987). Deanand Wellman (1991) were among the firstto try to tie\ntogethercontroltheoryandAIplanningsystems. Threeclassictextbooksonthemathematics\nofrobotmanipulation areduetoPaul(1981), Craig(1989), andYoshikawa(1990). Thearea\nofgraspingisalsoimportant inrobotics\u2014the problem ofdetermining astablegraspisquite\nGRASPING\ndifficult(MasonandSalisbury, 1985). Competentgrasping requires touchsensing, orhaptic\nfeedback,todetermine contactforcesanddetectslip(FearingandHollerbach, 1985).\nHAPTICFEEDBACK\nPotential-field control, which attempts to solve the motion planning and control prob-\nlemssimultaneously, wasintroduced into therobotics literature byKhatib(1986). Inmobile\nrobotics, thisideawasviewedasapractical solution tothe collision avoidance problem, and\nVECTORFIELD was later extended into an algorithm called vector field histograms by Borenstein (1991).\nHISTOGRAMS\nNavigation functions, the robotics version of a control policy for deterministic MDPs, were\nintroduced byKoditschek(1987). Reinforcementlearninginroboticstookoffwiththesemi-\nnalworkbyBagnellandSchneider(2001)andNgetal.(2004),whodevelopedtheparadigm\ninthecontextofautonomous helicoptercontrol.\nThe topic of software architectures for robots engenders much religious debate. The\ngood old-fashioned AI candidate\u2014the three-layer architecture\u2014dates back to the design of\nShakeyandisreviewedbyGat(1998). ThesubsumptionarchitectureisduetoBrooks(1986),\nalthough similar ideas were developed independently by Braitenberg (1984), whose book,\nVehicles, describes a series of simple robots based on the behavioral approach. The suc-\ncess of Brooks\u2019s six-legged walking robot was followed by many other projects. Connell,\nin his Ph.D. thesis (1989), developed a mobile robot capable of retrieving objects that was\nentirely reactive. Extensions of the behavior-based paradigm to multirobot systems can be\nfound in (Mataric, 1997) and (Parker, 1996). GRL (Horswill, 2000) and COLBERT (Kono-\nlige,1997)abstracttheideasofconcurrentbehavior-based roboticsintogeneralrobotcontrol\nlanguages. Arkin(1998)surveyssomeofthemostpopularapproaches inthisfield.\nResearchonmobileroboticshasbeenstimulatedoverthelastdecadebyseveralimpor-\ntantcompetitions. Theearliestcompetition, AAAI\u2019sannualmobilerobotcompetition, began\nin 1992. The first competition winner was CARMEL (Congdon et al., 1992). Progress has 1014 Chapter 25. Robotics\nbeensteadyandimpressive: inmorerecentcompetitions robotsenteredtheconference com-\nplex, found theirwayto the registration desk, registered forthe conference, and even gave a\nshort talk. The Robocup competition, launched in 1995 by Kitano and colleagues (1997a),\nROBOCUP\naims to \u201cdevelop a team of fully autonomous humanoid robots that can win against the hu-\nman world champion team in soccer\u201d by 2050. Play occurs in leagues for simulated robots,\nwheeled robots of different sizes, and humanoid robots. In 2009 teams from 43 countries\nparticipated and theeventwasbroadcast tomillions ofviewers. VisserandBurkhard (2007)\ntracktheimprovements thathavebeenmadeinperception, teamcoordination, andlow-level\nskillsoverthepastdecade.\nDARPAGRAND The DARPA Grand Challenge, organized by DARPA in 2004 and 2005, required\nCHALLENGE\nautonomous robots to travel more than 100 miles through unrehearsed desert terrain in less\nthan 10 hours (Buehler et al., 2006). In the original event in 2004, no robot traveled more\nthan 8miles, leading manyto believe the prize would neverbe claimed. In 2005, Stanford\u2019s\nrobot STANLEY wonthe competition in just under 7 hours of travel (Thrun, 2006). DARPA\nthenorganizedtheUrbanChallenge,acompetitioninwhichrobotshadtonavigate60miles\nURBANCHALLENGE\nin an urban environment with other traffic. Carnegie Mellon University\u2019s robot BOSS took\nfirstplaceandclaimedthe$2millionprize(UrmsonandWhittaker, 2008). Earlypioneersin\nthedevelopmentofroboticcarsincludedDickmannsandZapp(1987)andPomerleau(1993).\nTwoearly textbooks, by Dudek and Jenkin (2000) and Murphy (2000), cover robotics\ngenerally. A more recent overview is due to Bekey (2008). An excellent book on robot\nmanipulation addresses advanced topics such as compliant motion (Mason, 2001). Robot\nmotion planning is covered in Choset et al. (2004) and LaValle (2006). Thrun et al. (2005)\nprovide an introduction into probabilistic robotics. The premiere conference for robotics is\nRobotics: Science and Systems Conference, followed bythe IEEEInternational Conference\nonRoboticsandAutomation. LeadingroboticsjournalsincludeIEEERoboticsandAutoma-\ntion,theInternational JournalofRoboticsResearch,andRoboticsandAutonomousSystems.\nEXERCISES\n25.1 MonteCarlolocalization isbiasedforanyfinitesamplesize\u2014i.e., theexpected value\nof the location computed by the algorithm differs from the true expected value\u2014because of\nthewayparticlefilteringworks. Inthisquestion, youareaskedtoquantify thisbias.\nTosimplify,consideraworldwithfourpossiblerobotlocations: X = {x ,x ,x ,x }.\n1 2 3 4\nInitially, we draw N \u2265 1 samples uniformly from among those locations. As usual, it is\nperfectly acceptable if morethan one sample is generated forany of the locations X. LetZ\nbeaBooleansensorvariable characterized bythefollowing conditional probabilities:\nP(z | x ) = 0.8 P(\u00acz | x ) = 0.2\n1 1\nP(z | x ) = 0.4 P(\u00acz | x ) = 0.6\n2 2\nP(z | x ) = 0.1 P(\u00acz | x ) = 0.9\n3 3\nP(z | x ) = 0.1 P(\u00acz | x ) = 0.9.\n4 4 Exercises 1015\nB\nA A\nB\nStarting configuration <\u22120.5, 7> Ending configuration <\u22120.5, \u22127>\nFigure25.31 ARobotmanipulatorintwoofitspossibleconfigurations.\nMCLusestheseprobabilitiestogenerateparticleweights,whicharesubsequentlynormalized\nand used in the resampling process. Forsimplicity, let us assume wegenerate only one new\nsample in the resampling process, regardless of N. This sample might correspond to any of\nthefourlocationsinX. Thus,thesamplingprocessdefinesaprobabilitydistributionoverX.\na. Whatistheresulting probability distribution over X forthisnewsample? Answerthis\nquestionseparately forN = 1,...,10,andforN = \u221e.\nb. Thedifference between twoprobability distributions P and Q can be measured by the\nKLdivergence, whichisdefinedas\n(cid:12)\nP(x )\ni\nKL(P,Q) = P(x )log .\ni\nQ(x )\ni\ni\nWhataretheKLdivergences betweenthedistributions in(a)andthetrueposterior?\nc. What modification of the problem formulation (not the algorithm!) would guarantee\nthat the specific estimator above is unbiased even for finite values of N? Provide at\nleasttwosuchmodifications (eachofwhichshouldbesufficient).\n25.2 Implement Monte Carlolocalization forasimulated robot withrange sensors. Agrid\nmap and range data are available from the code repository at aima.cs.berkeley.edu.\nYoushoulddemonstrate successful globallocalization oftherobot.\n25.3 Considerarobotwithtwosimplemanipulators,asshowninfigure25.31. Manipulator\nA is a square block of side 2 which can slide back and on a rod that runs along the x-axis\nfrom x=\u221210 to x=10. Manipulator B is a square block of side 2 which can slide back and\non a rod that runs along the y-axis from y=\u221210 to y=10. The rods lie outside the plane of 1016 Chapter 25. Robotics\nmanipulation, sothe rodsdo notinterfere withthe movement oftheblocks. Aconfiguration\nisthenapair(cid:16)x,y(cid:17)wherexisthex-coordinateofthecenterofmanipulatorAandwhere yis\nthey-coordinate ofthecenterofmanipulator B.Drawtheconfiguration space forthisrobot,\nindicating thepermittedandexcluded zones.\n25.4 Suppose that you are working with the robot in Exercise 25.3 and you are given the\nproblem of finding a path from the starting configuration of figure 25.31 to the ending con-\nfiguration. Considerapotentialfunction\n1\nD(A,Goal)2+D(B,Goal)2+\nD(A,B)2\nwhereD(A,B)isthedistance betweentheclosest pointsofAandB.\na. Showthathillclimbinginthispotential fieldwillgetstuckinalocalminimum.\nb. Describe a potential field where hill climbing will solve this particular problem. You\nneednotworkouttheexact numerical coefficients needed, justthegeneral formofthe\nsolution. (Hint: Add a term that \u201crewards\u201d the hill climber for moving A out of B\u2019s\nway,eveninacaselikethiswherethisdoesnotreducethedistance fromAtoBinthe\nabovesense.)\n25.5 Consider the robot arm shown in Figure 25.14. Assume that the robot\u2019s base element\nis60cmlongandthatitsupperarmandforearmareeach40cmlong. Asarguedonpage987,\ntheinversekinematicsofarobotisoftennotunique. Stateanexplicitclosed-formsolutionof\ntheinversekinematicsforthisarm. Underwhatexactconditionsisthesolutionunique?\n25.6 Implement an algorithm for calculating the Voronoi diagram of an arbitrary 2D en-\nvironment, described by an n\u00d7n Boolean array. Illustrate your algorithm by plotting the\nVoronoidiagram for10interesting maps. Whatisthecomplexity ofyouralgorithm?\n25.7 This exercise explores the relationship between workspace and configuration space\nusingtheexamplesshowninFigure25.32.\na. Consider the robot configurations shown in Figure 25.32(a) through (c), ignoring the\nobstacleshownineachofthediagrams. Drawthecorresponding armconfigurations in\nconfigurationspace. (Hint: Eacharmconfigurationmapstoasinglepointinconfigura-\ntionspace,asillustrated inFigure25.14(b).)\nb. Draw the configuration space for each of the workspace diagrams in Figure 25.32(a)\u2013\n(c). (Hint: The configuration spaces share with the one shown in Figure 25.32(a) the\nregionthatcorrespondstoself-collision, butdifferencesarisefromthelackofenclosing\nobstacles andthedifferent locations oftheobstacles intheseindividual figures.)\nc. ForeachoftheblackdotsinFigure25.32(e)\u2013(f),drawthecorrespondingconfigurations\noftherobotarminworkspace. Pleaseignoretheshadedregionsinthisexercise.\nd. The configuration spaces shown in Figure 25.32(e)\u2013(f) have all been generated by a\nsingle workspace obstacle (dark shading), plus the constraints arising from the self-\ncollision constraint (light shading). Draw, for each diagram, the workspace obstacle\nthatcorresponds tothedarklyshadedarea. Exercises 1017\n(a) (b) (c)\n(d) (e) (f)\nFigure25.32 DiagramsforExercise25.7.\ne. Figure 25.32(d) illustrates that a single planar obstacle can decompose the workspace\ninto two disconnected regions. What is the maximum number of disconnected re-\ngionsthatcanbecreatedbyinsertingaplanarobstacleinto anobstacle-free, connected\nworkspace, for a 2DOF robot? Give an example, and argue why no larger number of\ndisconnected regionscanbecreated. Howaboutanon-planar obstacle?\n25.8 Consider a mobile robot moving on a horizontal surface. Suppose that the robot can\nexecutetwokindsofmotions:\n\u2022 Rollingforwardaspecifieddistance.\n\u2022 Rotatinginplacethrough aspecifiedangle.\nThe state of such a robot can be characterized in terms of three parameters (cid:16)x,y,\u03c6, the x-\ncoordinate and y-coordinate of the robot (more precisely, of its center of rotation) and the\nrobot\u2019sorientationexpressedastheanglefromthepositivexdirection. Theaction\u201cRoll(D)\u201d\nhas the effect of changing state (cid:16)x,y,\u03c6 to (cid:16)x +Dcos(\u03c6),y +Dsin(\u03c6),\u03c6(cid:17), and the action\nRotate(\u03b8)hastheeffectofchanging state(cid:16)x,y,\u03c6(cid:17)to(cid:16)x,y,\u03c6+\u03b8(cid:17).\na.\nSupposethattherobotisinitiallyat(cid:16)0,0,0(cid:17)andthenexecutestheactionsRotate(60\u25e6\n),\n\u25e6\nRoll(1),Rotate(25 ),Roll(2). Whatisthefinalstateoftherobot? 1018 Chapter 25. Robotics\nrobot\nsensor\nrange\ngoal\nFigure25.33 Simplifiedrobotinamaze. SeeExercise25.9.\nb. Now suppose that the robot has imperfect control of its own rotation, and that, if it\nattemptstorotateby\u03b8,itmayactuallyrotatebyanyanglebetween\u03b8\u221210\u25e6 and\u03b8+10\u25e6\n.\nIn that case, if the robot attempts to carry out the sequence of actions in (A), there is\na range of possible ending states. What are the minimal and maximal values of the\nx-coordinate, they-coordinate andtheorientation inthefinalstate?\nc. Let us modify the model in (B) to a probabilistic model in which, when the robot\nattempts to rotate by \u03b8, its actual angle of rotation follows a Gaussian distribution\n\u25e6\nwith mean \u03b8 and standard deviation 10 . Suppose that the robot executes the actions\n\u25e6\nRotate(90 ), Roll(1). Giveasimpleargument that (a)theexpected value oftheloca-\n\u25e6\ntionattheendisnotequaltotheresultofrotatingexactly 90 andthenrollingforward\n1 unit, and (b) that the distribution of locations at the end does not follow a Gaussian.\n(Donotattempttocalculate thetruemeanorthetruedistribution.)\nThepoint of this exercise is that rotational uncertainty quickly gives rise to alot of\npositional uncertainty and that dealing with rotational uncertainty is painful, whether\nuncertainty istreated in terms of hard intervals orprobabilistically, due to the fact that\ntherelation betweenorientation andposition isbothnon-linearandnon-monotonic.\n25.9 Consider the simplified robot shown in Figure 25.33. Suppose the robot\u2019s Cartesian\ncoordinates are known at all times, as are those of its goal location. However, the locations\nof the obstacles are unknown. The robot can sense obstacles in its immediate proximity, as\nillustrated in this figure. For simplicity, let us assume the robot\u2019s motion is noise-free, and\nthestatespaceisdiscrete. Figure25.33isonlyoneexample;inthisexerciseyouarerequired\ntoaddress allpossible gridworldswithavalidpathfromthe starttothegoallocation.\na. Design a deliberate controller that guarantees that the robot always reaches its goal\nlocation ifatallpossible. Thedeliberate controller canmemorizemeasurements inthe\nformofamapthatisbeing acquired astherobot moves. Betweenindividual moves,it\nmayspendarbitrarytimedeliberating. Exercises 1019\nb. Nowdesign a reactive controller forthe same task. Thiscontroller may not memorize\npastsensormeasurements. (Itmaynotbuildamap!) Instead,ithastomakealldecisions\nbased on the current measurement, which includes knowledge of its own location and\nthat of the goal. The time to make a decision must be independent of the environment\nsizeorthenumberofpasttimesteps. Whatisthemaximumnumberofstepsthatitmay\ntakeforyourrobottoarriveatthegoal?\nc. Howwillyourcontrollersfrom(a)and(b)performifanyofthefollowingsixconditions\napply: continuous state space, noise in perception, noise in motion, noise in both per-\nception andmotion, unknown location ofthegoal (thegoal canbedetected only when\nwithinsensorrange),ormovingobstacles. Foreachcondition andeachcontroller, give\nanexampleofasituation wheretherobotfails(orexplainwhyitcannotfail).\n25.10 InFigure25.24(b) onpage 1001, weencountered anaugmented finitestatemachine\nfor the control of a single leg of a hexapod robot. In this exercise, the aim is to design an\nAFSM that, when combined with six copies of the individual leg controllers, results in effi-\ncient, stable locomotion. Forthispurpose, you havetoaugment theindividual legcontroller\ntopassmessagestoyournewAFSMandtowaituntilothermessagesarrive. Arguewhyyour\ncontroller is efficient, in that it does not unnecessarily waste energy (e.g., by sliding legs),\nandinthatitpropels therobot atreasonably high speeds. Provethatyourcontroller satisfies\nthedynamicstabilitycondition givenonpage977.\n25.11 (This exercise was first devised by Michael Genesereth and Nils Nilsson. It works\nfor first graders through graduate students.) Humans are so adept at basic household tasks\nthat they often forget how complex these tasks are. In this exercise you will discover the\ncomplexity and recapitulate the last 30 years of developments in robotics. Consider the task\nofbuilding anarchoutofthreeblocks. Simulatearobotwith fourhumansasfollows:\nBrain. The Brain direct the hands in the execution of a plan to achieve the goal. The\nBrain receives input from the Eyes, but cannot see the scene directly. The brain is the only\nonewhoknowswhatthegoalis.\nEyes. TheEyesreportabriefdescription ofthescenetotheBrain: \u201cThereisaredbox\nstandingontopofagreenbox,whichisonitsside\u201dEyescanalsoanswerquestionsfromthe\nBrain such as, \u201cIs there agap between the Left Hand and the red box?\u201d If you have a video\ncamera,pointitatthesceneandallowtheeyestolookattheviewfinderofthevideocamera,\nbutnotdirectlyatthescene.\nLefthandandrighthand. OnepersonplayseachHand. ThetwoHandsstandnextto\neach other, each wearing an oven mitt on one hand, Hands execute only simple commands\nfrom the Brain\u2014for example, \u201cLeft Hand, move two inches forward.\u201d They cannot execute\ncommandsotherthanmotions;forexample,theycannotbecommandedto\u201cPickupthebox.\u201d\nThe Hands must be blindfolded. The only sensory capability they have is the ability to tell\nwhen their path is blocked by an immovable obstacle such as a table or the other Hand. In\nsuchcases, theycanbeeptoinformtheBrainofthedifficulty. 26\nPHILOSOPHICAL\nFOUNDATIONS\nIn which we consider what it means to think and whether artifacts could and\nshouldeverdoso.\nPhilosophers have been around far longer than computers and have been trying to resolve\nsome questions that relate to AI: How do minds work? Is it possible for machines to act\nintelligently in the way that people do, and if they did, would they have real, conscious\nminds? Whataretheethicalimplications ofintelligent machines?\nFirst,someterminology: theassertionthatmachinescouldactasiftheywereintelligent\niscalled theweakAIhypothesis byphilosophers, andtheassertion thatmachines thatdoso\nWEAKAI\nareactuallythinking (notjustsimulating thinking) iscalledthestrongAIhypothesis.\nSTRONGAI\nMostAIresearchers take the weakAIhypothesis forgranted, and don\u2019t careabout the\nstrong AI hypothesis\u2014as long as their program works, they don\u2019t care whether you call it a\nsimulation of intelligence or real intelligence. All AI researchers should be concerned with\ntheethicalimplications oftheirwork.\n26.1 WEAK AI: CAN MACHINES ACT INTELLIGENTLY?\nThe proposal for the 1956 summer workshop that defined the field of Artificial Intelligence\n(McCarthyetal.,1955)madetheassertionthat\u201cEveryaspectoflearningoranyotherfeature\nofintelligencecanbesopreciselydescribedthatamachinecanbemadetosimulateit.\u201d Thus,\nAI wasfounded on theassumption that weakAI ispossible. Others have asserted that weak\nAI is impossible: \u201cArtificial intelligence pursued within the cult of computationalism stands\nnotevenaghostofachanceofproducing durableresults\u201d (Sayre,1993).\nClearly, whetherAI isimpossible depends on how itisdefined. InSection 1.1, wede-\nfinedAIasthequestforthebestagentprogramonagivenarchitecture. Withthisformulation,\nAI isby definition possible: forany digital architecture with k bits ofprogram storage there\nareexactly2k agentprograms,andallwehavetodotofindthebestoneisenumerateandtest\nthem all. This might not be feasible for large k, but philosophers deal with the theoretical,\nnotthepractical.\n1020 Section26.1. WeakAI:CanMachinesActIntelligently? 1021\nOur definition of AI works well for the engineering problem of finding a good agent,\ngiven an architecture. Therefore, we\u2019re tempted to end this section right now, answering the\ntitle question in the affirmative. But philosophers are interested in the problem of compar-\ning two architectures\u2014human and machine. Furthermore, they have traditionally posed the\nCANMACHINES question notintermsofmaximizingexpected utilitybutratheras,\u201cCanmachinesthink?\u201d\nTHINK?\nThecomputerscientist EdsgerDijkstra (1984) said that \u201cThequestion ofwhether Ma-\nCANSUBMARINES chines CanThink ...isabout asrelevant asthequestion ofwhether Submarines CanSwim.\u201d\nSWIM?\nThe American Heritage Dictionary\u2019s first definition of swim is \u201cTo move through water by\nmeans of the limbs, fins, or tail,\u201d and most people agree that submarines, being limbless,\ncannot swim. Thedictionary alsodefines flyas\u201cTomovethrough theairbymeansofwings\norwinglikeparts,\u201dandmostpeopleagreethatairplanes,havingwinglikeparts,canfly. How-\never,neitherthequestionsnortheanswershaveanyrelevance tothedesignorcapabilities of\nairplanes andsubmarines; rathertheyareabout theusage of wordsinEnglish. (Thefactthat\nships do swim in Russian only amplifies this point.). The practical possibility of \u201cthinking\nmachines\u201dhasbeenwithusforonly50yearsorso,notlongenoughforspeakersofEnglishto\nsettleonameaningfortheword\u201cthink\u201d\u2014does itrequire\u201cabrain\u201dorjust\u201cbrain-like parts.\u201d\nAlanTuring,inhisfamouspaper\u201cComputingMachineryandIntelligence\u201d(1950),sug-\ngested that instead of asking whether machines can think, we should ask whether machines\ncanpassabehavioral intelligencetest,whichhascometobecalledtheTuringTest. Thetest\nTURINGTEST\nis for a program to have a conversation (via online typed messages) with an interrogator for\nfive minutes. The interrogator then has to guess if the conversation is with a program or a\nperson; the program passes the test if it fools the interrogator 30% of the time. Turing con-\njectured that,bytheyear2000, acomputerwithastorage of 109 unitscouldbeprogrammed\nwellenoughtopassthetest. Hewaswrong\u2014programshaveyettofoolasophisticatedjudge.\nOn the other hand, many people have been fooled when they didn\u2019t know they might\nbe chatting with a computer. The ELIZA program and Internet chatbots such as MGONZ\n(Humphrys, 2008) and NATACHATA have fooled their correspondents repeatedly, and the\nchatbotCYBERLOVER hasattractedtheattentionoflawenforcementbecauseofitspenchant\nfortrickingfellowchattersintodivulging enoughpersonalinformation thattheiridentitycan\nbe stolen. The Loebner Prize competition, held annually since 1991, is the longest-running\nTuringTest-likecontest. Thecompetitions haveledtobettermodelsofhumantypingerrors.\nTuring himself examined a wide variety of possible objections to the possibility of in-\ntelligent machines, including virtually all of those that have been raised in the half-century\nsincehispaperappeared. Wewilllookatsomeofthem.\n26.1.1 The argument from disability\nThe\u201cargument from disability\u201d makesthe claim that\u201camachine can neverdo X.\u201dAsexam-\nplesofX,Turingliststhefollowing:\nBekind,resourceful,beautiful,friendly,haveinitiative,haveasenseofhumor,tellright\nfrom wrong, make mistakes, fall in love, enjoy strawberries and cream, make someone\nfallin lovewith it, learnfromexperience,use wordsproperly,bethe subjectofitsown\nthought,haveasmuchdiversityofbehaviorasman,dosomethingreallynew. 1022 Chapter 26. Philosophical Foundations\nIn retrospect, some of these are rather easy\u2014we\u2019re all familiar with computers that \u201cmake\nmistakes.\u201d We are also familiar with a century-old technology that has had a proven ability\nto \u201cmake someone fall in love with it\u201d\u2014the teddy bear. Computer chess expert David Levy\npredicts that by 2050 people will routinely fall in love with humanoid robots (Levy, 2007).\nAsforarobotfallinginlove,thatisacommonthemeinfiction,1 buttherehasbeenonlylim-\nitedspeculation aboutwhetheritisinfactlikely(Kimetal.,2007). Programsdoplaychess,\ncheckers and other games; inspect parts on assembly lines, steer cars and helicopters; diag-\nnose diseases; and do hundreds of other tasks as well as or better than humans. Computers\nhave made small but significant discoveries in astronomy, mathematics, chemistry, mineral-\nogy, biology, computer science, and other fields. Each of these required performance at the\nlevelofahumanexpert.\nGiven what we now know about computers, it is not surprising that they do well at\ncombinatorial problems such as playing chess. Butalgorithms also perform athuman levels\non tasks that seemingly involve human judgment, oras Turing put it, \u201clearning from experi-\nence\u201d and the ability to \u201ctell right from wrong.\u201d As far back as 1955, Paul Meehl (see also\nGroveand Meehl, 1996) studied the decision-making processes of trained experts atsubjec-\ntive tasks such as predicting the success of a student in a training program orthe recidivism\nof a criminal. In 19 out of the 20 studies he looked at, Meehl found that simple statistical\nlearning algorithms(suchaslinearregression ornaiveBayes)predictbetterthantheexperts.\nThe Educational Testing Service has used an automated program to grade millions of essay\nquestions on the GMAT exam since 1999. The program agrees with human graders 97% of\nthetime,aboutthesamelevelthattwohumangraders agree(Burstein etal.,2001).\nItisclearthatcomputerscandomanythingsaswellasorbetterthanhumans,including\nthingsthatpeoplebelieverequiregreathumaninsightandunderstanding. Thisdoesnotmean,\nofcourse,thatcomputersuseinsightandunderstanding inperformingthesetasks\u2014thoseare\nnot part of behavior, and we address such questions elsewhere\u2014but the point is that one\u2019s\nfirstguessaboutthementalprocesses requiredtoproduceagivenbehaviorisoftenwrong. It\nis also true, of course, that there are many tasks at which computers do not yet excel (to put\nitmildly),including Turing\u2019staskofcarrying onanopen-ended conversation.\n26.1.2 The mathematical objection\nIt is well known, through the work of Turing (1936) and Go\u00a8del (1931), that certain math-\nematical questions are in principle unanswerable by particular formal systems. Go\u00a8del\u2019s in-\ncompleteness theorem (see Section 9.5) isthe most famous example of this. Briefly, forany\nformal axiomatic system F powerful enough to do arithmetic, it is possible to construct a\nso-called Go\u00a8delsentence G(F)withthefollowingproperties:\n\u2022 G(F)isasentence ofF,butcannotbeprovedwithinF.\n\u2022 IfF isconsistent, thenG(F)istrue.\n1 Forexample,theoperaCoppe\u00b4lia(1870),thenovelDoAndroidsDreamofElectricSheep?(1968),themovies\nAI(2001)andWall-E(2008),andinsong,NoelCoward\u2019s1955versionofLet\u2019sDoIt:Let\u2019sFallinLovepredicted\n\u201cprobablywe\u2019lllivetoseemachinesdoit.\u201dHedidn\u2019t. Section26.1. WeakAI:CanMachinesActIntelligently? 1023\nPhilosophers suchasJ.R.Lucas(1961) haveclaimedthatthistheorem showsthatmachines\narementallyinferiortohumans,becausemachinesareformalsystemsthatarelimitedbythe\nincompletenesstheorem\u2014theycannotestablishthetruthoftheirownGo\u00a8delsentence\u2014while\nhumans have no such limitation. This claim has caused decades of controversy, spawning a\nvast literature, including two books by the mathematician Sir Roger Penrose (1989, 1994)\nthatrepeattheclaimwithsomefreshtwists(suchasthehypothesisthathumansaredifferent\nbecausetheirbrainsoperatebyquantumgravity). Wewillexamineonlythreeoftheproblems\nwiththeclaim.\nFirst,Go\u00a8del\u2019sincompletenesstheoremappliesonlytoformalsystemsthatarepowerful\nenough to do arithmetic. This includes Turing machines, and Lucas\u2019s claim is in part based\nontheassertionthatcomputersareTuringmachines. Thisisagoodapproximation, butisnot\nquitetrue. Turingmachines areinfinite, whereascomputers arefinite,andanycomputercan\ntherefore bedescribed asa(verylarge) system inpropositional logic, whichisnotsubject to\nGo\u00a8del\u2019s incompleteness theorem. Second, anagent should notbetooashamed thatitcannot\nestablish thetruthofsomesentence whileotheragentscan. Considerthesentence\nJ.R.Lucascannotconsistentlyassertthatthissentenceistrue.\nIf Lucas asserted this sentence, then he would be contradicting himself, so therefore Lucas\ncannotconsistently assertit,andhenceitmustbetrue. Wehavethusdemonstrated thatthere\nisasentencethatLucascannotconsistentlyassertwhileotherpeople(andmachines)can. But\nthatdoesnotmakeusthinklessofLucas. Totakeanotherexample,nohumancouldcompute\nthe sum of a billion 10 digit numbers in his or her lifetime, but a computer could do it in\nseconds. Still, wedonotseethisasafundamental limitation inthehuman\u2019s ability tothink.\nHumanswerebehavingintelligentlyforthousandsofyearsbeforetheyinventedmathematics,\nsoitisunlikelythatformalmathematicalreasoningplaysmorethanaperipheralroleinwhat\nitmeanstobeintelligent.\nThird, and most important, even if we grant that computers have limitations on what\nthey can prove, there is no evidence that humans are immune from those limitations. It is\nall too easy to show rigorously that a formal system cannot do X, and then claim that hu-\nmanscandoX usingtheirowninformalmethod,withoutgivinganyevidenceforthisclaim.\nIndeed, itisimpossible toprovethathumansarenotsubjecttoGo\u00a8del\u2019sincompleteness theo-\nrem,becauseanyrigorousproofwouldrequireaformalizationoftheclaimedunformalizable\nhuman talent, and hence refute itself. So weare left with an appeal to intuition that humans\ncan somehow perform superhuman feats of mathematical insight. This appeal is expressed\nwitharguments suchas\u201cwemustassumeourownconsistency, ifthought istobepossible at\nall\u201d (Lucas, 1976). But if anything, humans are known to be inconsistent. This is certainly\ntrue for everyday reasoning, but it is also true for careful mathematical thought. A famous\nexample is the four-color map problem. Alfred Kempe published a proof in 1879 that was\nwidely accepted and contributed to his election as a Fellow of the Royal Society. In 1890,\nhowever,PercyHeawoodpointed outaflawandthetheorem remainedunproveduntil1977. 1024 Chapter 26. Philosophical Foundations\n26.1.3 The argument from informality\nOneofthemostinfluentialandpersistentcriticismsofAIasanenterprise wasraisedbyTur-\ningasthe\u201cargument from informality ofbehavior.\u201d Essentially, this istheclaim thathuman\nbehavior is fartoo complex to be captured by any simple set of rules and that because com-\nputers candonomorethanfollowasetofrules, theycannot generate behavior asintelligent\nas that of humans. The inability to capture everything in a set of logical rules is called the\nQUALIFICATION qualificationprobleminAI.\nPROBLEM\nThe principal proponent of this view has been the philosopher Hubert Dreyfus, who\nhasproduced aseriesofinfluentialcritiques ofartificialintelligence: WhatComputersCan\u2019t\nDo (1972), the sequel What Computers Still Can\u2019t Do (1992), and, with his brother Stuart,\nMindOverMachine(1986).\nThe position they criticize came to be called \u201cGood Old-Fashioned AI,\u201d or GOFAI, a\nterm coined by philosopher John Haugeland (1985). GOFAI is supposed to claim that all\nintelligent behaviorcanbecapturedbyasystemthatreasonslogicallyfromasetoffactsand\nrules describing thedomain. Ittherefore corresponds tothe simplest logical agent described\ninChapter7. Dreyfusiscorrectinsayingthatlogicalagentsarevulnerabletothequalification\nproblem. AswesawinChapter13,probabilistic reasoning systemsaremoreappropriate for\nopen-ended domains. TheDreyfus critique therefore isnot addressed against computers per\nse, but rather against one particular way of programming them. It is reasonable to suppose,\nhowever,thatabookcalled WhatFirst-OrderLogicalRule-BasedSystemsWithoutLearning\nCan\u2019tDomighthavehadlessimpact.\nUnderDreyfus\u2019sview,humanexpertisedoesincludeknowledgeofsomerules,butonly\nasa\u201cholistic context\u201d or\u201cbackground\u201d withinwhichhumans operate. Hegivestheexample\nofappropriate social behavior in giving and receiving gifts: \u201cNormally one simply responds\nintheappropriate circumstances bygiving anappropriate gift.\u201d Oneapparently has\u201cadirect\nsense ofhow things are done andwhattoexpect.\u201d Thesameclaim ismadeinthe context of\nchess playing: \u201cAmere chess mastermight need tofigure out whatto do, but a grandmaster\njustseestheboardasdemandingacertainmove...therightresponsejustpopsintohisorher\nhead.\u201d Itiscertainlytruethatmuchofthethoughtprocessesofapresent-giverorgrandmaster\nis done at a level that is not open to introspection by the conscious mind. But that does not\nmean that the thought processes do not exist. The important question that Dreyfus does not\nanswer is how the right move gets into the grandmaster\u2019s head. One is reminded of Daniel\nDennett\u2019s(1984)comment,\nItisratherasifphilosophersweretoproclaimthemselvesexpertexplainersofthemeth-\nods of stage magicians, and then, when we ask how the magician does the sawing-the-\nlady-in-halftrick,theyexplainthatitisreallyquiteobvious: themagiciandoesn\u2019treally\nsawherinhalf;hesimplymakesitappearthathedoes. \u201cButhowdoeshedothat?\u201d we\nask. \u201cNotourdepartment,\u201dsaythephilosophers.\nDreyfus and Dreyfus (1986) propose a five-stage process of acquiring expertise, beginning\nwith rule-based processing (of the sort proposed in GOFAI) and ending with the ability to\nselect correct responses instantaneously. In making this proposal, Dreyfus and Dreyfus in\neffectmovefrombeingAIcriticstoAItheorists\u2014theyproposeaneuralnetworkarchitecture Section26.1. WeakAI:CanMachinesActIntelligently? 1025\norganized into a vast \u201ccase library,\u201d but point out several problems. Fortunately, all of their\nproblems havebeenaddressed, somewithpartialsuccess and somewithtotalsuccess. Their\nproblemsincludethefollowing:\n1. Good generalization from examples cannot be achieved without background knowl-\nedge. They claim no one has any idea how to incorporate background knowledge into\nthe neural network learning process. In fact, we saw in Chapters 19 and 20 that there\nare techniques for using prior knowledge in learning algorithms. Those techniques,\nhowever,relyontheavailability ofknowledgeinexplicitform,somethingthatDreyfus\nandDreyfus strenuously deny. Inourview, thisisagood reason foraserious redesign\nof current models of neural processing so that they can take advantage of previously\nlearnedknowledgeinthewaythatotherlearning algorithms do.\n2. Neural network learning is a form of supervised learning (see Chapter 18), requiring\nthe prior identification of relevant inputs and correct outputs. Therefore, they claim,\nit cannot operate autonomously without the help of a human trainer. In fact, learning\nwithout a teacher can be accomplished by unsupervised learning (Chapter 20) and\nreinforcementlearning(Chapter21).\n3. Learning algorithms do not perform well with many features, and if we pick a subset\noffeatures,\u201cthereisnoknownwayofaddingnewfeaturesshouldthecurrentsetprove\ninadequate to account for the learned facts.\u201d In fact, new methods such as support\nvector machines handle large feature sets very well. With the introduction of large\nWeb-based data sets, manyapplications inareas such aslanguage processing (Shaand\nPereira, 2003) and computervision (ViolaandJones, 2002a) routinely handle millions\nof features. We saw in Chapter 19 that there are also principled ways to generate new\nfeatures, although muchmoreworkisneeded.\n4. The brain is able to direct its sensors to seek relevant information and to process it\nto extract aspects relevant to the current situation. But, Dreyfus and Dreyfus claim,\n\u201cCurrently, nodetails ofthismechanism areunderstood orevenhypothesized inaway\nthat could guide AI research.\u201d In fact, the field of active vision, underpinned by the\ntheory of information value (Chapter 16), is concerned with exactly the problem of\ndirecting sensors, and already some robots have incorporated the theoretical results\nobtained. STANLEY\u2019s132-mile trip through thedesert (page 28) wasmadepossible in\nlargepartbyanactivesensingsystem ofthiskind.\nInsum, manyofthe issues Dreyfus has focused on\u2014background commonsense knowledge,\nthe qualification problem, uncertainty, learning, compiled forms of decision making\u2014are\nindeed important issues, and have by now been incorporated into standard intelligent agent\ndesign. Inourview,thisisevidence ofAI\u2019sprogress, notof itsimpossibility.\nOne of Dreyfus\u2019 strongest arguments is for situated agents rather than disembodied\nlogicalinferenceengines. Anagentwhoseunderstandingof\u201cdog\u201dcomesonlyfromalimited\nset of logical sentences such as \u201cDog(x) \u21d2 Mammal(x)\u201d is at a disadvantage compared\nto an agent that has watched dogs run, has played fetch with them, and has been licked by\none. As philosopher Andy Clark (1998) says, \u201cBiological brains are first and foremost the\ncontrol systems for biological bodies. Biological bodies move and act in rich real-world 1026 Chapter 26. Philosophical Foundations\nsurroundings.\u201d Tounderstandhowhuman(orotheranimal)agentswork,wehavetoconsider\nEMBODIED thewholeagent,notjusttheagentprogram. Indeed,theembodiedcognitionapproachclaims\nCOGNITION\nthat it makes no sense to consider the brain separately: cognition takes place within a body,\nwhich is embedded in an environment. We need to study the system as a whole; the brain\naugmentsitsreasoning byreferringtotheenvironment, asthereaderdoesinperceiving (and\ncreating) marks on paper to transfer knowledge. Under the embodied cognition program,\nrobotics, vision, andothersensorsbecomecentral, notperipheral.\n26.2 STRONG AI: CAN MACHINES REALLY THINK?\nMany philosophers have claimed that a machine that passes the Turing Test would still not\nbe actually thinking, but would be only a simulation of thinking. Again, the objection was\nforeseen byTuring. HecitesaspeechbyProfessorGeoffreyJefferson(1949):\nNotuntilamachinecouldwriteasonnetorcomposeaconcertobecauseofthoughtsand\nemotionsfelt,andnotbythechancefallofsymbols,couldweagreethatmachineequals\nbrain\u2014thatis,notonlywriteitbutknowthatithadwrittenit.\nTuring calls this theargument from consciousness\u2014the machine has to beaware ofits own\nmentalstatesandactions. Whileconsciousness isanimportant subject, Jefferson\u2019s keypoint\nactually relates to phenomenology, or the study of direct experience: the machine has to\nactually feel emotions. Others focus on intentionality\u2014that is, the question of whether the\nmachine\u2019s purported beliefs, desires, and other representations are actually \u201cabout\u201d some-\nthingintherealworld.\nTuring\u2019s response to the objection is interesting. He could have presented reasons that\nmachines can in fact be conscious (or have phenomenology, or have intentions). Instead, he\nmaintains that the question is just as ill-defined as asking, \u201cCan machines think?\u201d Besides,\nwhy should we insist on a higher standard for machines than we do for humans? After all,\nin ordinary life we never have any direct evidence about the internal mental states of other\nhumans. Nevertheless,Turingsays,\u201cInsteadofarguingcontinuallyoverthispoint,itisusual\ntohavethepoliteconvention thateveryonethinks.\u201d\nTuring argues that Jefferson would be willing to extend the polite convention to ma-\nchinesifonlyhehadexperiencewithonesthatactintelligently. Hecitesthefollowingdialog,\nwhichhasbecomesuchapartofAI\u2019soraltraditionthatwesimplyhavetoinclude it:\nHUMAN: Inthefirstlineofyoursonnetwhichreads\u201cshallIcomparetheetoasummer\u2019s\nday,\u201dwouldnota\u201cspringday\u201ddoaswellorbetter?\nMACHINE: Itwouldn\u2019tscan.\nHUMAN: Howabout\u201cawinter\u2019sday.\u201dThatwouldscanallright.\nMACHINE: Yes,butnobodywantstobecomparedtoawinter\u2019sday.\nHUMAN: WouldyousayMr.PickwickremindedyouofChristmas?\nMACHINE: Inaway.\nHUMAN: YetChristmasisawinter\u2019sday,andIdonotthinkMr.Pickwickwouldmind\nthecomparison. Section26.2. StrongAI:CanMachines ReallyThink? 1027\nMACHINE: Idon\u2019tthinkyou\u2019reserious. Byawinter\u2019sdayonemeansatypicalwinter\u2019s\nday,ratherthanaspecialonelikeChristmas.\nOne can easily imagine some future time in which such conversations with machines are\ncommonplace, and it becomes customary to make no linguistic distinction between \u201creal\u201d\nand\u201cartificial\u201dthinking. Asimilartransition occurred in theyearsafter1848,whenartificial\nurea wassynthesized forthe first timeby Frederick Wo\u00a8hler. Priorto this event, organic and\ninorganic chemistry were essentially disjoint enterprises and many thought that no process\ncould existthatwouldconvert inorganic chemicals intoorganic material. Oncethesynthesis\nwas accomplished, chemists agreed that artificial urea was urea, because it had all the right\nphysical properties. Those who had posited an intrinsic property possessed by organic ma-\nterial that inorganic material could never have were faced with the impossibility of devising\nanytestthatcouldrevealthesupposed deficiencyofartificialurea.\nFor thinking, we have not yet reached our 1848 and there are those who believe that\nartificialthinking,nomatterhowimpressive,willneverbereal. Forexample,thephilosopher\nJohnSearle(1980)argues asfollows:\nNoonesupposesthatacomputersimulationofastormwillleaveusallwet...Whyon\nearthwouldanyoneinhisrightmindsupposeacomputersimulationofmentalprocesses\nactuallyhadmentalprocesses?(pp.37\u201338)\nWhile it is easy to agree that computer simulations of storms do not make us wet, it is not\nclear how to carry this analogy over to computer simulations of mental processes. After\nall, a Hollywood simulation of a storm using sprinklers and wind machines does make the\nactors wet, andavideo gamesimulation ofastorm does makethe simulated characters wet.\nMost people are comfortable saying that a computer simulation of addition is addition, and\nofchessischess. Infact,wetypically speakofanimplementation ofaddition orchess,nota\nsimulation. Arementalprocesses morelikestorms,ormorelikeaddition?\nTuring\u2019s answer\u2014the polite convention\u2014suggests that the issue will eventually go\naway by itself once machines reach a certain level of sophistication. This would have the\neffect ofdissolving the difference between weakand strong AI. Against this, one may insist\nthat there is a factual issue at stake: humans do have real minds, and machines might or\nmight not. To address this factual issue, we need to understand how it is that humans have\nreal minds, not just bodies that generate neurophysiological processes. Philosophical efforts\nMIND\u2013BODY to solve this mind\u2013bodyproblem are directly relevant to the question ofwhether machines\nPROBLEM\ncouldhaverealminds.\nThemind\u2013bodyproblemwasconsideredbytheancientGreekphilosophers andbyvar-\nious schools of Hindu thought, but was first analyzed in depth by the 17th-century French\nphilosopher andmathematician Rene\u00b4 Descartes. HisMeditations onFirstPhilosophy(1641)\nconsidered themind\u2019s activity ofthinking (aprocess withnospatial extent ormaterial prop-\nerties) andthephysical processes ofthebody, concluding that thetwomustexist inseparate\nrealms\u2014what we would now call a dualist theory. The mind\u2013body problem faced by du-\nDUALISM\nalists is the question of how the mind can control the body if the two are really separate.\nDescartesspeculated thatthetwomightinteractthroughthepinealgland,whichsimplybegs\nthequestion ofhowthemindcontrolsthepinealgland. 1028 Chapter 26. Philosophical Foundations\nThemonisttheoryofmind,oftencalled physicalism,avoids thisproblem byasserting\nMONISM\nthemindisnotseparate fromthebody\u2014that mentalstates arephysical states. Mostmodern\nPHYSICALISM\nphilosophers ofmindarephysicalistsofoneformoranother, andphysicalismallows,atleast\nin principle, for the possibility of strong AI. The problem for physicalists is to explain how\nphysicalstates\u2014in particular, themolecularconfigurations andelectrochemical processes of\nthebrain\u2014cansimultaneouslybementalstates,suchasbeinginpain,enjoyingahamburger,\nMENTALSTATES\nknowingthatoneisridingahorse,orbelieving thatViennaisthecapitalofAustria.\n26.2.1 Mental states andthe brainina vat\nPhysicalistphilosophershaveattemptedtoexplicatewhatitmeanstosaythataperson\u2014and,\nbyextension, acomputer\u2014is inaparticularmentalstate. Theyhavefocused inparticularon\nintentionalstates. Thesearestates,suchasbelieving, knowing,desiring, fearing,andsoon,\nINTENTIONALSTATE\nthatrefertosomeaspectoftheexternalworld. Forexample, theknowledgethatoneiseating\nahamburgerisabelief aboutthehamburgerandwhatishappening toit.\nIf physicalism is correct, it must be the case that the proper description of a person\u2019s\nmental state is determined by that person\u2019s brain state. Thus, if I am currently focused on\neatingahamburgerinamindfulway,myinstantaneousbrainstateisaninstanceoftheclassof\nmentalstates\u201cknowingthatoneiseatingahamburger.\u201d Ofcourse,thespecificconfigurations\nof all the atoms ofmy brain are not essential: there are manyconfigurations of mybrain, or\nofotherpeople\u2019sbrain,thatwouldbelongtothesameclassofmentalstates. Thekeypointis\nthat the same brain state could not correspond toa fundamentally distinct mental state, such\nastheknowledge thatoneiseatingabanana.\nThe simplicity of this view is challenged by some simple thought experiments. Imag-\nine, if you will, that your brain was removed from your body at birth and placed in a mar-\nvelouslyengineered vat. Thevatsustainsyourbrain,allowingittogrowanddevelop. Atthe\nsametime,electronic signalsarefedtoyourbrainfromacomputersimulation ofanentirely\nfictitious world, and motor signals from your brain are intercepted and used to modify the\nsimulation as appropriate.2 In fact, the simulated life you live replicates exactly the life you\nwould have lived, had your brain not been placed in the vat, including simulated eating of\nsimulatedhamburgers. Thus,youcouldhaveabrainstateidenticaltothatofsomeonewhois\nreally eating areal hamburger, but it would be literally false to say that you have the mental\nstate \u201cknowing that one is eating a hamburger.\u201d You aren\u2019t eating a hamburger, you have\nneverevenexperienced ahamburger, andyoucouldnot,therefore, havesuchamentalstate.\nThisexampleseemstocontradicttheviewthatbrainstatesdeterminementalstates. One\nwaytoresolvethedilemmaistosaythatthecontentofmentalstatescanbeinterpreted from\ntwo different points of view. The \u201cwide content\u201d view interprets it from the point of view\nWIDECONTENT\nof an omniscient outside observer with access to the whole situation, who can distinguish\ndifferencesintheworld. Underthisview,thecontentofmentalstatesinvolvesboththebrain\nstate and the environment history. Narrow content, on the other hand, considers only the\nNARROWCONTENT\nbrainstate. Thenarrowcontentofthebrainstatesofarealhamburger-eater andabrain-in-a-\nvat\u201chamburger\u201d-\u201ceater\u201d isthesameinbothcases.\n2 Thissituationmaybefamiliartothosewhohaveseenthe1999filmTheMatrix. Section26.2. StrongAI:CanMachines ReallyThink? 1029\nWidecontent isentirelyappropriate ifone\u2019sgoalsaretoascribe mentalstatestoothers\nwho share one\u2019s world, to predict their likely behavior and its effects, and so on. This is the\nsettinginwhichourordinarylanguage aboutmentalcontent hasevolved. Ontheotherhand,\nif one is concerned with the question of whether AI systems are really thinking and really\ndo have mental states, then narrow content is appropriate; it simply doesn\u2019t make sense to\nsay that whether or not an AI system is really thinking depends on conditions outside that\nsystem. Narrow content is also relevant if we are thinking about designing AI systems or\nunderstandingtheiroperation,becauseitisthenarrowcontentofabrainstatethatdetermines\nwhatwillbethe(narrow content ofthe)nextbrainstate. Thisleads naturally totheideathat\nwhat matters about a brain state\u2014what makes it have one kind of mental content and not\nanother\u2014is itsfunctional rolewithinthementaloperation oftheentityinvolved.\n26.2.2 Functionalism andthe brainreplacement experiment\nThe theory of functionalism says that a mental state is any intermediate causal condition\nFUNCTIONALISM\nbetween input and output. Under functionalist theory, any two systems with isomorphic\ncausal processes would have the same mental states. Therefore, a computer program could\nhave thesamemental states as aperson. Ofcourse, wehavenot yetsaid what\u201cisomorphic\u201d\nreally means, but the assumption is that there is some level of abstraction below which the\nspecificimplementation doesnotmatter.\nThe claims of functionalism are illustrated most clearly by the brain replacement ex-\nperiment. This thought experiment was introduced by the philosopher Clark Glymour and\nwastouchedonbyJohnSearle(1980),butismostcommonlyassociatedwithroboticistHans\nMoravec(1988). Itgoeslikethis: Supposeneurophysiology hasdevelopedtothepointwhere\ntheinput\u2013outputbehaviorandconnectivityofalltheneuronsinthehumanbrainareperfectly\nunderstood. Supposefurtherthatwecanbuildmicroscopicelectronicdevicesthatmimicthis\nbehavior and can be smoothly interfaced to neural tissue. Lastly, suppose that some mirac-\nulous surgical technique can replace individual neurons with the corresponding electronic\ndevices without interrupting the operation of the brain as a whole. The experiment consists\nofgradually replacing alltheneurons insomeone\u2019s headwithelectronic devices.\nWe are concerned with both the external behavior and the internal experience of the\nsubject, during and after the operation. By the definition of the experiment, the subject\u2019s\nexternal behavior must remain unchanged compared with what would be observed if the\noperation were not carried out.3 Now although the presence or absence of consciousness\ncannot easily be ascertained by a third party, the subject of the experiment ought at least to\nbe able to record any changes in his or her own conscious experience. Apparently, there is\na direct clash of intuitions as to what would happen. Moravec, a robotics researcher and\nfunctionalist, isconvinced hisconsciousness wouldremainunaffected. Searle,aphilosopher\nandbiological naturalist, isequallyconvinced hisconsciousness wouldvanish:\nYou find, to your total amazement, that you are indeed losing control of your external\nbehavior. You find, forexample, thatwhen doctorstest yourvision, youhearthem say\n\u201cWeareholdinguparedobjectinfrontofyou;pleasetelluswhatyousee.\u201d Youwant\n3 Onecanimagineusinganidentical\u201ccontrol\u201dsubjectwhoisgivenaplacebooperation,forcomparison. 1030 Chapter 26. Philosophical Foundations\ntocryout\u201cIcan\u2019tseeanything. I\u2019mgoingtotallyblind.\u201d Butyouhearyourvoicesaying\nin a way thatis completelyout of yourcontrol, \u201cI see a red object in frontof me.\u201d ...\nyour conscious experience slowly shrinks to nothing, while your externally observable\nbehaviorremainsthesame.(Searle,1992)\nOne can do more than argue from intuition. First, note that, for the external behavior to re-\nmainthesamewhilethesubject gradually becomesunconscious, itmustbethecasethatthe\nsubject\u2019s volition is removed instantaneously and totally; otherwise the shrinking of aware-\nnesswouldbereflectedinexternalbehavior\u2014\u201cHelp, I\u2019mshrinking!\u201d orwordstothateffect.\nThis instantaneous removal of volition as a result of gradual neuron-at-a-time replacement\nseemsanunlikely claimtohavetomake.\nSecond, consider what happens if we do ask the subject questions concerning his or\nherconscious experience during the period when no real neurons remain. Bythe conditions\nof the experiment, we will get responses such as \u201cI feel fine. I must say I\u2019m a bit surprised\nbecauseIbelievedSearle\u2019sargument.\u201d Orwemightpokethesubjectwithapointedstickand\nobservetheresponse, \u201cOuch,thathurt.\u201d Now,inthenormalcourseofaffairs,theskepticcan\ndismiss suchoutputs fromAIprograms asmerecontrivances. Certainly, itiseasy enough to\nusearule suchas\u201cIfsensor12reads \u2018High\u2019thenoutput \u2018Ouch.\u2019\u201d Butthepoint hereisthat,\nbecause we have replicated the functional properties of a normal human brain, we assume\nthattheelectronic braincontainsnosuchcontrivances. Thenwemusthaveanexplanation of\nthemanifestations ofconsciousness produced bytheelectronic brainthatappealsonlytothe\nfunctional properties of the neurons. Andthis explanation must also apply to the real brain,\nwhichhasthesamefunctional properties. Therearethreepossible conclusions:\n1. Thecausalmechanismsofconsciousnessthatgeneratethesekindsofoutputsinnormal\nbrainsarestilloperating intheelectronic version, which istherefore conscious.\n2. Theconsciousmentaleventsinthenormalbrainhavenocausalconnectiontobehavior,\nandaremissingfromtheelectronic brain, whichistherefore notconscious.\n3. Theexperimentisimpossible, andtherefore speculation aboutitismeaningless.\nAlthoughwecannotruleoutthesecondpossibility, itreducesconsciousnesstowhatphiloso-\npherscallanepiphenomenalrole\u2014something thathappens, butcastsnoshadow,asitwere,\nEPIPHENOMENON\non the observable world. Furthermore, if consciousness is indeed epiphenomenal, then it\ncannotbethecasethatthesubjectsays\u201cOuch\u201dbecauseithurts\u2014thatis,becauseofthecon-\nsciousexperience ofpain. Instead, thebrainmustcontain a second, unconscious mechanism\nthatisresponsible forthe\u201cOuch.\u201d\nPatricia Churchland (1986) points out that the functionalist arguments that operate at\nthe level of the neuron can also operate at the level of any larger functional unit\u2014a clump\nof neurons, a mental module, a lobe, a hemisphere, or the whole brain. That means that if\nyouacceptthenotionthatthebrainreplacementexperiment showsthatthereplacementbrain\nis conscious, then you should also believe that consciousness is maintained when the entire\nbrainisreplacedbyacircuitthatupdatesitsstateandmaps frominputstooutputsviaahuge\nlookup table. This is disconcerting to many people (including Turing himself), who have\ntheintuition thatlookup tablesarenotconscious\u2014or atleast, thattheconscious experiences\ngenerated during table lookup are not the same as those generated during the operation of a Section26.2. StrongAI:CanMachines ReallyThink? 1031\nsystem thatmightbedescribed (eveninasimple-minded, computational sense)asaccessing\nandgenerating beliefs, introspections, goals,andsoon.\n26.2.3 Biologicalnaturalism and theChinese Room\nA strong challenge to functionalism has been mounted by John Searle\u2019s (1980) biological\nBIOLOGICAL naturalism,accordingtowhichmentalstatesarehigh-levelemergentfeaturesthatarecaused\nNATURALISM\nby low-level physical processes in the neurons, and it is the (unspecified) properties of the\nneurons that matter. Thus, mental states cannot be duplicated just on the basis of some pro-\ngram having the same functional structure with the same input\u2013output behavior; we would\nrequirethattheprogramberunningonanarchitecturewiththesamecausalpowerasneurons.\nTosupporthisview,Searledescribes ahypothetical system thatisclearlyrunning aprogram\nandpassestheTuringTest,butthatequallyclearly(according toSearle)doesnotunderstand\nanything of its inputs and outputs. His conclusion is that running the appropriate program\n(i.e.,havingtherightoutputs) isnotasufficient condition forbeingamind.\nThe system consists of a human, who understands only English, equipped with a rule\nbook, writteninEnglish, and various stacks ofpaper, someblank, somewithindecipherable\ninscriptions. (The human therefore plays the role of the CPU, the rule book is the program,\nand the stacks of paper are the storage device.) The system is inside a room with a small\nopening totheoutside. Through theopening appear slips ofpaperwithindecipherable sym-\nbols. Thehuman findsmatching symbols intherule book, andfollows theinstructions. The\ninstructionsmayincludewritingsymbolsonnewslipsofpaper,findingsymbolsinthestacks,\nrearrangingthestacks,andsoon. Eventually,theinstructionswillcauseoneormoresymbols\ntobetranscribed ontoapieceofpaperthatispassedbacktotheoutsideworld.\nSofar, so good. But from the outside, wesee a system that is taking input in the form\nof Chinese sentences and generating answers in Chinese that are as \u201cintelligent\u201d as those\nin the conversation imagined by Turing.4 Searle then argues: the person in the room does\nnot understand Chinese (given). The rule book and the stacks of paper, being just pieces of\npaper, do not understand Chinese. Therefore, there is no understanding of Chinese. Hence,\naccordingtoSearle,runningtherightprogramdoesnotnecessarilygenerateunderstanding.\nLike Turing, Searle considered and attempted to rebuff a number of replies to his ar-\ngument. Several commentators, including John McCarthy and Robert Wilensky, proposed\nwhat Searle calls the systems reply. The objection is that asking if the human in the room\nunderstands Chinese is analogous to asking if the CPU can take cube roots. In both cases,\nthe answer is no, and in both cases, according to the systems reply, the entire system does\nhavethecapacityinquestion. Certainly,ifoneaskstheChineseRoomwhetheritunderstands\nChinese, theanswerwouldbeaffirmative(influentChinese). ByTuring\u2019spoliteconvention,\nthisshouldbeenough. Searle\u2019sresponse istoreiteratethe pointthattheunderstanding isnot\ninthehumanand cannot beinthepaper, sothere cannot beanyunderstanding. Heseemsto\nbe relying on the argument that a property of the whole must reside in one of the parts. Yet\n4 Thefact that thestacks of paper might contain trillionsof pages and thegeneration of answerswould take\nmillionsofyearshasnobearingonthelogicalstructureoftheargument. Oneaimofphilosophicaltrainingisto\ndevelopafinelyhonedsenseofwhichobjectionsaregermaneandwhicharenot. 1032 Chapter 26. Philosophical Foundations\nwateriswet, even though neither HnorO is. Therealclaim madebySearle rests upon the\n2\nfollowingfouraxioms(Searle,1990):\n1. Computerprogramsareformal(syntactic).\n2. Humanmindshavementalcontents (semantics).\n3. Syntaxbyitselfisneitherconstitutive ofnorsufficient forsemantics.\n4. Brainscauseminds.\nFrom the first three axioms Searle concludes that programs are not sufficient for minds. In\notherwords,anagentrunningaprogram mightbeamind,butitisnotnecessarilyamindjust\nby virtue of running the program. From the fourth axiom he concludes \u201cAny other system\ncapable of causing minds would have to have causal powers (at least) equivalent to those\nof brains.\u201d From there he infers that any artificial brain would have to duplicate the causal\npowers of brains, not just run a particular program, and that human brains do not produce\nmentalphenomena solelybyvirtueofrunningaprogram.\nThe axioms are controversial. For example, axioms 1 and 2 rely on an unspecified\ndistinction between syntax and semantics that seems to be closely related to the distinction\nbetweennarrowandwidecontent. Ontheonehand,wecanviewcomputersasmanipulating\nsyntactic symbols; on the other, we can view them as manipulating electric current, which\nhappens to be what brains mostly do (according to our current understanding). So it seems\nwecouldequally saythatbrainsaresyntactic.\nAssuming we are generous in interpreting the axioms, then the conclusion\u2014that pro-\ngrams are not sufficient for minds\u2014does follow. But the conclusion is unsatisfactory\u2014all\nSearlehasshownisthatifyouexplicitly denyfunctionalism (thatiswhathisaxiom3does),\nthen you can\u2019t necessarily conclude that non-brains are minds. This is reasonable enough\u2014\nalmost tautological\u2014so the whole argument comes down to whether axiom 3 can be ac-\ncepted. AccordingtoSearle,thepointoftheChineseRoomargumentistoprovideintuitions\nfor axiom 3. The public reaction shows that the argument is acting as what Daniel Dennett\n(1991) calls an intuition pump: it amplifies one\u2019s prior intuitions, so biological naturalists\nINTUITIONPUMP\nare more convinced of their positions, and functionalists are convinced only that axiom 3 is\nunsupported, or that in general Searle\u2019s argument is unconvincing. The argument stirs up\ncombatants, but has done little to change anyone\u2019s opinion. Searle remains undeterred, and\nhas recently started calling the Chinese Room a\u201crefutation\u201d ofstrong AI rather than just an\n\u201cargument\u201d (Snell,2008).\nEventhosewhoaccept axiom 3,andthus acceptSearle\u2019s argument, haveonly theirin-\ntuitionstofallbackonwhendecidingwhatentitiesareminds. Theargumentpurportstoshow\nthattheChineseRoomisnotamindbyvirtueofrunningtheprogram,buttheargumentsays\nnothing about how to decide whether the room (ora computer, some other type of machine,\noranalien)isamindbyvirtueofsomeotherreason. Searlehimselfsaysthatsomemachines\ndo have minds: humans are biological machines with minds. According to Searle, human\nbrains may or may not be running something like an AI program, but if they are, that is not\nthe reason they are minds. It takes more to make a mind\u2014according to Searle, something\nequivalent tothecausal powersof individual neurons. What these powersare isleftunspec-\nified. Itshould benoted, however, that neurons evolved to fulfill functional roles\u2014creatures Section26.2. StrongAI:CanMachines ReallyThink? 1033\nwithneuronswerelearninganddecidinglongbeforeconsciousness appearedonthescene. It\nwouldbearemarkable coincidence ifsuchneurons justhappened togenerate consciousness\nbecause of some causal powers that are irrelevant to their functional capabilities; after all, it\nisthefunctional capabilities thatdictate survivaloftheorganism.\nIn the case of the Chinese Room, Searle relies on intuition, not proof: just look at the\nroom; what\u2019s there to be a mind? But one could make the same argument about the brain:\njust look at this collection of cells (or of atoms), blindly operating according to the laws of\nbiochemistry(orofphysics)\u2014what\u2019stheretobeamind? Whycanahunkofbrainbeamind\nwhileahunkoflivercannot? Thatremainsthegreatmystery.\n26.2.4 Consciousness,qualia, and theexplanatory gap\nRunning through all the debates about strong AI\u2014the elephant in the debating room, so\nto speak\u2014is the issue of consciousness. Consciousness is often broken down into aspects\nCONSCIOUSNESS\nsuch as understanding and self-awareness. The aspect we will focus on is that of subjective\nexperience: whyitisthatitfeelslikesomethingtohavecertainbrainstates(e.g.,whileeating\nahamburger), whereasitpresumablydoesnotfeellikeanythingtohaveotherphysicalstates\n(e.g.,whilebeingarock). Thetechnicaltermfortheintrinsicnatureofexperiences isqualia\nQUALIA\n(fromtheLatinwordmeaning,roughly, \u201csuchthings\u201d).\nQualia present a challenge for functionalist accounts of the mind because different\nqualia could be involved in what are otherwise isomorphic causal processes. Consider, for\nINVERTED example,theinvertedspectrumthoughtexperiment,whichthesubjectiveexperienceofper-\nSPECTRUM\nson X when seeing red objects is the same experience that the rest of us experience when\nseeinggreenobjects,andviceversa. X stillcallsredobjects\u201cred,\u201dstopsforredtrafficlights,\nand agrees that the redness of red traffic lights is a more intense red than the redness of the\nsettingsun. Yet,X\u2019ssubjective experience isjustdifferent.\nQualiaarechallengingnotjustforfunctionalism butforallofscience. Suppose,forthe\nsakeofargument,thatwehavecompletedtheprocessofscientificresearchonthebrain\u2014we\nhave found that neural process P in neuron N transforms molecule Ainto molecule B,\n12 177\nand so on, and on. There is simply no currently accepted form of reasoning that would lead\nfrom such findings to the conclusion that the entity owning those neurons has any particular\nsubjective experience. This explanatory gap has led some philosophers to conclude that\nEXPLANATORYGAP\nhumansaresimplyincapable offorming aproperunderstanding oftheirownconsciousness.\nOthers, notably Daniel Dennett (1991), avoid the gap by denying the existence of qualia,\nattributing themtoaphilosophical confusion.\nTuringhimselfconcedesthatthequestionofconsciousness isadifficultone,butdenies\nthat it has much relevance to the practice of AI: \u201cI do not wish to give the impression that I\nthink there is no mystery about consciousness ... But I do not think these mysteries neces-\nsarily need to be solved before we can answer the question with which we are concerned in\nthispaper.\u201d WeagreewithTuring\u2014weareinterested increating programs thatbehave intel-\nligently. Theadditional project ofmaking themconscious isnotonethatweareequipped to\ntakeon,noronewhosesuccesswewouldbeabletodetermine. 1034 Chapter 26. Philosophical Foundations\n26.3 THE ETHICS AND RISKS OF DEVELOPING ARTIFICIAL INTELLIGENCE\nSo far, we have concentrated on whether we can develop AI, but we must also consider\nwhetherweshould. IftheeffectsofAItechnologyaremorelikelytobenegativethanpositive,\nthen it would be the moral responsibility of workers in the field to redirect their research.\nMany new technologies have had unintended negative side effects: nuclear fission brought\nChernobyl and the threat of global destruction; the internal combustion engine brought air\npollution, global warming, and the paving-over of paradise. In a sense, automobiles are\nrobotsthathaveconquered theworldbymakingthemselves indispensable.\nAll scientists and engineers face ethical considerations of how they should act on the\njob, what projects should or should not be done, and how they should be handled. See the\nhandbook on the Ethics of Computing (Berleur and Brunnstein, 2001). AI, however, seems\ntoposesomefreshproblemsbeyondthatof,say,building bridgesthatdon\u2019tfalldown:\n\u2022 Peoplemightlosetheirjobstoautomation.\n\u2022 Peoplemighthavetoomuch(ortoolittle)leisuretime.\n\u2022 Peoplemightlosetheirsenseofbeingunique.\n\u2022 AIsystemsmightbeusedtowardundesirable ends.\n\u2022 TheuseofAIsystemsmightresultinalossofaccountability.\n\u2022 ThesuccessofAImightmeantheendofthehumanrace.\nWewilllookateachissueinturn.\nPeoplemightlose theirjobsto automation. Themodern industrial economy has be-\ncomedependent oncomputers ingeneral,andselectAIprogramsinparticular. Forexample,\nmuch of the economy, especially in the United States, depends on the availability of con-\nsumer credit. Credit card applications, charge approvals, and fraud detection are now done\nby AI programs. One could say that thousands of workers have been displaced by these AI\nprograms, but in fact if you took away the AI programs these jobs would not exist, because\nhuman laborwould addanunacceptable costtothetransactions. Sofar, automation through\ninformation technology in general and AI in particular has created more jobs than it has\neliminated, and has created more interesting, higher-paying jobs. Nowthat thecanonical AI\nprogram isan\u201cintelligent agent\u201d designed toassistahuman,lossofjobsislessofaconcern\nthan it was when AI focused on \u201cexpert systems\u201d designed to replace humans. But some\nresearchersthinkthatdoingthecompletejobistherightgoalforAI.Inreflectingonthe25th\nAnniversaryoftheAAAI,NilsNilsson(2005)setasachallenge thecreation ofhuman-level\nAIthat could pass theemployment testrather than the Turing Test\u2014arobot that could learn\ntodoanyoneofarangeofjobs. Wemayendupinafuturewhereunemploymentishigh,but\neventheunemployed serveasmanagers oftheirowncadreofrobotworkers.\nPeoplemighthavetoomuch(ortoolittle)leisuretime. AlvinTofflerwroteinFuture\nShock (1970), \u201cThe work week has been cut by 50 percent since the turn of the century. It\nis not out of the way to predict that it will be slashed in half again by 2000.\u201d Arthur C.\nClarke (1968b) wrote that people in 2001 might be \u201cfaced with a future of utter boredom,\nwherethemainproblem inlifeisdeciding whichofseveral hundred TVchannels toselect.\u201d Section26.3. TheEthicsandRisksofDeveloping ArtificialIntelligence 1035\nThe only one of these predictions that has come close to panning out is the number of TV\nchannels. Instead, people working inknowledge-intensive industries have found themselves\npartofanintegratedcomputerized systemthatoperates24hoursaday;tokeepup,theyhave\nbeenforcedtoworklongerhours. Inanindustrialeconomy,rewardsareroughlyproportional\nto the time invested; working 10% more would tend to mean a 10% increase in income. In\nan information economy marked by high-bandwidth communication and easy replication of\nintellectualproperty(whatFrankandCook(1996)callthe\u201cWinner-Take-AllSociety\u201d),there\nisalargerewardforbeingslightlybetterthanthecompetition;working10%morecouldmean\na 100% increase in income. So there is increasing pressure on everyone to work harder. AI\nincreases the pace of technological innovation and thus contributes to this overall trend, but\nAIalso holds the promise ofallowing ustotake some timeoffand let ourautomated agents\nhandlethingsforawhile. TimFerriss(2007)recommendsusingautomationandoutsourcing\ntoachieveafour-hour workweek.\nPeoplemightlosetheirsenseofbeingunique. InComputer PowerandHumanRea-\nson, Weizenbaum (1976), theauthorofthe ELIZA program, points outsome ofthe potential\nthreatsthatAIposestosociety. OneofWeizenbaum\u2019sprincipalargumentsisthatAIresearch\nmakespossibletheideathathumansareautomata\u2014anideathatresultsinalossofautonomy\norevenofhumanity. WenotethattheideahasbeenaroundmuchlongerthanAI,goingback\nat least to L\u2019Homme Machine (La Mettrie, 1748). Humanity has survived other setbacks to\nour sense of uniqueness: De Revolutionibus Orbium Coelestium (Copernicus, 1543) moved\nthe Earth away from the center of the solar system, and Descent of Man (Darwin, 1871) put\nHomosapiens atthesamelevelasotherspecies. AI,ifwidelysuccessful, maybeatleast as\nthreatening tothemoralassumptions of21st-century societyasDarwin\u2019stheoryofevolution\nwastothoseofthe19thcentury.\nAI systems might be used toward undesirable ends. Advanced technologies have\noftenbeenusedbythepowerfultosuppress theirrivals. Asthenumbertheorist G.H.Hardy\nwrote(Hardy,1940),\u201cAscienceissaidtobeusefulifitsdevelopmenttendstoaccentuatethe\nexisting inequalities in the distribution of wealth, or more directly promotes the destruction\nof human life.\u201d Thisholds forall sciences, AIbeing no exception. Autonomous AI systems\nare now commonplace onthe battlefield; the U.S.military deployed over5,000 autonomous\naircraft and 12,000 autonomous ground vehicles in Iraq (Singer, 2009). One moral theory\nholds thatmilitary robots arelikemedieval armortaken toitslogical extreme: noonewould\nhave moral objections to a soldier wanting to wear a helmet when being attacked by large,\nangry, axe-wielding enemies, and a teleoperated robot is like a very safe form of armor. On\nthe other hand, robotic weapons pose additional risks. To the extent that human decision\nmaking is taken out of the firing loop, robots may end up making decisions that lead to the\nkilling of innocent civilians. At a larger scale, the possession of powerful robots (like the\npossession ofsturdyhelmets)maygiveanationoverconfidence, causingittogotowarmore\nrecklessly than necessary. In most wars, at least one party is overconfident in its military\nabilities\u2014otherwise theconflictwouldhavebeenresolvedpeacefully.\nWeizenbaum (1976) also pointed out that speech recognition technology could lead to\nwidespread wiretapping, andhencetoalossofcivilliberties. Hedidn\u2019tforesee aworldwith\nterroristthreatsthatwouldchangethebalanceofhowmuchsurveillancepeoplearewillingto 1036 Chapter 26. Philosophical Foundations\naccept, buthedidcorrectly recognize thatAIhasthepotential tomass-produce surveillance.\nHisprediction has inpart come true: the U.K.now has anextensive network ofsurveillance\ncameras, andothercountries routinely monitorWebtrafficandtelephone calls. Someaccept\nthatcomputerization leads toaloss ofprivacy\u2014Sun Microsystems CEOScottMcNealyhas\nsaid \u201cYou have zero privacy anyway. Get over it.\u201d David Brin (1998) argues that loss of\nprivacy is inevitable, and the way to combat the asymmetry of power of the state over the\nindividual is to make the surveillance accessible to all citizens. Etzioni (2004) argues for a\nbalancing ofprivacyandsecurity; individual rightsandcommunity.\nTheuseofAIsystemsmightresultinalossofaccountability. Inthelitigious atmo-\nsphere that prevails in the United States, legal liability becomes an important issue. When a\nphysicianreliesonthejudgmentofamedicalexpertsystemforadiagnosis, whoisatfaultif\nthediagnosisiswrong? Fortunately,dueinparttothegrowinginfluenceofdecision-theoretic\nmethods in medicine, it is now accepted that negligence cannot be shown if the physician\nperformsmedicalproceduresthathavehigh expectedutility,eveniftheactualresultiscatas-\ntrophic for the patient. The question should therefore be \u201cWho is at fault if the diagnosis is\nunreasonable?\u201d So far, courts have held that medical expert systems play the same role as\nmedicaltextbooksandreferencebooks;physiciansareresponsibleforunderstanding therea-\nsoning behind any decision and for using their own judgment in deciding whether to accept\nthe system\u2019s recommendations. In designing medical expert systems as agents, therefore,\nthe actions should be thought of not as directly affecting the patient but as influencing the\nphysician\u2019sbehavior. Ifexpertsystemsbecomereliablymoreaccuratethanhumandiagnosti-\ncians,doctorsmightbecomelegallyliableiftheydon\u2019tusetherecommendationsofanexpert\nsystem. AtulGawande(2002)exploresthispremise.\nSimilarissuesarebeginningtoariseregardingtheuseofintelligentagentsontheInter-\nnet. Someprogress hasbeen madeinincorporating constraints intointelligent agents sothat\ntheycannot,forexample,damagethefilesofotherusers(WeldandEtzioni,1994). Theprob-\nlem is magnified when money changes hands. If monetary transactions are made \u201con one\u2019s\nbehalf\u201d byan intelligent agent, isone liable forthedebts incurred? Would itbepossible for\nan intelligent agent to have assets itself and to perform electronic trades on its own behalf?\nSo far, these questions do not seem to be well understood. To our knowledge, no program\nhas been granted legal status as an individual for the purposes of financial transactions; at\npresent, it seems unreasonable to do so. Programs are also not considered to be \u201cdrivers\u201d\nforthepurposes ofenforcing trafficregulations onrealhighways. InCalifornia law,atleast,\nthere do not seem to be any legal sanctions to prevent an automated vehicle from exceeding\nthespeedlimits,althoughthedesignerofthevehicle\u2019s controlmechanismwouldbeliablein\nthecase ofanaccident. Aswithhuman reproductive technology, the lawhasyet tocatch up\nwiththenewdevelopments.\nThe success of AI might mean the end of the human race. Almost any technology\nhasthepotentialtocauseharminthewronghands,butwithAIandrobotics,wehavethenew\nproblemthatthewronghandsmightbelongtothetechnologyitself. Countlesssciencefiction\nstories have warned about robots or robot\u2013human cyborgs running amok. Early examples Section26.3. TheEthicsandRisksofDeveloping ArtificialIntelligence 1037\ninclude MaryShelley\u2019s Frankenstein, ortheModernPrometheus(1818)5 andKarelCapek\u2019s\nplay R.U.R.(1921), in which robots conquer the world. In movies, we have The Terminator\n(1984), which combines the cliches of robots-conquer-the-world with time travel, and The\nMatrix(1999), whichcombinesrobots-conquer-the-world withbrain-in-a-vat.\nItseemsthat robots are theprotagonists ofso manyconquer-the-world stories because\ntheyrepresent theunknown, justlikethewitches andghosts oftales fromearliereras, orthe\nMartians from The War of the Worlds (Wells, 1898). The question is whether an AI system\nposesabiggerriskthantraditional software. Wewilllookatthreesourcesofrisk.\nFirst, the AI system\u2019s state estimation may be incorrect, causing it to do the wrong\nthing. Forexample,anautonomous carmightincorrectly estimatethepositionofacarinthe\nadjacent lane, leading toan accident that might kill the occupants. Moreseriously, amissile\ndefense system might erroneously detect an attack and launch a counterattack, leading to\nthe death of billions. These risks are not really risks of AI systems\u2014in both cases the same\nmistakecouldjustaseasilybemadebyahumanasbyacomputer. Thecorrectwaytomitigate\nthese risks is to design a system with checks and balances so that a single state-estimation\nerrordoesnotpropagatethrough thesystemunchecked.\nSecond, specifying the right utility function for an AI system to maximize is not so\neasy. Forexample,wemightproposeautilityfunctiondesignedtominimizehumansuffering,\nexpressed as anadditive reward function overtimeasinChapter17. Giventhewayhumans\nare, however, we\u2019ll always find a way to suffer even in paradise; so the optimal decision for\nthe AI system is toterminate thehuman race as soon as possible\u2014no humans, no suffering.\nWith AI systems, then, we need to be very careful what we ask for, whereas humans would\nhave no trouble realizing that the proposed utility function cannot be taken literally. On the\notherhand,computersneednotbetaintedbytheirrationalbehaviorsdescribedinChapter16.\nHumans sometimes use their intelligence in aggressive ways because humans have some\ninnately aggressive tendencies, due tonatural selection. Themachines webuild need not be\ninnately aggressive, unless we decide to build them that way (or unless they emerge as the\nend product ofamechanism design that encourages aggressive behavior). Fortunately, there\naretechniques, suchasapprenticeship learning, thatallowsustospecifyautilityfunctionby\nexample. One can hope that a robot that is smart enough to figure out how to terminate the\nhumanraceisalsosmartenoughtofigureoutthatthatwasnottheintended utilityfunction.\nThird, the AI system\u2019s learning function may cause it to evolve into a system with\nunintended behavior. This scenario is the most serious, and is unique to AI systems, so we\nwillcoveritinmoredepth. I.J.Goodwrote(1965),\nULTRAINTELLIGENT Let an ultraintelligent machine be defined as a machine that can far surpass all the\nMACHINE\nintellectualactivitiesofanymanhoweverclever. Sincethedesignofmachinesisoneof\ntheseintellectualactivities,anultraintelligentmachinecoulddesignevenbettermachines;\nthere would then unquestionablybe an \u201cintelligence explosion,\u201d and the intelligence of\nmanwouldbeleftfarbehind. Thusthefirstultraintelligentmachineisthelastinvention\nthat man need evermake, providedthat the machine is docile enoughto tell us how to\nkeepitundercontrol.\n5 Asayoungman,CharlesBabbagewasinfluencedbyreadingFrankenstein. 1038 Chapter 26. Philosophical Foundations\nTECHNOLOGICAL The \u201cintelligence explosion\u201d has also been called the technological singularity by mathe-\nSINGULARITY\nmatics professor and science fiction author Vernor Vinge, who writes (1993), \u201cWithin thirty\nyears,wewillhavethetechnological meanstocreatesuperhuman intelligence. Shortlyafter,\nthehumanerawillbeended.\u201d GoodandVinge(andmanyothers)correctlynotethatthecurve\nof technological progress (on many measures) is growing exponentially at present (consider\nMoore\u2019sLaw). However,itisaleaptoextrapolatethatthecurvewillcontinuetoasingularity\nofnear-infinitegrowth. Sofar,everyothertechnologyhasfollowedanS-shapedcurve,where\nthe exponential growth eventually tapers off. Sometimes new technologies step in when the\nold ones plateau; sometimes wehit hard limits. With less than acentury ofhigh-technology\nhistorytogoon,itisdifficulttoextrapolate hundreds ofyearsahead.\nNote that the concept of ultraintelligent machines assumes that intelligence is an es-\npecially important attribute, and if you have enough of it, all problems can be solved. But\nwe know there are limits on computability and computational complexity. If the problem\nofdefining ultraintelligent machines (orevenapproximations tothem) happens to fallin the\nclass of, say, NEXPTIME-complete problems, and if there are no heuristic shortcuts, then\neven exponential progress in technology won\u2019t help\u2014the speed of light puts a strict upper\nbound on how much computing can bedone; problems beyond that limitwill not besolved.\nWestilldon\u2019tknowwherethoseupperbounds are.\nVinge is concerned about the coming singularity, but some computer scientists and\nfuturists relish it. HansMoravec (2000) encourages ustogiveeveryadvantage toour\u201cmind\nchildren,\u201d the robots we create, which may surpass us in intelligence. There is even a new\nword\u2014transhumanism\u2014fortheactivesocialmovementthatlooksforwardtothisfuturein\nTRANSHUMANISM\nwhich humans are merged with\u2014or replaced by\u2014robotic and biotech inventions. Suffice it\ntosaythatsuchissuespresentachallengeformostmoraltheorists,whotakethepreservation\nofhuman lifeand thehuman species tobe agood thing. RayKurzweiliscurrently themost\nvisibleadvocate forthesingularity view,writingin TheSingularity isNear(2005):\nTheSingularitywillallowustotranscendtheselimitationsofourbiologicalbodiesand\nbrain. We will gainpoweroverourfates. Ourmortality will be in ourown hands. We\nwillbeabletoliveaslongaswewant(asubtlydifferentstatementfromsayingwewill\nliveforever).Wewillfullyunderstandhumanthinkingandwillvastlyextendandexpand\nitsreach. Bytheendofthiscentury,thenonbiologicalportionofourintelligencewillbe\ntrillionsoftrillionsoftimesmorepowerfulthanunaidedhumanintelligence.\nKurzweil also notes the potential dangers, writing \u201cButthe Singularity will also amplify the\nabilitytoactonourdestructive inclinations, soitsfullstoryhasnotyetbeenwritten.\u201d\nIf ultraintelligent machines are a possibility, we humans would do well to make sure\nthat wedesign their predecessors in such a way that they design themselves to treat us well.\nScience fiction writer Isaac Asimov (1942) was the first to address this issue, with his three\nlawsofrobotics:\n1. A robot may not injure a human being or, through inaction, allow a human being to\ncometoharm.\n2. Arobotmustobeyordersgiventoitbyhumanbeings,except wheresuchorderswould\nconflictwiththeFirstLaw. Section26.3. TheEthicsandRisksofDeveloping ArtificialIntelligence 1039\n3. Arobotmustprotectitsownexistenceaslongassuchprotection doesnotconflictwith\ntheFirstorSecondLaw.\nTheselawsseemreasonable, atleast toushumans.6 Butthetrickishowtoimplement these\nlaws. In the Asimov story Roundabout a robot is sent to fetch some selenium. Later the\nrobot isfound wandering inacircle around theselenium source. Everytimeitheads toward\nthe source, it senses a danger, and the third law causes it to veer away. But every time it\nveers away, the danger recedes, and the power of the second law takes over, causing it to\nveer back towards the selenium. The set of points that define the balancing point between\nthetwolawsdefinesacircle. Thissuggests thatthelawsarenotlogical absolutes, butrather\nare weighed against each other, with a higher weighting for the earlier laws. Asimov was\nprobably thinking of an architecture based on control theory\u2014perhaps a linear combination\noffactors\u2014while todaythemostlikelyarchitecture wouldbeaprobabilistic reasoning agent\nthat reasons over probability distributions of outcomes, and maximizes utility as defined by\nthe three laws. But presumably we don\u2019t want our robots to prevent a human from crossing\nthe street because of the nonzero chance of harm. That means that the negative utility for\nharm to a human must be much greater than for disobeying, but that each of the utilities is\nfinite,notinfinite.\nYudkowsky(2008)goesintomoredetailabouthowtodesignaFriendlyAI.Heasserts\nFRIENDLYAI\nthat friendliness (adesire notto harm humans) should bedesigned infrom the start, but that\nthedesigners shouldrecognize boththattheirowndesigns maybeflawed,andthattherobot\nwilllearnandevolveovertime. Thusthechallenge isoneofmechanism design\u2014to definea\nmechanism for evolving AI systems under a system of checks and balances, and to give the\nsystemsutilityfunctions thatwillremainfriendly inthefaceofsuchchanges.\nWecan\u2019tjustgiveaprogramastaticutilityfunction,becausecircumstances,andourde-\nsired responses tocircumstances, change overtime. Forexample, iftechnology hadallowed\nus to design a super-powerful AI agent in 1800 and endow it with the prevailing morals of\nthetime, itwouldbefighting today toreestablish slavery andabolish women\u2019s right tovote.\nOntheotherhand,ifwebuild anAIagenttodayandtellittoevolveitsutility function, how\ncanweassure that itwon\u2019treason that\u201cHumans think itismoraltokill annoying insects, in\npart because insect brains are so primitive. But human brains are primitive compared to my\npowers,soitmustbemoralformetokillhumans.\u201d\nOmohundro (2008) hypothesizes that even an innocuous chess program could pose a\nrisk to society. Similarly, Marvin Minsky once suggested that an AI program designed to\nsolve the Riemann Hypothesis might end up taking over all the resources of Earth to build\nmore powerful supercomputers to help achieve its goal. The moral is that even if you only\nwant your program to play chess or prove theorems, if you give it the capability to learn\nand alter itself, you need safeguards. Omohundro concludes that \u201cSocial structures which\ncause individuals to bearthecost oftheirnegative externalities would go along way toward\nensuringastableandpositivefuture,\u201dThisseemstobeanexcellentideaforsocietyingeneral,\nregardlessofthepossibility ofultraintelligent machines.\n6 Arobotmightnoticetheinequitythatahumanisallowedtokillanotherinself-defense,butarobotisrequired\ntosacrificeitsownlifetosaveahuman. 1040 Chapter 26. Philosophical Foundations\nWe should note that the idea of safeguards against change in utility function is not a\nnewone. IntheOdyssey,Homer(ca. 700B.C.) describedUlysses\u2019encounterwiththesirens,\nwhose song was so alluring it compelled sailors to cast themselves into the sea. Knowing it\nwould have that effect on him, Ulysses ordered his crew to bind him to the mast so that he\ncould not perform the self-destructive act. It is interesting to think how similar safeguards\ncouldbebuiltintoAIsystems.\nFinally, let us consider the robot\u2019s point of view. If robots become conscious, then to\ntreat them as mere \u201cmachines\u201d (e.g., to take them apart) might be immoral. Science fiction\nwritershaveaddressed theissueofrobotrights. Themovie A.I.(Spielberg, 2001) wasbased\non a story by Brian Aldiss about an intelligent robot who was programmed to believe that\nhe was human and fails to understand his eventual abandonment by his owner\u2013mother. The\nstory(andthemovie)arguefortheneedforacivilrightsmovementforrobots.\n26.4 SUMMARY\nThischapterhasaddressed thefollowingissues:\n\u2022 Philosophers use the term weak AI for the hypothesis that machines could possibly\nbehave intelligently, andstrong AIforthehypothesis thatsuch machines would count\nashavingactualminds(asopposed tosimulatedminds).\n\u2022 Alan Turing rejected the question \u201cCan machines think?\u201d and replaced it with a be-\nhavioral test. He anticipated many objections to the possibility of thinking machines.\nFew AI researchers pay attention to the Turing Test, preferring to concentrate on their\nsystems\u2019performance onpractical tasks,ratherthantheabilitytoimitatehumans.\n\u2022 Thereisgeneralagreementinmoderntimesthatmentalstatesarebrainstates.\n\u2022 Argumentsforandagainststrong AIareinconclusive. Fewmainstream AIresearchers\nbelievethatanything significanthingesontheoutcomeofthedebate.\n\u2022 Consciousness remainsamystery.\n\u2022 We identified six potential threats to society posed by AI and related technology. We\nconcluded that someofthethreats areeitherunlikely ordifferlittle from threats posed\nby \u201cunintelligent\u201d technologies. One threat in particular is worthy of further consider-\nation: that ultraintelligent machines might lead to a future that is very different from\ntoday\u2014we may not like it, and at that point we may not have a choice. Such consid-\nerations lead inevitably to the conclusion that we must weigh carefully, and soon, the\npossibleconsequences ofAIresearch.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nSources for the various responses to Turing\u2019s 1950 paper and for the main critics of weak\nAIweregiveninthechapter. Although itbecamefashionable inthepost-neural-network era Bibliographical andHistorical Notes 1041\nto deride symbolic approaches, not all philosophers are critical of GOFAI. Someare, in fact,\nardent advocates and even practitioners. Zenon Pylyshyn (1984) has argued that cognition\ncan best be understood through a computational model, not only in principle but also as a\nway of conducting research at present, and has specifically rebutted Dreyfus\u2019s criticisms of\nthe computational model of human cognition (Pylyshyn, 1974). Gilbert Harman (1983), in\nanalyzingbeliefrevision,makesconnections withAIresearchontruthmaintenancesystems.\nMichaelBratmanhasappliedhis\u201cbelief-desire-intention\u201dmodelofhumanpsychology(Brat-\nman, 1987) to AI research on planning (Bratman, 1992). At the extreme end of strong AI,\nAaron Sloman (1978, p. xiii) has even described as \u201cracialist\u201d the claim by Joseph Weizen-\nbaum(1976)thatintelligent machinescanneverberegarded aspersons.\nProponents of the importance of embodiment in cognition include the philosophers\nMerleau-Ponty, whose Phenomenology of Perception (1945) stressed the importance of the\nbodyandthesubjectiveinterpretationofrealityaffordedbyoursenses,andHeidegger,whose\nBeing and Time(1927) asked what it means to actually be an agent, and criticized all of the\nhistoryofphilosophyfortakingthisnotionforgranted. Inthecomputerage,AlvaNoe(2009)\nand Andy Clark (1998, 2008) propose that our brains form a rather minimal representation\nof the world, use the world itself in ajust-in-time basis to maintain the illusion of adetailed\ninternal model, use props in the world (such as paper and pencil as well as computers) to\nincrease the capabilities of the mind. Pfeifer et al. (2006) and Lakoff and Johnson (1999)\npresentarguments forhowthebodyhelpsshapecognition.\nThe nature of the mind has been a standard topic of philosophical theorizing from an-\ncient times to the present. In the Phaedo, Plato specifically considered and rejected the idea\nthat the mind could be an \u201cattunement\u201d orpattern of organization of the parts of the body, a\nviewpoint that approximates the functionalist viewpoint in modern philosophy of mind. He\ndecided instead that the mind had to be an immortal, immaterial soul, separable from the\nbody anddifferent insubstance\u2014the viewpoint ofdualism. Aristotle distinguished avariety\nofsouls(Greek\u03c8\u03c5\u03c7\u03b7)inlivingthings,someofwhich,atleast,hedescribedinafunctionalist\nmanner. (SeeNussbaum(1978)formoreonAristotle\u2019s functionalism.)\nDescartesisnotoriousforhisdualisticviewofthehumanmind,butironicallyhishistor-\nicalinfluencewastowardmechanismandphysicalism. Heexplicitlyconceivedofanimalsas\nautomata, and he anticipated the Turing Test, writing \u201cit is not conceivable [that a machine]\nshould produce different arrangements of words so as to give an appropriately meaningful\nanswer to whatever is said in its presence, as even the dullest of men can do\u201d (Descartes,\n1637). Descartes\u2019s spirited defense of the animals-as-automata viewpoint actually had the\neffectofmakingiteasiertoconceiveofhumansasautomataaswell,eventhoughhehimself\ndid not take this step. The book L\u2019Homme Machine (La Mettrie, 1748) did explicitly argue\nthathumansareautomata.\nModernanalyticphilosophyhastypicallyacceptedphysicalism,butthevarietyofviews\non the content of mental states is bewildering. Theidentification of mental states with brain\nstates is usually attributed to Place (1956) and Smart (1959). The debate between narrow-\ncontentandwide-contentviewsofmentalstateswastriggeredbyHilaryPutnam(1975),who\nintroduced so-called twin earths (rather than brain-in-a-vat, as we did in the chapter) as a\nTWINEARTHS\ndevicetogenerate identical brainstateswithdifferent (wide)content. 1042 Chapter 26. Philosophical Foundations\nFunctionalism isthephilosophy ofmindmostnaturally suggested byAI.Theideathat\nmental states correspond to classes of brain states defined functionally is due to Putnam\n(1960, 1967) and Lewis (1966, 1980). Perhaps the most forceful proponent of functional-\nism is Daniel Dennett, whose ambitiously titled work Consciousness Explained (Dennett,\n1991)hasattractedmanyattemptedrebuttals. Metzinger(2009)arguesthereisnosuchthing\nasanobjective self, thatconsciousness isthesubjective appearance ofaworld. Theinverted\nspectrum argument concerning qualia was introduced by John Locke (1690). Frank Jack-\nson (1982) designed aninfluential thought experiment involving Mary, acolor scientist who\nhas been brought up in an entirely black-and-white world. There\u2019s Something About Mary\n(Ludlowetal.,2004)collectsseveralpapersonthistopic.\nFunctionalism hascomeunderattack from authors whoclaim thattheydonotaccount\nfor the qualia or \u201cwhat it\u2019s like\u201d aspect of mental states (Nagel, 1974). Searle has focused\ninstead on the alleged inability of functionalism to account for intentionality (Searle, 1980,\n1984, 1992). Churchland and Churchland (1982) rebut both these types of criticism. The\nChinese Room has been debated endlessly (Searle, 1980, 1990; Preston and Bishop, 2002).\nWe\u2019ll just mention here a related work: Terry Bisson\u2019s (1990) science fiction story They\u2019re\nMade out of Meat, in which alien robotic explorers who visit earth are incredulous to find\nthinking humanbeings whosemindsaremadeofmeat. Presumably, therobotic alien equiv-\nalentofSearlebelieves thathecanthink duetothespecial causal powersofrobotic circuits;\ncausalpowersthatmeremeat-brains donotpossess.\nEthical issues in AI predate the existence of the field itself. I. J. Good\u2019s (1965) ul-\ntraintelligent machine idea was foreseen a hundred years earlier by Samuel Butler (1863).\nWritten four years after the publication of Darwin\u2019s On the Origins of Species and at a time\nwhenthemostsophisticatedmachinesweresteamengines,Butler\u2019sarticleonDarwinAmong\ntheMachinesenvisioned\u201ctheultimatedevelopmentofmechanicalconsciousness\u201d bynatural\nselection. Thethemewasreiterated byGeorgeDyson(1998)inabookofthesametitle.\nThephilosophical literature onminds,brains, andrelated topicsislargeanddifficultto\nread without training in the terminology and methods of argument employed. The Encyclo-\npedia of Philosophy (Edwards, 1967) is an impressively authoritative and very useful aid in\nthis process. The Cambridge Dictionary of Philosophy (Audi, 1999) is a shorter and more\naccessible work, and the online Stanford Encyclopedia of Philosophy offers many excellent\narticles and up-to-date references. The MITEncyclopedia of Cognitive Science (Wilson and\nKeil, 1999) covers the philosophy of mind as well as the biology and psychology of mind.\nThere are several general introductions to the philosophical \u201cAI question\u201d (Boden, 1990;\nHaugeland, 1985; Copeland, 1993; McCorduck, 2004; Minsky, 2007). The Behavioral and\nBrain Sciences, abbreviated BBS, is a major journal devoted to philosophical and scientific\ndebates about AI and neuroscience. Topics of ethics and responsibility in AI are covered in\nthejournals AIandSociety andJournal ofArtificialIntelligence andLaw. Exercises 1043\nEXERCISES\n26.1 Gothrough Turing\u2019s listofalleged \u201cdisabilities\u201d ofmachines, identifying whichhave\nbeen achieved, which are achievable in principle by aprogram, andwhich are still problem-\naticbecausetheyrequire conscious mentalstates.\n26.2 Findandanalyze anaccount inthepopular mediaofoneormore ofthearguments to\ntheeffectthatAIisimpossible.\n26.3 In the brain replacement argument, it is important to be able to restore the subject\u2019s\nbrain to normal, such that its external behavior is as it would have been if the operation had\nnot taken place. Can the skeptic reasonably object that this would require updating those\nneurophysiological properties of the neurons relating to conscious experience, as distinct\nfromthoseinvolved inthefunctional behavioroftheneurons?\n26.4 Suppose that a Prolog program containing many clauses about the rules of British\ncitizenship is compiled and run on an ordinary computer. Analyze the \u201cbrain states\u201d of the\ncomputerunderwideandnarrowcontent.\n26.5 AlanPerlis(1982)wrote,\u201cAyearspentinartificialintelligenceisenoughtomakeone\nbelieve in God\u201d. Healso wrote, in a letter to Philip Davis, that one of the central dreams of\ncomputer science is that \u201cthrough theperformance of computers and their programs wewill\nremove all doubt that there is only a chemical distinction between the living and nonliving\nworld.\u201d To what extent does the progress made so far in artificial intelligence shed light on\ntheseissues? Supposethatatsomefuturedate,theAIendeavorhasbeencompletelysuccess-\nful;thatis,wehavebuildintelligent agentscapableofcarryingoutanyhumancognitivetask\nathumanlevelsofability. Towhatextentwouldthatshedlightontheseissues?\n26.6 Comparethesocialimpactofartificialintelligenceinthelastfiftyyearswiththesocial\nimpact of the introduction of electric appliances and the internal combustion engine in the\nfiftyyearsbetween1890and1940.\n26.7 I. J. Good claims that intelligence is the most important quality, and that building\nultraintelligent machines will change everything. A sentient cheetah counters that \u201cActually\nspeedismoreimportant;ifwecouldbuildultrafastmachines,thatwouldchangeeverything,\u201d\nand a sentient elephant claims \u201cYou\u2019re both wrong; what we need is ultrastrong machines.\u201d\nWhatdoyouthinkofthesearguments?\n26.8 AnalyzethepotentialthreatsfromAItechnologytosociety. Whatthreatsaremostse-\nrious,andhowmighttheybecombated? Howdotheycomparetothepotentialbenefits?\n26.9 HowdothepotentialthreatsfromAItechnology comparewiththosefromothercom-\nputersciencetechnologies, andtobio-,nano-, andnuclear technologies?\n26.10 Some critics object that AI is impossible, while others object that it is too possible\nand that ultraintelligent machines pose a threat. Which of these objections do you think is\nmorelikely? Woulditbeacontradiction forsomeonetoholdbothpositions? 27\nAI: THE PRESENT AND\nFUTURE\nInwhichwetakestockofwhereweareandwherewearegoing,thisbeingagood\nthingtodobeforecontinuing.\nIn Chapter 2, we suggested that it would be helpful to view the AI task as that of designing\nrational agents\u2014that is, agents whose actions maximize their expected utility given their\npercept histories. We showed that the design problem depends on the percepts and actions\navailable to the agent, the utility function that the agent\u2019s behavior should satisfy, and the\nnature of the environment. A variety of different agent designs are possible, ranging from\nreflex agents to fully deliberative, knowledge-based, decision-theoretic agents. Moreover,\nthecomponentsofthesedesignscanhaveanumberofdifferentinstantiations\u2014for example,\nlogicalorprobabilisticreasoning,andatomic,factored, orstructuredrepresentationsofstates.\nTheintervening chapterspresented theprinciples bywhich thesecomponents operate.\nForall the agent designs and components, there has been tremendous progress both in\nour scientific understanding and in our technological capabilities. In this chapter, we stand\nback from the details and ask, \u201cWill all this progress lead to a general-purpose intelligent\nagent that can perform well in a wide variety of environments?\u201d Section 27.1 looks at the\ncomponents ofanintelligent agent toassess what\u2019s knownandwhat\u2019smissing. Section 27.2\ndoesthesamefortheoverallagentarchitecture. Section27.3askswhetherdesigningrational\nagents is the right goal in the first place. (The answer is, \u201cNot really, but it\u2019s OK for now.\u201d)\nFinally,Section27.4examinestheconsequences ofsuccessinourendeavors.\n27.1 AGENT COMPONENTS\nChapter 2 presented several agent designs and their components. To focus our discussion\nhere, wewilllook attheutility-based agent, whichweshow againinFigure 27.1. Whenen-\ndowedwithalearningcomponent(Figure2.15),thisisthemostgeneralofouragentdesigns.\nLet\u2019sseewherethestateoftheartstandsforeachofthecomponents.\nInteraction with the environment through sensors and actuators: For much of the\nhistory of AI, this has been a glaring weak point. With a few honorable exceptions, AI sys-\ntemswerebuiltinsuchawaythathumanshadtosupplytheinputsandinterpret theoutputs,\n1044 Section27.1. AgentComponents 1045\nSensors\nState\nWhat the world\nHow the world evolves is like now\nWhat it will be like\nWhat my actions do if I do action A\nHow happy I will be\nUtility\nin such a state\nWhat action I\nshould do now\nAgent Actuators\nFigure27.1 Amodel-based,utility-basedagent,asfirstpresentedinFigure2.14.\nwhile robotic systems focused on low-level tasks in which high-level reasoning and plan-\nning were largely absent. This was due in part to the great expense and engineering effort\nrequired to get real robots to work at all. The situation has changed rapidly in recent years\nwith the availability of ready-made programmable robots. These, in turn, have benefited\nfromsmall,cheap,high-resolution CCDcamerasandcompact,reliablemotordrives. MEMS\n(micro-electromechanicalsystems)technologyhassuppliedminiaturizedaccelerometers,gy-\nroscopes, and actuators for an artificial flying insect (Floreano et al., 2009). It may also be\npossible tocombinemillionsofMEMSdevicestoproduce powerfulmacroscopic actuators.\nThus, we see that AI systems are at the cusp of moving from primarily software-only\nsystems to embedded robotic systems. The state of robotics today is roughly comparable to\nthe state of personal computers in about 1980: at that time researchers and hobbyists could\nexperimentwithPCs,butitwouldtakeanotherdecadebefore theybecamecommonplace.\nKeepingtrack of thestate of the world: This is one of the core capabilities required\nforan intelligent agent. It requires both perception and updating of internal representations.\nChapter 4 showed how to keep track of atomic state representations; Chapter 7 described\nhow to do it for factored (propositional) state representations; Chapter 12 extended this to\nfirst-orderlogic;andChapter15described filteringalgorithms forprobabilistic reasoning in\nuncertainenvironments. Currentfilteringandperception algorithmscanbecombinedtodoa\nreasonable job of reporting low-level predicates such as \u201cthe cup is on the table.\u201d Detecting\nhigher-level actions, such as \u201cDr. Russell is having a cup of tea with Dr. Norvig while dis-\ncussingplansfornextweek,\u201dismoredifficult. Currentlyit canbedone(seeFigure24.25on\npage961)onlywiththehelpofannotated examples.\nAnotherproblemisthat,althoughtheapproximatefiltering algorithmsfromChapter15\ncan handle quite large environments, they are still dealing with a factored representation\u2014\ntheyhaverandomvariables,butdonotrepresentobjectsand relationsexplicitly. Section14.6\nexplained how probability and first-order logic can be combined to solve this problem, and\nEnvironment 1046 Chapter 27. AI:ThePresentandFuture\nSection14.6.3showedhowwecanhandleuncertaintyabouttheidentityofobjects. Weexpect\nthattheapplicationoftheseideasfortrackingcomplexenvironmentswillyieldhugebenefits.\nHowever, weare still faced withadaunting task ofdefining general, reusable representation\nschemesforcomplexdomains. AsdiscussedinChapter12,wedon\u2019tyetknowhowtodothat\ningeneral; only forisolated, simple domains. Itispossible that anewfocus onprobabilistic\nratherthanlogicalrepresentationcoupledwithaggressivemachinelearning(ratherthanhand-\nencoding ofknowledge) willallowforprogress.\nProjecting,evaluating,andselectingfuturecoursesofaction: Thebasicknowledge-\nrepresentation requirements hereare thesameasforkeeping track ofthe world; theprimary\ndifficulty is coping with courses of action\u2014such as having a conversation or a cup of tea\u2014\nthat consist eventually of thousands ormillions of primitive steps fora real agent. It is only\nby imposing hierarchical structure on behavior that we humans cope at all. We saw in\nSection 11.2 how to use hierarchical representations to handle problems of this scale; fur-\nthermore, work in hierarchical reinforcement learning has succeeded in combining some\nofthese ideas withthe techniques fordecision making under uncertainty described inChap-\nter 17. As yet, algorithms for the partially observable case (POMDPs) are using the same\natomicstaterepresentation weusedforthesearchalgorithmsofChapter3. Thereisclearlya\ngreatdealofworktodohere,butthetechnical foundations arelargelyinplace. Section27.2\ndiscusses thequestion ofhowthesearchforeffectivelong-range plansmightbecontrolled.\nUtility asan expression ofpreferences: Inprinciple, basing rational decisions on the\nmaximization of expected utility is completely general and avoids many of the problems of\npurely goal-based approaches, such as conflicting goals and uncertain attainment. As yet,\nhowever,therehasbeenverylittleworkonconstructing realistic utilityfunctions\u2014imagine,\nforexample,thecomplexwebofinteractingpreferencesthatmustbeunderstoodbyanagent\noperating asan officeassistant forahuman being. Ithasproven verydifficult todecompose\npreferences over complex states in the same way that Bayes nets decompose beliefs over\ncomplex states. One reason may be that preferences over states are really compiled from\npreferences overstate histories, which are described by reward functions (see Chapter 17).\nEveniftherewardfunctionissimple,thecorrespondingutilityfunctionmaybeverycomplex.\nThissuggests that wetake seriously the task ofknowledge engineering forreward functions\nasawayofconveying toouragentswhatitisthatwewantthemtodo.\nLearning: Chapters 18 to 21 described how learning in an agent can be formulated as\ninductive learning (supervised, unsupervised, or reinforcement-based) of the functions that\nconstitute the various components of the agent. Very powerful logical and statistical tech-\nniques have been developed that can cope with quite large problems, reaching or exceeding\nhuman capabilities in many tasks\u2014as long as we are dealing with a predefined vocabulary\nof features and concepts. On the other hand, machine learning has made very little progress\non the important problem of constructing new representations at levels of abstraction higher\nthan theinput vocabulary. Incomputer vision, forexample, learning complex concepts such\nasClassroom andCafeteria would bemadeunnecessarily difficult iftheagent wereforced\nto work from pixels as the input representation; instead, the agent needs to be able to form\nintermediate concepts first, such as Desk and Tray, without explicit human supervision.\nSimilar considerations apply to learning behavior: HavingACupOfTea is a very important Section27.2. AgentArchitectures 1047\nhigh-levelstepinmanyplans,buthowdoesitgetintoanactionlibrarythatinitiallycontains\nmuch simpler actions such as RaiseArm and Swallow? Perhaps this will incorporate some\nDEEPBELIEF oftheideasofdeepbeliefnetworks\u2014Bayesiannetworksthathavemultiplelayersofhidden\nNETWORKS\nvariables, asintheworkofHinton etal.(2006), HawkinsandBlakeslee (2004), andBengio\nandLeCun(2007).\nThe vast majority of machine learning research today assumes a factored representa-\ntion, learning afunction h : Rn \u2192 R for regression and h : Rn \u2192 {0,1} forclassification.\nLearning researchers will need to adapt their very successful techniques for factored repre-\nsentations to structured representations, particularly hierarchical representations. The work\noninductive logicprogramming inChapter19isafirststepin thisdirection; thelogicalnext\nstepistocombinetheseideaswiththeprobabilistic languages ofSection14.6.\nUnless weunderstand such issues, weare faced withthe daunting task ofconstructing\nlarge commonsense knowledge bases by hand, an approach that has not fared well to date.\nThere is great promise in using the Web as a source of natural language text, images, and\nvideos to serve as a comprehensive knowledge base, but so far machine learning algorithms\narelimitedintheamountoforganized knowledge theycanextractfromthesesources.\n27.2 AGENT ARCHITECTURES\nIt is natural to ask, \u201cWhich of the agent architectures in Chapter 2 should an agent use?\u201d\nThe answer is, \u201cAll of them!\u201d We have seen that reflex responses are needed for situations\nin which time is of the essence, whereas knowledge-based deliberation allows the agent to\nHYBRID plan ahead. A complete agent must be able to do both, using a hybrid architecture. One\nARCHITECTURE\nimportant property of hybrid architectures is that the boundaries between different decision\ncomponents are not fixed. For example, compilation continually converts declarative in-\nformationatthedeliberativelevelintomoreefficientrepresentations, eventuallyreachingthe\nreflexlevel\u2014seeFigure27.2. (Thisisthepurposeofexplanation-basedlearning,asdiscussed\nin Chapter 19.) Agent architectures such as SOAR (Laird et al., 1987) and THEO (Mitchell,\n1990) have exactly this structure. Every time they solve a problem by explicit deliberation,\nthey save away a generalized version of the solution for use by the reflex component. A\nless studied problem is the reversal of this process: when the environment changes, learned\nreflexes may no longer be appropriate and the agent must return to the deliberative level to\nproduce newbehaviors.\nAgents also need ways to control their own deliberations. They must be able to cease\ndeliberating when action is demanded, and they must be able to use the time available for\ndeliberation to execute the most profitable computations. For example, a taxi-driving agent\nthat sees an accident ahead must decide in a split second either to brake or to take evasive\naction. It should also spend that split second thinking about the most important questions,\nsuch as whether the lanes to the left and right are clear and whether there is a large truck\nclose behind, rather than worrying about wear and tear on the tires or where to pick up the\nnext passenger. These issues are usually studied under the heading of real-time AI. As AI\nREAL-TIMEAI 1048 Chapter 27. AI:ThePresentandFuture\nKnowledge-based\ndeliberation\nPercepts Reflex system Actions\nFigure 27.2 Compilation serves to convertdeliberative decision making into more effi-\ncient,reflexivemechanisms.\nsystems move into more complex domains, all problems will become real-time, because the\nagentwillneverhavelongenough tosolvethedecision problem exactly.\nClearly, thereisapressing needfor general methodsofcontrolling deliberation, rather\nthan specific recipes forwhattothink about ineach situation. The firstuseful idea isto em-\nANYTIME ploy anytimealgorithms (Deanand Boddy, 1988; Horvitz, 1987). Ananytimealgorithm is\nALGORITHM\nan algorithm whose output quality improves gradually over time, so that it has a reasonable\ndecision readywheneveritisinterrupted. Suchalgorithms arecontrolled bya metalevel de-\ncisionprocedurethatassesseswhetherfurthercomputationisworthwhile. (SeeSection3.5.4\nfor a brief description of metalevel decision making.) Example of an anytime algorithms\nincludeiterativedeepening ingame-treesearchandMCMCin Bayesiannetworks.\nDECISION-\nThesecondtechniqueforcontrollingdeliberationisdecision-theoreticmetareasoning\nTHEORETIC\nMETAREASONING\n(Russell and Wefald, 1989, 1991; Horvitz, 1989; Horvitz and Breese, 1996). This method\napplies the theory of information value (Chapter 16) to the selection of individual computa-\ntions. Thevalue ofacomputation depends onboth its cost (interms ofdelaying action) and\nitsbenefits(intermsofimproveddecisionquality). Metareasoning techniquescanbeusedto\ndesign better search algorithms and to guarantee that the algorithms have the anytime prop-\nerty. Metareasoning isexpensive, ofcourse, andcompilation methods can beapplied sothat\ntheoverhead issmallcompared tothecostsofthecomputations being controlled. Metalevel\nreinforcement learning mayprovide anotherwaytoacquire effective policies forcontrolling\ndeliberation: inessence,computationsthatleadtobetterdecisionsarereinforced,whilethose\nthat turn out to have no effect are penalized. This approach avoids the myopia problems of\nthesimplevalue-of-information calculation.\nREFLECTIVE Metareasoning is one specific example of a reflective architecture\u2014that is, an archi-\nARCHITECTURE\ntecturethatenablesdeliberationaboutthecomputationalentitiesandactionsoccurringwithin\nthe architecture itself. A theoretical foundation for reflective architectures can be built by\ndefiningajointstatespacecomposedfromtheenvironment stateandthecomputational state\nof the agent itself. Decision-making and learning algorithms can be designed that operate\nover this joint state space and thereby serve to implement and improve the agent\u2019s compu-\ntational activities. Eventually, we expect task-specific algorithms such as alpha\u2013beta search\nandbackwardchainingtodisappearfromAIsystems,tobereplacedbygeneralmethodsthat\ndirecttheagent\u2019scomputations towardtheefficientgeneration ofhigh-quality decisions.\nCompilation Section27.3. AreWeGoingintheRightDirection? 1049\n27.3 ARE WE GOING IN THE RIGHT DIRECTION?\nTheprecedingsectionlistedmanyadvancesandmanyopportunitiesforfurtherprogress. But\nwhere is this all leading? Dreyfus (1992) gives the analogy of trying to get to the moon by\nclimbing a tree; one can report steady progress, all the way to the top of the tree. In this\nsection, weconsiderwhetherAI\u2019scurrentpathismorelikea treeclimborarockettrip.\nInChapter1,wesaidthatourgoalwastobuildagentsthatactrationally. However,we\nalsosaidthat\n...achievingperfectrationality\u2014alwaysdoingtherightthing\u2014isnotfeasibleincompli-\ncatedenvironments.Thecomputationaldemandsarejusttoohigh.Formostofthebook,\nhowever,wewilladopttheworkinghypothesisthatperfectrationalityisagoodstarting\npointforanalysis.\nNowitistimetoconsider againwhatexactly thegoalofAIis. Wewanttobuild agents, but\nwithwhatspecification inmind? Herearefourpossibilities:\nPERFECT Perfect rationality. A perfectly rational agent acts at every instant in such a way as to\nRATIONALITY\nmaximizeitsexpectedutility,giventheinformationithasacquiredfromtheenvironment. We\nhave seen thatthe calculations necessary toachieve perfect rationality inmostenvironments\naretootimeconsuming, soperfectrationality isnotarealisticgoal.\nCALCULATIVE Calculative rationality. This is the notion of rationality that we have used implicitly in de-\nRATIONALITY\nsigninglogicalanddecision-theoretic agents,andmostoftheoreticalAIresearchhasfocused\non this property. A calculatively rational agent eventually returns what would have been the\nrationalchoiceatthebeginningofitsdeliberation. Thisisaninterestingpropertyforasystem\nto exhibit, but in most environments, the right answer at the wrong time is of no value. In\npractice, AIsystemdesignersareforcedtocompromiseondecisionqualitytoobtainreason-\nable overall performance; unfortunately, the theoretical basis of calculative rationality does\nnotprovideawell-founded waytomakesuchcompromises.\nBOUNDED Boundedrationality. Herbert Simon(1957) rejected the notion ofperfect (oreven approx-\nRATIONALITY\nimately perfect) rationality and replaced it with bounded rationality, a descriptive theory of\ndecision makingbyrealagents. Hewrote,\nThecapacityofthe humanmindforformulatingandsolvingcomplexproblemsis very\nsmallcomparedwiththesizeoftheproblemswhosesolutionisrequiredforobjectively\nrationalbehaviorintherealworld\u2014orevenforareasonable approximationtosuchob-\njectiverationality.\nHe suggested that bounded rationality works primarily by satisficing\u2014that is, deliberating\nonly long enough to come up with an answer that is \u201cgood enough.\u201d Simon won the Nobel\nPrizein economics forthis workand has written about itin depth (Simon, 1982). Itappears\nto be a useful model of human behaviors in many cases. It is not a formal specification\nfor intelligent agents, however, because the definition of \u201cgood enough\u201d is not given by the\ntheory. Furthermore,satisficingseemstobejustoneofalargerangeofmethodsusedtocope\nwithbounded resources. 1050 Chapter 27. AI:ThePresentandFuture\nBOUNDED Bounded optimality (BO). A bounded optimal agent behaves as well as possible, given its\nOPTIMALITY\ncomputational resources. That is, the expected utility of the agent program for a bounded\noptimalagentisatleastashighastheexpectedutilityofanyotheragentprogramrunningon\nthesamemachine.\nOfthesefourpossibilities, boundedoptimalityseemstoofferthebesthopeforastrong\ntheoreticalfoundationforAI.Ithastheadvantageofbeingpossibletoachieve: thereisalways\natleast onebestprogram\u2014something thatperfect rationality lacks. Bounded optimal agents\nareactuallyusefulintherealworld,whereascalculativelyrationalagentsusuallyarenot,and\nsatisficingagentsmightormightnotbe,depending onhowambitioustheyare.\nThe traditional approach in AI has been to start with calculative rationality and then\nmakecompromises tomeetresource constraints. Ifthe problems imposed bytheconstraints\nare minor, one would expect the final design to be similar to a BO agent design. But as the\nresource constraints become more critical\u2014for example, as the environment becomes more\ncomplex\u2014onewouldexpectthetwodesignstodiverge. Inthetheoryofboundedoptimality,\ntheseconstraints canbehandledinaprincipled fashion.\nAs yet, little is known about bounded optimality. It is possible to construct bounded\noptimal programs for very simple machines and for somewhat restricted kinds of environ-\nments (Etzioni, 1989; Russell et al., 1993), but as yet we have no idea what BO programs\nare like for large, general-purpose computers in complex environments. If there is to be a\nconstructive theory of bounded optimality, we have to hope that the design of bounded op-\ntimal programs does not depend too strongly on the details of the computer being used. It\nwould make scientific research very difficult if adding afew kilobytes of memory to a giga-\nbyte machine made a significant difference to the design of the BO program. One way to\nmake sure this cannot happen is to be slightly more relaxed about the criteria for bounded\noptimality. Byanalogy with the notion of asymptotic complexity (Appendix A), wecan de-\nASYMPTOTIC\nfine asymptotic bounded optimality (ABO) as follows (Russell and Subramanian, 1995).\nBOUNDED\nOPTIMALITY\nSuppose a program P is bounded optimal for a machine M in a class of environments E,\n(cid:2)\nwhere the complexity of environments in E is unbounded. Then program P is ABOfor M\nin E if it can outperform P by running on a machine kM that is k times faster (or larger)\nthan M. Unless k were enormous, we would be happy with a program that was ABO for\na nontrivial environment on a nontrivial architecture. There would be little point in putting\nenormous effort into finding BO rather than ABO programs, because the size and speed of\navailable machinestendstoincrease byaconstant factorinafixedamountoftimeanyway.\nWecan hazard a guess that BO orABO programs for powerful computers in complex\nenvironments willnotnecessarilyhaveasimple,elegantstructure. Wehavealreadyseenthat\ngeneral-purposeintelligencerequiressomereflexcapabilityandsomedeliberativecapability;\navarietyofformsofknowledgeanddecision making;learning andcompilationmechanisms\nforallofthoseforms;methodsforcontrollingreasoning;andalargestoreofdomain-specific\nknowledge. Abounded optimal agent must adapt tothe environment inwhich it findsitself,\nso that eventually its internal organization will reflect optimizations that are specific to the\nparticular environment. This is only to be expected, and it is similar to the way in which\nracing cars restricted by engine capacity have evolved into extremely complex designs. We Section27.4. WhatIfAIDoesSucceed? 1051\nsuspect that a science of artificial intelligence based on bounded optimality will involve a\ngood deal of study of the processes that allow an agent program to converge to bounded\noptimality andperhapslessconcentration onthedetailsof themessyprograms thatresult.\nInsum,theconcept ofbounded optimality isproposed asaformaltaskforAIresearch\nthatisbothwelldefinedandfeasible. Bounded optimality specifies optimal programs rather\nthan optimal actions. Actions are, after all, generated by programs, and it is over programs\nthatdesigners havecontrol.\n27.4 WHAT IF AI DOES SUCCEED?\nInDavidLodge\u2019sSmallWorld(1984),anovelabouttheacademicworldofliterarycriticism,\nthe protagonist causes consternation by asking a panel of eminent but contradictory literary\ntheorists the following question: \u201cWhat if you were right?\u201d None of the theorists seems to\nhaveconsideredthisquestionbefore,perhapsbecausedebatingunfalsifiabletheoriesisanend\ninitself. Similarconfusion canbeevoked byaskingAIresearchers, \u201cWhatifyousucceed?\u201d\nAs Section 26.3 relates, there are ethical issues to consider. Intelligent computers are\nmorepowerfulthandumbones,butwillthatpowerbeusedforgoodorill? Thosewhostrive\ntodevelopAIhavearesponsibility toseethattheimpactoftheirworkisapositiveone. The\nscopeoftheimpactwilldependonthedegreeofsuccessofAI.EvenmodestsuccessesinAI\nhavealreadychangedthewaysinwhichcomputerscienceistaught(Stein,2002)andsoftware\ndevelopment ispracticed. AIhasmadepossible newapplications suchasspeechrecognition\nsystems,inventory controlsystems,surveillance systems,robots, andsearchengines.\nWe can expect that medium-level successes in AI would affect all kinds of people in\ntheirdaily lives. Sofar, computerized communication networks, such ascell phones and the\nInternet,havehadthiskindofpervasiveeffectonsociety,butAIhasnot. AIhasbeenatwork\nbehind the scenes\u2014for example, in automatically approving or denying credit card transac-\ntionsforeverypurchasemadeontheWeb\u2014buthasnotbeenvisibletotheaverageconsumer.\nWecan imagine that truly useful personal assistants forthe office orthe home would have a\nlarge positive impact on people\u2019s lives, although they might cause some economic disloca-\ntion in the short term. Automated assistants fordriving could prevent accidents, saving tens\nof thousands of lives per year. A technological capability at this level might also be applied\nto the development of autonomous weapons, which many view as undesirable. Some of the\nbiggestsocietal problemswefacetoday\u2014such astheharnessing ofgenomicinformation for\ntreatingdisease,theefficientmanagementofenergyresources,andtheverificationoftreaties\nconcerning nuclearweapons\u2014are beingaddressed withthehelpofAItechnologies.\nFinally,itseemslikelythatalarge-scalesuccessinAI\u2014thecreationofhuman-levelin-\ntelligence andbeyond\u2014would change thelivesofamajorityofhumankind. Theverynature\nofourworkandplaywouldbealtered,aswouldourviewofintelligence, consciousness, and\nthefuturedestinyofthehumanrace. AIsystemsatthislevelofcapability couldthreatenhu-\nmanautonomy,freedom,andevensurvival. Forthesereasons,wecannotdivorceAIresearch\nfromitsethicalconsequences (seeSection26.3). 1052 Chapter 27. AI:ThePresentandFuture\nWhich waywillthe future go? Science fiction authors seem tofavor dystopian futures\nover utopian ones, probably because they make for more interesting plots. But so far, AI\nseemstofitinwithotherrevolutionarytechnologies(printing,plumbing,airtravel,telephony)\nwhosenegativerepercussions areoutweighedbytheirpositiveaspects.\nIn conclusion, wesee that AI has made great progress in its short history, but the final\nsentence of Alan Turing\u2019s (1950) essay on Computing Machinery and Intelligence is still\nvalidtoday:\nWecanseeonlyashortdistance ahead,\nbutwecanseethatmuchremainstobedone. "}