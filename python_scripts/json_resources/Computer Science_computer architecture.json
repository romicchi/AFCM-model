{"text":"1\nIntroduction to Computer Architecture\nWelcometotheexcitingworldofcomputer architecture. Computerarchitectureisthestudy\nof computers. We shall study the basic design principles of computers in this book including\nthe basic technologies, algorithms, design methodologies and future trends.\nThe field of computer architecture is a very fast moving field, and every couple of years\nthere are a plethora of new inventions. Fifty years ago, the existence of computers was almost\nunknown to the common man. Computers were visible only in large financial institutions or in\ntop universities. However, today billions of people all over the world have access to some form\nof computing device. They use it actively, and have found a place for it in their daily activities.\nSuch kind of an epic transformation in the use, and ubiquity of computers has made the field\nof computer architecture extremely interesting.\nIn this chapter, we shall present an overview of computer architecture from an academic\nstandpoint, and explain the major principles behind today\u2019s computers. We shall observe that\nthere are two perspectives in computer architecture. We can look at computer architecture\nfrom the point of view of software applications. This point of view is sometimes referred to as\narchitecture in literature. It is very important for students of computer architecture to study\ncomputer architecture from the viewpoint of a software designer because they need to know\nabout the expectations of software writers from hardware. Secondly, it is also important for\nsoftware writers to know about computer architecture because they can tailor their software\nappropriatelytomakeitmoreefficient. Inthecaseofsystemsoftwaresuchasoperatingsystems\nand device drivers, it is absolutely essential to know the details of the architecture because the\ndesign of such kind of software is very strongly interlinked with low level hardware details.\nThe other perspective is the point of view of hardware designers. Given the software inter-\nface, they need to design hardware that is compatible with it and also implement algorithms\nthat make the system efficient in terms of performance and power. This perspective is also\nreferred to as organisation in literature.\n13 (cid:13)c Smruti R. Sarangi 14\nDefinition 1\nArchitecture The view of a computer presented to software designers.\nOrganisation The actual implementation of a computer in hardware.\nComputer architecture is a beautiful amalgam of software concepts and hardware concepts.\nWe design hardware to make software run efficiently. Concomitantly, we also design software\nkeeping in mind the interface and constraints presented by hardware. Both the perspectives\nrun hand in hand. Let us start out by looking at the generic definition of a computer.\n1.1 What is a Computer?\nLet us now answer the following questions.\nQuestion 1\nWhat is a computer?\nWhat it can do, and what it cannot do?\nHow do we make it do intelligent things?\nLet us start out with some basic definitions. The first question that we need to answer is \u2013\nWhat is a computer? Well to answer this question, we just need to look all around us. We are\nsurrounded by computers. Nowadays, computers are embedded in almost any kind of device\nsuch as mobile phones, tablets, mp3 players, televisions, dvd players, and obviously desktops\nand laptops. What is common between all of these devices? Well, each one of them has a\ncomputer that performs a specific task. For example, the computer in a mp3 player can play a\nsong, andthecomputerinadvdplayercanplayamovie. Itisabsolutelynotnecessarythatthe\nmp3 player and dvd player contain different types of computers. In fact, the odds are high that\nboth the devices contain the same type of computer. However, each computer is programmed\ndifferently, and processes different kinds of information. An mp3 player processes music files,\nand a dvd player processes video files. One can play a song, while the other can play a video.\nUsing these insights, let us formally define a computer in Definition 2.\nDefinition 2\nA computer is a general purpose device that can be programmed to process information, and\nyield meaningful results. 15 (cid:13)c Smruti R. Sarangi\nInformation\nProgram\nstore\nComputer\nResults\nFigure 1.1: A basic computer\nNote that there are three important parts to the definition as shown in Figure 1.1 \u2013 the\ncomputer, information store, and the program. The computer takes as an input a program,\nand in response performs a set of operations on the information store. At the end it yields\nmeaningful results. A typical program contains a set of instructions that tell the computer\nregarding the operations that need to be performed on the information store. The information\nstore typically contains numbers and pieces of text that the program can use. Let us consider\nan example.\nExample 1\nHere is a snippet of a simple C program.\n1: a = 4;\n2: b = 6;\n3: c = a + b;\n4: print c\nA computer will produce the output - 10. This C program contains four statements.\nHere, eachstatementcanconceptuallybetreatedasaninstruction. Eachstatementinstructs\nthe computer to do something. Statements 1 and 2 instruct the computer to assign the\nvariables a and b, the values 4 and 6 respectively. Statement 3 instructs the computer to\nadd a and b, and assign the result to variable c. Finally, statement 4 instructs the computer\nto print the value of c (output of the program).\nGiven the fact that we have defined a computer as a sophisticated device that follows the\ninstructions in a program to produce an output, let us see how it can be built. Modern day\ncomputers are made of silicon based transistors and copper wires to connect them. How-\never, it is absolutely not necessary that computers need to be built out of silicon and copper. (cid:13)c Smruti R. Sarangi 16\nFigure 1.2:\nResearchers are now looking at building computers with electrons (quantum computers), pho-\ntons(optical computers), and even DNA. If we think about it, our own brains are extremely\npowerfulcomputersthemselves. Theyarealwaysintheprocessofconvertingthoughts(program)\ninto action(output).\n1.2 Structure of a Typical Desktop Computer\nLet us now open the lid of a desktop computer, and see what is inside (shown in Figure 1.2).\nThere are three main parts of a typical desktop computer \u2013 CPU (Central Processing Unit),\nMain Memory, and Hard Disk. The CPU is also referred to as the processor or simply machine\nin common parlance. We will use the terms interchangeably in this book. The CPU is the\nmain part of the computer that takes a program as input, and executes it. It is the brain of\nthe computer. The main memory is used to store data that a program might need during its\nexecution (information store). For example, let us say that we want to recognise all the faces in\nan image. Then the image will be stored in main memory. There is some limited storage on the\nprocessor itself. However, we shall discuss this aspect later. When we turn off the power, the\nprocessor and main memory lose all their data. However, the hard disk represents permanent\nstorage. We do not expect to lose our data when we shut down the system. This is because all\nour programs, data, photos, videos, and documents are safely backed up in the hard disk.\nFigure1.3showsasimplisticblockdiagramofthethreecomponents. Alongwiththesemain\ncomponents, thereareahostofperipheralcomponentsthatareconnectedtothecomputer. For\nexample, thekeyboardandmouseareconnectedtoacomputer. Theytakeinputsfromtheuser\nand communicate them to programs running on the processor. Similarly, to show the output 17 (cid:13)c Smruti R. Sarangi\nMemory Hard disk\nComputer\nFigure 1.3: Block diagram of a simple computer\nof a program, the processor typically sends the output data to a monitor that can graphically\ndisplay the result. It is also possible to print the result using a printer. Lastly the computer\ncan be connected to other computers through the network. A revised block diagram with all\nthe peripherals is shown in Figure 1.4.\nMemory Hard disk\nComputer\nKeyboard Monitor\nMouse Printer\nFigure 1.4: Block diagram of a simple computer with peripherals\nInthisbook,wewillmainlystudytheprocessor. Theprocessorhasthecentralresponsibility\nof executing programs, communicating with the main memory, hard disk, and peripherals. It is\nthe only active unit in our entire system. The others are passive and only respond to requests.\nThey do not have any computational capability of their own.\n1.3 Computers are Dumb Machines\nIrrespective of the underlying technology, a fundamental concept that we need to understand\nis that a computer is fundamentally a dumb machine. Unlike our brains, it is not endowed with\nabstract thought, reason, and conscience. At least at the moment, computers cannot take very (cid:13)c Smruti R. Sarangi 18\nsophisticated decisions on their own. All they can do is execute a program. Nonetheless, the\nreason computers are so powerful is because they are extremely good at executing programs.\nThey can execute billions of basic instructions per second. This makes them dumb yet very\nfast. A comparison of the computer with the human brain is shown in Table 1.1.\nFeature Computer Our Brilliant Brain\nIntelligence Dumb Intelligent\nSpeed of basic calculations Ultra-fast Slow\nCan get tired Never After some time\nCan get bored Never Almost always\nTable 1.1: Computer vs the brain\nIf we combine the processing power of computers, with intelligent programs written by\nthe human brain, we have the exquisite variety of software available today. Everything from\noperating systems to word processors to computer games is written this way.\nThe basic question that we need to answer is :\nQuestion 2\nHow, do we make a dumb machine do intelligent things?\nComputers are these tireless machines that can keep on doing calculations very quickly\nwithout ever complaining about the monotonicity of the work. As compared to computers, our\nbrains are creative, tire easily, and do not like to do the same thing over and over again. To\ncombine the best of both worlds, our brains need to produce computer programs that specify\nthe set of tasks that need to be performed in great detail. A computer can then process the\nprogram, and produce the desired output by following each instruction in the program.\nHence, we can conclude that we should use the creative genius of our brains to write pro-\ngrams. Each program needs to contain a set of basic instructions that a computer can process.\nHenceforth, a computer can produce the desired output. An instruction is defined as a basic\ncommand that can be given to a computer.\n1.4 The Language of Instructions\nWeobservethattocommunicatewithacomputer,weneedtospeakitslanguage. Thislanguage\nconsists of a set of basic instructions that the computer can understand. The computer is not\nsmart enough to process instructions such as, \u201ccalculate the distance between New Delhi and\nthe North Pole\u201d. However, it can do simple things like adding two numbers. This holds for\npeople as well. For example, if a person understands only Spanish, then there is no point\nspeaking to her in Russian. It is the responsibility of the person who desires to communicate to\narrange for a translator. Likewise, it is necessary to convert high level thoughts and concepts\nto basic instructions that are machine understandable. 19 (cid:13)c Smruti R. Sarangi\nProgrammers typically write programs in a high level language such as C or JavaTM. These\nlanguagescontaincomplexconstructssuchasstructures,unions,switch-casestatements,classes\nand inheritance. These concepts are too complicated for a computer to handle. Hence, it is\nnecessary to pass a C or C++ program through a dedicated program called a compiler that\ncan convert it into a sequence of basic instructions. A compiler effectively removes the burden\nof creating machine (computer) readable code from the programmer. The programmer can\nconcentrate only on the high level logic. Figure 1.5 shows the flow of actions. The first step\nis to write a program in a high level language (C or C++). Subsequently, the second step\ninvolves compiling it. The compiler takes the high level program as input, and produces a\nprogram containing machine instructions. This program is typically called an executable or\nbinary. Note, that the compiler itself is a program consisting of basic machine instructions.\ncompile execute\nProgram Executable Output\nFigure 1.5: Write-compile-execute\nLet us now come to the semantics of instructions themselves. The same way that any\nlanguagehasafinitenumberofwords,thenumberofbasicinstructions\/rudimentarycommands\nthat a processor can support have to be finite. This set of instructions is typically called the\ninstruction set. Some examples of basic instructions are: add, subtract, multiply, logical or,\nand logical not. Note that each instruction needs to work on a set of variables and constants,\nand finally save the result in a variable. These variables are not programmer defined variables;\ntheyareinternallocationswithinthecomputer. Wedefinetheterminstructionsetarchitecture\nas:\nDefinition 3\nThe semantics of all the instructions supported by a processor is known as the instruction\nset architecture (ISA). This includes the semantics of the instructions themselves, along\nwith their operands, and interfaces with peripheral devices.\nThe instruction set architecture is the way that software perceives hardware. We can think\nof it as the list of basic functions that the hardware exports to the external world. It is the,\n\u201clanguage of the computer\u201d. For example, Intel and AMD CPUs use the x86 instruction set,\nIBM processors use the PowerPC(cid:13)R instruction set, HP processors use the PA-RISC instruction\nset, and the ARM processors use the ARM(cid:13)R instruction set (or variants of it such as Thumb-1\nand Thumb-2). It is thus not possible to run a binary compiled for an Intel system on an ARM\nbased system. The instruction sets are not compatible. However, in most cases it is possible\nto reuse the C program. To run a C program on a certain architecture, we need to procure a\ncompiler for that specific architecture, and then appropriately compile the C program. (cid:13)c Smruti R. Sarangi 20\n1.5 Instruction Set Design\nLet us now begin the difficult process of designing an instruction set for a processor. We can\nthink of an instruction set as a legal contract between software and hardware. Both sides\nneed to implement their side of the contract. The software part needs to ensure that all the\nprograms that users write can be successfully and efficiently translated to basic instructions.\nLikewise, hardware needs to ensure that all the instructions in the instruction set are efficiently\nimplementable. On both sides we need to make reasonable assumptions. An ISA needs to have\nsome necessary properties and some desirable properties for efficiency. Let us first look at a\nproperty, which is absolutely necessary.\n1.5.1 Complete - The ISA should be able to Implement all User Programs\nThis is an absolutely necessary requirement. We want an ISA to be able to represent all\nprograms that users are going to write for it. For example, if we have an ISA with just an ADD\ninstruction, then we will not be able to subtract two numbers. To implement loops, the ISA\nshould have some method to re-execute the same piece of code over and over again. Without\nthis support for and while loops in C programs will not work. Note that for general purpose\nprocessors, we are looking at all possible programs. However, a lot of processors for embedded\ndevices have limited functionality. For example, a simple processor that does string processing\ndoes not require support for floating point numbers (numbers with a decimal point). We need\nto note that different processors are designed to do different things, and hence their ISAs can\nbe different. However, the bottom line is that any ISA should be complete in the sense that it\nshould be able to express all the programs in machine code that a user intends to write for it.\nLet us now explore the desirable properties of an instruction set.\n1.5.2 Concise \u2013 Limited Size of the Instruction Set\nWe should ideally not have a lot of instructions. We shall see in Chapter 8 that it takes a fairly\nnon-trivialamountofhardwaretoimplementaninstruction. Implementingalotofinstructions\nwillunnecessarilyincreasethenumberoftransistorsintheprocessorandincreaseitscomplexity.\nConsequently, most instruction sets have somewhere between 64 to 1000 instructions. For\nexample, the MIPS instruction set contains 64 instructions, whereas the Intel x86 instruction\nset has roughly a 1000 instructions as of 2012. Note that 1000 is considered a fairly large\nnumber for the number of instructions in an ISA.\n1.5.3 Generic \u2013 Instructions should Capture the Common Case\nMost of the common instructions in programs are simple arithmetic instructions such as add,\nsubtract, multiply, divide. The most common logical instructions are logical and, or, exclusive-\nor, and not. Hence, it makes sense to dedicate an instruction to each of these common opera-\ntions.\nIt is not a good idea to have instructions that implement a very rarely used computation.\nFor example, it might not make sense to implement an instruction that computes sin\u22121(x). It\nis possible to provide dedicated library functions that compute sin\u22121(x) using existing mathe-\nmatical techniques such as Taylor series expansion. Since this function is rarely used by most 21 (cid:13)c Smruti R. Sarangi\nprograms, they will not be adversely affected if this function takes a relatively long time to\nexecute.\n1.5.4 Simple \u2013 Instructions should be Simple\nLet us assume that we have a lot of programs that add a sequence of numbers. To design a\nprocessor especially tailored towards such programs, we have several options with regards to\nthe add instruction. We can implement an instruction that adds two numbers, or we can also\nimplement an instruction that can take a list of operands, and produce the sum of the list.\nThere is clearly a difference in complexity here, and we cannot say which implementation is\nfaster. The former approach requires the compiler to generate more instructions; however, each\nadd operation executes quickly. The latter approach generates a fewer number of instructions;\nbut, each instruction takes longer to execute. The former type of ISA is called a Reduced\nInstruction Set, and the latter ISA type is called a Complex Instruction Set. Let us give two\nimportant definitions here.\nDefinition 4\nA reduced instruction set computer (RISC) implements simple instructions that have a\nsimple and regular structure. The number of instructions is typically a small number (64\nto 128). Examples: ARM, IBM PowerPC, HP PA-RISC\nDefinition 5\nA complex instruction set computer (CISC) implements complex instructions that are highly\nirregular, take multiple operands, and implement complex functionalities. Secondly, the\nnumber of instructions is large (typically 500+). Examples: Intel x86, VAX\nTheRISCvsCISCdebateusedtobeaverycontentiousissuetillthelatenineties. However,\nsince then designers, programmers, and processor vendors have been tilting towards the RISC\ndesign style. The consensus seems to be go for a small number of relatively simple instructions\nthat have a regular structure and format. It is important to note that this point is still debat-\nable as CISC instructions are sometimes preferable for certain types of applications. Modern\nprocessors typically use a hybrid approach where they have simple, as well as some complicated\ninstructions. However, underthehoodCISCinstructionsaretranslatedintoRISCinstructions.\nHence, we believe that the scale tilts slightly more towards RISC instructions. We shall thus\nconsider it a desirable property to have simple instructions.\nImportant Point 1\nAn ISA needs to be complete, concise, generic, and simple. It is necessary to be complete,\nwhereas the rest of the properties are desirable (and sometimes debatable). (cid:13)c Smruti R. Sarangi 22\nWay Point 1\nWe have currently considered the following concepts.\n\u2022 Computers are dumb yet ultra-fast machines.\n\u2022 Instructionsarebasicrudimentarycommandsusedtocommunicatewiththeprocessor.\nA computer can execute billions of instructions per second.\n\u2022 The compiler transforms a user program written in a high level language such as C to\na program consisting of basic machine instructions.\n\u2022 The instruction set architecture(ISA) refers to the semantics of all the instructions\nsupported by a processor.\n\u2022 The instruction set needs to be complete. It is desirable if it is also concise, generic,\nand simple.\nLet us subsequently look at the conditions that ensure the completeness of an ISA. We will\nthen try to create a concise, simple, and generic ISA in Chapter 3.\n1.6 How to Ensure that an ISA is Complete?\nThis is a very interesting, difficult, and theoretically profound question. The problem of finding\nif a given ISA is complete for a given set of programs, is a fairly difficult problem, and is beyond\nthescopeofthebook. Thegeneralcaseisfarmoreinteresting. Weneedtoanswerthequestion:\nQuestion 3\nGiven an ISA, can it represent all possible programs?\nWewillneedtotakerecoursetotheoreticalcomputersciencetoanswerthisquestion. Casual\nreaderscanskipSections1.6.1to1.6.6withoutanylossincontinuity. Theycandirectlyproceed\nto Section 1.6.7, where we summarise the main results.\n1.6.1 Towards a Universal ISA*\nLet us try to answer Question 3. Assume that we are given an ISA that contains the basic\ninstructions add, and multiply. Can we use this ISA to run all possible programs? The answer\nis no, because we cannot subtract two numbers using the basic instructions that we have. If we\nadd the subtract instruction to our repertoire of instructions, can we compute the square root\nof a number? Even if we can, is it guaranteed that we can do all types of computations? To\nanswer such vexing questions we need to first define a universal machine. 23 (cid:13)c Smruti R. Sarangi\nDefinition 6\nA machine that can execute any program is known as a universal machine.\nItisamachinethatcanexecuteallprograms. Wecantreateachbasicactionofthismachine\nas an instruction. Thus the set of actions of a universal machine is its ISA, and this ISA is\ncomplete. Consequently, when we say that an ISA is complete, it is the same as saying that\nwe can build a universal machine exclusively based on the given ISA. Hence, we can solve the\nproblem of completeness of an ISA by solving the problem of designing universal machines.\nThey are dual problems. It is easier to reason in terms of universal machines. Hence, let us\ndelve into this problem.\nComputer scientists started pondering at the design of universal machines at the beginning\nof the 20th century. They wanted to know what is computable, and what is not, and the power\nof different classes of machines. Secondly, what is the form of a theoretical machine that can\ncompute the results of all possible programs? These fundamental results in computer science\nform the basis of today\u2019s modern computer architectures.\nAlan Turing was the first to propose a universal machine that was extremely simple and\npowerful. This machine is aptly named after him, and is known as the Turing machine. This is\nmerelyatheoreticalentity, andistypicallyusedasamathematicalreasoningtool. Itispossible\nto create a hardware implementation of a Turing machine. However, this would be extremely\ninefficient, and require a disproportionate amount of resources. Nonetheless, Turing machines\nform the basis of today\u2019s computers and modern ISAs are derived from the basic actions of a\nTuring machine. Hence, it is very essential for us to study its design. Note that we provide\na very cursory treatment in this book. Interested readers are requested to take a look at the\nseminal text on the theory of computation by Hopcroft, Motwani and Ulmann [Hopcroft et al.,\n2006].\n1.6.2 Turing Machine*\nThe general structure of a Turing machine is shown in Figure 1.6. A Turing machine contains\nan infinite tape that is an array of cells. Each cell can contain a symbol from a finite alphabet.\nThere is a special symbol $ that works as a special marker. A dedicated tape head points to\na cell in the infinite tape. There is a small piece of storage to save the current state among a\nfinite set of states. This storage element is called a state register.\nThe operation of the Turing machine is very simple. In each step, the tape head reads the\nsymbol in the current cell, its current state from the state register, and looks up a table that\ncontains the set of actions for each combination of symbol and state. This dedicated table is\ncalledatransition function tableoraction table. Eachentryinthistablespecifiesthreethings\u2013\nwhether to move the tape head one step to the left or right, the next state, and the symbol that\nshould be written in the current cell. Thus, in each step, the tape head can overwrite the value\nof the cell, change its state in the state register and move to a new cell. The only constraint\nis that the new cell needs to be to the immediate left or right of the current cell. Formally, its\nformat is (state,symbol) \u2192 ({L,R},new state,new symbol). L stands for left, and R stands\nfor right. (cid:13)c Smruti R. Sarangi 24\nInfinite tape\nL R\nState register Tape head\nAction table\nFigure 1.6: A Turing machine\nThis seemingly abstract and obscure computing device is actually very powerful. Let us\nexplain with examples. See Examples 2, 3, and 4. In all the cases, we assume that the input is\ndemarcated by the special marker symbol $.\nExample 2\nDesign a Turing machine to increment a number by 1.\n$ 7 3 4 6 9 $\nTape head\nAnswer: Each cell contains a single digit. The number is demarcated at both ends by the\nspecial marker $. Lastly, the tape head points to the unit\u2019s digit.\nWe first define four states (S ,S ): pre-exit and exit. The computation is over when\n0 1\nthe Turing machine reaches the exit state. The states S and S represent the value of the\n0 1\ncarry, 0 or 1, respectively. The state register is initialised to S since we are incrementing\n1\nthe number by 1. In other words, we can assume that the starting value of the carry digit\nis equal to 1.\nAt each step, the tape head reads the current digit, d, and the value of the carry, c, from\nthe state register. For each combination of d, and c, the action table contains the next state\n(new value of carry), and the result digit. The tape head always moves to the left. For 25 (cid:13)c Smruti R. Sarangi\nexample, if (d,c) = (9,1), then we are effectively adding (9 + 1). The next state is equal to\nS (output carry), the Turing machine writes 0 in the current cell, and the tape head moves\n1\nto the cell on the left.\nThe only special case arises when the computation is ending. When the tape head en-\ncounters the $ symbol, then it looks at the value of the carry. If it is equal to 0, then it\nleaves the value untouched and moves to the exit state. If it is equal to 1, then it moves to\nthe pre-exit state, writes 1 to the cell, and moves to the left. Subsequently, it writes $ to\nthe cell under the tape head, and then moves to the exit state.\nExample 3\nDesign a Turing machine to find out if a string is of the form aaa...abb...bb.\nAnswer: Let us define two states (S ,S ), and two special states \u2013 exit and error. If the\na b\nstate becomes equal to exit or error, then the computation stops. The Turing machine can\nstart scanning the input from right to left as Example 2. It starts in state S . The action\nb\ntable is as follows:\n(S ,b) \u2192 (L,S ,b)\nb b\n(S ,a) \u2192 (L,S ,a)\nb a\n(S ,$) \u2192 (L,error,$)\nb\n(S ,b) \u2192 (L,error,b)\na\n(S ,a) \u2192 (L,S ,a)\na a\n(S ,$) \u2192 (L,exit,$)\na\nExample 4\nDesign a Turing machine to find out if a string of characters is a palindrome. A palindrome\nis a word that reads the same forward and backwards. Example: civic, rotator, rotor.\nFurthermore, assume that each character is either \u2018a\u2019 or \u2018b\u2019.\nAnswer: Let us assume that the Turing machine starts at the rightmost character in the\nbegin state. Let us consider the case when the symbol under the tape head is a in the begin\nstate. The machine enters the state L (move left, starting symbol is a) and replaces a with\na\n$. Now it needs to see if the leftmost character is a. Hence, the tape head moves towards\nthe left until it encounters $. It then enters the Rcheck state. It moves one cell to the\na\nright and checks if the symbol is equal to a. If it is a, then the string might be a palindrome.\nOtherwise, it is definitely not a palindrome and the procedure can terminate by entering the\nerror state. The tape head again rewinds by moving all the way to the right and starts at\nthe cell, which is to the immediate left of the starting cell in the previous round. The same (cid:13)c Smruti R. Sarangi 26\nalgorithm is performed iteratively till either an error is encountered or all the symbols are\nreplaced with $.\nIf the starting symbol was b, the procedure would have been exactly the same albeit with\na different set of states \u2013 L and Rcheck . The action table is shown below.\nb b\n(begin,$) \u2192 (L,exit,$)\n(begin,a) \u2192 (L,L ,$)\na\n(L a,a) \u2192 (L,L a,a) (begin,b) \u2192 (L,L b,$)\n(L a,b) \u2192 (L,L a,b) (L b,a) \u2192 (L,L b,a)\n(L a,$) \u2192 (R,Rcheck a,$) (L b,b) \u2192 (L,L b,b)\n(Rcheck a,a) \u2192 (R,Rmove,$) (L b,$) \u2192 (R,Rcheck b,$)\n(Rcheck a,b) \u2192 (R,error,$) (Rcheck b,a) \u2192 (R,error,$)\n(Rmove,a) \u2192 (R,Rmove,a) (Rcheck b,b) \u2192 (R,Rmove,$)\n(Rmove,b) \u2192 (R,Rmove,b)\n(Rmove,$) \u2192 (L,begin,$)\nIn these examples we have considered three simple problems and designed Turing machines\nfrom them. We can immediately conclude that designing Turing machines for even simple\nproblems is difficult, and cryptic. The action table can contain a lot of states, and quickly blow\nout of size. However, the baseline is that it is possible to solve complex problems with this\nsimple device. It is in fact possible to solve all kinds of problems such as weather modelling,\nfinancial calculations, and solving differential equations with this machine!\nDefinition 7\nChurch-Turing thesis: Any real-world computation can be translated into an equivalent com-\nputation involving a Turing machine. (source: Wolfram Mathworld)\nThis observation is captured by the Church-Turing thesis, which basically says that all\nfunctions that are computable by any physical computing device are computable by a Turing\nmachine. In lay man\u2019s terms, any program that can be computed by deterministic algorithms\non any computer known to man, is also computable by a Turing machine.\nThis thesis has held its ground for the last half century. Researchers have up till now not\nbeen able to find a machine that is more powerful than a Turing machine. This means that\nthere is no program that can be computed by another machine, and not by a Turing machine.\nThere are some programs that might take forever to compute on a Turing machine. However,\nthey would also take infinite time on all other computing machines. We can extend the Turing\nmachine in all possible ways. We can consider multiple tapes, multiple tape heads, or multiple\ntracks in each tape. It can be shown that each of these machines is as powerful as a simple\nTuring machine. 27 (cid:13)c Smruti R. Sarangi\n1.6.3 Universal Turing Machine*\nThe Turing machine described in the Section 1.6.2 is not a universal machine. This is because\nit contains an action table, which is specific to the function being computed by the machine.\nA true universal machine will have the same action table, symbols, and also the same set of\nstates for every function. We can make a universal Turing machine, if we can design a Turing\nmachine that can simulate another Turing machine. This Turing machine will be generic and\nwill not be specific to the function that is being computed.\nLet the Turing machine that is being simulated be called M, and the universal Turing\nmachine be called U. Let us first create a generic format for the action table of M, and save\nit in a designated location on the tape of U. This simulated action table contains a list of\nactions, and each action requires the five parameters \u2013 old state, old symbol, direction(left or\nright), new state, new symbol. We can use a common set of basic symbols that can be the 10\ndecimal digits (0-9). If a function requires more symbols then we can consider one symbol to be\ncontained in a set of contiguous cells demarcated by special delimiters. Let such a symbol be\ncalled a simulated symbol. Likewise, the state in the simulated action table can also be encoded\nas a decimal number. For the direction, we can use 0 for left, and 1 for right. Thus a single\naction table entry might look something like (@1334@34@0@1335@10@). Here the \u2018@\u2019 symbol\nis the delimiter. This entry is saying that we are moving from state 1334 to 1335 if symbol\n34 is encountered. We move left (0), and write a value of 10. Thus, we have found a way of\nencoding the action table, set of symbols, and states of a Turing machine designed to compute\na certain function.\nSimilarly, we can designate an area of the tape to contain the state register of M. We call\nthis the simulated state register. Let the tape of M be given a dedicated space in the tape of\nU, and let us call this space the work area.\nThe organisation is shown in Figure 1.7.\nSimulated state register\nSimulated action table Work area\nL R\nGeneric state register Tape head\nGeneric action table\nFigure 1.7: Layout of a universal Turing machine\nThe tape is thus divided into three parts. The first part contains the simulated action table,\nthe second part contains the simulated state register, and the last part contains the work area (cid:13)c Smruti R. Sarangi 28\nthat contains a set of simulated symbols.\nThe universal Turing machine(U) has a very simple action table and set of states. The idea\nis to find the right entry in the simulated action table that matches the value in the simulated\nstate register and simulated symbol under the tape head. Then the universal Turing machine\nneedstocarryoutthecorrespondingactionbymovingtoanewsimulatedstate,andoverwriting\nthe simulated symbol in the work area if required.\nThe devil is in the details. For doing every basic action, U needs to do tens of tape head\nmovements. The details are given in Hopcroft, Motwani, and Ulmann [Hopcroft et al., 2006].\nHowever, the conclusion is that we can construct a universal Turing machine.\nImportant Point 2\nIt is possible to construct a universal Turing machine that can simulate any other Turing\nmachine.\nTuring Completeness\nSince the 1950s, researchers have devised many more types of hypothetical machines with their\nownsetsofstatesandrules. Eachofthesemachineshavebeenproventobeatmostaspowerful\nastheTuringmachine. Thereisagenericnameforallmachinesandcomputingsystemsthatare\nas expressive and powerful as a Turing machine. Such systems are said to be Turing complete.\nAny universal machine and ISA is thus Turing complete.\nDefinition 8\nAny computing system that is equivalent to a Turing machine is said to be Turingcomplete.\nWe thus need to prove that an ISA is complete or universal if it is Turing complete.\n1.6.4 A Modified Universal Turing Machine*\nLet us now consider a variant of a universal Turing machine (see Figure 1.8) that is more\namenable to practical implementations. Let it have the following features. Note that such a\nmachine has been proven to be Turing complete.\n1. The tape is semi-infinite (extends to infinity in only one direction).\n2. The simulated state is a pointer to an entry in the simulated action table.\n3. There is one unique entry in the simulated action table for each state. While looking up\nthe simulated action table, we do not care about the symbol under the tape head.\n4. An action directs the tape head to visit a set of locations in the work area, and based\non their values computes a new value using a simple arithmetical function. It writes this\nnew value into a new location in the work area. 29 (cid:13)c Smruti R. Sarangi\naction\n2.compute\nSimulated state register\n1.read\nSimulated action table Work area 3.write\nstate pointing to\naction table entry L R\nGeneric state register Tape head\nGeneric action table\nFigure 1.8: A modified universal Turing machine\n5. The default next state is the succeeding state in the action table.\n6. An action can also arbitrarily change the state if a symbol at a certain location on the\ntape is less than a certain value. Changing the state means that the simulated tape head\nwill start fetching actions from a new area in the simulated action table.\nThis Turing machine suggests a machine organisation of the following form. There is a large\narray of instructions (action table). This array of instructions is commonly referred to as the\nprogram. There is a state register that maintains a pointer to the current instruction in the\narray. We can refer to this register as the program counter. It is possible to change the program\ncounter to point to a new instruction. There is a large work area, where symbols can be stored,\nretrieved and modified. This work area is also known as the data area. The instruction table\n(program) and the work area (data) were saved on the tape in our modified Turing machine.\nIn a practical machine, we call this infinite tape as the memory. The memory is a large array\nof memory cells, where a memory cell contains a basic symbol. A part of the memory contains\nthe program, and another part of it contains data.\nDefinition 9\nThe memory in our conceptual machine is a semi-infinite array of symbols. A part of it\ncontains the program consisting of basic instructions, and the rest of it contains data. Data\nrefers to variables and constants that are used by the program.\nFurthermore, each instruction can read a set of locations in the memory, compute a small\narithmetic function on them, and write the results back to the memory. It can also jump to any\nother instruction depending on values in the memory. There is a dedicated unit to compute (cid:13)c Smruti R. Sarangi 30\nthese arithmetic functions, write to memory, and jump to other instructions. This is called the\nCPU(Central Processing Unit). Figure 1.9 shows a conceptual organisation of this machine.\nCPU\nProgram Control Arithmetic\ncounter (PC) unit unit\nInstruction\nProgram\nData\nMemory\nFigure 1.9: A basic instruction processing machine\nInterested readers might want to prove the fact that this machine is equivalent to a Turing\nmachine. It is not very difficult to do so. We need to note that we have captured all aspects\nof a Turing machine: state transition, movement of the tape head, overwriting symbols, and\ndecisions based on the symbol under the tape head. We shall see in Section 1.7.2 that such a\nmachine is very similar to the Von Neumann machine. Von Neumann machines form the basis\nof today\u2019s computers. Readers can also refer to books on computational complexity.\nImportant Point 3\nFigure 1.9 represents a universal machine that can be practically designed.\n1.6.5 Single Instruction ISA*\nLet us now try to design an ISA for our modified Turing machine. We shall see that it is\npossible to have a complete ISA that contains just a single instruction. Let us consider an\ninstruction that is compatible with the modified Turing machine and has been proven to be\nTuring complete.\nsbn a, b, c\nsbn stands for subtract and branch if negative. Here, a, and b are memory locations. This\ninstruction subtracts b from a, saves the results in a, and if a < 0, it jumps to the instruction\nat location c in the instruction table. Otherwise, the control transfers to the next instruction.\nFor example, we can use this instruction to add two numbers saved in locations a and b. Note\nthat exit is a special location at the end of the program. 31 (cid:13)c Smruti R. Sarangi\n1: sbn temp, b, 2\n2: sbn a, temp, exit\nHere, we assume that the memory location temp already contains the value 0. The first\ninstruction saves \u2212b in temp. Irrespective of the value of the result it jumps to the next\ninstruction. Note that the identifier (number :) is a sequence number for the instruction. In\nthe second instruction we compute a = a+b = a\u2212(\u2212b). Thus, we have successfully added\ntwo numbers. We can now use this basic piece of code to add the numbers from 1 to 10. We\nassume that the variable counter is initialised to 9, index is initialised to 10, one is initialised\nto 1, and sum is initialised to 0.\n1: sbn temp, temp, 2 \/\/ temp = 0\n2: sbn temp, index, 3 \/\/ temp = -1 * index\n3: sbn sum, temp, 4 \/\/ sum += index\n4: sbn index, one, 5 \/\/ index -= 1\n5: sbn counter, one, exit \/\/ loop is finished, exit\n6: sbn temp, temp, 7 \/\/ temp = 0\n7: sbn temp, one, 1 \/\/ (0 - 1 < 0), hence goto 1\nWe observe that this small sequence of operations runs a for loop. The exit condition is in\nline 5, and the loop back happens in line 7. In each iteration it computes \u2013 sum+ = index.\nThere are many more similar single instruction ISAs that have been proven to be complete\nsuch as subtract and branch if less than equal to, reverse subtract and skip if borrow, and a\ncomputer that has generic memory move operations. The interested reader can refer to the\nbook by Gilreath and Laplante [Gilreath and Laplante, 2003].\n1.6.6 Multiple Instruction ISA*\nWriting a program with just a single instruction is very difficult, and programs tend to be very\nlong. There is no reason to be stingy with the number of instructions. We can make our life\nsignificantly easier by considering a multitude of instructions. Let us try to break up the basic\nsbn instructions into several instructions.\nArithmetic Instructions We can have a set of arithmetic instructions such as add, subtract,\nmultiply and divide.\nMove Instructions We can have move instructions that move values across different memory\nlocations. They should allow us to also load constant values into memory locations.\nBranch Instructions We require branch instructions that change the program counter to\npointtonewinstructionsbasedontheresultsofcomputationsorvaluesstoredinmemory.\nKeeping these basic tenets in mind, we can design many different types of complete ISAs.\nThe point to note is that we definitely need three types of instructions \u2013 arithmetic (data\nprocessing), move (data transfer), and branch (control). (cid:13)c Smruti R. Sarangi 32\nImportant Point 4\nIn any instruction set, we need at least three types of instructions:\n1. We need arithmetic instructions to perform operations such as add, subtract, multiply,\nand divide. Most instruction sets also have specialised instructions in this category to\nperform logical operations such as logical OR and NOT.\n2. We need data transfer instructions that can transfer values between memory locations\nand can load constants into memory locations.\n3. We need branch instructions that can start executing instructions at different points\nin the program based on the values of instruction operands.\n1.6.7 Summary of Theoretical Results\nLetussummarisethemainresultsthatwehaveobtainedfromourshortdiscussionontheoretical\ncomputer science.\n1. The problem of designing a complete ISA is the same as that of designing a universal\nmachine. A universal machine can run any program. We can map each instruction in\nthe ISA to an action in this universal machine. A universal machine is the most powerful\ncomputing machine known to man. If a universal machine cannot compute the result of\na program because it never terminates (infinite loop), then all other computing machines\nare also guaranteed to fail for this program.\n2. Universal machines have been studied extensively in theoretical computer science. One\nsuch machine is the Turing machine named after the father of computer science \u2013 Alan\nTuring.\n3. TheTuringmachineisaveryabstractcomputingdevice, andisnotamenabletopractical\nimplementations. A practical implementation will be very slow and consume a lot of\nresources. However, machines equivalent to it can be much faster. Any such machine,\nISA, and computing system that is equivalent to a Turing machine is said to be Turing\ncomplete.\n4. We defined a modified Turing machine that is Turing complete in Section 1.6.4. It has\nthe structure shown in Figure 1.10. Its main parts and salient features are as follows.\n(a) It contains a dedicated instruction table that contains a list of instructions.\n(b) It has a program counter that keeps track of the current instruction that is being\nexecuted. The program counter contains a pointer to an entry in the instruction\ntable.\n(c) It has a semi-infinite array of storage locations that can save symbols belonging to\na finite set. This array is known as the memory. 33 (cid:13)c Smruti R. Sarangi\nCPU\nProgram Control Arithmetic\ncounter (PC) unit unit\nInstruction\nProgram\nData\nMemory\nFigure 1.10: A basic processing machine\n(d) The memory contains the instruction table (also referred to as the program), and\ncontains a data area. The data area saves all the variables and constants that are\nrequired by the program.\n(e) Each instruction can compute the result of a simple arithmetic function using values\nstored at different memory locations. It can then save the result in another memory\nlocation.\n(f) The machine starts with the first instruction in the program, and then by default,\nafter executing an instruction, the machine fetches the next instruction in the in-\nstruction table.\n(g) It is possible for an instruction to direct the machine to fetch a new instruction from\nan arbitrary location in the instruction table based on the value stored in a memory\nlocation.\n5. A simple one instruction ISA that is compatible with our modified Turing machine, con-\ntains the single instruction sbn (subtract the values of two memory locations, and branch\nto a new instruction if the result is negative).\n6. We can have many Turing complete ISAs that contain a host of different instructions.\nSuch ISAs will need to have the following types of instructions.\nArithmetic Instructions Add, subtract, multiply and divide. These instructions can\nbe used to simulate logical instructions such as OR and AND.\nMove Instructions Move values across memory locations, or load constants into mem-\nory.\nBranch Instructions Fetch the next instruction from a new location in the instruction\ntable, if a certain condition on the value of a memory location holds. (cid:13)c Smruti R. Sarangi 34\n1.7 Design of Practical Machines\nA broad picture of a practical machine has emerged from our discussion in Section 1.6.7. We\nhave summarised the basic structure of such a machine in Figure 1.10. Let us call this machine\nas the concept machine. Ideas similar to our concept machine were beginning to circulate in\nthe computer science community after Alan Turing published his research paper proposing the\nTuringmachinein1936. Severalscientistsgotinspiredbyhisideas, andstartedpursuingefforts\nto design practical machines.\n1.7.1 Harvard Architecture\nOne of the earliest efforts in this direction was the Harvard Mark-I. The Harvard architecture\nis very similar to our concept machine shown in Figure 1.10. Its block diagram is shown in\nFigure1.11. Thereareseparatestructuresformaintainingtheinstructiontableandthememory.\nThe former is also known as instruction memory because we can think of it as a specialised\nmemory tailored to hold only instructions. The latter holds data values that programs need.\nHence, it is known as the data memory. The engine for processing instructions is divided\ninto two parts \u2013 control and ALU. The job of the control unit is to fetch instructions, process\nthem, and co-ordinate their execution. ALU stands for arithmetic-logic-unit. It has specialised\ncircuits that can compute arithmetic expressions or logical expressions (AND\/OR\/NOT etc.).\nCPU\nALU\nData\nInstruction Control\nmemory\nmemory\nI\/O devices\nFigure 1.11: The Harvard architecture\nNote that every computer needs to take inputs from the user\/programmer and needs to\nfinally communicate results back to the programmer. This can be done through a multitude of\nmethods. Today we use a keyboard and monitor. Early computers used a set of switches and\nthe final result was printed out on a piece of paper.\n1.7.2 Von Neumann Architecture\nJohn von Neumann proposed the Von Neumann architecture for general purpose Turing com-\nplete computers. Note that there were several other scientists such as John Mauchly and J.\nPresper Eckert who independently developed similar ideas. Eckert and Mauchly designed the 35 (cid:13)c Smruti R. Sarangi\nfirstgeneralpurposeTuringcompletecomputer(withoneminorlimitation)calledENIAC(Elec-\ntronic Numerical Integrator and Calculator) based on this architecture in 1946. It was used to\ncompute artillery firing tables for the US army\u2019s ballistic research laboratory. This computer\nwas later succeeded by the EDVAC computer in 1949, which was also used by the US army\u2019s\nballistics research laboratory.\nThe basic Von Neumann architecture, which is the basis of ENIAC and EDVAC is shown\nin Figure 1.12. This is pretty much the same as our concept machine. The instruction table is\nsaved in memory. The processing engine that is akin to our modified Turing machine is called\nthe CPU (central processing unit). It contains the program counter. Its job is to fetch new\ninstructions, and execute them. It has dedicated functional units to calculate the results of\narithmetic functions, load and store values in memory locations, and compute the results of\nbranch instructions. Lastly, like the Harvard architecture, the CPU is connected to the I\/O\nsubsystem.\nCPU\nALU\nMemory Control I\/O devices\nFigure 1.12: Von Neumann architecture\nThe path breaking innovation in this machine was that the instruction table was stored in\nmemory. It was possible to do so by encoding every instruction with the same set of symbols\nthat are normally stored in memory. For example, if the memory stores decimal values, then\neach instruction needs to be encoded into a string of decimal digits. A Von Neumann CPU\nneeds to decode every instruction. The crux of this idea is that instructions are treated as\nregular data(memory values). We shall see in later chapters that this simple idea is actually a\nvery powerful tool in designing elegant computing systems. This idea is known as the stored\nprogram concept.\nDefinition 10\nStored-program concept: A program is stored in memory and instructions are treated as\nregular memory values.\nThe stored program concept tremendously simplifies the design of a computer. Since mem-\nory data and instructions are conceptually treated the same way, we can have one unified\nprocessing system and memory system that treats instructions and data the same way. From (cid:13)c Smruti R. Sarangi 36\nthe point of view of the CPU, the program counter points to a generic memory location whose\ncontents will be interpreted as that of an encoded instruction. It is easy to store, modify,\nand transmit programs. Programs can also dynamically change their behavior during runtime\nby modifying themselves and even other programs. This forms the basis of today\u2019s complex\ncompilers that convert high level C programs into machine instructions. Furthermore, a lot\nof modern systems such as the Java virtual machine dynamically modify their instructions to\nachieve efficiency.\nLastly, astute readers would notice that a Von Neumann machine or a Harvard machine do\nnot have an infinite amount of memory like a Turing machine. Hence, strictly speaking they\nare not exactly equivalent to a Turing machine. This is true for all practical machines. They\nneed to have finite resources. Nevertheless, the scientific community has learnt to live with this\napproximation.\n1.7.3 Towards a Modern Machine with Registers and Stacks\nMany extensions to the basic Von-Neumann machine have been proposed in literature. In fact\nthis has been a hot field of study for the last half century. We discuss three important variants\nof Von Neumann machines that augment the basic model with registers, hardware stacks, and\naccumulators. The register based design is by far the most commonly used today. However,\nsome aspects of stack based machines and accumulators have crept into modern register based\nprocessors also. It would be worthwhile to take a brief look at them before we move on.\nVon-Neumann Machine with Registers\nTheterm\u201cregistermachine\u201dreferstoaclassofmachinesthatinthemostgeneralsensecontain\nan unbounded number of named storage locations called registers. These registers can be\naccessedrandomly, andallinstructionsuseregisternamesastheiroperands. TheCPUaccesses\nthe registers, fetches the operands, and then processes them. However, in this section, we look\nat a hybrid class of machines that augment a standard Von Neumann machine with registers.\nA register is a storage location that can hold a symbol. These are the same set of symbols that\nare stored in memory. For example, they can be integers.\nLet us now try to motivate the use of registers. The memory is typically a very large\nstructure. In modern processors, the entire memory can contain billions of storage locations.\nAny practical implementation of a memory of this size is fairly slow in practice. There is a\ngeneral rule of thumb in hardware, \u201clarge is slow, and small is fast.\u201d Consequently, to enable\nfast operation, every processor has a small set of registers that can be quickly accessed. The\nnumberofregistersistypicallybetween8and64. Mostoftheoperandsinarithmeticandbranch\noperations are present in these registers. Since programs tend to use a small set of variables\nrepeatedly at any point of time, using registers saves many memory accesses. However, it\nsometimes becomes necessary to bring in memory locations into registers or writeback values\nin registers to memory locations. In those cases, we use dedicated load and store instructions\nthat transfer values between memory and registers. Most programs have a majority of pure\nregister instructions. The number of load and store instructions are typically about a third of\nthe total number of executed instructions.\nLet us give an example. Assume that we want to add the cubes of the numbers in the\nmemory locations b and c, and we want to save the result in the memory location a. A machine 37 (cid:13)c Smruti R. Sarangi\nwith registers would need the following instructions. Assume that r1, r2, and r3 are the names\nofregisters. Here, wearenotusinganyspecificISA(theexplanationisgenericandconceptual).\n1: r1 = mem[b] \/\/ load b\n2: r2 = mem[c] \/\/ load c\n3: r3 = r1 * r1 \/\/ compute b^2\n4: r4 = r1 * r3 \/\/ compute b^3\n5: r5 = r2 * r2 \/\/ compute c^2\n6: r6 = r2 * r5 \/\/ compute c^3\n7: r7 = r4 + r6 \/\/ compute b^3 + c^3\n4: mem[a] = r7 \/\/ save the result\nHere, mem is an array representing memory. We need to first load the values into registers,\nthen perform arithmetic computations, and then save the result back in memory. We can\nsee in this example that we are saving on memory accesses by using registers. If we increase\nthe complexity of the computations, we will save on even more memory accesses. Thus, our\nexecution with registers will get even faster. The resulting processor organisation is shown in\nFigure 1.13.\nCPU\nRegisters\nALU\nMemory Control I\/O devices\nFigure 1.13: Von Neumann machine with registers\nVon-Neumann Machine with a Hardware Stack\nA stack is a standard data structure that obeys the semantics \u2013 last in, first out. Readers are\nrequested to lookup a book on data structures such as [Lafore, 2002] for more information. A\nstack based machine has a stack implemented in hardware.\nFirst, it is necessary to insert values from the memory into the stack. After that arithmetic\nfunctions operate on the top k elements of the stack. These values get replaced by the result\nof the computation. For example, if the stack contains the values 1 and 2 at the top. They get\nremoved and replaced by 3. Note that here arithmetic operations do not require any operands.\nIf an add operation takes two operands, then they do not need to be explicitly specified. The\noperands are implicitly specified as the top two elements in the stack. Likewise, the location\nof the result also does not need to be specified. It needs to be inserted at the top of the stack.\nEven though, generating instructions for such a machine is difficult and flexibility is an issue,\nthe instructions can be very compact. Most instructions other than load and store do not\nrequire any operands. We can thus produce very dense machine code. Systems in which the (cid:13)c Smruti R. Sarangi 38\nsize of the program is an issue can use a stack based organisation. They are also easy to verify\nsince they are relatively simpler systems.\nA stack supports two operations \u2013 push and pop. Push pushes an element to the top of\nthe stack. Pop removes an element from the top of the stack. Let us now try to compute\nw = x+y\/z\u2212u\u2217v using a stack based Von Neumann machine, we have:\n1: push u \/\/ load u\n2: push v \/\/ load v\n3: multiply \/\/ u*v\n4: push z \/\/ load y\n5: push y \/\/ load z\n6: divide \/\/ y\/z\n7: subtract \/\/ y\/z - u*v\n8: push x \/\/ load x\n9: add \/\/ x + y\/z - u*v\n10: pop w \/\/ store result in w\nIt is clearly visible that scheduling a computation to work on a stack is difficult. There will\nbe many redundant loads and stores. Nonetheless, for machines that are meant to evaluate\nlong mathematical expressions, and machines for which program size is an issue, typically opt\nfor stacks. There are few practical implementations of stack based machines such as Burroughs\nLargeSystems,UCSDPascal,andHP3000(classic). TheJavalanguageassumesahypothetical\nstack based machine during the process of compilation. Since a stack based machine is simple,\nJava programs can virtually run on any hardware platform. When we run a compiled Java\nprogram, then the Java Virtual Machine(JVM) dynamically converts the Java program into\nanother program that can run on a machine with registers.\nAccumulator based Machines\nAccumulatorbasedmachinesuseasingleregistercalledanaccumulator. Eachinstructiontakes\na single memory location as an input operand. For example, an add operation adds the value\nin the accumulator to the value in the memory address and then stores the result back in the\naccumulator. Early machines in the fifties that could not accommodate a register file used to\nhave accumulators. Accumulators were able to reduce the number of memory accesses and\nspeed up the program.\nSome aspects of accumulators have crept into the Intel x86 set of processors that are the\nmost commonly used processors for desktops and laptops as of 2012. For multiplication and\ndivision of large numbers, these processors use the register eax as an accumulator. For other\ngeneric instructions, any register can be specified as an accumulator.\n1.8 The Road Ahead\nWe have outlined the structure of a modern machine in Section 1.7.3, which broadly follows\na Von Neumann architecture, and is augmented with registers. Now, we need to proceed to\nbuild it. As mentioned at the outset, computer architecture is a beautiful amalgam of software 39 (cid:13)c Smruti R. Sarangi\nand hardware. Software engineers tell us what to build? Hardware designers tell us how to\nbuild?\nSystem design\nSoftware interface Processor Design\n10Memory 4 ARM assembly 8 Processor 9 Pipelining system\n2 Language 3 Assembly design 11 Multiprocessors\nof bits language 6 Building blocks: 7 Computer\n5 gates, registers, arithmetic 12 I\/O and storage\nx86 assembly and memories\nSoftware Hardware Hardware\nengineer designer designer\nFigure 1.14: Roadmap of chapters\nLet us thus first take care of the requirements of software engineers. Refer to the roadmap\nof chapters in Figure 1.14. The first part of the book will introduce computer architecture from\nthe point of view of system software designers and application developers. Subsequently, we\nshall move on to designing processors, and lastly, we shall look at building a full systems of\nprocessors, memory elements, and I\/O cum storage devices.\n1.8.1 Representing Information\nIn modern computers, it is not possible to store numbers or pieces of text directly. Today\u2019s\ncomputers are made of transistors. A transistor can be visualised as a basic switch that has two\nstates \u2013 on and off. If the switch is on, then it represents 1, otherwise it represents 0. Every\nsingle entity inclusive of numbers, text, instructions, programs, and complex software needs to\nbe represented using a sequence of 0s and 1s. We have only two basic symbols that we can use\nnamely 0 and 1. A variable\/value that can either be 0 or 1, is known as a bit. Most computers\ntypically store and process a set of 8 bits together. A set of 8 bits is known as a byte. Typically,\na sequence of 4 bytes is known as a word.\nDefinition 11\nbit A value that can either be 0 or 1.\nbyte A sequence of 8 bits.\nword A sequence of 4 bytes.\nerutcetihcra\ntes\nnoitcurtsnI\nrossecorp\nelpmis\na\nfo\nngiseD (cid:13)c Smruti R. Sarangi 40\nFigure 1.15: Memory \u2013 a large array of switches\nWe can thus visualise all the internal storage structures of a computer such as the memory\nor the set of registers as a large array of switches as shown in Figure 1.15. In Chapter 2, we\nshall study the language of bits. We shall see that using bits it is possible to express logical\nconcepts, arithmetic concepts (integer and real numbers), and pieces of text.\nThis chapter is a prerequisite for the next chapter on assembly language. Assembly\nlanguage is a textual representation of an ISA. It is specific to the ISA. Since an instruction\nis a sequence of 0s and 1s, it is very difficult to study it in its bare form. Assembly language\ngives us a good handle to study the semantics of instructions in an ISA. Chapter 3 introduces\nthe general concepts of assembly language and serves as a common introduction to the next\ntwo chapters that delve into the details of two very popular real world ISAs \u2013 ARM and x86.\nWe introduce a simple ISA called SimpleRisc in Chapter 3. Subsequently, in Chapter 4 we\nintroduce the ARM ISA, and in Chapter 5 we briefly cover the x86 ISA. Note that it is not\nnecessary to read both these chapters. After reading the introductory chapter on assembly\nlanguage and obtaining an understanding of the SimpleRisc assembly language, the interested\nreadercanreadjustonechaptertodeepenherknowledgeaboutarealworldISA.Atthispoint,\nthe reader should have a good knowledge of what needs to be built.\n1.8.2 Processing Information\nIn this part, we shall actually build a basic computer. Chapter 6 will start out with the\nbasic building blocks of a processor \u2013 logic gates, registers, and memories. Readers who have\nalready taken a digital design course can skip this chapter. Chapter 7 deals with computer\narithmetic. It introduces detailed algorithms for addition, subtraction, multiplication, and\ndivision for both integers as well as real numbers. Most computers today perform very heavy 41 (cid:13)c Smruti R. Sarangi\nnumericalcomputations. Hence,itisnecessarytoobtainafirmunderstandingofhownumerical\noperations are actually implemented, and get an idea of the tradeoffs of different designs.\nAfter these two chapters, we would be ready to actually design a simple processor in Chap-\nter 8. We shall assemble a simple processor part by part, and then look at two broad design\nstyles \u2013 hardwired, and micro-programmed. Modern processors are able to process many in-\nstructionssimultaneously,andhavecomplexlogicfortakingthedependencesacrossinstructions\ninto account. The most popular technique in this area is known as pipelining. We shall discuss\npipelining in detail in Chapter 9.\n1.8.3 Processing More Information\nBythispoint, wewouldhavegottenafairunderstandingofhowsimpleprocessorsaredesigned.\nWeshallproceedtooptimisethedesign,addextracomponents,andmakeafullsystemthatcan\nsupport all the programs that users typically want to run. We shall describe three subsystems\n\u2013\nMemory System We shall see in Chapter 10 that it is necessary to build a fast and efficient\nmemory system, because it is a prime driver of performance. To build a fast memory\nsystem, we need to introduce many new structures and algorithms.\nMultiprocessors Nowadays, vendors are incorporating multiple processors on a single chip.\nThe future belongs to multiprocessors. The field of multiprocessors is very extensive and\ntypicallyformsthecoreofanadvancedarchitecturecourse. Inthisbook, weshallprovide\na short overview of multiprocessors in Chapter 11.\nI\/O and Storage In Chapter 12, we shall look at methods to interface with different I\/O\ndevices, especially storage devices such as the hard disk. The hard disk saves all our\nprograms and data when the computer is powered off, and it also plays a crucial role in\nsupplying data to our programs during their operations. Hence, it is necessary to study\nthe structure of the hard disk, and optimise it for performance and reliability.\n1.9 Summary and Further Reading\n1.9.1 Summary\nSummary 1\n1. A computer is a dumb device as compared to the human brain. However, it can\nperform routine, simple and monotonic tasks, very quickly.\n2. A computer is defined as a device that can be programmed to process information.\n3. A program consists of basic instructions that need to be executed by a computer. (cid:13)c Smruti R. Sarangi 42\n4. The semantics of all the instructions supported by a computer is known as the in-\nstruction set architecture (ISA).\n5. Ideally, an ISA should be complete, concise, simple, and generic.\n6. An ISA is complete, if it is equivalent to an universal Turing machine.\n7. A practical implementation of any complete ISA requires:\n(a) A memory to hold instructions and data.\n(b) A CPU to process instructions and perform arithmetic and logical operations.\n(c) A set of I\/O devices for communicating with the programmer.\n8. Harvard and Von Neumann architectures are practical implementations of complete\nISAs, and are also the basis of modern computers.\n9. Modern processors typically have a set of registers, which are a set of named storage\nlocations. They allow the processor to access data quickly by avoiding time consuming\nmemory accesses.\n10. Some early processors also had a stack to evaluate arithmetic expressions, and had\naccumulators to store operands and results.\n1.9.2 Further Reading\nThe field of computer architecture is a very exciting and fast moving field. The reader can\nrefer to the books by Jan Bergstra [Bergstra and Middelburg, 2012] and Gilreath [Gilreath and\nLaplante, 2003] to learn more about the theory of instruction set completeness and classes of\ninstructions. The book on formal languages by by Hopcroft, Motwani, and Ulmann [Hopcroft\net al., 2006] provides a good introduction to Turing machines and theoretical computer science\nin general. To get a historical perspective, readers can refer to the original reports written by\nAlan Turing [Carpenter and Doran, 1986] and John von Neumann [von Neumann, 1945].\nExercises\nProcessor and Instruction Set\nEx. 1 \u2014 Find out the model and make of at least 5 processors in devices around you. The\ndevices can include desktops, laptops, cell phones, and tablet PCs.\nEx. 2 \u2014 MakealistofperipheralI\/Odevicesforcomputers. Keyboardsaremicearecommon\ndevices. Search for uncommon devices. (HINT: joysticks, game controllers, fax machines) 43 (cid:13)c Smruti R. Sarangi\nEx. 3 \u2014 What are the four properties of an instruction set?\nEx. 4 \u2014 Design an instruction set for a simple processor that needs to perform the following\noperations:\n1.Add two registers\n2.Subtract two registers\nEx. 5 \u2014 Design an instruction set for a simple processor that needs to perform the following\noperations:\n1.Add two registers\n2.Save a register to memory\n3.Load a register from memory\n4.Divide a value in a register by two\nEx. 6 \u2014 Designaninstructionsettoperformthebasicarithmeticoperations\u2013add, subtract,\nmultiply, and divide. Assume that all the instructions can have just one operand.\n* Ex. 7 \u2014 Consider the sbn instruction that subtracts the second operand from the first\noperand, and branches to the instruction specified by the label (third operand), if the result is\nnegative. Write a small program using only the sbn instruction to compute the factorial of a\npositive number.\n* Ex. 8 \u2014 Write a small program using only the sbn instruction to test if a number is prime.\nTheoretical Aspects of an ISA*\nEx. 9 \u2014 Explain the design of a modified Turing machine.\nEx. 10 \u2014 Prove that the sbn instruction is Turing complete.\nEx. 11 \u2014 Prove that a machine with memory load, store, branch, and subtract instructions\nis Turing complete.\n** Ex. 12 \u2014 Find out other models of universal machines from the internet and compare\nthem with Turing Machines.\nPractical Machine Models\nEx. 13 \u2014 What is the difference between the Harvard architecture and Von Neumann archi-\ntecture?\nEx. 14 \u2014 What is a register machine?\nEx. 15 \u2014 What is a stack machine?\nEx. 16 \u2014 Write a program to compute a+b+c\u2212d on a stack machine. (cid:13)c Smruti R. Sarangi 44\nEx. 17 \u2014 Write a program to compute a+b+(c\u2212d)\u22173 on a stack machine.\nEx. 18 \u2014 Write a program to compute (a+b\/c)\u2217(c\u2212d)+e on a stack machine.\n** Ex. 19 \u2014 Try to search the internet, and find answers to the following questions.\n1.When is having a separate instruction memory more beneficial?\n2.When is having a combined instruction and data memory more beneficial? Part I\nArchitecture: Software Interface\n45  2\nThe Language of Bits\nA computer does not understand words or sentences like human beings. It understands only a\nsequence of 0s and 1s. We shall see in the rest of this book that it is very easy to store, retrieve\nand process billions of 0s and 1s. Secondly, existing technologies to implement computers using\nsilicon transistors are very compatible with the notion of processing 0s and 1s. A basic silicon\ntransistor is a switch that can set the output to a logical 0 or 1, based on the input. The\nsilicon transistor is the basis of all the electronic computers that we have today right from\nprocessors in mobile phones to processors in supercomputers. Some early computers made\nin the late nineteenth century processed decimal numbers. They were mostly mechanical in\nnature. It looks like for the next few decades, students of computer architecture need to study\nthe language of 0s and 1s in great detail.\nNow, let us define some simple terms. A variable that can be either 0 or 1, is called a bit.\nA set of 8 bits is called a byte.\nDefinition 12\nBit: A variable that can have two values: 0 or 1.\nDefinition 13\nByte: A sequence of 8 bits.\nIn this chapter, we shall look at expressing different concepts in terms of bits. The first\nquestion is, \u201c what can we do with our notion of bits?\u201d. Well it turns out that we can do\neverything that we could have done if our basic circuits were able to process normal decimal\n47 (cid:13)c Smruti R. Sarangi 48\nnumbers. We can divide the set of operations into two major types \u2013 logical and arithmetic.\nLogicaloperationsexpressconceptsoftheform, \u201ctheredlightisonANDtheyellowlightison\u201d,\nor \u201cthe bank account is closed if the user is inactive AND the account is a current account.\u201d\nArithmetic operations refer to operations such as addition, multiplication, subtraction, and\ndivision.\nWe shall first look at logical operations using bits in Section 2.1. Then, we shall look\nat methods to represent positive integers using 0s and 1s in Section 2.2. A representation\nof a number using 0s and 1s is also known as a binary representation. We shall then look\nat representing negative integers in Section 2.3, representing floating point numbers(numbers\nwith a decimal point) in Section 2.4, and representing regular text in Section 2.5. Arithmetic\noperations using binary values will be explained in detail in Chapter 7.\nDefinition 14\nRepresentation of numbers or text using a sequence of 0s and 1s is known as a binary\nrepresentation.\n2.1 Logical Operations\nBinary variables (0 or 1) were first described by George Boole in 1854. He used such variables\nand their associated operations to describe logic in a mathematical sense. He defined a full\nalgebra consisting of simple binary variables, along with a new set of operators, and basic\noperations. In the honour of George Boole, a binary variable is also known as a Boolean\nvariable, and an algebraic system of Boolean variables is known as Boolean algebra.\nHistorical Note 1\nGeorge Boole(1815 \u2013 1864) was a professor of mathematics at Queen\u2019s college, Cork, Ire-\nland. He proposed his theory of logic in his book \u2013 An Investigation of the Laws of Thought,\non Which are Founded the Mathematical Theories of Logic and Probabilities. During his\nlifetime, the importance of his work was not recognised. It was only in 1937 that Claude\nShannon observed that it is possible to describe the behavior of electronic digital circuits\nusing Boole\u2019s system.\nDefinition 15\nBoolean variable A variable that can take only two values \u2013 0 or 1.\nBoolean algebra An algebraic system consisting of Boolean variables and some special\noperators defined on them. 49 (cid:13)c Smruti R. Sarangi\n2.1.1 Basic Operators\nAsimpleBooleanvariablecantaketwovalues\u20130or1. Itcorrespondstotwostatesofasystem.\nFor example, it can represent the fact that a light bulb is off(0) or on(1). It is easy to represent\naBooleanvariableinanelectroniccircuit. Ifthevoltageonawireis0, thenwearerepresenting\na logical 0. If the voltage is equal to the supply voltage V , then we are representing a logical\ndd\n1. We shall have an opportunity to read more about electronic circuits in Chapter 6.\nLet us consider a simple Boolean variable, A. Let us assume that A represents the fact that\na light bulb is on. If A = 1, then the bulb is on, else it is off. Then the logical complement\nor negation of A, represented by A, represents the fact that the bulb is off. If A = 1, then the\nbulb is off, otherwise, it is on. The logical complement is known as the NOT operator. Any\nBoolean operator can be represented by the means of a truth table, which lists the outputs of\nthe operator for all possible combinations of inputs. The truth table for the NOT operator is\nshown in Table 2.1.\nA A\n0 1\n1 0\nTable 2.1: Truth table for the NOT operator\nLet us now consider multiple Boolean variables. Let us consider the three bulbs in a typical\ntrafficlight\u2013red, yellow, green. LettheirstatesatagiventimetberepresentedbytheBoolean\nvariables \u2013 R, Y, and G \u2013 respectively. At any point of time, we want one and only one of the\nlights to be on. Let us try to represent the first condition (one light on) symbolically using\nBoolean logic. We need to define the OR operator that represents the fact that either of the\noperandsisequalto1. Forexample, AORB, isequalto1, ifA = 1orB = 1. Twosymbolsfor\nthe OR operator are used in literature \u2013 \u2019+\u2019 and \u2019\u2228\u2019. In most cases \u2019+\u2019 is preferred. The reader\nneeds to be aware that \u2019+\u2019 is not the same as the addition operator. The correct connotation\nfor this operator needs to be inferred from the context. Whenever, there is a confusion, we will\nrevert to the \u2228 operator in this book. By default, we will use the \u2019+\u2019 operator to represent\nBoolean OR. Thus, condition 1 is: R+Y +G = 1. The truth table for the OR operator is\nshown in Table 2.2.\nA B A OR B\n0 0 0\n0 1 1\n1 0 1\n1 1 1\nTable 2.2: Truth table for the OR operator\nNow, let us try to formalise condition 2. This states that only one light needs to be on. We\ncan alternatively say that it is not possible to find a pair of bulbs that are on together. We (cid:13)c Smruti R. Sarangi 50\nA B A AND B\n0 0 0\n0 1 0\n1 0 0\n1 1 1\nTable 2.3: Truth table for the AND operator\nneed to define a new operator called the AND operator (represented by \u2019.\u2019 or \u2019\u2227\u2019). A AND B\nis equal to 1, when both A and B are 1. The truth table for the AND operator is shown in\nTable 2.3. Now, R.Y represents the fact that both the red and yellow bulbs are on. This is\nnot possible. Considering all such pairs, we have condition 2 as: R.Y +R.G+G.Y = 0. This\nformula represents the fact that no two pairs of bulbs are on simultaneously.\nWe thus observe that it is possible to represent complex logical statements using a combi-\nnation of Boolean variables and operators. We can say that NOT, AND, and OR, are basic\noperators. We can now derive a set of operators from them.\n2.1.2 Derived Operators\nTwosimpleoperatorsnamelyNANDandNORareveryuseful. NANDisthelogicalcomplement\nof AND (truth table in Table 2.4) and NOR is the logical complement of OR (truth table in\nTable 2.5).\nA B A NAND B\nA B A NOR B\n0 0 1\n0 0 1\n0 1 1\n0 1 0\n1 0 1\n1 0 0\n1 1 0\n1 1 0\nTable2.4: TruthtablefortheNANDoperator\nTable 2.5: Truth table for the NOR operator\nNAND and NOR are very important operators because they are known as universal oper-\nators. We can use just the NAND operator or just the NOR operator to construct any other\noperator. For more details the reader can refer to Kohavi and Jha [Kohavi and Jha, 2009].\nLet us now define the XOR operator that stands for exclusive-or. A XOR B is equal to 1,\nwhen A = 1,B = 0, or A = 0,B = 1. The truth table is shown in Table 2.6. They symbol for\nXOR is \u2295. The reader can readily verify that A\u2295B = A.B+A.B by constructing truth tables.\n2.1.3 Boolean Algebra\nGiven Boolean variables and basic operators, let us define some rules of Boolean algebra. 51 (cid:13)c Smruti R. Sarangi\nA B A XOR B\n0 0 0\n0 1 1\n1 0 1\n1 1 0\nTable 2.6: Truth table for the XOR operator\nNOT Operator\nLet us look at some rules governing the NOT operator.\n1. Definition: 0 = 1, and 1 = 0 \u2013 This is the definition of the NOT operator.\n2. Double negation: A = A \u2013 The NOT of (NOT of A) is equal to A itself.\nOR and AND Operators\n1. Identity: A+0 = A, and A.1 = A \u2013 If we compute the OR of a Boolean variable, A,\nwith 0, or AND with 1, the result is equal to A.\n2. Annulment: A+1 = 1, and A.0 = 0 \u2013 If we compute A OR 1, then the result is always\nequal to 1. Similarly, A AND 0, is always equal to 0 because the value of the second\noperand determines the final result.\n3. Idempotence: A+A = A, and A.A = A \u2013 The result of computing the OR or AND of\nA with itself, is A.\n4. Complementarity: A+A = 1, and A.A = 0 \u2013 Either A = 1, or A = 1. In either case\nA+A will have one term, which is 1, and thus the result is 1. Similarly, one of the terms\nin A.A is 0, and thus the result is 0.\n5. Commutativity: A.B = B.A, and A+B = B +A \u2013 The order of Boolean variables\ndoes not matter.\n6. Associativity: A+(B +C) = (A+B)+C, and A.(B.C) = (A.B).C \u2013 We are free to\nparenthesise expressions containing only OR or AND operators in any way we choose.\n7. Distributivity: A.(B +C) = A.B +A.C, and A+B.C = (A+B).(A+C) \u2013 We can\nuse this law to open up a parenthesis and simplify expressions.\nWe can use these rules to manipulate expressions containing Boolean variables in a variety\nof ways. Let us now look at a basic set of theorems in Boolean algebra. (cid:13)c Smruti R. Sarangi 52\n2.1.4 De Morgan\u2019s Laws\nThere are two De Morgan\u2019s laws that can be readily verified by constructing truth tables for\nthe LHS and RHS.\nA+B = A.B (2.1)\nThe NOT of (A+B) is equal to the AND of the complements of A and B.\nAB = A+B (2.2)\nThe NOT of (A.B) is equal to the OR of the complements of A and B.\nExample 5\nProve the consensus theorem: X.Y +X.Z +Y.Z = X.Y +X.Z.\nAnswer:\nX.Y +X.Z +Y.Z = X.Y +X.Z +(X +X).Y.Z\n= X.Y +X.Z +X.Y.Z +X.Y.Z\n(2.3)\n= X.Y.(1+Z)+X.Z.(1+Y)\n= X.Y +X.Z\nExample 6\nProve the theorem: (X +Z).(X +Y) = X.Y +X.Z.\nAnswer:\n(X +Z).(X +Y) = X.X +X.Y +Z.X +Z.Y\n= 0+X.Y +X.Z +Y.Z\n(2.4)\n= X.Y +X.Z +Y.Z\n= X.Y +X.Z (see Example 5)\n2.1.5 Logic Gates\nLet us now try to implement circuits to realise complex Boolean formulae. We will discuss\nmore about this in Chapter 6. We shall just provide a conceptual treatment in this section.\nLet us define the term \u201clogic gate\u201d as a device that implements a Boolean function. It can be\nconstructed from silicon, vacuum tubes, or any other material. 53 (cid:13)c Smruti R. Sarangi\nDefinition 16\nA logic gate is a device that implements a Boolean function.\nGiven a set of logic gates, we can design a circuit to implement any Boolean function. The\nsymbols for different logic gates are shown in Figure 2.1.\nNOT\nA.B A.B\nAND NAND\nA+B A+B\nOR NOR\nA B A B\nXOR XNOR\nFigure 2.1: List of logic gates\n2.1.6 Implementing Boolean Functions\nLet us now consider a generic boolean function f(A,B,C...). To implement it we need to\ncreate a circuit out of logic gates. Our aim should be to minimise the number of gates to save\narea, power, and time. Let us first look at a brute force method of implementing any Boolean\nfunction.\nSimple Method\nWe can construct the truth table of the function, and then try to realise it with an optimal\nnumber of logic gates. The reason we start from a truth table is as follows. In some cases,\nthe Boolean function that we are trying to implement might not be specified in a concise form.\nIt might be possible to simplify it significantly. Secondly, using truth tables ensures that the (cid:13)c Smruti R. Sarangi 54\nprocess can be automated. For example, let us consider the following truth table of some\nfunction, f. We show only those lines that evaluate to 1.\nA B C Result\n1 1 0 1\n1 1 1 1\n1 0 1 1\nLet us consider the first line. It can be represented by the Boolean function A.B.C. Sim-\nilarly, the second and third lines can be represented as A.B.C and A.B.C respectively. Thus,\nthe function can be represented as:\nf(A,B,C) = A.B.C +A.B.C +A.B.C (2.5)\nWe see that we have represented the function as an OR function of several terms. This\nrepresentationisknownasasum-of-productsrepresentation,orarepresentationinthecanonical\nform. Each such term is known as a minterm. Note that in a minterm, each variable appears\nonly once. It is either in its original form or in its complemented form.\nDefinition 17\nLet us consider a Boolean function f with n arguments.\nminterm A minterm is an AND function on all n Boolean variables, where each vari-\nable appears only once (either in its original form or in its complemented form). A\nminterm corresponds to one line in the truth table, whose result is 1.\nCanonical representation It is a Boolean formula, which is equivalent to the function\nf. It computes an OR operation of a set of minterms.\nTosummarise,toimplementatruthtable,wefirstgetalistofmintermsthatmightevaluate\nto a logical 1 (true), then create a canonical representation, and then realise it with logic\ngates. To realise the canonical representation using logic gates, we need to realise each minterm\nseparately, and then compute an OR operation.\nThis process works, but is inefficient. The formula: A.B.C + A.B.C + A.B.C, can be\nsimplified as A.B + A.B.C. Our simple approach is not powerful enough to simplify this\nformula.\nKarnaugh Maps\nInsteadofdirectlyconvertingthecanonicalrepresentationintoacircuit, letusbuildastructure\ncalled a Karnaugh map. This is a rectangular grid of cells, where each cell represents one\nminterm. To construct a Karnaugh map, let us first devise a method of numbering each\nminterm. Letusfirstrepresentallmintermssuchthattheorderofvariablesinthemisthesame\n(original or complemented). Second, if a variable is not complemented, then let us represent it 55 (cid:13)c Smruti R. Sarangi\nMinterm Representation\nA.B.C 000\nA.B.C 001\nA.B.C 010\nA.B.C 100\nA.B.C 011\nA.B.C 101\nA.B.C 110\nA.B.C 111\nTable 2.7: Representation of minterms\nby 1, otherwise, let us represent it by 0. Table 2.7 shows the representation of all the possible\n8 minterms in a three variable function.\nNow, given the representation of a minterm we use some bits to specify the row in the\nKarnaughmap,andtherestofthebitstospecifythecolumn. Wenumbertherowsandcolumns\nsuch that adjacent rows or columns differ in the value of only one variable. We treat the last\nrow, and the first row as adjacent, and likewise, treat the first and last columns as adjacent.\nThismethodofnumberingensuresthatthedifferenceinrepresentationacrossanytwoadjacent\ncells (same row, or same column) in the Karnaugh map is in only one bit. Moreover, this also\nmeans that the corresponding minterms differ in the value of only one variable. One minterm\ncontains the variable in its original form, and the other contains it in its complemented form.\nAB\nC 00 01 11 10\n0\n1\nMinterm is equal to 1\nMinterm is equal to 0\nFigure 2.2: Karnaugh Map for f(A,B,C) = A.B.C(110)+A.B.C(111)+A.B.C(101)\nNow, let us proceed to simplify or minimise the function. We construct the Karnaugh map\nas shown in Figure 2.2 for our simple function f(A,B,C) = A.B.C + A.B.C + A.B.C. We\nmark all the cells(minterms) that are 1 using a dark colour. Let us consider the first minterm,\nA.B.C. Its associated index is 110. We thus, locate the cell 110 in the Karnaugh map, and\nmark it. Similarly, we mark the cells for the other minterms \u2013 A.B.C(111), and A.B.C(101). (cid:13)c Smruti R. Sarangi 56\nWe see that we have three marked cells. Furthermore, since adjacent cells differ in the value\nof only one variable, we can combine them to a single Boolean expression. In Figure 2.2, we\ntry to combine the cells with indices 110, and 111. They differ in the value of the Boolean\nvariable, C. After combining them, we have the boolean expression: A.B.C +A.B.C = A.B.\nWe have thus replaced two minterms by a smaller yet equivalent Boolean expression. We were\nable to combine the two adjacent cells, because they represented a logical OR of the Boolean\nexpressions, which had the variable C in both its original and complemented form. Hence, the\nfunction f gets minimised to A.B+A.B.C.\nAB\nC 00 01 11 10\n0\n1\nMinterm is equal to 1\nMinterm is equal to 0\nFigure 2.3: Karnaugh Map for f(A,B,C) = A.B.C(110)+A.B.C(111)+A.B.C(101)\nInstead of combining, two cells in the same column, let us try to combine two cells in\nthe same row as shown in Figure 2.3. In this case, we combine the minterms, A.B.C, and\nA.B.C. Since the variable B is present in both its original and complemented forms, we can\neliminate it. Thus, the Boolean expression denoting the combination of the cells is A.C. Hence,\nfunction f is equal to A.C +A.B.C. We can readily verify that both the representations for f\n\u2013 (A.C +A.B.C) and (A.B +A.B.C), are equivalent and optimal in terms of the number of\nBoolean terms.\nNotethatwecannotarbitrarilydrawrectanglesintheKarnaughmap. Theycannotinclude\nany minterm that evaluates to 0 in the truth table. Secondly, the size of each rectangle needs\nto be a power of 2. This is because to remove n variables from a set of m minterms, we need\nto have all combinations of the n variables in the rectangle. It thus needs to have 2n minterms.\nTo minimise a function, we need to draw rectangles that are as large as possible. It is\npossible that two rectangles might have an overlap. However, one rectangle should not be a\nstrict subset of the other.\n2.1.7 The Road Ahead\nWay Point 2 57 (cid:13)c Smruti R. Sarangi\n\u2022 Boolean algebra is a symbolic algebra that uses Boolean variables, which can be either\n0 or 1.\n\u2022 The basic Boolean operators are AND, OR, and NOT.\n\u2022 These operators are associative, commutative, and reflexive.\n\u2022 NAND, NOR, XOR are very useful Boolean operators.\n\u2022 De Morgan\u2019s laws help convert an expression with an AND operator, to an expression\nthat replaces it with an OR operator.\n\u2022 A logic gate is a physical realisation of a simple Boolean operator or function.\n\u2022 Our aim is to minimise the number of logic gates while designing a circuit for a\nBoolean function.\n\u2022 One effective way of minimising the number of logic gates is by using Karnaugh maps.\nUptillnow,wehavelearntaboutthebasicpropertiesofBooleanvariables,andasimplemethod\ntodesignefficientcircuitstorealiseBooleanfunctions. AnextensivediscussiononBooleanlogic\nor optimal circuit synthesis is beyond the scope of this book. Interested readers can refer to\nseminal texts by Zvi Kohavi [Kohavi and Jha, 2009] and [Micheli, 1994].\nNevertheless, we are at a position to appreciate the nature of Boolean circuits. Up till now,\nwe have not assigned a meaning to sets of bits. We shall now see that sequences of bits can\nrepresent integers, floating point numbers, and strings(pieces of text). Arithmetic operations\non such sequences of bits are described in detail in Chapter 7.\n2.2 Positive Integers\n2.2.1 Ancient Number Systems\nEversincemandevelopedhigherintelligence, hehasfacedtheneedtocount. Fornumbersfrom\none to ten, human beings can use their fingers. For example, the little finger of the left hand\ncan signify one, and the little finger of the right hand can signify ten. However, for counting\nnumbers greater than ten, we need to figure out a way for representing numbers. In the ancient\nworld, two number systems prevailed \u2013 the Roman numerals used in ancient Rome, and the\nIndian numerals used in the Indian subcontinent.\nThe Roman numerals used the characters \u2013 I, II ... X, for the numbers 1...10 respectively.\nHowever, there were significant issues for representing numbers greater than ten. For example,\nto represent 50, 100, 500, and 1000, Romans used the symbols L, C, D, and M respectively. To\nrepresentalargenumber, theRomansrepresenteditasasequenceofsymbols. Thenumber204\ncan be represented as CCIV (C + C + IV = 100 + 100 + 4). Hence, to derive the real value\nof a number, we need to scan the number from left to right, and keep on adding the values. To\nmakethingsfurthercomplicated,thereisanadditionalrulethatifasmallernumberispreceded (cid:13)c Smruti R. Sarangi 58\nby a larger value, then we need to subtract it from the total sum. Note that there is no notion\nof negative numbers, and zero in this number system. Furthermore, it is extremely difficult to\nrepresent large numbers, and perform simple operations such as addition and multiplication.\nTheancientIndiansusedanumbersystemthatwassignificantlysimpler,andfundamentally\nmorepowerful. TheArabscarriedthenumbersystemtoEuropesometimeafterseventhcentury\nAD, and thus this number system is popularly known as the Arabic number system. The magic\ntricks used by ancient Indian mathematicians are the number \u20190\u2019, and the place value system.\nTheIndianmathematiciansusedasequenceoftensymbolsincludingzero,asthebasicalphabet\nfor numbers. Figure 2.4 shows ten symbols obtained in the Bakhshali manuscript obtained in\nthe north west frontier province of modern Pakistan (dated seventh century AD). Each such\nsymbol is known as a \u2018digit\u2019.\nFigure 2.4: Numerals from the Bakhshali Manuscript (source Wikipedia(cid:13)R ) This article uses\nmaterial from the Wikipedia article \u201cBakhshali Manuscript\u201d [bak, ], which is released under\nthe Creative Commons Attribution-Share-Alike License 3.0 [ccl, ]\nEvery number was represented as a sequence of digits. Each digit represents a number\nbetween zero and nine. The first problem is to represent a number that is greater than nine\nby one unit. This is where we use the place value system. We represent it as 10. The left\nmost number, 1, is said to be in the ten\u2019s place, and the right most number, 0, is in the unit\u2019s\nplace. We can further generalise this representation to any two digit number of the form, x x .\n2 1\nThe value of the number is equal to 10\u00d7x +x . As compared to the Roman system, this\n2 1\nrepresentation is far more compact, and can be extended to represent arbitrarily large integers.\nA number of the form x x ...x is equal to x \u00d710n\u22121 +x \u00d710n\u22121 +...+x =\nn n\u22121 1 n n\u22121 1\n(cid:80)n x 10i\u22121. Each decimal digit is multiplied with a power of 10, and the sum of the products\ni=1 i\nis equal to the value of the number. As we have all studied in elementary school, this number\nsystem makes the job of addition, multiplication, and subtraction substantially easier. In this\ncase, the number \u201810\u2019, is known as the base of the number system.\nHistorical Note 2\nThe largest number known to ancient Indian mathematicians was 1053 [ind, ].\nLet us now ponder about a basic point. Why did the Indians choose ten as the base. They\nhad the liberty to choose any other number such as seven or eight or nine. The answer can\nbe found by considering the most basic form of counting again, i.e., with fingers. Since human\nbeings have ten fingers, they use them to count till one to ten, or from zero to nine. Hence,\nthey were naturally inclined to use ten as the base.\nLet us now move to a planet, where aliens have seven fingers. It would not be surprising\nto see them use a base seven number system. In their world, a number of the form, 56, would 59 (cid:13)c Smruti R. Sarangi\nactually be 7\u00d75+6 in our number system. We thus observe that it is possible to generalise\nthe concept of a base, and it is possible to represent any number in any base. We introduce the\nnotation 3243 , which means that the number 3243 is being represented in base 10.\n10\nExample 7\nThe number 1022 is equal to : 83+0+2\u221781+2 = 530 .\n8 10\n2.2.2 Binary Number System\nWhat if we consider a special case? Let us try to represent numbers in base 2. The number 7\n10\ncan be represented as 111 , and 12 is equal to 1100 . There is something interesting about\n2 10 2\nthisnumbersystem. Everydigitiseither0or1. AsweshallseeinChapters6and7, computers\nare best suited to process values that are either 0 or 1. They find it difficult to process values\nfrom a larger set. Hence, representing numbers in base 2 should be a natural fit for computers.\nWe call this a binary number system (see Definition 18). Likewise, a number system that uses\na base of 10, is known as a decimal number system.\nDefinition 18\n\u2022 A number system based on Indian numerals that uses a base equal to 2, is known as\na binary number system.\n\u2022 A number system based on Indian numerals that uses a base equal to 10, is known as\na decimal number system.\nFormally, any number A can be represented as a sequence of n binary digits:\nn\n(cid:88)\nA = x 2i\u22121 (2.6)\ni\ni=1\nHere, x ...x arebinarydigits(0or1). Werepresentanumberasasumofthepowersof2,\n1 n\nas shown in Equation 2.6. The coefficients of the equation, are the binary digits. For example,\nthe decimal number 23 is equal to (16 + 4 + 2 + 1) = 1\u00d724 +0\u00d723 +1\u00d722 +1\u00d72+1.\nThus, its binary representation is 10111.\nLet us consider some more examples, as shown in Table 2.8. (cid:13)c Smruti R. Sarangi 60\nNumber in decimal Number in binary\n5 101\n100 1100100\n500 111110100\n1024 10000000000\nTable 2.8: Examples of binary numbers\nExample 8\nConvert the decimal number 27 to binary.\nAnswer: 27 = 16 + 8 + 0 + 2 + 1 = 11011\n2\n(cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125)\n1 1 0 1 1\nLet us now define two more terms, the most significant bit (MSB), and the least significant\nbit (LSB). The LSB is the rightmost bit, and the MSB is the leftmost bit.\nDefinition 19\n\u2022 MSB (Most Significant Bit) : The leftmost bit of a binary number. For example the\nMSB of 1110 is 1.\n\u2022 LSB (Least Significant Bit) : The rightmost bit of a binary number. For example the\nLSB of 1110 is 0.\nHexadecimal and Octal Numbers\nIf we have a 32-bit number system, then representing each number in binary will take 32 binary\ndigits (0\/1). For the purposes of explanation, this representation is unwieldy. We can thus\nmake our representation more elegant by representing numbers in base 8 or base 16. We shall\nsee that there is a very easy method of converting numbers in base 8, or base 16, to base 2.\nNumbers represented in base 8 are known as octal numbers. They are traditionally repre-\nsented by adding a prefix, \u20190\u2019. The more popular representation is the hexadecimal number\nsystem. It uses a base equal to 16. We shall use the hexadecimal representation extensively in\nthis book. Numbers in this format are prefixed by \u20180x\u2019. Secondly, the word \u2018hexadecimal\u2019 is\npopularly abbreviated as \u2018hex\u2019. Note that we require 16 hex digits. We can use the digits 0-9\nfor the first ten digits. The next six digits require special characters. These six characters are\ntypically \u2013 A (10), B(11), C(12), D(13), E(14), and F(15). We can use the lower case versions\nof ABCDEF also. 61 (cid:13)c Smruti R. Sarangi\nTo convert a binary number (A) to a hexadecimal number, or do the reverse, we can use\nthe following relationship:\nn\n(cid:88)\nA = x 2i\u22121\ni\ni=1\nn\/4\n(cid:88)\n= (23\u00d7x +22\u00d7x +21\u00d7x +x )\u00d724(j\u22121)\n4(j\u22121)+4 4(j\u22121)+3 4(j\u22121)+2 4(j\u22121)+1\nj=1\n(2.7)\nn\/4\n(cid:88)\n= (23\u00d7x +22\u00d7x +21\u00d7x +x )\u00d724(j\u22121)\n4(j\u22121)+4 4(j\u22121)+3 4(j\u22121)+2 4(j\u22121)+1\nj=1(cid:124) (cid:123)(cid:122) (cid:125)\nyj\nn\/4\n(cid:88)\n= y 16(j\u22121)\nj\nj=1\nWecanthusrepresentthenumber(A)inbase16(hexadecimalnotation)bycreatinggroups\nof four consecutive binary digits. The first group is comprised of the binary digits x x x x ,\n4 3 2 1\nthe second group is comprised of x x x x and so on. We need to convert each group of 4\n8 7 6 5\nbinary digits, to represent a hexadecimal digit (y ). Similarly, for converting a number from\nj\nhextobinary, weneedtoreplaceeachhexdigitwithasequenceof4binarydigits. Likewise, for\nconverting numbers from binary to octal and back, we need to consider sequences of 3 binary\ndigits.\nExample 9\nConvert 110001010 to the octal format.\n2\nAnswer: 110 001 010 \u2192 0612\n(cid:124)(cid:123)(cid:122)(cid:125)(cid:124)(cid:123)(cid:122)(cid:125)(cid:124)(cid:123)(cid:122)(cid:125)\n6 1 2\nExample 10\nConvert 110000101011 to the hexadecimal format.\nAnswer: 110000101011 \u2192 0xC2B\n(cid:124)(cid:123)(cid:122)(cid:125)(cid:124)(cid:123)(cid:122)(cid:125)(cid:124)(cid:123)(cid:122)(cid:125)\nC 2 B\n2.2.3 Adding Binary Numbers\nAdding binary numbers is as simple as adding decimal numbers. For adding decimal numbers,\nwestartfromtherightmostpositionandadddigitbydigit. Ifthesumexceeds10,thenwewrite\nthe unit\u2019s digit at the respective position in the result, and carry the value at the ten\u2019s place\nto the next position in the result. We can do something exactly similar for binary numbers. (cid:13)c Smruti R. Sarangi 62\nLet us start out by trying to add two 1-bit binary numbers, A and B. Table 2.9 shows\nthe different combinations of numbers and results. We observe that for two bits, a carry is\ngenerated only when the input operands are both equal to 1. This carry bit needs to be added\nto the bits in the higher position. At that position, we need to add three bits \u2013 two input\noperand bits and a carry bit. This is shown in Figure 2.5. In this figure, the input operand bits\nare designated as A and B. The input carry bit is designated as C . The result will have two\nin\nbits in it. The least significant bit (right most bit) is known as the sum, and the output carry\nis referred to as C .\nout\nTable 2.10 shows the results for the different combinations of input and carry bits.\nA B (A+B)\n2\n0 0 00\n0 1 01\n1 0 01\n1 1 11\nTable 2.9: Addition of two binary bits\nA C\nin\nB\nC sum\nout\nFigure 2.5: Addition of two binary bits and a carry bit\nA B C Sum C\nin out\n0 0 0 0 0\n0 1 0 1 0\n1 0 0 1 0\n1 1 0 0 1\n0 0 1 1 0\n0 1 1 0 1\n1 0 1 0 1\n1 1 1 1 1\nTable 2.10: A truth table that represents the addition of three bits 63 (cid:13)c Smruti R. Sarangi\nLet us now try to add two n-bit binary numbers. Our addition needs to proceed exactly\nthe same ways as decimal numbers. We add the values at a position, compute the result, and\ncarry a value to the next (more significant) position. Let us explain with an example (see\nExample 11).\nExample 11\nAdd the two binary numbers, 1011 and 0011.\nAnswer: The process of addition is shown in the figure, and the values of the intermediate\nvalues of the carry bits are shown in shaded boxes. Let us now verify if the result of the\naddition is correct. The two numbers expressed in the decimal number system are 11 and 3.\n11 + 3 = 14. The binary representation of 14 is 1110. Thus, the computation is correct.\n0 1 1\n1 0 1 1\n0 0 1 1\n1 1 1 0\n2.2.4 Sizes of Integers\nNote that up till now we have only considered positive integers. We shall consider negative\nintegers in Section 2.3. Such positive integers are known as unsigned integers in high level\nprogramming languages such as C or C++. Furthermore, high level languages define three\ntypes of unsigned integers \u2013 short (2 bytes), int (4 bytes), long long int (8 bytes). A short\nunsigned integer is represented using 16 bits. Hence, it can represent all the integers from 0 to\n216\u22121 (for a proof, see Example 12). Likewise, a regular 32-bit unsigned integer can represent\nnumbers from 0 till 232\u22121. The ranges of each data type are given in Table 2.11.\nExample 12\nCalculate the range of unsigned 2-byte short integers.\nAnswer: A short integer is represented by 16 bits. The smallest short integer is repre-\nsented by 16 zeros. It has a decimal value equal to 0. The largest short integer is represented\nby all 1s. Its value, V, is equal to 215 +...+20 = 216 \u22121. Hence, the range of unsigned\nshort integers is 0 to 216\u22121. (cid:13)c Smruti R. Sarangi 64\nExample 13\nCalculate the range of an n-bit integer.\nAnswer: 0 to 2n\u22121.\nExample 14\nWe need to represent a set of decimal numbers from 0 till m\u22121. What is the minimum\nnumber of binary bits that we require?\nAnswer: Let us assume that we use n binary bits. The range of numbers that we can\nrepresent is 0 to 2n \u22121. We note that 2n \u22121 needs to be at least as large as m. Thus, we\nhave:\n2n\u22121 \u2265 m\u22121\n\u21d2 2n \u2265 m\n\u21d2 n \u2265 log (m)\n2\n\u21d2 n \u2265 (cid:100)log (m)(cid:101)\n2\nHence, the minimum number of bits that we require is (cid:100)log (m)(cid:101).\n2\nData Type Size Range\nunsigned short int 2 bytes 0 to 216\u22121\nunsigned int 4 bytes 0 to 232\u22121\nunsigned long long int 8 bytes 0 to 264\u22121\nTable 2.11: Ranges of unsigned integers in C\/C++\nImportant Point 5\nFor the more mathematically inclined, we need to prove that for a n bit integer, there is a\none to one mapping between the set of n bit binary numbers, and the decimal numbers, 0\nto 2n\u22121. In other words, every n bit binary number has a unique decimal representation.\nWe leave this as an exercise for the reader. 65 (cid:13)c Smruti R. Sarangi\n2.3 Negative Integers\nWe represent a negative decimal number by adding a \u2018-\u2019 sign before it. We can in principle do\nthe same with a binary number, or devise a better representation.\nLet us consider the generic problem first. For a number system comprising of a set of\nnumbers, S (both positive and negative), we wish to create a mapping between each number\nin S, and a sequence of zeros and ones. A sequence of zeros and ones can alternatively be\nrepresented as an unsigned integer. Thus, putting it formally, we propose to devise a method\nfor representing both positive and negative integers as a function F : S \u2192 N that maps a\nset of numbers, S, to a set of unsigned integers, N. Let us define the function SgnBit(u) of\na number, u. It is equal to 1 when u is negative, and equal to 0 when u is positive or zero.\nSecondly, unless specified otherwise, we assume that all our numbers require n bits per storage\nin the next few subsections.\n2.3.1 Sign-Magnitude based Representation\nWecanreserveabitforrepresentingthesignofanumber. Ifitisequalto0, thenthenumberis\npositive, elseitisnegative. Thisisknownasthesign-magnituderepresentation. Letusconsider\nan n bit integer. We can use the MSB as the designated signed bit, and use the rest of the\nnumber to represent the number\u2019s magnitude. The magnitude of a number is represented using\nn\u22121 bits. This is a simple and intuitive representation. In this representation, the range of\nthe magnitude of a n bit integer is from 0 till 2n\u22121\u22121. Hence, the number system has a range\nequal to \u00b1(2n\u22121 \u22121). Note that there are two zeros \u2013 a positive zero(00...0) and a negative\nzero(10...0).\nFormally, the mapping function \u2013 F(u) \u2013 where u is a number in the range of the number\nsystem, is shown in Equation 2.8.\nF(u) = SgnBit(u)\u00d72n\u22121+ | u | (2.8)\nFor example, if we consider a 4-bit number system, then we can represent the number -2 as\n1010 . Here, the MSB is 1 (represents a negative number), and the magnitude of the number\n2\nis 010, which represents 2.\nThe issues with this system are that it is difficult to perform arithmetic operations such as\naddition, subtraction, and multiplication. For example in our 4-bit number system, -2 + 2, can\nbe represented as 1010+0010. If we naively do simple unsigned addition, then the result is\n1100, which is actually -6. This is the wrong result. We need to use a more difficult approach\nto add numbers.\n2.3.2 The 1\u2019s Complement Approach\nFor positive numbers, let us use the same basic scheme that assigns the MSB to a dedicated\nsign bit, which is 0 in this case. Moreover, let the rest of the (n\u22121) bits represent the number\u2019s\nmagnitude. For a negative number, -u(u \u2265 0), let us simply flip all the bits of +u. If a bit is\n0, we replace it by 1, and vice versa. Note that this operation flips the sign bit also, effectively\nnegating the number. The number system can represent numbers between \u00b1(2n\u22121\u22121) like the\nsign-magnitude system. (cid:13)c Smruti R. Sarangi 66\nFormally, the mapping function F is defined as:\n(cid:40)\nu u \u2265 0\nF(u) = (2.9)\n\u223c (| u |) or (2n\u22121\u2212 | u |) u < 0\nNote that a bitwise complement(\u223c) is the same as subtracting the number from 11...1\n(2n\u22121).\nLet us consider some examples with a 4-bit number system. We represent the number 2 as\n0010. Here the sign bit is 0, signifying that it is a positive number. To compute -2, we need to\nflip each bit. This process yields 1101. Note that the sign bit is 1 now.\nThe 1\u2019s complement approach also suffers from similar deficiencies as the sign magnitude\nscheme. First, there are two representations for zero. There is a positive zero - 0000, and a\nnegative zero - 1111.\nSecond, adding two numbers is difficult. Let us try to add 2 and -2. 2 + (-2) = 0010 +\n1101. Using simple binary addition, we get 1111, which is equal to 0(negative zero). Hence, in\nthis case simple binary addition works. However, now let us try to add 1 to -0. We have: -0 +\n1 = 1111 + 0001 = 0000. This leads to a mathematical contradiction. If we add one to zero,\nthe result should be one. However, in this case, it is still zero! This means that we need to\nmake the process of addition more sophisticated. This will slow down the process of addition\nand make it more complex.\n2.3.3 Bias-based Approach\nLet us adopt a different approach now. Let us assume that the unsigned representation of a\nnumber (F(u)) is given by:\nF(u) = u+bias (2.10)\nHere, bias is a constant.\nLet us consider several examples using a 4-bit number system. The range of unsigned\nnumbers is from 0 to 15. Let the bias be equal to 7. Then, the actual range of the number\nsystem is from -7 to +8. Note that this method avoids some pitfalls of the sign-magnitude\nand 1\u2019s complement approach. First, there is only one representation for 0. In this case it is\n0111. Second, it is possible to use standard unsigned binary addition to add two numbers with\na small modification.\nLet us try to add 2 and -2. 2 is represented as +9 or 1001 . Likewise, -2, is represented as\n2\n+5, or 0101 . If we add 2 and -2, we are in effect adding the unsigned numbers 5 and 9. 5 + 9\n2\n= 14. This is not the right answer. The right answer should be 0, and it should be represented\nas 0111 or +7. Nonetheless, we can get the right answer by subtracting the bias, i.e., 7. 14 - 7\n= 7. Hence, the algorithm for addition is to perform simple binary unsigned addition, and then\nsubtract the bias. Performing simple binary subtraction is also easy (need to add the bias).\nHence, in the case of addition, for two numbers, u and v, we have:\nF(u+v) = F(u)+F(v)\u2212bias (2.11)\nHowever, performing binary multiplication is difficult. The bias values will create issues. In\nthis case, if the real value of a number is A, we are representing it as A+bias. If we multiply\nA and B naively, we are in effect multiplying A+bias and B +bias. To recover the correct\nresult, AB, from (A+bias)\u00d7(B+bias) is difficult. We desire an even simpler representation. 67 (cid:13)c Smruti R. Sarangi\n2.3.4 The 2\u2019s Complement Method\nHere are the lessons that we have learnt from the sign-magnitude, 1\u2019s complement, and bias\nbased approaches.\n1. We need a representation that is simple.\n2. We would ideally like to perform signed arithmetic operations, using the same kind of\nhardware that is used for unsigned numbers.\n3. It is not desirable to have two representations for zero. The number zero, should have a\nsingle representation.\nKeeping all of these requirements in mind, the 2\u2019s complement system was designed. To\nmotivate this number system, let us consider a simple 4-bit number system, and represent the\nnumbers in a circle. Let us first consider unsigned numbers. Figure 2.6 shows the numbers\npresented in a circular fashion. As we proceed clockwise, we increment the number, and as\nwe proceed anti-clockwise, we decrement the number. This argument breaks at one point as\nshown in the figure. This is between 15 and 0. If we increment 15, we should get 16. However,\nbecause of the limited number of bits, we cannot represent 16. We can only capture its four\nlow order bits which are 0000. This condition is also called an overflow. Likewise, we can also\ndefine the term, underflow, that means that a number is too small to be represented in a given\nnumber system (see Definition 20). In this book, we shall sometimes use the word \u201coverflow\u201d\nto denote both overflow as well as underflow. The reader needs to infer the proper connotation\nfrom the context.\nDefinition 20\noverflow Anoverflowoccurswhenanumberistoolargetoberepresentedinagivennumber\nsystem.\nunderflow An underflow occurs when a number is too small to be represented in a given\nnumber system.\nLet us now take a look at these numbers slightly differently as shown in Figure 2.7. We\nconsider the same circular order of numbers. However, after 7 we have -8 instead of +8.\nHenceforth, as we travel clockwise, we effectively increment the number. The only point of\ndiscontinuity is between 7 and -8. Let us call this point of discontinuity as the \u201cbreak point\u201d.\nThis number system is known as the 2\u2019s complement number system. We shall gradually refine\nthe definition of a 2\u2019s complement number to make it more precise and generic.\nDefinition 21\nThe point of discontinuity in the number circle is called the break point. (cid:13)c Smruti R. Sarangi 68\n0000 (0)\n1111 (15)\n0001 (1)\n1110 (14)\n0010 (2)\n1101 (13)\n0011 (3)\nIncrement\n1100 (12) 0100 (4)\n0101 (5)\n1011 (11)\n0110 (6)\n1010 (10)\n0111 (7)\n1001 (9)\n1000 (8)\nFigure 2.6: Unsigned 4-bit binary numbers\n0000 (0)\n1111 (-1)\n0001 (1)\n1110 (-2)\n0010 (2)\n1101 (-3)\n0011 (3)\nIncrement\n1100 (-4) 0100 (4)\n0101 (5)\n1011 (-5)\n0110 (6)\n1010 (-6)\n0111 (7)\n1001 (-7)\n1000 (-8)\nFigure 2.7: Signed 4-bit binary numbers 69 (cid:13)c Smruti R. Sarangi\nLet us now try to understand what we have achieved through this procedure. We have\n16 numbers in the circle, and we have assigned each one of them to numbers from -8 to +7.\nEach number is represented by a 4-bit value. We observe that incrementing a signed number,\nis tantamount to incrementing its unsigned 4-bit representation. For example, -3 is represented\nas 1101. If we increment, -3, we get -2, which is represented as 1110. We also observe that 1101\n+ 1 = 1110.\nLet us now try to formalise the pattern of numbers shown in the circle in Figure 2.7. First,\nlet us try to give the circular representation a name. Let us call it a Number Circle. In a\nnumber circle, we observe that for numbers between 0 and 7, their representation is the same\nas their unsigned representation. The MSB is 0. For numbers between -8 and -1, the MSB is\n1. Secondly, the representation of a negative number, -u (u \u2265 0), is the same as the unsigned\nrepresentation for 16\u2212u.\nDefinition 22\nThe steps for creating a n bit number circle are:\n1. We start by writing 0 at the top. Its representation is a sequence of n zeros.\n2. We proceed clockwise and add the numbers 1 to (2n\u22121\u22121). Each number is represented\nby its n bit unsigned representation. The MSB is 0.\n3. We introduce a break point after 2n\u22121\u22121.\n4. Then next number is \u22122n\u22121 represented by 1 followed by n\u22121 zeros.\n5. We then proceed clockwise incrementing both the numbers, and their unsigned repre-\nsentations by 1 till we reach 0.\nWe can generalise the process of creating a number circle, to create a n bit number circle\n(see Definition 22). To add a positive number, A, to a number B, we need to proceed A steps\nin the clockwise direction from B. If A is negative, then we need to proceed A steps in the anti-\nclockwise direction. Note that moving k steps in the clockwise direction is the same as moving\n2n \u2212k steps in the anti-clockwise direction. This magical property means that subtracting k\nis the same as adding 2n\u2212k. Consequently, every subtraction can be replaced by an addition.\nSecondly, a negative number, \u2212u, is located in the number circle by moving | u | steps anti-\nclockwise from 0, or alternatively, 2n\u2212 | u | steps clockwise. Hence, the number circle assigns\nthe unsigned representation 2n\u2212 | u |, to a negative number of the form \u2212u (u \u2265 0).\nSuccinctly,anumbercirclecanbedescribedbyEquation2.12. Thisnumbersystemiscalled\na 2\u2019s complement number system.\n(cid:40)\nu 0 \u2264 u \u2264 2n\u22121\u22121\nF(u) = (2.12)\n2n\u2212 | u | \u22122n\u22121 \u2264 u < 0 (cid:13)c Smruti R. Sarangi 70\nProperties of the 2\u2019s Complement Representation\n1. There is one unique representation for 0, i.e., 000...0.\n2. The MSB is equal to the sign bit (SgnBit(u)).\nProof: Refer to the number circle. A negative number\u2019s unsigned representation is\ngreater than or equal to 2n\u22121. Hence, its MSB is 1. Likewise all positive numbers are less\nthan 2n\u22121. Hence, their MSB is 0.\n3. Negation Rule: F(\u2212u) = 2n\u2212F(u)\nProof: Ifu \u2265 0, thenF(\u2212u) = 2n\u2212u = 2n\u2212F(u)accordingto Equation 2.12. Similarly,\nif u < 0, then F(\u2212u) = |u| = 2n\u2212(2n\u2212|u|) = 2n\u2212F(u).\n4. Every number in the range [\u22122n\u22121,2n\u22121\u22121] has a unique representation.\nProof: Every number is a unique point on the number circle.\n5. Addition Rule:\nF(u+v) \u2261 F(u)+F(v) (2.13)\nFor the sake of brevity, we define the \u2261 operator. (a \u2261 b) means that (a mod 2n = b\nmod 2n). Recallthatthemodulo( mod )operatorcomputestheremainderofadivision,\nand the remainder is assumed to be always non-negative, and less than the divisor. The\nphysical significance of ( mod 2n) is that we consider the n LSB bits. This is always the\ncase because we have a n bit number system, and in all our computations we only keep\nthe n LSB bits, and discard the rest of the bits if there are any. In our number circle\nrepresentation, if we add or subtract 2n to any point (i.e. move 2n hops clockwise or\nanti-clockwise), we arrive at the same point. Hence, a \u2261 b implies that they are the same\npointonthenumbercircle,ortheirnLSBbitsarethesameintheirbinaryrepresentation.\nProof: Let us consider the point u on the number circle. Its binary representation is\nF(u). Now, if we move v points, we arrive at u+v. If v is positive, we move v steps\nclockwise; otherwise, we move v steps anticlockwise. The binary representation of the\nnew point is F(u+v).\nWe can interpret the movement on the number circle in another way. We start at u. We\nmove F(v) steps clockwise. If v \u2265 0, then v = F(v) by Equation 2.12, hence we can\nconclude that we arrive at u+v. If v < 0, then F(v) = 2n \u2212|v|. Now, moving |v| steps\nanticlockwise is the same as moving 2n\u2212|v| steps clockwise. Hence, in this case also we\narriveatu+v,whichhasabinaryrepresentationequaltoF(u+v). Since,eachstepmoved\nin a clockwise direction is equivalent to incrementing the binary representation by 1, we\ncan conclude that the binary representation of the destination is equal to: F(u)+F(v).\nSince,weonlyconsider,thelastnbits,thebinaryrepresentationisequalto(F(u)+F(v))\nmod 2n. Hence, F(u+v) \u2261 F(u)+F(v).\n6. Subtraction Rule\nF(u\u2212v) \u2261 F(u)+(2n\u2212F(v)) (2.14)\nProof: We have: 71 (cid:13)c Smruti R. Sarangi\nF(u\u2212v) \u2261 F(u)+F(\u2212v) (addition rule)\n(2.15)\n\u2261 F(u)+2n\u2212F(v) (negation rule)\n7. Loop Rule: F(u) \u2261 2n+F(u)\nProof: After moving 2n points on the number circle, we come back to the same point.\n8. Multiplication Rule: (assuming no overflows)\nF(u\u00d7v) \u2261 F(u)\u00d7F(v) (2.16)\nProof: If u and v are positive, then this statement is trivially true. If u and v are\nnegative, then we have, u = \u2212|u| and v = \u2212|v|:\nF(u)\u00d7F(v) \u2261 (2n\u2212F(|u|))\u00d7(2n\u2212F(|v|))\n\u2261 2n+1\u22122n(F(|u|)+F(|v|)+F(|u|)\u00d7F(|v|)\n\u2261 F(|u|)\u00d7F(|v|) (2.17)\n\u2261 F(|u|\u00d7|v|)\n\u2261 F(u\u00d7v)\nNow, let us assume that u is positive and v is negative. Thus, u = |u| and v = \u2212|v|. We\nhave:\nF(u)\u00d7F(v) \u2261 F(u)\u00d7(2n\u2212F(|v|))\n\u2261 2nF(u)\u2212F(u)\u00d7F(|v|)\n\u2261 \u2212F(u)\u00d7F(|v|) (loop rule)\n\u2261 \u2212(F(u\u00d7|v|)) (u \u2265 0,|v| \u2265 0)\n(2.18)\n\u2261 2n\u2212F(u\u00d7|v|) (loop rule)\n\u2261 F(\u2212(u\u00d7(|v|))) (negation rule)\n\u2261 F(u\u00d7(\u2212|v|))\n\u2261 F(u\u00d7v)\nLikewise, we can prove the result for a negative u and positive v. We have thus covered\nall the cases.\nWe thus observe that the 2\u2019s complement number system, and the number circle based\nmethod make the process of representing both positive and negative numbers easy. It has a\nunique representation for zero. It is easy to compute its sign. We just need to take a look at\nthe MSB. Secondly, addition, subtraction, and multiplication on signed numbers is as simple\nas performing the same operations on their unsigned representations. (cid:13)c Smruti R. Sarangi 72\nExample 15\nAdd 4 and -3 using a 4-bit 2\u2019s complement representation.\nAnswer: Letusfirsttrytoadditgraphically. Wecanstartat4andmove3positionsanti-\nclockwise. We arrive at 1, which is the correct answer. Let us now try a more conventional\napproach. 4 is represented as 0100, -3 is represented as 1101. If we add, 0100 and 1101\nusing a regular unsigned binary adder, the result is 10001. However, we cannot represent\n5 bits in our simple 4-bit system. Hence, the hardware will discard the fifth bit, and report\nthe result as 0001, which is the correct answer.\nComputing the 2\u2019s Complement Representation\nLetusnowtrytoexplorethemethodstocomputea2\u2019scomplementrepresentation. Forpositive\nnumbers it is trivial. However, for negative numbers of the form, -u (u \u2265 0), the representation\nis 2n\u2212u. A simple procedure is outlined in Equation 2.19.\n2n\u2212u = (2n\u22121\u2212u)+1\n(2.19)\n= (\u223c u)+1\nAccording to Equation 2.9, we can conclude that (2n\u22121\u2212u) is equivalent to flipping every\nbit, or alternatively computing \u223c u. Hence, the procedure for negating a number in the 2\u2019s\ncomplement system, is to first compute its 1\u2019s complement, and then add 1.\nThe Sign Extension Trick\nLet us assume that we want to convert a number\u2019s representation from a 16-bit number system\nto a 32-bit number system. If the number is positive, then we just need to prefix it with 16\nzeros. Let us consider the case when it is negative. Let the number again be of the form, -u\n(u \u2265 0). Its representation in 16 bits is F (u) = 216 \u2212u. Its representation using 32 bits is\n16\nF (u) = 232\u2212u.\n32\nF (u) = 232\u2212u\n32\n= (232\u2212216)+(216\u2212u)\n(2.20)\n= 11...100...0+F (u)\n16\n(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)\n16 16\nFor a negative number, we need to prepend it with 16 ones. By combining both the results,\nwe conclude that to convert a number from a 16-bit representation to a 32-bit representation,\nwe need to prepend it with 16 copies of its sign bit(MSB). 73 (cid:13)c Smruti R. Sarangi\nRange of the 2\u2019s Complement Number System\nTherangeofthenumbersystemisfrom\u22122n\u22121 to2n\u22121\u22121. Thereisoneextranegativenumber,\n\u22122n\u22121.\nChecking if a 2\u2019s Complement Addition has Resulted in an Overflow\nLet us outline the following theorem for checking if a 2\u2019s complement addition results in an\noverflow.\nTheorem 2.3.4.1 Let us consider an addition operation, where both the operands are non-\nzero. If the signs of the operands are different, then we can never have an overflow. How-\never, if the signs of the operands are the same, and the result has an opposite sign or is\nzero, then the addition has led to an overflow.\nProof: Let us consider the number circle, and an addition operation of the form A+B.\nLet us first locate point A. Then, let us move B steps clockwise if B is positive, or B steps\nanti-clockwise if B is negative. The final point is the answer. We also note that if we cross the\nbreak point (see Definition 21), then there is an overflow, because we exceed the range of the\nnumber system. Now, if the signs of A and B are different, then we need to move a minimum of\n2n\u22121+1stepstocrossthebreakpoint. Thisisbecauseweneedtomoveoverzero(1), thebreak\npoint(1), and the set of all the positive numbers (2n\u22121\u22121), or all the negative numbers (2n\u22121).\nSince, we have 1 less positive number, we need to move at least 2n\u22121 \u22121+1+1 = 2n\u22121 +1\nsteps. Since B is a valid 2\u2019s complement number, and is in the range of the number system, we\nhave | B |< 2n\u22121 +1. Hence, we can conclude that after moving B steps, we will never cross\nthe break point, and thus an overflow is not possible.\nNow, let us consider the case in which the operands have the same sign. In this case, if the\nresult has an opposite sign or is zero, then we are sure that we have crossed the break point.\nConsequently, there is an overflow. It will never be the case that there is an overflow and the\nresult has the same sign. For this to happen, we need to move at least 2n\u22121 +1 steps (cross\nover 0, the break point, and all the positive\/negative numbers). Like the earlier case, this is\nnot possible.\nAlternative Interpretation of 2\u2019s Complement\nTheorem 2.3.4.2 A signed n bit number, A, is equal to (A - A 2n\u22121). A is the ith\n1...n\u22121 n i\nbit in A\u2019s 2\u2019s complement binary representation (A is the LSB, and A is the MSB). A\n1 n 1...j\nis a binary number containing the first j digits of A\u2019s binary 2\u2019s complement representation. (cid:13)c Smruti R. Sarangi 74\nProof: Let us consider a 4-bit representation. -2 is represented as 1110 . The last n\u22121 digits\n2\nare 110 . This is equal to 6 in decimal. The MSB represents 1000 or 8. Indeed -2 = 6 - 8.\n2 2\nIf A > 0, then A = 0, and the statement of the theorem is trivially true. Let us consider\nn\nthe case when A < 0. Here, A = 1. We observe that A = 2n \u2212|A| = 2n +A since A is\nn 1...n\nnegative. Thus, A = A \u22122n.\n1...n\nA = A \u22122n\n1...n\n= (A +A 2n\u22121)\u22122n\n1...n\u22121 n\n(2.21)\n= (A +2n\u22121)\u22122n (A = 1)\n1...n\u22121 n\n= A \u22122n\u22121\n1...n\u22121\n2.4 Floating Point Numbers\nFloating Point Numbers are numbers that contain a decimal point. Examples are: 3.923, -4.93,\n10.23e-7 (10.23\u00d710\u22127). Note that the set of integers are a subset of the set of floating point\nnumbers. An integer such as 7 can be represented as 7.0000000. We shall describe a method\nto represent floating point numbers in the binary format in this section.\nIn specific, we shall describe the IEEE 754 [Kahan, 1996] standard for representing floating\npoint numbers. We shall further observe that representing different kinds of floating point\nnumbers is slightly complicated, and requires us to consider many special cases. To make our\nlife easy, let us first slightly simplify the problem and consider representing a set of numbers\nknown as fixed point numbers.\n2.4.1 Fixed Point Numbers\nA fixed point number has a fixed number of digits after the decimal point. For example, any\nvalue representing money typically has two digits after the decimal point for most currencies in\nthe world. In most cases, there is no reason for having more than three digits after the decimal\npoint. Such numbers can be represented in binary easily.\nLet us consider the case of values representing a monetary amount. These values will only\nbe positive. A value such as 120.23 can be represented in binary as the binary representation\nof 12023. Here, the implicit assumption is that there are two digits after the decimal point. It\nis easy to add two numbers using this notation. It is also easy to subtract two numbers as long\nas the result is positive. However, multiplying or dividing such numbers is difficult.\n2.4.2 Generic Form of Floating Point Numbers\nUnlike fixed point numbers, there can potentially be many more digits after the decimal point\nin floating point numbers. We need a more generic representation. Let us first look at how\nwe represent floating point numbers in a regular base 10 number system. For simplicity, let us\nlimit ourselves to positive floating point numbers in this section. 75 (cid:13)c Smruti R. Sarangi\nRepresenting Floating Point Numbers in Base-10\nExamples of positive floating point numbers in base 10 are: 1.344, 10.329, and 2.338. Alterna-\ntively, a floating point number, A, can be expanded according to Equation 2.22.\nn\n(cid:88)\nA = x 10i (2.22)\ni\ni=\u2212n\nFor example, 1.344 = 1\u00d7100+3\u00d710\u22121+4\u00d710\u22122+4\u00d710\u22123. The coefficient x can vary\ni\nfrom 0 to 9. Let us try to use the basic idea in this equation to create a similar representation\nfor floating point numbers in base 2.\nRepresenting Floating Point Numbers in Binary\nLet us try to extend the expansion shown in Equation 2.22 to expand positive floating point\nnumbers in base 2. A is a positive floating point number. We can try to expand A as:\nn\n(cid:88)\nA = x 2i (2.23)\ni\ni=\u2212n\nHere, x is either 0 or 1. Note that the form of Equation 2.23 is exactly the same as\ni\nEquation 2.22. However, we have changed the base from 10 to 2.\nWe have negative exponents from -1 to \u2212n, and non-negative exponents from 0 to n. The\nnegative exponents represent the fractional part of the number, and the non-negative expo-\nnents represent the integer part of the number. Let us show a set of examples in Table 2.12.\nWe show only non-zero co-coefficients for the sake of brevity.\nNumber Expansion\n0.375 2\u22122+2\u22123\n1 20\n1.5 20+2\u22121\n2.75 21+2\u22121+2\u22122\n17.625 24+20+2\u22121+2\u22123\nTable 2.12: Representation of floating point numbers\nWe observe that using Equation 2.23, we can represent a lot of floating point numbers ex-\nactly. However, therearealotofnumberssuchas1.11, whichwillpotentiallyrequireaninfinite\nnumber of terms with negative exponents. It is not possible to find an exact representation\nfor it using Equation 2.23. However, if n is large enough, we can reduce the error between the\nactual number and the represented number to a large extent.\nLet us now try to represent a positive floating point number in a binary format using\nEquation 2.23. There are two parts in a positive floating point number \u2013 integer part and\nfractional part. We represent the integer part using a standard binary representation. We\nrepresent the fractional part also with a binary representation of the form: x x ...x .\n\u22121 \u22122 \u2212n\nLastly, we put a \u2019.\u2019 between the integer and fractional parts. (cid:13)c Smruti R. Sarangi 76\nNumber Expansion Binary Representation\n0.375 2\u22122+2\u22123 0.011\n1 20 1.0\n1.5 20+2\u22121 1.1\n2.75 21+2\u22121+2\u22122 10.11\n17.625 24+20+2\u22121+2\u22123 10001.101\nTable 2.13: Representation of floating point numbers in binary\nTable 2.13 shows the binary representation of numbers originally shown in Table 2.12.\nNormal Form\nLet us take a look at Table 2.13 again. We observe that there are a variable number of binary\nbits before and after the decimal point. We can limit the number of bits before and after the\ndecimal point to L and L respectively. By doing so, we can have a binary representation for\ni f\na floating point number that requires L +L bits \u2013 L bits for the integer part, and L bits\ni f i f\nfor the fractional part. The fractional part is traditionally known as the mantissa, whereas the\nentire number (both integer and fraction) is known as the significand. If we wish to devote 32\nbits for representing a floating point number, then the largest number that we can represent\nis approximately 216 = 65,536 (if L = L ), which is actually a very small number for most\ni f\npractical purposes. We cannot represent large numbers such as 250.\nLet us thus, slightly modify our generic form to expand the range of numbers that we can\nrepresent. We start out by observing that 101110 in binary can be represented as 1.01110\u00d725.\nThe number 1.01110 is the significand. As a convention, we can assume that the first binary\ndigit in the significand is 1, and the decimal point is right after it. Using this notation, we can\nrepresent all floating point numbers as:\nA = P \u00d72X, (P = 1+M,0 \u2264 M < 1,X \u2208 Z) (2.24)\nDefinition 23\nSignificand It is the part of the floating point number that just contains its digits. The\ndecimal point is somewhere within the significand. The significand of 1.3829\u00d7103 is\n1.3829.\nMantissa It represents the fractional part of the significand. The mantissa of 1.3829\u00d7103\nis 0.3829.\nZ is the set of integers, P is the significand, M is the mantissa, and X is known as the\nexponent. This representation is slightly more flexible. It allows us to specify large exponents, 77 (cid:13)c Smruti R. Sarangi\nbothpositiveaswellasnegative. Lastly, letustrytocreateagenericformforbothpositiveand\nfloatingpointnumbersbyintroducingasignbit,S. WeshowtheresultingforminEquation2.25\nand refer to it as the normal form henceforth.\nA = (\u22121)S \u00d7P \u00d72X, (P = 1+M,0 \u2264 M < 1,X \u2208 Z) (2.25)\nIf S = 0, the number is positive. If S = 1, the number is negative.\n2.4.3 IEEE 754 Format for Representing Floating Point Numbers\nLet us now try to represent a floating point number using a sequence of 32 bits. We shall\ndescribe the IEEE 754 format, which is the de facto standard for representing floating point\nnumbers in binary.\nLetusstartwiththenormalformasshowninEquation2.25. Weobservethattherearethree\nvariables in the equation: S(sign bit), M(mantissa), and X(exponent). Since all significands\nhave 1 as their first digit, there is no need to explicitly represent it. We can assume that we\nhave a 1 by default as the MSB of the significand, and we need to just represent the L bits of\nf\nthe mantissa. Secondly, since we are representing all our numbers in binary, the base is 2, and\nthis can be assumed to be the default value. The IEEE 754 format thus proposes to apportion\n32 bits as shown in Figure 2.8.\nSign(S)Exponent(X) Mantissa(M)\n1 8 23\nFigure 2.8: IEEE 754 format for representing 32-bit floating point numbers\nThe format allocates 1 bit for the sign bit, 8 bits for the exponent, and 23 bits for the\nmantissa. The exponent can be positive, negative or zero. The point to note here is that the\nexponent is not represented in the 2\u2019s complement notation. It is represented using the biased\nrepresentation (see Section 2.3.3). The exponent(X) is represented by a number, E, where:\nE = X +bias (2.26)\nIn this case, the bias is equal to 127. Thus, if the exponent is equal 10, it is represented\nas 137. If the exponent is -20, it is represented as 107. E is an unsigned number between 0\nand 255. 0 and 255 are reserved for special values. The valid range for E for normal floating\npoint numbers is 1 to 254. Thus, the exponent can vary from -126 to 127. We can represent\nthe normal form for IEEE 754 numbers as:\nA = (\u22121)S \u00d7P \u00d72E\u2212bias, (P = 1+M,0 \u2264 M < 1,1 \u2264 E \u2264 254) (2.27)\nExample 16\nFind the smallest and largest positive normal floating point numbers.\nAnswer: (cid:13)c Smruti R. Sarangi 78\n\u2022 The largest positive normal floating point number is 1.11...1\u00d72127.\n(cid:124) (cid:123)(cid:122) (cid:125)\n23\n\u221223\n(cid:88)\n1.11...1 = 1+ 2i\n(cid:124) (cid:123)(cid:122) (cid:125)\n23 i=\u22121\n\u221223\n(cid:88)\n= 2i\ni=0\n= 21\u22122\u221223\n= 2\u22122\u221223\nThe result is equal to (2\u22122\u221223)\u00d72127 = 2128\u22122104.\n\u2022 The smallest positive normal floating point number is 1.00...0\u00d72\u2212126 = 2\u2212126.\nExample 17\nWhat is the range of normal floating point numbers.\nAnswer: \u00b1(2128\u22122104).\nSpecial Numbers\nWe reserved two values of E, 0 and 255, to represent special numbers.\nE M Value\n255 0 \u221e if S = 0\n255 0 \u2212\u221e if S = 1\n255 (cid:54)= 0 NAN (Not a number)\n0 0 0\n0 (cid:54)= 0 Denormal number\nTable 2.14: Special floating point numbers\nIf (E=255), then we can represent two kinds of values: \u221e and NAN (Not a number).\nWe need to further look at the mantissa(M). If (M = 0), then the number represents \u00b1\u221e\ndepending on the sign bit. We can get \u221e as a result of trying to divide any non-zero number\nby 0, or as the result of other mathematical operations. The point to note is that the IEEE\n754 format treats infinities separately.\nIf we divide 0\/0 or try to compute sin\u22121(x) for x > 1, then the result is invalid. An invalid\nresult is known as a NAN. Any mathematical operation involving a NAN has as its result a 79 (cid:13)c Smruti R. Sarangi\nNAN. Even NAN \u2212NAN = NAN. If M (cid:54)= 0, then the represented number is a NAN. In this\ncase the exact value of M is not relevant.\nNow, letustakealookatthecase, whenE = 0. IfM isalso0, thenthenumberrepresented\nis 0. Note that there are two 0s in the IEEE 754 format \u2013 a positive zero and a negative\nzero. Ideally implementations of this format are supposed to treat both the zeros as the same.\nHowever, this can vary depending upon the processor vendor.\nThe last category of numbers are rather special. They are called denormal numbers. We\nshall discuss them separately in Section 2.4.4.\n2.4.4 Denormal Numbers\nWe have seen in Example 16 that the smallest positive normal floating point number is 2\u2212126.\nLet us consider a simple piece of code.\nf = 2^(-126);\ng = f \/ 2;\nif (g == 0)\nprint (\"error\");\nSadly,thispieceofcodewillcomputeg tobe0asperourcurrentunderstanding. Thereason\nfor this is that f is the smallest possible positive number that can be represented in our format.\ng can thus not be represented, and most processors will round g to 0. However, this leads to a\nmathematical fallacy. The IEEE 754 protocol designers thus tried to avoid situations like this\nby proposing the idea of denormal numbers. Denormal numbers have a slightly different form\nas given by Equation 2.28.\nA = (\u22121)S \u00d7P \u00d72\u2212126, (P = 0+M,0 \u2264 M < 1) (2.28)\nNote the differences with Equation 2.25. The implicit value of 1 is not there any more.\nInstead of (P = 1 + M), we have (P = 0 + M). Secondly, there is no room to specify any\nexponent. This is because E=0. The default exponent is -126. We can view denormal numbers\nas an extension of normal floating point numbers on both sides (smaller and larger). Refer to\nFigure 2.9.\nDenormal numbers\nNormal FP numbers\n0\nFigure 2.9: Denormal numbers on a conceptual number line (not drawn to scale) (cid:13)c Smruti R. Sarangi 80\nExample 18\nFind the smallest and largest positive denormal numbers.\nAnswer:\n\u2022 The smallest positive denormal number is 0.00...01\u00d72\u2212126 = 2\u2212149.\n(cid:124) (cid:123)(cid:122) (cid:125)\n22\n\u2022 The largest possible denormal number is 0.11...1\u00d72\u2212126 = 2\u2212126\u22122\u2212149.\n(cid:124) (cid:123)(cid:122) (cid:125)\n23\n\u2022 Note that the largest denormal number (2\u2212126 \u2212 2\u2212149) is smaller than the small-\nest positive normal number (2\u2212126). This justifies the choice of 2\u2212126 as the default\nexponent for denormal numbers.\nExample 19\nFind the ranges of denormal numbers.\nAnswer:\n\u2022 For positive denormal numbers, the range is [2\u2212149,2\u2212126\u22122\u2212149].\n\u2022 For negative denormal numbers, the range is [\u2212(2\u2212126\u22122\u2212149),\u22122\u2212149].\nByusingdenormalnumberswewillnotgetawronganswerifwetrytodivide2\u2212126 by2,and\nthen compare it with 0. Denormal numbers can thus be used as a buffer such that our normal\narithmetic operations do not give unexpected results. In practice, incorporating denormal\nnumbers in a floating point unit is difficult and they are very slow to process. Consequently,\na lot of small embedded processors do not support denormal numbers. However, most modern\nprocessors running on laptops and desktops have full support for denormal numbers.\n2.4.5 Double Precision Numbers\nWeobservethatbyusing32bits,thelargestnumberthatwecanrepresentisroughly2128,which\nis approximately 1038. We might need to represent larger numbers, especially while studying\ncosmology. Secondly, there are only 23 bits of precision (mantissa is 23 bits long). If we are\ndoing highly sensitive calculations, then we might need more bits of precision. Consequently,\nthere is a IEEE 754 standard for double precision numbers. These numbers require 64 bits\nof storage. They are represented by the double datatype in C or Java.\n64 bits are apportioned as follows:\nThe mantissa is now 52 bits long. We have 11 bits for representing the exponent. The bias\nis equal to 1023, and the range of the exponent is from -1022 to 1023. We can thus represent\nmany more numbers that are much larger, and we have more bits in the mantissa for added\nprecision. The format and semantics of \u00b1\u221e, zero, NAN, and denormal numbers remains the\nsame as the case for 32 bits. 81 (cid:13)c Smruti R. Sarangi\nField Size(bits)\nS 1\nE 11\nM 52\n2.4.6 Floating Point Mathematics\nBecause of limited precision, floating point formats do not represent most numbers accurately.\nThis is because, we are artificially constraining ourselves to expressing a generic real number\nas a sum of powers of 2, and restricting the number of mantissa bits to 23. It is possible that\nsome numbers such as 1\/7 can be easily represented in one base (base 7), and can have inexact\nrepresentations in other bases (base 2). Furthermore, there are a large set of numbers that\n\u221a\ncannot be exactly represented in any base. These are irrational numbers such as 2 or \u03c0. This\nis because a floating point representation is a rational number that is formed out of repeatedly\nadding fractions. It is a known fact that rational numbers cannot be used to represent numbers\n\u221a\nsuch as 2. Leaving theoretical details aside, if we have a large number of mantissa bits, then\nwe can get arbitrarily close to the actual number. We need to be willing to sacrifice a little bit\nof accuracy for the ease of representation.\nFloating point math has some interesting and unusual properties. Let us consider the\nmathematical expression involving two positive numbers A and B: A + B \u2212 A. We would\nideally expect the answer to be non-zero. However, this need not be the case. Let us consider\nthe following code snippet.\nA = 2^(50);\nB = 2^(10);\nC = (B + A) - A;\nDue to the limited number of mantissa bits (23), there is no way to represent 250 +210. If\nthe dominant term is 250, then our flexibility is only limited to numbers in the range 250\u00b123.\nHence, a processor will compute A+B equal to A, and thus C will be 0. However, if we slightly\nchange the code snippet to look like:\nA = 2^(50);\nB = 2^(10);\nC = B + (A - A);\nC is computed correctly in this case. We thus observe that the order of floating point\noperations is very important. The programmer has to be either smart enough to figure out the\nright order, or we need a smart compiler to figure out the right order of operations for us. As\nwe see, floating point operations are clearly not associative. The proper placement of brackets\nis crucial. However, floating point operations are commutative (A+B = B+A).\nDue to the inexact nature of floating point mathematics, programmers and compilers need\nto pay special attention while dealing with very large or very small numbers. As we have also\nseen, if one expression contains both small and large numbers, then the proper placement of\nbrackets is very important. (cid:13)c Smruti R. Sarangi 82\n2.5 Strings\nAstringdatatypeis asequenceofcharacters inagivenlanguagesuch asEnglish. Forexample,\n\u201ctest\u201d, is a string of four characters. We need to derive a bitwise representation for it, the same\nway we devised a representation for integers. Traditionally, characters in the English language\nare represented using the ASCII character set. Hence, we shall describe it first.\n2.5.1 ASCII Format\nASCII stands for \u201cAmerican Standard Code for Information Interchange\u201d. It is a format that\nassigns a 7 bit binary code for each English language character including punctuation marks.\nMostlanguagesthatusetheASCIIformat,use8bitstorepresenteachcharacter. Onebit(MSB)\nis essentially wasted.\nThe ASCII character set defines 128 characters. The first 32 characters are reserved for\ncontrol operations, especially for controlling the printer. For example, the zeroth character\nis known as the null character. It is commonly used to terminate strings in the C language.\nSimilarly, there are special characters for backspace(8), line feed(10), and escape(27). The\ncommon English language characters start from 32 onwards. First, we have punctuation marks\nand special characters, then we have 26 capital letters, and finally 26 small letters. We show a\nlist of ASCII characters along with their decimal encodings in Table 2.15.\nSince ASCII can represent only 128 symbols, it is suitable only for English. However, we\nneed an encoding for most of the languages in the world such as Arabic, Russian, French,\nSpanish, Swahili, Hindi, Chinese, Thai, and Vietnamese. The Unicode format was designed for\nthis purpose. The most popular Unicode standard until recently was UTF-8.\n2.5.2 UTF-8\nUTF (Universal character set Transformation Format - 8 bit) can represent every character in\nthe Unicode character set. The Unicode character set assigns a unsigned binary number to each\ncharacter of most of the world\u2019s writing systems. UTF-8 encodes 1,112,064 characters defined\nin the Unicode character set. It uses 1-6 bytes for this purpose.\nUTF-8iscompatiblewithASCII.Thefirst128charactersinUTF-8correspondtotheASCII\ncharacters. When using ASCII characters, UTF-8 requires just one byte. It has a leading 0.\nHowever, the first byte can contain extra information such as the total number of bytes. This\nis encoded by having leading ones followed by a zero in the first byte. For example, if the\nfirst byte is of the form 11100010, then it means that the character contains 3 bytes. Each\ncontinuation byte begins with 10. Most of the languages that use variants of the Roman script\nsuch as French, German, and Spanish require 2 bytes in UTF-8. Greek, Russian (Cyrillic),\nHebrew, and Arabic, also require 2 bytes.\nUTF-8 is a standard for the world wide web. Most browsers, applications, and operating\nsystems are required to support it. It is by far the most popular encoding as of 2012.\n2.5.3 UTF-16 and UTF-32\nUTF-8 has been superseded by UTF-16, and UTF-32. UTF-16 uses either 2 byte or 4 byte\nencodingstorepresentsalltheUnicodecharacters. ItisprimarilyusedbyJavaandtheWindows 83 (cid:13)c Smruti R. Sarangi\nCharacter Code Character Code Character Code\na 97 A 65 0 48\nb 98 B 66 1 49\nc 99 C 67 2 50\nd 100 D 68 3 51\ne 101 E 69 4 52\nf 102 F 70 5 53\ng 103 G 71 6 54\nh 104 H 72 7 55\ni 105 I 73 8 56\nj 106 J 74 9 57\nk 107 K 75 ! 33\nl 108 L 76 # 35\nm 109 M 77 $ 36\nn 110 N 78 % 37\no 111 O 79 & 38\np 112 P 80 ( 40\nq 113 Q 81 ) 41\nr 114 R 82 * 42\ns 115 S 83 + 43\nt 116 T 84 , 44\nu 117 U 85 . 46\nv 118 V 86 ; 59\nw 119 W 87 = 61\nx 120 X 88 ? 63\ny 121 Y 89 @ 64\nz 122 Z 90 \u2227 94\nTable 2.15: ASCII Character Set\noperating system. UTF-32 encodes all characters using exactly 32 bits. It is rarely used since\nit is an inefficient encoding.\n2.6 Summary and Further Reading\n2.6.1 Summary\nSummary 2\n1. In computer architecture, we represent information using the language of bits. A bit\ncan either take the value of 0 or 1. A sequence of 8 bits is called a byte. (cid:13)c Smruti R. Sarangi 84\n2. A variable representing a bit is also called a Boolean variable, and an algebra on such\nBoolean variables is known as Boolean algebra.\n3. (a) The basic operators in Boolean algebra are logical OR, AND, and NOT.\n(b) Some derived operators are NAND, NOR, and XOR.\n(c) We typically use the De Morgan\u2019s laws (see Section 2.1.4) to simplify Boolean\nexpressions.\n4. Any Boolean expression can be represented in a canonical form as a logical OR of\nminterms. It can then be minimised using Karnaugh Maps.\n5. We can represent positive integers in a binary representation by using a sequence of\nbits. Inthiscase, werepresentanumber, A, asx x ...x , whereA = (cid:80)n x 2i\u22121.\nn n\u22121 1 i=1 i\n6. The four methods to represent a negative integer are:\n(a) Sign Magnitude based Method\n(b) The 1\u2019s Complement Method\n(c) Bias based Method\n(d) The 2\u2019s Complement Method\n7. The 2\u2019s complement method is the most common. Its main properties are as follows:\n(a) The representation of a positive integer is the same as its unsigned representation\nwith a leading 0 bit.\n(b) The representation of a negative integer (\u2212u) is equal to 2n \u2212 u, in an n bit\nnumber system.\n(c) To convert an m-bit 2\u2019s complement number to an n-bit 2\u2019s complement number,\nwhere n > m, we need to extend its sign by n\u2212m places.\n(d) We can quickly compute the 2\u2019s complement of a negative number of the form\n\u2212u (u \u2265 0), by computing the 1\u2019s complement of u (flip every bit), and then\nadding 1.\n(e) Addition, subtraction, and multiplication (ignoring overflows) of integers in the\n2\u2019s complement representation can be done by assuming that the respective binary\nrepresentations represent unsigned numbers.\n8. Floating point numbers in the IEEE 754 format are always represented in their normal\nform.\n(a) A floating point number, A, is equal to\nA = (\u22121)S \u00d7P \u00d72X\nS is the sign bit, P is the significand, and X is the exponent. 85 (cid:13)c Smruti R. Sarangi\n(b) We assume that the significand is of the form 1+M, where 0 \u2264 M < 1. M is\nknown as the mantissa.\n9. The salient points of the IEEE 754 format are as follows:\n(a) The MSB is the sign bit.\n(b) We have a 8 bit exponent that is represented using the biased notation (bias equal\nto 127).\n(c) We do not represent the leading bit (equal to 1) in the significand. We represent\nthe mantissa using 23 bits.\n(d) The exponents, 0 and 255, are reserved for special numbers \u2013 denormal numbers,\nNAN, zero, and \u00b1\u221e.\n10. Denormal numbers are a special class of floating point numbers, that have a slightly\ndifferent normal form.\nA = (\u22121)S \u00d7P \u00d72\u2212126,(0 \u2264 P < 1,P = 0+M)\n11. Floating point arithmetic is always approximate; hence, arithmetic operations can lead\nto mathematical contradictions.\n12. We represent pieces of text as a contiguous sequence of characters. A character can\neither be encoded in the 7 bit ASCII format, or in the Unicode formats that use 1-4\nbytes per character.\n2.6.2 Further Reading\nBoolean algebra is a field of study by itself. Boolean formulae, logic, and operations form\nthe basis of modern computer science. We touched upon some basic results in this chapter.\nThe reader should refer to [Kohavi and Jha, 2009] for a detailed discussion on Boolean logic,\nKarnaugh Maps, and a host of other advanced techniques to minimise the number of terms\nin Boolean expressions. For Boolean logic and algebra, the reader can also consult [Gregg,\n1998, Patt and Patel, 2003, Whitesitt, 2010] The next step for the reader is to read more about\nthesynthesisandoptimisationoflargedigitalcircuits. ThebookbyGiovannideMichel[Micheli,\n1994] can be a very helpful reference in this regard. Number systems such as 2\u2019s complement\nnaturally lead to computer arithmetic where we perform complex operations on numbers. The\nreader should consult the book by Zimmermann [Brent and Zimmermann, 2010]. For learning\nmore about the representation of characters, and strings, especially in different languages, we\nrefer the reader to the unicode standard [uni, ]. (cid:13)c Smruti R. Sarangi 86\nExercises\nBoolean Logic\nEx. 1 \u2014 A, B, C and D are Boolean variables. Prove the following results:\na) A.B+A.B+B.C +B.C = 1\nb) (A+B).(A+B).(A+B.D+C) = A.B.D+A.C\nc) A.B+B.C = A.C +B\nd) A.B+A.B+A.B.C.D = B\nEx. 2 \u2014 Construct a circuit to compute the following functions using only NOR gates.\na)A\nb)A+B\nc)A.B\nd)A\u2295B\nEx. 3 \u2014 Construct a circuit to compute the following functions using only NAND gates.\na)A\nb)A+B\nc)A.B\nd)A\u2295B\n** Ex. 4 \u2014 Prove that any Boolean function can be realised with just NAND or NOR gates.\n[HINT: Use the idea of decomposing a function into its set of minterms.]\nEx. 5 \u2014 Why are the first and last rows or columns considered to be adjacent in a Karnaugh\nMap?\nEx. 6 \u2014 Minimise the following Boolean functions using a Karnaugh Map.\na)ABC +ABC +ABC\nb)ABCD+ABCD+AD\n* Ex. 7 \u2014 Consider the Karnaugh map of the function A \u2295A ...\u2295A . Prove that it looks\n1 2 n\nlike a chess board. Why cannot we minimise this expression further?\nInteger Number Systems\nEx. 8 \u2014 Convert the following 8 bit binary numbers in 1\u2019s complement form to decimal.\na) 01111101 87 (cid:13)c Smruti R. Sarangi\nb) 10000000\nc) 11111111\nd) 00000000\ne) 11110101\nEx. 9 \u2014 Convert the following unsigned numbers (in the given base) to decimal:\na) (243)\n5\nb) (77)\n8\nc) (FFA)\n16\nd) (100)\n4\ne) (55)\n6\nEx. 10 \u2014 Do the following calculations on unsigned binary numbers and write the result as\nan unsigned binary number.\na) 1100110101+1111001101\nb) 110110110+10111001\nc) 11101110\u2212111000\nd) 10000000\u2212111\nEx. 11 \u2014 What are the pros and cons of the 1\u2019s complement number system?\nEx. 12 \u2014 What are the pros and cons of the sign-magnitude number system?\nEx. 13 \u2014 What is a number circle? How is it related to the 2\u2019s complement number system?\nEx. 14 \u2014 What does the point of discontinuity on the number circle signify?\nEx. 15 \u2014 Why is moving k steps on the number circle in a clockwise direction equivalent to\nmoving 2n - k steps in an anti-clockwise direction? Assume that the number circle contains 2n\nnodes.\nEx. 16 \u2014 What are the advantages of the 2\u2019s complement notation over other number sys-\ntems?\nEx. 17 \u2014 Outline a method to quickly compute the 2\u2019s complement of a number.\nEx. 18 \u2014 Prove the following result in your own words:\nF(u\u2212v) \u2261 F(u)+(2n\u2212F(v)) (2.29)\nEx. 19 \u2014 Let us define sign contraction to be the reverse of sign extension. What are the\nrules for converting a 32-bit number to a 16-bit number by using sign contraction? Can we do\nthis conversion all the time without losing information?\nEx. 20 \u2014 Whataretheconditionsfordetectinganoverflowwhileaddingtwo2\u2019scomplement\nnumbers? (cid:13)c Smruti R. Sarangi 88\nFloating Point Number System\nEx. 21 \u2014 Describe the IEEE 754 format.\nEx. 22 \u2014 Why do we avoid representing the bit to the left of the decimal point in the\nsignificand?\nEx. 23 \u2014 Definedenormalnumbers. Howdotheyhelptoextendtherangeofnormalfloating\npoint numbers?\nEx. 24 \u2014 In the standard form of a denormal number, why is the exponent term equal to\n2\u2212126? Why is it not equal to 2\u2212127?\nEx. 25 \u2014 Convert the following floating point numbers into the IEEE 32-bit 754 format.\nWrite your answer in the hexadecimal format.\na) \u22121\u2217(1.75\u22172\u221229+2\u221240+2\u221245)\nb) 52\nEx. 26 \u2014 What is the range of positive and negative denormal floating point numbers num-\nbers?\nEx. 27 \u2014 WhatwillbetheoutputofthefollowingCcodesnippetassumingthatthefractions\nare stored in an IEEE 32-bit 754 format:\nfloat a=pow(2,-50);\nfloat b=pow(2,-74);\nfloat d=a;\nfor(i=0; i<100000; i++)\n{\nd=d+b;\n}\nif(d>a)\nprintf(\"%d\",1);\nelse\nprintf(\"%d\",2);\nEx. 28 \u2014 We claim that the IEEE 754 format represents real numbers approximately. Is this\nstatement correct?\n\u221a\n* Ex. 29 \u2014 Provethatitisnotpossibletoexactlyrepresent 2evenifwehaveanindefinitely\nlarge number of bits in the mantissa.\n* Ex. 30 \u2014 How does having denormal numbers make floating point mathematics slightly\nmore intuitive?\n* Ex. 31 \u2014 What is the correct way for comparing two floating point numbers for equality?\n** Ex. 32 \u2014 Assume that the exponent e is constrained to lie in the range 0 \u2264 e \u2264 X with\na bias of q , and the base is b . The significand is p digits in length. Use an IEEE 754 like 89 (cid:13)c Smruti R. Sarangi\nencoding. However, you need to devote one digit to store the value to the left of the decimal\npoint in the significand.\na) What are the largest and smallest positive values that can be written in normal form.\nb) What are the largest and smallest positive values that can be written in denormal form.\n* Ex. 33 \u2014 Mostofthefloatingpointnumberscannotberepresentedaccuratelyinhardware\ndue to the loss of precision. However, if we choose some other representation, we can represent\ncertain kinds of floating point numbers without error.\na) Give a representation for storing rational numbers accurately. Devise a normal form for\nit.\n\u221a\nb) Can other floating point numbers such as 2 be represented in a similar way?\nEx. 34 \u2014 Design a floating point representation, for a base 3 system on the lines of the IEEE\n754 format.\nStrings\nEx. 35 \u2014 Convert the string \u201c459801\u201d to ASCII. The ASCII representation of 0 is 0x30.\nAssume that all the numbers are represented in the ASCII format in sequence.\nEx. 36 \u2014 Find the Unicode representation for characters in a non-English language, and\ncompare it with the ASCII encoding.\nDesign Problems\nEx. 37 \u2014 In this section, we have minimised Boolean expressions using Karnaugh maps.\nWe solved all our examples manually. This method is not scalable for expressions containing\nhundreds of variables. Study automated techniques for minimising Boolean expressions such as\nthe Quinn-McCluskey tabulation method. Write a program to implement this method. (cid:13)c Smruti R. Sarangi 90 3\nAssembly Language\nAssembly language can broadly be defined as a textual representation of machine instructions.\nBefore building a processor, we need to know about the semantics of different machine instruc-\ntions, and a rigorous study of assembly language will be of benefit in this regard. An assembly\nlanguage is specific to an ISA and compiler framework; hence, there are many flavors of as-\nsembly languages. In this chapter we shall describe the broad principles underlying different\nvariants of assembly languages, some generic concepts and terms. We will subsequently design\nour own assembly language, SimpleRisc . It is a simple RISC ISA with a few instructions.\nSubsequently, in Chapter 8, we will design a processor that fully implements this ISA. Thus,\nthe plan for this chapter is as follows. We shall first convince ourselves of the need for assem-\nbly language in Section 3.1 from the point of view of both software developers and hardware\ndesigners. Then we shall proceed to discuss the generic semantics of assembly languages in\nSection 3.2. Once, we have a basic understanding of assembly languages, we shall design our\nown assembly language, SimpleRisc , in Section 3.3, and then design a method to encode it\nusing a sequence of 32 bits in Section 3.3.14.\nSubsequently, in Chapter 4 we shall describe the ARM assembly language that is meant for\nARM based processors, and in Chapter 5, we shall describe the x86 assembly language meant\nfor Intel\/AMD processors. In these two chapters, these machine specific assembly languages\nwill be covered in great detail. This chapter is introductory, and creates the framework for a\nmore serious study of different instruction sets and assembly languages.\n3.1 Why Assembly Language\n3.1.1 Software Developer\u2019s Perspective\nA human being understands natural languages such as English, Russian, and Spanish. With\nsome additional training a human can also understand computer programming languages such\nas C or Java. However, a computer is a dumb machine as mentioned in Chapter 1. It is\n91 (cid:13)c Smruti R. Sarangi 92\nnot smart enough to understand commands in a human language such as English, or even a\nprogramming language such as C. It only understands zeros and ones. Hence, to program a\ncomputer it is necessary to give it a sequence of zeros and ones. Indeed some of the early\nprogrammers used to program computers by turning on or off a set of switches. Turning on\na switch corresponded to a 1, and turning it off meant a 0. However, for today\u2019s massive\nmulti-million line programs, this is not a feasible solution. We need a better method.\nConsequently, we need an automatic converter that can convert programs written in high\nlevel languages such as C or Java to a sequence of zeros and ones known as machine code.\nMachine code contains a set of instructions known as machine instructions. Each machine\ninstruction is a sequence of zeros and ones, and instructs the processor to perform a certain\naction. A program that can convert a program written in a high level language to machine\ncode is called a compiler( see Figure 3.1).\nDefinition 24\n\u2022 A high level programming language such as C or Java uses fairly complex constructs\nand statements. Each statement in these languages typically corresponds to a multi-\ntude of basic machine instructions. These languages are typically independent of the\nprocessor\u2019s ISA.\n\u2022 A compiler is an executable program that converts a program written in a high level\nlanguage to a sequence of machine instructions that are encoded using a sequence of\nzeros and ones.\n01001001\n10001010\nCompiler\n10010010\n10001010\nProgram\nMachine code\nFigure 3.1: The compilation process\nNote that the compiler is an executable program that typically runs on the machine that\nit is supposed to generate machine code for. A natural question that can arise is \u2013 who wrote\nthe first compiler? See Trivia 1.\nTrivia 1 Who Wrote the First Compiler? If a programmer wrote the compiler in a high\nlevel language such as C or Java, then she must have needed a compiler to compile it into\nmachine code. However, she did not have a compiler with her at that point of time, because 93 (cid:13)c Smruti R. Sarangi\nshe was in the process of building one! Since she did not have a compiler, while building the\ncompiler, how did she ultimately build it? This is an example of a chicken and egg problem.\nThe classic chicken and egg problem poses a simple yet vexing question \u2013 did the chicken\ncome first or the egg come first? However, the chicken and egg problem has a solution that\ncan be explained in terms of evolution. Scientists believe that early organisms reproduced\nby replication. At some point of time, due to a genetic mutation, an organism started to\nlay eggs. These organisms perpetuated, and started reproducing by only laying eggs. They\nevolved into all kinds of birds and reptiles, including chickens.\nWe can explain this conundrum in a similar manner. The early programmers wrote sim-\nple compilers using machine instructions. A primitive compiler is just a sequence of zeros\nand ones. The early programmers then used these primitive compilers to compile programs.\nA special class of such programs were compilers themselves. They were written in high level\nlanguages and were better in terms of features, functionality and even performance. These\nfirst generation compilers were then used to create second generation compilers, and this\nprocess has continued till date. Nowadays, if a new processor is being developed, then it\nis not necessary to follow this procedure. Programmers, use another set of programs called\ncross compilers. A cross compiler runs on an existing processor, and produces an executable\nusing the machine instructions of the new processor that is being developed. Once the new\nprocessor is ready, this program can be moved to the new processor and executed directly. It\nis thus possible to develop a large range of software including compilers for processors with\nnew instruction sets. Hence, most modern day programmers do not have to write programs\nusing raw machine instructions.\nDefinition 25\nA cross compiler is a program that runs on machine A, and generates machine code for\nmachine B. It is possible that B has a different ISA.\nGiven the ubiquity of compilers, almost all programs are written in high level languages and\ncompilers are used to convert them to machine code. However, there are important exceptions\nto this rule. Note that the role of a compiler is two fold. First, it needs to correctly translate a\nprogram in a high level language to machine instructions. Second, it needs to produce efficient\nmachinecodethatdoesnottakealotofspace,andisfast. Consequently,algorithmsincompilers\nhavebecomeincreasinglycomplicatedovertheyears. However, itisnotalwayspossibletomeet\nthese requirements. For example, in some scenarios, compilers might not be able to produce\ncodethatisfastenough, orhasacertainkindoffunctionalitythattheprogrammerdesires. Let\nus elaborate further. Algorithms in compilers are limited by the amount of analysis that they\ncan perform on the program. For example, we do not want the process of compilation to be\nextremely slow. A lot of the problems in the area of compilers are computationally difficult to\nsolve and are thus time consuming. Secondly, the compiler is not aware of the broad patterns\nin the code. For example, it is possible that a certain variable might only take a restricted set (cid:13)c Smruti R. Sarangi 94\nof values, and on the basis of this, it might be possible to optimise the machine code further. It\nis hard for a compiler to figure this out. However, smart programmers can sometimes produce\nmachine code that is more optimal than a compiler because they are aware of some broad\npatterns of execution, and their brilliant brains can outsmart compilers.\nSecondly, itisalsopossiblethataprocessorvendormightaddnewinstructionsintheirISA.\nIn this case, compilers meant for older versions of the processor might not be able to leverage\nthe new instructions. It will be necessary to add them manually in programs. Continuing this\nargument further, we observe that popular compilers such as gcc (GNU compiler collection)\nare fairly generic. They do not use all possible machine instructions that a processor provides\nwhile generating machine code. Typically, a lot of the missed out instructions are required\nby operating systems and device drivers (programs that interface with devices such as the\nprinter, and scanner). These software programs require these instructions because they need\nlow level access to the hardware. Consequently, system programmers have a strong incentive\nto occasionally bypass the compiler.\nIn all of these situations, it is necessary for programmers to manually embed a sequence of\nmachine instructions in a program. As mentioned, there are two primary reasons for doing so \u2013\nefficiency and extra functionality. Hence, from the point of view of system software developers,\nit is necessary to know about machine instructions such that they can be more productive in\ntheir job.\nNow, our aim is to insulate modern day programmers from the intricate details of zeros\nand ones. Ideally, we do not want our programmers to program by manually turning on and\noff switches as was done fifty years ago. Consequently, a low level language called assembly\nlanguage was developed (see Definition 26). Assembly language is a human readable form\nof machine code. Each assembly language statement typically corresponds to one machine\ninstruction. Furthermore, it eases the burden on the programmer significantly by not forcing\nher to remember the exact sequence of zeros\/ones that are needed to encode an instruction.\nDefinition 26\n\u2022 A low level programming language uses simple statements that correspond to typically\njust one machine instruction. These languages are specific to the ISA.\n\u2022 The term \u201cassembly language\u201d refers to a family of low level programming languages\nthat are specific to each ISA. They have a generic structure that consists of a sequence\nof assembly statements. Typically, each assembly statement has two parts \u2013 (1) an\ninstruction code that is a mnemonic for a basic machine instruction, and (2) and a\nlist of operands.\nFrom a practical standpoint, it is possible to write stand alone assembly programs and\nconvert them to executables using a program called an assembler(Definition 27). Alternatively,\nit is also possible to embed snippets of assembly code in high level languages such as C or\nC++. The latter is more common. A compiler ensures that it is able to compile the combined\nprogram into machine code. The benefits of assembly languages are manifold. Since each 95 (cid:13)c Smruti R. Sarangi\nline in assembly code corresponds to one machine instruction, it is as expressive as machine\ncode. Because of this one to one mapping, we do not sacrifice efficiency by writing programs in\nassembly. Secondly, it is a human readable and elegant form of textually representing machine\ncode. It makes it significantly easier to write programs using it, and it is also possible to cleanly\nembed snippets of assembly code in software written in high level languages such as C. The\nthirdadvantageofassemblylanguageisthatitdefinesalevelofabstractionoverandabovereal\nmachine code. It is possible that two processors might be compatible with the same variant of\nassembly language, but actually have different machine encodings for the same instruction. In\nthis case, assembly programs will be compatible across both of these processors.\nDefinition 27\nAn assembler is an executable program that converts an assembly program into machine\ncode.\nExample 20\nThe core engines of high performance 3D games need to be optimised for speed as much\nas possible [Phelps and Parks, 2004]. Most compilers fail to produce code that runs fast\nenough. It becomes necessary for programmers to manually write sequences of machine\ninstructions.\nExample 21\nVranas et. al. [Vranas et al., 2006] describe a high performance computing application\nto study the structure of an atomic nucleus. Since the computational requirements are\nhigh, they needed to run their program on a supercomputer. They observed that the core\nof the program lies in a small set of functions that are just 1000 lines long. They further\nobserved that compilers were not doing a good in job in optimising the output machine code.\nConsequently, they decided to write the important functions in assembly code, and obtained\nrecord speedups on a supercomputer. Durr et. al. [Durr et al., 2009] subsequently used this\nframework to accurately calculate the mass of a proton and a neutron from first principles.\nThe results were in complete agreement with experimentally observed values.\n3.1.2 Hardware Designer\u2019s Perspective\nThe role of hardware designers is to design processors that can implement all the instructions\nin the ISA. Their main aim is to design an efficient processor that is optimal with regards to\narea, power efficiency, and design complexity. From their perspective, the ISA is the crucial\nlink between software and hardware. It answers the basic question for them \u2013 \u201cwhat to build?\u201d (cid:13)c Smruti R. Sarangi 96\nHence, it is very essential for them to understand the precise semantics of different instruction\nsets such that they can design processors for them. As mentioned in Section 3.1.1, it is cum-\nbersome to look at instructions as merely a sequence of zeros and ones. They can gain a lot\nby taking a look at the textual representation of a machine instruction, which is an assembly\ninstruction.\nAn assembly language is specific to an instruction set and an assembler. In this chapter, we\nuse the assembly language format of the popular GNU assembler [Elsner and Fenlason, 1994]\nto explain the syntax of a typical assembly language file. Note that other systems have similar\nformats, and the concepts are broadly the same.\n3.2 The Basics of Assembly Language\n3.2.1 Machine Model\nLet us reconsider the basic abstract machine model explained in Chapter 1. We had finished\nthe chapter by describing a form of the Harvard and Von Neumann machines with registers.\nAssembly languages do not see the instruction memory and data memory as different entities.\nThey assume an abstract Von Neumann machine augmented with registers.\nRefer to Figure 3.2 for a pictorial representation of the machine model. The program is\nstored in a part of the main memory. The central processing unit (CPU) reads out the program\ninstruction by instruction, and executes the instructions appropriately. The program counter\nkeeps track of the memory address of the instruction that a CPU is executing. We typically\nrefer to the program counter using the acronym \u2013 PC. Most instructions are expected to get\ntheir input operands from registers. Recall that every CPU has a fixed number of registers\n(typically < 64). However, a large number of instructions, can also get their operands from\nthe memory directly. It is the job of the CPU to co-ordinate the transfers to and from the\nmain memory and registers. Secondly, the CPU also needs to perform all the arithmetic\/logical\ncalculations, and liaise with external input\/output devices.\nCPU\nRegisters\nALU\nMemory Control I\/O devices\nFigure 3.2: The Von Neumann machine with registers\nMost flavors of assembly language assume this abstract machine model for a majority of\ntheir statements. However, since another aim of using assembly language is to have more fine\ngrained and intrusive control of hardware, there are a fair number of assembly instructions that\narecognisantoftheinternalsoftheprocessor. Theseinstructionstypicallymodifythebehaviour\noftheprocessorbychangingthebehaviourofsomekeyinternalalgorithms; theymodifybuilt-in 97 (cid:13)c Smruti R. Sarangi\nparameterssuchaspowermanagementsettings, orread\/writesomeinternaldata. Finally, note\nthat the assembly language does not distinguish between machine independent and machine\ndependent instructions.\nView of Registers\nEvery machine has a set of registers that are visible to the assembly programmer. ARM has 16\nregisters, x86 (32-bit) has 8 registers, and x86 64 (64 bits) has 16 registers. The registers have\nnames. ARM names them r0...r15, and x86 names them eax,ebx,ecx,edx,esi,edi,ebp, and\nesp. A register can be accessed using its name.\nIn most ISAs, a return address register is used for function calls. Let us assume that a\nprogram starts executing a function. It needs to remember the memory address that it needs to\ncome back to after executing the function. This address is known as the return address. Before\njumping to the starting address of a function, we can save the value of the return address in\nthis register. The return statement can simply be implemented by copying the value saved in\nthe return address register to the PC. The return address register is visible to the programmer\nin assembly languages such as ARM and MIPS. However, x86 does not use a return address\nregister. It uses another mechanism called a stack, which we shall study in Section 3.3.10.\nIn an ARM processor, the PC is visible to the programmer and it is the last register (r15).\nIt is possible to read the value of the PC, as well as set its value. Setting the value of the PC\nmeans that we want to branch to a new location within the program. However, in x86, the\nprogram counter is implicit, and is not visible to the programmer.\n3.2.2 View of Memory\nInSection1.6.7,weexplainedtheconceptofamemoryinanabstractmachine. Thememorycan\nbe thought of as one large array of bytes. Each byte has a unique address, which is essentially\nits location in the array. The address of the first byte is 0, the address of the second byte is\n1, and so on. Note that the finest granularity at which we can access memory is at the level\nof a byte. We do not have a method to uniquely address a given bit. The address is a 32-bit\nunsigned integer in 32-bit machines and it is a 64-bit unsigned integer in 64-bit machines.\nNow, in a Von Neumann machine, we assume that the program is stored in memory as a\nsequence of bytes, and the program counter points to the next instruction that is going to be\nexecuted.\nAssuming that memory is one large array of bytes is fine, if all our data items are only one\nbyte long. However, languages such as C and Java have data types of different sizes \u2013 char\n(1 byte), short (2 bytes), integer (4 bytes), and long integer (8 bytes). For a multi-byte data\ntype it is necessary to find a representation for it in memory. There are two possible ways of\nrepresenting a multibyte data type in memory \u2013 little endian and big endian. Secondly, we also\nneed to find methods to represent arrays or lists of data in memory.\nLittle Endian and Big Endian Representations\nLetusconsidertheproblemofstoringanintegerinlocations0-3. Lettheintegerbe0x87654321.\nIt can be broken into four bytes \u2013 87, 65, 43, and 21. One option is to store the most significant\nbyte, 87, in the lowest memory address 0. The next location can store 65, then 43, and then (cid:13)c Smruti R. Sarangi 98\n21. This is called the big endian representation because we are starting from the position of the\nmost significant byte. In comparison, we can save the least significant byte first in location 0,\nand then continue to save the most significant byte in location 3. This representation is called\nlittle endian. Figure 3.3 shows the difference.\nBig endian\n87 65 43 21\n0 1 2 3\n0x87654321\nLittle endian\n21 43 65 87\n0 1 2 3\nFigure 3.3: Big endian and little endian representations\nThere is as such no reason to prefer one representation over the other. It depends on the\nconvention. For example, x86 processors use the little endian format. Early versions of ARM\nprocessors used to be little endian. However, now they are bi-endian. This means an ARM\nprocessor can work as both a little endian and a big endian machine depending on the settings\nset by the user. Traditionally, IBM(cid:13)R POWER(cid:13)R processors, and Sun(cid:13)R SPARC(cid:13)R processors\nhave been big endian.\nRepresenting Arrays\nAn array is a linearly ordered set of objects, where an object can be a simple data type such\nas an integer or character, or can be a complex data type also.\nint a[100];\nchar c[100];\nLet us consider a simple array of integers, a. If the array has 100 entries, then the total size\nof the array in memory is equal to 100\u00d74 = 400 bytes. If the starting memory location of the\narray is loc. Then a[0] is stored in the locations (loc+0),(loc+1),(loc+2),(loc+3). Note\nthat there are two methods of saving the data \u2013 big endian and little endian. The next array\nentry, a[1], is saved in the locations (loc+4)...(loc+7). By continuing the argument further,\nwe note that the entry a[i] is saved in the locations \u2013 (loc+4\u00d7i)...(loc+4\u00d7i+3).\nMost programming languages define multidimensional arrays of the form:\nint a[100][100];\nchar c[100][100];\nTheyaretypicallyrepresentedasregularonedimensionalarraysinmemory. Thereisamapping\nfunction between the location in a multidimensional array and an equivalent 1-dimensional\narray. Let us consider Example 22. We can extend the scheme to consider multidimensional\narrays of dimensions greater than 2. 99 (cid:13)c Smruti R. Sarangi\nExample 22\nConsider a multidimensional array: a[100][100]. Map each entry (i,j) to an entry in a\n1-D array: b[10000].\nAnswer: Let us assume that each entry (i,j), is in a (row,column) format. Let us try\nto save the array in row-major fashion. We save the first row in contiguous locations, then\nthe second row and so on. The starting entry of each row is equal to 100\u00d7i. Within each\nrow the offset for column j is equal to j. Thus we can map (i,j) to the entry: (100\u00d7i+j)\nin the array b.\nWe observe that a two-dimensional array can be saved as a one dimensional array by saving\nit in row-major fashion. This means that data is saved row wise. We save the first row, then\nthe second row, and so on. Likewise, it is also possible to save a multidimensional array in\ncolumn major fashion, where the first column is saved, then the second column and so on.\nDefinition 28\nrow major In this representation, an array is saved row wise in memory.\ncolumn major In this representation, an array is saved column wise in memory.\n3.2.3 Assembly Language Syntax\nIn this section, we shall describe the syntax of assembly language. The exact syntax of an\nassembly file is dependent on the assembler. Different assemblers can use different syntax, even\nthough they might agree on the basic instructions, and their operand formats. In this chapter,\nwe explain the syntax of the GNU family of assembly languages. They are designed for the\nGNU assembler, which is a part of the GNU compiler collection (gcc). Like all GNU software,\nthis assembler and the associated compiler is freely available for most platforms. As of 2012,\nthe assembler is available at [gnu.org, ]. In this section, we shall provide a brief overview of\nthe format of assembly files. For additional details refer to the official manual of the GNU\nassembler [Elsner and Fenlason, 1994]. Note that other assemblers such as NASM, and MASM,\nhave their own formats. However, the overall structure is not conceptually very different from\nwhat we shall describe in this section.\nAssembly Language File Structure\nAn assembly file is a regular text file, and it has a (.s) suffix. The reader can quickly generate\nan assembly file for a C program (test.c), if she has the gcc (GNU Compiler) installed. It can\nbe generated by issuing the following command. (cid:13)c Smruti R. Sarangi 100\ngcc -S test.c\nAssembly file\n.file\n.text\n.data\nFigure 3.4: Assembly language file structure\nThe generated assembly file will be named test.s. GNU assembly files have a very simple\nstructure, asshowninFigure3.4. Theycontainalistofsections. Examplesofdifferentsections\nare text (actual program), data (data with initialised values), and bss (common data that is\ninitialised to 0). Each section starts with a section heading, which is the name of the section\nprefixedbythe\u2018.\u2019 symbol. Forexample,thetextsectionstartswiththeline\u201c.text\u201d. Thereafter,\nthere is a list of assembly language statements. Each statement is typically terminated by the\nnewline character. Likewise, the data section contains a list of data values. An assembly file\nbegins with the file section that contains a line of the form \u2013 \u201c.file <name of the file> \u201d. When\nwearegeneratinganassemblyfilefromaCprogramusingthegcccompiler, thenameofthefile\nin the .file section is typically the same as our original C program (test.c). The text section is\nmandatory, and the rest of the sections are optional. There might be one or more data sections.\nIt is also possible to define new sections using the .section directive. In this book, we primarily\nconcentrate on the text section because we are interested in learning about the nature of the\ninstruction set. Let us now look at the format of assembly statements.\nBasic Statements\nA bare bones assembly language statement specifies an assembly instruction and has two parts\n\u2013 the instruction and its list of operands, as shown in Figure 3.5. The instruction is a textual\nidentifier of the actual machine instruction. The list of operands contains the value or location\nof each operand. The value of an operand is a numeric constant. It is also known as an\nimmediate value. The operand locations can either be register locations or memory locations.\nInstruction operand 1 operand 2 operand n\nFigure 3.5: Assembly language statement 101 (cid:13)c Smruti R. Sarangi\nDefinition 29\nIn computer architecture, a constant value specified in an instruction is also known as an\nimmediate.\nNow, let us consider an example.\nadd r3, r1, r2\nIn this ARM assembly statement, the add instruction is specifying the fact that we wish\nto add two numbers and save the result in some pre-specified location. The format of the\nadd instruction in this case is as follows: < instruction > < destination register > <\noperandregister1 > < operandregister2 >. The name of the instruction is add, the des-\ntination register is r3, the operand registers are r1 and r2. The detailed steps of the instruction\nare as follows:\n1. Read the value of register r1. Let us refer to the value as v .\n1\n2. Read the value of register r2. Let us refer to the value as v .\n2\n3. Compute v = v +v .\n3 1 2\n4. Save v in register r3\n3\nLet us now give an example of two more instructions that work in a similar fashion(see\nExample 23).\nExample 23\nsub r3, r1, r2\nmul r3, r1, 3\nThesubinstructionssubtractstwonumbersstoredinregisters,andthemulinstructionmul-\ntiplies a number stored in the register, r1, with the numeric constant, 3. Both the instructions\nsave the result in the register, r3. Their mode of operation is similar to the add instruction.\nMoreover, the arithmetic instructions \u2013 add, sub, and mul \u2013 are also known as data processing\ninstructions. There are several other classes of instructions such as data transfer instructions\nthat load or store values from memory, and control instructions that implement branching.\nGeneric Statement Structure\nThegenericstructureofanassemblystatementisshowninFigure3.6. Itconsistsofthreefields\nnamely a label (identifier of the instruction), the key (an assembly instruction, or a directive to (cid:13)c Smruti R. Sarangi 102\nthe assembler), and a comment. All three of these fields are optional. However, any assembly\nstatement needs to have at least one of these fields.\nA statement can optionally begin with a label. A label is a textual identifier for the state-\nment. In other words, a label uniquely identifies an assembly statement in an assembly file.\nNote that we are not allowed to repeat labels in the same assembly file. We shall find labels to\nbe very useful while implementing branch instructions.\nDefinition 30\nA label in an assembly file uniquely identifies a given point or data item in the assembly\nprogram.\nAn example of a label is shown in Example 24. Here the name of the label is \u201clabel1\u201d, and\nit is succeeded by a colon. After the label we have written an assembly instruction and given\nit a list of operands. A label can consist of valid alpha-numeric characters [a\u2212z][A\u2212Z][0\u22129]\nand the symbols \u2018.\u2019, \u2018 \u2019, and \u2018$\u2019. Typically, we cannot start a label with a digit. After specifying\na label we can keep the line empty, or we can specify a key (part of an assembly statement).\nIf the key begins with a \u2018.\u2019, then it is an assembler directive, which is valid for all computers.\nIt directs the assembler to perform a certain action. This action can include starting a new\nsection, or declaring a constant. The directive can also take a list of arguments. If the key\nbegins with a letter, then it is a regular assembly instruction.\nExample 24\nlabel1: add r1, r2, r3\nAfter the label, assembly instruction, and list of operands, it is possible to optionally insert\ncomments. The GNU assembler supports two types of comments. We can insert regular C or\nJava style comments enclosed between \/* and *\/. It is also possible to have a small single line\ncomment by preceding the comment with the \u2018@\u2019 character in ARM assembly.\nExample 25\nlabel1: add r1, r2, r3 @ Add the values in r2 and r3\nlabel2: add r3, r4, r5 @ Add the values in r4 and r5\nadd r5, r6, r7 \/* Add the values in r6 and r7 *\/\nLet us not slightly amend our statement regarding labels. It is possible that an assembly\nstatement only contains a label, and does not contain a key. In this case, the label essentially\npoints to an empty statement, which is not very useful. Hence, the assembler assumes that in\nsuch a case a label points to the nearest succeeding assembly statement that contains a key. 103 (cid:13)c Smruti R. Sarangi\nLabel : Directive @ Comment\nConstant \/* Comment *\/\nAssembly\ninstruction\nFigure 3.6: Generic Structure of an assembly statement\n3.2.4 Types of Instructions\nClassification by Functionality\nThe four major types of instructions are as follows:\n1. Data Processing Instructions: Data processing instructions are typically arithmetic\ninstructions such as add, subtract, and multiply, or logical instructions that compute\nbitwise or, and exclusive or. Comparison instructions also belong to this family.\n2. Data Transfer Instructions: Theseinstructionstransfervaluesbetweentwolocations.\nA location can be a register or a memory address.\n3. Branch Instructions: Branch instructions help the processor\u2019s control unit to jump\nto different parts of the program based on the values of operands. They are useful in\nimplementing for loops and if-then-else statements.\n4. Exception Generating Instructions: These specialised instructions help transfer\ncontrol from a user level program to the operating system.\nIn this book we shall cover data processing, data transfer, and control instructions.\nClassification based on the Number of Operands\nAs mentioned in Section 3.2.3, all assembly language statements in the GNU assembler have\nthe same structure. They start with the name of the instruction, and are succeeded by a list of\noperands. We can classify instructions based on the number of operands that they require. If\nan instruction requires n operands, then we typically say that it is in the n-address format. For\nexample, an instruction that does not require any operands is a 0-address format instruction.\nIf it requires 3 operands, then it is a 3-address format instruction. (cid:13)c Smruti R. Sarangi 104\nDefinition 31\nIf an instruction requires n operands (including source and destination), then we say that\nit is a n-address format instruction.\nIn ARM most of the data processing instructions are in the 3-address format, and data\ntransfer instructions are in the 2-address format. However, in x86 most of the instructions are\ninthe2-addressformat. Thefirstquestionthatcomestoourmindiswhatisthelogicofhaving\na 3-address format instruction versus having a 2-address format instruction? There must be\nsome tradeoff here.\nLet us outline some general rules of thumb. If an instruction has more operands then it will\nrequire more bits to represent the instruction. Consequently, we will require more resources to\nstore, and handle instructions. However, there is a flip side to this argument. Having more\noperands will also make the instruction more generic and flexible. It will make the life of\ncompiler writers and assembly programmers much easier, because it will be possible to do more\nthings with an instruction that uses more operands. The reverse logic applies to instructions\nthat take less operands. They take less space to store, and are less flexible. Let us consider an\nexample. Assume that we are trying to add two numbers, 3 and 5, to produce a result, 8.\nAn ARM instruction for addition would look like this:\nadd r3, r1, r2\nThis instruction adds the contents of registers, r1(3), and r2(5), and saves it in r3(8). However,\nan x86 instruction would look like this:\nadd edx, eax\nHere, we assume that edx contains 3, and eax contains 5. The addition is performed, and\nthe result, 8, is stored back in edx. Thus, in this case the x86 instruction is in the 2-address\nformat because the destination register is the same as the first source register.\nWhen we describe the details of the ARM and x86 instruction sets in Chapters 4 and 5, we\nshall see many more examples of instructions that have different address formats. We will be\nable to appreciate the tradeoffs of having different address formats in all their glory.\n3.2.5 Types of Operands\nLet us now look at the different types of operands. The method of specifying and accessing an\noperand in an assembly statement is known as the addressing mode.\nDefinition 32\nThe method of specifying and accessing an operand in an assembly statement is known as\nthe addressing mode. 105 (cid:13)c Smruti R. Sarangi\nThesimplestwayofspecifyinganoperandisbyembeddingitsvalueintheinstruction. Most\nassemblylanguagesallowtheusertospecifythevaluesofintegerconstantsasanoperand. This\naddressing mode is known as the immediate addressing mode. This method is very useful for\ninitialising registers or memory locations, or for performing arithmetic operations.\nOnce the requisite set of constants have been loaded into registers and memory locations,\nthe program needs to proceed by operating on registers and memory locations. There are\nseveral addressing modes in this space. Before introducing them, let us introduce some extra\nterminology in the form of the register transfer notation.\nRegister Transfer Notation\nThis notation allows us to specify the semantics of instructions and operands. Let us look at\nthe various methods to represent the basic actions of instructions.\nr1 \u2190 r2\nThis expression has two register operands r1, and r2. r1 is the destination register, and r2\nis the source register. We are transferring the contents of register r2 to register r1.\nWe can specify an add operation with a constant as follows:\nr1 \u2190 r2+4\nWe can also specify operations on registers using this notation. We are adding the contents\nof r2 and r3 and saving the result in r1.\nr1 \u2190 r2+r3\nIt is also possible to represent memory accesses using this notation.\nr1 \u2190 [r2]\nIn this case the memory address is saved in r2. The processor hardware fetches the memory\naddress in r2, accesses the location, fetches the contents of the memory location, and saves the\ndata item in r1. Let us assume that the value in r2 is 100. In this case the processor accesses\nmemory with address 100, fetches the integer saved in locations (100-103), and saves it in r1.\nBy default we assume that we are loading and saving integers.\nWe can also specify a more complicated memory address of the form:\nr1 \u2190 [r2+4]\nHere, thememoryaddressisequaltothecontentsoftheregisterr2plus4. Wefetchtheinteger\nstarting at the contents of this memory address, and save it in the register r1.\nGeneric Addressing Modes for Operands\nLet us represent the value of an operand as V. In the subsequent discussion, we use expressions\nsuch as V \u2190 r1. This does not mean that we have a new storage location called V. It basically\nmeans that the value of an operand is specified by the RHS (right hand side). Let us briefly\ntake a look at some of the most commonly used addressing modes with examples. (cid:13)c Smruti R. Sarangi 106\nimmediate V \u2190 imm\nUses the constant imm as the value of the operand.\nregister V \u2190 r1\nIn this addressing mode, the processor uses the value contained in a register as the\noperand.\nregister-indirect V \u2190 [r1]\nThe register saves the address of the memory location that contains the value.\nbase-offset V \u2190 [r1+offset]\noffset is a constant. The processor fetches the base memory address from r1, adds the\nconstant offset to it, and accesses the new memory location to fetch the value of the\noperand. The offset is also known as the displacement.\nbase-index V \u2190 [r1+r2]\nr1 is the base register, and r2 is the index register. The memory address is equal to\n(r1+r2).\nbase-index-offset V \u2190 [r1+r2+offset]\nThe memory address that contains the value is (r1 + r2 + offset), where offset is a\nconstant.\nmemory-direct V \u2190 addr\nThe value is contained in memory starting from address addr. addr is a constant. In this\ncase the memory address is directly embedded in the instruction.\nmemory-indirect V \u2190 [[r1]]\nThe value is present in a memory location, whose address is contained in the memory\nlocation, M. Furthermore, the address of M is contained in the register, r1.\nPC-relative V \u2190 [PC +offset]\nHere, offset is a constant. The memory address is computed to be PC +offset, where\nPC represents the value contained in the PC. This addressing mode is useful for branch\ninstructions.\nLet us introduce a new term called the effective memory address by considering the base-\noffset addressing mode. The memory address is equal to the contents of the base register plus\nthe offset. The computed memory address is known as the effective memory address. We can\nsimilarlydefinetheeffectiveaddressforotheraddressingmodesinthecaseofmemoryoperands.\n.\nDefinition 33\nThe memory address specified by an operand is known as the effective memory address. 107 (cid:13)c Smruti R. Sarangi\n3.3 SimpleRisc\nIn this book, we shall introduce a simple, generic, complete and concise RISC ISA called\nSimpleRisc . The assembly language of SimpleRisc has just 21 instructions, and captures most\nof the features of full scale assembly languages. We will use SimpleRisc to demonstrate the\nflavour of different types of assembly programs, and also design a processor for the SimpleRisc\nISAinChapter8. WeshallassumethatSimpleRisc assemblyfollowstheGNUassemblyformat,\nand we shall only describe the text section in this book.\nBefore proceeding further, let us take a tour of different instruction sets, and take a look at\ntheir properties.\n3.3.1 Different Instruction Sets\nIn Chapter 1, we looked at properties of different instruction sets including necessary, and\ndesirable properties. In this book, we shall describe two real instruction sets namely the ARM\ninstruction set and x86 instruction set. ARM stands for \u201cAdvanced RISC Machines\u201d. It is\nan iconic company based out of Cambridge, UK. As of 2012, around 90% of mobile devices\nincluding the Apple iPhone, and iPad, run on ARM based processors. Similarly, as of 2012,\nmore than 90% of the desktops and laptops run on Intel or AMD based x86 processors. ARM\nis a RISC instruction set, and x86 is a CISC instruction set.\nThere are many other instruction sets tailored for a wide variety of processors. Another\npopularinstructionsetformobilecomputersistheMIPSinstructionset. MIPSbasedprocessors\nare also used in a wide variety of processors used in automobiles, and industrial electronics.\nISA Type Year Vendor Bits Endianness Registers\nVAX CISC 1977 DEC 32 little 16\nRISC 1986 Sun 32 big 32\nSPARC\nRISC 1993 Sun 64 bi 32\nRISC 1992 Apple,IBM,Motorola 32 bi 32\nPowerPC\nRISC 2002 Apple,IBM 64 bi 32\nRISC 1986 HP 32 big 32\nPA-RISC\nRISC 1996 HP 64 big 32\nCISC 1979 Motorola 16 big 16\nm68000\nCISC 1979 Motorola 32 big 16\nRISC 1981 MIPS 32 bi 32\nMIPS\nRISC 1999 MIPS 64 bi 32\nAlpha RISC 1992 DEC 64 bi 32\nCISC 1978 Intel,AMD 16 little 8\nx86 CISC 1985 Intel,AMD 32 little 8\nCISC 2003 Intel,AMD 64 64 little 16\nRISC 1985 ARM 32 bi (little default) 16\nARM\nRISC 2011 ARM 64 bi (little default) 31\nTable 3.1: List of instruction sets (cid:13)c Smruti R. Sarangi 108\nFor large servers, typically IBM (PowerPC), Sun (now Oracle)(UltraSparc), or HP (PA-\nRISC) processors are used. Each of these processor families has its own instruction set. These\ninstruction sets are typically RISC instruction sets. Most ISAs share simple instructions such\nas add, subtract, multiply, shifts, and load\/store instructions. However, beyond this simple\nset, they use a large number of more specialised instructions. As we shall see in the next few\nchapters, choosing the right set of instructions in an ISA is dependent on the target market of\nthe processor, the nature of the workload, and many design time constraints. Table 3.1 shows\na list of popular instruction sets. The SimpleRisc ISA is conceptually the closest to ARM and\nMIPS; however, it has some significant differences also.\n3.3.2 Model of the SimpleRisc Machine\nSimpleRisc assumes that we have 16 registers numbered r0...r15. The first 14 registers are\ngeneral purpose registers, and can be used for any purpose within the program. Register r14\nis known as the stack pointer. We shall also refer to it as sp. Register r15 is known as the\nreturn address register, and it will also be referred as ra. We shall discuss sp and ra, when we\ndiscuss how to implement functions in SimpleRisc . Each register is 32 bits wide. We assume\na special internal register called flags, which is not visible to the programmer. It contains two\nfields flags.E(equal) and flags.GT(greater than). E is set to 1 if the result of a comparison\nis equality, and GT is set to 1 if a comparison concludes that the first operand is greater than\nthe second operand. The default values of both the fields are 0.\nEachinstructionisencodedintoa32-bitvalue, anditrequires4bytesofstorageinmemory.\nSimpleRisc assumesamemorymodelsimilartotheVonNeumannmachineaugmentedwith\nregisters as described in Section 1.7.3. The memory is a large array of bytes. A part of it saves\nthe program and the rest of the memory is devoted to storing data. We assume that multibyte\ndata types such as integers are saved in the little endian format.\n3.3.3 Register Transfer Instruction \u2013 mov\nThe mov instruction is a 2-address format instruction that can transfer values from one register\nto another, or can load a register with a constant. Our convention is to always have the\ndestination register at the beginning. Refer to Table 3.2. The size of the signed immediate\noperand is limited to 16 bits. Hence, its range is between \u2212215 to 215\u22121.\nSemantics Example Explanation\nmov r1, r2 r1 \u2190 r2\nmov reg, (reg\/imm)\nmov r1, 3 r1 \u2190 3\nTable 3.2: Semantics of the mov instruction\n3.3.4 Arithmetic Instructions\nSimpleRisc has6arithmeticinstructions\u2013add,sub,mul,div,mod,andcmp. Theconnotations\nof add, sub, and mul are self explanatory (also see Table 3.3). For arithmetic instructions, we\nassume that the first operand in the list of operands is the destination register. The second 109 (cid:13)c Smruti R. Sarangi\noperand is the first source operand, and the third operand is the second source operand. The\nfirstandsecondoperandsneedtoberegisters, whereasthelastoperand(secondsourceregister)\ncan be an immediate value.\nSemantics Example Explanation\nadd r1, r2, r3 r1 \u2190 r2+r3\nadd reg, reg, (reg\/imm)\nadd r1, r2, 10 r1 \u2190 r2+10\nsub reg, reg, (reg\/imm) sub r1, r2, r3 r1 \u2190 r2\u2212r3\nmul reg, reg, (reg\/imm) mul r1, r2, r3 r1 \u2190 r2\u00d7r3\ndiv reg, reg, (reg\/imm) div r1, r2, r3 r1 \u2190 r2\/r3 (quotient)\nmod reg, reg, (reg\/imm) mod r1, r2, r3 r1 \u2190 r2 mod r3 (remainder)\ncmp reg, (reg\/imm) cmp r1, r2 set flags\nTable 3.3: Semantics of arithmetic instructions in SimpleRisc\nExample 26\nWrite assembly code in SimpleRisc to compute: 31 * 29 - 50, and save the result in r4.\nAnswer:\nSimpleRisc\nmov r1, 31\nmov r2, 29\nmul r3, r1, r2\nsub r4, r3, 50\nThediv instructiondividesthefirstsourceoperandbythesecondsourceoperand, computes\nthe quotient, and saves it in the destination register. For example it will compute 30\/7 to be\n4. The mod instruction computes the remainder of a division. For example, it will compute\n30 mod 7 as 2.\nExample 27\nWrite assembly code in SimpleRisc to compute: 31 \/ 29 - 50, and save the result in r4.\nAnswer:\nSimpleRisc\nmov r1, 31\nmov r2, 29\ndiv r3, r1, r2\nsub r4, r3, 50\nThe cmp instruction is a 2-address instruction that takes two source operands. The first\nsource operand needs to be a register, and the second one can be an immediate or a register. It (cid:13)c Smruti R. Sarangi 110\ncomparesboththeoperandsbysubtractingthesecondfromthefirst. Iftheoperandsareequal,\nor in other words the result of the subtraction is zero, then it sets flags.E to 1. Otherwise\nflags.E is set to 0. If the first operand is greater than the second operand, then the result of\nthe subtraction will be positive. In this case, the cmp instruction sets flags.GT to 1, otherwise\nit sets it to 0. We will require these flags when we implement branch instructions.\n3.3.5 Logical Instructions\nSimpleRisc has three logical instructions \u2013 and, or, and not. and and or are 3-address instruc-\ntions. They compute the bitwise AND and OR of two values respectively. The not instruction\nis a 2-address instruction that computes the bitwise complement of a value. Note that the\nsource operand of the not instruction can be an immediate or a register. Refer to Table 3.4.\nSemantics Example Explanation\nand reg, reg, (reg\/imm) and r1, r2, r3 r1 \u2190 r2\u2227r3\nor reg, reg, (reg\/imm) or r1, r2, r3 r1 \u2190 r2\u2228r3\nnot reg, (reg\/imm) not r1, r2 r1 \u2190\u223c r2\n\u2227 bitwise AND, \u2228 bitwise OR, \u223c logical complement\nTable 3.4: Semantics of logical instructions in SimpleRisc\nExample 28\nCompute (a\u2228b). Assume that a is stored in r0, and b is stored in r1. Store the result in\nr2.\nAnswer:\nSimpleRisc\nor r3, r0, r1\nnot r2, r3\n3.3.6 Shift Instructions \u2013 lsl, lsr, asr\nSimpleRisc has three types of shift instructions lsl (logical shift left), lsr (logical shift right),\nand asr (arithmetic shift right). Each of these instructions are in the 3-address format. The\nfirst source operand points to the source register, and the second source operand contains the\nshift amount. The second operand can either be a register or an immediate value.\nThe lsl instruction shifts the value in the first source register to the left Similarly, lsr, shifts\nthe value in the first source register to the right. Note that it is a logical right shift. This means\nthat it fills all the MSB positions with zeros. In comparison, asr, performs an arithmetic right\nshift. It fills up all the MSB positions with the value of the previous sign bit. Semantics of shift\ninstructions are shown in Table 3.5. 111 (cid:13)c Smruti R. Sarangi\nSemantics Example Explanation\nlsl r3, r1, r2 r3 \u2190 r1 (cid:28) r2 (shift left)\nlsl reg, reg, (reg\/imm)\nlsl r3, r1, 4 r3 \u2190 r1 (cid:28) 4 (shift left)\nlsr r3, r1, r2 r3 \u2190 r1 \u226b r2 (shift right logical)\nlsr reg, reg, (reg\/imm)\nlsr r3, r1, 4 r3 \u2190 r1 \u226b 4 (shift right logical)\nasr r3, r1, r2 r3 \u2190 r1 (cid:29) r2 (arithmetic shift right)\nasr reg, reg, (reg\/imm)\nasr r3, r1, 4 r3 \u2190 r1 (cid:29) 4 (arithmetic shift right)\nTable 3.5: Semantics of shift instructions in SimpleRisc\n3.3.7 Data Transfer Instructions: ld and st\nSimpleRisc has two data transfer instructions \u2013 load(ld) and store(st). The load instructions\nloads values from memory into registers, and the store instruction saves values in registers to\nmemory locations. Examples and semantics are shown in Table 3.6.\nSemantics Example Explanation\nld reg, imm[reg] ld r1, 12[r2] r1 \u2190 [r2+12]\nst reg, imm[reg] st r1, 12[r2] [r2+12] \u2190 r1\nTable 3.6: Semantics of load-store instructions in SimpleRisc\nLet us consider the load instruction: ld r1,12[r2]. Here, we are computing the memory\naddress as the sum of the contents of r2 and the number 12. The ld instructions accesses this\nmemory address, fetches the stored integer and stores it in r1. We assume that the computed\nmemory address points to the first stored byte of the integer. Since we assume a little endian\nrepresentation, the memory address contains the LSB. The details are shown in Figure 3.7(a).\nThe store operation does the reverse. It stores the value of r1 into the memory address (r2\n+ 12). Refer to Figure 3.7(b).\n3.3.8 Unconditional Branch Instructions\nSimpleRischasoneunconditionalbranchinstruction,b,whichmakestheprogramcounterjump\nto the address corresponding to a label in the code. It takes a single operand, which is a label\nin the program. Its semantics is shown in Table 3.7.\nSemantics Example Explanation\nb label b .foo branch to .foo\nTable 3.7: Semantics of unconditional branch instructions in SimpleRisc\nLet us explain its operation with the help of a simple example, as shown below. (cid:13)c Smruti R. Sarangi 112\nld r1, 12[r2] st r1, 12[r2]\nMemory Memory\nRegister Register\nfile 12 file 12\nr1 r1\nr2 r2\n(a) (b)\nFigure 3.7: Load and store operations in SimpleRisc\nadd r1, r2, r3\nb .foo\n...\n...\n.foo:\nadd r3, r1, r4\nIn this example, we add the values of r2, and r3, and then save the result in r1. After that,\nthe processor jumps to the code pointed to by the label, .foo. It proceeds to execute the code\nafter the label, .foo. It starts out by executing the instruction add r3,r1,r4. It then proceeds\nto execute subsequent instructions.\n3.3.9 Conditional Branch Instructions\nSimpleRisc has two conditional branch instructions \u2013 beq and bgt. Real world instruction\nsets typically have more branch instructions. Nonetheless, at the cost of code size, these two\ninstructions are sufficient for implementing all types of branches.\nThe beq instruction stands for \u201cbranch if equal\u201d. This means that if any preceding cmp\ninstruction has set the E flag, then the PC will branch to the label specified in this instruction.\nOtherwise, the branch is said to fail, and the processor will proceed to execute the instruction\nafter the branch. Similarly, the bgt instruction stands for \u201cbranch if greater than\u201d. This branch\ninstructionbasesitsoutcomeonthevalueoftheGT flag. Itifissetto1, thenitbranchestothe\nlabel specified in the branch instruction, otherwise the processor executes the next instruction\nafter the branch. Refer to Table 3.8. 113 (cid:13)c Smruti R. Sarangi\nSemantics Example Explanation\nbeq label beq .foo branch to .foo if flags.E = 1\nbgt label bgt .foo branch to .foo if flags.GT = 1\nTable 3.8: Semantics of ranch instructions in SimpleRisc\nExample 29\nWrite an iterative program to compute the factorial of a number stored in r0. Assume that\nthe number is greater than 2. Save the result in r1.\nAnswer: Let us first take a look at a small C program to compute the factorial of the\nvariable num.\nC\nint prod = 1;\nint idx;\nfor(idx = num; idx > 1; idx --) {\nprod = prod * idx\n}\nLet us now try to convert this program to SimpleRisc .\nSimpleRisc\nmov r1, 1 \/* prod = 1 *\/\nmov r2, r0 \/* idx = num *\/\n.loop:\nmul r1, r1, r2 \/* prod = prod * idx *\/\nsub r2, r2, 1 \/* idx = idx - 1 *\/\ncmp r2, 1 \/* compare (idx, 1) *\/\nbgt .loop \/* if (idx > 1) goto .loop*\/\nExample 30 Write an assembly program to find out if the number stored in r1 is a prime\nnumber. Assume that it is greater than 3. Save the Boolean result in r0.\nAnswer:\nSimpleRisc\nmov r2, 2\n.loop:\nmod r3, r1, r2 @ divide number by r2\ncmp r3, 0 @ compare the result with 0\nbeq .notprime @ if the result is 0, not prime\nadd r2, r2, 1 @ increment r2\ncmp r1, r2 @ compare r2 with the number (cid:13)c Smruti R. Sarangi 114\nbgt .loop @ iterate if r2 is smaller\nmov r0, 1 @ number is prime\nb .exit @ exit\n.notprime:\nmov r0, 0 @ number is not prime\n.exit:\nExample 31 Write an assembly program to find the least common multiple (LCM) of two\npositive numbers stored in r1 and r2. Save the result in r0.\nAnswer:\nSimpleRisc\n@ let the numbers be A(r1) and B(r2)\n@ iterate\nmov r3, 1 @ idx = 1\nmov r4, r1 @ L = A\n.loop:\nmod r5, r4, r2 @ tmp = L % b\ncmp r5, 0 @ compare mod with 0\nbeq .lcm @ LCM found (L is the LCM)\nadd r3, r3, 1 @ increment idx\nmul r4, r1, r3 @ L = A * idx\nb .loop\n.lcm:\nmov r0, r4 @ result is equal to L\n3.3.10 Functions\nNow, that we have seen generic instructions, operands, and addressing modes, let us come\nto one of the most advanced features in high level programming languages that makes their\nstructure extremely modular namely functions (also referred to as subroutines or procedures\nin some languages). If the same piece of code is used at different points in a program, then it\ncan be encapsulated in a function. The following example shows a function in C to add two\nnumbers. 115 (cid:13)c Smruti R. Sarangi\nint addNumbers(int a, int b) {\nreturn (a+b);\n}\nCalling and Returning from Functions\nLet us now go over the basic requirements to implement a simple function. Let us assume\nthat an instruction with address A calls a function foo. After executing function foo, we need\nto come back to the instruction immediately after the instruction at A. The address of this\ninstruction is A+4 (if we assume that the instruction at A is 4 bytes long). This process is\nknown as returning from a function, and the address (A+4) is known as the return address.\nDefinition 34\nReturn address: It is the address of the instruction that a process needs to branch to after\nexecuting a function.\nThus,therearetwofundamentalaspectsofimplementingafunction. Thefirstistheprocess\nof invoking or calling a function, and the second aspect deals with returning from a function.\nLet us consider the process of calling a function in bit more detail. A function is essentially\na block of assembly code. Calling a function is essentially making the PC point to the start\nof this block of code. We have already seen a method to implement this functionality when\nwe discussed branch instructions. We can associate a label with every function. The label\nshould be associated with the first instruction in a function. Calling a function is as simple as\nbranching to the label at the beginning of a function. However, this is only a part of the story.\nWe need to implement the return functionality as well. Hence, we cannot use an unconditional\nbranch instruction to implement a function call.\nLetusthusproposeadedicatedfunctioncallinstructionthatbranchestothebeginningofa\nfunction, and simultaneously saves the address that the function needs to return to (referred to\nas the return address). Let us consider the following C code, and assume that each C statement\ncorresponds to one line of assembly code.\na = foo(); \/* Line 1 *\/\nc = a + b; \/* Line 2 *\/\nIn this small code snippet, we use a function call instruction to call the foo function. The\nreturnaddressistheaddressoftheinstructioninLine2. Itisnecessaryforthecallinstructionto\nsave the return address in a dedicated storage location such that it can be retrieved later. Most\nRISC instruction sets (including SimpleRisc ) have a dedicated register known as the return\naddress register to save the return address. The return address register gets automatically\npopulated by a function call instruction. When we need to return from a function, we need to\nbranch the address contained in the return address register. In SimpleRisc , we devote register\n15 to save the return address, and refer to it as ra.\nWhathappensiffoocallsanotherfunction? Inthiscase,thevalueinrawillgetoverwritten.\nWe will look at this issue later. Let us now consider the problem of passing arguments to a\nfunction, and getting return values back. (cid:13)c Smruti R. Sarangi 116\nPassing Arguments and Return Values\nAssume that a function foo invokes a function foobar. foo is called the caller, and foobar is\ncalled the callee. Note that the caller-callee relationships are not fixed. It is possible for foo\nto call foobar, and also possible for foobar to call foo in the same program. The caller and\ncallee are decided for a single function call based on which function is invoking the other.\nDefinition 35\ncaller A function, foo, that has called another function, foobar.\ncallee A function, foobar, that has been called by another function, foo.\nBoth the caller and the callee see the same view of registers. Consequently, we can pass\narguments through the registers, and likewise pass the return values through registers also.\nHowever, there are several issues in this simple idea as we enumerate below (Assume that we\nhave 16 registers).\n1. A function can take more than 16 arguments. This is more than the number of general\npurposeregistersthatwehave. Hence,weneedtofindaextraspacetosavethearguments.\n2. A function can return a large amount of data, for example, a large structure in C. It\nmight not be possible for this piece of data to fit in registers.\n3. The callee might overwrite registers that the caller might require in the future.\nWe thus observe that passing arguments and return values through registers works only\nfor simple cases. It is not a very flexible and generic solution. Nonetheless, there are two\nrequirements that emerge from our discussion.\nSpace Problem We need extra space to send and return more arguments.\nOverwrite Problem We need to ensure that the callee does not overwrite the registers of the\ncaller.\nTo solve both the problems, we need to take a deeper look at how functions really work.\nWe can think of a function \u2013 foo \u2013 as a black box to begin with. It takes a list of arguments\nand returns a set of values. To perform its job, foo can take one nano-second, or one week,\nor even one year. foo might call other functions to do its job, send data to I\/O devices, and\naccess memory locations. Let us visualise the function, foo, in Figure 3.8.\nTo summarise, a generic function processes the arguments, reads and writes values from\nmemory and I\/O devices if required, and then returns the result. Regarding memory and I\/O\ndevices, we are not particularly concerned at this point of time. There is a large amount of\nmemory available, and space is not a major constraint. Reading and writing I\/O devices is also 117 (cid:13)c Smruti R. Sarangi\nRead\/write Read\/write\nI\/O devices memory\nArguments foo Return values\nFigure 3.8: Function foo as a black box.\ntypically not associated with space constraints. The main issue is with registers, because they\nare in short supply.\nLet us solve the space problem first. We can transfer values through both registers and\nmemory. For simplicity, if we need to transfer a small amount of data, we can use registers,\notherwise we can transfer them through memory. Similarly, for return values, we can transfer\nvalues through memory. We are not limited by space constraints if we use memory to transfer\ndata. However, this approach suffers from lack of flexibility. This is because there has to be\nstrict agreement between the caller and the callee regarding the memory locations to be used.\nNote that we cannot use a fixed set of memory locations, because it is possible for the callee to\nrecursively call itself.\nrecursive function call\nfoobar() {\n...\nfoobar();\n...\n}\nAn astute reader might argue that it is possible for the callee to read the arguments from\nmemory and transfer them to some other temporary area in memory and then call other func-\ntions. However, such approaches are not elegant and not very efficient also. We shall look at\nmore elegant solutions later.\nHence, at this point, we can conclude that we have solved the space problem partially. If\nwe need to transfer a few values between the caller and the callee or vice versa, we can use\nregisters. However, if the arguments\/return values do not fit in the set of available registers,\nthen we need to transfer them through memory. For transferring data through memory, we\nneed an elegant solution that does not require a strict agreement between the caller and the\ncallee regarding the memory locations used to transfer data. We shall consider such solutions\nin Section 3.3.10.\nDefinition 36\nThe notion of saving registers in memory and later restoring them is known as register\nspilling. (cid:13)c Smruti R. Sarangi 118\nTo solve the overwrite problem, there are two solutions. The first is that the caller can\nsavethesetofregistersitrequiresinadedicatedlocationinmemory. Itcanlaterretrieveitsset\nof registers after the callee finishes, and returns control to the caller. The second solution is for\nthe callee to save and restore the registers that it will require. Both the approaches are shown\nin Figure 3.9. This method of saving the values of registers in memory, and later retrieving\nthem is known as spilling.\nCaller Caller\nSave registers\nCallee Callee\nSave registers\nRestore registers\nRestore registers\n(a) Caller saved (b) Callee saved\nFigure 3.9: Caller saved and callee saved registers\nHere, we have the same problem again. Both the caller and the callee need to have a strict\nagreement on the locations in memory that need to be used. Let us now try to solve both the\nproblems together.\nThe Stack\nWesimplifiedtheprocessofpassingargumentstoandfromafunction, andsaving\/restoringthe\nregisters using dedicated locations in memory. However, this solution was found to be inflexible\nand it can be quite complex to implement for large real world programs. To simplify this idea,\nlet us find a pattern in function calls.\nA typical C or Java program starts with the main function. This function then calls other\nfunctions, which might in turn call other functions, and finally the execution terminates when\nthe main function exits. Each function defines a set of local variables and performs a com-\nputation on these variables and the function arguments. It might also call other functions.\nFinally, the function returns a value and rarely a set of values (structure in C). Note that after\na function terminates, the local variables, and the arguments are not required anymore. Hence,\nif some of these variables or arguments were saved in memory, we need to reclaim the space.\nSecondly, if the function has spilled registers, then these memory locations also need to be freed\nafter it exits. Lastly, we note that if the callee calls another function, then it will need to save\nthe value of the return address register in memory. We will need to free this location also after\nthe function exits. 119 (cid:13)c Smruti R. Sarangi\nIt is best to save all of these pieces of information contiguously in a single region of memory.\nThis is known as the activation block of the function. Figure 3.10 shows the memory map of\nthe activation block.\nActivation block\nArguments\nReturn address\nRegister spill area\nLocal variables\nFigure 3.10: Activation block\nThe activation block contains the arguments, return address, register spill area (for both\ncaller saved and callee saved schemes), and the local variables. Once a function terminates, it\nis possible to get rid of the activation block entirely. If a function wants to return some values,\nthen it can either do so using registers. However, if it wants to return a large structure, then\nit can write it into the activation block of the caller. The caller can supply a location within\nits activation block where this data can be written. We shall see that it is possible to do this\nmore elegantly. Prior to explaining how this can be done, we need to look at how to arrange\nactivation blocks in memory.\nWe can have one memory region where all the activation blocks are stored in contiguous\nregions. Let us consider an example. Let us assume that function foo calls function foobar,\nwhich in turn calls foobarbar. Figure 3.11(a) - (d) show the state of memory at four points \u2013\n(a) just before calling foobar, (b) just before calling foobarbar, (c) after calling foobarbar, (d)\njust after foobarbar returns.\nWe observe that there is a last in first out behavior in this memory region. The function\nthat was invoked the last is the first function to finish. Such kind of a last in-first out structure\nis traditionally known as a stack in computer science. Hence, the memory region dedicated to\nsaving activation blocks is known as the stack. Traditionally, the stack has been considered\nto be downward growing (growing towards smaller memory addresses). This means that the\nactivation block of the main function starts at a very high location and new activation blocks\nare added just below (towards lower addresses) existing activation blocks. Thus the top of the\nstack is actually the smallest address in the stack, and the bottom of the stack is the largest\naddress. The top of the stack represents the activation block of the function that is currently\nexecuting, and the bottom of the stack represents the initial main function. (cid:13)c Smruti R. Sarangi 120\nStack\nfoo foo foo foo\nfoobar foobar foobar\nfoobarbar\n(a) (b) (c) (d)\nFigure 3.11: The state of the stack after several function calls\nDefinition 37\nThe stack is a memory region that saves all the activation blocks in a program.\n\u2022 It is traditionally considered to be downward growing.\n\u2022 Before calling a function, we need to push its activation block to the stack.\n\u2022 When a function finishes execution, we need to pop its activation block off the stack.\nDefinition 38\nThe stack pointer register maintains a pointer to the top of the stack.\nMost architectures save a pointer to the top of the stack in a dedicated register called the\nstack pointer. This register is r14 in SimpleRisc . It is also called sp. Note that for a lot of\narchitectures, the stack is a purely software structure. For them, the hardware is not aware\nof the stack. However, for some architectures such as x86, hardware is aware of the stack and\nuses it to push the return address or the values of other registers. However, even in this case\nthe hardware is not aware of the contents of each activation block. The structure is decided\nby the assembly programmer or the compiler. In all cases, the compiler needs to explicitly add\nassembly instructions to manage the stack.\nCreating a new activation block for the callee involves the following steps.\n1. Decrement the stack pointer by the size of the activation block. 121 (cid:13)c Smruti R. Sarangi\n2. Copy the values of the arguments.\n3. Initialise any local variables by writing to their corresponding memory locations if re-\nquired.\n4. Spill any registers (store to the activation block) if required.\nIt is necessary to destroy the activation block upon returning from a function. This can be\ntrivially done by adding the size of the activation block to the stack pointer.\nBy using a stack, we have solved all of our problems. The caller and the callee cannot\noverwrite each other\u2019s local variables. The local variables are saved in the activation blocks,\nand two activation blocks do not overlap. Along with variables it is possible to stop the callee\nfrom overwriting the caller\u2019s registers by explicitly inserting instructions to save registers in the\nactivation blocks. There are two methods of achieving this \u2013 caller-saved scheme and callee-\nsaved scheme. Secondly, there is no need to have an explicit agreement regarding the memory\narea that will be used to pass arguments. The stack can be used for this purpose. The caller\ncan simply push the arguments on the stack. These arguments will get pushed into the callee\u2019s\nactivation block, and the callee can easily use them. Similarly, while returning from a function\nthe callee can pass return values through the stack. It needs to first destroy its activation block\nby decrementing the stack pointer, and then it can push the return values on the stack. The\ncallerwillbeawareofthesemanticsofthecallee, andthusafterthecalleereturnsitcanassume\nthat its activation block has been effectively enlarged by the callee. The additional space is\nconsumed by the return values.\n3.3.11 Function Call\/Return Instructions\nSimpleRisc hastwoinstructionsforfunctions\u2013call andret. Thecall instructionstakesasingle\nargument \u2013 the label of the first instruction of the function. It transfers control to the label\nand saves the return address in register ra. The ret instructions transfers the contents of ra\nto the PC. It is a 0-address instruction because it does not require any operands. Table 3.9\nshows the semantics of these instructions. In Table 3.9, we assume that the address method\nprovides the address of the first instruction of the foo function. Secondly, the return address is\nequal to PC +4 because we assume that each instruction is 4 bytes long. call and ret can be\nthought of as branch instructions because they change the value of the PC. However, they are\nnot dependent on any condition such as the value stored in a register. Hence, these instructions\ncan conceptually be considered to be unconditional branch instructions.\nSemantics Example Explanation\ncall label call .foo ra \u2190 PC +4 ; PC \u2190 address(.foo);\nret ret PC \u2190 ra\nTable 3.9: Semantics of function call\/return instructions in SimpleRisc (cid:13)c Smruti R. Sarangi 122\nExample 32\nWrite a function in SimpleRisc that adds the values in registers r0, and r1, and saves the\nresult in r2.\nAnswer:\nSimpleRisc\n.foo:\nadd r2, r0, r1\nret\nExample 33\nWrite a function, foo, in SimpleRisc that adds the values in registers r0, and r1, and saves\nthe result in r2. Then write another function that invokes this function. The invoking\nfunction needs to first set r0 to 3, r1 to 5, and then invoke foo. After foo returns, it needs\nto add 10 to the result of foo, and finally save the sum in r3.\nAnswer:\nSimpleRisc\n.foo:\nadd r2, r0, r1\nret\n.main:\nmov r0, 3\nmov r1, 5\ncall .foo\nadd r3, r2, 10\nExample 34\nWrite a recursive function to compute the factorial of 10 that is initially stored in r0. Save\nthe result in r1.\nAnswer: Let us first take a look at a small C program to compute the factorial of the\nvariable num.\nC\nint factorial(int num) {\nif (num <= 1) return 1;\nreturn num * factorial(num - 1);\n}\nvoid main() { 123 (cid:13)c Smruti R. Sarangi\nint result = factorial(10);\n}\nLet us now try to convert this program to SimpleRisc .\nSimpleRisc\n.factorial:\ncmp r0, 1 \/* compare (1,num) *\/\nbeq .return\nbgt .continue\nb .return\n.continue:\nsub sp, sp, 8 \/* create space on the stack *\/\nst r0, [sp] \/* push r0 on the stack *\/\nst ra, 4[sp] \/* push the return address register *\/\nsub r0, r0, 1 \/* num = num - 1 *\/\ncall .factorial \/* result will be in r1 *\/\nld r0, [sp] \/* pop r0 from the stack *\/\nld ra, 4[sp] \/* restore the return address *\/\nmul r1, r0, r1 \/* factorial(n) = n * factorial(n-1) *\/\nadd sp, sp, 8 \/* delete the activation block *\/\nret\n.return:\nmov r1, 1\nret\n.main:\nmov r0, 10\ncall .factorial\nThis example uses the stack to save and restore the value of r0. In this case, the caller\nsaves and restores its registers.\n3.3.12 The nop Instruction\nLetusnowaddaninstructioncallednopthatdoesnothing. Unlikeotherinstructions,wedonot\nneed a table explaining the semantics of the instruction, because it does absolutely nothing!!!\nQuestion 4 Why on earth would we add an instruction that does not do anything? (cid:13)c Smruti R. Sarangi 124\nWewilljustifytheneedtohaveanopinstructioninourportfolioofinstructionsinChapter9.\nWe shall see that it is important to have an instruction that does not do anything to ensure\ncorrectness in execution. Let us for the time being bear with this extra instruction that does\nnotseemtohaveanypurpose. Thereaderwilldefinitelyappreciatetheneedforthisinstruction\nin Chapter 9, when we discuss pipelining.\n3.3.13 Modifiers\nLet us now consider the problem of loading a 32-bit constant into a register. The following\ncode snippet shows us how to load the constant 0xFB12CDEF.\n\/* load the upper two bytes *\/\nmov r0, 0xFB12\nlsl r0, r0, 16\n\/* load the lower two bytes with 0x CD EF *\/\nmov r1, 0x CDEF\nlsl r1, r1, 16\nlsr r1, r1, 16 \/* top 16 bits are zeros *\/\n\/* load all the four bytes *\/\nadd r0, r0, r1\nThis problem requires 6 instructions. The reader needs to note that loading constants is a\ncommon operation in programs. Hence, let us devise a mechanism to speedup the process, and\nload a constant in a register in two operations. Most assemblers provide directives to directly\nload constants. Nevertheless, these directives need to get translated into a basic sequence of\nassembly instructions. Thus directives do not fundamentally solve of our problem of loading\nconstants into registers of memory locations efficiently.\nWe shall achieve this by using modifiers. Let us assign a modifier, \u2018u\u2019, or \u2018h\u2019, to an ALU\ninstruction other than shift instructions. By default, we assume that when we load a 16-bit\nimmediate into a 32-bit register, the processor automatically performs sign extension. This 125 (cid:13)c Smruti R. Sarangi\nmeans that it sets each of the 16 MSB bits to the sign of the immediate. This preserves the\nvalue of the immediate. For example, if our immediate is equal to -2, then its hexadecimal\nrepresentation is 0x FF FE. If we try to store it in a register, then in effect, we are storing \u2013\n0x FF FF FF FE.\nLet us have two additional modes. Let us add the suffix \u2018u\u2019 to an instruction to make\nit interpret the immediate as an unsigned number. For example, the instruction movu r0, 0x\nFEAB,willload0x0000FEABintoregisterr0. Thissuffixallowsustospecify16-bitunsigned\nimmediate values. Secondly, let us add the suffix \u2018h\u2019 to an instruction to instruct it to load the\n16-bit immediate into the upper half of a register. For example, movh r0, 0x FEAB, effectively\nloads 0x FE AB 00 00, into r0. We can use modifiers with all ALU instructions, with the\nexception of shift instructions.\nLet us now consider the previous example of loading a 32-bit constant into a register. We\ncan implement it with two instructions as follows:\nmovh r0, 0xFB12 \/* r0 = 0xFB 12 00 00 *\/\naddu r0, r0, 0xCDEF \/* r0 = r0 + 0x00 00 CD EF *\/\nBy using modifiers, we can load constants in 2 instructions, rather than 6 instructions.\nFurthermore, it is possible to create generic routines using modifiers that can set the value of\nany single byte in a 4 byte register. These routines will require a lesser number of instructions\ndue to the use of modifiers.\n3.3.14 Encoding the SimpleRisc Instruction Set\nLetusnowtrytoencodeeachinstructiontoa32-bitvalue. Weobservethatwehaveinstructions\nin0,1,2and3addressformats. Secondly,someoftheinstructionstakeimmediatevalues. Hence,\nwe need to divide 32 bits into multiple fields. Let us first try to encode the type of instruction.\nSince there are 21 instructions, we require 5 bits to encode the instruction type. The code for\neach instruction is shown in Table 3.10. We can use the five most significant bits in a 32-bit\nfield to specify the instruction type. The code for an instruction is also known as its opcode.\nDefinition 39\nAn opcode is a unique identifier for each machine instruction.\nNow, let us try to encode each type of instruction starting from 0-address instructions.\nEncoding 0-Address Instructions\nThe two 0-address instructions that we have are ret, and nop. The opcode is specified by the\nfive most significant bits. In this case it is equal to 10100 for ret, and 10010 for b (refer to\nTable 3.10). Their encoding is shown in Figure 3.12. We only need to specify the 5 bit opcode\nin the MSB positions. The rest of the 27 bits are not required. (cid:13)c Smruti R. Sarangi 126\nInstruction Code Instruction Code Instruction Code\nadd 00000 not 01000 beq 10000\nsub 00001 mov 01001 bgt 10001\nmul 00010 lsl 01010 b 10010\ndiv 00011 lsr 01011 call 10011\nmod 00100 asr 01100 ret 10100\ncmp 00101 nop 01101\nand 00110 ld 01110\nor 00111 st 01111\nTable 3.10: List of instruction opcodes\n32\nopcode\n5\nFigure 3.12: Encoding the ret instruction\nEncoding 1-Address Instructions\nThe 1-address instructions that we have are call, b, beq, and bgt. In SimpleRisc assembly, they\ntake a label as an argument. While encoding the instruction we need to specify the address of\nthe label as the argument. The address of a label is the same as the address of the instruction\nthat it is pointing to. If the line after the label is empty, then we need to consider the next\nassembly statement that has an instruction.\nThesefourinstructionsrequire5bitsfortheiropcode. Theremaining27bitscanbeusedfor\nthe address. Note that a memory address is 32 bits long. Hence, we cannot cover the address\nspace with 27 bits. However, we can make two key optimisations. The first is that we can\nassume PC-relative addressing. We can assume that the 27 bits specify an offset (both positive\nand negative) with respect to the current PC. The branch statements in modern programs are\ngenerated because of for\/while loops or if-statements. For these constructs the branch target is\ntypically within a range of several hundred instructions. If we have 27 bits to specify the offset,\nand we assume that it is a 2\u2019s complement number, then the maximum offset in any direction\n(positive or negative) is 226. This is more than sufficient for almost all programs.\nThere is another important observation to be made. An instruction takes 4 bytes. If\nwe assume that all instructions are aligned to 4-byte boundaries, then all starting memory\naddresses of instructions will be a multiple of 4. Hence, the least two significant binary digits\nof the address will be 00. There is no reason for wasting bits in trying to specify them. We can\nassume that the 27 bits specify the offset of the address of the memory word (in units of 4-byte\nmemory words) that contains the instruction. With this optimisation, the offset from the PC\nin terms of bytes becomes 29 bits. This number should suffice for even the largest programs. 127 (cid:13)c Smruti R. Sarangi\nJust in case, there is a pathological example, in which the branch target is more than 228 bytes\naway, then the assembler needs to chain the branches such that one branch will call another\nbranch and so on. However, this would be a very rare case. The encoding for these instructions\nis shown in Figure 3.13.\n32\nopcode offset\nop offset\n5 27\nFigure 3.13: Encoding of 1-address instructions(branch format)\nNote that the 1-address instruction format finds a use for the unused bits in the 0-address\nformat. We can think of the 0-address format for the ret instruction as a special case of the\n1-address format. Let us refer to the 1-address format as the branch format. Let us name\nthe fields in this format. Let us call the opcode portion of the format as op, and the offset as\noffset. The op field contains the bits in positions 28-32, and the offset field contains the bits\nin positions 1-27.\nEncoding 3-Address Instructions\nLet us consider 3-address instructions first, and then look at other types of instructions. The\n3-address instructions in SimpleRisc are add, sub, mul, div, mod, and, or, lsl, lsr, and asr.\nLet us consider a generic 3-address instruction. It has a destination register, one input\nsource register, and a second source operand that can either be a register or an immediate. We\nneed to devote one bit to find out if the second source operand is a register or an immediate.\nLet us call this the I bit and specify it just after the opcode in the instruction. If I = 1, then\nthe second source operand is an immediate. If I = 0, the second source operand is a register.\nLet us now consider the case of 3-address registers that have their second source operand as\na register(I = 0). Since we have 16 registers, we require 4 bits to uniquely specify each register.\nRegister ri can be encoded as the unsigned 4-bit binary equivalent of i. Hence, to specify the\ndestinationregisterandtwoinputsourceregisters, werequire12bits. Thestructureisshownin\nFigure 3.14. Let us call this instruction format as the register format. Like the branch format\nlet us name the different fields \u2013 op (opcode, bits: 28-32), I (immediate present, bits:27), rd\n(destination register, bits: 23-26), rs1 (source register 1, bits: 19-22), and rs2 (source register\n2, bits:15-18).\nNow, if we assume that the second source operand is an immediate, then we need to set I\nto 1. Let us calculate the number of bits we have left for specifying the immediate. We have\nalready devoted 5 bits for the opcode, 1 bit for the I bit, 4 bits for the destination register, and\n4 bits for the first source register. In all, we have expended 14 bits. Hence, out of 32 bits, we\nare left with 18 bits, and we can use them to specify the immediate.\nWe propose to divide the 18 bits into two parts \u2013 2 bits (modifier) + 16 bits (constant part (cid:13)c Smruti R. Sarangi 128\n32\nopcode 0 dest reg src reg1src reg2\nop I rd rs1 rs2\n5 1 4 4 4\nFigure 3.14: Encoding 3-address instructions with register operands (register format)\nof the immediate). The two modifier bits can take three values \u2013 00 (default), 01 (\u2018u\u2019), and 10\n(\u2018h\u2019). The remaining 16 bits are used to specify a 16-bit 2\u2019s complement number when we are\nusing default modifiers. For the u and h modifiers, we assume that the 16-bit constant in the\nimmediate field is an unsigned number. In the rest of this book, we assume that the immediate\nfield is 18 bits long with a modifier part, and a constant part. The processor internally expands\nthe immediate to a 32-bit value, in accordance with the modifiers.\nThis encoding is shown in Figure 3.15. Let us call this instruction format as the immediate\nformat. Like the branch format let us name the different fields \u2013 op (opcode, bits: 28-32), I\n(immediate present, bits:27), rd (destination register, bits: 23-26), rs1 (source register 1, bits:\n19-22), and imm (immediate, bits:1-18).\n32\nopcode 1 dest reg src reg1 immediate\nop I rd rs1 imm\n2\n5 1 4 4 modifier bits 18\nFigure 3.15: Encoding 3-address instructions with an immediate source operand (immediate\nformat)\nExample 35\nEncode the instruction: sub r1, r2, 3.\nAnswer: Let us encode each field of the instruction. We have:\nField Encoding\nsub 00001\nI 1\nr1 0001\nr2 0010\n3 11 129 (cid:13)c Smruti R. Sarangi\nThus, the binary encoding is (spaces added for readability): 00001 1 0001 0010 00 0000\n0000 0000 0011. When we convert to hex, we get: 0x0C480003.\nEncoding cmp, not, and mov\nThecmpinstructionhastwosourceoperands. Thesecondsourceoperandcanbearegisteroran\nimmediate. We will use the standard 3-address register or immediate formats for encoding the\ncmp instruction. The destination register field will remain empty. See Figure 3.16. One of our\naims in designing the encoding is to keep things as simple and regular as possible such that the\nprocessor can decode the instruction very easily. We could have designed a separate encoding\nfor a 2-address instruction such as cmp. However, the gains would have been negligible, and\nby sticking to a fixed format, the processor\u2019s instruction decode logic becomes more straight\nforward.\nThe not and mov instructions have one destination register, and one source operand. This\nsource operand can be either an immediate or a register. Hence, we can treat the source\noperand of these instructions as the second source operand in the 3-address format, and keep\nthe field for the first source register empty for both of these instructions. The format is shown\nin Figure 3.16.\n32\ncmp\n00101 I rs1 rs2 \/ imm\n5 1 4 4 18\n32\nmov 01001 I rd rs2 \/ imm\n5 1 4 4 18\n32\nnot 01000 I rd rs2\/ imm\n5 1 4 4 18\nFigure 3.16: cmp, not, and mov instructions (cid:13)c Smruti R. Sarangi 130\nLoad and Store Instructions\nIn SimpleRisc the instructions \u2013 ld and st \u2013 are 2-address instructions. The second operand\npoints to a memory address. It uses a base-offset addressing mode. There is a base register,\nand an integer offset.\nFor a load instruction, there are three unique pieces of information that need to be encoded:\ndestination register, base register, and offset. In this case, we propose to use the three address\nimmediate format. The I bit is set to 1, because we need to specify an offset. The first source\nregister represents the base register, and the immediate represents the offset. Note that this\nencoding follows our principle of regularity and simplicity. Our aim is to reuse the 3-address\nregister and immediate formats for as many instructions as possible.\nNow, let us look at store instructions. Store instructions are slightly special in the sense\nthat they do not have a destination register. The destination of a store instruction is a memory\nlocation. This information cannot be encoded in the immediate format. However, for reasons\nof simplicity, we still want to stick to the formats that we have defined. We need to take a\ncrucial design decision here by answering Question 5.\nQuestion 5\nShould we define a new instruction format for the store instruction?\nLet us adjudge this case in the favor of not introducing a new format. Let us try to reuse\nthe immediate format. The immediate format has four fields \u2013 op, rd, rs1, and imm. The\nopcode field (op) need not be touched. We can assume that the format of the store instruction\nis: st rd, imm[rs1]. In this case, the field rd represents the register to be stored. Like the load\ninstruction we can keep the base register as rs1, and use the imm field to specify the offset. We\nbreak the pattern we have been following up till now by saving a source register in rd, which\nis meant to save a destination register. However, we were compelled to do this at the cost of\nnot introducing a new instruction format. Such design tradeoffs need to be made continuously.\nWe have to always balance the twin objectives of elegance and efficiency. It is sometimes not\npossible to choose the best of both worlds. In this case, we have gone for efficiency, because\nintroducing a new instruction format for just one instruction is overkill.\nTo conclude, figure 3.17 shows the encoding for load and store instructions.\nExample 36\nEncode the instruction: st r8, 20[r2].\nAnswer: Let us encode each field of the instruction. We have:\nField Encoding\nst 01111\nI 1\nr8 1000\nr2 0010\n20 0001 0100 131 (cid:13)c Smruti R. Sarangi\n32\nld rd, imm[rs1] 01110 1 rd rs1 imm\n5 1 4 4 18\n32\nst rd, imm[rs1] 01111 1 rd rs1 imm\n5 1 4 4 18\nFigure 3.17: Encoding of load and store instructions\nThus, the binary encoding is (spaces added for readability): 01111 1 1000 0010 00 0000\n0000 0001 0100. When we convert to hex, we get: 0x7E080014.\nSummary of Instruction Formats\nInthelastfewsubsections,wehavedescribedamethodtoencodeaninstructionintoasequence\nof bits (machine code). A compiler can use this method to translate a program written in a\nhigh level language to machine code, and thus create an executable program. It is now the\njob of the processor to execute this program by reading the instructions one by one. We have\nsubstantially made our life easy by assuming that each instruction is exactly 4 bytes long. The\nprocessor simply needs to start at the starting address of the program in memory and fetch one\ninstruction after the other. If an instruction is a branch, then the processor needs to evaluate\nthebranchcondition,andjumptothebranchtarget. Thepartoftheprocessorthatisprimarily\nconcerned about the details of the ISA is the decode logic or the decoder . It is the role of the\ndecoder to understand and decode an instruction. While designing an encoding for an ISA,\ncreating a simple and efficient instruction decoder was our prime objective.\nFormat Definition\nbranch op (28-32) offset (1-27)\nregister op (28-32) I (27) rd (23-26) rs1 (19-22) rs2 (15-18)\nimmediate op (28-32) I (27) rd (23-26) rs1 (19-22) imm (1-18)\nop \u2192 opcode, offset \u2192 branch offset, I \u2192 immediate bit, rd \u2192 destination register\nrs1 \u2192 source register 1, rs2 \u2192 source register 2, imm \u2192 immediate operand\nTable 3.11: Summary of instruction formats\nTo decode a SimpleRisc instruction, the first task is to find the instruction format. We have (cid:13)c Smruti R. Sarangi 132\ndefined three formats \u2013 branch, immediate, and register. Let us refer to Table 3.10. The six\nbranch format instructions are call, ret, beq, bgt, b, and nop. Recall that we encode both 0 and\n1-address format instructions in the branch format.\nThe opcodes of all the five branch instructions (b, beq, bgt, call, ret) have 1 as their most\nsignificant bit, whereas all other instructions have a 0 in their most significant position. Hence,\nfor a decoder to find out if an instruction is a branch is very easy. It just needs to take a look\nat the three most significant bit of the opcode. It should be 1. Moreover, to find out if an\ninstruction is a nop, the decoder needs to compare it with 01101, which requires a small circuit.\nIf an instruction is not in the branch format, then it must be in the immediate or register\nformat. This can be quickly decided by taking a look at the I bit. If it is 1, then the instruction\nis in the immediate format, otherwise it is in the register format. The formats are summarised\nin Table 3.11.\nLessons Learnt\nNow that we have designed a small instruction set of our own, looked at sample programs, and\nencoded our instructions, we are all set to design a processor for our SimpleRisc ISA. It needs\nto decode every single instruction, and execute it accordingly. Before proceeding further, let us\nlook back at how we designed our ISA, and how should ISAs be designed in general.\n1. The first step in designing an ISA is to study the workload that the ISA is being designed\nfor. In the case of SimpleRisc , we wanted to use it for running general purpose programs.\nThis meant that SimpleRisc needed to be simple, concise, generic, and complete as out-\nlined in Chapter 1. However, for different target workloads, the requirements might be\nvery different.\n2. Afterstudyingtheworkload,weneedtonextdecideonthenumberofinstructionsthatwe\nneed to have. Unless there are compelling requirements otherwise, it is not advisable to\nhave more than 64-128 instructions. More than 128 instructions will make the instruction\ndecoder very complex. It will also complicate the design of the processor.\n3. After finalising the number of instructions, we need to finalise the different types of\ninstructions. If we are designing an ISA for extensive numerical computation, then we\nshould have many arithmetic operations. If we are designing an ISA for processing text,\nthen we should have many instructions that can process strings (pieces of text). In the\ncase of SimpleRisc we devoted 6 instructions to arithmetic operations, 3 instructions to\nshift operations, 3 instructions to logical operations, 3 instructions to data transfer, 5\ninstructions to branch operations, and designated 1 instruction as no-op (no operation).\nWe chose this distribution because we expect to run a lot of general purpose programs\nthatwillhavecomplexarithmeticalandlogicalconstructs. Wecouldhaveverywellgotten\nrid of an instruction such as mod and replaced it with a sophisticated branch instruction,\nif we wanted to look at programs that will have a lot of branches. These subtle tradeoffs\nneed to be evaluated thoroughly.\n4. Once, wehavefinalisedthebroadtypesofinstructionsandthedistributionofinstructions\nacross these types, we come to the actual instructions themselves. In this case also, we\nwant to make the common case fast. For example, there is no point in having a division 133 (cid:13)c Smruti R. Sarangi\ninstructioninprogramsthatdonothavedivisionsoperations. Secondly,weneedtodecide\nthe format of each instruction in terms of the number and type of operands. For example,\nin SimpleRisc , all our arithmetic operations are in the 3-address format. If there is a\nrequirement from the side of processor designers that they want to reduce the number of\nregisters, then we can opt for the 2-address format. Alternatively, if we want to process\na massive amount of information in one go such as add a list of 10 numbers, then we can\neven have a 11-address format instruction.\n5. Once the format of the instruction is decided, we need to decide on the different ad-\ndressing modes. This decision has many ramifications. For example, if we allow the\nregister-indirect addressing mode in arithmetic instructions, then we need to add addi-\ntional hardware to access the memory and fetch the operand values. On the other hand,\nif we have a register-only addressing mode for arithmetic instructions, then their imple-\nmentation will be fast. However, the flip side is that we will need more registers, and\nmore dedicated load-store instructions to access memory. This tradeoff needs to be kept\nin mind.\n6. Once we have designed the set of instructions, we need to decide a proper encoding for\nit. The main aim should be to reduce the work of the instruction decoder. It is best to\nhave a small set of generic instruction formats that the decoder can quickly discern. We\nneed to balance elegance and efficiency such that the decoder can be simple yet efficient.\n3.4 Summary and Further Reading\n3.4.1 Summary\nSummary 3\n1. Assembly language is a textual representation of machine instructions. Each state-\nment in an assembly language program typically corresponds to one machine instruc-\ntion.\n2. An assembler is a program that converts an assembly language program to machine\ncode.\n3. An assembly language is specific to an ISA and an assembler.\n4. Assembly language is a vital tool for writing efficient programs, and for designing the\ncore routines of operating systems, and device drivers.\n5. Hardware designers learn assembly languages to understand the semantics of an ISA.\nIt tells them what to build.\n6. An assembly language program typically assumes a Von Neumann machine augmented\nwith a finite set of registers. (cid:13)c Smruti R. Sarangi 134\n7. A typical GNU assembly file contains a list of sections. Two important sections are\ntext and data. The text section contains the assembly statements that correspond to\nmachine code. The data section holds data and constants that the program will need\nduring its operation.\n8. A typical assembly statement contains an optional label to uniquely identify it, an in-\nstruction with a set of operands, and an optional comment. Instead of an instruction,\nit can also contain a directive that is a command to the assembler.\n9. There are typically four types of generic assembly instructions:\n(a) Data processing instructions \u2013 arithmetic and logical\n(b) Data transfer instructions \u2013 move, load, and store\n(c) Branch instructions \u2013 branch, function call, return\n(d) Exception generating instructions \u2013 transfer control to the operating system\nAn assembly language for a specific ISA also contains some machine specific instruc-\ntions also that are mainly used to set its configuration or invoke some special feature.\n10. The semantics of operands is also known as the addressing mode.\n11. The main addressing modes are immediate (specify constant in instruction), register-\ndirect (specify the register\u2019s name in the instruction), register-indirect (a register con-\ntains the memory address), and base-offset (the offset is added to the memory location\nin the base register).\n12. We designed the SimpleRisc assembly language that contains 21 instructions. It is a\ncomplete RISC ISA.\n13. We designed an encoding for each SimpleRisc instruction. We broadly defined three\ninstruction formats\nbranch Contains a 5 bit opcode and 27 bit offset.\nregister Encodes a 3-address instruction with two register source operands and one\nregister destination operand.\nimmediate Encodes a 3-address instruction that has an immediate as one of the\noperands.\nIn this chapter we have looked at the generic principles underlying different flavors of assembly\nlanguage. We constructed a small assembly language of our own for the SimpleRisc ISA, and\nproceeded to encode it. This information is sufficient to design a basic processor for SimpleRisc\nin Chapter 8. However, we would like to strongly advise the reader to at least study one of\nthe chapters on real world assembly languages \u2013 either ARM (Chapter 4) or x86 (Chapter 5).\nStudying a real language in all its glory will help the reader deepen her knowledge, and she can\nappreciate all the tricks that are required to make an ISA expressive. 135 (cid:13)c Smruti R. Sarangi\n3.4.2 Further Reading\nInstruction set design and the study of assembly languages are very old fields. Readers should\nrefer to classic computer architecture textbooks by Henessey and Patterson [Henessey and\nPatterson, 2010], Morris Mano [Mano, 2007], and William Stallings [Stallings, 2010] to get a\ndifferent perspective. For other simple instruction sets such as SimpleRisc , readers can read\nabout the MIPS [Farquhar and Bunce, 2012], and Sparc [Paul, 1993] instruction sets. Their\nearly variants are simple RISC instruction sets with up to 64 instructions, and a very regular\nstructure. Along with the references that we provide, there are a lot of excellently written\ntutorials and guides on the web for different ISAs.\nSince the last 10 years, a trend has started to move towards virtual instruction sets. Pro-\ngrams compiled for these instruction sets need to be compiled once again on a real machine\nsuch that the virtual instruction set can be translated to a real instruction set. The reasons for\ndoing so shall be described in later chapters. The Java language uses a virtual instruction set.\nDetails can be found in the book by Meyer et. al. [Downing and Meyer, 1997]. Readers can\nalso refer to a highly cited research paper that proposes the LLVA [Adve et al., 2003] virtual\ninstruction set.\nExercises\nAssembly Language Concepts\nEx. 1 \u2014 What is the advantage of the register-indirect addressing mode over the memory-\ndirect addressing mode?\nEx. 2 \u2014 When is the base-offset addressing mode useful?\nEx. 3 \u2014 Consider the base-scaled-offset addressing mode, which directs the hardware to au-\ntomatically multiply the offset by 4. When is this addressing mode useful?\nEx. 4 \u2014 Which addressing modes are preferable in a machine with a large number of regis-\nters?\nEx. 5 \u2014 Which addressing modes are preferable in a machine with very few registers?\nEx. 6 \u2014 Assume that we are constrained to have at the most two operands per instruction.\nDesign a format for arithmetic instructions such as add and multiply in this setting.\nAssembly Programming\nEx. 7 \u2014 Write simple assembly code snippets in SimpleRisc to compute the following: (cid:13)c Smruti R. Sarangi 136\ni) a+b+c\nii) a+b\u2212c\/d\niii) (a+b)\u22173\u2212c\/d\niv) a\/b\u2212(c\u2217d)\/3\nv) (a (cid:28) 2)\u2212(b (cid:29) 3) (((cid:28) (left shift logical), (cid:29) (left shift arithmetic))\nEx. 8 \u2014 Write a program to load the value 0xFFEDFC00 into r0. Try to minimise the\nnumber of instructions.\nEx. 9 \u2014 Write an assembly program to set the 5th bit of register r0 to the value of the 3rd\nbit of r1. Keep the rest of the contents of r0 the same. The convention is that the LSB is\nthe first bit, and the MSB is the 32nd bit. (Use less than or equal to 5 SimpleRisc assembly\nstatements)\nEx. 10 \u2014 Write a program in SimpleRisc assembly to convert an integer stored in memory\nfrom the little endian to the big endian format.\nEx. 11 \u2014 Write a program in SimpleRisc assembly to compute the factorial of a positive\nnumber using an iterative algorithm.\nEx. 12 \u2014 Write a program in SimpleRisc assembly to find if a number is prime.\nEx. 13 \u2014 Write a program in SimpleRisc assembly to test if a number is a perfect square.\nEx. 14 \u2014 Given a 32-bit integer in r3, write a SimpleRisc assembly program to count the\nnumber of 1 to 0 transitions in it.\n* Ex. 15 \u2014 Write a program in SimpleRisc assembly to find the smallest number that is a\nsum of two different pairs of cubes. [Note: 1729 is the Hardy-Ramanujan number. 1729 =\n123+13 = 103+93].\nEx. 16 \u2014 Write a SimpleRisc assembly program that checks if a 32-bit number is a palin-\ndrome. Assume that the input is available in r3. The program should set r4 to 1 if it is a\npalindrome, otherwise r4 should contain a 0. A palindrome is a number which is the same\nwhen read from both sides. For example, 1001 is a 4-bit palindrome.\nEx. 17 \u2014 Design a SimpleRisc program that examines a 32-bit value stored in r1 and counts\nthe number of contiguous sequences of 1s. For example, the value:\n01110001000111101100011100011111\ncontains six sequences of 1s. Write the result in r2.\n** Ex. 18 \u2014 WriteaprograminSimpleRisc assemblytosubtracttwo64-bitnumbers, where\neach number is stored in two registers.\n** Ex. 19 \u2014 In some cases, we can rotate an integer to the right by n positions (less than or\nequal to 31) so that we obtain the same number. For example: a 8-bit number 11011011 can 137 (cid:13)c Smruti R. Sarangi\nbe right rotated by 3 or 6 places to obtain the same number. Write an assembly program to\nefficiently count the number of ways we can rotate a number to the right such that the result\nis equal to the original number.\n** Ex. 20 \u2014 A number is known as a cubic Armstrong number if the sum of the cubes of\nthe decimal digits is equal to the number itself. For example, 153 is a cubic Armstrong number\n(153 = 13+53+33). You are given a number in register, r0, and it is known to be between 1\nand 1 million. Can you write a piece of assembly code in SimpleRisc to find out if this number\nis a cubic Armstrong number. Save 1 in r1 if it is a cubic Armstrong number; otherwise, save\n0.\n*** Ex. 21 \u2014 Write a SimpleRisc assembly language program to find the greatest common\ndivisoroftwobinarynumbersuandv. Assumethetwoinputs(positiveintegers)tobeavailable\nin r3 and r4. Store the result in r5. [HINT: The gcd of two even numbers u and v is 2 \u2217\ngcd(u\/2,v\/2)]\nInstruction Set Encoding\nEx. 22 \u2014 Encode the following SimpleRisc instructions:\ni) sub sp, sp, 4\nii) mov r4, r5\niii) addu r4, r4, 3\niv) ret\nv) ld r0, [sp]\nvi) st r4, 8[r9]\nDesign Problems\nEx. 23 \u2014 Design an emulator for the SimpleRisc ISA. The emulator reads an assembly pro-\ngram line by line, checks each assembly statement for errors, and executes it. Furthermore,\ndefine two assembler directives namely .print, and .encode to print data on the screen. The\n.print directive takes a register or memory location as input. When the emulator encounters\nthe .print directive, it prints the value in the register or memory location to the screen. Sim-\nilarly, when the emulator encounters the .encode directive it prints the 32-bit encoding of the\ninstruction on the screen. Additionally, it needs to also execute the instruction. (cid:13)c Smruti R. Sarangi 138 4\nARM(cid:13) R Assembly Language\nIn this chapter, we will study the ARM instruction set. As of 2012, this instruction set is the\nmost widely used instruction set in smart phones, and tablets. It has more than 90% market\nshare1 in this space. ARM processors are also one of the most popular processors in hard disk\ndrives, and set top boxes for televisions. Hence, for any student of computer architecture it is\nvery important to learn about the ARM instruction set because it will prove to be useful in\nprogramming the mobile and handheld devices of the future.\nTheARMinstructionsetisa32-bitinstructionset. Thismeansthatthesizesofallregisters\nare 32 bits, and the size of the memory address is equal to 32 bits. It is a RISC instruction set\nwith a very regular structure. Each instruction is encoded into a string of exactly 32 bits like\nSimpleRisc . All arithmetic and logical operations, use only register operands, and lastly all the\ncommunication between registers and memory happens through two data transfer instructions\n\u2013 load and store.\n4.1 The ARM(cid:13)R Machine Model\nARM assembly language assumes a machine model similar to that explained in Section 3.2.1\nfor SimpleRisc . For the register file, it assumes that there are 16 registers that are visible to\nthe programmer at any point of time. All the registers in ARM are 32 bits or 4 bytes wide.\nThe registers are numbered from r0 to r15. Registers r11...r15 are known by certain\nmnemonics also as shown in Table 4.1. r11 is the frame-pointer. It points to the top of the\nactivation block. r12 is a scratch register that is not meant to be saved by the caller or the\ncallee. r13 is the stack pointer. It is important to understand that r11 and r12 are assigned\na special connotation by the GNU compiler collection. They are not assigned special roles by\nthe ARM ISA.\n1Most of the ARM code running on processors is actually written in the Thumb-2 ARM ISA. The Thumb-2\nISA is essentially a recoding (or a simpler variant) of the ISA presented in this chapter. Hence, it is necessary\nfor readers to get a thorough understanding of the material that follows.\n139 (cid:13)c Smruti R. Sarangi 140\nRegister Abbrv. Name\nr11 fp frame pointer\nr12 ip intra-procedure-call scratch register\nr13 sp stack pointer\nr14 lr link register\nr15 pc program counter\nTable 4.1: Registers with special names in ARM\nLet us differentiate between generic registers and registers with special roles. Registers\nr0...r12 are generic. The programmer and the compiler can use them in any way they like.\nHowever, the registers r13(sp), r14(lr) and r15(pc) have special roles. sp is the stack pointer,\nlr is the return address register, and pc is the program counter. In this chapter, we shall use\nthe little endian version of the ARM ISA, and we shall describe the syntax of the assembly\nlanguage used by the GNU ARM Assembler [arm, 2000].\n4.2 Basic Assembly Instructions\n4.2.1 Simple Data Processing Instructions\nRegister Transfer Instructions\nThe simplest type of assembly instructions transfer the value of one register into another, or\nstore a constant in a register. There are two instructions in this class \u2013 mov and mvn. Their\nsemantics are shown in Table 4.2. Note that we always prefix an immediate with \u2018#\u2019 in ARM\nassembly.\nSemantics Example Explanation\nmov r1, r2 r1 \u2190 r2\nmov reg, (reg\/imm)\nmov r1, #3 r1 \u2190 3\nmvn r1, r2 r1 \u2190 \u223c r2\nmvn reg, (reg\/imm)\nmvn r1, #3 r1 \u2190 \u223c 3\nTable 4.2: Semantics of the move instructions\nThe register based mov instruction simply moves the contents of r2 to register r1. Alter-\nnatively, it can store an immediate in a register. In Table 4.2, the mvn instruction flips every\nbit in the 32-bit register r2, and then transfers the contents of the result to r1. The \u223c symbol\nrepresents logical complement. For example, the complement of the 4-bit binary value, 0110,\nis 1001. The mov and mvn instructions take two inputs. These instructions are examples of\n2-address format instructions in ARM. 141 (cid:13)c Smruti R. Sarangi\nArithmetic Instructions\nThe simplest instructions in this class are add, sub, rsb (reverse subtract). Their semantics are\ngiven in Table 4.3. The second operand can also be an immediate.\nSemantics Example Explanation\nadd reg, reg, (reg\/imm) add r1, r2, r3 r1 \u2190 r2 + r3\nsub reg, reg, (reg\/imm) sub r1, r2, r3 r1 \u2190 r2 - r3\nrsb reg, reg, (reg\/imm) rsb r1, r2, r3 r1 \u2190 r3 - r2\nTable 4.3: Semantics of add and subtract instructions\nExample 37\nWrite an ARM assembly program to compute: 4+5 - 19. Save the result in r1.\nAnswer: Simple yet suboptimal solution.\nmov r1, #4\nmov r2, #5\nadd r3, r1, r2\nmov r4, #19\nsub r1, r3, r4\nOptimal solution.\nmov r1, #4\nadd r1, r1, #5\nsub r1, r1, #19\nLogical Instructions\nSemantics Example Explanation\nand reg, reg, (reg\/imm) and r1, r2, r3 r1 \u2190 r2 AND r3\neor reg, reg, (reg\/imm) eor r1, r2, r3 r1 \u2190 r2 XOR r3\norr reg, reg, (reg\/imm) orr r1, r2, r3 r1 \u2190 r2 OR r3\nbic reg, reg, (reg\/imm) bic r1, r2, r3 r1 \u2190 r2 AND (\u223c r3)\nTable 4.4: Semantics of logical instructions (cid:13)c Smruti R. Sarangi 142\nARM\u2019s bitwise logical instructions are shown in Table 4.4. and computes a bit-wise AND,\neor computes an exclusive OR, orr computes a regular bit-wise OR, and the bic(bit-clear)\ninstruction clears off the bits in r2 that are specified in r3. Like arithmetic instructions, the\nsecond operand can be an immediate.\nExample 38\nWrite an ARM assembly program to compute: A\u2228B, where A and B are 1 bit Boolean\nvalues. Assume that A = 0 and B = 1. Save the result in r0.\nAnswer:\nmov r0, #0x0\norr r0, r0, #0x1\nmvn r0, r0\nMultiplication Instructions\nWe shall introduce four multiply instructions with varying degrees of complexity. The fun-\ndamental issue with multiplication is that if we are multiplying two 32-bit numbers, then the\nresult will require 64 bits. The reason is that the largest unsigned 32-bit number is 232 \u22121.\nConsequently, when we try to square this number, our result is approximately 264. We would\nthus need a maximum of 64 bits.\nARM has two 32-bit multiplication instructions that truncate the result to 32 bits \u2013 mul\nand mla. They ignore the rest of the bits. mul multiplies the values in two registers and stores\nthe result in a third register. mla (multiply and accumulate) is in the 4-address format. It\nmultiplies the values of two registers, and adds the result to the value stored in a third register\n(see Table 4.5). The advantage of the mla instruction is that it makes it possible to represent\ncode sequences of the form (d = a+b\u2217c) with one instruction. Such instructions are extremely\nuseful when it comes to implementing linear algebra kernels such as matrix multiplication.\nSemantics Example Explanation\nmul reg, reg, reg mul r1, r2, r3 r1 \u2190 r2 \u00d7 r3\nmla reg, reg, reg, reg mla r1, r2, r3, r4 r1 \u2190 r2 \u00d7 r3 + r4\nsmull reg, reg, reg, reg smull r0, r1, r2, r3 r1 r0 \u2190 r2 \u00d7 r3\nsigned\n(cid:124)(cid:123)(cid:122)(cid:125)\n64\numull reg, reg, reg, reg umull r0, r1, r2, r3 r1 r0 \u2190 r2 \u00d7 r3\nunsigned\n(cid:124)(cid:123)(cid:122)(cid:125)\n64\nTable 4.5: Semantics of multiply instructions\nIn this chapter, we shall introduce two instructions that store the entire 64-bit result in\ntwo registers. The smull and umull instructions perform signed and unsigned multiplication 143 (cid:13)c Smruti R. Sarangi\nrespectively on two 32-bit values to produce a 64-bit result. Their semantics is shown in\nTable 4.5. r0 contains the lower 32 bits, and r1 contains the upper 32 bits.\nFor all the multiply instructions that we have introduced, all the operands need to be\nregisters. Secondly, the first source register, should not be the same as the destination register.\nExample 39\nCompute 123+1, and save the result in r3.\nAnswer:\n\/* load test values *\/\nmov r0, #12\nmov r1, #1\n\/* perform the logical computation *\/\nmul r4, r0, r0 @ 12*12\nmla r3, r4, r0, r1 @ 12*12*12 + 1\nDivision Instructions\nNewer versions of the ARM ISA have introduced two integer division instructions, sdiv and\nudiv. The former is used for signed division and the latter is used for unsigned division (see\nTable4.6). Bothofthemcomputethequotient. Theremaindercanbecomputedbysubtracting\nthe product of the dividend and the quotient from the dividend.\nSemantics Example Explanation\nsdiv reg, reg, reg sdiv r1, r2, r3 r1 \u2190 r2 \u00f7 r3 (signed)\nudiv reg, reg, reg udiv r1, r2, r3 r1 \u2190 r2 \u00f7 r3 (unsigned)\nTable 4.6: Semantics of divide instructions\n4.2.2 Advanced Data-Processing Instructions\nLet us consider the generic format of 3-address data-processing instructions.\ninstruction <destination register> <register operand 1> <operand 2>\nLikewise, the generic format for 2 address data processing instructions is\ninstruction <register operand 1> <operand 2>\nUp till now, we have been slightly quiet about < operand 2 >. It can be a register operand,\nan immediate, or a special class of operands called \u2013 shifter operands. The first two classes are (cid:13)c Smruti R. Sarangi 144\nGeneric format\nreg1 , lsl #shift_amt\nlsr reg2\nasr\nror\nExamples\nlsl #1\n10110 01100\nlsr #1\n10110 01011\nasr #1\n10110 11011\nror #1\n10110 01011\nFigure 4.1: Format of shifter operands\nintuitive. Let us describe shifter operands in this section. Their generic format is shown in\nFigure 4.1.\nAshifteroperandcontainstwoparts. Thisfirstpartisaregister,andthelatterpartspecifies\nan operation to be performed on the value in the register. The ARM instruction set defines\nfoursuchoperations\u2013lsl (logicalshiftleft), lsr (logicalshiftright), asr (arithmeticshiftright),\nand ror (rotate right). These operations are collectively called shift and rotate instructions.\nShift and Rotate Instructions\nA logical left shift operation is shown in Figure 4.1. In this example, we are shifting the value\n10110 one place to the left. We need to shift in an extra 0 at the LSB position. The final result\nis equal to 01100. A left shift operation is present in most programming languages including C\nand Java. It is denoted by the following symbol: (cid:28). Note that shifting a word (4 byte number)\nby k positions to the left is equivalent to multiplying it by 2k. This is in fact a quick way of\nmultiplying a number by a power of 2.\nLet us now consider the right shift operation. Unlike the left shift operation, this operation\ncomes in two variants. Let us first consider the case of unsigned numbers. Here, we treat a\nword as a sequence of 32 bits. In this case, if we shift the bits 1 position to the right, we fill\nthe MSB with a 0. This operation is known as \u2013 logical shift right (see Figure 4.1). Note that\nshifting a number right by k places is usually the same as dividing it by 2k. The right shift\noperation in C or Java is (cid:29).\nIf we consider a signed number, then we need to use the arithmetic right shift (asr) op-\neration. This operation preserves the sign bit. If we shift a number right using asr by one\nposition, then we fill the MSB with the previous value of the MSB. This ensures that if we shift\na negative number to the right, the number still remains negative. In a four bit number system,\nif we shift 1010 to the right by 1 place using asr, then we get 1101. The original number is -6,\nand the shifted number is equal to -3. We thus see that arithmetic right shift divides a signed 145 (cid:13)c Smruti R. Sarangi\nnumber by a power of two. Note that using the right shift operations for odd numbers is tricky.\nLet us consider the representation of -5 in a 4-bit number system. It is 1011. After performing\nan arithmetic right shift, the result is equal to 1101, which is equal to -3 in decimal. Whether\nwe consider -5\/2 = -3 as a correct answer or not depends on the semantics of the programming\nlanguage.\nThe right rotate operation performs a right shift on the number. However, it fills the MSB\nwith the number shifted out from the rightmost end. In Figure 4.1, if we right rotate 10110,\nwe get 01011. In this case we have moved the previous LSB (0) to the new MSB. Note that ror\n(right rotate) by 32 positions gives us the original value. ARM provides a special connotation\nfor ror #0. It performs a right shift. It moves the value of the carry flag to the MSB, and then\nsets the shifted out LSB to the carry flag. This is also referred to as the rrx operation. This\noperation does not take any arguments.\nUsing Shifter Operands\nA shifter operand of the form \u2013 r1, lsl #2 \u2013 means that we shift the value in r1 by 2 places to\nthe left. Note that the value in r1 is not affected in this process. Likewise, an operand of the\nform \u2013 r1, lsr r3 \u2013 means that we shift the value in r1 to the right by the value specified in r3.\nWe can now use the shifter operand as a valid second operand. See examples 40, and 41.\nExample 40\nWrite ARM assembly code to compute: r1 = r2 \/ 4. Assume that the number stored in r1\nis divisible by 4.\nAnswer:\nmov r1, r2, asr #2\nExample 41\nWrite ARM assembly code to compute: r1 = r2 + r3 \u00d7 4.\nAnswer:\nadd r1, r2, r3, lsl #2\nAddressing Modes\nWe have now seen different formats of operands. An operand can either be a register, an\nimmediate, or a shifted register.\nWe have up till now seen three addressing modes:\n1. register addressing mode: Example, r1, r2, r3 (cid:13)c Smruti R. Sarangi 146\n2. immediate addressing mode: Example, #1, #2\n3. scaled-register addressing mode: Example, (r1,lsl #2), (r1,lsl r2)\n4.2.3 Compare Instructions\nARM has four compare instructions \u2013 cmp, cmn, tst, and teq \u2013 in the 2-address format. These\ninstructions compare the values in the two registers and save some properties of the result of\nthe comparison in a dedicated internal register called the CPSR register. Other instructions\nbase their behavior based on the values saved in the CPSR register. This is similar to the flags\nregister in SimpleRisc .\nThe CPSR register\nThe CPSR (Current Program Status Register) maintains some state regarding the execution\nof the program. It is a 32-bit register like the other registers, and is usually used implicitly.\nIn this book, we are concerned with four bits that it stores in the positions [29-32]. They are\nN(Negative), Z(Zero), C(Carry), and V(Overflow). These four bits are known as condition code\nflags, or simply flags. It is similar to the flags register in SimpleRisc .\nThere are two sets of instructions that can set CPSR flags. The first set comprises of\ncompare instructions, and the second set includes flag setting variants of generic instructions.\nIn either case, the rules for setting the flags are as follows:\nN (Negative) This flag is set if the result is a 2\u2019s complement based signed integer. It is set\nto 1 if the result is negative, and 0 if it is non-negative.\nZ (Zero) This flag is set to 1 if the result is zero. In a comparison operation, if the operands\nare equal, then this flag is also set to 1.\nC (Carry) \u2022 For an addition, the C bit is set to 1 if the result produced a carry. This\ncan happen when there was an overflow while adding the unsigned numbers. For\nexample, if we add -1(1111 ) and -2(1110 ), then the result is -3(1101 ), and there\n2 2 2\nis a carry out at the MSB. Note that there is no real overflow, because -3 can be\nrepresented in the number system. However, if the numbers are treated as unsigned\nnumbers, then there is an unsigned overflow. Consequently, we can also say that the\ncarry bit is set if there is an unsigned overflow.\n\u2022 For a subtraction, the carry bit is set to 0 if there is an unsigned underflow. For\nexample, if we try to compute 0\u22121, then there is no real overflow\/underflow. How-\never, 0000 \u22120001 will lead to an unsigned underflow. This basically means that\n2 2\nwhen we subtract these two numbers, we will need to borrow a bit. In this case, we\nset the C flag to 0. Otherwise, we set it to 1.\n\u2022 For logical shift operations, C is equal to the last bit shifted out of the result value.\nV (Overflow) Vissetto1whenanactualsignedoverflow\/underflowoccurs. Notethatinthe\nrest of the book, we might casually refer to both overflow and underflow as just overflow. 147 (cid:13)c Smruti R. Sarangi\nCompare Instructions\nARM has four compare instructions \u2013 cmp, cmn, tst and teq. All four of them update the\nCPSR flags. Let us consider the cmp instruction. It is a 2-address instruction that takes two\ninputs. It essentially subtracts their values and sets the appropriate flags. For example, if the\nvalues are equal, then the zero flag is set. Later instructions can take some decisions based\non these flags. For example, they might decide if they need to branch, or perform a certain\ncomputation based on the value of the zero flag. We show the semantics of all four compare\ninstructions in Table 4.7.\nSemantics Example Explanation\ncmp reg, (reg\/imm) cmp r1, r2 Set flags after computing (r1 - r2)\ncmn reg, (reg\/imm) cmn r1, r2 Set flags after computing (r1 + r2)\ntst reg, (reg\/imm) tst r1, r2 Set flags after computing (r1 AND r2)\nteq reg, (reg\/imm) teq r1, r2 Set flags after computing (r1 XOR r2)\nTable 4.7: Semantics of compare instructions\ncmn computes the flags after adding the register values, tst computes a bitwise AND of the\ntwooperandsandthensetstheflags,andteq testsforequalitybycomputinganXOR(exclusive\nor) of the operands. For this set of instructions, the second operand can be an immediate also.\nNote that the compare instructions, are not the only instructions that can set the flags. Let us\ndiscuss a generic class of instructions that can set the CPSR flags.\n4.2.4 Instructions that Set CPSR Flags \u2013 The \u2018S\u2019 Suffix\nNormal instructions such as add and sub do not set the CPSR flags. However, it is possible to\nmake any data processing instruction set the flags by adding the suffix - \u2018s\u2019 - to it. For example,\nthe adds and subs instructions do the regular jobs of addition and subtraction respectively, and\nadditionally also set the CPSR flags. The rules for setting the flags are given in Section 4.2.3.\nLet us now see how we can use these flags.\n4.2.5 Data Processing Instructions that use CPSR Flags\nThere are three simple data processing instructions that use the CPSR flags in their computa-\ntion. They are sbc, rsc, and adc.\nLet us now motivate this section with an example. Our basic ARM instruction format does\nnot support 64-bit registers. Consequently, if we desire to implement the long data type that\nuses 64 bits, we need to use two registers. Let us assume that one long value is present in\nregisters, r2, and r1. Here, r2 contains the upper 32 bits, and r1 contains the lower 32 bits.\nLet the second long value be present in registers r4, and r3. Let us now try to add these two\nlong values to produce a 64-bit result, and save it in registers, r6 and r5. See Example 42. (cid:13)c Smruti R. Sarangi 148\nExample 42\nAdd two long values stored in r2,r1 and r4,r3.\nAnswer:\nadds r5, r1, r3\nadc r6, r2, r4\nThe (adds) instruction adds the values in r1 and r3. adc(add with carry) adds r2, r4, and\nthe value of the carry flag. This is exactly the same as normal addition.\nExample 43 shows how to subtract the values.\nExample 43\nSubtract two long values stored in r2,r1 and r4,r3.\nAnswer:\nsubs r5, r1, r3\nsbc r6, r2, r4\nsubs subtracts the value of r3 from the value in r1. sbc(subtract with carry) subtracts the\nvalue in r4 from the value in r2. Additionally, if the previous instruction resulted in a\nborrow (carry equal to 0), then it also subtracts the carry bit. This is the same as normal\nsubtraction.\nWe list the semantics of the instructions in Table 4.8. Note that in the case of a subtraction\nthe carry flag is set to 0, when there is a borrow. The NOT operation flips a 0 to 1, and vice\nversa. Lastly, rsc stands for \u2013 reverse subtract with carry.\nSemantics Example Explanation\nadc reg, reg, reg adc r1, r2, r3 r1 = r2 + r3 + Carry Flag\nsbc reg, reg, reg sbc r1, r2, r3 r1 = r2 - r3 - NOT(Carry Flag)\nrsc reg, reg, reg rsc r1, r2, r3 r1 = r3 - r2 - NOT(Carry Flag)\nTable 4.8: Semantics of adc, sbc, and rsc instructions\n4.2.6 Simple Branch Instructions\nAn ISA with just data processing instructions is very weak. We need branch instructions such\nthat we can implement if-statements and for-loops. ARM programs primarily use three branch 149 (cid:13)c Smruti R. Sarangi\ninstructions to do most of their work. They are: b, beq, bne. Their semantics are given in\nTable 4.9.\nSemantics Example Explanation\nb label b .foo Jump unconditionally to label .foo\nbeq label beq .foo Branch to .fooif the last flag setting\ninstruction has resulted in an equal-\nity and (Z flag is 1)\nbne label bne .foo Branch to .foo if the last flag set-\nting instruction has resulted in an\ninequality and (Z flag is 0)\nTable 4.9: Semantics of simple branch instructions\nExample 44\nWrite an ARM assembly program to compute the factorial of a positive number (> 1)\nstored in r0. Save the result in r1.\nAnswer:\nC\nint val = get_input();\nint idx;\nint prod = 1;\nfor (idx = 1; idx <= val ;\nidx++) {\nprod = prod * idx;\n}\nARM assembly\nmov r1, #1 \/* prod = 1 *\/\nmov r3, #1 \/* idx = 1 *\/\n.loop:\nmul r1, r3, r1 \/* prod = prod * idx *\/\ncmp r3, r0 \/* compare idx, with the input (num) *\/\nadd r3, r3, #1 \/* idx ++ *\/\nbne .loop \/* loop condition *\/\nLet us now see, how we can use the power of branches to write some powerful programs.\nLet us consider the factorial function. In Example 44, we show a small program to compute the (cid:13)c Smruti R. Sarangi 150\nfactorial of a natural number. r3 is a counter that is initialised to 0. We keep on incrementing\nit till it matches r0. r1 represents the product. We iteratively multiply the value of r3 with r1.\nAt the end of the set of iterations, r1 contains the factorial of the value given in r0.\nExample 45\nWrite an assembly program to find out if a natural number stored in r0 is a perfect square.\nSave the Boolean result in r1.\nAnswer:\nmov r1, #0 \/* result initialised to false *\/\n1\nmov r2, #1 \/* counter *\/\n2\n.loop:\n3\nmul r3, r2, r2\n4\ncmp r3, r0\n5\nbeq .square\n6\nadd r2, r2, #1\n7\ncmp r2, r0\n8\nbne .loop\n9\n10\nb .exit \/* number is not a square *\/\n11\n.square:\n12\nmov r1, #1 \/* number is a square *\/\n13\n.exit:\n14\nLet us show the example of another program to test if a number is a perfect square (see\nExample45). r1containstheresultoftheoperation. Ifthenumberisaperfectsquarewesetr1\nto 1, else we set r1 to 0. The main loop is between lines 3 and 9. Here, we increment the value\nof r2 iteratively, and test if its square equals r0. If it does, we jump to .square, set r1 to 1, and\njump to .exit. Here, we print the value (code not shown), and exit the program. We assume a\nhypothetical label \u2013 .exit \u2013 that is present at the end of the program (also shown in the code).\nThe exit condition of the loop is Line 9, where we consider the result of the comparison of r2\nand r0. If r2 is equal to r0, then r0 cannot contain a perfect square because r0 is at least equal\nto 2 at the end of any iteration.\n4.2.7 Branch and Link Instruction\nWe can use the simple branch instructions to implement for loops and if statements. However,\nwe need a stronger variant of the branch instruction to implement function calls. Function calls\nare different than regular branches because we need to remember the point in the program that\nthe function needs to return to. ARM provides the bl (branch-and-link) instruction for this\npurpose. The semantics of this instruction is shown in Table 4.10. 151 (cid:13)c Smruti R. Sarangi\nSemantics Example Explanation\nbl label bl .foo (1) Jump unconditionally to the function at .foo\n(2) Save the next PC (PC + 4) in the lr register\nTable 4.10: Semantics of the branch and link instruction\nThe bl instruction jumps to the function that begins at the specified label. Note that in the\nARM ISA, there is no special way for designating the start of a function. Any instruction can\nin principle be the start of a function. In ARM assembly, the starting instruction of a function\nneeds to have a label assigned to it. Along with branching to the given label, the bl instruction\nalso saves the value of the return address, which is equal to the current PC plus 4, into the lr\nregister (r14). We need to add 4 over here because the size of an instruction in ARM is exactly\nequal to 4 bytes.\nOnce a function starts executing, it is expected that it will preserve the value of the return\naddress saved in the lr register unless it invokes other functions. If a function invokes other\nfunctions, it needs to spill and restore registers as mentioned in Section 3.3.10. When we wish\nto return from a function, we need to move the value in the lr register to the pc register (r15).\nThe PC will point to the instruction at the return address and execution will proceed from that\npoint.\nExample 46\nExample of an assembly program with a function call.\nC\nint foo() {\nreturn 2;\n}\nvoid main() {\nint x = 3;\nint y = x + foo();\n}\nARM assembly\nfoo:\nmov r0, #2\nmov pc, lr\nmain:\nmov r1, #3 \/* x = 3 *\/\nbl foo \/* invoke foo *\/\n\/* y = x + foo() *\/\nadd r2, r0, r1 (cid:13)c Smruti R. Sarangi 152\nLet us take a look at Example 46. In this example, we consider a simple piece of C code\nthat calls a function foo that returns a constant value of 2. It adds the return value to the\nvariable x to produce y.\nIntheequivalentARMcode,wedefinetwolabels\u2013fooandmain. Weassumethatexecution\nstarts from the main label. We map x to r1, and set its value equal to 3. Then, we call the\nfunction foo. In it we set the value of register r0 to 2, and return by moving the value in the\nlr register to the PC. When the program returns, it begins execution at the subsequent line in\nthe main function. The register r0 maintains its value equal to 2 across functions. We add the\nvalue in r1 to the value in r0 to produce the value for y. It is saved in r2.\nNowadays, there is a simpler method is used to return from a function. We can use the bx\ninstruction that jumps to an address contained in a register (semantics shown in Figure 4.11).\nSemantics Example Explanation\nbx reg bx r2 (1) Jump unconditionally to the ad-\ndress contained in register, r2\nTable 4.11: Semantics of the bx instruction\nWe can simplify the assembly code in Example 46 as follows.\nARM assembly\nfoo:\nmov r0, #2\nbx lr\nmain:\nmov r1, #3 \/* x = 3 *\/\nbl foo \/* invoke foo *\/\n\/* y = x + foo() *\/\nadd r2, r0, r1\n4.2.8 Conditional Instructions\nNow, that we have a fairly good idea of basic branch instructions, let us elaborate some special\nfeatures of ARM assembly. These features help make the process of coding very efficient. Let\nus consider the instructions beq and bne again. We note that they are variants of the basic b\ninstruction. They are distinguished by their suffixes \u2013 eq and ne. The former denotes equality,\nand the latter denotes inequality. These suffixes are known as condition codes\nARM Condition Codes\nLetusfirstconsiderthelistofcondition codesshowninTable4.12. Thereare16conditioncodes\nin ARM. Each condition code has a unique number, and suffix. For example, the condition\ncode with suffix eq has a number equal to 0. Every condition code is associated with a unique\ncondition. For example, eq is associated with equality. To test if the condition holds, the ARM 153 (cid:13)c Smruti R. Sarangi\nNumber Suffix Meaning Flag State\n0 eq equal Z = 1\n1 ne not equal Z = 0\n2 cs\/hs carry set\/ unsigned higher or equal C = 1\n3 cc\/lo carry clear\/ unsigned lower C = 0\n4 mi negative\/ minus N = 1\n5 pl positive or zero\/ plus N = 0\n6 vs overflow V = 1\n7 vc no overflow V = 0\n8 hi unsigned higher (C = 1) \u2227 (Z = 0)\n9 ls unsigned lower or equal (C = 0) \u2228 (Z = 1)\n10 ge signed greater than or equal N = 0\n11 lt signed less than N = 1\n12 gt signed greater than (Z = 0) \u2227 ( N = 0)\n13 le signed less than or equal (Z = 1) \u2228 (N = 1)\n14 al always\n15 \u2013 reserved\nTable 4.12: Condition codes\nprocessor takes a look at the CPSR flags. The last column in Table 4.12 shows the values of\nthe flags that need to be set for the condition to hold.\nThe eq and ne conditions can be tested by considering the Z(zero) flag alone. The expec-\ntation is that an earlier cmp or subs instruction would have set these flags. If the comparison\nresulted in an equality, then the Z flag would be set to 1.\nAs described in Section 4.2.3, if a subtraction of unsigned numbers leads to a borrow, then\nthe carry flag is set to 0. This condition is also known as an unsigned underflow. If there is\nno borrow, then the carry flag is set to 1. Consequently, if the comparison between unsigned\nnumbers concludes that the first number is greater than or equal to the second number, then\nthe C(carry flag) needs to be set to 1. Likewise, if the carry flag is set to 0, then we can say\nthat the first operand is smaller than the second operand (unsigned comparison). These two\nconditions are captured by the hs and lo condition codes respectively.\nThe next four condition codes check if a number is positive or negative, and if there has\nbeen an overflow. These conditions can be trivially evaluated by considering the values of\nN(negative) and V(overflow) flags respectively. hi denotes unsigned higher. In this case, we\nneed to additionally test the Z flag. Likewise for ls (unsigned lower or equal), we need to test\nthe Z flag, along with the C flag.\nARM has four condition codes for signed numbers \u2013 ge(\u2265), le(\u2264), gt(>), and lt(<). The ge\ncondition code simply tests the N flag. It should be equal to 0. This means that a preceding\ncmp or subs instruction has subtracted two numbers, where the first operand was greater than\nor equal to the second operand. For the gt instruction, we need to consider the Z flag also. In\na similar manner, the less than condition codes \u2013 lt and le \u2013 work. The conditions for the flags\nare given in Table 4.12. (cid:13)c Smruti R. Sarangi 154\nNote that for signed numbers, we have not considered the possibility of an overflow in\nTable 4.12. Theorem 2.3.4.1 outlines the precise conditions for detecting an overflow. We\nleave the process of augmenting the conditions to consider overflow as an exercise for the\nreader. Lastly, the al(always) condition code means that the instruction is not associated with\nany condition. It executes according to its default specification. Hence, it is not required to\nexplicitly specify the al condition since it is the default.\nConditional Variants of Normal Instructions\nCondition codes are not just restricted to branches. We can use condition codes with normal\ninstructions such as add and sub also. For example, the instruction addeq performs an addition\nif the Z flag in the flags register is set to true. It means that the last time that the flags\nwere set (most likely by a cmp instruction), the instruction must have concluded an equality.\nHowever, if the last comparison instruction concluded that its operands are unequal, then the\nARM processor treats the addeq instruction as a nop instruction (no operation). We shall see\nin Chapter 9 that by using such conditional instructions, we can increase the performance of\nan advanced processor. Let us consider an example that uses the addeq instruction.\nExample 47\nWrite a program in ARM assembly to count the number of 1s in a 32-bit number stored in\nr1. Save the result in r4.\nAnswer:\nmov r2, #1 \/* idx = 1 *\/\nmov r4, #0 \/* count = 0 *\/\n\/* start the iterations *\/\n.loop:\n\/* extract the LSB and compare *\/\nand r3, r1, #1\ncmp r3, #1\n\/* increment the counter *\/\naddeq r4, r4, #1\n\/* prepare for the next iteration *\/\nmov r1, r1, lsr #1\nadd r2, r2, #1\n\/* loop condition *\/\ncmp r2, #32\nble .loop 155 (cid:13)c Smruti R. Sarangi\n4.2.9 Load-Store Instructions\nSimple Load-Store Instructions\nThe simplest load and store instructions are ldr and str respectively. Here, is an example.\nldr r1, [r0]\nThis instruction directs the processor to load the value in register r1, from the memory\nlocation stored in r0, as shown in Figure 4.2.\nldr r1, [r0]\nMemory\nRegister\nfile\nr0\nr1\nFigure 4.2: The ldr instruction\nNote that in this case, r0, contains the starting address of the data in memory. The ldr\ninstructions loads 4 bytes in a register. If the value contained in r0 is v, then we need to fetch\nthe bytes from v to v + 3. These 32 bits (4 bytes), are brought from memory and saved in\nregister r1.\nThe str instruction performs the reverse process. It reads the value in a register and saves\nit in a memory location. An example is shown in Figure 4.3. Here r0 is known as the base\nregister.\nstr r1, [r0]\nLoad-Store Instructions with an Offset\nWe can specify load and store instructions with a base register, and an optional offset. Let us\nconsider:\nldr r1, [r0, #4] (cid:13)c Smruti R. Sarangi 156\nstr r1, [r0]\nMemory\nRegister\nfile\nr0\nr1\nFigure 4.3: The str instruction\nHere, the memory address is equal to the value in r0 plus 4. It is possible to specify a\nregister in place of an immediate operand.\nldr r1, [r0, r2]\nThe memory address is equal to r0+r2. In this expression, r0 and r2 refer to the values\nstored in them. We can alternatively state the operation in this program as: r1 \u2190 [r0+r2]\n(see the register transfer notation defined in Section 3.2.5).\nSemantics Example Explanation Addressing Mode\nldr reg, [reg] ldr r1, [r0] r1 \u2190 [r0] register-indirect\nldr reg, [reg, imm] ldr r1, [r0, #4] r1 \u2190 [r0+4] base-offset\nldr reg, [reg, reg] ldr r1, [r0, r2] r1 \u2190 [r0+r2] base-index\nldr reg, [reg, reg, shift imm] ldr r1, [r0, r2, lsl #2] r1 \u2190 [r0+r2(cid:28)2] base-scaled-index\nstr reg, [reg] str r1, [r0] [r0] \u2190 r1 register-indirect\nstr reg, [reg, imm] str r1, [r0, #4] [r0+4] \u2190 r1 base-offset\nstr reg, [reg, reg] str r1, [r0, r2] [r0+r2] \u2190 r1 base-index\nstr reg, [reg, reg, shift imm] str r1, [r0, r2, lsl #2] [r0+r2(cid:28)2] \u2190 r1 base-scaled-index\nTable 4.13: Load and store instruction semantics\nTable 4.13 shows the semantics of different types of load store instructions. The third\ncolumn shows the addressing mode. The register r2 in this case is known as the index register\nbecause it contains a value that is added to the base register, and this value can be used as the\nindex of an array (see Section 4.3.1). Note that some authors call the base-offset mode as also\nthe displacement addressing mode. 157 (cid:13)c Smruti R. Sarangi\nLoad-Store instructions for Bytes and Half-Words\nThe ldr and str instructions load\/store 4 bytes of data. However, it is possible to also load and\nstore 1 and 2 bytes of data. 2 bytes is also known as a half-word, where a word is equal to 4\nbytes.\nSemantics Example Explanation\nldrb reg, [reg, imm] ldrb r1, [r0, #2] r1 \u2190 [r0+2] (1 unsigned byte)\nldrh reg, [reg, imm] ldrh r1, [r0, #2] r1 \u2190 [r0+2] (2 unsigned bytes)\nldrsb reg, [reg, imm] ldrsb r1, [r0, #2] r1 \u2190 [r0+2] (1 signed byte)\nldrsh reg, [reg, imm] ldrsh r1, [r0, #2] r1 \u2190 [r0+2] (2 signed bytes)\nstrb reg, [reg, imm] strb r1, [r0, #2] [r0+2] \u2190 r1 (1 unsigned byte)\nstrh reg, [reg, imm] strh r1, [r0, #2] [r0+2] \u2190 r1 (2 unsigned bytes)\nTable 4.14: Load and store instructions for bytes and half-words in the base-offset addressing\nmode\nTable 4.14 shows the load and store instructions for bytes and half words using the base-\noffset addressing mode. ldrb loads an unsigned byte to a register. It places the byte in the least\nsignificant8bits. Therestofthe24bitsaresetto0. ldrhsimilarlyloadsanunsignedhalf-word\n(16 bits). ldrsb, and ldrsh load a signed byte and half-word respectively. They extend the sign\nof the operand (see Section 2.3.4) to make it fit in 32 bits. This is done by replicating the MSB.\nstrb and strh store an unsigned byte in memory. Note that unlike loads, there are no ARM\ninstructions to extend the sign of the operand while saving it in memory.\n4.3 Advanced Features\nWe are in a good point to take a look at some of the advanced features in the ARM instruction\nset. Up till now, we have taken a look at basic instructions that allow us to implement simple\ndata types in a high level language such as C or Java. We can translate simple programs that\ncontain integers into assembly code, compute the results of mathematical functions, load and\nstorevaluesfrommemory. However,thereareotherhighlevelfeaturessuchasfunctions,arrays,\nand structures that are present in high level languages. They shall require special support at\nthe assembly level for creating efficient implementations.\nBy no means has the process of programming language development stopped. We expect\nthat over the next few decades, there will be many new kinds of programming languages. They\nwill make the process of programming easier for more programmers, and it should be easier to\nleveragenovelfeaturesoffuturistichardware. Thiswouldrequireextrainstructionsandsupport\natthelevelofassemblyprograms. Thisisthusanevolvingfield, anddeservesathoroughstudy. (cid:13)c Smruti R. Sarangi 158\n4.3.1 Arrays\nArray Specific Features\nNote that the starting memory location of entry i is equal to the base address of the array plus\n4i in an array with word (4 byte) sized elements. In a high level language, the programmer\nalways specifies the index in an array, and relies on the compiler to multiply the index by 4.\nARM assembly provides nice features to multiply i by 4 by using the lsl instruction. This\nfeature can be embedded in load-store instructions.\nldr r0, [r1, r2, lsl #2]\nIn this case the base address is stored in register, r1, and the offset is equal to r2 <<\n2 = 4\u2217r2. The advantage here is that we do not need a separate instruction to multiply the\nindex by 4. We have already seen this optimisation in Section 4.2.2. However, there are other\noptimisations that can make our life easier. Let us consider array accesses in a loop as shown\nin Example 48.\nExample 48 Convert the following C program to a program to ARM assembly. Assume\nthat the base address of the array is stored in r0.\nC\nvoid addNumbers(int a[100]) {\nint idx;\nint sum = 0;\nfor (idx = 0; idx < 100; idx++){\nsum = sum + a[idx];\n}\n}\nAnswer:\nARM assembly\n\/* base address of array a in r0 *\/\n1\nmov r1, #0 \/* sum = 0 *\/\n2\nmov r2, #0 \/* idx = 0 *\/\n3\n4\n.loop:\n5\nldr r3, [r0, r2, lsl #2]\n6\nadd r2, r2, #1 \/* idx ++ *\/\n7\nadd r1, r1, r3 \/* sum += a[idx] *\/\n8\ncmp r2, #100 \/* loop condition *\/\n9\nbne .loop\n10\nThere is a scope for added efficiency here. We note that Lines 6 and 7 form a standard\npattern. Line 6 reads the array entry, and Line 7 increments the index. Almost all sequential 159 (cid:13)c Smruti R. Sarangi\narray accesses follow a similar pattern. Hence, it makes sense to have one instruction that\nsimplifies this process.\nThe ARM architecture adds two extra addressing modes for the load and store instructions\nto achieve this. They are called pre-indexed and post-indexed with auto-update. In the pre-\nindexed addressing mode (with auto-update), the base address is updated first, and then the\neffective memory address is computed. In a post-indexed scheme, the base address is updated\nafter the effective address is computed.\nThe pre-indexed addressing mode with auto-update is implemented by adding a \u2018!\u2019 sign\nafter the address.\nExamples of the pre-indexed addressing mode\nldr r3, [r0, #4]! \/* r3 = [r0+4]; r0 = r0 + 4*\/\nldr r3, [r0, r1, lsl #2]! \/* r3 = [r0 + r1 << 2];\nr0 = r0 + r1 << 2; *\/\nThepost-indexedaddressingmodeisimplementedbyencapsulatingthebaseaddresswithin\n\u2018[\u2019 and \u2018]\u2019, and writing the offset arguments separated by commas after it.\nExamples of the post-indexed addressing mode\nldr r3, [r0], #4 \/* r3 = [r0], r0 = r0 + 4 *\/\nldr r3, [r0], r1, lsl #2 \/* r3 = [r0], r0 = r0 + r1 << 2 *\/\nLet us now see, how we can slightly make our addNumbers slightly more intuitive. The\nmodified ARM code is shown in Example 49.\nExample 49\nConvert the assembly code shown in Example 48 to use the post indexed addressing mode.\nAnswer:\nARM assembly\n\/* base address of array a in r0 *\/\n1\nmov r1, #0 \/* sum = 0 *\/\n2\nadd r4, r0, #400 \/* address of a[100]*\/\n3\n.loop:\n4\nldr r3, [r0], #4\n5\nadd r1, r1, r3 \/* sum += a[idx] *\/\n6\ncmp r0, r4\n7\nbne .loop\n8\nWe have eliminated the index variable saved in r2. It is not required anymore. We directly\nupdate the base address in Line 5. For the loop exit condition, we compute the first address\nbeyond the end of the array in Line 3. We compare the base address with this illegal address\nin Line 7, and then if they are unequal we keep iterating.\nExample 48 contains 5 lines in the loop, whereas the code in Example 49 contains 4 lines in\nthe loop. We have thus shown that it is possible to reduce the code size (of the loop) by 20%\nusing post-indexed addressing, and increase performance too since most cores do not impose\nadditional time overheads when auto-update addressing modes are used. (cid:13)c Smruti R. Sarangi 160\nStructures\nImplementing structures is very similar to implementing arrays. Let us look at a typical struc-\nture in C.\nstruct Container {\nint a;\nint b;\nchar c;\nshort int d;\nint e;\n};\nWe can treat each structure as an array. Consequently, a structure will have a base address\nand each element of the structure will have an offset. Unlike an array, different elements in a\nstructure can have different sizes, and thus they are not constrained to start with offsets that\nare multiples of the word size.\nType Element Offset\nint a 0\nint b 4\nchar c 8\nshort int d 10\nint e 12\nTable 4.15: Elements in the structure and their offsets\nTable 4.15 shows the offsets for different elements within a structure (as generated by the\nGNU ARM compiler). We need to note that compilers for the ARM architecture impose\nadditional constraints. They pad variable addresses, and align them with 2 byte or 4 byte\nboundaries as shown in Table 4.15 The rules for variable alignment are described in detail in\nthe ARM architecture manual [arm, 2000]. In a similar fashion it is possible to implement more\nhigh level data structures such as unions and classes. The interested reader is referred to a\nbook on compilers.\n4.3.2 Functions\nLet us now use two sophisticated ARM instructions for spilling and restoring registers in the\nstack. They can be used to implement both caller saved and callee saved functions.\nInstructions for Spilling and Restoring Registers\nLet us now describe two instructions to use the stack for saving and restoring a set of registers\n\u2013 ldmfd and stmfd. These registers load and store multiple registers in a memory region such\nas the stack. For brevity, we do not consider generic memory regions in this book. We limit our\ndiscussion to the stack. ldmfd and stmfd instructions take a base register (e.g., stack pointer), 161 (cid:13)c Smruti R. Sarangi\nand set of registers as arguments. They load or store the set of registers in the memory region\npointed to by the base register. Note that the order of the registers does not matter. The\nregisters are always rearranged in ascending order.\nLet us consider an example using the store instruction, stmfd.\nstmfd sp!, {r2,r3,r1,r4}\nInstruction Semantics\nldmfd sp!, {list of registers } Pop the stack and assign values to\nregistersinascendingorder. Update\nthe value of sp.\nstmfd sp!, {list of registers } Push the registers on the stack in\ndescending order. Update the value\nof sp.\nTable 4.16: Semantics of the ldmfd and stmfd instructions\nThe stmfd instruction assumes a downward growing stack, and it also assumes that the\nstack pointer points to the starting address of the value at the top of the stack. Recall that\nthe top of the stack in a downward growing stack is defined as the starting address of the last\nvalue pushed on the stack. In this case the registers are processed in ascending order \u2013 r1, r2,\nr3, r4. Secondly memory addresses are also accessed in ascending order. Consequently r1 will\nbe saved in sp\u221216, r2 in sp\u221212, r3 in sp\u22128, and r4 in sp\u22124. Alternatively, we can explain\nthis instruction by observing that registers are pushed into the stack in descending order. We\nuse the \u2018!\u2019 suffix with the base address register to instruct the processor to update the value of\nthe stack pointer after the execution of the instruction. In this case, we set sp equal to sp\u221216.\nThere is a variant of this instruction that does not set the stack pointer to the starting\naddress of the memory region used to save registers. An example with this variant is:\nstmfd sp, {r2,r3,r1,r4}\nNote that this variant is rarely used in practice, especially when the base register is sp.\nSimilarly, the ldmfd instruction loads a set of values starting at the stack pointer, and then\nupdates the stack pointer. Akin to the stmfd instruction, we use the \u2018!\u2019 suffix to use the base\nregister auto update feature.\nldmfd sp!, {r2,r3,r1,r4}\nFor example, in this case we set r1 = [sp], r2 = [sp+4], r3 = [sp+8], and r4 = [sp+12]. In\nother words, we iteratively pop the stack and assign the values to registers in ascending order.\nThe ldmfd instruction also has a variant that does not update the base register. We simply\nneed to delete the \u2018!\u2019 suffix after the base register.\nldmfd sp, {r2,r3,r1,r4} (cid:13)c Smruti R. Sarangi 162\nThe semantics of these instructions are shown in Table 4.16.\nLet us conclude this section with an example. We show a recursive power function in C\nthat takes two arguments x and n, and computes xn.\nExample 50\nWrite a function in C and implement it in ARM assembly to compute xn, where x and n\nare natural numbers. Assume that x is passed through r0, n through r1, and the return\nvalue is passed back to the original program via r0. Answer:\nC\nint power(int x, int n) {\nif (n == 0)\nreturn 1;\nint y = x * power(x, n-1);\nreturn y;\n}\nWhen we compile this function to ARM assembly, we get:\nARM assembly\npower:\n1\ncmp r1, #0 \/* compare n with 0 *\/\n2\nmoveq r0, #1 \/* return 1 *\/\n3\nbxeq pc, lr \/* return *\/\n4\n5\nstmfd sp!, {r4, lr} \/* save r4 and lr *\/\n6\nmov r4, r0 \/* save x in r4 *\/\n7\nsub r1, r1, #1 \/* n = n - 1 *\/\n8\nbl power \/* recursively call power *\/\n9\nmul r0, r4, r0 \/* power(x,n) = x * power(x,n-1) *\/\n10\nldmfd sp!, {r4, pc} \/* restore r4 and return *\/\n11\nWe first compare n with 0. If n is equal to 0, then we need to return 1 (Line 3). We\nsubsequently, return from the function. Note the use of the instruction moveq here.\nHowever, if n (cid:54)= 0, then we need to make a recursive function call to evaluate xn\u22121.\nWe start out by saving register r4, and the return address (lr) on the stack in Line 6 using\nthe stmfd instruction. We save the value of r0 in r4 because it will get overwritten by the\nrecursive call to the power function. Subsequently, we decrement r1 that contains the value\nof n, and then we call the power function recursively in Line 10. The result of the power\nfunction is assumed to be present in r0. We multiply this result with the value of x (stored\nin r4) in Line 10.\nWe simultaneously do two operations in Line 11. We load the value of r4, and pc\nfrom the stack. We first read the first operand, r4, which was saved on the stack by the\ncorresponding stmfd instruction in Line 6. The second operand saved on the stack was\nthe return address. We read this value and save it in pc. Effectively, we are executing the 163 (cid:13)c Smruti R. Sarangi\ninstruction mov pc, lr, and we are thus returning from the function. Hence, after executing\nLine 11, we start executing instructions from the return address of the function.\nThe ldm and stm instructions can also assume an upward growing stack. The interested\nreader can refer to the ARM manual [arm, 2000] for a thorough explanation.\n4.4 Encoding the Instruction Set\nLet us now see how to convert ARM assembly instructions to a sequence of 0s and 1s. Each\nARM instruction is represented using 32 bits. We need to encode the instruction type, values\nof conditional fields, register numbers, and immediate operands using these 32 bits only.\nLet us take a look at the generic format of ARM instructions. For every instruction we need\nto initially encode at least two pieces of information \u2013 condition codes (see Table 4.12), and the\nformat of the instruction (data processing, branch, load\/store, or others). Table 4.12 defines\n15 conditions on each instruction. It will take 4 bits to represent this information.\nImportant Point 6\nTo uniquely encode a set of n elements, we need at least (cid:100)log (n)(cid:101) bits. We can assign\n2\neach element a number between 0 and n\u22121. We can represent these numbers in the binary\nformat. The number of bits required is equal to the number of bits needed to represent the\nlargest number, n\u22121. If we have log (n) bits, then the largest number that we can represent\n2\nis 2log2(n)\u22121 = n\u22121. However, log 2(n) might be a fraction. Hence, we need to use (cid:100)log 2(n)(cid:101)\nbits.\nARM has four types of instructions \u2013 data processing (add\/ subtract\/ multiply\/ compare),\nload\/store, branch, and miscellaneous. We need 2 bits to represent this information. These\nbits determine the type of the instruction. Figure 4.4 shows the generic format for instructions\nin ARM.\n4 2\ncond type\n32 29 28 27\nFigure 4.4: Generic format of an ARM instruction\n4.4.1 Data Processing Instructions\nThe type field is equal to 00 for data processing instructions. The rest of the 26 bits need to\ncontain the instruction type, special conditions, and registers. Figure 4.5 shows the format for (cid:13)c Smruti R. Sarangi 164\ndata processing instructions.\n4 2 4 4 4 12\ncond 0 0 I opcode S rs rd shifter operand\/\n32 29 28 2726 25 222120 1716 13 immediate\n12 1\nFigure 4.5: Format of the data processing instruction\nThe 26th bit is called the I (immediate) bit. It is similar to the I bit in SimpleRisc . If it is\nset to 1, then the second operand is an immediate, otherwise, it is a register. Since ARM has\n16 data processing instructions, we require 4 bits to represent them. This information is saved\nin bits 22-25. The 21st bit saves the S bit. If it is turned on, then the instruction will set the\nCPSR (see Section 4.2.4).\nThe rest of the 20 bits save the input and output operands. Since ARM has 16 registers,\nwe require 4 bits to encode a register. Bits 17-20 save the identifier of the first input operand\n(rs), which needs to be a register. Bits 13-16 save the identifier of the destination register (rd).\nBits 1-12 are used to save the immediate value or the shifter operand. Let us see how to\nmake best use of these 12 bits.\nEncoding Immediate Values\nARM supports 32-bit immediate values. However, we observe that we have only 12 bits to\nencode them. Hence, we cannot possibly encode all the 232 possible values. We need to choose\na meaningful subset of them. The idea is to encode a subset of 32-bit values using 12 bits. The\nhardware is expected to decode these 12 bits, and expand them to 32 bits while processing the\ninstruction.\nNow, 12 bits is a rather unwieldy value. Neither is it 1 byte nor is it 2 bytes. Hence, it was\nnecessary to come up with a very ingenious solution. The idea is to split the 12 bits into two\nparts \u2013 a 4-bit constant (rot), and an 8 bit payload (payload) (see Figure 4.6).\n4 8\nrot payload\nFigure 4.6: Format of the immediate\nLet the actual number that is encoded in these 12 bits be n. We have:\nn = payload ror (2\u00d7rot)\nThe actual number n is obtained by right rotating the payload by 2 times the value in the\nrot field. Let us now try to understand the logic of doing so. 165 (cid:13)c Smruti R. Sarangi\nThe final number n is a 32-bit value. A naive solution would have been to use the 12 bits to\nspecify the least significant bits of n. The higher order bits could be 0. However, programmers\ntend to access data and memory in terms of bytes. Hence, 1.5 bytes is of no use to us. A better\nsolution is to have a 1 byte payload and place it in any location in the 32-bit field. The rest\nof the 4 bits are used for this purpose. They can encode a number from 0 to 15. The ARM\nprocessor doubles this value to consider all even numbers between 0 and 30. It right rotates\nthe payload by this amount. The advantage of doing so is that it is possible to encode a wider\nset of numbers. For all of these numbers, there are 8 bits that correspond to the payload, and\nthe rest of the 24 bits are all zeros. The rot bits just determine which 8 bits in a 32-bit field\nare occupied by the payload.\nLet us consider a set of examples.\nExample 51\nEncode the decimal number 42.\nAnswer: 42 in the hex format is 0x2A, or alternatively 0x00 00 00 2A. There is no right\nrotation involved. Hence, the immediate field is 0x02A.\nExample 52\nEncode the number 0x2A 00 00 00.\nAnswer: This number is obtained by right rotating 0x2A by 8 places. Note that we need\nto right rotate by 4 places to move a hex digit by one position. We need to now divide 8 by\n2, to get 4. Thus, the encoded format for this number is 0x42A.\nExample 53\nEncode 0x 00 00 2A 00.\nAnswer: The first step is to count the number of right rotations. We observe that the\nnumber 0x2A has been rotated to the right by 24 positions. We now proceed to divide 24 by\n2 to obtain 12. Thus, the encoded format of the number is 0xC2A.\nExample 54\nEncode the number 0x 00 02 DC 00 as an ARM immediate.\nAnswer: The first part is to figure out the payload. The payload is \u2013 10 1101 11 \u2013 in\nbinary. This is equal to 0xB7. The next step is to figure out the rotation. Let us simplify the\ntask by observing that right rotating by n places is the same as left rotating by 32\u2212n places.\nLet us concentrate on 0xC00. This is equal to 110000000000 in binary. The rightmost 1 is (cid:13)c Smruti R. Sarangi 166\nnow at the 11th position. It has moved 10 places from the 1st position. Thus the number\nhas been rotated to the left by 10 places. It has been rotated to the right by 22 places.\n22\/2 = 11(0xB). Hence, the encoded number is 0xBB7.\nThe reader needs to understand that this encoding is supposed to be done by the assembler\nor the compiler. The user simply needs to only use values in her assembly code that can be\nencoded as an ARM immediate. For example, a number like -1 cannot be encoded as an ARM\nimmediate. It is 0xFF FF FF FF. The payload is greater than 8 bits. Ideally, an instruction of\nthe form: add r1,r1,#\u22121 is wrong. Some assemblers will try to fix the problem by changing\nthe instruction to sub r1,r1,#1. However, all assemblers are not smart enough to figure this\nout. If the user wishes to uses a value that cannot be encoded in ARM\u2019s 12 bit format, then\nthe user (or the program loader) needs to load it byte by byte in a register, and use the register\nas an operand.\nEncoding the Shifter Operand\nWe have 12 bits to encode the shifter operand. Figure 4.7 shows the scheme for encoding it. A\nshifter operand is of the form: rt (lsl|lsr|asr|ror) (shift reg\/ shift imm.)\n5 2 4\nShift type\nshift imm shift type 0 rt\n12 8 7 6 5 4 1\nlsl 00\n(a)\nlsr 01\n4 2 4\nasr 10\nshift reg shift type 1 rt ror 11\n12 9 8 7 6 5 4 1\n(b) (c)\nFigure 4.7: Format of the shifter operand\nThe first four bits (1-4) encode the id of the register rt. The next bit determines the nature\nof the shift argument (immediate or register). If it is 0 then the argument is an immediate,\notherwise it is a register. Bits 6 and 7 specify the type of the shift (also see Figure 4.7(c)).\nFor example, the type can be lsl (logical shift left). It can also be lsr (logic shift right), asr\n(arithmetic shift right), or ror (right rotate). If we are shifting by an immediate value, then\nbits 8-12 specify a 32-bit value called a shift immediate. Otherwise, if we are shifting by a value\nin a register, then bits 9-12 specify the id of the register.\nLet us consider an instruction of the form: add r3,r1,r2. In this case, the second operand\nis r2. We can think of r2 as actually a shifter operand where it is being left shifted by 0. Hence,\nto encode we need to set the shift type to lsl (00), set the argument to immediate (0), and set 167 (cid:13)c Smruti R. Sarangi\nthe shift immediate to 00000. We thus see that specifying a register as the second argument is\neasy. It is a special case of a shifter operand, and we just need to set bits 5-12 as 0.\n4.4.2 Load-Store Instructions\nA simple load or store instruction can be represented as : (ldr | str) rd, [rs, (immediate\/shifter\noperand)]. Werequireadditionalsyntaxforpreandpost-indexedaddressing(seeSection4.3.1).\nThe format for the encoding of load and store instructions is shown in Figure 4.8.\n4 2 6 4 4 12\ncond 0 1 I P UBWL rs rd shifter operand\/\n32 2928 27 20 1716 13 immediate\n12 1\nFigure 4.8: Format of the load\/store instructions\nThe semantics of the bits I, P, U, B, W, and L is shown in Table 4.17. In this case, the\nI bit has reverse semantics as compared to the case of data processing instructions. If it is 1,\nthen the last 12 bits represent a shifter operand, otherwise they represent an immediate value.\nP represents the advanced addressing mode \u2013 pre or post, and W determines if the advanced\naddressing mode is used or a simple addressing mode is used. We can either add the offset from\nthe base register or we can subtract it from the base register. This is specified by the U bit.\nThe B bit determines the granularity of the transfer \u2013 byte level or word level. Lastly, the L\nbit determines if the instruction is a load or a store.\nBit Value Semantics\n0 last 12 bits represent an immediate value\nI\n1 last 12 bits represent a shifter operand\n0 post-indexed addressing\nP\n1 pre-indexed addressing\n0 subtract offset from base\nU\n1 add offset to base\n0 transfer word\nB\n1 transfer byte\n0 do not use pre or post indexed addressing\nW\n1 use pre or post indexed addressing\n0 store to memory\nL\n1 load from memory\nTable 4.17: Semantics of I, P, U, B, W, and L bits\nThesesixbitsIPUBWLcaptureallthedifferentvariantsoftheloadandstoreinstructions.\nThe rest of the format is the same as the data processing instruction other than the encoding of (cid:13)c Smruti R. Sarangi 168\nimmediates. Immediates in memory instructions do not follow the (rot+payload) format. The\n12 bit immediate fields represents an unsigned number between 0 and 4095.\nWe thus observe that like SimpleRisc , the designers of the ARM instruction set have tried\nto stick to the same instruction format with minor variations..\nQuestion 6 What is the necessity for having the U bit?\nAnswer: Negative numbers such as -4 or -8 cannot be represented in ARM\u2019s 12 bit format\nfor specifying offsets in memory instructions. However, we might need to use addresses with\na negative displacement, especially when they are relative to the frame pointer or the stack\npointer. The U bit allows us to represent an immediate such as -4 as +4. It additionally\ninstructs the processor to subtract the displacement from the base register.\n4.4.3 Branch Instructions\n4 3 24\ncond 101 L offset\n32 29 28 2625 24 1\nFigure 4.9: Format of the branch and branch-and-link instructions\nFigure 4.9 shows the format of the branch (b) and the branch-and-link (bl) instructions. If\nthe L(link) bit is equal to 1, then the instruction is bl, otherwise it is just b. The instruction\ncontains a 24-bit signed offset. The ARM processor first shifts the offset by 2 bits. This is\nbecause each instruction is 32 bits or 4 bytes long, and additionally the hardware expects in-\nstructions to be stored at 4 byte boundaries. Therefore, the starting address of each instruction\nwill contain two zeros in its two least significant positions. Hence, there is no necessity to waste\ntwo bits in the encoding for saving these two zeros. The next step is to extend the sign of this\nshifted offset to 32 bits. Lastly, the hardware computes the branch target by adding the shifted\nand sign-extended offset to the PC plus 8 bytes.\nTheinterestingthingtonoteisthatweareaddingthesign-extendedshiftedoffsettoPC+8,\nnot the PC. We shall see in Chapter 9 that the reason for doing this is to simplify the hardware.\nThe format for branches is different from the format used to encode data transfer and data\nprocessing instructions. This is because more bits have used to encode the displacement. We\nhad followed a similar approach in SimpleRisc also. However, we need to note that having a\nnew format is not a very bad thing if it is simple as is the case for a branch. 169 (cid:13)c Smruti R. Sarangi\n4.5 Summary and Further Reading\n4.5.1 Summary\nSummary 4\n1. The ARM ISA is a simple 32-bit RISC ISA.\n(a) It uses 16 registers r0...r15.\n(b) The return address register is known as lr (link register), and it is r14.\n(c) The PC is visible to the programmer. It is register r15.\n(d) All the instructions are encoded using 32 bits.\n2. Data processing instructions accept register operands, and at most one immediate\noperand. They are 3-address instructions.\n3. ARM has a set of compare instructions that can set flags in the CPSR register. Ad-\nditionally, it is possible to instruct a standard data processing instruction to set the\nCPSR flags by adding the suffix \u2018s\u2019 to it.\n4. ARM supports conditional instructions that either execute or not depending upon the\nvalues of the CPSR flags. They can be created by appending a condition code to a\nregular data processing or branch instruction. There are 15 such condition codes.\nExamples of some condition codes are: gt (greater than), and eq (equal).\n5. ARM has two variants of branch instructions.\n(a) It has simple branch instructions that branch to another instruction.\n(b) It has branch-and-link instructions that additionally save the return address in\nthe link register lr.\n6. ARM supports both the base-index and base-offset addressing modes for load and store\ninstructions. It has additional support for shifting the index register by treating it as\na shifter operand.\n7. ARM supports complex addressing modes such as pre-indexed and post-indexed ad-\ndressing. These addressing modes update the base register.\n8. ARM also has support for loading and storing bytes and half-words (2 bytes).\n9. The instruction set encoding for data processing instructions is as follows:\n(a) Condition code (4 bits)\n(b) Instruction type (2 bits)\n(c) Second operand: immediate or register (1 bit) (cid:13)c Smruti R. Sarangi 170\n(d) Opcode (4 bits)\n(e) S bit (should the CPSR flags be set) (1 bit)\n(f) Source register1 (4 bits)\n(g) Destination register (4 bits)\n(h) Immediate or shifter operand (12 bits)\n10. The data transfer instructions do not have the S bit. They instead have extra bits to\nencode the type of load\/store instructions, and the addressing mode.\n11. The branch instructions have an L bit to specify if the return address needs to be\nsaved or not. They use PC-relative addressing and have a 24-bit signed offset. Like\nSimpleRisc , the hardware assumes that instructions are aligned to 4 byte boundaries,\nand treats this offset as a distance in terms of memory words. It thus left shifts the\noffset by 2 positions.\n4.5.2 Further Reading\nWe have presented an overview of the major features of ARM\u2019s assembly language. The reader\ncan refer to ARM\u2019s assembly language manual [arm, 2000] for more details.\nWe have deliberately left out some advanced features. A subset of ARM cores support\nThumb-1andThumb-2instructions. Theseinstructionsarebasedonasubsetofgeneralpurpose\ninstructions and have implicit operands. They are used to decrease the size of compiled code.\nSome ARM processors have extensive support for floating point instructions (VFP instruction\nset),andSIMDinstructions(executeaninstructiononmultipleintegers\/floatingpointnumbers\nin one go). However, we have not discussed these extensions for the sake of brevity. Some\nother sophisticated features of ARM processors are security extensions that prevent malicious\nprograms or users from stealing data. Since 2013 ARM processors (conforming to the ARMv8-\nA architecture) have started using a new 64-bit ARM ISA called A64. The reader can refer\nto the books by Joseph Yiu [Yiu, 2011, Yiu, 2009], William Hohl [Hohl, 2009], and J. R.\nGibson [Gibson, 2011] for a detailed discussion on the ARM instruction set and its latest\nextensions. Needless to say the reader can always find up to date documentation at ARM\u2019s\nweb site http:\/\/www.arm.com.\nExercises\nBasic ARM Instructions\nEx. 1 \u2014 Translate the following code in C to the ARM instruction set using a minimum\nnumber of instructions. Assume the variables a, b, c, d and e are 32-bit integers and stored in\nr0, r1, r2, r3 and r4 respectively. 171 (cid:13)c Smruti R. Sarangi\n(a) a=a+b+c+d+e;\n(b) a=b+c;\nd=a+b;\n(c) a=b+c+d;\na=a+a;\n(d) a=2*a+b+c+d;\n(e) a=b+c+d;\na=3*a;\nEx. 2 \u2014 Translate the following pieces of code from the ARM assembly language to a high\nlevel language. Assume that the variables a, b, c, d and e (containing integers) are stored in\nthe registers r0, r1, r2, r3 and r4 respectively.\n(a) add r0, r0, r1\nadd r0, r0, r2\nadd r0, r0, r3\n(b) orr r0, r0, r1, lsl #1\nand r1, r0, r1, lsr #1\n(c) add r0, r1, r2\nrsb r1, r0, r2\n(d) add r0, r1, r2\nadd r0, r3, r4\nadd r0, r0, r1\n(e) mov r0 #1, lsl #3\nmov r0, r0, lsr #1\nEx. 3 \u2014 Answer the following:\n(a) Write the smallest possible ARM assembly program to load the constant 0xEFFFFFF2\ninto register r0.\n(*b) Write the smallest possible ARM assembly program to load the constant 0xFFFD67FF\ninto register r0.\n* Ex. 4 \u2014 Using valid ARM assembly instructions, load the constant, 0xFE0D9FFF, into\nregister r0. Try do to it with a minimum number of instructions. DO NOT use pseudo-\ninstructions or assembler directives.\nEx. 5 \u2014 Can you give a generic set of ARM instructions or a methodology using which you\ncanloadany32-bitimmediatevalueintoaregister? Trytominimisethenumberofinstructions.\nEx. 6 \u2014 Convert the following C program to ARM assembly. Store the integer, i, in register\nr0. Assume that the starting address of array a is saved in register r1, and the starting address\nof array b is saved in register r2. (cid:13)c Smruti R. Sarangi 172\nint i;\nint b[500];\nint a[500];\nfor(i=0; i < 500; i++) {\nb[i] = a[a[i]];\n}\n** Ex. 7 \u2014 Consider the instruction, mov lr, pc. Why does this instruction add 8 to the PC,\nand use that value to set the value of lr? When is this behaviour helpful?\nAssembly Language Programming\n\u2022 For all the questions below, assume that two specialised functions, div and mod, are\navailable. The div function divides the contents of r1 by the contents of r2, and saves\nthe result in r0. Similarly, the mod function is used to divide r1 by r2, and save the\nremainder in r0. Note that in this case both the functions perform integer division.\nEx. 8 \u2014 Write an ARM assembly language program to compute the 2\u2019s complement of a\nnumber stored in r0.\nEx. 9 \u2014 WriteanARMassemblylanguageprogramthatsubtractstwo64-bitintegersstored\nin four registers.\nAssumptions:\n\u2022Assume that you are subtracting A\u2212B\n\u2022A is stored in register, r4 and r5. The MSB is in r4, and the LSB is in r5.\n\u2022B is stored in register, r6 and r7. The MSB is in r6, and the LSB is in r7.\n\u2022Place the final result in r8(MSB), and r9(LSB).\nEx. 10 \u2014 Writeanassemblyprogramtoaddtwo96-bitnumbersAandB usingtheminimum\nnumber of instructions. A is stored in three registers r2, r3 and r4 with the higher byte in r2\nand the lower byte in r4. B is stored in registers r5, r6 and r7 with the higher byte in r5 and\nthe lower byte in r7. Place the final result in r8(higher byte), r9 and r10(lower byte).\nEx. 11 \u2014 Write an ARM assembly instruction code to count the number of 1\u2019s in a 32-bit\nnumber.\nEx. 12 \u2014 Givena32-bitintegerinr3, writeanARMassemblyprogramtocountthenumber\nof 1 to 0 transitions in it. 173 (cid:13)c Smruti R. Sarangi\n* Ex. 13 \u2014 WriteanARMassemblyprogramthatchecksifa32-bitnumberisapalindrome.\nAssume that the input is available in r3. The program should set r4 to 1 if it is a palindrome,\notherwise r4 should have 0. A palindrome is a number which is the same when read from both\nsides. For example, 1001 is a 4-bit palindrome.\nEx. 14 \u2014 Design an ARM Assembly Language program that will examine a 32-bit value\nstored in r1 and count the number of contiguous sequences of 1s. For example, the value:\n01110001000111101100011100011111\ncontains six sequences of 1s. Write the final value in register r2. Use conditional instructions\nas much as possible.\n** Ex. 15 \u2014 In some cases, we can rotate an integer to the right by n positions (less than\nor equal to 31) so that we obtain the same number. For example: an 8-bit number 01010101\ncan be right rotated by 2, 4, or 6 places to obtain the same number. Write an ARM assembly\nprogram to efficiently count the number of ways we can rotate a number to the right such that\nthe result is equal to the original number.\nEx. 16 \u2014 WriteanARMassemblyprogramtoloadandstoreanintegerfrommemory, where\nthe memory saves it in the big endian format.\nEx. 17 \u2014 WriteanARMassemblyprogramtofindoutifanumberisprimeusingarecursive\nalgorithm.\n* Ex. 18 \u2014 Suppose you decide to take your ARM device to some place with a high amount\nof radiation, which can cause some bits to flip, and consequently corrupt data. Hence, you\ndecide to store a single bit checksum, which stores the parity of all the other bits, at the\nleast significant position of the number (essentially you can now store only 31 bits of data in\na register). Write an ARM assembly program, which adds two numbers taking care of the\nchecksum. Assume that no bits flip while the program is running.\n* Ex. 19 \u2014 Let us encode a 16-bit number by using 2 bits to represent 1 bit. We shall\nrepresent logical 0 by 01, and logical 1 by 10. Now let us assume that a 16-bit number is\nencoded and stored in a 32-bit register r3. Write a program in ARM assembly to convert it\nback into a 16-bit number, and save the result in r4. Note that 00 and 11 are invalid inputs\nand indicate an error. The program should set r5 to 1 in case of an error; otherwise, r5 should\nbe 0.\n** Ex. 20 \u2014 Write an ARM assembly program to convert a 32-bit number to its 12 bit\nimmediate form, if possible, with first 4 bits for rotation and next 8 bits for the payload. If the\nconversion is possible, set r4 to 1 and store the result in r5, otherwise, r4 should be set to 0.\nAssume that the input number is available in register r3.\n** Ex. 21 \u2014 Suppose you are given a 32-bit binary number. You are told that the number\nhas exactly one bit equal to 1; the rest of the bits are 0. Provide a fast algorithm to find\nthe location of that bit. Implement the algorithm in ARM assembly. Assume the input to be\navailable in r9. Store the result in r10. (cid:13)c Smruti R. Sarangi 174\n*** Ex. 22 \u2014 WriteanARMassemblylanguageprogramtofindthegreatestcommondivisor\nof two binary numbers u and v. Assume the two inputs (positive integers) to be available in r3\nandr4. Storetheresultinr5. [HINT:Thegcdoftwoevennumbersuandv is2\u2217gcd(u\/2,v\/2)]\nARM Instruction Encoding\nEx. 23 \u2014 How are immediate values encoded in the ARM ISA?\nEx. 24 \u2014 Encode the following ARM instructions. Find the opcodes for instructions from\nthe ARM architecture manual [arm, 2000].\ni) add r3, r1, r2\nii) ldr r1, [r0, r2]\niii) str r0, [r1, r2, lsl #2]\nDesign Problems\nEx. 25 \u2014 RunyourARMprogramsonanARMemulatorsuchastheQEMU(www.qemu.org)\nemulator, or arm-elf-run (available at www.gnuarm.com). 5\nx86 Assembly Language\nIn this chapter, we shall study the basics of the x86 family of assembly languages. They are\nprimarily used in Intel and AMD processors, which have an overwhelmingly large market share\nin the desktop, laptop, and low end server markets. They are steadily making deep inroads\ninto the middle and high end server markets as well as the smart phone market. Hence, it\nis essential for the reader to have a good understanding of this important class of assembly\nlanguages. At this stage we expect the reader to have a basic understanding of assembly\nlanguage from Chapter 3.\n5.1 Overview of the x86 Family of Assembly Languages\n5.1.1 Brief History\nLet us start out by noting that x86 is not one language; it is actually a family of assembly\nlanguages with a very interesting history. Intel released the 8086 microprocessor in 1978, and\ncalled it 8086. It was Intel\u2019s first 16-bit microprocessor. This microprocessor proved to be very\nsuccessful in the market, and succeeded in displacing other 8-bit competitors at that time. This\nmotivated Intel to continue this line of processors. Intel then designed the 80186 and 80286\nprocessors in 1982. 80186 was aimed at the embedded processor market, and 80286 was aimed\nat desktops. Both of them were fairly successful and helped establish Intel processors firmly in\nthe desktop market. Those days IBM was the biggest vendor of PCs (personal computers), and\nmost IBM PCs used Intel processors. The rapid proliferation of PCs led Intel to release two\nmore processors, 80386 and 80486, in 1985 and 1989 respectively. These were 32-bit processors.\nNotethatasIntelmovedfrom8086to80486, itcontinuouslyaddedmoreandmoreinstructions\nto the instruction set. However, it also maintained backward compatibility. This means that\nany program meant to run on a 8086 machine, could also run on a 80486 machine. Secondly,\nit also maintained a consistent assembly language format for this family of processors whose\nname ended with \u201c86\u201d. Over time this family of processors came to be known as \u201cx86\u201d.\n175 (cid:13)c Smruti R. Sarangi 176\nGradually, other companies started using the x86 instruction set. Most notably, AMD\n(Advanced Micro Devices) started designing and selling x86 based processors. AMD released\ntheK5,K6,andK7processorsinthemidninetiesbasedonthe32-bitx86instructionset. Italso\nintroduced the x86 64 instruction set in 2003, which was a 64-bit extension to the standard 32-\nbit x86 Intel ISA. Many other vendors such as VIA, and Transmeta also started manufacturing\nx86 based processors starting from 2000.\nEach vendor has historically taken the liberty to add new instructions to the base x86\ninstruction set. For example, Intel has proposed many extensions over the years such as Intel(cid:13)R\nMMXTM, SSE1, SSE2, SSE3, and SSE4. The number of x86 instructions are more than 900\nas of 2012. Similarly, AMD introduced the 3D Now!TMinstruction set, and VIA introduced its\ncustom extensions. The rich history of x86 processors has led to many different extensions of\nthe basic instruction set, and there are numerous assemblers that have their unique syntax.\nAlmost all x86 vendors today support hundreds of instructions. Current 64-bit Intel processors\nsupport 16-bit, and 32-bit code that dates way back to the original 8086.\nIf we try to classify the entire family tree of x86 ISAs, we can broadly divide them as\n16-bit, 32-bit, and 64-bit instruction sets. 16-bit instruction sets are rarely used nowadays. 32-\nbit instruction sets are extremely popular in the smart phone, embedded, and laptop\/netbook\nmarkets. The64-bitISAs(alsoknownasthex86-64ISA)aremainlymeantforworkstationclass\ndesktop\/laptops and servers. Other than minor syntactic differences the assembly languages\nfor these instruction sets are mostly the same. Hence, learning one ISA is sufficient. In this\nbook, we try to strike a compromise between embedded processors, laptops, desktops, smart\nphones, and high end servers. We thus focus on the 32-bit x86 ISA because in our opinion it\nfalls in the middle of the usage spectrum of the x86 ISA. We shall mention the minor syntactic\ndifferences with other flavours of x86 whenever the need arises.\n5.1.2 Main Features of the x86 ISA\nBefore delving into the details of the 32-bit x86 ISA, let us list some of its main features.\n1. It is a CISC ISA. Instructions have varying lengths, and operands also do not have a fixed\nlength.\n2. There are at least 300 scalar instructions, and this number is increasing every year.\n3. Almost all the instructions can have a memory operand. In fact, most instructions allow\na source, and a destination memory operand.\n4. Most of the x86 instructions are in the 2-address format. For example, the assembly\ninstruction to add two registers eax, and ebx, is add eax, ebx. Here, we add the contents\nof the eax, and ebx registers, and save the results in the eax register.\n5. x86 has many complicated addressing modes for memory operands. Along with the tradi-\ntionalbase-offsetaddressingmode,itsupportsbase-indexandbase-index-offsetaddressing\nmodes.\n6. It does not have a return address register. Function call and return instructions, save and\nretrieve the return address from the stack. 177 (cid:13)c Smruti R. Sarangi\n7. Like ARM and SimpleRisc , x86 has a flags register that saves the outcome of the last\ncomparison. The flags register is used by conditional branch instructions.\n8. Unlike SimpleRisc , x86 instructions do not see an unified view of instruction and data\nmemory. The x86 memory is segmented. This means that instructions and data reside in\ndifferent memory regions (known as segments). x86 machines restrict the segments that\nan instruction can access.\nIt is true that the x86 architecture is a CISC instruction set, and it has hundreds of opcodes\nand many addressing modes. Nevertheless, we are sure that at the end of this chapter, the\nreader will concur with us that the x86 instruction set is in reality a fairly simple instruction\nset, is easy to understand, and is very elegant. A conventional argument supporting the case\nof RISC ISAs is that the hardware is simpler, and more efficient. Consequently, in modern\nIntel\/AMD processors (Pentium(cid:13)R 4 onwards), the x86 instructions are internally translated\ninto RISC instructions, and the entire processor is essentially a RISC processor. We can thus\nget the best of both worlds.\n5.2 x86 Machine Model\n5.2.1 Integer Registers\n64 bits\n32 bits\nax ah al\n16 bits\nrax eax ax bx bh bl\ncx ch cl\nrbx ebx bx\ndx dh dl\nrcx ecx cx\nrdx edx dx\n16 bit segment registers\nrsp esp sp\ncs es\nrbp ebp bp\nss fs\nsi\nrsi esi ds gs\nrdi edi di\n64 bits\nr8\n32 bits\nr9 16 bits\nrflags eflags flags\nr15 rip eip ip\nFigure 5.1: The x86 register set (cid:13)c Smruti R. Sarangi 178\nFigure 5.1 shows the x86 register set. The 16 and 32-bit x86 ISAs have 8 general purpose\nregisters. Theseregistershaveaninterestinghistory. Theoriginal8080microprocessordesigned\nfortyyearsagohadseven8-bitgeneralpurposeregistersnamelya, b,c,d, e, f andg. Inthelate\nseventies, x86 designers decided to create a 16-bit processor called 8086. They decided to keep\nfour registers (a, b, c, and d), and suffixed them with the \u2019x\u2019 tag (\u2019x\u2019 for extended). Thus, the\nfour general purpose registers got renamed to ax, bx, cx, and dx. Additionally, the designers\nof the 8086 machine decided to retain some 16-bit registers namely the stack pointer (sp), and\nthe register to save the PC (ip). The designers also introduced three extra registers in their\ndesign \u2013 bp (base pointer), si (starting index), and di (destination index). The intention of\nadding the bp register was to save the value of the stack pointer at the beginning of a function.\nCompilers are expected to set sp equal to bp at the end of the function. This operation destroys\nthe stack frame of the callee function. The registers si, and di are used by the rep instruction\nthat repeats a certain operation. Typically, a single rep instruction is equivalent to a simple\nfor loop. Thus, the 8086 processor had eight 16-bit general purpose registers \u2013 ax, bx, cx,\ndx, sp, bp, si, and di. It was further possible to access the two bytes (lower and upper) in the\nregisters ax \u2013 dx. For example, the lower byte in the ax register can be addressed as al, and\nthe upper byte can be addressed as ah. 16-bit x86 instructions can use combinations of 8-bit\nand 16-bit operands.\nThe 8086 processor had two special purpose registers. The first register called ip contained\nthe PC. The PC is typically not accessible to programmers on x86 machines (unlike the ARM\nISA). The second special purpose register is the flags register that saves the results of the last\ncomparison (similar to the flags register in ARM and SimpleRisc ). The flags register is used\nby subsequent conditional branch instructions to compute the outcome of the branch.\nIn the might eighties, when Intel decided to extend the 8086 design to support 32-bit\nregisters, it decided to keep the same set of registers (8 general purpose + ip + flags), and\nsimilar nomenclature. However, it extended their names by adding an \u2019e\u2019 prefix. Thus in a 32-\nbit machine, register eax is the 32-bit version of ax. To maintain backward compatibility with\nthe 8086, the lower 16 bits of eax can be addressed as ax (if we wish to use 16-bit operands).\nFurthermore, the two bytes in ax can be addressed as ah and al (similar to 8086). As shown\nin Figure 5.1, the names were changed for all the other registers also. Notably, in a 32-bit\nmachine, the stack pointer is stored in esp, the PC is stored in eip, and the flags are stored in\nthe eflags register.\nThere are many advantages to this strategy. The first is that 8086 code can run on a 32-bit\nx86 processor seamlessly. All of its registers are defined in the 32-bit ISA. This is because each\n16-bit register is represented by the lower 16 bits of a 32-bit register. Hence, there are no issues\nwithbackwardcompatibility. Secondly, wedonotneedtoaddnewregisters, becausewesimply\nextend each 16-bit register with 16 additional bits. We refer to the new register with a new\nname (16-bit name prefixed with \u2019e\u2019).\nExactly the same pattern was followed while extending the x86 ISA to create the 64-bit\nx86-64 ISA. The first letter was replaced from \u2019e\u2019 to \u2019r\u2019 to convert a 32-bit register to a 64-bit\nregister. For example, the register rax is the 64-bit version of eax. Its lower 32 bits can be\naddressed as eax. The connotation of ax, ah, and al remains the same as before. Additionally,\nthe x86-64 ISA introduced 8 more general purpose registers namely r8 \u2013 r15. However, their\nsubfields cannot be addressed directly. The 64-bit PC is saved in the rip register, and the flags\nare stored in the rflags register. 179 (cid:13)c Smruti R. Sarangi\nThe eflags register\nLet us now quickly discuss the structure of the eflags register. Like ARM and x86, the eflags\nregister contains a set of fields, where each field or bit indicates the status of execution of the\ninstruction that last set it. Table 5.1 lists some of the most commonly used fields in the eflags\nregister, along with their semantics.\nField Condition Semantics\nOF Overflow Set on an overflow\nCF Carry flag Set on a carry or borrow\nZF Zero flag Set when the result is a 0,\nor the comparison leads to an\nequality\nSF Sign flag Sign bit of the result\nTable 5.1: Fields in the eflags register\n5.2.2 Floating Point Registers\nThe floating point instructions in x86 have a dual view of the floating point register file. They\ncan either see them as normal registers or as a set of registers organised as a stack. Let us\nelaborate.\nTo start out, x86 defines 8 floating point registers named: st0 ... st7. These are 80-bit\nregisters, The x86 floating point format has a 64-bit mantissa, and a 15-bit exponent. It is thus\nmore precise than double precision numbers. The registers st0 to st7 are organised as a stack.\nHere, st0 is the top of the stack, and st7 is the bottom of the stack as shown in Figure 5.2.\nAdditionally, x86 has a tag register that maintains the status of each register in the stack.\nThe tag register has 8 fields (1 field for 1 register). Each field contains 2 bits. If the value of\nthese bits is 00, then the corresponding register contains valid data. If the value is 01, then\nthe register contains a 0, and if it is 11, then the register is empty. 10 is reserved for special\npurposes. We shall refer to the stack of registers, as the floating point stack, or simply the FP\nstack.\nThe registers st0 to st7 are positions on the FP stack. st0 is always the top of the stack,\nand st7 is always the bottom of the stack. If we push a data item on to the FP stack, then\nthe contents of each register get transferred to the register below it. If the stack is full (means\nthat st7 contains valid data), then a stack overflow occurs. This situation needs to be avoided.\nMost floating point instructions operate on data values saved at the top of the stack. They pop\nthe source operands, and push the destination operand.\n5.2.3 View of Memory\nLet us now describe the functionality of the segment registers (see Figure 5.1), and the view\nof memory. x86 instructions can have two views of memory. The first view is like ARM and\nSimpleRisc , which views memory as one large array of bytes that stores both code and data. (cid:13)c Smruti R. Sarangi 180\nTop of the stack\nst0\nst1\nst2\nst3\nst4\nst5\nst6\nst7\nFigure 5.2: The x86 floating point register stack\nThis is known as the linear memory model. In comparison, the segmented memory model views\nmemory as consisting of fixed size segments, where each segment is tailored to store one kind\nof data such as code, stack data, or heap data (for dynamically allocated data structures). We\nshall not discuss the linear model of memory because we have seen it before in Chapter 3. Let\nus discuss the segment registers, and the segmented memory model in this section.\nDefinition 40\nLinear Memory Model A linear memory model views the entire memory as one large\narray of bytes that saves both code and data.\nSegmented Memory Model A segmented memory model views the memory as a se-\nquence of multiple fixed size segments. Code, data, and the stack have their own\nsegments.\nThe Segmented Memory Model\nLet us define the term address space as the set of all memory addresses accessible to a program.\nThe aim of the segmented memory model is to divide the address space into separate smaller\naddress spaces. Each address space can be specialised to store a specific type of information\nsuch as code or data.\nThere are two reasons for using segmentation. The first is historical. In the early days\ndifferent parts of a program were physically saved at different locations. The code was saved on\npunch cards, and the memory data was stored in DRAM memories. Hence, it was necessary to\nFP\nstack 181 (cid:13)c Smruti R. Sarangi\npartition the address space among the devices that stored all the information that a program\nrequired (code, static data, dynamic data). This reason is not valid anymore. Nowadays, all\nthe information a program requires is typically stored at the same place. However, we still\nneed segmentation to enforce security. Hackers and viruses typically try to change the code of\na program and insert their own code. Thus a normal program can exhibit malicious behaviour\nand can corrupt data, or transfer sensitive data to third parties. To ensure added protection,\nthecoderegionissavedinacodesegment. Mostsystemsdonotallownormalstoreinstructions\nto modify the code segment. We can similarly partition the data segments for different classes\nof data. In Section 10.4.6, we will have a more thorough discussion on this topic.\nSegmentation in x86\nThe 8086 designers had 6 segment registers that stored the most significant 16 bits of the\nstarting location of the segment. The remaining bits were assumed to be all zeros. The cs\nregister stored the upper 16 bits of the starting location of the code segment. Similarly, the ds\nregisterstoredtheupper16bitsofthestartinglocationforthedatasegment,andthessregister\nstored the corresponding set of bits for the stack segment. The es (extra segment), fs, and\ngs registers could be used to store information for additional user defined segments. Till date\nall x86 processors have preserved this model (see Figure 5.1). The contents of instructions are\nsaved in the code segment, and the data that a program accesses is saved in the data segment.\nIn most small programs, the stack and data segments are the same. In 8086 processors the\nmemory address was 20 bits wide. Hence, to obtain the final address also known as the linear\naddress, the 8086 processor first shifted the contents of the segment register 4 positions to\nthe left to obtain the starting location of the segment. It then added this address with the\nmemory address specified by the instruction. We can think of the memory address specified by\nan instruction as an offset in the segment, where the starting memory location of the segment\nis indicated by the appropriate segment register.\nThis strategy served the needs of the 8086 designers well. However, this strategy is not\nsuitable for 32 and 64-bit machines. In this case, the memory addresses are 32 and 64 bits\nwide respectively. Thus, the segment registers need to be wider. In the interest of backward\ncompatibility, designers did not touch the segment registers. They just changed the semantics\nof its contents for newer processors. Instead of saving the upper 16 bits of the starting location\nof a segment, the registers now contain a segment id. The segment id uniquely identifies a\nsegment across all the programs running in a system. To get the starting location, 32\/64-bit\nx86 processors, lookup a segment descriptor table with 13 bits (bits 4 to 16) of the segment\nid. 13 bits can specify 8192 entries, which is more than sufficient for all the programs in the\nsystem.\nModern x86 processors have two kinds of segment descriptor tables namely the local de-\nscriptor table (LDT), and the global descriptor table (GDT). The LDT is typically local to\na process (running instance of a program) and contains the details of the segments for that\nprocess. The LDT is normally not used nowadays because programs do not use a lot of seg-\nments. In comparison there is only one system level GDT. The GDT can contain up to 8191\nentries ( the first entry is reserved). Each entry in the GDT contains the starting address of\nthe segment, the size of the segment, and the privileges required to access the segment. Every\nmemory access needs to go through the GDT for fetching the starting address of the segment. (cid:13)c Smruti R. Sarangi 182\nThis unnecessarily lengthens the critical path of a memory request, and creates contention at\nthe GDT. To make the access to the GDT faster, modern processors have a small structure\ncalled a segment descriptor cache that stores a few entries of the GDT that are relevant to\nthe currently executing process. The descriptor cache typically stores the details of all the\nsegments that the frequently running processes use. This strategy ensures that we do not need\nto access the GDT on every memory access. The small and fast descriptor cache is sufficient.\nAfter accessing the descriptor cache, or the GDT, x86 processors get the starting address of\nthe segment. They subsequently generate the memory address by adding the address specified\nin the instruction with the starting address of the segment. This address is then passed on to\nthe memory system.\nDefinition 41\nProcess It is defined as the running instance of a program. For example, if we run two\ncopies of a program, then we create two processes.\nLDT (Local Descriptor Table) The LDT is a per process table that saves the descrip-\ntion of all the segments that a process uses. The LDT is indexed by a segment id, and\ncontains the starting address of the segment, and the privileges required to access it.\nIt is not used very frequently in modern systems.\nGDT (Global Descriptor Table) The GDT is similar to the LDT. However, it is a\nsystem wide table that is shared by all the processes running on a machine.\nNow, that we have discussed the view of the register files, and the memory system, let us\ndescribe the addressing modes.\n5.2.4 Addressing Modes\nAddressing Modes for Specifying Immediates\nThebestthingaboutx86isthattherearenosizerestrictionsonimmediates. Immediatescanbe\naslargeasthesizeoftheregister. Forexample,ina32-bitsystem,thesizeoftheimmediatecan\nbeaslargeas32bits. Dependingupontheassemblylanguage, wecanspecifyimmediatesinthe\nhex format (0x...), binary format (e.g., 10101b), or in decimal. Most of the time programmers\npreferthehexordecimalformats. Forhexadecimalnumbersmostassemblersallowustospecify\nthe number with the standard 0x prefix. Additionally, we can specify a number with the h\/H\nsuffix. For example, 21H is the same as 0x21. For negative numbers, we need to simply put a\n\u2018-\u2019 before the number.\nAddressing Modes for Specifying Registers\nAll registers in x86 are addressed by their names. For example, the general purpose registers\non a 32-bit machine are addressed as eax, ebx ... edi, according to the rules mentioned in 183 (cid:13)c Smruti R. Sarangi\nSection 5.2.1. We can use 16-bit register names in 32-bit mode, and we can use 16 and 32-bit\nregisteraddressingin64-bitmode. Notethatwecannotdothereverse. Forexample, wecannot\nuse 64-bit register names in 32-bit mode.\nAddressing Modes for Memory Operands\nx86 supports a variety of addressing modes for main memory. In specific, it supports the\nregister-indirect, base-offset, base-index, and base-index-offset addressing modes as mentioned\nin Section 3.2.5. In addition, it also supports a new addressing mode called the base-scaled-\nindex-offset addressing mode that scales the index by a constant factor. Let us elaborate.\n\uf8ee \uf8f9\n\uf8eeeax\uf8f9\n\uf8ef\uf8eb eax\uf8f6 \uf8fa\n\uf8eecs :\uf8f9\uf8efebx\uf8fa \uf8ef \uf8fa\n\uf8ef \uf8fa\n\uf8ef \uf8fa \uf8ef\uf8ecebx\uf8f7\n\uf8fa\n\uf8efds :\uf8fa\uf8efecx\uf8fa \uf8ef\uf8ec \uf8f7 \uf8ee1\uf8f9\uf8fa\n\uf8ef \uf8fa \uf8ec \uf8f7\n\uf8ef \uf8fa \uf8ef \uf8fa \uf8ef \uf8ececx\uf8f7 \uf8fa\n\uf8ef \uf8fa \uf8ef \uf8fa\nss : \uf8efedx\uf8fa \uf8ec \uf8f7 \uf8ef2\uf8fa\n\uf8ef \uf8fa \uf8ef \uf8fa\naddress = \uf8ef \uf8fa\uf8ef \uf8fa+\uf8ef\uf8ecedx\uf8f7\u00d7\uf8ef \uf8fa \uf8fa+[displacement] (5.1)\n\uf8ef \uf8fa \uf8ec \uf8f7 \uf8ef \uf8fa\n\uf8efes :\uf8fa esp \uf8ef 4 \uf8fa\n\uf8ef\n\uf8ef\n\uf8fa \uf8fa\uf8ef\n\uf8ef\n\uf8fa\n\uf8fa\n\uf8ef \uf8ef\uf8ec \uf8ecebp\uf8f7\n\uf8f7\n\uf8f0 \uf8fb\uf8fa\n\uf8fa\n(cid:124) of(cid:123) f(cid:122)\nset\n(cid:125)\nfs : \uf8efebp\uf8fa \uf8ec \uf8f7 8\n\uf8f0 \uf8fb\uf8ef \uf8fa \uf8ef \uf8ef\uf8ecesi \uf8f7 \uf8fa\n\uf8fa\ngs : \uf8ef esi \uf8fa \uf8ef\uf8ed \uf8f8 (cid:124)(cid:123)(cid:122)(cid:125) \uf8fa\n\uf8f0 \uf8fb \uf8ef edi scale\uf8fa\n\uf8f0 \uf8fb\nedi\n(cid:124) (cid:123)(cid:122) (cid:125)\n(cid:124) (cid:123)(cid:122) (cid:125) index\nbase\nEquation 5.1 shows the generic format of a memory address in the 32-bit version of x86.\nThe interesting aspect of x86 memory addressing is that all of these fields are optional. Hence,\nit is possible to have a large number of addressing modes.\nLet us first consider the addressing modes that require a base register. With the base\nregister, we can optionally specify a segment register. If we do not specify a segment register,\nthen the hardware assumes default segments (ds for data, ss for stack, and cs for code). We\ncan subsequently specify an index. The index is contained in another register (excluding esp).\nWe can optionally multiply the index with a power of 2 (1, 2, 4, or 8). Lastly, we can specify a\n32-bit offset known as the displacement. The memory address is computed using Equation 5.1.\nNow, let us look at addressing modes that do not require a base register. We can just use an\nindex register and optionally scale it by 1, 2, 4, or 8. For example, we can specify that we want\nto access the memory address equal to 2\u00d7ecx. This approach uses the scaled-index addressing\nmode. We can optionally add a fixed offset (known as the displacement) to the address.\nLastly, it is possible to specify the entire 32-bit address in the displacement field, and not\nspecify any register at all. This approach is typically used in the operating system code to\ndirectly operate on memory addresses. Regular assembly programmers need to strictly avoid\nsuch direct memory addressing because most of the time we are not aware of the exact memory\naddresses. For example, the starting address of the stack pointer is typically allocated at run\ntime in modern systems, and tends to vary across runs. Secondly, this is not a portable and\nelegant approach. It is only meant for operating system writers.\nLet us explain with examples (see Table 5.2). (cid:13)c Smruti R. Sarangi 184\nDefinition 42\nIn the x86 ISA, the fixed offset used while specifying the effective address of a memory\noperand, is known as the displacement.\nMemory operand Value of the address Addressing mode\n(in register transfer notation)\n[eax] eax register-indirect\n[eax + ecx*2] eax + 2 * ecx base-scaled-index\n[eax + ecx*2 - 32] eax + 2 * ecx - 32 base-scaled-index-offset\n[edx - 12] edx - 12 base-offset\n[edx*2] edx * 2 scaled-index\n[0xFFE13342] 0xFFE13342 memory-direct\nTable 5.2: Example of memory operands\n5.2.5 x86 Assembly Language\nThere are various x86 assemblers such as MASM [mas, ], NASM [nas, ], and the GNU assem-\nbler [gx8, ]. In this book, we shall present code snippets that have been tested with the NASM\nassembler. The popular NASM assembler is freely available at [nas, ], and is known to work on\na variety of platforms including Windows(cid:13)R , Mac OS X, and different flavours of Linux. Note\nthat we shall mostly avoid using NASM specific features, and we shall keep the presentation of\nassembly code very generic. Our assembly codes should be compatible with any assembler that\nsupports the Intel format for x86 assembly. The only major feature of NASM that we shall use\nis that comments begin with a \u2018;\u2019 character.\nLet us now describe the structure of an assembly language statement in the Intel format.\nIts generic structure is as follows.\nStructure of an Assembly Statement\n<label>: <assembly instruction> ; <comment>\nFor an assembly instruction, the label and the comment are optional. Alternatively, we can\njust have a label or a comment, or a combination of both in a single line. In our code, we shall\nuse labels starting with a \u2019.\u2019. However, labels can start with regular alphabets and other special\ncharacters also. For a detailed description readers can refer to the NASM documentation.\nEach x86 assembly instruction has an opcode followed by a set of operands.\nx86 Assembly Instruction\n<opcode>\n<opcode> <operand1>\n<opcode> <operand1>, <operand2> 185 (cid:13)c Smruti R. Sarangi\nAn overwhelming majority of x86 instructions are in the 0, 1 and 2-address formats. 0-\naddress format instructions like nop instructions in SimpleRisc do not require any operands.\n1-addressformatinstructionshaveasinglesourceoperand. Inthiscasethedestinationoperand\nis equal to the source operand. For example, the instruction not eax computes the bitwise\ncomplement of eax, and saves the result in eax. In two operand instructions, the first operand\nis the first source operand and also the destination operand. The second operand is the second\nsource operand. For example, add eax, ebx, adds the contents of eax and ebx, and subsequently\nsaves the result in eax.\nThe source operands can be register, memory, or immediate operands. However, both the\nsources cannot be memory operands. Needless to say the destination operand cannot be an\nimmediate operand. When a single operand is both the source and destination, both the rules\napply.\n5.3 Integer Instructions\n5.3.1 Data Transfer Instructions\nThe mov Instruction\nSemantics Example Explanation\nmov (reg\/mem), (reg\/mem\/imm) mov eax, ebx eax \u2190 ebx\nTable 5.3: Semantics of the mov instruction\nThe mov instruction is a very simple yet versatile instruction in the x86 ISA. It moves the\ncontents of the second operand, into the first operand. The second operand can be a register,\na memory location, or an immediate. The first operand can be a register or a memory location\n(Table 5.3 shows the semantics). The reader needs to note that both the operands cannot be\nmemory locations.\nWe thus do not need any dedicated load\/store instructions in x86. The mov instruction\ncan achieve the function of loading and storing memory values because it accepts memory\noperands. Themov instructioncanalsotransfervaluesbetweenregisters(similartoSimpleRisc\nand ARM). Thus, we have fused the functionality of three RISC instructions into one CISC\ninstruction. Let us consider some examples.\nExample 55\nWrite an x86 assembly instruction to set the value of ebx to -17.\nAnswer:\nmov ebx, -17 (cid:13)c Smruti R. Sarangi 186\nExample 56\nWrite an x86 assembly instruction to load ebx with the contents of (esp - eax*4 -12).\nAnswer:\nmov ebx, [esp - eax*4 -12]\nExample 57\nWrite an x86 assembly instruction to store the contents of edx in (esp - eax*4 -12). An-\nswer:\nmov [esp - eax*4 -12], edx\nmovsx, and movzx Instructions\nSemantics Example Explanation\nmovsx reg, (reg\/mem) movsx eax, bx eax \u2190 sign extend(bx), the second\noperand is either 8 or 16 bits\nmovzx reg, (reg\/mem) movsx eax, bx eax \u2190 zero extend(bx), the second\noperand is either 8 or 16 bits\nTable 5.4: Semantics of the movsx, and movzx instructions\nThe simple mov instruction assumes that the sizes of the operands are the same (16, or\n32, or 64 bits). However, sometimes we face the need for saving a smaller register or memory\noperand in a larger register. For example, if we save the 16 bit register ax in ebx then we need\nwe have two options. We can either extend the sign of the input operand, or pad it with 0s.\nThe movsx instruction (see Table 5.4) copies a smaller register or memory operand to a larger\nregister and extends its sign. For example, the following code snippet extends the sign of bx\n(from 16 to 32 bits), and saves the results in eax.\nmovsx eax, bx ; eax = sign_extend(bx)\nThe movzx instruction is defined on the same lines. However, instead of performing a sign\nextension, it pads the MSB bits with 0s.\nmovzx eax, bx ; eax = bx (unsigned) 187 (cid:13)c Smruti R. Sarangi\nSemantics Example Explanation\nxchg (reg\/mem), (reg\/mem) xchg eax, [eax + edi] swapthecontentsofeax\nand [eax + edi] atomi-\ncally\nTable 5.5: Semantics of the xchg instruction\nThe Atomic Exchange (xchg) Instruction\nThe xchg instruction swaps the contents of the first and second operands. Here, also we cannot\nhave two memory operands. This instruction ensures that before the operation is done, no\nother operation can read temporary values. For example, if we are swapping the values of eax,\nand the memory operand [ebx], there might be an intermediate point in the execution where\nthe contents of eax are updated, but the contents of [ebx] are not updated. The x86 processor\ndoes not allow other threads (sub-programs that share the address space) to read the contents\nof [ebx] at this point. It makes other conflicting instructions in other execution threads wait\ntill the xchg instruction completes. This property is known as atomicity. An instruction is\natomic if it appears to execute instantaneously. Most of the time, atomic instructions such as\nxchg are used for implementing data structures that are shared across multiple threads. The\nreader should read Chapter 11 for a detailed discussion on parallel software that uses multiple\nthreads.\nDefinition 43\nAn instruction is atomic if it appears to execute instantaneously.\nExample 58\nWrite a function to swap the contents of eax, and [esp].\nAnswer:\nxchg eax, [esp]\npush and pop Instructions\nThe x86 architecture is explicitly aware of the stack. It has two dedicated instructions for\nsaving and retrieving operands off the stack. The push instruction pushes data on the stack. In\nspecific,thepushinstructioncanpushthecontentsofaregister,memorylocation,orimmediate\non the stack. It has just one source operand. Its operation is shown in Table 5.6. Conceptually,\nit first saves the value of the first operand as a temporary value temp. Then, it decrements the (cid:13)c Smruti R. Sarangi 188\nSemantics Example Explanation\npush (reg\/mem\/imm) push ecx temp \u2190 ecx; esp \u2190 esp - 4; [esp] \u2190 temp\npop (reg\/mem) pop ecx temp \u2190 [esp]; esp \u2190 esp + 4; ecx \u2190 temp\nTable 5.6: Semantics of the push and pop instructions\nstack pointer, and transfers the temporary value to the top of the stack. In a 32-bit system,\nwe decrement the stack pointer by 4. When we are pushing a register, the processor knows its\nsize based on the name of the register. For example, if the name of the register is ax, its size\nis 16 bits, and if the name of the register is eax, its size is 32 bits. However, if we are pushing\na memory operand or a constant, the assembler cannot determine the size of the operand. We\nmight be intending to push 2 bytes, 4 bytes, or 8 bytes on the stack. In this case, it is necessary\ntoindicatethesizeoftheoperandtotheassemblersuchthatitcangenerateappropriatebinary\ncode. In the NASM assembler, we specify this information as follows:\npush dword [esp]\nThe modifier dword (double word) represents the fact that we need to push 4 bytes on the\nstack. The starting address of the 4 bytes is stored in esp. Table 5.7 shows the list of modifiers\nfor different sized data types.\nModifier Size\nbyte 8 bits\nword 16 bits\ndword 32 bits\nqword 64 bits\nTable 5.7: Modifiers in the NASM assembler\nFor pushing in the value of immediate values, NASM assumes they are by default 32 bits\nlong (if we are running NASM in 32-bit mode). We can override this setting by specifying a\nsize modifier (word,dword,...) in the instruction.\nOn the same lines we can define a pop instruction as shown in Table 5.6. Conceptually,\nthe pop instruction saves the top of the stack in a temporary location. It then proceeds to\nincrement the stack pointer by 4 (in the case of 32 bits), and then it saves the temporary value\nin the destination. The destination can either be a register or a memory location. The push\nand pop instructions thus make working with the stack very easy in x86 assembly programs.\nExample 59 What is the final value of ebx?\nmov eax, 10\npush eax\nmov ebx, [esp] 189 (cid:13)c Smruti R. Sarangi\nAnswer: 10\nExample 60\nWhat is the final value of ebx?\nmov ebp, esp\nmov eax, 10\nmov [esp], eax\npush dword [esp]\nmov ebx, [ebp-4]\nAnswer: Note that ebp and esp are initially the same. After we push a value to the stack,\nesp gets decremented by 4. Hence, the new location of the top of the stack is equal to ebp\u22124.\nSince we push the value of eax (10) to the top of the stack using the push instruction, the\nvalue of ebx is equal to 10.\nExample 61 What is the final value of ebx?\nmov eax, 17\npush eax\npop dword [esp]\nmov dword ebx, [esp]\nAnswer: 17\n5.3.2 ALU Instructions\nLet us now discuss the rich set of ALU instructions that x86 processors support.\nAdd and Subtract Instructions\nTable 5.8 shows the add and subtract operations that are typically used in x86 processors. The\nbasic add and subtract instructions add the values of the first and second operands, and treat\nthe first operand also as the destination operand. They set the carry and overflow fields of\nthe eflags register. The adc instruction adds its two source operands, and also adds the value\nof the carry bit. Similarly, the sbb instruction subtracts the second operand from the first,\nand then subtracts the carry bit from the result. We can use the adc and sbb instructions to\nadd or subtract very large integers (refer to Example 62 and Example 63). In these examples, (cid:13)c Smruti R. Sarangi 190\nSemantics Example Explanation\nadd (reg\/mem), (reg\/mem\/imm) add eax, ebx eax \u2190 eax + ebx\nsub (reg\/mem), (reg\/mem\/imm) sub eax, ebx eax \u2190 eax - ebx\nadc (reg\/mem), (reg\/mem\/imm) adc eax, ebx eax \u2190 eax + ebx + (carry bit)\nsbb (reg\/mem), (reg\/mem\/imm) sbb eax, ebx eax \u2190 eax - ebx - (carry bit)\nTable 5.8: Semantics of add and subtract instructions\nwe first operate on the lower bytes. While operating on the higher bytes we need to take the\ncarry generated by adding or subtracting the lower bytes into account. We use the adc and sbb\ninstructions respectively for this purpose.\nExample 62\nWrite an x86 assembly program to add two 64-bit numbers. The first number is stored in\nthe registers ebx, and eax, where ebx stores the higher byte, and eax stores the lower byte.\nThe second number is stored in edx, and ecx. Save the result in ebx (higher byte), and\neax(lower byte).\nAnswer:\nadd eax, ecx\nadc ebx, edx\nExample 63\nWrite an x86 assembly program to subtract two 64-bit numbers. The first number is stored\nin the registers ebx, and eax, where ebx stores the higher byte, and eax stores the lower\nbyte. The second number is stored in edx, and ecx. Subtract the second number from the\nfirst number. Save the result in ebx (higher byte), and eax(lower byte).\nAnswer:\nsub eax, ecx\nsbb ebx, edx\ninc, dec, and neg Instructions\nTable 5.9 shows the semantics of increment (inc), decrement (dec), and negate (neg) instruc-\ntions. The inc instruction, adds 1 to the source operand. In this case also the source and\ndestination operands are the same. Similarly, the dec instruction subtracts 1 from the source\noperand, which is also the destination operand. Note that the operand can either be a register 191 (cid:13)c Smruti R. Sarangi\nSemantics Example Explanation\ninc (reg\/mem) inc edx edx \u2190 edx + 1\ndec (reg\/mem) dec edx edx \u2190 edx - 1\nneg (reg\/mem) neg edx edx \u2190 -1 * edx\nTable 5.9: Semantics of inc, dec, and neg instructions\nor a memory location. The neg instruction computes the negative of the value stored in the\nfirst operand (register or memory). Let us consider an example (see Example 64).\nExample 64\nWrite an x86 assembly code snippet to compute eax = -1 * (eax + 1).\nAnswer:\ninc eax\nneg eax\nThe Compare(cmp) Instruction\nSemantics Example Explanation\ncmp (reg\/mem), (reg\/mem\/imm) cmp eax, [ebx + 4] compare the values in eax,\nand [ebx+4], and set the flags\ncmp (reg\/mem), (reg\/mem\/imm) cmp ecx, 10 compare the contents of ecx\nwith 10, and set the flags\nTable 5.10: Semantics of the cmp instructions\nTable 5.10 shows the cmp (compare) instruction. It compares two operands and sets the\nvalues of the flags. It performs the comparison by subtracting the value of the second operand\nfromthefirstoperand. Itisconceptuallyasubtractinstructionthatdoesnothaveadestination\noperand.\nMultiplication and Division Instructions\nTable 5.11 shows the signed multiplication and division instructions in x86. They are known as\nimul andidiv respectively. Theunsignedvariantsoftheinstructionsareknownasmul anddiv.\nThey have exactly the same semantics as their signed counterparts. The signed instructions\nare more generic. Hence, we only discuss their operation in this section.\nTheimulinstructionhasthreevariants. The1-addressformatvarianthas1sourceoperand,\nwhich can either be a register or a memory address. This source operand is multiplied with the (cid:13)c Smruti R. Sarangi 192\nSemantics Example Explanation\nimul (reg\/mem) imul ecx edx:eax \u2190 eax * ecx\nimul reg, (reg\/mem) imul ecx, [eax + 4] ecx \u2190 ecx * [eax + 4]\nimul reg, (reg\/mem), imm imul ecx, [eax + 4], 5 ecx \u2190 [eax + 4] * 5\nidiv (reg\/mem) idiv ebx Divide (edx:eax) by the con-\ntents of ebx; eax contains the\nquotient,andedxcontainsthe\nremainder.\nTable 5.11: Semantics of the imul and idiv instructions\ncontents of eax. Note that when we multiply two 32-bit numbers, we require at most 64 bits\nto save the result (see Section 7.2.1). Hence, to avoid overflows, the processor saves the results\nin the register pair (edx,eax). edx contains the upper 32 bits, and eax contains the lower 32\nbits of the final product. The 2-address format version is similar to other ALU instructions\nthat we have studied. It multiplies the first and second source operands, and saves the result in\nthe destination register (which is the first operand). Note that in this variant of the multiply\ninstruction, the destination is always a register, and the result is truncated to fit in the register.\nThe imul instruction has another variant that requires 3 operands. Here, it multiplies the\ncontents of the second and third operands and stores the product in the register specified by\nthe first operand. For this variant of the imul instruction, the first operand needs to be a\nregister, the second operand can be a register or memory location, and the third operand needs\nto be an immediate value.\nThe idiv instruction takes just 1 operand (register or memory). It divides the contents of\nthe register pair (edx:eax) by the contents of the operand. It saves the quotient in eax, and the\nremainder in edx. Note that the remainder has the same sign as the dividend. A subtle point\nshould be noted here. While using a positive dividend that fits in 32 bits, we need to explicitly\nset edx to 0, and for a negative dividend that fits in 32 bits, we need to explicitly set edx to -1\n(for sign extension).\nLet us consider a set of examples.\nExample 65\nWrite an assembly code snippet to multiply 3 with -17, and save the result in eax.\nAnswer:\nmov ebx, 3\nimul eax, ebx, -17 193 (cid:13)c Smruti R. Sarangi\nExample 66\nWrite an assembly code snippet to compute k3, where k is the content of ecx, and save the\nresult in eax.\nAnswer:\nmov eax, ecx\nimul ecx\nimul ecx\nExample 67\nWrite an assembly code snippet to divide -50 by 3. Save the quotient in eax, and remainder\nin edx.\nAnswer:\nmov edx, -1\nmov eax, -50\nmov ebx, 3\nidiv ebx\nAt the end eax contains -16, and edx contains -2.\nLogical Instructions\nSemantics Example Explanation\nand (reg\/mem), (reg\/mem\/imm) and eax, ebx eax \u2190 eax AND ebx\nor (reg\/mem), (reg\/mem\/imm) or eax, ebx eax \u2190 eax OR ebx\nxor (reg\/mem), (reg\/mem\/imm) xor eax, ebx eax \u2190 eax XOR ebx\nnot (reg\/mem) not eax eax \u2190 \u223c eax\nTable 5.12: Semantics of and, or, xor, and not instructions\nTable 5.12 shows the semantics of four commonly used logical operations. and, or, and xor\ninstructions have exactly the same format as add and sub instructions, and most of the other\n2-address format instructions. They compute the bitwise AND, OR, and exclusive OR of the\nfirst two operands respectively. The not instruction computes the 1\u2019s complement (flips each\nbit) of the source operand, which is also the destination operand (format is similar to other\n1-address format instructions such as inc, dec, and neg). (cid:13)c Smruti R. Sarangi 194\nShift Instructions\nSemantics Example Explanation\nsar (reg\/mem), imm sar eax, 3 eax \u2190 eax \u226b 3\nshr (reg\/mem), imm shr eax, 3 eax \u2190 eax (cid:29) 3\nsal\/shl (reg\/mem), imm sal eax, 2 eax \u2190 eax (cid:28) 2\nTable 5.13: Semantics of shift instructions\nTable 5.13 shows the semantics of shift instructions. sar (shift arithmetic right) performs\nan arithmetic right shift by replicating the sign bit. shr (shift logical right), shifts the first\noperand to the right. Instead of replicating the sign bit, it fills the MSB bits with 0s. sal\nand shl are the same instruction. They perform a left shift. Recall that we do not have an\narithmetic left shift operation. Let us consider some examples.\nExample 68\nWhat is the final value of eax?\nmov eax, 0xdeadbeef\nsal eax, 4\nAnswer: 0xeadbeef0\nExample 69 What is the final value of eax?\nmov eax, 0xdeadbeef\nsar eax, 4\nAnswer: 0xfdeadbee\nExample 70 What is the final value of eax?\nmov eax, 0xdeadbeef\nshr eax, 4\nAnswer: 0xdeadbee 195 (cid:13)c Smruti R. Sarangi\n5.3.3 Branch\/ Function Call Instructions\nConditional and Unconditional Branch Instructions\nSemantics Example Explanation\njmp (cid:104)label(cid:105) jmp .foo jump to .foo\nj (cid:104)condcode(cid:105) j (cid:104)condcode(cid:105) .foo jump to .foo if the (cid:104)condcode(cid:105) con-\ndition is satisfied\nTable 5.14: Semantics of branch instructions\nCondition code Meaning\no Overflow\nno No overflow\nb Below (unsigned less than)\nnb Not below (unsigned greater than or equal to)\ne\/z Equal or zero\nne\/nz Not equal or not zero\nbe Below or equal (unsigned less than or equal)\ns Sign bit is 1 (negative)\nns Sign bit is 0 (0 or positive)\nl Less than (signed less than)\nle Less than or equal (signed)\ng Greater than (signed)\nge Greater than or equal (signed)\nTable 5.15: Condition codes in x86\nTable 5.14 shows the semantics of branch instructions. jmp is an unconditional branch\ninstruction that branches to a label. The assembler internally replaces the label by the PC of\nthe label. x86 defines a series of branch instructions with the j prefix. These are conditional\nbranchinstructions. Thej prefixisfollowedbythebranchcondition. Theconditionsareshown\nin Table 5.15. For example, the instruction je means jump if equal. If the last comparison has\nresulted in an equality, then the processor branches to the label; otherwise, it executes the\nnext instruction. If the condition is not satisfied, the conditional branch is equivalent to a nop\ninstruction.\nNow that we have introduced branch instructions, we can implement complex algorithms\nusingloops. Letuslookatacoupleofexamples. Wewouldliketoadvisethereaderatthispoint\nthat the best method to learn assembly language is by actually writing assembly programs. No\namount of theoretical reading can substitute for actual practice. (cid:13)c Smruti R. Sarangi 196\nExample 71\nWrite a program in x86 assembly to add the numbers from 1 to 10.\nAnswer:\nx86 assembly code\nmov eax, 0 ; sum = 0\n1\nmov ebx, 1 ; idx = 1\n2\n.loop:\n3\nadd eax, ebx ; sum += idx\n4\ninc ebx ; idx ++\n5\ncmp ebx, 10 ; compare idx and 10\n6\njle .loop ; jump if idx <= 10\n7\nHere, we store the running sum in eax and the index in ebx. In Line 4, we add the\nindex to the sum. We subsequently, increment the index, and compare it with 10 in Line 6.\nIf it is less than or equal to 10, then we continue iterating. eax contains the final value.\nExample 72\nWrite a program in x86 assembly to test if a number stored in eax is prime. Save the result\nin eax. If the number is prime, set eax to 1, otherwise set it to 0. Assume that the number\nin eax is greater than 10.\nAnswer:\nx86 assembly code\nmov ebx, 2 ; starting index\n1\nmov ecx, eax ; ecx contains the original number\n2\n.loop:\n3\nmov edx, 0 ; required for correct division\n4\nidiv ebx\n5\ncmp edx, 0 ; compare the remainder\n6\nje .notprime ; number is composite\n7\ninc ebx\n8\nmov eax, ecx ; set the value of eax again\n9\ncmp ebx, eax ; compare the index and the number\n10\njl .loop\n11\n12\n; end of the loop\n13\nmov eax, 1 ; number is prime\n14\njmp .exit ; exit\n15\n16\n.notprime:\n17\nmov eax, 0\n18\n.exit:\n19 197 (cid:13)c Smruti R. Sarangi\nIn this algorithm, we keep on dividing the input (stored in eax) by a monotonically\nincreasing index. If the remainder is equal to 0 in any iteration, then the number is com-\nposite (non prime). Otherwise, the number is prime. In specific, we perform the division\nin Line 5, and jump to the label .notprime if the remainder (stored in edx) is 0. Otherwise,\nwe increment the index in ebx, and keep iterating. Note that in each iteration, we need to\nset the values of eax and edx because they are overwritten by the idiv instruction.\nExample 73\nWrite a program in x86 assembly to find the factorial of a number stored in eax. Save your\nresult in ecx. You can assume that the number is greater than 10.\nAnswer:\nx86 assembly code\nmov ebx, 2 ; idx = 2\n1\nmov ecx, 1 ; prod = 1\n2\n3\n.loop:\n4\nimul ecx, ebx ; prod *= idx\n5\ninc ebx ; idx++\n6\ncmp ebx, eax ; compare num (number) and idx\n7\njle .loop ; jump to .loop if idx <= num\n8\nIn Line 2, we initialise the product to 1. Subsequently, we multiply the index with the\nproduct in Line 5. We then increment the index, and compare it with the input stored in\neax. We keep on iterating till the index is less than or equal to the input.\nFunction Call and Return Instructions\nSemantics Example Explanation\ncall (cid:104)label(cid:105) call .foo Push the return address on the\nstack. Jump to the label .foo.\nret ret Return to the address saved on the\ntop of the stack, and pop the entry\nTable 5.16: Semantics of the function call and return instructions\nUnlike ARM and SimpleRisc , x86 does not have a return address register. The call in-\nstruction pushes the return address on the stack, and jumps to the beginning of the function\nas explained in Table 5.16. Similarly, the ret instruction jumps to the entry at the top of the (cid:13)c Smruti R. Sarangi 198\nstack. The entry at the top of the stack needs to contain the return address. The ret instruc-\ntion subsequently pops the stack and removes the return address. Let us now consider a set of\nexamples.\nExample 74\nWrite a recursive function to compute the factorial of a number (\u2265 1) stored in eax. Save\nthe result in ebx.\nAnswer:\nx86 assembly code\nfactorial:\n1\nmov ebx, 1 ; default return value\n2\ncmp eax, 1 ; compare num (input) with 1\n3\nje .return ; return if input is equal to 1\n4\n5\n; recursive step\n6\npush eax ; save input on the stack\n7\ndec eax ; num--\n8\ncall factorial ; recursive call\n9\npop eax ; retrieve input\n10\nimul ebx, eax ; prod = prod * num\n11\n12\n.return:\n13\nret ; return\n14\nIn the factorial function, we assume that the input (num) is stored in eax. We first\ncompare the input with 1. If it is equal to 1, then we return 1 (Lines 2 to 4). However, if the\ninput is greater than 1, then we save the input by pushing it to the stack (7). Subsequently,\nwe decrement it and recursively call the factorial function in Line 9. The result of the\nrecursive call is stored in ebx. To compute the result (in ebx), we multiply ebx with num\n(stored in eax) in Line 11.\nIn Example 74 we pass arguments through registers. We use the stack to only store values\nthat are overwritten by the callee function. Let us now use the stack to pass arguments to the\nfactorial function (see Example 75)\nExample 75\nWrite a recursive function to compute the factorial of a number (\u2265 1) stored in eax. Save\nthe result in ebx. Use the stack to pass arguments.\nAnswer:\nx86 assembly code\n1\nfactorial:\n2 199 (cid:13)c Smruti R. Sarangi\nmov eax, [esp+4] ; get the value of eax from the stack\n3\nmov ebx, 1 ; default return value\n4\ncmp eax, 1 ; compare num (input) with 1\n5\nje .return ; return if input is equal to 1\n6\n7\n; recursive step\n8\npush eax ; save eax on the stack\n9\ndec eax ; num--\n10\npush eax ; push the argument\n11\ncall factorial ; recursive call\n12\npop eax ; pop the argument\n13\npop eax ; retrieve the value of eax\n14\nimul ebx, eax ; prod = prod * num\n15\n16\n.return:\n17\nret ; return\n18\nHere, we use the stack to pass arguments. Since the stack pointer gets automatically\ndecremented by 4 after a function call, the argument eax is available at [esp+4] because we\npush it on the stack just before we call the function. To call the factorial function again,\nwe push eax on the stack, and then pop it out after the function returns.\nLet us now assume that we have a lot of arguments. In this case, we need to push and\npop a lot of arguments from the stack. It is possible that we might lose track of the order of\npush and pop operations, and bugs might be introduced in our program. Hence, if we have a\nlot of arguments, it is a better idea to create space in the stack by subtracting the estimated\nsize of the activation block from the stack pointer and moving data between the registers and\nstack using regular mov instructions. Let us now modify our factorial example to use mov\ninstructions instead of push and pop instructions (see Example 76).\nExample 76\nWrite a recursive function to compute the factorial of a number (\u2265 1) stored in eax. Save\nthe result in ebx. Use the stack to pass arguments. Avoid push and pop instructions.\nAnswer:\nx86 assembly code\nfactorial:\n1\nmov eax, [esp+4] ; get the value of eax from the stack\n2\nmov ebx, 1 ; default return value\n3\ncmp eax, 1 ; compare num (input) with 1\n4\njz .return ; return if input is equal to 1\n5\n6 (cid:13)c Smruti R. Sarangi 200\n; recursive step\n7\nsub esp, 8 ; create space on the stack\n8\nmov [esp+4], eax ; save the input eax on the stack\n9\ndec eax ; num--\n10\nmov [esp], eax ; push the argument\n11\ncall factorial ; recursive call\n12\nmov eax, [esp+4] ; retrieve eax\n13\nimul ebx, eax ; prod = prod * num\n14\nadd esp, 8 ; restore the stack pointer\n15\n16\n.return:\n17\nret ; return\n18\nIn this example, we have avoided push and pop instructions altogether. We instead\ncreate space on the stack by subtracting 8 bytes from esp in Line 8. We use 4 bytes to save\nthe input (in eax) for later use. We use the rest of the 4 bytes to send the argument to the\nrecursive function call. After the function returns, we retrieve the value of eax from the\nstack in Line 13. Lastly, we restore the stack pointer in Line 15.\nHowever, this method is also not suitable for large functions in complex programming lan-\nguages such as C++. In a lot of C++ functions, we dynamically allocate space on the stack. In\nsuchcases, mostofthetime, wedonotknowthesizeoftheactivationblock(seeSection3.3.10)\nof a function in advance. Hence, deallocating an activation block becomes difficult. We need\nto dynamically keep track of the size of the activation block of the function. This introduces\nadditional complexity, and additional code. It is a better idea to save the value of esp in a\ndedicated register at the beginning of a function. At the end of the function, we can transfer\nthe saved value in the register to esp. This strategy effectively destroys the activation block.\nMostofthetime, weusetheebp(basepointer)registertosavethevalueofespatthebeginning\nof a function. This register is also referred to as the frame pointer. Now, it is possible that a\ncalled function might follow the same strategy, and overwrite the value of ebp set by the caller.\nThus, in this case, ebp needs to be a callee saved register. If an invoked function overwrites\nthe value of ebp, it needs to ensure that by the time it returns to the caller, the value of ebp\nis restored. By using the base pointer, we do not need to explicitly remember the size of the\nactivation block. We dynamically allocate data structures on the stack.\nLet us augment our running example with this feature (see Example 77).\nExample 77\nWrite a recursive function to compute the factorial of a number (\u2265 1) stored in eax. Save\nthe result in ebx. Use the stack to pass arguments. Avoid push and pop instructions.\nSecondly, use the ebp register to store the value of the stack pointer.\nAnswer: 201 (cid:13)c Smruti R. Sarangi\nx86 assembly code\nfactorial:\n1\nmov eax, [esp+4] ; get the value of eax from the stack\n2\n3\npush ebp ; save ebp\n4\nmov ebp, esp ; save the stack pointer\n5\n6\nmov ebx, 1 ; default return value\n7\ncmp eax, 1 ; compare num (input) with 1\n8\nje .return ; return if input is equal to 1\n9\n10\n; recursive step\n11\nsub esp, 8 ; create space on the stack\n12\nmov [esp+4], eax ; save input on the stack\n13\ndec eax ; num--\n14\nmov [esp], eax ; push the argument\n15\ncall factorial ; recursive call\n16\nmov eax, [esp+4] ; retrieve input\n17\nimul ebx, eax ; prod = prod * num\n18\n19\n.return:\n20\nmov esp, ebp ; restore the stack pointer\n21\npop ebp ; restore ebp\n22\nret ; return\n23\nHere, we save the old value of ebp on the stack, and set its new value to the stack pointer\nin Lines 4 and 5, respectively. At the end of the function, we restore the values of esp and\nebp in Lines 21 and 22.\nStack Management Instructions \u2013 enter and leave\nSemantics Example Explanation\nenter imm, 0 enter 32, 0 push ebp (push the value of ebp on\nthe stack); mov ebp, esp (save the\nstack pointer in ebp); esp \u2190 esp -\n32\nleave leave mov esp, ebp (restore the value of\nesp); pop ebp (restore the value of\nebp)\nTable 5.17: Semantics of the enter and leave instructions. (cid:13)c Smruti R. Sarangi 202\nThe four extra lines added in Example 77 are fairly generic, and are typically a part of most\nlarge functions. Programmers can add them if they are writing assembly code, or compilers can\nadd them during automatic code generation. In either case, using the base pointer is a very\nconvenient mechanism to manage the stack, and to destroy the activation block. Since these\nset of instructions are so commonly used, the designers of the x86 ISA decided to dedicate two\nspecialised instructions for this purpose. The enter instruction pushes the value of ebp on the\nstack, and sets its new value to be equal to the stack pointer. Additionally, it is also possible\nto set the initial size of the activation block. The first argument takes the size of the activation\nblock. If we specify 32 as the first argument, then the enter instruction decrements esp by 32.\nNote that during the course of execution of the function, the size of the activation block might\ncontinue to vary. The second argument for the enter instruction corresponds to the nesting\nlevel of the function. We shall refrain from discussing it here. Interested readers can refer to\nthe references mentioned at the end of the chapter. We shall simply use the value of 0 for the\nsecond argument.\nThe leave instruction performs the reverse set of computations. It first restores the value\nof esp, and then the value of ebp (see Table 5.17). Note that the leave instruction is meant to\nbe invoked just before the ret instruction. We can thus augment Example 77 to use the enter\nand leave instructions (see Example 78). Secondly, we can omit the statement that subtracted\n8 from esp (Line 12) in Example 77 because this functionality is now built in to the enter\ninstruction.\nExample 78\nWrite a recursive function to compute the factorial of a number (\u2265 1) stored in eax. Save\nthe result in ebx. Use the stack to pass arguments. Avoid push and pop instructions. Use\nthe enter and leave instructions to buffer the values of ebp and esp.\nAnswer:\nx86 assembly code\n1\nfactorial:\n2\nmov eax, [esp+4] ; get the value of eax from the stack\n3\n4\nenter 8, 0 ; save ebp and esp, decrement esp by 8\n5\n6\nmov ebx, 1 ; default return value\n7\ncmp eax, 1 ; compare num (input) with 1\n8\nje .return ; return if the input is equal to 1\n9\n10\n; recursive step\n11\nmov [esp+4], eax ; save input on the stack\n12\ndec eax ; num--\n13\nmov [esp], eax ; push the argument\n14\ncall factorial ; recursive call\n15\nmov eax, [esp+4] ; retrieve input\n16\nimul ebx, eax ; prod = prod * num\n17 203 (cid:13)c Smruti R. Sarangi\n18\n.return:\n19\nleave ; load esp and ebp\n20\nret ; return\n21\nLastly, we should mention that x86 processors have a nop instruction that does not do any-\nthing at all. It is mainly used for the purpose of ensuring correctness in modern processors (see\nChapter 9), and for ensuring that blocks of code are aligned to 16 byte or 64 byte boundaries.\nWe require the latter functionality for better behaviour at the level of the memory system.\n5.3.4 Advanced Memory Instructions\nString Instructions\nSemantics Example Explanation\nlea reg, mem lea ebx, [esi + edi*2 + 10] ebx \u2190 esi + edi*2 + 10\nstos(b\/w\/d\/q) stosd [edi]\u2190eax; edi\u2190edi+4*(\u22121)DF\nlods(b\/w\/d\/q) lodsd eax \u2190 [esi]; esi \u2190 esi + 4 * (\u22121)DF\nmovs(b\/w\/d\/q) movsd [edi] \u2190 [esi] ; esi \u2190 esi + 4 *\n(\u22121)DF; edi \u2190 edi + 4 * (\u22121)DF\nstd std DF \u2190 1\ncld cld DF \u2190 0\nDF \u2192 Direction Flag\nTable 5.18: Semantics of advanced memory instructions\nTheleainstructionstandsforloadeffectiveaddress. Ithasaregisteroperand,andamemory\noperand. The role of the lea instruction is to copy the address of the memory operand (not its\ncontents) to the register.\nLet us now introduce a special set of instructions known as string instructions. We shall\nintroduce the following instructions: stos, lods, and movs. The stos instruction transfers data\nfrom the eax register to the location specified by the edi register. It comes in four flavours\ndepending upon the amount of data that we wish to transfer. It uses the \u2019b\u2019 suffix for 1 byte,\n\u2019w\u2019 for 2 bytes, \u2019d\u2019 for 4 bytes, and \u2019q\u2019 for 8 bytes. We show an example of the stosd instruction\nin Table 5.18. The stosd instruction transfers the contents of eax (4 bytes) to the memory\naddress specified by edi. Subsequently, this instruction increments or decrements the contents\nof edi by 4 depending on the direction flag. The direction flag (DF) is a field in the flags\nregister similar to zero, carry, and overflow. If the direction flag is set (equal to 1), then the\nstos instruction subtracts the size of the operand from the contents of edi. Conversely, if DF\nis equal to 0, then the stos instruction adds the size of the operand to edi.\nWe introduce two 0-address format instructions namely std and cld to set and reset the\ndirection flag respectively. (cid:13)c Smruti R. Sarangi 204\nThe lods and movs set of instructions are defined in a similar manner. For example, the\nlodsd instruction transfers the contents of the memory location specified by esi to eax. It\nsubsequently increments or decrements the contents of esi by the size of the operands based\non the value of DF. The movs instruction combines the functionality of lods and stos. It first\nfetches a set of bytes from the memory address stored in esi. Subsequently, it writes the bytes\nto the memory address specified by edi. It increments or decrements esi and edi based on the\nvalue of the direction flag.\nTrivia 2\nThe si register (16-bit version of esi) stands for the source index register. Similarly, the di\nregister stands for the destination index register.\nLet us now look at a set of examples.\nExample 79 What is the value of ebx?\nlea edi, [esp+4]\nmov eax, 21\nstosd ; saves eax in [edi]\nmov ebx, [esp+4]\nAnswer: We save 21 (eax) in the memory address specified in edi by using the stosd\ninstruction. This memory address is equal to (esp + 4). After executing the stosd instruc-\ntion, we load the contents of this memory address into ebx. The result is equal to the value\nof eax seen by the stosd instruction, which is 21.\nExample 80 What is the value of eax after executing this code snippet?\nlea esi, [esp+4]\nmov dword [esp+4], 19\nlodsd ; eax <-- [esi]\nAnswer: Note the use of the modifier dword here. We need to use it because we are\nsaving an immediate to a memory location, and we need to specify its size. The value of\neax is equal to the value stored in [esp+4], which is 19. 205 (cid:13)c Smruti R. Sarangi\nExample 81 What is the value of eax after executing this code snippet?\nmov dword [esp+4], 192\nlea esi, [esp+4]\nlea edi, [esp+8]\nmovsd\nmov eax, [esp+8]\nAnswer: The movsd instruction transfer 4 bytes from the memory address specified in esi\nto the memory address specified in edi. Since we write 192 to the memory address specified\nin esi, we shall read back the same value in the last line.\nInstructions with the rep Prefix\nThe string instructions can additionally increment or decrement the values of esi and edi. We\nhave not used this feature up till now. Let us use this feature to transfer an array of 10 integers\nfrom one location to another. This feature is very frequently used in modern processors to\ntransfer large amounts of data between two locations.\nLet us first show a conventional solution in Example 82.\nExample 82 Write a program to create a copy of a 10 element integer array. Assume that\nthe starting address of the original array is stored in esi, and the starting address of the\ndestination array is stored in edi.\nAnswer:\nmov ebx, 0 ; initialise\n.loop:\nmov edx, [esi+ebx*4] ; transfer the contents\nmov [edi + ebx*4], edx\ninc ebx ; increment the index\ncmp ebx, 10 ; loop condition\njne .loop\nExample 83 Write a program to create a copy of a 10 element integer array. Assume that\nthe starting address of the original array is stored in esi, and the starting address of the\ndestination array is stored in edi. Use the movsd instruction.\nAnswer: (cid:13)c Smruti R. Sarangi 206\ncld ; DF = 0\nmov ebx, 0 ; initialisation of the loop index\n.loop:\nmovsd ; [edi] <-- [esi]\ninc ebx ; increment the index\ncmp ebx, 10 ; loop condition\njne .loop\nAs compared to Example 82, we reduce the number of instruction in the loop from 5 to\n4.\nIn Example 83, we use the movsd instruction to replace a pair of load\/store instructions\nwith just one instruction. This reduces the number of instructions in the loop from 5 to 4. We\nwere not able to get a bigger reduction because we still need to update the loop index, and\ncompute the loop condition.\nTo make our code look even more elegant, the x86 ISA defines a rep prefix that can used\nwith any string instruction. The rep prefix instructs the processor to execute a single string\ninstruction n times, where n is the value stored in the ecx register. Every time the processor\nexecutes the string instruction, it decrements ecx. At the end, the value of ecx becomes 0. Its\nsemantics is shown in Table 5.19.\nSemantics Example Explanation\nrep inst rep movsd val \u2190 ecx; Execute the movsd in-\nstruction val times; ecx \u2190 0\nTable 5.19: Semantics of rep instructions\nExample 84 Write a program to create a copy of a 10 element integer array. Assume that\nthe starting address of the original array is stored in esi, and the starting address of the\ndestination array is stored in edi. Use the rep prefix with the movsd instruction.\nAnswer:\ncld ; DF = 0\nmov ecx, 10 ; Set the count to 10\nrep movsd ; Execute movsd 10 times\nThe rep prefix thus allows us to fold an entire loop into just one instruction as shown in\nExample84. Therepprefixismeanttobeusedwithstringinstructionsforcopyinglargeregions 207 (cid:13)c Smruti R. Sarangi\nof data. It makes the code for operating on strings of data very compact and elegant. The\nrep instruction has two variants namely repe, and repne. These instructions use an additional\ntermination condition, along with the value of ecx. Instructions prefixed with repe can also\nterminate when the zero flag becomes 0, and an instruction prefixed with repne also terminates\nwhen the zero flag becomes 1.\n5.4 Floating Point Instructions\nx86 has a large set of floating point instructions. Let us first give a historical perspective. The\nearly 8086 processor, and many of its successors till the Intel 486 did not have a floating point\nunit in the processor. They used a separate co-processor chip called the x87 that provided\nfloating point capability. However, after the release of Intel 486, the floating point unit has\nbeen an integral part of the x86 architecture. Hence, many features of the floating point ISA\nare artefacts of the older era, in which a floating point instruction was essentially a message to\nan external processing unit.\nOne of the direct consequences of such a design strategy is that there are no direct commu-\nnicationpathsbetweenintegerregisters, andfloatingpointregisters. Secondly, itisnotpossible\nto load an immediate into a floating point register by specifying its value in an instruction. We\ncan only load the value of floating point registers via memory. For example, if we wish to store\na floating point constant in a floating point register, then we need to first load the constant in\nmemory. Subsequently, we need to issue a floating point load instruction to load the constant\ninto a floating point register. Figure 5.3 shows a conceptual organisation of the x86 ISA. The\ninteger instructions use the integer registers, and they have their own processor state. Likewise,\nthe floating point instructions use their set of registers, and have their own state. Both the\ntypes of instructions, however, share the memory.\nInteger FP\nConstants\nregisters registers\nMemory\nFigure 5.3: Conceptual organisation of the x86 ISA\nLet us start by looking at methods to load values into the floating point registers. We\nshall refer to the floating point register stack as the FP stack and designate the floating point\nregisters (st0 ... st7) as reg while describing the semantics of instructions. We shall also\nabbreviate floating point as FP for the sake of brevity. (cid:13)c Smruti R. Sarangi 208\n5.4.1 Data Transfer Instructions\nLoad Instruction\nSemantics Example Explanation\nfld mem fld dword [eax] Pushes an FP number stored in\n[eax] to the FP stack\nfld reg fld st1 Pushesthecontentsofst1tothetop\nof the stack\nfild mem fild dword [eax] Pushes an integer stored in [eax] to\nthe FP stack after converting it to a\n32-bit floating point number\nTable 5.20: Floating point load instructions\nTable 5.20 shows the semantics of the floating point load instructions. The most commonly\nusedfloatingpointloadinstructionisthefldinstruction. Thefirstvariantofthefldinstruction\ncan load a 32-bit floating point value from memory, and push it to the FP stack. We can use\nourstandardaddressingmodeswithintegerregistersasdescribedinSection5.2.4forspecifying\nan address in memory. The second variant can push the contents of an existing FP register\non the FP stack. We can alternatively use the fild instruction that can read an integer from\nmemory, convert it to floating point, and push it on the FP stack. Let us consider an example.\nExample 85\nPush the constant, 2.392, on the FP stack.\nAnswer: We need to first define the constant 2.392 in the data section. In NASM, we\ndo this as follows.\nsection .data\nnum: dd 2.392\nWe need to embed this code snippet at the beginning of the assembly file. Here, the\ndeclaration\u201csection.data\u201dmeansthatwearedeclaringthedatasection. Inthedatasection,\nwe further declare a variable, num, that is a double word (32 bits, specified by dd), and its\nvalue is 2.392. Let us now push this value to the FP stack. We need to embed the following\ncode snippet in the main assembly function.\nfld dword [num]\nThe assembler treats num as a memory address. While generating code, it will replace\nit with its actual address. However, in an assembly program, we can seamlessly treat num\nas a valid memory address, and its contents can thus be represented as [num]. The fld\ninstruction in this code snippet loads 32 bits (dword) from num to the top of the FP stack. 209 (cid:13)c Smruti R. Sarangi\nExchange Instruction\nSemantics Example Explanation\nfxch reg fxch st3 Exchange the contents of st0 and\nst3\nfxch fxch Exchange the contents of st0 and\nst1\nTable 5.21: Floating point exchange instructions\nTable 5.21 shows the format of the floating point exchange instruction, fxch. It exchanges\nthe contents of two floating point registers. The 1-address format fxch instruction exchanges\nthe contents of the register specified as the first operand and st0. If we do not specify any\noperands, then the processor exchanges st0 and st1 (the top of the stack, and the entry just\nbelow the top of the stack).\nStore Instructions\nSemantics Example Explanation\nfst mem fst dword [eax] [eax] \u2190 st0\nfst reg fst st4 st4 \u2190 st0\nfstp mem fstp dword [eax] [eax] \u2190 st0; pop the FP stack\nfist mem fist dword [eax] [eax] \u2190 int(st0)\nfistp mem fistp dword [eax] [eax] \u2190 int(st0); pop the FP stack\nTable 5.22: Floating point store instructions\nLet us now look at the store instructions in Table 5.22. The format is similar to the\nfloating point load instructions. We have three variants of the basic fst instruction. The first\nvariant requires a single memory operand. It stores the contents of st0 in the memory location\nspecifiedbythememoryoperand. ThesecondvariantrequiresaFPregisteroperandandstores\nthe contents of st0 in the FP register.\nThe third variant uses the \u2018p\u2019 suffix which is a generic suffix and is used by many other\ninstructions also. The fstp instruction initially saves the value contained in st0 in the memory\nlocation specified by the first operand, and then pops the stack. Since the stack size is limited,\nit is often necessary to pop the stack to create more space. When we are storing st0, we are\nsaving a copy of its contents in main memory. Hence, it makes sense to have a variant of the\nfst instruction that can free the entry from the stack by popping it.\nx86 has additional support for conversion of floating point values to integers. We can use\nthe fist instruction that first converts the contents of st0 to a signed integer by rounding it\nand then saves it in the location specified by the memory operand. Note that we always use a\nmodifier (byte\/word\/dword\/qword) for memory operands such that we can specify the number (cid:13)c Smruti R. Sarangi 210\nof bytes that need to be transferred. The fist instruction also supports the \u2018p\u2019 suffix (see the\nsemantics of the fistp instruction in Table 5.22).\nExample 86\nTransfer the contents of st0 to eax by converting the save FP number to an integer.\nAnswer:\nfist dword[esp]\nmov eax, [esp]\n5.4.2 Arithmetic Instructions\nLet us now consider arithmetic instructions. The floating point ISA in x86 has rich support\nfor floating point operations, and is thus extensively used in numerical computing. Let us start\nwith the basic floating point add instruction, and take a look at all of its variants.\nAdd Instruction\nSemantics Example Explanation\nfadd mem fadd dword [eax] st0 \u2190 st0 + [eax]\nfadd reg, reg fadd st0, st1 st0 \u2190 st0 + st1 (one of the registers\nhas to be st0)\nfaddp reg faddp st1 st1 \u2190 st0 + st1; pop the FP stack\nfiadd mem fiadd dword [eax] st0 \u2190 st0 + float([eax])\nTable 5.23: Floating point add instructions\nThe semantics of the floating point add instructions is shown in Table 5.23. The basic fadd\ninstruction has two variants. The first variant uses a single memory operand. Here, we add\nthe value of the floating point number contained in the memory location to the contents of st0.\nThe result is also stored in st0. The second variant of the fadd instruction uses two floating\npoint registers as arguments. It adds the contents of the second register to the first register.\nThe fadd instruction follows the same pattern as the floating point load and store instruc-\ntions. It accepts the \u2018p\u2019 suffix. The faddp instruction typically takes 1 argument, which is a\nregister. We show an example of the instruction faddp st1 in Table 5.23. Here, we add the\ncontents of st0 to st1, and save the result in st1. Then, we pop the stack. For working with\nintegers, we can use the fiadd instruction that takes the address of an integer in memory. It\nadds this integer to st0, and saves the results in st0.\nSubtraction, Multiplication, and Division Instructions\nx86 defines subtraction, multiplication, and division instructions that have exactly the same\nformat as the fadd instructions, and all of its variants as shown in Table 5.23. Let us just show 211 (cid:13)c Smruti R. Sarangi\nthe basic form of each instruction that uses a single memory operand in Table 5.24.\nSemantics Example Explanation\nfsub mem fsub dword [eax] st0 \u2190 st0 - [eax]\nfmul mem fmul dword [eax] st0 \u2190 st0 * [eax]\nfdiv mem fdiv dword [eax] st0 \u2190 st0 \/ [eax]\nTable 5.24: Floating point subtract, multiply, and divide instructions\nExample 87\nCompute the arithmetic mean of two integers stored in eax and ebx. Save the result (in\n64 bits) in esp+4. Assume that the data section contains the integer, 2, in the memory\naddress two.\nAnswer:\n; load the inputs to the FP stack\nmov [esp], eax\nmov [esp+4], ebx\nfild dword [esp]\nfild dword[esp + 4]\n; compute the arithmetic mean\nfadd st0, st1\nfdiv dword [two]\n; save the result (converted to 64 bits) to [esp+4]\n; use the qword identifier\nfstp qword [esp + 4]\n5.4.3 Instructions for Special Functions\nSemantics Example Explanation\nfabs fabs st0 \u2190 |st0|\n\u221a\nfsqrt fsqrt st0 \u2190 st0\nfcos fcos st0 \u2190 cos(st0)\nfsin fsin st0 \u2190 sin(st0)\nTable 5.25: Floating point instructions for special functions (cid:13)c Smruti R. Sarangi 212\nThe greatness of the x86 ISA is that it supports trigonometric functions, and complex\nmathematical operations such as the square root, and log operations (not covered in this book).\nTable 5.25 shows the x86 instructions for computing the values of special functions. The fabs\nfunction computes the absolute value of st0, the fsqrt function computes the square root, the\nfcos and fsin functions compute the sine and cosine of the value stored in st0 respectively. All\nof these instructions use st0 as their default operand, and also write the result back to st0.\nExample 88\nCompute the geometric mean of two integers stored in eax and ebx. Save the result (in 64\nbits) in esp+4.\nAnswer:\n; load the inputs to the FP stack\nmov [esp], eax\nmov [esp+4], ebx\nfild dword [esp]\nfild dword[esp + 4]\n; compute the geometric mean\nfmul st0, st1\nfsqrt\n; save the result (converted to 64 bits) to [esp+4]\n; use the qword identifier\nfstp qword [esp + 4]\n5.4.4 Compare Instruction\nSemantics Example Explanation\nfcomi reg, reg fcomi st0, st1 compare st0 and st1, and set the\neflags register (first register has to\nbe st0)\nfcomip reg, reg fcomi st0, st1 compare st0 and st1, and set the\neflags register; pop the FP stack\nTable 5.26: Floating point compare instructions\nThe x86 ISA has many compare instructions. In this section, we shall present only one\ncompare instruction called fcomi that compares two floating point values saved in registers,\nand sets the eflags register. Table 5.26 shows the semantics of the fcomi instruction with and 213 (cid:13)c Smruti R. Sarangi\nwithoutthe\u2018p\u2019suffix. Once,theeflagsregisterisset,wecanuseregularbranchinstructionsfor\nimplementing control flow within the program. Note that in x86 we need to use the condition\ncodes for unsigned comparison in this case. Most of the time programmers make the mistake\nof using the condition codes for signed comparison such as l, le, g, or ge for testing the results\nof floating point comparison. This leads to wrong results. We should instead use the a (above)\nand b (below) condition codes.\nLet us now consider an example (Example 89) that computes the value of sin(2\u03b8), and veri-\nfiesifitisequalto2sin(\u03b8)cos(\u03b8). Thereadersshouldrecallfromtheirhighschooltrigonometry\nclass that both these expressions are actually equal, and one can be derived from the other.\nExample 89 experimentally verifies this fact for any given value of \u03b8. We compute the value of\nsin(2\u03b8) and 2sin(\u03b8)cos(\u03b8), and compare them using fcomi. Note that floating point arithmetic\nis approximate (see Section 2.4.6). Hence, the correct way to compare floating point numbers is\nto first subtract them, compute the absolute value of the difference, and compare the difference\nwith a threshold. The threshold is typically a small number (10\u22125 in our case). If the difference\nis less than a threshold, we can infer equality.\nExample 89\nCompare sin(2\u03b8) and 2sin(\u03b8)cos(\u03b8). Verify that they have the same value for any given\nvalue of \u03b8. Assume that theta is stored in the data section at the label theta, and the\nthreshold for floating point comparison is stored at label threshold. Save the result in eax\n(1 if equal, and 0 if unequal).\nAnswer:\n; compute sin(2*theta), and save in [esp]\nfld dword [theta]\nfadd st0 ; st0 = theta + theta\nfsin\nfstp dword [esp]\n; compute (2*sin(theta)*cos(theta))\nfld dword [theta]\nfst st1\nfsin\nfxch\nfcos ; st0 = cos(theta)\nfmul st1 ; st0 = sin(theta) * cos (theta)\nfadd st0 ; st0 = 2 * st0\n; compute the modulus of the difference\nfld dword [esp] ; load (sin(2*theta))\nfsub st0, st1\nfabs\n; compare (cid:13)c Smruti R. Sarangi 214\nfld dword [threshold]\nfcomi st0, st1 ; compare\nja .equal\nmov eax, 0\njmp .exit\n.equal:\nmov eax, 1\n.exit:\nAfter the end of a function, it is time to clean up the floating point registers, and stack\nsuch that another function can use them. Let us conclude this section by taking a look at the\ncleanup instructions.\n5.4.5 Stack Cleanup Instructions\nSemantics Example Explanation\nffree reg ffree st4 Free st4\nfinit finit Reset the status of the FP unit in-\ncluding the FP stack and registers\nTable 5.27: Floating point stack cleanup instructions\nTable 5.27 shows two instructions for cleaning up the FP stack. The ffree instruction\nsets the status of the register specified as an operand to empty. Using ffree to free all the\nregistersisaquicksolution. Forfreeingtheentirestackweneedtoinvoketheffreeinstruction\niteratively. For deleting the entire FP stack, a cleaner solution is to use the finit instruction\nthat does not take any arguments. It resets the FP unit, frees all the registers, and resets the\nstack pointer. The finit instruction ensures that an unrelated function can start from a clean\nstate.\n5.5 Encoding the x86 ISA\nWe have taken a look at a wide variety of x86 instructions, addressing modes, and instruction\nformats. It is truly a CISC instruction set. However, the process of encoding is more regular.\nAlmost all the instructions follow a standard format. In the case of ARM and SimpleRisc , we\ndescribed the process of encoding instructions in great detail. We shall refrain from doing this\nhere for the sake of brevity. Secondly, an opcode in x86 typically has a variety of modes, and\nprefixes. We do not want to digress from the main theme of this book by describing x86 in\nsuch level of detail. Let us start out by looking at the broad structure of an encoded machine\ninstruction. 215 (cid:13)c Smruti R. Sarangi\n5.5.1 High Level View of x86 Instruction Encoding\nFigure 5.4 shows the structure of an encoded instruction in binary.\nPrefix Opcode ModR\/M SIB Displacement Immediate\n1-4 bytes 1-3 bytes 1 byte 1 byte 1\/2\/4 bytes 1\/2\/4 bytes\n(optional) (optional) (optional) (optional) (optional)\nFigure 5.4: x86 binary instruction format\nThe first set of 1-4 bytes are used to encode the prefix of the instruction. The rep prefix is\none such example of a prefix. There are many other kinds of prefixes that can be encoded in\nthe first group of 1-4 bytes.\nThe next 1-3 bytes are used for encoding the opcode. Recall that the entire x86 ISA has\nhundreds of instructions. Secondly, the opcode also encodes the format of operands. For\nexample, the add instruction can either have its first operand as a memory operand, or have\nits second operand as a memory operand. This information is also a part of the opcode.\nThe next two bytes are optional. The first byte is known as the ModR\/M byte, which\nspecifies the address of the source and destination registers, and the second byte is known as\nthe SIB (scale-index-base) byte. This byte records the parameters for the base-scaled-index\nand base-scaled-index-offset addressing modes. A memory address might optionally have a\ndisplacement (also referred to as the offset in this book) that can be as large as 32 bits. We\ncan thus optionally have 4 more bytes in an instruction to record the value of the displacement.\nLastly, some x86 instructions accept an immediate as an operand. The immediate can also be\naslargeas32bits. Hence, thelastfield, whichisagainoptional, isusedtospecifyanimmediate\noperand.\nLet us now discuss the ModR\/M and SIB bytes in more detail.\nModR\/M Byte\nThe ModR\/M byte has three fields as shown in Figure 5.5.\n2 3 3\nMod Reg R\/M\nFigure 5.5: The ModR\/M byte\nThe two MSB bits of the ModR\/M byte contain the Mod field. The Mod field indicates\nthe addressing mode of the instruction. It can take 4 values as shown in Table 5.28.\nThe Mod field indicates the addressing mode of one of the operands. It can either be a\nregister or a memory operand. If it is a memory operand, then we have three options. We (cid:13)c Smruti R. Sarangi 216\nMod bits Semantics\n00 We use the register indirect addressing mode for one of the operands.\nWhen R\/M = 100, we use the base-scaled-index addressing mode,Raengdister Code\nthere is no displacement. The ids of the scale, index, and baseeaaxre 000\nspecified in the SIB byte. When R\/M = 101, the memory addresseocnxly 001\nconsists of the displacement. The rest of the values of the R\/Medbxits 010\nspecify the id of the base register as shown in Table 5.29. ebx 011\nOther than the case of R\/M=101, the rest of the combinations oefstphe 100\nR\/M bits are not associated with a displacement (assumed to be 0e)b.p 101\n01 We use a single byte signed displacement. If R\/M = 100, then weesgiet 110\nthe ids of the base and index registers from the SIB byte. edi 111\n10 We use a 4 byte signed displacement. If R\/M = 100, then we get the\nids of the base and index registers from the SIB byte. Table 5.29:\n11 Register direct addressing mode. Register encoding\nTable 5.28: Semantics of the Mod field\ncan either have no displacement (Mod = 00), a 8 bit displacement (Mod = 01), or a 32-bit\ndisplacement (Mod=10). If it is a register operand, then the Mod field has a value of 11.\nThe important point to note is that for all the memory address modes, if the R\/M bits are\nequal to 100, then we need to use the information in the SIB byte for computing the effective\nmemory address.\nThe Reg field encodes the second operand if it is a register. Since both the operands cannot\nbe memory operands, we use the Mod and R\/M bits for encoding one of the operands that\nmight be a memory operand (source or destination), and use the Reg field for encoding the\notheroperand, whichhastobearegister. TheencodingfortheregistersisshowninTable5.29.\nFor floating point instructions, the default register operand is always st0. Some instructions\naccept another FP register operand. For such instructions, we use register direct addressing\n(Mod = 11). We use the R\/M bits for specifying the id of the additional FP register. We use 3\nbits to encode the index of the register. For example, st0 is encoded as 000, and st6 is encoded\nas 110. For the rest of the instructions that either assume default operands, or have a single\nmemory operand, we use the same format as defined for integer instructions.\nSIB Byte\nThe SIB byte is used to specify the base and index registers (possibly with scaling). For\nexample, it can be used to encode memory operands of the form [eax + ecx*4]. Recall that\nto use the SIB byte it is essential to set the Mod field in the ModR\/M register to 100. This\nindicates to the processor that the SIB byte follows the ModR\/M byte.\nThe structure of the SIB byte is shown in Figure 5.6.\nTheSIBbytehasthreefields\u2013scale,index,andbase. Theeffectivememoryaddress(before\nconsidering the displacement) is equal to base+index\u00d7scale. The base and index fields point\nto integer registers. Both of them are 3 bits each (can encode up to 8 registers), and use the\nencoding shown in Table 5.29. The two MSB bits are used to specify the scale. We can have 217 (cid:13)c Smruti R. Sarangi\n2 3 3\nScale Index Base\nFigure 5.6: The SIB byte\nfour values for the scale in x86 instructions namely 1 (00), 2 (01), 4 (10), and 8 (11).\nRules for Encoding Memory Operands\nNote that some rules need to be followed while encoding memory operands. The esp register\ncannot be an index, and if the value of the Mod field is 00, then ebp cannot be a valid base\nregister. Recall that if we set the R\/M bits to 101 (id of ebp), when the Mod field is 00, then\nthe memory address is only a displacement. Or, in other words we can use memory direct\naddressing here by directly specifying its address.\nIf (Mod = 00), then in the SIB byte ebp cannot be a valid base register. If we specify the\nbase register as ebp in the SIB byte, then the processor calculates the effective memory address\nbased on the value of the scale and the index.\nExample 90\nEncode the instruction add ebx, [edx + ecx*2 + 32]. Assume that the opcode for the add\ninstruction is 0x03.\nAnswer: Let us calculate the value of the ModR\/M byte. In this case, our displacement\nfits within 8 bits. Hence, we can set the Mod bits equal to 01 (corresponding to an 8 bit\ndisplacement). We need to use the SIB byte, because we have a scale, and an index. Thus,\nwe set the R\/M bits to 100. The destination register is ebx. Its code is 011 (according to\nTable 5.29). Thus, the ModR\/M byte is 01011100 (equal to 0x5C).\nNow, let us calculate the value of the SIB byte. The scale is equal to 2. This is encoded\nas 01. The index is ecx (001), and the base is edx (010). Hence, the SIB byte is: 01 001\n010 = 4A. The last byte is the displacement, which is equal to 0x20.\nThus, the encoding of the instruction is 03 5C 4A 20 in hex.\n5.6 Summary and Further Reading\n5.6.1 Summary\nSummary 5 (cid:13)c Smruti R. Sarangi 218\n1. The x86 ISA is a family of CISC instruction sets that is primarily used by Intel and\nAMD processors.\n(a) The original x86 ISA used by 8086 processors used a 16-bit ISA.\n(b) Since the mid eighties, x86 processors have moved to the 32-bit ISA.\n(c) Finally, since 2003, most of the high end x86 architectures have moved to the\n64-bit ISA.\n(d) The basic structures of all the ISAs is the same. There are minor differences in\nthe syntax.\n2. The 8 basic registers of the 16-bit x86 ISA are \u2013 ax, bx, cx, dx, sp, bp, si, and di.\nWe use the \u2018e\u2019 prefix in 32-bit mode, and the \u2018r\u2019 prefix in 64-bit mode.\n3. Additionally, the 16-bit x86 ISA has the ip register to save the program counter, and\nthe flags register to save the results of the last comparison, and other fields that\ninstructions may use.\n4. The x86 ISA predominantly uses instructions in the 2-address format. The first\noperandistypicallyboththesource, andthedestination. Secondly, oneoftheoperands\ncan be a memory operand. It is thus possible to fetch the value of a memory location,\noperate on it, and write it back to memory, in the same instruction.\n5. x86 processes see a segmented memory model. The entire memory space is partitioned\ninto different segments. Instructions reside in the code segment by default, and data\nresides in the data or stack segments by default. It is in general not possible for\ninstructionstoaccesssegmentsthattheytypicallyarenotmeantfor. Forexample, itis\nin general not possible for a store instruction to change the contents of an instruction\nin the code segment.\n(a) In the 16-bit mode, the top 16 bits of the starting address of each segment are\nstored in a segment register.\n(b) The effective memory address specified by a memory instruction is added to the\naddress contained in the segment register (after left shifting it by 4 positions) to\ncompute the actual memory address.\n(c) InlaterISAs(32and64-bitmode), thecontentsofsegmentregistersarelookedup\nin segment descriptor tables (referred to as the LDT and GDT) for obtaining the\nstarting address of segments. To speed up memory accesses, processors typically\nuse a small memory structure known as a segment descriptor cache that keeps\nthe most recently used entries.\n6. x86 integer instructions:\n(a) The mov instruction is one of the most versatile instructions. It can move values\nbetween two registers, or between registers and memory addresses. It can also be\nused to load immediates in registers or memory locations. 219 (cid:13)c Smruti R. Sarangi\n(b) x86 defines a host of other arithmetic, and branch instructions.\n(c) String instructions are a unique feature of the x86 ISA. They can be used to\ntransfer large amounts of data between memory locations. To compress an entire\nloop of string instructions into one instruction, we typically use the rep prefix\nthat repeats a given instruction n times, where n is the value stored in the ecx\nregister.\n7. The x86 floating point registers can either be accessed as normal registers (st0 ... st7),\nor as values on a floating point stack. Most of the floating point instructions operate\non st0, which is the top of the stack.\n8. There is no direct way to load immediates into the FP registers. We need to first\nload them into memory, and then load them to the floating point stack. x86 has\ninstructions for computing complex mathematical operations (such as square root),\nand trigonometric functions directly.\n9. Encoding the x86 instruction set is relatively simpler, since the encoded form has a\nvery regular structure.\n(a) We can optionally use 1-4 bytes to encode the prefix.\n(b) The opcode\u2019s encoding requires 1-3 bytes.\n(c) We can optionally use two additional bytes known as the ModR\/M and SIB bytes\nto encode the address of operands (both register and memory).\n(d) If the memory operand uses a displacement (offset), then we can add 1-4 bytes\nfor encoding the displacement after the SIB byte.\n(e) Lastly, the x86 ISA accepts 32-bit immediate values. Hence, we can use the last\n1-4 bytes to specify the value of an immediate operand if required.\n5.6.2 Further Reading\nThe most accurate source of information is the x86 developer manuals released by Intel on their\nwebsite [int, , INTEL, 2010].\nFor the sake of brevity, we have only discussed the popularly used instructions. However,\nthere are many instructions in the x86 ISA that might prove to be useful in a specific set\nof scenarios, which we have not covered in this book. Intel\u2019s software developer manuals are\nalways the best places to find this information. Secondly, we have only discussed the basic x86\nISA. The reader should definitely look at the extensions to the x86 ISA such as the MMXTM,\nSSE, and 3d Now! (by AMD) extensions. These extensions add vector instructions, which\ncan operate on arrays of data. These instructions are used in graphics, games, and scientific\napplications. The Intel AVX instruction set is the latest addition in the long line of x86 ISAs.\nIt introduces 512 bit registers that can contain multiple integers. The interested reader should\ndefinitely take a look at this instruction set and try to write programs with it. In this book,\nwe shall show an example using the SSE instruction set in Section 11.5.3. (cid:13)c Smruti R. Sarangi 220\nThe reader can additionally refer to books that describe the x86 instruction set in great\ndetail, and have a wealth of solved examples. The following books [Cavanagh, 2013, Das,\n2010, Kumar, 2003] are useful references in this regard.\nExercises\nx86 Machine Model\nEx. 1 \u2014 What are the advantages of the segmented addressing mode? Why do modern x86\nprocessors need the LDT and GDT tables?\nEx. 2 \u2014 Explain the memory addressing modes in x86.\nEx. 3 \u2014 Describe the floating point registers and the floating point stack in x86.\n* Ex. 4 \u2014 We can specify an entire 32-bit immediate in a single instruction in x86. Recall\nthatthiswasnotpossibleinARMandSimpleRisc. Whataretheadvantagesanddisadvantages\nof having this feature in the ISA?\n* Ex. 5 \u2014 We claim that using a stack based architecture makes the software very portable.\nIt does not need to be aware of the number and semantics of registers in an ISA. Comment on\nthis statement, and try to find other reasons for preferring a stack based machine.\n** Ex. 6 \u2014 Given an arithmetic expression containing floating point operands, how can we\nevaluate it using a floating point stack? What should be the order of loading and operating\non operands? [HINT: A regular arithmetic operation such as \u2013 (1 + 2.5) * 3.9 \u2013 is called an\ninfix expression. To evaluate expressions using a stack, we need to convert it into a postfix\nexpression of the form \u2013 1 2.5 + 3.9 *. Here, we first push 1 and 2.5 on the stack, add the\nresult, push 3.9 on the stack, and multiply the first two entries. The reader should read more\nabout postfix expressions in textbooks on discrete mathematics.]\nAssembly Programming using Integer Instructions\nEx. 7 \u2014 Write x86 assembly code snippets to compute the following:\ni) a+b+c\nii) a+b\u2212c\/d\niii) (a+b)\u22173\u2212c\/d\niv) a\/b\u2212(c\u2217d)\/3\nv) (a (cid:28) 2)\u2212(b (cid:29) 3)\nEx. 8 \u2014 Write an assembly program to convert an integer stored in memory from the little\nendian to the big endian format. 221 (cid:13)c Smruti R. Sarangi\nEx. 9 \u2014 Compute the factorial of a positive number using an iterative algorithm.\nEx. 10 \u2014 Compute the factorial of a positive number using a recursive algorithm.\nEx. 11 \u2014 Write an assembly program to find if a number is prime.\nEx. 12 \u2014 Write an assembly program to test if a number is a perfect square.\nEx. 13 \u2014 Write an assembly program to test if a number is a perfect cube.\nEx. 14 \u2014 Given a 32-bit integer, count the number of 1 to 0 transitions in it.\nEx. 15 \u2014 Write an assembly program that checks if a 32-bit number is a palindrome. A\npalindrome is a number which is the same when read from both sides. For example, 1001 is a\n4-bit palindrome.\nEx. 16 \u2014 Write an assembly program to examine a 32-bit value stored in eax and count the\nnumber of contiguous sequences of 1s. For example, the value:\n01110001000111101100011100011111\ncontains six sequences of 1s. Write the final value in register ebx.\nEx. 17 \u2014 Write an assembly program to count the number of 1\u2019s in a 32-bit number.\n* Ex. 18 \u2014 Write an assembly program to find the smallest number that is a sum of two\ndifferent pairs of cubes. [Note: 1729 is known as the Hardy-Ramanujan number. 1729 =\n123+13 = 103+93].\n** Ex. 19 \u2014 In some cases, we can rotate an integer to the right by n positions (less than or\nequal to 31) so that we obtain the same number. For example: a 8-bit number 01010101 can\nbe right rotated by 2, 4, or 6 places to obtain the same number. Write an assembly program to\nefficiently count the number of ways we can rotate a number to the right such that the result\nis equal to the original number.\n*** Ex. 20 \u2014 Write an assembly language program to find the greatest common divisor of\ntwo binary numbers u and v. Assume the two inputs (positive integers) to be available in eax\nandebx. Storetheresultinecx. [HINT:Thegcdoftwoevennumbersuandvis2\u2217gcd(u\/2,v\/2)]\nEx. 21 \u2014 Write an assembly program that uses string instructions to set the value of a range\nof memory addresses to 0. Reduce the code size by using the rep prefix.\nAssembly Programming using Floating Point Instructions\nEx. 22 \u2014 How do you load and store floating point numbers?\nEx. 23 \u2014 Write an assembly program to find the roots of the equation x2\u2212x\u22121 = 0. Recall\n\u221a\nthat the roots of a quadratic equation of the form ax2+bx+c are equal to \u2212b\u00b1 b2\u22124ac.\n2a (cid:13)c Smruti R. Sarangi 222\nEx. 24 \u2014 Verify the following trigonometric identities for random values of \u03b8 using assembly\nprograms. Use the rdrand instruction that loads a random 32-bit integer into a register.\nS. No. Identity\n1 sin2(\u03b8)+cos2(\u03b8) = 1\n2\nsin(cid:0)\u03c0 \u2212\u03b8(cid:1)\n= cos(\u03b8)\n2\n3 cos(\u03b8+\u03c6) = cos(\u03b8)cos(\u03c6)\u2212sin(\u03b8)sin(\u03c6)\n(cid:16) (cid:17) (cid:16) (cid:17)\n4 sin(\u03b8)+sin(\u03c6) = 2sin \u03b8+\u03c6 cos \u03b8\u2212\u03c6\n2 2\nEx. 25 \u2014 Assume that we have two arrays of 10 floating point numbers, where the starting\naddresses of the arrays are stored in eax and ebx respectively. Find the arithmetic mean\n(AM), geometric mean (GM), and harmonic mean (HM) using assembly routines. Verify that\nAM \u2265 GM \u2265 HM.\n* Ex. 26 \u2014 Let us compute the value of the constant e using an assembly program. Use the\nfollowing mathematical expression.\n1 1 1 1 1\ne = 1+ + + + +...+\n1! 2! 3! 4! 10!\n** Ex. 27 \u2014 For random values of \u03b8 show that the following identity holds:\n\u03b83 \u03b85\nsin(\u03b8) = \u03b8\u2212 + \u2212...\n3! 5!\nx86 ISA Encoding\nEx. 28 \u2014 What are the values of the SIB and ModR\/M bytes for the instruction, mov eax,\n[eax + ebx*4]?\nEx. 29 \u2014 What are the values of the SIB, ModR\/M, and displacement bytes for the instruc-\ntion, mov eax, [eax + ebx*4 + 32]?\nEx. 30 \u2014 WhatisthevalueoftheModR\/M bytewhenweneedtospecifyamemoryaddress\nthat does not have any base or index registers? Assume that the value of the reg field is 000.\n* Ex. 31 \u2014 Assume that we have an instruction that has two operands: eax and [ebp]. How\ndo we encode it (specify the values of the relevant bytes)?\n* Ex. 32 \u2014 What are the values of the SIB and ModR\/M bytes for the instruction, mov eax,\n[ebx*4]?\nDesign Problems\nEx. 33 \u2014 Write an x86 assembly emulator that can read an assembly file, and execute each\nassembly instruction one by one. 223 (cid:13)c Smruti R. Sarangi\nEx. 34 \u2014 Use the GNU compiler to generate an assembly file for a test program written in\nC using the command, gcc -S -masm=intel. (cid:13)c Smruti R. Sarangi 224 Part II\nOrganisation: Processor Design\n225  6\nLogic Gates, Registers, and Memories\nWe are ready to design a real computer now. Before we start, let us quickly take a glance at\nsome of the main requirements and constraints for designing a computer as described in the\nlast few chapters.\nWay Point 3\n\u2022 A computer needs a central processing unit, set of registers, and a large amount of\nmemory.\n\u2022 A computer needs to support a complete, concise, generic, and simple instruction set.\n\u2022 SimpleRisc is a representative instruction set. To implement it, we need to primarily\nhave support for logical operations, arithmetic computations, register and memory\naccesses.\nFigure 6.1 shows a plan for the next few chapters. In this chapter, we shall look at design-\ning simple circuits for logical operations, registers, and basic memory cells. We shall consider\narithmetic units such as adders, multipliers, and dividers in Chapter 7, and subsequently com-\nbine the basic elements to form advanced elements such as the central processing unit, and an\nadvanced memory system in Chapters 8, 9, and 10.\nBefore, we proceed further, let us warn the reader that this chapter is meant to give a brief\nintroduction to the design and operation of logic circuits. This chapter takes a cursory look at\ndigital logic, and focuses on introducing the broad ideas. A rigorous treatment of digital logic\nis beyond the scope of this book. The interested reader is referred to seminal texts on digital\nlogic [Taub and Schilling, 1977, Lin, 2011, Wakerly, 2000]. This chapter is primarily meant for\n227 (cid:13)c Smruti R. Sarangi 228\nChapter 6 Chapter 8\nCentral processing\nRegisters Memory\nunit\nChapter 9\nLogical operations\nPipelined processors\nChapter 7 Chapter 10\nArithmetic operations Memory system\nBasic elements Advanced elements\nFigure 6.1: Plan for the next few chapters\ntwotypesofreaders. Thefirsttypeofreadersareexpectedtohavetakenanintroductorycourse\nondigitallogic,andtheycanusethischaptertorefreshtheirknowledge. Thesecondcategoryof\nreaders are presumed to have little or no background in digital electronics. We provide enough\ninformation for them to appreciate the nuances of digital circuits, and their operation. They\ncan use this knowledge to understand the circuits required to perform computer arithmetic,\nand implement complex processors.\nFor implementing logical operations such as bitwise AND, OR, shifts, and register\/memory\ncells, we typically use silicon based circuits today. Note that this was not always the case. The\nearliest computers in the 19th century were made from mechanical parts. Till the sixties, they\nweremadeofvacuumtubes. Itisonlyafterthediscoveryofthetransistorandintegratedcircuit\ntechnology that computer processors started getting manufactured using silicon. However, this\nmight be a passing trend. It is perfectly possible in the future that we will have computers\nmade of other materials.\n6.1 Silicon based Transistors\nSilicon is the 14th element in the periodic table. It has four valence electrons and belongs to\nthe same group as carbon and germanium. However, it is less reactive than both.\nOver 90% of the earth\u2019s crust consists of silicon based minerals. Silicon dioxide is the\nprimary constituent of sand, and quartz. It is abundantly available, and is fairly inexpensive\nto manufacture.\nSiliconhassomeinterestingpropertiesthatmakeittheidealsubstratefordesigningcircuits\nand processors. Let us consider the molecular structure of silicon. It has a dense structure,\nwhere each silicon atom is connected to four other silicon atoms, and the tightly connected set 229 (cid:13)c Smruti R. Sarangi\nof silicon atoms are bound together to form a strong lattice. Other materials notably, diamond,\nhave a similar crystalline structure. Silicon atoms are thus more tightly packed than most\nmetals.\nDue to the paucity of free electrons, silicon does not have very good electrical conduction\nproperties. In fact it is midway between a good conductor, and an insulator. It is thus known\nas a semiconductor. It is possible to slightly modify its properties by adding some impurities\nin a controlled manner. This process is called doping.\nDefinition 44\nA semiconductor has an electrical conductivity, which is midway between a good conductor\nand an insulator.\n6.1.1 Doping\nTypically, two types of impurities are added to silicon to modify its properties: n-type and p-\ntype. N-type impurities typically consist of group V elements in the periodic table. Phosphorus\nis the most common n-type dopant. Arsenic is also occasionally used. The effect of adding a\ngroup V dopant with five valence electrons is that an extra electron gets detached from the\nlattice, and is available for conducting current. This process of doping effectively increases the\nconductivity of silicon.\nLikewise, it is possible to add a group III element such as boron or gallium to silicon to\ncreate p-type doped silicon. This produces the reverse effect. It creates a void in the lattice.\nThis void is also referred to as a hole. A hole denotes the absence of an electron. Like electrons,\nholes are free to move. Holes can also help in conducting current. Electrons have a negative\ncharge, and holes are conceptually associated with a positive charge.\nNow that we have created two kinds of semiconductor materials \u2013 n-type and p-type. Let\nus see what happens if we connect them to form a p-n junction.\nDefinition 45\n\u2022 A n-type semiconductor has group V impurities such as phosphorus and arsenic. Its\nprimary charge carriers are electrons.\n\u2022 A p-type semiconductor has group III impurities such as boron and gallium. Its pri-\nmary charge carriers are holes. Holes have an effective positive charge.\n\u2022 A p-n junction is formed when we place a p-type and n-type semiconductor side by\nside. (cid:13)c Smruti R. Sarangi 230\nDepletion\nregion\np n\nDiffusion\nHole\nDrift Electron\nFigure 6.2: A P-N junction\n6.1.2 P-N Junction\nLet us consider a p-n junction as shown in Figure 6.2. The p-type region has an excess of holes\nand the n-type region has an excess of electrons. At the junction, some of the holes cross over\nand move to the n region because they are attracted by electrons. Similarly, some electrons\ncross over and get amassed on the side of the p region. This migration of holes and electrons\nis known as diffusion. The area around the junction that witnesses this migration is known\nas the depletion region. However, due to the migration of electrons and holes, an electric field\nis produced in the opposite direction of migration in the depletion region. This electric field\ninduces a current known as the drift current. At steady state, the drift and diffusion currents\nbalance each other, and thus there is effectively no current flow across the junction.\nNow, let us see what will happen if we connect both sides of the p-n junction to a voltage\nsource such as a battery. If we connect the p side to the positive terminal, and the n side to\nthe negative terminal, then this configuration is known as forward bias. In this case, holes flow\nfrom the p side of the junction to the n side, and electrons flow in the reverse direction. The\njunction thus conducts current.\nIf we connect the p side to the negative terminal and the n side to the positive terminal,\nthen this configuration is known as reverse bias. In this case, holes and electrons are pulled\naway from the junction. Thus, there is no current flow across the junction and the p-n junction\nin this case does not conduct electricity.\nA simple p-n junction as described is known as a diode. It conducts current in only one\ndirection, i.e., when it is in forward bias.\nDefinition 46\nA diode is an electronic device typically made of a single p-n junction that conducts current\nin only one direction. 231 (cid:13)c Smruti R. Sarangi\nGate\nSource\nDrain\nGate\nDrain\nSource\nGate voltage Switch state\nSiO2\nHigh on\nn-type n-type Low off\n(b)\nSubstrate (p-type)\nGate\n(a)\nSource Drain\n(c)\nFigure 6.3: NMOS transistor\n6.1.3 NMOS Transistor\nNow, let us connect two p-n junctions to each other as shown in Figure 6.3(a). This structure is\nknown as an NMOS (Negative Metal-Oxide-Semiconductor) transistor. In this figure there is a\ncentralsubstrateofptypedopedsilicon. Therearetwosmallregionsonbothsidesthatcontain\nn type doped silicon. These regions are known as the drain and source respectively. Note that\nsince the structure is totally symmetric, any of these two regions can be designated as the\nsource or the drain. The region in the middle of the source and drain is known as the channel.\nOn top of the channel there is a thin insulating layer typically made of silicon dioxide(SiO )\n2\nand it is covered by a metallic or polysilicon based conducting layer. This is known as the gate.\nThere are thus three terminals of a typical NMOS transistor \u2013 source, drain and gate. Each\nof them can be connected to a voltage source. We now have two options for the gate voltage \u2013\nlogical 1 (V volts) or logical 0 (0 volts). If the voltage at the gate is logical 1 (V volts), then\ndd dd\nthe electrons in the channel get attracted towards the gate. In fact, if the voltage at the gate\nis larger that a certain threshold voltage (typically 0.15 V in current technologies), then a low\nresistance conducting path forms between the drain and the source due to the accumulation of\nelectrons. Thus current can flow between the drain and the source. If the effective resistance of\nthe channel is R , then we have V = IR +V . If the amount of current flow\nchannel drain channel source\nthrough the transistor is low, then V is roughly equal to V because of the low channel\ndrain source\nresistance (R ). We can thus treat the NMOS transistor as a switch (see Figure 6.3(b)).\nchannel\nIt is turned on when the voltage of the gate is 1.\nNow, if we set the voltage at the gate to 0, then a conducting path made up of electrons\ncannot form in the channel. Hence, the transistor will not be able to conduct current. It will (cid:13)c Smruti R. Sarangi 232\nbe in the off state. In this case, the switch is turned off.\nThe circuit symbol for a NMOS transistor is shown in Figure 6.3(c).\nGate\nSource\nDrain\nGate\nDrain\nSource\nGate voltage Switch state\nSiO2\nHigh off\np-type p-type Low on\n(b)\nSubstrate (n-type) Gate\n(a)\nSource Drain\n(c)\nFigure 6.4: PMOS transistor\n6.1.4 PMOS Transistor\nLike the NMOS transistor, we can have a PMOS transistor as shown in Figure 6.4(a). In this\ncase, the source and drain are regions made up of p type silicon. The logic for the operation of\nthe transistor is exactly the reverse of that of the NMOS transistor. In this case, if the gate is\nat logical 0, then holes get attracted to the channel and form a conducting path. Whereas, if\nthe gate is at logical 1, then holes get repelled from the channel and do not form a conducting\npath.\nThe PMOS transistor can also be treated as a switch (see Figure 6.4(b)). It is turned on,\nwhen the gate voltage is 0, and it is turned off when the voltage at the gate is a logical 1. The\ncircuit symbol of a PMOS transistor is shown in Figure 6.4(c).\n6.1.5 A Basic CMOS based Inverter\nNow, let us construct some basic circuits using NMOS and a PMOS transistors. When a circuit\nuses both these types of transistors, we say that it uses CMOS (combined mos) logic. The\ncircuit diagram of an inverter using CMOS logic is shown in Figure 6.5. In this circuit, an\nNMOS transistor is connected between the ground and the output, and a PMOS transistor is\nconnected between V and the output. The input is fed to the gates of both the transistors.\ndd\nIftheinputisalogical0,thenthePMOStransistorisswitchedon,andtheNMOStransistor\nis switched off. In this case, the output is equal to 1. Likewise, if the input is a logical 1, then 233 (cid:13)c Smruti R. Sarangi\nv\ndd\nCircuit symbol\nCMOS inverter\nFigure 6.5: CMOS inverter\nthe NMOS transistor is switched on and the PMOS transistor is switched off. In this case the\noutput is a logical 0. We thus see that this simple circuit inverts the value at the input. It can\nthus be used to implement the NOT operation.\nThe benefits of CMOS technology are manifold. Note that during steady state one of the\ntransistors is in the off state. It thus does not conduct any current. A little amount of current\ncan still leak through the transistors or flow through the output terminal. However, this is\nminimal. Hence, we can conclude that the power dissipation of a CMOS inverter at steady\nstate is vanishingly small since power is equal to current multiplied by voltage. However,\nsome power is dissipated, when the transistor switches its input value. In this case, both the\ntransistors are on for a small amount of time. There is some current flow from V to ground.\ndd\nNonetheless, as compared to competing technologies, the power dissipated by a CMOS inverter\nis significantly lower and is thus amenable for use by today\u2019s processors that have more than a\nbillion transistors.\n6.1.6 NAND and NOR Gates\nFigure 6.6 shows how to construct a NAND gate in CMOS technology. The two inputs, A and\nB, are connected to the gates of each NMOS-PMOS pair. If both A and B are equal to 1, then\nthe PMOS transistors will switch off, and both the NMOS transistors will conduct. This will\nset the output to a logical 0. However, if one of the inputs is equal to 0, then one of the NMOS\ntransistors will turn off and one of the PMOS transistors will turn on. The output will thus\nget set to a logical 1.\nNote that we use the \u2018.\u2019 operator for the AND operation. This notation is very widely used\nin representing Boolean formulae. Likewise for the OR operation, we use the \u2018+\u2019 sign.\nFigure 6.7 shows how to construct a NOR gate. In this case, the two inputs, A and B, are\nalso connected to the gates of each NMOS-PMOS pair. However, as compared to the NAND\ngate, the topology is different. If one of the inputs is a logical 1, then one of the NMOS (cid:13)c Smruti R. Sarangi 234\nv\ndd\nA B\nA.B\nB\nA.B\nA\nCircuit symbol\nCMOS NAND gate\nFigure 6.6: CMOS NAND gate\nv\ndd\nA\nA+B\nB\nA+B\nA B\nCircuit symbol\nCMOS NOR gate\nFigure 6.7: CMOS NOR gate\ntransistors will turn on and one of the PMOS transistors will turn off. The output will thus get\nset to 0. If both the inputs are equal to 0, then both the NMOS transistors will shut off, and\nboth the PMOS transistors will turn on. The output in this case will be equal to a logical 1.\nNow, thatwehaveconstructedabasicinverter, aNAND,andNORgateusingCMOSlogic;\nwehavethetoolstoconstructanytypeoflogicgate. ThisisbecauseNANDandNORgatesare\nknown as universal gates (see [Kohavi and Jha, 2009]). They can be used to construct any kind\nof logic gate and implement any logic function. In our circuits, we shall implement complex\nlogic gates using AND, OR, NAND, NOR, XOR, and NOT gates. Other than AND and OR\ngates, we have described the construction of the rest of the four gates in this section. We can 235 (cid:13)c Smruti R. Sarangi\nconstruct an AND gate by connecting the output of a NAND gate to a NOT gate. Similarly,\nwe can construct an OR gate by connecting the output of a NOR gate to a NOT gate.\nIn the next section, we shall look at structures that compute complex functions on a set of\nBoolean input bits. We call such structures combinational logic structures because they decide\nif a certain set of input boolean bits belong to a set containing restricted combinations of bits.\nFor example, a XOR gate produces 1 if the input bits are either 01 or 10. In this case the set\nS contains the combinations: {01,10}. A XOR logic structure decides if the two input bits are\nin the set S. If they are in the set, then it produces an output equal to 1, otherwise it produces\n0.\n6.2 Combinational Logic\n6.2.1 XOR Gate\nLet us implement the logic function for exclusive or (XOR). The truth table is shown in Ta-\nble 6.1. We shall use the \u2295 operator for the XOR operation. An exclusive or operation returns\na 1 if both the inputs are unequal, otherwise it returns a 0.\nA B A \u2295 B\n0 0 0\n1 0 1\n0 1 1\n1 1 0\nTable 6.1: The XOR operation\nWe observe that A\u2295B = A.B +A.B. The circuit for implementing a XOR gate is shown\nin Figure 6.8.\nFigure 6.8: XOR gate\n6.2.2 Decoder\nA decoder takes as input a log(n)-bit binary number and has n outputs. Based on the input it\nsets one of the outputs to 1. (cid:13)c Smruti R. Sarangi 236\nOutputs\nA\n0\nB\nA\n1\nB\nA\n2\nB\nA\n3\nB\nFigure 6.9: Design of a 2\u00d74 decoder\nLet us assume that n is a power of 2. Now any value from 0 to n \u2212 1 can be encoded\nusing log(n) bits. Let us thus treat the input as a Boolean representation of one of the outputs\n(0...(n\u22121)). For example, if the value of the log(n)-bit input is equal to i, then the ith output\nis set to 1. The rest of the outputs are set to 0. Decoders are extensively used in the design of\nmemory cells and other combinational elements such as multiplexers and demultiplexers.\nThe design of the decoder is shown in Figure 6.9. We show the design of a 2\u00d74 decoder\nthat has two inputs and four outputs. Let the inputs be A and B. We generate all possible\ncombinations: AB,AB,AB,andAB. TheseBooleancombinationsaregeneratedbycomputing\na logical NOT of A and B and then routing the values to a set of AND gates.\nLet us now explain with an example. Assume that the input is equal to 10. This means\nthat B = 1 and A = 0. We need to set the 2nd output line to 1, and the rest to 0. The\nreader can verify that this is indeed happening (note that we are counting from 0). The AND\ngate corresponding to the 2nd output needs to compute AB. In this case, it will be the only\ncondition that evaluates to 1.\nOn similar lines, we can create a generic log(n)\u00d7n decoder.\n6.2.3 Multiplexer\nThe block diagram of a multiplexer is shown in Figure 6.10. It takes n input bits and log(n)\nselect bits, and based on the value of the select bits, chooses one input as the output (refer to\nthe line with arrows in Figure 6.10). Multiplexers are heavily used in the design of processors,\nwhere we need to choose one output out of a set of inputs. A multiplexer is also known as a\nmux.\nTo choose1 input out ofninputs, weneedto specify the identifierof the input. Note thatit\ntakes (cid:100)log(n)(cid:101) bits to identify any input uniquely (see Section 4.4). We can number each input\nusing (cid:100)log(n)(cid:101) binary bits. Each input thus has a unique representation. Now, if the select\nbits match the binary encoding of an input, then the input gets reflected at the output. For\nexample, if the value of the select bits is i, then the value of the output is equal to the value of\nthe ith input. 237 (cid:13)c Smruti R. Sarangi\nlog(n) select bits\ns\nt\nu Output\np\nn\ni MUX\nn\nFigure 6.10: Block diagram of a multiplexer\nX\n00\nX\n01\nSelect 2 x 4\nOutput\nbits Decoder X\n10\nX\n11\nFigure 6.11: Design of a 4-input multiplexer\nA multiplexer consists of three stages (see Figure 6.11). The first stage is a decoder that\ntakes the log(n) select bits as its input and sets one of the n output lines to a logical 1. Each\noutput line is connected to an AND gate (second stage). Since only one of the output lines\nis set to 1, only the AND gate corresponding to that output is enabled. This means that the\noutput of this gate is equal to the value of the other input.\nIn the example in Figure 6.11, the multiplexer has four inputs: X , X , X , and X .\n00 01 10 11\nEach input is connected to an AND gate. If the select bits are equal to 01, then the AND\ngate corresponding to the input X is enabled by the decoder. Its output is equal to X .\n01 01\nThe outputs of the rest of the AND gates are a logical 0 because they are not enabled by the (cid:13)c Smruti R. Sarangi 238\ndecoder: one of their inputs is a logical 0.\nFinally, in the third stage, an OR gate computes the logical OR of all the outputs of the\nsecond stage. Note that for the OR gate, the inputs are n\u22121 zeros and X , where B and A\nBA\nare the values of the select bits, respectively. The final output is thus the value of the input\nthat was selected, X .\nBA\nInput\nY\n00\nY\n01\n2 x 4\nSelect\nbits Decoder\nY\n10\nY\n11\nFigure 6.12: Design of a demultiplexer\n6.2.4 Demultiplexer\nA demultiplexer takes as input a log(n)-bit binary number, a 1-bit input, and transfers the\ninput to one of n output lines. The block diagram is shown in Figure 6.12. Demultiplexers are\nused in the design of memory cells, where it is necessary for the input to reflect in exactly one\nof the output lines.\nThe operation is similar to that of a multiplexer. Instead of having multiple inputs, we have\njust 1 input. Like the case of the multiplexer, we first enable the appropriate AND gate with\na decoder. Then the second stage consists of an array of AND gates, where each gate reflects\nthe input bit at its output if it is enabled by the decoder. Recall that an AND gate is enabled\nif one of its inputs is a logical 1; in this case the output is equal to the value of the other input.\n6.2.5 Encoder\nLet us now consider a circuit with the reverse logic as that of a decoder. Its block diagram is\nshown in Figure 6.13. The circuit has n inputs, and log(n) outputs. One of the n inputs is\nassumedtobe1, andtherestareassumedtobe0. Theoutputbitsprovidethebinaryencoding\nof the input that is equal to 1. For example in a 8 input, 3 output encoder, if the fifth line is\nequal to 1, then the output is equal to 100 (count starts from 0).\nLet us construct a simple 4-2 encoder (4 inputs, 2 output bits). Let us number the bits X ,\n0\nX ,X , and X . If bit X is equal to 1, then the output should be equal to the binary encoding\n1 2 3 A 239 (cid:13)c Smruti R. Sarangi\nlog(n)\nEncoder\nn bits\nbits\nFigure 6.13: Block diagram of an n bit encoder\nof A. Let us designate the output as Y, with two bits Y , and Y . We have the following\n0 1\nequations for Y , and Y .\n0 1\nY = X +X (6.1)\n0 1 3\nY = X +X (6.2)\n1 2 3\n(6.3)\nThe intuition behind these equations is that the LSB of a 2-bit number is equal to 1, when\nit is equal to 01(1), or 11(3). The MSB is equal to 1, when it is equal to 10(2), or 11(3). We\ncan extend this logic to create a generic n-log(n) bit encoder. The circuit diagram of a 4-2-bit\nencoder is shown in Figure 6.14.\nX\n1\nX 0\n3\nX\n2\nX 1\n3\nFigure 6.14: Circuit diagram of a 4-2-bit encoder\nExample 91\nWrite the equations for a 8-3 bit encoder. Assume that the inputs are X ...X , and the\n0 7\noutputs are Y , Y , and Y .\n0 1 2 (cid:13)c Smruti R. Sarangi 240\nAnswer:\nY = X +X +X +X (6.4)\n0 1 3 5 7\nY = X +X +X +X (6.5)\n1 2 3 6 7\nY = X +X +X +X (6.6)\n3 4 5 6 7\n6.2.6 Priority Encoder\nLet us now assume that we do not have the restriction that only one input line can be equal to\n1. Let us assume that more than one inputs can be equal to 1. In this case, we need to report\nthe binary encoding of the input line that has the highest index (priority). For example, if lines\n3 and 5 and on, then we need to report the binary encoding of the 5th line. The block diagram\nremains the same as Figure 6.13.\nHowever, the equations for computing the output change. For a 4-2-bit priority encoder,\nthe equations are as follows.\nY = X .X +X (6.7)\n0 1 2 3\nY = X +X (6.8)\n1 2 3\nLet us consider Y . If X = 1, then Y = 1, because X has the highest priority. However,\n0 3 0 3\nif X = 1, then we cannot take a decision based on its value, because X , and X might also\n1 2 3\nbe equal to 1. If X = 1, then there is no issue, because it also sets the value of Y . However,\n3 0\nif X = 0, and X = 1, then we need to disregard X . Hence, we need to compute X .X +X\n3 2 1 1 2 3\nfor Y . The equation for Y remains the same (the reader should try to find the reason). The\n0 1\ncircuit diagram of a 4-2-bit encoder is shown in Figure 6.15.\nX\n2\nX\n1\n0\nX\n3\nX\n2\nX 1\n3\nFigure 6.15: Circuit diagram of a 4-2-bit priority encoder 241 (cid:13)c Smruti R. Sarangi\nExample 92\nWrite the equations for a 8-3 bit priority encoder. Assume that the inputs are X ...X ,\n0 7\nand the outputs are Y , Y , and Y .\n0 1 2\nAnswer:\nY = X .X .X .X +X .X .X +X .X +X (6.9)\n0 1 2 4 6 3 4 6 5 6 7\nY = X .X .X +X .X .X +X +X (6.10)\n1 2 4 5 3 4 5 6 7\nY = X +X +X +X (6.11)\n3 4 5 6 7\n6.3 Sequential Logic\nWe have looked at combinational logic circuits that compute different functions on bits. In this\nsection, we shall look at saving bits for later use. These structures are known as sequential logic\nelements because the output is dependent on past inputs, which came earlier in the sequence of\nevents. The basic idea in a logic gate was to modify the input values to get the desired outputs.\nIn a combinational logic circuit, if the inputs are set to 0, then the outputs also get reset. To\nensure that a circuit stores a value and maintains it for as long as the processor is powered on,\nwe need to design a different kind of circuit that has some kind of \u201dbuilt-in memory\u201d. Let us\nstart with formulating a set of requirements.\n1. The circuit should be self-sustaining, and should maintain its values after the external\ninputs are reset. should not rely on external signals to maintain its stored elements.\n2. There should be a method to read the stored value without destroying it.\n3. There should be a method to set the stored value to either 0 or 1.\nThe best way to ensure that a circuit maintains its value is to create a feedback path, and\nconnect the output back to the input. Let us take a look at the simplest logic circuit in this\nspace: an SR latch.\n6.3.1 SR Latch\nFigure 6.16 shows the SR latch. There are two inputs S(set) and R (reset). There are two\noutputs Q, and its complement Q. Let us now analyse this circuit that contains two cross-\ncoupled NAND gates. Note that if one of the inputs of a NAND gate is 0, then the output\nis guaranteed to be 1. However, if one of the inputs is 1, and the other input is A, then the\noutput is A.\nLet us consider the case when, S=1 and R=0. One of the inputs(S) to the top NAND gate\nis 0. Thus, Q=1. The bottom NAND gate has two inputs R = 1, and Q = 1. Thus, the output,\nQ = 0. Similarly, if S=0 and R=1, then Q=0, and Q = 1. The S input sets the bit in the latch,\nand the R bit resets it to 0. Let us now consider the case when both S and R are 0. In this case\none of the inputs to both the NAND gates is 1. The top NAND gate\u2019s output is Q = Q, and (cid:13)c Smruti R. Sarangi 242\nS\nQ\nQ\nR\nFigure 6.16: The SR latch\nthe bottom NAND gate\u2019s output is Q. Thus, the value is maintained and we have effectively\nachieved the objective of storing a bit.\nNow, let us see what happens if we set both S and R to 1. In this case, S = 0 and R = 0.\nThus, Q and Q are both equal to 1. In this case, Q is not the logical complement of Q. Now, let\nus say that S is set to 0. Then Q will become 0, and Q will become 1. Likewise, if R is set to 0,\nthen Q will become 1, and Q will become 0. However, if both S and R simultaneously become\n0, then we cannot predict the state of the latch before hand. This is because in practice, signal\ntransitions are never perfectly simultaneous. A non-zero time lag between the transitions of\nboth the inputs is almost always there. Hence, the circuit can see the following sequence of\ntransitions in the SR bits: 11 \u2192 10 \u2192 00, or 11 \u2192 01 \u2192 00. For the former sequence, Q will\nbe set to 1, and for the latter sequence Q will be set to 0. This is known as a race condition\nand causes unpredictable behaviour. Thus, we do not want to set both S and R to 1.\nS R Q Q Action\n0 0 Q Q maintain\nold old\n1 0 1 0 set\n0 1 0 1 reset\n1 1 ? ? indeterminate\nTable 6.2: State transition table for an SR latch\nTable 6.2 shows the state transition table for the SR latch. Q and Q are the old values\nold old\nof Q and Q respectively. The main feature is that setting SR=00 maintains the value stored in\nthe latch. During this period, we can read the value of the outputs infinitely often.\nThe main issues with the SR latch are as follows.\n\u2022 S=1 and R=1 is an invalid input.\n\u2022 We do not have a method of synchronising the transitions in the input and the output.\nWhenever the inputs change, the outputs also change. As we shall see in the next section,\nthis is not desired behaviour. 243 (cid:13)c Smruti R. Sarangi\n6.3.2 The Clock\nA typical processor contains millions or possibly billions of logic gates and thousands of latches.\nDifferent circuits take different amounts of time. For example, a multiplexer might take 1 ns,\nand a decoder might take 0.5 ns. Now, a circuit is ready to forward its outputs when it has\nfinished computing them. If we do not have a notion of global time, it is difficult to synchronise\nthe communication across different units, especially those that have variable latencies. Under\nsuch a scenario it is difficult to design, operate, and verify a processor. We need a notion of\ntime. For example, we should be able to say that an adder takes two units of time. At the end\nof the two units, the data is expected to be found in latch X. Other units can then pick up the\nvalue from latch X after two units of time and proceed with their computation.\nLet us consider the example of a processor that needs to send some data to the printer. Let\nus further assume that to communicate data, the processor sends a series of bits over a set of\ncopper wires, the printer reads them, and then prints the data. The question is, when does\nthe processor send the data? It needs to send the data, when it is done with the computation.\nThe next question that we can ask is, \u201cHow does the processor know, when the computation is\nover?\u201d It needs to know the exact delays of different units, and once the total duration of the\ncomputation has elapsed, it can write the output data to a latch and also set the voltages of the\ncopper wires used for communication. Consequently, the processor does need a notion of time.\nSecondly, the designers need to tell the processor the time that different sub-units take. Instead\nof dealing with numbers like 2.34 ns, and 1.92 ns, it is much simpler to deal with integers such\nas 1, 2, and 3. Here, 1, 2, and 3, represent units of time. A unit of time can be any number\nsuch as 0.9333 ns.\nDefinition 47\nclock signal Aperiodicsquarewavethatissenttoeverypartofalargecircuitorprocessor.\nclock cycle The period of a clock signal.\nclock frequency The inverse of the clock cycle period.\nClock\nperiod\nFigure 6.17: A Clock Signal\nHence, most digital circuits synchronise themselves with a clock signal that sends a periodic\npulse to every part of the processor at exactly the same time. A clock signal is a square wave as\nshown in Figure 6.17; most of the time the clock signal is generated externally by a dedicated (cid:13)c Smruti R. Sarangi 244\nunit on the motherboard. Let us consider the point at which the clock signal transitions from\n1 to 0 (downward\/negative edge) as the beginning of a clock cycle. A clock cycle is measured\nfrom one downward edge of the clock to the next downward edge. The duration of a clock cycle\nis also known as a clock period. The inverse of a clock period is known as the clock frequency.\nImportant Point 7\nA computer, laptop, tablet, or mobile phone typically has a line listing the frequency in its\nspecifications. For example, the specifications might say that a processor runs at 3 GHz.\nNow we know that this number refers to the clock frequency.\nThe typical model of computation is as follows. The time required to perform all basic\nactions in a circuit is measured in terms of clock cycles. If a producer unit takes n clock cycles,\nthen at the end of n clock cycles, it writes its value to a latch. Other consumer units are aware\nof this delay, and at the beginning of the (n+1)th clock cycle, they read the value from the\nlatch. Since all the units explicitly synchronise with the clock, and the processor is aware of\nthe delays of every unit, it is very easy to sequence the computation, communicate with I\/O\ndevices, avoid race conditions, debug and verify circuits. We can see that our simple example\nin which we wanted to send data to a printer can be easily solved by using a clock.\n6.3.3 Clocked SR Latch\nS\nQ\nClk\nQ\nR\nFigure 6.18: The Clocked SR latch\nFigure 6.18 shows the SR latch augmented with two NAND gates that have the clock as\none of the inputs. The other two inputs are the S and R bits respectively. If the clock is 0, then\nboth the inputs to the cross-coupled NAND gates are 1. This maintains the previous value.\nIf the clock is 1, then the inputs to the cross-coupled NAND gates are S and R respectively.\nThese are the same inputs as the basic SR latch. The rest of the operation follows Table 6.2.\nNote that a clocked latch is typically referred to as a flip-flop.\nDefinition 48\nFlip-flop : It is a clocked latch that can save a bit (0 or 1). 245 (cid:13)c Smruti R. Sarangi\nByusingtheclock,wehavepartiallysolvedtheproblemofsynchronisinginputsandoutputs.\nIn this case, when the clock is 0, the outputs are unaffected by the inputs. When the clock is\n1, the outputs are affected by the input. Such a latch is also called a level sensitive latch.\nDefinition 49\nA level sensitive latch is dependent on the value of the clock signal \u2013 0 or 1. Typically, it\ncan read in new values, only when the clock is 1.\nIn a level sensitive latch, circuits have half a clock cycle to compute the correct outputs\n(when the clock is 0). When the clock is 1, the outputs are visible. It would be better to have\none full clock cycle to compute the outputs. This would require an edge sensitive latch. An\nedge sensitive latch reflects the inputs at the output, only at the downward edge of the clock.\nDefinition 50\nAn edge sensitive latch reflects the inputs at the output only at a fixed clock edge, such as\nthe downward edge (transition from 1 to 0).\nLet us try to create an edge sensitive SR latch.\n6.3.4 Edge Sensitive SR Flip-flop\nS\nQ\nClk Clk\nQ\nR\nFigure 6.19: The clocked edge sensitive SR flip-flop\nFigure6.19showsaclockededgesensitiveSRflip-flop. WeconnecttwoclockedSRflip-flops\ntogether. The only difference is that the second SR flip-flop uses the complement of the clock\nsignal, CLK. The first flip-flop is known as the master, and the second flip-flop is known as\nthe slave. This flip-flop is also known as a master-slave SR flip-flop. Here, is how this circuit\nworks. (cid:13)c Smruti R. Sarangi 246\nDefinition 51\nA master-slave flip-flop contains two flip-flops that are connected to each other. The mas-\nter\u2019s output is the slave\u2019s input. Typically, the slave uses a clock signal that is the logical\ncomplement of the master\u2019s clock.\nWhenever the clock signal is high (1), the inputs (S and R) are read into the first SR flip-\nflop. When the clock signal becomes low (0), then the first flip-flop stops accepting new data;\nhowever,thesecondflip-floptakestheoutputofthefirstflip-flopandsetsitsoutputaccordingly.\nThus, new data arrives at the output terminals Q and Q when the clock transitions from 1 to\n0 (downward clock edge). We thus have created a flip-flop that is edge sensitive.\nHowever, some problems still remain. If both S and R are 1, then there might be a race\ncondition, and the output can be unpredictable. This problem needs to be fixed. Let us first\ntry to look at a complex solution that augments the clocked edge sensitive SR flip-flop, and\nthen look at simpler solutions.\n6.3.5 JK Flip-flop\nJ N1\nQ\nClk Clk\nQ\nK N2\nFigure 6.20: The JK flip-flop\nFigure6.20showsaJKflip-flop. Therearetwominordifferencesascomparedtothemaster-\nslave SR flip-flop. The first is that the inputs are now J and K, instead of S and R. The second\nis that Q and Q are now inputs to the input NAND gates (N1 and N2).\nLet us do a case by case analysis. Assume that the clock is high. If J and K are both 0,\nthen the outputs of N1 and N2 are both 1 and the case is same as that for the master-slave\nSR flip-flop. The outputs are maintained. If J=1 and K=0, then we need to consider the value\nof Q. If Q = 1, then Q = 0, and the output of NAND gate N1 (see Figure 6.20) is 0. The\noutputs of the master flip-flop are therefore 1 and 0 respectively. The output of the slave after\nthe downward\/ negative clock edge will therefore be: Q = 1,Q = 0.\nNow, assume that Q = 0, and Q=1. In this case, the outputs of both N1 and N2 are 1 and\nthus all the values are maintained. Hence, after the negative clock edge we have: Q=1, and\nQ = 0. We can thus conclude, that if J=1 and K=0, Q=1, and Q=0.\nSimilarly, if J=0 and K=1, we can prove that Q=0, and Q=1. 247 (cid:13)c Smruti R. Sarangi\nLetusnowconsiderthecasewhenbothJandKare1. InthiscasetheoutputofN1isQand\ntheoutputofN2isQ. Theoutputofthemasterflip-flopisequaltoQandQrespectively. After\nthe negative clock edge the outputs will be: Q = Q and Q = Q . Thus, the outputs will\nold old\nget toggled. We will not have a race condition anymore. Table 6.3 shows the state transition\ntable for the JK flip-flop.\nJ K Q Q Action\n0 0 Q Q maintain\nold old\n1 0 1 0 set\n0 1 0 1 reset\n1 1 Q Q toggle\nold old\nTable 6.3: State transition table for a JK flip-flop\n6.3.6 D Flip-flop\nD\nQ\nClk\nQ\nFigure 6.21: D flip-flop\nInstead of having a dedicated S(set) and R (reset) signal, we can make our life easy by\nmaking one the complement of the other. However, in this case, we will not have a method\nof maintaining the value. The input will get reflected at the output at every negative clock\nedge. In a lot of cases, this is sufficient and we do not need dedicated logic to either maintain\nor toggle the values. In this case, we can use the simplistic D flip-flop as shown in Figure 6.21.\nIt is basically a SR flip-flop where R = S.\nNote that the second input (to the lower NAND gate) is equal to D\u2227Clk. When Clk is\nequal to 1, the second input is equal to D. When Clk is 0, the flip-flop maintains the previous\nvalues and does not accept new data.\n6.3.7 Master-slave D Flip-flop\nAkin to the JK flip-flop we can have a master-slave version of the D flip-flop.\nFigure 6.22 shows a master-slave version of the D flip-flop. We connect one D flip-flop to a\nSR flip-flop. Here, we do not need wires connecting the inputs with Q and Q because we are\nnot interested in toggling the state. Secondly, we have avoided race conditions by not having\nthe evil (1,1) input. (cid:13)c Smruti R. Sarangi 248\nD\nQ\nClk Clk\nQ\nFigure 6.22: Master-slave D flip-flop\nA master-slave flip-flop uses 8 NAND gates and 1 inverter for the clock. A master slave\nD flip-flop requires 34 transistors, and a master-slave JK flip-flop requires 38 transistors. This\nis a large expenditure for saving just 1 bit! We should be able to do better. We shall see in\nlater sections that we can store a single bit with even 1 transistor. But the circuit will become\nextremely slow. We can thus conclude that flip-flops are inefficient as far as power and the\ntransistor budget are concerned; however, they are very fast. If we need to save a lot of data\nand we are willing to sacrifice on time, then we should opt for SRAM and DRAM memories\n(described in Section 6.4).\n6.3.8 Metastability\nUp till now we have assumed that at the negative edge of a clock the input instantaneously\nreflects at the output. This high level assumption is however not strictly correct. Readers need\nto appreciate the fact that every digital circuit is at its core an analog circuit. Quantities like\ncurrent and voltage take time to reach their optimal levels, and the circuit is sensitive to the\nvoltage levels, and the timing of the inputs. The readers can refer to standard text books [Taub\nand Schilling, 1977] on digital design for a thorough explanation.\nIn this section, we shall take a look at one particular aspect of the analog nature of flip flops\nknown as metastability. If there is a change in the input close to the negative clock edge, then\nthe output becomes non-deterministic, and might even fluctuate or oscillate for some period of\ntime. This phenomenon is known as metastability. To avoid such behaviour it is necessary to\nensure that the input is stable (does not change) for t units of time before the clock edge,\nsetup\nand is also stable for t units of time after the clock edge. This means that there is a window\nhold\nof time around the clock edge in which the input to the flip flop needs to remain stable. Only,\nin this case, we can guarantee the correct operation of a flip flop. This window of time in which\nwe want the inputs to be stable is known as the keep out region.\nLet us now display these concepts graphically in Figure 6.23. We need to note that in\npractice, the setup time, and the hold time are small fractions of the total cycle time (< 10%).\nDesignerstakespecialprecautionstoensurethattherearenotransitionsintheinputinthekeep\nout region. We shall see that this phenomenon has important implications when we discuss I\/O\ncircuits. These circuits have sophisticated delay elements that delay signals to keep transitions\nout of the keep out region. 249 (cid:13)c Smruti R. Sarangi\nclock\nedge\nsetup hold\ntime time\nkeep out\nregion\nFigure 6.23: Setup time, hold time, and the keep out region\n6.3.9 Registers\nParallel In\u2013Parallel Out\nWe can store n bit data by using a set of n master slave D flip flops. Each D flip flop is\nconnected to an input line, and its output terminal is connected to an output line. Such an n\nbit structure is known as an n bit register. Here, we can load n bits in parallel, and also read\nout n bits in parallel at every negative clock edge. Hence, this structure is known as a parallel\nin\u2013parallel out register. Its structure is shown in Figure 6.24.\nOutputs\nD Q D Q D Q\nclk clk clk\nInputs\nFigure 6.24: A parallel in\u2013parallel out register (cid:13)c Smruti R. Sarangi 250\nOutputs\nD Q D Q D Q\nclk clk clk\nInput\nFigure 6.25: A serial in\u2013parallel out register\nSerial In \u2013 Parallel Out\nLet us now consider a serial in\u2013parallel out register as shown in Figure 6.25. Here, we have\na single input that is fed into the leftmost D flip flop. Every cycle, the input moves to the\nadjacent flip flop on the right. Thus, to load n bits it will take n cycles. The first bit will get\nloaded into the leftmost flip flop in the first cycle, and it will take n cycles for it to reach the\nlast flip flop. By that time, the rest of the n\u22121 flip flops will get loaded with the rest of the\nn\u22121 bits. We can then read out all the n bits in parallel (similar to the parallel in\u2013parallel out\nregister). This register is also known as a shift register and is used for implementing circuits\nused in high speed I\/O buses.\n6.4 Memories\n6.4.1 Static RAM (SRAM)\nSRAM Cell\nSRAM refers to static random access memory. A basic SRAM cell contains two cross-coupled\ninverters as shown in Figure 6.26. In comparison, a basic SR flip-flop or a D flip-flop contains\ncross-coupled NAND gates. The design is shown in Figure 6.26.\nThe core of the SRAM cell contains 4 transistors (2 in each inverter). This cross-coupled\narrangementissufficienttosaveasinglebit(0or1). However,weneedsomeadditionalcircuitry\ntoreadandwritevalues. Atthispoint, thereadermightbewonderingifitisabadideatohave\ncross-coupled inverters in a latch. They after all require fewer transistors. We shall see that the\noverheads of implementing the circuitry for reading and writing a SRAM cell are non-trivial.\nThe overheads do not justify making a latch with a SRAM cell as its core.\nThe cross coupled inverters are connected to transistors on each side (W1, W2). The gates 251 (cid:13)c Smruti R. Sarangi\nWord line (WL)\nv\ndd\nW1\nW2\nBL BL\n(Bit line)\nFigure 6.26: A 6 transistor SRAM cell\nof W1 and W2 are connected to the same signal, known as the word line. The four transistors\nin the two inverters, W1, and W2, comprise the SRAM cell. It has six transistors in total.\nNow, if the voltage on the word line is low, then W1 and W2 are off. It is not possible to read\nor write the SRAM cell. However, if the signal on the word line is high, then W1 and W2 are\non. It is possible to access the SRAM cell.\nNow, the transistors, W1, and W2, are connected to copper wires on either side known as\nthe bit lines. The bit lines are designed to carry complementary values. One of them is BL and\nthe other is BL. To write a value into the cell it is necessary to set the values of BL and BL to\nA and A respectively, where A is the value that we intend to write. To read a value, we need\nto turn the word line on and read the voltages of the bit lines.\nLet us now delve slightly deeper into the operation of the SRAM cells. Note that SRAM\ncells are not solitary units like latches. They exist as a part of an array of SRAM cells. We\nneed to consider the array of SRAM cells in entirety.\nArray of SRAM Cells\nFigure 6.27 shows a typical SRAM array. SRAM cells are laid out as a two dimensional matrix.\nAll the SRAM cells in a row share the word line, and all SRAM cells in a column share a pair\nof bit lines. To activate a certain SRAM cell it is necessary to turn its associated word line\non. This is done by a decoder. It takes a subset of address bits, and turns the appropriate\nword line on. A single row of SRAM cells might contain 100+ SRAM cells. Typically, we will (cid:13)c Smruti R. Sarangi 252\nBL BL\nWL\nSRAM SRAM SRAM SRAM\ncell cell cell cell\nD\nWL\nAddress\ne\nc SRAM SRAM SRAM SRAM\no cell cell cell cell\nd\ne\nr\nWL\nSRAM SRAM SRAM SRAM\ncell cell cell cell\nAddress\nColumn mux\/demux\nWrite Write\ndriver driver\nData in Data in\nSense amplifier Sense amplifier\nData out\nFigure 6.27: Array of SRAM Cells\nbe interested in the values of 32 SRAM cells (on a 32-bit machine). In this case the column\nmux\/demux selects the bit lines belonging to the SRAM cells of interest. It uses a subset of the\nbits in the address as the column select bits. This design methodology is also known as 2.5D\nmemory organisation.\nAs the size of the array grows it may become more asymmetric. This needs to be avoided,\notherwise the capacitive loading on the word lines or bit lines will become prohibitive. Hence,\ncolumns need to become wider and the column mux\/demux structure needs to be driven by a\nlarge column decoder.\nThe process of writing is easy. The strong write drivers need to set the values of BL and\nBL. To write 1, BL needs to be driven to 1, and BL needs to be driven to 0. However, reading\na value is slightly more difficult. The reason is that a SRAM cell needs to charge an entire bit\nline to the stored value such that it can be read. Since hundreds of SRAM cells are typically\nconnected to a bit line, the bit line has a very high capacitance. Consequently, it will take a\nlong time to charge\/discharge the bit line to logical 0 or 1. 253 (cid:13)c Smruti R. Sarangi\nHence, something smarter needs to be done. The read operation is divided into two phases.\nIn the first phase, BL and BL are precharged to V \/2 volts. If the supply voltage is equal\ndd\nto 1 volt, then the bit lines are charged to 0.5 volts. This step is known as pre-charging.\nSubsequently, the SRAM cells of interest are accessed by setting the corresponding word line.\nThesenseamplifierssimplymonitorthedifferenceinvoltagebetweenBLandBL. Themoment\ntheabsolutevalueofthedifferenceexceedsathreshold, theresultcanbeinferred. Forexample,\nif we are reading a logical 1, we need not wait for BL to reach 1, and BL to reach 0. If the\nvoltage difference between BL and BL exceeds a threshold, then we can declare the result to\nbe 1.\nThis method is very fast because of the following reasons. Pre-charging bit lines is very\nfast because there are dedicated pre-charge circuits that can pump a large amount of current\ninto the bit lines to enable faster charging or discharging. After pre-charging inferring the\nstored value from the voltage swing between BL and BL is also very fast. This is because the\nthreshold for the voltage swing is much lower as compared to the supply voltage. Given the\nhigh capacitance of bit lines, the time to charge\/discharge bit lines is very crucial. Hence, if we\nreduce the amount of the voltage swing that is required to infer the value stored in the SRAM\ncell, it makes a significant difference.\nWe can justify the overhead of pre-charge circuits, write drivers, and sense amplifiers, if we\nhave a large number of SRAM cells. Hence, SRAMs are suitable for structures such as register\nfiles and on-chip memories. They should not be used for storing a few bits; flip-flops are better\nchoices.\n6.4.2 Content Addressable Memory (CAM)\nCAM Cell\nInthissection,weshalllookataspecialtypeofmemorycellcalledaCAM(ContentAddressable\nMemory) cell. First, consider an SRAM array. A typical SRAM array is a matrix of SRAM\ncells. Each row contains a vector of data. A given row is addressed or located by using a\nspecific set of address bits. However, it is possible to locate a row using a different method.\nWe can address a row by its content. For example, if each row contains 32 SRAM cells, then\nwe can think of the contents of a row as a 32-bit number. We wish to address the row this\n32-bit number. For example, if a row contains 0x AB 12 32 54, then we should be able to find\nthe index of the row that contains this value. Such a memory is known as a CAM memory,\nand each basic cell is known as a CAM cell. A CAM memory is typically used to implement\na hashtable(see [Cormen et al., 2009]) in hardware. A hashtable saves key-value pairs such as\n(name and address). We address a hashtable by its key, and read the value. It is a more flexible\ndata structure than an array, because we are not limited to integer indices. We shall use CAM\narrays to implement some memory structures in Chapter 10.\nLet us take a look at a 10 transistor CAM cell in Figure 6.28. If the value stored in the\nSRAM cell, V, is not equal to the input bit, A , then we wish to set the value of the match\ni\nline to 0. In the CAM cell, the upper half is a regular SRAM cell with 6 transistors. We have\n4 extra transistors in the lower half. Let us now consider transistor T1. It is connected to a\nglobal match line, and transistor T2. T1 is controlled, by the value, V, which is stored in the\nSRAM cell, and T2 is controlled by A . Let us assume that V = A . If both of them are 1, then\ni i\ntransistors T1, and T2 are in the ON stage, and there is a direct conducting path between the (cid:13)c Smruti R. Sarangi 254\nA\ni A\nBL BL i\nWord line (WL)\nv\ndd\nW1\nW2\nmatch\nT3\nT1\nT2 T4\nFigure 6.28: A 10 Transistor CAM cell\nmatch line and ground. Thus, the value of the match line will get set to 0. However, if V and\nA are both 0, then the path through T1 and T2 is not conducting. But, in this case, the path\ni\nthrough T3, and T4 becomes conducting, because the gates of these transistors are connected\nto V, and A respectively. The input to both the gates is a logical 1. Thus, the match line will\ni\nbe pulled down to 0. The reader can conversely verify that if V = A , no conducting path is\ni\nformed. Thus, a CAM cell drives the match line to a logical 0, if there is a mismatch between\nthe value stored and the input bit, A .\ni\nArray of CAM Cells\nFigure 6.29 shows an array of CAM cells. The structure is mostly similar to an SRAM array.\nWe can address a row by its index, and perform a read\/write access. Additionally, we can\ncompare each row of the CAM cell with the input, A. If any row matches the input, then the\ncorresponding match line will have a value of 1. We can compute a logical OR of all the match\nlines, and decide if we have a match in the CAM array or not. Additionally, we can connect\nall the match lines of the CAM array to a priority encoder to find the index of the row that\nmatches the data. 255 (cid:13)c Smruti R. Sarangi\nA A A A A A\nCAM mode B1 L BL 1 B2 L BL 2 Bn L BL n\nWL\nCAM CAM CAM\ncell cell cell\nmatch\nD\nAddress e WL\nc CAM CAM CAM\no cell cell cell\nd\ne\nr\nMatching\nindex\nWL\nCAM CAM CAM\ncell cell cell\nAddress\nColumn mux\/demux\nWrite Write\ndriver driver\nData in Data in\nSense amplifier Sense amplifier\nmatch\nData out\nFigure 6.29: Array of CAM Cells\n6.4.3 Dynamic RAM (DRAM)\nLet us now take a look at a memory technology that just uses one transistor to save a bit. It\nis thus very dense, area, and power efficient. However, it is also much slower than SRAMs and\nlatches. It is suitable for large off-chip memories.\nWord line (WL)\nBL\n(Bit line)\nFigure 6.30: A DRAM cell\nAbasicDRAM(dynamicRAM)cellisshowninFigure6.30. Thegateofthesingletransistor\nredocne\nytiroirP (cid:13)c Smruti R. Sarangi 256\nis connected to the word line, which enables or disables it. One of the terminals is connected\nto a capacitor that stores charge. If the bit stored is a logical 1, then the capacitor is fully\ncharged. Otherwise, it is not charged.\nThus, reading and writing values is very easy. We need to first set the word line such that\nthe capacitor can be accessed. To read the value we need to sense the voltage on the bit line.\nIf it is at ground potential, then the cell stores 0, else if it is close to the supply voltage, then\nthe cell stores 1. Similarly, to write a value, we need to set the bit line (BL) to the appropriate\nvoltage, and set the word line. The capacitor will get charged or discharged accordingly.\nBL\nWL\nDRAM DRAM DRAM DRAM\ncell cell cell cell\nD\nWL\nAddress\ne\nc DRAM DRAM DRAM DRAM\no cell cell cell cell\nd\ne\nr\nWL\nDRAM DRAM DRAM DRAM\ncell cell cell cell\nSense amplifier Sense amplifier\nAddress\nColumn mux\/demux\nRefresh Refresh\nWrite Write\ndriver driver\nData in Data in\nData out Data out\nFigure 6.31: Array of DRAM cells\nHowever, in the case of DRAMs, everything does not come for free. Let us assume that\nthe capacitor is charged to a voltage equal to the supply voltage. In practice, the capacitor\nwill gradually leak some charge through the dielectric, and the transistor. This current is very\nsmall, but the total loss of charge over a large duration of time can be significant and can 257 (cid:13)c Smruti R. Sarangi\nultimately discharge the capacitor. To prevent this, it is necessary to periodically refresh the\nvalue of a DRAM cell. We need to read it and write the data value back. This also needs to\nbe done after a read operation because the capacitor loses some charge while charging the bit\nline. Let us now try to make an array of DRAM cells.\nArray of DRAM Cells\nWe can construct an array of DRAM cells (see Figure 6.31) the same way that we created an\narray of SRAM cells. There are three differences. The first is that there is one bit line instead\nof two. Second, we also have a dedicated refresh circuit connected to the bit lines. This is used\nafter read operations, and is also invoked periodically. Finally, in this case the sense amplifiers\nappear before the column mux\/demux. The sense amplifiers additionally cache the data for the\nentire DRAM row (also called a DRAM page). They ensure that subsequent accesses to the\nsame DRAM row are fast because they can be serviced directly from the sense amplifiers.\nLet us now briefly discuss the timing aspects of modern DRAMs. In the good old days,\nDRAM memory was accessed asynchronously. This means that the DRAM modules did not\nmake any timing guarantees. However, nowadays every DRAM operation is synchronised with\na system clock. Hence, today\u2019s DRAM chips are synchronous DRAM chips (SDRAM chips).\nFigure 12.26 in Chapter 12 shows the timing diagram of a simple SDRAM chip for a read\naccess.\nSynchronous DRAM memories typically use the DDR4 or DDR5 standards as of today.\nDDR stands for Double Data Rate. Devices using the earliest standard, DDR1, send 8-byte\ndata packets to the processor on both the rising and falling edges of the clock. This is known\nas double pumped operation. The peak data rate of DDR1 is 1.6 GB\/s. Subsequent DDR\ngenerations extend DDR1 by transferring data at a higher frequency. For example, DDR2 has\ntwice the data rate as DDR1 devices (3.2 GB\/s). DDR3 further doubles the peak transfer rate\nby using a higher bus frequency, and has been in use since 2007 (peak rate of 6.4GB\/s).\n6.4.4 Read Only Memory (ROM)\nLet us now consider simpler memory structures that are read-only in nature. We require read\nonlymemoriesthatsavedatathatshouldneverbemodified. Thiscanincludesecurityinforma-\ntion in smart phones, BIOS chips in the motherboards of processors, or the programs in some\nmicrocontrollers. We desire read only memory such that it is not possible for users to mali-\nciously change the information stored in them. It turns that we can make simple modifications\nto our DRAM cell, and construct read only memories.\nROM Memories\nThe capacitor in a DRAM cell stores a logical bit. If it stores a logical 1, then the charge across\nthe capacitor is equal to the supply voltage V , and if it stores a logical 0, then the charge\ndd\nacross the capacitor is 0V. Instead of having a capacitor, we can directly connect one end of\nthe transistor to either ground or V depending upon the bit that we wish to store. This can\ndd\nbe done at the time of designing and manufacturing the chip. The ROM memory cell as shown\nin Figure 6.32 replaces the capacitor by a direct connection to V or ground. A ROM array is\ndd\nsimilar to a DRAM array. However, it does not have write drivers, and refresh circuitry. (cid:13)c Smruti R. Sarangi 258\nV\ndd\nWord line (WL) Word line (WL)\nBL BL\n(Bit line) (Bit line)\n(b)\n(a)\nFigure 6.32: (a) ROM cell storing a logical 0, (b) ROM cell storing a logical 1\nPROM (Programmable ROM) Memories\nWord line (WL)\nAntifuse\nBL\n(Bit line)\nFigure 6.33: PROM Cell\nLet us now look at a programmable ROM (PROM) cell that can be programmed only once\nto store either a 0 or 1. Typically vendors of PROM cells release PROM arrays to the market.\nThey are then programmed by microprocessor vendors to hold a given set of data values. Note\nthat once data has been written, we cannot make any further modifications. Hence, it acts like\nread only memory. A PROM cell is shown in Figure 6.33. We have a connection between the\ntransistor and ground through an element known as an antifuse. An antifuse is the reverse of a 259 (cid:13)c Smruti R. Sarangi\nconventional fuse. By default, an antifuse is a very weak conductor of current. However, when\nwe transfer a large amount of current through the antifuse, it changes its structure and forms\na conducting path. In the former case, the transistor is left floating, and it does not have the\nability to drive the bit line to a logical 0. Hence, we can infer that the cell stores a logical 1.\nOnce, the antifuse becomes conducting, the transistor (if enabled by the word line) can drive\nthe bit line to a logical 0. Thus, in this case, the transistor stores a logical 0. A PROM cell\nbased array is similar to an array of ROM cells. Each bit line is initially precharged. After\nenabling the word line, if the voltage at the sense amplifiers does not increase, then we can infer\na logical 1. However, if the voltage keeps decreasing towards 0 V, then we can infer a logical 0.\nWe can build antifuses with a variety of materials. Dielectric antifuses use a thin layer\nof a sensitive material (silicon oxides) between two conducting wires. This thin layer breaks\ndown and forms a conducting path upon the application of a large current pulse. In place of\ndielectrics we can use other materials such as amorphous silicon.\nRead only memories are useful in a limited set of scenarios since their contents cannot be\nmodified. WehavetworelatedsetsofdevicescalledEPROM(ErasablePROM),andEEPROM\n(Electrically Erasable PROM). Typically these memories are made of special transistors that\nallow charge storage in a structure known as a floating gate. EPROM memories could be erased\nby applying a strong ultraviolet light pulse to the floating gate. However, such memories are\nnot used today, and have been superseded by flash memories that can be read, written, and\nerased electrically. Flash memories are explained in detail in Section 12.8.4.\n6.4.5 Programmable Logic Arrays\nIt turns out that we can make a combinational logic circuit out of a memory cell similar to a\nPROM cell very easily. Such devices are known as programmable logic arrays, or PLAs. PLAs\nare used to implement complex logic functions consisting of tens or hundreds of minterms in\npractice. The advantage of a PLA over a hardwired circuit made up of logic gates is that\na PLA is flexible. We can change the Boolean logic implemented by the PLA at run time.\nIn comparison, a circuit made in silicon can never change its logic. Secondly, designing and\nprogramming a PLA is simpler, and there are a lot of software tools to design and work with\nPLAs. Lastly, a PLA can have multiple outputs, and it can thus implement multiple Boolean\nfunctions very easily. This additional flexibility comes at a cost, and the cost is performance.\nLet us now consider an example. Let us assume that we wish to implement the Boolean\nfunction (ABC+AB) using a PLA. Let us break the Boolean expression into a set of minterms\n(see Section 2.1.6). We thus have:\nABC +AB = ABC +ABC +ABC (6.12)\nSince, we have three variables (A, B, and C) here, we have 8 possible minterms. Let us\nthus have a PLA with 8 rows that generates the values of all the possible minterms. Let each\nrow correspond to a minterm. Let us design the PLA in such a way that a row has an output\nequal to 1 if the corresponding minterm evaluates to 1. We can compute the result of the entire\nBoolean expression by computing the logical OR of the values of all the minterms that we are\ninterested in. For this specific example (ABC+AB), we are interested in 3 out of 8 minterms. (cid:13)c Smruti R. Sarangi 260\nHence, we need a mechanism to filter these 3 minterms and compute a logical OR of their\nvalues.\nLet us start out with the design of a PLA cell.\nPLA Cell\nPLA cell\nResult line Result line\nE E E E\n1 2 n\nX X 1 X 2 X n\n(a) (b)\nFigure 6.34: A PLA cell\nThe PLA cell shown in Figure 6.34(a) is in principle similar to a basic PROM cell. If the\nvalue (E) at the gate is equal to 1, then the NMOS transistor is in the ON state. As a result\nof this, the voltage difference between the source and drain terminals of the NMOS transistor\nis very small. In other words, we can simplistically assume that the voltage of the result line\nis equal to the voltage of the signal, X. If (E = 0), the NMOS transistor is in the OFF state.\nThe result line is floating, and it maintains its precharged voltage. In this case, we propose to\ninfer a logical 1.\nLet us now construct a row of PLA cells where each PLA cell is connected to an input wire\nat its source terminal as shown in Figure 6.34(b). The inputs are numbered X ...X . The\n1 n\ndrains of all the NMOS transistors are connected to the result line. The gates of the transistors\nof the PLA cells are connected to a set of enable signals, E ...E . If any of the enable signals\n1 n\nis equal to 0, then that specific transistor is disabled, and we can think of it as being logically\nremoved from the PLA array.\nLet us consider all the PLA cells that are enabled (gate voltage is equal to a logical 1). If\nany of the source inputs (X ...X ) is equal to 0, then the voltage of the result line will be\n1 n\ndriven to 0. We assume that we precharge the result line to a voltage corresponding to logical\n1 at the beginning. Now, if none of the input voltages is equal to 0, then the value of the result\nline will be equal to a logical 1. We can thus conclude that the Boolean function computed\nby a row of PLA cells is equal to X .X ....X (assuming that all the cells are enabled). For\n1 2 n\nexample, if we want to compute the value of the minterm, ABCD, then we need to set X = A,\n1\nX = B, X = C, and X = D. The Boolean value represented by the result line will be equal\n2 3 4\nto ABCD.\nIn this manner, we can evaluate the values of all the minterms by connecting the source\nterminals ofPLAcells to the inputbits. Up tillnow we have notconsidered thecase ofBoolean\nvariables in minterms in their complemented form such as ABCD. For the minterm, ABCD,\nwe need to make the following connections. We need to connect 4 PLA cells to the result line, 261 (cid:13)c Smruti R. Sarangi\nwhere their source terminals are connected to the signals A, B, C, and D respectively. We need\nto generate, A, and D by complementing the values of A and D using inverters.\nArray of PLA Cells\nAND plane OR plane\nPrecharge\nA A B B C C driver\nOutput lines\nPrecharge ABC\ndriver\nPrecharge ABC\ndriver\nPrecharge ABC\ndriver\nPrecharge ABC\ndriver\nPrecharge ABC\ndriver\nPrecharge ABC\ndriver\nPrecharge ABC\ndriver\nABC\nPrecharge\ndriver\nPLA Cell Enabled in the\nOR plane ABC+ AB AB+ AB\nFigure 6.35: Array of PLA cells\nLet us now create an array of PLA cells as shown in Figure 6.35. Each row corresponds to\na minterm. For our 3 variable example, each row consists of 6 columns. We have 2 columns for\neach variable (original and complemented). For example, the first two columns correspond to\nA and A respectively. In any row, only one of these two columns contains a PLA cell. This is\nbecause A and A cannot both be true at the same time. In the first row, we compute the value\nof the minterm, ABC. Hence, the first row contains PLA cells in the columns corresponding to\nA, B, and C. We make similar connections in the rest of the rows for the remaining minterms.\nThis part of the PLA array is known as the AND plane because we are computing a logical\nANDofthevalues(originalorcomplemented)ofvariables. TheANDplaneofthePLAarrayis (cid:13)c Smruti R. Sarangi 262\nindependent of the Boolean functions that we wish to compute. Given the inputs, it calculates\nthe value of all possible minterms.\nNow, we need to compute the logical OR of the minterms that we are interested in. For\nexample, in Equation 6.12, we are interested in the logical OR of 3 minterms. To compute the\nOR function, we use another PLA array known as the OR plane. However, there is a small\nproblem here. A row of PLA cells is designed to compute a logical AND of all the inputs. We\ncan use DeMorgan\u2019s theorem to compute the OR of inputs using a PLA array. Let us use the\nfollowing relationship:\n(X +X +...X ) = X +X +...X = X .X ....X\n1 2 n 1 2 n 1 2 n\nThus, to compute the logical OR of (X ,X ,...X ), we need to complement each input,\n1 2 n\ncompute the logical AND of all the complemented inputs, and compute the complement of the\nresult. A similar computation needs to be performed in the OR plane of the PLA array. In\nFigure 6.35, we have inverters to compute the logical negation of each minterm. Then, we\ncompute their logical AND using a column of PLA cells (similar to a row of PLA cells). In\nthis case, only the PLA cells that correspond to minterms in the Boolean expression need to\nbe enabled (shown as an arrow in Figure 6.35). The rest of the PLA cells in each column are\ndisabled. Finally, we compute the logical negation of the result line, to get the value of the\nBoolean function.\nNote that we can have multiple output lines in the OR plane, and thus we can compute the\nvalues of multiple Boolean functions in parallel. In Figure 6.35 we also compute the value of\nA\u2295B = A.B +A.B in the second output line. For the result lines, and the output lines, we\nassume an array of sense amplifiers that perform appropriate voltage conversions. For the sake\nof simplicity, we do not show them in the figure.\nThe important point to remember here is that the OR plane is programmable. As shown\nin Figure 6.35, we compute the logical OR of a set of result lines by setting the gate voltage of\nthe connecting PLA cell to a logical 1. At any point of time, we can change the connections to\nthe output lines by changing the gate voltages, and we can thus change the Boolean expression\nthat is computed by the PLA.\nWay Point 4\n\u2022 We have assembled the arsenal required to implement circuits to perform complex\narithmetic operations.\n\u2022 Using our set of logic gates, flip-flops, memories, and arithmetic circuits(to be studied\nin Chapter 7), we are ready to implement a full fledged processor. 263 (cid:13)c Smruti R. Sarangi\n6.5 Summary and Further Reading\n6.5.1 Summary\nSummary 6\n1. Modernprocessorsusesiliconbasedtransistors. Siliconisasemi-conductor. However,\nits properties can be modified by adding impurities (known as doping) from the III,\nand V groups of the periodic table. If we add group III elements such as boron (p\ntype), we remove a charge carrier from the silicon atom lattice, and thus create a hole.\nSimilarly, if we add a group V element such as phosphorus (n type), then we introduce\nan additional electron to the lattice. In either case, we increase the conductivity of\nsilicon.\n2. We can create a p-n junction by juxtaposing p-type and n-type silicon structures. A\np-n junction conducts when it is forward biased (the p side is connected to the higher\nvoltage), and stops conducting when it is reverse biased.\n3. We can create an NMOS transistor by having two wells of n-type silicon in a p-\ntype substrate. These wells are connected to electrical terminals, and are known as\nthe source and drain. The area between the wells is known as the channel, and is\nseparated from an electrical terminal (known as the gate) by a thin layer of silicon\ndioxide. When we apply a positive voltage (> threshold voltage) at the gate, the NMOS\ntransistor can conduct current. Otherwise, it acts like an open circuit.\n4. In comparison, the PMOS transistor has two p-type wells in an n-type substrate. It\nforms a conducting path, when the voltage at the gate is 0V.\n5. We can use NMOS and PMOS transistors to implement a basic inverter, NAND,\nNOR, AND and OR gates.\n6. We can use these basic gates to create a host of complex structures such as the XOR\ngate, multiplexer, decoder, encoder, and priority encoder.\n7. By creating a feedback path between NAND or NOR gates, we can save a logical bit.\nThis structure is known as a latch. If we enable and disable access to a latch based\non the value of a clock signal, then the latch is known as a flip flop.\n8. We have considered SR, JK, and D flip flops in this chapter. The SR flip flop does\nnot have deterministic outputs for all combinations of input transitions.\n9. We use a master slave design for JK, and D flip flops because we can make the inputs\nappear at the outputs almost instantaneously at the negative clock edge.\n10. A set of flip flops can be used to make registers. (cid:13)c Smruti R. Sarangi 264\n11. We use a cross coupled pair of inverters to make an SRAM cell. It is connected\nby two transistors (enabled by a word line) to a pair of bit lines. A sense amplifier\nmonitors the difference in voltages across the pair of bit lines. Based on the sign of\nthe difference, we can infer a logical 0 or 1.\n12. In comparison, a DRAM cell uses a single transistor and a capacitor. It is a high\ndensity memory technology. However, we need to regularly refresh the value of DRAM\ncells to keep it operational.\n13. We can modify the basic structure of a DRAM cell by creating hardwired connections\nbetween the transistor and the supply rails to implement read only memory. Pro-\ngrammable ROM (PROM) cells use an antifuse to create an alterable connection to\nV .\ndd\n14. We can create programmable logic arrays for computing the values of Boolean expres-\nsions by arranging PLA cells (similar to PROM cells) in an array. In the AND plane,\neach row computes the value of a minterm. In the OR plane, we compute a logical\nOR of all the minterms that form a part of the Boolean expression.\n6.5.2 Further Reading\nSemiconductors and electronic devices are thoroughly studied in advanced courses on semicon-\nductor device physics. Readers can refer to the books by Sze [Sze and Ng, 2006], and Street-\nman [Streetman and Banerjee, 2005] for a deeper discussion on the physics of semiconductor\ndevices. In advanced courses on semiconductor device physics, students typically study the\nbasics of the operations of diodes, transistors, and other semiconductor devices from the point\nof view of quantum mechanics. After introducing semiconductor devices, we introduced com-\nbinational logic gates and sequential logic elements. The simple structures that we introduced\nin this chapter, are very commonly used. Students should however take a look at the following\nbooks[Lin, 2011,Wakerly, 2000,DallyandPoulton, 1998]forgettingathoroughunderstanding\nof the devices, including their behaviour from the perspective of analog electronics. We lastly\ntalk about memories. The book, \u201cIntroduction to VLSI Systems\u201d, by Ming Bo Lin [Lin, 2011]\nhas a fairly in-depth coverage of memory structures. Memory technology, especially DRAM\ntechnology, is advancing very quickly. A host of standards, and design styles have evolved over\nthe last decade. A lot of these trends are discussed in the book on memory systems by Jacob,\nNg, and Wang [Jacob et al., 2007]. This book is also very useful for professionals who are\nlooking to build commercial systems with state of the art memory technology. 265 (cid:13)c Smruti R. Sarangi\nExercises\nTransistors and Logic Gates\nEx. 1 \u2014 Describe the operation of a p-n junction.\nEx. 2 \u2014 Define drift and diffusion current.\nEx. 3 \u2014 Describe the operation of a NMOS transistor?\nEx. 4 \u2014 Draw the circuit of a CMOS inverter, NOR gate, and NAND gate.\nEx. 5 \u2014 Implement the AND, OR and NOT gates using NAND gates only.\nEx. 6 \u2014 Implement the AND, OR and NOT gates using NOR gates only.\nEx. 7 \u2014 Implement XOR and XNOR gates using NAND gates only.\nEx. 8 \u2014 ImplementthefollowingBooleanfunctionsbyusingonlyAND,ORandNOTgates.\n(a) A.B+A.B+A.B.C\n(b) (A+B+C).(A.B+C)\n(c) (A+B).(C +D)\n(d) A.B.C +A.B.C +D\nEx. 9 \u2014 Answer Question 8 by using only NAND gates.\nCombinational Logic and Sequential Logic\nEx. 10 \u2014 Draw the circuit diagram of a 3\u00d78 decoder.\nEx. 11 \u2014 Draw the circuit diagram of a 8-3 bit encoder.\nEx. 12 \u2014 Draw the circuit diagram of a 8-3 bit priority encoder.\nEx. 13 \u2014 Suppose a poll has to be conducted with three entities A, B and C, each of which\ncan either vote a \u2018yes\u2019 (encoded as 1) or a \u2018no\u2019 (encoded as 0). The final output is equal to the\nmajority opinion. Draw a truth table of the system, simplify the function, and implement it\nusing logic gates.\n* Ex. 14 \u2014 Most circuits in modern computers are built using NAND and NOR gates, be-\ncause they are easy to build using CMOS technology. Suppose another technology in invented\nin the near future, which implements a new gate, X, very efficiently. X takes 3 inputs A, B\nand C and computes: X(A,B,C) = A.B +C. Using only X gates and NOT gates, how will\nyou implement the following function: f(A,B,C) = A+B+C? (cid:13)c Smruti R. Sarangi 266\n** Ex. 15 \u2014 Implement the following logic functions using a 4 to 1 multiplexer, and a single\nNOT gate.\n(a) AB+BC +AC\n(b) A+B+C\n(c) A.B+A.B.C\n** Ex. 16 \u2014 Is it possible to implement every 3 variable Boolean function with a 4 to 1\nmultiplexer, and a single NOT gate? Prove your answer.\nSequential Logic\nEx. 17 \u2014 What is the difference between a flip-flop and a latch?\nEx. 18 \u2014 Define the following terms:\ni)Metastability\nii)Keep out region\niii)Setup time\niv)Hold time\nEx. 19 \u2014 Why do we wish to avoid the indeterminate state in an SR flip-flop?\nEx. 20 \u2014 What is the advantage of an edge sensitive flip-flop?\n* Ex. 21 \u2014 What is the fundamental advantage of a JK flip-flop over a D flip-flop?\nEx. 22 \u2014 Describe the design of registers in your own words.\nEx. 23 \u2014 An edge sensitive toggle flip-flop (or T flip-flop) has a single input T and toggles\nits state on a negative clock edge if T = 1. If (T = 0), then it maintains its state. How will you\nconstruct an edge sensitive T flip-flop form an edge sensitive J-K flip-flop?\n* Ex. 24 \u2014 Can you create a negative edge triggered D flip-flop using 2 multiplexers, and a\nNOT gate?\nEx. 25 \u2014 Design a SR flip-flop with NOR gates.\n* Ex. 26 \u2014 Using two edge triggered D flip-flops, design a circuit that divides the frequency\nof the clock signal by 4.\n** Ex. 27 \u2014 Counters are essential components of any complex digital circuit. They are\nessentially sequential circuits which loop through a specific set of states. Design a counter\nwhich generates a sequence of numbers (in binary form) from 0 to 7 and cycles back again to\n0. This is called a MOD 8 counter. 267 (cid:13)c Smruti R. Sarangi\n** Ex. 28 \u2014 UsingDflip-flopsandlogicgates, designacircuit, whichgeneratesthefollowing\nsequence of numbers:\n001 \u2192 100 \u2192 010 \u2192 101 \u2192 110 \u2192 111 \u2192 011 \u2192 001\nAssume that the circuit never generates 000. This circuit can be used to generate pseudo-\nrandom numbers.\nMemories\nEx. 29 \u2014 Compare the power, area and time of a SRAM, DRAM, and latch.\nEx. 30 \u2014 Propose a design for the column mux\/demux circuit.\nEx. 31 \u2014 What is the role of the match line in a CAM array?\nEx. 32 \u2014 What is the role of the refresh logic in a DRAM array?\nEx. 33 \u2014 Describe the design of a ROM and PROM cell.\nEx. 34 \u2014 Design a single PLA array to compute al the following Boolean functions:\na)A.B+B.C +C.A\nb)A.B.C +A.B.C\nc)A+B\nDesign Problems\nEx. 35 \u2014 DesignthefollowingcircuitsusingacircuitsimulatorsuchasSpiceandverifytheir\noperation:\na)NOT gate\nb)NAND gate\nc)D flip-flop\nd)SRAM cell\nEx. 36 \u2014 Prepare a report on novel memory technologies such as phase change memory,\nFerro-electric RAM, and magneto-resistive RAM. (cid:13)c Smruti R. Sarangi 268 7\nComputer Arithmetic\nInChapter6, wedescribedthebasiccircuitsforlogicaloperationsandstorageelements. Inthis\nchapter, we will use this knowledge to design hardware algorithms for arithmetic operations.\nThis chapter also requires the knowledge of binary 2\u2019s complement numbers and floating point\nnumbers that we gained in Chapter 2. The plan for this chapter is as follows.\nIn the first part, we describe algorithms for integer arithmetic. Initially, we describe the\nbasic algorithms for adding two binary numbers. It turns out that there are many ways of\ndoing these basic operations, and each method has its own set of pros and cons. Note that the\nproblemofbinarysubtractionisconceptuallythesameasbinaryadditioninthe2\u2019scomplement\nsystem. Consequently, we do not need to treat it separately. Subsequently, we shall see that\nthe problem of adding n numbers is intimately related to the problem of multiplication, and it\nis a fast operation in hardware. Sadly, very efficient methods do not exist for integer division.\nNevertheless, we shall consider two popular algorithms for dividing positive binary numbers.\nAfterintegerarithmetic,weshalllookatmethodsforfloatingpoint(numberswithadecimal\npoint) arithmetic. Most of the algorithms for integer arithmetic can be ported to the realm\nof floating point numbers with minor modifications. As compared to integer division, floating\npoint division can be done very efficiently.\n7.1 Addition\n7.1.1 Addition of Two 1-bit Numbers\nLet us look at the problem of adding two 1-bit numbers, a and b. Both a and b can take two\nvalues \u2013 0 or 1. Hence, there are four possible combinations of a and b. Their sum in binary\ncan be either 00, 01, or 10. Their sum will be 10, when both a and b are 1. We should make an\nimportant observation here. The sum of two 1 bit numbers might potentially be two bits long.\nLet us call the LSB of the result as the sum, and the MSB as the carry. We can relate this\nconcept to standard primary school addition of two 1 digit decimal numbers. If we are adding\n269 (cid:13)c Smruti R. Sarangi 270\n8 and 9, then the result is 17. We say that the sum is 7, and the carry is 1. Similarly, if we add\n3 and 4, then the result is 7. We say that the sum is 7, and the carry is 0.\nWe can extend the concept of sum and carry to adding three 1 bit numbers also. If we are\nadding three 1 bit numbers then the range of the result is between 00 and 11 in binary. In this\ncase also, we call the LSB as the sum, and the MSB as the carry.\nDefinition 52\nsum The sum is the LSB of the result of adding two or three 1 bit numbers.\ncarry The carry is the MSB of the result of adding two or three 1 bit numbers.\nFor an adder that can add two 1 bit numbers, there will be two output bits \u2013 a sum s and a\ncarry c. An adder that adds two bits is known as a half adder. The truth table of a half adder\nis shown in Table 7.1.\nDefinition 53\nA half adder adds two bits to produce a sum and a carry.\na b s c\n0 0 0 0\n0 1 1 0\n1 0 1 0\n1 1 0 1\nTable 7.1: Truth table of a half adder\nFrom the truth table, we can conclude that s = a \u2295 b = a.b + a.b, where \u2295 stands for\nexclusive or, \u2018.\u2019 stands for boolean AND, and \u2018+\u2019 stands for boolean OR. Secondly, c = a.b.\nThe circuit diagram of a half adder is shown in Figure 7.1. As we can see, a half adder is a\nvery simple structure and we have constructed it using just six gates in Figure 7.1.\n7.1.2 Addition of Three 1-bit Numbers\nThe aim is to be ultimately able to add 32-bit numbers. To add the two least significant bits,\nwe can use a half adder. However, for adding the second bit pair, we cannot use a half adder\nbecause there might be an output carry from the first half adder. In this case, we need to add\nthree 1-bit numbers. Hence, we need to implement a full adder that can add 3 bits. One of\nthese bits is a carry out of another adder and we call it the input carry. We represent the input\ncarry as c , and the two other input bits as a and b.\nin 271 (cid:13)c Smruti R. Sarangi\na Half S\nb adder C\na\nb\nS\na\nC\nb\nFigure 7.1: A half adder\nDefinition 54 An adder than can add 3 bits is known as a full adder.\nTable 7.2 shows the truth table for the full adder. We have three inputs \u2013 a, b, and c .\nin\nThere are two output bits \u2013 the sum (s), and the carry out (c ).\nout\na b c s c\nin out\n0 0 0 0 0\n0 1 0 1 0\n1 0 0 1 0\n1 1 0 0 1\n0 0 1 1 0\n0 1 1 0 1\n1 0 1 0 1\n1 1 1 1 1\nTable 7.2: Truth table of a full adder\nFrom the truth table, we can deduce the following relationships: (cid:13)c Smruti R. Sarangi 272\ns = a\u2295b\u2295c\nin\n= (a.b+a.b)\u2295c\nin\n= (a.b+a.b).c +(a.b+a.b).c\nin in\n= a.b.c +a.b.c +(a.b).a.b.c\nin in in\n= a.b.c +a.b.c +(a+b).(a+b).c\nin in in\n= a.b.c +a.b.c +a.b.c +a.b.c\nin in in in\nc = a.b+a.c +b.c\nout in in\nThecircuitdiagramofafulladderisshowninFigure7.2. Thisisfarmorecomplicatedthan\nthe circuit of a half adder. We have used 12 logic gates to build this circuit. Furthermore, some\nof these logic gates use three inputs. However, this degree of complexity is required because all\nour practical adders will use full adders as their basic element. We face the need of adding 3\nbits in all of our arithmetic algorithms.\na Full S\nb\nadder c\nc out\nin\na\na b\na\nb c c\nin out\nc\nc in\nin\nb\ns\nFigure 7.2: A full adder 273 (cid:13)c Smruti R. Sarangi\n7.1.3 Ripple Carry Adder\nLet us now try to add two n bit numbers. Let us start with an example: 1011 +0101 . The\n2 2\naddition is shown in Figure 7.3. We have seen in Section 2.2.3 that binary numbers can be\nadded the same way as decimal numbers. In the case of base 10 decimal numbers, we start at\nthe unit\u2019s digit and proceed towards higher digits. In each step, a carry might be generated,\nwhich is then added to the immediately higher digits. In the case of binary numbers also we\ndo the same. The only difference is that instead of base 10, we are using base 2.\n1 1 1 1\n1 0 1 1\n0 1 0 1\n1 0 0 0 0\nFigure 7.3: Addition of two binary numbers\nFor example, in Figure 7.3, we observe that when two binary bits are added a carry might\nbe generated. The value of the carry is equal to 1. This carry needs to be added to the bits\nin the next position (more significant position). The computation is complete when we have\nfinished the addition of the most significant bits. It is possible that a carry might propagate\nfrom one pair of bits to another pair of bits. This process of propagation of the carry from one\nbit pair to another is known as rippling.\nLet us construct a simple adder to implement this procedure. Let us try to add two n\nbit binary numbers \u2013 A and B. We number the bits of A and B as A ...A and B ...B\n1 n 1 n\nrespectively. Let A refer to A\u2019s LSB, and A refer to A\u2019s MSB. We can create an adder for\n1 n\nadding A and B as follows. We use a half adder to add the LSBs. Then we use n\u22121 full adders\nto add the rest of the corresponding bits of A and B and their input carry values. This n bit\nadder is known as a ripple carry adder. Its design is shown in Figure 7.4. We observe that we\nadd two n bit numbers to produce a n+1 bit result. The method of addition is exactly similar\nto the procedure we follow while adding two binary numbers manually. We start from the LSB\nand move towards the MSB. At every step we propagate the carry to the next pair of digits.\nNow, let us calculate the speed of this adder. Let us assume that it takes t units of time\nh\nfor a half adder to complete its operation, and t units of time for a full adder to complete\nf\nits operation. If we assume that carries are propagated instantaneously across blocks, then the\ntotal time, f(n), is equal to t +(n\u22121)t . Here, n is equal to the number of bits being added.\nh f\nHowever, as we shall see this is a rather cryptic basis of comparison, especially for large\nvalues of n. We do not wish to have a lot of constants in our timing model. Secondly, the\nvalues of these constants are heavily dependent on the specific technology used. It is thus hard\nto derive algorithmic insights. Hence, we introduce the notion of asymptotic time complexity\nthat can significantly simplify the timing models, yet retain their basic characteristics. For (cid:13)c Smruti R. Sarangi 274\nFull Half\nc carry\nadder adder\nA B A B A B A B\nn n c 3 3 c 2 2 c 1 1\nResult\nFigure 7.4: Addition of two binary numbers\nexample, in the case of a ripple carry adder, we can say that the complexity is almost equal to\nn multiplied by some constant. We can further abstract away the constant, and say that the\ntime complexity is the order of n. Let us now formally define this notion.\nAsymptotic Time Complexity\nLet us consider two functions f(n) = 2n2+3, and g(n) = 10n. Here, n is the size of the input,\nand f(n), and g(n) represent the number of time units it takes for a certain circuit to complete\nits operation. We plot the time values for different values of n in Figure 7.5. As we can see,\ng(n) is greater than f(n) for small values of n. However, for larger values of n, f(n) is larger,\nand it continues to be so. This is because it contains a square term, and g(n) does not. We can\nextend this argument to observe that even if g(n) would have been defined to be 100n, f(n)\nwould have ultimately exceeded it. The gist of the argument lies in the fact that f(n) contains\na quadratic term (n2) and g(n) only contains linear terms. For large n, we can conclude that\nf(n) is slower than g(n). Consequently, we need to define a new notion of time that precisely\ncaptures this fact. We call this new notion of time as the asymptotic time complexity.\nThe name comes from the fact that we are interested in finding an envelope or asymptote to\nthe time function such that the function is contained within this envelope for practically large\nvalues of n.\nFor example, we can define the asymptotic time complexity of f(n) to be n2 and that of\ng(n) to be n respectively. This notion of time is powerful enough to say that f(n) is greater\nthan g(n) for values of n larger than some threshold. What if we consider: f(n) = 2n2+3, and\nf(cid:48)(n) = 3n2 +10. Needless to say, f(cid:48)(n) > f(n). However, we might not be interested in the\ndifference. Ifwecomparetheasymptotictimecomplexityoff(n)orf(cid:48)(n)withanotherfunction\nthathastermswithdifferentexponents(otherthan2),thentheresultsofthecomparisonwillbe\nthe same. Consequently, for the sake of simplicity we can ignore the additive and multiplicative\nconstants. We capture the definition of one form of asymptotic time in the big-O notation. It\nis precisely defined in Definition 55. 275 (cid:13)c Smruti R. Sarangi\n250\nf(n)\ng(n)\n200\n150\n100\n50\n0\n0 2 4 6 8 10\nn\nFigure 7.5: f(n) = 2n2+3 and g(n) = 10n\nDefinition 55\nWe say that: f(n) = O(g(n))\nif | f(n) |\u2264 c | g(n) | for all n > n . Here, c is a positive constant.\n0\nThe big-O notation is actually a part of a set of asymptotic notations. For more details,\nthe reader can refer to a standard text in computer algorithms [Cormen et al., 2009]. From our\npoint of view, g(n) gives a worst case time bound for f(n) ignoring additive and multiplicative\nconstants. We illustrate this fact with two examples: Examples 93 and 94. In this book, we\nwill refer to asymptotic time complexity as time complexity.\nExample 93\nf(n) = 3n2+2n+3. Find its asymptotic time complexity.\nAnswer:\nf(n) = 3n2+2n+3\n\u2264 3n2+2n2+3n2 (n > 0)\n\u2264 8(n2)\nHence, f(n) = O(n2).\nemit (cid:13)c Smruti R. Sarangi 276\n800\nf(n)\n700 8n^2\n600\n500\n400\n300\n200\n100\n0\n0 2 4 6 8 10\nn\n8n2 is a strict upper bound on f(n) as shown in the figure.\nExample 94\nf(n) = 0.00001n100+10000n99+234344. Find its asymptotic time complexity.\nAnswer: f(n) = O(n100)\nTime Complexity of a Ripple Carry Adder\nThe worst case delay happens when the carry propagates from the least significant bit to the\nmostsignificantbit. Inthiscase,eachfulladderwaitsfortheinputcarry,performstheaddition,\nand then propagates the carry out to the next full adder. Since, there are n 1 bit adders, the\ntotal time taken is O(n).\n7.1.4 Carry Select Adder\nA ripple carry adder is extremely slow for large values of n such as 32 or 64. Consequently, we\ndesire faster implementations. We observe that in hardware we can potentially do a lot of tasks\nin parallel. Unlike purely sequential C or Java programs where one statement executes after\nthe next, hundreds or even thousands of actions can be performed in parallel in hardware. Let\n\u221a\nus use this insight to design a faster adder that runs in O( n) time.\nLet us consider the problem of adding two numbers A and B represented as: A ...A and\n32 1\nB ...B respectively. Let us start out by dividing the set of bits into blocks of let us say 4\n32 1\nbits. The blocks are shown in Figure 7.6. Each block contains a fragment of A and a fragment\nof B. We need to add the two fragments by considering the input carry to the block, and\nemit 277 (cid:13)c Smruti R. Sarangi\ngenerate a set of sum bits and a carry out. This carry out is an input carry for the subsequent\nblock.\nCarry propagating\nacross blocks\nA A A A A A A A A A A A\n32 31 30 29 8 7 6 5 4 3 2 1\nB B B B B B B B B B B B\n32 31 30 29 8 7 6 5 4 3 2 1\nFigure 7.6: Dividing the numbers into blocks\nIn this case, a carry is propagated between blocks rather that between bit pairs. To add the\npair of fragments within a block, we can use a simple ripple carry adder. For small values of n,\nripple carry adders are not very inefficient. However, our basic problem of carry propagation\nhas not been solved yet.\nLet us now introduce the basic idea of the carry select adder. We divide the computation\ninto two stages. In the first stage, we generate two results for each block. One result assumes\nthattheinputcarryis0,andtheotherresultassumesthattheinputcarryis1. Aresultconsists\nof 4 sum bits, and a carry out. We thus require two ripple carry adders per block. Note that\neach of these additions are independent of each other and thus can proceed in parallel.\nNow, at the beginning of the second stage two sets of results for the nth block are ready. If\nwe know the value of the input carry, C produced by the (n\u22121)th block, then we can quickly\nin\ncalculate the value of the output carry, C , by using a simple multiplexer. We do not need\nout\nto perform any extra additions. The inputs to the multiplexer are the values of C generated\nout\nby the two ripple carry adders that assume C to be 0 and 1 respectively. When the correct\nin\nvalue of C is available, it can be used to choose between the two values of C . This process\nin out\nis much faster than adding the two blocks. Simultaneously, we can also choose the right set of\nsum bits. Then we need to propagate the output carry, C , to the (n+1)th block.\nout\nLet us now evaluate the time complexity of the carry select adder. Let us generalise the\nproblem and assume the block size to be k. The first stage takes O(k) time because we add\neach pair of fragments within a block using a regular ripple carry adder, and all the pairs of\nfragments are added in parallel. The second phase takes time O(n\/k). This is because we have\nhave (cid:100)n\/k(cid:101) blocks and we assume that it takes 1 time unit for the input carry of a block to\nchoose the right output carry in the multiplexer. The total time is thus: O(k+n\/k). Note that\nwe are making some simplistic assumptions regarding the constants. However, our final answer\nwill not change if we make our model more complicated.\nLet us now try to minimise the time taken. This can be done as follows:\n\u2202(k+n\/k)\n= 0\n\u2202k\nn\n(7.1)\n\u21d21\u2212 = 0\nk2\n\u221a\n\u21d2k = n (cid:13)c Smruti R. Sarangi 278\n\u221a \u221a \u221a\nThus, the optimal block size is equal to n. The total time complexity is thus O( n+ n),\n\u221a\nwhich is the same as O( n).\n7.1.5 Carry Lookahead Adder\n\u221a\nWe have improved the time complexity from O(n) for a ripple carry adder to O( n) for a carry\nselect adder. The question is, \u201cCan we do better?\u201d In this section, we shall present the carry\nlookahead adder that can perform addition in O(log(n)) time. O(log(n)) has been proved as\nthe theoretical lower bound for adding two n bit numbers. Note that the log operation in this\nbook typically has a base equal to 2, unless explicitly mentioned otherwise. Secondly, since\nlogarithms to different bases differ by constant multiplicative factors, the base is immaterial in\nthe big-O notation.\nGenerate and Propagate Functions\nBeforeintroducingtheadder, weneedtointroducealittlebitoftheoryandterminology. Letus\nagainconsidertheadditionoftwonumbers\u2013AandB \u2013representedasA ...A andB ...B\n32 1 32 1\nrespectively. Let us consider a bit pair \u2013 A and B . If it is equal to (0,0), then irrespective of\ni i\nthe carry in, the carry out is 0. In this case, the carry is absorbed.\nHowever, if the bit pair is equal to (0,1) or (1,0) then the value of the carry out is equal to\nthe value of the carry in. If the carry in is 0, then the sum is 1, and the carry out is 0. If the\ncarry in is 1, then the sum is 0, and the carry out is 1. In this case, the carry is propagated.\nLastly, if the bit pair is equal to (1,1), then the carry out is always equal to 1, irrespective\nof the carry in. In this case, a carry is generated.\nWe can thus define a generate(g ) and propagate(p ) function as follows:\ni i\ng = A .B (7.2)\ni i i\np = A \u2295B (7.3)\ni i i\nThe generate function captures the fact that both the bits are 1. The propagate function\ncaptures the fact that only one of the bits is 1. We can now compute the carry out C in\nout\nterms of the carry in C , g , and p . Note that by our case by case analysis, we can conclude\nin i i\nthat the carry out is equal to 1, only if a carry is either generated, or it is propagated. Thus,\nwe have:\nC = g +p .C (7.4)\nout i i in\nExample 95\nA = 0, B = 1. Let the input carry be C . Compute g , p and C .\ni i in i i out\nAnswer:\ng = A .B = 0.1 = 0\ni i i\np = A \u2295B = 0\u22951 = 1 (7.5)\ni i i\nC = g +p .C = C\nout i i in in 279 (cid:13)c Smruti R. Sarangi\nLet us now try to generalise the notion of generate and propagate functions to multiple bits.\nLet us consider a two bit system that has an input carry, and an output carry. Let the bit\npairs be numbered 1 and 2, where 2 represents the most significant bit. Let Ci represent the\nout\noutput carry obtained after adding the ith bit pair. Likewise, Ci is the input carry for the ith\nin\nbit pair. The output carry of the two bit system is thus equal to C2 . We have:\nout\nC2 = g +p .C1\nout 2 2 out\n= g +p .(g +p .C1 ) (7.6)\n2 2 1 1 in\n= (g +p .g )+p .p .C1\n2 2 1 2 1 in\nSimilarly, for a 3 bit system, we have:\nC3 = g +p .C2\nout 3 3 out\n= g +p .((g +p .g )+p .p .C1 ) (7.7)\n3 3 2 2 1 2 1 in\n= (g +p .g +p .p .g )+p .p .p .C1\n3 3 2 3 2 1 3 2 1 in\nFor a 4-bit system, we have:\nC4 = g +p .C3\nout 4 4 out\n= g +p .((g +p .g +p .p .g )+p .p .p .C1 ) (7.8)\n4 4 3 3 2 3 2 1 3 2 1 in\n= (g +p .g +p .p .g +p .p .p .g )+p .p .p .p .C1\n4 4 3 4 3 2 4 3 2 1 4 3 2 1 in\nLet us now try to derive a pattern, in these results (see Table 7.3).\n1 bit C1 = g + p .C1\nout 1 1 in\n(cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125)\nG1 P1\n2 bit C2 = (g +p .g )+p .p .C1\nout 2 2 1 2 1 in\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124)(cid:123)(cid:122)(cid:125)\nG2 P2\n3 bit C3 = (g +p .g +p .p .g )+p .p .p .C1 )\nout 3 3 2 3 2 1 3 2 1 in\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)\nG3 P3\n4 bit C4 = (g +p .g +p .p .g +p .p .p .g )+p .p .p .p .C1 )\nout 4 4 3 4 3 2 4 3 2 1 4 3 2 1 in\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)\nG4 P4\nn bit Cn = G +P .C1\nout n n in\nTable 7.3: Generate and propagate functions for multi bit systems\nWe observe that for a system of n bits, it is possible to define a generate function (G ) and\nn\na propagate function (P ). If we are able to somehow precompute these functions, then we can\nn\ngenerate C from C in a single step. However, as we can see from the example of the 4-bit\nout in\nsystem, the functions are fairly difficult to compute for large values of n. Let us now derive an\ninteresting property of the generate and propagate functions. (cid:13)c Smruti R. Sarangi 280\nLet us consider a sequence of n bits. Let us divide it into two parts 1...m and (m +\n1)...n. Let the generate and propagate functions for both the parts be (G ,P ) and\n1,m 1,m\n(G ,P ) respectively. Furthermore, let the generate and propagate functions for the\nm+1,n m+1,n\nentireblockbeG andP . Wewishtofindarelationshipbetweenthegenerateandpropagate\n1,n 1,n\nfunctions for the whole block with n bits and the functions for the sub blocks.\nC\nn\nsub\nC C\nout in\nm+1,n 1,m\nFigure 7.7: A block of n bits divided into two parts\nLet the carry out and carry in of the n bit block be C and C respectively. Let the carry\nout in\nbetween the two sub-blocks be C . See Figure 7.7. We have:\nsub\nC = G +P .C\nout m+1,n m+1,n sub\n= G +P .(G +P .C )\nm+1,n m+1,n 1,m 1,m in\n= G +P .G +P .P .C (7.9)\nm+1,n m+1,n 1,m m+1,n 1,m in\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)\nG1,n P1,n\n= G +P .C\n1,n 1,n in\nThus, for a block of n bits, we can easily compute G and P from the corresponding\n1,n 1,n\nfunctions of its sub blocks. This is a very powerful property and is the basis of the carry\nlookahead adder.\nCarry Lookahead Adder \u2013 Stage I\nThecarrylookaheadadder\u2019soperationisdividedintotwostages. Inthefirststage, wecompute\nthe generate and propagate functions for different subsequences of bits. In the next stage, we\nuse these functions to generate the result.\nThe diagram for the first stage is shown in Figure 7.8. Like the carry select adder, we\ndivide bit pairs into blocks. In this diagram, we have considered a block size equal to 2. In the\nfirst level, we compute the generate and propagate functions for each block. We build a tree\nof (G,P) circuits(blocks) as follows. Each (G,P) block in level n takes as input the generate\nand propagate functions of two blocks in level n\u22121. Thus, at each level the number of (G,P)\nblocks decreases by a factor of 2. For example, the first (G,P) block in level 1 processes the\nbit pairs (1,2). Its parent processes the bit pairs (1...4), and so on. The ranges are shown in\nFigure 7.8. We create a tree of (G,P) blocks in this fashion.\nFor a n bit input, there are O(log(n)) levels. In each level, we are doing a constant amount\nof work since each (G,P) block is only processing the inputs from two other blocks. Hence, the\ntime complexity of this stage is equal to O(log(n)). 281 (cid:13)c Smruti R. Sarangi\nn\nBlock 16 Block 1\no 32 31 30 29 4 3 2 1 level 0\ni\nt\nG,P G,P G,P G,P level 1\na 32-31 30-29 4-3 2-1\nt\nu G,P G,P level 2\n32-29 4-1\np\nG,P G,P G,P G,P level 3\nm\n32-25 24-17 16-9 8-1\no G,P G,P level 4\n32-17 16-1\nC\nG,P level 5\n32-1\nFigure 7.8: Carry Lookahead Adder \u2013 Stage I\nCarry Lookahead Adder \u2013 Stage II\nIn this stage, we use the information generated in Stage I to compute the final sum bits, and\nthe carry out. The block diagram for the second stage is shown in Figure 7.9.\nResult Bits\n2-bit RC Adder 2-bit RC Adder 2-bit RC Adder 2-bit RC Adder 2-bit RC Adder\nn 32 31 30 29 18 17 4 3 2 1 level 0\no\ni G,P G,P G,P G,P G,P c1 level 1\nt 32-31 30-29 18-17 4-3 2-1 in\na\nG,P G,P G,P G,P c1 level 2\nt 32-29 28-25 20-17 4-1 in\nu\np G,P G,P G,P G,P c1 level 3\n32-25 24-17 16-9 8-1 in\nm\nG,P block\nG,P G,P c1 level 4\no c out G,P c in 32-17 16-1 in\nr2- r1 32\nC c G,P c1 level 5\nout 32-1 in\nFigure 7.9: Carry Lookahead Adder \u2013 Stage II\nLet us first focus at the rightmost (G,P) blocks in each level. The ranges for each of these (cid:13)c Smruti R. Sarangi 282\nblocks start at 1. They take the input carry, C1 , as input, and then calculate the output\nin\ncarry for the range of bit pairs that they represent as C = G+P.C1 . When we are adding\nout in\ntwo numbers, the input carry at the first bit is typically 0. However, some special instructions\n(ADC in ARM) can consider a non-zero value of C1 also.\nin\nEach (G,P) block with a range (r2,r1) (r2 > r1), is connected to all (G,P) blocks that have\na range of the form (r3,r2+1). The output carry of the block is equal to the input carry of\nthose blocks. To avoid excessive clutter in the diagram (Figure 7.9), we show the connections\nfor only the (G,P) block with range (16-1) using solid lines. Each block is connected to the\nblock to its left in the same level and to one (G,P) block in every lower level.\nThe arrangement of (G,P) blocks represents a tree like computation where the correct carry\nvalues propagate from different levels to the leaves. The leaves at level 0, contain a set of 2-bit\nripple carry(RC) adders that compute the result bits by considering the correct value of the\ninput carry. We show an example in Figure 7.9 of the correct carry in value propagating from\nthe block with range (16-1) to the 2-bit adder representing the bits 31 and 32. The path is\nshown using dotted lines.\nIn a similar manner, carry values propagate to every single ripple carry adder at the zeroth\nlevel. Theoperationcompletesoncealltheresultbitsandtheoutputcarryhavebeencomputed.\nThe time complexity of this stage is also O(log(n)) because there are O(log(n)) levels in\nthe diagram and there is a constant amount of work done per level. This work comprises of\ncomputing C and propagating it to (G,P) blocks at lower levels.\nout\nHence, the total time complexity of the carry lookahead adder is O(log(n)) .\nWay Point 5\nTime complexities of different adders:\n\u2022 Ripple Carry Adder: O(n)\n\u221a\n\u2022 Carry Select Adder: O( n)\n\u2022 Carry Lookahead Adder: O(log(n))\n7.2 Multiplication\n7.2.1 Overview\nLet us now consider the classic problem of binary multiplication. Similar to addition, let us\nfirst look at the most naive way of multiplying two decimal numbers. Let us try to multiply 13\ntimes 9. In this case, 13 is known as the multiplicand and 9 is known as the multiplier, and 117\nis the product.\nFigure 7.10(a) shows the multiplication in the decimal number system, and Figure 7.10(b)\nshows the multiplication in binary. Note that multiplying two binary numbers can be done\nexactly the same way as decimal numbers. We need to consider each bit of the multiplier from\nthe least significant position to the most significant position. If the bit is 1, then we write the 283 (cid:13)c Smruti R. Sarangi\n1 3 1 1 0 1\n9 1 0 0 1\n1 1 7 1 1 0 1\n0 0 0 0\n(a) Partial sums\n0 0 0 0\n1 1 0 1\n1 1 1 0 1 0 1\n(b)\nFigure 7.10: Multiplication in decimal and binary\nvalue of the multiplicand below the line, otherwise we write 0. For each multiplier bit, we shift\nthe multiplicand progressively one bit to the left. The reason for this is that each multiplier bit\nrepresents a higher power of two. We call each such value a partial sum (see Figure 7.10(b)).\nIf the multiplier has m bits, then we need to add m partial sums to obtain the product. In\nthis case the product is 117 in decimal and 1110101 in binary. The reader can verify that they\nactually represent the same number. Let us define another term called the partial product for\nease of representation later. It is the sum of a contiguous sequence of partial sums.\nDefinition 56\nPartial sum It is equal to the value of the multiplicand left shifted by a certain number of\nbits, or it is equal to 0.\nPartial product It is the sum of a set of partial sums.\nIn this example, we have considered unsigned numbers. What about signed numbers? In\nSection 2.3.4, we proved that multiplying two 2\u2019s complement signed n bit binary numbers, and\nconstrainingtheresulttonbitswithoutanyconcernforoverflows,isnotdifferentfromunsigned\nmultiplication. We need to just multiply the 2\u2019s complement numbers without bothering about\nthe sign. The result will be correct.\nLetusnowconsidertheissueofoverflowsinmultiplication. Ifwearemultiplyingtwosigned\n32-bit values, the product can be as large as (2\u221231)2 = 2\u221262. There will thus be an overflow if\nwe try to save the result in 32 bits. We need to keep this in mind. If we desire precision, then\nit is best to allot 64 bits for storing the result of 32-bit multiplication. Let us now look at a\nnaive approach for multiplying two 32-bit numbers by using an iterative multiplier. (cid:13)c Smruti R. Sarangi 284\nMultiplicand(N)\nU V\nFigure 7.11: Iterative Multiplier\n7.2.2 Iterative Multiplier\nIn this section, we present the design of an iterative multiplier (see Figure 7.11) that multiplies\ntwo signed 32-bit numbers to produce a 64-bit result. We cannot treat the numbers as unsigned\nanymore and the algorithm thus gets slightly complicated. We use a 33-bit register U, and a\n32-bit register V as shown in Figure 7.11. The multiplier is loaded into V at the beginning.\nThe multiplicand is stored separately in register N. The size of the register N is equal to 33\nbits, and we store the multiplicand in it by extending its sign by 1 position. The two registers\nU and V are treated as one large register for the purposes of shifting. If we perform a right\nshift on U and V, then the value shifted out of U, becomes the MSB of V. We have an adder\nthat adds the multiplicand to the current value of U, and updates U. U is initialised to 0. Let\nus represent the multiplicand by N, the multiplier by M, and the product by P. We need to\ncompute P = MN.\nThealgorithmusedbytheiterativemultiplierisverysimilartothemultiplicationalgorithm\nthat we learnt in elementary school. We need to consider each bit of the multiplier in turn and\nadd a shifted version of the multiplicand to the partial product if the bit is 1. The algorithm 285 (cid:13)c Smruti R. Sarangi\nis as follows:\nAlgorithm 1: Algorithm to multiply two 32-bit numbers and produce a 64-bit result\nData: Multiplier in V, U = 0, Multiplicand in N\nResult: The lower 64 bits of UV contains the product\n1 i \u2190 0\n2 for i < 32 do\n3 i \u2190 i + 1\n4 if LSB of V is 1 then\n5 if i < 32 then\n6 U \u2190 U +N\n7 end\n8 else\n9 U \u2190 U \u2212N\n10 end\n11 end\n12 UV \u2190 UV (cid:29) 1 (arithmetic right shift)\n13 end\nLet us now try to understand how this algorithm works. We iterate for 32 times to consider\neach bit of the multiplier. The multiplier is initially loaded into register V.\nNow, if the LSB of V is 1 (Line 4), then we add the multiplicand N to U and save the result\nin U. This basically means that if a bit in the multiplier is equal to 1, then we need to add\nthe multiplicand to the already accumulated partial product. The partial product is a running\nsum of the shifted values of the multiplicands. It is initialised to 0. In the iterative algorithm,\nthe part of UV that does not contain the multiplier, contains the partial product. We then\nshift UV one step to the right (Line 12). The reason for this is as follows. In each step we\nactually need to shift the multiplicand 1 bit to the left and add it to the partial product. This\nis the same as not shifting the multiplicand but shifting the partial product 1 bit to the right\nassuming that we do not lose any bits. The relative displacement between the multiplicand and\nthe partial product remains the same.\nIf in any iteration of the algorithm, we find the LSB of V to be 0, then nothing needs to be\ndone. We do not need to add the value of the multiplicand to the partial product. We simply\nneed to shift UV one position to the right using an arithmetic right shift operation.\nNote that till the last step we assume that the multiplier is positive. If in the last step we\nsee that the multiplier is not positive (MSB is 1), then we need to subtract the multiplicand\nfrom U (Line 9). This follows directly from Theorem 2.3.4.2. The theorem states that the value\nof the multiplier (M) in the 2\u2019s complement notation is equal to (\u2212M 2n\u22121 +(cid:80)n\u22121M 2i\u22121).\nn i=1 i\nHere M is the ith bit of the multiplier, M. In the first n\u22121 iterations, we effectively multiply\ni\nthe multiplicand with (cid:80)n\u22121M 2i\u22121. In the last iteration, we take a look at the MSB of the\ni=1 i\nmultiplier, M . If it is 0, then we need not do anything. If it is 1, then we need to subtract\nn\n2n\u22121 \u00d7N from the partial product. Since the partial product is shifted to the right by n\u22121\npositions with respect to the multiplicand, the multiplicand is effectively shifted n\u22121 positions\nto the left with respect to the partial product. To subtract 2n\u22121 \u00d7N to the partial product,\nwe need to simply subtract N from register U, which is our last step. (cid:13)c Smruti R. Sarangi 286\nImportant Point 8\nWe assume that register U is 33 bits wide. We did this to avoid overflows while adding or\nsubtracting N from U. Let us consider U and N again. |N| \u2264 231 because N is essentially\na 32-bit number. For our induction hypothesis, let us assume that |U| \u2264 231 (true for the\nbase case, U = 0). Thus |U \u00b1N| \u2264 232. Hence, if we store both the numbers and their sum\nin 33-bit registers, we will never have overflows while adding or subtracting them. Note\nthat we could have had overflows, if we would have used just 32 bits. Now, after the shift\noperation, the value in U is divided by 2. Since U \u00b1 N is assigned to U, and we have\nestablished that |U\u00b1N| \u2264 232, we can prove that |U| \u2264 231. Thus, our induction hypothesis\nholds, and we can thus conclude that during the operation of our algorithm, we shall never\nhave an overflow. The absolute value of the product can at the most be 231 \u00d7231 = 262.\nHence, the product can fit in 64 bits(proved in Section 7.2.1), and we thus need to only\nconsider the lower 64 bits of the UV register.\nExamples\nExample 96\nMultiply 2\u00d73 using an iterative multiplier. Assume a 4-bit binary 2\u2019s complement number\nsystem. Let 2 (0010 ) be the multiplicand and let 3 (0011 ) be the multiplier. For each\n2 2\niteration show the values of U and V just before the right shift on Line 12, and just after\nthe right shift.\nAnswer: 287 (cid:13)c Smruti R. Sarangi\nMultiplicand (N)\n0010 2\nMultiplier (M) 0011 3\nU V\nbeginning: 00000 0011\nbefore shift: 00010 0011\n1\nafter shift: 00001 0001\nbefore shift: 00011 0001\n2\nafter shift: 00001 1000\nbefore shift: 00001 1000\n3\nafter shift: 00000 1100\nbefore shift: 00000 1100\n4\nafter shift: 00000 0110\nProduct(P) 0110 6\nExample 97\nMultiply 3 \u00d7 (\u22122) using an iterative multiplier. Assume a 4-bit binary 2\u2019s complement\nnumber system. Let 3 (0011 ) be the multiplicand and let -2 (1110 ) be the multiplier. For\n2 2\neach iteration show the values of U and V just before the right shift on Line 12, and just\nafter the right shift.\nAnswer: (cid:13)c Smruti R. Sarangi 288\nMultiplicand (N) 0011 3\nMultiplier (M) 1110 -2\nU V\nbeginning: 00000 1110\nbefore shift: 00000 1110\n1\nafter shift: 00000 0111\nbefore shift: 00011 0111\n2\nafter shift: 00001 1011\nbefore shift: 00100 1011\n3\nafter shift: 00010 0101\nbefore shift: 11111 0101\n4\nafter shift: 11111 1010\nProduct(P) 1010 -6\nTime Complexity\nIf we are performing n bit multiplication, then there are n iterations of the loop, and each\niteration performs one addition at the most. This takes O(log(n)) time. Hence, the total time\nrequired is O(nlog(n)).\n7.2.3 Booth Multiplier\nThe iterative multiplier is a simple algorithm, yet it is slow. It is definitely not as fast as\naddition. Letustrytospeeditupbymakingasimplealteration. Thistrickwillnotchangethe\nasymptotictimecomplexityofthealgorithm. However, inpracticetheprocessofmultiplication\nwill become significantly faster. This algorithm is known as the Booth multiplication algorithm\nand has been used for designing fast multipliers in a lot of processors.\nWe observe that if a bit in the multiplier is 0, then nothing needs to be done other than a\nshift in the iterative multiplier. The complexity arises when a bit is 1. Let us assume that the\nmultiplier contains a run of 1s. It is of the form - 0000111100. Let the run of 1s be from the\nith to the jth position (i \u2264 j). The value of the multiplier M is thus: 289 (cid:13)c Smruti R. Sarangi\nk=j\n(cid:88)\nM = 2k\u22121 = 2j \u22122i\u22121 (7.10)\nk=i\nNow, the iterative multiplier will perform j\u2212i+1 additions. This is not required as we can\nsee from Equation 7.10. We just need to do one subtraction when we are considering the ith\nbit, and do one addition when we are considering the (j+1)th bit. We can thus replace j\u2212i+1\nadditions with one addition and one subtraction. This insight allows us to reduce the number\nof additions if there are long runs of 1s in the 2\u2019s complement notation of the multiplier. If\nthe multiplier is a small negative number, then it fits this pattern. It will have a long run of\n1s especially in the most significant positions. Even otherwise, most of the numbers that we\nencounter will at least have some runs of 1s. The worst case arises, when we have a number of\nthe form: 010101... . This is a very rare case.\nIf we consider our basic insight again, then we observe that we need to consider bit pairs\nconsisting of the previous and the current multiplier bit. Depending on the bit pair we need to\nperform a certain action. Table 7.4 shows the actions that we need to perform.\n(current value,previous value) Action\n0,0 -\n1,0 subtract multiplicand from U\n1,1 -\n0,1 add multiplicand to U\nTable 7.4: Actions in the Booth multiplier\nIf the current and previous bits are (0,0) respectively, then we do not need to do anything.\nWe need to just shift UV and continue. Similarly, if the bits are (1,1), nothing needs to be\ndone. However, if the current bit is 1, and the previous bit was 0, then a run of 1s is starting.\nWe thus need to subtract the value of the multiplicand from U. Similarly, if the current bit is\n0, and the previous bit was 1, then a run of 1s has just ended. In this case, we need to add the (cid:13)c Smruti R. Sarangi 290\nvalue of the multiplicand to U.\nAlgorithm 2: Booth\u2019s Algorithm to multiply two 32-bit numbers to produce a 64-bit\nresult\nData: Multiplier in V, U = 0, Multiplicand in N\nResult: The lower 64 bits of UV contain the result\n1 i \u2190 0\n2 prevBit \u2190 0\n3 for i < 32 do\n4 i \u2190 i + 1\n5 currBit \u2190 LSB of V\n6 if (currBit,prevBit) = (1,0) then\n7 U \u2190 U \u2212N\n8 end\n9 else if (currBit,prevBit) = (0,1) then\n10 U \u2190 U +N\n11 end\n12 prevBit \u2190 currBit\n13 UV \u2190 UV (cid:29) 1 (arithmetic right shift)\n14 end\nThe Booth\u2019s algorithm is shown in Algorithm 2. Here, also, we assume that U is 33 bits\nwide,andV is32bitswide. Weiteratefor32times,andconsiderbitpairs(currentbit,previous\nbit). For (0,0) and (1,1), we do not need to perform any action, else we need to perform an\naddition and subtraction.\nProof of Correctness*\nLet us try to prove that the Booth\u2019s algorithm produces the same result as the iterative algo-\nrithm for a positive multiplier.\nThere are two cases. The multiplier (M) can either be positive or negative. Let us consider\nthe case of the positive multiplier first. The MSB of a positive multiplier is 0. Now, let us\ndivide the multiplier into several sequences of contiguous 0s and 1s. For example, if the number\nis of the form: 000110010111. The sequences are: 000, 11, 00, 1, 0, and 111. For a run of 0s,\nboth the multipliers (Booth\u2019s and iterative) produce the same result result because they simply\nshift the UV register 1 step to the right.\nFor a sequence of continuous 1s, both the multipliers also produce the same result because\nthe Booth multiplier replaces a sequence of additions with an addition and a subtraction ac-\ncording to Equation 7.10. The only special case arises for the MSB bit, when the iterative\nmultiplier may subtract the multiplicand. In this case, the MSB is 0, and thus no special cases\narise. Each run of continuous 0s and 1s in the multiplier is accounted for in the partial product\ncorrectly. Therefore, we can conclude that the final result of the Booth multiplier is the same\nas that of a regular iterative multiplier.\nLet us now consider a negative multiplier M. Its MSB is 1. According to Theorem 2.3.4.2,\nM = \u22122n\u22121+(cid:80)n\u22121M 2i\u22121. Let M(cid:48) = (cid:80)n\u22121M 2i\u22121. Hence, for a negative multiplier (M):\ni=1 i i=1 i\nM = M(cid:48)\u22122n\u22121 (7.11) 291 (cid:13)c Smruti R. Sarangi\nM(cid:48) is a positive number (MSB is 0). Note that till we consider the MSB of the multiplier,\nthe Booth\u2019s algorithm does not know if the multiplier is equal to M or M(cid:48).\nNow, let us split our argument into two cases. Let us consider the MSB bit (nth bit), and\nthe (n\u22121)th bit. This bit pair can either be 10, or 11.\nCase 10: Let us divide the multiplier M into two parts as shown in Equation in Equation 7.11.\nThe first part is a positive number M(cid:48), and the second part is \u22122n\u22121, where M = M(cid:48) \u22122n\u22121.\nSincethetwoMSBbitsofthebinaryrepresentationofM are10,wecanconcludethatthebinary\nrepresentation of M(cid:48) contains 00 as its two MSB bits. Recall that the binary representation of\nM and M(cid:48) contain the same set of n\u22121 least significant bits, and the MSB of M(cid:48) is always 0.\nSince the Booth multiplier was proven to work correctly for positive multipliers, we can\nconclude that the Booth multiplier correctly computes the partial product as N \u00d7M(cid:48) in the\nfirst (n\u22121) iterations. The proof of this fact is as follows. Till the end of (n\u22121) iterations,\nwe are not sure if the MSB is 0 or 1. Hence, we do not know if we are multiplying N with M\nor M(cid:48). The partial product will be the same in both the cases. If we were multiplying N with\nM(cid:48), then no action will be taken in the last step because the two MSB bits of M(cid:48) are 00. This\nmeans that in the second last step ((n\u22121) iterations), the partial product contains NM(cid:48). We\ncan similarly prove that the partial product computed by the iterative multiplier after (n\u22121)\niterations is equal to NM(cid:48) because the MSB of M(cid:48) is 0.\nHence,tillthispoint,boththealgorithmscomputethesamepartialproduct,oralternatively\nhave the same contents in the U and V registers. In the last step, both the algorithms find\nout that the MSB is 1. The iterative algorithm subtracts the multiplicand(N) from U, or\nalternatively subtracts N \u00d7 2n\u22121 from the partial product. The reason that we treat the\nmultiplicand as shifted by n \u2212 1 places is because the partial product in the last iteration\nspans the entire U register and n\u22121 bits of the V register. Now, when we add or subtract the\nmultiplicand(N)toU, effectively, weareaddingN shiftedbyn\u22121placestotheleft. Hence, the\niterativemultipliercorrectlycomputestheproductasM(cid:48)N\u22122n\u22121N = MN (seeEquation7.11).\nThe Booth multiplier also does the same in this case. It sees a 0 \u2192 1 transition. It subtracts N\nfrom U, which is exactly the same step as taken by the iterative multiplier. Thus, the operation\nof the Booth multiplier is correct in this case (same result as the iterative multiplier).\nCase 11: Let us again consider the point at the beginning of the nth iteration. At this point\nof time, the partial product computed by the iterative algorithm is M(cid:48)N, whereas the partial\nproduct computed by the Booth multiplier is different because the two MSB bits of M(cid:48) are 0\nand 1, respectively. Let us assume that we were originally multiplying N with M(cid:48), then the\nMSB would have been 0, and this fact would have been discovered in the last iteration. The\nBooth\u2019s algorithm would have then added 2n\u22121N to obtain the final result in the last step\nbecause of a 1 \u2192 0 transition. Hence, after the (n \u2212 1)th iteration, the partial product of\nthe Booth multiplier is equal to M(cid:48)N \u2212 2n\u22121N. Note that till the last iteration, the Booth\nmultiplier does not know whether the multiplier is M or M(cid:48).\nNow, let us take a look at the last iteration. In this iteration both the algorithms find out\nthat the MSB is 1. The iterative multiplier subtracts 2n\u22121N from the partial product, and\ncorrectly computes the final product as MN = M(cid:48)N \u22122n\u22121N. The Booth multiplier finds the\ncurrent and previous bit to be 11, and thus does not take any action. Hence, its final product\nis equal to the partial product computed at the end of the (n\u22121)th iteration, which is equal to\nM(cid:48)N \u22122n\u22121N. Therefore, in this case also the outputs of both the multipliers match.\nWe have thus proved that the Booth multiplier works for both positive and negative multi- (cid:13)c Smruti R. Sarangi 292\npliers.\nImportant Point 9\nHere, we use a 33-bit U register to avoid overflows. Let us show an example of an overflow,\nif we would have used a 32-bit U register. Assume that we are trying to multiply \u2212231\n(multiplicand) with -1(multiplier). We will need to compute 0\u2212N in the first step. The\nvalue of U should be equal to 231; however, this number cannot be represented with 32 bits.\nHence, we have an overflow. We do not have this issue when we use a 33-bit U register.\nMoreover, we can prove that with a 33-bit U register, the additions or subtractions in the\nalgorithm will never lead to an overflow (similar to the proof for iterative multiplication).\nExample 98\nMultiply 2 \u00d7 3 using a Booth multiplier. Assume a 4-bit binary 2\u2019s complement number\nsystem. Let 3 (0011 ) be the multiplicand and let 2 (0010 ) be the multiplier. For each\n2 2\niteration show the values of U and V just before and after the right shift.\nAnswer: 293 (cid:13)c Smruti R. Sarangi\nMultiplicand (N) 00011 3\nMultiplier (M) 0010 2\nU V\nbeginning: 00000 0010\nbefore shift: 00000 0010\n1 00\nafter shift: 00000 0001\nbefore shift: 11101 0001\n2\n10\nafter shift: 11110 1000\nbefore shift: 00001 1000\n3\n01\nafter shift: 00000 1100\nbefore shift: 00000 1100\n4\n00\nafter shift: 00000 0110\nProduct(P) 0110 6\nExample 99\nMultiply 3\u00d7(\u22122) using a Booth multiplier. Assume a 4-bit binary 2\u2019s complement number\nsystem. Let 3 (0011 ) be the multiplicand and let -2 (1110 ) be the multiplier. For each\n2 2\niteration show the values of U and V just before and after the right shift.\nAnswer: (cid:13)c Smruti R. Sarangi 294\nMultiplicand (N) 00011 3\nMultiplier (M) 1110 -2\nU V\nbeginning: 00000 1110\nbefore shift: 00000 1110\n1\n00\nafter shift: 00000 0111\nbefore shift: 111010111\n2 10\nafter shift: 11110 1011\nbefore shift: 11110 1011\n3\n11\nafter shift: 11111 0101\nbefore shift: 111110101\n4 11\nafter shift: 111111010\nProduct(P) 1010 -6\n7.2.4 An O(log(n)2) Time Algorithm\nLetusmakeourlifeslightlyeasiernow. Letusmultiplytwonbitnumbers,andsavetheproduct\nas also a n bit number. Let us ignore overflows, and concentrate only on performance. The\nissueofdetectingoverflowsinahighperformancemultiplierisfairlycomplex, andisbeyondthe\nscope of this book. Using our results from Section 2.3.4, we use simple unsigned multiplication\nto compute the product of signed numbers. If there are no overflows then the result is correct.\nLet us take a look at the problem of multiplication again. We basically consider each bit\nof the multiplier in turn, and multiply it with a shifted version of the multiplicand. We obtain\nn such partial sums. The product is the sum of the n partial sums. Generating each partial\nsum is independent of the other. This process can be performed in parallel in hardware. To\ngenerate the ith partial sum, we need to simply compute an AND operation between the ith bit\nof the multiplier and each bit of the multiplicand. This takes O(1) time.\nNow, we can add the n partial sums(P1...Pn) in parallel using a tree of adders as shown\nin Figure 7.12. There are O(log(n)) levels. In each level we are adding two O(n) bit numbers;\nhence, each level takes O(log(n)) time. The total time requirement is thus O(log(n)2). By 295 (cid:13)c Smruti R. Sarangi\nPnPn-1 Pn-2Pn-3 P4P3 P2P1\nlog(n) levels\nFinal product\nFigure 7.12: Tree of adders for adding partial sums\nexploiting the inherent parallelism, we have significantly improved the time from O(nlog(n))\nto O(log(n)2). It turns out that we can do even better, and get an O(log(n)) time algorithm.\nA\nD\nCarry\nB save\nE\nadder\nC\nFigure 7.13: Carry Save Adder\n7.2.5 Wallace Tree Multiplier\nBefore, we introduce the Wallace tree multiplier, let us introduce the carry save adder. A carry\nsave adder adds three numbers, A, B, C, and produces two numbers D, and E such that:\nA+B+C = D+E(see Figure 7.13). We will extensively use carry save adders in constructing\nthe Wallace tree multiplier that runs in O(log(n)) time. (cid:13)c Smruti R. Sarangi 296\nCarry Save Adder\nLet us consider the problem of adding three bits a, b, and c. The sum can range from 0 to 3.\nWe can express all numbers between 0 to 3 in the form 2d+e, where (d,e) \u2208 [0,1]. Using this\nrelationship, we can express the sum of three numbers as the sum of two numbers as follows:\nn n n\n(cid:88) (cid:88) (cid:88)\nA+B+C = A 2i\u22121+ B 2i\u22121+ C 2i\u22121\ni i i\ni=1 i=1 i=1\nn\n(cid:88)\n= (A +B +C )2i\u22121\ni i i\ni=1\nn\n(cid:88)\n= (2D +E )2i\u22121 (7.12)\ni i\ni=1\nn n\n(cid:88) (cid:88)\n= D 2i+ E 2i\u22121\ni i\ni=1 i=1\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)\nD E\n= D+E\nThus, we have A+B +C = D+E. The question is how to compute the bits D , and E\ni i\nsuch that A +B +C = 2D +E . This is very simple. We note that if we add A , B , and C ,\ni i i i i i i i\nwe get a two bit result, where s is the sum bit and c is the carry bit. The result of the addition\ncan be written as 2\u00d7c+s. We thus have two equations as follows:\nA +B +C = 2D +E (7.13)\ni i i i i\nA +B +C = 2c+s (7.14)\ni i i\nIf we set D to the carry bit and E to the sum bit, then we are done! Now, E is equal to\ni i\n(cid:80)n E 2i\u22121. We can thus obtain E by concatenating all the E bits. Similarly, D is equal to\ni=1 i i\n(cid:80)n D 2i. D can be computed by concatenating all the D bits and shifting the number to\ni=1 i i\nthe left by 1 position.\nThe hardware complexity of a carry save adder is not much. We need n full adders to\ncompute all the sum and carry bits. Then, we need to route the wires appropriately to produce\nD and E. The asymptotic time complexity of a carry save adder is O(1) (constant time).\nAddition of n Numbers with Carry Save Adders\nWe can use carry save adders to add n partial sums (see Figure 7.14). In the first level, we\ncan use a set of n\/3 carry save adders to reduce the sum of n partial sums to a sum of 2n\/3\nnumbers in the second level. If we use 2n\/9 carry save adders in the second level, then we will\nhave 4n\/9 numbers in the third level, and so on. In every level the set of numbers gets reduced\nby a factor of 2\/3. Thus, after O(log (n)) levels, there will only be two numbers left. Note\n3\/2\nthat O(log (n) is equivalent to O(log(n)). Since each stage takes O(1) time because all the\n3\/2\ncarry save adders are working in parallel, the total time taken up till now is O(log(n)). 297 (cid:13)c Smruti R. Sarangi\nPn Pn-1Pn-2 Pn-3Pn-4 Pn-5 P6 P5 P4 P3 P2 P1\nCSA CSA CSA CSA\nCSA CSA log (n) levels\n3\/2\nCSA\nFinal product\nFigure 7.14: Wallace Tree Multiplier\nIn the last stage, when we have just two numbers left, we cannot use a carry save adder\nanymore. We can use a regular carry lookahead adder to add the two numbers. This will\ntake O(log(n)) time. Hence, the total time taken by the Wallace tree multiplier is O(log(n)+\nlog(n)) = O(log(n)). In terms of asymptotic time complexity, this is the fastest possible\nmultiplier. It is possible to reduce the number of full adders by slightly modifying the design.\nThis is known as the Dadda multiplier. The reader can refer to [Wikipedia, ] for further\ninformation on this topic.\n7.3 Division\n7.3.1 Overview\nLet us now look at integer division. Unfortunately, unlike addition, subtraction, and multipli-\ncation, division is a significantly slower process. Any division operation can be represented as\nfollows:\nN = DQ+R (7.15)\nHere,N isthedividend,D isthedivisor,Qisthequotient,andRistheremainder. Division\nalgorithms assume that the divisor and dividend are positive. The process of division needs to\nsatisfy the following properties.\nProperty 1 R < D, and R \u2265 0. (cid:13)c Smruti R. Sarangi 298\nProperty 2 Q is the largest positive integer that satisfies Equation 7.15.\nIf we wish to divide negative numbers, then we need to first convert them to positive\nnumbers, perform the division, and then adjust the sign of the quotient and remainder. Some\narchitectures try to ensure that the remainder is always positive. In this case, it is necessary to\ndecrement the quotient by 1, and add the divisor to the remainder to make it positive.\nLet us focus on the core problem, which is to divide two n bit positive numbers. The MSB\nis the sign bit, which is 0. Now, DQ = (cid:80)n DQ 2i\u22121. We can thus write:\ni=1 i\nN = DQ+R\n= DQ +DQ 2n\u22121+R\n1...n\u22121 n\n(N \u2212DQ 2n\u22121) = DQ +R (7.16)\nn 1...n\u22121\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)\nN(cid:48) Q(cid:48)\nN(cid:48) = DQ(cid:48)+R\nWe have thus reduced the original problem of division into a smaller problem. The original\nproblem was to divide N by D. The reduced problem is to divide N(cid:48) = N \u2212DQ 2n\u22121 by D.\nn\nThe remainder for both the problems is the same. The quotient, Q(cid:48), for the reduced problem\nhas the same least significant n\u22121 bits as the original quotient, Q. The nth bit of Q(cid:48) is 0.\nTo create the reduced problem it is necessary to find Q . We can try out both the choices\nn\n\u2013 0 and 1. We first try 1. If N \u2212D2n\u22121 \u2265 0, then Q = 1 (Property 1 and 2). Otherwise, it is\nn\n0.\nOnce, we have created the reduced problem, we can proceed to further reduce the problem\ntill we have computed all the quotient bits. Ultimately, the divided will be equal to the re-\nmainder, R, and we will be done. Let us now illustrate an algorithm that precisely follows the\nprocedure that we have just outlined.\nDivisor(D)\n(U-D)\nU V\nFigure 7.15: Iterative divider\n7.3.2 Restoring Division\nLet us consider a similar setup as the iterative multiplier to divide two positive 32-bit numbers.\nThe divisor is stored in a 32-bit register called D. We have a 33 bit register U, and a 32-bit\nregister V. If we left shift U and V, then the value shifted out of V, is shifted in to U. U is 299 (cid:13)c Smruti R. Sarangi\ninitialised to 0, and V is initialised to hold the dividend (see Figure 7.15). Note that the size\nof U is equal to 33 bits to avoid overflows (similar to the case of the iterative multiplier).\nAlgorithm 3: Restoring algorithm to divide two 32-bit numbers\nData: Divisor in D, Dividend in V, U = 0\nResult: U contains the remainder (lower 32 bits), and V contains the quotient\n1 i \u2190 0\n2 for i < 32 do\n3 i \u2190 i + 1\n\/* Left shift UV by 1 position *\/\n4 UV \u2190 UV << 1\n5 U \u2190 U - D\n6 if U \u2265 0 then\n7 q \u2190 1\n8 end\n9 else\n10 U \u2190 U + D\n11 q \u2190 0\n12 end\n\/* Set the quotient bit *\/\n13 LSB of V \u2190 q\n14 end\nAlgorithm 3 follows the discussion that we had in Section 7.3.1. We shall see that each\niteration of the algorithm reduces the problem according to Equation 7.16. Let us prove its\ncorrectness.\nProof of Correctness*\nTo start out we iterate 32 times for each bit of the dividend (Lines 2 to 14). Let us consider\nthe first iteration. At the beginning, the value in the combined register UV is equal to the\nvalue of the dividend N. The first step is to shift UV to the left by 1 position in Line 4. Since\nthe dividend is originally loaded into register V, we are shifting the dividend to the left by 1\nposition. The next step is to subtract the divisor from register U in Line 5. If U \u2212D \u2265 0, then\nwe set the MSB of the quotient to 1 (Line 7), otherwise we add D back to U in Line 10, and\nset the MSB of the quotient to 0.\nWewishtouseEquation7.16toreducetheproblemineveryiteration. Equation7.16states\nthat the new dividend(N(cid:48)) is equal to:\nN(cid:48) = N \u22122n\u22121DQ (7.17)\nn\nQ is the MSB of the quotient here. The divisor and remainder stay the same. The last n\u22121\nn\nbits of the new quotient match those of the old quotient.\nWewishtoprovethatthevalueofUV attheendofthefirstiterationisequalto(2N(cid:48))(ignoring\nquotient bits) such that we can reduce the problem according to Equation 7.16. Let us consider\nthe value stored in UV. Just after executing Line 4, it contains twice the dividend \u2013 2N \u2013\nbecause we shifted UV by 1 position to the left. Now, we are subtracting D from the upper n (cid:13)c Smruti R. Sarangi 300\nbits of UV. In effect, we are subtracting 2nD. Hence, after Line 5, UV contains UV \u22122nD.\nWe have:\nUV \u22122nD = 2N \u22122nD = 2\u00d7(N \u22122n\u22121D) (7.18)\nSubsequently, we test the sign of U\u2212D in Line 6. If U\u2212D is positive or zero, then it means\nthat UV is greater than 2nD because V \u2265 0. If U \u2212D is negative, then let U +\u2206 = D, where\n\u2206 \u2265 1. We have:\nUV \u22122nD = 2nU +V \u22122nD\n= (U \u2212D)2n+V (7.19)\n= V \u2212\u2206\u00d72n\nNow, V < 2n. Hence, V < \u2206\u00d72n, and thus UV \u22122nD is negative. We thus observe that\nthesignofU\u2212D isthesameasthesignofUV \u22122nD, whichissameasthesignof(N\u22122n\u22121D).\nsign(U \u2212D) = sign(N \u22122n\u22121D) (7.20)\nNow, for reducing the problem, if we observe that U \u2212D \u2265 0, then N \u22122n\u22121D \u2265 0. Hence,\nwe can set Q to 1, and set the new dividend to N(cid:48) = N\u22122n\u22121DQ , and also conclude that at\nn n\nthe end of the iteration UV contains 2N(cid:48)(Line 5 and 7). If U \u2212D < 0, then we cannot set Q\nn\nto 1. N(cid:48) will become negative. Hence, the algorithm sets Q to 0 in Line 11 and adds D back\nn\nto U. The value of UV is thus equal to 2N. Since Q = 0, we have N = N(cid:48)(Equation 7.17).\nn\nIn both the cases, the value of UV at the end of the iteration is 2N(cid:48). We thus conclude that in\nthe first iteration, the MSB of the quotient is computed correctly, and the value of UV ignoring\nthe quotient bit is equal to 2N(cid:48).\nIn the next iteration, we can use exactly the same procedure to prove that the value of\nUV(ignoring quotient bits) is equal to 4N(cid:48)(cid:48). Ultimately, after 32 iterations, V will contain the\nentire quotient. The value of UV(ignoring quotient bits) at that point will be 2n\u00d7N32. Here\nNi is the reduced dividend after the ith iteration. We have the following relation according to\nEquation 7.17:\nN31 = DQ +R\n1\n\u21d2N31\u2212DQ = R (7.21)\n1\n(cid:124) (cid:123)(cid:122) (cid:125)\nN32\nHence, U will contain the value of the remainder and V will contain the quotient.\nImportant Point 10\nLet us now try to prove that the restoring algorithm does not suffer from overflows while\nperforming a left shift, and adding or subtracting the divisor. Let us first prove that just\nbefore the shift operation in Line 4, U < D. Let us assume positive divisors (D > 0) and\nnon-negative dividends (N \u2265 0) for division. For the base case, (U = 0), the proposition 301 (cid:13)c Smruti R. Sarangi\nholds. Let us consider the nth iteration. Let the value of U before the shift operation be U\u02c6.\nFrom the induction hypothesis, we can conclude that U\u02c6 < D, or alternatively, U\u02c6 \u2264 D\u22121\nAfter the shift operation, we have U \u2264 2U\u02c6 + 1 because we are performing a left shift by\n1 position, and shifting in the MSB of V. If U < D, then the induction hypothesis holds\nfor the (n+1)th iteration. Otherwise, we subtract D from U. We have, U = U \u2212D \u2264\n2U\u02c6 +1\u2212D \u2264 2(D \u22121)+1\u2212D = D \u22121. Therefore, U < D. Thus, for the (n+1)th\niteration, the induction hypothesis holds. Now that we have proved that U < D, let us prove\nthat the largest value contained in register U is less than or equal to 2D\u22121.\nAfter the shift operation, the largest value that U can contain is (2(D\u22121)+1) = 2D\u22121.\nHenceforth, the value of U can only decrease. Since D is a 32-bit number, we require at the\nmost 33 bits to store (2D\u22121). Consequently, having a 33-bit U register avoids overflows.\nTime Complexity\nThere are n iterations of the for loop. Each iteration does one subtraction in Line 5 and maybe\none addition in Line 10. The rest of the operations can be done in O(1) time. Thus, per\niteration it takes O(log(n)) time. Hence, the total time complexity is O(nlog(n)).\nExample 100\nDivide two 4-bit numbers: 7 (0111) \/ 3(0011) using restoring division. Answer: (cid:13)c Smruti R. Sarangi 302\nDividend (N) 00111\nDivisor (D) 0011\nU V\nbeginning: 00000 0111\nafter shift: 00000 111X\n1\nend of 00000 1110\niteration:\nafter shift: 00001 110X\n2\nend of 00001 1100\niteration:\nafter shift: 00011 100X\n3\nend of 00000 1001\niteration:\nafter shift: 00001 001X\n4\nend of 00001 0010\niteration:\nQuotient(Q) 0010\nRemainder(R) 0001\n7.3.3 Non-Restoring Division\nWe observe that there can be a maximum of two add\/subtract operations per iteration. It is\npossible to circumvent it by using another temporary register to store the result of the subtract\noperationU\u2212D. We can move it to U only if U\u2212D \u2265 0. However, this also involves additional\ncircuitry. The U register will get complicated and slower too.\nThe non-restoring algorithm does either one addition or one subtraction per iteration.\nHence, it is more efficient even though the asymptotic time complexity is the same. The\nhardware setup (U and V registers, dividend (N), divisor (D)) is the same as that for the 303 (cid:13)c Smruti R. Sarangi\nrestoring algorithm.\nAlgorithm 4: Non-restoring algorithm to divide two 32-bit numbers\nData: Divisor in D, Dividend in V, U = 0\nResult: U contains the remainder, and V contains the quotient\n1 i \u2190 0\n2 for i < 32 do\n3 i \u2190 i + 1\n\/* Left shift UV by 1 position *\/\n4 UV \u2190 UV << 1\n5 if U \u2265 0 then\n6 U \u2190 U \u2212D\n7 end\n8 else\n9 U \u2190 U + D\n10 end\n11 if U \u2265 0 then\n12 q \u2190 1\n13 end\n14 else\n15 q \u2190 0\n16 end\n\/* Set the quotient bit *\/\n17 LSB of V \u2190 q\n18 end\n19 if U < 0 then\n20 U \u2190 U +D\n21 end\nWeseethatthenon-restoringalgorithmisverysimilartotherestoringalgorithmwithsome\nminordifferences. Thenon-restoringalgorithmshiftsUV asthefirststepinaniteration. Then,\nif the value of U is negative, it adds D to U. Otherwise, it subtracts D from U. If the addition\nor subtraction has resulted in a value that is greater than or equal to zero, the non-restoring\nalgorithm sets the appropriate quotient bit to 1, else it sets it to 0.\nFinally, at the end V contains the entire quotient. If U is negative, then we need to add\nthe divisor (D) to U. U will contain the remainder.\nProof of Correctness*\nLike the restoring algorithm, let us assume that when we refer to the value of UV, we assume\nthat all the quotient bits are equal to 0. As long as U remains positive or 0, the state of U and\nV is equal to that produced by the restoring algorithm. Let us assume that in the jth iteration,\nU becomes negative for the first time. Let us consider the value represented by the register\nUV just after it is shifted to the left by 1 position, and call it UV (j stands for the iteration\nj\nnumber).\nAt the end of the jth iteration, UV = UV \u2212D(cid:48), where D(cid:48) = D \u00d72n. We shift D by n\nj\nplaces to the left because we always add or subtract D from U, which is the upper half of UV. (cid:13)c Smruti R. Sarangi 304\nAccording to our assumption UV is negative. In this case the restoring algorithm would not\nj\nhave subtracted D(cid:48), and it would have written 0 to the quotient. The non-restoring algorithm\nsets the quotient bit correctly since it finds UV to be negative (Line 15). Let us now move to\nthe (j +1)th iteration.\nUV = 2UV \u2212 2D(cid:48). At the end of the (j + 1)th iteration, UV = 2UV \u2212 2D(cid:48) + D(cid:48) =\nj+1 j j\n2UV \u2212D(cid:48). If UV is not negative, then the non-restoring algorithm will save 1 in the quotient.\nj\nLet us now see at this point what the restoring algorithm would have done (assuming UV\nis non-negative). In the (j + 1)th iteration, the restoring algorithm would have started the\niteration with UV = UV . It would have then performed a shift and subtracted D(cid:48) to set\nj\nUV = 2UV \u2212D(cid:48), and written 1 to the quotient. We thus observe that at this point the state\nj\nof the registers U and V matches exactly for both the algorithms.\nHowever, if UV is negative then the restoring and non-restoring algorithms will have a\ndifferent state. Nonetheless the quotient bits will be set correctly. UV = 4UV \u22122D(cid:48). Since\nj+2 j\na negative number multiplied by 2 (left shifted by 1 position) is still negative, the non-restoring\nalgorithm will add D(cid:48) to U. Hence, the value of UV at the end of the (j +2)th iteration will\nbe 4UV \u2212D(cid:48). If this is non-negative, then the restoring algorithm would also have exactly the\nj\nsame state at this point.\nWe can continue this argument to observe that the quotient bits are always set correctly\nand the state of U and V exactly matches that of the restoring algorithm when UV \u2265 0 at\nthe end of an iteration. Consequently, for dividing the same pair of numbers the states of the\nrestoring and non-restoring algorithms will start as the same, then diverge and converge several\ntimes. If the last iteration leads to a non-negative UV then the algorithm is correct because\nthe state exactly matches that produced by the restoring algorithm.\nHowever, if the last iteration leaves us with UV as negative, then we observe that UV\n= 2n\u2212kUV \u2212D(cid:48), where k is the iteration number at which the states had converged for the last\nk\ntime. If we add D(cid:48) to UV, then the states of both the algorithms match, and thus the results\nare correct (achieved in Line 20).\nImportant Point 11\nLet us now try to prove that the non-restoring algorithm does not suffer from overflows while\nperforming a left shift, and adding or subtracting the divisor. Similar to the proof for the\nrestoring algorithm, let us first prove that just before the shift operation, |U| < D. For the\nbase case, (U = 0), the proposition holds. Let us consider the nth iteration, and let the value\nof U just before the shift operation be U\u02c6. Let us first assume that U\u02c6 \u2265 0. In this case, we\ncan use the same logic as the restoring algorithm, and prove that |U| < D at the beginning\nof the (n+1)th iteration. Let us now assume that U\u02c6 < 0. From the induction hypothesis,\n|U\u02c6| < D \u21d2 U\u02c6 \u2265 \u2212(D\u22121). Now, ifweshiftU\u02c6 by1position, andshiftina0or1, wecompute\nU \u2265 2U\u02c6 (for U\u02c6 < 0, shifting in a 1 reduces the absolute value). After the shift operation,\nwe add D to U. We thus have, U = U+D \u2265 2U\u02c6 +D \u2265 2\u00d7(1\u2212D)+D = 2\u2212D. Thus, in\nthis case also |U| < D, just before the shift, and after the shift we have |U| \u2264 2D\u22121. We\nthus need to allocate an extra bit to register U to correctly store all the possible intermediate\nvalues of U. Hence, the U register is 33 bits wide. We are thus guaranteed to not have\noverflows during the operation of the non-restoring algorithm. 305 (cid:13)c Smruti R. Sarangi\nExample 101\nDivide two 4-bit numbers: 7 (0111) \/ 3(0011) using non-restoring division. Answer:\nDividend (N) 00111\nDivisor (D) 0011\nU V\nbeginning: 00000 0111\nafter shift: 00000 111X\n1\nend of 11101 1110\niteration:\nafter shift: 11011 110X\n2\nend of 11110 1100\niteration:\nafter shift: 11101 100X\n3\nend of 00000 1001\niteration:\nafter shift: 00001 001X\n4\nend of 11110 0010\niteration:\nU V\nend (U=U+D): 0001 0010\nQuotient(Q) 0010\nRemainder(R) 0001\n7.4 Floating Point Addition and Subtraction\nThe problems of floating point addition and subtraction are actually different faces of the same\nproblem. A\u2212B can be interpreted in two ways. We can say that we are subtracting B from A,\nor we can say that we are adding \u2212B to A. Hence, instead of looking at subtraction separately,\nlet us look at it as a special case of addition. We shall first look at the problem of adding two\nnumbers with the same sign in Section 7.4.1, with opposite signs in Section 7.4.4 and then look\nat the generic problem of adding two numbers in Section 7.4.5.\nBefore going further, let us quickly recapitulate our knowledge of floating point numbers (cid:13)c Smruti R. Sarangi 306\n(see Table 7.5).\nNormalised form of a 32-bit (normal) floating point number.\nA = (\u22121)S \u00d7P \u00d72E\u2212bias, (1 \u2264 P < 2,E \u2208 Z,1 \u2264 E \u2264 254)\n(7.22)\nNormalised form of a 32-bit (denormal) floating point number.\nA = (\u22121)S \u00d7P \u00d72\u2212126, (0 \u2264 P < 1) (7.23)\nSymbol Meaning\nS Sign bit (0(+ve), 1(-ve))\nP Significand (form: 1.xxx(normal) or 0.xxx(denormal))\nM Mantissa (fractional part of significand)\nE (exponent + 127(bias))\nZ Set of integers\nTable 7.5: IEEE 754 format\n7.4.1 Simple Addition with Same Signs\nThe problem is to add two floating point numbers A and B with the same sign. We want\nto compute a new floating point number C = A + B. In this case, the sign of the result is\nknown in advance (sign of A or B). All of our subsequent discussion assumes the IEEE 32-bit\nformat. However, the techniques that we develop can be extended to other formats, especially\ndouble-precision arithmetic.\nFirst, the floating point unit needs to unpack different fields from the floating point rep-\nresentations of A and B. Let the E fields (exponent + bias) be E and E for A and B\nA B\nrespectively. Let the E field of the result, C, be E . In hardware, let us use a register called\nC\nE to save the exponent (in the bias notation). The final value of E needs to be equal to E .\nC\nUnpacking the significand is slightly more elaborate. We shall represent the significands\nas unsigned integers and ignore the decimal point. Moreover, we shall add a leading most\nsignificant bit that can act as the sign bit. It is initially 0. For example, if a floating point\nnumber is of the form: 1.0111 \u00d7 210, the significand is 1.0111, and we shall represent it as\n010111. Note that we have added a leading 0 bit. Figure 7.16 shows an example of how the\nsignificand is unpacked, and placed in a register for a normal floating point number. In the\n32-bit IEEE 754 format, there are 23 bits for the mantissa, and there is either a 0 or 1 before\nthe decimal point. The significand thus requires 24 bits, and if we wish to add a leading sign\nbit(0), then we need 25 bits of storage. Let us save this number in a register, and call it W.\nLet us start out by observing that we cannot add A and B the way we have added integers,\nbecause the exponents might be different. The first task is to ensure that both the exponents\nare the same. Without no loss of generality, let us assume that E \u2265 E . This can be effected\nA B\nwith a simple compare and swap in hardware. We can thus initialise the register E to E .\nA\nLet the significands of A and B be P and P respectively. Let us initially set W equal to\nA B 307 (cid:13)c Smruti R. Sarangi\nNumber in IEEE 754 format\n3231 2423 1\nS E Mantissa\n01\nW\nFigure 7.16: Expanding the significand and placing it in a register\nthe significand of B(P ) with a leading 0 bit as shown in Figure 7.16.\nB\nTo make the exponent of A and B equal, we need to right shift W by (E \u2212E ) positions.\nA B\nNow, we can proceed to add the significand of A termed as P to W.\nA\nW = W >> (E \u2212E ) (7.24)\nA B\nW = W +P (7.25)\nA\nLet the significand represented by W be P . There is a possibility that P might be\nW W\ngreater than or equal to 2. In this case, the significand of the result is not in normalised form.\nWe will thus need to right shift W by 1 position and increment E by 1. This process is called\nnormalisation. There is a possibility that incrementing E by 1 might make it equal to 255,\nwhich is not allowed. We can signal an overflow in this case. The final result can be obtained\nby constructing a floating point number out of the E, W, and the sign of the result (sign of\neither A or B).\nExample 102\nAdd the numbers: 1.01 \u00d723 + 1.11 \u00d721. Assume that the bias is 0.\n2 2\nAnswer:\n1. A = 1.01\u00d723 and B = 1.11\u00d721\n2. W = 01.11 (significand of B)\n3. E = 3\n4. W = 01.11 >> (3-1) = 00.0111\n5. W + P = 00.0111 + 01.0100 = 01.1011\nA\n6. Result: C = 1.1011\u00d723 (cid:13)c Smruti R. Sarangi 308\nExample 103\nAdd 1.01\u00d723 + 1.11\u00d722. Assume that the bias is 0.\nAnswer:\n1. A = 1.01\u00d723 and B = 1.11\u00d722\n2. W = 01.11 (significand of B)\n3. E = 3\n4. W = 01.11 >> (3-2) = 00.111\n5. W + P = 00.111 + 01.010 = 10.001\nA\n6. Normalisation: W = 10.001 >> 1 = 1.0001, E = 4\n7. Result: C = 1.0001\u00d724\n7.4.2 Rounding\nIn Example 103, let us assume that we were allowed only two mantissa bits. Then, there would\nhave been a need to discard all the mantissa bits other than the two most significant ones. The\nresult would have been 1.00. To incorporate the effect of the discarded bits, it might have been\nnecessary to round the result. For example, let us consider decimal numbers. If we wish to\nround 9.99 to the nearest integer, then we should round it to 10. Similarly, if we wish to round\n9.05 to the nearest integer, then we should round it to 9. Likewise, it is necessary to introduce\nrounding schemes while doing floating point operations such that the final result can properly\nreflect the value contained in the discarded bits.\nLet us first introduce some terminology. Let us consider the sum of the significands after\nwe have normalised the result. Let us divide the sum into two parts: (P(cid:98)+R)\u00d72\u221223(R < 1).\nHere, P(cid:98) is the significand of the temporary result in W multiplied by 223. It is an integer and\nit might need to be further rounded. R is a residue (beyond 23 bits) that will be discarded. It\nis less than 1. The aim is to modify P(cid:98) appropriately to take into account the value of R. Now,\nthere are two ways in which P(cid:98) can be modified because of rounding. Either we can leave P(cid:98)\nas it is, or we can increment P(cid:98) by 1. Leaving P(cid:98) as it is is also known as truncation. This is\nbecause we are truncating or discarding the residue.\nThe IEEE 754 format supports four rounding modes as shown in Table 7.6. An empty\nentry corresponds to truncating the result. We only show the conditions in which we need to\nincrement P(cid:98).\nWe give examples in decimal (base 10) in the next few subsections for the ease of under-\nstanding. Exactly the same operations can be performed on binary numbers. Our aim is to\nround P(cid:98)+R to an integer. There are four possible ways of doing this in the IEEE 754 format. 309 (cid:13)c Smruti R. Sarangi\nRounding Mode Condition for incrementing the significand\nSign of the result (+ve) Sign of the result (-ve)\nTruncation\nRound to +\u221e R>0\nRound to \u2212\u221e R>0\nRound to Nearest (R>0.5)||(R=0.5\u2227LSB(P(cid:98))=1) (R>0.5)||(R=0.5\u2227LSB(P(cid:98))=1)\nP(cid:98) (significand of the temporary result multiplied by 223), \u2227 (logical AND), R (residue)\nTable 7.6: IEEE 754 rounding modes\nTruncation\nThis is the simplest rounding mode. This rounding mode simply truncates the residue. For\nexample, in truncation based rounding, if P(cid:98)+R = 1.5, then we will discard 0.5, and we are left\nwith 1. Likewise, truncating -1.5 will give us -1. This is the easiest to implement in hardware,\nand is the least accurate out of the four methods.\nRound to +\u221e\nIn this rounding mode, we always round a number to the larger integer. For example, if\nP(cid:98)+R = 1.2, we round it to 2. If P(cid:98)+R = \u22121.2, we round it to -1. The idea here is to check the\nsign bit and the residue. If the number is positive, and the residue is non-zero, then we need\nto increment P(cid:98), or alternatively the LSB of the significand. Otherwise, in all the other cases\n(either R = 0 or the number is negative), it is sufficient to truncate the residue.\nRound to \u2212\u221e\nThis is the reverse of rounding to +\u221e. In this case, we round 1.2 to 1, and -1.2 to -2.\nRound to Nearest\nThis rounding mode is the most complicated, and is also the most common. Most processors\nuse this rounding mode as the default. In this case, we try to minimise the error by rounding\nP(cid:98) to the nearest possible value. If R > 0.5, then the nearest integer is P(cid:98) +1. For example,\nwe need to round 3.6 to 4, and -3.6 to -4. Similarly, if R < 0.5, then we need to truncate the\nresidue. For example, we need to round 3.2 to 3, and -3.2 to -3.\nThe special case arises when R = 0.5. In this case, we would like to round P(cid:98) to the nearest\neveninteger. Forexample,wewillround3.5to4, and4.5toalso4. Thisismoreofaconvention\nthan a profound mathematical concept. To translate this requirement in our terms, we need\nto take a look at the LSB of P(cid:98). If it is 0, then P(cid:98) is even, and we do not need to do anything\nmore. However, if LSB(P(cid:98)) = 1, then P(cid:98) is odd, and we need to increment it by 1.\n7.4.3 Implementing Rounding\nFrom our discussion on rounding, it is clear that we need to maintain some state regarding the\ndiscarded bits and P(cid:98) such that we can make the proper rounding decision. In specific, we need (cid:13)c Smruti R. Sarangi 310\nfour pieces of information \u2013 LSB(P(cid:98)), is R = 0.5, is R > 0, and is R > 0.5. The last three\nrequirements can be captured with two bits \u2013 round and sticky.\nThe round bit is the MSB of the residue, R. The sticky bit is a logical OR of the rest of\nthe bits of the residue. We can thus express the different conditions on the residue as shown in\nTable 7.7.\nCondition on Residue Implementation\nR > 0 r\u2228s = 1\nR = 0.5 r\u2227s = 1\nR > 0.5 r\u2227s = 1\nr (round bit), s(sticky bit)\nTable 7.7: Evaluating properties of the residue using round and sticky bits\nImplementing rounding is thus as simple as maintaining the round bit, and sticky bit, and\nthen using Table 7.6 to round the result. Maintaining the round and sticky bits requires us\nto simply update them on every single action of the algorithm. We can initialise these bits to\n0. They need to be updated when B is shifted to the right. Then, they need to be further\nupdated when we normalise the result. Now, it is possible that after rounding, the result is\nnot in normalised form. For example, if P(cid:98) contains all 1s, then incrementing it will produce 1\nfollowed by 23 0s, which is not in the normalised form.\nRenormalisation after Rounding\nIn case, the process of rounding brings the result to a state that is not in the normalised form,\nthen we need to re-normalise the result. Note that in this case, we need to increment the\nexponent by 1, and set the mantissa to all 0s. Incrementing the exponent can make it invalid\n(if E = 255). We need to explicitly check for this case.\n7.4.4 Addition of Numbers with Opposite Signs\nNow let us look at the problem of adding two floating point numbers, A and B, to produce C.\nThey have opposite signs. Again let us make the assumption that E \u2265 E .\nA B\nThe first step is to load the register W with the significand of B(P ) along with a leading 0.\nB\nSince the signs are different, in effect we are subtracting the significand of B (shifted by some\nplaces) from the significand of A. Hence, we can take the 2\u2019s complement of W that contains\nP with a leading 0 bit, and then shift it to the right by E \u2212E places. This value is written\nB A B\nback to the register W. Note that the shift needs to be an arithmetic right shift here such\nthat the value is preserved. Secondly, the order of operations (shift and 2\u2019s complement) is not\nimportant.\nWe can now add the significand of A (P ) to W. If the resulting value is negative, then we\nA\nneed to take its 2\u2019s complement, and set the sign of the result accordingly.\nNext, we need to normalise the result. It is possible that P < 1. In this case, we need to\nW\nshift W to the left till 1 \u2264 P < 2. Most implementations of the floating point standard, use\nW\nan extra bit called the guard bit. along with the round and sticky bits. They set the MSB of 311 (cid:13)c Smruti R. Sarangi\nthe residue to the guard bit, the next bit to the round bit, and the OR of the rest of the bits\nto the sticky bits. During the process of shifting a number left, they shift in the guard bit first,\nand then shift in 0s. At the end of the algorithm, it is necessary to set the round bit equal\nto the guard bit, and OR the sticky bit with the round bit such that our original semantics is\nmaintained. This added complexity is to optimise for the case of a left shift by 1 position. If\nwe did not have the guard bit, then we needed to shift the round bit into W, and we would\nthus lose the round bit forever.\nOnce W is normalised and the exponent(E) is updated, we need to round the result as per\nTable 7.6. This process might lead to another round of normalisation.\n7.4.5 Generic Algorithm for Adding Floating Point Numbers\nNote that we have not considered special values such as 0 in our analysis. The flowchart\nin Figure 7.17 shows the algorithm for adding two floating point numbers. This algorithm\nconsiders 0 values also.\nC = A + B\nY\nA=0? C=B\nN N\nW <0?\nY\nB=0? C=A Y\nN\nW -W (2's complement)\nSwap A and B\nsuch that E <E\nB A\nS = S\nW P B >> (E A - E B ) Normalise W and Overflow or Y\nE EA, S sign(A) update E underflow? Report\nN\nsign(A) = sign(B)? Y Overflow or Y Construct C out\nunderflow? of W, E, and S\nN\nReport\nN\nW -W (2's complement)\nRound W\nC\nW W + P\nA Normalise W and\nupdate E\nFigure 7.17: Flowchart for adding two floating point values\n7.5 Multiplication of Floating Point Numbers\nThe algorithm for multiplying floating point numbers is of exactly the same form as the algo-\nrithm for generic addition without a few steps. Let us again try to multiply A\u00d7B to obtain (cid:13)c Smruti R. Sarangi 312\nC = A * B\nY\nA=0? C=0\nNormalise W and\nN\nupdate E\nY\nB=0? C=0\nN\nY\nOverflow or\nunderflow?\nS sign(A) sign(B)\nReport\nN\nE E\nA\n+ E\nB\n- bias\nRound W Construct C out\nof W, E, and S\nNormalise W and\nOverflow or Y update E\nunderflow? C\nN\nReport\nOverflow or Y\nunderflow?\nW P * P\nA B Report\nN\nFigure 7.18: Flowchart for multiplying two floating point values\nthe product C. Again, let us assume without loss of generality that E \u2265 E .\nA B\nThe flowchart for multiplication is shown in Figure 7.18. We do not have to align the\nexponents in the case of multiplication. We initialise the algorithm as follows. We load the\nsignificand of B into register W. In this case, the width of W is equal to double the size\nof the operands such that the product can be accommodated. The E register is initialised to\nE +E \u2212bias. Thisisbecauseinthecaseofmultiplication, theexponentsareaddedtogether.\nA B\nWe subtract the bias to avoid double counting. Computing the sign of the result is trivial.\nAfter initialisation, we multiply the significand of A with W and save the product in W.\nTheproductcontains46bitsafterthefloatingpoint. Wemightneedtodiscardsomeofthebits\nto ensure that the final mantissa is 23 bits long. Hence, it might be necessary to normalise the\nresultbyshiftingittotheright(normalnumbers), orshiftingittotheleft(denormalnumbers).\nAs with the case of addition, we can then proceed to round the result to contain 23 bits in\nthemantissa,andrenormaliseifnecessary. Sincethereareaconstantnumberofaddoperations,\nthe time complexity is equal to the sum of the time complexity of normal multiplication and\naddition. Both of them are O(log(n)) operations. The total time taken is thus O(log(n)). 313 (cid:13)c Smruti R. Sarangi\n7.6 Division of Floating Point Numbers\n7.6.1 Simple Division\nThe major difference between integer and floating point division is that floating point division\ndoes not have a remainder. It only has a quotient. Let us try to divide A by B to obtain C.\nWe initialise the algorithm by setting the W register to contain the significand(P ) of A.\nA\nThe E register is initialised as E \u2212E +bias. This is done because in division, we subtract\nA B\nthe exponents. Hence, in their biased representation we need to subtract E from E , and we\nB A\nneed to add the value of the bias back. Computing the sign of the result is also trivial in this\ncase.\nWe start out by dividing P by P . The rest of the steps are the same as that of multipli-\nA B\ncation (see Section 7.5). We normalise the result, round it, and then renormalise if necessary.\nThe time complexity of this operation is the same as the time complexity of the restoring\nor non-restoring algorithms. It is equal to O(nlog(n)). It turns out that for the case of floating\npoint division, we can do much better.\n7.6.2 Goldschmidt Division\nLet us try to simplify the process of division by dividing it into two stages. In the first stage,\nwe compute the reciprocal of the divisor (1\/B). In the next stage, we multiply the obtained\nreciprocal with the dividend A. The product is equal to A\/B. Floating point multiplication is\nan O(log(n)) time operation. Hence, let us focus on trying to compute the reciprocal of B.\nLet us also ignore exponents in our discussion because, we just need to flip the sign of the\nexponentinthereciprocal. Letusonlyfocusonthesignificand,P ,andtokeepmatterssimple,\nB\nlet us further assume that B is a normal floating point number. Thus, 1 \u2264 P < 2. We can\nB\nrepresent P = 1+X(X < 1). We have:\nB\n1 1\n= (P = 1+X,X < 1)\nB\nP 1+X\nB\n1\n= (X(cid:48) = 1\u2212X,X(cid:48) < 1)\n1+1\u2212X(cid:48)\n1\n= (7.26)\n2\u2212X(cid:48)\n1 1\n= \u00d7\n2 1\u2212X(cid:48)\/2\n1 1\n= \u00d7 (Y = X(cid:48)\/2,Y < 1\/2)\n2 1\u2212Y\nLet us thus focus on evaluating 1\/(1\u2212Y). We have: (cid:13)c Smruti R. Sarangi 314\n1 1+Y\n=\n1\u2212Y 1\u2212Y2\n(1+Y)(1+Y2)\n=\n1\u2212Y4\n(7.27)\n= ...\n(1+Y)(1+Y2)...(1+Y16)\n=\n1\u2212Y32\n\u2248 (1+Y)(1+Y2)...(1+Y16)\nWe need not proceed anymore. The reason for this is as follows. Since Y < 1\/2, Yn\nis less than 1\/2n. The smallest mantissa that we can represent in the IEEE 32-bit floating\npoint notation is 1\/223. Hence, there is no point in having terms that have an exponent\ngreater than 23. Given the approximate nature of floating point mathematics, the product\n(1+Y)(1+Y2)...(1+Y16) is as close to the real value of 1\/(1\u2212Y) as we can get.\nLet us now consider the value \u2013 (1+Y)(1+Y2)...(1+Y16). It has 5 add operations that\ncan be done in parallel. To obtain Y ...Y16, we can repeatedly multiply each term with itself.\nFor example, to get Y8, we can multiply Y4 with Y4 and so on. Thus, generating the powers\nof Y takes 4 multiply operations. Lastly, we need to multiply the terms in brackets \u2013 (1+Y),\n(1+Y2),(1+Y4),(1+Y8), and (1+Y16). This will required 4 multiplications. We thus require\na total of 8 multiplications and 5 additions.\nLet us now compute the time complexity. For an n-bit floating point number, let us assume\nthat a fixed fraction of bits represent the mantissa. Thus, the number of bits required to\nrepresent the mantissa is O(n). Consequently, the number of terms of the form (1 + Y2k)\nthat we need to consider is O(log(n)). The total number of additions, and multiplications for\nfinding the reciprocal is also O(log(n)). Since each addition or multiplication takes O(log(n))\ntime, the time complexity of finding the reciprocal of B is equal to O(log(n)2). Since the rest\nof the operations such as adjusting the exponents and multiplying the reciprocal with A take\nO(log(n)) time, the total complexity is equal to O(log(n)2).\nWe observe that floating point division is asymptotically much faster than normal integer\ndivision. This is primarily because floating point mathematics is approximate, whereas integer\nmathematics is exact.\n7.6.3 Division Using the Newton-Raphson Method\nWe detail another algorithm that also takes O(log(n)2) time. We assume that we are trying to\ndivide A by B. Let us only consider normal numbers. Akin to Goldschmidt division, the key\npoint of the algorithm is to find the reciprocal of the significand of B. Adjusting the exponents,\ncomputing the sign bit, and multiplying the reciprocal with A are fast operations (O(log(n)).\nFor readability, let us designate P as b (1 \u2264 b < 2). We wish to compute 1\/b. Let us\nB\ncreate a function: f(x) = 1\/x\u2212b. f(x) = 0 when x = 1\/b. The problem of computing the\nreciprocal of b is thus the same as computing the root of f(x). Let us use the Newton Raphson\nmethod [Kreyszig, 2000].\nThe gist of this method is shown in Figure 7.19. We start with an arbitrary value of x\nsuch as x . We then locate the point on f(x) that has x as its x co-ordinate and then draw\n0 0 315 (cid:13)c Smruti R. Sarangi\nx ,f(x )\n0 0\nf(x)\nx ,f(x )\n1 1\nroot\nx x x\n2 1 0\nx\nFigure 7.19: The Newton-Raphson method\na tangent to f(x) at (x ,f(x )). Let the tangent intersect the x axis at x . We again follow\n0 0 1\nthe same procedure, and draw another tangent at (x ,f(x )). This tangent will intersect the x\n1 1\naxis at x . We continue this process. As we can observe in the figure, we gradually get closer\n2\nto the root of f(x). We can terminate after a finite number of steps with an arbitrarily small\nerror. Let us analyse this procedure mathematically.\nThe derivative of f(x) at x is df(x)\/dx = \u22121\/x2. Let the equation of the tangent be\n0 0\ny = mx+c. The slope is equal to \u22121\/x2. The equation is thus: y = \u2212x\/x2+c. Now, we know\n0 0\nthat at x , the value of y is 1\/x \u2212b. We thus have:\n0 0\n1 x\n0\n\u2212b = \u2212 +c\nx x2\n0 0\n1 1\n\u21d2 \u2212b = \u2212 +c (7.28)\nx x\n0 0\n2\n\u21d2c = \u2212b\nx\n0\nThe equation of the tangent is y = \u2212x\/x2 +2\/x \u2212b. This line intersects the x axis when\n0 0\ny = 0, and x = x . We thus have:\n1\nx 2\n1\n0 = \u2212 + \u2212b\nx2 0 x 0 (7.29)\n\u21d2x = 2x \u2212bx2\n1 0 0\nLet us now define an error function of the form: E(x) = bx\u22121. Note that E(x) is 0, when\nx is equal to 1\/b. Let us compute the error: E(x ) and E(x ).\n0 1 (cid:13)c Smruti R. Sarangi 316\nE(x ) = bx \u22121 (7.30)\n0 0\nE(x ) = bx \u22121\n1 1\n=\nb(cid:0)\n2x\n\u2212bx2(cid:1)\n\u22121\n0 0\n= 2bx \u2212b2x2\u22121\n0 0\n(7.31)\n= \u2212(bx \u22121)2\n0\n= \u2212E(x )2\n0\n| E(x ) | =| E(x ) |2\n1 0\nThus, the error gets squared every iteration, and if the starting value of the error is less\nthan 1, then it will ultimately get arbitrarily close to 0. If we can place bounds on the error,\nthen we can compute the number of iterations required.\nWe start out by observing that 1 \u2264 b < 2 since we only consider normal floating point\nnumbers. Let x be 1\/2. The range of bx \u2212 1 is thus [\u22121\/2,0]. We can thus bound the\n0 0\nerror(E(x )) as \u22121\/2 \u2264 E(x ) < 0. Therefore, we can say that | E(x ) |\u2264 1\/2. Let us now take\n0 0 0\na look at the maximum value of the error as a function of the iteration in Table 7.8.\nIteration max(E(x))\n0 1\n2\n1 1\n22\n2 1\n24\n3 1\n28\n4 1\n216\n5 1\n232\nTable 7.8: Maximum error vs iteration count\nSince we only have 23 mantissa bits, we need not go beyond the fifth iteration. Thus, in\nthis case also, the number of iterations is small, and bounded by a small constant. In every\nstep we have a multiply and subtract operation. These are O(log(n)) time operations.\nLet us compute the time complexity for n bit floating point numbers. Here, also we assume\nthat a fixed fraction of bits are used to represent the mantissa. Like the case of Goldschmidt\ndivision, we need O(log(n)) iterations, and each iteration takes O(log(n)) time. Thus, the total\ncomplexity is O(log(n)2).\n7.7 Summary and Further Reading\n7.7.1 Summary 317 (cid:13)c Smruti R. Sarangi\nSummary 7\n1. Adding two 1 bit numbers (a and b) produces a sum bit(s) and a carry bit(c)\n(a) s = a\u2295b\n(b) c = a.b\n(c) We can add them using a circuit called a half adder.\n2. Adding three 1 bit numbers (a, b, and c ) also produces a sum bit(s) and a carry\nin\nbit(c )\nout\n(a) s = a\u2295b\u2295c\nin\n(b) c = a.b+a.c +b.c\nout in in\n(c) We can add them using a circuit called a full adder.\n(d)\n3. We can create a n bit adder known as a ripple carry adder by chaining together n\u22121\nfull adders, and a half adder.\n4. We typically use the notion of asymptotic time complexity to express the time taken\nby an arithmetic unit such as an adder.\n(a) f(n) = O(g(n)) if |f(n)| \u2264 c|g(n)| for all n > n , where c is a positive constant.\n0\n(b) For example, if the time taken by an adder is given by f(n) = 2n3+1000n2+n,\nwe can say that f(n) = O(n3)\n5. We discussed the following types of adders along with their time complexities:\n(a) Ripple carry adder \u2013 O(n)\n\u221a\n(b) Carry select adder \u2013 O( n)\n(c) Carry lookahead adder \u2013 O(log(n))\n6. Multiplication can be done iteratively in O(nlog(n)) time using an iterative multiplier.\nThe algorithm is similar to the one we learned in elementary school.\n7. We can speed it up by using a Booth multiplier that takes advantage of a continuous\nrun of 1s in the multiplier.\n8. The Wallace tree multiplier runs in O(log(n)) time. It uses a tree of carrysaveadders\nthat express a sum of three numbers, as a sum of two numbers.\n9. We introduced two algorithms for division:\n(a) Restoring algorithm (cid:13)c Smruti R. Sarangi 318\n(b) Non-restoring algorithm\n10. Floating point addition and subtraction need not be considered separately. We can\nhave one algorithm that takes care of the generic case.\n11. Floating point addition requires us to perform the following steps:\n(a) Align the significand of the smaller number with the significand of the larger\nnumber.\n(b) If the signs are different then take a 2\u2019s complement of the smaller significand.\n(c) Add the significands.\n(d) Compute the sign bit of the result.\n(e) Normalise and round the result using one of four rounding modes.\n(f) Renormalise the result again if required.\n12. We can follow the same steps for floating point multiplication and division. The only\ndifference is that in this case the exponents get added or subtracted respectively.\n13. Floating point division is fundamentally a faster operation than integer division be-\ncause of the approximate nature of floating point mathematics. The basic operation is\nto compute the reciprocal of the denominator. It can be done in two ways:\n(a) Use the Newton-Raphson method to find the root of the equation f(x) = 1\/x\u2212b.\nThe solution is the reciprocal of b.\n(b) Repeatedly multiply the numerator and denominator of a fraction derived from\n1\/b such that the denominator becomes 1 and the reciprocal is the numerator.\n7.7.2 Further Reading\nFor more details on the different algorithms for computer arithmetic, the reader can refer to\nclassic texts such as the books by Israel Koren [Koren, 2001], Behrooz Parhami [Parhami,\n2009], and Brent and Zimmermann [Brent and Zimmermann, 2010]. We have not covered\nthe SRT division algorithm. It is used in a lot of modern processors. The reader can find\ngood descriptions of this algorithm in the references. The reader is also advised to look at\nalgorithms for multiplying large integers. The Karatsuba and Sc\u00a8onhage-Strassen algorithms\nare the most popular algorithms in this area. The area of approximate adders is gaining in\nprominence. These adders add two numbers by assuming certain properties such as a bound on\nthe maximum number of positions a carry propagates. It is possible that they can occasionally\nmake a mistake. Hence, they have additional circuitry to detect and correct errors. With high\nprobability such adders can operate in O(log(log(n)) time. Verma et. al. [Verma et al., 2008]\ndescribe one such scheme. 319 (cid:13)c Smruti R. Sarangi\nExercises\nAddition\nEx. 1 \u2014 Design a circuit to find the 1\u2019s complement of a number using half adders only.\nEx. 2 \u2014 Design a circuit to find the 2\u2019s complement of a number using half adders and logic\ngates.\nEx. 3 \u2014 Assume that the latency of a full adder is 2ns, and that of a half adder is 1ns. What\nis the latency of a 32-bit ripple carry adder?\n\u221a\n* Ex. 4 \u2014 Design a carry-select adder to add two n-bit numbers in O( n) time, where the\nsizes of the blocks are 1,2,...,m respectively.\nEx. 5 \u2014 Explain the operation of a carry lookahead adder.\n* Ex. 6 \u2014 Suppose there is an architecture which supports numbers in base 3 instead of base\n2. Design a Carry Lookahead Adder for this system. Assume that you have a simple full-adder\nwhich adds numbers in base 3.\n* Ex. 7 \u2014 Mostofthetime,acarrydoesnotpropagatetilltheend. Insuchcases,thecorrect\noutput is available much before the worst case delay. Modify a ripple carry adder to consider\nsuch cases and set an output line to high as soon as the correct output is available.\n* Ex. 8 \u2014 Design a fast adder, which uses only the propagate function, and simple logic\noperations. It should NOT use the generate function. What is its time and space complexity?\nEx. 9 \u2014 Design a hardware structure to compute the sum of m, n bit numbers. Make it run\nas fast as possible. Show the design of the structure. Compute a tight bound on its asymptotic\ntime complexity. [NOTE: Computing the time complexity is not as simple as it seems].\n** Ex. 10 \u2014 You are given a probabilistic adder, which adds two numbers and yields the\noutput ensuring that each bit is correct with probability, a. In other words, a bit in the output\nmay be wrong with probability, (1 \u2212 a), and this event is independent of other bits being\nincorrect. How will you add two numbers using probabilistic adders ensuring that each output\nbit is correct with at least a probability of b, where b > a?\n*** Ex. 11 \u2014 How frequently does the carry propagate to the end for most numbers? An-\nswer: Very infrequently. In most cases, the carry does not propagate beyond a couple of bits.\nLet us design an approximately correct adder. The insight is that a carry does not propagate\nby more than k positions most of the time. Formally, we have:\nAssumption 1: While adding two numbers, the largest length of a chain of propagates is at\nmost k.\nDesign an optimal adder in this case that has time complexity O(logk) assuming that Assump-\ntion 1 holds all the time. Now design a circuit to check if assumption 1 has ever been violated. (cid:13)c Smruti R. Sarangi 320\nVermaet. al.[Vermaetal.,2008]provedthatk isequaltoO(log(n))withveryhighprobability.\nVoila, we have an exactly correct adder, which runs most of the time in O(log(log(n))) time.!!!\n*** Ex. 12 \u2014 Let us consider two n-bit binary numbers, A, and B. Further assume that the\nprobability of a bit being equal to 1 is p in A, and q in B. Let us consider (A+B) as one large\nchunk(block).\n(a) What are the expected values of generate and propagate functions of this block as n tends\nto \u221e?\n(b) If p = q = 1, what are the values of these functions?\n2\n(c) What can we infer from the answer to part (b) regarding the fundamental limits of binary\naddition?\nMultiplication\nEx. 13 \u2014 Write a program in assembly language (any variant) to multiply two unsigned\n32-bit numbers given in registers r0 and r1 and store the product in registers r2 (LSB) and\nr3 (MSB). Instead of using the multiply instruction, simulate the algorithm of the iterative\nmultiplier.\nEx. 14 \u2014 Extend the solution to Exercise 13 for 32-bit signed integers.\n* Ex. 15 \u2014 Normally, intheBooth\u2019salgorithm, weconsiderthecurrentbit, andtheprevious\nbit. Based on these two values, we decide whether we need to add or subtract a shifted version\nofthemultiplicand. Thisisknownastheradix-2Booth\u2019salgorithm, becauseweareconsidering\ntwobitsatonetime. ThereisavariationofBooth\u2019salgorithm,calledradix-4Booth\u2019salgorithm\nin which we consider 3 bits at a time. Is this algorithm faster than the original radix-2 Booth\u2019s\nalgorithm? How will you implement this algorithm ?\n* Ex. 16 \u2014 Assume that in the sizes of the U and V registers are 32 bits in a 32-bit Booth\nmultiplier. Is it possible to have an overflow? Answer the question with an example. [HINT:\nCan we have an overflow in the first iteration itself?]\n* Ex. 17 \u2014 Prove the correctness of the Booth multiplier in your own words.\nEx. 18 \u2014 Explain the design of the Wallace tree multiplier. What is its asymptotic time\ncomplexity?\n** Ex. 19 \u2014 Design a Wallace tree multiplier to multiply two signed 32-bit numbers, and\nsave the result in a 32-bit register. How do we detect overflows in this case?\nDivision\nEx. 20 \u2014 Implementation of division using an assembly program.\ni) Write an assembly program for restoring division.\nii) Write an assembly program for non-restoring division. 321 (cid:13)c Smruti R. Sarangi\n* Ex. 21 \u2014 Design an O(log(n)k) time algorithm to find out if a number is divisible by 3.\nTry to minimise k.\n* Ex. 22 \u2014 Design an O(log(n)k) time algorithm to find out if a number is divisible by 5.\nTry to minimise k.\n** Ex. 23 \u2014 Designafastalgorithmtocomputetheremainderofthedivisionofanunsigned\nnumber by a number of the form (2m+1). What is its asymptotic time complexity?\n** Ex. 24 \u2014 Designafastalgorithmtocomputetheremainderofthedivisionofanunsigned\nnumber by a number of the form (2m\u22121). What is its asymptotic time complexity?\n** Ex. 25 \u2014 Design an O(log(uv)2) algorithm to find the greatest common divisor of two\nbinary numbers u and v. [HINT: The gcd of two even numbers u and v is 2\u2217gcd(u\/2,v\/2)]\nFloating Point Arithmetic\nEx. 26 \u2014 Givethesimplestpossiblealgorithmtocomparetwo32-bitIEEE754floatingpoint\nnumbers. Do not consider \u00b1\u221e, NAN, and (negative 0). Prove that your algorithm is correct.\nWhat is its time complexity ?\nEx. 27 \u2014 Design a circuit to compute (cid:100)log (n)(cid:101). What is its asymptotic time complexity?\n2\nAssume n is an integer. How can we use this circuit to convert n to a floating point number?\nEx. 28 \u2014 AandB,aresavedinthecomputerasA(cid:48) andB(cid:48). Neglectinganyfurthertruncation\nor roundoff errors, show that the relative error of the product is approximately the sum of the\nrelative errors of the factors.\nEx. 29 \u2014 Explain floating point addition with a flowchart.\nEx. 30 \u2014 Explain floating point multiplication with a flowchart.\nEx. 31 \u2014 Can we use regular floating point division for dividing integers also? If not, then\nhow can we modify the algorithm for performing integer division?\nEx. 32 \u2014 Describe in detail how the \u201cround to nearest\u201d rounding mode is implemented.\n*** Ex. 33 \u2014 We wish to compute the square root of a floating point number in hardware\nusing the Newton-Raphson method. Outline the details of an algorithm, prove it, and compute\nits computational complexity. Follow the following sequence of steps.\n1.Find an appropriate objective function.\n2.Find the equation of the tangent, and the point at which it intersects the x-axis.\n3.Find an error function.\n4.Calculate an appropriate initial guess for x.\n5.Prove that the magnitude of the error is less than 1.\n6.Prove that the error decreases at least by a constant factor per iteration. (cid:13)c Smruti R. Sarangi 322\n7.Evaluate the asymptotic complexity of the algorithm.\nDesign Problems\nEx. 34 \u2014 Implement an adder and a multiplier in a hardware description language such as\nVHDL or Verilog.\nEx. 35 \u2014 Extend your design for implementing floating point addition and multiplication.\nEx. 36 \u2014 ReadabouttheSRTdivisionalgorithm,commentonitscomputationalcomplexity,\nand try to implement it in VHDL\/Verilog. 8\nProcessor Design\nNow, we are all set to design a basic processor for the SimpleRisc instruction set. We have\nassembledthearsenaltodesignaprocessoraswellasmakingitefficientbyourstudyofassembly\nlanguages, basic elements of a digital circuit (logical elements, registers, and memories), and\ncomputer arithmetic.\nWe shall start out by describing the basic elements of a processor in terms of fetching\nan instruction from memory, fetching its operands, executing the instruction, and writing the\nresults back to memory or the register file. We shall see that along with the basic elements\nsuch as adders and multipliers, we need many more structures to efficiently route instructions\nbetweendifferentunits,andensurefastexecution. Formodernprocessors,therearetwoprimary\ndesign styles \u2013 hardwired, and microprogrammed. In the hardwired design style, we complicate\na simple processor by adding more elaborate structures to process instructions. There is a net\nincrease in complexity. In the microprogrammed design style, we have a very simple processor\nthat controls a more complicated processor. The simple processor executes basic instructions\nknown as microinstructions for each program instruction, and uses these microinstructions to\ncontrol the operation of the complicated processor. Even though the hardwired design style is\nmuch more common today, the microprogrammed design style is still used in many embedded\nprocessors. Secondly, some aspects of microprogramming have crept into today\u2019s complex\nprocessors.\n8.1 Design of a Basic Processor\n8.1.1 Overview\nThe design of a processor is very similar to that of a car assembly line (see Figure 8.1). A car\nassembly line first casts raw metal into the chassis of a car. Then the engine is built, and put\non the chassis. Then it is time to connect the wheels, the dashboard, and the body of the car.\nThe final operation is to paint the car, and test it for manufacturing defects. The assembly\n323 (cid:13)c Smruti R. Sarangi 324\nFigure 8.1: Car Assembly Line\nline represents a long chain of actions, where one station performs a certain operation, and\npasses on a half built car to the next station. Each station also uses a pool of raw materials to\naugment the half built car such as metal, paint, or accessories.\nWe can think of a processor on the same lines as a car assembly line. In the place of a car,\nwe have an instruction. The instruction goes through various stages of processing. The same\nway that raw metal is transformed to a beautiful finished product in a car factory, a processor\nacts upon a sequence of bits to do complex arithmetic and logical computations.\nInstruction Operand Execute Memory Register\nFetch Fetch Access Write\n(IF) (OF) (EX) (MA) (RW)\nFigure 8.2: Five stages of instruction processing\nWe can broadly divide the operation of a processor into five stages as shown in Figure 8.2.\nThe first step is to fetch the instruction from memory. The underlying organisation of the\nmachine does not matter. The machine can be a Von Neumann machine (shared instruction\nand data memory), or a Harvard machine (dedicated instruction memory). The fetch stage has\nlogical elements to compute the address of the next instruction. If the current instruction, is\nnot a branch, then we need to add the size of the current instruction (4 bytes) to the address\nstored in the PC. However, if the current instruction is a branch, then the address of the next\ninstruction depends on the outcome and target of the branch. This information is obtained\nfrom other units in the processor.\nThe next stage is to \u201cdecode\u201d the instruction and fetch its operands from registers. Simp-\nleRisc defines 21 instructions, and the processing required for different instruction types is very\ndifferent. Forexample,load-storeinstructionsuseadedicatedmemoryunit,whereasarithmetic\ninstructions do not. To decode an instruction, processors have dedicated logic circuits that gen-\nerate signals based on fields in the instruction. These signals are then used by other modules\nto properly process the instruction. The SimpleRisc format is very simple. Hence, decoding\nthe instruction is very easy. However, commercial processors such as Intel processors have very\nelaborate decode units. Decoding the x86 instruction set is very complicated. Irrespective of 325 (cid:13)c Smruti R. Sarangi\nthe complexity of decoding, the process of decoding typically contains the following steps \u2013\nextracting the values of the operands, calculating the embedded immediate values and extend-\ning them to 32 or 64 bits, and generating additional information regarding the processing of\nthe instruction. The process of generating more information regarding an instruction involves\ngenerating processor specific signals. For example, we can generate signals of the form \u201cenable\nmemory unit\u201d for load\/store instructions. For a store instruction, we can generate a signal to\ndisable register write functionality.\nIn our SimpleRisc processor, we need to extract the immediate, and branch offset values\nembedded in the instruction. Subsequently, we need to read the values of the source registers.\nThere is a dedicated structure in the processor called the register file that contains all the 16\nSimpleRisc registers. For a read operation, it takes the number of the register as input, and\nproduces the contents of the register as its output. In this step, we read the register file, and\nbuffer the values of register operands in latches.\nThe next stage executes arithmetic and logical operations. It contains a arithmetic and\nlogical unit(ALU) that is capable of performing all arithmetic and logical operations. The ALU\nis also required to compute the effective address of load-store operations. Typically this part\nof the processor computes the outcome of branches also.\nDefinition 57\nThe ALU (arithmetic logic unit) contains elements for performing arithmetic and logical\ncomputations on data values. The ALU typically contains an adder, multiplier, divider, and\nhas units to compute logical bitwise operations.\nThe next stage contains the memory unit for processing load-store instructions. This unit\ninterfaces with the memory system, and co-ordinates the process of loading and storing values\nfrom memory. We shall see in Chapter 10 that the memory system in a typical processor is\nfairly complex. Some of this complexity is implemented in this part of the processor. The last\nstep in processing an instruction is to write the values computed by the ALU or loaded values\nobtained from the memory unit to the register file.\n8.2 Units in a Processor\n8.2.1 Instruction Fetch \u2013 Fetch Unit\nWestartoutbyfetchinganinstructionfrommainmemory. RecallthataSimpleRisc instruction\nisencodedasasequenceof32bitsor4bytes. Hence,tofetchaninstructionweneedthestarting\naddressoftheinstruction. Letusstorethestartingaddressoftheinstructioninaregistercalled\nthe program counter (pc).\nImportant Point 12\nLet us make an important distinction here between the terms PC and pc. PC is an acronym (cid:13)c Smruti R. Sarangi 326\nfor \u201cprogram counter\u201d. In comparison, pc is a register in our pipeline, and will only be used\nto refer to the register, and its contents. However, PC is a general concept and will be used\nin the place of the term, \u201cprogram counter\u201d, for the sake of brevity.\nSecondly, we need a mechanism to update the PC to point to the next instruction. If the\ninstruction is not a branch then the PC needs to point to the next instruction, whose starting\naddress is equal to the value of the old PC plus 4 (REASON: each instruction is 4 bytes long).\nIf the instruction is a branch, and it is taken, then the new value of the PC needs to be equal\nto the address of the branch target. Otherwise, the address of the next instruction is equal to\nthe default value (current PC + 4).\nFetch unit\nisBranchTaken\n32 branchPC\n32 1\n4\n32 32 inst\npc\ntriggered by a negative\nInstruction\nclock edge\nmemory\n1 - input 1\n1\n0 - input 0\nMultiplexer\ncontrol signal\nFigure 8.3: The Fetch Unit\nFigure 8.3 shows an implementation of the circuit for the fetch unit. There are two basic\noperations that need to be performed in a cycle \u2013 (1) computation of the next PC, and (2)\nfetching the instruction.\nThe PC of the next instruction can come from two sources in SimpleRisc as shown in\nFigure 8.3. We can either use an adder and increment the current PC by 4, or we can get\n0\n0 327 (cid:13)c Smruti R. Sarangi\nthe address from another unit that calculates the branch target(branchPC), and the fact that\nthe branch is taken. We can use a multiplexer to choose between these two inputs. Once, the\ncorrect input is chosen, it needs to be saved in the pc register and sent to the memory system\nfor fetching the instruction. We can either use a combined memory for both instruction and\ndata (Von Neumann Machine) or use a separate instruction memory (Harvard Machine). The\nlatter option is more common. The instruction memory is typically implemented as an array\nof SRAM cells. The fetch unit provides the address in the SRAM array, and then uses the 32\nbits stored at the specified starting address as the contents of the instruction.\nBefore proceeding to decode the instruction, let us make an important observation. Let us\nlistdowntheexternalinputsofthefetchunit. Theyconsistofthe(1)branchtarget(branchPC),\n(2) instruction contents, (3) and the signal to control the multiplexer (isBranchTaken). The\nbranchtargetistypicallyprovidedbythedecodeunit,ortheinstructionexecutionunit. Thein-\nstructioncontentsareobtainedfromtheinstructionmemory. Letusnowconsiderthecaseofthe\nsignal to control the multiplexer \u2013 isBranchTaken. The conditions for settingisBranchTaken\nare shown in Table 8.1.\nInstruction Value of isBranchTaken\nnon-branch instruction 0\ncall 1\nret 1\nb 1\nbranch taken \u2013 1\nbeq\nbranch not taken \u2013 0\nbranch taken \u2013 1\nbgt\nbranch not taken \u2013 0\nTable 8.1: Conditions for setting the isBranchTaken signal\nIn our processor, a dedicated branch unit generates the isBranchTaken signal. It first\nanalyses the instruction. If the instruction is a non-branch instruction, or a call\/ret\/b instruc-\ntion, then the value of isBranchTaken can be decided according to Table 8.1. However, if the\ninstructionisaconditionalbranchinstruction(beq\/bgt), thenitisnecessarytoanalysetheflags\nregister. Recall that the flags register contains the results of the last compare instruction (also\nsee Section 3.3.2). We shall describe a detailed circuit for the branch unit in Section 8.2.4.\nLetusrefertothisstageofinstructionprocessingastheIF (instructionfetch)stage. Before,\nwe proceed to other stages, let us slightly digress here, and discuss two important concepts \u2013\ndata path, and control path.\n8.2.2 Data Path and Control Path\nWe need to make a fundamental observation here. There are two kinds of elements in a circuit.\nThe first type of elements are registers, memories, arithmetic, and logic circuits to process data\nvalues. The second type of elements are control units that decide the direction of the flow of\ndata. The control unit in a processor typically generates signals to control all the multiplexers.\nThesearecalledcontrol signalsprimarilybecausetheirroleistocontroltheflowofinformation. (cid:13)c Smruti R. Sarangi 328\nWe can thus conceptually think of a processor as consisting of two distinct subsystems. The\nfirst is known as the data path that contains all the elements to store and process information.\nFor example the data memory, instruction memory, register file, and the ALU (arithmetic logic\nunit), are a part of the data path. The memories and register file store information, whereas\nthe ALU processes information. For example, it adds two numbers, and produces the sum as\nthe result, or it can compute a logical function of two numbers.\nIn comparison, we have a control path that directs the proper flow of information by gener-\nating signals. We saw one example in Section 8.2.1, where the control path generated a signal\nto direct a multiplexer to choose between the branch target and the default next PC. The\nmultiplexer in this case was controlled by a signal isBranchTaken.\nWe can think of the control path and data path as two distinct elements of a circuit much\nlike the traffic network of a city. The roads and the traffic lights are similar to the data path,\nwhere instead of instructions, cars flow. The circuits to control traffic lights constitute the\ncontrol path. The control path decides the time of the transitions of lights. In modern smart\ncities, the process of controlling all the traffic lights in a city is typically integrated. If it is\npossible to intelligently control traffic to route cars around traffic jams, and accident sites.\nSimilarly, a processor\u2019s control unit is fairly intelligent. Its job is to execute instructions as\nquickly as possible. In this book, we shall study a basic version of a control unit. However, the\ncontrol unit will get very complicated in an advanced course on computer architecture.\nDefinition 58\nData Path The data path consists of all the elements in a processor that are dedicated to\nstoring, retrieving, and processing data such as register files, memories, and ALUs.\nControl Path The control path primarily contains the control unit, whose role is to gener-\nate appropriate signals to control the movement of instructions, and data in the data\npath.\nA conceptual diagram showing the relationship between the control path and the data path\nis shown in Figure 8.4. After this short digression, let us now move on to discuss the next stage\nof instruction processing.\n8.2.3 Operand Fetch Unit\nSimpleRisc Instruction Format\nLet us quickly recapitulate our knowledge about the SimpleRisc instruction format. The list of\nSimpleRisc instructions is shown in Table 8.2 along with their opcodes, and instruction format.\nSimpleRisc isasimpleandregularinstructionset. Ithasthreeclassesofinstructionformats\nas shown in Table 8.3. The instruction formats are branch, register, and immediate. The add,\nsub, mul, div, mod, and, or, cmp, not, lsl, lsr, asr, and mov instructions can have either the\nregister or the immediate format. This is decided by the I bit (27th bit) in the instruction. 329 (cid:13)c Smruti R. Sarangi\nControl path\nInterconnection network\nData path elements\nFigure 8.4: Relationship between the data path and control path\nInst. Code Format Inst. Code Format\nadd 00000 add rd, rs1, (rs2\/imm) lsl 01010 lsl rd, rs1, (rs2\/imm)\nsub 00001 sub rd, rs1, (rs2\/imm) lsr 01011 lsr rd, rs1, (rs2\/imm)\nmul 00010 mul rd, rs1, (rs2\/imm) asr 01100 asr rd, rs1, (rs2\/imm)\ndiv 00011 div rd, rs1, (rs2\/imm) nop 01101 nop\nmod 00100 mod rd, rs1, (rs2\/imm) ld 01110 ld rd, imm[rs1]\ncmp 00101 cmp rs1, (rs2\/imm) st 01111 st rd, imm[rs1]\nand 00110 and rd, rs1, (rs2\/imm) beq 10000 beq offset\nor 00111 or rd, rs1, (rs2\/imm) bgt 10001 bgt offset\nnot 01000 not rd, (rs2\/imm) b 10010 b offset\nmov 01001 mov rd, (rs2\/imm) call 10011 call offset\nret 10100 ret\nTable 8.2: List of instruction opcodes\nFormat Definition\nbranch op (28-32) offset (1-27)\nregister op (28-32) I (27) rd (23-26) rs1 (19-22) rs2 (15-18)\nimmediate op (28-32) I (27) rd (23-26) rs1 (19-22) imm (1-18)\nop \u2192 opcode, offset \u2192 branch offset, I \u2192 immediate bit, rd \u2192 destination register\nrs1 \u2192 source register 1, rs2 \u2192 source register 2, imm \u2192 immediate operand\nTable 8.3: Summary of instruction formats (cid:13)c Smruti R. Sarangi 330\nThe cmp instruction does not have a destination register. The mov and not instructions have\nonly one source operand. For further details, the reader can refer to Table 8.2, or Section 3.3.\nThe Operand Fetch Unit\nThe operand fetch unit has two important functions \u2013 (1) calculate the values of the immediate\noperand and the branch target by unpacking the offset embedded in the instruction, and (2)\nread the source registers.\nComputation of the Immediate Operand and the Branch Target\nimm 18 calculate 32 immx\ninst[1:18] immediate\npc\ninst 27 shift by 2 bits 32 branchTarget\nand extend sign\nFigure 8.5: Calculation of the immediate operand and the branch target\nFigure 8.5 shows the circuit for calculating the immediate operand, and the branch target.\nTo calculate the immediate operand, we need to first extract the imm field (bits 1-18) from\nthe instruction. Subsequently, we extract the lower 16 bits, and create a 32-bit constant in\naccordance with the modifiers (bit 17, and 18). When no modifier is specified, we extend the\nsign of the 16-bit number to make it a 32-bit number. For the u modifier, we fill the top 16\nbits with 0s, and for the h modifier, we shift the 16-bit number, 16 positions to the left. The\nnewly constructed 32-bit value is termed as immx.\nIn a similar manner, we can compute the signal, branchTarget (branch target for all types\nof branches excluding ret). We need to first extract the 27 bit offset (bits 1 to 27) from the\ninstruction. Note that these 27 bits represent the offset in terms of memory words as described\nin Section 3.3.14. Thus, we need to shift the offset to the left by 2 bits to make it a 29\nbit number, and then extend its sign to make it a 32-bit number. Since we use PC-relative\naddressing in SimpleRisc , to obtain the branch target we need to add the shifted offset to the\nPC. The branch target can either be derived from the instruction (branchTarget signal), as we\nhave just described, or in the case of a ret instruction, the branch target is the contents of the\nra register. In this case, the ra register comes from the register file. We choose between both\nthe values in the next stage, and compute branchPC.\nThere is a need to make an important observation here. We are calculating branchTarget\nand immx for all instructions. However, any instruction in the SimpleRisc format will only\nrequire at the most one of these fields (branchTarget or immx). The other field will have junk\nvalues. Nevertheless, it does not hurt to pre-compute both the values in the interest of speed.\nIt is necessary to ensure that the correct value is used in the later stages of processing. 331 (cid:13)c Smruti R. Sarangi\nReading the Registers\nisRet\nra(15)\n1\nop1\nA read port 1 D\nrs1\ninst[19:22]\ninst rd\ninst[23:26] 1 op2\nA read port 2 D\nrs2\ninst[15:18] A address\nRegister\nisSt\nfile D data\nFigure 8.6: Reading the Source Registers\nIn parallel, we can read the values of the source registers as shown. Here, also we follow the\nsame strategy. We read more than what we require. Critics might argue that this approach\nwastes power. However, there is a reason for doing so. Extra circuitry is required to decide if\na given operand is actually required. This has an adverse impact in terms of area, and time.\nThe operand fetch unit becomes slower. Hence, we prioritise the case of simplicity, and read\nall the operands that might be required.\nThe circuit for reading the values of source registers is shown in Figure 8.6. The register\nfile has 16 registers, two read ports, and one write port (not shown in the figure). A port is a\npoint of connection (an interface) in a hardware structure, and is used for the purpose of either\nentering inputs, or reading outputs. We can have a read port (exclusively for reading data), a\nwrite port (exclusively for writing data), and a read-write port (can be used for both reading\nand writing).\nDefinition 59\nA port is a point of connection in a hardware structure, and is used for the purpose of\neither entering inputs, or reading outputs. We can have a read port (exclusively for reading\ndata), a write port (exclusively for writing data), and a read-write port (can be used for both\nreading and writing).\nForthefirstregisteroperand, op1, wehavetwochoices. ForALU,andmemoryinstructions,\nwe need to read the first source register, rs1 (bits 19 to 22). For the ret instruction, we need to\nread the value of the return address register, ra. To choose between the contents of the field,\nrs1, in the instruction and ra, we use a multiplexer. The multiplexer is controlled by a signal,\nisRet. If isRet (is return) is equal to 1, then we choose ra, otherwise we choose rs1. This value\n0\n0 (cid:13)c Smruti R. Sarangi 332\nis an input to the register file\u2019s first read port. We term the output of the first read port as op1\n(operand 1).\nWe need to add a similar multiplexer for the second read port of the register file too. For\nall the instructions other than the store instruction, the second source register is specified by\nthe rs2 (bits 15 to 18) field in the instruction. However, the store instruction is an exception.\nIt contains a source register in rd (bits 23 to 26). Recall that we had to make this bitter choice\nat the cost of introducing a new instruction format. Since we have a very consistent instruction\nformat (see Table 8.3) the process of decoding is very simple. To extract different fields of the\ninstruction (rs1, rs2, opcode, and imm) we do not need additional logic elements. We need to\nsave each bit of the instruction in a latch, and then route the wires appropriately.\nComing back to our original problem of choosing the second register operand, we observe\nthat we need to choose the right source register \u2013 rs2 or rd. The corresponding multiplexer is\ncontrolled by the isSt (is store) signal. We can quickly find out if the instruction is a store by\nusing a set of logic gates to verify if the opcode is equal to 01111. The result of the comparison\nis used to set the isSt signal. The corresponding output of the register file is termed as op2\n(operand 2).\nOperand fetch unit Execute\nFetch unit\nunit\n(opcode, I bit) 6 Control\ninst[27:32] unit\ns\nd imm 18 calculate 32 immx\nn\na inst[1:18] immediate\npc r\ne\np\no\ninst . m 27 shift by 2 bits 32 branchTarget\nm and extend sign\ni\nisRet\nra(15)\n1\nop1\ns rs1 A read port 1 D\nd\nn\na inst[19:22]\nr\ne rd op2\np o inst[23:26] 1 D\n. A read port 2\ng\ne rs2 A address\nr Register\ninst[15:18]\nisSt file D data\nFigure 8.7: Operand Fetch Stage\nLastly, it is necessary to send the opcode (5 bits), and the immediate bit (1 bit) to the\ncontrol unit such that it can generate all the control signals. The complete circuit for the\noperand fetch unit is shown in Figure 8.7. op1, op2, branchTarget, and immx are passed to\n0\n0 333 (cid:13)c Smruti R. Sarangi\nthe execute unit.\n8.2.4 Execute Unit\nLet us now look at executing an instruction. Let us start out by dividing instructions into\ntwo types \u2013 branch and non-branch. Branch instructions are handled by a dedicated branch\nunit that computes the outcome, and final target of the branch. Non branch instructions are\nhandled by an ALU (arithmetic logic unit).\nBranch Unit\nbranchPC\nbranchTarget\n0\nop1\n1\nisRet\nisBranchTaken\nisUBranch\nisBeq\nflags.E\nisBgt\nflags.GT\nflags\nFigure 8.8: Branch Unit\nThe circuit for the branch unit is shown in Figure 8.8.\nFirst, we use a multiplexer to choose between the value of the return address (op1), and the\nbranchTarget embedded in the instruction. The isRet signal controls the multiplexer. If it is\nequal to 1, we choose op1; otherwise, we choose branchTarget. The output of the multiplexer,\nbranchPC, is sent to the fetch unit.\nNow, let us consider the circuit to compute the branch outcome. As an example, let us\nconsider the case of the beq instruction. Recall that the SimpleRisc instruction set requires a\nflags register that contains the result of the last compare (cmp) instruction. It has two bits\n\u2013 E and GT. If the last compare instruction led to an equality, then the E bit is set, and\nif the first operand was greater than the second operand then the GT bit is set. For the beq\ninstruction, the control unit sets the signal isBeq to 1. We need to compute a logical AND of\nthis signal and the value of the E bit in the flags register. If both are 1, then the branch is\nFO\nmorf (cid:13)c Smruti R. Sarangi 334\ntaken. Similarly, weneedanANDgatetocomputetheoutcomeofthebgtinstruction, asshown\nin Figure 8.8. The branch might also be unconditional (call\/ret\/b). In this case, the control\nunit sets the signal isUBranch to 1. If any of the above conditions is true, then the branch is\ntaken. We subsequently use an OR gate that computes the outcome of the branch, and sets\nthe isBranchTaken signal. This signal is used by the fetch unit to control the multiplexer that\ngenerates the next PC.\nALU\nA\nop1\ns\nt ALU\ns n aluResult\ni m immx B (Arithmetic\ne 1 logic unit)\nm\nd n op2 0 s\na\nU\nL\nA\nisImmediate aluSignals\nFigure 8.9: ALU\nFigure 8.9 shows the part of the execution unit that contains the ALU. The first operand\n(A) of the ALU is always op1 (obtained from the operand fetch unit). However, the second\noperand (B) can either be a register or the sign extended immediate. This is decided by the\nisImmediate signal generated by the control unit. The isImmediate signal is equal to the\nvalue of the immediate bit in the instruction. If it is 1, then the multiplexer in Figure 8.9\nchooses immx as the operand. If it is 0, then op2 is chosen as the operand. The ALU takes\nas input a set of signals known collectively as aluSignals. They are generated by the control\nunit, and specify the type of ALU operation. The result of the ALU is termed as aluResult.\nFigure 8.10 shows the design of the ALU. The ALU contains a set of modules. Each module\ncomputes a separate arithmetic or logical function such as addition or division. Secondly, each\nmodule has a dedicated signal that enables or disables it. For example, there is no reason to\nenable the divider when we want to perform simple addition. There are several ways that we\ncan enable or disable an unit. The simplest method is to use a transmission gate for every\ninput bit. A transmission gate is shown in Figure 8.11. If the signal(S) is turned on, then the\noutput reflects the value of the input. Otherwise, it maintains its previous value. Thus, if the\nenabling signal is off, then the module does not see the new inputs. It thus does not dissipate\nany power, and is effectively disabled.\nLet us consider each of the modules in the ALU one after another. The most commonly\nused module is the adder. It is used by add, sub, and cmp instructions, as well as by load and\nstore instructions to compute the memory address. It takes A and B as inputs. Here, A and B\nare the values of the source operands. If the isAdd signal is turned on, then the adder adds the\noperands. Likewise, if the isSub signal is turned on, then the adder adds the 2\u2019s complement of\nB with A. In effect, it subtracts B from A. If the isCmp flag is turned on, then the adder unit\nsubtractsB fromAandsetsthevalueoftheflagsregister. Iftheoutputis0, thenitsetstheE 335 (cid:13)c Smruti R. Sarangi\nisLsl isLsr isAsr\nisAdd isSub isCmp\nShift A\nflags\nunit B\nA\nAdder\nB\nisOr isNot isAnd\nisMul\nA\nLogical\nA\nB Multiplier unit B\nisDiv isMod\nisMov\nA\nDivider B\nB Mov\naluResult\nFigure 8.10: ALU\nS\nS\nFigure 8.11: A transmission gate\nbit. If the output is positive, it sets the GT bit. If none of these signals (isAdd\/isSub\/isCmp)\nis true, then the adder is disabled.\nThe multiplier and divider function in a similar manner. The multiplier is enabled by the\nisMul signal, and the divider is enabled by the isDiv or isMod signal. If the isDiv signal is\ntrue, then the result is the quotient of the division, whereas, if the isMod signal is true, the\nresult is the remainder of the division.\nThe shift unit left shifts, or right shifts A, by B positions. It takes three signals \u2013 isLsl,\nisLsr, and isAsr. The logical unit consists of a set of AND, OR, and NOT gates. They are\nenabled by the signals isOr, isAnd, and isNot respectively. The Mov unit is slightly special\nin the sense that it is the simplest. If the isMov signal is true, then the output is equal to B.\nOtherwise, it is disabled.\nTo summarise, we show the full design of the execution unit (branch unit and ALU) in (cid:13)c Smruti R. Sarangi 336\nFigure 8.12. To set the output (aluResult), we need a multiplexer that can choose the right\noutput out of all the modules in the ALU. We do not show this multiplexer in Figure 8.12.\nWe leave the detailed design of the ALU circuit along with the transmission gates and output\nmultiplexer as an exercise for the reader.\nExecute unit\nbranchPC\nbranchTarget\n0\nop1\n1\nisRet\nisBranchTaken\nisUBranch\nisBeq\nflags.E\nisBgt\nflags.GT\nflags\nA\nop1\ns\nt ALU\ns n aluResult\ni\nm immx\nB (Arithmetic\ne 1 logic unit)\nm\nd n op2 0 s\na\nU\nL\nA\nisImmediate aluSignals\nFigure 8.12: Execute Unit (Branch and ALU unit)\n8.2.5 Memory Access Unit\nFigure 8.13 shows the memory access unit. It has two inputs \u2013 data and address. The address\nis calculated by the ALU. It is equal to the result of the ALU (aluResult). Both the load and\nstore instructions use this address. The address is saved in a register traditionally known as\nMAR (memory address register).\nLet us now consider the case of a load instruction. In SimpleRisc , the format of the load\ninstruction is ld rd, imm[rs1]. The memory address is equal to the immediate value plus the\ncontents of register, rs1. This is the value of aluResult. The memory unit does not require any\nother inputs. It can proceed to fetch the value of the memory address from the data memory.\nThe memory unit reads 4 bytes starting from the memory address. The result (ldResult) is\nnow ready to be written to the destination register.\nFO\nmorf 337 (cid:13)c Smruti R. Sarangi\nisLd isSt\n32 op2\nmdr\n32 ldResult\nMemory unit\n32 aluResult\nmar\nmemory\nmdr\ndata reg.\nData memory\nmemory\nmar\naddress reg.\nFigure 8.13: Memory Unit\nThe format of the store instruction is : st rd, imm[rs1]. The address is computed using\nthe ALU similar to the way the address is calculated for the load instruction. For the store\ninstruction, rdisasourceregister. Thecontentsofregisterrd(reg[rd])arereadbytheoperand\nfetchunit(seeSection8.2.3). Thisvalueistermedasop2inFigure8.6. op2containsthecontents\nof register rd, and represents the data of the store instruction. The memory unit writes the\nvalue of op2 to the MDR (memory data register) register. In parallel, it proceeds to write the\ndata to data memory. The store instruction does not have an output.\nNote that here also we follow the same naming scheme as we had followed for PC and pc.\nMAR is an acronym for (memory address register), whereas mar refers specifically to the mar\nregister in the data path.\nNow, the memory unit takes two control signals as inputs \u2013 isLd, and isSt. For obvious\nreasons, at most one of these signals can be true at any one time. If none of these signals is\ntrue, then the instruction is not a memory instruction, and the memory unit is disabled.\nA subtle point needs to be discussed here. MAR and MDR are traditionally referred\nto as registers. However, they are not conventional edge triggered registers. They are used\nlike temporary buffers that buffer the address and the store values till the memory request\ncompletes.\n8.2.6 Register Writeback Unit\nThe last step of instruction processing is to write the computed values back to the register file.\nThis value can be the output of a load or ALU instruction, or the return address written by\nthe call instruction. This process is known as writeback, or register writeback. We refer to this (cid:13)c Smruti R. Sarangi 338\nRegister writeback unit\nRegister file\nA\nread port 1 isWb\nD E ra(15)\n1\nwrite port A\nrd (inst[23:26])\nA D 0\nread port 2\nD isCall\n32 aluResult\n00\n32 ldResult result E enable\n01\n32 pc A address\n10\nD data\nisLd\n4\nisCall\nFigure 8.14: Register Writeback Unit\nunit as the register writeback(RW) unit. Its circuit diagram is shown in Figure 8.14.\nWe first need to choose the right source operand. We have three choices \u2013 aluResult,\nldResult, or the return address. The return address is equal to the PC of the call instruction\nplus 4. We use a multiplexer to choose between the three input values. We use two control\nsignals to control the multiplexer. The first control signal is isLd (is load), and the second\ncontrol signal is isCall. We choose aluResult, when both the control signals are 0. We choose\nldResult, when isLd = 1, and isCall = 0, and lastly, we choose PC +4, when isCall is equal\nto 1. The output of the multiplexer, result, needs to be written to the register file.\nNote that we had shown a partial view of the register file when we discussed the operand\nfetch unit in Section 8.2.3. We showed only two read ports. However, the register file has also\na write port that is used to write data. The write port has three inputs \u2013 address, data, and\nenable bit. The address is either the number of the destination register rd or the id of the\nreturn address register (15). The correct address needs to be chosen with a multiplexer. The\ndestination register is specified by bits 23 to 26 of the instruction. The second multiplexer\nchooses the data that needs to be written. The output of this multiplexer is sent to the data\npins of the write port. Lastly, we need an enable signal (isWb) that specifies if we need to write\nthe value of a register. For example, the store, nop, and compare instructions do not need a\nregister writeback. Hence, for these instructions, the value of isWb is false. It is also false for\nbranch (excluding call) instructions. isWb is true for the rest of the ALU instructions, mov\nand ld instructions. 339 (cid:13)c Smruti R. Sarangi\n8.2.7 The Data Path\n1\npc + 4 0\npc\nInstruction\nmemory\nInstruction Control\nrd rs2 ra(15) rs1 unit\nisRet\n1 0 1 0\nisSt\nisWb\nImmediate and Register data\nbranch target file\nreg\nop2 op1\nimmx\n0 1 1 0 isImmediate\nisRet aluSignals\nisBranchTaken\nB A s\ng isBeq\nBranch\nALU a unit isBgt\nfl isUBranch\nmar mdr\nisLd\nData memory Memory isSt\nunit\npc\n4 isLd\n10 01 00 isCall\nrd 0\ndata ra(15) 1\nFigure 8.15: A basic processor\nLet us now form the whole by joining all the parts. We have up till now divided a processor\ninto five basic units: instruction fetch unit (IF), operand fetch unit (OF), execution unit (EX), (cid:13)c Smruti R. Sarangi 340\nmemory access unit (MA), and register writeback unit (RW). It is time to combine all the parts\nand look at the unified picture. Figure 8.15 shows the result of all our hard work. We have\nomitted detailed circuits, and just focused on the flow of data and control signals.\nEvery clock cycle, the processor fetches an instruction from a new PC, fetches the operands,\nexecutestheinstruction,andwritestheresultsbacktothedatamemoryandregisterfile. There\nare two memory elements in this circuit namely the data and instruction memory. They can\npossibly refer to the same physical memory structure, or refer to different structures. We shall\nhave ample opportunities to discuss the different schemes in Chapter 10.\nThe main state elements of the processor are the following registers: pc, and flags registers,\nandtheregisterfile. Wecanoptionallyaddmar andmdr registerswiththememoryunit. Note\nthat they are strictly not required in our simple version of the processor. However, they shall\nbe required in advanced designs where a memory request can possibly take multiple cycles. We\nshall also require them in our microprogrammed processor. Hence, it is a good idea to keep\nthem in our basic design.\nNote that most of the sets of wires in the data path have a top-down orientation i.e., the\nsource is above the destination in Figure 8.15. There are two notable exceptions. The source of\nthese wires is below the destination in Figure 8.15. The first such exception is the set of wires\nthat carry the branch target\/outcome information from the execute unit to the fetch unit. The\nsecondexceptionisthesetofwiresthatcarrythedatatobewrittenfromtheregisterwriteback\nunit to the register file.\nWe need to lastly note that the magic of a processor lies in the interplay of the data path\nand the control path. The control signals give a form to the data path. The unique values of\nthe set of all the control signals determine the nature of instructions. It is possible to change\nthebehaviorofinstructions, orinfactdefinenewinstructionsbyjustchangingthecontrolunit.\nLet us take a deeper look at the control unit.\n8.3 The Control Unit\nTable 8.4 shows the list of control signals that need to be generated by the control unit along\nwith their associated conditions. The only control signal that is not generated by the control\nunit is isBranchTaken. This is generated by the branch unit that is a part of the execute unit.\nHowever, the rest of the 22 signals need to be generated by the control unit. Recall that the\ninputs to the control unit are the opcode of the instruction, and the value of the immediate\nbit.\nopcode\ninst[28:32]\nControl\ncontrol\nI bit unit\nsignals\ninst[27]\nFigure 8.16: Abstraction of a hardwired control unit 341 (cid:13)c Smruti R. Sarangi\nSerial No. Signal Condition\n1 isSt Instruction: st\n2 isLd Instruction: ld\n3 isBeq Instruction: beq\n4 isBgt Instruction: bgt\n5 isRet Instruction: ret\n6 isImmediate I bit set to 1\n7 isWb Instructions: add, sub, mul, div,\nmod, and, or, not, mov, ld, lsl, lsr,\nasr, call\n8 isUBranch Instructions: b, call, ret\n9 isCall Instructions: call\naluSignals\n10 isAdd Instructions: add, ld, st\n11 isSub Instruction: sub\n12 isCmp Instruction: cmp\n13 isMul Instruction: mul\n14 isDiv Instruction: div\n15 isMod Instruction: mod\n16 isLsl Instruction: lsl\n17 isLsr Instruction: lsr\n18 isAsr Instruction: asr\n19 isOr Instruction: or\n20 isAnd Instruction: and\n21 isNot Instruction: not\n22 isMov Instruction: mov\nTable 8.4: List of control signals\nThe hardwired control unit for our simple processor can be thought of as a black box that\ntakes 6 bits as input (5 opcode bits, and 1 immediate bit), and produces 22 control signals as\nits output. This is shown in Figure 8.16.\nInternally, there are a set of logic gates that act on the input bits to produce each output\nbit. For example, to set the isAdd signal, we need to check if the opcode is equal to 00000. Let\nus number the five bits of the opcode as op , op , op , op and op . Here op is the LSB, and\n1 2 3 4 5 1\nop is the MSB. Let us refer to the immediate bit as I.\n5\nTable 8.5 shows the conditions for setting all the control signals. We leave the implemen-\ntation of Table 8.5 using logic gates as an exercise to the reader. Note that it will take the\nmaximum amount of time to compute the value of isWb. Nevertheless, this circuit is extremely\nsimple as compared to a multiplier or a carry lookahead adder. Hence, the total execution time\nof the control unit is expected to be small as compared to the execute unit.\nThehardwiredcontrolunitisthusfastandefficient. Thisisthereasonwhymostcommercial\nprocessors today use a hardwired control unit. However, hardwired control units are not very (cid:13)c Smruti R. Sarangi 342\nSerial No. Signal Condition\n1 isSt op .op .op .op .op\n5 4 3 2 1\n2 isLd op op .op .op .op\n5 4 3 2 1\n3 isBeq op .op .op .op .op\n5 4 3 2 1\n4 isBgt op .op .op .op .op\n5 4 3 2 1\n5 isRet op .op .op .op .op\n5 4 3 2 1\n6 isImmediate I\n7 isWb \u223c (op +op .op .op .(op +op ))+\n5 5 3 1 4 2\nop .op .op .op .op\n5 4 3 2 1\n8 isUbranch op .op .(op .op +op .op .op )\n5 4 3 2 3 2 1\n9 isCall op .op .op .op .op\n5 4 3 2 1\naluSignals\n10 isAdd op .op .op .op .op +op .op .op .op\n5 4 3 2 1 5 4 3 2\n11 isSub op .op .op .op .op\n5 4 3 2 1\n12 isCmp op .op .op .op .op\n5 4 3 2 1\n13 isMul op .op .op .op .op\n5 4 3 2 1\n14 isDiv op .op .op .op .op\n5 4 3 2 1\n15 isMod op .op .op .op .op\n5 4 3 2 1\n16 isLsl op .op .op .op .op\n5 4 3 2 1\n17 isLsr op .op .op .op .op\n5 4 3 2 1\n18 isAsr op .op .op .op .op\n5 4 3 2 1\n19 isOr op .op .op .op .op\n5 4 3 2 1\n20 isAnd op .op .op .op .op\n5 4 3 2 1\n21 isNot op .op .op .op .op\n5 4 3 2 1\n22 isMov op .op .op .op .op\n5 4 3 2 1\nTable 8.5: Boolean conditions for setting all the control signals\nflexible. For example, it is not possible to change the behavior of an instruction, or even\nintroduceanewinstruction,aftertheprocessorhasbeenshipped. Sometimesweneedtochange\nthe way an instruction is executed if there are bugs in functional units. For example, if the\nmultiplier has a design defect, then it is theoretically possible to run the Booth\u2019s multiplication\nalgorithm with the adder, and shift units. We will however, need a very elaborate control unit\nto dynamically reconfigure the way instructions are executed.\nThere are other more practical reasons for favoring a flexible control unit. Some instruction\nsets such as x86 have rep instructions that repeat an instruction a given number of times. They\nalso have complicated string instructions that operate on large pieces of data. Supporting such\ninstructions requires a very complicated data path. In principle, we can execute such instruc-\ntions by having elaborate control units that in turn have simple processors to process these\ninstructions. These sub processors can generate control signals for implementing complicated\nCISC instructions. 343 (cid:13)c Smruti R. Sarangi\nWay Point 6\n1. We have successfully designed a hardwired processor that implements the entire Sim-\npleRisc ISA.\n2. Our processor is broadly divided into five stages: IF, OF, EX, MA, and RW.\n3. The data path contains state elements (such as registers), arithmetic units, logical\nunits, and multiplexers to choose the right set of inputs for each functional unit.\n4. The multiplexers are controlled by control signals generated by the control unit.\n8.4 Microprogram-Based Processor\nLet us now look at a different paradigm for designing processors. We have up till now looked\nat a processor with a hardwired control unit. We designed a data path with all the elements\nrequired to process, and execute an instruction. Where there was a choice between the input\noperands, we added a multiplexer that was controlled by a signal from the control unit. The\ncontrol unit took the contents of the instruction as the input, and generated all the control\nsignals. This design style is typically adopted by modern high performance processors. Note\nthat efficiency comes at a cost. The cost is flexibility. It is fairly difficult for us to introduce\nnew instructions. We need to possibly add more multiplexers, and generate many more control\nsignals for each new instruction. Secondly, it is not possible to add new instructions to a\nprocessor after it has been shipped to the customer. Sometimes, we desire such flexibility.\nIt is possible to introduce this additional flexibility by introducing a translation table that\ntranslates instructions in the ISA to a set of simple microinstructions. Each microinstruction\nhas access to all the latches, and internal state elements of a processor. By executing a group\nof microinstructions associated with an instruction, we can realise the functionality of that\ninstruction. Thesemicroinstructionsormicrocodesaresavedinamicrocodetable. Itistypically\npossible to modify the contents of this table via software, and thus change the way hardware\nexecutes instructions. There are several reasons for wanting such kind of flexibility that allows\nus to add new instructions, or modify the behaviour of existing instructions. Some of the\nreasons are as follows:\nDefinition 60\nWe can have an alternate design style, where we break instructions in the ISA to a\nset of microinstructions (microcodes). For each instruction, a dedicated unit executes its\nassociated set of microinstructions to implement its functionality. It is typically possible\nto dynamically change the set of microinstructions associated with an instruction. This\nhelps us change the functionality of the instruction via software. Such kind of a processor\nis known as a microprogrammed processor. (cid:13)c Smruti R. Sarangi 344\n1. Processors sometimes have bugs in the execution of certain instructions [Sarangi et al.,\n2006]. This is because of mistakes committed by designers in the design process, or\ndue to manufacturing defects. One such famous example is the bug in division in the\nIntel(cid:13)R Pentium(cid:13)R processor. Intel had to recall all the Pentium processors that it had\nsold to customers [Pratt, 1995]. If it would have been possible to dynamically change\nthe implementation of the division instruction, then it would not have been necessary to\nrecall all the processors. Hence, we can conclude that some degree of reconfigurability of\nthe processor can help us fix defects that might have been introduced in various stages of\nthe design and manufacturing process.\n2. Processors such as Intel Pentium 4, and later processors such as Intel(cid:13)R CoreTMi3, and\nIntel(cid:13)R CoreTMi7implementsomecomplexinstructionsbyexecutingasetofmicroinstruc-\ntions saved in memory. Complicated operations with strings of data, or instructions that\nlead to a series of repetitive computations are typically implemented using microcode.\nThis means that the Intel processor internally replaces a complex instruction with a snip-\npet of code containing simpler instructions. This makes it easier for the processor to\nimplement complicated instructions. We do not need to unnecessarily make changes to\nthe data path, add extra state, multiplexers, and control signals to implement complex\ninstructions.\n3. Nowadays processors are part of a chip with many other elements. This is known as a\nsystem-on-chip (SOC). For example, a chip in a cellphone might contain a processor, a\nvideo controller, an interface to the camera, a sound and network controller. Processor\nvendors typically hardwire a set of simple instructions, and a lot of other instructions\nfor interfacing with peripherals such as the video and audio controllers are written in\nmicrocode. Depending on the application domain and the set of peripheral components,\nthe microcode can be customised.\n4. Sometimes custom diagnostic routines are written using a set of dedicated microinstruc-\ntions. These routines test different parts of the chip during its operation, report faults,\nandtakecorrectiveaction. Thesebuilt-in-self-test(BIST)routinesaretypicallycustomis-\nable, and are written in microcode. For example, if we desire high reliability, then we can\nmodify the behaviour of instructions that perform reliability checks on the CPU to check\nall components. However, in the interest of time, these routines can be compressed to\ncheck fewer components.\nWe thus observe that there are some compelling reasons to be able to programatically alter\nthebehaviourofinstructionsinaprocessortoachievereliability, implementadditionalfeatures,\nand improve portability. Hence, modern computing systems, especially, smaller devices such as\nphones, and tablets use chips that rely on microcode. Such microcode sequences are popularly\nreferred to as firmware.\nDefinition 61\nModern computing systems, especially, smaller devices such as phones, modems, printers, 345 (cid:13)c Smruti R. Sarangi\nandtabletsusechipsthatrelyonmicrocode. Suchmicrocodesequencesarepopularlyreferred\nto as firmware.\nLet us thus design a microprogram-based processor that provides us significantly more flexi-\nbilityintailoringtheinstructionset,evenaftertheprocessorhasbeenfabricatedandsenttothe\ncustomer. Before we proceed to design the data path and control path of a microprogrammed\nprocessor, we need to note that there is a fundamental tradeoff between a regular hardwired\nprocessor as presented in Section 8.2.7, and a microprogrammed processor. The tradeoff is\nefficiency versus flexibility. We cannot expect to have a very flexible processor that is fast and\npower efficient. Let us keep this important tenet in mind and proceed with the design.\n8.5 Microprogrammed Data Path\nLetusdesignthedatapathforamicroprogrammedprocessor. Letusnotdesignitfromscratch.\nLet us rather modify the data path for the processor as shown in Section 8.2.7. Recall that\nit had some major units such as the fetch unit, register file, ALU, branch unit, and memory\nunit. These units were connected with wires, and whenever there was a possibility of multiple\nsource operands, we added a multiplexer in the data path. The role of the control unit was to\ngenerate all the control signals for the multiplexers.\nThe issue is that the connections to the multiplexers are hardwired. It is not possible\nto establish arbitrary connections. For example, it is not possible to send the output of the\nmemory unit to the input of the execute unit. Hence, we wish to have a design that is free of\nfixed interconnections between components. It should be theoretically possible for any unit to\nsend data to any other unit.\nThe most flexible interconnect is a bus based structure. A bus is a set of common copper\nwires that connect all the units. It supports one writer, and multiple readers at any point of\ntime. For example, unit A can write to the bus at a certain point of time, and all the other\nunits can get the value that unit A writes. It is possible to send data from one unit to another\nunit, or from one unit to a set of other units if required. The control unit needs to ensure that\nat any point of time, only one unit writes to the bus, and the unit that needs to process the\nvalue that is being written, reads the value from the bus.\nDefinition 62 A bus is a set of common wires that connect all the functional units in a\nprocessor. It supports one writer, and multiple readers at any point of time.\nLet us now proceed to design simplified versions of all the units that we introduced for\nour hardwired processor. These simplified versions can aptly be used in the data path of our\nmicroprogrammed processor. (cid:13)c Smruti R. Sarangi 346\npc ir\nInstruction memory\nShared bus\nFigure 8.17: The fetch unit in a microprogrammed processor\n8.5.1 Fetch Unit\nLet us start out by explaining the design philosophy of the microprogrammed processor. We\nadd registers with every unit. These registers store the input data for the specific unit, and a\ndedicated output register stores the results generated by the unit. Both these sets of registers\nare connected to the common bus. Unlike the hardwired processor, where there was a good\namount of coupling across different units, units in a microprogrammed processor are fairly\nindependent of each other. Their\/ job is to perform a set of actions, and put the results back\non to the bus. Each unit is like a function in a programming language. It has an interface\ncomprising of a set of registers to read data in. It typically takes 1 cycle to compute its output,\nand then the unit writes the output value to an output register.\nIn concordance with this philosophy, we present the design of the fetch unit in Figure 8.17.\nIt has two registers \u2013 pc (PC), and ir (instruction register). We shall use the acronym, IR, for\nthe instruction register. ir contains the contents of the instruction. The pc register can read\nits value from the bus, and can also write its value to the bus. We have not connected ir to the\nbus because no other unit is typically interested in the exact contents of the instruction. Other\nunits are only interested in different fields of the instruction. Hence, it is necessary to decode\nthe instruction and break it into a set of different fields. This is done by the decode unit.\n8.5.2 Decode Unit\nThis unit is similar in function to the operand fetch unit as described in Section 8.2.3. How-\never, we do not include the register file in this unit. We treat it as a separate unit in the\nmicroprogrammed processor. Figure 8.18 shows the design of the operand fetch unit.\npc\nir\nI rd rs1 rs2 Immediate immx calc. branchTarget\nunit offset\nShared bus\nFigure 8.18: The decode unit in a microprogrammed processor\nThe job of the decode unit is to break down the contents of the instruction into multiple 347 (cid:13)c Smruti R. Sarangi\nfields, and export them as registers. In specific, the following registers are made available to the\nbus, I (immediate bit), rd (destination register), rs1 (source register 1), rs2 (source register\n2), immx (after processing the modifiers), and branchTarget (branch target). To compute the\nbranch target we calculate the offset from the current PC by extracting bits [1:27] from ir, and\nshifting it to the left by 2 places. This is added to the current value of the PC. Table 8.6 shows\nthe range of bits extracted from ir for each output register.\nRegister Bits in ir\nI 27\nrd 23-26\nrs1 19-22\nrs2 15-18\nimmx 1-18 (process modifiers)\nbranchTarget PC + (ir[1 : 27] (cid:28) 2)\nTable 8.6: List of bits in ir corresponding to each register in the decode unit\nIt is possible that a given program execution might not have values for all the registers.\nFor example, an instruction in the register format will not have an embedded immediate\nvalue. Hence, in this case the immx register will have junk data. However, it does not hurt\nto extract all possible fields from the instruction, and store them in registers. We can use\nonly those registers that contain valid data, and ignore those registers that are not relevant to\nthe instruction. This ensures that our data path remains simple, and we do not need costly\nmultiplexers in the decode unit.\n8.5.3 Register File\nWe had combined the decode unit and the register file, into one unit called the operand fetch\nunit of the hardwired processor. However, we prefer to keep the register file separate in the\nmicroprogrammed processor. This is because in the hardwired processor it was accessed right\nafter decoding the instruction. However, this might not be the case in the microprogrammed\nprocessor. It might need to be accessed possibly several times during the execution of an\ninstruction.\nThe register file has two source registers \u2013 regSrc, and regData. The regSrc register\ncontains the number of the register that needs to be accessed. In the case of a write operation,\nthe regData register contains the value to be written. The args values are directly read from\nthe bus. They contain the commands to the register file. We assume that there are dedicated\nwires in the shared bus to carry the arguments (args) values. They take different values, where\neach value corresponds to an unique operation of an unit. The value 00...0 is a distinguished\nvalue that corresponds to a nop (no operation).\nTheargumentstotheregisterfile, areverysimple\u2013read, write, andnop. Iftheargsspecify\na write operation, then the value in regData is written to the register specified by the regSrc\nregister. If a read operation is specified, then the register specified by regSrc is read and its\nvalue is stored in the register, regVal (register value). (cid:13)c Smruti R. Sarangi 348\nShared bus\ns\ng\nregVal regData ar regSrc\nRegister\nfile\nFigure 8.19: The register file in a microprogrammed processor\nToaccesstheregisterfileitisthusnecessarytowritethenumberoftheregistertotheregSrc\nregister, write the value to be written to the regData register if required, and finally specify\nthe appropriate arguments. The assumption is that after 1 cycle the operation is complete. In\nthe case of a read operation, the value is available in the regVal register.\n8.5.4 ALU\nShared bus\ns\ng\naluResult A r B flags.E flags.GT\na\nALU flags\nFigure 8.20: The ALU in a microprogrammed processor\nThe structure of the ALU is shown in Figure 8.20. It has two input registers, A and B.\nThe ALU performs actions on the values contained in registers, A and B. The nature of the\noperation is specified by the args value. For example, if it specifies an add operation, then the\nALU adds the values contained in registers, A and B. If it specifies a subtract operation, then\nthe value in B is subtracted from the value contained in A. For the cmp instruction, the ALU\nupdates the flags. Recall that in SimpleRisc we use two flags that specify the equality, and\ngreater than conditions. They are saved in the registers flags.E and flags.GT respectively.\nThe result of the ALU operation is then saved in the register aluResult. Here also, we assume\nthat the ALU takes 1 cycle to execute after the args values are specified on the bus. 349 (cid:13)c Smruti R. Sarangi\n8.5.5 Memory Unit\nShared bus\nldResult mar mdr\nData\nmemory\nFigure 8.21: The memory unit in a microprogrammed processor\nThe memory unit is shown in Figure 8.21. Like the hardwired processor, it has two source\nregisters \u2013 mar and mdr. The memory address register(mar) buffers the memory address, and\nthe memory data register (mdr) buffers the value that needs to be stored. Here also, we require\na set of arguments that specify the nature of the memory operation \u2013 load or store. Once, a\nload operation is done, the data is available in the ldResult register.\n8.5.6 Overview of the Data Path\nopcode\n\u03bccontrol\nunit\npc ir\nInstruction memory\nI rd rs1 rs2 Immediate immx calc. branchTarget\nunit offset\nShared bus\ngs gs gs\nldResult mar ar mdr aluResult A ar B flags.E flags.GT regVal regData ar regSrc\nRegister\nData ALU flags\nfile\nmemory\nFigure 8.22: The data path in a microprogrammed processor\nLet us now add all the individual units, and take a look at the entire data path as shown\nin Figure 8.22. Along with all the units that we just described, we have added an extra unit,\nwhich is the microprogrammed control unit (\u00b5control unit). Its role is to execute a set of\nmicroinstructions corresponding to each program instruction, and orchestrate the flow of data\nsgra (cid:13)c Smruti R. Sarangi 350\nvalues across the different units in the data path of the microprogrammed processor. It is\nmainly responsible for the execution of microinstructions, data transfers across the different\nunits, and for transferring control to the correct program instruction by updating the PC. Note\nthat we have also added an extra connection between the ir register and the \u00b5control unit to\ntransfer the opcode of the instruction. We require the \u00b5control unit to load the appropriate\nset of microinstructions corresponding to the program instruction. By design, we do not wish\nto make the value of the opcode available to other units. This is because, we have a set of\nmicroinstructions for each opcode, and there is no reason why other units should require the\nvalue of the opcode.\nDefinition 63\nThe microcontrol unit, also referred to as the \u00b5control unit is a dedicated piece of hard-\nware that is responsible for the execution of a set of microinstructions corresponding to each\nprogram instruction. Its role is to fetch the appropriate set of microinstructions from a ded-\nicated microprogram memory, and execute them in sequence. A register called the micro\nPC (\u00b5pc) points to the currently executing microinstruction.\nWe envision a microprogram memory that is a part of the \u00b5control unit. It contains the\nset of microinstructions corresponding to each program instruction. It is thus necessary for the\n\u00b5control unit to jump to the starting address of the set of microinstructions corresponding to\neach program instruction. We also need a microPC that points to the current microinstruction\nbeing executed.\nBeforediscussingthedesignandimplementationofthe\u00b5controlunit,letusfirstlookatpro-\ngramming, orrathermicroprogrammingournewprocessor. Weneedtodesignamicroassembly\nlanguage that will help us write programs for it.\n8.6 Microassembly Language\n8.6.1 Machine Model\nAll the internal registers in Figure 8.22 are the set of registers that are visible to microassem-\nbly instructions. Ideally microassembly instructions are not supposed to be aware of regular\narchitectural registers, and other aspects of architectural state. They are only supposed to be\naware of internal registers that are not externally visible.\nTable 8.7 shows the list of internal registers in our microprogrammed data path. Note that\nwe have 1-bit registers, 4-bit registers, and 32-bit registers.\nMicroprogrammed instructions do not access memory. Hence, they do not need a view of\nmemory.\n8.6.2 Microinstructions\nLet us look at the life cycle of a regular program instruction. The first step is to fetch the\ncontents of the instruction from the instruction memory. Let us introduce a microinstruction 351 (cid:13)c Smruti R. Sarangi\nSerial No. Register Size Function\n(bits)\n1 pc 32 program counter\n2 ir 32 instruction register\n3 I 1 immediate bit in the instruc-\ntion\n4 rd 4 destination register id\n5 rs1 4 id of the first source register\n6 rs2 4 id of the second source regis-\nter\n7 immx 32 immediate embedded in the\ninstruction (after processing\nmodifiers)\n8 branchTarget 32 branch target, computed as\nthesumofthePCandtheoff-\nset embedded in the instruc-\ntion\n9 regSrc 4 contains the id of the register\nthat needs to be accessed in\nthe register file\n10 regData 32 contains the data to be writ-\nten into the register file\n11 regVal 32 value read from the register\nfile\n12 A 32 first operand of the ALU\n13 B 32 second operand of the ALU\n14 flags.E 1 the equality flag\n15 flags.GT 1 the greater than flag\n16 aluResult 32 the ALU result\n17 mar 32 memory address register\n18 mdr 32 memory data register\n19 ldResult 32 thevalueloadedfrommemory\nTable 8.7: List of all the internal registers\nto read the contents of the instruction from the instruction memory and place it in the IR\n(ir). Let us call it mloadIR. Note that we will add the prefix m (m for micro) to every\nmicroinstruction. This is to denote the fact that it is a microinstruction, and differentiate it\nfrom regular program instructions.\nMicroinstruction Semantics\nmloadIR Loadstheir withthecontents\nof the instruction (cid:13)c Smruti R. Sarangi 352\nOnce, we have loaded the instruction register, it automatically sends the contents to all\nthe subunits in the decode unit, and they extract the appropriate bit fields, and save them\nin the decode registers \u2013 I, rd, rs1, rs2, immx, and branchTarget. We envision an mdecode\ninstruction in the 0-address format that makes the \u00b5control unit wait for 1 cycle. In this cycle,\nall the decode registers get populated.\nMicroinstruction Semantics\nmdecode Waits for 1 cycle. Meanwhile,\nall the decode registers get\npopulated.\nNotethatthesetwosteps(mloadIRandmdecode)arecommonforallprograminstructions.\nAfter this, we need to load the microinstructions for the specific program instruction. This\nis achieved through a mswitch instruction that instructs the \u00b5control unit to jump to the\nappropriate location in the microinstruction memory, and begins executing microinstructions\nstarting from that location.\nMicroinstruction Semantics\nmswitch Load the set of microinstruc-\ntions corresponding to the\nprogram instruction\nNow, the processing of the instruction can start. The aim here is to use as few microin-\nstructions as possible. We want to keep the microassembly interface very simple. Let us first\nintroduce the mmov instruction that moves data from the source register to a destination regis-\nter. Additionally, it can set the arguments of the unit corresponding to the destination register.\nWe thus introduce a 2-address and 3-address format of the mmov instruction. The 3-address\nformat contains the arguments (args) of the unit corresponding to the destination register, as\nshown below.\nMicroinstruction Semantics\nmmov r1, r2 r1 \u2190 r2\nmmov r1, r2, (cid:104)args(cid:105) r1 \u2190 r2, send the value of\nargs on the bus\nWesometimesfacetheneedtoloadconstantsintoregisters. Hence,weintroducethemmovi\ninstruction that loads a constant into a register.\nMicroinstruction Semantics\nmmovi r1, (cid:104)imm(cid:105) r1 \u2190 imm\nmmovi r1, (cid:104)imm(cid:105), (cid:104)args(cid:105) r1 \u2190 imm, send the value of\nargs on the bus\nWe need an madd instruction because we need to increment the values of registers such as\nthe pc. Instead of using the main ALU, we can have a small adder as a part of the \u00b5control\nunit. We refer to this as the \u00b5adder. Here, there is a tradeoff to make. Do we need an add\ninstruction that adds two registers, and saves it in another register? At the microinstruction 353 (cid:13)c Smruti R. Sarangi\nlevel, this is seldom required. We definitely do not require this instruction to implement the\nSimpleRisc instruction set. Hence, we do not see a reason to include this microinstruction. If\nthere is ever a need to have one such microinstruction, then we can always use the main ALU\nin the data path to perform the addition. We thus introduce a simple add instruction in the\n2-address format. It adds an immediate value to a register. The semantics of this instruction\nis shown below.\nMicroinstruction Semantics\nmadd r1, (cid:104)imm(cid:105) r1 \u2190 r1+imm\nmadd r1, (cid:104)imm(cid:105), (cid:104)args(cid:105) r1 \u2190 r1+imm,sendthevalue\nof (cid:104)args(cid:105) on the bus\nHere, the madd instruction adds imm to r1, and saves the result in r1. imm can be a\npositive or a negative number. We restrict it to a 12-bit number, because we do not need more\nbits in most cases. The range of the immediate is thus between -2048 and +2047.\nLastly, we need branch instructions. We need both conditional branches, and unconditional\nbranches. We thus introduce two new microinstructions \u2013 mb (branch) and mbeq (branch if the\narguments are equal). The mb microinstruction takes a single argument, which is the address\nof the target microinstruction (or its label while writing microassembly code). We use the\nPC-direct addressing mode here as compared to the PC-relative addressing mode because, we\nexpect the total number of microinstructions to be small. Secondly, if we would have use a\nPC-relative addressing mode, then we would have required an extra adder in our data path\nto add the offset to the PC. The SimpleRisc instruction set allocates 5 bits for the opcode.\nThis means that at the most we can have 32 instructions in our instruction set. Let us assume\nthat in the worst case, an instruction translates to 20 microinstructions. We would thus need\nto store 640 microinstructions. We can thus allocate 10 bits for the specifying the address of\nthe microinstruction and our \u00b5pc (micro-PC) can also be 10 bits wide. This means that at\nthe most we can support a total of 1024 microinstructions. This is much more than what we\nactually require. However, it is not a bad idea to over design hardware because it cannot be\nchanged later. Note that in the microinstruction memory, the address refers to the index of the\nmicroinstruction (not to the starting address of the first byte).\nThe mbeq instruction requires three operands. The first operand is a register, the second\noperand is an immediate, and the third operand is the address(label) of a microinstruction. If\nthe value contained in the register is equal to the immediate operand, then the microPC jumps\nto the microinstruction specified by the third operand. Otherwise, the next microinstruction in\nsequence is executed.\nMicroinstruction Semantics\nmb (cid:104)addr(cid:105) execute the microinstruction at\n(cid:104)addr(cid:105)(label) in the microprogram\nmemory\nmbeq reg, imm, (cid:104)addr(cid:105) If the value in the internal register\nreg is equal to imm, then the mi-\ncroPCneedstojumpto(cid:104)add(cid:105)(label) (cid:13)c Smruti R. Sarangi 354\nSerial No. Microinstruction Semantics\n1 mloadIR ir \u2190 [pc]\n2 mdecode populate all the decode regis-\nters\n3 mswitch jump to the \u00b5pc correspond-\ning to the opcode\n4 mmov reg1, reg2, (cid:104)args(cid:105) reg1 \u2190 reg2, send the value\nof args to the unit that owns\nreg1, (cid:104)args(cid:105) is optional\n5 mmovi reg1, imm, (cid:104)args(cid:105) reg1 \u2190 imm, (cid:104)args(cid:105) is op-\ntional\n6 madd reg1, imm, (cid:104)args(cid:105) reg1 \u2190 reg1+imm, (cid:104)args(cid:105) is\noptional\n7 mbeq reg1, imm, (cid:104)addr(cid:105) if (reg1 = imm) \u00b5pc \u2190\naddr(label)\n8 mb (cid:104)addr(cid:105) \u00b5pc \u2190 addr(label)\nTable 8.8: List of microinstructions\nTosummarise,Table8.8showsthe8microinstructionsthatwehavedescribedinthissection.\nWe have a compact list of 8 microinstructions, and thus we can encode each microinstruction\nusing just 3 bits.\n8.6.3 Implementing Instructions in the Microassembly Language\nLet us now try to implement program instructions in the microassembly language using the set\nof basic microinstructions enumerated in Table 8.8.\nFor all the instructions, they start with a common set of microinstructions as shown below.\nWe refer to these 4 microinstructions as the preamble.\n.begin:\n1\nmloadIR\n2\nmdecode\n3\nmadd pc, 4\n4\nmswitch\n5\nDefinition 64\nA set of microinstructions that is common to all program instructions and is executed at\nthe beginning before proceeding to implement the logic of the instruction, is known as the\npreamble, or microcode preamble. 355 (cid:13)c Smruti R. Sarangi\nEvery instruction needs to pass through at least three of these steps. We need to fetch the\ncontents of the PC and load them into the ir register. Then, we need to decode the instruction,\nand break it down into its constituent fields. For instructions, other than branches, we need\nto increment the value of the PC by 4. In our microcode we prefer to do this step for all the\ninstructions. For taken branches, we need to later overwrite the PC with the branch target.\nLastly, we need to execute the mswitch instruction to jump to the starting location of the set\nof microinstructions that are specific to the program instruction.\nThe label .begin points to the beginning of this routine. Note that after finishing the exe-\ncution of an instruction, we need to jump to the .begin label such that we can start processing\nthe next instruction in the program. Note that in our microassembly code we specify the\nlabel that we need to branch to. When the microassembly code is translated to actual ma-\nchine level microinstructions, then each label is replaced by the address of the corresponding\nmicroinstruction.\n8.6.4 3-Address Format ALU Instructions\nLet us now look at implementing 3-address format ALU instructions. These instructions are:\nadd, sub, mul, div, mod, and, or, lsl, lsr, and asr.\nFirst, we need to read the value of the first operand stored in rs1 from the register file, and\nsend it to the ALU. The microcode snippet to achieve this is as follows:\nmmov regSrc, rs1, <read>\n1\nmmov A, regVal\n2\nNote, that we are combining a functional unit operation, and a register transfer in the same\ncycle. This can be confusing at the beginning. Hence, the reader should read this example\nseveral times and ensure that she has a clear understanding. The reason that we fuse both\nthe operations is because microcode registers are typically very small, and thus they can be\naccessed very quickly. Hence, it is not a good idea to use a complete cycle for transferring data\nbetween micro registers. It is a better idea to fuse a register transfer with a functional unit\noperation, such that we can ensure that we are roughly doing a similar amount of work every\ncycle.\nLet us proceed. Subsequently, we need to check if the second operand is a register or an\nimmediate. This can be achieved by comparing the I register with 1. If it is 1, then the second\noperand is an immediate, else it is a register. The following piece of code first checks this\ncondition, and then performs data transfers accordingly.\nmbeq I, 1, .imm\n1\n\/* second operand is a register *\/\n2\nmmov regSrc, rs2, <read>\n3\nmmov B, regVal, <aluop>\n4\nmb .rw\n5\n\/* second operand is an immediate *\/\n6\n.imm:\n7\nmmov B, immx, <aluop>\n8 (cid:13)c Smruti R. Sarangi 356\n\/* write the ALU result to the register file*\/\n9\n.rw:\n10\nHere, we first check if the value stored in the I register is equal to 1, using the mbeq\ninstruction. If it is not 1, then the second operand is a register, and we start executing the\nsubsequent microinstruction. We move the contents of the register, rs2, to the regSrc register\nthatcontainstheindexoftheregisterthatweneedtoreadfromtheregisterfile. Thenwemove\nthe value of the operand read from the register file (regVal) to the ALU (register B). Since\nthe value in register A is already present, we can directly start the ALU operation. This is\nindicated to the ALU by sending an extra argument ((cid:104)aluop(cid:105)) that encodes the ALU operation.\n(cid:104)aluop(cid:105) corresponds to one of the following operations: add, sub, mul, div, mod, and, or, lsl,\nlsr, and asr.\nHowever, ifthevalueoftheI registeris1, thenweneedtobranchto.imm. Thevalueofthe\nimmediate embedded in the instruction is already available with appropriate sign extensions in\nthe register immx. We need to simply transfer the value of immx to B (second ALU register),\nand the arguments ((cid:104)aluop(cid:105)) to the ALU. Similar to the case with the second operand being a\nregister, (cid:104)aluop(cid:105) encodes the ALU operation. Once, we are done, we need to start execution at\nthe label, .rw.\nThe label .rw needs to point to code that writes the value of the computed result to the reg-\nister file, and then proceeds to execute the next instruction. The code for these two operations\nis shown below.\n.rw:\n1\nmmov regSrc, rd\n2\nmmov regData, aluResult, <write>\n3\nmb .begin\n4\nWe write the result of the ALU into the register file, and then branch to the beginning,\nwhere we proceed to execute the next instruction. To summarise, here is the code for any\n3-address format ALU instruction (other than the preamble).\n\/* transfer the first operand to the ALU *\/\n1\nmmov regSrc, rs1, <read>\n2\nmmov A, regVal\n3\n4\n\/* check the value of the immediate register *\/\n5\nmbeq I, 1, .imm\n6\n\/* second operand is a register *\/\n7\nmmov regSrc, rs2, <read>\n8\nmmov B, regVal, <aluop>\n9\nmb .rw\n10\n\/* second operand is an immediate *\/\n11\n.imm:\n12\nmmov B, immx, <aluop>\n13\n14 357 (cid:13)c Smruti R. Sarangi\n\/* write the ALU result to the register file*\/\n15\n.rw:\n16\nmmov regSrc, rd\n17\nmmov regData, aluResult, <write>\n18\nmb .begin\n19\nThis code snippet has 10 microinstructions. Recall that we also need to execute 4 more\nmicroinstructions as a part of the preamble before this. They read the PC, decode the instruc-\ntion, set the next PC, and jump to the beginning of the appropriate set of microinstructions.\nExecuting 14 microinstructions for 1 program instruction is clearly a lot of effort. However, the\nreader must recall that we are not really after performance here. We wanted to design a very\nclean and flexible means of accessing different units.\n8.6.5 2-Address Format ALU Instructions\nThe three ALU instructions in the 2-address format, are not, mov, and cmp. not and mov have\na similar format. They do not use the first source operand, rs1. They operate on either rs2,\nor immx, and transfer the result to the register pointed by rd.\nLet us look at the mov instruction first. We first check whether the second operand is an\nimmediate, or not, by comparing the value in register I with 1. If it is equal to 1, then we jump\nto the label, .imm. Otherwise, we proceed to execute the subsequent instructions in Lines 4,\nand 5. In Line 4, we transfer rs2 to regSrc, along with the (cid:104)read(cid:105) command. The operand is\nreadandstoredinregVal. Inthenextcycle, wetransferregVal toregDatasuchthatitcanbe\nwritten back to the register file. If the second operand was an immediate, then we execute the\ncode in Line 9. We transfer the immediate (stored in immx) to the regData register. In either\ncase, regData contains the value to be written to the register file. Then we transfer the id of\nthe destination register (stored in rd) to regSrc, and simultaneously issue the write command\nin Line 13.\nmov instruction\n\/* check the value of the immediate register *\/\n1\nmbeq I, 1, .imm\n2\n\/* second operand is a register *\/\n3\nmmov regSrc, rs2, <read>\n4\nmmov regData, regVal\n5\nmb .rw\n6\n\/* second operand is an immediate *\/\n7\n.imm:\n8\nmmov regData, immx\n9\n10\n\/* write to the register file*\/\n11\n.rw:\n12\nmmov regSrc, rd, <write>\n13\n14\n\/* jump to the beginning *\/\n15\nmb .begin\n16 (cid:13)c Smruti R. Sarangi 358\nLet us now write a similar routing for the not instruction. The only additional step is to\ntransfer the value read from the register to the ALU, compute the logical negation, and then\ntransfer the value back to the register file. A hallmark feature of our microassembly language\nis that we can transfer a value to a unit, and if all the other operands are in place, then we can\nalso perform an operation in the unit in the same cycle. The implicit assumption here is that 1\nclock cycle is enough to transfer the data between registers, and perform a computation. In line\nwith this philosophy we transfer the value of immx, or regVal to register B of the ALU, and\nalso perform a (cid:104)not(cid:105) operation in the same cycle (see Lines 5 and 9). Like the mov instruction,\nwe transfer the ALU result to the regData register, and write it to the register file in Lines 13,\nand 14.\nnot instruction\n\/* check the value of the immediate register *\/\n1\nmbeq I, 1, .imm\n2\n\/* second operand is a register *\/\n3\nmmov regSrc, rs2, <read>\n4\nmmov B, regVal, <not> \/* ALU operation *\/\n5\nmb .rw\n6\n\/* second operand is an immediate *\/\n7\n.imm:\n8\nmmov B, immx, <not> \/* ALU operation *\/\n9\n10\n\/* write to the register file*\/\n11\n.rw:\n12\nmmov regData, aluResult\n13\nmmov regSrc, rd, <write>\n14\n15\n\/* jump to the beginning *\/\n16\nmb .begin\n17\nLet us now look at the compare instruction that does not have a destination operand. It\ncompares two operands, where one is a register operand, and the other can be either a register\nor an immediate. It saves the results automatically in the flags.E and flags.GT registers.\nLet us now consider the microcode for the cmp instruction.\ncmp instruction\n\/* transfer rs1 to register A *\/\n1\nmmov regSrc, rs1, <read>\n2\nmmov A, regVal\n3\n4\n\/* check the value of the immediate register *\/\n5\nmbeq I, 1, .imm\n6\n\/* second operand is a register *\/\n7\nmmov regSrc, rs2, <read>\n8\nmmov B, regVal, <cmp> \/* ALU operation *\/\n9\nmb .begin\n10\n11 359 (cid:13)c Smruti R. Sarangi\n\/* second operand is an immediate *\/\n12\n.imm:\n13\nmmov B, immx, <cmp> \/* ALU operation *\/\n14\nmb .begin\n15\nHere, we first transfer the value in register rs1 to the ALU (in register A). Then, we check\nif the second operand is an immediate. If it is an immediate, then we transfer the value of\nimmx to the ALU (in register B), and simultaneously issue a command to execute a compare\nin Line 14. However, if the second operand is a register, then we need to read it from the\nregister file (Line 8), and then transfer it to the ALU (Line 9). The last step is to branch to\nthe beginning (mb .begin).\n8.6.6 The nop Instruction\nImplementing the nop instruction is trivial. We just need to branch to the beginning as shown\nbelow.\nmb .begin\n1\n8.6.7 ld and st instructions\nLetusnowlookattheloadinstruction. Weneedtotransferthevalueofthefirstsourceregister\nto the ALU. Then we transfer the value of the immediate to the second ALU register (B), and\ninitiatethe addoperationtocalculate theeffective address. Oncetheeffectiveaddress hasbeen\ncalculated, it is available in the aluResult register. Subsequently, we move the contents of the\naluResult register to the memory address register (mar), and initiate a load operation. The\nresult is available in the ldResult register in the next cycle. We write the loaded value to the\nregister specified by rd in the next two cycles.\nld instruction\n\/* transfer rs1 to register A *\/\n1\nmmov regSrc, rs1, <read>\n2\nmmov A, regVal\n3\n4\n\/* calculate the effective address *\/\n5\nmmov B, immx, <add> \/* ALU operation *\/\n6\n7\n\/* perform the load *\/\n8\nmmov mar, aluResult, <load>\n9\n10\n\/* write the loaded value to the register file *\/\n11\nmmov regData, ldResult\n12\nmmov regSrc, rd, <write>\n13\n14\n\/* jump to the beginning *\/\n15\nmb .begin\n16 (cid:13)c Smruti R. Sarangi 360\nThe microcode for the store instruction is similar to that of the load instruction. We first\ncalculate the effective memory address and store it in the mar register. Then we read the value\noftherdregisterthatcontainsthedatatobestored(Line10). Wesavethisinthemdr register,\nand issue the store (Line 11).\nst instruction\n\/* transfer rs1 to register A *\/\n1\nmmov regSrc, rs1, <read>\n2\nmmov A, regVal\n3\n4\n\/* calculate the effective address *\/\n5\nmmov B, immx, <add> \/* ALU operation *\/\n6\n7\n\/* perform the store *\/\n8\nmmov mar, aluResult\n9\nmmov regSrc, rd, <read>\n10\nmmov mdr, regVal, <store>\n11\n12\n\/* jump to the beginning *\/\n13\nmb .begin\n14\n8.6.8 Branch Instructions\nThere are five branch instructions in SimpleRisc : b, beq, bgt, call, and ret.\nImplementing the unconditional branch instruction is trivial.We simply need to transfer the\nvalue of the branch target to the PC.\nb instruction\nmmov pc, branchTarget\n1\nmb .begin\n2\nWe can make a minor modification to this code to implement the beq, and bgt instructions.\nWe need to check the value of the flags registers, and set the branchTarget to the PC only if\nthe corresponding flags register contains a 1.\nbeq instruction bgt instruction\n\/* test the flags register *\/ \/* test the flags register *\/\n1 1\nmbeq flags.E, 1, .branch mbeq flags.GT, 1, .branch\n2 2\nmb. begin mb. begin\n3 3\n4 4\n.branch: .branch:\n5 5\nmmov pc, branchTarget mmov pc, branchTarget\n6 6\nmb .begin mb .begin\n7 7\nThe last two instructions that we need to implement are the call and ret instructions. The\ncall instruction is a combination of a simple branch, and a register write operation that adds\nthe value of the next PC (PC +4) to the return address register (register 15). The microcode 361 (cid:13)c Smruti R. Sarangi\nis as follows. Note that we do not increment the PC by 4 because it is already incremented in\nthe preamble.\ncall instruction\n\/* save PC + 4 in the return address register *\/\n1\nmmov regData, pc\n2\nmmovi regSrc, 15, <write>\n3\n4\n\/* branch to the function *\/\n5\nmmov pc, branchTarget\n6\nmb .begin\n7\nWe save the address of the next PC in the register file in lines 2 to 3. Then we move\nthe branchTarget to the PC, and then proceed to execute the first instruction in the invoked\nfunction.\nThe ret instruction performs the reverse operation, and transfers the return address to the\nPC.\nret instruction\n1\n\/* save the contents of the return\n2\naddress register in the PC *\/\n3\nmmovi regSrc, 15, <read>\n4\nmmov pc, regVal\n5\nmb .begin\n6\nWe have thus implemented all our SimpleRisc instructions in microcode. A microcoded\nimplementation is definitely slower that our hardwired datapath. However, we have gained a\nlot in terms of flexibility. We can implement some very complex instructions in hardware, and\nthus make the task of software developers significantly easier. We can also dynamically change\nthe behaviour of instructions. For example, if we wish to store the return address on the stack\nrather than the return address register, we can do so easily (see Examples 104 and 105).\nExample 104\nChange the call instruction to store the return address on the stack. The preamble need not\nbe shown (study carefully).\nAnswer:\nstack based call instruction\n1\n\/* read and update the stack pointer *\/\n2\nmmovi regSrc, 14, <read> \/* regSrc contains the id\n3\nof the stack pointer *\/\n4\nmadd regVal, -4 \/* decrement the stack pointer *\/\n5\nmmov mar, regVal \/* MAR contains the new stack pointer *\/\n6\n7 (cid:13)c Smruti R. Sarangi 362\nmmov regData, regVal, <write> \/* update the stack pointer *\/\n8\n9\n\/* write the return address to the stack *\/\n10\nmmov mdr, pc, <store>\n11\n12\nmb. begin\n13\nExample 105\nChange the ret instruction to load the return address from the stack. The preamble need\nnot be shown.\nAnswer:\nstack based call instruction\n\/* read the stack pointer *\/\n1\nmmovi regSrc, 14, <read>\n2\n3\n\/* set the memory address to the stack pointer *\/\n4\nmmov mar, regVal, <load>\n5\n6\nmmov pc, ldResult \/* set the PC *\/\n7\n8\n\/* update the stack pointer *\/\n9\nmadd regVal, 4 \/* sp = sp + 4 *\/\n10\nmmov regData, regVal, <write> \/* update stack pointer *\/\n11\n12\n\/* jump to the beginning *\/\n13\nmb .begin\n14\nExample 106\nImplement an instruction to compute the factorial of the number saved in register r2. You\ncan destroy the contents of r2. Save the result in register r3. Assume that the number is\ngreater than 1.\nstack based call instruction\n1\n\/* code to set the inputs to the multiplier *\/\n2\nmmovi B, 1\n3\nmmovi regSrc, 2, <read>\n4\nmmov A, regVal\n5\n\/* at this point A = r2, B = 1 *\/\n6 363 (cid:13)c Smruti R. Sarangi\n7\n\/* loop *\/\n8\n.loop:\n9\n\/* Now begin the multiplication *\/\n10\nmmov B, B, <multiply> \/* aluResult = A * B *\/\n11\nmmov B, aluResult \/* B = aluResult *\/\n12\n13\n\/* decrement and test *\/\n14\nmadd A, -1 \/* A = A - 1 *\/\n15\nmbeq A, 1, .out \/* compare A with 1 *\/\n16\nmb .loop\n17\n18\n.out:\n19\nmmov regData, aluResult\n20\nmmovi regSrc, 3, <write> \/* all done *\/\n21\n22\nmb .begin\n23\nExample 107\nImplement an instruction to find if the value saved in register r2 is a cubic Armstrong\nNumber. A cubic Armstrong number is equal to the sum of the cubes of its decimal digits.\nFor example, 153 is one such number. 153 = 13 +53 +33. Save the Boolean result in r3.\nAssume two scratch registers: sr1 and sr2.\nstack based call instruction\n1\n\/* Set the inputs of the ALU *\/\n2\nmmovi regSrc, 2, <read>\n3\nmmov A, regVal\n4\nmmov sr1, regVal\n5\nmmovi B, 10\n6\nmmovi sr2, 0 \/* sum = 0 *\/\n7\n8\n\/* loop *\/\n9\n.loop:\n10\n\/* test *\/\n11\nmbeq A, 0, .out\n12\n13\n\/* compute the mod and cube it *\/\n14\nmmov B, B, <mod> \/* aluResult = A % B *\/\n15\nmmov B, aluResult \/* B = aluResult *\/\n16\nmmov A, aluResult, <multiply> \/* aluResult = (A%B)^2 *\/\n17 (cid:13)c Smruti R. Sarangi 364\nmmov A, aluResult, <multiply> \/* aluResult = (A%B)^3 *\/\n18\nmmov A, aluResult \/* A = (A%B)^3 *\/\n19\nmmov B, sr2, <add> \/* add the running sum *\/\n20\nmmov sr2, aluResult \/* sr2 has the new sum *\/\n21\n22\n\/* test *\/\n23\nmmov A, sr1 \/* old value of A *\/\n24\nmmovi B, 10, <divide>\n25\nmmov A, aluResult \/* A = A \/ 10 *\/\n26\nmmov sr1, A \/* sr1 = A *\/\n27\n28\nmb .loop\n29\n30\n\/* out of the loop *\/\n31\n.out:\n32\nmmov A, sr2 \/* A contains the sum *\/\n33\nmmov B, regVal, <cmp> \/* compare *\/\n34\nmmov regSrc, 3\n35\nmbeq flags.E, 1, .success\n36\n37\n\/* failure *\/\n38\nmmov regData, 0, <write>\n39\nmb .begin\n40\n41\n.success:\n42\nmmov regData, 1, <write>\n43\nmb .begin\n44\nExample 108\nImplement an instruction to test if a number saved in register r2 is prime. Assume that\nthe number is greater than 3. Save the result in r3.\nstack based call instruction\n1\n\/* Read the register and set the ALU inputs *\/\n2\nmmovi regSrc, 2, <read>\n3\nmmov A, regVal\n4\nmmovi B, 1\n5\n6\n.loop:\n7\n\/* test for divisibility *\/\n8\nmadd B, 1, <mod> \/* aluResult = A % (B+1), B = B + 1 *\/\n9 365 (cid:13)c Smruti R. Sarangi\nmbeq aluResult, 0, .failure\n10\n11\n\/* test B *\/\n12\nmmov A, A, <cmp> \/* compare A with B *\/\n13\nmbeq flags.E, 1, .success\n14\n15\nmb .loop\n16\n17\n.success:\n18\nmmovi regSrc, 3\n19\nmmovi regData, 1, <write>\n20\nmb .begin\n21\n22\n.failure:\n23\nmmovi regSrc, 3\n24\nmmovi regData, 0, <write>\n25\nmb. begin\n26\n8.7 Shared Bus and Control Signals\nLet us take a look again at the list of implemented microinstructions in Table 8.8. We observe\nthat each microinstruction has at the most one register read operand, and one register write\noperand. Wetypicallyreadfromoneinternalregister,andthenuseitasapartofacomputation\n(addition or comparison), and then write the result to another internal register.\nWe thus propose the design of a shared bus that actually consists of two buses as shown in\nFigure 8.23. The first bus is known as the write bus that is connected to all the registers that\nmight potentially write data to the bus. The output of the write bus, the embedded immediate\n(\u00b5imm) in the microinstruction, and the output of the \u00b5adder are sent to a multiplexer. Recall\nthat the \u00b5adder adds the embedded immediate with the contents of a register. Now, this\nmultiplexer chooses one value among the three, and then sends it on the read bus. We refer\nto this multiplexer as the transfer multiplexer. All the registers that might potentially read a\nvalue are connected to the read bus. The PC is connected to both the buses. The \u00b5adder has\ntwo inputs. One of them is the sign extended immediate that is a part of the microinstruction,\nand the other is the output of the write bus.\nSimultaneously, we compare the value sent on the write bus with the embedded immediate\n(\u00b5imm). The result is contained in the isMBranch signal. The isMBranch signal is required\nfor implementing the mbeq instruction.\nTo create a flexible data path, we need to add as many interconnections between units as\npossible. We thus decide to connect every register other than the decode, and flags registers\nto both the read and write buses. These registers are the input\/output registers of the register\nfile (regSrc, regData, and regVal), the ALU registers (A, B, aluResult), and the registers (cid:13)c Smruti R. Sarangi 366\nMicrocontrol unit\n\u03bcimm\nisMBranch\npc\nDecode unit pc\nRead bus\nWrite bus\nReg. file, ALU, Mem unit\nReg. file, ALU, Mem unit\nShared bus\nFigure 8.23: The design of the shared bus\nassociated with the memory unit (mar, mdr, ldResult). To support branch instructions, it is\nalso necessary to connect the PC to both the buses.\n8.7.1 Control Signals\nEach register that writes to the write bus needs a control signal. If it is asserted (equal to 1),\nthen the value of the register appears on the write bus. Otherwise, the value of the register\ndoes not get reflected on the write bus. For example, the register, aluResult, contains the\nresult of an ALU operation, and it is sometimes necessary to transfer its value to the write bus.\nThe signal aluResult controls the behaviour of the aluResult register. We associate similar\nout\nsignals with the subscript, with all the registers that need to access the write bus.\nout\nLikewise, we associate a set of signals with the registers that are connected to the read bus.\nFor example, the register mar is connected to the read bus. We associate the signal mar with\nin\nit. If it is 1, then the value of the data on the read bus is transferred to mar. If mar = 0, the\nin\nmar register is effectively disconnected from the read bus.\nThe PC has two signals associated with it: pc and pc . The \u00b5control unit ensures that\nin out\nat one point of time only one register can write to the write bus. However, it is theoretically\npossible for multiple registers to read from the read bus concurrently.\n8.7.2 Functional Unit Arguments\nWeaugmentthereadbustocarrytheargumentsforthefunctionalunits(referredtoas(cid:104)args(cid:105)).\nTheseargumentsspecifythenatureoftheoperation,whichthefunctionalunitneedstoperform.\nFor example, the two operations associated with the memory unit are (cid:104)load(cid:105), and (cid:104)store(cid:105), and\nthetwooperationsassociatedwiththeregisterfileare(cid:104)read(cid:105)and(cid:104)write(cid:105). EachALUoperation\nalso has its separate code. 367 (cid:13)c Smruti R. Sarangi\nWeproposetoencodeeachoperationinbinary, andreservethespecialvalueof0toindicate\nthat no operation needs to be performed. Each functional unit needs to be connected to the\nread bus, and needs to process the value of the arguments. The (cid:104)args(cid:105) field can be split into\ntwo parts: (cid:104)unit id(cid:105), and (cid:104)opcode(cid:105). The (cid:104)unit id(cid:105) specifies the identifier for the functional unit.\nFor example, we can assign 00 to the ALU, 01 to the register file, and 10 to the memory unit.\nThe (cid:104)opcode(cid:105) contains the details of the operation to be performed. This is specific to the\nfunctional unit. We propose a 10-bit (cid:104)args(cid:105) bus that is a part of the read bus. We devote 3 bits\nto the (cid:104)unit id(cid:105), and 7 bits to the (cid:104)opcode(cid:105). Thus, for each unit we can support 128 different\noperations. Implementingthecircuittoprocessthe(cid:104)args(cid:105)iseasy, andweleaveitasanexercise\nto the reader.\n8.8 The Microcontrol Unit\nWe now arrive at the last piece of our microprogrammed processor, which is the design of\nthe \u00b5control unit. It is a simple processor that executes microinstructions. It consists of a\n\u00b5fetch unit and a \u00b5pc. Every cycle we increment the \u00b5pc by 1 (addressed by the number of\nthe instruction, not by bytes), or set it to the branch target. Then, we proceed to read the\nmicroinstructionfromthemicroprogrammemory,andprocessit. Therearetwomainparadigms\nfor designing an encoding of microinstructions, and executing them using a \u00b5control unit. The\nfirst is known as vertical microprogramming. In principle, this paradigm is similar to executing\nregular program instructions using a hardwired processor. The second paradigm is known as\nhorizontal microprogramming. This is more common, and is also a more efficient.\n8.8.1 Vertical Microprogramming\nIn vertical microprogramming, we encode an instruction similar to encoding a regular RISC\ninstruction in a hardwired processor.\ntype src dest immediate branchTarget args\n3 5 5 12 10 10\nFigure 8.24: Encoding of a microinstruction (vertical microprogramming)\nFigure 8.24 shows an encoding for our scheme. Here, we devote 3 bits for encoding the type\nof the microinstruction. We need 3 bits because we have a total of 8 microinstructions (see\nTable 8.8). Each microinstruction embeds the 5 bit id (because we have 19 registers visible\nto microprograms) of an internal source register, 5 bit id of an internal destination register, a\n12-bit immediate, and a 10-bit branch target. At the end, we encode a 10-bit args value in the\nmicroinstruction. Each instruction thus requires 45 bits.\nNow, to process a vertically encoded microinstruction, we need a dedicated \u00b5decode unit\nthat can generate all the control signals. These signals include all the enable signals for the\ninternal registers, and the signals to select the right input in the transfer multiplexer. Addi-\ntionally it needs to extract some fields from the microinstruction such as the immediate, branch (cid:13)c Smruti R. Sarangi 368\ntarget, andtheargsvalue, andsubsequentlyextendtheirsign. Wehavealreadygonethrougha\nsimilar exercise for extracting the fields of an instruction, and generating control signals, when\nwe discussed the operand fetch unit and control unit for our hardwired processors in Sections\n8.2.3, and 8.3 respectively. The logic for generating the control signals, and extracting fields\nfrom the microinstruction is exactly the same. Hence, we leave the detailed design of these\nunits as an exercise for the reader.\n\u03bcmux\nswitch\nbranchTarget\n1\nMicroprogram Decode Execute\n\u03bcpc\nmemory unit unit\ncontrol\nsignals\nData path\nShared bus\nFigure 8.25: The \u00b5control unit (vertical microprogramming)\nThe design of the vertical \u00b5control unit is shown in Figure 8.25. We have a microPC (\u00b5pc),\nwhich saves the index of the currently executing microinstruction. Every cycle, we increment\nthe \u00b5pc by 1. This is because each row of the microprogram memory saves 1 microinstruction.\nWe assume that a row is wide enough to save the entire contents of the microinstructions.\nWe do not have the requirement of saving data at the granularity of a fixed number of bytes\nhere. After reading the microinstruction, we proceed to decode it. The process of decoding\na microinstruction breaks it into a set of fields (instruction type, immediate, branch target,\nargs, source, and destination registers). Subsequently, we generate all the control signals and\ndispatch the set of control signals to the execute unit. The execute unit sets all the control\nsignals, and orchestrates an operation in the data path of the processor. The execute unit also\nsets the control signals of the transfer multiplexer. We need some additional support to process\nthe mswitch instruction. We add a dedicated switch unit that takes inputs (the opcode) from\nthe ir register, and computes the starting address for the microcode of the currently executing\nprogram instruction. It sends the address to the multiplexer, \u00b5fetch (see Figure 8.25). The\nmultiplexer chooses between three inputs \u2013 default microPC, branch target, and the address\ngenerated by the switch unit. It is controlled by the execute unit. The rules for choosing the\ninput are shown in Table 8.9. In accordance with these rules, and the value of the isMBranch\nsignal (generated by comparing \u00b5imm, and the contents of the shared bus), the execute unit\ngenerates the control signals for the \u00b5fetch multiplexer.\nedocpo 369 (cid:13)c Smruti R. Sarangi\nInstruction Output of the \u00b5fetch multiplexer\nmloadIR next \u00b5pc\nmdecode next \u00b5pc\nmswitch output of the switch unit\nmmov next \u00b5pc\nmmovi next \u00b5pc\nmadd next \u00b5pc\nmb branch target\nmbeq branch target if isMBranch = 1, else next \u00b5pc\nTable 8.9: Rules for controlling the \u00b5fetch multiplexer\n8.8.2 Horizontal Microprogramming\nWe can further simplify the design of the \u00b5control unit. We do not need three steps (fetch,\ndecode, execute)toexecuteamicroinstruction. Thedecodestepisnotrequired. Wecanembed\nall the control signals in the microinstruction itself. It is thus not required to have a dedicated\nsignal generator to generate all the control signals. By doing so, we will increase the size of the\nencoding of an instruction. Since the number of microinstructions is small, and we do not have\nany significant constraints on the size of the encoding of a microinstruction, adding additional\nbits in the encoding is not an issue. This paradigm is known as horizontal microprogramming.\nThe encoding of a microinstruction is shown in Figure 8.26.\ncontrol signals immediate branchTarget args\n33 12 10 10\nFigure 8.26: Encoding of a microinstruction (horizontal microprogramming)\nWe need the following fields \u2013 control signals (saved as a bit vector whose size is 33 bits),\nimmediate (12 bits), branch target (10 bits), and args (10 bits). The reason we require 33\ncontrol signals is as follows. We have 19 registers (see Table 8.7) visible to microcode. Out\nof these register, the following 9 registers are exclusively connected to either the read bus or\nthe write bus: ir, flags.E, flags.GT, I, rd, rs1, rs2, branchTarget, and immx. Hence, these\nregisters require just one control signal. The rest of the registers have read-write functionality.\nHence, these registers require two control signals. Thus, the total number of register enabling\ncontrol signals are 29. We need 2 more signals each to control the transfer multiplexer, and\nthe \u00b5fetch multiplexer. We thus have a total of 33 control signals, and we require 65 bits to\nencode the instruction. Recall that with vertical microprogramming, we needed 45 bits.\nNow, withadditionalstoragewecancompletelyeliminatethesignalgeneratorinthedecode\nstage, and thus significantly simplify the \u00b5control unit as shown in Figure 8.27\nHere, we have eliminated the decode stage. All the signals are embedded in the instruction,\nand they are thus used to orchestrate a computation in the data path. The execute unit (cid:13)c Smruti R. Sarangi 370\nopcode\nswitch\n\u03bcmux\nisMBranch\nM1 branchTarget\n1\nMicroprogram Execute\n\u03bcpc\nmemory unit\ncontrol\nsignals\nData path\nShared bus\nFigure 8.27: The \u00b5control unit (horizontal microprogramming)\ngenerates the isMBranch signal (by comparing the \u00b5imm and the value on the read bus),\nwhich is used to choose between the next \u00b5pc, and the branch target using multiplexer, M1.\nHere, we slightly complicate the \u00b5fetch multiplexer, and add a little bit of redundancy in the\ninterest of simplicity. We make it a 4 input structure, and choose between the value from the\nswitch unit, the branch target, the output of M1, and the next \u00b5pc. The 2-bit control signals\nfor controlling the \u00b5fetch multiplexer are embedded in the instruction in accordance with the\nrules given in Table 8.9. The rest of the operation of the circuit is the same as the circuit for\nvertical microprogramming as shown in Figure 8.25.\n8.8.3 Tradeoffs between Horizontal and Vertical Microprogramming\nThe tradeoffs between horizontal and vertical microprogramming are the following:\n1. Horizontal microprogramming requires more storage. However, this is not an issue in a\nmicroprogrammed processor. The additional storage is minimal.\n2. Horizontal microprogramming eliminates the need for dedicated signal generation logic in\nthe \u00b5control unit.\n3. To program a horizontally microprogrammed processor, it is necessary to expose the con-\ntrol signals to the programmer and the microassembler. This makes the microassembler\nvery specific to a given processor. However, in vertical microprogramming, as long as the\ninternal register set remains the same, we do not need different microassemblers.\nTo summarise, microprogramming is a very potent method to implement an instruction\nset. We can design very expressive instruction sets using this method. However, this is not 371 (cid:13)c Smruti R. Sarangi\na preferable approach for implementing all the instructions (especially the common ones) in a\nhigh performance processor.\n8.9 Summary and Further Reading\n8.9.1 Summary\nSummary 8\n1. We design a processor by dividing it into multiple stages, where the stages are mostly\nindependent of each other. We divide our basic SimpleRisc processor into five stages:\ninstruction fetch(IF), operand fetch(OF), execute (EX), memory access (MA), and\nregister writeback (RW).\n2. The roles of these stages are as follows:\n(a) The IF stage computes the next PC, and fetches the contents of the instruction,\nwhose address is stored in the PC.\n(b) In the OF stage, we decode the instruction, and read its operands from the reg-\nister file. Specifically, we compute the branch target, and expand the embedded\nimmediate in the instruction according to the modifiers.\n(c) In the EX stage, we compute the branch outcome, branch target, and perform\nthe ALU operations.\n(d) In the MA stage, we perform loads and stores.\n(e) Lastly, in the RW stage, we write back the values computed by ALU or load\ninstructions, and the return address for a call instruction to the register file.\n3. The data path consists of all the elements for storing, retrieving, and processing infor-\nmation such as the registers, memory elements, and the ALU. In contrast, the control\npath generates all the signals for controlling the movement of instructions and data.\n4. We can use a hardwired control unit that generates all the signals for the control path.\n5. For additional flexibility, and portability, we presented the design of a micropro-\ngrammed processor. This processor replaces every program instruction by a sequence\nof microinstructions.\n6. We defined 8 microinstructions, and created a microprogrammed data path that con-\nnected all the units on a shared bus. Each unit in a microprogrammed data path\nexposes its input and output ports through registers. We use 19 registers in our de-\nsign.\n7. We subsequently showed implementations in microcode for all the instructions in the\nSimpleRisc ISA. (cid:13)c Smruti R. Sarangi 372\n8. We designed a shared bus for such processors by interconnecting two physical buses\n(write bus, and read bus) with a multiplexer. The multiplexer (known as the transfer\nmultiplexer) chooses between the output of the write bus, the output of the \u00b5adder,\nand the micro immediate.\n9. We showed the design of a \u00b5control unit for both vertical and horizontal micropro-\ngramming. Vertical microprogramming requires a decode stage for generating all the\ncontrol signals. In comparison, horizontal microprogramming requires all the control\nsignals to be embedded in the microinstruction.\n8.9.2 Further Reading\nProcessor design is very heavily studied in courses on computer architecture. Readers should\nfirst start with Chapter 9 that discusses pipelining. Chapter 9 is a sequel to the current\nchapter. The reader can then take a look at the \u201cFurther Reading\u201d section (Section 9.12.2) in\nChapter 9. In general, for basic processor design, the reader can also consult other books on\ncomputer architecture [Mano, 2007, Stallings, 2010, Henessey and Patterson, 2010] to get a\ndifferent perspective. The books by Morris Mano [Mano, 2007], and Carl Hamacher [Hamacher\net al., 2001] consider different flavours of microprogramming, and define their own semantics.\nIf the reader is interested in the history of microprogramming per se, then she can consult\nbooks dedicated to the design and history of microprogramming [Carter, 1995, Husson, 1970].\nThe PTLSim [Yourst, 2007] simulator translates x86 instructions into micro-instructions, and\nsimulates these microinstructions on a data path similar to that of commercial processors.\nReaders can take a look at the code of this simulator, and appreciate the nuances of processing\nand executing microinstructions.\nExercises\nHardwired Processor Design\nEx. 1 \u2014 We have divided a SimpleRisc processor into 5 distinct units. List them, and de-\nscribe their functions.\nEx. 2 \u2014 Explain the terms \u2013 data path and control path?\nEx. 3 \u2014 How does having a lesser number of instruction formats help in the process of de-\ncoding an instruction?\nEx. 4 \u2014 Draw the circuit for calculating the value of the 32-bit immediate, from the first 18\nbits of the instruction. Take the modifiers into account.\nEx. 5 \u2014 Why is it necessary for the register file in our SimpleRisc processor to have 2 read\nports, and 1 write port? 373 (cid:13)c Smruti R. Sarangi\nEx. 6 \u2014 Why do we need 2 multiplexers in the OF stage of the processor? What are their\nfunctions?\nEx. 7 \u2014 LetusproposetocomputethebranchoutcomeandtargetintheOFstage. Describe\nthe design of the OF stage with this functionality.\n* Ex. 8 \u2014 For the ALU we use a multiplexer with a large number of inputs. How can we\nimplement this multiplexer with transmission gates? (show a circuit diagram, and explain why\nyour idea will work)\nEx. 9 \u2014 Draw a circuit for implementing the cmp instruction. It should show the circuit for\nsubtraction, and the logic for updating the flags.\nEx. 10 \u2014 How do we implement the call instruction in our processor?\nEx. 11 \u2014 Draw the circuit diagram for computing the isWb signal.\nEx. 12 \u2014 Why do we use the isAdd control signal for the load, and store instructions also?\nMicroprogramming\nEx. 13 \u2014 Compare a hardwired control unit and a microprogrammed control unit.\nEx. 14 \u2014 Draw the block diagram of a microprogrammed processor.\nEx. 15 \u2014 Why do we need the mswitch instruction.\nEx. 16 \u2014 Describe the microcode implementation of the load and store instructions.\nEx. 17 \u2014 Write a program in microassembly to check if a number in register r2 is a perfect\nsquare. Save the Boolean result in register, r0.\nEx. 18 \u2014 Write a program in microassembly to check if the value in register r2 is a palin-\ndrome. Apalindromereadsthesamefrombothsides. Forexample, the8bitnumber, 11011011\nis a palindrome. Save the Boolean result in register, r0.\n* Ex. 19 \u2014 Write a program in microassembly to check if the value in register r2 can be\nexpressed as a sum of two cubes in two different ways. For example, 1729, is one such number.\n1729 = 123+13 = 103+93. Save the Boolean result in register, r0.\nEx. 20 \u2014 Outline the design of the shared bus, and microprogrammed data path. Explain\nthe functionalities of each of its components.\nEx. 21 \u2014 Draw a detailed diagram of the \u00b5control unit along with the transfer multiplexer\nin a vertically microprogrammed processor.\nEx. 22 \u2014 Draw a detailed diagram of the \u00b5control unit along with the transfer multiplexer\nin a horizontally microprogrammed processor. (cid:13)c Smruti R. Sarangi 374\nEx. 23 \u2014 Compare the tradeoffs between horizontal and vertical microprogramming.\nDesign Problems\nEx. 24 \u2014 Implement the hardwired SimpleRisc processor using Logisim, which is an ed-\nucational tool for designing and simulating digital circuits. It is freely available at http:\n\/\/ozark.hendrix.edu\/~burch\/logisim. Try to support all the instructions, and the modi-\nfiers.\nEx. 25 \u2014 Now, try to implement a horizontally microprogrammed processor using Logisim.\nThis project has two parts.\na)Write a microassembler that can translate microassembly instructions to their machine\nencodings. Use this microassembler to generate the microcode for all the instructions in\nthe SimpleRisc ISA.\nb)Create a data path and control path in Logisim for a horizontally microprogrammed pro-\ncessor. This processor should be able to directly execute the code generated by the mi-\ncroassembler.\nc)Run regular SimpleRisc programs on this processor.\nd)Implement custom SimpleRisc instructions such as multiply-add (a \u2190 b\u2217c+d), or in-\nstructions to find the square of a number on this processor.\nEx. 26 \u2014 Implement the basic hardwired processor in a high-level description language such\nas VHDL. You can use the freely available open source tool GNU HDL (http:\/\/gna.org\/\nprojects\/ghdl\/) to implement and simulate your circuit. 9\nPrinciples of Pipelining\n9.1 A Pipelined Processor\nLet us quickly review where, we are.\nWay Point 7\n1. We have designed a processor with five main stages \u2013 IF, OF, EX, MA, and RW.\n2. We have designed a detailed data path and control path for the hardwired implemen-\ntation of our processor.\n3. We introduced a microprogram based implementation of our processor in Section 8.4,\nand we designed a detailed data path and control path for it.\nNow, our aim is to make our processor fast and efficient. For this we focus on the hardwired\nimplementation of our processor. We exclude microprogrammed processors from our discussion\nbecause we are explicitly looking for high performance, and flexibility\/ reconfigurability are not\nimportant criteria for us in this section. Let us begin by pointing out some problems with the\ndesign of the hardwired processor as presented in Section 8.2.\n9.1.1 The Notion of Pipelining\nIssues with a Single-Cycle Processor\nWe assumed that our hardwired processor presented in Section 8.2 takes a single cycle to fetch,\nexecute, and write the results of an instruction to either the register file or memory. At an\n375 (cid:13)c Smruti R. Sarangi 376\nelectrical level, this is achieved by signals flowing from the fetch unit to ultimately the register\nwriteback unit via other units. It takes time for electrical signals to propagate from one unit\nto the other.\nFor example, it takes some time to fetch an instruction from the instruction memory. Then\nit takes time to read values from the register file, and to compute the results with the ALU.\nMemory access, and writing the results back to the register file, are also fairly time taking\noperations. We need to wait for all of these individual sub-operations to complete, before we\ncan begin processing the next instruction. In other words, this means that there is a significant\namount of idleness in our circuit. When the operand fetch unit is doing its job, all other units\nare idle. Likewise, when the ALUs are active, all the other units are inactive. If we assume\nthat each of the five stages (IF,OF,EX,MA,RW) takes the same amount of time, then at any\ninstant, about 80% of our circuit is idle! This represents a waste in computational power, and\nidling resources is definitely not a good idea.\nIf we can find a method to keep all the units of a chip busy, then we can increase the rate\nat which we execute instructions.\n9.1.2 Overview of Pipelining\nLet us try to find an analogy to the problem of idleness in a simple single-cycle processor as we\njust discussed. Let us go back to our original example of the car factory. If we assume, that we\nstart making a car, after the previous car has been completely manufactured, then we have a\nsimilar problem. When we are assembling the engine of a car, the paint shop is idle. Likewise,\nwhen we are painting a car, the engine shop is idle. Clearly, car factories cannot operate this\nway. They thus typically overlap the manufacturing stages of different cars. For example, when\ncar A is in the paint shop, car B is in the engine shop. Subsequently, these cars move to the\nnext stage of manufacturing and another new car enters the assembly line.\nWe can do something very similar here. When one instruction is in the EX stage, the next\ninstruction can be in the OF stage, and the subsequent instruction can be in the IF stage. In\nfact,ifwehave5stagesinourprocessor,wherewesimplisticallyassumethateachstageroughly\ntakesthesameamountoftime, wecanassumethatwehave5instructionssimultaneouslybeing\nprocessed at the same time. Each instruction undergoes processing in a different unit of the\nprocessor. Similar to a car in an assembly line, an instruction moves from stage to stage in the\nprocessor. This strategy ensures that we do not have any idle units in our processor because\nall the different units in a processor are busy at any point in time.\nIn this scheme, the life cycle of an instruction is as follows. It enters the IF stage in cycle n,\nenters the OF stage in cycle n+1, EX stage in cycle n+2, MA stage in cycle n+3, and finally\nit finishes its execution in the RW stage in cycle n+4. This strategy is known as pipelining,\nand a processor that implements pipelining is known as a pipelined processor. The sequence of\nfive stages (IF, OF, EX, MA, RW) conceptually laid out one after the other is known as the\npipeline(similartothecarassemblyline). Figure9.1showstheorganisationofapipelineddata\npath.\nIn Figure 9.1, we have divided the data path into five stages, where each stage processes a\nseparate instruction. In the next cycle, each instruction passes on to the next stage as shown\nin the figure. 377 (cid:13)c Smruti R. Sarangi\nInstruction Operand Execute Memory Register\nFetch Fetch Access Write\n(IF) (OF) (EX) (MA) (RW)\nFigure 9.1: A pipelined data path\nDefinition 65\nThe notion of dividing a processor into a set of stages where the stages are ordered one\nafter the other, and simultaneously process a set of instructions by assigning an instruction\nto each stage, is known as pipelining. The implicit assumption here is that it takes the same\namount of time for each stage to complete its work. After this time quanta is over, each\ninstruction moves to the subsequent stage.\nThe conceptual layout of stages where one stage is laid out after the other is known as\na pipeline, and a processor that incorporates pipelining is known as a pipelined processor.\n9.1.3 Performance Benefits\nLet us quantify the expected benefit in terms of performance of a pipelined processor. We shall\ntake a deeper look into performance issues in Section 9.9. Here, we shall look at this topic\nbriefly. Let us assume that it takes \u03c4 nanoseconds for an instruction to travel from the IF\nto RW stage of the pipeline in the worst case. The minimum value of the clock cycle is thus\nlimited to \u03c4 nanoseconds for the case of a single cycle pipeline. This is because in every clock\ncycle we need to ensure that an instruction executes completely. Alternatively, this mean that\nevery \u03c4 nanoseconds, we finish the execution of an instruction.\nNow, letusconsiderthecaseofapipelinedprocessor. Here, wehavebeenassumingthatthe\nstages are balanced. This means that it takes the same amount of time to execute each stage.\nMost of the time, processor designers try to achieve this goal to the maximum extent that is\npossible. We can thus divide \u03c4 by 5, and conclude that it takes \u03c4\/5 nanoseconds to execute\neach stage. We can thus set the cycle time to \u03c4\/5. After the end of a cycle, the instructions in\neach stage of the pipeline proceed to the next stage. The instruction in the RW stage moves\nout of the pipeline and finished its execution. Simultaneously, a new instructions enters the IF\nstage. This is graphically shown in Figure 9.2.\nIn the nth cycle, we have five instructions (1-5) occupying the five stages of the pipeline. In\nthe (n+1)th cycle each instruction progresses by 1 stage, and instruction 6 enters the pipeline.\nThis pattern continues.\nThe noteworthy point is that we are finishing the execution of a new instruction, every\n\u03c4\/5 nanoseconds. As compared to a single-cycle processor that finishes the execution of a new\ninstruction every \u03c4 nanoseconds, the instruction throughput is 5 times higher for a pipelined (cid:13)c Smruti R. Sarangi 378\ninst 5 inst 4 inst 3 inst 2 inst 1\nInstruction Operand Execute Memory Register\ncycle n\nFetch Fetch Access Write\n(IF) (OF) (EX) (MA) (RW)\ninst 6 inst 5 inst 4 inst 3 inst 2\ncycle n+1 Instruction Operand Execute Memory Register\nFetch Fetch Access Write\n(IF) (OF) (EX) (MA) (RW)\nFigure 9.2: Instructions in the pipeline\nprocessor. Inaspanof1000nanoseconds,asinglecycleprocessorcompletes1000\/\u03c4 instructions,\nwhereas a pipelined processor completes 5000\/\u03c4 instructions, and is thus 5 times more efficient.\nTherefore, we observe a fivefold advantage with pipelining.\nIf we can obtain a fivefold advantage with a 5-stage pipeline, then by the same logic we\nshould be able to obtain a 100-fold advantage with a 100-stage pipeline. In fact, we can keep on\nincreasingthenumberofstagestillastagejustcontainsonetransistor. However, thisisnotthe\ncase, and there are fundamental limitations to the performance of a pipelined processor, as we\nshall show in the subsequent sections. It is not possible to arbitrarily increase the performance\nof a processor by increasing the number of pipeline stages. In fact, after a certain point, adding\nmore stages is counterproductive.\n9.2 Design of a Simple Pipeline\nLet us now design a simple pipeline. Our main aim in this section is to split the data path\nof the single-cycle processor into five stages and ensure that five instructions can be processed\nconcurrently (one instruction in each stage). We need to also ensure the seamless movement\nof instructions between the pipeline stages. Note that the problem of designing a pipeline in\ngeneralisverycomplex, andwewillexploresomeofthemajornuancesinthenextfewsections.\nFor this section, let us not consider any dependences between instructions, or consider any\ncorrectness issues. We shall look at these issues in detail in Section 9.4. Let us reiterate that at\nthe moment, we want to design a simple pipeline that needs to have the capability to process\nfive instructions simultaneously, and ensure that they move to the subsequent stage every new\ncycle.\n9.2.1 Splitting the Data Path\nWe have five distinct units in our data path, and all instructions traverse the units in the same\norder. Theseunitsareinstructionfetch(IF),operandfetch(OF),execute(EX),memoryaccess\n(MA), and the register write (RW) units. A layout of these five units in a pipelined fashion has 379 (cid:13)c Smruti R. Sarangi\nalready been shown in Figure 9.1. Let us now discuss the issue of splitting a data path in some\nmore detail.\nThereaderneedstonotethatpipeliningisageneralconcept,andanycircuitcaninprinciple\nbesplitintomultiplepartsandpipelined. Therearehoweversomerulesthatneedtobefollowed.\nAll the subparts of the circuit must preferably be distinct entities that have as few connections\nbetween them as possible. This is true in the case of our data path. All our units are distinct\nentities. The second is that all kinds of data must flow through the units in the same order,\nand lastly the work done by each unit should roughly be the same. This minimises the idleness\nin the circuit. In our case, we have tried to follow all these rules. The reader needs to note that\nthediv andmodoperationsareexceptionstothisrule. Theyareingeneral, significantlyslower,\nthan add or multiply operations. They thus increase the maximum delay of the EX stage, and\nthe pipeline consequently becomes unbalanced. Hence, most simple pipelined processors either\nrefrain from implementing these instructions, or have specialised logic to deal with them. We\nshall show one solution for this problem in Section 9.6 that proposes to stall a pipeline till\na division operation completes. Let us nevertheless continue to assume that all our pipeline\nstages are balanced.\n9.2.2 Timing\nNow, we need to design a method that ensures that instructions seamlessly proceed to the\nsubsequent pipeline stage. We need a global mechanism that ensures that all the instructions\nproceed to the next stages simultaneously. We already have this global mechanism built in,\nand is nothing else, but the clock. We can have a protocol that for example, ensures that at\nthe falling edge of the clock, all the instructions proceed to the subsequent stages.\nLatches\nInstruction Operand Execute Memory Register\nFetch Fetch Access Write\n(IF) (OF) (EX) (MA) (RW)\nFigure 9.3: A pipelined data path with registers\nFigure 9.3 shows a simple method to achieve this. We insert a register between two consec-\nutive pipeline stages. Since we have five pipeline stages in our data path, we insert 4 registers.\nThe four registers are named after their locations \u2013 IF-OF, OF-EX, EX-MA, and MA-RW.\nEach of these registers are called pipeline registers or pipeline latches. The reader needs to\nnote that in this case, a latch, is actually referring to an edge triggered register. We shall use\nthe terms interchangeably. All the pipeline registers are connected to a common clock, and\nread-write data at the same time. (cid:13)c Smruti R. Sarangi 380\nDefinition 66\nA pipeline register, or a pipeline latch is a register that is added between two consecutive\npipeline stages. All the registers are connected to the common clock, and help in seamlessly\ntransferring instructions between pipeline stages.\nLet us explain with an example. When an instruction enters the pipeline, it enters the\nIF unit. At the end of the first cycle, it gets saved in the IF-OF register. At the beginning\nof the second cycle, it enters the OF stage, and then again at the end of the second cycle,\nit gets latched into the OF-EX register. This pattern continues till the instruction leaves the\npipeline. The pipeline registers essentially transfer their inputs to the outputs at the end of a\ncycle(negativeedgeoftheclock). Thenthelogicofthepipelinestageprocessestheinstruction,\nand at the end of the cycle, the instruction gets transferred to the register of the subsequent\npipeline stage. In this manner, an instruction hops between stages till it reaches the end of the\npipeline.\n9.2.3 The Instruction Packet\nLet us now proceed to design our data path with pipeline stages and registers in some more de-\ntail. Up till now we have been maintaining that the instruction needs to be transferred between\nregisters. Let us elaborate on the term \u201cinstruction\u201d. We actually mean an instruction packet\nhere, which contains all the details regarding the instruction, along with all of its intermediate\nresults, and the control signals that it may require.\nWe need to create such an elaborate instruction packet because there are multiple instruc-\ntions in the processor at the same time. We need to ensure that there is no overlap between\nthe information required to process two different instructions. A clean way of designing this\nis to confine all the information required to process an instruction in a packet, and transfer\nthe packet between pipeline registers every cycle. This mechanism also ensures that all the\nintermediate state required to process an instruction is removed after it leaves the pipeline.\nWhat should an instruction packet contain? It needs to contain at the least, the PC and\nthe contents of the instruction. It should also contain all the operands and control signals that\nare required by subsequent stages. The amount of information that needs to be stored in the\ninstruction packet reduces as the instruction proceeds towards the last stage. For the sake of\nuniformity, we assume that all the pipeline registers have the same size, and are sized to hold\nthe entire instruction packet. Some of the fields might not be used. However, this is a negligible\noverhead. Let us now proceed to design the data path of the pipeline. We shall use exactly the\nsame design as we had used for the single-cycle processor. The only difference is that we add a\nregister after each pipeline stage, other than the last stage, RW. Secondly, we add connections\nto transfer data in and out of the pipeline registers. Let us quickly take a look at each of the\npipeline stages in our pipelined data path. 381 (cid:13)c Smruti R. Sarangi\n9.3 Pipeline Stages\n9.3.1 IF Stage\nisBranchTaken\nbranchPC\n1\npc + 4 0\npc\nInstruction instruction\nmemory\npc instruction IF\/OF\nFigure 9.4: The IF stage in a pipelined processor\nFigure 9.4 shows the IF stage augmented with a pipeline register. We save the value of\nthe PC, and the contents of the instruction in the pipeline register. This is all the information\nthat we need to carry forward to the subsequent stages of the pipeline. Other than this small\nchange, we do not need to make any other change in this part of the data path.\n9.3.2 OF Stage\npc IF-OF\ninstruction\nrd rs2 ra(15) rs1\nisSt\n1 0 1 0\nControl\nisRet\nunit\nImmediate and Register\nbranch target file\nop2 op1\nimmx\nisImmediate\n1 0\npc branchTarget B A op2 instruction control OF-EX\nFigure 9.5: The OF stage in a pipelined processor\nFigure 9.5 shows the design of the operand fetch stage. The only extra additions are the (cid:13)c Smruti R. Sarangi 382\nconnections tothe two pipeline registers IF-OF,and OF-EX.The stage startsout by extracting\nthe fields rd (bits 23-26), rs1 (bits 19-22), rs2 (bits 15-18), and the immediate (bits 1-18)\nfrom the instruction. These are sent to the register file, the immediate and branch units.\nAdditionally, we send the contents of the instruction to the control unit that generates all the\ncontrol signals. Three of the control signals namely isRet, isImmediate, and isSt are used\nimmediately. The rest of the control signals are for controlling multiplexers in subsequent\nstages of the pipeline. Hence, it is necessary for us to save them in the OF-EX pipeline register\nsuch that they can traverse the pipeline along with the instruction, and control the actions of\ndifferent units accordingly. Therefore, we allocate some space within the instruction packet,\nand store all the control signals generated by the control unit (refer to the field control in\nFigure 9.5).\nWe need to carry all the intermediate results generated by the OF stage. In specific, the OF\nstagegeneratesthebranchTarget, boththeinputsfortheALU(A, andB), andthevaluetobe\nwritten to memory for a store instruction (op2). Thus, we allocate four fields in the instruction\npacket, andtheOF-EXpipelineregistertostorethisinformationasshowninFigure9.5. Letus\nrecall that the aim of designing the instruction packet was to have all the information required\nto process an instruction at one place. In accordance with this philosophy we have saved all\nthe details of the instruction including its address, contents, intermediate results, and control\nsignals in our pipeline registers.\n9.3.3 EX Stage\npc branchTarget B A op2 instruction control OF-EX\naluSignals\ns\n0 1 ALU g Branch isBeq\nbranchPC isRet fla unit ii ss UB Bg rt\nanch\nisBranchTaken\npc aluResult op2 instruction control EX-MA\nFigure 9.6: The EX stage in a pipelined processor\nLet us now take a look at the EX stage in Figure 9.6. The ALU receives its inputs(A and\nB) from the OF-EX pipeline register. The results generated by this stage are the aluResult\n(result of the ALU operation), the final branch target, and the branch outcome. The branch\noutcome is 1, if the branch is taken, otherwise it is 0. The result of the ALU operation is\nadded to the instruction packet, and saved in the EX-MA register. The EX-MA register also\ncontains the rest of the fields of the instruction packet namely the PC, instruction (contents\nof the instruction), control signals, and the second operand read from the register file (op2).\nFor computing the final branch target, we need to choose between the branch target com-\nputed in the OF stage and the value of the return address register (possibly stored in A). The\nresult of the choice is the final branch target(branchPC), and this is sent to the fetch unit. The\ntinu\nhctef\noT 383 (cid:13)c Smruti R. Sarangi\nbranch unit computes the value of the signal, isBranchTaken. If it is 1, then the instruction is\na branch, and it is taken. Otherwise, the fetch unit needs to use the default option of fetching\nthe next PC.\n9.3.4 MA Stage\npc aluResult op2 instruction control EX-MA\nmar mdr isLd\nData memory\nMemory\nunit\nisSt\npc ldResult aluResult instruction control MA-RW\nFigure 9.7: The MA stage in a pipelined processor\nTheMAstageisshowninFigure9.7. Theonlyoperandthattheloadinstructionusesisthe\nresult of the ALU, which contains the effective memory address. This is saved in the aluResult\nfield of the EX-MA register. The data to be stored resides in the rd register. This value was\nread from the register file in the OF stage, and was stored in the op2 field of the instruction\npacket. In this stage, the op2 field is connected to the MDR (memory data register) register.\nThe relevant control signals \u2013 isLd and isSt \u2013 are also a part of the instruction packet, and\nthey are routed to the memory unit.\nThe only output of this stage is the result of the load instruction. This is saved in the\nldResult field of the MA-RW register.\n9.3.5 RW Stage\npc ldResult aluResult instruction control\n4 isLd\n10 01 00 isCall isWb\nE\nrd Register\n0 A\nE enable file\nA address data ra(15) 1 D\nD data\nFigure 9.8: The RW stage in a pipelined processor (cid:13)c Smruti R. Sarangi 384\nWe need to lastly take a look at the RW stage in Figure 9.8. The inputs that it requires\nfrom the previous stages are the values of the ALU and load operations stored in the aluResult\nand ldResult fields respectively. These inputs along with the default next PC (current PC +\n4) are connected to a multiplexer that chooses the value to be written back. The rest of the\ncircuit is the same as that of the single-cycle processor. Note that there is no pipeline register\nat the end of the RW stage because it is the last stage in the pipeline.\n9.3.6 Putting it All Together\nLet us now summarise our discussion regarding the simple pipeline by showing our data path\nwith the pipeline registers in Figure 9.9. The figure is undoubtedly complex. However, the\nreader has seen all the parts of this figure before and thus should not have a significant amount\nof difficulty in putting the different parts together. Nonetheless, we should note that the design\nof our processor has already become fairly complex, and the size of our diagrams have already\nreached one page !!! We do not want to introduce more complicated diagrams. The reader\nshould note that up till now our aim was to introduce the entire circuit. However, we shall now\nintroduce some degree of abstraction such that we can introduce more complicated features\ninto our processor. Henceforth, we shall broadly concentrate on the logic of the pipeline, and\nnot talk about the implementation in detail. We shall leave the implementation of the exact\ncircuit as an exercise for the reader.\nFigure9.10showsanabstractionofourpipelinedatapath. Thisfigureprominentlycontains\nblock diagrams for the different units and shows the four pipeline registers. We shall use this\ndiagram as the baseline for our discussion on advanced pipelines. Recall that the first register\noperand can either be the rs1 field of the instruction, or it can be the return address register\nin the case of a ret instruction. A multiplexer to choose between ra and rs1 is a part of our\nbaseline pipeline design, and for the sake of simplicity, we do not show it in the diagram. We\nassume that it is a part of the register file unit. Similarly, the multiplexer to choose the second\nregister operand (between rd and rs2) is also assumed to be a part of the register file unit,\nand is thus not shown in the diagram. We only show the multiplexer that chooses the second\noperand (register or immediate).\n9.4 Pipeline Hazards\nIn our simple pipeline discussed in Section 9.2, we were not concerned with correctness issues in\nthe pipeline. We were simply concerned with designing the pipeline, and having the capability\nto process five instructions at the same time. Now, we want to take a look at correctness issues.\nLet us start out by introducing the pipeline diagram, which will prove to be a very useful tool\nin our analyses.\n9.4.1 The Pipeline Diagram\nWe typically use a pipeline diagram to study the behaviour of a pipeline. It shows the rela-\ntionships between instructions, clock cycles, and the different stages of the pipeline. It can be\nused to study the nature of dependences across different instructions, and their execution in\nthe pipeline. 385 (cid:13)c Smruti R. Sarangi\n1\npc + 4 0\npc\nInstruction instruction\nmemory\npc instruction\nrd rs2 ra(15) rs1\nisSt\n1 0 1 0\nControl\nisRet\nreg unit\nImmediate and Register data\nbranch target file\nop2 op1\nisWb\nimmx\nisImmediate\n1 0\npc branchTarget B A op2 instruction control\naluSignals\n0 1 ALU gs Branch isBeq\nisRet a unit isBgt\nfl isUBranch\nisBranchTaken\npc aluResult op2 instruction control\nmar mdr isLd\nData\nMemory\nmemory\nunit isSt\npc ldResult aluResult instruction control\n4 isLd\n10 01 00 isCall isWb\nrd\n0\nra(15) 1\ndata\nFigure 9.9: Pipelined data path\nFigure 9.11 shows a pipeline diagram for three instructions as they proceed through the\npipeline. Each row of the diagram corresponds to each pipeline stage. The columns correspond\ntoclockcycles. Inoursamplecode,wehavethreeinstructionsthatdonothaveanydependences\nbetween each other. We name these instructions \u2013 [1], [2], and [3] respectively. The earliest (cid:13)c Smruti R. Sarangi 386\nIF-OF OF-EX EX-MA MA-RW\nControl\nunit Branch\nMemory\nunit\nunit\nFetch Immediate Register\nunit and branch flags write unit\nunit\nData\nALU\nmemory\nop2 Unit\nInstruction Register\nmemory file op1\nFigure 9.10: An abstraction of the pipelined data path\nClock cycles\n1 2 3 4 5 6 7 8 9\nIF 1 2 3\n[1]: add r1, r2, r3\nOF 1 2 3\n[2]: sub r4, r5, r6 EX 1 2 3\nMA 1 2 3\n[3]: mul r8, r9, r10\nRW 1 2 3\nFigure 9.11: The Pipeline Diagram\ninstruction, [1] enters the IF stage of the pipeline in the first cycle, and leaves the pipeline in\nthe fifth cycle. Similarly, the second instruction, [2], enters the IF stage of the pipeline in the\nsecond cycle, and leaves the pipeline in the sixth cycle. Each of these instructions progresses to\nthe subsequent stage of the pipeline in each cycle. The trace of each instruction in the pipeline\ndiagram is a diagonal that is oriented towards the bottom-right. Note that this scenario will\nget fairly complicated after we consider dependences across instructions.\nHere, are the rules to construct a pipeline diagram.\n1. Constructagridofcells,whichhasfiverows,andN columns,whereN isthetotalnumber\nof clock cycles that we wish to consider. Each of the five rows corresponds to a pipeline\nstage.\n2. If an instruction ([k]) enters the pipeline in cycle m, then we add an entry corresponding\nto [k] in the mth column of the first row. 387 (cid:13)c Smruti R. Sarangi\n3. In the (m + 1)th cycle, the instruction can either stay in the same stage (because the\npipeline might be stalled, described later), or can move to the next row (OF stage). We\nadd a corresponding entry in the grid cell.\n4. Inasimilarmanner, theinstructionmovesfromtheIFstagetotheRWstageinsequence.\nIt never moves backwards. However, it can stay in the same stage across consecutive\ncycles.\n5. We cannot have two entries in a cell.\n6. We finally remove the instruction from the pipeline diagram after it leaves the RW stage.\nExample 109\nBuild a pipeline diagram for the following code snippet. Assume that the first instruction\nenters the pipeline in cycle 1.\n[1]: add r1, r2, r3\n[2]: sub r4, r2, r5\n[3]: mul r5, r8, r9\nAnswer:\nClock cycles\n1 2 3 4 5 6 7 8 9\nIF 1 2 3\n[1]: add r1, r2, r3\nOF 1 2 3\n[2]: sub r4, r2, r5 EX 1 2 3\nMA 1 2 3\n[3]: mul r5, r8, r9\nRW 1 2 3\n9.4.2 Data Hazards\nLet us consider the following code snippet.\n[1]: add r1, r2, r3\n[2]: sub r3, r1, r4\nHere, the add instruction is producing the value for register, r1, and the sub instruction is\nusingitasasourceoperand. Letusnowconstructapipelinediagramforjusttheseinstructions\nas shown in Figure 9.12. (cid:13)c Smruti R. Sarangi 388\nclock cycles\n1 2 3 4 5 6 7 8 9\nIF 1 2\n[1]: add r1, r2, r3\nOF 1 2\n[2]: sub r3, r1, r4 EX 1 2\nMA 1 2\nRW 1 2\nFigure 9.12: Pipeline diagram showing a RAW hazard\nThere is a problem. Instruction 1 writes the value of r1 in the fifth cycle, and instruction\n2 needs to read its value in the third cycle. This is clearly not possible. We have added an\narrow between the relevant pipeline stages of both the instructions to indicate that there is\na dependency. Since the arrow is towards the left (backwards in time), we cannot execute\nthis code sequence in a pipeline. This is known as a data hazard. A hazard is defined as the\npossibilityoferroneousexecutionofaninstructioninapipeline. Thisspecificcaseisclassifiedas\na data hazard, where it is possible that instruction 2 might get the wrong data unless adequate\nsteps are taken.\nDefinition 67\nA hazard is defined as the possibility of erroneous execution of an instruction in a pipeline.\nA data hazard represents the possibility of erroneous execution because of the unavailability\nof correct data.\nThis specific type of data hazard is known as a RAW (read after write) hazard. Here the\nsubtract instruction is trying to read r1, which needs to be written by the add instruction. In\nthis case, a read succeeds a write.\nNote that this is not the only kind of data hazard. The two other types of data hazards\nare WAW (write after write), and WAR (write after read) hazards. These hazards are not an\nissue in our pipeline because we never change the order of instructions. A preceding instruction\nis always ahead of a succeeding instruction in the pipeline. This is an example of an in-order\npipeline. Incomparison,modernprocessorshaveout-of-orderpipelinesthatexecuteinstructions\nin different orders. 389 (cid:13)c Smruti R. Sarangi\nDefinition 68\nInanin-orderpipeline(suchasours), aprecedinginstructionisalwaysaheadofasucceeding\ninstruction in the pipeline. Modern processors use out-of-order pipelines that break this rule\nand it is possible for later instructions to execute before earlier instructions.\nLet us take a look at the following assembly code snippet.\n[1]: add r1, r2, r3\n[2]: sub r1, r4, r3\nHere, instructions 1 and 2 are writing to register r1. In an in-order pipeline r1 will be\nwritten in the correct order, and thus there is no WAW hazard. However, in an out-of-order\npipeline we run the risk of finishing instruction 2 before instruction 1, and thus r1 can end up\nwith the wrong value. This is an example of a WAW hazard. The reader should note that\nmodern processors ensure that r1 does not get the wrong value by using a technique known as\nregister renaming (see Section 9.11.4).\nLet us give an example of a potential WAR hazard.\n[1]: add r1, r2, r3\n[2]: add r2, r5, r6\nHere, instruction 2 is trying to write to r2, and instruction 1 has r2 as a source operand. If\ninstruction 2 executes first, then instruction 1 risks getting the wrong value of r2. In practice\nthis does not happen in modern processors because of schemes such as register renaming. The\nreader needs to understand that a hazard is a theoretical risk of something wrong happening.\nIt is not a real risk because adequate steps are taken to ensure that programs are not executed\nincorrectly.\nIn this book, we will mostly focus on RAW hazards, because WAW and WAR hazards are\nrelevant only for modern out-of-order processors. Let us outline the nature of the solution.\nTo avoid a RAW hazard it is necessary to ensure that the pipeline is aware of the fact that it\ncontainsapairofinstructions,whereoneinstructionwritestoaregister,andanotherinstruction\nthat comes later in program order reads from the same register. It needs to ensure that the\nconsumer instruction correctly receives the value of the operand (in this case, register) from\nthe producer instruction. We shall look at solutions in both hardware and software.\n9.4.3 Control Hazards\nLet us now look at another type of hazards that arise when we have branch instructions in the\npipeline. Let us consider the following code snippet.\n[1]: beq .foo\n[2]: mov r1, 4\n[3]: add r2, r4, r3\n...\n... (cid:13)c Smruti R. Sarangi 390\n.foo:\n[100]: add r4, r1, r2\nLet us show the pipeline diagram for the first three instructions in Figure 9.13.\nClock cycles\n1 2 3 4 5 6 7 8 9\nIF 1 2 3\n[1]: beq .foo\nOF 1 2 3\n[2]: mov r1, 4 EX 1 2 3\nMA 1 2 3\n[3]: add r2, r4, r3\nRW 1 2 3\nFigure 9.13: Pipeline diagram\nHere,theoutcomeofthebranchisdecidedincycle3,andiscommunicatedtothefetchunit.\nThe fetch unit starts fetching the correct instruction from cycle 4. Now, if the branch is taken,\nthen instructions 2, and 3, should not be executed. Sadly, there is no way of knowing in cycles\n2 and 3, about the outcome of the branch. Hence, these instructions will be fetched, and will be\na part of the pipeline. If the branch is taken, then there is a possibility that instructions 2 and\n3 might corrupt the state of the program, and consequently introduce an error. Instructions 2\nand 3, are known as instructions in the wrong path. This scenario is known as a control hazard.\nDefinition 69\nInstructions that would have been executed if a branch would have had an outcome that is\ndifferent from its real outcome, are said to be on the wrong path. For example, instructions\nsucceeding a branch instruction in the program, are in the wrong path, if the branch is taken.\nDefinition 70\nA control hazard represents the possibility of erroneous execution in a pipeline because\ninstructions in the wrong path of a branch can possibly get executed and save their results\nin memory, or in the register file.\nTo avoid a control hazard, it is necessary to identify instructions in the wrong path, and\nensure that their results do not get committed to the register file, and memory. There should\nbe a way to nullify such instructions, or avoid them altogether. 391 (cid:13)c Smruti R. Sarangi\n9.4.4 Structural Hazards\nDefinition 71\nA structural hazard refers to the possibility of instructions not being able to execute because\nof resource constraints. For example, they can arise when multiple instructions try to access\na functional unit in the same cycle, and due to capacity limitations, the unit cannot allow all\nthe interested instructions to proceed. In this case, a few of the instructions in the conflict\nneed to stall their execution.\nStructural hazards do not arise in the SimpleRisc pipeline. However, for the sake of complete-\nness, we should still study them. They arise when different instructions try to access the same\nresource, and the resource cannot allow all of them to access it in the same cycle. Let us give\nan example. Let us suppose that we had an add instruction that could read one operand from\nmemory. It could have the following form:\nadd r1, r2, 10[r3]\nHere, we have one register source operand, r2, and a memory source operand, 10[r3]. Let\nus further assume that our pipeline reads the value of the memory operand in the OF stage.\nLet us now look at a potentially conflicting situation.\n[1]: st r4, 20[r5]\n[2]: sub r8, r9, r10\n[3]: add r1, r2, 10[r3]\nNote that there are no control and data hazards here. Let us nonetheless, consider a point\nin the pipeline diagram when the store instruction is in the MA stage. At this point instruction\n2 is in the EX stage, and instruction 3 is in the OF stage. Note that in this cycle, both\ninstructions 1 and 3 need to access the memory unit. However, if we assume that the memory\nunit can only service one request per cycle, then clearly there is a conflicting situation. One of\nthe instructions needs to stall its execution. This situation is an example of a structural hazard.\nWe claim that in our SimpleRisc pipeline there are no structural hazards. In other words,\nwe never have a situation in which multiple instructions across different pipeline stages wish\nto access the same unit, and that unit does not have the capacity to service all the requests.\nThis statement can be proved by considering that the only units that are accessed by multiple\nstages are the fetch unit, and the register file. The fetch unit is accessed by an instruction in\nthe IF stage, and by branch instructions in the EX stage. It is designed to handle both the\nrequests. Likewise, the register file is accessed by instructions in the OF stage, and RW stage.\nOur register file has two read ports, and one write port. It can thus handle both the requests\nin the same cycle.\nLet us thus focus on trying to eliminate RAW and control hazards. (cid:13)c Smruti R. Sarangi 392\n9.5 Solutions in Software\n9.5.1 RAW Hazards\nNow, let us find a way of avoiding a RAW hazard. Let us look at our example again.\n[1]: add r1, r2, r3\n[2]: sub r3, r1, r4\nInstruction 2 requires the value of r1 in the OF stage. However, at that point of time,\ninstruction 1 is in the EX stage, and it would not have written back the value of r1 to the\nregister file. Thus, instruction 2 cannot be allowed to proceed in the pipeline. Let us propose a\nnaive software solution to this problem. A smart compiler can analyse the code sequence and\nrealise that a RAW hazard exists. It can introduce nop instructions between these instructions\nto remove any RAW hazards. Let us consider the following code sequence\n[1]: add r1, r2, r3\n[2]: nop\n[3]: nop\n[4]: nop\n[5]: sub r3, r1, r4\nHere,whenthesubinstructionreachestheOFstage,theaddinstructionwouldhavewritten\nits value and left the pipeline. Thus, the sub instruction will get the correct value. Note that\nadding nop instructions is a costly solution, because we are essentially wasting computational\npower. Inthisexample, wehavebasicallywasted3cyclesbyaddingnopinstructions. However,\nif we consider a longer sequence of code, then the compiler can possibly reorder the instructions\nsuch that we can minimise the number of nop instructions. The basic aim of any compiler\nintervention needs to be that there have to be a minimum of 3 instructions between a producer\nand consumer instruction. Let us consider Example 110.\nExample 110\nReorder the following code snippet, and add a sufficient number of nop instructions to make\nit execute correctly on a SimpleRisc pipeline.\nadd r1, r2, r3\nadd r4, r1, 3\nadd r8, r5, r6\nadd r9, r8, r5\nadd r10, r11, r12\nadd r13, r10, 2\nAnswer: 393 (cid:13)c Smruti R. Sarangi\nadd r1, r2, r3\nadd r8, r5, r6\nadd r10, r11, r12\nnop\nadd r4, r1, 3\nadd r9, r8, r5\nadd r13, r10, 2\nWe need to appreciate two important points here. The first is the power of the nop in-\nstruction, and the next is the power of the compiler. The compiler is a vital tool in ensuring\nthe correctness of the program, and also improving its performance. In this case, we want to\nreorder code in such a way that we introduce the minimum number of nop instructions.\n9.5.2 Control Hazards\nLet us now try to use the same set of techniques to solve the issue of control hazards. If we take\na look at the pipeline diagram again, then we can conclude that there need to be a minimum\nof two instructions between the branch instruction and the instruction at the branch target.\nThis is because, we get both the branch outcome, and the branch target at the end of the EX\nstage. At this point of time there are two more instructions in the pipeline. These instructions\nhave been fetched when the branch instruction was in the OF, and EX stages respectively.\nThey might potentially be on the wrong path. After the branch target, and outcome have been\ndetermined in the EX stage, we can proceed to fetch the correct instruction in the IF stage.\nNow, let us consider these two instructions that were fetched, when we were not sure of the\nbranch outcome. If the PC of the branch is equal to p , then their addresses are p +4, and\n1 1\np +8 respectively. They are not on the wrong path if the branch is not taken. However, if the\n1\nbranch is taken, then these instructions need to be discarded from the pipeline since they are\non the wrong path. For doing this, let us look at a simple solution in software.\nLetusconsideraschemewherethehardwareassumesthatthetwoinstructionsimmediately\nafter a branch instruction are not on the wrong path. The positions of these two instructions\nare known as the delay slots. Trivially, we can ensure that the instructions in the delay slots do\nnotintroduceerrors,byinsertingtwonopinstructionsafterabranch. However,wewillnotgain\nany extra performance by doing this. We can instead find two instructions that execute before\nthe branch instruction, and move them to the two delay slots immediately after the branch.\nNote that we cannot arbitrarily move instructions to the delay slots. We cannot violate\nany data dependence constraints, and we need to also avoid RAW hazards. Secondly, we\ncannot move any compare instructions into the delay slots. If appropriate instructions are not\navailable, then we can always fall back to the trivial solution and insert nop instructions. It is\nalso possible that we may find just one instruction that we can reorder, then we just need to\ninsert one nop instruction after the branch instruction. The method of delayed branches is a\nvery potent method in reducing the number of nop instructions that need to be added to avoid\ncontrol hazards.\nThe reader should convince herself that to support this simple software scheme, we do not\nneed to make any changes in hardware. The pipelined data path shown in Figure 9.9 already (cid:13)c Smruti R. Sarangi 394\nsupports this scheme. In our simple pipelined data path, the two instructions fetched after the\nbranch have their PCs equal to p + 4, and p + 8 respectively (p is the PC of the branch\n1 1 1\ninstruction). Since the compiler ensures that these instructions are always on the correct path\nirrespective of the outcome of the branch, we do not commit an error by fetching them. After\nthe outcome of the branch has been determined, the next instruction that is fetched either has\na PC equal to p +12 if the branch is not taken, or the PC is equal to the branch target if the\n1\nbranch is taken. Thus, in both the cases, the correct instruction is fetched after the outcome\nof the branch is determined, and we can conclude that our software solution executes programs\ncorrectly on the pipelined version of our processor.\nTo summarise, the crux of our software technique is the notion of the delay slot. We need\ntwo delay slots after a branch because we are not sure about the two subsequent instructions.\nTheymightbeonthewrongpath. However, usingasmartcompilerwecanmanagetomovein-\nstructionsthatgetexecutedirrespectiveoftheoutcomeofthebranchtothedelayslots. Wecan\nthus avoid placing nop instructions in the delay slots, and consequently increase performance.\nSuch a branch instruction is known as a delayed branch instruction.\nDefinition 72\nA branch instruction is known as a delayed branch if the processor assumes that all the\nsucceeding instructions that are fetched before its outcome has been determined, are on\nthe correct path. If the processor fetches n instructions between the time that a branch\ninstruction has been fetched, and its outcome has been determined, then we say that we have\nn delay slots. The compiler needs to ensure that instructions on the correct path occupy the\ndelay slots, and no additional control or RAW hazards are introduced. The compiler can\nalso trivially introduce nop instructions in the delay slots.\nNow, let us consider a set of examples.\nExample 111\nReorder the following piece of assembly code to correctly run on a pipelined SimpleRisc\nprocessor with delayed branches. Assume two delay slots per branch instruction.\nadd r1, r2, r3\nadd r4, r5, r6\nb .foo\nadd r8, r9, r10\nAnswer:\nb .foo\nadd r1, r2, r3\nadd r4, r5, r6\nadd r8, r9, r10 395 (cid:13)c Smruti R. Sarangi\n9.6 Pipeline with Interlocks\nUptillnow,wehaveonlylookedatsoftwaresolutionsforeliminatingRAWandcontrolhazards.\nHowever, compiler approaches, are not very generic. Programmers can always write assembly\ncode manually, and try to run it on the processor. In this case, the likelihood of an error is\nhigh, because programmers might not have reordered their code properly to remove hazards.\nSecondly, there is an issue of portability. A piece of assembly code written for one pipeline,\nmight not run on another pipeline that follows the same ISA. This is because it might have\na different number of delay slots, or different number of stages. One of our main aims of\nintroducing assembly programs gets defeated, if our assembly programs are not portable across\ndifferent machines that use the same ISA.\nHence, letustrytodesignsolutionsatthehardwarelevel. Thehardwareshouldensurethat\nirrespective of the assembly program, it is run correctly. The output should always match that\nproduced by a single cycle processor. To design such kind of a processor, we need to ensure\nthat an instruction never receives wrong data, and wrong path instructions are not executed.\nThis can be done by ensuring that the following conditions hold.\n\u2022 Condition: Data-Lock : We cannot allow an instruction to leave the OF stage unless it\nhas received the correct data from the register file. This means that we need to effectively\nstall the IF and OF stages and let the rest of the stages execute till the instruction in the\nOF stage can safely read its operands. During this time, the instruction that passes from\nthe OF to the EX stage needs to be a nop instruction.\n\u2022 Condition: Branch-Lock : We never execute instructions on the wrong path. We either\nstall the processor till the outcome is known, or use techniques to ensure that instructions\non the wrong path are not able to commit their changes to the memory, or registers.\nDefinition 73\nIn a purely hardware implementation of a pipeline, it is sometimes necessary to stop a new\ninstruction from entering a pipeline stage, till a certain condition ceases to hold. The notion\nof stopping a pipeline stage from accepting and processing new data, is known as a pipeline\nstall, or a pipeline interlock. Its primary purpose is to ensure the correctness of program\nexecution.\nIf we ensure that both the Data-Lock and Branch-Lock conditions hold, then our pipeline\nwill execute instructions correctly. Note that both the conditions dictate that possibly some\nstages of the pipeline needs to be stalled for some time. These stalls are also known as pipeline\ninterlocks. In other words, by keeping our pipeline idle for some time, we can avoid executing\ninstructions that might potentially lead to an erroneous execution. Let us now quickly compare\nthe pure software and hardware schemes in Table 9.1, and see what are the pros and cons of\nimplementing the entire logic of the pipeline in hardware. Note that in the software solution we\ntry to reorder code, and subsequently insert the minimum number of nop instructions to nullify\nthe effect of hazards. In comparison, in the hardware solution, we dynamically stall parts of the (cid:13)c Smruti R. Sarangi 396\npipeline to avoid executing instructions in the wrong path, or with wrong values of operands.\nStalling the pipeline is tantamount to keeping some stages idle, and inserting nop instructions\nin other stages as we shall see later in this section.\nAttribute Software Hardware (with interlocks)\nPortability Limited to a specific Programs can be run on any pro-\nprocessor cessorirrespectiveofthenatureof\nthe pipeline\nBranches Possible to have no Needtostallthepipelinefor2cy-\nperformance penalty, cles in our design\nby using delay slots\nRAW hazards Possible to eliminate Need to stall the pipeline\nthem through code\nscheduling\nPerformance Highly dependent on The basic version of a pipeline\nthe nature of the pro- with interlocks is expected to be\ngram slower than the version that relies\non software\nTable 9.1: Comparison between software and hardware approaches for ensuring the correctness\nof a pipeline\nWe observe that the efficacy of the software solution is highly dependent on the nature of\nthe program. It is possible to reorder the instructions in some programs to completely hide the\ndeleterious effects of RAW hazards and branches. However, in some programs we might not\nfind enough instructions that can be reordered. We would be thus compelled to insert a lot\nof nop instructions, and this would reduce our performance. In comparison, a pure hardware\nscheme, which obeys the Data-Lock and Branch-Lock conditions stalls the pipeline whenever\nit detects an instruction that might execute erroneously. It is a generic approach, which is\nslower than a pure software solution.\nNow, it is possible to combine the hardware and software solutions to reorder code to make\nit \u201cpipeline friendly\u201d as much as possible, and then run it on a pipeline with interlocks. Note\nthat in this approach, no guarantees of correctness are made by the compiler. It simply spaces\nproducer and consumer instructions as far apart as possible, and takes advantage of delayed\nbranches if they are supported. This reduces the number of times we need to stall the pipeline,\nand ensures the best of both worlds. Before proceeding to design a pipeline with interlocks, let\nus study the nature of interlocks with the help of pipeline diagrams.\n9.6.1 A Conceptual Look at a Pipeline with Interlocks\nData Hazards\nLetusnowdrawthepipelinediagramofapipelinewithinterlocks. Letusconsiderthefollowing\ncode snippet.\n[1]: add r1, r2, r3\n[2]: sub r4, r1, r2 397 (cid:13)c Smruti R. Sarangi\nHere, instruction [1] writes to register r1 and instruction [2] reads from r1. Clearly, there\nis a RAW dependence. To ensure the Data-Lock condition, we need to ensure that instruction\n[2] leaves the OF stage only when it has read the value of r1 written by instruction [1]. This is\npossible only in cycle 6 (refer to the pipeline diagram in Figure 9.14). However, instruction [2]\nreaches the OF stage in cycle 3. If there would have been no hazard, then it would have ideally\nproceeded to the EX stage in cycle 4. Since we have an interlock, instruction [2] needs to stay\nin the OF stage in cycles 4,5 and 6 also. The question is, \u201cwhat does the EX stage do when\nit is not processing a valid instruction in cycles 4, 5 and 6?\u201d Similarly, the MA stage does not\nprocess any valid instruction in cycles 5, 6 and 7. We need to have a way to disable pipeline\nstages, such that we do not perform redundant work. The standard approach is to insert nop\ninstructions into stages, if we want to effectively disable them.\nClock cycles\nbubble\n1 2 3 4 5 6 7 8 9\nIF 1 2\n[1]: add r1, r2, r3\nOF 1 2 2 2 2\n[2]: sub r4, r1, r2 EX 1 2\nMA 1 2\nRW 1 2\nFigure 9.14: A pipeline diagram with bubbles\nLet us refer to Figure 9.14 again. At the end of cycle 3, we know that we need to introduce\nan interlock. Hence, in cycle 4, instruction [2] remains in the OF stage, and we insert a nop\ninstruction into the EX stage. This nop instructions moves to the MA stage in cycle 5, and RW\nstage in cycle 6. This nop instruction is called a pipeline bubble. A bubble is a nop instruction\nthat is dynamically inserted by the interlock hardware. It moves through the pipeline stages\nakin to normal instructions. Similarly, in cycles 5 and 6 also, we need to insert a pipeline\nbubble. Finally, in cycle 7, instruction [2] is free to proceed to the EX, and subsequent stages.\nA bubble by definition does not do anything, and thus none of the control signals are turned\non when a stage encounters a bubble. The other subtle point to note here is that we cannot\nread and write to the same register in the same cycle. We need to give preference to the write\nbecause it is an earlier instruction, and the read needs to stall for one cycle.\nThere are two ways to implement a bubble. The first is that we can have a separate bubble\nbit in the instruction packet. Whenever, the bit is 1, the instruction will be construed to be a\nbubble. The second is that we can change the opcode of the instruction to that of a nop, and\nreplace all of its control signals by 0s. The latter approach is more invasive, but can eliminate\nredundantworkinthecircuitcompletely. Intheformerapproach, thecontrolsignalswillbeon,\nand units that are activated by them, will remain operational. The hardware needs to ensure (cid:13)c Smruti R. Sarangi 398\nthat a bubble is not able to make changes to registers or memory.\nDefinition 74\nA pipeline bubble is a nop instruction that is inserted dynamically in a pipeline register by\nthe interlock hardware. A bubble propagates through the pipeline in the same way as normal\ninstructions.\nWe can thus conclude that it is possible to avoid data hazards, by dynamically inserting\nbubbles in the pipeline. Let us quickly take a look at the issue of slow instructions such as the\ndiv and mod instructions. It is highly likely that in most pipelines these instructions will take\nn (n > 1) cycles to execute in the EX stage. In each of the n cycles, the ALU completes a\npart of the processing of the div or mod instructions. Each such cycle is known as a T State.\nTypically, one stage has 1 T State; however, the EX stage for a slow instruction has many T\nstates. Hence, to correctly implement slow instructions, we need to stall the IF and OF stages\nfor (n\u22121) cycles till the operations complete.\nFor the sake of simplicity, we shall not discuss this issue further. Instead, we shall move on\nwith the simplistic assumption that all our pipeline stages are balanced, and take 1 cycle to\ncomplete their operation.\nControl Hazards\nNow, let us look at control hazards. Let us start out by considering the following code snippet.\n[1]: beq .foo\n[2]: add r1, r2, r3\n[3]: sub r4, r5, r6\n....\n....\n.foo:\n[4]: add r8, r9, r10\nInstead of using a delayed branch, we can insert bubbles in the pipeline if the branch is\ntaken. Otherwise, we do not need to do anything. Let us assume that the branch is taken. The\npipeline diagram for this case is shown in Figure 9.15.\nIn this case, the outcome of the branch condition of instruction [1] is decided in cycle 3.\nAt this point, instructions [2] and [3] are already in the pipeline (in the IF and OF stages,\nrespectively). Since the branch condition evaluates to taken, we need to cancel instructions\n[2] and [3], otherwise they will be executed erroneously. We thus, convert them to bubbles as\nshown in Figure 9.15. Instructions[2] and [3] are converted to bubbles in cycle 4. Secondly,\nwe fetch from the correct branch target (.foo) in cycle 4, and thus instruction [4] enters the\npipeline. Both of our bubbles proceed through all the pipeline stages, and finally leave the\npipeline in cycles 6 and 7 respectively.\nWe can thus ensure both the conditions (Data-Lock and Branch-Lock ) by dynamically\nintroducing bubbles in the pipeline. Now, let us look at these approaches in some more detail. 399 (cid:13)c Smruti R. Sarangi\nClock cycles\nbubble\n1 2 3 4 5 6 7 8 9\n[1]: beq. foo\n[2]: add r1, r2, r3 IF 1 2 3 4\n[3]: sub r4, r5, r6\nOF 1 2 4\n....\n.... EX 1 4\n.foo:\nMA 1 4\n[4]: add r8, r9, r10\nRW 1 4\nFigure 9.15: Pipeline diagram for a control hazard with bubbles\n9.6.2 Ensuring the Data-Lock Condition\nTo ensure the Data-Lock condition we need to ensure that there is no conflict between the\ninstruction in the OF stage, and any instruction in the subsequent stages. A conflict is defined\nas a situation that can cause a RAW hazard. In other words, a conflict exists if an instruction\nin a subsequent stage writes to a register that is read by the instruction in the OF stage. There\nare thus two pieces of hardware that we require to implement the Data-Lock condition. The\nfirst is to check if a conflict exists, and the second is to ensure that the pipeline gets stalled.\nLet us first look at the conflict detection hardware. The conflict detection hardware needs\nto compare the contents of the instruction in the OF stage with the contents of each of the\ninstructions in the other three stages namely EX, MA, and RW. If there is a conflict with any\nof these instructions, we can declare a conflict. Let us thus focus on the logic of detecting a\nconflict. We leave the design of the exact circuit as an exercise for the reader. Let us outline\nthe brief pseudo-code of a conflict detection circuit. Let the instruction in the OF stage be [A],\nand an instruction in a subsequent stage be [B]. The algorithm to detect a conflict is shown as (cid:13)c Smruti R. Sarangi 400\nAlgorithm 5.\nAlgorithm 5: Algorithm to detect conflicts between instructions\nData: Instructions: [A] and [B]\nResult: Conflict exists (true), no conflict (false)\n1 if [A].opcode \u2208 (nop,b,beq,bgt,call) then\n\/* Does not read from any register *\/\n2 return false\n3 end\n4 if [B].opcode \u2208 (nop, cmp, st, b, beq, bgt, ret) then\n\/* Does not write to any register *\/\n5 return false\n6 end\n\/* Set the sources *\/\n7 src1 \u2190 [A].rs1\n8 src2 \u2190 [A].rs2\n9 if [A].opcode = st then\n10 src2 \u2190 [A].rd\n11 end\n12 if [A].opcode = ret then\n13 src1 \u2190 ra\n14 end\n\/* Set the destination *\/\n15 dest \u2190 [B].rd\n16 if [B].opcode = call then\n17 dest \u2190 ra\n18 end\n\/* Check if the first operand exists *\/\n19 hasSrc1 \u2190 true\n20 if [A].opcode \u2208 (not,mov) then\n21 hasSrc1 \u2190 false\n22 end\n\/* Check the second operand to see if it is a register *\/\n23 hasSrc2 \u2190 true\n24 if [A].opcode \u2208\/ (st) then\n25 if [A].I = 1 then\n26 hasSrc2 \u2190 false\n27 end\n28 end\n\/* Detect conflicts *\/\n29 if (hasSrc1 = true) and (src1 = dest) then\n30 return true\n31 end\n32 else if (hasSrc2 = true) and (src2 = dest) then\n33 return true\n34 end\n35 return false 401 (cid:13)c Smruti R. Sarangi\nImplementing Algorithm 5 in hardware is straight forward. The reader can draw a simple\ncircuit and implement this algorithm. All we need is a set of logic gates and multiplexers.\nMost hardware designers typically write the description of a circuit similar to Algorithm 5 in\na hardware description language such as Verilog or VHDL, and rely on smart compilers to\nconvert the description to an actual circuit. Hence, we shall refrain from showing detailed\nimplementations of circuits henceforth, and just show the pseudo code.\nWe need three conflict detectors (OF \u2194 EX, OF \u2194 MA, OF \u2194 RW). If there are no\nconflicts, then the instruction is free to proceed to the EX stage. However, if there is at least\none conflict, we need to stall the IF and OF stages. Once an instruction passes the OF stage,\nit is guaranteed to have all of its source operands.\nStalling the Pipeline:\nLetusnowlookatstallingthepipeline. Weessentiallyneedtoensurethattillthereisaconflict\nno new instruction enters the IF and OF stages. This can be trivially ensured by disabling the\nwrite functionality of the PC and the IF-OF pipeline register. They thus cannot accept new\ndata on a clock edge, and thus will continue to hold their previous values.\nSecondly, we also need to insert bubbles in the pipeline. For example, the instruction that\npassesfromtheOFtotheEXstageneedstobeaninvalidinstruction,oralternativelyabubble.\nThiscanbeensuredbypassinganopinstruction. Hence,thecircuitforensuringtheData-Lock\ncondition is straight forward. We need a conflict detector that is connected to the PC, and the\nIF-OF register. Till there is a conflict, these two registers are disabled, and cannot accept new\ndata. We force the instruction in the OF-EX register to contain a nop. The augmented circuit\ndiagram of the pipeline is shown in Figure 9.16.\nbubble\nstall\nstall\nData-lock Unit\nControl\nunit Branch\nMemory\nunit\nunit\nFetch Immediate Register\nunit and branch flags write unit\nunit\nData\nALU memory\nop2 unit\nInstruction Register\nmemory file op1\nFigure 9.16: Data path of a pipeline with interlocks (implements the Data-Lock condition)\nFO-FI\nXE-FO AM-XE WR-AM (cid:13)c Smruti R. Sarangi 402\n9.6.3 Ensuring the Branch-Lock condition\nLetusnowassumethatwehaveabranchinstructioninthepipeline(b, beq, bgt, call, ret). Ifwe\nhave delay slots, then our data path is the same as that shown in Figure 9.16. We do not need\nto do any changes, because the entire complexity of execution has been offloaded to software.\nHowever, exposing the pipeline to software has its pros and cons as discussed in Table 9.1. If\nwe add more stages in the pipeline, then existing executables might cease to work. To avoid\nthis let us design a pipeline that does not expose delay slots to software.\nWe have two design options here. The first is that we can assume that a branch is not\ntaken till the outcome is decided. We can proceed to fetch the two instructions after a branch\nand process them. Once, the outcome of the branch is decided in the EX stage, we can take\nan appropriate action based on the outcome. If the branch is not taken, then the instructions\nfetched after the branch instruction, are on the correct path, and nothing more needs to be\ndone. However, if the branch is taken, then it is necessary to cancel those two instructions, and\nreplace them with pipeline bubbles (nop instructions).\nThe second option is to stall the pipeline till the outcome of the branch is decided, irrespec-\ntive of the outcome. Clearly, the performance of this design is less than the first alternative\nthat assumes that branches are not taken. For example, if a branch is not taken 30% of the\ntime, then with the first design, we do useful work 30% of the time. However, with the second\noption, we never do any useful work in the 2 cycles after a branch instruction is fetched. Hence,\nlet us go with the first design in the interest of performance. We cancel the two instructions\nafter the branch only if the branch is taken. We call this approach predict not taken, because\nwe are effectively predicting the branch to be not taken. Later on if this prediction is found to\nbe wrong, then can cancel the instructions in the wrong path.\nImportant Point 13\nIf the PC of a branch instruction is equal to p, then we choose to fetch the instructions\nat p+4, and p+8 over the next two cycles. If the branch is not taken, then we resume\nexecution. However, if the branch is taken, then we cancel these two instructions, and\nconvert them to pipeline bubbles.\nWe do not need to make any significant changes to the data path. We need a small branch\nhazard unit that takes an input from the EX stage. If the branch is taken, then in the next\ncycle it converts the instructions in the IF-OF and OF-EX stages to pipeline bubbles. The\naugmented data path with the branch interlock unit is shown in Figure 9.17.\n9.7 Pipeline with Forwarding\n9.7.1 Basic Concepts\nWe have now implemented a pipeline with interlocks. Interlocks ensure that a pipeline executes\ncorrectly irrespective of the nature of dependences across instructions. For the Data-Lock\ncondition we proposed to add interlocks in the pipeline that do not allow an instruction to 403 (cid:13)c Smruti R. Sarangi\nisBranchTaken\nbubble bubble\nBranch-lock unit\nstall\nstall\nData-lock unit\nControl\nunit Branch\nMemory\nunit\nunit\nFetch Immediate Register\nunit and branch flags write unit\nunit\nData\nALU memory\nop2 unit\nInstruction Register\nmemory file op1\nFigure 9.17: Data path of a pipeline with interlocks (implements both the Data-Lock and\nBranch-Lock conditions)\nleave the operand fetch stage until the correct values are available in the register file. However,\nwe shall see in this section that we do not need to add interlocks always. In fact, in a lot of\ninstances, the correct data is already present in pipeline registers, albeit not in the register\nfile. We can design a method to properly pass data from the internal pipeline registers to the\nappropriate functional unit. Let us consider a small example by considering this SimpleRisc\ncode snippet.\n[1]: add r1, r2, r3\n[2]: sub r4, r1, r2\nLet us take a look at the pipeline diagram with just these two instructions in Figure 9.18.\nFigure 9.18(a) shows the pipeline diagram with interlocks. Figure 9.18(b) shows a pipeline\ndiagram without interlocks and bubbles. Let us now try to argue that we do not need to insert\na bubble between the instructions.\nLet us take a deeper look at Figure 9.18(b). Instruction 1 produces its result at the end\nof the EX stage, or alternatively at the end of cycle 3, and writes to the register file in cycle\n5. Instruction 2 needs the value of r1 in the register file at the beginning of cycle 3. This is\nclearly not possible, and thus we had proposed to add pipeline interlocks to resolve this issue.\nHowever, let us try an alternative solution instead. Let us allow the instructions to execute.\nThen in cycle 3, [2] will get the wrong value. We allow it to proceed to the EX stage in cycle 4.\nAt this point of time, instruction [1] is in the MA stage, and its instruction packet contains the\ncorrect value of r1. This value of r1 was computed in the previous cycle, and is present in the\naluResult field of the instruction packet. [1]\u2019s instruction packet is in the EX-MA register in\nFO-FI\nXE-FO AM-XE WR-AM (cid:13)c Smruti R. Sarangi 404\nbubble Clock cycles Clock cycles\n1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9\nIF 1 2 1 2\n[1]: add r1, r2, r3\nOF 1 2 2 2 2 1 2\n[2]: sub r4, r1, r2 EX 1 2 1 2\nMA 1 2 1 2\nRW 1 2 1 2\n(a) (b)\nFigure 9.18: (a) Pipeline diagram with interlocks and bubbles (b) Pipeline diagram without\nbubbles\ncycle 4. Now, if we add a connection between the aluResult field of the EX-MA register and an\ninput of the ALU, then we can successfully transfer the correct value of r1 to the ALU. There\nwill be no error in our computation, because the operands to the ALU are correct, and thus\nthe result of the ALU operation will also be computed correctly. Figure 9.19 shows the result\nof our actions in the pipeline diagram. We add a line from the MA stage of instruction [1] to\nthe EX stage of instruction [2]. Since the arrow does not go backwards in time, it is possible\nto forward the data (value of r1) from one stage to the other.\nClock cycles\n1 2 3 4 5 6 7 8 9\nIF 1 2\n[1]: add r1, r2, r3\nOF 1 2\n[2]: sub r4, r1, r2 EX 1 2\nMA 1 2\nRW 1 2\nFigure 9.19: Example of forwarding in a pipeline\nDefinition 75\nForwarding is a method to transfer values of operands between instructions in different\npipeline stages through direct connections between the stages. We do not use the register file 405 (cid:13)c Smruti R. Sarangi\nfor transferring the values of operands across instructions. Forwarding allows us to avoid\ncostly pipeline interlocks.\nWe have just looked at an extremely powerful technique for avoiding stalls in pipelines.\nThis technique is known as forwarding. Essentially, we allow the values of operands to flow\nbetween instructions by directly transferring them across stages. We do not use the register\nfile to transfer values across instructions. The notion of forwarding has allowed us to execute\ninstructions [1] and [2] back to back (in consecutive cycles). We do not need to add any stall\ncycles. Hence, it is not necessary to reorder code, or insert nops.\nBefore, we proceed to the implementation of forwarding, let us discuss forwarding concep-\ntually using pipeline diagrams. To forward the value of r1 between instructions [1] and [2], we\nadded a connection between the MA stage and the EX stage. We showed this connection in\nFigure 9.19 by drawing an arrow between the corresponding stages of instructions [1] and [2].\nThe direction of this arrow was vertically upwards. Since it did not go backwards in time, we\nconcluded that it is possible to forward the value. Otherwise, it would not have been possible.\nLet us now try to answer a general question. Can we forward values between all pairs\nof instructions. Note that these need not be consecutive instructions. Even if there is one\ninstruction between an producer and a consumer ALU instruction, we still need to forward\nvalues. Let us now try to think of all possible forwarding paths between stages in a pipeline.\n9.7.2 Forwarding Paths in a Pipeline\nLet us discuss the basic tenets of forwarding that we shall broadly aim to follow.\n1. We add a forwarding path between a later stage and an earlier stage.\n2. We forward a value as late as possible in the pipeline. For example, if a given value is\nnot required in a given stage, and it is possible to get the value in a later stage from the\nproducer instruction, then we wait to get the forwarded value in the later stage.\nNote that both of these basic tenets do not affect the correctness of programs. They simply\nallow us to eliminate redundant forwarding paths. Let us now systematically look at all the\nforwarding paths that we require in our pipeline.\nRW \u2192 MA : Let us consider the MA stage. It needs a forwarding path from the RW stage.\nLet us consider the code snippet shown in Figure 9.20 Here, instruction [2] needs the value of\nr1 in the MA stage (cycle 5), and instruction [1] fetches the value of r1 from memory by the\nend of cycle 4. Thus, it can forward its value to instruction [2] in cycle 5.\nRW \u2192 EX : The code snippet shown in Figure 9.21 shows a load instruction that fetches the\nvalue of register r1 by the end of cycle 4, and a subsequent ALU instruction that requires the\nvalue of r1 in cycle 5. It is possible to forward the value because we are not going backwards\nin time.\nMA \u2192 EX : The code snippet shown in Figure 9.22 shows an ALU instruction that computes\nthe value of register r1 by the end of cycle 3, and a consecutive ALU instruction that requires (cid:13)c Smruti R. Sarangi 406\nClock cycles\n1 2 3 4 5 6 7 8 9\nIF 1 2\n[1]: ld r1, 4[r2]\nOF 1 2\n[2]: st r1, 10[r3] EX 1 2\nMA 1 2\nRW 1 2\nFigure 9.20: RW \u2192 MA forwarding\nClock cycles\n1 2 3 4 5 6 7 8 9\nIF 1 2 3\n[1]: ld r1, 4[r2]\nOF 1 2 3\n[2]: st r8, 10[r3] EX 1 2 3\nMA 1 2 3\n[3]: add r2, r1, r4\nRW 1 2 3\nFigure 9.21: RW \u2192 EX forwarding\nthe value of r1 in cycle 4. In this case also, it is possible to forward the data by adding an\ninterconnection (forwarding path) between the MA and EX stages.\nRW \u2192 OF : Typically the OF stage does not need forwarding paths because it does not\nhave any functional units. Hence, it does not need to use a value immediately. We can thus\nforward the value later according to tenet 2. However, the only exception is forwarding from\nthe RW stage. We cannot forward the value later because the instruction will not be there in\nthe pipeline. Hence, it is necessary to add a forwarding path from the RW to the OF stage.\nAn example of a code snippet that requires RW \u2192 OF forwarding is shown in Figure 9.23.\nInstruction [1] produces the value of r1 by reading its value from memory by the end of cycle 4.\nIt then writes the value of r1 to the register file in cycle 5. Meanwhile, instruction [4] tries to\nread the value of r1 in the OF stage in cycle 5. Unfortunately, there is a conflict here. Hence,\nwe propose to resolve the conflict by adding a forwarding path between the RW and OF stages.\nThus, we prohibit instruction [4] from reading the register file for the value of r1. Instead, 407 (cid:13)c Smruti R. Sarangi\nClock cycles\n1 2 3 4 5 6 7 8 9\nIF 1 2\n[1]: add r1, r2, r3\nOF 1 2\n[2]: sub r4, r1, r2 EX 1 2\nMA 1 2\nRW 1 2\nFigure 9.22: MA \u2192 EX forwarding\ninstruction [4] gets the value of r1 from instruction [1] using the RW \u2192 OF forwarding path.\nClock cycles\n1 2 3 4 5 6 7 8 9\nIF 1 2 3 4\n[1]: ld r1, 4[r2]\nOF 1 2 3 4\n[2]: st r4, 10[r3]\nEX 1 2 3 4\n[3]: st r5, 10[r6]\n[4]: sub r7, r1, r2 MA 1 2 3 4\nRW 1 2 3 4\nFigure 9.23: RW \u2192 OF forwarding\nImportant Point 14\nForwarding from the RW to the OF stage is a very tricky operation. This is because the\ninstruction in the RW stage is writing to the register file, and the instruction in the OF\nstage is also reading from the register file. If the value of the register is the same in these\ntwo instructions, then it is typically not possible to perform both the operations (read and\nwrite) in the same cycle. This is because reading and writing to the same SRAM cell can\nlead to incorrect operation of the circuit, and it is hard to ensure correctness. Consequently,\nit is a standard practice to allow the write from RW to go through, and cancel the register (cid:13)c Smruti R. Sarangi 408\nread operation issued by the instruction in the OF stage. Thus, this read operation does not\ngo to the register file. Instead, the instruction in the OF stage gets the value of the register\nthrough the forwarding path. This strategy ensures that we do not have any remote chances\nof leaving data in an inconsistent state in the register file. The instruction in the OF stage\nalso gets the right value of the operands.\nItisnotnecessarytoaddthefollowingforwardingpaths: MA\u2192OF,andEX\u2192OF.Thisis\nbecause, we can use the following forwarding paths (RW \u2192 EX), and (MA \u2192 EX) instead. In\naccordance with tenet 2, we need to avoid redundant forwarding paths. Hence, we do not add\nthe forwarding paths to the OF stage from the MA and EX stages. We do not add forwarding\npaths to the IF stage because at this stage, we have not decoded the instruction, and thus we\ndo not know about its operands.\n9.7.3 Data Hazards with Forwarding\nQuestion 7\nHas forwarding completely eliminated data hazards?\nLet us now answer this question. Let us consider ALU instructions. They produce their result\nin the EX stage, and they are ready to forward in the MA stage. Any succeeding consumer\ninstruction will need the value of an operand produced by the preceding ALU instruction at\nthe earliest in its EX stage. At this point, we can effect a successful forwarding because the\nvalue of the operand is already available in the MA stage. Any subsequent instruction can\nalways get the value using any of the available forwarding paths or from the register file if the\nproducer instructions has left the pipeline. The reader should be convinced that if the producer\ninstruction is an ALU instruction, then it is always possible to forward the result of the ALU\noperationtoaconsumerinstruction. Toprovethisfact, thereaderneedstoconsiderallpossible\ncombinations of instructions, and find out if it is possible to forward the input operands to the\nconsumer instruction.\nThe only other instruction that produces a register value explicitly is the load instruction.\nRecall that the store instruction does not write to any register. Let us look at the load instruc-\ntion. The load instruction produces its value at the end of the MA stage. It is thus ready to\nforward its value in the RW stage. Let us now consider a code snippet and its pipeline diagram\nin Figure 9.24.\nInstruction [1] is a load instruction that writes to register r1, and instruction [2] is an ALU\ninstruction that uses register r1 as a source operand. The load instruction is ready to forward\nat the beginning of cycle 5. Sadly, the ALU instruction needs the value of r1 at the beginning\nof cycle 4. We thus need to draw an arrow in the pipeline diagram that flows backwards in\ntime. Hence, we can conclude that in this case forwarding is not possible. 409 (cid:13)c Smruti R. Sarangi\nClock cycles\n1 2 3 4 5 6 7 8 9\nIF 1 2\n[1]: ld r1, 10[r2]\nOF 1 2\n[2]: sub r4, r1, r2\nEX 1 2\nMA 1 2\nRW 1 2\nFigure 9.24: The load-use hazard\nDefinition 76\nLoad-Use Hazard A load-use hazard is a situation where a load instruction supplies the\nloaded value to an immediately succeeding instruction that needs the value in its EX stage.\nA pipeline even with forwarding needs to insert a single stall cycle after the load instruction.\nThis is the only case in which we need to introduce a stall cycle in our pipeline. This\nsituation is known as a load-use hazard, where a load instruction supplies the loaded value\nto an immediately succeeding instruction that needs the value in its EX stage. The standard\nmethod of eliminating load-use hazards is by allowing the pipeline to insert a bubble, or by\nusing the compiler to either reorder instructions or insert a nop instruction.\nThus, we can conclude that a pipeline with forwarding does need interlocks, albeit rarely.\nThe only special condition is a load-use hazard.\nNote that if there is a store instruction after a load instruction that stores the loaded value,\nthen we do not need to insert a stall cycle. This is because the store instruction needs the value\nin its MA stage. At this point of time the load instruction is in the RW stage, and it is possible\nto forward the value.\n9.7.4 Implementation of a Pipeline with Forwarding\nNow, let us come to the most important part of our discussion. Let us design a pipeline with\nforwarding. We shall first design the data path, and then briefly look at the control path.\nTo implement a data path that supports forwarding, we need to make minor changes to our\npipeline stages. These changes will allow the functional units to use their default input values,\nas well as outputs of subsequent stages in the pipeline. The basic idea is to use a multiplexer\nbefore every input to a functional unit. The role of this multiplexer is to select the right input.\nLet us now look at each of the pipeline stages. Note that we do not need to make any changes (cid:13)c Smruti R. Sarangi 410\nto the IF stage because it does not send or receive any forwarded value.\nOF Stage with Forwarding\nIF-OF OF-EX\nControl\nunit\nImmediate\nand branch\nunit\nM2\nB\nM'\nop2\nRegister op2\nfile\nop1\nA\nM1\nfrom RW\nFigure 9.25: OF stage with forwarding\nThe OF stage with support for forwarding is shown in Figure 9.25. The multiplexers in our\nbaselinepipelinewithoutforwardingarecolouredwithalightercolour. Whereas, theadditional\nmultiplexers added to enable forwarding are coloured with a darker colour. We shall use this\nconventionfortherestofourdiscussiononforwarding. Letusfocusonthetwonewmultiplexers\nin the OF stage.\nWeonlyshowthosemultiplexersthatarerelevanttoourdiscussiononforwarding. Weneed\nto choose between the first operand read from the register file, and the value forwarded from\nthe RW stage. We thus add a multiplexer(M1) to help us choose between these two inputs.\nLikewise, we need to choose between the second operand read from the register file, and the\nvalue forwarded from the RW stage. To implement forwarding, we add a multiplexer (M2) to\nmakeachoicebetweenthevaluefetchedfromtheregisterfile, andthevalueforwardedfromthe\nRW stage (see Figure 9.25). Multiplexer (M(cid:48)), which is a part of our baseline design chooses\nbetween the second register operand and the immediate computed from the contents of the\ninstruction. Recall that the three fields in the instruction packet that save the results of the\nOF stage are as follows. A saves the value of the first register operand, op2 saves the value of\nthe second register operand (rd register in case of a store), and B saves the value of the second\noperand of the instruction (register or immediate). Recall that we had decided to read all the\nvalues that might possibly be required by any instruction in the interest of time. For example,\nthe not instruction does not require the first register operand. Nevertheless, we still read it\nbecause we do not have enough time to take a decision about whether to read or not read the\nregister operands. 411 (cid:13)c Smruti R. Sarangi\nEX Stage with Forwarding\nto IF\nOF-EX EX-MA\nBranch\nunit\nM3 flags\nA\nALU\nM4\nunit\nM5\nB\nop2\nfrom MA\nfrom RW\nFigure 9.26: EX stage with forwarding\nFigure 9.26 shows the modified EX stage. The three inputs that the EX stage gets from\nthe OF stage are A (first ALU operand), B (second ALU operand), and op2 (second register\noperand). For A and B, we add two multiplexers, M3, and M4, to choose between the values\ncomputed in the OF stage, and the values forwarded from the MA and RW stages respectively.\nFortheop2field,whichpossiblycontainsthestorevalue,wedonotneedMA \u2192EXforwarding.\nThis is because the store value is required in the MA stage, and thus we can use RW \u2192 MA\nforwarding. This observation allows us to reduce one forwarding path. Hence, multiplexer M5\nhas two inputs (default and the value forwarded from the RW stage).\nMA Stage with Forwarding\nFigure 9.27 shows the MA stage with additional support for forwarding. The memory address\nis computed in the EX stage, and saved in the aluResult field of the instruction packet. The\nmemory unit directly uses this value for the address. However, in the case of a store, the value\nthat needs to be stored (op2) can possibly be forwarded from the RW stage. We thus add\nmultiplexer M6, which chooses between the op2 field in the instruction packet and the value\nforwarded from the RW stage. The rest of the circuit remains the same.\nRW Stage with Forwarding\nFinally, Figure 9.28 shows the RW stage. Since this is the last stage, it does not use any\nforwarded value. However, it sends the value that it writes to the register file to the MA, EX, (cid:13)c Smruti R. Sarangi 412\nMA-RW MA-RW\nEX-MA\naluResult\nMemory\nop2 unit\nRegister\nwrite unit\nM6\nData\nmemory\nto EX\nfrom RW to OF, EX, and MA\nto EX and OF\nFigure 9.28: RW stage with for-\nFigure 9.27: MA stage with forwarding warding\nand OF stages, respectively.\nPutting it All Together\nMA-RW\nIF-OF OF-EX EX-MA\nControl\nunit Branch Memory\nunit unit\nRegister\nF ue ntc ith aIm ndm be rd ai na cte h flags write unit\nunit\nData\nmemory\nop2 ALU\nInstruction Register unit\nmemory file op1\nop2\nFigure 9.29: Pipelined data path with forwarding (abridged diagram)\nFigure 9.29 puts all the pieces together and shows the pipeline with support for forwarding.\nTo summarise, we need to add 6 multiplexers, and make some extra interconnections between\nunits to pass the forwarded values. We envision a dedicated forwarding unit that computes\nthe control signals for the multiplexers (not shown in the diagram). Other than these small\nchanges, no other major change needs to be done to the data path.\nWe have been using an abridged diagram (similar to Figure 9.29) in our discussions on\nforwarding. Thereaderneedstonotethattheactualcircuithasbecomefairlycomplicatednow. 413 (cid:13)c Smruti R. Sarangi\nAlong with the augmentations to the data path, we need to also add a dedicated forwarding\nunit to generate the control signals for the multiplexers. A detailed picture of the pipeline is\nshown in Figure 9.30.\nisBranchTaken\nbranchPC\n1\npc + 4 0\npc\nInstruction instruction\nmemory\npc instruction\nrd rs2 ra rs1\n1 0 1 0 ii ss RS et t Control\nreg unit\nImmediate and Register data\nbranch target file\nimmx op2 op1\nisWb\n1 0 1 0\nt 1 0 isImmediate\ni\nn\nu pc branchTarget B A op2 instruction control\ng\nn A\ni\nd\nr 0 1 aluSignals\na isRet ALU gs Branch isBeq\nw fla unit isi UsB Bg rat\nnch\nr isBranchTaken\no\nF pc aluResult op2 instruction control\nmar mdr isLd\nData\nMemory\nmemory\nunit isSt\npc ldResult aluResult instruction control\n4 isLd\n10 01 00 isCall isWb\nrd\n0\ndata ra(15) 1\nFigure 9.30: Pipelined data path with forwarding\nLet us now add the interlock logic to our pipeline. We need the interlock logic for both\nthe Data-Lock and Branch-Lock conditions. Note that now we have successfully handled all (cid:13)c Smruti R. Sarangi 414\nisBranchTaken\nbubble\nBranch-lock Unit\nstall\nData-lock Unit\nControl\nunit Branch Memory\nunit unit\nRegister\nF ue ntc ith aIm ndm be rd ai na cte h flags write unit unit\nData\nmemory\nop2 Execute\nInstruction Register unit\nmemory file op1\nop2\nForwarding unit\nFigure 9.31: Pipelined processor with forwarding\nRAW hazards other than the load-use hazard. In the case of a load-use hazard, we need to\nstall for only 1 cycle. This significantly simplifies our Data-Lock circuit. If there is a load\ninstruction in the EX stage, then we need to check if there is a RAW data dependence between\nthe load instruction, and the instruction in the OF stage. The only RAW hazard that we do\nnot need to consider here is a load-store dependence, where the load writes to a register that\ncontains the store value. We do not need need to stall because we can forward the value to\nbe stored from the RW to the MA stage. For all other data dependences, we need to stall\nthe pipeline by 1 cycle by introducing a bubble. This will take care of the load-use hazard.\nThe circuit for ensuring the Branch-Lock condition remains the same. Here also, we need to\ninspect the instruction in the EX stage, and if it is a taken branch, we need to invalidate the\ninstructions in the IF and OF stages. Lastly, the reader should note that interlocks always take\nprecedence over forwarding.\n9.7.5 Forwarding Conditions\nAfter designing the data path for supporting forwarding, let us design the control path. The\nonly extra addition to the control path is the forwarding unit. This unit computes the values\nof the signals to control the forwarding multiplexers. Let us now discuss the design of the\nforwarding unit.\nFO-FI XE-FO AM-XE WR-AM 415 (cid:13)c Smruti R. Sarangi\nThe Forwarding Unit\nAs shown in Figure 9.31 the forwarding unit receives inputs from all the four pipeline registers.\nThey provide the contents of the instructions resident in the OF, EX, MA, and RW stages\nrespectively. Based on the contents of the instructions, the forwarding unit computes the\nvalues of the control signals.\nSalient Points\nLet us now consider the four forwarding paths in our architecture \u2013 RW \u2192 OF, RW \u2192 EX,\nMA \u2192 EX, and RW \u2192 MA. We note that the distance between the producer and consumer\nstages for these four paths are 3, 2, 1, and 1 respectively. Alternatively, we can say that\ninstruction number i, can get its inputs from instructions i\u22121, i\u22122, and i\u22123. The reader\nneeds to note that there are two forwarding paths between adjacent stages (distance equal to\n1).\nForwarding Paths with Distance Equal to 1\nThese forwarding paths are MA \u2192 EX, and RW \u2192 MA. We actually need both these\nforwarding paths. The reason is as follows. The MA \u2192 EX path is required for forwarding\nresults between consecutive ALU instructions. The RW \u2192 MA path is required when the\nvalue of the input is generated in the MA stage, and it is also required in the MA stage. The\nonly instruction that generates a value in the MA stage is the load instruction, and the only\ninstruction that requires register operands in the MA stage, is the store instruction. Thus, we\nneed to use the RW \u2192 MA forwarding path between a load instruction, and an immediately\nsucceeding store instruction, when there is a register dependence. The following code snippet\ngives an example.\nld r1, 10[r2]\nst r1, 20[r4]\nNote that sometimes we might have a choice of forwarding paths (MA \u2192 EX, or RW \u2192\nMA). The following code snippet shows an example.\n[1]: add r1, r2, r3\n[2]: st r1, 20[r4]\nHere, instruction [1] is ready to forward the value of r1 when it reaches the MA stage.\nHowever, instruction [2] requires the value of r1 when instruction [1] reaches the RW stage.\nWe can thus use either forwarding path (MA \u2192 EX, or RW \u2192 MA). Let us choose to use\nRW \u2192 MA forwarding in this case (also see Section 9.7.4). This optimisation allows us to\nreduce a forwarding path between MA to EX for op2. This is also in accordance with tenet 2\nmentioned in Section 9.7.2 that says that we should forward as late as possible.\nCase of the mov Instruction\nThe other special case arises for the mov instruction. Since the EX stage does not produce its\noutput value, we can theoretically use RW \u2192 MA forwarding for it. Ideally, if the consumer\ninstruction in a load-use hazard, is a mov instruction, we should not have the necessity to stall\nthe pipeline. However, for the purpose of simplicity, let us choose to treat a mov instruction as\na regular ALU instruction, and choose to disregard any optimisations in this case. (cid:13)c Smruti R. Sarangi 416\nConflicts with Multiple Instructions\nLet us look at our four forwarding paths: RW \u2192 OF, RW \u2192 EX, MA \u2192 EX, and RW \u2192\nMA, again. We notice that the EX stage gets forwarded inputs from two stages \u2013 MA and RW.\nIt is possible that the instruction in the EX stage has a conflict (RAW register dependence)\nwith the instructions in both the MA and RW stages for the same input. In this case, we need\nto choose the input from the MA stage because it is an earlier instruction. Let us show an\nexample.\n[1]:add r1, r2, r3\n[2]:sub r1, r4, r5\n[3]:mul r8, r9, r1\nIn this case, when instruction [3] is in the EX stage, instruction [2] is in the MA stage, and\ninstruction [1] is in the RW stage. The second source operand (value of register r1) needs to be\nforwarded. We need to get the value from the MA stage because instruction [2] will overwrite\nthe value written by instruction [1]. We can design a simple circuit to give a higher priority to\nthe MA stage than the RW stage while forwarding results to the EX stage. We leave this as an\nexercise for the interested reader.\nAlgorithms for Forwarding Conditions\nWe show the pseudo codes for the forwarding conditions. We need to first detect if a conflict\nexists for the first operand, which is typically the rs1 field of the instruction packet. In the case\nof a ret instruction, the first operand is the ra (return address) register. If a conflict exists,\nthen we can potentially forward a value. For reasons of brevity, we do not show the code that\ndisregards the case of forwarding if one of the instructions is a pipeline bubble.\nAlgorithm 6 shows the algorithm for detecting a conflict on the first operand. We first rule\nout the trivial cases in which instruction [A] does not read from any register, and [B] does\nnot write to any register. Then, we set the first operand. It is equal to the rs1 field in the\ninstruction packet. The only exception is the ret instruction whose first operand is the ra\nregister. Similarly, the destination operand is always register rd, with the call instruction being\nthe only exception. Its destination operand is the return address register, ra. Then we detect\na conflict in Line 15, and we return true if a conflict (RAW dependence) exists, otherwise\nwe return false. We can use the output of Algorithm 6 to set the input of the forwarding\nmultiplexers for the first operand.\nAlgorithm 7 shows the pseudo code of the algorithm for detecting conflicts for the second\noperand. We first rule out the trivial cases, in which [A] does not read any register and [B] does\nnot write to any register. Then, we need to see if the second operand of [A] is an immediate.\nIn this case, forwarding is not required. The second operand is typically equal to the rs2 field\nof the instruction packet. However, in the case of a store instruction, it is equal to the rd field\nof the instruction packet. Similarly, we find the destination register of instruction [B], and take\ncare of the special case of the call instruction. We finally detect a conflict in Line 20. Note\nthat we do not consider the load-use hazard, or Branch-Lock conditions in the forwarding\nlogic, because we always assume that interlocks have higher priority over forwarding. Secondly,\nwhenever we do not have a forwarding path, the forwarding conditions do not apply. Finally, 417 (cid:13)c Smruti R. Sarangi\nAlgorithm 6: Conflict on the first operand (rs1\/ra)\nData: Instructions: [A] and [B] (possible forwarding: [B] \u2192 [A])\nResult: Conflict exists on rs1\/ra (true), no conflict (false)\n1 if [A].opcode \u2208 (nop,b,beq,bgt,call,not,mov) then\n\/* Does not read from the rs1 register *\/\n2 return false\n3 end\n4 if [B].opcode \u2208 (nop, cmp, st, b, beq, bgt, ret) then\n\/* Does not write to any register *\/\n5 return false\n6 end\n\/* Set the sources *\/\n7 src1 \u2190 [A].rs1\n8 if [A].opcode = ret then\n9 src1 \u2190 ra\n10 end\n\/* Set the destination *\/\n11 dest \u2190 [B].rd\n12 if [B].opcode = call then\n13 dest \u2190 ra\n14 end\n\/* Detect conflicts *\/\n15 if src1 = dest then\n16 return true\n17 end\n18 return false (cid:13)c Smruti R. Sarangi 418\nAlgorithm 7: Conflict on the second operand (rs2\/rd)\nData: Instructions: [A] and [B] (possible forwarding: [B] \u2192 [A])\nResult: Conflict exists on second operand (rs2\/rd) (true), no conflict (false)\n1 if [A].opcode \u2208 (nop,b,beq,bgt,call) then\n\/* Does not read from any register *\/\n2 return false\n3 end\n4 if [B].opcode \u2208 (nop, cmp, st, b, beq, bgt, ret) then\n\/* Does not write to any register *\/\n5 return false\n6 end\n\/* Check the second operand to see if it is a register *\/\n7 if [A].opcode \u2208\/ ( st) then\n8 if [A].I = 1 then\n9 return false\n10 end\n11 end\n\/* Set the sources *\/\n12 src2 \u2190 [A].rs2\n13 if [A].opcode = st then\n14 src2 \u2190 [A].rd\n15 end\n\/* Set the destination *\/\n16 dest \u2190 [B].rd\n17 if [B].opcode = call then\n18 dest \u2190 ra\n19 end\n\/* Detect conflicts *\/\n20 if src2 = dest then\n21 return true\n22 end\n23 return false 419 (cid:13)c Smruti R. Sarangi\nin the case of multiple conflicting instructions, the forwarding unit needs to ensure that the\ncorrect value is forwarded.\nSpecial Case of Forwarding from the Call Instruction\nLet us consider the following code snippet.\ncall .function\n..\n...\n.function:\nret\nHere, we call a function and immediately return. In this case, the call instruction will still be\nin the pipeline, when the ret instruction enters the pipeline. Recall that the call instruction\nwritestoregisterraandtheretinstructionreadsfromregisterra. Moreover,thecallinstruction\ncomputes the value of ra, and writes it to the register file in the RW stage. We shall prove that\nthis does not cause any correctness issues.\nA call instruction is a taken branch. This means that when it enters the EX stage, the\nBranch-Lock circuitry will detect that it is a taken branch, and convert the instructions in the\nIF and OF stages to bubbles. Any instruction that requires the value of the ra register will\nat least be three stages behind the call instruction. This means that when the call instruction\nwill reach the RW stage, the next valid instruction in the pipeline will be in the OF stage. If\nthis is a ret instruction, or any other instruction that needs the value of the ra register, then\nit can simply get its value through the RW \u2192 OF forwarding path. Hence, the special case of\nforwarding from the call instruction is handled correctly.\n9.8 Support for Interrupts\/ Exceptions*\nThe process of building our pipelined processor is almost done. We just need to put the final\npiece together. We have up till now focused on building a fast pipeline that has interlocks for\ncorrectnessandhasforwardingforenhancingperformance. Weshallnowdiscusstheinteraction\nof our processor with external devices such as I\/O devices and with specialised programs such\nastheoperatingsystem. Theoperatingsystemisamasterprogramthatcontrolsthebehaviour\nof other programs, the processor, and I\/O devices. The standard mechanism for supporting\nthe operating system, and other I\/O devices, is through a mechanism called an interrupt. We\nshall have ample opportunities to discuss interrupts in Chapter 12. In this section, we discuss\nthe implementation of an interrupt from the point of view of a pipeline.\n9.8.1 Interrupts\nThe main idea of an interrupt is as follows. Assume that we click a key on a keyboard. The\nkeyboard records the ASCII code of the clicked key, and then sends a message to the processor\nwith the code of the key that was clicked. This message is known as an interrupt. After the\nprocessor receives an interrupt, it stops the execution of the currently executing program and\njumps to a dedicated program known as the interrupt handler. The interrupt handler reads (cid:13)c Smruti R. Sarangi 420\nthe value sent by the I\/O device (in this case, the keyboard), and sends it to the program that\nhandles the display device (monitor\/ laptop screen). This program shows the character typed.\nFor example, if the user clicks the character, \u2019a\u2019, then monitor ultimately shows an \u2019a\u2019 on the\nscreen through a sequence of steps.\nDefinition 77\nAn interrupt is a signal sent by an I\/O device to the processor. An interrupt is typically\nused to draw the attention of the processor to new inputs, or changes in the status of an\nI\/O device. For example, if we click a key on a keyboard, then a new interrupt is generated\nand sent to the processor. Upon receiving an interrupt, the processor stops the execution of\nthe currently executing program, and jumps to an interrupt handler routine. This routine\nprocesses the interrupt, by reading the values sent by the I\/O device, and performing any\nother action if required.\n9.8.2 Exceptions\nInterrupts are not the only kind of events sent to the processor. Sometimes some actions of\nthe program can generate interrupts. For example, if a program accesses an illegal memory\naddress, then it is necessary to take corrective action. The memory system typically sends\nan interrupt to the processor. The processor in turn invokes the interrupt handler routine,\nwhich in turn calls dedicated modules in the operating system. Note that in this book, we shall\nuse the terms interrupt handler and exception handler interchangeably. These modules either\ntake some kind of corrective action, or terminate the program. Such kind of interrupts that\nare generated as a result of actions of the executing program are called exceptions. Readers\nfamiliar with languages such as Java can relate to the concept of exceptions. For example, in\nJava if we access an illegal array index such as -1, an exception is generated, and the processor\njumps to a pre-specified location to take corrective action.\nDefinition 78\nAn exception is a special event that is generated when the executing program typically per-\nforms an erroneous action, and it becomes necessary to take corrective action.\n9.8.3 Precise Exceptions\nLet us now discuss how we need to handle interrupts and exceptions. The processor needs\nto clearly stop what it is currently doing, and jump to the interrupt handling routine. After\nhandling the interrupt, and performing the desired action, it needs to come back and start from\nexactly the same point in the program, at which it had stopped. Let us now define the notion\nof a precise exception. The term \u201cprecise exception\u201d is also used in the case of interrupts. We\ncan think of it as a generic term for all kinds of interrupts and exceptions. 421 (cid:13)c Smruti R. Sarangi\nDefinition of Precise Exceptions\nAt any point of time, a program will typically have multiple instructions in the pipeline with\ndifferent PCs. When the processor encounters an interrupt, it needs to branch to the starting\nlocation of the interrupt handler. To facilitate this process, it can have an interrupt handler\ntable. Thistabletypicallystoresalistofinterrupttypes,andthestartingPCsoftheirinterrupt\nhandlers. The processor uses this table to branch to the appropriate interrupt handler. After\nfinishing the processing of the interrupt handler, it needs to come back to exactly the same\npoint in the original program. In other words, the original program should not be aware of the\nfact that another program such as the interrupt handler executed in the middle. This entire\nprocess needs to be orchestrated very carefully.\nLet us elaborate. Assume that a program, P, is executing on the processor. Let us record\nallitsdynamicinstructionsthatleavethepipelineaftersuccessfullycompletingtheirexecution,\nand number them I ,I ,...I . A dynamic instruction is the instance of an instruction created\n1 2 n\nby the processor. For example, if a loop has 5 instructions, and executes 100 times, then we\nhave 500 dynamic instructions. Furthermore, an instruction completes its execution when it\nfinishesitsjobandupdatesthestateoftheprocessor(registersormemory). Astoreinstruction\ncompletes in the MA stage, and instructions with a destination register complete in the RW\nstage. All other instructions, are assumed to complete in the MA stage. The nop instruction is\nexcluded from this discussion. Let I be the last instruction in P that completes its execution\nk\nbefore the first instruction in the interrupt handler completes its execution. We wish to ensure\nthat at the time that I leaves the pipeline, all the instructions in P before I have completed\nk k\ntheir execution and left the pipeline, and no instruction in P after I has completed or will\nk\ncomplete its execution before the program resumes. Let the set of completed instructions at\nthis point of time (when I leaves the pipeline) be C. Formally, we have:\nk\nI \u2208 C \u21d4 (j \u2264 k) (9.1)\nj\nAn interrupt or exception implemented in this manner is said to be precise.\nDefinition 79\nAn interrupt or exception is precise if the following conditions are met:\nCondition 1: Let I be the last dynamic instruction in the original program, P, that com-\nk\npletes its execution before the first instruction in the interrupt handler completes its\nexecution. Let I leave the pipeline at time, \u03c4. At \u03c4, all instructions I (j < k) have\nk j\nalso completed their execution.\nCondition 2: No instruction after I in P completes its execution before all the instruc-\nk\ntions in the interrupt handler complete, and the program resumes execution.\nCondition 3: After the interrupt handler finishes, we can seamlessly start executing all\nthe instructions starting from I (if it has not completed successfully) or I .\nk k+1\nWhen the interrupt handler returns, it needs to start executing instruction, I . For some\nk+1 (cid:13)c Smruti R. Sarangi 422\nspecial types of interrupts\/ exceptions it might be required to re-execute I . Secondly, the\nk\nregister state (values of all the registers) needs to be restored before the original program,\nP, starts executing again. We can thus ensure that a processor can seamlessly switch to an\ninterrupt handler and back without violating the correctness of the program.\nMarking Instructions\nLet us now discuss how to implement precise exceptions. Let us look at the three conditions in\nDefinition 79 in more detail.\nWhen an interrupt arrives, we can at the most have 5 instructions in the pipeline. We can\ndesignate one of these instructions as the last instruction before the interrupt handler executes\nsuch that the three conditions outlined in Definition 79 are satisfied. Now, we cannot designate\nthe instruction in the RW stage as the last instruction (I ) because the instruction in the MA\nk\nstage might be a store instruction. In the current cycle it will complete its execution, and thus\ncondition 2 will get violated. However, we are free to designate instructions in any of the four\nother stages as the last instruction. Let us decide to mark the instruction in the MA stage as\nthe last instruction.\nNow, let us look at exceptions. Exceptions are typically caused by the erroneous execution\nof instructions. For example, in the IF stage we might fetch from an illegal address, try to\nperform an illegal arithmetic operation in the EX stage, or write to a non-existent address\nin the MA stage. In these situations it is necessary to take corrective action. The processor\nneeds to invoke a dedicated exception handler. For example, a very common type of exception\nis a page fault as we shall discuss in Chapter 10. A page fault occurs when we try to read\nor write a memory address in a 4 KB block of memory for the first time. In this case, the\noperating system needs to read the 4 KB block from the hard disk and copy it to memory. The\nfaulting instruction executes again, and it succeeds the second time. In this case, we need to\nre-execute the exception causing instruction I , and needless to say we need to implement a\nk\nprecise exception. To properly take core of exceptions, the first step is to mark an instruction,\nimmediately after it causes an exception. For example, if we try to fetch from an uninitialised\nor illegal address we mark the instruction in the IF stage.\nMaking a Marked Instruction Proceed to the End of the Pipeline\nNow, that we have marked instructions, we need to ensure two conditions. The first is that\nall the instructions before the marked instruction need to complete. The second is that all the\ninstructions after the marked instruction should not be allowed to write to the register file, or\nthe main memory. We should ideally not allow any writes to the flags register also. However,\nit is difficult to implement this functionality, because we are typically aware of interrupts at the\nend of the clock cycle. We shall devise an ingenious solution to handle updates to the flags\nregister later.\nFor implementing a precise exception, we need to add an exception unit to our pipeline. Its\nrole is to process interrupts and exceptions. Once an instruction is marked, it needs to let the\nexception unit know. Secondly, we envision a small circuit that sends a code identifying the\nexception\/ interrupt to the exception unit. Subsequently, the exception unit needs to wait for\nthe marked instruction to reach the end of the pipeline such that all the instructions before it\ncompletetheirexecution. Instructionsfetchedafterthemarkedinstructionneedtobeconverted 423 (cid:13)c Smruti R. Sarangi\ninto bubbles. This needs to be done to ensure that instructions after a marked instruction do\nnot complete. Once, the marked instruction reaches the end of the pipeline, the exception unit\ncan load the PC with the starting address of the interrupt handler. The interrupt or exception\nhandler can then begin execution. This mechanism ensures that asynchronous events such as\ninterrupts and exceptions remain precise. Now, we have a mechanism to seamlessly transition\nto executing interrupt handlers. Sadly, we still do not have a mechanism to come back to\nexactly the same point in the original program, because we have not remembered the point at\nwhich we had left.\n9.8.4 Saving and Restoring Program State\nLet us define the term program state as the state of all the registers and memory elements\nassociated with the program. In specific, the program state, comprises of the contents of the\nregister file, PC, flags register, and main memory.\nDefinition 80\nThe term program state is defined as the state of all the registers and memory elements\nassociated with the program. In specific, the program state, comprises of the contents of the\nregister file, PC, flags register, and main memory.\nWe need to find effective means of saving and restoring the state of the executing program.\nLet us start by stating that we do not need a method to save and restore the state of main\nmemory because the assumption is that the interrupt handler uses a different region of main\nmemory. Weshalldiscussmethodstoenforceaseparationofmemoryregionsbetweenprograms\nin Chapter 10. Nonetheless, the bottom line is that there is no unintended overlap of the\nmemory regions of the executing program and the interrupt handler. In the case of exceptions,\nthe interrupt handler might access some parts of the memory space of the program such that it\ncan add some data that the program requires. One such example of exceptions is a page fault.\nWe will have ample opportunities to discuss page faults in Chapter 10.\nHence, we need to explicitly take care of the PC, the flags register, and the set of registers.\nThe state of all of these entities is known as the context of a program. Hence, our problem is\nto successfully save and retrieve the context of a program upon an interrupt.\nDefinition 81\nThe context of a program refers to the values of the PC, the flags register, and the values\ncontained in all the registers.\nThe oldPC Register\nLet us add an NPC field for the next PC in the instruction packet. By default, it is equal to\nPC +4. However, for branch instructions that are taken, the NPC field contains the branch (cid:13)c Smruti R. Sarangi 424\ntarget. We envision a small circuit in the EX stage that adds the branch target, or PC +4\nto the NPC field of the instruction packet. Recall that the instruction packet gets passed\nfrom one stage to the next in a pipeline. Once a marked instruction reaches the RW stage,\nthe exception unit looks up a small internal table indexed by the interrupt\/ exception code.\nFor some types of interrupts such as I\/O events, we need to return to the next PC (PC +4\nor the branch target). This value is stored in the NPC field of the MA-RW pipeline register.\nHowever, for some types of exceptions such as page faults, it is necessary to re-execute the\nfaulting instruction once again. A page fault happens because a certain memory location is not\nloaded with its data. The interrupt handler (for a page fault) needs to load the data of the\nmemory location by fetching values from the hard disk, and then re-execute the instruction. In\nthis case, we need to return to the PC of the marked instruction. In either case, the exception\nunit transfers the correct return address to an internal oldPC register, and then starts fetching\ninstructions for the interrupt handler.\nSpilling General Purpose Registers\nWe need a mechanism to save and restore registers akin to spilling and restoring registers as\nin the case of function calls. However, there is an important difference in the case of interrupt\nhandlers. Interrupt handlers have their own stacks that are resident in their private memory\nregions. Tousethestackpointerofaninterrupthandler, weneedtoloaditsvalueintosp. This\nstep will overwrite the previous value, which is the value of the stack pointer of the program.\nHence, to avoid losing the value of the stack pointer of the program, we add another register\ncalled oldSP. The interrupt handler first transfers the contents of sp to oldSP. Subsequently,\nit loads sp with the value of its stack pointer and then spills all the registers excluding sp to its\nstack. At the end of this sequence of steps, it transfers the contents of oldSP to the stack.\nThe oldFlags Register\nThe only part of the program state that we have not saved up till now is the flags register.\nLet us assume that the flags register is a 32-bit register. Its lower 2 bits contain the values,\nflags.E andflags.GT respectively. Moreover,letusaddaflagsfieldtotheinstructionpacket.\nInstructionsotherthanthecmpinstructionwritethecontentsoftheflagsregistertotheflags\nfield in the instruction packet, in the EX stage. The cmp instruction writes the updated value\nof the flags register to the flags field in the EX stage and moves to the subsequent stages.\nWhen a marked instruction reaches the RW stage, the exception unit extracts the contents of\nthe flags field in the instruction packet, and saves it in the oldFlags register. The oldFlags\nregister is a special register that is visible to the ISA, and helps store the last value of the flags\nregister that a valid instruction in the program had seen.\nSaving and Restoring Program State\nFor saving the program state, the interrupt handler contains assembly routines to save the\ngeneral purpose registers (excluding sp) and the oldSP, oldFlags, and oldPC registers. We\nsave all of these values in the stack of the interrupt handler. Likewise, we can restore program\nstate in almost the reverse order. We restore the value of oldPC, the flags register, the general 425 (cid:13)c Smruti R. Sarangi\npurpose registers, and the stack pointer. As the last step, we need to transfer the contents of\noldPC to PC such that we can resume executing the original program.\nPrivileged Instructions\nWe have added the following special registers namely oldPC, oldSP, oldFlags and flags. Note\nthat we had the flags register before also. However, it was not accessible as a register. Next,\nwe add a special category of instructions called privileged instructions that are only accessible\nto specialised programs such as operating systems, and interrupt handlers. The first privileged\ninstruction that we introduce is movz. It transfers values between regular registers and the\nspecial registers (oldPC, oldSP, oldFlags, and flags).\nThe other privileged instruction that we introduce in this section is retz. It reads the value\nof oldPC, and transfers its contents to PC. In other words, we jump to the location contained\ninoldPC. Wedonotallowinstructionstodirectlytransferthevaluesofspecialregisterstoand\nfrom memory, because we have to create privileged versions of both load and store instructions.\nWe wish to avoid creating two additional instructions.\nDefinition 82\nA privileged instruction is a special instruction that has access to the internals of the pro-\ncessor. It is typically meant to be used only by operating system programs such as the kernel\n(core of the operating system), device drivers (programs to interact with I\/O devices), and\ninterrupt handlers.\nTo implement the movz instruction, we add a new instruction opcode. Recall that we\nintroduced only 21 instructions in the SimpleRisc instruction set. We can afford to have 11\nmore instructions in the ISA. movz uses the same register format based encoding as the mov\ninstruction. However, it sees a different view of registers. The registers visible to privileged\ninstructions, and their identifiers are shown in Table 9.2.\nRegister Encoding\nr0 0000\noldPC 0001\noldSP 0010\nflags 0011\noldFlags 0100\nsp 1110\nTable 9.2: View of registers for privileged instructions\nPrivileged instructions use a different register encoding. They can only see the four special\nregisters, r0, and sp. We need to make a small modification to the OF and RW stages to\nimplement the movz instruction. The first is that we need to have a circuit in the OF stage (cid:13)c Smruti R. Sarangi 426\nto quickly find out if the opcode of an instruction is movz. We can use a fast circuit similar\nto the one that we use to find out if an instruction is a store. Then, we can choose the right\nset of register inputs from either the normal register file, or from one of the privileged registers\nusing multiplexers. Similarly, in the RW stage, we can choose to either write the value in\nthe normal register file, or in one of the special registers, again, with the help of additional\nmultiplexers. For the sake of brevity, we do not show the circuit. We leave implementing movz\nas an exercise for the reader. We can implement retz in a similar way as the ret instruction.\nThe only difference is that instead of getting the return value from the ra register, we get it\nfrom the oldPC register. Note that we will also require forwarding and interlock logic that\ntakes special registers into account. The pseudocode of the forwarding and interlock logic needs\nto be updated.\nLet us summarise the discussion in terms of two new concepts that we have learnt. The\nfirst is the notion of privileged instructions. These instructions are typically used by interrupt\nhandlers, and other modules of the operating systems. They have more visibility into the\ninternalsoftheprocessor. Sincetheyareverypowerful,itisnotagoodideatogiveprogrammers\nthe ability to invoke them. They might corrupt system state, and introduce viruses. Hence,\nmost systems typically disallow the usage of privileged instructions by normal programs. Most\nprocessorshavearegisterthatcontainsthecurrentprivilegelevel(CPL).Itistypically1foruser\nprograms, and 0 for operating system programs such as interrupt handlers. There is a privilege\nlevel change, when we switch to processing an interrupt handler (1 to 0), and when we execute\nthe retz instruction to return to a user program (0 to 1). Whenever, we execute a privileged\ninstruction, the processor checks the CPL register, and if the program is not allowed to execute\nthe instruction, then an exception is flagged. The operating system typically terminates the\nprogram, since it may be a virus.\nDefinition 83\nMost processors have a register that contains the currentprivilegelevel (CPL). It is typically\n1 for user programs, and 0 for operating system programs such as interrupt handlers. We\nare allowed to execute privileged instructions, only when the CPL is equal to 0.\nThe second important concept is the notion of different register views for different instruc-\ntions, ordifferentpiecesofcode. Thisconceptisknownasaregister window, andwaspioneered\nby the Sun Ultrasparc processors. The Sun processors used different register windows for dif-\nferent functions. This allowed the compiler to avoid costly register spills. Here, we use register\nwindowstoseparatethesetofregistersthatcanbeaccessedbyuserprogramsandtheinterrupt\nhandlers. The interrupt handlers can see all the special registers and two regular registers (r0\nand sp).\nDefinition 84\nA register window is defined as the set of registers that a particular instruction or function\ncan access. For example, in our case, privileged instructions can access only six registers, 427 (cid:13)c Smruti R. Sarangi\nout of which four are special registers. In comparison regular instructions have a register\nwindow that contains all the 16 general purpose registers, but no special register.\n9.8.5 SimpleRisc Assembly Code of an Interrupt Handler\nLet us now quickly conclude our discussion by showing the assembly code of an interrupt\nhandler. The code for saving the context is shown in Figure 9.32, and the code for restoring\nthe context and returning to the user program is shown in Figure 9.33. We assume that the\nstack pointer for the interrupt handler starts at : 0x FF FF FF FC.\n9.8.6 Processor with Support for Exceptions\nFigure 9.34 shows an abridged diagram of the data path with support for exceptions. We\nhave added an exception unit that takes inputs from all the pipeline registers. Whenever, an\ninstruction detects an exception, or an interrupt is detected, the exception unit is notified. The\nexception unit proceeds to mark an instruction as the last instruction. It waits till the marked\ninstruction leaves the pipeline, and concurrently converts all the instructions fetched after the\nmarked instruction to bubbles. Finally, when the marked instruction reaches the RW stage,\nthe exception unit stores the PC, or NPC (next PC) value in the oldPC register. It also saves\nthe flags field in the instruction packet to the oldFlags register. We add four registers namely\noldPC, oldSP, oldFlags, and flags. The ALU immediately updates the flags register if it\nprocesses a cmp instruction. The RW stage can also write to the flags register. These four\nregisters are bundled with the regular register file. We call the new structure as the register\nunit (shown in Figure 9.34). We do not show the multiplexers to choose between the inputs\nfrom the register file, and the special registers. We assume that the multiplexers are embedded\ninside the register unit.\n9.9 Performance Metrics\n9.9.1 The Performance Equation\nLet us now discuss the performance of our pipelined processor. We need to first define the\nmeaning of \u201cperformance\u201d in the context of processors. Most of the time, when we lookup the\nspecifications of a laptop or smart phone, we are inundated with a lot of terms such as the\nclock frequency, RAM, and hard disk size. Sadly, none of these terms are directly indicative of\nthe performance of a processor. The reason that the performance is never explicitly mentioned\non the label of a computer, is because the term \u201cperformance\u201d is rather vague. The term\nperformance of a processor is always with respect to a given program or set of programs. This\nis because processors perform differently with respect to different programs.\nGiven a program, P, let us try to quantify the performance of a given processor. We say\nthat processor A performs better than processor B, if it takes less time for P to execute P on A\nthan on B. Thus, quantifying performance with respect to a given program is very simple. We\nmeasure the time it takes to run the program, and then compute its reciprocal. This number (cid:13)c Smruti R. Sarangi 428\nSaving the context\n\/* save the stack pointer *\/\nmovz oldSP, sp\nmov sp, 0x FF FC\n\/* spill all the registers other than sp*\/\nst r0, -4[sp]\nst r1, -8[sp]\nst r2, -12[sp]\nst r3, -16[sp]\nst r4, -20[sp]\nst r5, -24[sp]\nst r6, -28[sp]\nst r7, -32[sp]\nst r8, -36[sp]\nst r9, -40[sp]\nst r10, -44[sp]\nst r11, -48[sp]\nst r12, -52[sp]\nst r13, -56[sp]\nst r15, -60[sp]\n\/* save the stack pointer *\/\nmovz r0, oldSP\nst r0, -64[sp]\n\/* save the flags register *\/\nmovz r0, oldFlags\nst r0, -68[sp]\n\/* save the oldPC *\/\nmovz r0, oldPC\nst r0, -72[sp]\n\/* update the stack pointer *\/\nsub sp, sp, 72\n\/* code of the interrupt handler *\/\n....\n....\n....\nFigure 9.32: SimpleRisc assembly code for saving the context 429 (cid:13)c Smruti R. Sarangi\nRestoring the context\n\/* update the stack pointer *\/\nadd sp, sp, 72\n\/* restore the oldPC register *\/\nld r0, -72[sp]\nmovz oldPC, r0\n\/* restore the flags register *\/\nld r0, -68[sp]\nmovz flags, r0\n\/* restore all the registers other than sp*\/\nld r0, -4[sp]\nld r1, -8[sp]\nld r2, -12[sp]\nld r3, -16[sp]\nld r4, -20[sp]\nld r5, -24[sp]\nld r6, -28[sp]\nld r7, -32[sp]\nld r8, -36[sp]\nld r9, -40[sp]\nld r10, -44[sp]\nld r11, -48[sp]\nld r12, -52[sp]\nld r13, -56[sp]\nld r15, -60[sp]\n\/* restore the stack pointer *\/\nld sp, -64[sp]\n\/* return to the program *\/\nretz\nFigure 9.33: SimpleRisc assembly code for restoring the context\ncan be interpreted to be proportional to the performance of the processor with respect to the\nprogram.\nLet us first compute the time(\u03c4) it takes to run program P. (cid:13)c Smruti R. Sarangi 430\nPC of exception handler\nisBranchTaken\nbubble\nCPL\nBranch-lock Unit\nException unit\nstall\nData-lock Unit\nControl\nunit Branch Memory\nunit unit\nRegister\nF ue ntc ith aIm ndm be rd ai na cte h flags write unit unit\nData\nRegister unit memory\nALU\nInstruction Register op2 unit\nmemory file op1 op2\noldFlags\noldPC\noldSP\nflags\nForwarding unit\nFigure 9.34: Pipelined data path with support for exceptions\n\u03c4 = #seconds\n#seconds #cycles\n= \u00d7 \u00d7(#instructions)\n#cycles #instructions\n#seconds #cycles\n= \u00d7 \u00d7(#instructions) (9.2)\n#cycles #instructions\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)\n1\/f CPI\nCPI \u00d7#insts\n=\nf\nThenumberofcyclespersecondistheprocessor\u2019sclockfrequency(f). Theaveragenumber\nof cycles per instruction is known as the CPI, and its inverse (number of instructions per cycle)\nis known as the IPC. The last term is the number of instructions (abbreviated to #insts). Note\nthat this is the number of dynamic instructions, or, alternatively, the number of instructions\nthat the processor actually executes. Note that it is NOT the number of instructions in the\nprogram\u2019s executable file.\nFO-FI XE-FO AM-XE WR-AM\nsgafl\n,)cpn\/cp( 431 (cid:13)c Smruti R. Sarangi\nDefinition 85\nStatic Instruction The binary or executable of a program contains a list of instructions.\nEach such instruction is a static instruction.\nDynamic Instruction Adynamicinstructionistheinstanceofastaticinstruction, which\nis created by the processor when an instruction enters the pipeline.\nDefinition 86\nCPI Cycles per instruction\nIPC Instructions per cycle\nWe can now define the performance P as a quantity that is inversely proportional to the\ntime, \u03c4. Equation 9.3 is known as the Performance Equation.\nIPC \u00d7f\nP \u221d (9.3)\n#insts\nWe can thus quickly conclude that the performance of a processor with respect to a pro-\ngram is proportional to the IPC, and frequency, and inversely proportional to the number of\ninstructions.\nLet us now look at the performance of a single cycle processor. Its CPI is equal to 1 for\nall instructions. The performance is thus proportional to f\/#insts. This is a rather trivial\nresult. It says that as we increase the frequency, a single cycle processor keeps getting faster\nproportionally. Likewise, if we are able to reduce the number of instructions in our program\nby a factor of X, then the performance also increases by a factor of X. Let us consider the\nperformance of a pipelined processor. The analysis is more complicated, and the insights are\nvery profound.\n9.9.2 Performance of an Ideal Pipelined Processor\nLet us look at the three terms in the performance equation (Equation 9.3), and consider them\none by one. Let us first consider the number of instructions.\nNumber of Instructions\nThe number of instructions in a program is dependent on the intelligence of the compiler. A\nreally smart compiler can reduce instructions by choosing the right set of instructions from the (cid:13)c Smruti R. Sarangi 432\nISA, and by using smart code transformations. For example, programmers typically have some\ncode, which can be categorised as dead code. This code has no effect on the final output. A\nsmart compiler can remove all the dead code that it can find. Another source of additional\ninstructions is the code to spill and restore registers. Compilers often perform function inlining\nforverysmallfunctions. Thisoptimisationdynamicallyremovessuchfunctionsandpastestheir\ncode in the code of the calling function. For small functions, this is a very useful optimisation\nsince we are getting the rid of the code to spill and restore registers. There are many more\ncompiler optimisations that help in reducing code size. The reader is referred to [Aho et al.,\n2006, Muchnick, 1997] for a detailed discussion on compiler design. For the rest of this section,\nwe shall assume that the number of instructions is a constant. Let us exclusively focus on the\nhardware aspect.\nComputing the Total Number of Cycles\nLet us assume an ideal pipeline that does not need to insert any bubbles, or stalls. It will be\nable to complete one instruction every cycle, and thus will have a CPI of 1. Let us assume a\nprogram containing n instructions, and let the pipeline have k stages. Let us compute the total\nnumber of cycles it will take for all the n instructions to leave the pipeline.\nLet the first instruction enter the pipeline in cycle 1. It leaves the pipeline in cycle k.\nHenceforth, one instruction will leave the pipeline every cycle. Thus, after (n\u22121) cycles, all\nthe instructions would have left the pipeline. The total number of cycles is therefore, n+k\u22121.\nThe CPI is equal to:\nn+k\u22121\nCPI = (9.4)\nn\nNote that the CPI tends to 1, as n tends to \u221e.\nRelationship with the Frequency\nLet the maximum amount of time that an instruction takes to finish its execution on a single\ncycle processor be t . This is also known as the total amount of algorithmic work. We are\nmax\nignoring the delays of pipeline registers while computing t . Now, let us divide the data path\nmax\ninto k pipeline stages. We need to add k \u2212 1 pipeline registers. Let the delay of a pipeline\nregister be l. If we assume that all the pipeline stages are balanced (do the same amount of\nwork, and take the same amount of time), then the time that the slowest instruction will take\nto finish its work in a stage is equal to tmax. The total time per stage is equal to the circuit\nk\ndelay and the delay of a pipeline register.\nt\nmax\nt = +l (9.5)\nstage\nk\nNow, the minimum clock cycle time has to be equal to the delay of a pipeline stage. This\nis because, the assumption while designing a pipeline is that each stage takes exactly one clock\ncycle. We thus have the minimum clock cycle time (t ), or the maximum frequency (f) equal\nclk\nto:\n1 t\nmax\nt = = +l (9.6)\nclk\nf k 433 (cid:13)c Smruti R. Sarangi\nPerformance of a Pipeline\nLet us now compute the performance of this pipeline, and make a simplistic assumption that\nperformance is equal to (f \/ CPI) because the number of instructions is a constant(n).\nf\nP =\nCPI\n1\ntmax+l\n= k\nn+k\u22121\n(9.7)\nn\nn\n=\n(t \/k+l)\u00d7(n+k\u22121)\nmax\nn\n=\n((n\u22121)t \/k+(t +ln\u2212l)+lk\nmax max\nLet us try to maximise performance by choosing the right value of k. We have:\n\u2202((n\u22121)t \/k+(t +ln\u2212l)+lk)\nmax max\n= 0\n\u2202k\n(n\u22121)t\nmax\n\u21d2\u2212 +l = 0 (9.8)\nk2\n(cid:114)\n(n\u22121)t\nmax\n\u21d2k =\nl\nEquation 9.8 provides a theoretical estimate of the optimal number of pipeline stages as a\nfunctionofthelatchdelay(l), thetotalalgorithmicwork(t ), andthenumberofinstructions\nmax\n(n). Let us gauge the trends predicted by this equation. The first is that as we increase the\nnumber of instructions, we can afford more pipeline stages. This is because the startup delay of\nk cycles, gets nullified when there are more instructions. Secondly, as we increase the amount\nof algorithmic work (t ), we need a deeper pipeline. More are the number of pipeline stages,\nmax\nless is the amount of work we need to do per stage. We can thus have a higher frequency, and\nthus have a higher instruction throughput. Lastly, the optimal number of stages is inversely\n\u221a\nproportional to l. As we increase the latch delay, we start wasting more time inserting and\nremoving data from latches. Hence, it is necessary to adjust the number of pipeline stages with\nthe latch delay. If the latches are very slow, we need to reduce the number of pipeline stages\nalso such that we do not waste a lot of time in adding, and removing data from pipeline latches.\nSadly, an ideal pipeline does not exist in practice. This means that they do not have a CPI\nequal to (n+k\u22121)\/n. Almost all programs have dependences between instructions, and thus it\nbecomes necessary to insert bubbles in the pipeline. Inserting bubbles increases the CPI from\nthe ideal CPI computed in Equation 9.4. Equation 9.8 provides us with interesting insights.\nHowever, the reader needs to note that it is hypothetical. It predicts that the optimal number\nof stages approaches infinity, for very large programs. This is unfortunately not the case in\npractical scenarios. (cid:13)c Smruti R. Sarangi 434\n9.9.3 Performance of a Non-Ideal Pipeline\nMathematical Characterisation\nWe need to incorporate the effect of stalls in the CPI equation. Let us assume that the number\nof instructions (n) is very very large. Let the ideal CPI be CPI . In our case, CPI = 1.\nideal ideal\nWe have:\nCPI = CPI +stall rate\u00d7stall penalty (9.9)\nideal\nExample 112\nAssume that the ideal CPI is 1. Assume that 10% of the instructions suffer a load-use\nhazard, and 20% of the instructions are taken branches. Find the CPI of the program.\nAnswer: We need to insert 1 bubble for a load-use hazard, and 2 bubbles for a taken\nbranch. Thus, the average number of bubbles that we need to insert per instruction is equal\nto: 0.1 * 1 + 0.2 * 2 = 0.5. Thus,\nCPI = CPI +0.5 = 1+0.5 = 1.5\nnew ideal\nExample 113\nCompare the performance of two programs, P and P . Assume that the ideal CPI for\n1 2\nboth of them is 1. For P , 10% of the instructions have a load-use hazard, and 15% of its\n1\ninstructions are taken branches. For P , 20% of the instructions have a load-use hazard,\n2\nand 5% of its instructions are taken branches.\nAnswer:\nCPI = 1+0.1\u22171+0.15\u22172 = 1.4\nP1\nCPI = 1+0.2\u22171+0.05\u22172 = 1.3\nP2\nThe CPI of P is less than the CPI of P . Hence, P is faster.\n2 1 2\nThe final CPI is equal to the sum of the ideal CPI and number of mean stall cycles per\ninstruction. The mean stall cycles per instruction is equal to the product of the average stall\nrateperinstructionmultipliedbytheaveragenumberofbubblesthatweneedtoinsertperstall\n(stall penalty). The stall rate term is typically a function of the nature of dependences across\ninstructions in a program. The stall penalty term is also typically dependent on the design of\nthe pipeline, and its forwarding paths. In our case, we need to stall for at most one cycle for\nRAW hazards, and for 2 cycles for taken branches. However, pipelines with more stages might\nhave different behaviours. Let us now try to model this pipeline mathematically.\nWe assume that the stall rate is only dependent on the program, and the stall penalty is\nproportional to the number of stages in a pipeline. This assumption is again not completely 435 (cid:13)c Smruti R. Sarangi\ncorrect. However, it is good enough for developing a coarse mathematical model. The reason,\nwe assume that stall penalty is proportional to the number of stages is because, we assume\nthat we create deeper pipelines by essentially splitting the stages of our simple pipeline further.\nFor example, we can pipeline the functional units. Let us assume that we divide each stage,\ninto two sub-stages. Then, we need to stall for 2 cycles on a load-use hazard, and stall for 4\ncycles for a taken branch.\nLet us thus assume that CPI = (n+k \u22121)\/n+rck, where r and c are constants, and k\nis the number of pipeline stages. r is equal to the average number of stalls per instruction\n(stall rate). We assume that the stall penalty \u221d k, or alternatively, stall penalty = ck, where\nc is the constant of proportionality.\nWe thus have:\nf\nP =\nCPI\n1\n= tmax\/k+l (9.10)\n(n+k\u22121)\/n+rck\nn\n=\n((n\u22121)t \/k+(rcnt +t +ln\u2212l)+lk(1+rcn)\nmax max max\nTo maximise performance, we need to minimise the denominator. We get:\n\u2202((n\u22121)t \/k+(rcnt +t +ln\u2212l)+lk(1+rcn))\nmax max max\n= 0\n\u2202k\n(n\u22121)t\nmax\n\u21d2\u2212 +l(1+rcn) = 0\nk2 (9.11)\n(cid:115) (cid:114)\n(n\u22121)t t\nmax max\n\u21d2k = \u2248 (as n \u2192 \u221e)\nl(1+rcn) lrc\nEquation 9.11 is more realistic than Equation 9.8. It is independent of the number of\ninstructions. The implicit assumption is that the number of instructions tends to infinity,\nbecause in most programs, we execute billions of instructions. Akin to Equation 9.8, the\n\u221a \u221a\noptimal number of pipeline stages is proportional to t , and inversely proportional to l.\n\u221a max\nAdditionally, k \u221d 1\/ rc. This means that as the penalty for a stall increases, or the number\nof stall events per instruction increase, we need to use less pipeline stages.\nLetusnowfindtheperformancefortheoptimalnumberofpipelinestages. InEquation9.10,\nwe assume that n \u2192 \u221e. Thus (n+k\u22121)\/n \u2192 1. Hence, we have: (cid:13)c Smruti R. Sarangi 436\n1\nP =\nideal\n(t \/k+l)\u00d7(1+rck)\nmax\n1\n=\nt \/k+l+rct +lrck\nmax max\n1\n=\n(cid:114)\n(cid:16) (cid:17) (cid:113)\nt \u00d7 lrc +l+rct +lrc\u00d7 (cid:0)tmax(cid:1) (9.12)\nmax tmax max lrc\n1\n= \u221a\nrct +2 lrct +l\nmax max\n1\n=\n(cid:16)\u221a \u221a (cid:17)2\nrct + l\nmax\nImplications of Equation 9.11 and Equation 9.12\nLet us now study the different implications of the result regarding the optimal number of\npipeline stages.\nImplication 1\nThecrucialimplicationoftheseresultsisthatforprogramswithalotofdependences,weshould\nuse processors with a lesser number of pipeline stages. Inversely, for programs that have high\nIPC(lessdependencesacrossinstructions), weshoulduseprocessorsthathavedeeperpipelines.\nImplication 2\nLetuscomparetwoversionsofourpipeline. Oneversionusesinterlocksforalldependences,and\nthe other uses forwarding. For the pipeline with forwarding, the stall penalty is much lower.\nConsequently, the value of the constant, c, is smaller in the case of the pipeline with forwarding\nturned on. This means that a pipeline with forwarding ideally requires more pipeline stages\nfor optimal performance. As a general rule, we can conclude that as we increase the amount of\nforwarding in a pipeline, we should make it deeper.\nImplication 3\n(cid:112)\nThe optimal number of pipeline stages is directly proportional to (t \/l). If we have faster\nmax\nlatches, we can support deeper pipelines. Secondly, with the progress of technology, t \/l is\nmax\nnot changing significantly [ITRS, 2011], because both logic gates, and latches are getting faster\n(roughly equally). Hence, the optimal number of pipeline stages for a processor has remained\nalmost the same for at least the last 5 years.\nImplication 4\nAs we increase l, r, c, and t the ideal performance goes down as per Equation 9.12. The\nmax\nlatch delay can be a very sensitive parameter, especially, for processors that are designed to\nrun workloads with few dependences. In this case, r, and c, will have relatively small values,\nand Equation 9.12 will be dominated by the value of the latch delay. 437 (cid:13)c Smruti R. Sarangi\nExample 114\nFind the optimal number of pipeline stages for the following configuration. t \/l = 20, r\nmax\n= 0.2, c = 0.6.\nAnswer: We have:\n(cid:114)\nt max (cid:112)\nk = = 20\/(0.2\u22170.6) = 12.9 \u2248 13\nlrc\nExample 115\nConsider two programs that have the following characteristics.\nProgram 1 Program 2\nInstruction Type Fraction Instruction Type Fraction\nloads 0.4 loads 0.3\nbranches 0.2 branches 0.1\nratio(taken branches) 0.5 ratio(taken branches) 0.4\nThe ideal CPI is 1 for both the programs. Let 50% of the load instructions suffer from\na load-use hazard. Assume that the frequency of P is 1, and the frequency of P is 1.5.\n1 2\nHere, the units of the frequency are not relevant. Compare the performance of P and P .\n1 2\nAnswer:\nCPI =CPI +0.5\u00d7(ratio(loads))\u00d71\nnew ideal\n(9.13)\n+ratio(branches)\u00d7ratio(takenbranches)\u00d72\nWe thus have:\nCPI = 1+0.5\u00d70.4+0.2\u00d70.5\u00d72 = 1+0.2+0.2 = 1.4\nP1\nCPI = 1+0.5\u00d70.3+0.1\u00d70.4\u00d72 = 1+0.15+0.08 = 1.23\nP2\nThe performance of P can be expressed as f\/CPI = 1 \/ 1.4 = 0.71 (arbitrary units).\n1\nSimilarly, the performance of P is equal to f\/CPI = 1.5\/1.23 = 1.22 (arbitrary units).\n2\nHence, P is faster than P . We shall often use the term, arbitrary units, a.u., when the\n2 1\nchoice of units is irrelevant.\n9.9.4 Performance of a Suite of Programs\nMost of the time, we do not measure the performance of a processor with respect to one\nprogram. We consider a set of known benchmark programs and measure the performance of\nour processor with respect to all the programs to get a consolidated figure. Most processor\nvendors typically summarise the performance of their processor with respect to the SPEC\n(http:\/\/www.spec.org) benchmarks. SPEC stands for \u201cStandard Performance Evaluation (cid:13)c Smruti R. Sarangi 438\nCorporation\u201d. Theydistributesuitesofbenchmarksformeasuring, summarising, andreporting\nthe performance of processors, and software systems.\nComputer architectures typically use the SPEC CPU benchmark suite to measure the per-\nformance of a processor. The SPEC CPU 2006 benchmarks have two types of programs \u2013\ninteger arithmetic benchmarks (SPECint), and floating point benchmarks (SPECfp). There\nare 12 SPECint benchmarks that are written in C\/C++. The benchmarks contain parts of\nC compilers, gene sequencers, AI engines, discrete event simulators, and XML processors. On\nsimilar lines, the SPECfp suite contains 17 programs. These programs solve different problems\nin the domains of physics, chemistry, and biology.\nMost processor vendors typically compute a SPEC score, which is representative of the\nperformance of the processor. The recommended procedure is to take the ratio of the time\ntaken by a benchmark on a reference processor, and the time taken by the benchmark on the\ngiven processor. The SPEC score is equal to the geometric mean of all the ratios. In computer\narchitecture, when we report the mean relative performance (as in the case of SPEC scores),\nwe typically use the geometric mean. For just reporting the average time of execution (absolute\ntime), we can use the arithmetic mean.\nSometimes, instead of reporting SPEC scores, we report the average number of instructions\nthatweexecutepersecond,andinthecaseofscientificprograms,theaveragenumberoffloating\npoint operations per second. These metrics give us an indication of the speed of a processor,\nor a system of processors. We typically use the following terms:\nKIPS Kilo(103) instructions per second\nMIPS Million(106) instructions per second\nMFLOPS Million(106) floating point operations per second\nGFLOPS Giga(109) floating point operations per second\nTFLOPS Tera(1012) floating point operations per second\nPFLOPS Peta(1015) floating point operations per second\n9.9.5 Inter-Relationship between Performance, the Compiler, Architecture,\nand Technology\nLet us now summarise our discussion by looking at the relationships between performance,\ncompiler design, processor architecture, and manufacturing technology. Let us consider the\nperformance equation again (see Equation 9.14) (let us assume arbitrary units for performance\nand replace the proportional sign by an equality).\nf \u00d7IPC\nP = (9.14)\n#insts\nIfourfinalaimistomaximiseperformance,thenweneedtomaximisethefrequency(f),and\nthe IPC. Simultaneously, we need to minimise the number of dynamic instructions (#insts).\nThere are three knobs that are under our control namely the processor architecture, manufac-\nturing technology, and the compiler. Note that we loosely use the term \u201carchitecture\u201d here. 439 (cid:13)c Smruti R. Sarangi\nWe wish to use the term \u201carchitecture\u201d to refer to the actual organisation and design of the\nprocessor. However, in literature, it is common to use the term \u201carchitecture\u201d to refer to both\nthe ISA, and the design of a processor. Hence, we use the same terminology here. Let us look\nat each of our knobs in detail.\nThe Compiler\nBy using smart compiler technology we can reduce the number of dynamic instructions, and\nalso reduce the number of stalls. This will improve the IPC. Let us consider two examples:\nExamples116and117. Here,weremoveonestallcyclebyreorderingtheaddandldinstructions.\nOn similar lines, compilers typically analyse hundreds of instructions, and optimally reorder\nthem to reduce stalls as much as possible.\nExample 116\nReorder the following piece of code without violating the correctness of the program to reduce\nstalls.\nadd r1, r2, r3\nld r4, 10[r5]\nsub r1, r4, r2\nAnswer: We have a load-use hazard here, between the ld and sub instructions. We can\nreorder the code as follows.\nld r4, 10[r5]\nadd r1, r2, r3\nsub r1, r4, r2\nNow, we do not have any load-use hazards, and the logic of the program remains the\nsame.\nExample 117\nReorder the following piece of code without violating the correctness of the program to reduce\nstalls. Assume delayed branches with 2 delay slots\nadd r1, r2, r3\nld r4, 10[r5]\nsub r1, r4, r2\nadd r8, r9, r10\nb .foo\nAnswer: (cid:13)c Smruti R. Sarangi 440\nadd r1, r2, r3\nld r4, 10[r5]\nb .foo\nsub r1, r4, r2\nadd r8, r9, r10\nWe eliminate the load-use hazard, and optimally used the delay slots.\nThe Architecture\nWe have designed an advanced architecture in this chapter by using pipelining. Note that\npipelining by itself, does not increase performance. In fact because of stalls, pipelining reduces\nthe IPC of a program as compared to a single cycle processor. The main benefit of pipelining is\nthat it allows us to run the processor at a higher frequency. The minimum cycle time reduces\nfrom t for a single cycle pipeline to t \/k +l for a k-stage pipelined machine. Since we\nmax max\ncomplete the execution of a new instruction every cycle unless there are stalls, we can execute\na set of instructions much faster on a pipelined machine. The instruction execution throughput\nis much higher.\nImportant Point 15\nThe main benefit of pipelining is that it allows us to run the processor at a higher frequency.\nBy running the processor at a higher frequency, we can ensure a higher instruction through-\nput (more instructions complete their execution per second). Pipelining by itself, reduces\nthe IPC of a program as compared to a single cycle processor, and it also increases the time\nit takes to process any single instruction.\nTechniques such as delayed branches, and forwarding help increase the IPC of a pipelined\nmachine. Weneedtofocusonincreasingtheperformanceofcomplexpipelinesthroughavariety\nof techniques. The important point to note here is that architectural techniques affect both\nthe frequency (via the number of pipeline stages), and the IPC (via the optimisations such as\nforwarding and delayed branches).\nManufacturing Technology\nManufacturing technology affects the speed of transistors, and in turn the speed of combina-\ntional logic blocks, and latches. Transistors are steadily getting smaller and faster. Conse-\nquently, the total algorithmic work (t ) and the latch delay (l), are also steadily reducing.\nmax\nHence, it is possible to run processors at higher frequencies leading to improvements in per-\nformance (also see Equation 9.12). Manufacturing technology exclusively affects the frequency\nat which we can run a processor. It does not have any effect on the IPC, or the number of\ninstructions. 441 (cid:13)c Smruti R. Sarangi\nP f IPC\nTechnology Compiler\nArchitecture Architecture\nFigure 9.35: Relationship between performance, the compiler, architecture and technology\nWe can thus summarise our discussion in Figure 9.35.\nNote that the overall picture is not as simple as we describe in this section. We need to\nconsiderpowerandcomplexityissues also. Typically, implementinga pipeline beyond20stages\nis very difficult because of the increase in complexity. Secondly, most modern processors have\nsevere power and temperature constraints. This problem is also known as the power wall. It\nis often not possible to ramp up the frequency, because we cannot afford the increase in power\nconsumption. As a thumb rule, power increases as the cube of frequency. Hence, increasing the\nfrequency by 10% increases the power consumption by more than 30%, which is prohibitively\nlarge. Designers are thus increasingly avoiding deeply pipelined designs that run at very high\nfrequencies.\n9.10 Power and Temperature Issues\n9.10.1 Overview\nLetusnowbrieflylookatpowerandtemperatureissues. Theseissueshaveincreasinglybecome\nmore important over the last decade. High performance processor chips typically dissipate 60-\n120W of power during normal operation. If we have four chips in a server class computer,\nthen we shall roughly dissipate 400W of power. As a general rule of thumb the rest of the\ncomponents in a computer such as the main memory, hard disk, peripherals, and fans, also\ndissipate a similar amount of power. The total power consumption is roughly 800W. If we add\nadditional overheads such as the non-ideal efficiency of the power supply, the display hardware,\nthe power requirement goes up to about 1KW. Now, a typical server farm that has 100 servers\nwill require 100 kW of power for running the computers. Additionally, it will require extra\npower for the cooling units such as air conditioners. Typically, to remove 1 W of heat, we\nrequire 0.5W of cooling power. Thus the total power dissipation of our server farm is about 150\nkW. In comparison, a typical home has a rated power of 6-8 kW. This means that the power\ndissipated by one server farm is equivalent the power used by 20-25 homes, which is significant.\nNote that a server farm containing 100 machines is a relatively small setup, and in practice we\nhave much larger server farms containing thousands of machines. They require megawatts of\npower, which is enough for the needs of a small town.\nLet us now consider really small devices such as the processors in cell phones. Here, also\npower consumption is an important issue because of the limited amount of battery life. All of (cid:13)c Smruti R. Sarangi 442\nus would love devices that have very long battery lifes especially feature rich smart phones. Let\nus now consider even smaller devices such as small processors embedded inside the body for\nmedical applications. We typically use small microchips in devices such as pacemakers. In such\ncases, we do not want to inconvenience the patient by forcing him or her to also carry heavy\nbatteries, or recharge the batteries often. To prolong battery life, it is important to dissipate\nas little power as possible.\nHeat flow\nHeat sink\nThermal interface material (TIM)\nHeat spreader\nTIM\nSilicon die\nProcessor socket\nFigure 9.36: Diagram of a chip\u2019s package\nNow, let us consider temperature, which is a very closely related concept. Let us take a\nlook at the diagram of the typical package of a chip in Figure 9.36. We typically have a 200-400\nmm2 silicon die. The die refers to a rectangular block of silicon that contains the circuit of the\nchip. Since this small piece of silicon dissipates 60-100 W of power (equivalent to 6-10 CFL\nlight bulbs), its temperature can rise to 200\u25e6C unless we take additional measures to cool the\nsilicon die. We first add a 5cm \u00d7 5cm nickel plated copper plate on the silicon die. This is\nknown as the spreader . The spreader helps in creating a homogeneous temperature profile on\nthe die by spreading the heat, and thus eliminating hot spots. We need a spreader because all\nthe parts of a chip do not dissipate the same amount of heat. The ALUs typically dissipate a\nlot of heat. However, the memory elements, are relatively cooler. Secondly, the heat dissipation\ndepends on the nature of the program. For integer benchmarks, the floating point ALU is idle,\nand thus it will be much cooler. To ensure that heat properly flows from the silicon die to the\nspreader we typically add a thermally conducting gel known as the thermal interface material\n(TIM).\nMost chips have a structure known as the heat sink on top of the spreader. It is a copper\nbased structure that has an array of fins as shown in Figure 9.36. We add an array of fins to\nincrease its surface area. This ensures that most of the heat generated by the processors can\nget dissipated to the surrounding air. In chips that are used in desktops, laptops, and servers,\nwe have a fan mounted on the heat sink, or in the chassis of the computer that blows air over\nthe heat sink. This ensures that hot air is dissipated away, and colder air from outside flows\nover the heat sink. The assembly of the spreader, heat sink, and fan help in dissipating most\nof the heat generated by the processor. 443 (cid:13)c Smruti R. Sarangi\nIn spite of advanced cooling technology, processors still heat up to 60-100\u25e6C . While playing\nhighly interactive computer games, or while running heavy number crunching applications like\nweather simulation, on-chip temperatures can go up to 120\u25e6C . Such temperatures are high\nenoughtoboilwater,cookvegetables,andevenwarmasmallroominwinter. Insteadofbuying\nheaters, we can just run a computer!!! Note that temperature has a lot of deleterious effects.\nIn particular, the reliability of on-chip copper wires, and transistors decreases exponentially\nwith increasing temperature [Srinivasan et al., 2004]. Secondly, chips tend to age over time\ndue to an effect known as NBTI (Negative Bias Temperature Instability). Ageing effectively\nslows down transistors. Hence, it becomes necessary to reduce the frequency of processors over\ntime to ensure correct operation. Secondly, some power dissipation mechanisms such as leakage\npower are dependent on temperature. This means that as the temperature goes up the leakage\ncomponent of the total power also goes up, and this further increases temperature.\nLetusthusconcludethatit is very importanttoreduceonchippowerandtemperaturein\nthe interest of lower electricity bills, reduced cooling costs, longer battery life, higher reliability,\nand slower ageing.\nLetusnowquicklyreviewthemainpowerdissipationmechanisms. Weshallprimarilyfocus\non two mechanisms namely dynamic and leakage power. Leakage power is also known as static\npower.\n9.10.2 Dynamic Power\nLet us consider a chip\u2019s package as a closed black box. We have electrical energy flowing in, and\nheatcomingout. Overasufficientlylongperiodoftime, theamountofelectricalenergyflowing\nin to the chip is exactly equal to the amount of energy dissipated as heat according to the law\nof conservation of energy. Note that we disregard the energy spent in sending electrical signals\nalong I\/O links. In any case, this energy is negligible as compared to the power dissipation of\nthe entire chip.\nAny circuit consisting of transistors, and copper wires can be modelled as an equivalent\ncircuit with resistors, capacitors, and inductors. Capacitors and inductors do not dissipate\nheat. However, resistors convert a part of the electrical energy that flows through them to\nheat. This is the only mechanism through which electrical energy can get converted to thermal\nenergy in our equivalent circuit.\nLet us now consider a small circuit that has a single resistor and a single capacitor as shown\nin Figure 9.37. The resistor represents the resistance of the wires in the circuit. The capacitor\nrepresentstheequivalentcapacitanceoftransistorsinthecircuit. Weneedtonotethatdifferent\npartsofacircuitsuchasthegatesoftransistorshaveacertainpotentialatagivenpointintime.\nThis means that the gate of a transistor is functioning as a capacitor, and hence storing charge.\nSimilarly, the drain and source of a transistor have an equivalent drain and source capacitance.\nWe typically do not consider equivalent inductance in a simplistic analysis, because most wires\nare typically short, and they do not function as inductors.\nIf we analyse this simple circuit, then we can conclude that the total energy required to\ncharge the capacitor is CV2. 1CV2 is dissipated by the resistor while charging the capacitor,\n2\nand the remaining energy is stored in the capacitor. Now, if the capacitor gets discharged, then\nthe remaining 1CV2 gets dissipated via the resistor.\n2\nNow, letusgeneralisethisresult. Inalargecircuitwithbillionsoftransistors, weessentially (cid:13)c Smruti R. Sarangi 444\nR\n+\nC\nV\n-\nFigure 9.37: A circuit with a resistance and capacitance\nhave billions of subcircuits with resistive and capacitive elements. Each cycle, we can either\nhave a transition in a bit (0 \u2192 1 or 1 \u2192 0), or we might have no transitions at all. If there\nis a transition in the value of a bit, then either a capacitor gets charged or gets discharged.\nHowever, if there are no transitions, then there is no current flow, and thus there is no heat\ndissipation.\nLet us assume that we have n subcircuits. Let, \u03b1 be known as the activity factor. It is 1 if\ni\nthere is a transition, and 0 if there is no transition in subcircuit i. Let E ...E be the energy\n1 n\ndissipated by all the n subcircuits. We thus have:\n1\nE = \u03b1 C V2 (9.15)\n1 1 1\n2\n1\nE = \u03b1 C V2 (9.16)\n2 2 2\n2\n...\n1\nE = \u03b1 C V2 (9.17)\nn n n\n2\nThe total energy dissipated is equal to\n(cid:80)n\nE . Let us now group the small subcircuits\ni=1 i\ninto functional units, and assume that the capacitance values across all the subcircuits in a\nfunctional unit are roughly similar. Thus, for a given functional unit j, we can say that:\nE \u221d \u03b1 C V2 (9.18)\nj j j\nHere, C is a representative value of capacitance for the entire functional unit, and \u03b1 is the\nj j\nactivity factor for the entire functional unit. 0 represents no activity, and 1 represents 100%\nactivity. 0 \u2264 \u03b1 \u2264 1. Note that we have also replaced the equality by a proportional sign\nj\nbecause we are interested in the nature of power dissipation rather than the exact values.\nWe can thus express the total energy consumption of a circuit having n(cid:48) functional units as:\nn(cid:48)\n(cid:88)\nE \u221d \u03b1 C V2 (9.19)\ni i\ni=1\nThis equation represents the energy consumed per cycle. Power is equal to energy divide\nby time. In this case the time is equal to the clock cycle time, or the reciprocal of the chip\u2019s\nfrequency (f). Thus the total power (P) is equal to: 445 (cid:13)c Smruti R. Sarangi\nn(cid:48)\n(cid:88)\nP \u221d \u03b1 C V2f (9.20)\ni i\ni=1\nThe power dissipated is thus proportional to the frequency, and the square of the supply\nvoltage. Note that this power dissipation represents the resistive loss due to the transitions in\nthe inputs and outputs. Hence, it is known as the dynamic power, P . Thus, we have:\ndyn\nn(cid:48)\n(cid:88)\nP \u221d \u03b1 C V2f (9.21)\ndyn i i\ni=1\nDefinition 87\nDynamic power is the cumulative power dissipated due the transitions of inputs and outputs\nacross all the transistors in a circuit.\n9.10.3 Leakage Power\nNote that dynamic power is not the only power dissipation mechanism in processors. Static or\nleakage power is a major component of the power dissipation of high performance processors.\nIt accounts for roughly 20-40% of the total processor power budget.\nThe main insight is as follows. We have up till now been assuming that a transistor does\nnot allow any current to flow through it when it is in the off state. There is absolutely no\ncurrent flow across the terminals of a capacitor, or between the gate and the source of an\nNMOS transistors. All of these assumptions are not strictly correct. No structure is a perfect\ninsulator in practice. There is a small amount of current flow across its terminals, even in the\noff state. We can have many other sources of leakage power across other interfaces that are\nideally not supposed to pass current. Such sources of current are together referred to as leakage\ncurrent, and the associated power dissipation is known as the leakage power.\nDefinition 88\nLeakage current is the minimal amount of current that flows across two terminals of a\ncircuit element that are ideally supposed to be completely electrically isolated from each\nother. For example, we do not expect any current flow between the drain and the source\nof an NMOS transistor in the off state. However, a small amount of current does flow,\nand this is known as the sub-threshold leakage current. When leakage current flows across\na resistive element, it dissipates leakage power. Leakage power is static in nature and is\ndissipated all the time irrespective of the level of activity in a circuit.\nThere are different mechanisms for leakage power dissipation such as sub-threshold leak-\nage, and gate induced drain leakage. Researchers typically use the following equation from (cid:13)c Smruti R. Sarangi 446\nthe BSIM3 model [Cheng and Hu, 1999] for leakage power (primarily captures sub-threshold\nleakage):\nP\nleak\n= A\u00d7\u03bd T2\n\u00d7eVGS\u2212 nV \u00d7th \u03bdT\u2212Voff (cid:18) 1\u2212e\u2212V \u03bdTDS(cid:19)\n(9.22)\nVariable Definition (SI units)\nA Area dependent constant of proportionality\n\u03bd Thermal voltage (kT\/q)\nT\nk Boltzmann\u2019s constant (1.38\u00d710\u221223) (SI units)\nq 1.6\u00d710\u221219\nT Temperature (in Kelvins)\nV Voltage between the gate and source\nGS\nV Threshold voltage. It is also dependent on temperature.\nth\n\u2202V th = \u22122.5mV\/K\n\u2202T\nV Offset voltage\noff\nn Sub-threshold swing coefficient\nV Voltage between the drain and source\nDS\nTable 9.3: Definition of variables in Equation 9.22\nTable 9.3 defines the variables used in Equation 9.22. Note that the leakage power is\ndependent on temperature via the variable \u03bd = kT\/q. To show the temperature dependence,\nT\nwe can simplify Equation 9.22 to obtain Equation 9.23.\n(cid:16) (cid:17)\nP \u221d T2\u00d7eA\/T \u00d7 1\u2212eB\/T (9.23)\nleak\nIn Equation 9.23, A and B are constants, and can be derived from Equation 9.22. Around\n10 years ago (as of 2002), when the transistor threshold voltages used to be higher (around\n500 mV), leakage power was exponentially dependent on temperature. Hence, a small increase\nin temperature would translate to a large increase in leakage power. However, nowadays, the\nthreshold voltages are between 100-150 mV. Consequently, the relationship between tempera-\nture and leakage has become approximately linear [Sarangi et al., 2014].\nThe important point to note here is that leakage power is dissipated all the time by all\nthe transistors in a circuit. The amount of leakage current might be very small; but when we\nconsider the cumulative effect of billions of transistors, the total amount of leakage power dis-\nsipation is sizeable, and can even become a large fraction of the dynamic power. Consequently,\ndesigners try to control temperature to keep leakage power under control.\nHence, the total power, P , is given by:\ntot\nP = P +P (9.24)\ntot dyn leak\n9.10.4 Modeling Temperature*\nModeling the temperature on a chip is a fairly complex problem, and requires a fair amount of\nbackground in thermodynamics and heat transfer. Let us state a basic result here, and move 447 (cid:13)c Smruti R. Sarangi\non.\nLet us divide the area of a silicon die into a grid. Let us number the grid points 1...m.\nLet the power vector P represent the total power dissipated by each grid point. Similarly, let\ntot\nthe temperature of each grid point be represented by the vector T. Power and temperature are\ntypically related by the following linear equation for a large number of grid points.\nT \u2212T = \u2206T = A\u00d7P (9.25)\namb tot\nT is known as the ambient temperature, and it is the temperature of the surrounding\namb\nair. A is an m\u00d7m matrix, and is also known as the thermal resistance matrix. According to\nEquation9.25thechangeintemperature(\u2206T), andthepowerconsumptionarelinearlyrelated\nto each other.\nNotethatP = P +P ,andP isafunctionoftemperature. Hence,Equations9.24,\ntot dyn leak leak\nand9.25formafeedbackloop. Wethusneedtoassumeaninitialvalueoftemperature,compute\ntheleakagepower,estimatethenewtemperature,computetheleakagepower,andkeepiterating\ntill the values converge.\n9.10.5 The ED2 Metric\nNow, let us try to integrate performance, and energy into one model. The performance of a\nprogram is given by the performance equation (Equation 9.3). Let us simplistically assume\nthat the time a program takes, or its delay (D) is inversely proportional to the frequency.\nAgain, this is not strictly correct because the IPC is dependent on the frequency. We cannot\nappreciate the relationship between IPC and frequency right now, because we do not have\nadequate background. However, we shall touch this topic in Section 10.3, and see that there\nare components to the IPC that are frequency dependent such as the latency of main memory.\nIn any case, let us move ahead with the approximation that D \u221d 1\/f.\nLet us compare two processor designs for the same program. One design dissipates E\n1\nJoules for the execution of the entire program, and it takes D units of time. The second design\n1\ndissipates E Joules, and takes D units of time. How do we say, which design is better? It is\n2 2\npossible that the second design is slightly faster but dissipates 3 times more energy per cycle.\nThere has to be a common metric.\nTo derive a common metric, we need to either make the performance the same (D = D ),\n1 2\nand then compare the energy, or make the energy the same (E = E ), and compare the\n1 2\nperformance. To ensure that D = D we need to either speed up one design or slowdown the\n1 2\nother one. To achieve this, we can use a standard technique called dynamic voltage-frequency\nscaling (DVFS).\nAccording to the DVFS technique, to scale up the frequency by a factor of \u03ba , we scale\n1\nthe voltage by a factor of \u03ba . Typically, we assume that \u03ba = \u03ba . For example, to double the\n2 1 2\nfrequency, we double the voltage also. Note that with a higher frequency and consequent lower\nclock cycle time, we need to ensure that signals can rise and fall quickly. To ensure quicker\nsignal transition, we increase the voltage such that it takes a lesser amount of time for a signal\ntoriseandfallby\u2206V volts. Thisfactcanbeprovedbyconsideringthebasiccapacitorcharging\nand discharging equations. From our point of view, we need to appreciate the fact that the\nvoltage and frequency need to be scaled together. (cid:13)c Smruti R. Sarangi 448\nDefinition 89\nDVFS is a technique that is used to adjust the voltage and frequency of a processor at run\ntime. If we scale the frequency by a factor of \u03ba , then we need to scale the voltage by a\n1\nfactor of \u03ba . In most cases, we assume that \u03ba = \u03ba .\n2 1 2\nNow, let us try to equalise the execution time of designs 1 and 2, and compare the energy.\nWe have made the following assumptions: D \u221d 1\/f, and f \u221d V. Thus, D \u221d 1\/V. To make the\ndelays equal we need to scale the delay of design 2 by D \/D , or alternatively we need to scale\n1 2\nits voltage and frequency by D \/D . After equalising the delay, let the energy dissipation of\n2 1\ndesign 2 be E(cid:48). Since E \u221d \u03b1V2, we have:\n2\nV2\nE(cid:48) = E \u00d7 1\n2 2 V2\n2\nf2\n= E \u00d7 1 (9.26)\n2 f2\n2\nD2\n= E \u00d7 2\n2 D2\n1\nNow, let us compare E and E(cid:48).\n1 2\nE(cid:48) <=> E\n2 1\nD2\n\u21d4E \u00d7 2 <=> E (9.27)\n2 D2 1\n1\n\u21d4E D2 <=> E D2\n2 2 1 1\nIn this case, we observe that comparing E(cid:48) and E is tantamount to comparing E D2,\n2 1 2 2\nand E D2. Since E \u221d V2(\u221d 1\/D2), ED2 = \u03ba. Here, \u03ba is a constant that arises out of the\n1 1\ndifferent constants of proportionality. It is thus a property that is independent of the voltage\nand frequency of the system. It is related to the activity factor, and the capacitance of the\ncircuits, and is inherent to the design. Consequently, the ED2 metric is used as an effective\nbaseline metric to compare two designs.\nDesigners aim to reduce the ED2 metric of a design as much as possible. This ensures that\nirrespective of the DVFS settings, a design with a lower value of ED2 is a much better design\nthan other designs that have a higher ED2 metric. Note that a lot of performance enhancing\nschemes do not prove to be effective because they do not show any benefit with regards to the\nED2 metric. They do increase performance, but also disproportionately increase the energy\ndissipation. Likewise a lot of power reduction schemes are impractical because they increase\nthe delay, and the ED2 metric increases. Consequently, whenever we need to jointly optimise\nenergy\/power and performance we use the ED2 metric to evaluate candidate designs. 449 (cid:13)c Smruti R. Sarangi\n9.11 Advanced Techniques*\nWay Point 8\n\u2022 We designed a complete single cycle processor for the SimpleRisc instruction set in\nSection 8.1. This processor had a hardwired control unit.\n\u2022 We designed a more flexible variant of our SimpleRisc processor using a micro-\nprogrammed control unit. This required a bus based data path along with a new set of\nmicroinstructions, and microassembly based code snippets for each program instruc-\ntion.\n\u2022 We observed that our processors could be significantly sped up by pipelining. However,\na pipelined processor suffers from hazards that can be significantly eliminated by a\ncombination of software techniques, pipeline interlocks, and forwarding.\nIn this section, we shall take a brief look at advanced techniques for implementing processors.\nNotethatthissectionisbynomeansselfcontained,anditsprimarypurposeistogivethereader\npointers for additional study. We shall cover a few of the broad paradigms for substantially\nincreasing performance. These techniques are adopted by state of the art processors.\nModern processors typically execute multiple instructions in the same cycle using very deep\npipelines (12-20 stages), and employ advanced techniques to eliminate hazards in the pipeline.\nLet us look at some of the common approaches.\n9.11.1 Branch Prediction\nLet us start with the IF stage, and see how we can make it better. If we have a taken branch\nin the pipeline then the IF stage in particular needs to stall for 2 cycles in our pipeline, and\nthen needs to start fetching from the branch target. As we add more pipeline stages, the\nbranch penalty increases from 2 cycles to more than 20 cycles. This makes branch instructions\nextremely expensive, and they are known to severely limit performance. Hence, it is necessary\nto avoid pipeline stalls even for taken branches.\nWhat if, it is possible to predict the direction of branches, and also predict the branch\ntarget? In this case, the fetch unit can immediately start fetching from the predicted branch\ntarget. If the prediction is found to be wrong at a later point of time, then all the instructions\nafterthe mispredictedbranchinstructionneed tobecancelled, anddiscarded fromthepipeline.\nSuch instructions are also known as speculative instructions.\nDefinition 90\nModern processors typically execute large sets of instructions on the basis of predictions. For (cid:13)c Smruti R. Sarangi 450\nexample, they predict the direction of branches, and accordingly fetch instructions starting\nfrom the predicted branch target. The prediction is verified later when the branch instruction\nis executed. If the prediction is found to be wrong, then all the instructions that were incor-\nrectly fetched or executed are discarded from the pipeline. These instructions are known as\nspeculative instructions. Conversely, instructions that were fetched and executed correctly,\nor whose predictions have been verified are called non-speculative instructions.\nNote that it is extremely essential to prohibit speculative instructions from making changes\nto the register file or writing to the memory system. Thus, we need to wait for instructions to\nbecome non-speculative before we allow them to make permanent changes. Second, we also do\nnot allow them to leave the pipeline before they become non-speculative. However, if there is\na need to discard speculative instructions, then modern pipelines adopt a simpler mechanism.\nInstead of selectively converting speculative instructions into pipeline bubbles as we have done\ninoursimplepipeline, modernprocessorstypicallyremovealltheinstructionsthatwerefetched\nafter the mispredicted branch instruction. This is a simple mechanism that works very well in\npractice. It is known as a pipeline flush.\nDefinition 91\nModern processors typically adopt a simple approach of discarding all speculative instruc-\ntions from a pipeline. They completely finish the execution of all instructions till the mis-\npredicted instruction, and then clean up the entire pipeline, effectively removing all the\ninstructions that were fetched after the mispredicted instruction. This mechanism is known\nas a pipeline flush.\nMain Challenges\nLet us now outline the main challenges in branch prediction.\n1. We need to first find out in the fetch stage if an instruction is a branch, and if it is a\nbranch, we need to find the address of the branch target.\n2. Next, we need to predict the expected direction of the branch.\n3. It is necessary to monitor the result of a predicted instruction. If there is a misprediction,\nthenweneedtoperformapipelineflushatalaterpointoftimesuchthatwecaneffectively\nremove all the speculative instructions.\nDetecting a misprediction in the case of a branch is fairly straight forward. We add the\nprediction to the instruction packet, and verify the prediction with the actual outcome. If they\nare different, then we schedule a pipeline flush. The main challenge is to predict the target of\na branch instruction, and its outcome. 451 (cid:13)c Smruti R. Sarangi\nBranch Target Buffer\nModern processors use a simple hardware structure called a branch target buffer (BTB). It is\na simple memory array that saves the program counter of the last N branch instructions, and\ntheir targets (N typically varies from 128 to 8192). There is a high likelihood of finding a\nmatch, because programs typically exhibit some degree of locality. This means that they tend\nto execute the same piece of code repeatedly over a period of time such as loops. Hence, entries\nin the BTB tend to get repeatedly reused in a small window of time. If there is a match, then\nwe can also automatically infer that the instruction is a branch.\n2-bit Saturating Counter based Branch Predictor\nItismuchmoredifficulttoeffectivelypredictthedirectionofabranch. However, wecanexploit\na pattern here. Most branches in a program typically are found in loops, or in if statements\nwhere both the directions are not equally likely. In fact, one direction is far more likely that\nthe other. For example, branches in loops are most of the time taken. Sometimes, we have if\nstatements that are only evaluated if a certain exceptional condition is true. Most of the time,\nthe branches associated with these if statements are not taken. Similarly, for most programs,\ndesigners have observed that almost all the branch instructions follow certain patterns. They\neither have a strong bias towards one direction, or can be predicted on the basis of past history,\nor can be predicted on the basis of the behaviour of other branches. There is of course no\ntheoretical proof of this statement. This is just an observation made by processor designers,\nand they consequently design predictors to take advantage of such patterns in programs.\nWe shall discuss a simple 2-bit branch predictor in this book. Let us assume that we have\na branch prediction table that assigns a 2-bit value to each branch in the table, as shown in\nFigure 9.38. If this value is 00, or 01, then we predict that the branch is not taken. If it is\nequal to 10, or 11, then we predict that the branch is taken. Moreover, every time the branch\nis taken, we increment the associated counter by 1, and every time, the branch is not taken we\ndecrement the counter by 1. To avoid overflows, we do not increment 11 by 1 to produce 00,\nand we do not decrement 00 to produce 11. We follow the rules of saturating arithmetic that\nstate that (in binary): (11 + 1 = 11), and (00 - 1 = 00). This 2-bit value is known as a 2-bit\nsaturating counter. The state diagram for the 2-bit counter is shown in Figure 9.39.\nTherearetwobasicoperationsforpredictingabranch\u2013prediction, andtraining. Topredict\nabranch, welookupthevalueofitsprogramcounterinthebranchpredictiontable. Inspecific,\nwe use the last n bits of the address of the pc to access a 2n entry branch predictor table. We\nread the value of the 2-bit saturating counter, and predict the branch on the basis of its value.\nWhen, wehavetherealoutcomeofthebranchavailable, wetrainourpredictorbyincrementing\nor decrementing the value of our counter using saturating arithmetic (as per Table 9.39).\nLet us now see why this predictor works. Let us consider a simple piece of C code, and its\nequivalent SimpleRisc code.\nC\nvoid main(){\nfoo();\n...\nfoo();\n} (cid:13)c Smruti R. Sarangi 452\nTable of 2-bit\nsaturating counters\nn bits from\nthe branch prediction\naddress\nFigure 9.38: A branch prediction table\nnot taken\n00 01\nNot taken Not taken\ntaken\nn\nn e\ne k\nk a\na t\nt t\no\nn\ntaken\n11 10\nTaken Taken\nnot taken\nFigure 9.39: 2-bit saturating counter\nint foo() {\nint i, sum = 0\nfor(i=0; i < 10; i++) {\nsum = sum + i;\n}\nreturn sum;\n}\nSimpleRisc\n.main:\n1\ncall .foo\n2 453 (cid:13)c Smruti R. Sarangi\n...\n3\ncall .foo\n4\n5\n.foo:\n6\nmov r0, 0 \/* sum = 0 *\/\n7\nmov r1, 0 \/* i = 0 *\/\n8\n.loop:\n9\nadd r0, r0, r1 \/* sum = sum + i *\/\n10\nadd r1, r1, 1 \/* i = i + 1 *\/\n11\ncmp r1, 10 \/* compare i with 10 *\/\n12\nbgt .loop \/* if(r1 > 10) jump to .loop *\/\n13\nret\n14\nLet us take a look at the branch in the loop statement (Line 13). For all the iterations\nother than the last one, the branch is taken. If we start our predictor in the state 10, then\nthe first time, the branch is predicted correctly (taken). The counter gets incremented and\nbecomes equal to 11. For each of the subsequent iterations, the branch is predicted correctly\n(taken). However, in the last iteration, it needs to be predicted as not taken. Here, there is a\nmisprediction. The2-bitcounterthusgetsdecremented, andgetssetto10. Letusnowconsider\nthe case when we invoke the function foo again. The value of the 2-bit counter is 10, and the\nbranch (Line 13) is correctly predicted as taken.\nWe thus observe that our 2-bit counter scheme, adds a little bit of hysteresis (or past\nhistory) to the prediction scheme. If a branch has historically been taking one direction, then\none anomaly, does not change the prediction. This pattern is very useful for loops, as we have\nseen in this simple example. The direction of the branch instruction in the last iteration of\na loop is always different. However, the next time we enter a loop, the branch is predicted\ncorrectly, as we have seen in this example. Note that this is only one pattern. There are many\nmore types of patterns that modern branch predictors exploit.\n9.11.2 Multiple Issue In-Order Pipeline\nIn our simple pipeline, we executed only one instruction per cycle. However, this is not a strict\nnecessity. We can design a processor such as the original Intel Pentium that had two parallel\npipelines. This processor could execute two instructions simultaneously in one cycle. These\npipelinehaveextrafunctionalunitssuchthatinstructionsinboththepipelinescanbeexecuted\nwithout any significant structural hazards. This strategy increases the IPC. However, it also\nmakes the processor more complex. Such a processor is said to contain a multiple issue in-order\npipeline, because we can issue multiple instructions to the execution units in the same cycle.\nA processor, which can execute multiple instructions per cycle is also known as a superscalar\nprocessor.\nSecondly,thisprocessorisknownasanin-orderprocessor,becauseitexecutesinstructionsin\nprogramorder. Theprogram orderistheorderofexecutionofdynamicinstancesofinstructions\nastheyappearintheprogram. Forexample,asinglecycleprocessor,orourpipelinedprocessor,\nexecutes instructions in program order. (cid:13)c Smruti R. Sarangi 454\nDefinition 92\nA processor that can execute multiple instructions per cycle is known as a superscalar\nprocessor.\nDefinition 93\nAn in-order processor executes instructions in program order. The program order is defined\nas the order of dynamic instances of instructions that is the same as that is perceived if we\nexecute each instruction of the program sequentially.\nNow, we need to look for dependences and potential hazards across both the pipelines.\nSecondly, the forwarding logic is also far more complex, because results can be forwarded from\neither pipeline. The original Pentium processor released by Intel had two pipelines namely the\nU pipe and the V pipe. The U pipe could execute any instruction, whereas the V pipe was\nlimited to only simple instructions. Instructions were fetched as 2-instruction bundles. The\nearlier instruction in the bundle was sent to the U pipe, and the later instruction was sent to\nthe V pipe. This strategy allowed the parallel execution of those instructions.\nLet us try to conceptually design a simple processor on the lines of the original Pentium\nprocessor with two pipelines \u2013 U and V. We envisage a combined instruction and operand fetch\nunit that forms 2-instruction bundles, and dispatches them to both the pipelines for execution\nsimultaneously. However, if the instructions do not satisfy some constraints, then this unit\nforms a 1-instruction bundle, and sends it to the U pipeline. Whenever, we form such bundles,\nwe can broadly adhere to some generic rules. We should avoid having two instructions that\nhave a RAW dependence. In this case, the pipeline will stall.\nSecondly,weneedtobeparticularlycarefulaboutmemoryinstructionsbecausedependences\nacross them cannot be discovered till the end of the EX stage. Let us assume that the first\ninstruction in a bundle is a store instruction, and the second instruction is a load instruction,\nand they happen to access the same memory address. We need to detect this case, at the end\nof the EX stage, and forward the value from the store to the load. For the reverse case, when\nthe first instruction is a load instruction, and the second is a store to the same address, we need\nto stall the store instruction till the load completes. If both the instructions in a bundle store\nto the same address, then the earlier instruction is redundant, and can be converted into a nop.\nWe thus need to design a processor that adheres to these rules, and has a complex interlock\nand forwarding logic.\nLet us show a simple example.\nExample 118\nDraw a pipeline diagram for the following SimpleRisc assembly code assuming a 2 issue\nin-order pipeline. 455 (cid:13)c Smruti R. Sarangi\n[1]: add r1, r2, r3\n[2]: add r4, r5, r6\n[3]: add r9, r8, r8\n[4]: add r10, r9, r8\n[5]: add r3, r1, r2\n[6]: ld r6, 10[r1]\n[7]: st r6, 10[r1]\nAnswer: Here, the pipeline diagram contains two entries for each stage, because two\ninstructions can be in a stage at the same time. We start out by observing that we can\nexecute instructions [1] and [2] in parallel. However, we cannot execute instructions [3]\nand [4] in parallel. This is because instruction [3] writes to r9, and instruction [4] has r9\nas a source operand. We cannot execute both the instructions in the same cycle, because\nthe value of r9 is produced in the EX stage, and is also required in the EX stage. We thus\ninsert a bubble. We proceed to execute [4], and [5] in parallel. We can use forwarding to\nget the value of r9 in the case of instruction [4]. Lastly, we observe that we cannot execute\ninstructions [6] and [7] in parallel. They access the same memory address. The load needs\nto complete before the store starts. We thus insert another bubble.\nbubble Clock cycles\n1 2 3 4 5 6 7 8 9\nIF 1 3 4 6 7\nIF 2 5\n[1]: add r1, r2, r3\nOF 1 3 4 6 7\n[2]: add r4, r5, r6\n[3]: add r9, r8, r8 OF 2 5\n[4]: add r10, r9, r8 EX 1 3 4 6 7\n[5]: add r3, r1, r2 EX 2 5\n[6]: ld r6, 10[r1] MA 1 3 4 6 7\n[7]: st r6, 10[r1] MA 2 5\nRW 1 3 4 6 7\nRW 2 5\n9.11.3 EPIC and VLIW Processors\nNow, instead of preparing bundles in hardware, we can prepare them in software. The com-\npiler has far more visibility into the code, and can perform extensive analyses to create multi-\ninstruction bundles. The Itanium(cid:13)R processor designed by Intel and HP was a very iconic\nprocessor, which was based on similar principles.\nLet us first start out by defining the terms \u2013 EPIC and VLIW. (cid:13)c Smruti R. Sarangi 456\nDefinition 94\nVLIW \u2192 Very Long Instruction Word: Compilers create bundles of instructions that\ndo not have dependences between them. The hardware executes the instructions in each\nbundle in parallel. The complete onus of correctness is on the compiler. EPIC \u2192 Explicitly\nParallel Instruction Computing: This paradigm extends VLIW computing. However, in\nthis case the hardware ensures that the execution is correct regardless of the code generated\nby the compiler.\nEPIC\/VLIW processors require very smart compilers to analyse programs and create bun-\ndles of instructions. For example, if a processor has 4 pipelines, then each bundle contains 4\ninstructions. The compilers create bundles such that there are no dependences across instruc-\ntions in a bundle. The broader aim of designing EPIC\/VLIW processors is to move all the\ncomplexity to software. Compilers arrange the bundles in a way such that we can minimise the\namount of interlock, forwarding, and instruction handling logic required in the processor.\nHowever, in hindsight, such processors failed to deliver on their promise because the hard-\nware could not be made as simple as the designers had originally planned for. A high per-\nformance processor still needed a fair amount of complexity in hardware, and required some\nsophisticated architectural features. These features increased the complexity and power con-\nsumption of hardware.\n9.11.4 Out-of-Order Pipelines\nWe have up till now been considering primarily in-order pipelines. These pipelines execute\ninstructions in the order that they appear in the program. This is not strictly necessary. Let\nus consider the following code snippet.\n[1]: add r1, r2, r3\n[2]: add r4, r1, r1\n[3]: add r5, r4, r2\n[4]: mul r6, r5, r2\n[5]: div r8, r9, r10\n[6]: sub r11, r12, r13\nHere, we are constrained to execute instructions 1 to 4 in sequence because of data de-\npendences. However, we can execute instructions, 5 and 6 in parallel, because they are not\ndependent on instructions 1-4. We will not be sacrificing on correctness if we execute instruc-\ntions 5 and 6 out-of-order. For example, if we can issue two instructions in one cycle, then\nwe can issue (1,5) together, then (2,6), and finally, instructions 3, and 4. In this case, we can\nexecute the sequence of 6 instructions in 4 cycles by executing 2 instructions for the first two\ncycles. Recall that such a processor that can potentially execute multiple instructions per cycle\nis known as a superscalar processor (see Definition 92). 457 (cid:13)c Smruti R. Sarangi\nDefinition 95\nA processor that can execute instructions in an order that is not consistent with their pro-\ngram order is known as an out-of-order(OOO) processor.\nAn out-of-order(OOO) processor fetches instructions in-order. After the fetch stage, it\nproceeds to decode the instructions. Most real world instructions require more than one cycle\nfor decoding. These instructions are simultaneously added to a queue called the reorder buffer\n(ROB) in program order. After decoding the instruction, we need to perform a step called\nregister renaming. The broad idea is as follows. Since we are executing instructions out of\norder, we can have WAR and WAW hazards. Let us consider the following code snippet.\n[1]: add r1, r2, r3\n[2]: sub r4, r1, r2\n[3]: add r1, r5, r6\n[4]: add r9, r1, r7\nIf we execute instructions [3] and [4] before instruction [1], then we have a potential WAW\nhazard. This is because instruction [1] might overwrite the value of r1 written by instruction\n[3]. This will lead to an incorrect execution. Thus, we try to rename the registers such that\nthese hazards can be removed. Most modern processors define a set of architectural registers,\nwhicharethesameastheregistersexposedtosoftware(assemblyprograms). Additionally,they\nhave a set of physical registers that are only visible internally. The renaming stage converts\narchitecturalregisternamestophysicalregisternames. ThisisdonetoremoveWARandWAW\nhazards. TheonlyhazardsthatremainatthisstageareRAWhazards,whichindicateagenuine\ndata dependency. The code snippet will thus look as follows after renaming. Let us assume\nthat the physical registers range from p1...p128.\n[1]: add p1, p2, p3 \/* p1 contains r1 *\/\n[2]: sub p4, p1, p2\n[3]: add p100, p5, p6 \/* r1 is now begin saved in p100 *\/\n[4]: add p9, p100, p7\nWe have removed the WAW hazard by mapping r1 in instruction 3, to p100. The only\ndependences that exist are RAW dependences between instructions [1] \u2192 [2], and [3] \u2192 [4].\nTheinstructionsafterrenamingenteraninstructionwindow. Notethatuptillnowinstructions\nhave been proceeding in-order.\nThe instruction window or instruction queue typically contains 64-128 entries (refer to\nFigure 9.40). For each instruction, it monitors its source operands. Whenever all the source\noperands of an instruction are ready, the instruction is ready to be issued to its corresponding\nfunctional unit. It is not necessary for instructions to access the physical register file all the\ntime. They can also get values from forwarding paths. After the instructions finish their\nexecution, they broadcast the value of their result to the waiting instructions in the instruction\nwindow. Instructions waiting for the result, mark their corresponding source operand as ready. (cid:13)c Smruti R. Sarangi 458\nInstruction Functional units Load store queue\nwindow\nRegister\nFetch Decode\nrename Memory\nsystem\nRegister file\nMemory\nsystem Reorder buffer\nFigure 9.40: An out-of-order pipeline\nThis process is known as instruction wakeup. Now, it is possible that multiple instructions are\nready in the same cycle. To avoid structural hazards, an instruction select unit chooses a set\nof instructions for execution.\nWe need another structure for load and store instructions known as the load-store queue. It\nsaves the list of loads and stores in program order. It allows loads to get their values through\nan internal forwarding mechanism if there is an earlier store to the same address.\nAfter an instruction finishes its execution, we mark its entry in the reorder buffer. Instruc-\ntions leave the reorder buffer in program order. If an instruction does not finish quickly for\nsome reason, then all the instructions after it in the reorder buffer need to stall. Recall that\ninstructionentriesinthereorderbufferareorderedinprogramorder. Instructionsneedtoleave\nthe reorder buffer in program order such that we can ensure precise exceptions.\nTosummarise, themainadvantageofanout-of-orderprocessor(OOO)isthatitcanexecute\ninstructions that do not have any RAW dependences between them, in parallel. Most programs\ntypically have such sets of instructions at most points of time. This property is known as\ninstruction level parallelism (abbreviated as ILP). Modern OOO processors are designed to\nexploit as much of ILP as possible.\nDefinition 96\nTypically, most programs have multiple instructions in a pipeline that can be executed in\nparallel. This is because they do not have any RAW dependences between them. Modern\nsuperscalar processors exploit this fact to increase their IPC by executing multiple instruc-\ntions in the same cycle. This property of a program is known as instruction level parallelism\n(abbreviated as ILP).\ntceleS\npuekaW 459 (cid:13)c Smruti R. Sarangi\n9.12 Summary and Further Reading\n9.12.1 Summary\nSummary 9\n1. We observe that large parts of our basic SimpleRisc processor are idle while processing\nan instruction. For example, the IF stage is idle, when the instruction is in the MA\nstage.\n2. We thus propose the notion of \u201cpipelining\u201d. Here, we execute 5 instructions simul-\ntaneously (1 in each stage). At the negative edge of the clock, all the instructions\nproceed to the next stages simultaneously, the instruction in the RW stage completes\nits execution, and a new instruction enters the IF stage.\n3. To design a pipeline we split the data path into five parts (1 stage per part), and add\npipeline registers between subsequent stages. A pipeline register stores the instruc-\ntion packet (instruction contents, control signals, source operands and intermediate\nresults).\n4. Each pipeline stage reads operands for its functional units from its corresponding\npipeline register at the beginning of a clock cycle. It processes them, and writes the\nresults to the pipeline register between the given stage and its adjacent stage, before\nthe end of the clock cycle.\n5. We can have RAW hazards, and control hazards in our pipeline because we cannot\nascertain data dependences and branch outcomes before fetching subsequent instruc-\ntions.\n6. We can avoid RAW, and control hazards using pure software solutions. We can intro-\nduce nop instructions between producer and consumer instructions, and after branch\ninstructions. Alternatively, we can reorder instructions to minimise the addition of\nnop instructions, and use delayed branching.\n7. In the absence of software solutions, we can use pipeline interlocks to avoid hazards\nby stalling and cancelling instructions.\n8. An efficient method of minimising stall cycles is forwarding.\n(a) If a later stage contains the value of an operand, then we can forward the value\nfrom the producer stage to the consumer stage. We can thus bypass the register\nfile.\n(b) This allows us to avoid hazards because a consumer instruction can quickly get\nits operands from other pipeline stages. (cid:13)c Smruti R. Sarangi 460\n(c) To detect dependences, and implement forwarding we propose a dedicated for-\nwarding unit. Furthermore, it is necessary to augment, every functional unit\nwith multiplexers to choose between the default inputs, and forwarded inputs.\nForwarding eliminates all the data hazards, other than the load-use hazard.\n9. Modern processors have interrupts and exceptions that require us to save the state\nof a program, and branch to an interrupt handler. We need to implement precise\nexceptions such that we can return to the exact same point at which we had stopped\nthe execution of our original program.\n10. Performance of a processor with respect to a program is defined to be proportional to\nthe inverse of the time required to execute the program.\n11. The performance equation is as follows:\nIPC \u00d7f\nP \u221d\n#insts\nIPC (instructions per cycle), f(frequency), #insts (number of dynamic instructions)\n12. The performance of a processor is dependent on the manufacturing technology, ar-\nchitecture, and compiler optimisations. In specific, a pipelined processor has higher\nperformance as compared to a single cycle processor, because it allows us to increase\nthe frequency roughly as many times as the number of stages. There is a consequent\nloss in IPC, and wastage of time due to the latch delay. Hence, it is necessary to\nchoose an optimal pipelining strategy.\n13. The clock frequency is limited by power and temperature constraints.\n(a) There are two power dissipation mechanisms in modern processors namely dy-\nnamic power and leakage power. Dynamic power is dissipated due to the switch-\ning activity in circuits. It is proportional to \u03b1CV2f, where \u03b1 is the activity\nfactor, C is the lumped circuit capacitance, V is the supply voltage, and f is the\nfrequency.\n(b) Leakage power or static power is dissipated due to the flow of current through\nthe terminals of a transistor, when it is in the off state. Leakage power is a\nsuperlinear function of the current temperature.\n(c) Power and temperature for different points on a chip are typically related by a\nset of linear equations.\n(d) Dynamic voltage-frequency scaling is a technique to dynamically modify the volt-\nage and frequency of a processor. We typically assume that the frequency is\nproportional to voltage.\n(e) We use the ED2 metric to simultaneously compare the power and performance\nof competing processor designs.\n14. Some advanced techniques for speeding up a processor are branch prediction, super-\nscalar execution, EPIC\/VLIW processors, and out-of-order pipelines. 461 (cid:13)c Smruti R. Sarangi\n9.12.2 Further Reading\nThe design of high performance pipelines is a prime focus of computer architecture researchers.\nResearchers mostly look at optimising performance of pipelines and simultaneously reducing\npower consumption. The reader can start out with textbooks on advanced computer architec-\nture [Hennessy and Patterson, 2012, Hwang, 2003, Baer, 2010, Sima et al., 1997, Culler et al.,\n1998]. After getting a basic understanding of the techniques underlying advanced processors\nsuchasout-of-orderandsuperscalarexecution, thereadershouldbeabletograduatetoreading\nresearchpapers. Thefirststepinthisjourneyshouldbethebooktitled,\u201cReadingsinComputer\nArchitecture\u201d [Hill et al., 1999]. This book comprises of a set of foundational research papers\nin different areas of computer architecture. Subsequently, the reader can move on to reading\nresearch papers for getting a deeper understanding of state of the art techniques in processor\ndesign.\nThe reader may start with some of the basic papers in the design of out-of-order proces-\nsors [Brown et al., 2001, Smith and Sohi, 1995, Hwu and Patt, 1987]. After getting a basic\nunderstanding, she can move on to read papers that propose important optimisations such as\n[Brown et al., 2001, Petric et al., 2005, Akkary et al., 2003]. For a thorough understanding of\nbranch prediction schemes and fetch optimisation, the reader should definitely look at the work\nof Yeh and Patt [Yeh and Patt, 1991, Yeh and Patt, 1992, Yeh and Patt, 1993], and the patent\non Pentium 4 trace caches [Krick et al., 2000].\nSimultaneously, the reader can also look at papers describing the complete architecture of\nprocessors such as the Intel Pentium 4 [Boggs et al., 2004], Intel ATOM [Halfhill, 2008], Intel\nSandybridge [Gwennap, 2010], AMD Opteron [Keltcher et al., 2003], and IBM Power 7 [Ware\netal.,2010]. Finally,readerscanfinddescriptionsofstateoftheartprocessorsintheperiodical,\n\u201cMicroprocessor Report\u201d, along with emerging trends in the processor industry.\nExercises\nPipeline Stages\nEx. 1 \u2014 Show the design of the IF, OF, EX, MA, and RW pipeline stages. Explain their\nfunctionality in detail.\nEx. 2 \u2014 Why do we need to store the op2 field in the instruction packet? Where is it used?\nEx. 3 \u2014 Why is it necessary to have the control field in the instruction packet?\nEx. 4 \u2014 Why do we require latches in a pipeline? Why are edge sensitive latches preferred?\nEx. 5 \u2014 Whyisitnecessarytosplittheworkinadatapathevenlyacrossthepipelinestages?\n* Ex. 6 \u2014 We know that in an edge sensitive latch, the input signal has to be stable for\nt units of time after the negative edge. Let us consider a pipeline stage between latches L\nhold 1\nand L . Suppose the output of L is ready immediately after the negative edge, and almost\n2 1 (cid:13)c Smruti R. Sarangi 462\ninstantaneously reaches the input of L . In this case, we violate the hold time constraint at L .\n2 2\nHow can this situation be avoided?\nPipeline Design\nEx. 7 \u2014 Enumerate the rules for constructing a pipeline diagram.\nEx. 8 \u2014 Describe the different types of hazards in a pipeline.\nEx. 9 \u2014 In the SimpleRisc pipeline, why don\u2019t we have structural hazards?\nEx. 10 \u2014 Why does a branch have two delay slots in the SimpleRisc pipeline?\nEx. 11 \u2014 What are the Data-Lock and Branch-Lock conditions?\nEx. 12 \u2014 Write pseudo-code for detecting and handling the Branch-Lock condition? (with-\nout delayed branches)\nEx. 13 \u2014 What is delayed branching?\n* Ex. 14 \u2014 Let us consider two designs: D and D . D uses a software-based approach for\n1 2 1\nhazards, and assumes delayed branching. D uses interlocks, and assumes that a branch is not\n2\ntaken till the outcome is decided. Intuitively, which design is faster?\nEx. 15 \u2014 Assume that 20% of the dynamic instructions executed on a computer are branch\ninstructions. We use delayed branching with one delay slot. Estimate the CPI, if the compiler\nis able to fill 85% of the delay slots. Assume that the base CPI is 1.5. In the base case, we do\nnot use any delay slot. Instead, we stall the pipeline for the total number of delay slots.\nEx. 16 \u2014 Describe the role of the forwarding multiplexers in each stage of the pipeline.\nEx. 17 \u2014 Why do we not require a forwarding path from MA to EX for the op2 field?\nEx. 18 \u2014 Answer the following questions.\ni) What are the six possible forwarding paths in our SimpleRisc processor?\nii) Which four forwarding paths, are required, and why? (Give examples to support your\nanswer).\nEx. 19 \u2014 Assume that we have an instruction immediately after a call instruction that reads\nra. We claim that this instruction will get the correct value of ra in a pipeline with forwarding.\nIs this true? Prove your answer.\nEx. 20 \u2014 Reorderthefollowingcodesnippettominimisetheexecutiontimeforthefollowing\nconfigurations:\n1.We use software techniques, and have 2 delay slots.\n2.We use interlocks, and predict not taken.\n3.We use forwarding, and predict not taken. 463 (cid:13)c Smruti R. Sarangi\nadd r1, r2, r3\nsub r4, r1, r1\nmul r8, r9, r10\ncmp r8, r9\nbeq .foo\nEx. 21 \u2014 Reorder the following code snippet to minimise execution time for the following\nconfigurations:\n1.We use software techniques, and have 2 delay slots.\n2.We use interlocks, and predict not taken.\n3.We use forwarding, and predict not taken.\nadd r4, r3, r3\nst r3, 10[r4]\nld r2, 10[r4]\nmul r8, r9, r10\ndiv r8, r9, r10\nadd r4, r2, r6\nEx. 22 \u2014 Answer the following:\nadd r1, r2, r3\nsub r4, r1, r6\nld r5, 10[r4]\nadd r6, r5, r5\nsub r8, r8, r9\nmul r10, r10, r11\ncmp r8, r10\nbeq .label\nadd r5, r6, r8\nst r3, 20[r5]\nld r6, 20[r5]\nld r7, 20[r6]\nlsl r7, r7, r10\ni) AssumingatraditionalSimpleRisc pipeline,howmanycycleswillthiscodetaketoexecute\nin a pipeline with just interlocks? Assume that time starts when the first instruction\nreaches the RW stage. This means that if we had just one instruction, then it would have\ntaken exactly 1 cycle to execute (Not 5). Moreover, assume that the branch is not taken.\n[Assumptions: No forwarding, No delayed branches, No reordering]\nii) Now, compute the number of cycles with forwarding (no delayed branches, no reordering).\niii) Compute the minimum number of cycles when we have forwarding, and we allow instruc-\ntion reordering. We do not have delayed branches, and in the reordered code, the branch\ninstruction cannot be one of the last three instructions.\niv) Compute the minimum number of cycles when we have forwarding, allow instruction re-\nordering, and have delayed branches. Here, again, we are not allowed to have the branch\ninstruction as one of the last three instructions in the reordered code. (cid:13)c Smruti R. Sarangi 464\n** Ex. 23 \u2014 We have assumed up till now that each memory access requires one cycle. Now,\nlet us assume that each memory access takes two cycles. How will you modify the data path\nand the control path of the SimpleRisc processor in this case.\n** Ex. 24 \u2014 Assumeyouhaveapipelinethatcontainsavaluepredictorformemory. Ifthere\nis a miss in the L2 cache, then we try to predict the value and supply it to the processor. Later\nthis value is compared with the value obtained from memory. If the value matches, then we are\nfine, else we need to initiate a process of recovery in the processor and discard all the wrong\ncomputation. Design a scheme to do this effectively.\nPerformance and Power Modelling\nEx. 25 \u2014 If we increase the average CPI (Cycles per Instruction) by 5%, decrease the in-\nstruction count by 20% and double the clock rate, what is the expected speedup, if any, and\nwhy?\nEx. 26 \u2014 Whatshouldbetheidealnumberofpipelinestages(x)foraprocessorwithCPI =\n(1+0.2x) and clock cycle time t = (1+50\/x)?\nclk\nEx. 27 \u2014 What is the relationship between dependences in a program, and the optimal\nnumber of pipeline stages it requires?\nEx. 28 \u2014 Is a 4 GHz machine faster than a 2 GHz machine? Justify your answer.\nEx. 29 \u2014 How do the manufacturing technology, compiler, and architecture determine the\nperformance of a processor?\nEx. 30 \u2014 Define dynamic power and leakage power.\n* Ex. 31 \u2014 We claim that if we increase the frequency, the leakage power increases. Justify\nthis statement.\nEx. 32 \u2014 What is the justification of the ED2 metric?\n* Ex. 33 \u2014 How do power and temperature considerations limit the number of pipeline\nstages? Explain your answer in detail. Consider all the relationships between power, tem-\nperature, activity, IPC, and frequency that we have introduced in this chapter.\n* Ex. 34 \u2014 Define the term DVFS.\n** Ex. 35 \u2014 Assumethatwewishtoestimatethetemperatureatdifferentpointsofaproces-\nsor. We know the dynamic power of different components, and the leakage power as a function\nof temperature. Furthermore, we divide the surface of the die into a grid as explained in Sec-\ntion 9.10.4. How do we use this information to arrive at a steady state value of temperature\nfor all the grid points? 465 (cid:13)c Smruti R. Sarangi\nInterrupts and Exceptions\nEx. 36 \u2014 What are precise exceptions? How does hardware ensure that every exception is a\nprecise exception?\nEx. 37 \u2014 Why do we need the movz and retz instructions?\nEx. 38 \u2014 List the additional registers that we add to a pipeline to support interrupts and\nexceptions.\nEx. 39 \u2014 What is the role of the CPL register? How do we set and reset it?\nEx. 40 \u2014 How do we locate the correct interrupt handler? What is the structure and role of\nan interrupt handler?\nEx. 41 \u2014 Why do we need the registers oldPC, and oldSP?\nEx. 42 \u2014 Why do we need to add a flags field to the instruction packet? How do we use\nthe oldFlags register?\n* Ex. 43 \u2014 Consider a hypothetical situation where a write back to a register may generate\nanexception(register-faultexception). Proposeamechanismtohandlethisexceptionprecisely.\n* Ex. 44 \u2014 Define the concept of register windows. How can we use register windows to\nspeedup the implementation of functions?\nAdvanced Topics\nEx. 45 \u2014 Can you intuitively say why most of the branches in programs are predictable.\nEx. 46 \u2014 Is the following code sequence amenable to branch prediction. Why or why not?\nint status=flip_random_unbiased_coin();\nif (status==Head)\nprint(\\head\");\nelse\nprint(\\tail\");\nEx. 47 \u2014 We need to design a 2-issue inorder pipeline that accepts a bundle of two instruc-\ntions every cycle. These bundles are created by the compiler.\n(a) Given the different instruction types, design an algorithm that tells the compiler the\ndifferent constraints in designing a bundle. For example, you might decide that you don\u2019t\nwant to have two instructions in a bundle if they are of certain types, or have certain\noperands.\n(b) To implement a two issue pipeline, what kind of additional functionality will you need in\nthe MEM stage? (cid:13)c Smruti R. Sarangi 466\nEx. 48 \u2014 Describe the main insight behind out-of-order pipelines? What are their major\nstructures?\nDesign Problems\nEx. 49 \u2014 Implement a basic pipelined processor with interlocks using Logisim (refer to the\ndesign problems in Chapter 8).\nEx. 50 \u2014 Implement a basic pipelined processor in a hardware description language such as\nVerilog or VHDL. Try to add forwarding paths and interrupt processing logic.\nEx. 51 \u2014 Learn the language SystemC. It is used to model hardware at a high level. Imple-\nment the SimpleRisc pipeline in SystemC. Part III\nOrganisation: System Design\n467  10\nThe Memory System\nUp till now, we have considered the memory system to be one large array of bytes. This\nabstraction was good enough for designing an instruction set, studying assembly language, and\neven for designing a basic processor with a complicated pipeline. However, from a practical\nstandpoint, this abstraction will need to be further refined to design a fast memory system. In\nour basic SimpleRisc pipeline presented in Chapter 8 and 9, we have assumed that it takes 1\ncycle to access both data and instruction memory. We shall see in this chapter, that this is\nnot always true. In fact, we need to make significant optimisations in the memory system to\ncome close to the ideal latency of 1 cycle. We need to introduce the notion of a \u201ccache\u201d and a\nhierarchical memory system to solve the dual problems of having large memory capacity, and\nlow latency.\nSecondly, up till now we have been assuming that only one program runs on our system.\nHowever, most processors typically run multiple programs on a time shared basis. For example,\nif there are two programs, A and B, a modern desktop or laptop typically runs program A for\na couple of milliseconds, executes B for a few milliseconds, and subsequently switches back and\nforth. In fact as your author is writing this book, there are a host of other programs running on\nhissystemsuchasawebbrowser,anaudioplayer,andacalendarapplication. Ingeneral,auser\ndoes not perceive any interruptions, because the time scale at which the interruptions happen\nis much lower than what the human brain can perceive. For example, a typical video displays a\nnew picture 30 times every second, or alternatively one new picture every 33 milliseconds. The\nhuman brain creates the illusion of a smoothly moving object by piecing the pictures together.\nIf the processor finishes the job of processing the next picture in a video sequence, before 33\nmilliseconds, then it can execute a part of another program. The human brain will not be\nable to tell the difference. The point here is that without our knowledge, the processor in\nco-operation with the operating system switches between multiple programs many many times\na second. The operating system is itself a specialised program that helps the processor manage\nitself, and other programs. Windows and Linux are examples of popular operating systems.\nWe shall see that we require special support in the memory system to support multiple\n469 (cid:13)c Smruti R. Sarangi 470\nprograms. If we do not have this support, then multiple programs can overwrite each other\u2019s\ndata, which is not desired behavior. Secondly, we have been living with the assumption that we\nhave practically an infinite amount of memory. This is also not true. The amount of memory\nthat we have is finite, and it can get exhausted by large memory intensive programs. Hence, we\nshould have a mechanism to still run such large programs. We shall introduce the concept of\nvirtual memory to solve both of these issues \u2013 running multiple programs, and handling large\nmemory intensive programs.\nTo summarise, we observe that we need to design a memory system that is fast, and is\nflexible enough to support multiple programs with very large memory requirements.\n10.1 Overview\n10.1.1 Need for a Fast Memory System\nLet us now look at the technological requirements for building a fast memory system. We\nhave seen in Chapter 6 that we can design memory elements with four kinds of basic circuits\n\u2013 latches, SRAM cells, CAM cells and DRAM cells. There is a tradeoff here. Latches and\nSRAM cells are much faster than DRAM or CAM cells. However, as compared to a DRAM\ncell, a latch, CAM or SRAM cell is an order of magnitude larger in terms of area, and also\nconsumes much more power. We observe that a latch is designed to read in and read out data\nat a negative clock edge. It is a fast circuit that can store and retrieve data in a fraction of\na clock cycle. On the other hand, an SRAM cell is typically designed to be used as a part of\na large array of SRAM cells along with a decoder and sense amplifiers. With this additional\noverhead, an SRAM cell is typically slower than a typical edge triggered latch. In comparison,\nCAM cells are best for memories that are content associative, and DRAM cells are best for\nmemories that have very large capacities.\nNow, our SimpleRisc pipeline assumes that memory accesses take 1 cycle. To satisfy this\nrequirement, we need to build our entire memory from latches, or small arrays of SRAM cells.\nTable 10.1 shows the size of a typical latch, SRAM cell, and DRAM cell as of 2012.\nCell type Area Typical latency\n(array of cells)\nMaster Slave D flip flop 0.8 \u00b5m2 fraction of a cycle\nSRAM cell 0.08 \u00b5m2 1-5 cycles\nDRAM Cell 0.005 \u00b5m2 50-200 cycles\nTable 10.1: Sizes of a Latch, SRAM cell, and DRAM cell\nWe observe that a typical latch (master slave D flip flop) is 10 times larger than an SRAM\ncell, whichinturnisaround16timeslargerthanaDRAMcell. Thismeansthatgivenacertain\namount of silicon, we can save 160 times more data if we use DRAM cells. However, DRAM\nmemory is also 200 times slower (if we consider a representative array of DRAM cells). Clearly,\nthere is a tradeoff between capacity, and speed. The sad part is that we actually need both.\nLet us consider the issue of capacity first. Due to several constraints in technology and\nmanufacturability, as of 2012, it is not possible to manufacture chips with an area more than 471 (cid:13)c Smruti R. Sarangi\n400-500mm2 [ITRS, 2011]. Consequently, the total amount of memory that we can have on\nchip is limited. It is definitely possible to supplement the amount of available memory with\nadditional chips exclusively containing memory cells. Keep in mind that off-chip memory is\nslow,andittakestensofcyclesfortheprocessortoaccesssuchmemorymodules. Toachieveour\ngoal of having a 1-cycle memory access, we need to use the relatively faster on-chip memory\nmost of the time. Here, also our options are limited. We cannot afford to have a memory\nsystem consisting exclusively of latches. For a large number of programs, we will not be able\nto fit all our data in memory. For example, modern programs typically require hundreds of\nmegabytes of memory. Moreover, some large scientific programs require gigabytes of memory.\nSecond, it is difficult to integrate large DRAM arrays along with a processor on the same chip\ndue to technological constraints. Hence, designers are compelled to use large SRAM arrays for\non-chip memories. As shown in Table 10.1 SRAM cells(arrays) are much larger than DRAM\ncells(arrays), and thus have much less capacity.\nThere is a conflicting requirement of latency. Let us assume that we decide to maximise\nstorage, and make our memory entirely consisting of DRAM cells. Let us assume a 100 cy-\ncle latency for accessing DRAM. If we assume that a third of our instructions are memory\ninstructions, then the effective CPI of a perfect 5 stage SimpleRisc pipeline is calculated to\nbe 1+1\/3\u00d7(100\u22121) = 34. The point to note is that our CPI increases by 34X, which is\ncompletely unacceptable.\nHence, we need to make an equitable tradeoff between latency and storage. We want to\nstore as much of data as possible, but not at the cost of a very low IPC. Unfortunately, there is\nno way out of this situation, if we assume that our memory accesses are completely random. If\nthere is some pattern in memory accesses, then we can possibly do something better such that\nwe can get the best of both worlds \u2013 high storage capacity, and low latency.\n10.1.2 Memory Access Patterns\nBefore considering the technical topic of patterns in memory accesses, let us consider a simple\npractical problem that your author is facing at this point of time. He unfortunately, has a lot of\nbooks on his desk that are not organised. Not only are these books cluttering up his desk, it is\nalso hard to search for a book when required. Hence, he needs to organise his books better and\nalso keep his desk clean. He observes that he does not require all the books all the time. For\nexample, he needs books on computer architecture very frequently; however, he rarely reads his\nbooks on distributed systems. Hence, it makes sense for him to move his books on distributed\nsystems to the shelf beside his desk. Unfortunately, it is a small shelf, and there are still a lot\nof books on his desk. He observes that he can further classify the books in the small shelf. He\nhas some books on philosophy that he never reads. These can be moved to the large cabinet\nin the corner of the room. This will create more space in the shelf, and also help him clean up\nhis desk. What is the fundamental insight here? It is that your author does not read all his\nbooks with the same frequency. There are some books that he reads very frequently; hence,\nthey need to be on his desk. Then there is another class of books that he reads infrequently;\nhence, they need to be in the small shelf beside his desk. Lastly, he has a large number of books\nthat he reads extremely infrequently. He can safely keep them in the large cabinet. Pattern\n1:He reads a small set of books very frequently, and the rest of the books rather\ninfrequently. Hence, if he keeps the frequently accessed set of books on computer architecture (cid:13)c Smruti R. Sarangi 472\non his desk, and the large infrequent set of books in the shelf and the cabinet, he has solved\nhis problems.\nWell, not quite. This was true for last semester, when he was teaching the computer\narchitecture course. However, in the current semester, he is teaching a course on distributed\nsystems. Hence, he does not refer to his architecture books anymore. It thus makes sense for\nhim to bring his distributed systems books to his desk. However, there is a problem. What\nhappens to his architecture books that are already there on his desk. Well, the simple answer\nis that they need to be moved to the shelf and they will occupy the slots vacated by the\ndistributed systems books. In the interest of time, it makes sense for your author to bring a\nset of distributed systems books on to his desk, because in very high likelihood, he will need\nto refer to numerous books in that area. It does not make sense to fetch just one book on\ndistributed systems. Hence, as a general rule we can conclude that if we require a certain book,\nthen most likely we will require other books in the same subject. Pattern 2:If your author\nrequires a certain book, then most likely he will require other books in the same\nsubject area in the near future.\nWe can think of patterns 1 and 2, as general laws that are applicable to everybody. Instead\nof books, if we consider TV channels, then also both the patterns apply. We do not watch all\nTV channels equally frequently. Secondly, if a user has tuned in to a news channel, then most\nlikely she will browse through other news channels in the near future. In fact this is how retail\nstores work. They typically keep spices and seasonings close to vegetables. This is because it is\nhighly likely that a user who has just bought vegetables will want to buy spices also. However,\nthey keep bathroom supplies and electronics far away.\nPattern 1 is called temporal locality. This means that users will tend to reuse the same item\nin a given time interval. Pattern 2 is called spatial locality. It means that if a user has used a\ncertain item, then she will tend to use similar items in the near future.\nDefinition 97\nTemporal Locality It is a concept that states that if a resource is accessed at some point\nof time, then most likely it will be accessed again in a short time interval.\nSpatial Locality It is a concept that states that if a resource is accessed at some point of\ntime, then most likely similar resources will be accessed in the near future.\nThe question that we need to ask is \u2013 \u201cIs there temporal and spatial locality in memory\naccesses?\u201d. Ifthereissomedegreeoftemporalandspatiallocality,thenwecanpossiblydosome\ncritical optimisations that will help us solve the twin problems of large memory requirement,\nandlowlatency. Incomputerarchitecture,wetypicallyrelyonsuchpropertiessuchastemporal\nand spatial locality to solve our problems. 473 (cid:13)c Smruti R. Sarangi\n10.1.3 Temporal and Spatial Locality of Instruction Accesses\nThe standard approach for tackling this problem, is to measure and characterise locality in a\nrepresentative set of programs such as the SPEC benchmarks(see Section 9.9.4). Let us first\nstart out by dividing memory accesses into two broad types \u2013 instruction and data. Instruction\naccesses are much easier to analyse informally. Hence, let us look at it first.\nLet us consider a typical program. It has assignment statements, decision statements\n(if,else), and loops. Most of the code in large programs is part of loops or some pieces of\ncommon code. There is a standard rule of thumb in computer architecture, which states that\n90% of the code runs for 10% of time, and 10% of the code runs for 90% of the time. Let us\nconsider a word processor. The code to process the user\u2019s input, and show the result on the\nscreen runs much more frequently than the code for showing the help screen. Similarly, for\nscientific applications, most of the time is spent in a few loops in the program. In fact for most\ncommon applications, we find this pattern. Hence, computer architects have concluded that\ntemporal locality for instruction accesses holds for an overwhelming majority of programs.\nLet us now consider spatial locality for instruction accesses. If there are no branch state-\nments, then the next program counter is the current program counter plus 4 bytes for an ISA\nsuch as SimpleRisc . We consider two accesses to be \u201csimilar\u201d, if their memory addresses are\nclose to each other. Clearly, we have spatial locality here. A majority of the instructions in\nprograms are non-branches; hence, spatial locality holds. Moreover, a nice pattern in branches\ninmostprograms isthatthebranchtarget isactuallynotveryfar away. Ifweconsidera simple\nif-then statement or for loop then the distance of the branch target is equal to the length of\nthe loop or the if part of the statement. In most programs this is typically 10 to 100 instruc-\ntions long, definitely not thousands of instructions long. Hence, architects have concluded that\ninstruction memory accesses exhibit a good amount of spatial locality also.\nThe situation for data accesses is slightly more complicated; however, not very different.\nFor data accesses also we tend to reuse the same data, and access similar data items. Let us\nlook at this in more detail.\n10.1.4 Characterising Temporal Locality\nLet us describe a method called the method of stack distances to characterise temporal locality\nin programs.\nStack Distance\nWe maintain a stack of accessed data addresses For each memory instruction (load\/store), we\nsearch for the corresponding address in the stack. The position at which the entry is found (if\nfound) is termed the \u201dstack distance\u201d. Here, the distance is measured from the top of the stack.\nThe top of the stack has distance equal to zero, whereas the 100th entry has a stack distance\nequal to 99. Whenever, we detect an entry in the stack we remove it, and push it to the top of\nthe stack.\nIf the memory address is not found, then we make a new entry and push it to the top of\nthe stack. Typically, the depth of the stack is bounded. It has length, L. If the number of\nentries in the stack exceeds L because of the addition of a new entry, then we need to remove\nthe entry at the bottom of the stack. Secondly, while adding a new entry, the stack distance (cid:13)c Smruti R. Sarangi 474\nis not defined. Note that since we consider bounded stacks, there is no way of differentiating\nbetween a new entry, and an entry that was there in the stack, but had to be removed because\nit was at the bottom of the stack. Hence, in this case we take the stack distance to be equal to\nL (bound on the depth of the stack).\nNote that the notion of stack distance gives us an indication of temporal locality. If the\naccesses have high temporal locality, then the mean stack distance is expected to be lower.\nConversely, if memory accesses have low temporal locality, then the mean stack distance will\nbe high. We can thus use the distribution of stack distances as a measure of the amount of\ntemporal locality in a program.\nExperiment to Measure Stack Distance\nWe perform a simple experiment with the SPEC2006 benchmark, Perlbench, which runs differ-\nentPerlprograms1. Wemaintaincounterstokeeptrackofthestackdistance. Thefirstmillion\nmemory accesses serve as a warm-up period. During this time the stack is maintained, but the\ncounters are not incremented. For the next million memory accesses, the stack is maintained,\nand the counters are also incremented. Figure 10.1 shows a histogram of the stack distance.\nThe size of the stack is limited to 1000 entries. It is sufficient to capture an overwhelming\nmajority of memory accesses.\n0.30\n0.25\n0.20\n0.15\n0.10\n0.05\n0.00\n0 50 100 150 200 250\nstack distance\nFigure 10.1: Stack distance distribution\nWe observe that most of the accesses have a very low stack distance. A stack distance\nbetween 0-9 is the most common value. Approximately 27% of all the accesses are in this bin.\nInfact,morethantwothirdsofthememoryaccesseshaveastackdistancelessthan100. Beyond\n100, the distribution tapers off, yet remains fairly steady. The distribution of stack distances is\n1Dataset size \u2019ref\u2019, input \u2019split-mail\u2019\nytilibaborp 475 (cid:13)c Smruti R. Sarangi\ntypically said to follow a heavy tailed distribution. This means that the distribution is heavily\nskewed towards smaller stack distances; however, large stack distances are not uncommon. The\ntail of the distribution continues to be non-zero for large stack distances. We observe a similar\nbehavior here.\nTrivia 3 Researchers have tried to approximate the stack distance using the log-normal\ndistribution.\nf(x) =\n\u221a1 e\u2212(ln( 2x \u03c3)\u2212 2\u00b5)2\nx\u03c3 2\u03c0\n10.1.5 Characterising Spatial Locality\nAddress Distance\nAkin to stack distance, we define the term address distance. The ith address distance is the\ndifference in the memory address of the ith memory access, and the closest address in the set\nof the last K memory accesses. Here, a memory access can be either a load or a store. There\nis an intuitive reason for defining address distance in this manner. Programs typically access\ndifferent regions of main memory in the same time interval. For example, an operation on\narrays, accesses an array item, then accesses some constants, performs an operation, saves the\nresult, and then moves on the next array entry using a for loop. There is clearly spatial locality\nhere, in the sense that consecutive iterations of a for loop access proximate addresses in an\narray. However, to quantify it, we need to search for the closest access (in terms of memory\naddresses)overthelastK accesses. Here, K isthenumberofmemoryaccessesineachiteration\nof the enclosing loop. We can readily observe that in this case that the address distance turns\nout to be a small value, and is indicative of high spatial locality. However, K needs to be well\nchosen. It should not be too small, nor too large. We have empirically found K = 10 to be an\nappropriate value for a large set of programs.\nTo summarise, we can conclude that if the average address distance is small, then it means\nthat we have high spatial locality in the program. The program tends to access nearby memory\naddresses with high likelihood in the same time interval. Conversely, if the address distances\nare high, then the accesses are far apart from each other, and the program does not exhibit\nspatial locality.\nExperiment to Characterise Address Distance\nHere, we repeat the same experiment as described in Section 10.1.4 with the SPEC2006 bench-\nmark, Perlbench. We profile the address distance distribution for the first 1 million accesses.\nFigure 10.2 shows the address distance distribution.\nHere also, more than a quarter of the accesses have an address distance between -5 and\n+5, and more than two thirds of the accesses have an address distance between -25 and +25.\nBeyond \u00b150, the address distance distribution tapers off. Empirically, this distribution also\nhas a heavy tailed nature. (cid:13)c Smruti R. Sarangi 476\n0.30\n0.25\n0.20\n0.15\n0.10\n0.05\n0.00\n100 50 0 50 100\naddress distance\nFigure 10.2: Address distance distribution\n10.1.6 Utilising Spatial and Temporal Locality\nSection 10.1.4 and 10.1.5 showed the stack and address distance distributions for a sample\nprogram. Similar experiments have been performed for thousands of programs that users use\nin their daily lives. These programs include computer games, word processors, databases,\nspreadsheet applications, weather simulation programs, financial applications, and software\napplications that run on mobile computers. Almost all of them exhibit a very high degree of\ntemporal and spatial locality. In other words, temporal and spatial locality, are basic human\ntraits. Whatever we do, including fetching books, or writing programs, these properties tend\nto hold. Note that these are just mere empirical observations. It is always possible to write\na program that does not exhibit any form of temporal and spatial locality. Additionally, it\nis always possible to find regions of code in commercial programs that do not exhibit these\nproperties. However, these examples are exceptions. They are not the norm. We need to\ndesign computer systems for the norm, and not for the exceptions. This is how we can boost\nperformance for a large majority of programs that users are expected to run.\nFromnowon,letustaketemporalandspatiallocalityforgranted,andseewhatcanbedone\nto boost the performance of the memory system, without compromising on storage capacity.\nLet us look at temporal locality first.\n10.1.7 Exploiting Temporal Locality \u2013 Hierarchical Memory System\nLet us reconsider the way in which we tried to exploit temporal locality for our simple example\nwith books. If your author decides to look at a new topic, then he brings the set of books\nassociated with that topic to his desk. The books that were already there on his desk, are\nmoved to the shelf, and to create space in the shelf, some books are shifted to the cabinet. This\nytilibaborp 477 (cid:13)c Smruti R. Sarangi\nbehavior is completely consistent with the notion of stack distance as shown in Figure 10.1.\nWe can do the same with memory systems also. Akin to a desk, shelf, and cabinet, let us\ndefine a storage location for memory values. Let us call it a cache. Each entry in the cache\nconceptually contains two fields \u2013 memory address, and value. Like your author\u2019s office, let us\ndefine a hierarchy of caches as shown in Figure 10.3.\nDefinition 98\nA cache contains a set of values for different memory locations.\nCache hierarchy\nL1 cache\nL2 cache\nMain memory\nFigure 10.3: Memory hierarchy\nDefinition 99 The main memory(physical memory) is a large DRAM array that contains\nvalues for all the memory locations used by the processor.\nThe L1 cache corresponds to the desk, the L2 cache corresponds to the shelf, and the main\nmemory corresponds to the cabinet. The L1 cache is typically a small SRAM array (8-64 KB).\nThe L2 cache is a larger SRAM array (128 KB - 4 MB). Some processors such as the Intel\nSandybridge processor have another level of caches called the L3 cache (4MB+). Below the\nL2\/L3 cache, there is a large DRAM array containing all the memory locations. This is known\nas the main memory or physical memory. Note that in the example with books, a book could\neither exclusively belong to the shelf or the cabinet. However, in the case of memory values,\nwe need not follow this rule. In fact, we shall see later that it is easier to maintain a subset of\nvalues of the L2 cache in the L1 cache, and so on. This is known as a system with inclusive\ncaches. We thus have \u2013 values(L1) \u2282 values(L2) \u2282 values(main memory) \u2013 for an inclusive\ncache hierarchy. Alternatively, we can have exclusive caches, where a higher level cache does\nnot necessarily contain a subset of values in the lower level cache. Inclusive caches are by far\nused universally in all processors. This is because of the ease of design, simplicity, and some\nsubtle correctness issues that we shall discuss in Chapter 11. There are some research level (cid:13)c Smruti R. Sarangi 478\nproposals that advocate exclusive caches. However, their utility for general purpose processors\nhas not been established as of 2012.\nDefinition 100 A memory system in which the set of memory values contained in the\ncache at the nth level is a subset of all the values contained in the cache at the (n+1)th\nlevel, is known as an inclusive cache hierarchy. A memory system that does not follow strict\ninclusion is referred to as an exclusive cache hierarchy.\nLet us now consider the cache hierarchy as shown in Figure 10.3. Since the L1 cache is\nsmall, it is faster to access. The access time is typically 1-2 cycles. The L2 cache is larger and\ntypically takes 5-15 cycles to access. The main memory is much slower because of its large\nsize and use of DRAM cells. The access times are typically very high and are between 100-300\ncycles. The memory access protocol is similar to the way your author accesses his books.\nThe memory access protocol is as follows. Whenever, there is a memory access (load or\nstore), theprocessorfirstchecksintheL1cache. Notethateachentryinthecacheconceptually\ncontains both the memory address and value. If the data item is present in the L1 cache, we\ndeclareacache hit,otherwisewedeclareacache miss. Ifthereisacachehit,andthememory\nrequest is a read, then we need to just return the value to the processor. If the memory request\nis a write, then the processor writes the new value to the cache entry. It can then propagate the\nchanges to the lower levels, or resume processing. We shall look at these the different methods\nof performing a cache write in detail, when we discuss different write policies in Section 10.2.3.\nHowever, if there is a cache miss, then further processing is required.\nDefinition 101\nCache hit Whenever a memory location is present in a cache, the event is known as a\ncache hit.\nCache miss Whenever a memory location is not present in a cache, the event is known as\na cache miss.\nIn the event of an L1 cache miss, the processor needs to access the L2 cache and search for\nthe data item. If an item is found (cache hit), then the protocol is the same as the L1 cache.\nSince, we consider inclusive caches in this book, it is necessary to fetch the data item to the\nL1 cache. If there is an L2 miss, then we need to access the lower level. The lower level can\nbe another L3 cache, or can be the main memory. At the lowest level, i.e., the main memory,\nwe are guaranteed to not have a miss, because we assume that the main memory contains an\nentry for all the memory locations. 479 (cid:13)c Smruti R. Sarangi\nPerformance Benefit of a Hierarchical Memory System\nInstead of having a single flat memory system, processors use a hierarchical memory system\nto maximise performance. A hierarchical memory system is meant to provide the illusion of a\nlarge memory with an ideal single cycle latency.\nExample 119 Find the average memory access latency for the following configurations.\nConfiguration 1\nLevel Miss Rate(%) Latency\nL1 10 1\nL2 10 10\nMain Memory 0 100\nConfiguration 2\nMain Memory 0 100\nAnswer: Let us consider the first configuration. Here, 90% of the accesses hit in the L1\ncache. Hence, their memory access time is 1 cycle. Note that even the accesses that miss\nin the L1 cache still incur the 1 cycle delay, because we do not know if an access will hit\nor miss in the cache. Subsequently, 90% of the accesses that go to the L2 cache hit in the\ncache. They incur a 10-cycle delay. Finally, the remaining accesses (1%) hit in the main\nmemory, and incur an additional delay. The average memory access time(T) is thus:\nT = 1+0.1\u2217(10+0.1\u2217100) = 1+1+1 = 3\nThus, the average memory latency of a hierarchical memory system such as configuration\n1 is 3 cycles.\nConfiguration 2 is a flat hierarchy, which uses the main memory for all its accesses.\nThe average memory access time is 100 cycles.\nThere is thus a speedup of 100\/3 = 33.3 times using a hierarchical memory system.\nLet us consider an example (see Example 119). It shows that the performance gain using\na hierarchical memory system is 33.33 times that of a flat memory system with a single level\nhierarchy. The performance improvement is a function of the hit rates of different caches and\ntheirlatencies. Moreover,thehitrateofacacheisdependentonthestackdistanceprofileofthe\nprogram,andthecachemanagementpolicies. Likewisethecacheaccesslatencyisdependenton\nthe cache manufacturing technology, design of the cache, and the cache management schemes.\nWeneedtomentionthatoptimisingcacheaccesseshasbeenaveryimportanttopicincomputer\narchitecture research for the past two decades. Researchers have published thousands of papers\nin this area. We shall only cover some basic mechanisms in this book. The interested reader\ncan take a look at Section 10.5.2 for appropriate references. (cid:13)c Smruti R. Sarangi 480\n10.1.8 Exploiting Spatial Locality \u2013 Cache Blocks\nLet us now consider spatial locality. We observe in Figure 10.2 that a majority of accesses\nhave an address distance within \u00b125 bytes. Recall that the address distance is defined as the\ndifference in memory addresses between the current address and the closest address among the\nlast K addresses. The address distance distribution suggests that if we group a set of memory\nlocations into one block, and fetch it at one go from the lower level, then we can increase the\nnumberofcachehitsbecausethereisahighdegreeofspatiallocalityinaccesses. Thisapproach\nis similar to the way we decided to fetch all the architecture books at the same time from the\nshelf in Section 10.1.2.\nConsequently, almost all processors create blocks of contiguous addresses, and the cache\ntreats each block as an atomic unit. The entire block is fetched at once from the lower level,\nand also an entire block is evicted from the cache if required. A cache block is also known as a\ncache line. A typical cache block or a line is 32-128 bytes long. For ease of addressing, its size\nneeds to be a strict power of 2.\nDefinition 102\nA cache block or a line is a contiguous set of memory locations. It is treated as an atomic\nunit of data in a cache.\nThus, we need to slightly redefine the notion of a cache entry. Instead of having an entry\nfor each memory address, we have a separate entry for each cache line. Note that in this book,\nwe shall use the terms cache line and block synonymously. Also note that it is not necessary\nto have the same cache line size in the L1 cache and the L2 cache. They can be different.\nHowever, for maintaining the property of inclusiveness of caches, and minimising additional\nmemory accesses, it is typically necessary to use an equal or larger block size in the L2 cache\nas compared to the L1 cache.\nWay Point 9\nHere is what we have learnt up till now.\n1. Temporal and spatial locality are properties inherent to most human actions. They\napply equally well to reading books and writing computer programs.\n2. Temporal locality can be quantified by the stack distance, and spatial locality can be\nquantified by the address distance.\n3. We need to design memory systems to take advantage of temporal and spatial locality.\n4. Totakeadvantageoftemporallocality, weuseahierarchicalmemorysystemconsisting\nof a set of caches. The L1 cache is typically a small and fast structure that is meant\nto satisfy most of the memory accesses quickly. The lower level of the caches store\nlarger amounts of data, are accessed infrequently, and have larger access times. 481 (cid:13)c Smruti R. Sarangi\n5. To take advantage of spatial locality, we group sets of contiguous memory locations\ninto blocks (also known as lines). A block is treated as an atomic unit of data in a\ncache.\nGiven that we have studied the requirements of a cache qualitatively, we shall proceed to\ndiscuss the design of caches.\n10.2 Caches\n10.2.1 Overview of a Basic Cache\nLet us consider a cache as a black box as shown in Figure 10.4. In the case of a load operation,\nthe input is the memory address, and the output is the value of the memory location if there is\na cache hit. We envision the cache having a status line that indicates if the request suffered a\nhit or miss. If the operation is a store, then the cache takes two inputs \u2013 memory address, and\nvalue. The cache stores the value in the entry corresponding to the memory location if there is\na cache hit. Otherwise, it indicates that there is a cache miss.\nMemory Load value\naddress\nCache\nStore value\nHit\/Miss\nFigure 10.4: A cache as a black box\nLetusnowlookatmethodstopracticallyimplementthisblackbox. WeshalluseanSRAM\narray as the building block (see Section 6.4). The reader might wish to revisit that section to\nrecapitulate her knowledge on memory structures.\nTo motivate a design, let us consider an example. Let us consider a 32-bit machine with a\nblock size of 64 bytes. In this machine, we thus have 226 blocks. Let the size of the L1 cache\nbe 8 KB. It contains 27 or 128 blocks. We can thus visualise the L1 cache at any point of time\nas a very small subset of the entire memory address space. It contains at the most 128 out of\n226 blocks. To find out if a given block is there in the L1 cache, we need to see if any of the\n128 entries contains it.\nWe assume that our L1 cache, is a part of a memory hierarchy. The memory hierarchy as a\nwhole supports two basic requests \u2013 read and write. However, we shall see that at the level of\na cache, we require many basic operations to implement these two high level operations.\nBasic Cache Operations\nAkin to a memory address, let us define a block address as the 26 MSB bits of the memory\naddress. The first problem is to find if a block with the given block address is present in the (cid:13)c Smruti R. Sarangi 482\ncache. We need to perform a lookup operation that returns a pointer to the block if it is present\nin the cache. If the block is present in the cache then we can declare a cache hit and service\nthe request. For a cache hit, we need two basic operations to service the request namely data\nread, and data write. They read or write the contents of the block, and require the pointer to\nthe block as an argument.\nIf there is a cache miss, then we need to fetch the block from the lower levels of the memory\nhierarchy and insert it in the cache. The procedure of fetching a block from the lower levels\nof the memory hierarchy, and inserting it into a cache, is known as a fill operation. The fill\noperation is a complex operation, and uses many atomic sub-operations. We need to first send\na load request to the lower level cache to fetch the block, and then we need to insert in into the\nL1 cache.\nThe process of insertion is also a complex process. We need to first check, if we have space\nto insert a new block in a given set of blocks. If we have sufficient space in a set, then we can\npopulate one of the entries using an insert operation. However, if all the locations at which we\nwant to insert a block in the cache are already busy, then we need to evict an already existing\nblock from the cache. We thus need to invoke a replace operation to find the cache block that\nneeds to be evicted. Once, we have found an appropriate candidate block for replacement, we\nneed to evict it from the cache using an evict operation.\nThus, to summarise the discussion up till now, we can conclude that we broadly need these\nbasic operations to implement a cache \u2013 lookup, data read, data write, insert, replace, and evict.\nThe fill operation is just a sequence of lookup, insert, and replace operations at different levels\nof the memory hierarchy. Likewise, the read operation is either primarily a lookup operation,\nor the combination of a lookup and fill operation.\n10.2.2 Cache Lookup and Cache Design\nAs outlined in Section 10.2.1, we wish to design a 8 KB cache with a block size of 64 bytes\nfor a 32-bit system. To do an efficient cache lookup, we need to find an efficient way to find\nout if the 26 bit block address exists among the 128 entries in the cache. There are thus two\nproblemshere. Thefirstproblemistoquicklylocateagivenentry, andthesecondistoperform\na read\/write operation. Instead of using a single SRAM array to solve both the problems, it is\na better idea to split it into two arrays as shown in Figure 10.5.\nIn a typical design, a cache entry is saved in two SRAM based arrays. One SRAM array\nknown as the tag array, contains information pertaining to the block address, and the other\nSRAM array known as the data array contains the data for the block. The tag array contains\na tag that uniquely identifies a block. The tag is typically a part of the block address, and\ndepends on the type of the cache. Along with the tag and data arrays, there is a dedicated\ncache controller that executes the cache access algorithm.\nFully Associative(FA) Cache\nLet us first consider a very simple way of locating a block. We can check each of the 128 entries\nin the cache possibly simultaneously to see if the block address is equal to the block address in\nthe cache entry. This cache is known as a fully associative cache or a content addressable cache.\nThe phrase \u201cfully associative\u201d means that a given block can be associated with any entry in\nthe cache. 483 (cid:13)c Smruti R. Sarangi\nTag array Data array\nAddress\nLoad\nCache controller\nStore value\nvalue\nHit \/ Miss\nFigure 10.5: Structure of a cache\nHit\/Miss\nTag array\n(CAM cells) Data array\nIndex of the\nmatching entry\nTag\nAddress format\nTag Offset\nFigure 10.6: A fully associative cache\nEach cache entry in a fully associative(FA) cache thus needs to contain two fields \u2013 tag and\ndata. In this case, we can set the tag to be equal to the block address. Since the block address\nis unique to each block, it fits the definition of a tag. Block data refers to the contents of the\nblock (64 bytes in this case). The block address requires 26 bits, and the block data requires 64\nbytes, in our running example. The search operation needs to span the entire cache, and once\nredocnE (cid:13)c Smruti R. Sarangi 484\nan entry is located, we need to either read out the data, or write a new value.\nLet us first take a look at the tag array. Each tag in this case is equal to the 26 bit block\naddress. After a memory request reaches a cache, the first step is to compute the tag by\nextracting the 26 most significant bits. Then, we need to match the extracted tag with each\nentry in the tag array using a set of comparators. If there is no match, then we can declare a\ncache miss and do further processing. However, if there is a cache hit, then we need to use the\nnumber of the entry that matches the tag to access the data entry. For example, in our 8 KB\ncache that contains 128 entries, it is possible that the 53rd entry in the tag array matches the\ntag. In this case, the cache controller needs to fetch the 53rd entry from the data array in the\ncase of a read access, or write to the 53rd entry in the case of a write access..\nThere are two ways to implement the tag array in a fully associative cache. Either we can\ndesign it as a normal SRAM array in which the cache controller iterates through each entry,\nand compares it with the given tag. Or, we can use a CAM array (see Section 6.4.2) that has\ncomparators in every row. They can compare the value of the tag with the data stored in the\nrow and produce an output (1 or 0) depending on the result of the comparison. A CAM array\ntypically uses an encoder to compute the number of the row that matches the result. A CAM\nimplementation of the tag array of a fully associative cache is more common, primarily because\nsequentially iterating through the array is very time consuming.\nFigure 10.6 illustrates this concept. We enable each row of the CAM array by setting\nthe corresponding word line to 1. Subsequently, the embedded comparators in the CAM cells\ncompare the contents of each row with the tag, and generate an output. We use an OR gate\nto determine if any of the outputs is equal to 1. If any of the outputs is 1, then we have a\ncache hit, otherwise, we have a cache miss. Each of these output wires are also connected to\nan encoder that generates the index of the row that has a match. We use this index to access\nthe data array and read the data for the block. In the case of a write, we write to the block,\ninstead of reading it.\nAfullyassociativecacheisveryusefulforsmallstructures(typically2-32)entries. However,\nit is not possible to use CAM arrays for larger structures. The area and power overheads of\ncomparison, and encoding are very high. It is also not possible to sequentially iterate through\nevery entry of an SRAM implementation of the tag array. This is very time consuming. Hence,\nwe need to find a better way for locating data in larger structures.\nDirect Mapped(DM) Cache\nWe saw that in a fully associative cache, we can store any block at any location in the cache.\nThis scheme is very flexible; however, it cannot be used when the cache has a large number of\nentries primarily because of prohibitive area and power overheads. Instead of allowing a block\nto be stored anywhere in the cache, let us assign only one fixed location for a given block. This\ncan be done as follows.\nInourrunningexample,wehavea8KBcachewith128entries. Letusrestricttheplacement\nof 64 byte blocks in the cache. For each block, let us assign a unique location in the tag array\nat which the tag corresponding to its address can be stored. We can generate such a unique\nlocation as follows. Let us consider the address of a block A, and the number of entries in our\ncache (128), and compute A%128. The % operator computes the remainder of the division of\nA by 128. Since A is a binary value, and 128 is a power of 2, computing the remainder is very 485 (cid:13)c Smruti R. Sarangi\neasy. We need to just extract the 7 LSB bits out of the 26-bit block address. These 7 bits can\nthen be used to access the tag array. We can then compare the value of the tag saved in the tag\narray with the tag computed from the block address to determine if we have a hit or a miss.\nInstead of saving the block address in the tag array as we did for a fully associative cache,\nwe can slightly optimise its design. We observe that 7 out of the 26 bits in the block address\nare used to access the tag in the tag array. This means that all the blocks that can possibly be\nmapped to a given entry in the tag array will have their last 7 bits common. Hence, these 7 bits\nneed not explicitly be saved as a part of the tag. We need to only save the remaining 19 bits of\nthe block address that can vary across blocks. Thus a tag in a direct mapped implementation\nof our cache needs to contain 19 bits only.\nTag array Data array\nIndex\nIndex\nTag\nHit\/Miss\nTag(19) Index(7) Offset(6)\nAddress format\nFigure 10.7: A direct mapped cache\nFigure 10.7 describes this concept graphically. We divide a 32-bit address into three parts.\nThe most significant 19 bits comprise the tag, the next 7 bits are referred to as the index (index\nin the tag array), and the remaining 6 bits point to the offset of the byte in the block. The rest\noftheaccessprotocolisconceptuallysimilartothatofafullyassociativecache. Inthiscase, we\nuse the index to access the corresponding location in the tag array. We read the contents and\ncompare it with the computed tag. If they are equal, then we declare a cache hit, otherwise,\nwe declare a cache miss. Subsequently, in the case of a cache hit, we use the index to access\nthe data array. In this case, we use the cache hit\/miss result, to enable\/disable the data array.\nWay Point 10\nUp till now we have taken a look at the fully associative and direct mapped caches.\n\u2022 The fully associative cache is a very flexible structure since a block can be saved in\nany entry in the cache. However, it has higher latency and power consumption. Since (cid:13)c Smruti R. Sarangi 486\na given block can potentially be allocated in more entries of the cache, it has a higher\nhit rate than the direct mapped cache.\n\u2022 The direct mapped cache on the other hand is a faster and less power consuming\nstructure. Here, a block can reside in only one entry in the cache. Thus, the expected\nhit rate of this cache is less than that of a fully associative cache.\nWe thus observe that there is a tradeoff between power, latency, and hit rate between the\nfully associative and direct mapped caches.\nSet Associative Cache\nA fully associative cache is more power consuming because we need to search for a block in\nall the entries of the cache. In comparison, a direct mapped cache is faster and power efficient\nbecause we need to check just one entry. However, it clearly has a lower hit rate, and that is\nnot acceptable either. Hence, let us try to combine both the paradigms.\nLet us design a cache in which a block can potentially reside in any one of a set of multiple\nentries in a cache. Let us associate a set of entries in the cache with a block address. Like a\nfully associative cache, we will have to check all the entries in the set before declaring a hit\nor a miss. This approach combines the advantages of both the fully associative and direct\nmapped schemes. If a set contains 4 or 8 entries, then we do not have to use an expensive CAM\nstructure, nor, do we have to sequentially iterate through all the entries. We can simply read\nout all the entries of the set from the tag array in parallel and compare all of them with the tag\npart of the block address in parallel. If there is a match, then we can read the corresponding\nentry from the data array. Since multiple blocks can be associated with a set, we call this\ndesign a set associative cache. The number of blocks in a set is known as the associativity of\nthe cache. Secondly, each entry in a set is known as a way.\nDefinition 103\nAssociativity The number of blocks contained in a set is defined as the associativity of\nthe cache.\nWay Each entry in a set is known as also known as a way.\nLetusnowdescribeasimplemethodtogroupcacheentriesintosetsforoursimpleexample,\nin which we considered a 32-bit memory system with an 8-KB cache and 64-byte blocks. As\nshown in Figure 10.8, we first remove the lowest 6 bits from the 32-bit address because these\nspecify the address of a byte within a block. The remaining 26 bits specify the block address.\nOur 8-KB cache has a total of 128 entries. If we want to create sets containing 4 entries each, 487 (cid:13)c Smruti R. Sarangi\n19 2 5 6\nIndex Block\nTag\nFigure 10.8: Division of a block address into sets\nthen we need to divide all the cache entries into sets of 4 entries. There will be 32(25) such\nsets.\nIn a direct mapped cache, we devoted the lowest 7 bits out of the 26 bit block address to\nspecify the index of the entry in the cache. We can now split these 7 bits into two parts as\nshown in Figure 10.8. One part contains 5 bits and indicates the address of the set, and the\nsecond part containing 2 bits is ignored. The group of 5 bits indicating the address of the set\nis known as the set index.\nAfter computing the set index, i, we need to access all the elements belonging to the set in\nthe tag array. We can arrange the tag array as follows. If the number of blocks in a set is S,\nthen we can group all the entries belonging to a set contiguously. For the ith set, we need to\naccess the elements iS, (iS +1) ... (iS +S \u22121) in the tag array.\nFor each entry in the tag array, we need to compare the tag saved in the entry to the tag\npart of the block address. If there is a match, then we can declare a hit. The notion of a tag\nin a set associative cache is rather tricky. As shown in Figure 10.8, it consists of the bits that\nare not a part of the index. In the case of our running example, it is the (21=26-5) MSB bits\nof the block address. The logic for deciding the number of tag bits is as follows.\nEach set is specified by a 5-bit set index. These 5 bits are common to all the blocks that\ncan be potentially mapped to the given set. We need to use the rest of the bits (26-5=21) to\ndistinguish between the different blocks that are mapped to the same set. Thus, a tag in a set\nassociative cache has a size between that of a direct mapped cache (19) and a fully associative\ncache (26).\nFigure 10.9 shows the design of a set associative cache. We first compute the set index from\nthe address of the block. For our running example, we use bits 7-11. Subsequently, we use the\nset index to generate the indices of its corresponding four entries in the tag array using the\ntag array index generator. Then, we access all the four entries in the tag array in parallel, and\nread their values. It is not necessary to use a CAM array here. We can use a single multi-\nport (multiple input, output) SRAM array. Next, we compare each element with the tag, and\ngenerate an output (0 or 1). If any of the outputs is equal to 1 (determined by an OR gate),\nthen we have a cache hit. Otherwise, we have a cache miss. We use an encoder to find the\nindex of the tag in the set that matched. Since, we are assuming a 4 way associative cache,\nthe output of the encoder is between 00 to 11. Subsequently, we use a multiplexer to choose\nthe index of the matching entry in the tag array. This index, can now be used to access the\ndata array. The corresponding entry in the data array contains the data for the block. We can\neither read it or write to it.\nWe can perform a small optimisation here, for read operations. Note that in the case of a\nread operation, the access to the data array and tag array can proceed in parallel. If a set has (cid:13)c Smruti R. Sarangi 488\nHit\/Miss\nData array\nTag array\nIndex of the\nTag array index matched entry\ngenerator\nSet index\nTag\nFigure 10.9: A set associative cache\n4 ways, then while we are computing a tag match, we can read the 4 data blocks corresponding\nto the 4 ways of the set. Subsequently, in the case of a cache hit, and after we have computed\nthe matching entry in the tag array, we can choose the right data block using a multiplexer. In\nthis case, we are effectively overlapping some or all of the time required to read the blocks from\nthe data array with the tag computation, tag array access, and match operations. We leave the\nresulting circuit as an exercise to the reader.\nTo conclude, we note that the set associative cache is by far the most common design for\ncaches. It has acceptable power consumption values and latencies for even very large caches.\nThe associativity of a set associative cache is typically 2, 4 or 8. A set with an associativity of\nK is also known as a K \u2212way associative cache.\nImportant Point 16 We need to answer a profound question while designing a set asso-\nciative cache. What should be the relative ordering of the set index bits and the ignored\nbits? Should the ignored bits be towards the left (MSB) of the index bits, or towards the\nright (LSB) of the index bits? In Figure 10.8, we have chosen the former option. What is\nthe logic behind this?\nAnswer: If we have the ignored bits to the left(MSB) of the index bits, then contiguous\nblocksmaptodifferentsets. However, forthereversecaseinwhichtheignoredbitsaretothe\nright(LSB) of the index bits, contiguous blocks map to the same set. Let us call the former\nscheme NON-CONT, and the latter scheme CONT. We have chosen NON-CONT in\nour design.\nLet us consider two arrays, A, and B. Let the sizes of A and B be significantly smaller\nthan the size of the cache. Moreover, let some of their constituent blocks map to the same\ngroup of sets. The figure below shows a conceptual map of the regions of the cache that store\nboth the arrays for the CONT and NON-CONT schemes. We observe that even though,\nwe have sufficient space in the cache, it is not possible to save both the arrays in the cache\nredocnE 489 (cid:13)c Smruti R. Sarangi\nconcurrently using the CONT scheme. Their memory footprints overlap in a region of the\ncache, and it is not possible to save data for both the programs simultaneously in the cache.\nHowever, the NON-CONT scheme tries to uniformly distribute the blocks across all the\nsets. Thus, it is possible to save both the arrays in the cache at the same time.\nConflict\nCache locations occupied by A\nCache locations occupied by B\nThis is a frequently occurring pattern in programs. The CONT scheme reserves an\nentire area of the cache and thus it is not possible to accommodate other data structures\nthat map to conflicting sets. However, if we distribute the data in the cache, then we can\naccommodate many more data structures and reduce conflicts.\nExample 120\nA cache has the following parameters in a 32-bit system.\nParameter Value\nSize N\nAssociativity K\nBlock Size B\nWhat is the size of the tag?\nAnswer:\n\u2022 The number of bits required to specify a byte within a block is log(B).\n\u2022 The number of blocks is equal to N\/B, and the number of sets is equal to N\/(BK).\n\u2022 Thus, the number of set index bits is equal to: log(N)\u2212log(B)\u2212log(K).\nsnoitacol\nehcaC (cid:13)c Smruti R. Sarangi 490\n\u2022 The remaining number of bits are tag bits. It is equal to: 32\u2212(log(N)\u2212log(B)\u2212\nlog(K)+log(B)) = 32\u2212log(N)+log(K).\n10.2.3 Data read and data write Operations\nThe data read Operation\nOnce,wehaveestablishedthatagivenblockispresentinacache,weusethebasicreadoperation\nto get the value of the memory location from the data array. We establish the presence of a\nblock in a cache if the lookup operation returns a cache hit. If there is a miss in the cache, then\nthe cache controller needs to raise a read request to the lower level cache, and fetch the block.\nThe data read operation can start as soon as data is available.\nThe first step is to read out the block in the data array that corresponds to the matched tag\nentry. Then, we need to choose the appropriate set of bytes out of all the bytes in the block.\nWe can use a set of multiplexers to achieve this. The exact details of the circuit are left to the\nreader as an exercise.\nSecondly, as described in Section 10.2.2, it is not strictly necessary to start the data read\noperation after the lookup operation. We can have a significant overlap between the operations.\nFor example, we can read the tag array and data array in parallel. We can subsequently select\nthe right set of values using multiplexers after the matching tag has been computed.\nThe data write Operation\nBefore, we can write a value, we need to ensure that the entire block is already present\nin the cache. This is a very important concept. Note that we cannot make an argument that\nsince we are creating new data, we do not need the previous value of the block. The reason\nis as follows. We typically write 4 bytes or at the most 8 bytes for a single memory access.\nHowever, a block is at least 32 or 64 bytes long. A block is an atomic unit in our cache. Hence,\nwe cannot have different parts of it at different places. For example, we cannot save 4 bytes of\na block in the L1 cache, and the rest of the bytes in the L2 cache. Secondly, for doing so, we\nneed to maintain additional state that keeps track of the bytes that have been updated with\nwrites. Hence, in the interest of simplicity, even if we wish to write just 1 byte, we need to\npopulate the cache with the entire block.\nAfter that we need to write the new values in the data array by enabling the appropriate\nset of word lines and bit lines. We can design a simple circuit to achieve this using a set of\ndemultiplexers. The details are left to the reader.\nThere are two methods of performing a data write \u2013 write-back and write-through. Write-\nthrough is a relatively simpler scheme. In this approach, whenever we write a value into the\ndata array, we also send a write operation to the lower level cache. This approach increases\nthe amount of cache traffic. However, it is simpler to implement the cache because we do not\nhave to keep track of the blocks that have been modified after they were brought into the\ncache. We can thus seamlessly evict a line from the cache if required. Here cache evictions and\nreplacements are simple, at the cost of writes. We shall also see in Chapter 11 that it is easy\nto implement caches for mutiprocessors if the L1 caches follow a write-through protocol. 491 (cid:13)c Smruti R. Sarangi\nIn the write-back scheme, we explicitly keep track of blocks that have been modified using\nwrite operations. We can maintain this information by using an additional bit in the tag array.\nThis bit is typically known as the modified bit. Whenever, we get a block from the lower level\nof the memory hierarchy, the modified bit is 0. However, when we do a data write and update\nthe data array, we set the modified bit in the tag array to 1. Evicting a line requires us to\ndo extra processing that we shall describe in Section 10.2.6. For a write-back protocol, writes\nare cheap, and evict operations are more expensive. The tradeoff here is the reverse of that in\nwrite-through caches.\nThe structure of an entry in the tag array with the additional modified bit is shown in\nFigure 10.10.\nModified Tag\nbit\nFigure 10.10: An entry in the tag array with the modified bit\n10.2.4 The insert Operation\nIn this section, we shall discuss the protocol to insert a block in a cache. This operation is\ninvoked when a block arrives from a lower level. We need to first take a look at all the ways\nof the set that a given block is mapped to, and see if there are any empty entries. If there\nare empty entries then we can choose one of the entries arbitrarily, and populate it with the\ncontents of the given block. If we do not find any empty entries, we need to invoke the replace\nand evict operations to choose and remove an already existing block from the set.\nWe need to maintain some extra status information to figure out if a given entry is empty\nor non-empty. In computer architecture parlance, these states are also known as invalid and\nvalid respectively. We need to store just 1 extra bit in the tag array to indicate the status of a\nblock. It is known as the valid bit. We shall use the tag array for saving additional information\nregarding an entry, because it is smaller and typically faster than the data array.\nThe structure of an entry in the tag array with the addition of the valid bit is shown in\nFigure 10.11.\nThe cache controller needs to check the valid bits of each of the tags while searching for\ninvalid entries. Note that all the entries of a cache are invalid initially. If an invalid entry is\nfound, then the corresponding entry in the data array can be populated with the contents of\nthe block. The entry subsequently becomes valid. However, if there is no invalid entry, then\nwe need to replace one entry with the given block that needs to be inserted into the cache.\n10.2.5 The replace Operation\nThe task is here to find an entry in the set that can be replaced by a new entry. We do not\nwish to replace an element that is accessed very frequently. This will increase the number of\ncache misses. We ideally want to replace an element that has the least probability of being (cid:13)c Smruti R. Sarangi 492\nValid bit\nModified Tag\nbit\nFigure 10.11: An entry in the tag array with the modified, and valid bits\naccessed in the future. However, it is difficult to predict future events. Hence, we need to make\nreasonable guesses based on past behavior. We can have different policies for the replacement\nof blocks in a cache. These are known as replacement schemes or replacement policies.\nDefinition 104\nA cache replacement scheme or replacement policy is a method to replace an entry in the\nset by a new entry.\nRandom Replacement Policy\nThe most trivial replacement policy is known as the random replacement policy. Here, we pick\na block at random and replace it. This scheme is very simple to implement. However, it is not\nvery optimal in terms of performance, because it does not take into account the behaviour of\ntheprogramandthenatureofthememoryaccesspattern. Thisschemeendsupoftenreplacing\nvery frequently accessed blocks.\nFIFO Replacement Policy\nThe next scheme is slightly more complicated, and is known as the FIFO (first in first out)\nreplacement policy. Here, the assumption is that the block that was brought into the cache at\nthe earliest point of time, is the least likely to be accessed in the future. To implement the\nFIFO replacement policy, we need to add a counter to the tag array. Whenever, we bring in a\nblock, we assign it a counter value equal to 0. We increment the counter values for the rest of\nthe blocks. The larger is the counter, the earlier the block was brought into the cache.\nNow to find a candidate for replacement, we need to find an entry with the largest value of\nthe counter. This must be the earliest block. Unfortunately, the FIFO scheme does not strictly\nalign with our principles of temporal locality. It penalises blocks that are present in the cache\nfor a long time. However, they may also be very frequently accessed blocks, and should not be\nevicted in the first place.\nLet us now consider the practical aspects of implementing a FIFO replacement policy. The\nmaximum size of the counter needs to be equal to the number of elements in a set, i.e., the 493 (cid:13)c Smruti R. Sarangi\nassociativity of the cache. For example, if the associativity of a cache is 8, we need to have a 3\nbit counter. The entry that needs to be replaced should have the largest counter value.\nNote that in this case, the process of bringing in a new value into the cache is rather expen-\nsive. Weneedtoincrementthecountersofalltheelementsinthesetexceptone. However,cache\nmisses, are more infrequent as compared to cache hits. Hence, the overhead is not significant\nin practice, and this scheme can be implemented without large performance overheads.\nLRU Replacement Policy\nThe LRU (least recently used) replacement policy is known to be as one of the most efficient\nschemes. The LRU scheme follows directly from the definition of stack distance. We ideally\nwant to replace a block that has the lowest chance of being accessed in the future. According\nto the notion of stack distance, the probability of being accessed in the future is related to the\nprobability of accesses in the recent past. If a processor has been accessing a block frequently\nin the last window of n (n is not a very large number) accesses, then there is a high probability\nthat the block will be accessed in the immediate future. However, if the last time that a block\nwas accessed is long back in the past, then the chances are unlikely that it will be accessed\nsoon.\nIn the LRU replacement policy, we maintain the time that a block was last accessed. We\nchoose the block that was last accessed at the earliest point of time as a candidate for replace-\nment. InahypotheticalimplementationofaLRUreplacementpolicy, wemaintainatimestamp\nfor every block. Any time that a block is accessed, its timestamp is updated to match the cur-\nrent time. For finding an appropriate candidate for replacement, we need to find the entry with\nthe smallest timestamp in a set.\nLet us now consider the implementation of an LRU scheme. The biggest issue is that\nwe need to do additional work for every read and write access to the cache. There will be\na significant performance impact because typically 1 in 3 instructions are memory accesses.\nSecondly, we need to dedicate bits to save a timestamp that is sufficiently large. Otherwise,\nwe need to frequently reset the timestamps of every block in a set. This process will induce\na further slowdown, and additional complexity in the cache controller. Implementing an LRU\nscheme that is as close to an ideal LRU implementation as possible, and that does not have\nsignificant overheads, is thus a difficult task.\nHence, let us try to design LRU schemes that use small timestamps (typically 1-3 bits), and\napproximately follow the LRU policy. Such kind of schemes are called pseudo-LRU schemes.\nLetusoutlineasimplemethodforimplementingabasicpseudo-LRUscheme. Notethatwecan\nhave many such approaches, and the reader is invited to try different approaches and test them\non a cache simulator such as Dinero [Edler and Hill, 1999], or sim-cache [Austin et al., 2002].\nInstead of trying to explicitly mark the least recently used element, let us try to mark the more\nrecently used elements. The elements that are not marked will automatically get classified as\nthe least recently used elements.\nLet us start out by associating a counter with each block in the tag array. Whenever, a\nblockisaccessed(read\/write),weincrementthecounter. However,oncethecounterreachesthe\nmaximum value, we stop incrementing it further. For example, if we use a 2-bit counter, then\nwe stop incrementing the counter beyond 3. Now, we need to do something more. Otherwise,\nthe counter associated with every block will ultimately reach 3 and stay there. To solve this (cid:13)c Smruti R. Sarangi 494\nproblem, wecanperiodicallydecrementthecountersofeveryblockinasetby1, orwecaneven\nreset them to 0. Subsequently, some of the counters will start increasing again. This procedure\nwill ensure that for most of the time, we can identify the least recently used blocks by taking a\nlook at the value of counters. The block associated with the lowest value of the counter is one\nof the least recently used blocks, and most likely \u201cthe most least recently used block\u201d. Note\nthat this approach does involve some amount of activity per access. However, incrementing a\nsmall counter has little additional overhead. Secondly, it is not in the critical path in terms of\ntiming. It can be done in parallel or sometime later also. Finding a candidate for replacement\ninvolves looking at all the counters in a set, and finding the block with the lowest value of the\ncounter. After we replace the block with a new block, most processors typically set the counter\nof the new block to the largest possible value. This indicates to the cache controller, that the\nnew block should have the least priority with respect to being a candidate for replacement.\n10.2.6 The evict Operation\nLastly, let us take a look at the evict operation. If the cache follows a write-through policy,\nthen nothing much needs to be done. The block can simply be discarded. However, if the cache\nfollows a write-back policy, then we need to take a look at the modified bit. If the data is not\nmodified, then it can be seamlessly evicted. However, if the data has been modified, then it\nneeds to written back to the lower level cache.\n10.2.7 Putting all the Pieces Together\nCache Read Operation\nTime\ninsert replace evict insert\nlookup lookup read block\nif write\nback cache\nhit data read miss\nLower level Lower level\ncache cache\nFigure 10.12: The read operation\nThe sequence of steps in a cache read operation is shown in Figure 10.12. We start with a\nlookup operation. As mentioned in Section 10.2.2, we can have a partial overlap between the\nlookup and data read operations. If there is a cache hit, then the cache returns the value to the\nprocessor, or the higher level cache (whichever might be the case). However, if there is a cache\nmiss, then we need to cancel the data read operation, and send a request to the lower level\ncache. The lower level cache will perform the same sequence of accesses, and return the entire\ncache block (not just 4 bytes). The cache controller can then extract the requested data from 495 (cid:13)c Smruti R. Sarangi\nthe block, and send it to the processor. Simultaneously, the cache controller invokes the insert\noperation to insert the block into the cache. If there is an invalid entry in the set, then we can\nreplace it with the given block. However, if all the ways in a set are valid, it is necessary to\ninvoke the replace operation to find a candidate for replacement. The figure appends a question\nmark with this operation, because this operation is not invoked all the time (only when all the\nways of a set contain valid data). Then, we need to evict the block, and possibly write it to\nthe lower level cache if the line is modified, and we are using a write-back cache. The cache\ncontroller then invokes the insert operation. This time it is guaranteed to be successful.\nCache Write Operation (write-back Cache)\nTime\nwrite\nlookup lookup block insert replace evict insert\nhit data write\nmiss\nLower level Lower level\ncache cache\nFigure 10.13: The write operation (write-back Cache)\nFigure 10.13 shows the sequence of operations for a cache write operation for a write-back\ncache. The sequence of operations are roughly similar to that of a cache read. If there is a\ncache hit, then we invoke a data write operation, and set the modified bit to 1. Otherwise,\nwe issue a read request for the block to the lower level cache. After the block arrives, most\ncache controllers typically store it in a small temporary buffer. At this point, we write the\n4 bytes (that we are interested in) to the buffer, and return. In some processors, the cache\ncontroller might wait till all the sub-operations complete. After writing into the temporary\nbuffer (write block operation in Figure 10.13), we invoke the insert operation for writing the\ncontents (modified) of the block. If this operation is not successful (because all the ways are\nvalid), then we follow the same sequence of steps as the read operation (replace, evict, and\ninsert).\nCache Write Operation (write-through Cache)\nFigure 10.14 shows the sequence of operations for a write-through cache. The first point of\ndifference is that we write the block to the lower level, even if the request hits in the cache.\nThe second point of difference is that after we write the value into the temporary buffer (after\na miss), we write back the new contents of the block to the lower level cache also. The rest of\nthe steps are similar to the sequence of steps that we followed for the write-back cache. (cid:13)c Smruti R. Sarangi 496\nTime\nlookup write\nlookup block insert replace evict insert\nhit data write\nmiss\nLower level\nLower level\ncache\ncache\nFigure 10.14: The write operation (write-through Cache)\n10.3 The Memory System\nWe now have a fair understanding of the working of a cache, and all its constituent operations.\nA memory system is built using a hierarchy of caches as mentioned in Section 10.1.7. The\nmemory system as a whole supports two basic operations: read, and write, or alternatively, load\nand store.\nMemory system\nInstruction L1 Data\ncache cache\nL2 cache\nMain memory\nFigure 10.15: The memory system\nWe have two caches at the highest level \u2013 The data cache (also referred to as the L1 cache),\nand the instruction cache (also referred to as the I Cache). Almost all the time both of them\ncontain different sets of memory locations. The protocol for accessing the I Cache and L1 cache\nis the same. Hence, to avoid repetition let us just focus on the L1 cache from now on. The\nreader needs to just remember that accesses to the instruction cache follow the same sequence\nof steps.\nThe processor starts by accessing the L1 cache. If there is a L1 hit, then it typically receives\nthe value within 1-2 cycles. Otherwise, the request needs to go to the L2 cache, or possibly\neven lower levels such as the main memory. In this case, the request can take tens or hundreds\nof cycles. In this section, we shall look at the system of caches in totality, and treat them as\none single black box referred to as the memory system. 497 (cid:13)c Smruti R. Sarangi\nIfweconsiderinclusivecaches,whichistheconventioninmostcommercialsystems,thetotal\nsizeofthememorysystemisequaltothesizeofthemainmemory. Forexample, ifasystemhas\n1 GB of main memory, then the size of the memory system is equal to 1 GB. It is possible that\ninternally, the memory system might have a hierarchy of caches for improving performance.\nHowever, they do not add to the total storage capacity, because they only contain subsets of\nthe data contained in main memory. Moreover, the memory access logic of the processor also\nviews the entire memory system as a single unit, conceptually modelled as a large array of\nbytes. This is also known as the physical memory system, or the physical address space.\nDefinition 105\nThe physical address space comprises of the set of all memory locations contained in the\ncaches, and main memory.\n10.3.1 Mathematical Model of the Memory System\nPerformance\nThememory systemcan be thoughtofas ablack boxthatjust servicesreadand writerequests.\nThe time a request takes is variable. It depends on the level of the memory system at which\nthe request hits. The pipeline is attached to the memory system in the memory access (MA)\nstage, and issues requests to it. If the reply does not come within a single cycle, then additional\npipeline bubbles need to be introduced in our 5 stage SimpleRisc in-order pipeline.\nLet the average memory access time be AMAT (measured in cycles), and the fraction of\nload\/store instructions be f . Then the CPI can be expressed as:\nmem\nCPI = CPI +stall rate\u2217stall cycles\nideal\n(10.1)\n= CPI +f \u00d7(AMAT \u22121)\nideal mem\nCPI is the CPI assuming a perfect memory system having a 1 cycle latency for all\nideal\naccesses. Note that in our 5 stage in-order pipeline the ideal instruction throughput is 1\ninstruction per cycle, and the memory stage is allotted 1 cycle. In practice, if a memory\naccess takes n cycles, then we have n\u22121 stall cycles, and they need to be accounted for by\nEquation 10.1. In this equation, we implicitly assume that every memory access suffers a stall\nfor AMAT \u22121 cycles. In practice this is not the case since most of the instructions will hit in\nthe L1 cache, and the L1 cache typically has a 1 cycle latency. Hence, accesses that hit in the\nL1 cache will not stall. However, long stall cycles will be introduced by accesses that miss in\nthe L1 and L2 caches.\nNonetheless, Equation 10.1 still holds because we are only interested in the average CPI for\na large number of instructions. We can derive this equation by considering a large number of\ninstructions, summing up all the memory stall cycles, and computing the average number of\ncycles per instruction. (cid:13)c Smruti R. Sarangi 498\nAverage Memory Access Time\nIn Equation 10.1, CPI is determined by the nature of the program and the nature of the\nideal\nother stages (other than MA) of the pipeline. f is also an inherent property of the program\nmem\nrunning on the processor. We need a formula to compute AMAT. We can compute it in a way\nsimilar to Equation 10.1.\nAssuming, a memory system with an L1 and L2 cache, we have:\nAMAT = L1 +L1 \u00d7L1\nhittime missrate misspenalty\n(10.2)\n= L1 +L1 \u00d7(L2 +L2 \u00d7L2 )\nhittime missrate hittime missrate misspenalty\nAll the memory accesses need to access the L1 cache irrespective of a hit or a miss. Hence,\nthey need to incur a delay equal to L1 . A fraction of accesses, L1 , will miss in\nhittime missrate\nthe L1 cache, and move to the L2 cache. Here also, irrespective of a hit or a miss, we need\nto incur a delay of L2 cycles. If a fraction of accesses (L2 ) miss in the L2 cache,\nhittime missrate\nthen they need to proceed to main memory. We have assumed that all the accesses hit in the\nmain memory. Hence, the L2 is equal to the main memory access time.\nmisspenalty\nNow, if we assume that we have a n level memory system where the first level is the L1\ncache, and the last level is the main memory, then we can use a similar equation.\nAMAT = L1 +L1 \u00d7L1\nhittime missrate misspenalty\nL1 = L2 +L2 \u00d7L2\nmisspenalty hittime missrate misspenalty\nL2 = L3 +L3 \u00d7L3 (10.3)\nmisspenalty hittime missrate misspenalty\n... = ...\nL(n\u22121) = Ln\nmisspenalty hittime\nWe need to note that the miss rate used in these equations for a certain level i is equal to\nthe number of accesses that miss at that level divided by the total number of accesses to that\nlevel. This is known as the local miss rate. In comparison, we can define a global miss rate for\nlevel i, which is equal to the number of misses at level i divided by the total number of memory\naccesses.\nDefinition 106\nlocal miss rate It is equal to the number of misses in a cache at level i divided by the total\nnumber of accesses at level i.\nglobal miss rate It is equal to the number of misses in a cache at level i divided by the\ntotal number of memory accesses.\nLetustakeadeeperlookatEquation10.1. Weobservethatwecanincreasetheperformance\nof a system by either reducing the miss rate, the miss penalty or by decreasing the hit time.\nLet us first look at the miss rate. 499 (cid:13)c Smruti R. Sarangi\n10.3.2 Cache Misses\nClassification of Cache Misses\nLet us first try to categorise the different kinds of misses in a cache.\nThe first category of misses are known as compulsory misses or cold misses. These misses\nhappen, when data is loaded into a cache for the first time. Since the data values are not there\nin the cache, a miss is bound to happen. The second category of cache misses are known as\ncapacity misses. We have a capacity miss, when the amount of memory required by a program\nis more than the size of the cache. For example, let us assume that a program repeatedly\naccesses all the elements of an array. The size of the array is equal to 1 MB, and the size of\nthe L2 cache is 512 KB. In this case, there will be capacity misses in the L2 cache, because\nit is too small to hold all the data. The set of blocks that a program accesses in a typical\ninterval of time is known as its working set. We can thus alternatively say that conflict misses\nhappenwhenthesizeofthecacheissmallerthantheworkingsetoftheprogram. Notethatthe\ndefinition of the working set is slightly imprecise because the length of the interval is considered\nrather subjectively. However, the connotation of the time interval is that it is a small interval\ncompared to the total time of execution of the program. Nevertheless, it is large enough to\nensure that the behaviour of the system achieves a steady state. The last category of misses\nare known as conflict misses. These misses occur in direct mapped and set associative caches.\nLet us consider a 4 way set associative cache. If there are 5 blocks that map to the same set\nin the working set of a program, then we are bound to have cache misses. This is because the\nnumber of blocks accessed is larger than the maximum number of entries that can be part of a\nset. These misses are known as conflict misses.\nDefinition 107\nThememorylocationsaccessedbyaprograminashortintervaloftimecomprisetheworking\nset of the program at that point of time.\nThe categorisation of misses into these three categories \u2013 compulsory, capacity, and conflict\n\u2013 is also known as the three \u2019C\u2019s.\nReduction of the Miss Rate\nTo sustain a high IPC, it is necessary to reduce the cache miss rate. We need to adopt different\nstrategies to reduce the different kinds of cache misses.\nLetusstartoutwithcompulsorymisses. Weneedamethodtopredicttheblocksthatwillbe\naccessed in the future, and fetch the blocks in advance. Typically schemes that leverage spatial\nlocality serve as effective predictors. Hence, increasing the block size should prove beneficial in\nreducing the number of compulsory misses. However, increasing the block size beyond a certain\nlimit can have negative consequences also. It reduces the number of blocks that can be saved\nin a cache, and secondly the additional benefit might be marginal. Lastly, it will take more\ntime to read and transfer bigger blocks from the lower levels of the memory system. Hence,\ndesigners avoid very large block sizes. Any value between 32-128 bytes is reasonable. (cid:13)c Smruti R. Sarangi 500\nModern processors typically have sophisticated predictors that try to predict the addresses\nof blocks that might be accessed in the future based on the current access pattern. They\nsubsequently fetch the predicted blocks from the lower levels of the memory hierarchy in an\nattempt to reduce the miss rate. For example, if we are sequentially accessing the elements\nof a large array, then it is possible to predict the future accesses based on the access pattern.\nSometimesweaccesselementsinanarray,wheretheindicesdifferbyafixedvalue. Forexample,\nwemighthaveanalgorithmthataccesseseveryfourthelementinanarray. Inthiscasealso,itis\npossible to analyse the pattern and predict future accesses because the addresses of consecutive\naccesses differ by the same value. Such kind of a unit is known as a hardware prefetcher. It\nis present in most modern processors, and uses sophisticated algorithms to \u201cprefetch\u201d blocks\nand consequently reduce the miss rate. Note that the hardware prefetcher should not be very\naggressive. Otherwise, it will tend to displace more useful data from the cache than it brings\nin.\nDefinition 108\nA hardware prefetcher is a dedicated hardware unit that predicts the memory accesses in the\nnear future, and fetches them from the lower levels of the memory system.\nLet us now consider capacity misses. The only effective solution is to increase the size of the\ncache. Unfortunately, the cache design that we have presented in this book requires the size of\nthe cache to be equal to a power of two (in bytes). It is possible to violate this rule by using\nsome advanced techniques. However, by and large most of the caches in commercial processors\nhave a size that is a power of two. Hence, increasing the size of a cache is tantamount to at\nleast doubling its size. Doubling the size of a cache requires twice the area, slows it down, and\nincreases the power consumption. Here again, prefetching can help if used intelligently and\njudiciously.\nThe classical solution to reduce the number of conflict misses is to increase the associativity\nof a cache. However, increasing the associativity of a cache increases the latency and power\nconsumption of the cache also. Consequently, it is necessary for designers to carefully balance\nthe additional hit rate of a set associative cache, with the additional latency. Sometimes, it is\nthe case that there are conflict misses in a few sets in the cache. In this case, we can have a\nsmall fully associative cache known as the victim cache along with the main cache. Any block\nthat is displaced from the main cache, can be written to the victim cache. The cache controller\nneeds to first check the main cache, and if there is a miss, then it needs to check the victim\ncache, before proceeding to the lower level. A victim cache at level i can thus filter out some\nof the requests that go to level (i+1).\nNote that along with hardware techniques, it is possible to write programs in a \u201ccache\nfriendly\u201d way. These methods can maximise temporal and spatial locality. It is also possible\nfor the compiler to optimise the code for a given memory system. Secondly, the compiler can\ninsertprefetchingcodesuchthatblockscanbeprefetchedintothecachebeforetheyareactually\nused. Discussion of such techniques are beyond the scope of this book.\nLet us now quickly mention two rules of thumb. Note that these rules are found to approx-\nimately hold empirically, and are by no means fully theoretically justified. The first is known 501 (cid:13)c Smruti R. Sarangi\nas the Square Root Rule [Hartstein et al., 2006]. It says that the miss rate is proportional to\nthe square root of the cache size.\n1\nmissrate \u221d \u221a [SquareRootRule] (10.4)\ncachesize\nHartsteinet. al.[Hartsteinetal.,2006]trytofindatheoreticaljustificationforthisrule,and\nexplain the basis of this rule by using results from probability theory. From their experimental\nresults, they arrive at a generic version of this rule that says that the exponent of the cache\nsize in the Square Root Rule varies from -0.3 to -0.7.\nThe other rule is known as the \u201cAssociativity Rule\u201d. It states that the effect of doubling\nassociativity is almost the same as doubling the cache size with the original associativity. For\nexample, the miss rate of a 64 KB 4-way associative cache is almost the same as that of a 128\nKB 2-way associative cache.\nWe would further like to caution the reader that the Associativity Rule and the Square\nRoot Rule are just thumb rules, and do not hold exactly. They can be used as mere conceptual\naids. We can always construct examples that violate these rules.\n10.3.3 Reduction of Hit Time and Miss Penalty\nHit Time\nThe average memory access time can also be reduced by reducing the hit time and the miss\npenalty. To reduce the hit time, we need to use small and simple caches. However, by doing\nso, we increase the miss rate also.\nMiss Penalty\nProcessor\nWrite\nL1 cache\nbuffer\nL2 cache\nFigure 10.16: Write buffer\nLet us now discuss ways to reduce the miss penalty. Note that the miss penalty at level i,\nis equal to the memory latency of the memory system starting at level (i+1). The traditional (cid:13)c Smruti R. Sarangi 502\nmethodsforreducing hittime, andmiss ratecanalwaysbeused toreducethe misspenaltyat a\ngiven level. However, we are looking at methods that are exclusively targeted towards reducing\nthe miss penalty. Let us first look at write misses in the L1 cache. In this case the entire block\nhastobebroughtintothecachefromtheL2cache. Thistakestime(> 10cycles), andsecondly\nunless the write has completed, the pipeline cannot resume. Hence, processor designers use a\nsmall set associative cache known as a write buffer as shown in Figure 10.16. The processor\ncan write the value to the write buffer, and then resume, or alternatively, it can write to the\nwrite buffer only if there is a miss in the L1 cache (as we have assumed). Any subsequent read\nneeds to check the write buffer along with accessing the L1 cache. This structure is typically\nvery small and fast (4-8 entries). Once, the data arrives in the L1 cache, the corresponding\nentry can be removed from the write buffer. Note that if a free entry is not available in the\nwrite buffer, then the pipeline needs to stall. Secondly, before the write miss has been serviced\nfrom the lower levels of the cache, it is possible that there might be another write to the same\naddress. This can be seamlessly handled by writing to the allocated entry for the given address\nin the write buffer.\nLet us now take a look at read misses. Let us start out by observing that the processor\nis typically interested in only up to 4 bytes per memory access. The pipeline can resume if it\nis provided those crucial 4 bytes. However, the memory system needs to fill the entire block\nbefore the operation can complete. The size of a block is typically between 32-128 bytes. It\nis thus possible to introduce an optimisation here, if the memory system is aware of the exact\nset of bytes that the processor requires. In this case, the memory system can first fetch the\nmemory word (4 bytes) that is required. Subsequently, or in parallel it can fetch the rest of\nthe block. This optimisation is known as critical word first. Then, this data can be quickly\nsent to the pipeline such that it can resume its operation. This optimisation is known as early\nrestart. Implementing both of these optimisations increases the complexity of the memory\nsystem. However, critical word first and early restart are fairly effective in reducing the miss\npenalty.\n10.3.4 Summary of Memory System Optimisation Techniques\nTable 10.2 shows a summary of the different techniques that we have introduced to optimise\nthe memory system. Note that every technique has some negative side effects. If a technique\nimproves the memory system in one aspect, then it is detrimental in some other aspect. For\nexample, by increasing the cache size we reduce the number of capacity misses. However, we\nalso increase the area, latency, and power.\nTo summarise, we can conclude that it is necessary to design the memory system very\ncarefully. The requirements of the target workload have to be carefully balanced with the\nconstraints placed by the designers, and the limits of manufacturing technology. We need\nto maximise performance, and at the same time be mindful of power, area, and complexity\nconstraints.\n10.4 Virtual Memory\nUp till now, we have considered only one program in our system. We have designed our\nentire system using this assumption. However, this assumption is not correct. For example, 503 (cid:13)c Smruti R. Sarangi\nTechnique Application Disadvantages\nlarge block size compulsory reducesthenumberofblocks\nmisses in the cache\nprefetching compulsory extra complexity and the\nmisses, capacity risk of displacing useful data\nmisses from the cache\nlarge cache size capacity misses high latency, high power,\nmore area\nincreased associativity conflict misses high latency, high power\nvictim cache conflict misses extra complexity\ncompiler based all types of misses not very generic\ntechniques\nsmall and simple cache hit time high miss rate\nwrite buffer miss penalty extra complexity\ncritical word first miss penalty extra complexity and state\nearly restart miss penalty extra complexity\nTable 10.2: Summary of different memory system optimisation techniques\nat the moment there are 232 programs running on your author\u2019s workstation. The reader can\neasily find out the number of programs running on her system by opening the Task Manger\non Windows, or by entering the command \u201cps -ef\u201d on a Linux or a Macintosh system. It is\npossible for one processor to run multiple programs by switching between different programs\nvery quickly. For example, while a user is playing a game, her processor might be fetching her\nnew email. The reason she does not feel any interruption, is because the time scale at which the\nprocessor switches back and forth between programs (typically several milliseconds) is much\nsmaller than what humans can perceive.\nSecondly, we have assumed up till now that all the data that a program needs is resident in\nmain memory. However, this assumption is also not correct. Back in the old days, the size of\nmain memory used to be several megabytes, whereas, users could run very large programs that\nneeded hundreds of megabytes of data. Even now, it is possible to work with data that is much\nlarger than the amount of main memory. Readers can easily verify this statement, by writing\na C program that creates data structures that are larger than the amount of physical memory\ncontained in their machine. In most systems, this C program will compile and run successfully.\nWe shall see in this section that by making a small change in the memory system, we can\nsatisfy both of these requirements.\n10.4.1 Process \u2013 A Running Instance of a Program\nUp till now, we have assumed the existence of only one program in the system. We assumed\nthat it was in complete control of the memory system, and the processor pipeline. However,\nthis is not the case in practice.\nLetusfirststartoutbyaccuratelydefiningthenotionofaprocessanddifferentiatingitfrom\na program. Up till now we have been loosely using the term \u2013 program \u2013 and sometimes using (cid:13)c Smruti R. Sarangi 504\nit in place of a process. A program is an array of bytes and is saved as a file in the file system.\nThe file is typically known as a binary or as an executable. The executable contains some meta\ndata about the program such that its name and type, the constants used by the program, and\nthe set of instructions. In comparison, a process is a running instance of a program. If we run\none program several times, we create multiple processes. A process has access to the processor,\nperipheral devices, and the memory system. There is a dedicated area in the memory system\nthat contains the data and code of the process. The program counter of the processor points\nto a given location in the code region of the process in memory when the process is executing.\nMemoryvaluesrequiredbytheprocessareobtainedfromitsdataregioninthememorysystem.\nThe operating system starts and ends a process, and manages it throughout its lifetime.\nDefinition 109\nA process is a running instance of a program.\nOperating System\nMost of our readers must have heard of the term operating system. Most people mostly view\nan operating system such as Windows, Linux, or Mac OS X from the point of view of its user\ninterface. However, this is a minor aspect of the operating system. It does many more things\ninvisibly. Let us look at some of its important functionalities.\nThe operating system consists of a set of dedicated programs that manage the machine,\nperipheral devices, and all the processes running on the machine. Furthermore, the operating\nsystem facilitates efficient transfer of information between the hardware and software compo-\nnentsofacomputersystem. Thecorecomponentofanoperatingsystemisknownasthekernel.\nIts main role is to manage the execution of processes, and manage memory. We shall look at\nthe memory management aspect in Section 10.4.5. Let us now look at the process management\naspect.\nTo run a program, a user needs to compile the program, and then either double click the\nprogram, or write the name of the program in the command line, and click the \u201center\u201d button.\nOnce, this is done, the control passes to the operating system kernel. A component of the\nkernel known as the loader reads the content of the program, and copies it to a region in the\nmemory system. Notably, it copies all the instructions in thetext section, allocates space for all\nthe data, and initialises memory with all the constants that a program will require during its\nexecution. Subsequently, it initialises the values of registers, copies command line arguments\nto the stack, possibly initialises the stack pointer, and jumps to the entry point of the program.\nThe user program can then begin to execute, in the context of a running process. Every process\nhas a unique number associated with it. It is known as the pid (process id). After completion,\nit is the kernel\u2019s job to tear down the process, and reclaim all of its memory.\nThe other important aspect of process management is scheduling. A dedicated component\nof the kernel manages all the processes, including the kernel itself, which is a special process.\nIt typically runs each process for a certain amount of time, and then switches to another\nprocess. As a user, we typically do not perceive this because every second, the kernel switches\nbetween processes hundreds of times. The time interval is too small for us to detect. However, 505 (cid:13)c Smruti R. Sarangi\nbehind the scenes, the kernel is busy at work. For example, it might be running a game for\nsometime, running a program to fetch data from the network for some time, and then running\nsome of its own tasks for sometime. The kernel also manages aspects of the file system, inter-\nprocess communication, and security. The discussion of such topics is beyond the scope of\nthis book. The reader is referred to textbooks on operating systems such as the book by\nTanenbaum [Tanenbaum, 2007] or Silbserchatz and Galvin [Silberschatz et al., 2008].\nThe other important components in an operating system are device drivers, and system\nutilities. Device drivers are dedicated programs that communicate with dedicated devices and\nensure the seamless flow of information between them and user processes. For example, a\nprinter and scanner have dedicated device drivers that make it possible to print and scan\ndocuments, respectively. Network interfaces have dedicated device drivers that allow us to\nexchange messages over the internet. Lastly, system utilities provide generic services to all\nthe processes such as file management, device management (Control Panel in Windows), and\nsecurity.\nDefinition 110\nOperating System Theoperatingsystemconsistsofasetofdedicatedprogramsthatman-\nage the machine, peripheral devices, and the processes running on it. It facilitates the\ntransfer of information between the hardware and software components of a computer\nsystem.\nKernel The kernel is a program that is the core of the operating system. It has complete\ncontrol over the rest of the processes in the operating system, the user processes, the\nprocessor, and all external devices. It mainly performs the task of managing multiple\nprocesses, devices, and filesystems.\nProcess Management The two important components in the kernel to perform process\nmanagement are the loader, and the scheduler. The loader creates a process out of\na program by transferring its contents to memory, and setting up the appropriate\nexecution environment. The scheduler schedules the execution of multiple processes\nincluding that of the kernel itself.\nDevice Drivers These dedicated programs help the kernel and user processes communicate\nwith devices.\nSystem Utilities These are generic services provided by the operating system such as the\nprint queue manager and file manager. They can be used by all the processes in the\nsystem. (cid:13)c Smruti R. Sarangi 506\nVirtual \u2018View\u2019 of Memory\nSince multiple processes are live at the same point of time. It is necessary to partition the\nmemory between processes. If this is not done, then it is possible that processes might end\nup modifying each others\u2019 values. At the same time, we do not want the programmer or\nthe compiler to be aware of the existence of multiple processes. This introduces unwanted\ncomplexity. Secondly, if a given program is compiled with a certain memory map, it might not\nrun on another machine that has a process with an overlapping memory map. Even worse, it\nwill not be possible to run two copies of the same program. Hence, it is essential that each\nprogram sees a virtual view of memory, in which it assumes that it owns the entire memory\nsystem.\nAs we can observe, there are two conflicting requirements. The memory system, and the\noperating system want different processes to access different memory addresses, whereas, the\nprogrammer and the compiler do not want to be aware of this requirement. Additionally, the\nprogrammer wishes to layout her memory map according to her wish. It turns out that there\nis a method to make both the programmer and the operating system happy.\nWe need to define a virtual and a physical view of of memory. In the physical view of\nmemory, different processes operate in non-overlapping regions of the memory space. However,\nin the virtual view, every process accesses any address that it wishes to access, and the virtual\nviews of different processes can overlap. The solution is obtained through a method called\npaging that we shall explain in Section 10.4.3. However, before proceeding to the solution, let\nusdiscussthevirtualviewofmemorythataprocesstypicallysees. Thevirtualviewofmemory,\nis also referred to as virtual memory. It is defined as a hypothetical memory system, in which\na process assumes that it owns the entire memory space, and there is no interference from any\nother process.\nDefinition 111\nThe virtual memory system is defined as a hypothetical memory system, in which a process\nassumes that it owns the entire memory space, and there is no interference from any other\nprocess. The size of the memory is as large as the total addressable memory of the system.\nFor example, in a 32-bit system, the size of virtual memory is 232 bytes (4 GB). The set of\nall memory locations in virtual memory is known as the virtual address space.\nIn the virtual memory space, the operating system lays out the code and data in different\nregions. This arrangement of code, data, constants, and other information pertaining to a\nprocess is known as the memory map.\nDefinition 112\nThe memory map of a process refers to the way an operating system lays out the code and\ndata in memory. 507 (cid:13)c Smruti R. Sarangi\nMemory Map of a Process\n0xC0000000\nStack\nMemory mapping\nsegment\nHeap\nBss\nStatic variables not\ninitialised, filled with zeros\nData\nStatic variables\nwith initial values\nText\n0x08048000\nHeader\n0\nFigure 10.17: Memory map of a process in the Linux operating system (32 bits)\nFigure 10.17 shows a simplified view of the memory map of a process in the 32-bit Linux\noperating system. Let us start from the bottom (lowest address). The first section contains\nthe header. It starts out with details about the process, its format, and the target machine.\nSubsequently, the header contains the details of each section in the memory map. For example,\nit contains the details of the text section that contains the code of the program including its\nsize, starting address, and additional attributes. The text section starts after the header. The\noperating system sets the program counter to the start of the text section while loading a\nprogram. All the instructions in a program are typically contained within the text section.\nThe text section is followed by two more sections that are meant to contain static and global\nvariables. Optionally some operating systems, also have an additional area to contain read only\ndata such as constants.\nThe text section is typically followed by the data section. It contains all the static\/global\nvariables that have been initialised by the programmer. Let us consider a declaration of the\nform (in C or C++):\nstatic int val = 5;\nHere the 4 bytes corresponding to the variable \u2013 val \u2013 are saved in the data section. The\ndata section is followed by the bss section. The bss section saves static and global variables (cid:13)c Smruti R. Sarangi 508\nthat have not been explicitly initialised by the programmer. Most operating systems, fill the\nmemory area corresponding to the bss section with zeros. This needs to be done in the interest\nof security. Let us assume that program A runs and writes its values in the bss section.\nSubsequently, program B runs. Before, writing to a variable in the bss section, B can always\ntry to read its value. In this case, it will get the value written by program A. However, this\nis not desirable behavior. Program A might have saved some sensitive data in the bss section\nsuch as a password or a credit card number. Program B can thus gain access to this sensitive\ndata without program A\u2019s knowledge, and possibly misuse the data. Hence, it is necessary to\nfill up the bss section with zeros such that such kind of security lapses do not happen.\nThe bss section is followed by a memory area known as the heap. The heap area is used\nto save dynamically allocated variables in a program. C programs typically allocate new data\nwith the malloc call. Java and C++ use the new operator. Let us look at some examples.\nint *intarray = (int *)malloc(10 * sizeof(int)); [C]\nint *intarray = new int[10]; [C++]\nint[] intarray = new int[10]; [Java]\nNotethatintheselanguages, dynamicallyallocatingarraysisveryusefulbecausetheirsizes\nare not known at compile time. The other advantage of having data in the heap is that they\nsurvive across function calls. The data in the stack remains valid for only the duration of the\nfunction call. After that it gets deleted. However, data in the heap stays for the entire life\nof the program. It can be used by all the functions in the program, and pointers to different\ndata structures in the heap can be shared across functions. Note that the heap grows upward\n(towards higher addresses). Secondly, managing the memory in a heap is a fairly difficult task.\nThis is because dynamically, regions of the heap are allocated with malloc\/new calls and freed\nwith the free\/delete calls in high level languages. Once an allocated memory region is freed, a\nhole gets created in the memory map. It is possible to allocate some other data structure in the\nhole if its size is less than the size of the hole. In this case, another smaller hole gets created in\nthe memory map. Over time as more and more data structures are allocated and de-allocated,\nthe number of holes tend to increase. This is known as fragmentation. Hence, it is necessary\nto have an efficient memory manager that can reduce the number of holes in the heap. A view\nof the heap with holes, and allocated memory is shown in Figure 10.18.\nThe next segment is reserved for storing data corresponding to memory mapped files, and\ndynamically linked libraries. Most of the time, operating systems transfer the contents of a file\n(such as a music, text, or video file) to a memory region, and treat the contents of the file as a\nregular array. This memory region is referred to as a memory mapped file. Secondly, programs\nmight occasionally read the contents of other programs (referred to as libraries) dynamically,\nand transfer the contents of their text sections to their memory map. Such libraries are known\nas dynamically linked libraries, or dlls. The contents of such memory mapped structures are\nstored in a dedicated section in the process\u2019s memory map.\nThe next section is the stack, which starts from the top of the memory map and grows\ndownwards (towards smaller addresses) as discussed in Section 3.3.10. The stack continuously\ngrows and shrinks depending on the behavior of the program. Note that Figure 10.17 is not\ndrawntoscale. Ifweconsidera32-bitmemorysystem,thenthetotalamountofvirtualmemory\nis 4 GB. However, the total amount of memory that a program might use is typically limited to 509 (cid:13)c Smruti R. Sarangi\nHeap\nAllocated\nmemory regions\nFigure 10.18: The memory map of a heap\nhundreds of megabytes. Hence, there is a massive empty region in the map between the start\nof the heap and stack sections.\nNote that the operating system needs to run very frequently. It needs to service device\nrequests, and perform process management. As we shall see in Section 10.4.3 changing the\nvirtual view of memory from process to process is slightly expensive. Hence, most operating\nsystems partition the virtual memory between a user process and the kernel. For example,\nLinuxgivesthelower3GBtoauserprocess, andkeepstheupper1GBforthekernel. Similarly,\nWindows keeps the upper 2GB for the kernel, and the lower 2 GB for user processes. Hence, it\nisnotnecessarytochangetheviewofmemoryastheprocessortransitionsfromtheuserprocess\nto the kernel. Secondly, this small modification does not greatly impair the performance of a\nprogram because 2GB or 3GB is much more than the typical memory footprint of a program.\nMoreover, this trick does not also conflict with our notion of virtual memory. A program just\nneeds to assume that it has a reduced memory space (reduced from 4GB to 3GB in the case of\nLinux). Refer to Figure 10.19.\nOS kernel 1 GB\n2 GB\nOS kernel\nUser programs 3 GB User programs\n2 GB\nLinux Windows\nFigure 10.19: The memory map \u2013 user and kernel (cid:13)c Smruti R. Sarangi 510\n10.4.2 The \u201cOverlap\u201d and \u201cSize\u201d Problems\nLet us summarise all our discussion up till now. We basically want to solve two problems.\nOverlap Problem Programmers and compilers write a program assuming that they own the\nentire memory space and they can write to any location at will. Unfortunately, the same\nassumptionismadebyallprocessesthataresimultaneouslyactive. Unlessstepsaretaken,\nthey may end up inadvertently writing to each other\u2019s memory space and corrupting each\nother\u2019s data. In fact, given that they use the same memory map, the chances of this\nhappening in a naive system are extremely high. The hardware somehow needs to ensure\nthat different processes are isolated from each other. This is the overlap problem.\nSize Problem Occasionally we need to run processes that require more memory than the\navailable physical memory. It is desirable if some space in other storage media such as\nthe hard disk can be repurposed for storing the memory footprint of a process. This is\nknown as the size problem.\nAny implementation of virtual memory needs to effectively solve the size and overlap prob-\nlems.\n10.4.3 Implementation of Virtual Memory with Paging\nTo balance the requirements of the processor, operating system, compiler, and programmer we\nneedtodesignatranslationsystemthatcantranslatetheaddressgeneratedbyaprocessintoan\naddressthatthememorysystemcanuse. Byusingatranslator, wecansatisfytherequirements\noftheprogrammer\/compiler,whoneedvirtualmemory,andtheprocessor\/memorysystem,who\nneed physical memory. A translation system is similar to what a translator in real life would\ndo. For example, if we have a Russian delegation visiting Dubai, then we need a translator who\ncan translate Russian to Arabic. Both the sides can then speak their own language, and thus\nbe happy. A conceptual diagram of the translation system is shown in Figure 10.20.\nAddress\nVirtual Physical\ntranslation\naddress system address\nFigure 10.20: Address translation system\nLet us now try to design this address translation system. Let us first succinctly list the\nrequirements that a program and compiler place on the nature of virtual memory.\n1. Any address in the range of valid addresses should be accessible. For example, in a Linux\nbased machine, a process\u2019s virtual memory size is limited to 3 GB. Hence, it should be\npossible to access any address in this range.\n2. Thevirtualmemoryshouldbeperceivedasonecontiguousmemoryspacewheretheentire\nspace is available to the program. 511 (cid:13)c Smruti R. Sarangi\n3. Unless explicitly desired by the program, there should be no interference from any other\nprogram.\nHere are the requirements from the side of the memory system.\n1. Different programs should access non-overlapping sets of addresses.\n2. A program cannot be allotted a large continuous chunk of memory addresses. This will\ncause a high degree of wastage in space due to fragmentation.\n3. If the total amount of physical memory is less than the size of the virtual memory, then\nthere should be additional storage space available to support programs that require more\nspace than the total amount of physical memory.\nLet us now try to satisfy these requirements by designing a translation system that takes\nan address as specified in the program, and translates it to a real address that can be presented\nto the memory system. The address specified in the program is known as the virtual address,\nand the address sent to the memory system is known as the physical address.\nDefinition 113\nVirtual Address An address specified by the program in the virtual address space.\nPhysical Address An address presented to the memory system after address translation.\nWe can trivially achieve a translation system by uniquely mapping every virtual address\nto a physical address at the level of every byte or memory word (4 bytes). In this case, the\nprogram perceives one contiguous memory space. Secondly, we need to only map those virtual\naddresses that are actually used by the program. If a program actually requires 3 MB of space,\nthen we end up using only 3 MB of physical memory. Whenever, the process requires a new set\nof bytes that have not been already mapped, a smart memory management unit can allocate\nnew space in physical memory. Lastly, note that it is necessary for every memory access to pass\nthrough this translation system.\nEven though our basic translation system satisfies all our requirements, it is not efficient.\nWe need to maintain a large table that maps every byte in the virtual address space to a byte\nin the physical address space. This mapping table between the virtual and physical addresses\nwill be very large and slow. It is also not a very power efficient scheme. Secondly, our scheme\ndoes not take advantage of spatial and temporal locality. Hence, let us try to make our basic\nsystem more efficient.\nPages and Frames (cid:13)c Smruti R. Sarangi 512\nDefinition 114\nPage It is a block of memory in the virtual address space.\nFrame It is a block of memory in the physical address space. A page and frame have the\nsame size.\nPage Table It is a mapping table that maps the address of each page to an address of a\nframe. Each process has its own page table.\nInstead of translating addresses at the granularity of bytes, let us translate addresses at the\ngranularity of larger blocks. This will reduce the amount of state that we need to maintain,\nand also take advantage of spatial locality. Let us define a block of memory in the virtual\naddress space and call it a page. Similarly, let us define a block of the same size in the physical\naddress space and call it a frame. The size of a page or a frame is typically 4 KB. Secondly,\nnote that the virtual address space is unique to each process; whereas, the physical address\nspace is the same for all processes. For each process, we need to maintain a mapping table\nthat maps each page to a frame. This is known as the page table. A page table can either\nbe implemented in hardware or in software. A hardware implementation of the page table has\ndedicated structures to store the mapping between virtual and physical addresses. The lookup\nlogic is also in hardware. In the case of a software implementation, the mappings are stored\nin a dedicated region of the physical address space. In most processors that use software page\ntables, the lookup logic is also in hardware. They typically do not use custom routines in\nsoftware to lookup page tables because this approach is slow and complicated. Since the lookup\nlogic of page tables is primarily in hardware, the design of page tables needs to be relatively\nsimple. The page tables that we describe in the next few sections are oblivious to how they are\nimplemented (software or hardware).\nLet us consider a 32-bit memory address. We can now split it into two parts. If we\nconsider a 4 KB page, then the lower 12 bits specify the address of a byte in a page (reason:\n212 = 4096 = 4KB). This is known as the offset. The upper 20 bits specify the page number\n(see Figure 10.21). Likewise, we can split a physical address into two parts \u2013 frame number\nand offset. The process of translation as shown in Figure 10.21, first replaces the 20 bit page\nnumber with an equivalent 20 bit frame number. Then it appends the 12 bit offset to the\nphysical frame number.\nA Single Level Page Table\nFigure 10.22 shows a basic page table that contains 220 (\u2248 1,000,000) rows. Each row is\nindexed by the page number, and it contains the corresponding 20 bit (2.5 byte) frame number.\nThe total size of the table is thus 2.5 MB. If we have 200 processes in the system at any point of\ntime, thenweneedtowaste500MBofpreciousmemoryforjustsavingpagetables! Ifourtotal\nmain memory is 2 GB, then we are spending 25% of it in saving page tables, which appears to\nbe a big waste of space. Secondly, it is possible that in some systems, we might not even have 513 (cid:13)c Smruti R. Sarangi\n20 12\nVirtual\nPage number Offset\naddress\nPage\ntable\nPhysical\nFrame number Offset\naddress\n20 12\nFigure 10.21: Translation of a virtual to a physical address\nPage table\n20\nPage number Frame number\n20\nFigure 10.22: A single level page table\n500 MB of main memory available. In this case, we cannot support 200 live processes at the\nsame time. We need to look for better solutions.\nLet us now look for insights that might help us reduce the amount of storage. We start\nout by noticing that large parts of the virtual address space of a process are actually empty.\nIn a 32-bit system, the size of the virtual address space is 4 GB. However, large programs do\nnot use more than 100 MB. There is a massive empty region between the stack and the heap\nsections in the memory map, and thus it is not necessary to allocate space for mapping this\nregion. Ideally, the number of entries in the page table should be equal to the number of pages\nactually used by a process rather than the theoretically maximum number of pages a process\ncan use. If a process uses only 400 KB of memory space, then ideally its page table should just\ncontain 100 entries. Let us design a two level page table to realise this goal. (cid:13)c Smruti R. Sarangi 514\n20 Primary page\ntable\nPage number\n10 10\n20\nFrame number\nSecondary page tables\nFigure 10.23: A two level page table\nTwo Level Page Table\nLet us further split a page number into two equal parts. Let us split the 20 bits into two parts\ncontaining 10 bits each as shown in Figure 10.23. Let us use the upper 10 bits to access a top\nlevel page table known as the primary page table. Each entry in the top level page table points\nto a secondary page table. Subsequently, each secondary page table is indexed by the lower\n10 bits of the page number. An entry in the secondary page table contains the frame number.\nIf no addresses map to a given entry in the primary page table, then it does not point to a\nsecondary page table, and thus there is no need to allocate space for it. In a typical program,\nmost of the entries in the primary page table are expected to be empty. Let us now calculate\nthe size of this structure.\nThe primary page table contains 1024 entries, where each entry is 10 bits long. The total\nsize is 1.25 KB (10 bits = 1.25 bytes). Let the number of secondary page tables be N. Each\nsecondary page tables contains 1024 entries, where each entry is 20 bits long. Therefore, the\nsizeofeachsecondarypagetableis2.5KB,andthetotalstoragerequirementis(1.25+2.5\u00d7N)\nKB. Because of spatial locality in a program, N is not expected to be a large number. Let\nus consider a program that has a memory footprint of 10 MB. It contains roughly 2500 pages.\nEach secondary page table can map at the most 1024 pages (4 MB of data). It is highly likely\nthat this program might map to only 3 secondary page tables. Two page tables will contain the\nmappings for the text, data, and heap sections, and one page table will contain the mappings\nfor the stack section. In this case, the total storage requirement for the page tables will be\nequal to 8.75 KB, which is very reasonable. Even, if we require double the number of secondary\npage tables because of lower spatial locality in the memory map, then also the total storage\nrequirement is equal to 16.25 KB. This is an order of magnitude better than a single level page\ntable that required 2.5 MB of storage per process. Hence, two level page tables are used in\nmost commercial systems. 515 (cid:13)c Smruti R. Sarangi\nInverted Page Table\nInverted\npage table\n20\nFrame number Page number\n20\n(a)\n20 Inverted\nPage number Pid page table Compare the page\nHashtable num, process id with\neach entry\nHashing\nengine\nFrame number\n20\n(b)\nFigure 10.24: Inverted page table\nSome processors such as the Intel Itanium, and PowerPC 603, use a different design for a\npage table. Instead of addressing the page table using the page number, they address it using\nthe frame number. In this case, there is one page table for the entire system. Since one frame is\ntypically uniquely mapped to a page in a process, each entry in this inverted page table contains\nthe process id, and page number. Figure 10.24(a) shows the structure of an inverted page table.\nThe main advantage of an inverted page table is that we do not need to keep a separate page\ntable for each process. We can save space if there are a lot of processes, and the size of physical\nmemory is small.\nThe main difficulty in inverted page tables is in performing a lookup for a virtual address.\nScanning all the entries is a very slow process, and is thus not practical. Hence, we need to\nhave a hashing function that maps the (process id, page number) pair to an index in a hash\ntable. This index in the hash table needs to point to an entry in the inverted page table. Since\nmultiple virtual addresses can point to the same entry in the hash table, it is necessary to verify\nthat the (process id, page number) matches that stored in the entry in the inverted page table.\nReaderscanreferto[Cormenetal., 2009]foradetailedexplanationofthetheoryandoperation\nof hash tables.\nWe show one scheme for using an inverted page table in Figure 10.24(b). After computing\na hash of the page number, and process id pair, we access a hashtable indexed by the contents\nof the hash. The contents of the hashtable entry point to a frame, f, that might possibly map\nto the given page. However, we need to verify, since it is possible that the hash function maps (cid:13)c Smruti R. Sarangi 516\nmultiple pages to the same frame. Subsequently, we access the inverted page table, and access\nthe entry, f. An entry of the inverted page table, contains the page number, process id pair\nthat is mapped to the given entry (or given frame). If we find that the contents do not match,\nthen we keep searching for the page number, process id pair in the subsequent K entries. This\nmethod is called linear probing (see [Cormen et al., 2009]), where we keep searching in the\ntarget data structure till we get a match. If we do not get a match within K entries, then we\nmay conclude that the page is not mapped. We need to then create a mapping, by evicting an\nentry (similar to caches), and writing it to a dedicated region in main memory that buffers all\nthe entries that are evicted from the inverted page table. We need to always guarantee that\nthe entry pointed to by the hash table, and the actual entry that contains the mapping, do not\ndiffer by more than K entries. If we do not find any free slots, then we need to evict an entry.\nAn astute reader might argue that we can directly use the output of the hashing engine to\naccess the inverted page table. Typically, we add accessing a hashtable as an intermediate step,\nbecause it allows us to have better control over the set of frames that are actually used. Using\nthis process, it is possible to disallow mappings for certain frames. These frames can be used\nfor other purposes. Lastly, we need to note that the overhead of maintaining, and updating\nhash tables outweighs the gains in having a system wide page table. Hence, an inverted page\ntable is typically not used in commercial systems.\nTranslation Lookaside Buffer (TLB)\nForeverysinglememoryaccessitisnecessarytolookupthepagetablefortranslatingthevirtual\naddress. The page table itself is stored in physical memory. Hence, we need to do a separate\nmemory access to read the corresponding entry of the page table. This approach doubles the\nnumber of memory accesses, and is thus very inefficient. However, we can minimise the number\nof extra memory accesses by maintaining a small cache of mappings in the processor. We\ntypically use a structure known as the Translation Lookaside Buffer (TLB) that is a small fully\nassociative cache. A TLB contains 32-64 entries. Each entry is indexed by the page number,\nand contains the corresponding frame number.\nOnce a memory address is calculated in the EX stage of the pipeline. It is sent to the\nTLB. The TLB is a very fast structure, and typically its access time is a fraction of a cycle.\nIf there is a TLB hit, then the physical address is ready by the time we reach the memory\naccess (MA) stage. The MA stage of the pipeline can then issue the read\/write request to the\nmemory system using the physical address obtained from the TLB. However, if there is a TLB\nmiss, then the pipeline needs to stall, and the page table needs to be accessed. This is a slow\nprocess and takes tens of cycles. Fortunately, the hit rate of a TLB is very high (\u2248 99%) in\nmost programs because of two reasons. First, programs have a high degree of temporal locality.\nSecond, a 64 entry TLB covers 256 KB of the virtual address space (assuming a 4 KB page).\nThe working set of most programs fits within this limit for small windows of time.\n10.4.4 Swap Space\nWe have solved the first problem, i.e., ensuring that processes do not overwrite each other\u2019s\ndata. Now, we need to solve the second problem, which is to ensure that our system can run\neven when the memory footprint of a program is more than the amount of physical memory.\nFor example, we might need to run a program with a memory footprint of 3 GB on a machine 517 (cid:13)c Smruti R. Sarangi\nwith only 512 MB of main memory. Even on regular desktop machines it is possible that the\ncombined memory footprint of all the processes is more than the size of main memory.\nTo support this requirement, we first need to find a location to save all the data that does\nnot fit in main memory. Most processors typically have peripheral devices connected to the\nprocessorsuchastheharddisk,orUSBflashdrivesthathavealargeamountofstoragecapacity.\nWe shall study about storage devices in detail in Chapter 12. In this section, we only need to\nappreciate the following aspects of such connected storage devices.\n1. Connected storage devices are very slow as compared to main memory. The access time\nto main memory is about 100-300 ns; whereas, the access time to a hard disk is of the\norder of milliseconds.\n2. Storage devices typically have several orders of magnitude more storage than main mem-\nory. A hard disk contains about 500 GB of storage in a system with 4 GB of main\nmemory.\n3. They are conceptually treated as a large array of bytes similar to the way we treat the\nmemory system. However, an address in the memory system is unrelated to the address\nin a hard disk. The storage device is not a part of the memory system.\n4. It is not necessary to have a storage device physically close to the processor. It can be\naccessible over the network, and be in another part of the world.\nA storage device can define an area known as the swap space that has space to contain all\nthe frames that cannot be saved in main memory. Furthermore, this storage region need not\nbe a superset of the main memory. If it is an extension of main memory, then we can define\na larger physical memory. For example, if we have 2 GB of main memory, and 3 GB of swap\nspace, then the total amount of physical memory can be 5 GB. In this case, if we need to\ndisplace a frame from main memory, then we need to allocate a location for it in swap space.\nAlternatively, the swap space can be inclusive. In the above example, we will effectively have 3\nGB of physical memory, and the main memory acts like a cache for the swap space. In either\ncase, the role of the swap space is to increase the amount of available physical memory.\nNow, the obvious question that arises is, \u201cHow does the memory system know if a frame is\npresent in main memory or the swap space? \u201d We can augment each entry in the page table\nwith an extra bit. If this bit is 1, then the frame is in main memory, else it is in the swap\nspace. Note that this system can be made more complicated also. Instead of one swap space,\nwe can have multiple swap spaces, and use multiple bits in a page table entry to indicate the\ncorresponding swap space.\n10.4.5 Memory Management Unit (MMU)\nUp till now we have not discussed how page tables are actually managed and stored. Let us\nconsiderthetypicallifecycleofaprocess. Whenaprocessbegins,thekernelallocatesaprimary\npagetableinmainmemory,andclearsofftheTLB.Itcantheninsertthemappingsforthetext,\nand data, sections. Secondly, the kernel can optionally allocate some space and insert some\nmappings for the heap, and stack sections. As long as there is a TLB hit, there is no problem.\nOnce, there is a TLB miss, it is necessary to access the page tables, and secondly, the pipeline (cid:13)c Smruti R. Sarangi 518\nneeds to stall. The job of accessing the page tables is typically handled by a dedicated unit\nknown as the MMU (memory management unit). It can either be a hardware structure, or a\nsoftware structure. If it is a hardware structure, then we have dedicated logic in the processor.\nOtherwise, it is necessary to invoke the MMU process by suspending the current process.\nIn either case, the operation of the MMU is the same. It needs to first locate the starting\naddress of the primary page table. Note that this address cannot be a virtual address. Other-\nwise, we will need a page table for a page table. It is typically a physical address that does need\nadditional translation. This starting address is either kept in a dedicated processor register\n(CR3 in x86), or in a designated location in physical memory. The MMU then needs to access\nthe appropriate entry in the primary page table, and get the address of the secondary page\ntable. The address of the secondary page table is another physical address. If a secondary page\ntable exists, then the MMU accesses the relevant entry in the secondary page table, and gets\nthe frame number. Subsequently, it evicts an entry from the TLB, and adds the new mapping.\nItcanfollowaLRUreplacementschemeasdescribedinSection10.2.5. Notethatitisnecessary\nto have all the page tables in the main memory. They cannot be in the swap space.\nPage Fault\nThere are several things that can go wrong in this process. If a page is being accessed for the\nfirsttime, itispossiblethatitmightnothaveasecondarypagetable, oritscorrespondingentry\ninthesecondarypagetablemightbeempty. Inthiscase, itisnecessarytofirstfindafreeframe\nin main memory, create a secondary page table if required, and then insert the mapping in the\nsecondary page table. To find a free frame in memory the MMU must maintain information\nabout each frame. This information can be kept in the form of a bit vector, where each bit\ncorresponds to a frame in main memory. If it is free, then the bit is 0, else if it is mapped, the\nbit is 1. If a free frame is available, then it can be used to map the new page. Otherwise, we\nneed to forcibly free a frame by writing its data to the swap space. The method of finding a\nframe to evict from main memory is known as the page replacement policy. Subsequently, we\nneed to change the page table entry of the page that was previously mapped to this frame. It\nneeds to now say that the page is available in swap space. Once a frame has been freed, it can\nbe mapped to another page.\nAlternatively, it is also possible that the entry in the page table indicates that the frame is\nthere in swap space. In this case, it is necessary to bring the frame into main memory. We first\nneed to find a free frame, or if necessary evict a frame from main memory. Then, we need to\ncreate an appropriate page table mapping.\nDefinition 115\nWhenever a page is not found in main memory, the event is known as a page fault.\nWhenever a page is not found in main memory, we term the event as a page fault. It is\nsubsequentlynecessarytocreateappropriatemappingsinthepagetableandfetchthedatafrom\nthe swap space. Fetching an entire page from the swap space is a rather slow operation, and\ntakes millions of cycles. Hence, it is very important for the MMU to manage pages efficiently. 519 (cid:13)c Smruti R. Sarangi\nIn specific, the page fault rate is very sensitive to the page replacement policy. Similar to\ncache block replacement policies, we can have different kinds of page replacement policies such\nas FIFO (first in first out), and LRU (least recently used). For more information on page\nreplacement policies, the reader is referred to a textbook on operating systems [Silberschatz\net al., 2008, Tanenbaum, 2007].\nMemory\naccess\nYes\nSend mapping\nTLB hit?\nto processor\nNo\nYes\nPage table Populate Send mapping\nhit? TLB to processor\nNo\nNo\nFree frame (1) Evict a frame\nto swap space\navailable?\n(2) Update its page\ntable entry\nYes\nRead in the new frame\nfrom swap space (if possible),\nor create a new empy frame\nCreate\/update mapping\nin the page table\nPopulate Send mapping\nTLB to processor\nFigure 10.25: The process of address translation\nFigure 10.25 summarises the major steps in the process of address translation.\n10.4.6 Advanced Features of the Paging System\nIt turns out that we can do several interesting things with the page table mechanism. Let us\nlook at a few examples. (cid:13)c Smruti R. Sarangi 520\nShared Memory\nLet us assume that two processes want to share some memory between each other such that\nthey can exchange data between them. Then each process needs to let the kernel know about\nthis. The kernel can then map two pages in both the virtual address spaces to the same frame.\nNow, each process can write to a page in its own virtual address space, and magically, the data\nwill get reflected in the virtual address space of the other process. It is sometimes necessary\nfor several processes to communicate among each other, and the shared memory mechanism is\none of the fastest methods.\nProtection\nComputer viruses typically change the code of a running process such that they can execute\ntheir own code. This is typically achieved by a giving a specific sequence of erroneous inputs to\nthe program. If appropriate checks are not in place, then the values of specific variables within\nthe program get overwritten. Some variables can get changed to pointers to the text section,\nand it is possible to exploit this mechanism to change instructions in the text section. It is\npossible to solve this problem by marking all the pages in text section as read-only. It will thus\nnot be possible to modify their contents in run time.\nSegmentation\nWe have been assuming that a programmer is free to layout the memory map according to\nher wish. She might for example decide to start the stack at a very high address such as\n0xFFFFFFF8. However, this code might not run on a machine that uses 16-bit addresses\nevenifthememoryfootprintoftheprogramisverysmall. Secondly, itispossiblethatacertain\nsystemmighthavereservedsomepartsofthevirtualmemoryandmadethemunavailabletothe\nprocess. For example, operating systems typically reserve the upper 1 or 2 GB for the kernel.\nTo solve these problems, we need to create another virtual layer on top of virtual memory.\nIn a segmented memory (used in x86 systems), there are specific segment registers for the\ntext, data, and stack sections. Each virtual address is specified as an offset to the specific\nsegment register. By default instructions use the code segment register, and data uses the data\nsegment register. The memory access (MA) stage of the pipeline adds the offset to the value\nstored in the segment register to generate the virtual address. Subsequently, the MMU uses\nthis virtual address to generate the physical address.\n10.5 Summary and Further Reading\n10.5.1 Summary\nSummary 10 521 (cid:13)c Smruti R. Sarangi\n1. A program perceives the memory system to be one large array of bytes. In practice,\nwe need to design a memory system that preserves this abstraction, and is also fast\nand power efficient.\n2. A physical memory system needs to be built out of SRAM and DRAM cells. An SRAM\narray is faster than a DRAM array. However, it takes much more area and consumes\nmuch more power. Building a memory with just DRAM cells will be too slow, and\nbuilding a memory with just SRAM cells will be consume too much power.\n3. We can use the properties of temporal and spatial locality to design more efficient\nmemory systems. Temporal locality refers to the fact that there is a high likelihood\nof the same data item being accessed again in the near future. Spatial locality means\nthat there is a high likelihood of adjacent memory locations being accessed in the near\nfuture.\n4. To utilise temporal locality, we build a hierarchical memory system of caches. A cache\nis a memory structure that contains a subset of all the memory locations.\n(a) The cache at the highest level is known as the L1 cache. It is small and fast.\n(b) The L2 cache is at the next level. It is larger and slower.\n(c) Some recent processors also have a third level of cache known as the L3 cache.\n(d) The last level in the memory system is known as the main memory. It is a large\nDRAM array of cells, and contains an entry for all the memory locations in the\nsystem.\n(e) Caches are typically inclusive. This means that a cache at a level i contains a\nsubset of memory locations present at level (i+1).\n5. To utilise spatial locality we group adjacent memory locations at the granularity of\n32-128 byte blocks.\n6. A cache contains a tag array and a data array. The tag array contains some of the\nbits of the address of the block, and the data array contains the contents of the block.\n7. The basic operations needed to implement a cache are \u2013 lookup, dataread, datawrite,\ninsert, replace, and evict.\n(a) There are three ways to store data in a cache \u2013 direct mapped, set associative,\nand fully associative.\n(b) It is necessary to evict a block in a set if all the ways are non-empty.\n(c) There are two major write policies \u2013 write-through (every write is immediately\nsent to the lower level), and write-back (writes are sent to the lower level, only\nupon an eviction)\n(d) Some of the prominent replacement policies are \u2013 Random, FIFO, and LRU. (cid:13)c Smruti R. Sarangi 522\n8. The average memory access time is given by:\nAMAT = L1 +L1 \u00d7L1\nhittime missrate misspenalty\n= L1 +L1 \u00d7(L2 +L2 \u00d7L2 )\nhittime missrate hittime missrate misspenalty\n9. There are three types of cache misses \u2013 compulsory, capacity, and conflict.\n10. Some of the methods and structures to optimise the memory system are: hardware\nprefetching, increased associativity\/block size, victim cache, compiler techniques, write\nbuffers, early restart and critical word first.\n11. We need virtual memory to ensure that:\n(a) Multiple programs do not overwrite each other\u2019s data unintentionally, or mali-\nciously.\n(b) The memory footprint of a program can be larger than the amount of available\nmain memory.\n12. To implement virtual memory, we divide a memory address into two parts \u2013 virtual\npage number, and an offset within a page. The virtual page number gets mapped to a\nphysical frame number. The mapping is stored in a structure called a page table.\n13. If a page is not found in main memory, then the event is known as a page fault.\nServicing a page fault takes millions of cycles. Hence, it is necessary to avoid page\nfaults by using sophisticated page replacement algorithms.\n14. Some of the advanced features of the virtual memory system include shared memory,\nprotection, and segmented addressing.\n10.5.2 Further Reading\nThe reader can refer to advanced text books on computer architecture by Henessey and Pat-\nterson [Hennessy and Patterson, 2012], Kai Hwang [Hwang, 2003], and Jean Loup Baer [Baer,\n2010] for a discussion on advanced memory systems. Specifically, the books discuss advanced\ntechniques for prefetching, miss rate reduction, miss penalty reduction, and compiler directed\napproaches. The reader can also refer to the book on memory systems by Bruce Jacob [Jacob,\n2009]. This book gives a comprehensive survey of most of the major techniques employed in\ndesigning state of the art memory systems till 2009. The book by Balasubramaniam, Jouppi,\nandMuralimanoharoncachehierarchiesalsodiscussessomeofthemoreadvancedtopicsonthe\nmanagementofcaches[Balasubramonianetal., 2011]. ManagingtheMMUismostlystudiedin\ncourses on operating systems [Tanenbaum, 2007, Silberschatz et al., 2008]. Research in DRAM\nmemories [Mitra, 1999], and systems using advanced memory technologies is a hot topic of\ncurrent research. A lot of research work is now focusing on phase change memories that do not\nrequirecostlyrefreshcycleslikeDRAM.ReaderscanrefertothebookbyQureshi,Gurumurthi,\nandRajendran[Qureshietal., 2011]forathoroughexplanationofmemorysystemsusingphase 523 (cid:13)c Smruti R. Sarangi\nchange memories.\nExercises\nOverview\nEx. 1 \u2014 Define temporal locality, and spatial locality.\nEx. 2 \u2014 Experimentallyverifythatthelog-normaldistributionisaheavytaileddistribution.\nWhat is the implication of a heavy tailed distribution in the context of the stack distance and\ntemporal locality?\nEx. 3 \u2014 Define the term, address distance. Why do we find the nearest match in the last K\naccesses?\nEx. 4 \u2014 How do we take advantage of temporal locality in the memory system?\nEx. 5 \u2014 How do we take advantage of spatial locality in the memory system?\nCaches and the Memory System\nEx. 6 \u2014 Consider a fully associative cache following the LRU replacement scheme and con-\nsisting of only 8 words. Consider the following sequence of memory accesses (the numbers\ndenote the word address):\n20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 22, 30, 21, 23, 31\nAssume that we begin when the cache is empty. What are the contents of the cache after the\nend of the sequence of memory accesses.\nEx. 7 \u2014 Answer Exercise 6 assuming a FIFO replacement scheme.\nEx. 8 \u2014 Consider a two-level cache using a write back policy. The L1 cache can store 2\nwords, and the L2 cache can store 4 words. Assume the caches to be fully associative (block\nsize = 1 word); they follow the LRU replacement scheme. Consider the following sequence of\nmemory accesses. The format of a write access is write <address> <value>, and the format\nfor a read access is read <address> .\nwrite 20 200\nwrite 21 300\nwrite 22 400\nwrite 23 500\nwrite 20 201\nwrite 21 301\nread 22\nread 23 (cid:13)c Smruti R. Sarangi 524\nwrite 22 401\nwrite 23 501\nWhat are the contents of the caches at the end of the sequence of memory accesses? What are\nthe contents of the caches, if we assume a write through policy ?\nEx. 9 \u2014 What is the total size (in bytes) of a direct mapped cache with the following con-\nfiguration in a 32 bit system? It has a 10 bit index, and a block size of 64 bytes. Each block\nhas 1 valid bit and 1 dirty bit.\nEx. 10 \u2014 Which sorting algorithm will have a better cache performance \u2013 bubble sort or\nselection sort? Explain your answer.\nEx. 11 \u2014 You have a cache with the following parameters:\n\u2022size : n bytes\n\u2022associativity : k\n\u2022block size : b bytes\nAssuming a 32-bit address space, answer the following:\n(a) What is the size of the tag in bits?\n(b) What is the size of the set index in bits?\n* Ex. 12 \u2014 Consider a direct mapped cache with 16 cache lines, indexed 0 to 15, where each\ncache line contains 32 integers (block size : 128 bytes).\nConsider a two-dimensional, 32\u00d732 array of integers a. This array is laid out in memory such\nthat a[0,0] is next to a[0,1], and so on. Assume the cache is initially empty, and a[0,0] maps\nto the first word of cache line 0.\nConsider the following column-first traversal:\nint sum = 0;\nfor (int i = 0; i < 32; i++) {\nfor( int j=0; j < 32; j++) {\nsum += a[i,j];\n}\n}\nand the following row-first traversal:\nint sum = 0;\nfor (int i = 0; i < 32; i++) {\nfor( int j=0; j < 32; j++) {\nsum += a[j,i];\n}\n} 525 (cid:13)c Smruti R. Sarangi\nCompare the number of cache misses produced by the two traversals, assuming the oldest cache\nline is evicted first. Assume that i, j, and sum are stored in registers, and that no part of array\na is saved in registers. It is always stored in the cache.\nEx. 13 \u2014 A processor has a baseline IPC of 1.5, an L1 miss rate of 5%, and an L2 miss rate\nof 50%. The hit time of the L1 cache is 1 cycle (part of the baseline IPC computation), the L2\nhit time is 10 cycles, and the L2 miss penalty is 100 cycles. Compute the final IPC. Assume\nthat all the miss rates are local miss rates.\nEx. 14 \u2014 Consider the designs shown below\nDesign Base CPI L1 local L2 local L1 hit L2 hit L2 miss\nmissrate missrate time time penalty\n(%) (%) (cycles) (cycles) (cycles)\nD 1 5 20 1 10 200\n1\nD 1.5 10 25 1 20 150\n2\nD 2 15 20 1 5 300\n3\nThe base CPI assumes that all the instructions hit in the L1 cache. Furthermore, assume that\na third of the instructions are memory instructions.\nWrite the formula for the average memory access time. What is the CPI of D , D and D ?\n1 2 3\n* Ex. 15 \u2014 Assume a cache that has n levels. For each level, the hit time is x cycles, and\nthe local miss rate is y per cycle.\n(a) What is the recursive formula for the average memory access time?\n(b) What is the average memory access time as n tends to \u221e?\n** Ex. 16 \u2014 Assume that you are given a machine with an unknown configuration. You\nneed to find out a host of cache parameters by measuring the time it takes to execute different\nprograms. These programs will be tailor made in such a way that they will reveal something\nabout the underlying system. For answering the set of questions, you need to broadly describe\nthe approach. Assume that the cache follows the LRU scheme for replacement.\n(a) How will you estimate the size of the L1 cache?\n(b) How will you estimate the L1 block size?\n(c) How will you estimate the L1 cache associativity?\nVirtual Memory\nEx. 17 \u2014 In a 32-bit machine with a 4 KB page size, how many entries are there in a single\nlevel page table? What is the size of each entry in the page table in bits?\nEx. 18 \u2014 Consider a 32-bit machine with a 4 KB page size, and a two level page table. If\nwe address the primary page table with 12 bits of the page address, then how many entries are\nthere in each secondary page table? (cid:13)c Smruti R. Sarangi 526\nEx. 19 \u2014 In a two level page table, should we index the primary page table with the most\nsignificant bits of the page address, or the least significant bits? Explain your answer.\nEx. 20 \u2014 We have a producer-consumer interaction between processes A and B. A writes\ndata that B reads in a shared space. However, B should never be allowed to write anything\ninto that shared space. How can we implement this using paging? How do we ensure that B\nwill never be able to write into the shared space?\nEx. 21 \u2014 Assume a process A, forks a process B. Forking a process means that B inherits a\ncopy of A\u2019s entire address space. However, after the fork call, the address spaces are separate.\nHow can we implement this using our paging mechanism?\nEx. 22 \u2014 How is creating a new thread different from a fork() operation in terms of memory\naddressing?\nEx. 23 \u2014 Most of the time, the new process generated by a fork call does not attempt to\nchange or modify the data inherited from the parent process. So is it really necessary to copy\nall the frames of the parent process to the child process? Can you propose an optimisation?\nEx. 24 \u2014 Explain the design of an inverted page table.\n* Ex. 25 \u2014 Calculate the expected value of the final CPI:\n\u2022Baseline CPI: 1\n\u2022Percentage of memory accesses: 30%\n\u2022TLB lookup time: 1 cycle (part of the baseline CPI)\n\u2022TLB miss rate: 20%\n\u2022Page table lookup time: 20 cycles (do not assume any page faults). Assume we can\ninstantaneously insert entries into the TLB.\n\u2022L1 cache hit time: 1 cycle (Part of the baseline CPI)\n\u2022L1 local miss rate: 10%\n\u2022L2 cache hit time: 20 cycles\n\u2022L2 local miss rate: 50%\n\u2022L2 miss penalty: 100 cycles\n** Ex. 26 \u2014 Most of the time, programmers use libraries of functions in their programs.\nThese libraries contain functions for standard mathematical operations, for supporting I\/O\noperations, and for interacting with the operating system. The machine instructions of these\nfunctionsareapartofthefinalexecutable. Occasionally,programmersprefertousedynamically\nlinkedlibraries(DLLs). DLLscontainthemachinecodeofspecificfunctions. However,theyare\ninvoked at run time, and their machine code is not a part of the program executable. Propose\na method to implement a method to load and unload DLLs with the help of virtual memory. 527 (cid:13)c Smruti R. Sarangi\nDesign Problems\nEx. 27 \u2014 You need to learn to use the CACTI tool (http:\/\/www.hpl.hp.com\/research\/\ncacti\/) to estimate the area, latency, and power of different cache designs. Assume a 4-way\nassociative 512 KB cache with 64 byte blocks. The baseline design has 1 read port, and 1 write\nport. You need to assume the baseline design, vary one parameter as mentioned below, and\nplot its relationship with the area, latency, or power consumption of a cache.\na)Plot the area versus the number of read ports.\nb)Plot the energy per read access versus the number of read ports.\nc)Plot the cache latency versus the associativity.\nd)Vary the size of the cache from 256 KB to 4 MB (powers of 2), and plot its relationship\nwith area, latency, and power.\nEx. 28 \u2014 Write a cache simulator that accepts memory read\/write requests and simulates\nthe execution of a hierarchical system of caches. (cid:13)c Smruti R. Sarangi 528 11\nMultiprocessor Systems\nUp till now, we have discussed the design and implementation of a processor in great detail\nincluding several methods to optimise its performance such as pipelining. We observed that\nby optimising the processor, and the memory system, it is possible to significantly increase the\nperformance of a program. Now, the question is, \u201cIs this enough?\u201d Or, is it possible to do\nbetter?\nA short answer to this question is \u201cMaybe Not.\u201d For a long answer to this question, the\nreader needs to read through this entire chapter, and possibly take a look at the references. Let\nus start out by saying that processor performance has its limits. It is not possible to increase\nthe speed of a processor indefinitely. Even with very complicated superscalar processors (see\nChapter 9), and highly optimised memory systems, it is typically not possible to increase the\nIPC by more than 50%. Secondly, because of power and temperature considerations, it is\nvery difficult to increase processor frequency beyond 3 GHz. The reader should note that\nprocessor frequencies have remained more of less the same over the last ten years (2002-2012).\nConsequently, CPU performance has also been increasing very slowly over the last ten years.\nWe illustrate these points in Figures 11.1, and 11.2. Figure 11.1 shows the peak frequency\nof processors released by multiple vendors such as Intel, AMD, Sun, Qualcomm, and Fujitsu\nfrom 2001 till 2010. We observe that the frequency has stayed more or less constant (mostly\nbetween 1 GHz to 2.5 GHz). The trends do not indicate a gradual increase in frequency. We\nexpect that in the near future also, the frequency of processors will be limited to 3 GHz.\nFigure 11.2 shows the average Spec Int 2006 score for the same set of processors from 2001\ntill 2010. We observe that CPU performance is slowly saturating over time, and it is getting\nincreasingly difficult to increase performance.\nEven though the performance of a single processor is not expected to significantly increase\nin the future, the future of computer architecture is not bleak. This is because processor\nmanufacturing technology is steadily getting better, and this is leading to smaller and faster\ntransistors. Till the late nineties processor designers were utilising the gains in transistor\ntechnology to increase the complexity of a processor by implementing more features. However,\n529 (cid:13)c Smruti R. Sarangi 530\nFigure 11.1: CPU frequencies (source [Danowitz et al., 2012])\nFigure 11.2: CPU performance (source [Danowitz et al., 2012])\ndue tolimitations in complexity, andpower, designers resortedto using simplerprocessors after\n2005. Insteadofimplementingmorefeaturesinprocessors, vendorsinsteaddecidedtoputmore\nthan one processor on a single chip. This helps us run more than one program at the same\ntime. Alternatively, sometimes it is possible to split a single program into multiple parts and\nrun all the parts in parallel.\nThis paradigm of using multiple computing units running in parallel is known as multipro-\ncessing. The term \u201cmultiprocessing\u201d is a rather generic term. It can either refer to multiple\nprocessors in the same chip working in parallel, or it can refer to multiple processors across\nchips working in parallel. A multiprocessor is a piece of hardware that supports multiprocess-\ning. When we have multiple processors within a chip, each processor is known as a core, and\nthe chip is called a multicore processor. 531 (cid:13)c Smruti R. Sarangi\nDefinition 116\nThetermmultiprocessingreferstomultipleprocessorsworkinginparallel. Thisisageneric\ndefinition, and it can refer to multiple processors in the same chip, or processors across\ndifferent chips. A multicore processor is a specific type of multiprocessor that contains all\nof its constituent processors in the same chip. Each such processor is known as a core.\nWe are now entering the era of multiprocessors, especially multicore systems. The number\nof cores per chip is increasing by roughly a factor of two every two years. New applications are\nbeing written to leverage this extra hardware. Most experts opine that the future of computing\nlies in multiprocessor systems.\nBefore proceeding to the design of different types of multiprocessors, let us quickly take a\nlook at the background and history of multiprocessors.\n11.1 Background\nIn the 60s and 70s large computers were primarily used by banks and financial institutions.\nThey had a growing base of consumers, and consequently they needed computers that could\nperform more and more transactions per second. Typically just one processor proved to be\ninsufficient in providing the computing throughput that was required. Hence, early computer\ndesigners decided to put multiple processors in a computer. The processors could share the\ncomputing load, and thus increase the computing throughput of the entire system.\nOne of the earliest multiprocessors was the Burroughs 5000, which had two processors \u2013\nA and B. A was the main processor, and B was an auxiliary processor. When the load was\nhigh, processor A gave processor B some work to do. Almost all the other major vendors at\nthat time also had multiprocessor offerings such as the IBM 370, PDP 11\/74, VAX-11\/782, and\nUnivac 1108-II. These computers supported a second CPU chip. This was connected to the\nmain processor. In all of these early machines, the second CPU was on a second chip that was\nphysically connected to the first with wires or cables. They came in two flavors: symmetric and\nasymmetric. Asymmetricmultiprocessorconsistsofmultipleprocessors,whereeachprocessoris\nofthesametype,andhasaccesstotheservicesofferedbytheoperatingsystem,andperipherals;\nwhereas, an asymmetric multiprocessor assigns different roles to different processors. There is\ntypically a distinguished processor that controls the operating system, and the peripherals.\nThe rest of the processors are slaves. They take work from the main processor, and return the\nresults.\nDefinition 117\nSymmetric Multiprocessing This paradigm treats all the constituent processors in a\nmultiprocessor system as the same. Each processor has equal access to the operating\nsystem, and the I\/O peripherals. These are also known as SMP systems. (cid:13)c Smruti R. Sarangi 532\nAsymmetric Multiprocessing This paradigm does not treat all the constituent proces-\nsors in a multiprocessor system as the same. There is typically one master processor\nthat has exclusive control of the operating system and I\/O devices. It assigns work to\nthe rest of the processors.\nIn the early days, the second processor was connected to the main processor using a set of\ncables. It was typically housed in a different area of the main computer. Note that in those\ndays, computers used to be the size of a room. With increased miniaturisation, gradually both\nthe processors started coming closer. In the late eighties and early nineties, companies started\nputtingmultipleprocessorsonthesamemotherboard. Amotherboardisaprintedcircuitboard\nthat contains all the chips that a computer uses. The reader can take the lid off her laptop or\ndesktop. The large green board with chips and metallic lines is the motherboard. By the late\nnineties, it was possible to have four or eight processors on a single motherboard. They were\nconnected to each other using dedicated high speed buses.\nGradually, the era of multicore processors commenced. It was now possible to have multiple\nprocessors in the same chip. IBM was the first to announce a dual core (2 cores) multicore\nprocessor called the Power 4 in 2001. Intel and AMD followed with similar offerings in 2005.\nAs of 2012, 8 core, and 10 core versions of multicore processors are available.\n11.1.1 Moore\u2019s Law\nLet us now take a deeper look at what happened between 1960 and 2012 in the world of\nprocessors. In the sixties, a computer was typically the size of a room, and today a computer\nfits in the pocket. A processor in a cell phone is around 1.6 million times faster than the IBM\n360 machines in the early sixties. It is also several orders of magnitude more power efficient.\nThe main driver for this continued evolution of computer technology is the miniaturisation of\nthe transistor. Transistors used to have a channel length of several millimetres in the sixties,\nand now they are about 20-30 nanometers long. In 1971, a typical chip used to have 2000-3000\ntransistors. Nowadays, a chip has billions of transistors.\nOver the last forty to fifty years, the number of transistors per chip has been roughly\ndoubling every 1-2 years. In fact, the co-founder of Intel, Gordon Moore, had predicted this\ntrend in 1965. The Moore\u2019s law (named in the honour of Gordon Moore) predicts that the\nnumber of transistors on a chip is expected to double every one to two years. Originally, Moore\nhad predicted the period of doubling to be every year. However, over time, this period has\nbecome about 2 years. This was expected to happen because of the steady rate of advances in\nmanufacturing technology, new materials, and fabrication techniques.\nHistorical Note 3\nIn 1965, Gordon Moore (co-founder of Intel) conjectured that the number of transistors on\na chip will double roughly every one to two years. Initially, the number of transistors was\ndoubling every year. Gradually, the rate slowed down to 18 months, and now it is about two\nyears. 533 (cid:13)c Smruti R. Sarangi\nThe Moore\u2019s law has approximately held true since it was proposed in the mid sixties.\n\u221a\nNowadays, almost every two years, the dimensions of transistors shrink by a factor of 2. This\nensures that the area of a transistor shrinks by a factor of 2, and thus it is possible to fit twice\nthe number of transistors on a chip. Let us define the feature size as the size of the smallest\nstructure that can be fabricated on a chip. Table 11.1 shows the feature sizes of Intel processors\n\u221a\nover the last 10 years. We observe that the feature size decreases by a factor of roughly 2\n(1.41) every two years. This results in a doubling of the number of transistors.\nYear Feature Size\n2001 130 nm\n2003 90 nm\n2005 65 nm\n2007 45 nm\n2009 32 nm\n2011 22 nm\nTable 11.1: Feature sizes between 2001 and 2012\n11.1.2 Implications of the Moore\u2019s Law\nNote that the Moore\u2019s law is an empirical law. However, because of the fact that it has\npredicted trends correctly for the last forty years, it is widely quoted in technical literature. It\ndirectly predicts a miniaturisation in the transistor size. A smaller transistor is more power\nefficientandfaster. Designersweretraditionallyusingthesebenefitstodesignbiggerprocessors\nwith extra transistors. They were using the additional transistor budget to add complexity to\ndifferent units, increase cache sizes, increase the issue width, and the number of functional\nunits. Secondly, the number of pipeline stages were also steadily increasing till about 2002,\nand there was an accompanying increase in the clock frequency also. However, after 2002 there\nwas a radical change in the world of computer architecture. Suddenly, power and temperature\nbecame major concerns. The processor power consumption figures started to exceed 100 W,\nand on chip temperatures started to exceed 100\u25e6C. These constraints effectively put an end to\nthe scaling in complexity, and clock frequencies of processors.\nInstead, designers started to pack more cores per chip without changing its basic design.\nThis ensured that the number of transistors per core remained constant, and according to\nMoore\u2019slawthenumberofcoresdoubledonceeverytwoyears. Thisstartedtheeraofmulticore\nprocessors, and processor vendors started doubling the number of cores on chip. As of 2012,\nwe have processors that have 8-10 cores per chip. The number of cores per chip are expected to\nreach 32 or 64 in the next 5 to 10 years (by 2020). A large multiprocessor today has multiple\ncores per chip, and multiple chips per system. For example, your author is at the moment\nwriting this book on a 32 core server. It has 4 chips, and each chip has 8 cores.\nAlong with regular multicore processors, there has been another important development.\nInstead of having 4 large cores per chip, there are architectures that have 64-256 very small\ncores on a chip such as graphics processors. These processors also follow the Moore\u2019s law,\nand are doubling their cores every 2 years. Such processors are increasingly being used in (cid:13)c Smruti R. Sarangi 534\ncomputer graphics, numerical and scientific computing. It is also possible to split the resources\nof a processor to make it support two program counters, and run two programs at the same\ntime. These special kind of processors are known as multithreaded processors. It is not possible\nto cover the entire design space of multiprocessors in this book. This is the topic of a book\non advanced architecture, and the reader can consult [Hwang, 2003, Hennessy and Patterson,\n2012, Culler et al., 1998] for a detailed description of different kinds of multiprocessors.\nIn this chapter, we wish to make the reader aware of the broad trends in multiprocessor\ndesign. Weshallfirstlookatmultiprocessingfromthepointofviewofsoftware. Onceweestab-\nlish the software requirements, we shall proceed to design hardware to support multiprocessing.\nWe shall broadly consider multicore, multithreaded, and vector processors in this chapter.\n11.2 Software for Multiprocessor Systems\n11.2.1 Strong and Loosely Coupled Multiprocessing\nLoosely Coupled Multiprocessing\nThere are two primary ways to harness the power of multiprocessors. The first method is to\nrun multiple unrelated programs in parallel. For example, it is possible to run a text editor\nand a web browser at the same time. The text editor can run on processor 1, and the web\nbrowser can run on processor 2. Both of them can occasionally request for OS services, and\nconnect to I\/O devices. Users often use large multiprocessor systems containing more than\n64-128 processors to run a set of jobs (processes) that are unrelated. For example, a user might\nwant to conduct a weather simulation with 128 different sets of parameters. Then she can start\n128 separate instances of the weather simulation software on 128 different processors on a large\nmultiprocessor system. We thus have a speedup of 128 times as compared to a single processor\nsystem, which is significant. This paradigm is known as loosely coupled multiprocessing. Here,\nthedependencesbetweenprogramsisalmostnegligible. Notethatusingamultiprocessorinthis\nmanner, is not conceptually very different from using a cluster of computers that comprises of\ncompletelyunrelatedmachinesthatcommunicateoveralocalareanetwork. Theonlydifference\nis that the latency between machines in a multiprocessor is lower than cluster computers. A\nloosely coupled multiprocessor such as a cluster of PCs is also known as a multicomputer.\nDefinition 118\nA multicomputer consists of a set of computers typically connected over the network. It\nis capable of running a set of programs in parallel, where the programs do not share their\nmemory space with each other.\nStrongly Coupled Multiprocessing\nHowever, the real benefit of a multiprocessor is accrued when there is a strong degree of overlap\nbetween different programs. This paradigm is known as strongly coupled multiprocessing. Here\nprograms can share their memory space, file and network connections. This method of using 535 (cid:13)c Smruti R. Sarangi\nmultiprocessors harnesses their true power, and helps us speed up a large amount of existing\nsoftware. The design and programming of strongly coupled multiprocessors is a very rich field,\nand is expected to grow significantly over the coming decade.\nDefinition 119\nLoosely Coupled Multiprocessing Running multiple unrelated programs in parallel on\na multiprocessor is known as loosely coupled multiprocessing.\nStrongly Coupled Multiprocessing Running a set of programs in parallel that share\ntheir memory space, data, code, file, and network connections is known as strongly\ncoupled multiprocessing.\nIn this book, we shall mainly look at strongly coupled multiprocessing, and primarily focus\non systems that allow a set of programs to run co-operatively by sharing a large amount of data\nand code.\n11.2.2 Shared Memory vs Message Passing\nLet us now explain the methods of programming multiprocessors. For ease of explanation, let\nusdrawananalogyhere. Consideragroupofworkersinafactory. Theyco-operativelyperform\na task by communicating with each other orally. A supervisor often issues commands to the\ngroup of workers, and then they perform their work. If there is a problem, a worker indicates\nit by raising an alarm. Immediately, other workers rush to his assistance. In this small and\nsimple setting, all the workers can hear each other, and see each other\u2019s actions. This proximity\nenables them to accomplish complex tasks.\nWe can alternatively consider another model, where workers cannot necessarily see or hear\neach other. In this case, they need to communicate with each other through a system of\nmessages. Messages can be passed through letters, phone calls, or emails. In this setting, if\na worker discovers a problem, he needs to send a message to his supervisor such that she can\ncomeandrectifytheproblem. Workersneedtobetypicallyawareofeachother\u2019sidentities, and\nexplicitly send messages to all or a subset of them. It is not possible any more to shout loudly,\nand communicate with everybody at the same time. However, there are some advantages of\nthis system. We can support many more workers because they do not have to be co-located.\nSecondly, since there are no constraints on the location of workers, they can be located at\ndifferent parts of the world, and be doing very different things. This system is thus far more\nflexible, and scalable.\nInspired by these real life scenarios, computer architects have designed a set of protocols for\nmultiprocessors following different paradigms. The first paradigm is known as shared memory,\nwhere all the individual programs see the same view of the memory system. If program A\nchanges the value of x to 5, then program B immediately sees the change. The second setting\nisknownasmessagepassing. Heremultipleprogramscommunicateamongeachotherbypassing\nmessages. The shared memory paradigm is more suitable for strongly coupled multiprocessors, (cid:13)c Smruti R. Sarangi 536\nand the message passing paradigm is more suitable for loosely coupled multiprocessors. Note\nthatitispossibletoimplementmessagepassingonastronglycoupledmultiprocessor. Likewise,\nit is also possible to implement an abstraction of a shared memory on an otherwise loosely\ncoupled multiprocessor. This is known as distributed shared memory [Keleher et al., 1994].\nHowever, this is typically not the norm.\nShared Memory\nLet us try to add n numbers in parallel using a multiprocessor. The code for it is shown in\nExample 121. We have written the code in C++ using the OpenMP language extension.\nExample 121\nWrite a shared memory program to add a set of numbers in parallel.\nAnswer: Letusassumethatallthenumbersarealreadystoredinanarraycallednumbers.\nThe array numbers has SIZE entries. Assume that the number of parallel sub-programs\nthat can be launched is equal to N.\n\/* variable declaration *\/\nint partialSums[N];\nint numbers[SIZE];\nint result = 0;\n\/* initialise arrays *\/\n...\n\/* parallel section *\/\n#pragma omp parallel {\n\/* get my processor id *\/\nint myId = omp_get_thread_num();\n\/* add my portion of numbers *\/\nint startIdx = myId * SIZE\/N;\nint endIdx = startIdx + SIZE\/N;\nfor(int jdx = startIdx; jdx < endIdx; jdx++)\npartialSums[myId] += numbers[jdx];\n}\n\/* sequential section *\/\nfor(int idx=0; idx < N; idx++)\nresult += partialSums[idx];\nIt is easy to mistake the code for a regular sequential program, except for the directive 537 (cid:13)c Smruti R. Sarangi\n#pragma omp parallel. This is the only extra semantic difference that we have added in our\nparallel program. It launches each iteration of this loop as a separate sub-program. Each such\nsub-program is known as a thread. A thread is defined as a sub-program that shares its address\nspace with other threads. It communicates with them by modifying the values of memory\nlocations in the shared memory space. Each thread has its own set of local variables that are\nnot accessible to other threads.\nThe number of iterations, or the number of parallel threads that get launched is a system\nparameter that is set in advance. It is typically equal to the number of processors. In this case,\nit is equal to N. Thus, N copies of the parallel part of the code are launched in parallel. Each\ncopy runs on a separate processor. Note that each of these copies of the program can access all\nthe variables that have been declared before the invocation of the parallel section. For example,\nthey can access partialSums, and the numbers arrays. Each processor invokes the function\nomp get thread num, which returns the id of the thread. Each thread uses the thread id to\nfind the range of the array that it needs to add. It adds all the entries in the relevant portion\nof the array, and saves the result in its corresponding entry in the partialSums array. Once\nall the threads have completed their job, the sequential section begins. This piece of sequential\ncode can run on any processor. This decision is made dynamically at runtime by the operating\nsystem, or the parallel programming framework. To obtain the final result it is necessary to\nadd all the partial sums in the sequential section.\nDefinition 120\nA thread is a sub-program that shares its address space with other threads. It has a dedicated\nprogram counter, and a local stack that it can use to define its local variables. We refer to\na thread as a software thread to distinguish it from a hardware thread that we shall define\nlater.\nA graphical representation of the computation is shown in Figure 11.3. A parent thread\nspawns a set of child threads. They do their own work, and finally join when they are done.\nThe parent thread takes over, and aggregates the partial results.\nThere are several salient points to note here. The first is that each thread has its separate\nstack. A thread can use its stack to declare its local variables. Once it finishes, all the local\nvariables in its stack are destroyed. To communicate data between the parent thread and the\nchild threads, it is necessary to use variables that are accessible to both the threads. These\nvariables need to be globally accessible by all the threads. The child threads can freely modify\nthese variables, and even use them to communicate amongst each other also. They are addi-\ntionally free to invoke the operating system, and write to external files and network devices.\nOnce, all the threads have finished executing, they perform a join operation, and free their\nstate. The parent thread takes over, and finishes the role of aggregating the results. Here, join\nis an example of a synchronisation operation between threads. There can be many other types\nof synchronisation operations between threads. The reader is referred to [Culler et al., 1998] for\na detailed discussion on thread synchronisation. All that the reader needs to understand is that\nthere are a set of complicated constructs that threads can use to perform very complex tasks\nco-operatively. Adding a set of numbers is a very simple example. Multithreaded programs can (cid:13)c Smruti R. Sarangi 538\nParent thread\nInitialisation\nSpawn child threads\nChild\nthreads\nThread join operation\nSequential\nsection\nFigure 11.3: Graphical representation of the program to add numbers in parallel\nbe used to perform other complicated tasks such as matrix algebra, and even solve differential\nequations in parallel.\nMessage Passing\nLet us now briefly look at message passing. Note that message passing based loosely coupled\nsystems are not the main focus area of this book. Hence, we shall just give the reader a flavor\nof message passing programs. Note that in this case, each program is a separate entity and\ndoes not share code, or data with other programs. It is a process, where a process is defined as\na running instance of a program. Typically, it does not share its address space with any other\nprocess.\nDefinition 121\nA process represents the running instance of a program. Typically, it does not share its\naddress space with any other process.\nLet us now quickly define our message passing semantics. We shall primarily use two\nemiT 539 (cid:13)c Smruti R. Sarangi\nfunctions, send and receive as shown in Table 11.2. The send(pid,val) function is used to send\nan integer (val) to the process whose id is equal to pid. The receive(pid) is used to receive\nan integer sent by a process whose id is equal to pid. If pid is equal to ANYSOURCE, then\nthe receive function can return with the value sent by any process. Our semantics is on the\nlines of the popular parallel programming framework, MPI (Message Passing Interface) [Snir\net al., 1995]. MPI calls have many more arguments, and their syntax is much more complicated\nthan our simplistic framework. Let us now consider the same example of adding n numbers in\nparallel in Example 122.\nFunction Semantics\nsend (pid, val) Send the integer, val, to the process with an id\nequal to pid\nreceive (pid) (1) Receive an integer from process pid\n(2) The function blocks till it gets the value\n(3) If the pid is equal to ANYSOURCE, then\nthe receive function returns with the value sent\nby any process\nTable 11.2: send and receive calls\nExample 122\nWrite a message passing based program to add a set of numbers in parallel. Make appro-\npriate assumptions.\nAnswer: Let us assume that all the numbers are stored in the array, numbers, and this\narray is available with all the N processors. Let the number of elements in the numbers\narray be SIZE. For the sake of simplicity, let us assume that SIZE is divisible by N.\n\/* start all the parallel processes *\/\nSpawnAllParallelProcesses();\n\/* For each process execute the following code *\/\nint myId = getMyProcessId();\n\/* compute the partial sums *\/\nint startIdx = myId * SIZE\/N;\nint endIdx = startIdx + SIZE\/N;\nint partialSum = 0;\nfor(int jdx = startIdx; jdx < endIdx; jdx++)\npartialSum += numbers[jdx];\n\/* All the non-root nodes send their partial sums to the root *\/ (cid:13)c Smruti R. Sarangi 540\nif(myId != 0) {\n\/* send the partial sum to the root *\/\nsend (0, partialSum);\n} else {\n\/* for the root *\/\nint sum = partialSum;\nfor (int pid = 1; pid < N; pid++) {\nsum += receive(ANYSOURCE);\n}\n\/* shut down all the processes *\/\nshutDownAllProcesses();\n\/* return the sum *\/\nreturn sum;\n}\n11.2.3 Amdahl\u2019s Law\nWe have now taken a look at examples for adding a set of n numbers in parallel using both the\nparadigmsnamelysharedmemoryandmessagepassing. Wedividedourprogramintotwoparts\n\u2013asequentialpartandaparallelpart(refertoFigure11.3). Intheparallelpartoftheexecution,\neach thread completed the work assigned to it, and created a partial result. In the sequential\npart, the root or master or parent thread initialised all the variables and data structures, and\nspawned all the child threads. After all the child threads completed (or joined), the parent\nthread aggregated the results produced by all the child threads. This process of aggregating\nresults is also known as reduction. The process of initialising variables, and reduction, are both\nsequential.\nLet us now try to derive the speedup of a parallel program vis-a-vis its sequential counter-\npart. LetusconsideraprogramthattakesT unitsoftimetoexecute. Letf bethefraction\nseq seq\nof time that it spends in its sequential part, and 1\u2212f be the fraction of time that it spends\nseq\nin its parallel part. The sequential part is unaffected by parallelism; however, the parallel part\ngets equally divided among the processors. If we consider a system of P processors, then the\nparallel part is expected to be sped up by a factor of P. Thus, the time (T ) that the parallel\npar\nversion of the program takes is equal to:\n(cid:18) (cid:19)\n1\u2212f\nseq\nT = T \u00d7 f + (11.1)\npar seq seq\nP\nAlternatively, the speedup (S) is given by:\nT 1\nseq\nS = = (11.2)\nT par f seq + 1\u2212 Pfseq 541 (cid:13)c Smruti R. Sarangi\nEquation 11.2 is known as the Amdahl\u2019s Law. It is a theoretical estimate (or rather the\nupper bound in most cases) of the speedup that we expect with additional parallelism.\n45\n10%\n40\n5%\n35 2%\n30\n25\n20\n15\n10\n5\n0\n0 50 100 150 200\nNumber of processors(P)\nFigure 11.4: Speedup (S) vs number of processors (P)\nFigure 11.4 plots the speedups as predicted by Amdahl\u2019s Law for three values of f (10%,\nseq\n5%, and 2%). We observe that with an increasing number of processors the speedup gradually\nsaturatesandtendstothelimitingvalue, 1\/f . Weobservediminishingreturnsasweincrease\nseq\nthe number of processors beyond a certain point. For example, for f = 5%, there is no\nseq\nappreciable difference in speedups between a system with 35 processors, and a system with 200\nprocessors. We approach similar limits for all three values of f . The important point to\nseq\nnote here is that increasing speedups by adding additional processors has its limits. We cannot\nexpect to keep getting speedups indefinitely by adding more processors, because we are limited\nby the length of the sequential sections in programs.\nTo summarise, we can draw two inferences. The first is that to speedup a program it is\nnecessary to have as much parallelism as possible. Hence, we need to have a very efficient\nparallel programming library, and parallel hardware. However, parallelism has its limits and\nit is not possible to increase the speedup appreciably beyond a certain limit. The speedup is\nlimited by the length of the sequential section in the program. To reduce the sequential section,\nwe need to adopt approaches both at the algorithmic level, and at the system level. We need\nto design our algorithms in such a way that the sequential section is as short as possible. For\nexample, in Examples 121, and 122, we can also perform the initialisation in parallel (reduces\nthe length of the sequential section). Secondly, we need a fast processor that can minimise the\ntime it takes to execute the sequential section.\nWe looked at the latter requirement (designing fast processors) in Chapters 8, 9, and 10.\nNow, let us look at designing fast and power efficient hardware for the parallel section.\n)S(pudeepS (cid:13)c Smruti R. Sarangi 542\n11.3 Design Space of Multiprocessors\nMichael J. Flynn proposed the famous Flynn\u2019s classification of multiprocessors in 1966. He\nstarted out by observing that an ensemble of different processors might either share code,\ndata, or both. There are four possible choices \u2013 SISD (single instruction single data), SIMD\n(singleinstructionmultipledata),MISD(multipleinstructionsingledata),andMIMD(multiple\ninstruction multiple data).\nLet us describe each of these types of multiprocessors in some more detail.\nSISD This is a standard uniprocessor with a single pipeline as described in Chapter 8 and\nChapter 9. A SISD processor can be thought of as a special case of the set of multipro-\ncessors with just a single processor.\nSIMD A SIMD processor can process multiple streams of data in a single instruction. For\nexample, a SIMD instruction can add 4 sets of numbers with a single instruction. Modern\nprocessors incorporate SIMD instructions in their instruction set, and have special SIMD\nexecution units also. Examples include x86 processors that contain the SSE set of SIMD\ninstructionsets. Graphicsprocessors, andvectorprocessorsarespecialexamplesofhighly\nsuccessful SIMD processors.\nMISD MISD systems are very rare in practice. They are mostly used in systems that have\nvery high reliability requirements. For example, large commercial aircraft typically have\nmultiple processors running different versions of the same program. The final outcome\nis decided by voting. For example, a plane might have a MIPS processor, an ARM\nprocessor, and an x86 processor, each running different versions of the same program\nsuch as an autopilot system. Here, we have multiple instruction streams, yet a single\nsource of data. A dedicated voting circuit computes a majority vote of the three outputs.\nFor example, it is possible that because of a bug in the program or the processor, one\nof the systems can erroneously take a decision to turn left. However, both of the other\nsystems might take the correct decision to turn right. In this case, the voting circuit will\ndecide to turnright. Since MISD systems are hardly everused in practice otherthan such\nspecialised examples, we shall not discuss them any more in this book.\nMIMD MIMD systems are by far the most prevalent multiprocessor systems today. They\nhave multiple instruction streams and multiple data streams. Multicore processors, and\nlarge servers are all MIMD systems. Examples 121 and 122 pertained to MIMD systems.\nWe need to carefully explain the meaning of multiple instruction streams. This means\nthat instructions come from multiple sources. Each source has its unique location, and\nassociatedprogramcounter. TwoimportantbranchesoftheMIMDparadigmhaveformed\nover the last few years.\nThe first is SPMD (single program multiple data), and the second is MPMD (multiple\nprogram multiple data). Most parallel programs are written in the SPMD style (Exam-\nple 121 and 122). Here, multiple copies of the same program run on different cores, or\nseparate processors. However, each individual processing unit has a separate program\ncounter, and thus perceives a different instruction stream. Sometimes SPMD programs\nare written in such a way that they perform different actions depending on their thread 543 (cid:13)c Smruti R. Sarangi\nids. We saw a method in Example 121 on how to achieve this using OpenMP functions.\nThe advantage of SPMD is that we do not have to write different programs for differ-\nent processors. Parts of the same program can run on all the processors, though their\nbehaviour might be different.\nAcontrastingparadigmisMPMD.Here, theprogramsrunningondifferentprocessorsare\nactuallydifferent. Theyaremoreusefulforspecialisedprocessorsthathaveheterogeneous\nprocessing units. There is typically a single master program that assigns work to slave\nprograms. The slave programs complete the quanta of work assigned to them, and then\nreturn the results to the master program. The nature of work of both the programs is\nactually very different, and it is often not possible to seamlessly combine them into one\nprogram.\nFrom the above description, it is clear that the systems that we need to focus on are SIMD\nand MIMD. MISD systems are very rarely used, and thus will not be discussed anymore. Let\nus first discuss MIMD multiprocessing. Note that we shall only describe the SPMD variant of\nMIMD multiprocessing because it is the most common approach.\n11.4 MIMD Multiprocessors\nLet us now take a deeper look at strongly-coupled shared memory based MIMD machines. We\nshall first take a look at them from the point of view of software. After we have worked out\na broad specification of these machines from the point of view of software, we can proceed to\ngive a brief overview of the design of the hardware. Note that the design of parallel MIMD\nmachines can take an entire book to describe. For additional information, or for added clarity,\nthe reader can refer to the following references [Culler et al., 1998, Sorin et al., 2011].\nLet us call the software interface of a shared memory MIMD machine as the \u201clogical point\nof view\u201d, and refer to the actual physical design of the multiprocessor as the \u201cphysical point\nof view\u201d. When we describe the logical point of view, we are primarily interested in how\nthe multiprocessor behaves with respect to software. What guarantees does the hardware make\nregardingitsbehaviour, andwhatcansoftwareexpect? Thisincludescorrectness, performance,\nand even resilience to failures. The physical point of view is concerned with the actual design\nof the multiprocessor. This includes the physical design of the processors, the memory system,\nand the interconnection network. Note that the physical point of view has to conform to the\nlogicalpointofview. Thereaderwillrecallthatwearetakingasimilarapproachhereaswedid\nfor uniprocessors. We first explained the software view (architecture) by looking at assembly\ncode. Then we provided an implementation for the assembly code by describing a pipelined\nprocessor (organisation). We shall follow a similar approach here.\n11.4.1 Logical Point of View\nFigure 11.5 shows a logical view of a shared memory MIMD multiprocessor. Each processor\nis connected to the memory system that saves both code and data. The program counter of\neach processor points to the location of the instruction that it is executing. This is in the code\nsection of memory. This section is typically read only, and thus is not affected by the fact that\nwe have multiprocessors. (cid:13)c Smruti R. Sarangi 544\nProc 1 Proc 2 Proc n\nShared memory\nFigure 11.5: Logical view of a multiprocessor system\nThemainchallengeinimplementingasharedmemorymultiprocessorisincorrectlyhandling\ndata accesses. Figure 11.5 shows a scheme in which each computing processor is connected to\nthe memory, and it is treated as a black box. If we are considering a system of processes with\ndifferentvirtualaddressspaces,thenthereisnoproblem. Eachprocessorcanworkonitsprivate\ncopy of data. Since the memory footprints are effectively disjoint, we can easily run a set of\nparallel processes in this system. However, the main complexity arises when we are looking at\nshared memory programs that have multiple threads, and there is data sharing across threads.\nNote that we can also share memory across processes by mapping different virtual pages to the\nsame physical frame as described in Section 10.4.6. We shall threat this scenario as a special\ncase of parallel multi-threaded software.\nA set of parallel threads typically share their virtual and physical address spaces. However,\nthreads do have private data also, which is saved in their stacks. There are two methods to\nimplement disjoint stacks. The first is that all the threads can have identical virtual address\nspaces, and different stack pointers can start at different points in the virtual address space.\nWe need to further ensure that the size of the stack of a thread is not large enough to overlap\nwith the stack of another thread. Another approach is to map the stack portion of the virtual\naddress space of different threads to different memory frames. Thus, each thread can have\ndifferent entries in its page table for the stack portion, yet have common entries for the rest\nof the sections of the virtual address space such as code, read-only data, constants, and heap\nvariables.\nIn any case, the main problems of complexity of parallel software are not because of code\nthat is read-only, or local variables that are not shared across threads. The main problem is\ndue to data values that are potentially shared across multiple threads. This is what gives the\npower to parallel programs, and also makes them very complex. In the example that we showed\nfor adding a set of numbers in parallel, we can clearly see the advantage that we obtain by\nsharing values and results of computation through shared memory.\nHowever, sharing values across threads is not that simple. It is actually a rather profound\ntopic, and advanced texts on computer architecture devote several chapters to this topic. We\nshall briefly look at two important topics in this area namely coherence, and memory consis-\ntency. Coherence is also known as cache coherence, when we refer to it in the context of caches.\nHowever,thereaderneedstobeawarethatcoherenceisjustnotlimitedtocaches,itisageneric\nterm. 545 (cid:13)c Smruti R. Sarangi\n11.4.2 Coherence\nThe term coherence in the memory system refers to the way multiple threads access the same\nlocation. Weshallseethatmanydifferentbehavioursarepossible,whenmultiplethreadsaccess\nthe same memory location. Some of the behaviours are intuitively wrong, yet possible. Before\nlooking at coherence, we need to note that inside the memory system, we have many different\nentities such as caches, write buffers, and different kinds of temporary buffers. Processors\ntypically write values to temporary buffers, and resume their operation. It is the job of the\nmemory system to transfer the data from these buffers to a location in the cache subsystem.\nIt is thus possible that internally, a given memory address might be associated with many\ndifferent physical locations at a given point of time. Secondly, the process of transferring data\nfrom the processor to the correct location in the memory system (typically a cache block) is\nnot instantaneous. It sometimes takes more than tens of cycles for the memory read or write\nrequest to reach its location. Sometimes these memory request messages can wait even longer,\nif there is a lot of memory traffic. Messages can also get reordered with other messages that\nwere sent after them.\nHowever, for the moment, let us assume that the memory looks like a large array of bytes to\nall the processors; although, internally it is a complicated network of different components that\nstrivetoprovideasimplelogicalabstractionforread\/writeoperations. Theinternalcomplexity\nof a multiprocessor memory system leads to several interesting behaviours for programs that\naccess the same set of shared variables. Let us consider a set of examples.\nIn each of these examples, all shared values are initialised to 0. All the local variables start\nwith t such as t1, t2, and t3. Let us say that thread 1 writes to a variable x that is shared\nacross threads. Immediately later, thread 2 tries to read its value.\nThread 1: Thread 2:\nx = 1 t1 = x\nIs thread 2 guaranteed to read 1? Or, can it get the previous value 0? What if thread\n2, reads the value of x, 2 ns later, or even 10 ns later? What is the time that it takes for a\nwrite in one thread to propagate to the other threads? This depends on the implementation\nof the memory system. If a memory system has fast buses, and fast caches, then a write can\npropagate very quickly to other threads. However, if the buses and caches are slow then it can\ntake some time for other threads to see a write to a shared variable.\nNow, let us further complicate the example. Let us assume that thread 1 writes to x twice.\nExample 123\nThread 1: Thread 2:\nx = 1 t1 = x\nx = 2 t2 = x (cid:13)c Smruti R. Sarangi 546\nLet us now look at the set of possible outcomes. (t1,t2)=(1,2) is possible. (t1,t2) = (0,1) is\nalso possible. This is possible when t1 was written before thread 1 started, and t2 was written\nafter the first statement of thread 1 completed. Likewise we can systematically enumerate\nthe set of all possible outcomes, which are: (0,0), (0,1), (0,2), (1,1), (1,2), (2,2). The reader is\nrequestedtowriteasimpleprogramusingaparallelmultithreadedframeworksuchasOpenMP\nor pthreads and look at the set of possible outcomes. The interesting question is whether the\noutcome (2,1) is possible? This might be possible if somehow the first write to x got delayed\nin the memory system, and the second write overtook it. The question is whether we should\nallow such behaviour.\nThe answer is NO. If we were to allow such behaviour, then implementing a multiprocessor\nmemory system would undoubtedly become simpler. However, it will become very difficult to\nwrite and reason about parallel programs. Hence, most multiprocessor systems disallow such\nbehaviour.\nLet us now look at the issue of accesses to the same memory location by multiple threads\nslightly more formally. Let us define the term, coherence, as he behaviour of memory accesses\nto the same memory address such as x in our examples. We ideally want our memory system\nto be coherent. This basically means that it should observe a set of rules while dealing with\ndifferent accesses to the same memory address such that it is easier to write programs.\nDefinition 122\nThe behaviour of memory accesses to the same memory address is known as coherence.\nTypically, coherence has two axioms. These are as follows:\n1. Completion A write must ultimately complete.\n2. Order All the writes to the same memory address need to be seen by all the threads in\nthe same order.\nBoth of these axioms are fairly sublime in nature. The completion axiom says that no write\nis ever lost in the memory system. For example, it is not possible that we write a value of 10\nto variable x, and the write request gets dropped by the memory system. It needs to reach\nthe memory location corresponding to x, and then it needs to update its value. It might get\noverwritten later by another write request. However, the bottom line is that the write request\nneeds to update the memory location at some point of time in the future.\nThe order axiom says that all the writes to a memory location are perceived to be in the\nsame order by all the threads. This means that it is not possible to read (2,1) in Example 123.\nLet us now explain the reasons for this. Thread 1 is aware that 2 was written after 1 to the\nmemory location x. By the second axiom of coherence, all other threads need to perceive the\nsame order of writes to x. Their view of x cannot be different from that of thread 1. Hence,\nthey cannot read 2 after 1. If we think about it, the axioms of coherence make intuitive sense.\nThey basically mean that all writes eventually complete, as is true for uniprocessor systems.\nSecondly, all the processors see the same view of a single memory location. If its value changes\nfrom0to1to2, thenalltheprocessorsseethesameorderofchanges(0-1-2). Noprocessorsees 547 (cid:13)c Smruti R. Sarangi\nthe updates in a different order. This further means that irrespective of how a memory system\nisimplementedinternally, externallyeachmemorylocationisseenasagloballyaccessiblesingle\nlocation.\n11.4.3 Memory Consistency\nOverview\nCoherence was all about accesses to the same memory location. What about access to different\nmemory locations? Let us explain with a series of examples.\nExample 124\nThread 1: Thread 2:\nx = 1 t1 = y\ny = 1 t2 = x\nLet us look at the permissible values of t1, and t2 from an intuitive standpoint. We can\nalways read (t1,t2)=(0,0). This can happen when thread 2 is scheduled before thread 1. We\ncanalsoread(t1,t2)=(1,1). Thiswillhappenwhenthread2isscheduledafterthread1finishes.\nLikewise it is possible to read (t1,t2)=(0,1). Figure 11.6 shows how we can get all the three\noutcomes.\nt1 = y x = 1 x = 1\nt2 = x t1 = y y = 1\nx = 1 t2 = x t1 = y\ny = 1 y = 1 t2 = x\n(0,0) (0,1) (1,1)\nFigure 11.6: Graphical representation of all the possible outcomes\nThe interesting question is whether (t1,t2)=(1,0) is allowed? This will happen when the\nwrite to x is somehow delayed by the memory system, whereas the write toy completes quickly.\nIn this case t1 will get the updated value of y, and t2 will get the old value of x. The question is\nwhether such kind of behaviour should be allowed. Clearly if such kind of behaviour is allowed\nit will become hard to reason about software, and the correctness of parallel algorithms. It will\nalso become hard to program. However, if we allow such behaviour then our hardware design\nbecomes simpler because we do not have to provide strong guarantees to software.\nThere is clearly no right or wrong answer? It all depends on how we want to program\nsoftware, and what hardware designers want to build for software writers. But, still there is\nsomething very profound about this example, and the special case of (t1,t2) equal to (1,0). To\nfind out why, let us take a look again at Figure 11.6. In this figure, we have been able to reason (cid:13)c Smruti R. Sarangi 548\nabout three outcomes by creating an interleaving between the instructions of the two threads.\nIn each of these interleavings, the order of instructions in the same thread is the same as the\nway it is specified in the program. This is known as program order.\nDefinition 123\nAn order of instructions (possibly belonging to multiple threads) that is consistent with the\ncontrol-flowsemanticsofeachconstituentthreadissaidtobeinprogramorder. Thecontrol-\nflow semantics of a thread is defined as the set of rules that determine which instructions\ncan execute after a given instruction. For example, the set of instructions executed by a\nsingle cycle processor is always in program order.\nObservation: It is clear that we cannot generate the outcome (t1,t2)=(1,0) by interleaving\nthreads in program order.\nIt would be nice if we can somehow exclude the output (1,0) from the set of possible\noutputs. It will allow us to write parallel software, where we can predict the possible outcomes\nvery easily. A model of the memory system that determines the set of possible outcomes for\nparallel programs is known as a memory model.\nDefinition 124\nThe model of a memory system that determines the set of likely outcomes for parallel pro-\ngrams is known as a memory model.\nSequential Consistency\nWe can have different kinds of memory models corresponding to different kinds of processors.\nOne of the most important memory models is known as sequential consistency(SC). Sequential\nconsistencystatesthatonlythoseoutcomesareallowedthatcanbegeneratedbyaninterleaving\nof threads in program order. This means that all the outcomes shown in Figure 11.6 are\nallowed because they are generated by interleaving thread 1 and 2 in all possible ways, without\nviolating their program order. However, the outcome (t1,t2)=(1,0) is not allowed because it\nviolates program order. Hence, it is not allowed in a sequentially consistent memory model.\nNote that once we interleave multiple threads in program order, it is the same as saying that\nwe have one processor that executes an instruction of one thread in one cycle and possibly\nanother instruction from some other thread in the next cycle. Hence, a uniprocessor processing\nmultiple threads produces a SC execution. In fact, if we think about the name of the model,\nthe word \u201csequential\u201d comes from the notion that the execution is equivalent to a uniprocessor\nsequentially executing the instructions of all the threads in some order. 549 (cid:13)c Smruti R. Sarangi\nDefinition 125\nA memory model is sequentially consistent if the outcome of the execution of a set of parallel\nthreads is equivalent to that of a single processor executing instructions from all the threads\ninsomeorder. Alternatively, wecandefinesequentialconsistencyasamemorymodelwhose\nset of possible outcomes are those that can be generated by interleaving a set of threads in\nprogram order.\nSequential consistency is a very important concept and is widely studied in the fields of\ncomputer architecture, and distributed systems. It reduces a parallel system to a serial system\nwith one processor by equating the execution on a parallel system with the execution on a\nsequential system. An important point to note is that SC does not mean that the outcome of\nthe execution of a set of parallel programs is the same all the time. This depends on the way\nthat the threads are interleaved, and the time of arrival of the threads. All that it says that\ncertain outcomes are not allowed.\nWeak Consistency (WC)*\nThe implementation of SC comes at a cost. It makes software simple, but it makes hardware\nveryslow. TosupportSCitisoftennecessarytowaitforareadorwritetocomplete, beforethe\nnext read or write can be sent to the memory system. A write request W completes when all\nsubsequent reads by any processor will get the value that W has written, or the value written\nby a later write to the same location. A read request completes, after it reads the data, and\nthe write request that originally wrote the data completes.\nThese requirements\/restrictions become a bottleneck in high performance systems. Hence,\nthe computer architecture community has moved to weak memory models that violate SC. A\nweak memory model will allow the outcome (t1,t2)=(1,0) in the following multithreaded code\nsnippet.\nThread 1: Thread 2:\nx = 1 t1 = y\ny = 1 t2 = x\nDefinition 126\nA weakly consistent (WC) memory model does not obey SC. It typically allows arbitrary\nmemory orderings.\nThere are different kinds of weak memory models. Let us look at a generic variant, and\ncall it weak consistency (WC). Let us now try to find out why WC allows the (1,0) outcome.\nAssume that thread 1 is running on core 1, and thread 2 is running on core 2. Moreover,\nassume that the memory location corresponding to x is near core 2, and the memory location\ncorrespondingtoy isnearcore1. Alsoassumethatittakestensofcyclestosendarequestfrom (cid:13)c Smruti R. Sarangi 550\nthe vicinity of core 1 to core 2, and the delay is variable. Let us first investigate the behaviour\nof the pipeline of core 1. From the point of view of the pipeline of core 1, once a memory write\nrequest is handed over to the memory system, the memory write instruction is deemed to have\nfinished. The instruction moves on to the RW stage. Hence, in this case, the processor will\nhand over the write to x to the memory system in the nth cycle, and subsequently pass on the\nwrite to y in the (n+1)th cycle. The write to y will reach the memory location of y quickly,\nwhile the write to x will take a long time.\nIn the meanwhile, core 2 will try to read the value of y. Assume that the read request\narrives at the memory location of y just after the write request (to y) reaches it. Thus, we will\nget the new value of y, which is equal to 1. Subsequently, core 2 will issue a read to x. It is\npossible that the read to x reaches the memory location of x just before the write to x reaches\nit. In this case, it will fetch the old value of x, which is 0. Thus, the outcome (1,0) is possible\nin a weak memory model.\nNow,toavoidthissituation,wecouldhavewaitedforthewritetoxtocompletefully,before\nissuing the write request to y. It is true that in this case, this would have been the right thing\nto do. However, in general when we are writing to shared memory locations, other threads are\nnot reading them at exactly the same point of time. We have no way of distinguishing both\nthe situations at run time since processors do not share their memory access patterns between\neach other. Hence, in the interest of performance, it is not worthwhile to delay every memory\nrequest till the previous memory requests complete. High performance implementations thus\nprefer memory models that allow memory accesses from the same thread to be reordered by the\nmemorysystem. Weshallinvestigatewaysofavoidingthe(1,0)outcomeinthenextsubsection.\nLet us summarise our discussion that we have had on weak memory models by defining\nthe assumptions that most processors make. Most processors assume that a memory request\ncompletes instantaneously at some point of time after it leaves the pipeline. Furthermore, all\nthe threads assume that a memory request completes instantaneously at exactly the same point\nof time. This property of a memory request is known as atomicity. Secondly, we need to note\nthat the order of completion of memory requests might differ from their program order. When\nthe order of completion is the same as the program order of each thread, the memory model\nobeys SC. If the completion order is different from the program order, then the memory model\nis a variant of WC.\nDefinition 127\nA memory request is said to be atomic or observe atomicity, when it is perceived to execute\ninstantaneously by all threads at some point of time after it is issued.\nImportant Point 17\nTo be precise, for every memory request, there are three events of interest namely start,\nfinish, and completion. Let us consider a write request. The request starts when the in-\nstruction sends the request to the L1 cache in the MA stage. The request finishes, when\nthe instruction moves to the RW stage. In modern processors, there is no guarantee that 551 (cid:13)c Smruti R. Sarangi\nthe write would have reached the target memory location when the memory request finishes.\nThe point of time at which the write request reaches the memory location, and the write\nis visible to all the processors, is known as the time of completion. In simple processors,\nthe time of completion of a request, is in between the start and finish times. However, in\nhigh performance processors, this is not the case. This concept is shown in the following\nillustration.\ntime\nstart finish complete\nWhat about a read request? Most readers will naively assume that the completion time\nof a read is between the start and finish times, because it needs to return with the value of\nthe memory location. This is however not strictly true. A read might return the value of\na write that has not completed. In a memory model that requires write atomicity (illusion\nof writes completing instantaneously), a read completes, only when the corresponding write\nrequest completes. All the memory consistency models that assume write atomicity are\ndefined using properties of the memory access completion order.\nTrivia 4\nHere, is an incident from your author\u2019s life. He had 200 US dollars in his bank account.\nHe had gotten a cheque for 300$ from his friend. He went to his bank\u2019s nearest ATM, and\ndeposited the cheque. Three days later, he decided to pay his rent (400$). He wrote a cheque\nto his landlord, and sent it to his postal address. A day later, he got an angry phone call\nfrom his landlord informing him that his cheque had bounced. How was this possible?\nYour author then enquired. It had so happened that because of a snow storm, his bank\nwas not able to send people to collect cheques from the ATM. Hence, when his landlord\ndeposited the cheque, the bank account did not have sufficient money.\nThis example is related with the problem of memory consistency. Your author leaving\nhis house to drop the cheque in the ATM is the start time. He finished the job when he\ndropped the cheque in the ATM\u2019s drop box. However, the completion time was 5 days later,\nwhen the amount was actually credited to his account. Concurrently, another thread (his\nlandlord) deposited his cheque, and it bounced. This is an example of weak consistency in\nreal life.\nThere is an important point to note here. In a weak memory model, the ordering between\nindependent memory operations in the same thread is not respected. For example, when we\nwrote to x, and then to y, thread 2 perceived them to be in the reverse order. However, (cid:13)c Smruti R. Sarangi 552\nthe ordering of operations of dependent memory instructions belonging to the same thread is\nalways respected. For example, if we set the value of a variable x to 1, and later read it in\nthe same thread. We will either get 1 or the value written by a later write to x. All the\nother threads will perceive the memory requests to be in the same order. There is NEVER\nany memory order violation between between dependent memory accesses by the same thread\n(refer to Figure 11.7).\ncompletion\ntime\nThread 1 Thread 2\nx = 1 t1 = y\ny = 1 t2 = x\nt3 = x\nFigure 11.7: Actual completion time of memory requests in a multithreaded program\nExamples\nLet us now illustrate the difficulty with using a weak memory model that does not obey any\nordering rules. Let us write our program to add numbers in parallel assuming a sequentially\nconsistent system. Note that here we do not use OpenMP because OpenMP does a lot behind\nthe scenes to ensure that programs run correctly in machines with weak memory models. Let\nus define a parallel construct that runs a block of code in parallel, and a getThreadId() function\nthat returns the identifier of the thread. The range of the thread ids is from 0 to N \u22121. The\ncode for the parallel add function is shown in Example 125. We assume that before the parallel\nsection begins, all the arrays are initialised to 0. In the parallel section, each thread adds its\nportion of numbers, and writes the result to its corresponding entry in the array, partialSums.\nOnce, it is done, it sets its entry in the finished array to 1.\nLet us now consider the thread that needs to aggregate the results. It needs to wait for\nall the threads to finish the job of computing the partial sums. It does this by waiting till all\nthe entries of the finished array are equal to 1. Once, it establishes that all the entries in the\nfinished array are equal to 1, it proceeds to add all the partial sums to get the final result.\nThe reader can readily verify that if we assume a sequentially consistent system then this piece\nof code executes correctly. She needs to note that we compute the result, only when we read\nall the entries in the array finished to be 1. An entry in the finished array is equal to 1, if\nthe partial sum is computed, and written to the partialSums array. Since we add the elements\nof the partialSums array to compute the final result, we can conclude that it is calculated 553 (cid:13)c Smruti R. Sarangi\ncorrectly. Note that this is not a formal proof (left as an exercise for the reader).\nExample 125\nWrite a shared memory program to add a set of numbers in parallel on a sequentially\nconsistent machine.\nAnswer: Letusassumethatallthenumbersarealreadystoredinanarraycallednumbers.\nThe array numbers has SIZE entries. The number of parallel threads is given by N.\n\/* variable declaration *\/\nint partialSums[N];\nint finished[N];\nint numbers[SIZE];\nint result = 0;\nint doneInit = 0;\n\/* initialise all the elements in partialSums and finished to 0 *\/\n...\ndoneInit = 1;\n\/* parallel section *\/\nparallel {\n\/* wait till initialisation *\/\nwhile (!doneInit()){};\n\/* compute the partial sum *\/\nint myId = getThreadId();\nint startIdx = myId * SIZE\/N;\nint endIdx = startIdx + SIZE\/N;\nfor(int jdx = startIdx; jdx < endIdx; jdx++)\npartialSums[myId] += numbers[jdx];\n\/* set an entry in the finished array *\/\nfinished[myId] = 1;\n}\n\/* wait till all the threads are done *\/\ndo {\nflag = 1;\nfor (int i=0; i < N; i++){\nif(finished[i] == 0){\nflag = 0;\nbreak;\n}\n} (cid:13)c Smruti R. Sarangi 554\n} while (flag == 0);\n\/* compute the final result *\/\nfor(int idx=0; idx < N; idx++)\nresult += partialSums[idx];\nNow, let us consider a weak memory model. We implicitly assumed in our example with\nsequential consistency that when the last thread reads finished[i] to be 1, partialSums[i]\ncontains the value of the partial sum. However, this assumption does not hold if we assume a\nweak memory model because the memory system might reorder the writes to finished[i] and\npartialSums[i]. It is thus possible that the write to the finished array happens before the\nwrite to the partialSums array in a system with a weak memory model. In this case, the fact\nthat finished[i] is equal to 1 does not guarantee that partialSums[i] contains the updated\nvalue. This distinction is precisely what makes sequential consistency extremely programmer\nfriendly.\nImportant Point 18\nIn a weak memory model, the memory accesses issued by the same thread are always per-\nceived to be in program order by that thread. However, the order of memory accesses can\nbe perceived differently by other threads.\nLet us come back to the problem of ensuring that our example to add numbers in parallel\nruns correctly. We observe that the only way out of our quagmire is to have a mechanism to\nensure that the write to partialSums[i] is completed before another threads reads finished[i]\nto be 1. We can use a generic instruction known as a fence. This instruction ensures that\nall the reads and writes issued before the fence complete before any read or write after the\nfence begins. Trivially, we can convert a weak memory model to a sequentially consistent one\nby inserting a fence after every instruction. However, this can induce a large overhead. It is\nbest to introduce a minimal number of fence instructions as and when required. Let us look at\nour example for adding a set of numbers in parallel for weak memory models by adding fence\ninstructions.\nExample 126\nWrite a shared memory program to add a set of numbers in parallel on a machine with a\nweak memory model.\nAnswer: Letusassumethatallthenumbersarealreadystoredinanarraycallednumbers.\nThe array numbers has SIZE entries. The number of parallel threads is given by N. 555 (cid:13)c Smruti R. Sarangi\n\/* variable declaration *\/\nint partialSums[N];\nint finished[N];\nint numbers[SIZE];\nint result = 0;\n\/* initialise all the elements in partialSums and finished to 0 *\/\n...\n\/* fence *\/\n\/* ensures that the parallel section can read the initialised arrays *\/\nfence();\n\/* All the data is present in all the arrays at this point *\/\n\/* parallel section *\/\nparallel {\n\/* get the current thread id *\/\nint myId = getThreadId();\n\/* compute the partial sum *\/\nint startIdx = myId * SIZE\/N;\nint endIdx = startIdx + SIZE\/N;\nfor(int jdx = startIdx; jdx < endIdx; jdx++)\npartialSums[myId] += numbers[jdx];\n\/* fence *\/\n\/* ensures that finished[i] is written after\npartialSums[i] *\/\nfence();\n\/* set the value of done *\/\nfinished[myId] = 1;\n}\n\/* wait till all the threads are done *\/\ndo {\nflag = 1;\nfor (int i=0; i < N; i++){\nif(finished[i] == 0){\nflag = 0;\nbreak;\n}\n} (cid:13)c Smruti R. Sarangi 556\n} while (flag == 0) ;\n\/* sequential section *\/\nfor(int idx=0; idx < N; idx++)\nresult += partialSums[idx];\nExample 126 shows the code for a weak memory model. The code is more or less the same\nas it was for the sequentially consistent memory model. The only difference is that we have\nadded two additional fence instructions. We assume a function called fence() that internally\ninvokes a fence instruction. We first call fence() before invoking all the parallel threads. This\nensures that all the writes for initialising data structures have completed. After that we start\ntheparallelthreads. Theparallelthreadsfinishtheprocessofcomputingandwritingthepartial\nsum, and then we invoke the fence operation again. This ensures that before finished[myId]\nis set to 1, all the partial sums have been computed and written to their respective locations in\nmemory. Secondly, ifthe the lastthread readsfinished[i] tobe 1, then we can say forsure that\nthe value of partialSums[i] is up to date and correct. Hence, this program executes correctly,\nin spite of a weak memory model.\nWe thus observe that weak memory models do not sacrifice on correctness if the program-\nmer is aware of them, and inserts fences at the right places. Nonetheless, it is necessary for\nprogrammers to be aware of weak memory models, and they need to also understand that a\nlot of subtle bugs in parallel programs occur because programmers do not take the underly-\ning memory model into account. Weak memory models are currently used by most processors\nbecause they allow us to build high performance memory systems. In comparison, sequential\nconsistency is very restrictive, and other than the MIPS R10000 [Yeager, 1996] no other ma-\njor vendor offers machines with sequential consistency. All our current x86 and ARM based\nmachines use different versions of weak memory models.\n11.4.4 Physical View of Memory\nOverview\nWe have looked at two important aspects of the logical view of a memory system for multi-\nprocessors namely coherence, and consistency. We need to implement a memory system that\nrespects both of these properties. In this section, we shall study the design space of multipro-\ncessor memory systems, and provide an overview of the design alternatives. We shall observe\nthat there are two ways of designing a cache for a multiprocessor memory system. The first\ndesign is called a shared cache, where a single cache is shared among multiple processors. The\nsecond design uses a set of private caches, where each processor or set of processors typically\nhave a private cache. All the private caches co-operate to provide the illusion of a shared cache.\nThis is known as cache coherence.\nWe shall study the design of shared caches in Section 11.4.5, and private caches in Sec-\ntion11.4.6. Subsequently,weshallbrieflylookatensuringmemoryconsistencyinSection11.4.7.\nWe shall conclude that an efficient implementation of a given consistency model such as se- 557 (cid:13)c Smruti R. Sarangi\nquential, or weak consistency is difficult, and is a subject of study in an advanced computer\narchitecture course. In this book, we propose a simple solution to this problem, and request\nthe reader to look at research papers for more information. The casual reader can skip most of\nthis section without any loss in continuity. Subsequently, we shall summarise the main results,\nobservations, and insights; it is suitable for all our readers.\nDesign of a Multiprocessor Memory System \u2013 Shared and Private Caches\nLet us start out by considering the first level cache. We can give every processor its individual\ninstruction cache. Instructions represent read only data, and typically do not change during\nthe execution of the program. Since sharing is not an issue here, each processor can benefit\nfrom its small private instruction cache. The main problem is with the data caches. There\nare two possible ways to design a data cache. We can either have a shared cache, or a private\ncache. Asharedcacheisasinglecachethatisaccessibletoalltheprocessors. Aprivate cacheis\naccessible to either only one processor, or a set of processors. It is possible to have a hierarchy\nof shared caches, or a hierarchy of private caches as shown in Figure 11.8. We can even have\ncombinations of shared and private caches in the same system.\nProc 1 Proc 2 Proc n Proc 1 Proc 2 Proc n Proc 1 Proc 2 Proc n\nShared L1 cache L1 L1 L1 L1 L1 L1\nShared L2 cache Shared L2 cache L2 L2 L2\n(a) (b) (c)\nFigure 11.8: Examples of systems with shared and private caches\nLet us now evaluate the tradeoffs between a shared and private cache. A shared cache is\naccessible to all the processors, and contains a single entry for a cached memory location. The\ncommunication protocol is simple, and is like any regular cache access. The additional com-\nplexity arises mainly from the fact that we need to properly schedule the requests coming from\ndifferentindividualprocessors. However,atthecostofsimplicity,asharedcachehasitsshareof\nproblems. Toservicerequestscomingfromalltheprocessors, asharedcacheneedstohavealot\nof read and write ports for handling requests simultaneously. Unfortunately, the size of a cache\nincreases approximately as a square of the number of ports [Tarjan et al., 2006]. Additionally,\nthe shared cache needs to accommodate the working sets of all the currently running threads.\nHence, shared caches tend to become very large and slow. Because of physical constraints, it\nbecomes difficult to place a shared cache close to all the processors. In comparison, private\ncaches are typically much smaller, service requests for fewer cores, and have a lower number\nof read\/write ports. Hence, they can be placed close to their associated processors. A private\ncache is thus much faster because it can be placed closer to a processor and is also much smaller\nin size. (cid:13)c Smruti R. Sarangi 558\nTo solve the problems with shared caches, designers often use private caches, especially in\nthe higher levels of the memory hierarchy. A private cache can only be accessed by either one\nprocessor, or a small set of processors. They are small, fast, and consume a lesser amount of\npower. The major problem with private caches is that they need to provide the illusion of a\nshared cache to the programmer. For example, let us consider a system with two processors,\nand a private data cache associated with each processor. If one processor writes to a memory\naddress, x, the other processor needs to be aware of the write. However, if it only accesses its\nprivate cache, then it will never be aware of a write to address x. This means that a write to\naddress x is lost, and thus the system is not coherent. Hence, there is a need to tie the private\ncaches of all the processors such that they look like one unified shared cache, and observe the\nrules of coherence. Coherence in the context of caches, is popularly known as cache coherence.\nMaintaining cache coherence represents an additional source of complexity for private caches,\nand limits the scalability of this approach. It works well for small private caches. However,\nfor larger private caches, the overhead of maintaining coherence becomes prohibitive. For large\nlower level caches, the shared cache is more appropriate. Secondly, there is typically some data\nreplication across multiple private caches. This wastes space.\nDefinition 128\nCoherence in the context of a set of private caches is known as cache coherence.\nByimplementingacachecoherenceprotocol, itispossibletoconvertasetofdisjointprivate\ncaches to appear as a shared cache to software. Let us now outline the major tradeoffs between\nshared and private caches in Table 11.3.\nAttribute Private Cache Shared Cache\nArea low high\nSpeed fast slow\nProximity to the processor near far\nScalability in size low high\nData replication yes no\nComplexity high (needs cache coherence) low\nTable 11.3: Comparison of shared and private caches\nFrom the table it is clear that the first level cache should ideally be private because we\ndesire low latency and high throughput. However, the lower levels need to be larger in size,\nand service a significantly lesser number of requests, and thus they should comprise of shared\ncaches. Let us now describe the design of coherent private caches, and large shared caches.\nTo keep matters simple we shall only consider a single level private cache, and not consider\nhierarchical private caches. They introduce additional complexity, and are best covered in an\nadvanced textbook on computer architecture.\nLet us discuss the design of shared caches first because they are simpler. Before proceeding\nfurther, let us review where we stand. 559 (cid:13)c Smruti R. Sarangi\nWay Point 11\n1. We defined a set of correctness requirements for caches in Section 11.4.1. They were\ntermed as coherence and consistency.\n2. In a nutshell, both the concepts place constraints on reordering memory requests in the\nmemory system. The order and semantics of requests to the same memory location is\nreferred to as coherence, and the semantics of requests to different memory locations\nby the same thread is referred to as consistency.\n3. For ensuring that a memory system is consistent with a certain model of memory,\nwe need to ensure that the hardware follows a set of rules with regards to reordering\nmemoryrequestsissuedbythesameprogram. Thiscanbeensuredbyhavingadditional\ncircuitry that stalls all the memory requests, till a set of memory requests issued in the\npast complete. Secondly, programmer support is also required for making guarantees\nabout the correctness of a program.\n4. There are two approaches for designing caches \u2013 shared or private. A shared cache\nhas a single physical location for each memory location. Consequently, maintaining\ncoherence is trivial. However, it is not a scalable solution because of high contention,\nand high latency.\n5. Consequently, designers often use private caches at least for the L1 level. In this case,\nwe need to explicitly ensure cache coherence.\n11.4.5 Shared Caches\nIn the simplest embodiment of a shared cache, we can implement it as a regular cache in a\nuniprocessor. However, this will prove to be a very bad approach in practice. The reason for\nthis is that in a uniprocessor, only one thread accesses the cache; however, in a multiprocessor\nmultiple threads might access the cache, and thus we need to provide more bandwidth. If all\nthe threads need to access the same data and tag array, then either requests have to stall or we\nhave to increase the number of ports in the arrays. This will have very negative consequences\nin terms of area and power. Lastly, cache sizes (especially L2 and L3) are roughly doubling as\nper Moore\u2019s law. As of 2012, on-chip caches can be as large as 4-8 MB. If we have a single tag\narray for the entire cache, then it will be very large and slow. Let us define the term last level\ncache (LLC) as the on chip cache that has the lowest position in the memory hierarchy (with\nmain memory being the lowest). For example, if a multicore processor has an on-chip L3 cache\nthat is connected to main memory, then the LLC is the L3 cache. We shall use the term LLC\nfrequently from now onwards.\nTocreateamulti-megabyteLLCthatcansimultaneouslysupportmultiplethreads, weneed\ntosplititintomultiplesubcaches. Letusassumethatwehavea4MBLLC.Inatypicaldesign,\nthis will be split into 8-16 smaller subcaches. Thus each subcache will be 256-512 KB in size, (cid:13)c Smruti R. Sarangi 560\nwhich is an acceptable size. Each such subcache is a cache in its own right, and is known as a\ncache bank. Hence, we have in effect split a large cache into a set of cache banks. A cache bank\ncan either be direct mapped, or can be set associative.\nThere are two steps in accessing a multibank cache. We first calculate the bank address,\nand then perform a regular cache access at the bank. Let us explain with an example. Let us\nconsider a 16-bank, 4 MB cache. Each bank thus contains 256 KB of data. Now 4 MB = 222\nbytes. We can thus dedicate bits 19-22 for choosing the bank address. Note that bank selection\nis independent of associativity in this case. After choosing a bank, we can split the remaining\n28 bits between the offset within the block, set index, and tag.\nThere are two advantages of dividing a cache into multiple banks. The first is that we\ndecrease the amount of contention at each bank. If we have 4 threads, and 16 banks, then the\nprobability that 2 threads access the same bank is low. Secondly, since each bank is a smaller\ncache, it is more power efficient, and faster. We have thus achieved our twin aims of supporting\nmultiplethreads,anddesigningafastcache. Weshalllookattheproblemofplacingprocessors,\nand cache banks in a multicore processor in Section 11.6.\n11.4.6 Coherent Private Caches\nOverview of a snoopy Protocol\nThe aim here is to make a set of private caches behave as if it is one large shared cache. From\nthe point of view of software we should not be able to figure out whether a cache is private\nor shared. A conceptual diagram of the system is shown in Figure 11.9. It shows a set of\nprocessors along with their associated caches. The set of caches form a cache group. The entire\ncache group needs to appear as one cache.\nProc 1 Proc 2 Proc n Proc 1 Proc 2 Proc n\nOne logical\nShared L1 cache L1 L1 L1\ncache\nShared L2 cache Shared L2 cache\nFigure 11.9: A system with many processors and their private caches\nThese caches are connected via an interconnection network, which can range from a simple\nshared bus type topology to more complex topologies. We shall look at the design of different\ninterconnection networks in Section 11.6. In this section, let us assume that all the caches are\nconnected to a shared bus. A shared bus allows a single writer and multiple readers at any\npoint of time. If one cache writes a message to the bus, then all the other caches can read it. 561 (cid:13)c Smruti R. Sarangi\nThe topology is shown in Figure 11.10. Note that the bus gives exclusive access to only one\ncache at any point of time for writing a message. Consequently, all the caches perceive the\nsame order of messages. A protocol that implements cache coherence with caches connected on\na shared bus is known as a snoopy protocol.\nProc 1 Proc 2 Proc n\nL1 L1 L1\nShared bus\nFigure 11.10: Caches connected with a shared bus\nLet us now consider the operation of the snoopy protocol from the point of view of the\ntwo axioms of coherence \u2013 writes always complete (completion axiom), and writes to the same\nblock are seen in the same order by all processors (order axiom). If cache i, wishes to perform\na write operation on a block, then this write needs to be ultimately visible to all the other\ncaches. We need to do this to satisfy the completion axiom, because we are not allowed to lose\na write request. Secondly, different writes to the same block need to arrive at all the caches\nthat might contain the block in the same order (order axiom). This ensures that for any given\nblock, all the caches perceive the same order of updates. The shared bus automatically satisfies\nthis requirement (the order axiom).\nWe present the design of two snoopy protocols \u2013 write-update and write-invalidate.\nWrite-Update Protocol\nLetusnowdesignaprotocol,wereaprivatecachekeepsacopyofawriterequest,andbroadcasts\nthewriterequesttoallthecaches. Thisstrategyensuresthatawriteisneverlost,andthewrite\nmessages to the same block are perceived in the same order by all the caches. This strategy\nrequires us to broadcast, whenever we want to write. This is a large additional overhead;\nhowever, this strategy will work. Now, we need to incorporate reads into our protocol. A read\nto a location, x, can first check the private cache to see if a copy of it is already available. If a\nvalid copy is available, then the value can be forwarded to the requesting processor. However,\nif there is a cache miss, then it is possible that it might be present with another sister cache in\nthe cache group, or it might need to be fetched from the lower level. We need to first check if\nthe value is present with a sister cache. We follow the same process here. The cache broadcasts\na read request to all the caches. If any of the caches, has the value, then it replies, and sends\nthe value to the requesting cache. The requesting cache inserts the value, and forwards it to\nthe processor. However, if it does not get any reply from any other cache, then it initiates a\nread to the lower level.\nThis protocol is known as the write-update protocol. Each cache block needs to maintain\nthree states, M, S, and I. M refers to the modified state. It means that the cache has modified (cid:13)c Smruti R. Sarangi 562\nthe block. S(shared) means that the cache has not modified the block, and I(invalid) denotes\nthe fact that the block does not contain valid data.\nRead hit\/\nEvict\/\nI\nS\nRead miss\/ Broadcast miss\nE v i c t \/\nW\nW\nr ri t ie\nt e\nm bis as c\/\nk\nBr\no a d c a st m i s s\nM\nWrit e h i t \/ B r o a dcast\nwrite\nWrite hit\/ Broadcast write\nRead hit\/\nFigure 11.11: State transition diagram in the write-update protocol\nFigure11.11showsafinitestatemachine(FSM)foreachcacheblock. ThisFSMisexecuted\nbythecachecontroller. Theformatforstatetransitionsisevent\/action. Ifthecachecontroller\nis sent an event, then it takes a corresponding action, which may include a state transition.\nNote that in some cases, the action field is blank. This means that in those cases, no action is\ntaken. Note that the state of a cache block is a part of its entry in the tag array. If a block is\nnot present in the cache, then its state is assumed to be invalid (I). Lastly, it is important to\nmention that Figure 11.11 shows the transitions for events generated by the processor. It does\nnot show the actions for events sent over the bus by other caches in the cache group. Now, let\nus discuss the protocol in detail.\nAll blocks, initially are in the I state. If there is a read miss then it moves to the S state.\nWe additionally need to broadcast the read miss to all the caches in the cache group, and either\nget the value from a sister cache, or from the lower level. Note that we give first preference to\na sister cache, because it might have modified the block without writing it back to the lower\nlevel. Similarly, if there is a write miss in the I state, then we need to read the block from\nanother sister cache if it is available, and move to the M state. If no other sister cache has the\nblock, then we need to read the block from the lower level of the memory hierarchy.\nIf there is a read hit in the S state, then we can seamlessly pass the data to the processor.\nHowever, if we need to write to the block in the S state, we need to broadcast the write to all\nthe other caches such that they get the updated value. Once, the cache gets a copy of its write\nrequest from the bus, it can write the value to the block, and change its state to M. To evict a\nblock in the S state, we need to just evict it from the cache. It is not necessary to write back\nits value because the block has not been modified.\nNow, let us consider the M state. If we need to read a block in the M state, then we can\nread it from the cache, and send the value to the processor. There is no need to send any\nmessage. However, if we wish to write to it, then it is necessary to send a write request on the\nbus. Once, the cache sees its own write request arrive on the shared bus, it can write its value\nto the memory location in its private cache. To evict a block in the M state, we need to write 563 (cid:13)c Smruti R. Sarangi\nit back to the lower level in the memory hierarchy, because it has been modified.\nQueue of requests\nBroadcast to\nall the caches\nFigure 11.12: The bus arbiter\nEvery bus has a dedicated structure called an arbiter that receives requests to use the bus\nfrom different caches. It allots the bus to the caches in FIFO order. A schematic of the bus\narbiter is shown in Figure 11.12. It is a very simple structure. It contains a queue of requests\nto transmit on the bus. Each cycle it picks a request from the queue, and gives permission to\nthe corresponding cache to transmit a message on the bus.\nLet us now consider a sister cache. Whenever it gets a miss message from the bus, it checks\nits cache to find if it has the block. If there is a cache hit, then it sends the block on the bus,\nor directly to the requesting cache. If it receives the notification of a write by another cache,\nthen it updates the contents of the block if it is present in its cache.\nDirectory Protocol\nNotethatinthesnoopyprotocolwealwaysbroadcastawrite, areadmiss, orawritemiss. This\nis strictly not required. We need to send a message to only those caches that contain a copy of\nthe block. The directory protocol uses a dedicated structure called a directory to maintain this\ninformation. For each block address, the directory maintains a list of sharers. A sharer is the\nid of a cache that might contain the block. The list of sharers is in general a superset of caches\nthat might contain the given block. We can maintain the list of sharers as a bit vector (1 bit\nper sharer). If a bit is 1, then a cache contains a copy, otherwise it does not.\nThewrite-updateprotocolwithadirectorygetsmodifiedasfollows. Insteadofbroadcasting\ndata on the bus, a cache sends all of its messages to the directory. For a read or write miss,\nthe directory fetches the block from a sister cache if it has a copy. It then forwards the block\nto the requesting cache. Similarly, for a write, the directory sends the write message to only\nthose caches that might have a copy of the block. The list of sharers needs to be updated when\na cache inserts or evicts a block. Lastly, to maintain coherence the directory needs to ensure\nthat all the caches get messages in the same order, and no message is ever lost. The directory\nprotocol minimises the number of messages that need to be sent, and is thus more scalable.\nDefinition 129\nsnoopy Protocol In a snoopy protocol, all the caches are connected to a shared bus. A\ncache broadcasts each message to the rest of the caches. (cid:13)c Smruti R. Sarangi 564\nDirectory Protocol In a directory protocol, we reduce the number of messages by adding\na dedicated structure known as a directory. The directory maintains the list of caches\nthat might potentially contain a copy of the block. It sends messages for a given block\naddress to only the caches in the list.\nQuestion 8\nWhy is it necessary to wait for the broadcast from the bus to perform a write?\nAnswer: Let us assume this is not the case, and processor 1, wishes to write 1 to x, and\nprocessor 2 wishes to write 2 to x. They will then first write 1 and 2 to their copies of x\nrespectively, and then broadcast the write. Thus, the writes to x will be seen in different\norders by both the processors. This violates the order axiom. However, if they wait for a\ncopy of their write request to arrive from the bus, then they write to x in the same order.\nThe bus effectively resolves the conflict between processor 1 and 2, and orders one request\nafter the other.\nWrite-Invalidate Protocol\nWe need to note that broadcasting a write request for every single write is an unnecessary\noverhead. It is possible that most of the blocks might not be shared in the first place. Hence,\nthere is no need to send an extra message on every write. Let us try to reduce the number of\nmessages in the write update protocol by proposing the write-invalidate protocol. Here again,\nwe can either use the snoopy protocol, or the directory protocol. Let us show an example with\nthe snoopy protocol.\nLet us maintain three states for each block \u2013 M, S, and I. Let us however, change the\nmeaning of our states. The invalid state (I) retains the same meaning. It means that the entry\nis effectively not present in the cache. The shared state (S) means that a cache can read the\nblock, but it cannot write to it. It is possible to have multiple copies of the same block in\ndifferent caches in the shared state. Since the shared state assumes that the block is read-only,\nhaving multiple copies of the block does not affect cache coherence. The M (modified) state\nsignifies the fact that the cache can write to the block. If a block is in the M state, then all the\nother caches in the cache group need to have the block in the I state. No other cache is allowed\nto have a valid copy of the block in the S or M states. This is where the write-invalidate\nprotocol differs from the write-update protocol. It allows either only one writer at a time, or\nmultiple readers at a time. It never allows a reader and a writer to co-exist at the same time.\nBy restricting the number of caches that have write access to a block at any point of time, we\ncan reduce the number of messages.\nThebasicinsightisasfollows. Thewrite-updateprotocoldidnothavetosendanymessages\non a read hit. It sent extra messages on a write hit, which we want to eliminate. It needed to\nsend extra messages because multiple caches could read or write a block concurrently. For the\nwrite-invalidate protocol, we have eliminated this behaviour. If a block is in the M state, then 565 (cid:13)c Smruti R. Sarangi\nno other cache contains a valid copy of the block.\nRead hit\/\nEvict\/\nI\nS\nRead miss\/ Broadcast read miss\nE v i c t \/ W\nW rr ii tt e\ne\nm bais\ncs\nk\n\/\nB r o a d c a s t w r it e m Miss Writ e h i t \/ B r o a dcast\nwrite\nWrite hit\/\nRead hit\/\nFigure 11.13: State transition diagram of a block due to actions of the processor\nRead miss\/ Send data\nWrite miss\/ Send data\nW r it e \/\nI\nS\nd\nn\nW r i t e\nmiss\/\nS\ne\nn\nd\nd\nat a M\nRea\nd\nm i s s \/\nS e\nn d\nw r\nid\nt\na et a\nb\naa\nck\nFigure 11.14: State transition diagram of a block due to messages on the bus\nFigure 11.13 shows the state transition diagram because of actions of the processor. The\nstate transition diagram is mostly the same as the state transition diagram of the write-update\nprotocol. Let us look at the differences. The first is that we define three types of messages that\nare put on the bus namely write, writemiss, and readmiss. When we transition from the I\nto the S state, we place a read miss on the bus. If a sister cache does not reply with the data,\nthe cache controller reads the block from the lower level. The semantics of the S state remains\nthe same. To write to a block in the S state, we need to transition to the M state, after writing\na write message on the bus. Now, when a block is in the M state, we are assured of the fact (cid:13)c Smruti R. Sarangi 566\nthat no other cache contains a valid copy. Hence, we can freely read and write a block in the\nM state. It is not necessary to send any messages on the bus. If the processor decides to evict\na block in the M state, then it needs to write its data to the lower level.\nFigure11.14showsthestatetransitionsduetomessagesreceivedonthebus. IntheS state,\nif we get a readmiss, then it means that another cache wants read access to the block. Any\nof the caches that contains the block sends it the contents of the block. This process can be\norchestrated as follows. All the caches that have a copy of the block try to get access to the\nbus. The first cache that gets access to the bus sends a copy of the block to the requesting\ncache. The rest of the caches immediately get to know that the contents of the block have been\ntransferred. They subsequently stop trying. If we get a write or write miss message in the S\nstate, then the block transitions to the I state.\nLet us now consider the M state. If some other cache sends a writemiss message then\nthe cache controller of the cache that contains the block, sends the contents of the block to\nit, and transitions to the I state. However, if it gets a readmiss, then it needs to perform a\nsequence of steps. We assume that we can seamlessly evict a block in the S state. Hence, it is\nnecessary to write the data to the lower level before moving to the S state. Subsequently, the\ncache that originally has the block also sends the contents of the block to the requesting cache,\nand transitions the state of the block to the S state.\nWrite-Invalidate Protocol with a Directory\nImplementingthewrite-invalidateprotocolwithadirectoryisfairlytrivial. Thestatetransition\ndiagramsremainalmostthesame. Insteadofbroadcastingamessage,wesendittothedirectory.\nThe directory sends the message to the sharers of the block.\nThe life cycle of a block is as follows. Whenever, a block is brought in from the lower level,\na directory entry is initialised. At this point it has only one sharer, which is the cache that\nbrought it from the lower level. Now, if there are read misses to the block, then the directory\nkeeps adding sharers. However, if there is a write miss, or a processor decides to write to the\nblock, then it sends a write or writemiss message to the directory. The directory cleans the\nsharers list, and keeps only one sharer, which is the processor that is performing the write\naccess. When a block is evicted, its cache informs the directory, and the directory deletes a\nsharer. When the set of sharers becomes empty, the directory entry can be removed.\nIt is possible to make improvements to the write-invalidate and update protocols by adding\nan additional state known as the exclusive (E) state. The E state can be the initial state for\nevery cache block fetched from the lower level of the memory hierarchy. This state stores the\nfact that a block exclusively belongs to a cache. However, the cache has read-only access to\nit, and does not have write access to it. For an E to M transition, we do not have to send a\nwritemiss or write message on the bus, because the block is owned exclusively by one cache.\nWe can seamlessly evict data from the E state if required. Implementing the MESI protocol is\nleft as an exercise for the reader.\n11.4.7 Implementing a Memory Consistency Model*\nAtypicalmemoryconsistencymodelspecifiesthetypesofre-orderingsthatareallowedbetween\nmemory operations issued by the same thread. For example, in sequential consistency all\nread\/write accesses are completed in program order, and all other threads also perceive the 567 (cid:13)c Smruti R. Sarangi\nmemory accesses of any thread in its program order. Let us give a simple solution to the\nproblem of implementing sequential consistency first.\nOverview of an Implementation of Sequential Consistency*\nLet us build a memory system that is coherent, and provides certain guarantees. Let us assume\nthat all the write operations are associated with a time of completion, and appear to execute\ninstantaneously at the time of completion. It is not possible for any read operation to get the\nvalue of a write before it completes. After a write completes, all the read operations to the\nsame address either get the value written by the write operation or a newer write operation.\nSince we assume a coherent memory, all the write operations to the same memory address are\nseen in the same order by all the processors. Secondly, each read operation returns the value\nwritten by the latest completed write to that address. Let us now consider the case in which\nprocessor 1 issues a write to address x, and at the same time processor 2 issues a read request\nto the same address, x. In this case, we have a concurrent read and write. The behaviour is not\ndefined. The read can either get the value set by the concurrent write operation, or it can get\nthe previous value. However, if the read operation gets the value set by the concurrent write\noperation, then all subsequent reads issued by any processor, need to get that value or a newer\nvalue. We can say that a read operation completes, once it has finished reading the value of the\nmemory location, and the write that generated its data also completes.\nNow, let us design a multiprocessor where each processor issues a memory request after\nall the previous memory requests that it had issued have completed. This means that after\nissuing a memory request (read\/write), a processor waits for it to complete before issuing the\nnext memory request. We claim that a multiprocessor with such processors is sequentially\nconsistent. Let us now outline a brief informal proof.\nLet us first introduce a theoretical tool called an access graph.\nAccess Graph*\nFigure 11.15 shows the execution of two threads and their associated sequence of memory\naccesses. For each read or write access, we create a circle or node in the access graph (see\nFigure 11.15(c)). In this case, we add an arrow (or edge) between two nodes if one access\nfollows the other in program order, or if there is a read-write dependence across two accesses\nfrom different threads. For example, if we set x to 5 in thread 1, and the read operation in\nthread 2, reads this value of x, there is a dependence between this read and write of x, and thus\nwe add an arrow in the access graph. The arrow signifies that the destination request must\ncomplete after the source request.\nLet us now define a happens-before relationship between nodes a and b, if there is a path\nfrom a to b in the access graph.\nDefinition 130\nLet us define a happens-before relationship between nodes a and b, if there is a path from\na to b in the access graph. A happens-before relationship signifies that b must complete its\nexecution after a completes. (cid:13)c Smruti R. Sarangi 568\nAccess\nThread 1 Thread 2\ngraph\n1A: x = 5 1B: y = 7 1A 1B\n2A: t1 = y 2B: t3 = x\n2A 2B\n3A: z = 4 3B: t4 = z\n4A: t2 = z 4B: t5 = 3 3A 3B\n(a) (b) 4A 4B\n(c)\nSequential order\n1A 1B 2A 2B 3A 3B 4A 4B\n(d)\nFigure 11.15: Graphical representation of memory accesses\nThe access graph is a general tool and is used to reason about concurrent systems. It\nconsists of a set of nodes, where each node is a dynamic instance of an instruction (most often\na memory instruction). There are edges between nodes. An edge of the from A \u2192 B means\nthat B needs to complete its execution after A. In our simple example in Figure 11.15 we have\nadded two kinds of edges namely program order edges, and causality edges. Program order\nedges indicate the order of completion of memory requests in the same thread. In our system,\nwhere we wait for an instruction to complete, before executing the next instruction, there are\nedges between consecutive instructions of the same thread.\nCausality edges are between load and store instructions across threads. For example, if a\ngiven instruction writes a value, and another instruction reads it in another thread, we add an\nedge from the store to the load.\nTo prove sequential consistency, we need to add additional edges to the access graph as\nfollows (see [Arvind and Maessen, 2006]). Let us first assume that we have an oracle(a hypo-\nthetical entity that knows everything) with us. Now, since we assume coherent memory, all the\nstores to the same memory location are sequentially ordered. Furthermore, there is an order\nbetween loads and stores to the same memory location. For example, if we set x to 1, then\nset x to 3, then read t1 = x, and then set x to 5, there is a store-store-load-store order for the\nlocation, x. The oracle knows about such orderings between loads and stores for each memory\nlocation. Let us assume that the oracle adds the corresponding happens-before edges to our\naccess graph. In this case the edge between the store and the load is a causality edge, and the\nstore-store, and load-store edges are examples of coherence edges.\nNext, let us describe how to use an access graph for proving properties of systems. First, we\nneedtoconstructanaccessgraphofaprogramforagivenmemoryconsistencymodel,M,based\non a given run of the program. We add coherence, and causality edges based on the memory\naccess behaviour. Second, we add program order edges between instructions in the same thread\nbased on the consistency model. For SC, we add edges between consecutive instructions, and 569 (cid:13)c Smruti R. Sarangi\nfor WC, we add edges between dependent instructions, and between regular instructions, and\nfences. It is very important to understand that the access graph is a theoretical tool, and it\nis most often not a practical tool. We shall reason about the properties of an access graph\nwithout actually building one for a given program, or system.\nNow, if the access graph does not contain cycles, then we can arrange the nodes\nin a sequential order. Let us prove this fact. In the access graph, if there is a path from a\nto b, then let a be known as b\u2019s ancestor. We can generate a sequential order by following an\niterative process. We first find a node, which does not have an ancestor. There has to be such a\nnode because some operation must have been the first to complete (otherwise there is a cycle).\nWe remove it from our access graph, and proceed to find another node that does not have any\nancestors. We add each such node in our sequential order as shown in Figure 11.15(d). In each\nstep the number of nodes in the access graph decreases by 1, till we are finally left with just\none node, which becomes the last node in our sequential order. Now, let us consider the case\nwhen we do not find any node in the access graph that does not have an ancestor. This is only\npossible if there is a cycle in the access graph, and thus it is not possible.\nArrangingthenodesinasequentialorderisequivalenttoprovingthattheaccess\ngraph obeys the memory model that it is designed for. The fact that we can list the\nnodes in a sequential order without violating any happens-before relationships, means that\nthe execution is equivalent to a uniprocessor executing each node in the sequential order one\nafter the other. This is precisely the definition of a consistency model. Any consistency model\nconsists of the ordering constraints between memory instructions, along with assumptions of\ncoherence. Thedefinitionfurtherimpliesthatitshouldbepossibleforauniprocessortoexecute\ninstructionsinasequentialorderwithoutviolatinganyofthehappens-beforerelationships. This\nis precisely what we have achieved by converting the access graph to an equivalent sequential\nlist of nodes. Now, the fact that program order, causality, and coherence edges are enough to\nspecify a consistency model is more profound. For a detailed discussion, the reader is referred\nto the paper by Arvind and Maessen [Arvind and Maessen, 2006].\nHence, if an access graph (for memory model, M) does not contain cycles, we\ncan conclude that a given execution follows M. If we can prove that all the possible\naccess graphs that can be generated by a system are acyclic, then we can conclude\nthat the entire system follows M.\nProof of Sequential Consistency*\nLet us thus prove that all possible access graphs (assuming SC) that can be generated by our\nsimple system that waits for memory requests to complete before issuing subsequent memory\nrequests are acyclic. Let us consider any access graph, G. We have to prove that it is possible\nto write all the memory accesses in G in a sequential order such that if node b is placed after\nnode a, then there is no path from b to a in G. In other words, our sequential order respects\nthe order of accesses as shown in the access graph.\nLet us assume that the access graph has a cycle, and it contains a set of nodes, S, belonging\ntothesamethread,t . LetabetheearliestnodeinprogramorderinS,andbbethelatestnode\n1\nin program order in S. Clearly, a happens before b because we execute memory instructions in\nprogram order, and we wait for a request to complete before starting the next request in the\nsame thread. For a cycle to form due to a causality edge, b needs to write to a value that is (cid:13)c Smruti R. Sarangi 570\nread by another memory read request (node), c, belonging to another thread. Alternatively,\nthere can be a coherence edge between b and a node c belonging to another thread. Now, for\na cycle to exist, c needs to happen before a. Let us assume that there are a chain of nodes\nbetween c and a and the last node in the chain of nodes is d. By definition, d \u2208\/ t . This means\n1\nthat either d writes to a memory location, and node a reads from it, or there is a coherence\nedge from d to a. Because there is a path from node b to node a (through c and d), it must be\nthe case that the request associated with node b happens before the request of node a. This\nis not possible since we cannot execute the memory request associated with node b till node\na\u2019s request completes. Thus, we have a contradiction, and a cycle is not possible in the access\ngraph. Hence, the execution is in SC.\nNow, letusclarifythenotionofanoracle. Thereaderneedstounderstandthattheproblem\nhere is not to generate a sequential order, it is to rather prove that a sequential order exists.\nSince we are solving the latter problem, we can always presume that a hypothetical entity adds\nadditional edges to our access graph. The resulting sequential order respects program orders\nfor each thread, causality and coherence-based happens-before relationships. It is thus a valid\nordering.\nConsequently, we can conclude that it is always possible to find a sequential order for\nthreads in our system. Therefore, our multiprocessor is in SC. Now that we have proved that\nour system is sequentially consistent, let us describe a method to implement a multiprocessor\nwiththeassumptionsthatwehavemade. WecanimplementasystemasshowninFigure11.16.\nDesign of a Simple (yet impractical) Sequentially Consistent Machine*\nProc 1 Proc 2 Proc n\nShared L1 cache\nFigure 11.16: A simple sequentially consistent system\nFigure 11.16 shows a design that has a large shared L1 cache across all the processors of\na multiprocessor. There is only one copy of each memory location that can support only one\nread or write access at any single time. This ensures coherence. Secondly, a write completes,\nwhen it changes the value of its memory location in the L1 cache. Likewise, a read completes\nwhen it reads the value of the memory address in the L1 cache. We need to modify the simple\nin-order RISC pipeline described in Chapter 9 such that an instructions leaves the memory\naccess (MA) stage only after it completes its read\/write access. If there is a cache miss, then\ntheinstructionwaitstilltheblockcomestotheL1cache, andtheaccesscompletes. Thissimple\nsystem ensures that memory requests from the same thread complete in program order, and is\nthus sequentially consistent.\nNote that the system described in Figure 11.16 makes some unrealistic assumptions and is\nthusimpractical. Ifwehave16processors, andifthefrequencyofmemoryinstructionsis1in3, 571 (cid:13)c Smruti R. Sarangi\nthen every cycle, 5-6 instructions will need to access the L1 cache. Hence, the L1 cache requires\nat least 6 read\/write ports, which will make the structure too large and too slow. Additionally,\nthe L1 cache needs to be large enough to contain the working sets of all the threads, which\nfurther makes the case for a very large and slow L1 cache. Consequently, a multiprocessor\nsystem with such a cache will be very slow in practice. Hence, modern processors opt for more\nhigh performance implementations with more complicated memory systems that have a lot of\nsmaller caches. These caches co-operate among each other to provide the illusion of a larger\ncache (see Section 11.4.6).\nItisfairlydifficulttoprovethatacomplexsystemfollowssequentialconsistency(SC).Hence,\ndesigners opt to design systems with weak memory models. In this case, we need to prove that\na fence instruction works correctly. If we take all the subtle corner cases that are possible with\ncomplicated designs, this also turns out to be a fairly challenging problem. Interested readers\ncan take a look at pointers mentioned at the end of this chapter for research work in this area.\nImplementing a Weak Consistency Model*\nLetusconsidertheaccessgraphforaweaklyconsistentsystem. Wedonothaveedgestosignify\nprogram order for nodes in the same thread. Instead, for nodes in the same thread, we have\nedges between regular read\/write nodes and fence operations. We need to add causality and\ncoherence edges to the access graph as we did for the case of SC.\nAn implementation of a weakly consistent machine needs to ensure that this access graph\ndoesnothavecycles. Wecanprovethatthefollowingimplementationdoesnotintroducecycles\nto the access graph.\nLet us ensure that a fence instruction starts after all the previous instructions in program\norder complete for a given thread. The fence instruction is a dummy instruction that simply\nneeds to reach the end of the pipeline. It is used for timing purposes only. We stall the\nfence instruction in the MA stage till all the previous instructions complete. This strategy also\nensuresthatnosubsequentinstructionreachestheMAstage. Once,allthepreviousinstructions\ncomplete,thefenceinstructionproceedstotheRWstage,andsubsequentinstructionscanissue\nrequests to memory.\nSummary of the Discussion on Implementing a Memory Consistency Model\nLet us summarise the previous section on implementing memory consistency models for readers\nwho decided to skip it. Implementing a memory consistency model such as sequential or weak\nconsistency is possible by modifying the pipeline of a processor, and ensuring that the memory\nsystemsendsanacknowledgementtotheprocessoronceitisdoneprocessingamemoryrequest.\nMany subtle corner cases, are possible in high performance implementations and ensuring that\nthey implement a given consistency model is fairly complicated.\n11.4.8 Multithreaded Processors\nLet us now look at a different method for designing multiprocessors. Up till now we have\nmaintained that we need to have physically separate pipelines for creating multiprocessors. We\nhave looked at designs that assign a separate program counter to each pipeline. However, let\nus look at a different approach that runs a set of threads on the same pipeline. This approach (cid:13)c Smruti R. Sarangi 572\nis known as multithreading. Instead of running separate threads on separate pipelines, we run\nthem on the same pipeline. Let us illustrate this concept by discussing the simplest variant of\nmulti-threading known as coarse-grained multithreading.\nDefinition 131\nMultithreading is a design paradigm that proposes to run multiple threads on the same\npipeline. Aprocessorthatimplementsmultithreadingisknownasamultithreadedprocessor.\nCoarse-Grained Multithreading\nLet us assume that we wish to run four threads on a single pipeline. Recall that multiple\nthreads belonging to the same process have their separate program counters, stacks, registers;\nyet, theyhaveacommonviewofmemory. Allthesefourthreadshavetheirseparateinstruction\nstreams,anditisnecessarytoprovideanillusionthatthesefourthreadsarerunningseparately.\nSoftware should be oblivious of the fact that threads are running on a multithreaded processor.\nItshouldperceivethateachthreadhasitsdedicatedCPU.Alongwiththetraditionalguarantees\nof coherence and consistency, we now need to provide an additional guarantee, which is that\nsoftware should be oblivious to multithreading.\nLet us consider a simple scheme, as shown in Figure 11.17.\n1\n4 2\n3\nFigure 11.17: Conceptual view of coarse grain multithreading\nHere, we run thread 1 for n cycles, then we switch to thread 2 and run it for n cycles, then\nwe switch to thread 3, and so on. After executing thread 4 for n cycles, we start executing\nthread 1 again. To execute a thread we need to load its state or context. Recall that we had a\nsimilar discussion with respect to loading and unloading the state of a program in Section 9.8.\nWe had observed that the context of the program comprises of the flags register, the program\ncounter, and the set of registers. We had observed that it is not necessary to keep track of main 573 (cid:13)c Smruti R. Sarangi\nmemory because the memory regions of different processes do not overlap, and in the case of\nmultiple threads, we explicitly want all the threads to share the same memory space.\nInstead of explicitly loading and unloading the context of a thread, we can adopt a simpler\napproach. We can save the context of a thread in the pipeline. For example, if we wish to\nsupport coarse-grained multithreading then we can have four separate flags registers, four\nprogram counters, and four separate register files (one per each thread). Additionally, we can\nhave a dedicated register that contains the id of the currently running thread. For example, if\nwearerunningthread2, thenweusethecontextofthread2, andifwearerunningthread3, we\nuse the context of thread 3. In this manner it is not possible for multiple threads to overwrite\neach other\u2019s state.\nLet us now look at some subtle issues. It is possible that we can have instructions belonging\nto multiple threads at the same point of time in the pipeline. This can happen when we are\nswitchingfromonethreadtothenext. Letusaddathreadidfieldtotheinstructionpacket,and\nfurther ensure that the forwarding and interlock logic takes the id of the thread into account.\nWe never forward values across threads. In this manner it is possible to execute four separate\nthreads on a pipeline with a negligible overhead of switching between threads. We do not\nneed to engage the exception handler to save and restore the context of threads, or invoke the\noperating system to schedule the execution of threads.\nLet us now look at coarse-grained multithreading in entirety. We execute n threads in\nquick succession, and in round robin order. Furthermore, we have a mechanism to quickly\nswitch between threads, and threads do not corrupt each other\u2019s state. However, we still do\nnot execute four threads simultaneously. Then, what is the advantage of this scheme?\nLet us consider the case of memory intensive threads that have a lot of irregular accesses\nto memory. They will thus frequently have misses in the L2 cache, and their pipelines need\nto be stalled for 100-300 cycles till the values come back from memory. Out-of-order pipelines\ncan hide some of this latency by executing some other instructions that are not dependent on\nthe memory value. Nonetheless, it will also stall for a long time. However, at this point, if we\ncan switch to another thread, then it might have some useful work to do. If that thread suffers\nfrom misses in the L2 cache also, then we can switch to another thread and finish some of its\nwork. In this way, we can maximise the throughput of the entire system as a whole. We can\nenvision two possible schemes. We can either switch periodically every n cycles, or switch to\nanother thread upon an event such as an L2 cache miss. Secondly, we need not switch to a\nthread if it is waiting on a high latency event such as an L2 cache miss. We need to switch to a\nthread that has a pool of ready-to-execute instructions. It is possible to design a large number\nof heuristics for optimising the performance of a coarse-grained multithreaded machine.\nImportant Point 19\nLet us differentiate between software threads and hardware threads. A software thread\nis a subprogram that shares a part of its address space with other software threads. The\nthreads can communicate with each other to co-operatively to achieve a common objective.\nIn comparison, a hardware thread is defined as the instance of a software thread or a single\nthreaded program running on a pipeline along with its execution state. A multithreaded\nprocessor supports multiple hardware threads on the same processor by splitting its resources (cid:13)c Smruti R. Sarangi 574\nacross the threads. A software thread might physically be mapped to a separate processor, or\nto a hardware thread. It is agnostic to the entity that is used to execute it. The important\npoint to be noted here is that a software thread is a programming language concept, whereas\na hardware thread is physically associated with resources in a pipeline. We shall use the\nword \u201cthread\u201d for both software and hardware threads. The correct usage needs to be inferred\nfrom the context.\nFine-Grained Multithreading\nFine-grained multithreading is a special case of coarse-grained multithreading where the switch-\ning interval, n, is a very small value. It is typically 1 or 2 cycles. This means that we quickly\nswitch between threads. We can leverage grained multithreading to execute threads that are\nmemory intensive. However, fine-grained multithreading is also useful for executing a set of\nthreads that for example have long arithmetic operations such as division. In a typical proces-\nsor,divisionoperationsandotherspecialisedoperationssuchastrigonometricortranscendental\noperations are slow (3-10 cycles). During this period when the original thread is waiting for\nthe operation to finish, we can switch to another thread and execute some of its instructions\nin the pipeline stages that are otherwise unused. We can thus leverage the ability to switch\nbetween threads very quickly for reducing the idle time in scientific programs that have a lot\nof mathematical operations.\nWe can thus visualise fine-grained multithreading to be a more flexible form of coarse-\ngrained multithreading where we can quickly switch between threads and utilise idle stages to\nperform useful work. Note that this concept is not as simple as it sounds. The devil is in the\ndetails. We need elaborate support for multithreading in all the structures in a regular in-order\nor out-of-order pipeline. We need to manage the context of each thread very carefully, and\nensure that we do not omit instructions, and errors are not introduced. A thorough discussion\non the implementation of multithreading is beyond the scope of this book.\nThe reader needs to appreciate that the logic for switching between threads is non-trivial.\nMost of the time the logic to switch between threads is a combination of time based criteria\n(number of cycles), and event based criteria (high latency event such as L2 cache miss or page\nfault). The heuristics have to be finely adjusted to ensure that the multithreaded processor\nperforms well for a host of benchmarks.\nSimultaneous Multithreading\nFor a single issue pipeline, if we can ensure that every stage is kept busy by using sophisticated\nlogicforswitchingbetweenthreads,thenwecanachievehighefficiency. Recallthatanystagein\nasingleissuepipelinecanprocessonlyoneinstructionpercycle. Incomparison,amultipleissue\npipeline can process multiple instructions per cycle. We had looked at multiple issue pipelines\n(both in-order and out-of-order) in Section 9.11. Moreover, we had defined the number of issue\nslots to be equal to the number of instructions that can be processed by the pipeline every\ncycle. For example, a 3 issue processor, can at the most fetch, decode, and finally execute 3\ninstructions per cycle. 575 (cid:13)c Smruti R. Sarangi\nFor implementing multithreading in multiple issue pipelines, we need to consider the nature\nof dependences between instructions in a thread also. It is possible that fine and coarse-grained\nschemes do not perform well because a thread cannot issue instructions to the functional units\nfor all the issue slots. Such threads are said to have low instruction level parallelism. If we use\na 4 issue pipeline, and the maximum IPC for each of our threads is 1 because of dependences\nin the program, then 3 of our issue slots will remain idle in each cycle. Thus the overall IPC of\nour system of 4 threads will be 1, and the benefits of multithreading will be limited.\nHence, it is necessary to utilise additional issue slots such that we can increase the IPC of\nthe system as a whole. A naive approach is to dedicate one issue slot to each thread. Secondly,\ntoavoidstructuralhazards,wecanhavefourALUsandallotoneALUtoeachthread. However,\nthisisasuboptimalutilisationofthepipelinebecauseathreadmightnothaveaninstructionto\nissue every cycle. It is best to have a more flexible scheme, where we dynamically partition the\nissueslotsamongthethreads. Thisschemeisknownassimultaneousmultithreading(popularly\nknown as SMT). For example, in a given cycle we might find 2 instructions from thread 2, and\n1 instruction each from threads 3, and 4. This situation might reverse in the next cycle. Let us\ngraphically illustrate this concept in Figure 11.18, and simultaneously also compare the SMT\napproach with fine and coarse-grained multithreading.\nThe columns in Figure 11.18 represent the issue slots for a multiple issue machine, and the\nrows represent the cycles. Instructions belonging to different threads have different colours.\nFigure 11.18(a) shows the execution of instructions in a coarse-grained machine, where each\nthreadexecutesfortwoconsecutivecycles. Weobservethatalotofissueslotsareemptybecause\nwe do not find sufficient number of instructions that can execute. Fine-grained multithreading\n(shown in Figure 11.18(b)) also has the same problem. However, in an SMT processor, we\nare typically able to keep most of the issue slots busy, because we always find instructions\nfrom the set of available threads that are ready to execute. If one thread is stalled for some\nreason, other threads compensate by executing more instructions. In practice, all the threads\ndo not have low ILP1 phases simultaneously. Hence, the SMT approach has proven to be a very\nversatile and effective method for leveraging the power of multiple issue processors. Since the\nPentium 4 (released in the late nineties), most of the Intel processors support different variants\nof simultaneous multithreading. In Intel\u2019s terminology SMT is known as hyperthreading . The\nlatest (as of 2012) IBM Power 7 processor has 8 cores, where each core is a 4-way SMT (can\nrun 4 threads per each core).\nNote that the problem of selecting the right set of instructions to issue is very crucial to\nthe performance of an SMT processor. Secondly, the memory bandwidth requirement of an\nn-way SMT processor is higher than that of an equivalent uniprocessor. The fetch logic is also\nfar more complicated, because now we need to fetch from four separate program counters in\nthe same cycle. Lastly, the issues of maintaining coherence, and consistency further complicate\nthe picture. The reader can refer to the research papers mentioned in the \u201cFurther Reading\u201d\nsection at the end of this chapter.\n1ILP(instructionlevelparallelism,definedasthenumberofinstructionsthatarereadytoexecuteinparallel\neach cycle) (cid:13)c Smruti R. Sarangi 576\nCoarse-grained Fine-grained Simultaneous\nmultithreading multithreading multithreading\nThread 1\nThread 2\nThread 3\nThread 4\n(a) (b) (c)\nFigure 11.18: Instruction execution in multithreaded processors\n11.5 SIMD Multiprocessors\nLet us now discuss SIMD multiprocessors. SIMD processors are typically used for scientific\napplications, high intensity gaming, and graphics. They do not have a significant amount of\ngeneral purpose utility. However, for a limited class of applications, SIMD processors tend to\noutperform their MIMD counterparts.\nSIMD processors have a rich history. In the good old days we had processors arranged\nas arrays. Data typically entered through the first row, and first column of processors. Each\nprocessor acted on input messages, generated an output message, and sent the message to its\nneighbours. Suchprocessorswereknownassystolic arrays. Systolicarrayswereusedformatrix\nmultiplication,andotherlinearalgebraoperations. Subsequently,severalvendorsnotably,Cray,\nincorporated SIMD instructions in their processors to design faster and more power efficient\nsupercomputers. Nowadays, most of these early efforts have subsided. However, some aspects\nof classical SIMD computers where a single instruction operates on several streams of data,\nhave crept into the design of modern processors.\nWe shall discuss an important development in the area of modern processor design, which is\nthe incorporation of SIMD functional units, and instructions, in high-performance processors.\n11.5.1 SIMD \u2013 Vector Processors\nBackground\nLet us consider the problem of adding two n element arrays. In a single threaded implemen-\ntation, we need to load the operands from memory, add the operands, and store the result in\nmemory. Consequently, for computing each element of the destination array, we require two\nload instructions, one add instruction, and one store instruction. Traditional processors try to\nattain speedups by exploiting the fact that we can compute (c[i] = a[i]+b[i]), in parallel with\n(c[j] = a[j]+b[j]) because these two operations do not have any dependences between them.\nemiT 577 (cid:13)c Smruti R. Sarangi\nHence, it is possible to increase IPC by executing many such operations in parallel.\nLet us now consider superscalar processors. If they can issue 4 instructions per cycle,\nthen their IPC can at the most be 4 times that of a single cycle processor. In practice, the\npeak speedup over a single cycle processor that we can achieve with such inherently parallel\narray processing operations is around 3 to 3.5 times for a 4 issue processor. Secondly, this\nmethod of increasing IPC by having wide issue widths is not scalable. We do not have 8 or\n10 issue processors in practice because the logic of the pipeline gets very complicated, and the\narea\/power overheads become prohibitive.\nHence, designers decided to have special support for vector operations that operate on large\nvectors (arrays) of data. Such processors were known as vector processors. The main idea here\nis to process an entire array of data at once. Normal processors use regular scalar data types\nsuch as integers and floating point numbers; whereas, vector processors use vector data types,\nwhich are essentially arrays of scalar data types.\nDefinition 132\nA vector processor considers a vector of primitive data types (integer or floating point num-\nbers) as its basic unit of information. It can load, store, and perform arithmetic operations\non entire vectors at once. Such instructions that operate on vectors of data, are known as\nvector instructions.\nOne of the most iconic products that predominantly used vector processors was the Cray\n1 Supercomputer. Such supercomputers were primarily used for scientific applications that\nmainly consisted of linear algebra operations. Such operations work on vectors of data and\nmatrices, and are thus well suited to be run on vector processors. Sadly, beyond the realm of\nhigh intensity scientific computing, vector processors did not find a general purpose market till\nthe late nineties.\nIn the late nineties, personal computers started to be used for research, and for running\nscientific applications. Secondly, instead of designing custom processors for supercomputers,\ndesigners started to use regular commodity processors for building supercomputers. Since then,\nthe trend continued till the evolution of graphics processors. Most supercomputers between\n1995 and 2010, consisted of thousands of commodity processors. Another important reason to\nhave vector instructions in a regular processor was to support high intensity gaming. Gaming\nrequires a massive amount of graphics processing. For example, modern games render complex\nscenes with multiple characters, and thousands of visual effects. Most of these visual effects\nsuch as illumination, shadows, animation, depth, and colour processing, are at its core basic\nlinear algebra operations on matrices containing points or pixels. Due to these factors regular\nprocessors started to incorporate a limited amount of vector support. Specifically, the Intel\nprocessors provided the MMX, SSE 1-4 vector instruction sets, AMD processors provided the\n3DNow! vector extensions, and ARM processors provide the ARM(cid:13)R NeonTMvector ISA. There\nare a lot of commonalities between these ISAs, and hence let us not focus on any specific\nISA. Let us instead the discuss the broad principles behind the design and operation of vector\nprocessors. (cid:13)c Smruti R. Sarangi 578\n11.5.2 Software Interface\nLet us first consider the model of the machine. We need a set of vector registers. For example,\nthe x86 SSE (Streaming SIMD Extensions) instruction set defines sixteen 128-bit registers\n(XMM0...XMM15). Eachsuchregistercancontainfourintegers, orfourfloatingpointvalues.\nAlternatively, it can also contain eight 2-byte short integers, or sixteen 1-byte characters. On\nthe same lines, every vector ISA defines additional vector registers that are wider than normal\nregisters. Typically, each register can contain multiple floating point values. Hence, in our\nSimpleRisc ISA, let us define eight 128-bit vector registers: vr0...vr7.\nNow, we need instructions to load, store, and operate on vector registers. For loading,\nvector registers, there are two options. We can either load values from contiguous memory\nlocations, or from non-contiguous memory locations. The former case is more specific, and is\ntypically suitable for array based applications, where all the array elements are anyway stored\nin contiguous memory locations. Most vector extensions to ISAs support this variant of the\nload instruction because of its simplicity, and regularity. Let us try to design such a vector load\ninstruction v.ld for our SimpleRisc ISA. Let us consider the semantics shown in Table 11.4.\nHere, the v.ld instruction reads in the contents of the memory locations ([r1+12], [r1+16],\n[r1+20], [r1+ 24]) into the vector register vr1. In the table below note that (cid:104)vreg(cid:105) is a vector\nregister.\nExample Semantics Explanation\nv.ld vr1, 12[r1] v.ld (cid:104)vreg(cid:105), (cid:104)mem(cid:105) vr1 \u2190 ([r1+12], [r1+16], [r1+20], [r1+ 24])\nTable 11.4: Semantics of the contiguous variant of the vector load instruction\nNow, let us consider the case of matrices. Let us consider a 10,000 element matrix,\nA[100][100], and assume that data is stored in row major order (see Section 3.2.2). Assume\nthat we want to operate on two columns of the matrix. In this case, we have a problem because\nthe elements in a column are not saved in contiguous locations. Hence, a vector load instruc-\ntion that relies on the assumption that the input operands are saved in contiguous memory\nlocations, will cease to work. We need to have dedicated support to fetch all the data for the\nlocations in a column and save them in a vector register. Such kind of an operation, is known\nas a scatter-gather operation. This is because, the input operands are essentially scattered in\nmain memory. We need to gather, and put them in one place, which is the vector register. Let\nus consider a scatter-gather variant of the vector load instruction, and call it v.sg.ld. Instead\nof making assumptions about the locations of the array elements, the processor reads another\nvector register that contains the addresses of the elements (semantics shown in Table 11.5). In\nthis case, a dedicated vector load unit reads the memory addresses stored in vr2, fetches the\ncorresponding values from memory, and writes them in sequence to the vector register, vr1.\nExample Semantics Explanation\nv.sg.ld vr1, vr2 v.sg.ld (cid:104)vreg(cid:105), (cid:104)vreg(cid:105) vr1 \u2190 ([vr2[0]], [vr2[1]], [vr2[2]], [vr2[3]])\nTable 11.5: Semantics of the non-contiguous variant of the vector load instruction 579 (cid:13)c Smruti R. Sarangi\nOnce, we have data loaded in vector registers, we can operate on two such registers directly.\nForexample,ifweconsider128-bitvectorregisters,vr1,andvr2. Then,theassemblystatement\nv.add vr3,vr1,vr2, adds each pair of corresponding 4-byte floating point numbers stored in the\ninput vector registers (vr1 and vr2), and stores the results in the relevant positions in the\noutput vector register (vr3). Note that we use the vector add instruction (v.add) here. We\nshow an example of a vector add instruction in Figure 11.19.\nvr1\nvr2\nvr3\nFigure 11.19: Example of a vector addition\nVector ISAs define similar operations for vector multiplication, division, and logical opera-\ntions. Note that it is not necessary for a vector instruction to always have two input operands,\nwhich are vectors. We can multiply, a vector with a scalar, or we can have an instruction\nthat operates on just one vector operand. For example, the SSE instruction set has dedicated\ninstructions for computing trigonometric functions such as sin, and cos, for a set of floating\npoint numbers packed in a vector register. If a vector instruction can simultaneously perform\noperations on n operands, then we say that we have n data lanes, and the vector instruction\nsimultaneously performs an operation on all the n data lanes.\nDefinition 133\nIf a vector instruction can simultaneously perform operations on n operands, then we say\nthat we have n data lanes, and the vector instruction simultaneously performs an operation\non all the n data lanes.\nThelaststepistostorethevectorregisterinmemory. Hereagain,therearetwooptions. We\ncan either store to contiguous memory locations, which is simpler, or save to non-contiguous\nlocations. We can design two variants of the vector store instruction (contiguous and non-\ncontiguous) on the lines of the two variants of vector load instructions (v.ld and v.sg.ld).\nSometimesitisnecessarytointroduceinstructionsthattransferdatabetweenscalarandvector\nregisters. We shall not describe such instructions for the sake of brevity. We leave designing\nsuch instructions as an exercise for the reader.\n11.5.3 A Practical Example using SSE Instructions\nLet us now consider a practical example using the x86 based SSE instruction set. We shall not\nuse actual assembly instructions. We shall instead use functions provided by the gcc compiler\nthat act as wrappers for the assembly instructions. These functions are called gcc intrinsics. (cid:13)c Smruti R. Sarangi 580\nLet us now solve the problem of adding two arrays of floating point numbers. In this case,\nwe wish to compute c[i] = a[i]+b[i], for all values of i.\nThe SSE instruction set contains 128-bit registers. Each register can be used to store four\n32-bitfloatingpointnumbers. Hence, ifwehaveanarrayofN numbers, weneedtohave(cid:100)N\/4(cid:101)\niterations, because we can add at the most 4 pairs of numbers in each cycle. In each iteration,\nwe need to load vector registers, add them, and store the result in memory. This process of\nbreaking up a vector computation into a sequence of loop iterations based on the sizes of vector\nregisters is known as strip mining.\nDefinition 134\nThe process of breaking up a vector computation into a sequence of loop iterations based on\nthe sizes of vector registers is known as strip mining. For example, if a vector register can\nhold 16 integers, and we wish to operate on 1024 integer vectors, then we need a loop with\n64 iterations.\nExample 127\nWrite a function in C\/C++ to add the elements in the arrays a and b pairwise, and save\nthe results in the array, c, using the SSE extensions to the x86 ISA. Assume that the number\nof entries in a and b are the same, and are a multiple of 4.\nAnswer:\nvector addition\nvoid sseAdd (const float a[], const float b[], float c[], int N)\n1\n{\n2\n\/* strip mining *\/\n3\nint numIters = N \/ 4;\n4\n5\n\/* iteration *\/\n6\nfor (int i = 0; i < numIters; i++) {\n7\n\/* load the values *\/\n8\n__m128 val1 = _mm_load_ps (a);\n9\n__m128 val2 = _mm_load_ps (b);\n10\n11\n\/* perform the vector addition *\/\n12\n__m128 res = _mm_add_ps(val1, val2);\n13\n14\n\/* store the result *\/\n15\n_mm_store_ps(c, res);\n16\n17\n\/* increment the pointers *\/\n18\na += 4 ; b += 4; c+= 4;\n19 581 (cid:13)c Smruti R. Sarangi\n}\n20\n}\n21\nLet us consider the C code snippet in Example 127. We first calculate the number of\niterations in Line 4. In each iteration, we consider a block of 4 array elements. In Line 9,\nwe load a set of four floating point numbers into the 128-bit vector variable, val1. val1 is\nmapped to a vector register by the compiler. We use the function mm load ps to load a set\nof 4 contiguous floating point values from memory. For example, the function mm load ps(a)\nloads four floating point values in the locations, a, a+4, a+8, and a+12 into a vector register.\nSimilarly, we load the second vector register, val2, with four floating point values starting from\nthememoryaddress, b. InLine13, weperformthevectoraddition, andsavetheresultina128-\nbit vector register associated with the variable res. We use the intrinsic function, mm add ps,\nfor this purpose. In Line 16, we store the variable, res, in the memory locations namely c, c+4,\nc+8, and c+12.\nBefore proceeding to the next iteration, we need to update the pointers a, b, and c. Since\nwe process 4 contiguous array elements every cycle, we update each of the pointer by 4 (4 array\nelements) in Line 19.\nWe can quickly conclude that vector instructions facilitate bulk computations such as bulk\nloads\/stores and adding a set of numbers pairwise, in one go. We compared the performance\nof this function, with a version of the function that does not use vector instructions on a\nquad core Intel Core i7 machine. The code with SSE instructions ran 2-3 times faster for\nmillion element arrays. If we would have had wider SSE registers, then we could have gained\nmore speedups. The latest AVX vector ISA on x86 processors supports 256 and 512-bit vector\nregisters. Interested readers can implement the function shown in Example 127 using the AVX\nvector ISA, and compare the performance.\n11.5.4 Predicated Instructions\nWe have up till now considered vector load, store, and ALU operations. What about branches?\nTypically, branches have a different connotation in the context of vector processors. For exam-\nple, let us consider a processor with vector registers that are wide enough to hold 32 integers,\nandwehaveaprogramwhichrequiresustopair-wiseaddonly18integers, andthenstorethem\nin memory. In this case, we cannot store the entire vector register to memory because we risk\noverwriting valid data.\nLet us consider another example. Assume that we want to apply the function inc10(x) on\nall elements of an array. In this case, we wish to add 10 to the input operand, x, if it is less\nthan 10. Such patterns are very common in programs that run on vector processors, and thus\nwe need additional support in vector ISAs to support them.\nfunction inc10(x):\nif (x < 10)\nx = x + 10; (cid:13)c Smruti R. Sarangi 582\nLetusaddanewvariantofaregularinstruction, andcallitapredicated instruction(similar\nto conditional instructions in ARM). For example, we can create predicated variants of regular\nload, store, and ALU instructions. A predicated instruction executes if a certain condition is\ntrue, otherwise it does not execute at all. If the condition is false, a predicated instruction is\nequivalent to a nop.\nDefinition 135\nA predicated instruction is a variant of a normal load, store, or ALU instruction. It\nexecutes normally, if a certain condition is true. However, if the associated condition is\nfalse, then it gets converted to a nop. For example, the addeq instruction in the ARM ISA,\nexecutes like a normal add instruction if the last comparison has resulted in an equality.\nHowever, if this is not the case, then the add instruction does not execute at all.\nLet us now add support for predication in the SimpleRisc ISA. Let us first create a vector\nform of the cmp instruction, and call it v.cmp. It compares two vectors pair-wise, and saves the\nresults of the comparison in the v.flags register, which is a vector form of the flags register.\nEachcomponentofthev.flagsregistercontainsanE andGT field,similartotheflagsregister\nin a regular processor.\nv.cmp vr1, vr2\nThis example compares vr1, and vr2, and saves the results in the v.flags register. We can\nhave an alternate form of this instruction that compares a vector with a scalar.\nv.cmp vr1, 10\nNow, let us define the predicated form of the vector add instruction. This instruction adds\nthe ith elements of two vectors, and updates the ith element of the destination vector register,\nif the v.flags[i] (ith element of v.flags) register satisfies certain properties. Otherwise, it does\nnot update the ith element of the destination register. Let the generic form of the predicated\nvector add instruction be: v.p.add. Here, p is the predicate condition. Table 11.6 lists the\ndifferent values that p can take.\nPredicate Condition Meaning\nlt less than\ngt greater than\nle less than or equal\nge greater than or equal\neq equal\nne not equal\nTable 11.6: List of conditions for predicated vector instructions in SimpleRisc\nNow, let us consider the following code snippet. 583 (cid:13)c Smruti R. Sarangi\nv.lt.add vr3, vr1, vr2\nHere, the value of the vector register vr3 is the sum of the vectors represented by vr1 and\nvr2. The predication condition is less than (lt). This means that if both the E and GT flags\nare false for element i in the v.flags register, then only we perform the addition for the ith\nelement, and set its value in the vr3 register. The elements in the vr3 register that are not set\nby the add instruction maintain their previous value. Thus, the code to implement the function\ninc10(x) is as follows. We assume that vr1 contains the values of the input array.\nv.cmp vr1, 10\nv.lt.add vr1, vr1, 10\nLikewise, we can define predicated versions of the load\/store instructions, and other ALU\ninstructions.\n11.5.5 Design of a Vector Processor\nLet us now briefly consider the design of vector processors. We need to add a vector pipeline\nsimilar to the scalar pipeline. In specific, the OF stage reads the vector register file for vector\noperands, and the scalar register file for scalar operands. Subsequently, it buffers the values\nof operands in the pipeline registers. The EX stage sends scalar operands to the scalar ALU,\nand sends vector operands to the vector ALU. Similarly, we need to augment the MA stage\nwith vector load and store units. For most processors, the size of a cache block is an integral\nmultiple of the size of a vector register. Consequently, vector load and store units that operate\non contiguous data do not need to access multiple cache blocks. Hence, a vector load and store\naccess is almost as fast as a scalar load and store access because the atomic unit of storage in a\ncache is a block. In both cases (scalar and vector), we read the value of a block and choose the\nrelevant bytes using the column muxes. We need to change the structure of the L1 cache to\nread in more data at a time. Lastly, the writeback stage writes back scalar data to the scalar\nregister file and vector data to the vector register file.\nIn a pipelined implementation of a vector processor, the interlock and forwarding logic is\ncomplicated. We need to take into account the conflicts between scalar instructions, between\nvector instructions, and between a scalar and vector instruction. The forwarding logic needs\nto forward values between different functional unit types, and thus ensure correct execution.\nNote that vector instructions need not always be as fast as their scalar counterparts. Espe-\ncially, scatter-gather based vector load store instructions are slower. Since modern out-of-order\npipelines already have dedicated support for processing variable latency instructions, vector\ninstructions can seamlessly plug into this framework.\n11.6 Interconnection Networks\n11.6.1 Overview\nLet us now consider the problem of interconnecting different processing and memory elements.\nTypically multicore processors use a checkerboard design. Here, we divide the set of processors\ninto tiles. A tile typically consists of a set of 2-4 processors. A tile has its private caches (L1 (cid:13)c Smruti R. Sarangi 584\nand possibly L2). It also contains a part of the shared last level cache (L2 or L3). The part of\nthe shared last level cache that is a part of a given tile is known as a slice . Typically, a slice\nconsists of 2-4 banks (see Section 11.4.5). Additionally, a tile, or a group of tiles might share a\nmemory controller in modern processors. The role of the memory controller is to co-ordinate\nthe transfer of data between the on-chip caches, and the main memory. Figure 11.20 shows a\nrepresentative layout of a 32 core multiprocessor. The cores have a darker colour as compared\nto the cache banks. We use a tile size of 2 (2 processors and 2 cache banks), and assume that\nthe shared L2 cache has 32 cache banks evenly distributed across the tiles. Moreover, each tile\nhas a dedicated memory controller, and a structure called a router.\nA router is a specialised unit, and is defined in Definition 136.\nDefinition 136\n1. A router sends messages originating from processors or caches in its tile to other tiles\nthrough the on chip network.\n2. The routers are interconnected with each other via an on-chip network.\n3. A message travels from the source router to the destination router (of a remote tile)\nvia a series of routers. Each router on the way forwards the message to another\nrouter, which is closer to the destination.\n4. Finally the router associated with the destination tile forwards the message to a pro-\ncessor or cache in the remote tile.\n5. Adjacent routers are connected via a link. A link is a set of passive copper wires that\nare used to transmit messages (more details in Chapter 12).\n6. A router typically has many incoming links, and many outgoing links. One set of\nincoming and outgoing links connect it to processors and caches in its tile. Each link\nhas a unique identifier.\n7. A router has a fairly complicated structure, and typically consists of a 3 to 5-stage\npipeline. Most designs typically dedicate pipeline stages to buffering a message, com-\nputing the id of the outgoing link, arbitrating for the link, and sending the message\nover the outgoing link.\n8. The arrangement of routers and links is referred to as the onchipnetwork, or network\non chip. It is abbreviated as the NOC.\n9. Let us refer to each router connected to the NOC as a node. Nodes communicate with\neach other by sending messages.\nDuringthecourseoftheexecutionofaprogram, itsendsbillionsofmessagesovertheNOC.\nThe NOC carries coherence messages, LLC (last level cache) request\/response messages, and 585 (cid:13)c Smruti R. Sarangi\nTile\nCore\nCache bank\nMemory\ncontroller\nRouter\nFigure 11.20: The layout of a multicore processor\nmessages between caches, and memory controllers. The operating system also uses the NOC to\nsend messages to cores for loading and unloading threads. Due to the high volume of messages,\nlarge parts of the NOC often experience a sizeable amount of congestion. Hence, it is essential\nto design NOCs that reduce congestion to the maximum extent possible, are easy to design\nand manufacture, and ensure that messages quickly reach their destination. Let us define two\nimportant properties of an NOC namely bisection bandwidth and diameter.\n11.6.2 Bisection Bandwidth and Network Diameter\nBisection Bandwidth\nLet us consider a network topology where the vertices are the nodes, and the edges between\nthe vertices are the links. Suppose there is a link failure, or for some other reason such as\ncongestion, a link is unavailable, then it should be possible to route messages through alternate\npaths. For example, let us consider a network arranged as a ring. If one link fails, then we can\nalways send a message through the other side of the ring. If we were sending the message in a\nclockwise fashion, we can send it in an anti-clockwise fashion. However, if there are two link\nfailures, it is possible that the network can get disconnected into two equal parts. We would\nthus like to maximise the number of link failures that are required to completely disconnect the\nnetwork into sizeably large parts (possibly equal). Let us refer to the number of such failures as\nthe bisection bandwidth. The bisection bandwidth is a measure of the reliability of the network.\nIt is precisely defined as the minimum number of links that need to fail to partition the network\ninto two equal parts.\nThere can be an alternative interpretation of the bisection bandwidth. Let us assume that\nnodes in one half of the network are trying to send messages to nodes in the other half of the (cid:13)c Smruti R. Sarangi 586\nnetwork. Then the number of messages that can be simultaneously sent is at least equal to the\nbisection bandwidth. Thus, the bisection bandwidth is also a measure of the bandwidth of a\nnetwork.\nDefinition 137\nThe bisectionbandwidth is defined as the minimum number of link failures that are required\nto partition a network into two equal parts.\nNetwork Diameter\nWe have discussed reliability, and bandwidth. Now, let us focus on latency. Let us consider\npairs of nodes in the network. Let us subsequently consider the shortest path between each pair\nof nodes. Out of all of these shortest paths, let us consider the path that has the maximum\nlength. The length of this path is an upper bound on the proximity of nodes in the network,\nand is known as the diameter of the network. Alternatively, we can interpret the diameter of a\nnetwork as an estimate of the worst case latency between any pair of nodes.\nDefinition 138\nLet us consider all pairs of nodes, and compute the shortest path between each pair. The\nlength of the longest such path is known as the network diameter. It is a measure of the\nworst case latency of the network.\n11.6.3 Network Topologies\nLet us review some of the most common network topologies in this section. Some of these\ntopologies are used in multicore processors. However, most of the complex topologies are used\nin loosely coupled multiprocessors that use regular Ethernet links to connect processors. For\neach topology, let us assume that it has N nodes. For computing the bisection bandwidth, we\ncanfurthermakethesimplisticassumptionthatN isdivisibleby2. Notethatmeasureslikethe\nbisection bandwidth, and the diameter are approximate measures, and are merely indicative\nof broad trends. Hence, we have the leeway to make simplistic assumptions. Let us start out\nwith considering simpler topologies that are suitable for multicores. We need to aim for a high\nbisection bandwidth, and low network diameter.\nChain and Ring\nFigure 11.21 shows a chain of nodes. Its bisection bandwidth is 1, and the network diameter is\nN \u22121. This is our worst configuration. We can improve both the metrics by considering a ring\nof nodes (Figure 11.22). The bisection bandwidth is now 2, and the network diameter is N\/2.\nBothofthesetopologiesarefairlysimple, andhavebeensupersededbyothertopologies. Letus 587 (cid:13)c Smruti R. Sarangi\nFigure 11.21: Chain\nFigure 11.22: Ring\nnow consider a topology known as a fat tree, which is commonly used in cluster computers. A\ncluster computer refers to a loosely coupled multiprocessor that consists of multiple processors\nconnected over the local area network.\nDefinition 139\nA cluster computer refers to a loosely coupled computer that consists of multiple processors\nconnected over the local area network.\nFat Tree\nFigure 11.23 shows a fat tree. In a fat tree, all the nodes are at the leaves, and all the internal\nnodes of the tree are routers dedicated to routing messages. Let us refer to these internal\nnodes as switches. A message from node a to node b first travels to the closest node that is a\ncommon ancestor of both a and b. Then it travels downwards towards b. Note that the density\nof messages is the highest near the root. Hence, to avoid contention, and bottlenecks, we\ngradually increase the number of links connecting a node and its children, as we move towards\nthe root. This strategy reduces the message congestion at the root node.\nIn our example, two subtrees are connected to the root node. Each subtree has 4 nodes. At\nthe most the root can receive 4 messages from each subtree. Secondly, at the most, it needs\nto send 4 messages to each subtree. Assuming a duplex link, the root needs to have 4 links\nconnecting it to each of its children. Likewise, the next level of nodes need 2 links between\nthem and each of their child nodes. The leaves need 1 link each. We can thus visualise the tree\ngrowing fatter, as we proceeds towards the root, and hence it is referred to as a fat tree.\nThenetworkdiameterisequalto2log(N).Thebisectionbandwidthisequaltotheminimum\nnumber of links that connect the root node to each of its children. If we assume that the tree (cid:13)c Smruti R. Sarangi 588\nFigure 11.23: Fat Tree\nis designed to ensure that there is absolutely no contention for links at the root, then we need\nto connect the root with N\/2 links to each subtree. Thus, the bisection bandwidth in this case\nis N\/2. Note that we do not allot N\/2 links between the root and its children in most practical\nscenarios. This is because the probability of all the nodes in a subree transmitting messages at\nthe same time is low. Hence, in practice we reduce the number of links at each level.\nMesh and Torus\nFigure 11.25: Torus\nFigure 11.24: Mesh 589 (cid:13)c Smruti R. Sarangi\nLetusnowlookattopologiesthataremoresuitableformulticores. Oneofthemostcommon\ntopologiesisameshwhereallthenodesareconnectedinamatrixlikefashion(seeFigure11.24).\nNodes at the corner have two neighbours, nodes on the rim have three neighbours, and the rest\nof the nodes have four neighbours. Let us now compute the diameter and bisection bandwidth\n\u221a\nof a mesh. The longest path is between two corner nodes. The diameter is thus equal to (2 N\n- 2). To divide the network into two equal halves we need to either split the mesh in the\n\u221a\nmiddle (horizontally or vertically). Since we have N nodes in a row, or column, the bisection\n\u221a\nbandwidth is equal to N. The mesh is better than a chain and a ring in terms of these\nparameters.\nUnfortunately, the mesh topology is asymmetric in nature. Nodes that are at the rim of\nthe mesh are far away from each other. Consequently, we can augment a mesh with cross links\nbetween the extremities of each row and column. The resulting structure is known as a torus,\nand is shown in Figure 11.25. Let us now look at the properties of tori (plural of torus). In\nthis case, nodes on opposite sides of the rim of the network, are only one hop apart. The\nlongest path is thus between any of the corner nodes and a node at the center of the torus.\n\u221a \u221a \u221a\nThe diameter is thus again equal to (ignoring small additive constants) N\/2+ N\/2 = N.\n\u221a\nRecall that the length of each side of the torus is equal to N.\nNow, to divide the network into two equal parts let us split it horizontally. We need to thus\n\u221a \u221a\nsnap N vertical links, and N cross links (links between the ends of each column). Hence,\n\u221a\nthe bisection bandwidth is equal to 2 N.\nFigure 11.26: Folded Torus (cid:13)c Smruti R. Sarangi 590\n\u221a \u221a \u221a\nBy adding 2 N cross links ( N for rows, and N for columns), we have halved the\ndiameter, and doubled the bisection bandwidth of a torus. However, this scheme still has some\nproblems. Let us elaborate.\nWhile defining the diameter, we made an implicit assumption that the length of every link\nis almost the same, or alternatively the time a message takes to traverse a link is almost the\nsame for all the links in the network. Hence, we defined the diameter in terms of the number of\nlinks that a message traverses. This assumption is not very unrealistic because in general the\npropagation time through a link is small as compared to the latencies of routers along the way.\nNevertheless, there are limits to the latency of a link. If a link is very long, then our definition\nof the diameter needs to be revised. In the case of tori, we have such a situation. The cross\n\u221a\nlinks are physically N times longer than regular links between adjacent nodes. Hence, as\ncompared to a mesh, we have not significantly reduced the diameter in practice because nodes\nat the ends of a row are still far apart.\nWe can fortunately solve this problem by using a slightly modified structure called a folded\ntorus as shown in Figure 11.26. Here, the topology of each row and column is like a ring. One\nhalf of the ring consists of regular links that were originally a part of the mesh topology, and\nthe other half comprises of the cross links that were added to convert a mesh into a torus. We\nalternately place nodes on the regular links and on the cross links. This strategy ensures that\nthe distance between adjacent nodes in a folded torus is twice the distance between adjacent\n\u221a\nnodes in a regular torus. However, we avoid the long cross links ( N hops long) between the\ntwo ends of a row or column.\nThe bisection bandwidth and the diameter of the network remain the same as that of the\ntorus. In this case, there are several paths that can qualify as the longest path. However, the\npath between a corner to the center is not the longest. One of the longest paths is between op-\nposite corners. The folded torus is typically the preferred configuration in multicore processors\nbecause it avoids long cross links.\nHypercube\nLet us now consider a network that has O(log(N)) diameter. These networks use a lot of\nlinks; hence, they are not suitable for multicores. However, they are used often in larger cluster\ncomputers. Thisnetworkisknownasahypercube. Ahypercubeisactuallyafamilyofnetworks,\nwhere each network has an order. A hypercube of order k is referred to as H .\nk\nFigure11.27(a)showsahypercubeoforder0(H ). Itisasinglepoint. Tocreateahypercube\n0\nof order 1, we take two copies of a hypercube of order 0, and connect the corresponding points\nwith lines. In this case, H has a single point. Therefore, H is a simple line segment (see\n0 1\nFigure 11.27(b)). Now, let us follow the same procedure to create a hypercube of order 2.\nWe place two copies of H close to each other and connect corresponding points with lines.\n1\nHence, H is a rectangle (see Figure 11.27(c)). Let us follow the same procedure to create\n2\na hypercube of order 3 in Figure 11.27(d). This network is equivalent to a normal cube (the\n\u201ccube\u201dinhypercube). Finally, Figure11.27(e), showsthetopologyofH , whereweconnectthe\n4\ncorresponding points of two cubes. We can proceed in a similar manner to create hypercubes\nof order k.\nLet us now investigate the properties of a hypercube. The number of nodes in H is equal\nk\nto twice the number of nodes in H . This is because, we form H by joining two copies of\nk\u22121 k 591 (cid:13)c Smruti R. Sarangi\n00 01 000 001\n0 0 1 010 011\n100 101\n10 11\nH H H 110 111\n0 1 2 H\n(c) 3\n(a) (b)\n(d)\nH\n4\n(e)\nFigure 11.27: Hypercube\nH . Since H has 1 node, we can conclude that H has 2k nodes. Let us propose a method\nk\u22121 1 k\nto label the nodes of a hypercube as shown in Figure 11.27. We label the single node in H as\n0\n0. When we join two copies of a hypercube of order, k\u22121, we maintain the same labelling of\nnodes for (k\u22121) least significant digits. However, for nodes in one copy, we set the MSB as 1,\nand for nodes in the other copy, we set the MSB to be 0.\nLet us consider, H (Figure 11.27(c)). The nodes are labelled 00, 01, 10, and 11. We\n2\nhave created a similar 3-bit labelling for H (Figure 11.27(d)). In our labelling scheme, a\n3\nnode is connected to all other nodes that have a label differing in only one bit. For example,\nthe neighbours of the node 101, are 001, 111, and 100. A similar labelling for H requires\nk\nk = log(N)) bits per node.\nThis insight will help us compute the diameter of a hypercube. Let us explain through an\nexample. Consider the 8 node hypercube, H . Assume that we want a message to travel from\n3\nnode A (000) to node B (110). Let us scan the labels of both the nodes from the MSB to the\nLSB. The first bit (MSB) does not match. Hence, to make the first bit match, let us route the\nmessage to node A (100). Let us now scan the next bit. Here, again there is a mismatch.\n1\nHence, let us route the message to node A (110). Now, we take a look at the third bit (LSB),\n2\nand find it to match. The message has thus reached its destination. We can follow the same\napproach for an N-bit hypercube. Since each node has a log(N) bit label, and in each step we\nflip at most one bit in the label of the current node, we require a maximum of log(N) routing\nsteps. Thus the network diameter is log(N).\nLet us now compute the bisection bandwidth. We shall state the result without proof be-\ncausethecomputationofthebisectionbandwidthofahypercuberequiresathoroughtheoretical\ntreatment of hypercubes. This is beyond the scope of this book. The bisection bandwidth of\nan N-node hypercube is equal to N\/2. (cid:13)c Smruti R. Sarangi 592\nButterfly\n1 1\n00 00 00\n2 2\n3 3\n01 01 01\n4 4\n5 5\n10 10 10\n6 6\n7 7\n11 11 11\n8 8\nFigure 11.28: Butterfly\nLet us now look at our last network called the butterfly that also has O(log(N)) diameter,\nyet, is suitable for multicores. Figure 11.28 shows a butterfly network for 8 nodes. Each node\nis represented by a circle. Along with the nodes, we have a set of switches or internal nodes\n(shown with rectangles) that route messages between nodes. The messages start at the left\nside, pass through the switches, and reach the right side of the diagram. Note that the nodes\non the leftmost and rightmost sides of the figure are actually the same set of nodes. We did\nnot want to add left to right cross links to avoid complicating the diagram; hence, we show the\nset of nodes in duplicate.\nLet us start from the left side of the diagram. For N nodes (assuming N is a power of 2),\nwe have N\/2 switches in the first column. Two nodes are connected to each switch. In our\nexample in Figure 11.28, we have labelled the nodes 1...8. Each node is connected to a switch.\nOnce a message enters a switch, it gets routed to the correct switch in the rightmost column\nthrough the network of switches. For 8 nodes, we have 4 switches in each column. For each\ncolumn, we have labelled them 00...11 in binary.\nLet us consider an example. Assume that we want to send a message from node 4 to node 7.\nIn this case, the message enters switch 01 in the first column. It needs to reach switch 11 in the\nthird column. We start out by comparing the MSBs of the source switch, and the destination\nswitch. If they are equal, then the message proceeds horizontally rightwards to a switch in\nthe adjacent column and same row. However, if the MSBs are unequal (as is the case in our\nexample), then we need to use the second output link to send it to a switch in a different row 593 (cid:13)c Smruti R. Sarangi\nin the next column. The label of the new switch differs from the label of the original switch\nby just 1 bit, which is the MSB bit in this case. Similarly, to proceed from the second to the\nthird column, we compare the second bit position (from the MSB). If they are equal, then the\nmessage proceeds horizontally, otherwise it is routed to a switch such that the first two bits\nmatch. In this case, the label of the switch that we choose in the second column is 11. The\nfirst two bits match the label of the switch in the third column. Hence, the message proceeds\nhorizontally, and is finally routed to node 7.\nWe can extend this method for a butterfly consisting of k columns. In the first column, we\ncompare the MSB. Similarly, in the ith column, we compare the ith bit (MSB is the 1st bit).\nNote that the first k \u2212 i bits of the label of the switch that handles the message in the ith\ncolumn are equal to the first k\u2212i bits of the destination switch. This strategy ensure that the\nmessage gets routed to ultimately the correct switch in the kth column.\nNow, let us investigate the properties of the network. Let us assume that we have N\nnodes, where N is a power of 2. We require log(N) columns, where each column contains N\/2\nswitches. Thus, we require an additional Nlog(N)\/2 switches. An astute reader would have\nalready concluded that routing a message in a butterfly network is almost the same as routing\nin a message in a hypercube. In every step we increase the size of the matching prefix between\nthe labels of the source and destination switches by 1. We thus require log(N)+1 steps (1\nadditionalstepforsendingthemessagetothedestinationnodefromthelastswitch)forsending\na message between a pair of nodes. We can thus approximate the diameter of the network to\nlog(N).\nLet us now compute the bisection bandwidth. Let us consider our example with 8 nodes\nfirst. Here, we can split the network horizontally. We thus need to snap 4 links, and hence the\nbisection bandwidth of the network shown in Figure 11.28 is 4. Let us now consider N > 8\nnodes. In this case, also the best solution is to split the network horizontally. This is because\nif we draw an imaginary horizontal line between the (N\/4)th and (N\/4+1)th row of switches\nthen it will only intersect the links between the first and second columns. The outgoing links\nof the rest of the columns will not intersect with our imaginary line. They will either be below\nit or above it. Since each switch in the first column has only one outgoing link that intersects\nthe imaginary line, a total of N\/2 links intersect the imaginary line. All of these links need to\nbe disconnected to divide the network into two equal parts. Hence, the bisection bandwidth is\nequal to N\/2.\nComparison of Topologies\nLet us now compare the topologies with respect to four parameters \u2013 number of internal nodes\n(or switches), number of links, diameter, and bisection bandwidth in Table 11.7. In all the\ncases, we assume that the networks have N nodes that can send and receive messages, and N\nis a power of 2. (cid:13)c Smruti R. Sarangi 594\nTopology # Switches # Links Diameter Bisection Bandwidth\nChain 0 N-1 N-1 1\nRing 0 N N\/2 2\nFat Tree N - 1 N log(N)\u2021 2 log(N) N\/2\u2020\n\u221a \u221a \u221a\nMesh 0 2N \u22122 N 2 N \u22122 N\n\u221a \u221a\nTorus 0 2N N 2 N\n\u221a \u221a\nFolded Torus 0 2N N 2 N\nHypercube 0 Nlog(N)\/2 log(N) N\/2\nButterfly Nlog(N)\/2 N +Nlog(N) log(N)+1 N\/2\n\u2021 Assume that the size of each link is equal to the size of the subtree under it.\n\u2020 Assume that the capacity of each link is equal to the number of leaves in its subtree\nTable 11.7: Comparison of topologies\n11.7 Summary and Further Reading\n11.7.1 Summary\nSummary 11\n1. Processor frequency and performance is beginning to saturate.\n2. Concomitantly, the number of transistors per chip is roughly doubling very two years\nas per the original predictions of Gordon Moore. This empirical law is known as the\nMoore\u2019s Law.\n3. The additional transistors are not being utilised to make a processor larger, or more\ncomplicated. They are instead being used to add more processors on chip. Each such\nprocessor is known as a core, and a chip with multiple cores is known as a multicore\nprocessor.\n4. We can have multiprocessor systems where the processors are connected over the net-\nwork. In this case the processors do not share any resources between them, and such\nmultiprocessors are known as loosely coupled multiprocessors. In comparison, multi-\ncore processors, and most small sized server processors that have multiple processors\non the same motherboard, share resources such as the I\/O devices, and the main\nmemory. Programs running on multiple processors in these systems might also share\na part of their virtual address space. These systems are thus referred to as strongly\ncoupled multiprocessors.\n5. Multiprocessorsystemscanbeusedtorunmultiplesequentialprogramssimultaneously,\nor can be used to run parallel programs. A parallel program contains many sub-\nprograms that run concurrently. The sub-programs co-operate among themselves to 595 (cid:13)c Smruti R. Sarangi\nachieve a bigger task. When sub-programs share their virtual memory space, they are\nknown as threads.\n6. Parallel programs running on strongly coupled multiprocessors typically communicate\nvalues between themselves by writing to a shared memory space. In comparison, pro-\ngrams running on loosely coupled multiprocessors communicate by passing messages\nbetween each other.\n7. Most parallel programs have a sequential section, and a parallel section. The parallel\nsection can be divided into smaller units and distributed among the processors of a\nmultiprocessor system. If we have N processors, then we ideally expect the parallel\nsection to be sped up by a factor of N. An equation describing this relationship is\nknown as the Amdahl\u2019s Law. The speedup, S is given by:\nT 1\nseq\nS = =\nT par f seq + 1\u2212 Pfseq\n8. The Flynn\u2019s taxonomy classifies computing systems into four types : SISD (single\ninstruction, single data), SIMD (single instruction, multiple data), MISD (multiple\ninstruction, single data), and MIMD (multiple instruction, multiple data).\n9. The memory system in modern shared memory MIMD processors is in reality very\ncomplex. Coherence and consistency are two important aspects of the behaviour of the\nmemory system.\n10. Coherence refers to the rules that need to be followed for accessing the same memory\nlocation. Coherence dictates that a write is never lost, and all writes to the same\nlocation are seen in the same order by all the processors.\n11. Consistency refers to the behaviour of the memory system with respect to different\nmemory locations. If memory accesses from the same thread get reordered by the\nmemory system (as is the case with modern processors), many counter intuitive be-\nhaviours as possible. Hence, most of the time we reason in terms of the sequentially\nconsistent memory model that prohibits reordering of messages to the memory system\nfrom the same thread. In practice, multiprocessors follow a weak consistency model\nthat allows arbitrary reorderings. We can still write correct programs because such\nmodels define synchronisation instructions (example: fence) that try to enforce an\nordering between memory accesses when required.\n12. We can either have a large shared cache, or multiple private caches (one for each core\nor set of cores). Shared caches can be made more performance and power efficient\nby dividing it into a set of subcaches known as banks. For a set of private caches to\nlogically function as one large shared cache, we need to implement cache coherence.\n(a) The snoopy cache coherence protocol connects all the processors to a shared bus.\n(b) The MSI write-update protocol works by broadcasting every write to all the cores. (cid:13)c Smruti R. Sarangi 596\n(c) The MSI write-invalidate protocol guarantees coherence by ensuring that only\none cache can write to a block at any single point of time.\n13. To further improve performance, we can implement a multithreaded processor that\nshares a pipeline across many threads. We can either quickly switch between threads\n(fine and coarse-grained multithreading), or execute instructions from multiple threads\nin the same cycle using a multi-issue processor (simultaneous multithreading).\n14. SIMD processors follow a different approach. They operate on arrays of data at once.\nVector processors have a SIMD instruction set. Even though, they are obsolete now,\nmost modern processors have vector instructions in their ISA.\n(a) Vector arithmetic\/logical instructions fetch their operands from the vector regis-\nter file, and operate on large vectors of data at once.\n(b) Vector load-store operations can either assume that data is stored in contiguous\nmemory regions, or assume that data is scattered in memory.\n(c) Instructions on a branch path are implemented as predicated instructions in vec-\ntor ISAs.\n15. Processors and memory elements are connected through an interconnection network.\nThe basic properties of an interconnection network are the diameter (worst case end\nto end delay), and the bisection bandwidth (number of links that need to be snapped to\npartition the network equally). We discussed several topologies: chain, ring, fat tree,\nmesh, torus, folded torus, hypercube, and butterfly.\n11.7.2 Further Reading\nThe reader should start by reading the relevant sections in advanced textbooks on computer\narchitecture [Culler et al., 1998, Hwang, 2003, Baer, 2010, Jacob, 2009]. For parallel pro-\ngramming, the reader can start with Michael Quinn\u2019s book on parallel programming with\nOpenMP and MPI [Quinn, 2003]. The formal MPI specifications are available at http:\n\/\/www.mpi-forum.org. For an advanced study of cache coherence the reader can start with\nthe survey on coherence protocols by Stenstrom [Stenstrom, 1990], and then look at one of\nthe earliest practical implementations [Borrill, 1987]. The most popular reference for memory\nconsistencymodelsisatutorialbyAdveandGharachorloo[AdveandGharachorloo, 1996], and\na paper published by the same authors [Gharachorloo et al., 1992]. For a different perspective\non memory consistency models in terms of ordering, and atomicity, readers can refer to [Arvind\nand Maessen, 2006]. [Guiady et al., 1999] looks at memory models from the point of view\nof performance. [Peterson et al., 1991] and [russell, 1978] describe two fully functional SIMD\nmachines. For interconnection networks the reader can refer to [Jerger and Peh, 2009]. 597 (cid:13)c Smruti R. Sarangi\nExercises\nOverview of Multiprocessor Systems\nEx. 1 \u2014 Differentiate between strongly coupled, and loosely coupled multiprocessors.\nEx. 2 \u2014 Differentiate between shared memory, and message passing based multiprocessors.\nEx. 3 \u2014 Why is the evolution of multicore processors a direct consequence of Moore\u2019s Law?\nEx. 4 \u2014 The fraction of the potentially parallel section in a program is 0.6. What is the\nmaximum speedup that we can achieve over a single core processor, if we run the program on\na quad-core processor?\nEx. 5 \u2014 You need to run a program, 60% of which is strictly sequential, while the rest 40%\ncan be fully parallelised over a maximum of 4 cores. You have 2 machines:\n(a) A single core machine running at 3.2 GHz\n(b) A 4-core machine running at 2.4 GHz\nWhich machine is better if you have to minimise the total time taken to run the program?\nAssume that the two machines have the same IPC per thread and only differ in the clock\nfrequency and the number of cores.\n* Ex. 6 \u2014 Consider a program, which has a sequential and a parallel portion. The sequential\nportion is 40% and the parallel portion is 60%. Using Amdahl\u2019s law, we can compute the\nspeedup with n processors, as S(n). However, increasing the number of cores increases the cost\nof the entire system. Hence, we define a utility function, g(n), of the form:\ng(n) = e\u2212n\/3(2n2+7n+6)\nThe buyer wishes to maximise S(n)\u00d7g(n). What is the optimal number of processors, n?\nEx. 7 \u2014 Define the terms: SISD, SIMD, MISD, and MIMD. Give an example of each type\nof machine.\nEx. 8 \u2014 What are the two classes of MIMD machines introduced in this book?\nCoherence and Consistency\nEx. 9 \u2014 What are the axioms of cache coherence?\nEx. 10 \u2014 Define sequential and weak consistency.\nEx. 11 \u2014 Is the outcome (t1,t2) = (2,1) allowed in a system with coherent memory? (cid:13)c Smruti R. Sarangi 598\nThread 1: Thread 2:\nx = 1; t1 = x;\nx = 2; t2 = x;\nEx. 12 \u2014 Assume that all the global variables are initialised to 0, and all variables local to\na thread start with \u2018t\u2019. What are the possible values of t1 for a sequentially consistent system,\nand a weakly consistent system? (source [Adve and Gharachorloo, 1996])\nThread 1: Thread 2:\nx = 1; while(y == 0){}\ny = 1; t1 = x;\nEx. 13 \u2014 Is the outcome (t1,t2) = (1,1) possible in a sequentially consistent system?\nThread 1: Thread 1:\nx = 1; y = 1;\nif(y == 0) if(x == 0)\nt1 = 1; t2 = 1;\nEx. 14 \u2014 Is the outcome t1 (cid:54)= t2 possible in a sequentially consistent system? (source [Adve\nand Gharachorloo, 1996])\nThread 3: Thread 4:\nThread 1: Thread 2:\nwhile (x != 1) {} while (x != 1) {}\nz = 1; z = 2;\nwhile (y != 1) {} while (y != 1) {}\nx = 1; y = 1;\nt1 = z; t2 = z;\n* Ex. 15 \u2014 Is the outcome (t1 = 0) allowed in a system with coherent memory and atomic\nwrites? Consider both sequential and weak consistency?\nThread 2: Thread 3:\nThread 1:\nwhile(x != 1) {} while (y != 1) {}\nx = 1;\ny = 1; t1 = x;\n* Ex. 16 \u2014 Considerthefollowingcodesnippetforimplementingacritical section. Acritical\nsection is a region of code that can only be executed by one thread at any single point of time.\nAssume that we have two threads with ids 0 and 1 respectively. The function getTid() returns\nthe id of the current thread.\nvoid enterCriticalSection() {\ntid = getTid();\notherTid = 1 - tid;\ninterested[tid] = true;\nflag = tid;\nwhile ( (flag == tid) && (interested[otherTid] == 1) ) {} 599 (cid:13)c Smruti R. Sarangi\n}\nvoid leaveCriticalSection{\ntid = getTid();\ninterested[tid] = false;\n}\nIs it possible for two threads to be in the critical section at the same point of time?\nEx. 17 \u2014 In the snoopy protocol, why do we write back data to the main memory upon a\nM to S transition?\nEx. 18 \u2014 Assume that two nodes desire to transition from the S state to the M state at\nexactly the same point of time. How will the snoopy protocol ensure that only one of these\nnodes enters the M state, and finishes its write operation? What happens to the other node?\nEx. 19 \u2014 The snoopy protocol clearly has an issue with scalability. If we have 64 cores with\na private cache per core, then it will take a long time to broadcast a message to all the caches.\nCan you propose solutions to circumvent this problem?\nEx. 20 \u2014 Let us assume a cache coherent multiprocessor system. The L1 cache is private\nand the coherent L2 cache is shared across the processors. Let us assume that the system issues\na lot of I\/O requests. Most of the I\/O requests perform DMA (Direct Memory Access) from\nmain memory. It is possible that the I\/O requests might overwrite some data that is already\npresent in the caches. In this case we need to extend the cache coherence protocol that also\ntakes I\/O accesses into account. Propose one such protocol.\nEx. 21 \u2014 Let us define a new state in the traditional MSI states based snoopy protocol.\nThe new E state refers to the \u201cexclusive\u201d state, in which a processor is sure that no other cache\ncontains the block in a valid state. Secondly, in the E state, the processor hasn\u2019t modified the\nblock yet. What is the advantage of having the E state? How are evictions handled in the E\nstate?\nEx. 22 \u2014 Show the state transition diagrams for a MSI protocol with a directory. You need\nto show the following:\n1.Structure of the directory\n2.State transition diagram for events received from the host processor.\n3.State transition diagram for events received from the directory.\n4.State transition diagram for an entry in the directory (if required).\nEx. 23 \u2014 Assume that we have a system with private L1 and L2 caches. The L1 layer is\nnot coherent. However, the L2 layer maintains cache coherence. How do we modify our MSI\nsnoopy protocol to support cache coherence for the entire system?\nEx. 24 \u2014 In the snoopy write-invalidate protocol, when should a processor actually perform\nthe write operation? Should it perform the write as soon as possible, or should it wait for the\nwrite-invalidate message to reach all the caches? Explain your answer. (cid:13)c Smruti R. Sarangi 600\n* Ex. 25 \u2014 Assumethataprocessorwantstoperformanatomicexchangeoperationbetween\ntwo memory locations a and b. a and b cannot be allocated to registers. How will you modify\nthe MSI coherence protocol to support this operation? Before proceeding with the answer\nthink about what are the things that can go wrong. An exchange is essentially equivalent to\nthe following sequence of operations: (1) temp = a; (2) a = b; (3) b = temp. If a read arrives\nbetween operations (2) and (3) it might get the wrong value of b. We need to prevent this\nsituation.\n** Ex. 26 \u2014 Assume that we want to implement an instruction called MCAS. The MCAS\ninstruction takes k (known and bounded) memory locations as arguments, a set of k old values,\nand a set of k new values. Its pseudo-code is shown below. We assume here that mem is a\nhypothetical array representing the entire memory space.\n\/* multiple compare and set *\/\nboolean MCAS(int memLocs[], int oldValues[], int newValues[]){\n\/* compare *\/\nfor(i=0; i < k; i++) {\nif(mem[memLocs[i]] != oldValues[i]) {\nreturn false;\n}\n}\n\/* set *\/\nfor(i=0; i < k; i++) {\nmem[memLocs[i]] = newValues[i];\n}\nreturn true;\n}\nThe challenge is to implement this instruction such that it appears to execute instantaneously.\nLet us look at some subtle cases. Assume that we want to write (4,5,6) to three memory\nlocations if their previous contents are (1,2,3). It is possible that after writing 4, and 5, there is\nasmalldelay. Duringthistimeanotherthreadreadsthethreememorylocations, andconcludes\nthat their values are 4,5, and 3 respectively. This result is incorrect because it violates our\nassumption that MCAS executes instantaneously. We should either read (1,2,3) or (4,5,6).\nNow, let us look at the case of reading the three memory locations. Let us say that their initial\nvalues are 1,2, and 0. Our MCAS instruction reads the first two locations and since they are\nequal to the old values, proceeds to the third location. Before reading it, a store operation from\nanother thread changes the values of the three locations as follows. (1,2,0) \u2192 (5,2,0) \u2192 (5,2,3).\nSubsequently, the MCAS instruction takes a look at the third memory location and finds it to\nbe 3. Note that the three memory locations were never equal to (1,2,3). We thus arrive at a\nwrong conclusion.\nHow should we fix these problems? We want to implement a MCAS instruction purely in\nhardware, which provides an illusion of instantaneous execution. It should be free of deadlocks,\nand should complete in a finite amount of time. How can we extend our coherence protocols to\nimplement it? 601 (cid:13)c Smruti R. Sarangi\n*** Ex. 27 \u2014 Assume a processor that has a sequentially consistent(SC) memory. We im-\nplement SC by making each thread wait for a memory request to complete before issuing the\nnext request. Now, assume that we modify the architecture by allowing a processor to read a\nvaluethattheimmediatelyprecedinginstructionhaswrittenwithoutwaitingforittocomplete.\nProve that the memory system still follows SC.\n*** Ex. 28 \u2014 Assume a processor with a weak consistency model. Let us run a \u201cproperly\nlabelled\u201d program on it. A properly labelled(PL) program does not allow conflicting accesses\n(read-write, write-read, or write-write) to a shared variable at the same time. For example, the\nfollowing code sequence is not properly labelled because it allows x to be modified concurrently.\nThread 1: Thread 2:\nx = 0 x = 1\nIn reality, the coherence protocol orders one write access before the other. Nevertheless, both\nthe threads try to modify x concurrently at the programmer\u2019s level. This is precisely the\nbehaviour that we wish to avoid.\nIn a PL program, two threads do not try to modify x at the same time. This is achieved by\nhaving two magic instructions known as lock and unlock. Only one thread can lock a memory\nlocation at any point of time. If another thread tries to lock the location before it is unlocked,\nthen it stalls till the lock is free. If multiple threads are waiting on the same lock, only one\nof them is given the lock after an unlock instruction. Secondly, both the lock and unlock\ninstructions have a built in fence operation, and all the lock and unlock instructions execute\nin program order. The PL version of our program is as follows:\nThread 1: Thread 2:\nlock(x) lock(x)\nx = 0 x = 1\nunlock(x) unlock(x)\nWe can thus think of a lock-unlock block as a sequential block that can only be executed by one\nthread at a given time. Moreover, assume that a lock-unlock block can only have one memory\ninstruction inside it.\nNow, prove that all PL programs running on a weakly consistent machine have a sequentially\nconsistent execution. In other words we can interleave the memory accesses of all the threads\nsuch that they appear to be executed by a single cycle processor that switches among the\nthreads. [HINT: Construct access graphs for your system, and prove that they are acyclic.]\nMultithreading\nEx. 29 \u2014 What is the difference between a fine grained and coarse grained multithreaded\nmachine?\nEx. 30 \u2014 Describe a simultaneous multithreaded (SMT) processor in detail.\nEx. 31 \u2014 Describes the steps that we need to take to ensure that a SMT processor executes\ncorrectly.\nEx. 32 \u2014 Assume a mix of workloads in a 4-way SMT processor. 2 threads are computa- (cid:13)c Smruti R. Sarangi 602\ntionally intensive, 1 thread is I\/O intensive, and the last thread sleeps for a long time. Design\nan efficient instruction selection scheme.\nInterconnection Networks\nEx. 33 \u2014 What is the bisection bandwidth and diameter of a 2D n\u00d7n mesh?\nEx. 34 \u2014 What is the bisection bandwidth and diameter of a 3D n\u00d7n\u00d7n mesh?\nEx. 35 \u2014 What is the diameter of a ring containing n nodes? Give a precise answer that\nholds for even and odd n.\nEx. 36 \u2014 What is the bisection bandwidth and diameter of a hypercube of order n.\nEx. 37 \u2014 What is the bisection bandwidth and diameter of a n\u00d7n\u00d7n, 3D torus?\nEx. 38 \u2014 What is the bisection bandwidth and diameter of a clique of n nodes (n is even)?\nIn a clique, all pairs of nodes are connected.\n** Ex. 39 \u2014 Assume we have a n \u00d7 n mesh. There are n2 routers, and each processor is\nconnected to one router. Note that at any point of time, a router can only store 1 message. It\nwilldiscardamessageonlyifthemessagegetsstoredinanotherrouter. Inourpreviousexample,\nrouter (i,j) will keep the message until it has been delivered and stored at a neighbouring\nrouter such as (i+1,j). Now, an interesting deadlock situation can develop. Let us assume the\nfollowing scenario.\n\u2022(1,1) wants to send a message to (1,2).\n\u2022(1,2) wants to send a message to (2,2).\n\u2022(2,2) wants to send a message to (2,1).\n\u2022(2,1) wants to send a message to (1,1).\nIn this case all the four nodes have 1 message each. They are not able to forward the packet\nto the next node, because the next node already stores a packet, and is thus busy. Since there\nis a cyclic wait, we have a deadlock. Design a message routing protocol between a source and\ndestination node that is provably deadlock free.\nVector Processors\nEx. 40 \u2014 What is the advantage of vector processors over scalar processors?\nEx. 41 \u2014 Why are vector load-store instructions easy to implement in systems that have\ncaches with large block sizes?\nEx. 42 \u2014 How can we efficiently implement a scatter-gather based load-store unit?\nEx. 43 \u2014 Whatisapredicatedinstruction,andhowdoesithelpspeedupavectorprocessor?\n* Ex. 44 \u2014 Assume that we have a processor with a 32 entry vector register file. We wish\nto add two arrays that have 17 entries each. How can we implement this operation, with the 603 (cid:13)c Smruti R. Sarangi\nSimpleRisc vector instructions introduced in the chapter? Feel free to introduce new vector\ninstructions if required.\n* Ex. 45 \u2014 Design a dedicated SIMD hardware unit to sort n integers in roughly n time\nsteps by using the bubble sort algorithm. You have a linear array of n processors connected\nend to end. Each processor is capable of storing two integers, and has some logic inside it.\nDesign the logic for each processor and explain the overall working of the system.\nDesign Problems\nEx. 46 \u2014 Write a program to sort a billion integers using OpenMP and MPI.\nEx. 47 \u2014 Implement a distributed shared memory system on a cluster of computers con-\nnected via an Ethernet LAN. (cid:13)c Smruti R. Sarangi 604 12\nI\/O and Storage Devices\nWe have now arrived at a very interesting point in our study of processors. We have learnt\nhow to design a full processor and its accompanying memory system using basic transistors.\nThis processor can execute the entire SimpleRisc instruction set, and can run very complicated\nprograms ranging from chess games to weather simulations. However, there is a vital aspect\nmissing in our design. There is no way for us to communicate with our computer. To render\nour computer usable, we need to have a method to write input data, and display the outputs.\nMemory Hard disk\nComputer\nKeyboard Monitor\nMouse Printer\nFigure 12.1: A typical computer system\nWe thus need an I\/O (Input\/Ouput) system in our computer. Let us look at the structure\nof a typical computer in Figure 12.1. The processor is the core of the computer. It is connected\nto a host of I\/O devices for processing user inputs, and for displaying results. These I\/O\ndevices are known as peripherals. The most common user input devices are the keyboard and\nthe mouse. Likewise, the most common display devices, are the monitor, and the printer. The\ncomputer can also communicate with a host of other devices such as cameras, scanners, mp3\nplayers, camcorders, microphones, and speakers through a set of generic I\/O ports. An I\/O\nport consists of: (1) a set of metallic pins that help the processor to connect with external\n605 (cid:13)c Smruti R. Sarangi 606\ndevices, and (2) a port controller that manages the connection with the peripheral device. The\ncomputer can also communicate with the outside world through a special peripheral device\ncalled a network card. The network card contains the circuits to communicate with other\ncomputers via wired or wireless connections.\nDefinition 140\nAn I\/O port consists of a set of metallic pins that are used to attach to connectors provided\nby external devices. Every port is associated with a port controller that co-ordinates the\nexchange of data on the communication link.\nWe give a special preference to a particular class of devices in this chapter known as storage\ndevices. Storage devices such as the hard disk, and flash drives help us permanently store data\neven when the system is powered off. We looked at them briefly in Chapter 10 while discussing\nswap space. In this chapter, we shall study them in more detail, and look at the methods of\ndata storage, and retrieval. The reason we stress on storage devices in this chapter is because\nthey are integral to computer architecture. The nature of peripherals across computers varies.\nFor example, a given computer might have a microphone, whereas another computer might\nnot have a monitor because it is accessed remotely over the network. However, invariably all\ncomputers from small handheld phones to large servers have some form of permanent storage.\nThis storage is used to save files, system configuration data, and the swap space during the\noperation of a program. Hence, architects pay special attention to the design an optimisation\nof storage systems, and no book in computer architecture is complete without discussing this\nvital aspect of computer architecture.\n12.1 I\/O System \u2013 Overview\n12.1.1 Overview\nLet us now distance ourselves from the exact details of an I\/O device. While designing a\ncomputer system, it is not possible for designers to consider all possible types of I\/O devices.\nEven if they do, it is possible that a new class of devices might come up after the computer has\nbeensold. Forexample, tabletPCssuchastheAppleiPadwerenottherein2005. Nonetheless,\nitisstillpossibletotransferdatabetweenaniPadandolderPCs. Thisispossiblebecausemost\ndesigners provide standard interfaces in their computer system. For example, a typical desktop\nor laptop has a set of USB ports. Any device that is compliant with the USB specification can\nbe connected to the USB port, and can then communicate with the host computer. Similarly, it\nispossibletoattachalmostanymonitororprojectorwithanylaptopcomputer. Thisisbecause\nlaptops have a generic DVI port that can be connected to any monitor. Laptop companies obey\ntheir part of the DVI specification by implementing a DVI port that can seamlessly transfer\ndata between the processor and the port. On similar lines, monitor companies obey their part\noftheDVIspecificationbyensuringthattheirmonitorscanseamlesslydisplayallthedatathat\nis being sent on the DVI port. Thus, we need to ensure that our computer provides support for 607 (cid:13)c Smruti R. Sarangi\na finite set of interfaces with peripherals. It should then be possible to attach any peripheral\nat run time.\nThe reader should note that just because it is possible to attach a generic I\/O device by\nimplementing the specification of a port, it does not mean that the I\/O device will work. For\nexample, we can always connect a printer to the USB port. However, the printer might not\nbe able to print a page. This is because, we need additional support at the software level to\noperatetheprinter. Thissupportisbuiltintotheprinterdevicedriversintheoperatingsystem\nthat can efficiently transfer data from user programs to the printer.\nRoles of Software and Hardware\nWe thus need to clearly differentiate between the roles of software and hardware. Let us first\nlook at software. Most operating systems define a very simple user interface for accessing I\/O\ndevices. For example, the Linux operating system has two system calls, read and write, with\nthe following specifications.\nread(int file_descriptor, void *buffer, int num_bytes)\nwrite(int file_descriptor, void *buffer, int num_bytes)\nLinux treats all devices as files, and allots them a file descriptor. The file descriptor is the\nfirst argument, and it specifies the id of the device. For example, the speaker has a certain file\ndescriptor, and a printer has a different file descriptor. The second argument points to an area\ninmemorythatcontainsthesourceordestinationofthedata, andthelastargumentrepresents\nthe number of bytes that need to be transferred. From the point of view of an user, this is all\nthat needs to be done. It is the job of the operating system\u2019s device drivers, and the hardware\nto co-ordinate the rest of the process. This approach has proved to be an extremely versatile\nmethod for accessing I\/O devices.\nUnfortunately, the operating system needs to do more work. For each I\/O call it needs\nto locate the appropriate device driver and pass on the request. It is possible that multiple\nprocesses might be trying to access the same I\/O device. In this case, the different requests\nneed to be properly scheduled.\nThe job of the device driver is to interface with native hardware and perform the desired\naction. The device driver typically uses assembly instructions to communicate with the hard-\nware device. It first assesses its status, and if it is free, then it asks the peripheral device to\nperform the desired action. The device driver initiates the process of transfer of data between\nthe memory system and the peripheral device.\nFigure 12.2 encapsulates the discussion up till now. The upper part of the diagram shows\nthe software modules (application, operating system, device driver), and the lower part of the\ndiagram shows the hardware modules. The device driver uses I\/O instructions to communicate\nwith the processor, and the processor then routes the commands to the appropriate I\/O device.\nWhen the I\/O device has some data to send to the processor, it sends an interrupt, and then\nthe interrupt service routine reads the data, and passes it on to the application.\nWe summarised the entire I\/O process in just one paragraph. However, the reader should\nnote that this is an extremely complicated process, and entire books are devoted to the study\nand design of device drivers. In this book, we shall limit our discussion to the hardware\npart of the I\/O system, and take a cursory look at the software support that is required. The (cid:13)c Smruti R. Sarangi 608\nApplication\nRequest\nResponse\nOperating Device\nKernel\nsystem driver\nSoftware\nHardware\nI\/O I\/O\nProcessor\ndevice system\nFigure 12.2: The I\/O system (software and hardware)\nimportantpointsofdifferencebetweenthesoftwareandhardwarecomponentsoftheI\/Osystem\nare enumerated in Point 20.\nImportant Point 20\nThe role of software and hardware in the I\/O system:\n1. The software component of the I\/O system consists of the application, and the operat-\ning system. The application is typically provided a very simple interface to access I\/O\ndevices. The role of the operating system is to collate the I\/O requests from differ-\nent applications, appropriately schedule them, and pass them on to the corresponding\ndevice drivers.\n2. The device drivers communicate with the hardware device through special assembly\ninstructions. They co-ordinate the transfer of data, control, and status information\nbetween the processor and the I\/O devices.\n3. The role of the hardware (processor, and associated circuitry) is to just act as a\nmessenger between the operating system, and the I\/O devices, which are connected to\ndedicated I\/O ports. For example, if we connect a digital camera to an USB port,\nthen the processor is unaware of the details of the connected device. Its only role is\nto ensure seamless communication between the device driver of the camera, and the\nUSB port that is connected to the camera.\n4. It is possible for the I\/O device to initiate communication with the processor by send-\ning an interrupt. This interrupts the currently executing program, and invokes the\ninterrupt service routine. The interrupt service routine passes on control to the cor-\nresponding device driver. The device driver then processes the interrupt, and takes\nappropriate action.\nLet us now discuss the architecture of the hardware component of the I\/O system in detail. 609 (cid:13)c Smruti R. Sarangi\n12.1.2 Requirements of the I\/O System\nDevice Bus Technology Bandwidth Typical Values\nVideo display via graphics card PCI Express (version 4) High 1-10 GB\/s\nHard disks ATA\/SCSI\/SAS Medium 150-600 MB\/s\nNetwork card (wired\/wireless) PCI Express Medium 10-100 MB\/s\nUSB devices USB Medium 60-625 MB\/s\nDVD audio\/video PCI Medium 1-4 MB\/s\nSpeaker\/Microphone AC\u201997\/Intel High. Def. Audio Low 100 KB\/s to 3 MB\/s\nKeyboard\/Mouse USB\/PCI Very Low 10-100 B\/s\nTable 12.1: List of I\/O devices along with bandwidth requirements (as of 2012)\nLet us now try to design the architecture of the I\/O system. Let us start out by listing out\nall the devices that we want to support, and their bandwidth requirements in Table 12.1. The\ncomponent that requires the maximum amount of bandwidth is the display device (monitor,\nprojector, TV). It is attached to a graphics card. The graphics card contains the graphics\nprocessor that processes image and video data.\nFigure 12.3: Photograph of a network card This article uses material from the Wikipedia\narticle \u201cNetwork Interface Controller\u201d [nic, ], which is released under the Creative Commons\nAttribution-Share-Alike License 3.0 [ccl, ]\nNote that we shall use the term card often in the discussion of I\/O devices. A card is\na printed circuit board(PCB), which can be attached to the I\/O system of a computer to (cid:13)c Smruti R. Sarangi 610\nimplement a specific functionality. For example, a graphics card helps us process images and\nvideos, asoundcardhelpsusprocesshighdefinitionaudio, andanetworkcardhelpsusconnect\ntothenetwork. ThepictureofanetworkcardisshowninFigure12.3. Wecanseeasetofchips\ninterconnected on a printed circuit board. There are a set of ports that are used to connect\nexternal devices to the card.\nDefinition 141\nA card is a printed circuit board(PCB), which can be attached to the I\/O system of a\ncomputer to implement a specific functionality.\nAlong with the graphics card, the other high bandwidth device that needs to be connected\nto the CPU is the main memory. The main memory bandwidth is of the order of 10-20 GB\/s.\nHence, we need to design an I\/O system that gives special treatment to the main memory and\nthe graphics card.\nTherestofthedeviceshavearelativelylowerbandwidth. Thebandwidthrequirementofthe\nharddisk,USBdevices,andthenetworkcardislimitedto500-600MB\/s. Thekeyboard,mouse,\nCD-DVD drives, and audio peripherals have an extremely minimal bandwidth requirement\n(< 3\u22124 MB\/s).\n12.1.3 Design of the I\/O System\nI\/O Buses\nLet us take a look at Table 12.1 again. We notice that there are different kinds of bus tech-\nnologies such as USB, PCI Express, and SATA. Here, a bus is defined as a link between two\nor more than two elements in the I\/O system. We use different kinds of buses for connecting\ndifferent kinds of I\/O devices. For example, we use the USB bus to connect USB devices such\nas pen drives, and cameras. We use SATA or SCSI buses to connect to the hard disk. We need\nto use so many different types of buses for several reasons. The first is that the bandwidth\nrequirements of different I\/O devices are very different. We need to use extremely high speed\nbuses for graphics cards. Whereas, for the keyboard, and mouse, we can use a significantly\nsimpler bus technology because the total bandwidth demand is minimal. The second reason is\nhistorical. Historically, hard disk vendors have used the SATA or IDE buses, whereas graphics\ncard vendors have been using the AGP bus. After 2010, graphics card companies shifted to\nusing the PCI Express bus. Hence, due to a combination of factors, I\/O system designers need\nto support a large variety of buses.\nDefinition 142\nA bus is a set of wires that is used to connect multiple devices in parallel. The devices can\nuse the bus to transmit data and control signals between each other. 611 (cid:13)c Smruti R. Sarangi\nNow, let us look deeper into the structure of a bus. A bus is much more than a set of copper\nwires between two end points. It is actually a very complex structure and its specifications are\ntypically hundreds of pages long. We need to be concerned about its electrical properties, error\ncontrol, transmitter and receiver circuits, speed, power, and bandwidth. We shall have ample\nopportunity to discuss high speed buses in this chapter. Every node (source or destination)\nconnected to a bus requires a bus controller to transmit and receive data. Although the design\nof a bus is fairly complicated, we can abstract it as a logical link that seamlessly and reliably\ntransfers bytes from a single source to a set of destinations.\nThe Chipset and Motherboard\nFor designing the I\/O system of a computer, we need to first provide external I\/O ports that\nconsist of a set of metallic pins or sockets. These I\/O ports can be used to attach external\ndevices. The reader can look at the side of her laptop or the back of her desktop to find the\nset of ports that are supported by her computer. Each port has a dedicated port controller\nthat interfaces with the device, and then the port controller needs to send the data to the CPU\nusing one of the buses listed in Table 12.1.\nHere, the main design issue is that it is not possible to connect the CPU to each and every\nI\/O port through an I\/O bus. There are several reasons for this.\n1. If we connect the CPU to each and every I\/O port, then the CPU needs to have bus\ncontrollers for every single bus type. This will increase the complexity, area, and power\nutilisation of the CPU.\n2. The number of output pins of a CPU is limited. If the CPU is connected to a host of\nI\/O devices, then it requires a lot of extra pins to support all the I\/O buses. Most CPUs\ntypically do not have enough pins to support this functionality.\n3. From a commercial viewpoint, it is a good idea to separate the design of the CPU from\nthe design of the I\/O system. It is best to keep both of them separate. This way, it is\npossible to use the CPU in a large variety of computers.\nHence, most processors are connected to only a single bus, or at most 2 to 3 buses. We need\nto use ancillary chips that connect the processor to a host of different I\/O buses. They need to\naggregate the traffic from I\/O devices, and properly route data generated by the CPU to the\ncorrect I\/O devices and vice versa. These extra chips comprise the chipset of a given processor.\nThe chips of the chipset are interconnected with each other on a printed circuit board known\nas the motherboard.\nDefinition 143\nChipset These are a set of chips that are required by the main CPU to connect to main\nmemory, the I\/O devices, and to perform system management functions.\nMotherboard All the chips in the chipset are connected to each other on a printed circuit\nboard, known as the motherboard. (cid:13)c Smruti R. Sarangi 612\nArchitecture of the Motherboard\nProcessor\nFront side\nbus\nPCI express Memory\nGraphics bus North Bridge modules\nprocessor chip\nPCI express SATA\nNetwork bus South Bridge bus\ncard chip Hard\ndisk\nIntel high\nPCI express\ndef. audio on\nbus\na PCI bus\nUSB USB USB USB Audio\/ Mic\nports\nUSB ports\nFigure 12.4: Architecture of the I\/O system\nMost processors typically have two important chips in their chipset \u2013 North Bridge and\nSouth Bridge \u2013 as shown in Figure 12.4. The CPU is connected to the North Bridge chip using\nthe Front Side Bus (FSB). The North Bridge chip is connected to the DRAM memory modules,\nthegraphicscard, andtheSouthBridgechip. Incomparison, theSouthBridgechipismeantto\nhandle much slower I\/O devices. It is connected to all the USB devices including the keyboard\nand mouse, audio devices, network cards, and the hard disk.\nFor the sake of completeness, let us mention two other common types of buses in computer\nsystems. The first type of bus is called a back side bus, which is used to connect the CPU to\nthe L2 cache. In the early days, processors used off chip L2 caches. They communicated with\nthem through the back side bus. Nowadays, the L2 cache has moved on chip, and consequently\nthe back side bus has also moved on chip. It is typically clocked at the core frequency, and is a\nvery fast bus. The second type of bus is known as a backplane bus. It is used in large computer\nor storage systems that typically have multiple motherboards, and peripheral devices such as\nharddisks. Alltheseentitiesareconnectedinparalleltoasinglebackplanebus. Thebackplane\nbus itself consists of multiple parallel copper wires with a set of connectors that can be used to\nattach devices. 613 (cid:13)c Smruti R. Sarangi\nDefinition 144\nFront side bus A bus that connects the CPU to the memory controller, or the North\nBridge chip in the case of Intel systems.\nBack side bus A bus that connects the CPU to the L2 cache.\nBackplane bus A system wide bus that is attached to multiple motherboards, storage, and\nperipheral devices.\nBoththeNorthBridge, andSouthBridgechipsneedtohavebuscontrollersforallthebuses\nthat they are attached with. Each bus controller co-ordinates the access to its associated bus.\nAfter successfully receiving a packet of data, it sends the packet to the destination (towards the\nCPU,ortheI\/Odevice). Sincethesechipsinterconnectvarioustypesofbuses, andtemporarily\nbuffer data values if the destination bus is busy, they are known as bridges (bridge between\nbuses).\nThe memory controller is a part of the North Bridge chip and implements read\/write re-\nquests to main memory. In the last few years processor vendors have started to move the\nmemory controller into the main CPU chip, and also make it more complicated. Most of the\naugmentationstothememorycontrollerarefocusedonreducingmainmemorypower, reducing\nthe number of refresh cycles, and optimising performance. Starting from the Intel Sandybridge\nprocessor the graphics processor has also moved on chip. The reason for moving things into the\nCPU chip is because (1) we have extra transistor\u2019s available, and (2) on-chip communication\nis much faster than off-chip communication. A lot of embedded processors also integrate large\nparts of the South Bridge chip, and port controllers, along with the CPU in a single chip. This\nhelpsinreducingthesizeofthemotherboard, andallowsmoreefficientcommunicationbetween\nthe I\/O controllers and the CPU. Such kind of systems are known as SOCs (System on Chip).\nDefinition 145\nA system on a chip (SOC) typically packages all the relevant parts of a computing system\ninto one single chip. This includes the main processor, and most of the chips in the I\/O\nsystem.\n12.1.4 Layers in the I\/O System\nMost complex architectures are typically divided into layers such as the architecture of the\ninternet. Onelayerismostlyindependentoftheotherlayer. Hence,wecanchoosetoimplement\nit in any way we want, as long as it adheres to a standard interface. The I\/O architecture of a\nmodern computer is also fairly complicated, and it is necessary to divide its functionality into\ndifferent layers. (cid:13)c Smruti R. Sarangi 614\nWe can broadly divide the functionality of the I\/O system into four different layers. Note\nthat our classification of the functionality of an I\/O system into layers is broadly inspired from\nthe 7 layer OSI model for classifying the functions of wide area networks into layers. We try to\nconform as much as possible to the OSI model such that readers can relate our notion of layers\nto concepts that they would study in a course on networking.\nPhysical Layer Thephysicallayerofabusprimarilydefinestheelectricalspecificationsofthe\nbus. It is divided into two sublayers namely the transmission sublayer and the synchro-\nnisation sublayer. The transmission sublayer defines the specifications for transmitting a\nbit. For example, a bus can be active high (logical 1, if the voltage is high), and another\nbus can be active low (logical 1, if the voltage is zero). Today\u2019s high speed buses use high\nspeed differential signalling. Here, we use two copper wires to transmit a single bit. A\nlogical 0 or 1 is inferred by monitoring the sign of the difference of voltages between the\ntwo wires (similar to the concept of bit lines in SRAM cells). Modern buses extend this\nidea and encode a logical bit using a combination of electrical signals. The synchronisa-\ntion sublayer specifies the timing of signals, and methods to recover the data sent on the\nbus by the receiver.\nData Link Layer Thedatalinklayerisprimarilydesignedtoprocesslogicalbitsthatareread\nby the physical layer. This layer groups sets of bits into frames, performs error checking,\ncontrols the access to the bus, and helps implement I\/O transactions. In specific, it\nensures that at any point of time, only one entity can transmit signals on the bus, and it\nimplements special features to leverage common message patterns.\nNetwork Layer This layer is primarily concerned with the successful transmission of a set of\nframes from the processor to an I\/O device or vice versa through various chips in the chip\nset. We uniquely define the address of an I\/O device, and consider approaches to embed\nthe addresses of I\/O devices in I\/O instructions. Broadly we discuss two approaches \u2013\nI\/O port based addressing, and memory mapped addressing. In the latter case, we treat\naccesses to I\/O devices, as regular accesses to designated memory locations.\nProtocol Layer The top most layer referred to as the protocol layer is concerned with execut-\ningI\/Orequestsendtoend. Thisincludesmethodsforhighlevelcommunicationbetween\nthe processor, and the I\/O devices in terms of the message semantics. For example, I\/O\ndevices can interrupt the processor, or the processor can explicitly request the status\nof each I\/O device. Secondly, for transferring data between the processor and devices,\nwe can either transfer data directly or delegate the responsibility of transferring data to\ndedicated chips in the chipset known as DMA controllers.\nFigure 12.5 summaries the 4 layered I\/O architecture of a typical processor.\n12.2 Physical Layer \u2013 Transmission Sublayer\nThe physical layer is the lower most layer of the I\/O system. This layer is concerned with the\nphysical transmission of signals between the source and receiver. Let us divide the physical\nlayer into two sublayers. Let us call the first sublayer as the transmission sublayer because it 615 (cid:13)c Smruti R. Sarangi\nProtocol layer\nNetwork layer\nData link layer\nTransmission Synchronisation\nPhysical layer\nFigure 12.5: The 4 layers of the I\/O system\ndeals with the transmission of bits from the source to the destination. This layer is concerned\nwith the electrical properties of the links (voltage, resistance, capacitance), and the methods of\nrepresenting a logical bit (0 or 1) using electrical signals.\nLet us refer to the second sublayer as the synchronisation sublayer. This sublayer is con-\ncerned with reading an entire frame of bits from the physical link. Here, a frame is defined\nas a group of bits demarcated by special markers. Since I\/O channels are plagued with jitter\n(unpredictable signal propagation time), it is necessary to properly synchronise the arrival of\ndata at the receiver, and read each frame correctly.\nIn this section, we shall discuss the transmission sublayer. We shall discuss the synchroni-\nsation sublayer in the next section.\nNote that the reason that we create multiple sublayers, instead of creating multiple layers\nis because sublayers need not be independent of each other. However, in general layers should\nbe independent of each other. It should be theoretically possible to use any physical layer, with\nany other data link layer protocol. They should ideally be completely oblivious of each other.\nIn this case, the transmission and synchronisation sublayers have strong linkages, and thus it\nis not possible to separate them into separate layers.\nI\/O Link\nTransmitter Receiver\nFigure 12.6: A generic view of an I\/O link\nFigure12.6showsthegenericviewofanI\/Olink. Thesource(transmitter)sendsasequence\nofbitstothedestination(receiver). Atthetimeoftransmission,thedataisalwayssynchronised\nwith respect to the clock of the source. This means that if the source runs at 1 GHz, then it (cid:13)c Smruti R. Sarangi 616\nsends bits at the rate of 1 GHz. Note that the frequency of the source is not necessarily equal\nto the frequency of the processor, or I\/O element that is sending the data. The transmission\ncircuitry, is typically a separate submodule, which has a clock that is derived from the clock of\nthe module that it is a part of. For example, the transmission circuitry of a processor might\nbe transmitting data at 500 MHz, whereas the processor might be running at 4 GHz. In any\ncase, we assume that the transmitter transmits data at its internal clock rate. This clock rate\nis also known as the frequency of the bus, or bus frequency , and this frequency is in general\nlower than the clock frequency of the processors, or other chips in the chipset. The receiver can\nrun at the same frequency, or can use a faster frequency. Unless explicitly stated, we do not\nassume that the source and destination have the same frequency. Lastly, note that we shall use\nthe terms sender, source, and transmitter interchangeably. Likewise, we shall use the terms\ndestination, and receiver interchangeably.\n12.2.1 Single Ended Signalling\nLet us consider a naive approach, where we send a sequence of 1s and 0s, by sending a sequence\nof pulses from the source to the destination. This method of signalling is known as single ended\nsignalling, and it is the simplest approach.\nIn specific, we can associate a high voltage pulse with a 1, and a low voltage pulse with a\n0. This convention is known as active high. Alternatively, we can associate a low voltage pulse\nwith a logical 1, and a high voltage pulse with a logical 0. Conversely, this convention is known\nas active low. Both of these conventions are shown in Figure 12.7.\nSadly,bothofthesemethodsareextremelyslowandoutdated. Recallfromourdiscussionof\nSRAM cells in Section 6.4.1 that a fast I\/O bus needs to reduce the voltage difference between\na logical 0, and 1 to as low a value as possible. This is because the voltage difference is detected\nafter it has charged the detector that has an internal capacitance. The higher the voltage\nrequired, the longer it takes to charge the capacitors. If the voltage difference is 1 Volt, then\nit will take a long time to detect a transition from 0 to 1. This will limit the speed of the bus.\nHowever, if the voltage difference is 30 mV, then we can detect the transition in voltage much\nsooner, and we can thus increase the speed of the bus.\nHence, modern bus technologies try to minimise the voltage difference between a logical 0\nand1toaslowavalueaspossible. Notethatwecannotarbitrarilyreducethevoltagedifference\nbetween a logical 0 and 1, in the quest for increasing bus speed. For example, we cannot make\nthe required voltage difference 0.001 mV. This is because there is a certain amount of electrical\nnoise in the system that is introduced due to several factors. Readers might have noticed that\nif a cell phone starts ringing when the speakers of a car or computer are on, then there is some\namountofnoiseinthespeakersalso. Ifwetakeacellphoneclosetoamicrowaveovenwhileitis\nrunning,thenthereisadecreaseinthesoundqualityofthecellphone. Thishappensbecauseof\nelectromagnetic interference. Likewise there can be electromagnetic interference in processors\nalso, and voltage spikes can be introduced. Let us assume that the maximum amplitude of such\nvoltage spikes is 20 mV. Then the voltage difference between a 0 and 1, needs to be more than\n20 mV. Otherwise, a voltage spike due to interference can flip the value of a signal leading to\nan error. Let us take a brief look at one of the most common technologies for on-chip signalling\nnamely LVDS. 617 (cid:13)c Smruti R. Sarangi\nClock\n0 1 1 1 0 1 0 1 0 0 0\nData\n(a) Active high signalling\nClock\n0 1 1 1 0 1 0 1 0 0 0\nData\n(b) Active low signalling\nFigure 12.7: Active High and Active Low Signalling Methods\n12.2.2 Low Voltage Differential Signalling (LVDS)\nLVDS uses two wires to transmit a single signal. The difference between the voltages of these\nwires is monitored. The value transferred is inferred from the sign of the voltage difference.\nThe basic LVDS circuit is shown in Figure 12.8. There is a fixed current source of 3.5 mA.\nDepending on the value of the input A, the current flows to the destination through either line\n1 or line 2. For example, if A is 1, then the current flows through line 1 since transistor T1\nstarts conducting, whereas T is off. In this case, the current reaches the destination, passes\n2\nthrough the resistor R , and then flows back through line 2. Typically the voltage of both\nd\nthe lines when no current is flowing is maintained at 1.2 V. When current flows through them,\nthere is a voltage swing. The voltage swing is equal to 3.5 mA times R . R is typically 100\nd d\nOhms. Hence, the total differential voltage swing is 350 mV. The role of the detector is to sense\nthe sign of the voltage difference. If it is positive, it can declare a logical 1. Otherwise, it can\ndeclare a logical 0. Because of the low swing voltage (350 mV), LVDS is a very fast physical\nlayer protocol. (cid:13)c Smruti R. Sarangi 618\nV\ncc\n3.5mA\nOp amp\nA T2 T1 A\nline 1 +\n100\u03a9 R d\nline 2 -\nA A\nFigure 12.8: LVDS Circuit\n12.2.3 Transmission of Multiple Bits\nLet us now consider the problem of transmitting multiple bits in sequence. Most I\/O channels\nare not busy all the time. They are busy only when data is being transmitted, and thus their\nduty cycle (percentage of time that a device is in operation) tends to be highly variable, and\nmost of the time it is not very high. However, detectors are on almost all the time and they\nkeepsensingthevoltageofthebus. Thiscanhaveimplicationsonbothpowerconsumptionand\ncorrectness. Power is an issue because the detectors keep sensing either a logical 1 or 0 every\ncycle, and it thus becomes necessary for the higher level layers to process the data. To avoid\nthis, most systems typically have an additional line that indicates if the data bits are valid or\ninvalid. This line is traditionally known as the strobe. The sender can indicate the period of the\nvalidity of data to the receiver by setting the value of the strobe. Again, it becomes necessary\nto synchronise the data lines and the strobe. This is getting increasingly difficult for high speed\nI\/O buses, because it is possible that signals on the data lines, and the strobe can suffer from\ndifferent amounts of delay. Hence, there is a possibility that both the lines might move out of\nsynchronisation. It is thus a better idea to define three types of signals \u2013 zero, one, and idle.\nZero and one refer to the transmission of a logical 0 and 1 on the bus. However, the idle state\nrefers to the fact that no signal is being transmitted. This mode of signalling is also known as\nternary signalling because we are using three states.\nDefinition 146\nTernarysignallingreferstoaconventionthatusesthreestatesforthetransmissionofsignals\n\u2013 one (logical one), zero (logical zero), and idle (no signal).\nWe can easily implement ternary signalling with LVDS. Let us refer to the wires in LVDS, 619 (cid:13)c Smruti R. Sarangi\nasAandB respectively. LetV bethevoltageoflineA. Likewise, letusdefinetheterm, V . If\nA B\n| V \u2212V |< \u03c4, where \u03c4 is the detection threshold, then we infer that the lines are idle, and we\nA B\nare not transmitting anything. However, if V \u2212V > \u03c4, we conclude that we are transmitting\nA B\na logical 1. Similarly, if V \u2212V > \u03c4, we conclude that we are transmitting a logical 0. We\nB A\nthus do not need to make any changes to our basic LVDS protocol.\nLet us now describe a set of techniques that are optimised for transmitting multiple bits in\nthe physical layer. We present examples that use ternary signalling. Some of the protocols can\nalso be used with simple binary signalling (zero and one state) also.\n12.2.4 Return to Zero (RZ) Protocols\nIn this protocol we transmit a pulse (positive or negative), and then pause for a while in a\nbit period. Here, we define the bit period as the time it takes to transmit a bit. Most I\/O\nprotocols assume that the bit period is independent of the value of the bit (0 or 1) that is\nbeing transmitted. Typically, a 1-bit period is equal to the length of one I\/O clock cycle.\nThe I\/O clock is a dedicated clock that is used by the elements of the I\/O system. We shall\ninterchangeably use the terms clock cycle, and bit period, where we do not wish to emphasise a\ndifference between the terms.\nDefinition 147\nbit period The time it takes to transfer a single bit over a link.\nI\/O clock We assume that there is a dedicated I\/O clock in our system that is typically\nsynchronised with the processor clock. The I\/O clock is slower than the processor\nclock, and is used by elements of the I\/O subsystem.\nIn the RZ protocol, if we wish to transmit a logical 1, then we send a positive voltage pulse\non the link for a fraction of a bit period. Subsequently, we stop transmitting the pulse, and\nensure that the voltage on the link returns to the idle state. Similarly, while transmitting a\nlogical0, wesendanegativevoltagepulsealongthelinesforafractionofacycle. Subsequently,\nwe wait till the line returns to the idle state. This can be done by allowing the capacitors to\ndischarge, or by applying a reverse voltage to bring the lines to the idle state. In any case, the\nkey point here is that while we are transmitting, we transmit the actual value for some part\nof the bit period, and then we allow the lines to fall back to the default state, which in our\ndiscussion we have assumed to be the idle state. We shall see that returning to the idle state\nhelps the receiver circuitry synchronise with the clock of the sender, and thus read the data\ncorrectly. The implicit assumption here is that the sender sends out one bit every cycle (sender\ncycle). Note that the clock period of the sender and the receiver may be different. We shall\ntake a look at such timing issues in Section 12.4.\nFigure 12.9 shows an example of the RZ protocol with ternary signalling. If we were to use\nbinary signalling, then we can have an alternative scheme as follows. We could transmit a short\npulse in a cycle for a logical 1, and not transmit anything for a logical 0. Here, the main issue (cid:13)c Smruti R. Sarangi 620\nClock\n0 1 1 1 0 1 0 1 0 0 0\nData\nFigure 12.9: Return to zero (RZ) protocol (example)\nis to figure out if a logical 0 is being sent or not by taking a look at the length of the pause\nafter transmitting a logical 1. This requires complicated circuitry at the end of the receiver.\nNevertheless, a major criticism of the RZ (return to zero) approaches is that it wastes\nbandwidth. We need to introduce a short pause (period of idleness) after transmitting a logical\n0 or 1. It turns out that we can design protocols that do not have this limitation.\n12.2.5 Manchester Encoding\nBefore proceeding to discuss Manchester encoding, let us differentiate between a physical bit,\nand a logical bit. Up till now we have assumed that they mean the same thing. However,\nthis will cease to be true from now onwards. A physical bit such as a physical one or zero, is\nrepresentative of the voltage across a link. For example, in an active high signalling method,\na high voltage indicates that we are transmitting the bit, 1, and a low voltage (physical bit 0)\nindicatesthatwearetransmittingthe0bit. However, thisceasestobethecasenowbecausewe\nassumethatalogicalbit(logical0or1)isafunctionofthevaluesofphysicalbits. Forexample,\nwe can infer a logical 0, if the current and the previous physical bit are equal to 10. Likewise,\nwe can have a different rule for inferring a logical 1. It is the job of the receiver to translate\nphysical signals (or rather physical bits), into logical bits, and pass them to the higher layers of\nthe I\/O system. The next layer (data link layer discussed in Section 12.4) accepts logical bits\nfrom the physical layer. It is oblivious to the nature of the signalling, and the connotations of\nphysical bits transmitted on the link.\nLet us now discuss one such mechanism known as Manchester encoding. Here, we encode\nlogical bits as a transition of physical bits. Figure 12.10 shows an example. A 0 \u2192 1 transition\nof physical bits encodes a logical 1, and conversely a 1 \u2192 0 transition of physical bits encodes\na logical 0.\nA Manchester code always has a transition to encode data. Most of the time at the middle\nof a bit period, we have a transition. If there is no transition, we can conclude that no signal\nis being transmitted and the link is idle. One advantage of Manchester encoding is that it is\neasy to decode the information that is sent on the link. We just need to detect the nature of\nthe transition. Secondly, we do not need external strobe signals to synchronise the data. The 621 (cid:13)c Smruti R. Sarangi\nClock\n0 1 1 1 0 1 0 1 0 0 0\nData\nFigure 12.10: Manchester code (example)\ndata is said to be self clocked. This means that we can extract the clock of the sender from the\ndata, and ensure that the receiver reads in data at the same speed at which it is sent by the\nsender.\nDefinition 148\nA self clocked signal allows the receiver to extract the clock of the sender by examining the\ntransition of the physical bits in the signal. If there are periodic transitions in the signal,\nthen the period of these transitions is equal to the clock period of the sender, and thus the\nreceiver can read in data at the speed at which it is sent.\nManchester encoding is used in the IEEE 802.3 communication protocol that forms the\nbasis of today\u2019s Ethernet protocol for local area networks. Critics argue that since every logical\nbit is associated with a transition, we unnecessarily end up dissipating a lot of power. Every\nsingle transition requires us to charge\/discharge a set of capacitors associated with the link,\nthe drivers, and associated circuitry. The associated resistive loss is dissipated as heat. Let us\nthus try to reduce the number of transitions.\n12.2.6 Non Return to Zero (NRZ) Protocol\nHere, we take advantage of a run of 1s and 0s. For a transmitting a logical 1, we set the voltage\nof the link equal to high. Similarly, for transmitting a logical 0, we set the voltage of the link\nto low. Let us now consider a run of two 1 bits. For the second bit, we do not induce any\ntransitions in the link, and we maintain the voltage of the link as high. Similarly, if we have a\nrun of n 0s. Then for the last (n\u22121) 0s we maintain the low voltage of the link, and thus we\ndo not have transitions. Figure 12.11 shows an example. We observe that we have minimised\nthe number of transitions by completely avoiding voltage transitions when the value of the\nlogical bit that needs to be transmitted remains the same. This protocol is fast because we are (cid:13)c Smruti R. Sarangi 622\nClock\n0 1 1 1 0 1 0 1 0 0 0\nData\nFigure 12.11: Non return to zero protocol (example)\nnot wasting any time (such as the RZ protocols), and is power efficient because we eliminate\ntransitions for a run of the same bit (unlike RZ and Manchester codes).\nHowever, the added speed and power efficiency comes at the cost of complexity. Let us\nassume that we want to transmit a string of hundred 1s. In this case, we will have a transition\nonly for the first and last bit. Since the receiver does not have the clock of the sender, it has\nno way of knowing the length of a bit period. Even if the sender and receiver share the same\nclock, due to delays induced in the link, the receiver might conclude that we have a run of\n99 or 101 bits with a non-zero probability. Hence, we have to send additional synchronisation\ninformation such that the receiver can properly read all the data that is being sent on the link.\n12.2.7 Non Return to Zero (NRZI) Inverted Protocol\nThis is a variant of the NRZ protocol. Here, we have a transition from 0 to 1, or 1 to 0, when\nwe wish to encode a logical 1. For logical 0s, there are no transitions. Figure 12.12 shows an\nexample.\nClock\n0 1 1 1 0 1 0 1 0 0 0\nData\nFigure 12.12: Non return to zero inverted protocol (example) 623 (cid:13)c Smruti R. Sarangi\n12.3 Physical Layer \u2013 Synchronisation Sublayer\nThe transmission sublayer ensures that a sequence of pulses is successfully sent from the trans-\nmitter to either one receiver, or to a set of receivers. However, this is not enough. The receiver\nneedstoreadthesignalattherighttime, andneedstoassumethecorrectbitperiod. Ifitreads\nthe signal too early or too late, then it risks getting the wrong value of the signal. Secondly, if\nit assumes the wrong values of the bit period, then the NRZ protocol might not work. Hence,\nthere is a need to maintain a notion of time between the source and destination. The destina-\ntion needs to know exactly when to transfer the value into a latch. Let us consider solutions\nfor a single source and destination. Extending the methods to a set of destinations is left as an\nexercise to the reader.\nTo summarise, the synchronisation sublayer receives a sequence of logical bits from the\ntransmission sublayer without any timing guarantees. It needs to figure out the values of the\nbit periods, and read in an entire frame (a fixed size chunk) of data sent by the sender, and\nsend it to the data link layer. Note that the actual job of finding out the frame boundaries,\nand putting sets of bits in a frame is done by data link layer.\n12.3.1 Synchronous Buses\nSimple Synchronous Bus\nLet us first consider the case of a synchronous system where the sender and the receiver share\nthe same clock, and it takes a fraction of a cycle to transfer the data from the sender to the\nreceiver. Moreover, let us assume that the sender is transmitting all the time. Let us call this\nsystem a simple synchronous bus.\nIn this case, the task of synchronising between the sender and receiver is fairly easy. We\nknow that data is sent at the negative edge of a clock, and in less than a cycle it reaches the\nreceiver. The most important issue that we need to avoid is metastability (see Section 6.3.8).\nA flip flop enters a metastable state when the data makes a transition within a small window\nof time around the negative edge of the clock. In specific, we want the data to be stable for\nan interval known as the setup time before the clock edge, and the data needs to be stable for\nanother interval known as the hold time after the clock edge. The interval comprising of the\nsetup and hold intervals, is known as the keep-out region of the clock as defined in Section 6.3.8\nand [Dally and Poulton, 1998].\nIn this case, we assume that the data reaches the receiver in less than t \u2212t units of\nclk setup\ntime. Thus, there are no metastability issues, and we can read the data into a flip-flop at the\nreceiver. Since digital circuits typically process data in larger chunks (bytes or words), we use\na serial in \u2013 parallel out register at the receiver. We serially read in n bits, and read out an\nn-bit chunk in one go. Since the sender and the receiver clocks are the same, there is no rate\nmismatch. The circuit for the receiver is shown in Figure 12.13.\nMesochronous Bus\nIn a mesochronous system, the phase difference between the signal and the clock is a constant.\nThe phase difference can be induced in the signal because of the propagation delay in the link,\nand because there might be a phase difference in the clocks of the sender and the receiver. In (cid:13)c Smruti R. Sarangi 624\nI\/O link\nD Q\nFigure 12.13: The receiver of a simple synchronous bus\nthis case, it is possible that we might have a metastability issue because the data might arrive\nin the crucial keep-out region of the receiver clock.\nHence, we need to add a delay element that can delay the signal by a fixed amount of time\nsuch that there are no transitions in the keep-out region of the receiver clock. The rest of the\ncircuit remains the same as that used for the simple synchronous bus. The design of the circuit\nis shown in Figure 12.14.\nI\/O Link\nD Q\nTunable delay\nelement\nFigure 12.14: The receiver of a mesochronous bus\nA delay element can be constructed by using a delay locked loop (DLL). DLLs can have\ndifferent designs and some of them can be fairly complex. A simple DLL consists of a chain of\ninverters. Note that we need to have an even number of inverters to ensure that the output is\nequal to the input. To create a tunable delay element, we can tap the signals after every pair of\ninverters. These signals are logically equivalent to the input, but have a progressive phase delay\ndue to the propagation delay of the inverters. We can then choose the signal with a specific\namount of phase delay by using a multiplexer.\nPlesiochronous Bus*\nLet us now consider a more realistic scenario. In this case the clocks of the sender and receiver\nmight not exactly be the same. We might have a small amount of clock drift. We can assume\nthat over a period of tens or hundreds of cycles it is minimal. However, we can have a couple\ncycles of drift over millions of cycles. Secondly, let us assume that the sender does not transmit\ndata all the time. There are idle periods in the bus. Such kind of buses are found in server\ncomputers where we have multiple motherboards that theoretically run at the same frequency,\nbut do not share a common clock. There is some amount of clock drift (around 200 ppm [Dally 625 (cid:13)c Smruti R. Sarangi\nandPoulton, 1998])betweentheprocessorswhenweconsider timescalesoftheorderofmillions\nof cycles.\nLet us now make some simplistic assumptions. Typically a given frame of data contains\n100s or possibly 1000s of bits. We need not worry about clock drift when we are transmitting\na few bits (< 100). However, for more bits (> 100), we need to periodically resynchronise the\nclocks such that we do not miss data. Secondly, ensuring that there are no transitions in the\nkeep-out region of the receiver\u2019s clock is a non-trivial problem.\nTo solve this problem, we use an additional signal known as the strobe that is synchronised\nwith the sender\u2019s clock. We toggle a strobe pulse at the beginning of the transmission of a\nframe (or possibly a few cycles before sending the first data bit). We then periodically toggle\nthe strobe pulse once every n cycles. In this case, the receiver uses a tunable delay element.\nIt tunes its delay based on the interval between the time at which it receives the strobe pulse,\nand the clock transition. After sending the strobe pulse for a few cycles, we start transmitting\ndata. Since the clocks can drift, we need to readjust or retune the delay element. Hence, it is\nnecessary to periodically send strobe pulses to the receiver. We show a timing diagram for the\ndata and the strobe in Figure 12.15.\nClock\nStrobe\nData\nFigure 12.15: The timing diagram of a plesiochronous bus\nSimilar to the case of the mesochronous bus, every n cycles the receiver can read out all the\nn bits in parallel using a serial in \u2013 parallel out register. The circuit for the receiver is shown\nin Figure 12.16. We have a delay calculator circuit that takes the strobe and the receiver clock\n(rclk) as input. Based on the phase delay, it tunes the delay element such that data from the\nsource arrives at the middle of the receiver\u2019s clock cycle. This needs to be done because of the\nfollowing reason. Since the sender and receiver clock periods are not exactly the same, there\ncan be an issue of rate mismatch. It is possible that we might get two valid data bits in one\nreceiver clock cycle, or get no bits at all. This will happen, when a bit arrives towards the\nbeginning or end of a clock cycle. Hence, we want to ensure that bits arrive at the middle of a\nclock cycle. Additionally, there are also metastability avoidance issues.\nSadly, the phase can gradually change and bits might start arriving at the receiver at the\nbeginning of a clock cycle. It can then become possible to receive two bits in the same cycle.\nIn this case, dedicated circuitry needs to predict this event, and a priori send a message to the\nsender to pause sending bits. Meanwhile, the delay element should be retuned to ensure that (cid:13)c Smruti R. Sarangi 626\nbits arrive at the middle of a cycle.\nI\/O link\nD Q\nTunable delay\nelement\nStrobe\nDelay calculator rclk\nrclk\nto sender\nFigure 12.16: The receiver of a plesiochronous bus\n12.3.2 Source Synchronous Bus*\nSadly, even plesiochronous buses are hard to manufacture. We often have large and unpre-\ndictable delays while transmitting signals, and even ensuring tight clock synchronisation is\ndifficult. For example, the AMD hypertransport [Consortium et al., 2006] protocol that is used\nto provide a fast I\/O path between different processors on the same motherboard does not as-\nsume synchronised or plesiosynchronised clocks. Secondly, the protocol assumes an additional\njitter (unpredictability in the signal propagation time) of up to 1 cycle.\nIn such cases, we need to use a more complicated strobe signal. In a source synchronous\nbus, we typically send the sender clock as the strobe signal. The main insight is that if delays\nare introduced in the signal propagation time, then the signal and the strobe will be equally\naffected. This is a very realistic assumption, and thus most high performance I\/O buses use\nsource synchronous buses as of 2013. The circuit for a source synchronous bus is again not\nvery complicated. We clock in data to the serial in \u2013 parallel out register using the clock of the\nsender (sent as the strobe). It is referred to as xclk. We read the data out using the clock of the\nreceiver as shown in Figure 12.17. As a rule whenever a signal travels across clock boundaries\nwe need a tunable delay element to keep transitions out of the keep-out region. We thus have\na delay calculator circuit that computes the parameters of the delay element depending upon\nthe phase difference between the sender clock received as a strobe (xclk), and the receiver clock\n(rclk).\nNote that it is possible to have multiple parallel data links such that a set of bits can be\nsent simultaneously. All the data lines can share the strobe that carries the synchronising clock\nsignal. 627 (cid:13)c Smruti R. Sarangi\nI\/O link\nD Q\nxclk\nrclk Tunable delay\nDelay calculator element\nto receiver\nD Q\nrclk\nFigure 12.17: The receiver of a source synchronous bus\n12.3.3 Asynchronous Buses\nClock Detection and Recovery*\nNow, let us consider the most general class of buses known as asynchronous buses. Here, we\ndo not make any guarantees regarding the synchronisation of the clocks of the sender and the\nreceiver. Nor, do we send the clock of the sender along with the signal. It is the job of the\nreceiver, to extract the clock of the sender from the signal, and read the data in correctly. Let\nus take a look at the circuit for reading in the data as shown in Figure 12.18.\nI\/O Link\nD Q\nClock recovery\ncircuit Tunable delay\nDelay calculator element\nrclk to receiver\nD Q\nrclk\nFigure 12.18: The receiver circuit in an asynchronous bus\nFor the sake of explanation, let us assume that we use the NRZ method of encoding bits.\nExtending the design to other kinds of encodings is fairly easy, and we leave it as an exercise for\nthe reader. The logical bit stream passed on by the transmission sublayer is sent to the first D (cid:13)c Smruti R. Sarangi 628\nflip-flop, and simultaneously to the clock detector and recovery circuit. These circuits examine\nthe transitions in the I\/O signal and try to guess the clock of the sender. Specifically, the clock\nrecovery circuit contains a PLL (phase locked loop). A PLL is an oscillator that generates a\nclock signal, and tries to adjust its phase and frequency such that it is as close as possible to\nthe sequence of transitions in the input signal. Note that this is a rather involved operation.\nIn the case of the RZ or Manchester encodings, we have periodic transitions. Hence, it\nis easier to synchronise the PLL circuits at the receiver. However, for the NRZ encoding, we\ndo not have periodic transitions. Hence, it is possible that the PLL circuits at the receiver\nmight fall out of synchrony. A lot of protocols that use the NRZ encoding (notably the USB\nprotocol) insert periodic transitions or dummy bits in the signal to resynchronise the PLLs at\nthe receiver. Secondly, the PLL in the clock recovery circuit also needs to deal with the issue of\nlong periods of inactivity in the bus. During this time, it can fall out of synchronisation. There\nare advanced schemes to ensure that we can correctly recover the clock from an asynchronous\nsignal. These topics are taught in advanced courses in communication, and digital systems. We\nshall only take a cursory look in this chapter, and assume that the clock recovery circuit does\nits job correctly.\nWe connect the output of the clock detection and recovery circuit to the clock input of the\nfirst D flip-flop. We thus clock in data according to the sender\u2019s clock. To avoid metastability\nissues we introduce delay elements between the two D flip-flops. The second D flip-flop is in\nthe receiver\u2019s clock domain. This part of the circuit is similar to that of source synchronous\nbuses.\nNote that in the case of ternary signalling, it is easy to find out when a bus is active (when\nwe see a physical 0 or 1 on the bus). However, in the case of binary signalling, we do not know\nwhen the bus is active, because in principle we have a 0 or 1 bit being transmitted all the time.\nHence, it is necessary to use an additional strobe signal to indicate the availability of data. Let\nus now look at protocols that use a strobe signal to indicate the availability of data on the\nbus. The strobe signals can also be optionally used by ternary buses to indicate the beginning\nand end of an I\/O request. In any case, the reader needs to note that both the methods that\nwe present using strobe signals are rather basic, and have been superseded by more advanced\nmethods.\nAsynchronous Communication with Strobe Signals\nLet us assume that the source wishes to send data to the destination. It first places data on\nthe bus, and after a small delay sets (sets to 1) the strobe as shown in the timing diagram\nin Figure 12.19. This is done to ensure that the data is stable on the bus before the receiver\nperceives the strobe to be set. The receiver immediately starts to read data values. Till the\nstrobe is on, the receiver continues to read data, places it in a register, and transfers chunks of\ndata to higher layers. When the source decides to stop sending data, it resets (sets to 0) the\nstrobe. Note that timing is important here. We typically reset the strobe just before we cease\nsending data. This needs to be done because we want the receiver to treat the contents of the\nbus after the strobe is reset as the last bit. In general, we want the data signal to hold its value\nfor some time after we have read it (for metastability constraints). 629 (cid:13)c Smruti R. Sarangi\nData\nStrobe\nFigure 12.19: Timing diagram of a strobe based asynchronous communication system\nAsynchronous Communication with Handshaking (4 Phase)\nNote that in simple asynchronous communication with strobe signals the source has no way of\nknowing if the receiver has read the data. We thus introduce a handshaking protocol where the\nsourceisexplicitlymadeawareofthefactthatthereceiverhasreadallitsdata. Theassociated\ntiming diagram is shown in Figure 12.20.\nData\nStrobe\nAck\nFigure 12.20: Timing diagram of a strobe based asynchronous communication system with\nhandshaking\nAt the outset, the sender places data on the bus, and then sets the strobe. The receiver\nbegins to read data off the bus, as soon as it observes the strobe to be set. After it has read\nthe data, it sets the ack line to 1. After the transmitter observes the ack line set to 1, it can be\nsure of the fact that the receiver has read the data. Hence, the transmitter resets the strobe,\nand stops sending data. When the receiver observes that the strobe has been reset, it resets\nthe ack line. Subsequently, the transmitter is ready to transmit again using the same sequence\nof steps.\nThis sequence of steps ensures that the transmitter is aware of the fact that the receiver\nhas read the data. Note that this diagram makes sense when the receiver can ascertain that\nit has read all the data that the transmitter wished to transmit. Consequently, designers\nmostly use this protocol for transmitting single bits. In this case, after the receiver has read\nthe bit, it can assert the ack line. Secondly, this approach is also more relevant for the RZ\nand Manchester coding approaches because the transmitter needs to return to the default state\nbeforetransmittinganewbit. Afteritreceivestheacknowledgement, thetransmittercanbegin\nthe process of returning to the default state, as shown in Figure 12.19.\nTo transmit multiple bits in parallel, we need to have a strobe for each data line. We can (cid:13)c Smruti R. Sarangi 630\nhowever, have a common acknowledgement line. We need to set the ack signal when all the\nreceivers have read their bits, and we need to reset the ack line, when all the strobe lines have\nbeen reset. Lastly, let us note that there are four separate events in this protocol (as shown in\nthe diagram). Hence, this protocol is known as a 4-phase handshake protocol.\nAsynchronous Communication with Handshaking (2 Phase)\nIf we are using the NRZ protocols, then we do not need to return to the default state. We can\nimmediately start transmitting the next bit after receiving the acknowledgement. However, in\nthis case, we need to slightly change the semantics of the strobe and acknowledgement signals.\nFigure 12.21 shows the timing diagram.\nData\nStrobe\nAck\nFigure 12.21: Timing diagram of a strobe based asynchronous communication system with\n2-phase handshaking\nIn this case, after placing the data on the bus, the transmitter toggles the value of the\nstrobe. Subsequently, after reading the data, the receiver toggles the value of the ack line.\nAfter the transmitter detects that the ack line has been toggled, it starts transmitting the next\nbit. After a short duration, it toggles the value of the strobe to indicate the presence of data.\nAgain, after reading the bit, the receiver toggles the ack line, and the protocol thus continues.\nNote that in this case, instead of setting and resetting the ack and strobe lines, we toggle them\ninstead. This reduces the number of events that we need to track on the bus. However, this\nrequires us to keep some additional state at the side of the sender and the receiver. This a\nnegligible overhead. Our 4-phase protocol thus gets significantly simplified. The NRZ protocols\nare more amenable to this approach because they have continuous data transmission, without\nany intervening pause periods.\nDefinition 149\nSimple Synchronous Bus A simple synchronous bus that assumes that the transmitter\nand the receiver share the same clock, and there is no skew (deviation) between the\nclocks.\nMesochronous Bus Here, the transmitter and receiver have the same clock frequency, but\nthere can be a phase delay between the clocks. 631 (cid:13)c Smruti R. Sarangi\nPlesiochronous Bus In a plesiochronous bus, there is a small amount of mismatch be-\ntween the frequencies of the clocks of the transmitter and receiver.\nSource Synchronous Bus In a source synchronous bus, there is no relationship between\nthe clocks of the transmitter and receiver. Consequently, we send the clock of the\ntransmitter to the receiver along with the message, such that it can use it to sample\nthe bits in the message.\nAsynchronous Bus An asynchronous bus does not assume any relationship between the\nclocks of the transmitter and receiver. It typically has sophisticated circuitry to recover\nthe clock of the transmitter by analysing the voltage transitions in the message.\n12.4 Data Link Layer\nNow, we are ready to discuss the data link layer. The data link layer gets sequences of logical\nbits from the physical layer. If the width of the serial in \u2013 parallel out register is n bits,\nthen we are guaranteed to get n bits at one go. The job of the data link layer is to break\nthe data into frames, and buffer frames for transmission on other outgoing links. Secondly, it\nperforms rudimentary error checking and correction. It is possible that due to electromagnetic\ninterference, errors might be induced in the signal. For example, a logical 1 might flip to a\nlogical 0, and vice versa. It is possible to correct such single bit errors in the data link layer.\nIf there are a lot of errors, and it is not possible to correct the errors, then at this stage, the\nreceiver can send a message to the transmitter requesting for a retransmission. After error\nchecking the frame is ready to be forwarded on another link if required.\nIt is possible that multiple senders might be trying to access a bus at the same time. In\nthis case, we need to arbitrate between the requests, and ensure that only one sender can send\ndata at any single point of time. This process is known as arbitration, and is also typically\nperformed in the data link layer. Lastly, the arbitration logic needs to have special support for\nhandling requests that are part of a transaction. For example, the bus to the memory units\nmight contain a load request as a part of a memory transaction. In response, the memory unit\nsends a response message containing the contents of the memory locations. We need a little bit\nof additional support at the level of the bus controller to support such message patterns.\nTo summarise the data link layer breaks data received from the physical layer into frames,\nperforms error checking, manages the bus by allowing a single transmitter at a single time, and\noptimises communication for common message patterns.\n12.4.1 Framing and Buffering\nThe processing in the data link layer begins by reading sets of bits from the physical layer.\nWe can either have one serial link, or multiple serial links that transmit bits simultaneously.\nA set of multiple serial links is known as a parallel link. In both cases, we read in data, save\nthem in serial in \u2013 parallel out shift registers, and send chunks of bits to the data link layer.\nThe role of the data link layer is to create frames of bits from the values that it gets from the (cid:13)c Smruti R. Sarangi 632\nphysical layer. A frame might be one byte for links that transfer data from the keyboard and\nmouse, and might be as high as 128 bytes for links that transfer data between the processor\nand the main memory, or the main memory and the graphics card. In any case, the data link\nlayer for each bus controller is aware of the frame size. The main problem is to demarcate the\nboundaries of a frame.\nDemarcation by Inserting Long Pauses Between two consecutive frames, the bus con-\ntroller can insert long pauses. By examining, the duration of these pauses, the receiver\ncan infer frame boundaries. However, because of jitter in the I\/O channel, the duration\nof these pauses can change, and new pauses can be introduced. This is not a very reliable\nmethod and it also wastes valuable bandwidth.\nBit Count Wecanfixthenumberofbitsinaframeapriori. Wecansimplycountthenumber\nof bits that are sent, and declare a frame to be over once the required number of bits have\nreached the receiver. However, the main issue is that sometimes pulses can get deleted\nbecause of signal distortion, and it is very easy to go out of synchronisation.\nBit\/Byte Stuffing This is the most flexible approach and is used in most commercial imple-\nmentations of I\/O buses. Here, we use a pre-specified sequence of bits to designate the\nstart and end of a frame. For example, we can use the pattern 0xDEADBEEF to indicate\nthe start of a frame, and 0x12345678 to indicate the end of a frame. The probability that\nany 32-bit sequence in the frame will match the special sequences at the start and end\nis very small. The probability is equal to 2\u221232, or 2.5e \u2212 10. Sadly, the probability is\nstill non zero. Hence, we can adopt a simple solution to solve this problem. If the se-\nquence, 0xDEADBEEF appears in the content of the frame, then we add 32 more dummy\nbits and repeat this pattern. For example, the bit pattern 0xDEADBEEF gets replaced\nwith 0xDEADBEEFDEADBEEF. The link layer of the receiver can find out that the\npattern repeats an even number of times. Half of the bits in the pattern are a part of\nthe frame, and the rest are dummy bits. The receiver can then proceed to remove the\ndummy bits. This method is flexible because it can be made very resilient to jitter and\nreliability problems. These sequences are also known as commas.\nOnce, the data link layer creates a frame, it sends it to the error checking module, and also\nbuffers it.\n12.4.2 Error Detection and Correction\nErrors can get introduced in signal transmission for a variety of reasons. We can have external\nelectromagnetic interference due to other electronic gadgets operating nearby. Readers would\nhave noticed a loss in the voice quality of a mobile phone after they switch on an electronic\ngadget such as a microwave oven. This happens because electromagnetic waves get coupled to\nthe copper wires of the I\/O channel and introduce current pulses. We can also have additional\ninterference from nearby wires (known as crosstalk), and changes in the transmission delay of a\nwire due to temperature. Cumulatively, interference can induce jitter (introduce variabilities in\nthe propagation time of the signal), and introduce distortion (change the shape of the pulses).\nWe can thus wrongly interpret a 0 as a 1, and vice versa. It is thus necessary to add redundant\ninformation, such that the correct value can be recovered. 633 (cid:13)c Smruti R. Sarangi\nThe reader needs to note that the probability of an error is very low in practice. It is\ntypically less than 1 in every million transfers for interconnects on motherboards. However,\nthis is not a very small number either. If we have a million I\/O operations per second, which\nis plausible, then we will typically have 1 error per second. This is actually a very high error\nrate. Hence, we need to add extra information to bits such that we can detect and recover from\nerrors. This approach is known as forward error correction. In comparison, in backward error\ncorrection, we detect an error, discard the message, and request the sender to retransmit. Let\nus now discuss the prevalent error detection and recovery schemes.\nDefinition 150\nForward Error Correction In this method, we add additional bits to a frame. These\nadditional bits contain enough information to detect and recover from single or double\nbit errors if required.\nBackward Error Correction In this method also, we add additional bits to a frame,\nand these bits help us detect single or double bit errors. However, they do not allow\nus to correct errors. We can discard the message, and ask the transmitter for a\nretransmission.\nSingle Error Detection\nSince single bit errors are fairly improbable, it is extremely unlikely that we shall have two\nerrors in the same frame. Let us thus focus on detecting a single error, and also assume that\nonly one bit flips its state due to an error.\nLet us simplify our problem. Let us assume that a frame contains 8 bits, and we wish to\ndetect if there is a single bit error. Let us number the bits in the frame as D ,D ,...,D\n1 2 8\nrespectively. Let us now add an additional bit known as the parity bit. The parity bit, P is\nequal to:\nP = D \u2295D \u2295...\u2295D (12.1)\n1 2 8\nHere, the \u2295 operation is the XOR operator. In simple terms, the parity bit represents the\nXOR of all the data bits (D ...D ). For every 8 bits, we send an additional bit, which is\n1 8\nthe parity bit. Thus, we convert a 8-bit message to an equivalent 9 bit message. In this case,\nwe are effectively adding a 12.5% overhead in terms of available bandwidth, at the price of\nhigher reliability. Figure 12.22 shows the structure of a frame or message using our 8-bit parity\nscheme. Note that we can support larger frame sizes also by associating a separate parity bit\nwith each sequence of 8 data bits.\nWhen the receiver receives the message, it computes the parity by computing the XOR of\nthe 8 data bits. If this value matches the parity bit, then we can conclude that there is no error.\nHowever, if the parity bit in the message does not match the value of the computed parity bit,\nthen we can conclude that there is a single bit error. The error can be in any of the data bits (cid:13)c Smruti R. Sarangi 634\nData bits Parity bit\nFigure 12.22: An 8-bit message with a parity bit\nin the message, or can even be in the parity bit. In this case, we have no way of knowing. All\nthat we can detect is that there is a single bit error. Let us now try to correct the error also.\nSingle Error Correction\nTo correct a single bit error, we need to know the index of the bit that has been flipped if there\nis an error. Let us now count the set of possible outcomes. For an n-bit block, we need to\nknow the index of the bit that has an error. We can have n possible indices in this case. We\nalso need to account for the case, in which we do not have an error. Thus for a single error\ncorrection (SEC) circuit there are a total of n+1 possible outcomes (n outcomes with errors,\nand one outcome with no error). Thus, from a theoretical point of view, we need (cid:100)log(n+1)(cid:101)\nadditional bits. For example, for an 8-bit frame, we need (cid:100)log(8+1)(cid:101) = 4 bits. Let us design\na (8,4) code that has four additional bits for every 8-bit data word.\nLet us start out by extending the parity scheme. Let us assume that each of the four\nadditional bits are parity bits. However, they are not the parity functions of the entire set of\ndata bits. Instead, each bit is the parity of a subset of data bits. Let us name the four parity\nbits P , P , P , and P . Moreover, let us arrange the 8 data bits, and the 4 parity bits as shown\n1 2 3 4\nin Figure 12.23.\nP P D P D D D P D D D D\n1 2 1 3 2 3 4 4 5 6 7 8\nFigure 12.23: Arrangement of data and parity bits\nWe keep the parity bits, P , P , P , and P in positions 1, 2, 4 and 8 respectively. We\n1 2 3 4\narrange the data bits, D ...D , in positions 3, 5, 6, 7, 9, 10, 11, and 12 respectively. The\n1 8\nnext step is to assign a set of data bits to each parity bit. Let us represent the position of each\ndata bit in binary. In this case, we need 4 binary bits because the largest number that we need\nto represent is 12. Now, let us associate the first parity bit, P , with all the data bits whose\n1\npositions (represented in binary) have 1 as their LSB. In this case, the data bits with 1 as their\nLSB are D (3), D (5), D (7), D (9), and D (11). We thus compute the parity bit P as:\n1 2 4 5 7 1\nP = D \u2295D \u2295D \u2295D \u2295D (12.2)\n1 1 2 4 5 7\nSimilarly, we associate the second parity bit, P , with all the data bits that have a 1 in their\n2\n2nd position (assumption is that the LSB is in the first position). We use similar definitions for 635 (cid:13)c Smruti R. Sarangi\nthe 3rd, and 4th parity bits.\nData Bits\nParity Bits D D D D D D D D\n1 2 3 4 5 6 7 8\n0011 0101 0110 0111 1001 1010 1011 1100\nP X X X X X\n1\nP X X X X X\n2\nP X X X X\n3\nP X X X X\n4\nTable 12.2: Relationship between data and parity bits\nTable 12.2 shows the association between data and parity bits. An \u201cX\u201d indicates that a\ngiven parity bit is a function of the data bit. Based, on this table, we arrive at the following\nequations for computing the parity bits.\nP = D \u2295D \u2295D \u2295D \u2295D (12.3)\n1 1 2 4 5 7\nP = D \u2295D \u2295D \u2295D \u2295D (12.4)\n2 1 3 4 6 7\nP = D \u2295D \u2295D \u2295D (12.5)\n3 2 3 4 8\nP = D \u2295D \u2295D \u2295D (12.6)\n4 5 6 7 8\nThe algorithm for message transmission is as follows. We compute the parity bits according\nto Equations 12.3 \u2013 12.6. Then, we insert the parity bits in the positions 1, 2, 4, and 8\nrespectively, and form a message according to Figure 12.23 by adding the data bits. Once the\ndata link layer of the receiver gets the message it first extracts the parity bits, and forms a\nnumber of the form P = P P P P , that is composed of the four parity bits. For example, if\n4 3 2 1\nP = 0, P = 0, P = 1, andP = 1, thenP = 1100. Subsequently, theerrordetectioncircuitat\n1 2 3 4\nthe receiver computes a new set of parity bits (P(cid:48), P(cid:48), P(cid:48), P(cid:48)) from the received data bits, and\n1 2 3 4\nforms another number of the form P(cid:48) = P(cid:48)P(cid:48)P(cid:48)P(cid:48). Ideally P should be equal to P(cid:48). However,\n4 3 2 1\nif there is an error in the data or parity bits, then this will not be the case. Let us compute\nP \u2295P(cid:48). This value is also known as the syndrome.\nLet us now try to correlate the value of the syndrome with the position of the erroneous bit.\nLet us first assume that there is an error in a parity bit. In this case, the first four entries in\nTable12.3showthepositionoftheerroneousbitinthemessage, andthevalueofthesyndrome.\nThe value of the syndrome is equal to the position of the erroneous bit in the message. This\nshould come as no surprise to the reader, because we designed our message to explicitly ensure\nthis. The parity bits are at positions 1, 2, 4, and 8 respectively. Consequently, if any parity\nbit has an error, its corresponding bit in the syndrome gets set to 1, and the rest of the bits\nremain 0. Consequently, the syndrome matches the position of the erroneous bit.\nLet us now consider the case of single bit errors in data bits. Again from Table 12.3, we\ncan conclude that the syndrome matches the position of the data bit. This is because once a\ndata bit has an error, all its associated parity bits get flipped. For example, if D has an error\n5\nthen the parity bits, P and P , get flipped. Recall that the reason we associate P and P with\n1 4 1 4 (cid:13)c Smruti R. Sarangi 636\nBit Position Syndrome Bit Position Syndrome\nP 1 0001 D 6 0110\n1 3\nP 2 0010 D 7 0111\n2 4\nP 4 0100 D 9 1001\n3 5\nP 8 1000 D 10 1010\n4 6\nD 3 0011 D 11 1011\n1 7\nD 5 0101 D 12 1100\n2 8\nTable 12.3: Relationship between the position of an error and the syndrome\nD is because D is bit number 9 (1001), and the two 1s in the binary representation of 9 are\n5 5\nin positions 1 and 4 respectively. Subsequently, when there is an error in D , the syndrome is\n5\nequal to 1001, which is also the index of the bit in the message. Similarly, there is a unique\nsyndrome for every data and parity bit (refer to Table 12.2).\nThus, we can conclude that if there is an error, then the syndrome points to the index of\nthe erroneous bit (data or parity). Now, if there is no error, then the syndrome is equal to 0.\nWe thus have a method to detect and correct a single error. This method of encoding messages\nwith additional parity bits is known as the SEC (single error correction) code.\nSingle Error Correction, Double Error Detection (SECDED)\nLet us now try to use the SEC code to additionally detect double errors (errors in two bits).\nLet us show a counterexample, and prove that our method based on syndromes will not work.\nLet us assume that there are errors in bits D , and D . The syndrome will be equal to 0111.\n2 3\nHowever, if there is an error in D , the syndrome will also be equal to 0111. There is thus no\n4\nway of knowing whether we have a single bit error (D ), or a double bit error (D and D ).\n4 2 3\nLet us slightly augment our algorithm to detect double errors also. Let us add an additional\nparity bit, P , that computes the parity of all the data bits (D ...D ), and the four parity bits\n5 1 8\n(P ...P ) used in the SEC code, and then let us add P to the message. Let us save it in the\n1 4 5\n13th position in our message, and exclude it from the process of calculation of the syndrome.\nThe new algorithm is as follows. We first calculate the syndrome using the same process as\nused for the SEC (single error correction) code. If the syndrome is 0, then there can be no error\n(single or double). The proof for the case of a single error can be readily verified by taking a\nlook at Table 12.2. For a double error, let us assume that two parity bits have gotten flipped.\nIn this case, the syndrome will have two 1s. Similarly, if two data bits have been flipped, then\nthe syndrome will have at least one 1 bit, because no two data bits have identical columns in\nTable 12.2. Now, if a data and a parity bit have been flipped, then also the syndrome will be\nnon-zero, because a data bit is associated with multiple parity bits. The correct parity bits will\nindicate that there is an error.\nHence, ifthesyndromeisnon-zero, wesuspectanerror; otherwise, weassumethatthereare\nno errors. If there is an error, we take a look at the bit P in the message, and also recompute\n5\nit at the receiver. Let us designate the recomputed parity bit as P(cid:48). Now, if P = P(cid:48), then we\n5 5 5\ncan conclude that there is a double bit error. Two single bit errors are essentially cancelling\neach other while computing the final parity. Conversely, if P (cid:54)= P(cid:48), then it means that we have\n5 5 637 (cid:13)c Smruti R. Sarangi\na single bit error. We can thus use this check to detect if we have errors in two bits or one bit.\nIf we have a single bit error, then we can also correct it. However, for a double bit error, we\ncan just detect it, and possible ask the source for retransmission. This code is popularly known\nas the SECDED code .\nHamming Codes\nAll of the codes described up till now are known as Hamming codes. This is because they\nimplicitlyrelyontheHammingdistance. TheHammingdistanceisthenumberofcorresponding\nbitsthataredifferentbetweentwosequencesofbinarybits. Forexample,theHammingdistance\nbetween 0011 and 1010 is 2 (MSB and LSB are different).\nLet us now consider a 4-bit parity code. If a message is 0001, then the parity bit is equal\nto 1, and the transmitted message with the parity bit in the MSB position is 10001. Let us\nrefer to the transmitted message as the code word. Note that 00001 is not a valid code word,\nand the receiver will rely on this fact to adjudge if there is an error or not. In fact, there is no\nother valid code word within a Hamming distance of 1 of a valid code word. The reader needs\nto prove this fact. Likewise for a SEC code, the minimum Hamming distance between code\nwords is 2, and for a SECDED code it is 3. Let us now consider a different class of codes that\nare also very popular.\nCyclic Redundancy Check (CRC) Codes\nCRC codes are mostly used for detecting errors, even though they can be used to correct single\nbit errors in most cases. To motivate the use of CRC codes let us take a look at the patterns of\nerrors in practical I\/O systems. Typically in I\/O channels, we have interference for a duration\nof time that is longer than a bit period. For example, if there is some external electro-magnetic\ninterference, then it might last for several cycles, and it is possible that several bits might get\nflipped. This pattern of errors is known as a burst error. For example, a 32-bit CRC code can\ndetect burst errors as long as 32 bits. It typically can detect most 2-bit errors, and all single\nbit errors.\nThe mathematics behind CRC codes is complicated, and interested readers are referred to\ntexts on coding theory [Neubauer et al., 2007]. Let us show a small example in this section.\nLet us assume, that we wish to compute a 4-bit CRC code, for an 8-bit message. Let the\nmessage be equal to 10110011 in binary. The first step is to pad the message by 4 bits, which\n2\nis the length of the CRC code. Thus, the new message is equal to 101100110000 (a space has\nbeen added for improving readability). The CRC code requires another 5 bit number, which is\nknown as the generator polynomial or the divisor. In principle, we need to divide the number\nrepresented by the message with the number represented by the divisor. The remainder is the\nCRC code. However, this division is different from regular division. It is known as modulo-2\ndivision. In this case, let us assume that the divisor is 11001 . Note that for an n-bit CRC\n2\ncode, the length of the divisor is n+1 bits.\nLet us now show the algorithm. We start out by aligning the MSB of the divisor with\nthe MSB of the message. If the MSB of the message is equal to 1, then we compute a XOR\nof the first n+1 (5 in this case) bits, and the divisor, and replace the corresponding bits in\nthe message with the result. Otherwise, if the MSB is 0, we do not do anything. In the next\nstep, we shift the divisor one step to the right, treat the bit in the message aligned with the (cid:13)c Smruti R. Sarangi 638\nMSB of the divisor as the MSB of the message, and repeat the same process. We continue this\nsequence of steps till the LSB of the divisor is aligned with the LSB of the message. We show\nthe sequence of steps in Example 128. At the end, the least significant n (4 bits) contain the\nCRC code. For sending a message, we append the CRC code with the message. The receiver\nrecomputes the CRC code, and matches it with the code that is appended with the message.\nExample 128\nShow the steps for computing a 4-bit CRC code, where the message is equal to 10110011 ,\n2\nand the divisor is equal to 11001 .\n2\nAnswer:\n10110011\nPadding\n101100110000\n11001\n011110110000\n11001\n000111110000\n11001\n000001100000\n11001\n000000000100\nCRC code\nIn this figure, we ignore the steps in which the MSB of the relevant part of the message is\n0, because in these cases nothing needs to be done.\n12.4.3 Arbitration\nLet us now consider the problem of bus arbitration. The word \u201carbitration\u201d literally means\n\u201cresolutionofdisputes.\u201d Letusconsideramultidropbus,wherewecanpotentiallyhavemultiple\ntransmitters. Now, if multiple transmitters are interested in sending a value over the bus, we\nneed to ensure that only one transmitter can send a value on the bus at any point of time.\nThus, we need an arbitration policy to choose a device that can send data over the bus. If we\nhave point-to-point buses, where we have one sender and one receiver, then arbitration is not\nrequired. If we have messages of different types waiting to be transmitted, then we need to 639 (cid:13)c Smruti R. Sarangi\nschedule the transmission of messages on the link with respect to some optimality criteria.\nDefinition 151\nWe need to ensure that only one transmitter sends values on the bus at any point of time.\nSecondly, we need to ensure that there is fairness, and a transmitter does not need to\nwait for an indefinite amount of time for getting access to the bus. Furthermore, different\ndevices connected to a bus, typically have different priorities. It is necessary to respect\nthese priorities also. For example, the graphics card, should have more priority than the\nhard disk. If we delay the messages to the graphics card, the user will perceive jitter on her\nscreen, and this will lead to a bad user experience. We thus need a bus allocation policy that\nis fair to all the transmitters, and is responsive to the needs of the computer system. This\nbus allocation policy is popularly known as the arbitration policy.\nWe envision a dedicated structure known as an arbiter, which performs the job of bus\narbitration. All the devices are connected to the bus, and to the arbiter. They indicate their\nwillingness to transmit data by sending a message to the arbiter. The arbiter chooses one of\nthe devices. There are two topologies for connecting devices to an arbiter. We can either use\na star like topology, or we can use a daisy chain topology. Let us discuss both the schemes in\nthe subsequent sections.\nStar Topology\nIn this centralised protocol, we have a single central entity called the arbiter. It is a dedicated\npiece of circuitry that accepts bus request requests from all the devices that are desirous of\ntransmitting on the bus. It enforces priorities and fairness policies, and grants the right to\nindividual devices to send data on the bus. Specifically, after a request finishes, the arbiter\ntakes a look at all the current requests, and then asserts the bus grant signal for the device that\nis selected to send data. The selected device subsequently becomes the bus master and gets\nexclusive control of the bus. It can then configure the bus appropriately, and transmit data.\nAn overview the system is shown in Figure 12.24.\nWe can follow two kinds of approaches to find out when a current request has finished. The\nfirst approach is that every device connected to the bus transmits for a given number of cycles,\nn. In this case, after n cycles have elapsed, the arbiter can automatically presume that the bus\nis free, and it can schedule another request. However, this might not always be the case. We\nmight have different speeds of transmission, and different message sizes. In this case, it is the\nresponsibility of each transmitting device to let the arbiter know that it is done. We envision\nan additional signal bus release. Every device has a dedicated line to the arbiter that is used to\nsend the bus release signal. Once it is done with the process of transmitting, it asserts this line\n(sets it equal to 1). Subsequently, the arbiter allocates the bus to another device. It typically\nfollows standard policies such as round-robin or FIFO. (cid:13)c Smruti R. Sarangi 640\nDevice 4\nRequest Request\nDevice 1 Grant Arbiter Grant Device 3\nRelease Release\nDevice 2\nFigure 12.24: Centralised arbiter-based architecture\nBus request\nBus release\nArbiter Device 1 Device 2 Device n\nFigure 12.25: Daisy chain architecture\nDaisy Chain Based Arbitration\nIf we have multiple devices connected to a single bus, the arbiter needs to be aware of all of\nthem, and their relative priorities. Moreover, as we increase the number of devices connected\nto the bus, we start having high contention at the arbiter, and it becomes slow. Hence, we wish\nto have a scheme, where we can easily enforce priorities, guarantee some degree of fairness, and\nnot incur slowdowns in making bus allocation decisions as we increase the number of connected\ndevices. The daisy chain bus was proposed with all of these requirements in mind.\nFigure12.25showsthetopologyofadaisychainbasedbus. Thetopologyresemblesalinear\nchain, with the arbiter at one end. Each device other than the last one has two connections.\nThe protocol starts as follows. A device starts out by asserting its bus request lines. The bus\nRelease\nRelease\nGrant\nGrant\nRequest\nRequest 641 (cid:13)c Smruti R. Sarangi\nrequest lines of all the devices are connected in a wired OR fashion. The request line that goes\nto the arbiter essentially computes a logical OR of all the bus request lines. Subsequently, the\narbiter passes a token to the device connected to it if it has the token. Otherwise, we need to\nwait till the arbiter gets the release signal. Once a device gets the token, it becomes the bus\nmaster. It can transmit data on the bus if required. After transmitting messages, each device\npasses the token to the next device on the chain. This device also follows the same protocol.\nIt transmits data if it needs to, otherwise, it just passes the token. Finally, the token reaches\nthe end of the chain. The last device on the chain asserts the bus release signal, and destroys\nthe token. The release signal is a logical OR of all the bus release signals. Once, the arbiter\nobserves the release signal to be asserted, it creates a token. It re-inserts this token into the\ndaisy chain after it sees the request line set to 1.\nThere are several subtle advantages to this scheme. The first is that we have an implicit\nnotionofpriority. Thedevicethatisconnectedtothearbiterhasthehighestpriority. Gradually,\nas we move away form the arbiter the priority decreases. Secondly, the protocol has a degree\nof fairness because after a high priority device has relinquished the token, it cannot get it back\nagain, until all the low priority devices have gotten the token. Thus, it is not possible for\na device to wait indefinitely. Secondly, it is easy to plug in and remove devices to the bus.\nWe never maintain any individual state of a device. All the communication to the arbiter is\naggregated, and we only compute OR functions for the bus request, and bus release lines. The\nonly state that a device has to maintain is the information regarding its relative position in the\ndaisy chain, and the address of its immediate neighbour.\nWe can also have purely distributed schemes that avoid a centralised arbiter completely.\nIn such schemes, all the nodes take decisions independently. However, such schemes are rarely\nused, and thus we shall refrain from discussing them.\n12.4.4 Transaction-Oriented Buses\nUptillnow, wehavebeenonlyfocussingonunidirectionalcommunication, whereonlyonenode\ncan transmit to the other nodes at any single point of time. Let us now consider more realistic\nbuses. In reality, most high performance I\/O buses are not multidrop buses. Multidrop buses\npotentially allow multiple transmitters, albeit not at the same point of time. Modern I\/O buses\nare instead point-to-point buses, which typically have two end points. Secondly, an I\/O bus\ntypically consists of two physical buses such that we can have bidirectional communication. For\nexample, if we have an I\/O bus connecting nodes A and B. Then it is possible for them to send\nmessages to each other simultaneously.\nSome early systems had a bus that connected the processor directly to the memory. In this\ncase, the processor was designated as the master, because it could only initiate the transfer of\na bus message. The memory was referred to as the slave, which could only respond to requests.\nNowadays, the notion of a master and a slave has become diluted. However, the notion of\nconcurrent bidirectional communication is still common. A bidirectional bus is known as a\nduplex bus or full duplex bus. In comparison, we can have a half duplex bus, which only allows\none side to transmit at any point of time. (cid:13)c Smruti R. Sarangi 642\nDefinition 152\nFull Duplex Bus It is a bus that allows both of the nodes connected at its endpoints to\ntransmit data at the same time.\nHalf Duplex Bus It only allows one of its endpoints to transmit at any point of time.\nclock\nRAS\nCAS\nAddress row col row col\nCommand read read\nData ready\nData\nFigure 12.26: DRAM read timing\nLet us look at a typical scenario of duplex communication between the memory controller\nchip, and the DRAM module in Figure 12.26. Figure 12.26 shows the sequence and timing of\nmessages for a memory read operation. In practice, we have two buses. The first bus connects\nthe memory controller to the DRAM module. It consists of address lines (lines to carry the\nmemory address), and lines to carry dedicated control signals. The control signals indicate the\ntiming of operations, and the nature of operation that needs to be performed on the DRAM\narrays. The second bus connects the DRAM module to the memory controller. This contains\ndata lines (lines to carry the data read from the DRAM), and timing lines (lines to convey\ntiming information).\nThe protocol is as follows. The memory controller starts out by asserting the RAS (row\naddress strobe) signal. The RAS signal activates the decoder that sets the values of the word\nlines. Simultaneously, the memory controller places the address of the row on the address\nlines. It has an estimate of the time (t ) it takes for the DRAM module to buffer the row\nrow\naddress. After t units of time, it asserts the CAS signal (column address strobe), and\nrow 643 (cid:13)c Smruti R. Sarangi\nplaces the address of the columns in the DRAM array on the bus. It also enables the read\nsignal indicating to the DRAM module that it needs to perform a read access. Subsequently,\nthe DRAM module reads the contents of the memory locations and transfers it to its output\nbuffers. It then asserts the ready signal, and places the data on the bus. However, at this point\nof time, the memory controller is not idle. It begins to place the row address of the next request\non the bus. Note that the timing of a DRAM access is very intricate. Often the processing of\nconsecutive messages is overlapped. For example, we can proceed to decode the row address of\nthe (n+1)th request, when the nth request is transferring its data. This reduces the DRAM\nlatency. However, to support this functionality, we need a duplex bus, and a complex sequence\nof messages.\nLet us note a salient feature of the basic DRAM access protocol that we showed in Fig-\nure12.26. Here,therequestandresponseareverystronglycoupledwitheachother. Thesource\n(memory controller) is aware of the intricacies of the destination (DRAM module), and there\nis a strong interrelationship between the nature and timing of the messages sent by both the\nsource and destination. Secondly, the I\/O link between the memory controller and the DRAM\nmodule is locked for the duration of the request. We cannot service any intervening request\nbetween the original request and response. Such a sequence of messages is referred to as a bus\ntransaction.\nDefinition 153\nA bus transaction is defined as a sequence of messages on a duplex or multidrop bus by\nmore than one node, where there is a strong relationship between the messages in terms of\ntiming and semantics. It is in general not possible to send another unrelated sequence of\nmessages in the middle, and then resume sending the original sequence of messages. The\nbus is locked for the entire duration.\nThere are pros and cons of transaction oriented buses. The first is complexity. They make\na lot of assumptions regarding the timing of the receiver. Hence, the message transfer protocol\nbecomes very specific to each type of receiver. This is detrimental to portability. It becomes\nvery difficult to plug in a device that has different message semantics. Moreover, it is possible\nthat the bus might get locked for a long duration, with idle periods. This wastes bandwidth.\nHowever, in some scenarios such as the example that we showed, transaction-oriented buses\nperform very well and are preferred over other types of buses.\n12.4.5 Split Transaction Buses\nLet us now look at split transaction buses that try to rectify the shortcomings of transaction\noriented buses. Here, we do not assume a strict sequence of messages between different nodes.\nForexample,fortheDRAMandmemorycontrollerexample,webreakthemessagetransferinto\ntwosmallertransactions. First,thememorycontrollersendsthememoryrequesttotheDRAM.\nThe DRAM module buffers the message, and proceeds with the memory access. Subsequently,\nit sends a separate message to the memory controller with the data from memory. The interval\nbetween both the message sequences can be arbitrarily large. Such a bus is known as a split (cid:13)c Smruti R. Sarangi 644\ntransaction bus, which breaks a larger transaction into smaller and shorter individual message\nsequences.\nThe advantage here is simplicity and portability. All our transfers are essentially unidirec-\ntional. Wesendamessage, andthenwedonotwaitforitsreplybylockingthebus. Thesender\nproceeds with other messages. Whenever, the receiver is ready with the response, it sends a\nseparate message. Along with simplicity, this method also allows us to connect a variety of re-\nceivers to the bus. We just need to define a simple message semantics, and any receiver circuit\nthat conforms with the semantics can be connected to the bus. We cannot use this bus to do\ncomplicated operations such as overlapping multiple requests, and responses, and fine grained\ntiming control. For such requirements, we can always use a bus that supports transactions.\n12.5 Network Layer\nIn Sections 12.2,12.3, and 12.4, we studied how to design a full duplex bus. In specific, we\nlooked at signalling, signal encoding, timing, framing, error checking, and transaction related\nissues. Now, we have arrived at a point, where we can assume I\/O buses that correctly transfer\nmessages between end points, and ensure timely and correct delivery. Let us now look at the\nentire chipset, which is essentially a large network of I\/O buses.\nThe problems that we intend to solve in this section, are related to I\/O addressing. For\nexample, if the processor wishes to send a message to an USB port, then it needs to have a\nway of uniquely addressing the USB port. Subsequently, the chipset needs to ensure that it\nproperly routes the message to the appropriate I\/O device. Similarly, if a device such as the\nkeyboard, needs to send the ASCII code (see Section 2.5) of the key pressed to the processor,\nit needs to have a method of addressing the processor. We shall looking at routing messages in\nthe chipset in this section.\n12.5.1 I\/O Port Addressing\nSoftware Interface of an I\/O Port\nIn Definition 140, we defined a hardware I\/O port as a connection endpoint for an externally\nattached device. Let us now consider a software port, which we define to be an abstract entity\nthat is visible to software as a single register, or a set of registers. For example, the USB\nport physically contains a set of metallic pins, and a port controller to run the USB protocol.\nHowever, the \u201csoftware version of the USB port\u201d, is an addressable set of registers. If we wish\nto write to the USB device, then we write to the set of registers exposed by the USB port to\nsoftware. The USB port controller implements the software abstraction, by physically writing\nthe data sent by the processor to the connected I\/O device. Likewise, for reading the value sent\nby an I\/O device through the USB port, the processor issues a read instruction to the software\ninterface of the USB port. The corresponding port controller forwards the output of the I\/O\ndevice to the processor.\nLet us graphically illustrate this concept in Figure 12.27. We have a physical hardware\nport that has a set of metallic pins, and associated electrical circuitry that implements the\nphysical and data link layers. The port controller implements the network layer by fulfilling\nrequests sent by the processor. It also exposes a set of 8 to 32-bit registers. These registers 645 (cid:13)c Smruti R. Sarangi\nSoftware interface\nRegisters\nInput Output\nPort controller\nPort connector\nFigure 12.27: Software interface of an I\/O port\ncan be either read-only, write-only, or read-write. For example, the port for the display device\nsuch as the monitor contains write-only registers, because we do not get any inputs from it.\nSimilarly, the port controller of a mouse, contains read-only registers, and the port controller\nof a scanner contains read-write registers. This is because we typically send configuration data\nand commands to the scanner, and read the image of the document from the scanner.\nFor example, Intel processors define 64K (216) 8-bit I\/O ports. It is possible to fuse 4\nconsecutiveportstohavea32-bitport. Theseportsareequivalenttoregistersthatareaccessible\nto assembly code. Secondly, a given physical port such as the Ethernet port or the USB port\ncan have multiple such software ports assigned to them. For example, if we wish to write a\nlargepieceofdatatotheEthernetinonego, thenwemightusehundredsofports. Eachportin\nthe Intel processor is addressed using a 16-bit number that varies from 0 to 0xFFFF. Similarly,\nother architectures define a set of I\/O ports that act as software interfaces for actual hardware\nports.\nLet us define the term I\/O address space as the set of all the I\/O port addresses that are\naccessible to the operating system and user programs. Each location in the I\/O address space\ncorresponds to an I\/O port, which is the software interface to a physical I\/O port controller.\nDefinition 154\nTheI\/Oaddressspace is defined asthe set of all the I\/O port addresses thatare accessible to\nthe operating system and user programs. Each location in the I\/O address space corresponds\nto an I\/O port, which is the software interface to a physical I\/O port controller. (cid:13)c Smruti R. Sarangi 646\nISA Support for I\/O Ports\nMost instruction set architectures have two instructions: in and out. The semantics of the\ninstructions are as follows.\nInstruction Semantics\nin r1, (cid:104)I\/O port(cid:105) r1 \u2190 contents of (cid:104)I\/O port(cid:105)\nout r1, (cid:104)I\/O port(cid:105) contents of (cid:104)I\/O port(cid:105) \u2190 r1\nTable 12.4: Semantics of the in and out instructions\nThe in instruction transfers data from an I\/O port to a register. Conversely, the out\ninstruction transfers data from a register to an I\/O port. This is a very generic and versatile\nmechanism for programming I\/O devices. For example, if we want to print a page, then we\ncan transfer the contents of the entire page to the I\/O ports of the printer. Finally, we write\nthe print command to the I\/O port that accepts commands for the printer. Subsequently, the\nprinter can start printing.\nRouting Messages to I\/O Ports\nLet us now implement the in and out instructions. The first task is to ensure that a message\nreaches the appropriate port controller, and the second task is to route the response back to\nthe processor in the case of an out instruction.\nLet us again take a look at the architecture of the motherboard in Figure 12.4. The CPU is\nconnected to the North Bridge chip via the front side bus. The DRAM memory modules, and\nthe graphics card are also connected to the North Bridge chip. Additionally, the North Bridge\nchip is connected to the South Bridge chip that handles slower devices. The South Bridge chip\nis connected to the USB ports, the PCI Express Bus (and all the devices connected to it), the\nhard disk, the mouse, keyboard, speakers and the network card. Each of these devices has a\nset of associated I\/O ports, and I\/O port numbers.\nTypically the motherboard designers have a scheme for allocating I\/O ports. Let us try\nto construct one such scheme. Let us suppose that we have 64K 8-bit I\/O ports like the Intel\nprocessors. The addresses of the I\/O ports thus range from 0 to 0xFFFF. Let us first allocate\nI\/O ports to high-bandwidth devices that are connected to the North Bridge chip. Let us give\nthem port addresses in the range of 0 to 0x00FF. Let us partition the rest of the addresses for\nthe devices connected to the South Bridge chip. Let us assume that the hard disk has a range\nof ports from 0x0100 to 0x0800. Let the USB ports have a range from 0x0801 to 0x0FFF. Let\nus assign the network card the following range: 0x1000 to 0x4000. Let us assign a few of the\nremaining ports to the rest of the devices, and keep a part of the range empty for any new\ndevices that we might want to attach later.\nNow, when the processor issues an I\/O instruction (in or out), the processor recognises that\nit is an I\/O instruction, sends the I\/O port address, and the instruction type to the North\nBridge chip through the FSB (front side bus). The North Bridge chip maintains a table of\nranges for each I\/O port type, and their locations. Once it sees the message from the processor,\nitaccessesthistableandfindsouttherelativelocationofthedestination. Ifthedestinationisa 647 (cid:13)c Smruti R. Sarangi\ndevice that is directly connected to it, then the North Bridge chip forwards the message to the\ndestination. Otherwise, it forwards the request to the South Bridge chip. The South Bridge\nchip maintains a similar table of I\/O port ranges, and device locations. After performing a\nlookup in this table, it forwards the received message to the appropriate device. These tables\nare called I\/O routing tables. I\/O routing tables are conceptually similar to network routing\ntables used by large networks and the internet.\nFor the reverse path, the response is typically sent to the processor. We assign a unique\nidentifier to the processor, and the messages gets routed appropriately by the North Bridge and\nSouth Bridge chips. Sometimes it is necessary to route the message to the memory modules\n(see Section 12.6.3). We use a similar addressing scheme.\nThis scheme essentially maps the set of physical I\/O ports to locations in the I\/O address\nspace, and the dedicated I\/O instructions use the port addresses to communicate with them.\nThis method of accessing and addressing I\/O devices is commonly known as I\/O mapped I\/O.\nDefinition 155 I\/O mapped I\/O is a scheme for addressing and accessing I\/O devices by\nassigning each physical I\/O port a unique address in the I\/O space, and by using dedicated\nI\/O instructions to transfer data to\/from locations in the I\/O address space.\n12.5.2 Memory Mapped Addressing\nLet us now take a look at the in and out I\/O instructions again. The executing program needs\nto be aware of the naming schemes for I\/O ports. It is possible that different chipsets and\nmotherboards use different addresses for the I\/O ports. For example, one motherboard might\nassigntheUSBportstheI\/Oportaddressrange,0xFF80to0xFFC0,andanothermotherboard\nmight assign the range, 0xFEA0 to 0xFFB0. Consequently, a program that runs on the first\nmotherboard might not work on the second motherboard.\nTo solve this issue, we need to add an additional layer between the I\/O ports and software.\nLet us propose a solution similar to virtual memory. In fact, virtualisation is a standard\ntechnique for solving various problems in computer architecture. Let us proceed to design a\nvirtual layer between user programs and the I\/O address space.\nLetusassumethatwehaveadedicateddevicedriverintheoperatingsystemthatisspecific\nto the chipset and motherboard. It needs to be aware of the semantics of the I\/O ports, and\ntheir mappings to actual devices. Now, let us consider a program (user program or OS) that\nwishes to access the USB ports. At the outset, it is not aware about the I\/O port addresses\nof the USB ports. Hence, it needs to first request the relevant module in the operating system\nto map a memory region in its virtual address space to the relevant portion of the I\/O address\nspace. For example, if the I\/O ports for the USB devices are between 0xF000 to 0xFFFF, then\nthis 4 KB region in the I\/O address space can be mapped to a page in the program\u2019s virtual\naddress space. We need to add a special bit in the TLB and page table entries to indicate that\nthis page actually maps to I\/O ports. Secondly, instead of storing the address of the physical\nframe, weneedtostoretheI\/Oportaddresses. Itistheroleofthemotherboarddriverthatisa\na part of the operating system to create this mapping. After the operating system has mapped\nthe I\/O address space to a process\u2019s virtual address space, the process can proceed with the (cid:13)c Smruti R. Sarangi 648\nI\/O access. Note that before creating the mapping, we need to ensure that the program has\nsufficient privileges to access the I\/O device.\nAfter the mapping has been created, the program is free to access the I\/O ports. Instead\nof using I\/O instructions such as in, and out, it uses regular load and store instructions to\nwrite to locations in its virtual address space. After such instructions reach the memory access\n(MA) stage of the pipeline, the effective address is sent to the TLB for translation. If there is a\nTLB hit, then the pipeline also becomes aware of the fact that the virtual address maps to the\nI\/O address space rather than the physical address space. Secondly, the TLB also translates\nthe virtual address to an I\/O port address. Note that at this stage it is not necessary to use\nthe TLB, we can use another dedicated module to translate the address. In any case, the\nprocessor receives the equivalent I\/O port address in the MA stage. Subsequently, it creates an\nI\/O request and dispatches the request to the I\/O port. This part of the processing is exactly\nsimilar to the case of I\/O mapped I\/O.\nDefinition 156 Memory mapped I\/O is a scheme for addressing and accessing I\/O devices\nby assigning each address in the I\/O address space to a unique address in the process\u2019s\nvirtual address space. For accessing an I\/O port, the process uses regular load and store\ninstructions.\nThisschemeisknownasmemorymappedI\/O.Itsmainadvantageisthatitusesregularload\nand store instructions to access I\/O devices instead of dedicated I\/O instructions. Secondly,\nthe programmer need not be aware of the actual addresses of the I\/O ports in the I\/O address\nspace. Since dedicated modules in the operating system, and the memory system, set up a\nmapping between the I\/O address space and the process\u2019s virtual address space, the program\ncan be completely oblivious of the semantics of addressing the I\/O ports.\n12.6 Protocol Layer\nLet us now discuss the last layer in the I\/O system. The first three layers ensure that a message\nis correctly delivered from one device to another in the I\/O system. Let us now look at the\nlevel of a complete I\/O request such as printing an entire page, scanning an entire document,\nor reading a large block of data from the hard disk. Let us consider the example of printing a\ndocument.\nAssume that the printer is connected to an USB port. The printer device driver starts out\nby instructing the processor to send the contents of the document to the buffers associated with\nthe USB port. Let us assume that each such buffer is assigned a unique port address, and the\nentire document fits within the set of buffers. Moreover, let us assume that the device driver\nis aware that the buffers are empty. To send the contents of the document, the device driver\ncan use a sequence of out instructions, or can use memory mapped I\/O. After transferring\nthe contents of the document, the last step is to write the PRINT command to a pre-specified\nI\/O port. The USB controller manages all the I\/O ports associated with it, and ensures that\nmessages sent to these ports are sent to the attached printer. The printer starts the job of\nprinting after receiving the PRINT command from the USB controller. 649 (cid:13)c Smruti R. Sarangi\nLet us now assume that the user clicks the print button for another document. Before\nsending the new document to the printer, the driver needs to ensure that the printer has\nfinished printing the previous document. The assumption here is that we have a simple printer\nthat can only handle one document at a time. There should thus be a method for the driver to\nknow if the printer is free.\nBefore looking at different mechanisms for the printer to communicate with its driver, let\nus consider an analogy. Let us consider a scenario in which Sofia is waiting for a letter to be\ndelivered to her. If the letter is being sent through one of Sofia\u2019s friends, then Sofia can keep\ncalling her friend to find out when she will be back in town. Once she is back, Sofia can go to\nher house, and collect the letter. Alternatively, the sender can send the letter through a courier\nservice. In this case, Sofia simply needs to wait for the courier delivery boy to come and deliver\nthe letter. The former mechanism of receiving messages is known as polling, and the latter is\nknown as interrupts. Let us now elaborate.\n12.6.1 Polling\nLet us assume that there is a dedicated register called the status register in the printer that\nmaintains the status of the printer. Whenever there is a change in the status of the printer, it\nupdates the value of the status register. Let us assume that the status register can contain two\nvalues namely 0 (free) and 1 (busy). When the printer is printing a document, the value of the\nstatus register is 1 (busy). Subsequently, when the printer completes printing the document, it\nsets the value of the status register to 0 (free).\nNow, let us assume that the printer driver wishes to read the value of the status register\nof the printer. It sends a message to the printer asking it for the value of the status register.\nThe first step in sending a message is to send a sequence of bytes to the relevant I\/O ports of\nthe USB port controller. The port controller in turn sends the bytes to the printer. If it uses a\nsplit transaction bus, then it waits for the response to arrive. Meanwhile, the printer interprets\nthe message, and sends the value of the status register as the response, which the USB port\ncontroller forwards to the processor through the I\/O system.\nIf the printer is free, then the device driver can proceed to print the next document. Oth-\nerwise, it needs to wait for the printer to finish. It can keep on requesting the printer for its\nstatus till it is free. This method of repeatedly querying a device for its state till its state has\na certain value is called polling.\nDefinition 157\nPolling is a method for waiting till an I\/O device reaches a given state. It is implemented\nby repeatedly querying the device for its state in a loop.\nLet us show a snippet of SimpleRisc code that implements polling in a hypothetical system.\nWe assume that the message for getting the status of the printer is 0xDEADBEEF. We need to\nfirst send the message to the I\/O port 0xFF00, and then subsequently read the response from\nthe I\/O port 0xFF04. (cid:13)c Smruti R. Sarangi 650\nAssembly Code for Polling\n\/* load DEADBEEF in r0 *\/\nmovh r0, 0xDEAD\naddu r0, r0, 0xBEEF\n\/* polling loop *\/\n.loop:\nout r0, 0xFF00\nin r1, 0xFF04\ncmp r1, 1\nbeq .loop \/* keep looping till status = 1 *\/\n12.6.2 Interrupts\nThere are several shortcomings of the polling based approach. It keeps the processor busy,\nwastes power, and increases I\/O traffic. We can use interrupts instead. Here, the idea is to\nsend a message to the printer to notify the processor when it becomes free. After the printer\nbecomes free, or if it is already free, the printer sends an interrupt to the processor. The I\/O\nsystem typically treats the interrupt as a regular message. It then delivers the interrupt to\nthe processor, or a dedicated interrupt controller. These entities realise that an interrupt has\ncome from the I\/O system. Subsequently, the processor stops executing the current program\nas described in Section 9.8, and jumps to the interrupt handler.\nNote that every interrupt needs to identify itself, or the device that has generated it. Every\ndevice that is on the motherboard typically has a unique code. This code is a part of the\ninterrupt. In some cases, when we connect devices to generic ports such as the USB port, the\ninterrupt code contains two parts. One part is the address of the port on the motherboard that\nis connected to the external device. The other part is an id that is assigned to the device by\nthe I\/O port on the motherboard. Such interrupts that contain a unique code are known as\nvectored interrupts.\nInsomesystemssuchasx86machines,thefirststageofinterruptprocessingisdonebyapro-\ngrammable interrupt controller (PIC). These interrupt controllers are called APICs (advanced\nprogrammable interrupt controllers) in x86 processors. The role of these interrupt controllers\nis to buffer interrupt messages, and send them to the processor according to a set of rules.\nLet us take a look at the set of rules that PICs follow. Most processors disable interrupt\nprocessing during some critical stages of computation. For example, when an interrupt handler\nis saving the state of the original program, we cannot allow the processor to get interrupted.\nAfter the state is successfully saved, interrupt handlers might re-enable interrupts. In some\nsystems, interruptsarecompletelydisabledwheneveraninterrupthandlerisrunning. Aclosely\nrelated concept is interrupt masking that selectively enables some interrupts, and disables some\nother interrupts. For example, we might allow high priority interrupts from the temperature\ncontroller during the processing of an interrupt handler, and choose to temporarily ignore low\npriority interrupts from the hard disk. The PIC typically has a vector that has one entry per\ninterrupt type. It is known as the interrupt mask vector. For an interrupt, if the corresponding\nbit in the interrupt mask vector is 1, then the interrupt is enabled, otherwise it is disabled.\nLastly, PICsneedtorespectthepriorityofinterruptsifwehavemultipleinterruptsarriving 651 (cid:13)c Smruti R. Sarangi\nin the same window of time. For example, interrupts from a device with real time constraints\nsuch as an attached high speed communication device have a high priority, whereas keyboard\nand mouse interrupts have lower priority. The PIC orders interrupts using heuristics that take\ninto account their priority and time of arrival, and presents them to the processor in that\norder. Subsequently, the processor processes the interrupt according to the methods explained\nin Section 9.8.\nDefinition 158\nVectored Interrupt An interrupt that contains the id of the device that generated it, or\nthe I\/O port address that is connected to the external device.\nProgrammable Interrupt Controller(PIC) A dedicated module called the pro-\ngrammable interrupt controller (PIC) buffers, filters, and manages the interrupts sent\nto a processor.\nInterrupt Masking Theuser, oroperatingsystemcanchoosetoselectivelydisableasetof\ninterruptsatsomecriticalphasesofprogramssuchaswhilerunningdevicedriversand\ninterrupt handlers. This mechanism is known as interrupt masking. The interrupt\nmask vector in the PIC is typically a bit vector (one bit per each interrupt type). If a\nbit is set to 1, then the interrupt is enabled, otherwise it is disabled, and the interrupt\nwill either be ignored, or buffered in the PIC and processed later.\n12.6.3 DMA\nFor accessing I\/O devices, we can use both polling and interrupts. In any case, for each I\/O\ninstruction we transfer typically 4 bytes at a time. This means that if we need to transfer a\n4KB block to an I\/O device, we need to issue 1024 out instructions. Similarly, if we wish to\nread in 4 KB of data, we need to issue 1024 in instructions. Each I\/O instruction typically\ntakes more than ten cycles, because it reaches an I\/O port after several levels of indirection.\nSecondly, the frequency of I\/O buses is typically a third to a quarter of the processor frequency.\nThus, I\/O for large blocks of data is a fairly slow process, and it can keep the processor busy\nfor a long time. Our objective is to keep sensitive code such as device drivers and interrupt\nhandlers as short as possible.\nHence, let us try to devise a solution that can offload some of the work of the processor.\nLet us consider an analogy. Let us assume that a professor is teaching a class of more than 100\nstudents. After an exam, she needs to grade more than 100 scripts. This will keep her busy for\nat least a week, and the process of grading scripts is a very tiring and time consuming process.\nHence, she can offload the work of grading exam scripts to teaching assistants. This will ensure\nthat the professor has free time, and she can focus on solving state of the art research problems.\nWe can take cues from this example, and design a similar scheme for processors.\nLet us envision a dedicated unit called a DMA (direct memory access) engine that can do\nsome work on behalf of the processor. In specific, if the processor wishes to transfer a large (cid:13)c Smruti R. Sarangi 652\namount of data in memory to an I\/O device, or vice versa, then instead of issuing a large\nnumber of I\/O instructions, the DMA engine can take over the responsibility. The procedure\nfor using a DMA engine is as follows. At the outset, the device driver program, determines\nthat there is a necessity to transfer a large amount of data between memory and an I\/O device.\nSubsequently, it sends the details of the memory region (range of bytes), and the details of\nthe I\/O device (I\/O port addresses) to the DMA engine. It further specifies, whether the data\ntransfer is from memory to I\/O or in the reverse direction. Subsequently, the device driver\nprogram suspends itself, and the processor is free to run other programs. Meanwhile, the DMA\nengine or the DMA controller begins the process of transferring data between the main memory\nand I\/O devices. Depending on the direction of the transfer, it reads the data, temporarily\nbuffers it, and sends it to the destination. Once the transfer is over, it sends an interrupt to\nthe processor indicating that the transfer is over. Subsequently, the device driver of the I\/O\ndevice is ready to resume operation and complete any remaining steps.\nTheDMAbasedapproachistypicallyusedbymodernprocessorstotransferalargeamount\nof data between main memory, and the hard disk, or the network card. The transfer of data is\ndone in the background, and the processor is mostly oblivious of this process. Secondly, most\noperating systems have libraries to program the DMA engine to perform data transfers.\nThere are two subtle points that need to be discussed in the context of DMA engines. The\nfirst is that the DMA controller needs to occasionally become the bus master. In most designs,\ntheDMAengineistypicallyapartoftheNorthBridgechip. TheDMAengineneedstobecome\nthe bus master of the bus to memory, and the bus to the South Bridge chip, when required. It\ncan either transfer all the data in one go (also known as a burst), or it can wait for idle periods\nin the bus, and use these cycles to schedule its own transfers. The former approach is known\nas the burst mode, and the latter approach is known as the cycle stealing mode.\nThe second subtle point is that there might be correctness issues if we are not careful. For\nexample, it is possible that we have a given location in the cache, and simultaneously, the\nDMA engine is writing to the location in main memory. In this case, the value in the cache\nwill become stale, and sadly, the processor will have no way of knowing this fact. Hence, it is\nimportant to ensure that locations accessed by DMA controllers are not present in the cache.\nThis is typically achieved through a dedicated piece of logic called a DMA snoop circuit that\ndynamically evicts locations present in the cache, if they are written to by the DMA engine.\n12.7 Case Studies \u2013 I\/O Protocols\nIn this section, we shall describe the operation of several state of the art I\/O protocols. We\nshall provide a brief overview of each of these protocols in this book. For a detailed study, or\nwherever there is a doubt, the reader should take a look at their formal specifications posted\non the web. The formal specifications are typically released by a consortium of companies\nthat support the I\/O protocol. Most of the material that we present is sourced from these\nspecifications. 653 (cid:13)c Smruti R. Sarangi\n12.7.1 PCI Express(cid:13)R\nOverview\nMost motherboards require local buses that can be used to attach devices such as dedicated\nsound cards, network cards, and graphics cards to the North Bridge or South Bridge chips.\nIn response to this requirement, a consortium of companies created the PCI (Peripheral Com-\nponent Interconnect) bus specification in 1993. In 1996, Intel created the AGP (Accelerated\nGraphicsPort)busforconnectinggraphicscards. Inthelatenineties,manynewbustypeswere\nbeing proposed for connecting a variety of hardware devices to the North Bridge and South\nBridge chips. Designers quickly realised that having many different bus protocols hampers\nstandardisation efforts, and compels device vendors to support multiple bus protocols. Hence,\na consortium of companies started a standardisation effort, and created the PCI Express bus\nstandard in 2004. This technology superseded most of the earlier technologies, and till date it\nis the most popular bus on the motherboard.\nThe basic idea of the PCI express bus is that it is a high speed point to point serial (single\nbit) interconnect. A point to point interconnect has only two end points. To connect multiple\ndevices to the South Bridge chip, we create a tree of PCI express devices. The internal nodes\nof the tree are PCI express switches that can multiplex traffic from multiple devices. Secondly,\nas compared to older protocols, each PCI Express bus sends bits serially on a single bit line.\nTypically high speed buses avoid transmitting multiple bits in parallel using several copper\nwires, because different links experience different degrees of jitter and signal distortion. It\nbecomes very hard to keep all the signals in the different wires in synchrony with each other.\nHence, modern buses are mostly serial.\nA single PCI Express bus is actually composed of many individual serial buses known as\nlanes. Eachlanehasitsseparatephysicallayer. APCIExpresspacketisstripedacrossthelanes.\nStriping means dividing a block of data (packet) into smaller blocks of data and distributing\nthem across the lanes. For example, in a bus with 8 lanes, and a 8-bit packet, we can send\neach bit of the packet on a separate lane. The reader needs to note that sending multiple bits\nin parallel across different lanes is not the same as a parallel bus that has multiple wires to\nsend data. This is because a parallel bus has one physical layer circuit for all the copper wires,\nwhereasinthiscase, eachlanehasitsseparatesynchronisation, andtiming. Thedatalinklayer\ndoes the job of framing by aggregating the subparts of each packet collected from the different\nlanes.\nDefinition 159\nThe process of striping refers to dividing a block of data into smaller blocks of data and\ndistributing them across a set of entities.\nA lane consists of two LVDS based wires for full duplex signalling. One wire is used to send\na message from the first end point to the second, and the second wire is to send a signal in the\nreverse direction. A set of lanes are grouped together to form an I\/O link that is assumed to\ntransfer a full packet (or frame) of data. The physical layer then transfers a packet to the data\nlink layer that performs error correction, flow control, and implements transactions. The PCI (cid:13)c Smruti R. Sarangi 654\nExpress protocol is a layered protocol, where the functionality of each layer is roughly similar\nto the I\/O layers that we have defined. Instead of considering transactions to be a part of the\ndata link layer, it has a separate transaction layer. We shall however use the terminology that\nwe have defined in this chapter for explaining all the I\/O protocols unless mentioned otherwise.\nSummary\nPCI Express (Peripheral Component Interconnect Express)\nUsage As a motherboard bus\nSpecification [pci, ]\nTopology\nConnection Point to point with multiple lanes\nLane A single bit full duplex channel with data striping\nNumber of Lanes 1 \u2013 32\nPhysical Layer\nSignalling LVDS based differential signalling\nEncoding 8 bit\/ 10 bit\nTiming Source synchronous\nData Link Layer\nFrame Size 1 byte\nError Correction 32-bit CRC\nTransactions Split transaction bus\nBandwidth 250 MB\/s per lane\nNetwork Layer\nRouting Nodes Switches\nTable 12.5: The PCI Express I\/O Protocol\nA summary of the specifications of the PCI Express protocol is shown in Table 12.5. We\ncan have 1-32 lanes. Here, each lane is an asynchronous bus, which uses a sophisticated version\nof data encoding called the 8bit\/10bit encoding. The 8bit\/10bit encoding can be conceptually\nthoughtofasanextensionoftheNRZprotocol. Itmapsasequenceof8logicalbitstoasequence\nof 10 physical bits. It ensures that we do not have more than five 1s or 0s consecutively such\nthat we can efficiently recover the clock. Recall that the receiver recovers the sender\u2019s clock\nby analysing transitions in the data. Secondly, the encoding ensures that we have almost the\nsame number of physical 1s and 0s in the transmitted signal. In the data link layer the PCI\nExpress protocol implements a split transaction bus with a 1-128 byte frame, and 32-bit CRC\nbased error correction.\nThe PCI Express bus is normally used to connect generic I\/O devices. Sometimes some\nslots are left unused such that users can later connect cards for their specific applications. For\nexample, if a user is interested in working with specialised medical devices, then she can attach\nan I\/O card that can connect with medical devices externally, and to the PCI Express bus\ninternally. Such free PCI Express slots are known as expansion slots. 655 (cid:13)c Smruti R. Sarangi\n12.7.2 SATA\nOverview\nLet us now take a look at a bus, which was primarily developed for connecting storage devices\nsuch as hard disks, and optical drives. Since the mid eighties, designers, and storage vendors,\nbegandesigningsuchbuses. SeveralsuchbusesdevelopedovertimesuchastheIDE(Integrated\nDrive Electronics) and PATA (Parallel Advanced Technology Attachment) buses. These buses\nwere predominantly parallel buses, and their constituent communication links suffered from\ndifferent amounts of jitter and distortion. Thus, these technologies got replaced by a serial\nstandard known as SATA (Serial ATA), that is a point to point link like PCI Express.\nThe SATA protocol for accessing storage devices is now used in an overwhelming majority\nof laptop and desktop processors. It has become the de facto standard. The SATA protocol\nhas three layers \u2013 physical, data link, and transport. We map the transport layer of the SATA\nprotocoltoourprotocollayer. EachSATAlinkcontainsapairofsingle-bitlinksthatuseLVDS\nsignalling. Unlike PCI Express, it is not possible for an end point in the SATA protocol to read\nand write data at the same time. Only one of the actions can be performed at any point of\ntime. It is thus a half duplex bus. It uses 8b\/10b encoding, and it is an asynchronous bus. The\ndata link layer does the job of framing. Let us now discuss the network layer. Since SATA is\na point to point protocol, a set of SATA devices can be connected in a tree structure. Each\ninternal node of the tree is know as a multiplier. It routes requests from the parent to one of\nits children, of from one of its children to its parent. Finally, the protocol layer acts on the\nframes and ensures that they are transmitted in the correct sequence, and implements SATA\ncommands. In specific, it implements DMA requests, accesses the storage devices, buffers data,\nand sends it to the processor in a predefined order.\nSummary\nTable 12.6 shows the specification of the SATA protocol. We need to note that the SATA\nprotocol has a very rich protocol layer. It defines a wide variety of commands for storage based\ndevices. For example, it has dedicated commands to perform DMA accesses, perform direct\nhard disk accesses, encode and encrypt data, and control the internals of storage devices. The\nSATA bus is a split transaction bus, and the data link layer differentiates between commands\nand their responses. The protocol layer implements the semantics of all the commands.\n12.7.3 SCSI and SAS\nOverview of SCSI\nLet us now discuss another I\/O protocol also meant for peripheral devices known as the SCSI\nprotocol (pronounced as \u201cscuzzy\u201d). SCSI was originally meant to be a competitor of PCI.\nHowever, over time the SCSI protocol metamorphosed to a protocol for connecting storage\ndevices.\nThe original SCSI bus was a multidrop parallel bus that could have 8 to 16 connections.\nThe SCSI protocol differentiates between a host and a peripheral device. For example, the\nSouth Bridge chip is a host, whereas the controller of a CD drive is a peripheral. Any pair of\nnodes (host or peripheral) can communicate between each other. The original SCSI bus was (cid:13)c Smruti R. Sarangi 656\nSATA (Serial ATA)\nUsage Used to connect storage devices such as hard disks\nSource [sat, ]\nTopology\nConnection Point to point, half duplex\nTopology Tree based, internal nodes known as multipliers\nPhysical Layer\nSignalling LVDS based differential signalling\nNumber of parallel links 4\nEncoding 8 bit\/ 10 bit\nTiming Asynchronous (clock recovery + comma symbols)\nData Link Layer\nFrame Size variable\nError Correction CRC\nTransactions Split transaction bus, command driven\nBandwidth 150-600 MB\/s\nNetwork Layer\nRouting Nodes Multipliers\nProtocol Layer\nEach SATA node has dedicated support for processing commands, and their\nresponses. Examples of commands can be DMA reads, or I\/O transfers\nTable 12.6: The SATA Protocol\nsynchronous and ran at a relatively low frequency as compared to today\u2019s high speed buses.\nSCSI has still survived till date and state of the art SCSI buses use a 80-160 MHz clock to\ntransmit 16 bits in parallel. They thus have a theoretical maximum bandwidth of 320-640\nMB\/s. Note that serial buses can go up till 1 GHz, are more versatile, and can support larger\nbandwidths.\nGiventhefactthatthereareissueswithmultidropparallelbuses,designersstartedretarget-\nting the SCSI protocol for point to point serial buses. Recall that PCI Express and SATA buses\nwere also created for the same reason. Consequently, designers proposed a host of buses that\nextended the original SCSI protocols, but were essentially point to point serial buses. Two such\nimportant technologies are the SAS (Serially Attached SCSI), and FC (fibre channel) buses.\nFC buses are mainly used for very high end systems such as supercomputers. The SAS bus is\nmore commonly used for enterprise and scientific applications.\nLetusthusprimarilyfocusontheSASprotocol,becauseitisthemostpopularvariantofthe\nSCSIprotocolinusetoday. SASisaserialpointtopointtechnologythatisalsocompatiblewith\nprevious versions of SATA based devices, and its specification is very close to the specification\nof SATA. 657 (cid:13)c Smruti R. Sarangi\nOverview of SAS\nSAS was designed to be backward compatible with SATA. Hence, both the protocols are not\nverydifferentinthephysicalanddatalinklayers. However,therearestillsomedifferences. The\nbiggest difference is that SAS allows full duplex transmission, whereas SATA allows only half\nduplex transmission. Secondly, SAS can in general support larger frame sizes, and it supports\na larger cable length between the end points as compared to SATA (8m for SAS, as compared\nto 1m for SATA).\nThe network layer is different from SATA. Instead of using a multiplier (used in SATA),\nSAS uses a much more sophisticated structure known as an expander for connecting to multiple\nSAS targets. Traditionally, the bus master of a SAS bus is known as the initiator, and the\nother node is known as the target. There are two kinds of expanders \u2013 edge expander, and\nfanout expander. An edge expander can be used to connect up to 255 SAS devices, and a fanout\nexpander can be used to connect up to 255 edge expanders. We can add a large number of\ndevices in a tree based topology using a root node, and a set of expanders. Each device at\nboot up time is assigned a unique SCSI id. A device might further be subdivided into several\nlogical partitions. For example, your author at this moment is working on a storage system\nthat is split into two logical partitions. Each partition has a logical unit number (LUN). The\nrouting algorithm is as follows. The initiator sends a command to either a device directly if\nthere is a direct connection or to an expander. The expander has a detailed routing table that\nmaintains the location of the device as a function of its SCSI id. It looks up this routing table\nand forwards the packet to either the device, or to an edge expander. This edge expander has\nanother routing table, which it uses to forward the command to the appropriate SCSI device.\nTheSCSIdevicethenforwardsthecommandtothecorrespondingLUN.Forsendingamessage\nto another SCSI device, or to the processor, a request follows the reverse path.\nLastly, the protocol layer is very flexible for SAS buses. It supports three kinds of protocols.\nWe can either use SATA commands, SCSI commands, or SMP (SAS Management Protocol)\ncommands. SMP commands are specialised commands for configuring and maintaining the\nnetwork of SAS devices. The SCSI command set is very extensive, and is designed to control\na host of devices (mostly storage devices). Note that a device has to be compatible with the\nSCSI protocol layer before we can send SCSI commands to it. If a device does not understand\na certain command, then there is a possibility that something catastrophic might happen. For\nexample, if we wish to read a CD, and the CD driver does not understand the command, then\nit might eject the CD. Even worse, it is possible that it might never eject the CD because\nit does not understand the eject command. The same argument holds true for the case of\nSATA also. We need to have SATA compatible devices such as SATA compatible hard drives\nand SATA compatible optical drives, if we wish to use SATA commands. SAS buses are by\ndesign compatible with both SATA devices and SAS\/SCSI devices because of the flexibility of\nthe protocol layer. For the protocol layer, SAS initiators send SCSI commands to SAS\/SCSI\ndevices, and SATA commands to SATA devices.\nNearline SAS (NL-SAS) drives are essentially SATA drives, but have a SCSI interface that\ntranslates SCSI commands to SATA commands. NL-SAS drives can thus be seamlessly used on\nSAS buses. Since the SCSI command set is more expressive and more efficient, NL-SAS drives\nare 10-20% faster than pure SATA drives.\nLet us now very briefly describe the SCSI command set in exactly 4 sentences. The initiator (cid:13)c Smruti R. Sarangi 658\nbegins by sending a command to the target. Each command has a 1-byte header, and it has\na variable length payload. The target then sends a reply with the execution status of the\ncommand. The SCSI specifications defines at least 60 different commands for device control,\nandtransferringdata. Foradditionalinformation,thereaderscanlookuptheSCSIspecification\nat [scs, ].\n12.7.4 USB\nOverview\nLet us now consider the USB protocol, which was primarily designed for connecting external\ndevices to a laptop or desktop computer such as keyboards, mice, speakers, web cameras, and\nprinters. In the mid nineties vendors realised that there are many kinds of I\/O bus protocols\nand connectors. Consequently, motherboard designers, and device driver writers were finding\nit hard to support a large range of devices. There was thus a need for standardisation. Hence,\na consortium of companies (DEC, IBM, Intel, Nortel, NEC, and Microsoft) conceived the USB\nprotocol (Universal Serial Bus).\nThemainaimoftheUSBprotocolwastodefineastandardinterfaceforallkindsofdevices.\nThe designers started out by classifying devices into three types namely low speed (keyboards,\nmice), full speed (high definition audio), and high speed (scanners, and video cameras). Three\nversions of the USB protocol have been proposed till 2012 namely versions 1.0, 2.0, and 3.0.\nThe basic USB protocol is more or less the same. The protocols are backward compatible. This\nmeans that a modern computer that has a USB 3.0 port supports USB 1.0 devices. Unlike the\nSASorSATAprotocolsthataredesignedforaspecificsetofhardware, andcanthusmakealot\nof assumptions regarding the behaviour of the target device, the USB protocol was designed to\nbe very generic. Consequently, designers needed to provide extensive support for the operating\nsystem to discover the type of the device, its requirements, and configure it appropriately.\nSecondly, a lot of USB devices do not have their power source such as keyboards and mice. It\nis thus necessary to include a power line for running connected devices in the USB cable. The\ndesigners of the USB protocol kept all of these requirements in mind.\nFrom the outset, the designers wanted USB to be a fast protocol that could support high\nspeed devices such as high definition video in the future. They thus decided to use a point to\npointserialbus(similartoPCIExpress, SATA,andSAS).Everylaptop, desktop, andmidsized\nserver, has an array of USB ports on the front or back panels. Each USB port, is considered a\nhost that can connect with a set of USB devices. Since we are using serial links, we can create\na tree of USB devices similar to trees of PCI Express and SAS devices. Most of the time we\nconnect only one device to a USB port. However, this is not the only configuration. We can\nalternatively connect a USB hub, which acts like an internal node of the tree. An USB hub is\nin principle similar to a SATA multiplier and SAS expander.\nA USB hub is most of the time a passive device, and typically has four ports to connect\nto other devices and hubs downstream. The most common configuration for a hub consists of\none upstream port (connection to the parent), and four downstream ports. We can in this\nmanner create a tree of USB hubs, and connect multiple devices to a single USB host on the\nmotherboard. TheUSBprotocolsupports127devicesperhost, andwecanatthemostconnect\n5 hubs serially. Hubs can either be powered by the host, or be self powered. If a hub is self\npowered it can connect more devices. This is because, the USB protocol has a limit on the 659 (cid:13)c Smruti R. Sarangi\namount of current that it can deliver to any single device. At the moment, it is limited to 500\nmA, and power is allocated in blocks of 100 mA. Hence, a hub that is powered by the host can\nhave at the most 4 ports because it can give each device 100 mA, and keep 100 mA for itself.\nOccasionally, a hub needs to become an active device. Whenever, a USB device is disconnected\nfrom a hub, the hub detects this event, and sends a message to the processor.\nLayers of the USB Protocol\nPhysical Layer\nLet us now discuss the protocol in some more detail, and start with the physical layer. The\nstandard USB connector has 4 pins. The first pin is a power line that provides a fixed 5V DC\nvoltage. It is typically referred to as V of V . We shall use V . There are two pins namely\ncc bus cc\nD+ and D\u2212 for differential signalling. Their default voltage is set to 3.3V. The fourth pin is\nthe ground pin (GND). The mini and micro USB connectors have an additional pin called ID\nthat helps differentiate between a connection to the host, and to a device.\nThe USB protocol uses differential signalling. It uses a variant of the NRZI protocol. For\nencoding logical bits, it assumes that a logical 0 is represented by a transition in physical bits,\nwhereas a logical 1 is represented by no transitions (reverse of the traditional NRZI protocol).\nA USB bus is an asynchronous bus that recovers the clock. To aid in clock recovery, the syn-\nchronisation sublayer introduces dummy transitions if there are no transitions in the data. For\nexample, if we have a continuous run of 1s, then there will be no transitions in the transmitted\nsignal. In this case, the USB protocol introduces a 0 bit after every run of six 1s. This strategy\nensures that we have some guaranteed transitions in the signal, and the receiver can recover the\nclock of the transmitter without falling out of synchrony. The USB connectors only have one\npair of wires for differential signalling. Hence, full duplex signalling is not possible. Instead,\nUSB links use half duplex signalling.\nData Link Layer\nFor the data link layer, the USB protocol uses CRC based error checking, and variable frame\nlengths. It uses bit stuffing (dedicated frame begin and end symbols) to demarcate frame\nboundaries. Arbitration is a rather complex issue in USB hubs. This is because, we have many\nkinds of traffic and many kinds of devices. The USB protocol defines four kinds of traffic.\nControl Control messages that are used to configure devices.\nInterrupt A small amount of data that needs to be sent to a device urgently.\nBulk A large amount of data without any guarantees of latency and bandwidth. E.g., image\ndata in scanners.\nIsochronous A fixed rate data transfer with latency and bandwidth guarantees. E.g., au-\ndio\/video in web cameras.\nAlong with the different kinds of traffic, we have different categories of USB devices namely\nlowspeeddevices(192KB\/s), fullspeeddevices(1.5MB\/s), andhighspeeddevices(60MB\/s).\nThe latest USB 3.0 protocol has also introduced super speed devices that require up to 384 (cid:13)c Smruti R. Sarangi 660\nMB\/s. However, this category is still not very popular (as of 2012); hence, we shall refrain from\ndiscussing it.\nNow, it is possible to have a high-speed and a low-speed device connected to the same hub.\nLet us assume that the high speed device is doing a bulk transfer, and the low speed device is\nsending an interrupt. In this case, we need to prioritise the access to the upstream link of the\nhub. Arbitration is difficult because we need to conform to the specifications of each class of\ntraffic and each class of devices. We have a dilemma between performing the bulk transfer, and\nsendingtheinterrupt. Wewouldideallyliketostrikeabalancebetweenconflictingrequirements\nby having different heuristics for traffic prioritisation. A detailed explanation of the arbitration\nmechanisms can be found in the USB specification [usb, ].\nLet us now consider the issue of transactions. Let us assume that a high speed hub is\nconnected to the host. The high speed hub is also connected to full and low speed devices\ndownstream. In this case, if the host starts a transaction to a low speed device through the\nhigh speed hub, then it will have to wait to get the reply from the device. This is because the\nlink between the high speed hub and the device is slow. There is no reason to lock up the bus\nbetween the host, and the hub in this case. We can instead implement a split transaction. The\nfirst part of the split transaction sends the command to the low speed device. The second part\nof the split transaction consists of a message from the low speed device to the host. In the\ninterval between the split transactions, the host can communicate with other devices. A USB\nbus implements similar split transactions for many other kinds of scenarios (refer to the USB\nSpecification [usb, ]).\nNetwork Layer\nLet us now consider the network layer. Each USB device including the hubs is assigned a\nunique ID by the host. Since we can support up to 127 devices per host, we need a 7-bit device\nid. Secondly, each device has multiple I\/O ports. Each such I\/O port is known as an end point.\nWe can either have data end points (interrupt, bulk, or isochronous), or control end points.\nAdditionally, we can classify end points as IN or OUT. The IN end point represents an I\/O\nport that can only send data to the processor, and the OUT end point accepts data from the\nprocessor. Every USB device can have at the most 16 IN end points, and 16 OUT end points.\nAny USB request clearly specifies the type of end point that it needs to access (IN or OUT).\nGiven that the type of the end point is fixed by the request, we need only 4 bits to specify the\naddress of the end point.\nAll USB devices have a default set of IN and OUT end points whose id is equal to 0.\nThese end points are used for activating the device, and establishing communication with it.\nSubsequently, each device defines its custom set of end points. Simple devices such as a mouse\nor keyboard that typically send data to the processor define just one IN end point. However,\nmore complicated devices such as web cameras define multiple end points. One end point is\nfor the video feed, one end point is for the audio feed, and there can be multiple end points for\nexchanging control and status data.\nThe responsibility of routing messages to the correct USB device lies with the hubs. The\nhubs maintain routing tables that associate USB devices with local port ids. Once a message\nreaches the device, it routes it to the correct end point.\nProtocol Layer 661 (cid:13)c Smruti R. Sarangi\nThe USB protocol layer is fairly elaborate. It starts out by defining two kinds of connections\nbetween end points known as pipes. It defines a stream pipe to be a stream of data without\nany specific message structure. In comparison, message pipes are more structured and define\na message sequence that the sender and receiver must both follow. A typical message in the\nmessage pipe consists of three kinds of packets. The communication starts with a token packet\nthat contains the device id, id of the end point, nature of communication, and additional\ninformation regarding the connection. The hubs on the path route the token packet to the\ndestination, and a connection is thus set up. Then depending upon the direction of the transfer\n(host to device or device to host), the host or the device sends a sequence of data packets.\nFinally, attheendofthesequenceofdatapackets, thereceiverofthepacketssendsahandshake\npacket to indicate the successful completion of the I\/O request.\nSummary\nTable 12.7 summarises our discussion on USB up till now. The reader can refer to the specifi-\ncations of the USB protocol [usb, ] for additional information.\n12.7.5 FireWire Protocol\nOverview\nFireWirestartedoutwithbeingahighspeedserialbusinApplecomputers. However,nowadays\nit is being perceived as a competitor to USB. Even though it is not as popular as USB, it is still\ncommonly used. Most laptops have FireWire ports. The FireWire ports are primarily used for\nconnecting video cameras, and high speed optical drives. FireWire is now an IEEE standard\n(IEEE 1394), and its specifications are thus open and standardised. Let us take a brief look at\nthe FireWire protocol.\nLike all the buses that we have studied, FireWire is a high speed serial bus. For the same\ngeneration, FireWire is typically faster than USB. For example, FireWire (S800) by default has\na bandwidth of 100 MB\/s, as compared to 60 MB\/s for high speed USB devices. Secondly,\nthe FireWire bus was designed to be a hybrid of a peripheral bus and a computer network. A\nsingle FireWire bus can support up to 63 devices. It is possible to construct a tree of devices\nby interconnecting multiple FireWire buses using FireWire bridges.\nThe most interesting thing about the FireWire protocol is that it does not presume a\nconnection to a computer. Peripherals can communicate among themselves. For example, a\nprintercantalktoascannerwithoutgoingthroughthecomputer. Itimplementsarealnetwork\ninthetruesense. Consequently,wheneveraFireWirenetworkbootsup,allthenodesco-operate\nand elect a leader. The leader node, or the root node is the root of a tree. Subsequently, the\nroot node sends out messages, and each node is aware of its position in the tree.\nThephysicallayeroftheFireWireprotocolconsistsoftwoLVDSlinks(onefortransmitting\ndata, and one for transmitting the strobe). The channel is thus half duplex. Note that latest\nversions of the Firewall protocol that have bandwidths greater than 100 MB\/s also support full\nduplex transmission. They however, have a different connector that requires more pins. For\nencoding logical bits, most FireWire buses use a method of encoding known as data strobe (DS)\nencoding. The DS encoding has two lines. One line contains the data (NRZ encoding), and the\nother line contains the strobe. The strobe is equal to the data XORed with the clock. Let the (cid:13)c Smruti R. Sarangi 662\nUSB (Universal Serial Bus)\nUsage Connecting peripheral devices such as key-\nboards, mice, web cameras, and pen drives\nSource [usb, ]\nTopology\nConnection Point to point, serial\nWidth Single bit, half duplex\nPhysical Layer\nSignalling LVDS based differential signalling.\nEncoding NRZI (transition represents a logical 0)\nTiming Asynchronous(a0addedaftersixcontinuous1s\nfor clock recovery)\nData Link Layer\nFrame Size 46 \u2013 1058 bits\nError Correction CRC\nTransactions Split transaction bus\nBandwidth 192 KB\/s (low speed), 1.5 MB\/s (full speed), 60\nMB\/s (high speed)\nNetwork Layer\nAddress 7-bit device id, 4-bit end point id\nRouting Using a tree of hubs\nHub Hasoneupstreamport,andupto4downstream\nports\nUSB network Can support a maximum of 127 devices\nProtocol Layer\nConnections Messagepipe(structured),andstreampipe(un-\nstructured)\nTypes of traffic Control, Interrupt, Bulk, Isochronous\nTable 12.7: The USB Protocol\ndata signal be D (sequence of 0s and 1s), the strobe signal be S, and the clock of the sender\nbe C. We have:\nS = D\u2295C\n(12.7)\n\u21d2D\u2295S = C\nAt the side of the receiver, it can recover the clock of the sender by computing D\u2295S (XOR\nof the data and the strobe). Thus, we can think of the DS encoding as a variant of source\nsynchronous transmission, where instead of sending the clock of the sender, we send the strobe.\nThe link layer of the FireWire protocol implements CRC based error checking, and split\ntransactions. FireWire protocols have a unique way of performing arbitration. We divide time 663 (cid:13)c Smruti R. Sarangi\ninto 125 \u00b5s cycles. The root node broadcasts a start packet to all the nodes. Nodes that wish\nto transmit isochronous data (data at a constant bandwidth) send their requests along with\nbandwidth requirements to the root node. The root node typically uses FIFO scheduling. It\ngives one device permission to use the bus and transmit data for a portion of the 125 \u00b5s cycle.\nOnce the request is over, it gives permission to the next isochronous request and so on. Note\nthat in a given cycle, we can only allot 80% of the time for isochronous transmission. Once, all\nthe requests are over, or we complete 80% of the cycle, all isochronous transactions stop. The\nroot subsequently considers asynchronous requests (single message transfers).\nDevicesthatwishtosendasynchronousdatasendtheirrequeststotherootthroughinternal\nnodesinthetree(otherFireWiredevices). IfaninternalnoderepresentedbyaFireWiredevice\nwishes to send an asynchronous packet in the current cycle, it denies the request to all the\nrequesters that are in its subtree. Once a request reaches the root node, it sends a packet\nback to the requester to grant it permission to transmit a packet. The receiver is supposed\nto acknowledge the receipt of the packet. After a packet transmission has finished, the root\nnode schedules the next request. The aspect of denying requests made by downstream nodes\nis similar to the concept of a daisy chain (see Section 12.4.3).\nFor the network layer, each FireWire device also defines its internal set of I\/O ports. All\nthe devices export a large I\/O space to the processor. Each I\/O port contains a device address\nand a port address within the device. The tree of devices first routes a request to the right\ndevice, and then the device routes the request to the correct internal port. Typically, the entire\nFireWireI\/Oaddressspaceismappedtomemory,andmostofthetimeweusememorymapped\nI\/O for FireWire devices.\nSummary\nTable 12.8 summarises our discussion on the FireWire protocol.\n12.8 Storage\nOut of all the peripheral devices that are typically attached to a processor, storage devices have\na special place. This is primarily because they are integral to the functioning of the computer\nsystem.\nThe storage devices maintain persistent state. Persistent state refers to all the data that\nis stored in the computer system even when it is powered off. Notably, the storage systems\nstore the operating system, all the programs, and their associated data. This includes all\nour documents, songs, images, and videos. From the point of view of a computer architect,\nthe storage system plays an active role in the boot process, saving files and data, and virtual\nmemory. Let us discuss each of these roles one by one.\nWhen a processor starts (process is known as booting), it needs to load the code of the\noperating system. Typically, the code of the operating system is available at the beginning of\nthe address space of the primary hard disk. The processor then loads the code of the operating\nsystemintomainmemory, andstartsexecutingit. Afterthebootprocess, theoperatingsystem\nis available to users, who can use it to run programs, and access data. Programs are saved as\nregular files in the storage system, and data is also saved in files. Files are essentially blocks (cid:13)c Smruti R. Sarangi 664\nFireWire (IEEE 1394)\nUsage Connection to video cameras and optical drives\nSource [fir, ]\nTopology\nConnection Point to Point, serial, daisy chain based tree\nWidth Single bit, half duplex (till FireWire 400, full duplex beyond\nthat)\nPhysical Layer\nSignalling LVDS based differential signalling.\nEncoding DataStrobeEncoding(FireWire800andabovealsosupport\n8bit\/10 bit encoding)\nTiming Source Synchronous (sends a data strobe rather than a\nclock)\nData Link Layer\nFrame Size 12.5 KB (FireWire 800 protocol)\nError Correction CRC\nTransactions Split Transaction Bus\nArbitration (1) Elect a leader, or root node.\n(2) Each 125 \u00b5s cycle, the root sends a start packet, and\neach device willing to transmit sends its requirements to the\nroot.\n(3) The root allots 100 \u00b5s for isochronous traffic, and the\nrest for asynchronous traffic\nBandwidth 100 MB\/s (FireWire 800)\nNetwork Layer\nAddress Space The tree of FireWire devices export a large I\/O address\nspace.\nRouting Using a tree of bridges\nTable 12.8: The FireWire Protocol\nof data in the hard disk, or similar storage devices. These blocks of data need to be read into\nmain memory such that they are accessible by the processor.\nLastly storage devices play a very important role in implementing virtual memory. They\nstoretheswapspace(seeSection10.4.4). Recallthattheswapspacecontainsalltheframesthat\ncannot be contained in main memory. It effectively helps to extend the physical address space\nto match the size of the virtual address space. A part of the frames are stored in main memory,\nand the remaining frames are stored in the swap space. They are brought into (swapped in),\nwhen there is a page fault.\nAlmost all types of computers have attached storage devices. There, however can be some\nexceptions. Some machines especially, in a lab setting, might access a hard disk over the\nnetwork. They typically use a network boot protocol to boot from a remote hard disk, and 665 (cid:13)c Smruti R. Sarangi\naccess all the files including the swap space over the network. Conceptually, they still have an\nattachedstoragedevice. Itisjustnotphysicallyattachedtothemotherboard. Itisnonetheless,\naccessible over the network.\nNow, let us take a look at the main storage technologies. Traditionally, magnetic storage\nhas been the dominant technology. This storage technology records the values of bits in tiny\nareas of a large ferro-magnetic disk. Depending on the state of magnetisation, we can either\ninfer a logical 0 or 1. Instead of magnetic disk technology, we can use optical technology such\nas CD\/DVD\/Blu-ray drives. A CD\/DVD\/Blu-ray disk contains a sequence of pits (aberrations\non the surface) that encodes a sequence of binary values. The optical disk drive uses a laser to\nread the values stored on the disk. Most of the operations of a computer typically access the\nhard disk, whereas optical disks are mainly used to archive videos and music. However, it is\nnot uncommon to boot from the optical drives.\nAfastemergingalternativetomagneticdisks, andopticaldrivesissolidstatedrives. Unlike\nmagnetic, and optical drives that have moving parts, solid state drives are made of semiconduc-\ntors. The most common technology used in solid state drives is flash. A flash memory device\nuses charge stored in a semiconductor to signify a logical 0 or 1. They are much faster than\ntraditional hard drives. However, they can store far less data, and as of 2012, are 5-6 times\nmore expensive. Hence, high end servers opt for hybrid solutions. They have a fast SSD drive\nthat acts as a cache for a much larger hard drive.\nLet us now take a look at each of these technologies. Note that in this book, our aim is to\ngive the reader an understanding of the basic storage technologies such that she can optimise\nthe computer architecture. For a deeper understanding of storage technologies the reader can\ntake a look at [Brewer and Gill, 2008, Micheloni et al., 2010].\n12.8.1 Hard Disks\nA hard disk is an integral part of most computer systems starting from laptops to servers. It is\na storage device made of ferromagnetic material and mechanical components that can provide\na large amount of storage capacity at low cost. Consequently, for the last three decades hard\ndisks have been exclusively used to save persistent state in personal computers, servers, and\nenterprise class systems.\nSurprisingly, the basic physics of data storage is very simple. We save 0s and 1s in a series\nof magnets. Let us quickly review the basic physics of data storage in hard disks.\nPhysics of Data Storage in Hard Disks\nLet us consider a typical magnet. It has a north pole, and a south pole. Like poles repel each\nother, and opposite poles attract each other. Along with mechanical properties, magnets have\nelectrical properties also. For example, when the magnetic field passing through a coil of wire\nchanges due to the relative motion between the magnet and the coil, an EMF (voltage) is\ninduced across the two ends of the wire according to Faraday\u2019s law. Hard disks use Faraday\u2019s\nlaw as the basis of their operation.\nThe basic element of a hard disk is a small magnet. Magnets used in hard disks are\ntypicallymadeofironoxidesandexhibitpermanentmagnetism. Thismeansthattheirmagnetic\nproperties hold all the time. They are called permanent magnets or Ferromagnets (because of\nironoxides). Incomparison, wecanhaveelectromagnetsthatconsistofcoilsofcurrentcarrying (cid:13)c Smruti R. Sarangi 666\nwires wrapped around iron bars. Electromagnets lose their magnetism after the current is\nswitched off.\nN S N S S N S N N S\n0 1 0 1\nFigure 12.28: A sequence of tiny magnets on the surface of a hard disk\nNow, let us consider a set of magnets in series as shown in Figure 12.28. There are two\noptions for their relative orientation namely N-S (north\u2013south), or S-N (south\u2013north). Let\nus now move a small coil of wire over the arrangement of magnets. Whenever, it crosses the\nboundary of two magnets that have opposite orientations, there is a change in the magnetic\nfield. Hence, as a direct consequence of Faraday\u2019s law, an EMF is induced across the two ends\nof the coil. However, when there is no change in the orientation of the magnetic field, the EMF\ninduced across the ends of the coil is negligible. The transition in the orientation of the tiny\nmagnets corresponds to a logical 1 bit, and no transition represents a logical 0 bit. Thus, the\nmagnets in Figure 12.28 represent the bit pattern 0101. In principle, this is similar to the NRZI\nencoding for I\/O channels.\nSince we encode data in transitions, we need to save blocks of data. Consequently, hard\ndisks save a block of data in a sector. A sector has traditionally between 512 bytes for hard\ndisks. It is treated as an atomic block, and an entire sector is typically read or written in one\ngo. The structure that contains the small coil, and passes over the magnets is known as the\nread head.\nLet us now look at writing data to the hard disk. In this case, the task is to set the\norientation of the magnets. We have another structure called the write head that contains a\ntiny electromagnet. An electromagnet can induce magnetisation of a permanent magnet if it\npasses over it. Secondly, the direction of magnetisation is dependent on the direction of the\ncurrent. If we reverse the direction of the current, the direction of magnetisation changes.\nFor the sake of brevity, we shall refer to the combined assembly of the read head, and write\nhead, as the head.\nStructure of the Platter\nA hard disk typically consists of a set of platters. A platter is a circular disk with a hole in\nthe middle. A spindle is attached to the platter through the circular hole in the middle. The\nplatter is divided into a set of concentric rings called tracks. A track is further divided into\nfixed length sectors as shown in Figure 12.29.\nDefinition 160\nA hard disk consists of multiple platters. A platter is a circular disk that is attached to a 667 (cid:13)c Smruti R. Sarangi\nSector\nTrack\nFigure 12.29: The structure of a platter\nspindle. A platter further consists of a set of concentric rings called tracks, and each track\nconsists of a set of sectors. A sector typically contains a fixed number of bytes irrespective\nof the track.\nNow,letusoutlinethebasicoperationofaharddisk. Theplattersareattachedtoaspindle.\nDuring the operation of a hard disk, the spindle, and its attached platters are constantly in\nrotation. Let us for the sake of simplicity assume a single platter disk. Now, the first step is\nto position the head on the track that contains the desired data. Next, the head needs to wait\nat this position till the desired sector arrives under the head. Since the platter is rotating at a\nconstant speed, we can calculate the amount of time that we need to wait based on the current\nposition of the head. Once the desired sector, arrives under the head, we can proceed to read\nor write the data.\nThere is an important question that needs to be considered here. Do we have the same\nnumber of sectors per track, or do we have a different number of sectors per track? Note that\nthere are technological limitations on the number of bits that can be saved per track. Hence, if\nwe have the same number of sectors per track, then we are effectively wasting storage capacity\nin the tracks towards the periphery. This is because we are limited by the number of bits that\nwe can store in the track that is closest to the center. Consequently, modern hard disks avoid\nthis approach.\nLet us try to store a variable number of sectors per track. Tracks towards the center contain\nfewer sectors, and tracks towards the periphery contain more sectors. This scheme also has its\nshare of problems. Let us compare the innermost and outermost tracks, and let us assume that (cid:13)c Smruti R. Sarangi 668\nthe innermost track contains N sectors, and the outermost track contains 2N sectors. If we\nassume that the number of rotations per minute is constant, then we need to read data twice\nas fast on the outermost track as compared to the innermost track. In fact for every track, the\nrate of data retrieval is different. This will complicate the electronic circuitry in the disk. We\ncan explore another option, which is to rotate the disk at different speeds for each track, such\nthattherateofdatatransferisconstant. Inthiscase, theelectroniccircuitryissimpler, butthe\nsophistication required to run the spindle motor at a variety of different speeds is prohibitive.\nHence, both the solutions are impractical.\nHow about, combining two impractical solutions to make it practical !!! We have been\nfollowing similar approaches throughout this book. Let us divide the set of tracks into a set of\nzones. Each zone consists of a consecutive set of m tracks. If we have n tracks in the platter,\nthen we have n\/m zones. In each zone, the number of sectors per track is the same. The\nplatter rotates with a constant angular velocity for all the tracks in a zone. In a zone, data is\nmore densely packed for tracks that are closer to the center as compared to tracks towards the\nperiphery of the platter. In other words, sectors have physically different sizes for tracks in a\nzone. This is not a problem since the disk drive assumes that it takes the same amount of time\nto pass over each sector in a zone, and rotation at a constant angular velocity ensures this.\nFigure 12.30 shows a conceptual breakup of the platter into zones. Note that the number of\nsectors per track varies across zones. This method is known as Zoned-Bit Recording(ZBR). The\ntwo impractical designs that we refrained from considering, are special cases of ZBR. The first\ndesign assumes that we have one zone, and the second design assumes that each track belongs\nto a different zone.\nZone 1\nZone 2\nZone 3\nFigure 12.30: Zoned-Bit Recording\nLet us now try to see why this scheme works. Since we have multiple zones, the storage\nspace wasted is not as high as the design with just a single zone. Secondly, since the number of 669 (cid:13)c Smruti R. Sarangi\nzones is typically not very large, the motor of the spindle does not need to readjust its speed\nfrequently. In fact because of spatial locality, the chances of staying within the same zone are\nfairly high.\nStructure of the Hard Disk\nSpindle\nPlatter\nHead\nArm\nFigure 12.31: The structure of a hard disk (source [har, ])\nLet us now put all the parts together and take a look at the structure of the hard disk\nin Figure 12.31 and Figure 12.32. We have a set of platters connected to a single rotating\nspindle, and a set of disk arms (one for each side of the platter) that contain a head at the\nend. Typically, all the arms move together, and all the heads are vertically aligned on the same\ncylinder. Here, a cylinder is defined as a set of tracks from multiple platters, which have the\nsame radius. In most hard disks only one head is activated at a point of time. It performs a\nread or write access on a given sector. In the case of a read access, the data is transmitted\nback to the drive electronics for post processing (framing, error correction), and then sent on\nthe bus to the processor through the bus interface.\nLet us now consider some subtle points in the design of a hard disk (refer to Figure 12.32).\nIt shows two platters connected to a spindle, and each platter has two recording surfaces. The\nspindleisconnectedtoamotor(knownasthespindlemotor),whichadjustsitsspeeddepending\non the zone that we wish to access. The set of all the arms move together, and are connected\nusing a spindle to the actuator. The actuator is a small motor used for moving the arms clock\nwise or anti-clockwise. The role of the actuator is to position the head of an arm on a given\ntrack by rotating it clockwise or ant-clockwise a given number of degrees.\nA typical disk drive in a desktop processor has a track density of about 10,000 tracks per\ninch. This means that the distance between tracks is 2.5 \u00b5m, and thus the actuator has to be\nincredibly accurate. Typically there are some markings on a sector indicating the number of\nthe track. Consequently, the actuator typically needs to make slight adjustments to come to (cid:13)c Smruti R. Sarangi 670\nSpindle\nRead\/Write\nhead\nPlatter\nArm\nBus\nBus\nActuator\ninterface\nSpindle motor\nDrive\nelectronics\nFigure 12.32: Internals of a hard disk\nthe exact point. This control mechanism is known as servo control. Both the actuator and the\nspindle motor are controlled by electronic circuits that are inside the chassis of the hard disk.\nOnce the actuator has placed the head on the right track, it needs to wait for the desired sector\nto come under the head. A track has markings to indicate the number of the sector. The head\nkeeps reading the markings after its positioned on a track. Based on these markings it can\naccurately predict when the desired sector will be underneath the head.\nAlong with the mechanical components, a hard disk has electronic components including\nsmall processors. They receive and transmit data on the bus, schedule requests on the hard\ndisk, and perform error correction. The reader needs to appreciate the fact that we have just\nscratched the surface in this book. A hard disk is an incredible feat of human engineering. The\nhard disk can most of the time seamlessly tolerate errors, dynamically invalidate bad sectors\n(sectors with faults), and remap data to good sectors. The reader is referred to [Jacob et al.,\n2007] for further study.\nMathematical Model of a Hard Disk Access\nLet us now construct a quick mathematical model for the time a request takes to complete its\naccess to the hard disk. We can divide the time taken into three parts. The first is the seek\ntime, which is defined as the time required for the head to reach the right track. Subsequently,\nthe head needs to wait for the desired sector to arrive under it. This time interval is known as\nthe rotational latency. Lastly, the head needs to read the data, process it to remove errors and\nredundant information, and then transmit the data on the bus. This is known as the transfer\ntime. Thus, we have the simple equation. 671 (cid:13)c Smruti R. Sarangi\nT = T +T +T (12.8)\ndisk access seek rot latency transfer\nDefinition 161\nSeek Time The time required for the actuator to move the head to the right track.\nRotational Latency The time required for the desired sector to arrive under the head,\nafter the head is correctly positioned on the right track.\nTransfer Time The time required to transfer the data from the hard disk to the processor.\nExample 129\nAssume that a hard disk has an average seek time of 12 ms, rotates at 600 rpm, and has\na bandwidth of 106 B\/s. Find the average time to transfer 10,000 bytes of data (assuming\nthat the data is in consecutive sectors).\nAnswer: T = 12 ms\nseek\nSincethediskrotatesat600rpm, ittakes100msperrotation. Onanaverage, therotational\nlatency is half this amount because the offset between the current position of the head, and\nthe desired offset is assumed to be uniformly distributed between 0\u25e6 and 360\u25e6. Thus, the\nrotational latency(T ) is 50 ms. Now, the time it takes to transfer 104 contiguous\nrot latency\nbytes is 0.01s or 10 ms, because the bandwidth is 106 B\/s.\nHence, the average time per disk access is 12 ms + 50 ms + 10 ms = 72 ms\n12.8.2 RAID Arrays\nMost enterprise systems have an array of hard disks because their storage, bandwidth, and\nreliability requirements are very high. Such arrays of hard disks are known as RAID arrays\n(Redundant Arrays of Inexpensive Disks). Let us review the design space of RAID based\nsolutions in this section.\nDefinition 162\nRAID (Redundant Array of Inexpensive Disks) is a class of technologies for deploying large\narrays of disks. There are different RAID levels in the design space of RAID solutions.\nEach level makes separate bandwidth, capacity, and reliability guarantees. (cid:13)c Smruti R. Sarangi 672\nRAID 0\nLet us consider the simplest RAID solution known as RAID 0. Here, the aim is to increase\nbandwidth, andreliabilityisnotaconcern. Itistypicallyusedinpersonalcomputersoptimised\nfor high performance gaming.\nThe basic idea is known as data striping. Here, we distribute blocks of data across disks.\nA block is a contiguous sequence of data similar to cache blocks. Its size is typically 512 B;\nhowever, its size may vary depending on the RAID system. Let us consider a two disk system\nwith RAID 0 as shown in Figure 12.33. We store all the odd numbered blocks (B1, B3, ...)\nin disk 1, and all the even numbered blocks (B2, B4, ...) in disk 2. If a processor has a page\nfault, then it can read blocks from both the disks in parallel, and thus in effect, the hard disk\nbandwidth is doubled. We can extend this idea and implement RAID 0, using N disks. The\ndisk bandwidth can thus be theoretically increased N times.\nB1 B2\nB3 B4\nB5 B6\nB7 B8\nB9 B10\nFigure 12.33: RAID 0\nRAID 1\nLetusnowaddreliabilitytoRAID0. Notethattheprocessofreadingandwritingbitsinahard\ndisk is a mechanical process. It is possible that some bits might not be read or written correctly\nbecause there might be a slight amount of deviation from ideal operation in the actuator, or the\nspindle motor. Secondly, external electro-magnetic radiation, and cosmic particle strikes can\nflip bits in the hard disk and its associated electronic components. The latter type of errors are\nalso known as soft errors. Consequently, each sector of the disk typically has error detecting\nand correcting codes. Since an entire sector is read or written atomically, error checking and\ncorrection is a part of hard disk access. We typically care about more catastrophic failures\nsuch as the failure of an entire hard disk drive. This means that the there is break down in\nthe actuator, spindle motor, or any other major component that prevents us from reading or\nwriting to most of the hard disk. Let us consider disk failures from this angle. Secondly, let us\nalso assume that disks follow the fail stop model of failure. This means that whenever there is\na failure, the disks are not operational anymore, and the system is aware of it.\nIn RAID 1, we typically have a 2 disk system (see Figure 12.34), and we mirror data of one\ndisk on the other. They are essentially duplicates of each other. We are definitely wasting half\nof our storage space here. In the case of reads, we can leverage this structure to theoretically 673 (cid:13)c Smruti R. Sarangi\ndouble the disk bandwidth. Let us assume that we wish to read the blocks 1 and 3. In the case\nof RAID 0, we needed to serialise the accesses, because both the blocks map to the same disk.\nHowever, in this case, since each disk has a copy of all the data, we can read block 1 from disk\n1, and read block 3 from disk 3 in parallel. Thus, the read bandwidth is potentially double that\nof a single disk. However, the write bandwidth is still the same as that of a single disk, because\nwe need to write to both the disks. Note that here it is not necessary to read both the disks\nand compare the contents of a block in the interest of reliability. We assume that if a disk is\noperational, it contains correct data.\nB1 B1\nB2 B2\nB3 B3\nB4 B4\nB5 B5\nFigure 12.34: RAID 1\nRAID 2\nLet us now try to increase the efficiency of RAID 1. In this case, we consider a system of N\ndisks. Instead of striping data at the block level, we stripe data at the bit level. We dedicate a\ndisk for saving the parity bit. Let us consider a system with 5 disks as shown in Figure 12.35.\nWe have 4 data disks, and 1 parity disk. We distribute contiguous sequences of 4 bits in a\nlogical block across the 4 data disks. A logical block is defined as a block of contiguous data as\nseen by software. Software should be oblivious of the fact that we are using a RAID system\ninstead of a single disk. All software programs perceive a storage system as an array of logical\nblocks. Now, the first bit of a logical block is saved in disk 1, the second bit is saved in disk\n2, and finally the fourth bit is saved in disk 4. Disk 5, contains the parity of the first 4 bits.\nEach physical block in a RAID 2 disk thus contains a subset of bits of the logical blocks. For\nexample, B1 contains bit numbers 1,5,9,... of the first logical block saved in the RAID array.\nSimilarly, B2 contains bit numbers 2,6,10,.... To read a logical block, the RAID controller\nassembles the physical blocks and creates a logical block. Similarly, to write a logical block, the\nRAID controller breaks it down into its constituent physical blocks, computes the parity bits,\nand writes to all the disks.\nReads are fast in RAID 2. This is because we can read all the 9 disks in parallel. Writes\nare also fast, because we can write parts of a block to different disks, in parallel. RAID 2 is\ncurrentlynotusedbecauseitdoesnotallowparallelaccesstodifferentlogicalblocks,introduces\ncomplexity because of bit level striping, and every I\/O request requires access to all the disks.\nWe would like to iterate again that the parity disk is not accessed on a read. The parity disk\nis accessed on a write because its contents need to be updated. Its main utility is to keep the (cid:13)c Smruti R. Sarangi 674\nsystem operational if there is a single disk failure. If a disk fails, then the contents of a block\ncan be recovered by reading other blocks in the same row from the other disks, and by reading\nthe parity disk.\nStriping\nB1 B2 B3 B4 P1\nB5 B6 B7 B8 P2\nB9 B10 B11 B12 P3\nB13 B14 B15 B16 P4\nB17 B18 B19 B20 P5\nData disks Parity disk\nFigure 12.35: RAID 2\nRAID 3\nRAID 3 is almost the same as RAID 2. Instead of striping at the bit level, it stripes data at\nthe byte level. It has the same pros and cons as RAID 2, and is thus seldom used.\nRAID 4\nRAID 4 is designed on the same lines as RAID 2 and 3. It stripes data at the block level. It has\nadedicatedparitydiskthatsavestheparityofalltheblocksonthesamerow. Inthisscheme, a\nread access for a single block is not as fast as RAID 2 and 3 because we cannot access different\nparts of a block in parallel. A write access is also slower for the same reason. However, we can\nread from multiple blocks at the same time if they do not map to the same disk. We cannot\nunfortunately do this for writes.\nFor a write access, we need to access two disks \u2013 the disk that contains the block, and the\ndisk that contains the parity. An astute reader might try to argue that we need to access all\nthe disks because we need to compute the parity of all the blocks in the same row. However,\nthis is not true. Let us assume that there are m data disks, and the contents of the blocks in\na row are B ...B respectively. Then the parity block, P, is equal to B \u2295B \u2295...\u2295B .\n1 m 1 2 m\nNow, let us assume that we change the first block from B to B(cid:48). The new parity is given by\n1 1\nP(cid:48) = B(cid:48) \u2295B ...\u2295B . We thus have:\n1 2 m\nP(cid:48) = B(cid:48) \u2295B ...\u2295B\n1 2 m\n= B \u2295B \u2295B(cid:48) \u2295B ...\u2295B (12.9)\n1 1 1 2 m\n= B \u2295B(cid:48) \u2295P\n1 1\nThe results used in Equation 12.9 are: B \u2295B = 0 and 0\u2295P(cid:48) = P(cid:48). Thus, to compute P(cid:48),\n1 1\nwe need the values of B , and P. Hence, for performing a write to a block, we need two read\n1 675 (cid:13)c Smruti R. Sarangi\naccesses (for reading B and P), and two write accesses (for writing B(cid:48) and P(cid:48)) to the array of\n1 1\nhard disks. Since all the parity blocks are saved in one disk, this becomes a point of contention,\nand the write performance becomes very slow. Hence, RAID 4 is also seldom used.\nRAID 5\nRAID 5 mitigates the shortcomings of RAID 4. It distributes the parity blocks across all the\ndisks for different rows as shown in Figure 12.36. For example, the 5th disk stores the parity for\nthe first row, and then the 1st disk stores the parity for the second row, and the pattern thus\ncontinues in a round robin fashion. This ensures that no disk becomes a point of contention,\nand the parity blocks are evenly distributed across all the disks.\nNote that RAID 5 provides high bandwidth because it allows parallel access for reads, has\nrelatively faster write speed, and is immune to one disk failure. Hence, it is heavily used in\ncommercial systems.\nStriping\nB1 B2 B3 B4 P1\nP2 B5 B6 B7 B8\nB9 P3 B10 B11 B12\nB13 B14 P4 B15 B16\nB17 B18 B19 P5 B20\nFigure 12.36: RAID 5\nRAID 6\nSometimes, we might desire additional reliability. In this case, we can add a second parity\nblock, and distribute both the parity blocks across all the disks. In this case, a write to the\nRAID array becomes slightly slower at the cost of higdher reliability. RAID 6 is mostly used in\nenterprises that desire highly reliable storage. It is important to note that the two parity blocks\nin RAID 6 are not a simple XOR of bits. The contents of the two parity blocks for each row\ndiffer from each other, and are complex functions of the data. The reader requires background\nin field theory to understand the operation of the error detection blocks in RAID 6.\n12.8.3 Optical Disks \u2013 CD, DVD, Blu-ray\nWe typically use optical disks such as CDs, DVDs, and Blu-ray disks to store videos, music,\nand software. As a matter of fact, optical disks have become the default distribution media\nfor videos and music (other than the internet of course). Consequently, almost all desktops\nand laptops have a built-in CD or DVD drive. The reader needs to note that the physics of\noptical disks is very different from that of hard disks. We read the data stored in a hard disk by (cid:13)c Smruti R. Sarangi 676\nStriping\nB1 B2 B3 B4 P1A P1B\nP2B B5 B6 B7 B8 P2A\nP3A P3B B9 B10 B11 B12\nB13 P4A P4B B14 B15 B16\nB17 B18 P5A P5B B19 B20\nFigure 12.37: RAID 6\nmeasuring the change in magnetic field due to the relative motion of tiny magnets embedded in\nthe platters of the hard disk. In comparison, in an optical disk, we read data by using photo-\ndetectors (light detectors) to measure the intensity of optical signals reflected off the surface of\nthe disk.\nThe reader needs to note that CDs (compact disks), DVDs (Digital Video Disks, or Digital\nVersatile Disks), and Blu-ray disks, basically use the same technology. CDs represent first\ngeneration optical disks, DVDs represent second generation optical disks, and Blu-ray disks\nare representative of the third generation. Successive generations are typically faster and can\nprovide more storage capacity. Let us now consider the physics of optical storage media.\nBasic Physics of Optical Storage Media\nAnopticaldiskisshowninFigure12.38. Itisacirculardisk, andistypically12cmindiameter.\nIt has a hole in the center that is meant for attaching to a spindle (similar to hard disks). The\nhole is 1.5 cm in diameter, and the entire optical disk is 1.2 mm thick. An optical disk is made\nof multiple layers. We are primarily concerned with the reflective layer that reflects laser light\nto a set of detectors. We encode data bits by modifying the surface of the reflective layer. Let\nus elaborate.\nThe data is saved in a spiral pattern that starts from the innermost track, covers the entire\nsurface of the disk, and ends at the outermost track. The width of the track, and the spacing\nbetween the tracks depends on the optical disk generation. Let us outline the basic mechanism\nthat is used to encode data on the spiral path. The spiral path has two kinds of regions namely\nlandsandpits. Landsreflecttheopticalsignal, andthusrepresentthephysicalbit, 1. Landsare\nrepresented by a flat region in the reflective layer. In comparison, pits have lower reflectivity,\nand the reflected light is typically out of phase with the light reflected off the lands, and thus\nthey represent the physical bit, 0. A pit is a depression on the surface of the reflective layer.\nThe data on a CD is encoded using the NRZI encoding scheme (see Section 12.2.7). We infer\na logical 1 when there is a pit to land, or land to pit transition. However, if there are no\ntransitions, then we keep on inferring logical 0s.\nOptical Disk Layers\nAn optical disk typically has four layers (refer to Figure 12.39). 677 (cid:13)c Smruti R. Sarangi\nFigure 12.38: An optical disk (source [dis, ])\nPolycarbonate Layer The polycarbonate layer is a layer of polycarbonate plastic. Lands\nand pits are created at its top using an injection moulding process.\nReflective Layer The reflective layer consists a thin layer of aluminium or gold that reflects\nthe laser light.\nLacquer Layer The lacquer based layer on top of the reflective layer protects the reflective\nlayer from scratches, and other forms of accidental damage.\nSurface Layer Most vendors typically add a plastic layer over the lacquer layer such that it\nis possible to add a label to the optical disk. For example, most optical disks typically\nhave a poster of the movie on their top surface.\nThe optical disk reader sends a laser signal that passes through the polycarbonate layer and\ngets focused on the lands or pits. In CDs, the polycarbonate layer is typically very deep and\nit occupies most of the volume. In comparison the polycarbonate layer occupies roughly half\nthe volume in DVDs. For third generation optical disks the reflective layer is very close to the\nbottom surface.\nPlastic surface\nLacquer layer\nReflective layer\nPit Polycarbonate layer\nLand\nFigure 12.39: Optical disk layers (cid:13)c Smruti R. Sarangi 678\nOptical Disk Reader\nBus\ninterface\nSpindle Laser assembly\nDrive\nelectronics\nRails\nActuator\nSpindle\nmotor motor\nFigure 12.40: Optical disk reader\nAn optical disk reader is very similar to a hard disk drive (refer to Figure 12.40). The\noptical disk rotates on a spindle. The label of the optical disk is oriented towards the top. The\nactuator and head assembly are located at the bottom. Unlike a hard disk that uses a rotary\nactuator (rotating arm), an optical disk drive uses a linear actuator [Abramovitch, 2001] that\nslides radially in or out. Figure 12.40 shows a laser assembly that slides on a set of rails. The\nlaser assembly is connected to an actuator via a system of gears and mechanical components.\nThe actuator motor can very precisely rotate its spindle, and the system of gears translate\nrotational motion into linear motion of the laser assembly.\nThe head is a part of the laser assembly. The head typically contains a light source (laser)\nthat is focused on the reflective layer through a system of lenses. The reflective layer then\nreflects the light, and a part of the reflected light gets captured by the optical disk head.\nThe reflected light is converted to electrical signals within the head by photodetectors. The\nsequence of electrical signals are processed by dedicated circuitry in the drive, and converted\nto a sequence of logical bits. Similar to hard disks, optical drives perform error detection and\ncorrection.\nOneimportantpointofdifferencefromharddisksisthattheopticaldiskrotatesatconstant\nlinear velocity. This means that pits and lands traverse under the head at the same velocity\nirrespective of the track. In other words, the data transfer rate is the same irrespective of the\npositionofthehead. Tosupportthisfeature,itisnecessarytochangetherotationalspeedofthe\nspindleaccordingtothepositionofthehead. Whentheheadistravellingtowardstheperiphery 679 (cid:13)c Smruti R. Sarangi\nof the disk, it is necessary to slow the disk down. The spindle motor has sophisticated support\nfor acceleration and deceleration in optical drives. To simplify the logic, we can implement\nzoning here similar to the zoning in hard disk drives (see Section 12.8.1). However, in the case\nof optical drives, zoning is mostly used in high performance drives.\nAdvanced Features\nMost audio, video, and software CDs\/DVDs are written once by the original vendors, and are\nsold as read-only media. Users are not expected to overwrite the optical storage media. Such\nkind of optical disks use 4 layers as described in Figure 12.39. Optical disks are also used\nto archive data. Such disks are typically meant to be written once, and read multiple times\n(CD-R and DVD-R formats). To create such recordable media, the polycarbonate layer is\ncoated with an organic dye that is sensitive to light. The organic dye layer is coated with the\nreflective metallic layer, the lacquer layer, and the surface layer. While writing the CD, a high\npowered write-laser focuses light on the dye and changes its reflectivity. Lands are regions of\nhigh reflectivity, and pits are regions of low reflectivity. Such write-once optical media were\nsuperseded by optical media (CDs or DVDs) that can be read and written many times. Here,\nthe reflective layer is made of a silver-indium-antimony-tellurium alloy. When it is heated to\n500\u25e6C, spots in the reflective layer lose their reflectivity because the structure of the alloy\nbecomes amorphous. To make the spots reflective they are heated to 200\u25e6C such that the state\nof the alloy changes to the polycrystalline state. We can thus encode lands and pits in the\nreflective layer, erase them, and rewrite them as required.\nModern disks can additionally have an extra layer of lands and pits. The first layer is\ncoated with a chemical that is partially transparent to light. For example, pits can be coated\nwith fluorescent material. When irradiated with red light they glow and emit light of a certain\nwavelength. However, most of the red light passes to the second layer, and then interacts with\nthe fluorescent material in the pits, which is different from the material in the first layer. By\nanalysing the nature of reflected light, and by using sophisticated image filtering algorithms, it\nis possible to read the encoded data in both the layers. A simpler solution is to encode data\non both sides of the optical disk. This is often as simple as taking two single side disks and\npasting their surface layers together. To read such a disk, we need two laser assemblies.\nComparison of CDs, DVDs, and Blu-ray Disks\nCD DVD Blu-ray\nGeneration 1st 2nd 3rd\nCapacity 700 MB 4.7 GB 25 GB\nUses Audio Video High definition Video\nLaser wavelength 780 nm 650 nm 405 nm\nRaw 1X transfer rate 153 KB\/s 1.39 MB\/s 4.5 MB\/s\nTable 12.9: Comparison between CD, DVD, and Blu-ray disks\nRefer to Table 12.9 for a comparison of CD, DVD, and Blu-ray disks. (cid:13)c Smruti R. Sarangi 680\n12.8.4 Flash Memory\nHard disks, and optical drives are fairly bulky, and need to be handled carefully because they\ncontain sensitive mechanical parts. An additional shortcoming of optical storage media is that\nthey are very sensitive to scratches and other forms of minor accidental damage. Consequently,\nthese devices are not ideally suited for portable and mobile applications. We need a storage\ndevice that does not consist of sensitive mechanical parts, can be carried in a pocket, can be\nattached to any computer, and is extremely durable. Flash drives such as USB pen drives\nsatisfy all these requirements. A typical pen drive can fit in a wallet, can be attached to all\nkinds of devices, and is extremely robust and durable. It does not lose its data when it is\ndisconnected from the computer. We have flash based storage devices in most portable devices,\nmedical devices, industrial electronics, disk caches in high end servers, and small data storage\ndevices. Flash memory is an example of an EEPROM (Electrically Erasable Programmable\nRead Only Memory) or EPROM (Erasable Programmable Read Only Memory). Note that\ntraditionally EPROM based memories used ultraviolet light for erasing data. They have been\nsuperseded by flash based devices.\nLet us look at flash based technology in this section. The basic element of storage is a\nfloating gate transistor.\nThe Floating Gate Transistor\nControl gate\nSiO Floating gate\n2\nSource Drain\nSymbol\n(a) (b)\nFigure 12.41: A floating gate transistor\nFigure 12.41 shows a floating gate transistor. The figure shows a regular NMOS transistor\nwithtwogatesinsteadofone. Thegateontopisknownasthecontrolgate, andisequivalentto\nthe gate in normal MOS transistors. The gate below the control gate is known as the floating\ngate. It is surrounded on all sides by an SiO based electrical insulation layer. Hence, the\n2\nfloating gate is electrically isolated from the rest of the device. By some means if we are able to\nimplant a certain amount of charge in the floating gate, then the floating gate will maintain its\npotential for a very long time. In practice, there is a negligible amount of current flow between\nthe floating gate and the rest of the components in the floating gate transistor under normal\nconditions. Let us consider two scenarios. In the first scenario, the floating gate is not charged.\nIn this case, the floating gate transistor acts as a regular NMOS transistor. However, if the 681 (cid:13)c Smruti R. Sarangi\nfloating gate has accumulated electrons containing negative charge, then we have a negative\npotential gradient between the channel and the control gate. Recall that to create a n-type\nchannel in the transistor, it is necessary to apply a positive voltage to the gate, where this\nvoltage is greater than the threshold voltage. In this case the threshold voltage is effectively\nhigher because of the accumulation of electrons in the floating gate. In other words, to induce\na channel in the substrate, we need to apply a larger positive voltage at the control gate.\nLet the threshold voltage when the floating gate is not charged with electrons be V , and\nT\nlet the threshold voltage when the floating gate contains negative charge be V+ (V+ > V ). If\nT T T\nwe apply a voltage that is in between V and V+ to the control gate, then the NMOS transistor\nT T\nconducts current if no charge is stored in the floating gate (threshold voltage is V ). If the\nT\nthreshold voltage of the transistor is equal to V+, then the transistor remains in the off state.\nT\nIt thus does not conduct any current. We typically assume that the default state (no charge on\nthe floating gate) corresponds to the 1 state. When the floating gate is charged with electrons,\nwe assume that the transistor is in the 0 state. When we set the voltage at the control gate to\na value between V and V+, we enable the floating gate transistor.\nT T\nNow, to write a value of 0 or program the transistor, we need to deposit electrons in the\nfloating gate. This can be done by applying a strong positive voltage to the control gate, and\na smaller positive voltage to the drain terminal. Since there is a positive potential difference\nbetween the drain and source, a channel gets established between the drain and source. The\ncontrol gate has an even higher voltage, and thus the resulting electric field pulls electrons from\nthe n-type channel and deposits some of them in the floating gate.\nSimilarly, to erase the stored 0 bit, we apply a strong negative voltage between the control\ngate and the source terminal. The resulting electric field pulls the electrons away from the\nfloating gate into the substrate, and source terminal. At the end of this process, the floating\ngate loses all its negative charge, and the flash device comes back to its original state. It now\nstores a logical 1.\nTo summarise, programming a flash cell means writing a logical 0, and erasing it means\nwriting a logical 1. There are two fundamental ways in which we can arrange such floating gate\ntransistors to make a basic flash memory cell. These methods are known as NOR flash and\nNAND flash respectively.\nNOR FLash\nFigure 12.42 shows the topology of a two transistor NOR flash cell that saves 2 bits. Each\nfloating gate transistor is connected to a bit line on one side, and to the ground on the other\nside. Each of the control gates are connected to distinct word lines. After we enable a floating\ngate transistor it pulls the bit line low if it stores a logical 1, otherwise it does not have any\neffect because it is in the off state. Thus the voltage transition in the bit line is logically the\nreverse of the value stored in the transistor. The bit line is connected to a sense amplifier that\nsenses its voltage, flips the bit, and reports it as the output. Similarly, for writing and erasing\nwe need to set the word lines, bit lines, and source lines to appropriate voltages. The advantage\nof NOR flash is that it is very similar to a traditional DRAM cell. We can build an array of\nNOR flash cells similar to a DRAM array. The array based layout allows us to access each\nindividual location in the array uniquely. (cid:13)c Smruti R. Sarangi 682\nBit\nline\nWL1\nWL2\nFigure 12.42: NOR Flash Cell\nNAND Flash\nBit line\nG sr eo leu cn td WL8 WL7 WL6 WL5 WL4 WL3 WL2 WL1 B si et lelin cte\nFigure 12.43: NAND Flash Cell\nA NAND flash cell has a different topology. It consists of a set of NMOS floating gate\ntransistors in series similar to series connections in CMOS NAND gates (refer to Figure 12.43).\nThere are two dedicated transistors at both ends known as the bit line select transistor, and\nground select transistor. A typical array of transistors connected in the NAND configuration\ncontains 8 or 16 transistors. To read the value saved in a certain transistor in a NAND flash\narray,therearethreesteps. Thefirststepistoenablethegroundselect,andbitlinetransistors.\nThe second step is to turn on the rest of the floating gate transistors other than the one we\nwish to read by setting their word line voltages to V+. Lastly, we read a specific transistor by\nT\nsetting its word line voltage to some value between V and V+. If the cell is not programmed\nT T\n(contains a 1), it drives the bit line low, otherwise it does not change the voltage on the bit\nline. Sense amplifiers infer the value of the logical bit saved in the transistor. Such arrays of 683 (cid:13)c Smruti R. Sarangi\nfloating gate transistors known as NAND flash cells are connected in a configuration similar to\nNOR flash cells.\nThis scheme might look complicated at the outset; however, it has a lot of advantages.\nConsequently, most of the flash devices in use today use NAND flash memories instead of NOR\nflash memories. The bit storage density is much higher. A typical NAND flash cell uses a lesser\nnumber of wires than a NOR flash cell because all the floating gate transistors are directly\nconnected to each other, and there is just one connection to the bit line and ground terminal.\nHence, NAND flash memories have at least 40-60% higher density as compared to NOR flash\ncells. Let us thus only consider NAND flash memories from now on, and refrain from discussing\nNOR flash memories.\nBlocks and Pages\nThe most important point to note here is that a (NAND) flash memory device is not a memory\ndevice, it is a storage device. Memory devices provide byte level access. In comparison, storage\ndevices typically provide block level access, where one block can be hundreds of kilobytes long.\nDue to temporal and spatial locality in accesses to storage media, the working set of most\nprograms is restricted to a few blocks. Secondly, to reduce the number of accesses to storage\ndevices, most operating systems have in-memory storage caches such as hard disk caches. Most\nof the time, the operating system reads and writes to the in-memory caches. This reduces the\nI\/O access time.\nHowever, after certain events it is necessary to synchronise the cache with the underlying\nstorage device. For example, after executing a sync() system call in Linux, the hard disk cache\nwritesitsupdatestotheharddisk. Dependingonthesemanticsoftheoperatingsystem,andfile\nsystem, writes are sent to the underlying storage media after a variety of events. For example,\nwhen we right click on the icon for an USB drive in the \u201cMy Computer\u201d screen on Windows\nand select the eject option, the operating system ensures that all the outstanding write requests\nare sent to the USB device. Most of the time users simply unplug an USB device. This practice\ncan occasionally lead to data corruption, and unfortunately your author has committed this\nmistake several times. This is because, when we pull out an USB drive, some uncommitted\nchanges are still present in the in-memory cache. Consequently, the USB pen drive contains\nstale and possibly half-written data.\nData in NAND flash devices is organised in the granularity of pages and blocks. A page\nof data typically contains 512 \u2013 4096 bytes (in powers of 2). Most NAND flash devices can\ntypically read or write data at the granularity of pages. Each page additionally has extra bits\nfor error correction based on CRC codes. A set of pages are organised into a block. Blocks can\ncontain 32 \u2013 128 pages, and their total size ranges from 16 \u2013 512 KB. Most NAND flash devices\ncan erase data at the level of blocks. Let us now look at some of the salient points of NAND\nflash devices.\nProgram\/Erase Cycles\nWriting to a flash device essentially means writing a logical 0 bit since by default each floating\ngate transistor contains a logical 1. In general, after we have written data to a block, we\ncannot write data again to the same block without performing additional steps. For example,\nif we have written 0110 to a set of locations in a block, we cannot write 1001 to the same (cid:13)c Smruti R. Sarangi 684\nset of locations without erasing the original data. This is because, we cannot convert a 0 to\na 1, without erasing data. Erasing is a slow operation, and consumes a lot of power. Hence,\nthe designers of NAND flash memories decided to erase data at large granularities, i.e., at the\ngranularity of a block. We can think of accesses to flash memory as consisting of a program\nphase, where data is written, and an erase phase, where the data stored in all the transistors of\nthe block is erased. In other words, after the erase phase, each transistor in the block contains\na logical 1. We can have an indefinite number of read accesses between the program phase, and\nthe erase phase. A pair of program and erase operations is known as a program\/erase cycle, or\nP\/E cycle.\nUnfortunately, flash devices can endure a finite number of P\/E cycles. As of 2013, this\nnumber is between 100,000 to 1 million. This is because each P\/E cycle damages the silicon\ndioxide layer surrounding the floating gate. There is a gradual breakdown of this layer, and\nultimately after hundreds of thousands of P\/E cycles it does not remain an electrical insulator\nanymore. It starts to conduct current, and thus a flash cell loses its ability to hold charge. This\ngradual damage to the insulator layer is known as wear and tear. To mitigate this problem,\ndesigners use a technique called wear levelling.\nWear Levelling\nThe main objective of wear levelling is to ensure that accesses are symmetrically distributed\nacross blocks. If accesses are non-uniformly distributed, then the blocks that receive a large\nnumber of requests will wear out faster, and develop faults. Since data accesses follow both\ntemporal and spatial locality we expect a small set of blocks to be accessed most often. This\nis precisely the behaviour that we wish to prevent. Let us further elaborate with an example.\nConsider a pen drive that contains songs. Most people typically do not listen to all the songs\nin a round robin fashion. Instead they most of the time listen to their favourite songs. This\nmeans that a few blocks that contain their favourite songs are accessed most often, and these\nblocks will ultimately develop faults. Hence, to maximise the lifetime of the flash device, we\nneed to ensure that all the blocks are accessed with roughly the same frequency. This is the\nbest case scenario, and is known as wear levelling.\nThe basic idea of wear levelling is that we define a logical address and a physical address\nfor a flash device. A physical address corresponds to the address of a block within the flash\ndevice. Thelogicaladdressisusedbytheprocessorandoperatingsystemtorefertodatainthe\nflash drive. We can think of the logical address as virtual memory, and the physical address as\nphysical memory. Every flash device contains a circuit that maps logical addresses to physical\naddresses. Now, we need to ensure that accesses to blocks are uniformly distributed. Most\nflash devices have an access counter associated with each block. This counter is incremented\nonce every P\/E cycle. Once the access count for a block exceeds the access counts of other\nblocksbyapredefinedthreshold, itistimetoswapthecontentsofthefrequentlyaccessedblock\nwith another less frequently accessed block. Flash devices use a separate temporary block for\nimplementing the swap. First the contents of block 1 are copied to it. Subsequently, block 1\nis erased, and the contents of block 2 are copied to block 1. The last step is to erase block 2,\nand copy the contents of the temporary block to it. Optionally, at the end, we can erase the\ncontents of the temporary block. By doing such periodic swaps, flash devices ensure that no\nsingle block wears out faster than others. The logical to physical block mapping needs to be 685 (cid:13)c Smruti R. Sarangi\nupdated to reflect the change.\nDefinition 163\nA technique to ensure that no single block wears out faster than other blocks is known as\nwear levelling. Most flash devices implement wear levelling by swapping the contents of a\nblock that is frequently accessed with a block that is less frequently accessed.\nRead Disturbance\nAnother reliability issue in flash memories is known as read disturbance. If we read the contents\nof one page continuously, then the neighbouring transistors in each NAND cell start getting\nprogrammed. Recall that the control gate voltage of the neighbouring transistors needs to be\ngreater than V+ such that they can pass current. In this case, the voltage of the gate is not\nT\nas high as the voltage that is required to program a transistor, and it also lasts for a shorter\nduration. Nonetheless, a few electrons do accumulate in the floating gate. After thousands of\nread accesses to just one transistor, the neighbouring transistors start accumulating negative\ncharge in their floating gates, and ultimately get programmed to store a 0 bit.\nTo mitigate this problem, most designs have a read counter with each page or block. If the\nread counter exceeds a certain threshold, then the flash controller needs to move the contents\nof the block to another location. Before copying the data, the new block needs to be erased.\nSubsequently, we transfer the contents of the old block to the new block. In the new block, all\nthe transistors that are not programmed start out with a negligible amount of negative charge\nin their floating gates. As the number of read accesses to the new block increases, transistors\nstart getting programmed. Before we reach a threshold, we need to migrate the block again.\n12.9 Summary and Further Reading\n12.9.1 Summary\nSummary 12\n1. The I\/O system connects the processor to the I\/O devices. The processor and all the\nchips for processing I\/O data (chipset), are attached to a printed circuit board known\nas the motherboard. The motherboard also contains ports (hardware connectors) for\nattaching I\/O devices.\n(a) The most important chips in the chipset are known as the North Bridge and\nSoutbridge chips in Intel-based systems.\n(b) The North Bridge chip connects the processor to the graphics card, and main\nmemory. (cid:13)c Smruti R. Sarangi 686\n(c) The South Bridge chip is connected to the North Bridge chip and a host of I\/O\nand storage devices such as the keyboard, mouse, hard disk, and network card.\n2. Most operating system define two basic I\/O operations namely read and write. An\nI\/O request typically passes from the application to the I\/O device through the kernel,\ndevice driver, the processor, and elements of the I\/O system.\n3. We divide the functionality of the I\/O system into 4 layers.\n(a) The physical layer defines the electrical specifications, signalling and timing pro-\ntocols of a bus. It is further divided into two sublayers namely the transmission\nsublayer, and the synchronisation sublayer.\n(b) Thedatalinklayergetsasequenceoflogicalbitsfromthephysicallayer, andthen\nperforms the tasks of framing, buffering, and error correction. If multiple devices\nwant to access the bus, then the process of scheduling the requests is known as\narbitration. The data link layer implements arbitration, and also has support for\nI\/O transactions (sequence of messages between sender and receiver), and split\ntransactions (transactions divided into multiple mini transactions).\n(c) The network layer helps to route data from the processor to I\/O devices and back.\n(d) The protocol layer is concerned with implementing an entire I\/O request end-to-\nend.\n4. Physical Layer:\n(a) Transmission Sublayer Protocols: active high, active low, return to zero (RZ),\nnon return to zero (NRZ), non return to zero inverted (NRZI), and Manchester\nencoding.\n(b) Synchronisation Sublayer Protocols: synchronous (same clock for both ends),\nmesochronous (fixed phase delay), plesiochronous (slow drift in the clock), source\nsynchronous (clock passed along with the data), and asynchronous (2 phase hand-\nshake, and 4 phase handshake).\n5. Data Link Layer\n(a) Framing protocols: bit stuffing, pauses, bit count\n(b) Error Detection\/ Correction: parity, SEC, SECDED, CRC\n(c) Arbitration: centralised, daisy chain (supports priority, and notion of tokens)\n(d) Transaction: single transaction (example, DRAM bus), split transaction (break\na transaction into smaller transactions)\n6. Network Layer\n(a) I\/O Mapped I\/O: Each I\/O port is mapped to a set of registers that have unique\naddresses. The in and out instructions are used to read and write data to the\nports respectively. 687 (cid:13)c Smruti R. Sarangi\n(b) Memory Mapped I\/O: Here, we map the I\/O ports to the virtual address space.\n7. Protocol Layer:\n(a) Polling: Keep querying the device for a change in its state.\n(b) Interrupts: The device sends a message to the processor, when its status changes.\n(c) DMA (Direct Memory Access): Instead of transferring data from an I\/O device\ntomainmemorybyissuingI\/Oinstructions, thedevicedriverinstructstheDMA\nengine to transfer a chunk of data between I\/O devices and main memory. The\nDMA engine interrupts the processor after it is done.\n8. Case Studies:\nProtocol Usage Salient Points\nPCI Express motherboard bus high speed asynchronous bus, sup-\nports multiple lanes\nSATA storage devices half duplex, asynchronous bus, sup-\nports low level commands on storage\ndevices\nSAS storage devices full duplex, asynchronous bus, back-\nward compatible with SATA, exten-\nsive SCSI command set\nUSB peripherals single bit, half duplex, asynchronous\nbus. Extensive support for all\nkinds of traffic (bulk, interrupt, and\nisochronous).\nFireWire peripherals full duplex, data strobe encoding.\nPeripherals organised as a computer\nnetwork with a leader node.\n9. Storage Devices: Hard Disk\n(a) In a hard disk we encode data by changing the relative orientations of tiny mag-\nnets on the surface of the platter.\n(b) We group a set of typically 512 bytes into a sector. On a platter, sectors are\narranged in concentric circles known as tracks.\n(c) Theheadofaharddiskfirstneedstomovetotherighttrack(seektime), thenwait\nfor the correct sector to arrive under the head (rotational latency). Finally, we\nneed to transfer the data to the processor after post processing (transfer latency).\n10. Storage Devices: Optical disc\n(a) The surface of an optical disc contains flat region (lands), and depressions (pits).\nThe pits have lower reflectivity. (cid:13)c Smruti R. Sarangi 688\n(b) Optical discs rotate on a spindle similar to a platter in a hard disk. The optical\nhead focuses a laser light on the surface of the disc, and then an array of pho-\ntodetectors analyse the reflected light. A transition between a pit and a land (or\nvice versa) indicates a logical 1. Otherwise, we read a logical 0.\n(c) CDs (compact discs) are first generation optical discs, DVDs are second gener-\nation optical discs, and Blu-Ray discs are third generation optical discs.\n11. Storage Devices: Flash Memory\n(a) Flash memory contains a floating gate transistor that has two gates \u2013 control and\nfloating. If the floating gate has accumulated electrons then the transistor stores\na logical 0 (else it stores a logical 1).\n(b) Weprogram(setto0)afloatinggatetransistorbyapplyingahighpositivevoltage\npulse to the control gate. Likewise, we erase the value when we apply a pulse\nwith the opposite polarity.\n(c) Floating gate transistors can be connected in the NAND and NOR configurations.\nThe NAND configuration has much higher density and is thus more commonly\nused.\n(d) While designing flash devices we need to perform wear levelling, and take the\nphenomenon of read disturbance into account.\n12.9.2 Further Reading\nFor the latest designs of motherboards, and chipsets, the most accurate and up to date source\nof information is the vendor\u2019s website. Most vendors such as Intel and AMD post the details\nand configurations of their motherboards and chipsets after they are released. However, they\ntypically do not post a lot of details about the architecture of the chips. The reader can refer to\nresearch papers for the architectural details of the AMD Opteron North Bridge chip [Conway\nand Hughes, 2007], Intel Blackford North Bridge chip [Radhakrishnan et al., 2007], and AMD\nNorth Bridge chip [Owen and Steinman, 2008] for the Griffin processor family. The book by\nDally and Poulton [Dally and Poulton, 1998] is one of the best sources for information on the\nphysical layer, and is a source of a lot of information presented in this book. Other books on\ndigital communication [Proakis and Salehi, 2007, Sklar, 2001] are also excellent resources for\nfurther reading. Error control codes are mostly taught in courses on coding and information\ntheory. Hence, for a deeper understanding of error control codes, the reader is referred to\n[Ling and Xing, 2004, Cover and Thomas, 2013]. For a detailed description of the Intel\u2019s I\/O\narchitecture and I\/O ports, we shall point the reader to Intel\u2019s software developer manuals at\n[int, ]. The best sources for the I\/O protocols are their official specifications \u2013 PCI Express [pci,\n], SATA [sat, ], SCSI and SAS [scs, ], USB [usb, ], and FireWire [fir, ]. The book on memory\nsystems by Jacob, Ng, and Wang [Jacob et al., 2007] is one of the best resources for additional\ninformation on hard disks, and DRAM memories. They explain the structure of a hard disk,\nand its internals in great detail. The official standards for compact discs are documented in\nthe rainbow books. Each book in this collection contains the specifications of a certain type "}