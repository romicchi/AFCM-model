{
    "text": "The Art of Polynomial Interpolation  The Art of Polynomial Interpolation\nSTUART MURPHY The Art of Polynomial Interpolation by Stuart Murphy is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License,\nexcept where otherwise noted. Contents\nIntroduction 1\nTechniques 3\nChapter One - Elimination (Substitution) Interpolation 7\nChapter One \u2013 Practice Exercises 11\nChapter Two - Newton's Divided Difference Interpolation 14\nChapter Two \u2013 Practice Exercises 24\nChapter Three - Quadratic Spline Interpolation 27\nChapter Three - Practice Exercises 41\nChapter Four - Least Squares Regression 43\nChapter Four \u2013 Practice Exercise 51\nChapter Five - Measuring the Least Squares Fit/Exponential Least Squares Regression 52\nChapter Five - Practice Exercise 58\nChapter Six - Approximation with Taylor Series 59\nChapter Six \u2013 Practice Exercise 66\nChapter Seven - Taylor Series Remainder Test 67\nChapter Seven - Practice Exercise 68\nSolutions to Selected Practice Exercises 69\nAcknowledgements\nAbout the Author 83\nVersioning History 84  Introduction\nThe inspiration for this text grew out of a simple question that emerged over a number of years of teaching math to\nMiddle School, High School and College students.\nPractically speaking, what is the origin of a particular polynomial?\nSo much time is spent analyzing, factoring, simplifying and graphing polynomials that it is easy to lose sight of the\nfact that polynomials have a wealth of practical uses. Exploring the techniques of interpolating data allows us to view\nthe development and birth of a polynomial. This text is focused on laying a foundation for understanding and applying\nseveral common forms of polynomial interpolation. The principal goals of the text are:\n1. Breakdown the process of developing polynomials to demonstrate and give the student a feel for the process and\nmeaning of developing estimates of the trend (s) a collection of data may represent.\n2. Introduce basic matrix algebra to assist students with understanding the process without getting bogged down in\npurely manual calculations. Some manual calculations have been included, however, to assist with understanding\nthe concept.\n3. Assist students in building a basic foundation allowing them to add additional techniques, of which there are many,\nnot covered in this text.\nWhat this text is not:\nIt is not a comprehensive survey of interpolation techniques.\nThe techniques presented are ones the author believes will provide a basic understanding of polynomial interpolation\nthat students can build upon. There are many flavors and sub flavors of interpolation and I encourage students who are\ninterested to check them out.\nIt is not a lesson in using interpolation apps:\nQuite the opposite. By engaging in exercising calculations, the student is better equipped to understand how and why\nthese techniques work.\nWhat is polynomial interpolation?\nWe experience information in discreet ways.\nTypically, it comes from measurements or observations. However, what we often want to do is look at a continuous\nprocess the data represents; all at once or at least at any point we choose. While we cannot represent a continuous\nprocess with a single number we can do so with an equation. Graphically this equation could be a single point (not\nusually that interesting). A straight line (degree one polynomial), a curved line (degree two polynomial) that we often\ncall a quadratic equation or parabola; or some higher degree that graphically, often begins to look like a wave repeating\nitself.\nWhen dealing with data the specific numbers always represent a snapshot. For example, if we measure rainfall and\nwind speed each day for a year, we have a collection of data points that compare rainfall to wind speed. We might ask if\nthere is a relationship between wind speed and rainfall. For example, hurricane force winds are usually accompanied by\nheavy rainfall. It would be nice to develop an equation that can predict rainfall when high winds are expected. Normally\nsomeone analyzing this data would plot the points on the x, y coordinate plane. In this text, the sample data used to\nillustrate the various interpolation methods will be plotted in this way.\nCan the math stand alone? Most certainly not. The challenge for someone utilizing interpolation techniques is to\napply expertise and experience to determine the most appropriate polynomial structure. In other words, is the model\nmost likely to accurately (or at least reasonably) produce useful estimated values? This is what I mean by the \u201cArt\u201d of\npolynomial interpolation.\nInterpolation uses a known set of independent and dependent values to estimate other dependent values, typically\nalong a continuous line represented by a polynomial. Technically if you use the model to identify additional data points\nIntroduction | 1 outside of the range of the given points this is known as extrapolation. Our focus will be on interpolation within the\ngiven range.\nAdaptations of the techniques we explore have been used in pre-computer times to generate tables of trig or log\nvalues used in applications such as navigation. Nowadays they are adapted for use by computers and calculators and\nthey are an important part of the tool kit researchers use to predict future events such as emerging storms tracks,\nclimate change, political elections, changing demographics, spread of disease, and so forth.\nWe will explore five Interpolation techniques: Elimination (Substitution), Newton\u2019s Divided Difference, Splines, Least\nSquares and Taylor Series.\n2 | Introduction Techniques\nA Brief Explanation of the Techniques Presented in This Text\nA) Elimination (Substitution) (or solving a linear system of n equations with n\nunknowns)\nEssentially this technique utilizes a process known to high school Algebra One students: Elimination (or substitution).\nThis allows for solving a set of n-unknown variables in a set of n-equations.\nB) Newton\u2019s Divided Difference\nNewton\u2019s Divided Difference interpolation has many applications. Historically it and similar techniques have been used\nto develop trigonometric and logarithmic tables.\nAn important advantage is that if you start out with a handful of known points plotted on a coordinate plane you can\ndecide on an appropriate degree polynomial that would be representative of the general trend. A benefit is that any new\ngiven points can easily be added one at a time thus increasing the degree of the polynomial for each new point added,\nwithout having to start over.\nC) Splines\nSplines interpolation is a great technique to employ if there are certain discreet points that modify the nature of the\ntrend. For example, the trajectory of a rocket launch could be broken into segments: Launch to Stage One separation,\ninterval to Stage Two separation, a major scheduled course adjustment and so on. Each of the resulting intervals could be\nrepresented by a separate polynomial. Spline interpolation creates just such separate polynomials while at the same time\nrecognizing the continuity inherent in the event and building that into the resulting set of equations that collectively\nrepresent the event. A variation of this would be a single spline developed in a sub-interval of the domain that is of\nparticular interest.\nD) Least Squares Regression\nPolynomial Least Squares regression is useful for fitting a polynomial such as a quadratic equation to many data\npoints ensuring that each point influences the resulting polynomial in such a manner that the resulting polynomial is\nconsidered a best fit for that set of data.\nTechniques | 3 E) Taylor Series Polynomial\nA technique that employs use of the Taylor Series to develop a polynomial that approximates the actual function at and\nnear a given domain value. It does not require a set of data points. The major limitation is that it works for a limited class\nof functions.\nNote there are plenty of applications that will provide the desired results very quickly. However, this textbook is\nmeant to assist students with an understanding of the computations and reasons for them. Included are the manual\ncalculations with explanation as well as basic Matrix commands that students can use to mirror the manual calculations.\nLet\u2019s look at a simple example.\nAssumption: The faster a car is driven the lower the fuel efficiency.\nSample Vehicle Fuel Efficiency Measurements\nX (Miles per Hour) Y (Miles per Gallon)\nMPH MPG\n45 43\n55 42\n65 38\n75 32\nLong Description\nChart of Sample Vehicle Speed\n4 | Techniques Figure 1 \u2013 Comparing Linear to Quadratic Interpolation Methods\nLong Description\nFigure 1 - Comparing Linear to Quadratic Interpolation Methods\nThe above plot suggests two likely scenarios. The question is: Which more accurately represents what is really\nhappening?\nLinear: Pick two points that seem reasonable and draw a straight line (red) through them.\nQuadratic: Someone else looking at the data might conclude the curved line (blue) is more reasonable and accurate.\nVisually we would conclude that the quadratic is mathematically a better fit because the curve is significantly closer\nto the given data points. However, it is important to remember that while this is true, an automotive expert applying\nexpertise and experience may conclude that in fact the linear interpolation is more meaningful or that more data points\nare needed. We want to keep in mind that the \u201cArt\u201d component is what has to be applied to determine what degree\npolynomial and which technique will provide the best approximation.\nTechniques | 5 6 | Techniques Chapter One - Elimination (Substitution)\nInterpolation\nA common method for solving the resulting system of equations is using linear algebra and matrix math. However,\nneither are necessary to illustrate this technique and apply to a practical problem. We will use elimination to solve the\nexample below. While I think it is important students experience how basic algebra works for interpolation, they will\nquickly see that unless the numbers are small and simple this particular technique quickly becomes unwieldy for large\nvalues generated during the process.\nFor example:\nSample Vehicle Fuel Efficiency Measurements\nX (Miles per Hour) Y (Miles per Gallon)\nMPH MPG\n45 43\n55 42\n65 38\n75 32\nLong Description\nSample Vehicle Fuel Efficiency Measurement\nApply expertise and experience to create a polynomial that will reasonably predict the fuel efficiency of the particular\nvehicle used to gather the above data.\nStep one: Deciding that a quadratic equation looks like the best fit, we select the first, second and fourth points to\nconstruct a second degree (quadratic) polynomial.\nStep Two: Even though the result will be a quadratic equation we are able to use straightforward linear techniques of\nelimination and substitution. The reason for this is that we are not trying to find x and y. The three points we selected\nalready give us those. Instead, we are trying to create the quadratic in standard form by solving for the unknown\nconstants a, b and c:\nStep Three: Lets create three quadratic equations with the same three unknowns a, b, c and replacing x, y in each\nwith the actual data point values.\nEq. one: \u2014\u2014\u2014\u2013>\nChapter One - Elimination (Substitution) Interpolation | 7 Eq. two: \u2014\u2014\u2014\u2013>\nEq. three: \u2014\u2014\u2014\u2013>\nStep Four: The elimination process:\n45(Eq. two) \u2013 55( Eq one):\n[ ] \u2013 [ ]\nEq. four: b is eliminated\n55(Eq. three) \u2013 75(Eq. two):\n[ ] \u2013 [ ]\nEq. five: b is eliminated\nConduct elimination on the resulting two equations with two unknowns to eliminate c.\n2(Eq. four) \u2013 Eq. five:\n______________________\nEq. six: c is eliminated\nPlugging the resulting value of a into Eq. 4 allows us to solve for c:\nStep Five: Substitute a and c into any of the original equations to find b:\nOur interpolated polynomial is:\nFor students looking for a less manual process here is the setup using matrix math to run the calculations in a\nspreadsheet.\n8 | Chapter One - Elimination (Substitution) Interpolation Figure 1.1 The Matrix Math formula\nFigure 1.2 \u2013 Setup of Solution in Matrix Notation\nChapter One - Elimination (Substitution) Interpolation | 9 Long Description\nFigure 1.2 - Setup of Solution in Matrix Notation\nLet\u2019s look between 45 and 55 at and see how well our polynomial estimates a reasonable value:\nIt is recommended that the original points also be plugged into the equation as a check.\nPlotting on our graph shows that this is indeed a very good estimate.\nFigure 1.3 \u2013 Line graph displaying the results of the Quadratic Interpolation\nLong Description\nFigure 1.3 - Line graph displaying the results of the Quadratic Interpolation\n10 | Chapter One - Elimination (Substitution) Interpolation Chapter One \u2013 Practice Exercises\n1a)\nThe owner of the ABC Children\u2019s Party Company has offered a limited menu of pricing options depending on the number\nof children attending the party. The available prices are included in the table below:\nABC Children's Party Company\nMaximum children attending the party Cost per Child Total Cost of Party\n10 $37 $370\n25 $28 $700\n50 $22 $1100\n100 $15 $1500\nLong Description\nABC Children's Party Company\nThe prices cover the cost plus acceptable profit and have worked well in the past. To improve the companies\ncompetitiveness, the owner would like to offer more flexible pricing that is specific to the actual number of children.\nShe would like to develop a cubic (3rd degree) polynomial that will generate the unit price when she inputs the expected\nnumber of children attending the party. To develop this polynomial the student must use the algebraic technique of\nsubstitution (elimination) discussed in this chapter.\n(Solution Given)\n1b)\nThis exercise offers practice in using basic matrix commands either manually or in a spreadsheet program to solve n-\nequations in n-unknowns.\n(Solution given for 2nd to 5th row of data)\nChapter One \u2013 Practice Exercises | 11 Given the following data points, develop a polynomial that will interpolate any value of p(x) on the given interval, for\nthe bracketed points. It will result in a third-degree polynomial:\nExercise 1b Sample Data Point\nx y or f(x)\n-4 12\n[-1.75] [-2]\n[1] [-3.7]\n[3.3] [-1.4]\n[6.9] [4]\n7 3.9\n9.1 6\nLong Description\nExercise 1b Sample Data Point\nTables are provided to assist students\nFigure 1.4 Guide for students\n12 | Chapter One \u2013 Practice Exercises Long Description\nTables to Assist Atudents\n1c)\nSelect any three data points from the above table and develop a 2nd degree (quadratic) Polynomial.\nChapter One \u2013 Practice Exercises | 13 Chapter Two - Newton's Divided Difference\nInterpolation\nA quick word regarding Divided Difference. The title might suggest that derivatives are involved, and in a way that would\nbe correct. The good news is that knowledge of derivatives is not necessary for this technique. However, students should\nbe familiar with the concept of slope, slope-intercept form and how slope is calculated since the process utilizes the\nchange in the dependent variable (commonly known as y or f(x)) divided by the change in the independent variable\n(commonly known as x).\nStudents may have already encountered Divided Difference technique in high school algebra when asked to analyze\na set of data to determine the non-linear (usually quadratic) equation that produced the dependent variable, as the\nfollowing example illustrates.\nExample\nGiven the following set of x values, determine the quadratic (2nd degree polynomial) that correctly produces the\ncorresponding y values. Show in standard form:\nSample Data\nx y\n-2 25.2\n-1 11.3\n0 2\n1 -2.7\n2 -2.8\nLong Description\nSample Data\n14 | Chapter Two - Newton's Divided Difference Interpolation Solution\nThis simplified use of Newton\u2019s Divided Difference works because one of the x values is zero and there is a uniform\ndistance of one between each x value.\nFigure 2.1 Simplified use of Newton\u2019s Divided Difference\nLong Description\nFigure 2.1 Simplified use of Newton\u2019s Divided Difference\nSince the 2nd divided differences are all the same this tells us that there is a quadratic solution\nwith\nBy plugging in the x,y values (0,2) we can easily solve for c as follows:\nOr simply . Now that we know a and c we plug those in using one of the other points such as (1,-2.7) and solve\nfor b as follows: which simplifies to\nResulting in the solution equation of which works for all given points and approximates\neverything in between.\nChapter Two - Newton's Divided Difference Interpolation | 15 Newton\u2019s Divided Difference Interpolation generalizes the above process. The given points no longer have to be in any\nparticular order and the x values do not have to be spaced at uniform intervals; offering a welcome flexible technique.\nThe Generalized Process\nUsing Newton\u2019s Divided Difference approach, let\u2019s develop a polynomial that takes a limited number of data points (think\npoints plotted on the coordinate plane) and fit them to a polynomial that is continuous across the interval.\nThis method is an iterative process that allows us to begin with one point. We can then add additional data points at\nour discretion, especially if we believe they will produce a better, more representative, polynomial.\nEach time we add a point the resulting polynomial increases by a degree resulting in a polynomial of degree one less\nthan the number of points included in the interpolation process.\n(x1, y1): Constant Function:\n(x1, y1), (x2, y2): Linear Function: f_1(x) = a_1x + C\n(x1, y1), (x2, y2), (x3, y3): Quadratic Function:\n.\n.\n.\nresulting in an degree Function:\nThe following example illustrates the iterative process and demonstrates its validity.\nI) The Constant Solution\nThe Constant Solution\nx f(x)\n-2 3\nLong Description\nThe Constant Solution\n16 | Chapter Two - Newton's Divided Difference Interpolation the constant solution\nII) The Linear Solution: By adding a second point we move to a straight-line solution\nThe Linear Solution\nx f(x)\n-2 3\n-1 -4\nLong Description\nThe Linear Solution\nThis is accomplished by preserving the constant solution while adding a linear component that works\nfor all points on the straight line passing through both given points as follows.\nThis added component will not alter the solution for while\nintroducing the appropriate linear structure (degree one polynomial).\nSolving for ensuring f(x) will satisfy both points and everything on the line passing through the two\ngiven points.\nThus\nSimplifying since this is valid slope intercept form, we have a linear solution\nChecking 1st point it checks\nChecking 2nd point it checks\nChapter Two - Newton's Divided Difference Interpolation | 17 III) The Quadratic Solution \u2013 2nd degree polynomial\nThe Quadratic Solution\nx f(x)\n-2 3\n-1 -4\n3 6\nLong Description\nThe Quadratic Solution\nAdding a third point, allows for the development of a quadratic (2nd degree) equation. We repeat the process with the\nsame goal:\npreserving the constant solution at the first point and the linear solution for first two points. The newly added third\npoint will be satisfied by the previous linear solution plus the added quadratic component.\nthis component (in red) ensures this new solution works for previous points as well as establishing a valid quadratic\nform.\nRemember\nSolving for the constant:\nPlug in and simplify\nAs a check we will plug in our three given values of x to verify it produces the corresponding given y values.\nCheck One\n18 | Chapter Two - Newton's Divided Difference Interpolation CheckTwo\nCheck Three\nWe have engaged in an iterative process. Utilizing generalized notation for the above we conducted three iterations,\nwith an additional point added at each iteration.\nSingle point: :\nConstant Solution\nSecond Point Added: :\nsolving for linear\nThird Point Added: :\nsolve for quadratic\nEach new iteration builds upon and preserves the previous solutions.\nIn general, the solution polynomial can continue to be increased one degree at a time solving for each new variable as\nlong as additional points become available. This results in the following general form:\nNormally it is best to select the lowest order polynomial that is reasonable. Higher order polynomials can introduce\nunwanted error.\nThe table approach below offers a convenient methodology for manually calculating the constants. It lends itself to\nadding additional points as needed without having to start over.\nThe following Table Methodology illustrates and simplifies the above process.\nChapter Two - Newton's Divided Difference Interpolation | 19 Figure 2.2 Table Methodology\nLong Description\nFigure 2.2 Table Methodology\nStarting at the right-hand column we backtrack diagonally left and up (circled in red). Backtracking left and downward\nwould have produced the same simplified equation (circled in green)\nThis produces the following results:\nSimplifying:\nThis satisfies the three given points as well as any interpolated points between the minimum and maximum value of\nx. Because it is a continuous function, it also produces extrapolated points beyond the range. These extrapolated points\nmay or may not be valid for any particular situation being analyzed. That is part of the \u201cArt\u201d of interpolation which relies\non the experience and expertise of the one studying a particular phenomenon.\n20 | Chapter Two - Newton's Divided Difference Interpolation The Sin function \u2013 An interesting example\nOne of the neat things we can use interpolation for is to create a polynomial that provides reasonable estimates of\nthe sin (or cos) of an angle for any given measure. In fact, the numbers we will use are small and simple that even the\nElimination (Substitution) approach will easily produce the desired result.\nThe Sine function illustrated on the coordinate plane\nFigure 2.3 Sine Function Graph\nLong Description\nFigure 2.3 Sine Function Graph\nChapter Two - Newton's Divided Difference Interpolation | 21 Figure 2.4 Estimating sin wave \u2013 Newton\u2019s Divided Difference Table\nLong Description\nFigure 2.4 Estimating sin wave - Newton's Divided Difference Table\nSimplifying the resulting equation produces:\n22 | Chapter Two - Newton's Divided Difference Interpolation Figure 2.5 An approximation of sin value\nLong Description\nFigure 2.5 Sine Function Approximation\nChapter Two - Newton's Divided Difference Interpolation | 23 Chapter Two \u2013 Practice Exercises\n2a)\nWhile the owner in exercise 1a) was happy with the results of using elimination/substitution, she was curious to see\nif the results would differ using Newton\u2019s Divided Difference (NDD) interpolation. You have decided to assist her by\ngenerating a cubic polynomial using NDD. (Solution given) The data is:\nABC Children's Party Company\nMaximum children attending the party Cost per Child Total Cost of Party\n10 $37 $370\n25 $28 $700\n50 $22 $1100\n100 $15 $1500\nLong Description\nABC Children's Party Company\n2b)\nUsing the same seven data points from the previous chapter select three data points and plug into the grid below to\nproduce a quadratic solution. Simplify the resulting polynomial and put in standard form. Note solution given for the\nthree bracketed points.\n(Solution given)\n24 | Chapter Two \u2013 Practice Exercises Seven Data Points\nx y or f(x)\n-6.2 -8\n[-3] [-7]\n-1.5 -2.2\n[1] [0.7]\n3.5 3\n4.25 5\n[7.9] [11]\nLong Description\nSeven Data Points\nExercise 2b Answer Grid\n- x f(x) 1st divided difference 2nd divided difference\n- -\n- - - - -\n- - - - -\n- - - - -\n- - - - -\n- - - - -\nLong Description\nExercise 2b answer grid\nChapter Two \u2013 Practice Exercises | 25 2c)\nAdd an additional data point and develop a 3rd degree (cubic) polynomial. Compare this to the solution from 2a) and\ndecide whether or not it improves the interpolation. Note student answers may vary\n26 | Chapter Two \u2013 Practice Exercises Chapter Three - Quadratic Spline Interpolation\nThis technique offers several advantages over other techniques. It produces a smooth curve over the interval being\nstudied while at the same time offering a distinct polynomial for each subinterval (known as Splines). Secondly it\neliminates some of the problems inherent in trying fit a single higher order polynomial which can actually produce\nmisleading estimates by being too precise.\nOne disadvantage that we quickly discover is that the resulting set of polynomials can be taxing to solve manually\nusing techniques such as elimination/substitution, Gauss-Jordan reduction or Cramer\u2019s rule. Fortunately, many\napplications including most spreadsheet programs allow us to solve the resulting system, easily producing the family of\nequations.\nLet\u2019s begin with a simple case that the student can choose to solve manually to can gain an understanding of the\nprocess. The matrix operations are shown as well.\nSpline Example\nFigure 3.1 Spline Example\nChapter Three - Quadratic Spline Interpolation | 27 Long Description\nFigure 3.1 Spline Example\nInstead of one equation we could have an equation representing the interval [2,5] and a second equation [5,7]. The key\nis that the point in the middle contributes to both equations creating a connection that ensures a smooth handoff from\nthe first to the second equation. The general form is:\nSince we want to solve for the six constants in a proper linear fashion, we need four more equations. To find them we\nemploy the connection at . Since each equation satisfies two endpoints this allows us to double the number of\nequations as follows:\nWe now have four equations. The fifth equation we can develop at the point (5,8) known as an internal knot. Note the\ntwo endpoints are sometimes referred to as external knots.\nIf we take the derivative of the two equations at , we know they have to be equal because the slope has to be\nthe same at that point. We can set them equal to each other and simplify. This results in:\nRearrange\nWe now have five of the six equations\nThis is the sixth equation; see explanation below.\nThe sixth equation is based on the assumption that the line leaving the endpoint is a straight line. The quadratic\ncomponent zeros out thus our sixth equation is simply . The other endpoint would have produced\nwhich would have worked equally well. We\u2019ll use these six equations and solve with matrix operations.\n28 | Chapter Three - Quadratic Spline Interpolation Constants Displayed in Matrix Form\na1 b1 c1 a2 b2 c2 y\n4 2 1 0 0 0 1\n25 5 1 0 0 0 8\n0 0 0 25 5 1 8\n0 0 0 49 7 1 3\n10 1 0 -2 -10 0 0\n1 0 0 0 0 0 0\nFor illustrative purposes a detailed flow of the matrix operation is offered below:\nFigure 3.2 Matrix Operation Flow\nChapter Three - Quadratic Spline Interpolation | 29 Long Description\nFigure 3.2 Matrix Operation Flow\nFigure 3.3 Two Spline Interpolation Equations\nLong Description\nFigure 3.3 Two Spline Interpolation Equations\n30 | Chapter Three - Quadratic Spline Interpolation Example \u2013 Space Launch Data\nThe following combines a general explanation of the technique along with a specific example. We will use the following\nlaunch data for the Saturn 5 rocket. Note this data was pulled from readily available data for several launches and in\nfact does not represent any one launch. The data tracks a hypothetical Saturn 5 from launch until third stage shutdown\nshortly before entering earth orbit.\nSpace Launch Data\nx (time in minutes) y (velocity in 1000ft per second)\n0 1\n1 2\n2.5 9\n3 9.2\n4 10\n5 12\n6 14.5\n7 17\n8 20\n8.75 23\n9 23.5\n10 24\n11 25.5\n11.25 25.9\n11.5 25.9\nSelected Interval Points (knots)\nx y Interval\n1 2 start of first interval\n2.5 9 1st stage separation\n8.75 23 2nd stage separation\n11.25 25.9 3rd stage shutdown\nChapter Three - Quadratic Spline Interpolation | 31 Figure 3.4 Plot of Launch-Knots Identified\nLong Description\nFigure 3.4 Plot of Launch-Knots Identified\nSince we have selected data points we create quadratic spline equations each with three\nunknowns:\nWe want to solve for the unknowns. However, with only three equations we need to create six additional\nequations in order to apply one of the standard techniques for solving n equations in n unknowns.\nNotice that each equation is a solution for two of the knots as shown in figure 1. This allows us to split each spline\nequation into two equations providing a total of n= 6 equations as follows:\n32 | Chapter Three - Quadratic Spline Interpolation We are getting closer. We will now create two more equations using basic knowledge of the derivative and the fact that\ntwo pairs of equations are solutions for the two interior knots. This works because the first derivative of each equation\nin a pair will have the same slope at the common data point (knot).\nThis is not a course in calculus so I will simply show the first derivatives for each pair to obtain our additional\nequations.\nat\nthe seventh equation\nat\nthe eighth equation\nFor our ninth equation we recognize that at each endpoint the resulting line extending beyond the interval is a straight\nline. Since this eliminates the quadratic component, we can simply make our ninth equation be:\na_1 = 0\nWe now have our nine equations with nine unknowns. Figure 4 below includes the nine equations.\nChapter Three - Quadratic Spline Interpolation | 33 Figure 3.5 The Nine Equations\nLong Description\nFigure 3.5 The Nine Equations\nGathering the equations and squaring the quadratic variables results in following nine equations with nine unknowns.\nThe x variables are replaced with the x-value from the related knot.\na_1(1) + b_1(1) + c_1 = 2\na_1(6.25) + b_1(2.5) + c_1 = 9\na_2(76.56) + b_2(8.75) + c_2 = 23\n34 | Chapter Three - Quadratic Spline Interpolation It would be a cumbersome task to solve the above system by hand. Instead, we will put the data in matrix form and\nsolve.\nNine Equations Solved with Matrix Math\n1 1 1 0 0 0 0 0 0 a1 2\n6.25 2.5 1 0 0 0 0 0 0 b1 9\n0 0 0 6.25 2.5 1 0 0 0 c1 9\n0 0 0 76.56 8.75 1 0 0 0 a2 23\n0 0 0 0 0 0 76.56 8.75 1 b2 23\n0 0 0 0 0 0 126.56 11.25 1 c1 25.9\n5 1 0 -5 -1 0 0 0 0 a3 0\n0 0 0 17.5 1 0 -17.5 -1 0 b3 0\n1 0 0 0 0 0 0 0 0 c3 0\nPlug the above into a spreadsheet and apply matrix operations as follows:\nFigure 3.6 Nine Equations Solved with Matrix Math\nChapter Three - Quadratic Spline Interpolation | 35 Long Description\nFigure 3.6 Nine Equations Solved with Matrix Math\nThe calculations produced three polynomials for the interval\nThese equations produce reasonable estimates for the overall flight pattern as shown in figure 3.7.\nFigure 3.7 A solution for a space launch\n36 | Chapter Three - Quadratic Spline Interpolation Long Description\nFigure 3.7 Saturn 5 Rocket Possible Solution\nDirect Method Cubic Interpolation\nCubic interpolation takes us to the next level and is a common method for developing an equation that approximates\nf(x) for a particular value of x as well the neighborhood on either side made up of the four closest given data points. It\nis well suited if we want to interpolate for a particular interval of x. This will not provide a family of polynomials that\nsatisfy the domain of the function. Rather it provides that single cubic polynomial that gives us a good picture of what is\nhappening at and near a particular point of interest. This approach allows us to setup and solve a single cubic equation.\nThe principal limitation is that it is not valid for the entire domain of x only the four closest points. Since we often only\nwant to look at a limited range the benefits of a significant reduction in algebraic manipulation outweighs the limitation.\nWe will use our table of data from the previous example.\nSaturn 5 Rocket Launch Data\nx (time in minutes) y (velocity in 1000 ft per second\n0 1\n1 2\n2.5 9\n3 9.2\n4 10\n5 12\n6 14.5\n7 17\n8 20\n8.75 23\n9 23.5\n10 24\n11 25.5\n11.25 25.9\n11.5 25.9\nChapter Three - Quadratic Spline Interpolation | 37 Let\u2019s say we want to estimate the velocity when minutes. We check the points on either side to determine\nthe four closest values to 5.85 (shown in red).\nClosest Values to x=5.85\nChecking Distances Four Data Points\n5.85 - 3 = 2.85 -\n5.85 - 4 = 1.85 (4,10)\n5.85 - 5 = 0.85 (5,12)\n6 - 5.85 = 0.15 (6,14.5)\n7 - 5.85 = 1.15 (7,17)\n8 - 5.85 = 2.15 -\nOther Data Points From Example\nx (time in minutes) y (velocity in 1000 ft per second)\n4 10\n5 12\n6 14.5\n7 17\nUtilizing the standard form for a cubic polynomial allows us to quickly set up four equations with four unknowns.\nRemember we are not finding x and y we already know those. Rather our unknowns are the constants a,b,c,d.\nUsing high school algebra (elimination/substitution), Gauss Jordan reduction or some other method, solve for the four\nunknowns. Below shows the setup using Matrix math to solve the cubic polynomial in a spreadsheet program.\n38 | Chapter Three - Quadratic Spline Interpolation Figure 3.8 Solved Cubic Polynomial via Spreadsheet Program\nLong Description\nFigure 3.8 Solved Cubic Polynomial via Spreadsheet Program\nResulting Equation\nfor the interval\nLet\u2019s see how well our cubic polynomial fits when plotted against all the given points plus x = 5.85\nChapter Three - Quadratic Spline Interpolation | 39 Figure 3.9 Graph of Cubic Interpolation\nLong Description\nFigure 3.9 Direct Method Cubic Interpolation\nNotice that the solution provides the best estimate in the neighborhood of the closest points.\n40 | Chapter Three - Quadratic Spline Interpolation Chapter Three - Practice Exercises\n3a)\nUsing the data from the Saturn launch example in chapter three calculate the family of quadratic splines for the\nfollowing different Selected Interval Data Points (knots) and compare to the example.\nSaturn 5 Rocket Launch Data\nx (time in minutes) y (velocity in 1000 ft per second\n0 1\n1 2\n2.5 9\n3 9.2\n4 10\n5 12\n6 14.5\n7 17\n8 20\n8.75 23\n9 23.5\n10 24\n11 25.5\n11.25 25.9\n11.5 25.9\nSelected Interval Points (knots)\nx y Interval\n1 2 start of first interval\n2.5 9 1st stage separation\n8.75 23 2nd stage separation\n11.25 25.9 3rd stage shutdown\nChapter Three - Practice Exercises | 41 3b)\nUsing the table in 3a) for time = 7.5, conduct a Direct Method Cubic Interpolation. Show the resulting polynomial in\nstandard form and graph the solution manually or with your favorite graphing tool.\n(Solution given)\n42 | Chapter Three - Practice Exercises Chapter Four - Least Squares Regression\nThis technique is often used when many points of data are involved and the analyst would like the resulting polynomial\nto be influenced by all the identified points. The degree of the Interpolated polynomial should be selected ahead of time\nbased on the expertise of the analyst. As a general rule of thumb, the lowest degree polynomial that appears to fit is the\nbetter choice. So, one might fit a quadratic or cubic solution to a large number of points which could run to dozens or\neven hundreds of points. The result will always be considered mathematically a best fit to the data.\nTo gain an understanding of the underlying principle and process we will begin with a simple data set consisting of\nfive points.\nScenario\nA helium balloon that gathers meteorological data is released. For each mile it rises, the distance it travels downrange is\nalso recorded. The data is recorded in the following table.\nAltitude and Downrange\nAltitude - x miles Downrange - y miles\n1 2\n2 3\n3 5\n4 5\n5 4\nChapter Four - Least Squares Regression | 43 Figure 4.1 Data points for a Helium Balloon\nLong Description\nFigure 4.1 Helium Balloon Data Points\nLet\u2019s begin with the simplest model \u2013 the straight line. We want to find a best fit linear equation that minimizes the\nsum of the distances between the actual and interpolated values of y for a given value of x.\n1) A generalized linear equation will serve as our starting point.\n2) It is easy to see that with a little rearranging we have an equation that lends itself to finding that minimum distance\nmentioned above: y - (ax + b) = 0\nWe will square this equation so that resulting differences in distance are always positive as we are not interested in\nthe direction of the difference but the sum of the differences.\nSince we want the sum of these squared equations, we have the following for this example:\nInterestingly by squaring these equations we will obtain a quadratic equation which will be useful in finding a linear\nsolution. In fact, it will allow us to create two partial derivative equations for each of the constants we are trying to solve\nfor. In this case a, b. This will result in two linear equations in two unknowns which we can solve using elimination/\n44 | Chapter Four - Least Squares Regression substitution or more advanced techniques such as matrix computations. And because they are upward facing quadratics,\nwe minimize each equation be setting them to zero.\n1)\n2)\nNext, we simplify each equation by distributing the summation notation. And, since they are equal to zero, we simply\ndivide out the -2. We now have two equations in two unknowns a,b.\nSimplify 1)\nSimplify 2)\nWe now have two equations in two unknowns a, b. Let\u2019s calculate the various sums and plug in.\nChapter Four - Least Squares Regression | 45 I) Plug in to set up the two equations as follows:\nOne:\nTwo:\nII) Rearrange:\nOne: 55a + 15b = 63\nTwo: 15a + 5b = 19\nIII) Apply substitution/elimination to solve for a, b\nWe now have a polynomial that can interpolate values in the\ninterval [1,5]\nor\nFigure 4.2 Graph of Linear Solution\n46 | Chapter Four - Least Squares Regression Long Description\nFigure 4.2 Graph of Linear Solution\nAs we can see, the linear solution offers an estimate that is closer to some of the given points than others. Can we do\nbetter by generating a curved line? (2nd degree polynomial)\nThe Quadratic Solution\nThe challenge is to expand on the above technique and apply it to develop the best fit quadratic equation.\nIn the linear, our goal was to solve two equations in two unknowns. Now we want to solve three equations in three\nunknowns. The unknowns are the constants of our quadratic equation in standard form:\nRearranging the standard form, we develop the Least Squares Summation equation:\nNow we take partial derivatives with respect to each of the three constants a, b, c as follows:\na \u2014>\nb \u2014>\nc \u2014>\nSimplify by dividing out the -2 and distributing the summation notation\nChapter Four - Least Squares Regression | 47 Let\u2019s calculate the additional sums needed. We already calculated some of the sums for the linear\nequation. These are:\nAdditional sums:\nPlugging in shows the three equations in three unknowns:\n48 | Chapter Four - Least Squares Regression Rearranging\nSolving manually or using spreadsheet software the following equation is obtained:\nThis is the interpolation polynomial that generates a curved line (parabola) that is the best fit for the five given data\npoints and it estimates y values for any other point within interval.\nMatrix Operations simplify the calculations\nNote: multiplying the transpose by the matrix produces the summation in n-equations with n-unknowns. This holds\ntrue no matter how many data points are involved.\nFigure 4.3 Matrix Operations Simplifying Calculations\nLong Description\nFigure 4.3 Matrix Operations Simplifying Calculations\nChapter Four - Least Squares Regression | 49 Figure 4.4 Graph of Quadratic Solution\nLong Description\nFigure 4.4 Graph of Quadratic Solution\nVisually, the quadratic is a better fit than the linear solution.\nIn the next section we\u2019ll show how to measure the goodness of the fit quantitatively.\n50 | Chapter Four - Least Squares Regression Chapter Four \u2013 Practice Exercise\n4a)\nUse weekly closing data for the Dow Jones Industrial Average and run a Least Squares Regression to produce a 3rd\ndegree (cubic) interpolation polynomial. Plot the data on a chart for a visual representation. Solution given uses data\nfrom January 2020 through July 2021, during height of the COVID-19 pandemic.\n(Solution given)\nChapter Four \u2013 Practice Exercise | 51 Chapter Five - Measuring the Least Squares Fit/\nExponential Least Squares Regression\nHow Well Does the Linear Polynomial Fit the Data?\nIt is natural and useful to ask: How good a predictor is the resulting polynomial for the given values of x. In other words,\nhow close do the predicted values of y come to the actual values of y for a particular value of x.\nLet\u2019s look at the chart for the linear regression we calculated (red dotted line) in Chapter Four. The length of red\nvertical lines between the actual and predicted values tells us how good the fit is. The smaller the red lines (closer), the\nbetter the fit.\nFigure 5.1 The Linear Fit\nLong Description\nFigure 5.1 The Linear Fit\nHowever, simply measuring each distance and adding them together presents some problems. We want to eliminate\n52 | Chapter Five - Measuring the Least Squares Fit/Exponential Least Squares Regression direction because the negatives and positives tend to cancel each other out. An easy way to do this is measure each\ndistance and then square the result. Hence the name Least Squared Regression.\nNext, we need a baseline or something to compare our summed squared regression. It turns out a horizontal line\npassing through the mean of the y values offers us a worst-case scenario. In other words, the distance between the\ngiven y and the horizontal line is essentially no fit. So we add the given y values and divide by 5 (number of data points in\nthis example). Shown in green above. The closer the predicted\nvalue is from the actual value and the farther it is from the mean value, the better our prediction.\nUsing the data above we will conduct a (Squared Regression) analysis to gauge numerically how well the linear\nand quadratic polynomials fit the data.\nSquared Regression Analysis\nGenerated y values\nx y Difference between actual and generated squared:\n1 2 2.6 0.36\n2 3 3.2 0.04\n3 5 3.8 1.44\n4 5 4.4 0.36\n5 4 5 1\n- - -\nHowever, to put this in perspective we need to add a column and calculate the sum of the squared distance between\nthe actual values of y and the mean value of y.\nSquared Regression Analysis with Total Differences\nGenerated y values Difference between actual and generated Total Squared difference between actual and\nx y\nsquared: mean.\n1 2 2.6 0.36 3.24\n2 3 3.2 0.04 0.64\n3 5 3.8 1.44 1.44\n4 5 4.4 0.36 1.44\n5 4 5 1 0.04\n- - -\nBy taking the ratio of the sum of our squared error to the sum of the No-Fit values and subtracting from one we get a\nnumber (percent) that tells us how good our fit is in terms that is understandable.\nChapter Five - Measuring the Least Squares Fit/Exponential Least Squares Regression | 53 The value of 53% suggests that this may not be the best fit.\nLet\u2019s calculate for the quadratic fit to see if it is a better fit.\nSquared Regression Analysis with Different Generated y Values\nGenerated y values Difference between actual and Total Squared difference between\nx y\ngenerated squared: actual and mean.\n1 2 1.7428 0.06615184 3.24\n2 3 3.6284 0.39488656 0.64\n3 5 4.6568 0.11778624 1.44\n4 5 4.828 0.029584 1.44\n5 4 4.142 0.020164 0.04\n- - -\nThe quadratic is a better fit than the straight line. However, part of the \u201cArt\u201d of interpolation means the analyst still\nhas to decide which is more meaningful and representative of the situation being analyzed.\nExponential Least Squares Regression\nAn important interpolation is one involving exponential polynomials. It has many applications in finance, biochemistry,\nand radioactive decay.\nWe will focus on the standard form using the constant e. This is known as the natural number or Euler\u2019s number\nvalue. Its importance lies in the fact that it represents the fundamental rate of growth shared by continually growing\nprocesses. One example is continuous compounding of money in a savings account.\nThe form of the polynomial is\nIn this, we can think of r as the rate and A we can think of as both the y intercept and demonstrating whether it is\ngrowth (positive value) or decay (negative value).\nGraphically it looks like (A and r are both set to 1):\n54 | Chapter Five - Measuring the Least Squares Fit/Exponential Least Squares Regression Figure 5.2 Exponential Growth\nLong Description\nFigure 5.2 Exponential Growth.\ndoes not lend itself to directly calculating an interpolative polynomial. This is due in part because\nstandard deviation does not apply to this type of continuous and ever accelerating growth.\nSince we already know how to deal with standard polynomials that can be solved used linear techniques such as matrix\narithmetic, our goal is to eliminate e. Solve for r and A then plug the results back into the original polynomial.\nSince we are dealing with the natural number e, we can convert the above to a linear function by taking the natural\nlog of both sides as follows:\nWhen we rearrange, we have a linear equation in slope intercept form:\nLet\u2019s use the following sample set of data points and use Matrix math to develop the interpolated data:\nChapter Five - Measuring the Least Squares Fit/Exponential Least Squares Regression | 55 Interpolated Data\nx actual y Iny\n-1 0.4 -0.916\n0 1.1 0.095\n1 2.62 0.963\n2 8.1 2.092\n3 24.03 3.179\n4 57.9 4.059\nFigure 5.3 Matrix Math Solution\nLong Description\nFigure 5.3 Matrix Math Solution\n56 | Chapter Five - Measuring the Least Squares Fit/Exponential Least Squares Regression Figure 5.4 Graph of a line of fit for exponential function\nLong Description\nFigure 5.4 The Exponential Fit\nThis resulted in a very good fit.\nChapter Five - Measuring the Least Squares Fit/Exponential Least Squares Regression | 57 Chapter Five - Practice Exercise\n5a)\nMeasure the accuracy of the Fit from Exercise 4a, i.e. find .\n(Solution given)\n58 | Chapter Five - Practice Exercise Chapter Six - Approximation with Taylor Series\nWhile this text is not about calculus, I believe it is important for students to become familiar with approximation using\nTaylor Series. References to derivatives are necessary but the actual derivatives in the examples will be given.\nA way to think about Taylor Series polynomials is that they are simply a polynomial of any degree you wish to use that\napproximates a function being studied. Similar to Newton\u2019s divided difference we start with the simplest approximation,\nthe constant.\nLet\u2019s call our approximation . We will let be a particular point on the x-axis that will be the center of\nour approximation. The approximation improves the closer the value of x is to a. The function we are approximating is\nf(x).\nFor a straight line at a particular point, we can say an approximation polynomial is .\nSuppose we choose a point , the graph might look something like:\nFigure 6.1 The Horizontal Straight Line Estimator\nLong Description\nFigure 6.1 The Horizontal Straight Line Estimator\nChapter Six - Approximation with Taylor Series | 59 At , the horizontal line is an excellent approximation.\nWe can say that which is a constant.\nClearly, once we move away from a in either direction it turns out the constant does not serve us very well.\nOur next step is adding a linear component while still retaining the constant. Which means we now have a polynomial\nthat allows us to adjust the slope of the line. Let\u2019s try where is the first\nderivative of the function.\nBy adding the linear component, we can think of as the slope. This improves our approximation:\nFigure 6.2 The Linear Solution\nLong Description\nFigure 6.2 The Linear Solution\nBy adding the linear component, we can see how the picture improves at the point because we now have a\nline tangent (representing the slope at ). Definitely an improvement over the constant as our approximation is\npretty good as long as we stay near a.\nSo far, we have brought to bear a constant value and the slope. Because Taylor series allows us to add higher degree\n60 | Chapter Six - Approximation with Taylor Series terms to our polynomial, we can now bring to bear the effect of concavity to the approximation. Think of concavity as\nadding curviness to what so far has been a straight line.\nLet\u2019s add a quadratic (second degree) and cubic (third degree) component to our polynomial. These will introduce\nthe curviness by adjusting the line at any given x value up or down. Figure 3 also illustrates the effect of higher order\npolynomials.\nFigure 6.3 Effect of Higher Order Polynomials\nLong Description\nFigure 6.3 Effect of Higher Order Polynomials\nWe can see that as each higher-level component is added the approximation improves the farther we travel from the\npoint x = a.\nQuadratic:\nCubic:\nWe could continue this indefinitely:\n. . .\nFrom here we will develop the general form of the Taylor series employing basic algebra.\nThis is done iteratively by solving one constant at a time. We set since in fact all Taylor polynomials either\nstart with or include the adjustment, so that in effect the center will always equal zero.\nChapter Six - Approximation with Taylor Series | 61 We will solve for a fourth-degree polynomial. This will be enough to demonstrate the general pattern of the Taylor\nseries. To solve for each constant, we replace each of the with as follows:\nsince\nnext we take first derivative of both sides\nAgain so we are left with\nThe second derivative of both sides\nsince we\u2019re left with\nThe third derivative of both sides\nsince we\u2019re left with\nThe fourth derivative\nPlugging in the solution for the four constants produces the general form:\nNormally we don\u2019t show the denominators when they are simply one. However, I\u2019ve done so to illustrate the emerging\npattern. Remember and . This allows us to observe that the denominators are really successive\nfactorials.\n. . .\n62 | Chapter Six - Approximation with Taylor Series Sin Function\nLet\u2019s use an actual example to illustrate the process. Some things to remember. Taylor Series approximation only works\nfor certain functions; typically, those that are continuous, repeatedly differential and irrational. They are also known as\ntranscendental functions. Trig functions such as sin and cos, as well as exponential and logarithmic functions, imperfect\nroots, along with several other categories work well. Suppose we have been assigned a project to create our own App\nthat will generate sin values.\nWe will focus on the mechanics of the process. For students who would like to delve deeper into Taylor Series there\nare a wealth of texts and videos available.\nStep One: Select the function to be approximated. For this example, we will choose the sin function. It is well suited\nfor Taylor Series approximation. It is continuous over the real numbers and it is repeatedly differential.\nStep Two: Select an value that we want to center our approximation around. It turns out 0 degree is an easy\nvalue to work with as we differentiate sin.\nStep Three: Repeatedly differentiate sin until the desired final degree of our Taylor Polynomial is reached. In this\nexample we arbitrarily decided a ninth degree Taylor polynomial will produce Sin values accurate enough to meet our\nneeds. Note we will work with radians as the angle measure.\nDerivatives\nStep Four: Plug in our derivatives into the general form of the Taylor polynomial:\nSince every other term has zero in the numerator we can drop these and condense p(0). Further since a = 0, we can\nsimplify the binomials.\nThe resulting Taylor Series polynomial is:\n\\large p(0) = \\frac {1}{1!}(x) + \\frac {-1}{3!}(x)^3 + \\frac {1}{5!}(x)^5 + \\frac {-1}{7!}(x)^7 + \\frac {1}{9!}(x)^9\nWe have a relatively simple polynomial we can program into our app to produce values of sin for angles between 0 and\n90 degrees . Since sin is periodic, we can program in computations that give us the reference angle for angles greater\nthan 90 or less than 0 degrees .\nStep Five: We are now ready to test p(a) for various angles between 0 and 90 degrees. Since it is easier to work\nwith Radians, I\u2019ve included a conversion for students not familiar with them. f(x) is generated from an app precise to 15\ndecimal positions. p(x) is our Taylor approximation.\nChapter Six - Approximation with Taylor Series | 63 Step 5 of Taylor Approximation\nDegrees Radians f(x) p(x)\n0 0 0.000000000000000 0.000000000000000\n18 0.309016994374947 0.309016994375021\n22.5 0.382683432365090 0.382683432365947\n30 0.500000000000000 0.500000000000000\n45 0.707106781186547 0.707106782936867\n72 0.951056516295154 0.951056822327524\n90 1.000000000000000 1.000003542584290\np(x) provides an excellent approximation out to at least six decimal places for the values of x we tested. The symmetry\n0 0\nand reflectivity properties of the sin function will allow us to generate values less than 0 and greater than 90 .\n64 | Chapter Six - Approximation with Taylor Series Figure 6.4 Graph of Taylor Approximation\nLong Description\nFigure 6.4 Taylor Approximation of Sin Function\nNote: Difference slight enough that lines appear to overlap on the graph.\nChapter Six - Approximation with Taylor Series | 65 Chapter Six \u2013 Practice Exercise\n6a)\nReplicate the above example (sin) for the cos. Compare the resulting graph to the one for sin.\n(Solution given)\n66 | Chapter Six \u2013 Practice Exercise Chapter Seven - Taylor Series Remainder Test\nA formal way to test the accuracy of a Taylor polynomial approximation is to employ the Taylor Remainder test. By\nadding a remainder term to our Taylor polynomial approximation, we in effect convert it into an equation,\nOur function . Written more compactly we have\nf_n(x) = p_n(x) + r_n(x) . This remainder term becomes the difference between at a particular point and\nat that same value of x.\nIn the above example we ran our polynomial out to the ninth-degree term.\nactually looks like the next higher degree term:\nwhere c is between a and x\nThe question we ask is what value for c should we use. The answer in this case is to solve the remainder twice for the\nendpoints of the range we are interested in. In this case we want to know how accurate c will be between and .\nThis will provide a range of possible values between and\nFor\nFor we drop the\nnegative as it\u2019s a matter of distance, not direction.\nThis bounds the possible error of our approximation:\nChapter Seven - Taylor Series Remainder Test | 67 Chapter Seven - Practice Exercise\n7a)\nConduct the Taylor Remainder Test on your solution for Practice Problem 6a.\n(Solution given)\n68 | Chapter Seven - Practice Exercise Solutions to Selected Practice Exercises\nSolution to Exercise One Practice Problems\nExercise 1a)\nABC Children's Party Company\nMaximum children attending the party Cost per Child Total Cost of Party\n10 $37 $370\n25 $28 $700\n50 $22 $1100\n100 $15 $1500\nThe four equations in four unknowns:\nEquations in Table Form\na b c d cost\n1000 100 10 1 37\n15625 625 25 1 28\n125000 2500 50 1 22\n1000000 10000 100 1 15\nSolutions to Selected Practice Exercises | 69 Figure 8.1 Matrix Setup for Exercise 1a\nLong Description\nFigure 8.1 Matrix Setup for Exercise 1a\nResulting Pricing Polynomial\n70 | Solutions to Selected Practice Exercises Figure 8.2 Exercise 1b\nLong Description\nFigure 8.2 Exercise 1b)\nSolutions to Selected Practice Exercises | 71 Solution to Exercise Two Practice Problems\n2a)\nNewton\u2019s Divided Difference Table is populated as follows:\nNewton\u2019s Divided Difference Table\nCubic\nx y Linear Quadratic\n10 37 37 \u2013 \u2013 \u2013\n\u2013 \u2013 \u2013 \u2013 \u2013\n25 28 28 \u2013 \u2013\n\u2013 \u2013 \u2013 \u2013\n50 22 22 \u2013 \u2013\n\u2013 \u2013 \u2013 \u2013 \u2013\n100 15 15 \u2013 \u2013 \u2013\nSimplifies to:\n72 | Solutions to Selected Practice Exercises 2b)\n2b Table\nx y or f(x)\n-6.2 -8\n-3 -7\n-1.5 -2.2\n1 0.7\n3.5 3\n4.25 5\n7.9 8\n2b Difference Table\nx f(x) 1st Divided Difference 2nd Divided Difference\n-\n-3 -7 - -\n- - -\n1 0.7 -\n- - -\n7.9 11 - -\n2\nSimplifies to: -0.040x + 1.845x \u2013 1.105\nSolutions to Selected Practice Exercises | 73 Solution to Chapter Three Practice Exercises\nExercise 3b)\nFigure 8.3 Exercise 3b\nLong Description\nFigure 8.3 Exercise 3b\n74 | Solutions to Selected Practice Exercises Solution to Chapter Four Practice Exercise\n4a)\nThe Setup:\nAbbreviated List of weekly Dow Jones closing averages:\nWeekly Closing Averages\nWeek Actual Interpolation\n1 28,583.68 1.00 1.00 1.00 1 28,416.89149\n2 28,939.67 8.00 4.00 2.00 1 28,034.20169\n3 29,196.04 27.00 9.00 3.00 1 27,677.60694\n4 28,722.85 64.00 16.00 4.00 1 27,346.5493\n- - - - - - -\n- - - - - - -\n- - - - - - -\n78 34,292.29 474,552.00 6,084.00 78.00 1 34,491.0287\n79 34,577.37 493,039.00 6,241.00 79.00 1 34,485.14307\n80 34,888.79 512,000.00 6,400.00 80.00 1 34,462.39157\n81 34,511.99 531,441.00 6,561.00 81.00 1 34,422.21628\n82 35,058.52 551,368.00 6.724.00 82.00 1 34,364.05925\n83 35,084.53 571,787.00 6,889.00 83.00 1 34,287.36256\nSolutions to Selected Practice Exercises | 75 Figure 8.4 Matrix Solution 4a\nLong Description\nFigure 8.4 Matrix Solution 4a\n76 | Solutions to Selected Practice Exercises Figure 8.5 Graph of Weekly DJIA\nLong Description\nFigure 8.5 Weekly DJIA Close January 2020 through July 2021\nSolution to Chapter Five Practice Exercises\nStep One:\n1a) Find the difference between each actual value and its associated value generated by the interpolative polynomial.\nSquare the result.\n1b) Find the difference between each actual value and the Mean of the actual values. Square the result.\nStep Two:\n2a) Sum the results from 1a\n2b) Sum the results from 1b\nStep Three:\nSolutions to Selected Practice Exercises | 77 Divide 2a by 2b subtracting the result from 1.\nAnswer:\nSolution to Chapter Six Practice Exercises\n6a)\nSelect the Function to be approximated. Cos function centered at x=0\nDerivatives of cos\nPlug derivatives into the general form of the Taylor polynomial:\nEvery other term has zero in the numerator so we can drop these and condense p(0). Further since a = 0 we can\nsimplify the binomials.\nf(x) is generated from an app precise to 15 decimal positions. p(x) is the Taylor approximation for Cosine.\nFigure 8.6 Speech Bubble\n78 | Solutions to Selected Practice Exercises Taylor Approximation for Cosine\nDegrees Radians f(x) p(x)\n0 0 1.000000000000000 1.000000000000000\n18 0.951056516295154 0.951056516297732\n22.5 0.923879532511287 0.923879532535293\n30 0.866025403784439 0.866025404210352\n45 0.707106781186548 0.707106805683294\n72 0.309016994374947 0.309019668329804\n90 0.000000000000000 0.000000000000000\nSolution to Chapter Seven Practice Exercise\n7a)\nwhere c is between a and x\nSolving the remainder twice for 0 and\nThis will provide a range of possible values between 0 and\nSolutions to Selected Practice Exercises | 79 for\nfor\nDrop negative as it is a matter of distance not direction.\nGives us an error possibility\n80 | Solutions to Selected Practice Exercises ACKNOWLEDGEMENTS\nContent Editor: Darius Rub, MS University at Buffalo\nContent Editor: Charlene Cardinale, Math and Science educator\nCover Design: Denise J. Murphy-Rohr, Graphic Designer\nCopy Editor: Christina Riehman-Murphy, Open & Affordable Educational Resources Librarian, Penn State Libraries\nAcknowledgements | 81 82 | Acknowledgements About the Author\nFigure 10.1 Stuart Murphy\nStuart Murphy spent a number of years working in the insurance industry, managing and implementing health plans for\ncommercial and government entities. During this time, he also served as a registered lobbyist.\nOver the years Stu has taught middle, high school, and college level math; as well as COBOL and Assembler. Stu\ncurrently teaches middle school mathematics.\nHe and his wife Sharon have three children and eight grandchildren. They make their home in Pennsylvania.\nAbout the Author | 83 Versioning History\nThis page will provide a record of edits and changes made to this book since its initial publication in July 2022. Whenever\nedits or updates are made, we make the required changes in the text and provide a record and description of those\nchanges here. If the change is minor, the version number increases by 0.1. However, if the edits involve substantial\nupdates, the version number goes up to the next full number.\nIf you find an error in this book, please contact smurph11@gmail.com or cer20@psu.edu. We will make the necessary\nchanges, and update this Versioning History page to reflect the edits made.\nVersion Date Change Details\n1.1 July 2022 Initial Publication \u2013\n84 | Versioning History"
}