{"text":"Introduction\nAround 1998 Free Software emerged from a happily subterranean\nand obscure existence stretching back roughly twenty years. At the\nvery pinnacle of the dotcom boom, Free Software suddenly popu-\nlated the pages of mainstream business journals, entered the strat-\negy and planning discussions of executives, confounded the radar\nof political leaders and regulators around the globe, and permeated\nthe consciousness of a generation of technophile teenagers growing\nup in the 1990s wondering how people ever lived without e-mail.\nFree Software appeared to be something shocking, something that\neconomic history suggested could never exist: a practice of creat-\ning software\u2014good software\u2014that was privately owned, but freely\nand publicly accessible. Free Software, as its ambiguous moniker\nsuggests, is both free from constraints and free of charge. Such\ncharacteristics seem to violate economic logic and the principles of\nprivate ownership and individual autonomy, yet there are tens of millions of people creating this software and hundreds of millions\nmore using it. Why? Why now? And most important: how?\nFree Software is a set of practices for the distributed collabora-\ntive creation of software source code that is then made openly and\nfreely available through a clever, unconventional use of copyright\nlaw.1 But it is much more: Free Software exemplifies a considerable\nreorientation of knowledge and power in contemporary society\u2014a\nreorientation of power with respect to the creation, dissemination,\nand authorization of knowledge in the era of the Internet. This book\nis about the cultural significance of Free Software, and by cultural\nI mean much more than the exotic behavioral or sartorial traits of\nsoftware programmers, fascinating though they be. By culture, I\nmean an ongoing experimental system, a space of modification and\nmodulation, of figuring out and testing; culture is an experiment\nthat is hard to keep an eye on, one that changes quickly and some-\ntimes starkly. Culture as an experimental system crosses economies\nand governments, networked social spheres, and the infrastructure\nof knowledge and power within which our world functions today\u2014\nor fails to. Free Software, as a cultural practice, weaves together a\nsurprising range of places, objects, and people; it contains patterns,\nthresholds, and repetitions that are not simple or immediately obvi-\nous, either to the geeks who make Free Software or to those who\nwant to understand it. It is my goal in this book to reveal some of\nthose complex patterns and thresholds, both historically and an-\nthropologically, and to explain not just what Free Software is but\nalso how it has emerged in the recent past and will continue to\nchange in the near future.\u0018\nThe significance of Free Software extends far beyond the arcane\nand detailed technical practices of software programmers and\n\u201cgeeks\u201d (as I refer to them herein). Since about 1998, the practices\nand ideas of Free Software have extended into new realms of life\nand creativity: from software to music and film to science, engineer-\ning, and education; from national politics of intellectual property\nto global debates about civil society; from UNIX to Mac OS X and\nWindows; from medical records and databases to international dis-\nease monitoring and synthetic biology; from Open Source to open\naccess. Free Software is no longer only about software\u2014it exempli-\nfies a more general reorientation of power and knowledge.\nThe terms Free Software and Open Source don\u2019t quite capture the\nextent of this reorientation or their own cultural significance. They\n\u0018 introduction refer, quite narrowly, to the practice of creating software\u2014an ac-\ntivity many people consider to be quite far from their experience.\nHowever, creating Free Software is more than that: it includes a\nunique combination of more familiar practices that range from\ncreating and policing intellectual property to arguing about the\nmeaning of \u201copenness\u201d to organizing and coordinating people and\nmachines across locales and time zones. Taken together, these prac-\ntices make Free Software distinct, significant, and meaningful both\nto those who create it and to those who take the time to understand\nhow it comes into being.\nIn order to analyze and illustrate the more general cultural sig-\nnificance of Free Software and its consequences, I introduce the\nconcept of a \u201crecursive public.\u201d A recursive public is a public that is\nvitally concerned with the material and practical maintenance and modi-\nfication of the technical, legal, practical, and conceptual means of its\nown existence as a public; it is a collective independent of other forms of\nconstituted power and is capable of speaking to existing forms of power\nthrough the production of actually existing alternatives. Free Software\nis one instance of this concept, both as it has emerged in the recent\npast and as it undergoes transformation and differentiation in the\nnear future. There are other instances, including those that emerge\nfrom the practices of Free Software, such as Creative Commons,\nthe Connexions project, and the Open Access movement in science.\nThese latter instances may or may not be Free Software, or even\n\u201csoftware\u201d projects per se, but they are connected through the same\npractices, and what makes them significant is that they may also\nbe \u201crecursive publics\u201d in the sense I explore in this book. Recursive\npublics, and publics generally, differ from interest groups, corpora-\ntions, unions, professions, churches, and other forms of organization\nbecause of their focus on the radical technological modifiability of\ntheir own terms of existence. In any public there inevitably arises\na moment when the question of how things are said, who controls\nthe means of communication, or whether each and everyone is be-\ning properly heard becomes an issue. A legitimate public sphere is\none that gives outsiders a way in: they may or may not be heard,\nbut they do not have to appeal to any authority (inside or outside\nthe organization) in order to have a voice.\u0018 Such publics are not\ninherently modifiable, but are made so\u2014and maintained\u2014through\nthe practices of participants. It is possible for Free Software as we\nknow it to cease to be public, or to become just one more settled\nintroduction \u0018 form of power, but my focus is on the recent past and near future\nof something that is (for the time being) public in a radical and\nnovel way.\nThe concept of a recursive public is not meant to apply to any and\nevery instance of a public\u2014it is not a replacement for the concept\nof a \u201cpublic sphere\u201d\u2014but is intended rather to give readers a spe-\ncific and detailed sense of the non-obvious, but persistent threads\nthat form the warp and weft of Free Software and to analyze simi-\nlar and related projects that continue to emerge from it as novel\nand unprecedented forms of publicity and political action.\nAt first glance, the thread tying these projects together seems to\nbe the Internet. And indeed, the history and cultural significance of\nFree Software has been intricately mixed up with that of the Inter-\nnet over the last thirty years. The Internet is a unique platform\u2014\nan environment or an infrastructure\u2014for Free Software. But the\nInternet looks the way it does because of Free Software. Free Soft-\nware and the Internet are related like figure and ground or like\nsystem and environment; neither are stable or unchanging in and of\nthemselves, and there are a number of practical, technical, and his-\ntorical places where the two are essentially indistinguishable. The\nInternet is not itself a recursive public, but it is something vitally\nimportant to that public, something about which such publics care\ndeeply and act to preserve. Throughout this book, I will return to\nthese three phenomena: the Internet, a heterogeneous and diverse,\nthough singular, infrastructure of technologies and uses; Free Soft-\nware, a very specific set of technical, legal, and social practices that\nnow require the Internet; and recursive publics, an analytic concept\nintended to clarify the relation of the first two.\nBoth the Internet and Free Software are historically specific, that\nis, not just any old new media or information technology. But the In-\nternet is many, many specific things to many, many specific people.\nAs one reviewer of an early manuscript version of this book noted,\n\u201cFor most people, the Internet is porn, stock quotes, Al Jazeera\nclips of executions, Skype, seeing pictures of the grandkids, porn,\nnever having to buy another encyclopedia, MySpace, e-mail, online\nhousing listings, Amazon, Googling potential romantic interests,\netc. etc.\u201d It is impossible to explain all of these things; the meaning\nand significance of the proliferation of digital pornography is a\nvery different concern than that of the fall of the print encyclopedia\n\u0018 introduction and the rise of Wikipedia. Yet certain underlying practices relate\nthese diverse phenomena to one another and help explain why they\nhave occurred at this time and in this technical, legal, and social\ncontext. By looking carefully at Free Software and its modulations,\nI suggest, one can come to a better understanding of the changes\naffecting pornography, Wikipedia, stock quotes, and many other\nwonderful and terrifying things.\u0018\nTwo Bits has three parts. Part I of this book introduces the reader\nto the concept of recursive publics by exploring the lives, works,\nand discussions of an international community of geeks brought to-\ngether by their shared interest in the Internet. Chapter 1 asks, in an\nethnographic voice, \u201cWhy do geeks associate with one another?\u201d\nThe answer\u2014told via the story of Napster in \u0018000 and the stan-\ndards process at the heart of the Internet\u2014is that they are making\na recursive public. Chapter \u0018 explores the words and attitudes of\ngeeks more closely, focusing on the strange stories they tell (about\nthe Protestant Reformation, about their practical everyday poly-\nmathy, about progress and enlightenment), stories that make sense\nof contemporary political economy in sometimes surprising ways.\nCentral to part I is an explication of the ways in which geeks argue\nabout technology but also argue with and through it, by building,\nmodifying, and maintaining the very software, networks, and legal\ntools within which and by which they associate with one another.\nIt is meant to give the reader a kind of visceral sense of why certain\narrangements of technology, organization, and law\u2014specifically\nthat of the Internet and Free Software\u2014are so vitally important to\nthese geeks.\nPart II takes a step back from ethnographic engagement to ask,\n\u201cWhat is Free Software and why has it emerged at this point in\nhistory?\u201d Part II is a historically detailed portrait of the emergence\nof Free Software beginning in 1998\u201399 and stretching back in time\nas far as the late 1950s; it recapitulates part I by examining Free\nSoftware as an exemplar of a recursive public. The five chapters\nin part II tell a coherent historical story, but each is focused on a\nseparate component of Free Software. The stories in these chapters\nhelp distinguish the figure of Free Software from the ground of the\nInternet. The diversity of technical practices, economic concerns,\ninformation technologies, and legal and organizational practices\nis huge, and these five chapters distinguish and describe the spe-\ncific practices in their historical contexts and settings: practices of\nintroduction \u0018 proselytizing and arguing, of sharing, porting, and forking source\ncode, of conceptualizing openness and open systems, of creating\nFree Software copyright, and of coordinating people and source\ncode.\nPart III returns to ethnographic engagement, analyzing two re-\nlated projects inspired by Free Software which modulate one or\nmore of the five components discussed in part II, that is, which take\nthe practices as developed in Free Software and experiment with\nmaking something new and different. The two projects are Creative\nCommons, a nonprofit organization that creates copyright licenses,\nand Connexions, a project to develop an online scholarly textbook\ncommons. By tracing the modulations of practices in detail, I ask,\n\u201cAre these projects still Free Software?\u201d and \u201cAre these projects still\nrecursive publics?\u201d The answer to the first questions reveals how\nFree Software\u2019s flexible practices are influencing specific forms of\npractice far from software programming, while the answer to the\nsecond question helps explain how Free Software, Creative Com-\nmons, Connexions, and projects like them are all related, strategic\nresponses to the reorientation of power and knowledge. The conclu-\nsion raises a series of questions intended to help scholars looking at\nrelated phenomena.\nRecursive Publics and the Reorientation of Power\nand Knowledge\nGovernance and control of the creation and dissemination of knowl-\nedge have changed considerably in the context of the Internet over\nthe last thirty years. Nearly all kinds of media are easier to produce,\npublish, circulate, modify, mash-up, remix, or reuse. The number\nof such creations, circulations, and borrowings has exploded, and\nthe tools of knowledge creation and circulation\u2014software and\nnetworks\u2014have also become more and more pervasively available.\nThe results have also been explosive and include anxieties about\nvalidity, quality, ownership and control, moral panics galore, and\nnew concerns about the shape and legitimacy of global \u201cintellec-\ntual property\u201d systems. All of these concerns amount to a reorienta-\ntion of knowledge and power that is incomplete and emergent, and\nwhose implications reach directly into the heart of the legitimacy,\ncertainty, reliability and especially the finality and temporality of\n\u0018 introduction the knowledge and infrastructures we collectively create. It is a re-\norientation at once more specific and more general than the grand\ndiagnostic claims of an \u201cinformation\u201d or \u201cnetwork\u201d society, or the\nrise of knowledge work or knowledge-based economies; it is more\nspecific because it concerns precise and detailed technical and legal\npractices, more general because it is a cultural reorientation, not\nonly an economic or legal one.\nFree Software exemplifies this reorientation; it is not simply a\ntechnical pursuit but also the creation of a \u201cpublic,\u201d a collective\nthat asserts itself as a check on other constituted forms of power\u2014\nlike states, the church, and corporations\u2014but which remains inde-\npendent of these domains of power.\u0018 Free Software is a response to\nthis reorientation that has resulted in a novel form of democratic\npolitical action, a means by which publics can be created and main-\ntained in forms not at all familiar to us from the past. Free Software\nis a public of a particular kind: a recursive public. Recursive publics\nare publics concerned with the ability to build, control, modify, and\nmaintain the infrastructure that allows them to come into being in\nthe first place and which, in turn, constitutes their everyday practi-\ncal commitments and the identities of the participants as creative\nand autonomous individuals. In the cases explored herein, that spe-\ncific infrastructure includes the creation of the Internet itself, as\nwell as its associated tools and structures, such as Usenet, e-mail,\nthe World Wide Web (www), UNIX and UNIX-derived operating\nsystems, protocols, standards, and standards processes. For the last\nthirty years, the Internet has been the subject of a contest in which\nFree Software has been both a central combatant and an important\narchitect.\nBy calling Free Software a recursive public, I am doing two things:\nfirst, I am drawing attention to the democratic and political signifi-\ncance of Free Software and the Internet; and second, I am suggest-\ning that our current understanding (both academic and colloquial)\nof what counts as a self-governing public, or even as \u201cthe public,\u201d\nis radically inadequate to understanding the contemporary reori-\nentation of knowledge and power. The first case is easy to make:\nit is obvious that there is something political about Free Software,\nbut most casual observers assume, erroneously, that it is simply\nan ideological stance and that it is anti\u2013intellectual property or\ntechnolibertarian. I hope to show how geeks do not start with ide-\nologies, but instead come to them through their involvement in the\nintroduction \u0018 practices of creating Free Software and its derivatives. To be sure,\nthere are ideologues aplenty, but there are far more people who\nstart out thinking of themselves as libertarians or liberators, but\nwho become something quite different through their participation\nin Free Software.\nThe second case is more complex: why another contribution to\nthe debate about the public and public spheres? There are two\nreasons I have found it necessary to invent, and to attempt to make\nprecise, the concept of a recursive public: the first is to signal the\nneed to include within the spectrum of political activity the cre-\nation, modification, and maintenance of software, networks, and\nlegal documents. Coding, hacking, patching, sharing, compiling,\nand modifying of software are forms of political action that now\nroutinely accompany familiar political forms of expression like\nfree speech, assembly, petition, and a free press. Such activities are\nexpressive in ways that conventional political theory and social\nscience do not recognize: they can both express and \u201cimplement\u201d\nideas about the social and moral order of society. Software and\nnetworks can express ideas in the conventional written sense as\nwell as create (express) infrastructures that allow ideas to circulate\nin novel and unexpected ways. At an analytic level, the concept of\na recursive public is a way of insisting on the importance to public\ndebate of the unruly technical materiality of a political order, not\njust the embodied discourse (however material) about that order.\nThroughout this book, I raise the question of how Free Software\nand the Internet are themselves a public, as well as what that pub-\nlic actually makes, builds, and maintains.\nThe second reason I use the concept of a recursive public is that\nconventional publics have been described as \u201cself-grounding,\u201d as\nconstituted only through discourse in the conventional sense of\nspeech, writing, and assembly.\u0018 Recursive publics are \u201crecursive\u201d\nnot only because of the \u201cself-grounding\u201d of commitments and iden-\ntities but also because they are concerned with the depth or strata\nof this self-grounding: the layers of technical and legal infrastruc-\nture which are necessary for, say, the Internet to exist as the infra-\nstructure of a public. Every act of self-grounding that constitutes a\npublic relies in turn on the existence of a medium or ground through\nwhich communication is possible\u2014whether face-to-face speech,\nepistolary communication, or net-based assembly\u2014and recursive\npublics relentlessly question the status of these media, suggesting\n8 introduction that they, too, must be independent for a public to be authentic. At\neach of these layers, technical and legal and organizational deci-\nsions can affect whether or not the infrastructure will allow, or\neven ensure, the continued existence of the recursive publics that\nare concerned with it. Recursive publics\u2019 independence from power\nis not absolute; it is provisional and structured in response to the\nhistorically constituted layering of power and control within the\ninfrastructures of computing and communication.\nFor instance, a very important aspect of the contemporary In-\nternet, and one that has been fiercely disputed (recently under\nthe banner of \u201cnet neutrality\u201d), is its singularity: there is only one\nInternet. This was not an inevitable or a technically determined\noutcome, but the result of a contest in which a series of decisions\nwere made about layers ranging from the very basic physical con-\nfiguration of the Internet (packet-switched networks and routing\nsystems indifferent to data types), to the standards and protocols\nthat make it work (e.g., TCP\/IP or DNS), to the applications that\nrun on it (e-mail, www, ssh). The outcome of these decisions has\nbeen to privilege the singularity of the Internet and to champion\nits standardization, rather than to promote its fragmentation into\nmultiple incompatible networks. These same kinds of decisions are\nroutinely discussed, weighed, and programmed in the activity of\nvarious Free Software projects, as well as its derivatives. They are,\nI claim, decisions embedded in imaginations of order that are si-\nmultaneously moral and technical.\nBy contrast, governments, corporations, nongovernmental orga-\nnizations (NGOs), and other institutions have plenty of reasons\u2014\nprofit, security, control\u2014to seek to fragment the Internet. But it is\nthe check on this power provided by recursive publics and espe-\ncially the practices that now make up Free Software that has kept\nthe Internet whole to date. It is a check on power that is by no\nmeans absolute, but is nonetheless rigorously and technically con-\ncerned with its legitimacy and independence not only from state-\nbased forms of power and control, but from corporate, commercial,\nand nongovernmental power as well. To the extent that the Internet\nis public and extensible (including the capability of creating private\nsubnetworks), it is because of the practices discussed herein and\ntheir culmination in a recursive public.\nRecursive publics respond to governance by directly engaging in,\nmaintaining, and often modifying the infrastructure they seek, as a\nintroduction 9 public, to inhabit and extend\u2014and not only by offering opinions or\nprotesting decisions, as conventional publics do (in most theories\nof the public sphere). Recursive publics seek to create what might\nbe understood, enigmatically, as a constantly \u201cself-leveling\u201d level\nplaying field. And it is in the attempt to make the playing field\nself-leveling that they confront and resist forms of power and con-\ntrol that seek to level it to the advantage of one or another large\nconstituency: state, government, corporation, profession. It is im-\nportant to understand that geeks do not simply want to level the\nplaying field to their advantage\u2014they have no affinity or identity\nas such. Instead, they wish to devise ways to give the playing field\na certain kind of agency, effected through the agency of many dif-\nferent humans, but checked by its technical and legal structure and\nopenness. Geeks do not wish to compete qua capitalists or entre-\npreneurs unless they can assure themselves that (qua public actors)\nthat they can compete fairly. It is an ethic of justice shot through\nwith an aesthetic of technical elegance and legal cleverness.\nThe fact that recursive publics respond in this way\u2014through di-\nrect engagement and modification\u2014is a key aspect of the reori-\nentation of power and knowledge that Free Software exemplifies.\nThey are reconstituting the relationship between liberty and knowl-\nedge in a technically and historically specific context. Geeks create\nand modify and argue about licenses and source code and proto-\ncols and standards and revision control and ideologies of freedom\nand pragmatism not simply because these things are inherently or\nuniversally important, but because they concern the relationship\nof governance to the freedom of expression and nature of consent.\nSource code and copyright licenses, revision control and mailing\nlists are the pamphlets, coffeehouses, and salons of the twenty-first\ncentury: Tischgesellschaften become Schreibtischgesellschaften.\u0018\nThe \u201creorientation of power and knowledge\u201d has two key as-\npects that are part of the concept of recursive publics: availability\nand modifiability (or adaptability). Availability is a broad, dif-\nfuse, and familiar issue. It includes things like transparency, open\ngovernance or transparent organization, secrecy and freedom of\ninformation, and open access in science. Availability includes the\nbusiness-school theories of \u201cdisintermediation\u201d and \u201ctransparency\nand accountability\u201d and the spread of \u201caudit culture\u201d and so-called\nneoliberal regimes of governance; it is just as often the subject of\nsuspicion as it is a kind of moral mandate, as in the case of open\n10 introduction access to scientific results and publications.8 All of these issues are\ncertainly touched on in detailed and practical ways in the creation\nof Free Software. Debates about the mode of availability of infor-\nmation made possible in the era of the Internet range from digital-\nrights management and copy protection, to national security and\ncorporate espionage, to scientific progress and open societies.\nHowever, it is modifiability that is the most fascinating, and un-\nnerving, aspect of the reorientation of power and knowledge. Modi-\nfiability includes the ability not only to access\u2014that is, to reuse in\nthe trivial sense of using something without restrictions\u2014but to\ntransform it for use in new contexts, to different ends, or in order\nto participate directly in its improvement and to redistribute or re-\ncirculate those improvements within the same infrastructures while\nsecuring the same rights for everyone else. In fact, the core practice\nof Free Software is the practice of reuse and modification of soft-\nware source code. Reuse and modification are also the key ideas\nthat projects modeled on Free Software (such as Connexions and\nCreative Commons) see as their goal. Creative Commons has as its\nmotto \u201cCulture always builds on the past,\u201d and they intend that to\nmean \u201cthrough legal appropriation and modification.\u201d Connexions,\nwhich allows authors to create online bits and pieces of textbooks\nexplicitly encourages authors to reuse work by other people, to\nmodify it, and to make it their own. Modifiability therefore raises a\nvery specific and important question about finality. When is some-\nthing (software, a film, music, culture) finished? How long does it\nremain finished? Who decides? Or more generally, what does its\ntemporality look like, and how does that temporality restructure\npolitical relationships? Such issues are generally familiar only to\nhistorians and literary scholars who understand the transforma-\ntion of canons, the interplay of imitation and originality, and the\ntheoretical questions raised, for instance, in textual scholarship.\nBut the contemporary meaning of modification includes both a vast\nincrease in the speed and scope of modifiability and a certain au-\ntomation of the practice that was unfamiliar before the advent of\nsophisticated, distributed forms of software.\nModifiability is an oft-claimed advantage of Free Software. It\ncan be updated, modified, extended, or changed to deal with other\nchanging environments: new hardware, new operating systems,\nunforeseen technologies, or new laws and practices. At an infra-\nstructural level, such modifiability makes sense: it is a response to\nintroduction 11 and an alternative to technocratic forms of planning. It is a way of\nplanning in the ability to plan out; an effort to continuously secure\nthe ability to deal with surprise and unexpected outcomes; a way of\nmaking flexible, modifiable infrastructures like the Internet as safe\nas permanent, inflexible ones like roads and bridges.\nBut what is the cultural significance of modifiability? What does\nit mean to plan in modifiability to culture, to music, to education\nand science? At a clerical level, such a question is obvious when-\never a scholar cannot recover a document written in WordPerfect\n\u0018.0 or on a disk for which there are no longer disk drives, or when\na library archive considers saving both the media and the machines\nthat read that media. Modifiability is an imperative for building\ninfrastructures that can last longer. However, it is not only a so-\nlution to a clerical problem: it creates new possibilities and new\nproblems for long-settled practices like publication, or the goals\nand structure of intellectual-property systems, or the definition of\nthe finality, lifetime, monumentality, and especially, the identity\nof a work. Long-settled, seemingly unassailable practices\u2014like the\nauthority of published books or the power of governments to con-\ntrol information\u2014are suddenly confounded and denaturalized by\nthe techniques of modifiability.\nOver the last ten to fifteen years, as the Internet has spread expo-\nnentially and insinuated itself into the most intimate practices of all\nkinds of people, the issues of availability and modifiability and the\nreorientation of knowledge and power they signify have become\ncommonplace. As this has happened, the significance and practices\nassociated with Free Software have also spread\u2014and been modu-\nlated in the process. These practices provide a material and mean-\ningful starting point for an array of recursive publics who play with,\nmodulate, and transform them as they debate and build new ways\nto share, create, license, and control their respective productions.\nThey do not all share the same goals, immediate or long-term, but\nby engaging in the technical, legal, and social practices pioneered\nin Free Software, they do in fact share a \u201csocial imaginary\u201d that\ndefines a particular relationship between technology, organs of\ngovernance (whether state, corporate, or nongovernmental), and\nthe Internet. Scientists in a lab or musicians in a band; scholars\ncreating a textbook or social movements contemplating modes of\norganization and protest; government bureaucrats issuing data\nor journalists investigating corruption; corporations that manage\n1\u0018 introduction personal data or co-ops that monitor community development\u2014\nall these groups and others may find themselves adopting, modu-\nlating, rejecting, or refining the practices that have made up Free\nSoftware in the recent past and will do so in the near future.\nExperiment and Modulation\nWhat exactly is Free Software? This question is, perhaps surpris-\ningly, an incredibly common one in geek life. Debates about def-\ninition and discussions and denunciations are ubiquitous. As an\nanthropologist, I have routinely participated in such discussions\nand debates, and it is through my immediate participation that\nTwo Bits opens. In part I I tell stories about geeks, stories that are\nmeant to give the reader that classic anthropological sense of be-\ning thrown into another world. The stories reveal several general\naspects of what geeks talk about and how they do so, without get-\nting into what Free Software is in detail. I start in this way because\nmy project started this way. I did not initially intend to study Free\nSoftware, but it was impossible to ignore its emergence and mani-\nfest centrality to geeks. The debates about the definition of Free\nSoftware that I participated in online and in the field eventually\nled me away from studying geeks per se and turned me toward the\ncentral research concern of this book: what is the cultural signifi-\ncance of Free Software?\nIn part II what I offer is not a definition of Free Software, but\na history of how it came to be. The story begins in 1998, with an\nimportant announcement by Netscape that it would give away the\nsource code to its main product, Netscape Navigator, and works\nbackward from this announcement into the stories of the UNIX\noperating system, \u201copen systems,\u201d copyright law, the Internet, and\ntools for coordinating people and code. Together, these five stories\nconstitute a description of how Free Software works as a practice.\nAs a cultural analysis, these stories highlight just how experimental\nthe practices are, and how individuals keep track of and modulate\nthe practices along the way.\nNetscape\u2019s decision came at an important point in the life of Free\nSoftware. It was at just this moment that Free Software was be-\ncoming aware of itself as a coherent movement and not just a di-\nverse amalgamation of projects, tools, or practices. Ironically, this\nintroduction 1\u0018 recognition also betokened a split: certain parties started to insist\nthat the movement be called \u201cOpen Source\u201d software instead, to\nhighlight the practical over the ideological commitments of the\nmovement. The proposal itself unleashed an enormous public dis-\ncussion about what defined Free Software (or Open Source). This\nenigmatic event, in which a movement became aware of itself at\nthe same time that it began to question its mission, is the subject\nof chapter \u0018. I use the term movement to designate one of the five\ncore components of Free Software: the practices of argument and\ndisagreement about the meaning of Free Software. Through these\npractices of discussion and critique, the other four practices start to\ncome into relief, and participants in both Free Software and Open\nSource come to realize something surprising: for all the ideologi-\ncal distinctions at the level of discourse, they are doing exactly the\nsame thing at the level of practice. The affect-laden histrionics with\nwhich geeks argue about the definition of what makes Free Soft-\nware free or Open Source open can be matched only by the sober\nspecificity of the detailed practices they share.\nThe second component of Free Software is just such a mundane\nactivity: sharing source code (chapter \u0018). It is an essential and fun-\ndamentally routine practice, but one with a history that reveals\nthe goals of software portability, the interactions of commercial\nand academic software development, and the centrality of source\ncode (and not only of abstract concepts) in pedagogical settings.\nThe details of \u201csharing\u201d source code also form the story of the rise\nand proliferation of the UNIX operating system and its myriad de-\nrivatives.\nThe third component, conceptualizing openness (chapter \u0018), is\nabout the specific technical and \u201cmoral\u201d meanings of openness,\nespecially as it emerged in the 1980s in the computer industry\u2019s\ndebates over \u201copen systems.\u201d These debates concerned the creation\nof a particular infrastructure, including both technical standards\nand protocols (a standard UNIX and protocols for networks), and\nan ideal market infrastructure that would allow open systems to\nflourish. Chapter 5 is the story of the failure to achieve a market\ninfrastructure for open systems, in part due to a significant blind\nspot: the role of intellectual property.\nThe fourth component, applying copyright (and copyleft) licenses\n(chapter \u0018), involves the problem of intellectual property as it faced\nprogrammers and geeks in the late 19\u00180s and early 1980s. In this\n1\u0018 introduction chapter I detail the story of the first Free Software license\u2014the\nGNU General Public License (GPL)\u2014which emerged out of a con-\ntroversy around a very famous piece of software called EMACS.\nThe controversy is coincident with changing laws (in 19\u0018\u0018 and\n1980) and changing practices in the software industry\u2014a general\ndrift from trade secret to copyright protection\u2014and it is also a story\nabout the vaunted \u201chacker ethic\u201d that reveals it in its native practi-\ncal setting, rather than as a rarefied list of rules.\nThe fifth component, the practice of coordination and collabora-\ntion (chapter \u0018), is the most talked about: the idea of tens or hun-\ndreds of thousands of people volunteering their time to contribute\nto the creation of complex software. In this chapter I show how\nnovel forms of coordination developed in the 1990s and how they\nworked in the canonical cases of Apache and Linux; I also highlight\nhow coordination facilitates the commitment to adaptability (or\nmodifiability) over against planning and hierarchy, and how this\ncommitment resolves the tension between individual virtuosity and\nthe need for collective control.\nTaken together, these five components make up Free Software\u2014\nbut they are not a definition. Within each of these five practices,\nmany similar and dissimilar activities might reasonably be in-\ncluded. The point of such a redescription of the practices of Free\nSoftware is to conceptualize them as a kind of collective technical\nexperimental system. Within each component are a range of differ-\nences in practice, from conventional to experimental. At the cen-\nter, so to speak, are the most common and accepted versions of a\npractice; at the edges are more unusual or controversial versions.\nTogether, the components make up an experimental system whose\ninfrastructure is the Internet and whose \u201chypotheses\u201d concern the\nreorientation of knowledge and power.\nFor example, one can hardly have Free Software without source\ncode, but it need not be written in C (though the vast majority\nof it is); it can be written in Java or perl or TeX. However, if one\nstretches the meaning of source code to include music (sheet music\nas source and performance as binary), what happens? Is this still\nFree Software? What happens when both the sheet and the per-\nformance are \u201cborn digital\u201d? Or, to take a different example, Free\nSoftware requires Free Software licenses, but the terms of these\nlicenses are often changed and often heatedly discussed and vigi-\nlantly policed by geeks. What degree of change removes a license\nintroduction 1\u0018 from the realm of Free Software and why? How much flexibility is\nallowed?\nConceived this way, Free Software is a system of thresholds,\nnot of classification; the excitement that participants and observ-\ners sense comes from the modulation (experimentation) of each of\nthese practices and the subsequent discovery of where the thresh-\nolds are. Many, many people have written their own \u201cFree Soft-\nware\u201d copyright licenses, but only some of them remain within the\nthreshold of the practice as defined by the system. Modulations\nhappen whenever someone learns how some component of Free\nSoftware works and asks, \u201cCan I try these practices out in some\nother domain?\u201d\nThe reality of constant modulation means that these five practices\ndo not define Free Software once and for all; they define it with re-\nspect to its constitution in the contemporary. It is a set of practices\ndefined \u201caround the point\u201d 1998\u201399, an intensive coordinate space\nthat allows one to explore Free Software\u2019s components prospec-\ntively and retrospectively: into the near future and the recent past.\nFree Software is a machine for charting the (re)emergence of a\nproblematic of power and knowledge as it is filtered through the\ntechnical realities of the Internet and the political and economic\nconfiguration of the contemporary. Each of these practices has its\nown temporality of development and emergence, but they have re-\ncently come together into this full house called either Free Software\nor Open Source.9\nViewing Free Software as an experimental system has a strategic\npurpose in Two Bits. It sets the stage for part III, wherein I ask what\nkinds of modulations might no longer qualify as Free Software per\nse, but still qualify as recursive publics. It was around \u0018000 that\ntalk of \u201ccommons\u201d began to percolate out of discussions about Free\nSoftware: commons in educational materials, commons in biodi-\nversity materials, commons in music, text, and video, commons in\nmedical data, commons in scientific results and data.10 On the one\nhand, it was continuous with interest in creating \u201cdigital archives\u201d\nor \u201conline collections\u201d or \u201cdigital libraries\u201d; on the other hand, it\nwas a conjugation of the digital collection with the problems and\npractices of intellectual property. The very term commons\u2014at once\na new name and a theoretical object of investigation\u2014was meant\nto suggest something more than simply a collection, whether of\n1\u0018 introduction digital objects or anything else; it was meant to signal the public in-\nterest, collective management, and legal status of the collection.11\nIn part III, I look in detail at two \u201ccommons\u201d understood as mod-\nulations of the component practices of Free Software. Rather than\ntreating commons projects simply as metaphorical or inspirational\nuses of Free Software, I treat them as modulations, which allows me\nto remain directly connected to the changing practices involved.\nThe goal of part III is to understand how commons projects like\nConnexions and Creative Commons breach the thresholds of these\npractices and yet maintain something of the same orientation.\nWhat changes, for instance, have made it possible to imagine new\nforms of free content, free culture, open source music, or a science\ncommons? What happens as new communities of people adopt and\nmodulate the five component practices? Do they also become re-\ncursive publics, concerned with the maintenance and expansion of\nthe infrastructures that allow them to come into being in the first\nplace? Are they concerned with the implications of availability and\nmodifiability that continue to unfold, continue to be figured out, in\nthe realms of education, music, film, science, and writing?\nThe answers in part III make clear that, so far, these concerns are\nalive and well in the modulations of Free Software: Creative Com-\nmons and Connexions each struggle to come to terms with new ways\nof creating, sharing, and reusing content in the contemporary legal\nenvironment, with the Internet as infrastructure. Chapters 8 and\n9 provide a detailed analysis of a technical and legal experiment:\na modulation that begins with source code, but quickly requires\nmodulations in licensing arrangements and forms of coordination.\nIt is here that Two Bits provides the most detailed story of figuring\nout set against the background of the reorientation of knowledge\nand power. This story is, in particular, one of reuse, of modifiability\nand the problems that emerge in the attempt to build it into the\neveryday practices of pedagogical writing and cultural production\nof myriad forms. Doing so leads the actors involved directly to the\nquestion of the existence and ontology of norms: norms of scholarly\nproduction, borrowing, reuse, citation, reputation, and ownership.\nThese last chapters open up questions about the stability of modern\nknowledge, not as an archival or a legal problem, but as a social\nand normative one; they raise questions about the invention and\ncontrol of norms, and the forms of life that may emerge from these\nintroduction 1\u0018 practices. Recursive publics come to exist where it is clear that such\ninvention and control need to be widely shared, openly examined,\nand carefully monitored.\nThree Ways of Looking at Two Bits\nTwo Bits makes three kinds of scholarly contributions: empiri-\ncal, methodological, and theoretical. Because it is based largely\non fieldwork (which includes historical and archival work), these\nthree contributions are often mixed up with each other. Fieldwork,\nespecially in cultural and social anthropology in the last thirty\nyears, has come to be understood less and less as one particular\ntool in a methodological toolbox, and more and more as distinctive\nmode of epistemological encounter.1\u0018 The questions I began with\nemerged out of science and technology studies, but they might end\nup making sense to a variety of fields, ranging from legal studies\nto computer science.\nEmpirically speaking, the actors in my stories are figuring some-\nthing out, something unfamiliar, troubling, imprecise, and oc-\ncasionally shocking to everyone involved at different times and\nto differing extents.1\u0018 There are two kinds of figuring-out stories:\nthe contemporary ones in which I have been an active participant\n(those of Connexions and Creative Commons), and the historical\nones conducted through \u201carchival\u201d research and rereading of cer-\ntain kinds of texts, discussions, and analyses-at-the-time (those of\nUNIX, EMACS, Linux, Apache, and Open Systems). Some are stories\nof technical figuring out, but most are stories of figuring out a prob-\nlem that appears to have emerged. Some of these stories involve\ncallow and earnest actors, some involve scheming and strategy,\nbut in all of them the figuring out is presented \u201cin the making\u201d\nand not as something that can be conveniently narrated as obvi-\nous and uncontested with the benefit of hindsight. Throughout this\nbook, I tell stories that illustrate what geeks are like in some re-\nspects, but, more important, that show them in the midst of figuring\nthings out\u2014a practice that can happen both in discussion and in\nthe course of designing, planning, executing, writing, debugging,\nhacking, and fixing.\nThere are also myriad ways in which geeks narrate their own\nactions to themselves and others, as they figure things out. Indeed,\n18 introduction there is no crisis of representing the other here: geeks are vocal,\nloud, persistent, and loquacious. The superalterns can speak for\nthemselves. However, such representations should not necessar-\nily be taken as evidence that geeks provide adequate analytic or\ncritical explanations of their own actions. Some of the available\nwriting provides excellent description, but distracting analysis. Eric\nRaymond\u2019s work is an example of such a combination.1\u0018 Over the\ncourse of my fieldwork, Raymond\u2019s work has always been present\nas an excellent guide to the practices and questions that plague\ngeeks\u2014much like a classic \u201cprincipal informant\u201d in anthropology.\nAnd yet his analyses, which many geeks subscribe to, are distract-\ning. They are fanciful, occasionally enjoyable and enlightening\u2014\nbut they are not about the cultural significance of Free Software. As\nsuch I am less interested in treating geeks as natives to be explained\nand more interested in arguing with them: the people in Two Bits\nare a sine qua non of the ethnography, but they are not the objects\nof its analysis.1\u0018\nBecause the stories I tell here are in fact recent by the standards\nof historical scholarship, there is not much by way of comparison\nin terms of the empirical material. I rely on a number of books\nand articles on the history of the early Internet, especially Janet\nAbbate\u2019s scholarship and the single historical work on UNIX, Peter\nSalus\u2019s A Quarter Century of Unix.1\u0018 There are also a couple of ex-\ncellent journalistic works, such as Glyn Moody\u2019s Rebel Code: Inside\nLinux and the Open Source Revolution (which, like Two Bits, relies\nheavily on the novel accessibility of detailed discussions carried out\non public mailing lists). Similarly, the scholarship on Free Software\nand its history is just starting to establish itself around a coherent\nset of questions.1\u0018\nMethodologically, Two Bits provides an example of how to study\ndistributed phenomena ethnographically. Free Software and the In-\nternet are objects that do not have a single geographic site at which\nthey can be studied. Hence, this work is multisited in the simple\nsense of having multiple sites at which these objects were investi-\ngated: Boston, Bangalore, Berlin, Houston. It was conducted among\nparticular people, projects, and companies and at conferences and\nonline gatherings too numerous to list, but it has not been a study\nof a single Free Software project distributed around the globe. In all\nof these places and projects the geeks I worked with were randomly\nand loosely affiliated people with diverse lives and histories. Some\nintroduction 19 identified as Free Software hackers, but most did not. Some had never\nmet each other in real life, and some had. They represented mul-\ntiple corporations and institutions, and came from diverse nations,\nbut they nonetheless shared a certain set of ideas and idioms that\nmade it possible for me to travel from Boston to Berlin to Bangalore\nand pick up an ongoing conversation with different people, in very\ndifferent places, without missing a beat.\nThe study of distributed phenomena does not necessarily im-\nply the detailed, local study of each instance of a phenomenon,\nnor does it necessitate visiting every relevant geographical site\u2014\nindeed, such a project is not only extremely difficult, but confuses\nmap and territory. As Max Weber put it, \u201cIt is not the \u2018actual\u2019 inter-\nconnection of \u2018things\u2019 but the conceptual inter-connection of prob-\nlems that define the scope of the various sciences.\u201d18 The decisions\nabout where to go, whom to study, and how to think about Free\nSoftware are arbitrary in the precise sense that because the phe-\nnomena are so widely distributed, it is possible to make any given\nnode into a source of rich and detailed knowledge about the dis-\ntributed phenomena itself, not only about the local site. Thus, for\ninstance, the Connexions project would probably have remained\nlargely unknown to me had I not taken a job in Houston, but it\nnevertheless possesses precise, identifiable connections to the other\nsites and sets of people that I have studied, and is therefore rec-\nognizable as part of this distributed phenomena, rather than some\nother. I was actively looking for something like Connexions in order\nto ask questions about what was becoming of Free Software and\nhow it was transforming. Had there been no Connexions in my back\nyard, another similar field site would have served instead.\nIt is in this sense that the ethnographic object of this study is not\ngeeks and not any particular project or place or set of people, but\nFree Software and the Internet. Even more precisely, the ethno-\ngraphic object of this study is \u201crecursive publics\u201d\u2014except that this\nconcept is also the work of the ethnography, not its preliminary\nobject. I could not have identified \u201crecursive publics\u201d as the object\nof the ethnography at the outset, and this is nice proof that ethno-\ngraphic work is a particular kind of epistemological encounter, an\nencounter that requires considerable conceptual work during and\nafter the material labor of fieldwork, and throughout the mate-\nrial labor of writing and rewriting, in order to make sense of and\nreorient it into a question that will have looked deliberate and\n\u00180 introduction answerable in hindsight. Ethnography of this sort requires a long-\nterm commitment and an ability to see past the obvious surface\nof rapid transformation to a more obscure and slower temporality\nof cultural significance, yet still pose questions and refine debates\nabout the near future.19 Historically speaking, the chapters of part\nII can be understood as a contribution to a history of scientific\ninfrastructure\u2014or perhaps to an understanding of large-scale, col-\nlective experimentation.\u00180 The Internet and Free Software are each\nan important practical transformation that will have effects on the\npractice of science and a kind of complex technical practice for\nwhich there are few existing models of study.\nA methodological note about the peculiarity of my subject is also\nin order. The Attentive Reader will note that there are very few\nfragments of conventional ethnographic material (i.e., interviews\nor notes) transcribed herein. Where they do appear, they tend to be\n\u201cpublicly available\u201d\u2014which is to say, accessible via the Internet\u2014\nand are cited as such, with as much detail as necessary to allow the\nreader to recover them. Conventional wisdom in both anthropology\nand history has it that what makes a study interesting, in part, is\nthe work a researcher has put into gathering that which is not al-\nready available, that is, primary sources as opposed to secondary\nsources. In some cases I provide that primary access (specifically in\nchapters \u0018, 8, and 9), but in many others it is now literally impossi-\nble: nearly everything is archived. Discussions, fights, collaborations,\ntalks, papers, software, articles, news stories, history, old software,\nold software manuals, reminiscences, notes, and drawings\u2014it is all\nsaved by someone, somewhere, and, more important, often made\ninstantly available by those who collect it. The range of conversa-\ntions and interactions that count as private (either in the sense of\ndisappearing from written memory or of being accessible only to\nthe parties involved) has shrunk demonstrably since about 1981.\nSuch obsessive archiving means that ethnographic research is\nstratified in time. Questions that would otherwise have required\n\u201cbeing there\u201d are much easier to research after the fact, and this\nis most evident in my reconstruction from sources on USENET and\nmailing lists in chapters 1, \u0018, and \u0018. The overwhelming availability\nof quasi-archival materials is something I refer to, in a play on the\nEMACS text editor, as \u201cself-documenting history.\u201d That is to say,\none of the activities that geeks love to participate in, and encour-\nage, is the creation, analysis, and archiving of their own roles in the\nintroduction \u00181 development of the Internet. No matter how obscure or arcane, it\nseems most geeks have a well-developed sense of possibility\u2014their\ncontribution could turn out to have been transformative, important,\noriginary. What geeks may lack in social adroitness, they make up\nfor in archival hubris.\nFinally, the theoretical contribution of Two Bits consists of a re-\nfinement of debates about publics, public spheres, and social imagi-\nnaries that appear troubled in the context of the Internet and Free\nSoftware. Terminology such as virtual community, online community,\ncyberspace, network society, or information society are generally not\ntheoretical constructs, but ways of designating a subgenre of disci-\nplinary research having to do with electronic networks. The need\nfor a more precise analysis of the kinds of association that take\nplace on and through information technology is clear; the first step\nis to make precise which information technologies and which spe-\ncific practices make a difference.\nThere is a relatively large and growing literature on the Internet\nas a public sphere, but such literature is generally less concerned\nwith refining the concept through research and more concerned\nwith pronouncing whether or not the Internet fits Habermas\u2019s defi-\nnition of the bourgeois public sphere, a definition primarily con-\nceived to account for the eighteenth century in Britain, not the\ntwenty-first-century Internet.\u00181 The facts of technical and human\nlife, as they unfold through the Internet and around the practices\nof Free Software, are not easy to cram into Habermas\u2019s definition.\nThe goal of Two Bits is not to do so, but to offer conceptual clarity\nbased in ethnographic fieldwork.\nThe key texts for understanding the concept of recursive publics\nare the works of Habermas, Charles Taylor\u2019s Modern Social Imagi-\nnaries, and Michael Warner\u2019s The Letters of the Republic and Publics\nand Counterpublics. Secondary texts that refine these notions are\nJohn Dewey\u2019s The Public and Its Problems and Hannah Arendt\u2019s The\nHuman Condition. Here it is not the public sphere per se that is the\ncenter of analysis, but the \u201cideas of modern moral and social order\u201d\nand the terminology of \u201cmodern social imaginaries.\u201d\u0018\u0018 I find these\nconcepts to be useful as starting points for a very specific reason: to\ndistinguish the meaning of moral order from the meaning of moral\nand technical order that I explore with respect to geeks. I do not seek\nto test the concept of social imaginary here, but to build something\non top of it.\n\u0018\u0018 introduction If recursive public is a useful concept, it is because it helps elabo-\nrate the general question of the \u201creorientation of knowledge and\npower.\u201d In particular it is meant to bring into relief the ways in\nwhich the Internet and Free Software are related to the political\neconomy of modern society through the creation not only of new\nknowledge, but of new infrastructures for circulating, maintaining,\nand modifying it. Just as Warner\u2019s book The Letters of the Republic\nwas concerned with the emergence of the discourse of republican-\nism and the simultaneous development of an American republic of\nletters, or as Habermas\u2019s analysis was concerned with the relation-\nship of the bourgeois public sphere to the democratic revolutions\nof the eighteenth century, this book asks a similar series of ques-\ntions: how are the emergent practices of recursive publics related\nto emerging relations of political and technical life in a world that\nsubmits to the Internet and its forms of circulation? Is there still\na role for a republic of letters, much less a species of public that\ncan seriously claim independence and autonomy from other consti-\ntuted forms of power? Are Habermas\u2019s pessimistic critiques of the\nbankruptcy of the public sphere in the twentieth century equally ap-\nplicable to the structures of the twenty-first century? Or is it possible\nthat recursive publics represent a reemergence of strong, authentic\npublics in a world shot through with cynicism and suspicion about\nmass media, verifiable knowledge, and enlightenment rationality?\nintroduction \u0018\u0018  Part I the internet\nThe concept of the state, like most concepts which are introduced\nby \u201cThe,\u201d is both too rigid and too tied up with controversies to be\nof ready use. It is a concept which can be approached by a flank\nmovement more easily than by a frontal attack. The moment we\nutter the words \u201cThe State\u201d a score of intellectual ghosts rise to\nobscure our vision. Without our intention and without our notice,\nthe notion of \u201cThe State\u201d draws us imperceptibly into a consider-\nation of the logical relationship of various ideas to one another, and\naway from the facts of human activity. It is better, if possible, to\nstart from the latter and see if we are not led thereby into an idea\nof something which will turn out to implicate the marks and signs\nwhich characterize political behavior.\n\u2014john dewey, The Public and Its Problems  1.\nGeeks and Recursive Publics\nSince about 1997, I have been living with geeks online and off. I\nhave been drawn from Boston to Bangalore to Berlin to Houston to\nPalo Alto, from conferences and workshops to launch parties, pubs,\nand Internet Relay Chats (IRCs). All along the way in my research\nquestions of commitment and practice, of ideology and imagina-\ntion have arisen, even as the exact nature of the connections be-\ntween these people and ideas remained obscure to me: what binds\ngeeks together? As my fieldwork pulled me from a Boston start-up\ncompany that worked with radiological images to media labs in\nBerlin to young entrepreneurial elites in Bangalore, my logistical\nquestion eventually developed into an analytical concept: geeks are\nbound together as a recursive public.\nHow did I come to understand geeks as a public constituted\naround the technical and moral ideas of order that allow them to\nassociate with one another? Through this question, one can start to\nunderstand the larger narrative of Two Bits: that of Free Software as an exemplary instance of a recursive public and as a set of prac-\ntices that allow such publics to expand and spread. In this chapter I\ndescribe, ethnographically, the diverse, dispersed, and novel forms\nof entanglements that bind geeks together, and I construct the con-\ncept of a recursive public in order to explain these entanglements.\nA recursive public is a public that is constituted by a shared con-\ncern for maintaining the means of association through which they\ncome together as a public. Geeks find affinity with one another\nbecause they share an abiding moral imagination of the technical\ninfrastructure, the Internet, that has allowed them to develop and\nmaintain this affinity in the first place. I elaborate the concept of\nrecursive public (which is not a term used by geeks) in relation to\ntheories of ideology, publics, and public spheres and social imagi-\nnaries. I illustrate the concept through ethnographic stories and\nexamples that highlight geeks\u2019 imaginations of the technical and\nmoral order of the Internet. These stories include those of the fate\nof Amicas, a Boston-based healthcare start-up, between 1997 and\n2003, of my participation with new media academics and activists\nin Berlin in 1999\u20132001, and of the activities of a group of largely\nBangalore-based information technology (IT) professionals on and\noffline, especially concerning the events surrounding the peer-to-\npeer file sharing application Napster in 2000\u20132001.\nThe phrase \u201cmoral and technical order\u201d signals both technology\u2014\nprincipally software, hardware, networks, and protocols\u2014and an\nimagination of the proper order of collective political and com-\nmercial action, that is, how economy and society should be or-\ndered collectively. Recursive publics are just as concerned with the\nmoral order of markets as they are with that of commons; they are\nnot anticommercial or antigovernment. They exist independent\nof, and as a check on, constituted forms of power, which include\nmarkets and corporations. Unlike other concepts of a public or of\na public sphere, \u201crecursive public\u201d captures the fact that geeks\u2019\nprincipal mode of associating and acting is through the medium of\nthe Internet, and it is through this medium that a recursive public\ncan come into being in the first place. The Internet is not itself a\npublic sphere, a public, or a recursive public, but a complex, het-\nerogeneous infrastructure that constitutes and constrains geeks\u2019\neveryday practical commitments, their ability to \u201cbecome public\u201d\nor to compose a common world. As such, their participation qua\nrecursive publics structures their identity as creative and autono-\n28 geeks and recursive publics mous individuals. The fact that the geeks described here have been\nbrought together by mailing lists and e-mail, bulletin-board ser-\nvices and Web sites, books and modems, air travel and academia,\nand cross-talking and cross-posting in ways that were not possible\nbefore the Internet is at the core of their own reasoning about why\nthey associate with each other. They are the builders and imagin-\ners of this space, and the space is what allows them to build and\nimagine it.\nWhy recursive? I call such publics recursive for two reasons: first,\nin order to signal that this kind of public includes the activities of\nmaking, maintaining, and modifying software and networks, as\nwell as the more conventional discourse that is thereby enabled;\nand second, in order to suggest the recursive \u201cdepth\u201d of the pub-\nlic, the series of technical and legal layers\u2014from applications to\nprotocols to the physical infrastructures of waves and wires\u2014that\nare the subject of this making, maintaining, and modifying. The\nfirst of these characteristics is evident in the fact that geeks use\ntechnology as a kind of argument, for a specific kind of order: they\nargue about technology, but they also argue through it. They express\nideas, but they also express infrastructures through which ideas can\nbe expressed (and circulated) in new ways. The second of these\ncharacteristics\u2014regarding layers\u2014is reflected in the ability of\ngeeks to immediately see connections between, for example, Nap-\nster (a user application) and TCP\/IP (a network protocol) and to\ndraw out implications for both of them. By connecting these layers,\nNapster comes to represent the Internet in miniature. The question\nof where these layers stop (hardware? laws and regulations? physi-\ncal constants? etc.) circumscribes the limits of the imagination of\ntechnical and moral order shared by geeks.\nAbove all, \u201crecursive public\u201d is a concept\u2014not a thing. It is in-\ntended to make distinctions, allow comparison, highlight salient\nfeatures, and relate two diverse kinds of things (the Internet and\nFree Software) in a particular historical context of changing rela-\ntions of power and knowledge. The stories in this chapter (and\nthroughout the book) give some sense of how geeks interact and\nwhat they do technically and legally, but the concept of a recursive\npublic provides a way of explaining why geeks (or people involved\nin Free Software or its derivatives) associate with one another, as\nwell as a way of testing whether other similar cases of contempo-\nrary, technologically mediated affinity are similarly structured.\ngeeks and recursive publics 29 Recursion\nRecursion (or \u201crecursive\u201d) is a mathematical concept, one which is a standard\nfeature of any education in computer programming. The definition from the\nOxford English Dictionary reads: \u201c2. a. Involving or being a repeated procedure\nsuch that the required result at each step except the last is given in terms of\nthe result(s) of the next step, until after a finite number of steps a terminus is\nreached with an outright evaluation of the result.\u201d It should be distinguished\nfrom simple iteration or repetition. Recursion is always subject to a limit and\nis more like a process of repeated deferral, until the last step in the process, at\nwhich point all the deferred steps are calculated and the result given.\nRecursion is powerful in programming because it allows for the definition\nof procedures in terms of themselves\u2014something that seems at first counter-\nintuitive. So, for example,\n(defun (factorial n) ; This is the name of the function and its input n.\n(if (=n 1) ; This is the final limit, or recursive depth\n1 ; if n=1, then return 1\n(* n (factorial (- n 1))))) ; otherwise return n times factorial of n-1;\n; call the procedure from within itself, and\n; calculate the next step of the result before\n; giving an answer.1\nIn Two Bits a recursive public is one whose existence (which consists solely in\naddress through discourse) is only possible through discursive and technical\nreference to the means of creating this public. Recursiveness is always contin-\ngent on a limit which determines the depth of a recursive procedure. So, for\ninstance, a Free Software project may depend on some other kind of software\nor operating system, which may in turn depend on particular open protocols\nor a particular process, which in turn depend on certain kinds of hardware\nthat implement them. The \u201cdepth\u201d of recursion is determined by the openness\nnecessary for the project itself.\nJames Boyle has also noted the recursive nature, in particular, of Free Soft-\nware: \u201cWhat\u2019s more, and this is a truly fascinating twist, when the produc-\ntion process does need more centralized coordination, some governance that\nguides how the sticky modular bits are put together, it is at least theoretically\npossible that we can come up with the control system in exactly the same way.\nIn this sense, distributed production is potentially recursive.\u201d2\n1. Abelson and Sussman, The Structure and Interpretation of Computer Programs, 30.\n2. Boyle, \u201cThe Second Enclosure Movement and the Construction of the Public Do-\nmain,\u201d 46.\n30 geeks and recursive publics From the Facts of Human Activity\nBoston, May 2003. Starbucks. Sean and Adrian are on their way\nto pick me up for dinner. I\u2019ve already had too much coffee, so I sit\nat the window reading the paper. Eventually Adrian calls to find\nout where I am, I tell him, and he promises to show up in fifteen\nminutes. I get bored and go outside to wait, watch the traffic go\nby. More or less right on time (only post-dotcom is Adrian ever on\ntime), Sean\u2019s new blue VW Beetle rolls into view. Adrian jumps\nout of the passenger seat and into the back, and I get in. Sean has\nbeen driving for a little over a year. He seems confident, cautious,\nbut meanders through the streets of Cambridge. We are destined\nfor Winchester, a township on the Charles River, in order to go to\nan Indian restaurant that one of Sean\u2019s friends has recommended.\nWhen I ask how they are doing, they say, \u201cGood, good.\u201d Adrian of-\nfers, \u201cWell, Sean\u2019s better than he has been in two years.\u201d \u201cReally?\u201d\nI say, impressed.\nSean says, \u201cWell, happier than at least the last year. I, well, let\nme put it this way: forgive me father for I have sinned, I still have\nunclean thoughts about some of the upper management in the com-\npany, I occasionally think they are not doing things in the best in-\nterest of the company, and I see them as self-serving and sometimes\nwish them ill.\u201d In this rolling blue confessional Sean describes some\nof the people who I am familiar with whom he now tries very hard\nnot to think about. I look at him and say, \u201cTen Hail Marys and\nten Our Fathers, and you will be absolved, my child.\u201d Turning to\nAdrian, I ask, \u201cAnd what about you?\u201d Adrian continues the joke:\n\u201cI, too, have sinned. I have reached the point where I can see abso-\nlutely nothing good coming of this company but that I can keep my\ninvestments in it long enough to pay for my children\u2019s college tu-\nition.\u201d I say, \u201cYou, my son, I cannot help.\u201d Sean says, \u201cWell, funny\nthing about tainted money . . . there just taint enough of it.\u201d\nI am awestruck. When I met Sean and Adrian, in 1997, their\nstart-up company, Amicas, was full of spit, with five employees\nworking out of Adrian\u2019s living room and big plans to revolution-\nize the medical-imaging world. They had connived to get Massa-\nchusetts General Hospital to install their rudimentary system and\nlet it compete with the big corporate sloths that normally stalked\nback offices: General Electric, Agfa, Siemens. It was these behe-\nmoths, according to Sean and Adrian, that were bilking hospitals\ngeeks and recursive publics 31 and healthcare providers with promises of cure-all technologies\nand horribly designed \u201csilos,\u201d \u201clegacy systems,\u201d and other closed-\nsystem monsters of corporate IT harkening back to the days of IBM\nmainframes. These beasts obviously did not belong to the gleaming\nfuture of Internet-enabled scalability. By June of 2000, Amicas had\nhired new \u201cprofessional\u201d management, moved to Watertown, and\ngrown to about a hundred employees. They had achieved their goal\nof creating an alternative Picture Archiving and Communication\nSystem (PACS) for use in hospital radiology departments and based\non Internet standards.\nAt that point, in the spring of 2000, Sean could still cheerfully in-\ntroduce me to his new boss\u2014the same man he would come to hate,\ninasmuch as Sean hates anyone. But by 2002 he was frustrated by\nthe extraordinary variety of corner-cutting and, more particularly,\nby the complacency with which management ignored his recom-\nmendations and released software that was almost certainly going\nto fail later, if not sooner. Sean, who is sort of permanently callow\nabout things corporate, could find no other explanation than that\nthe new management was evil.\nBut by 2003 the company had succeeded, having grown to more\nthan 200 employees and established steady revenue and a stable\npresence throughout the healthcare world. Both Sean and Adrian\nwere made rich\u2014not wildly rich, but rich enough\u2014by its success.\nIn the process, however, it also morphed into exactly what Sean\nand Adrian had created it in order to fight: a slothlike corporate\npurveyor of promises and broken software. Promises Adrian had\nmade and software Sean had built. The failure of Amicas to trans-\nform healthcare was a failure too complex and technical for most\nof America to understand, but it rested atop the success of Amicas\nin terms more readily comprehensible: a growing company mak-\ning profit. Adrian and Sean had started the company not to make\nmoney, but in order to fix a broken healthcare system; yet the sys-\ntem stayed broken while they made money.\nIn the rolling confessional, Sean and Adrian did in fact see me,\nhowever jokingly, as a kind of redeemer, a priest (albeit of an or-\nder with no flock) whose judgment of the affairs past was essential\nto their narration of their venture as a success, a failure, or as an\nunsatisfying and complicated mixture of both. I thought about this\nstrange moment of confession, of the combination of recognition\nand denial, of Adrian\u2019s new objectification of the company as an\n32 geeks and recursive publics investment opportunity, and of Sean\u2019s continuing struggle to make\nhis life and his work harmonize in order to produce good in the\nworld. Only the promise of the next project, the next mission (and\nthe ostensible reason for our dinner meeting) could possibly have\nmitigated the emotional disaster that their enterprise might other-\nwise be. Sean\u2019s and Adrian\u2019s endless, arcane fervor for the promise\nof new technologies did not cease, even given the quotidian calami-\nties these technologies leave in their wake. Their faith was strong,\nand continuously tested.\nAdrian\u2019s and Sean\u2019s passion was not for money\u2014though money\nwas a powerful drug\u2014it was for the Internet: for the ways in which\nthe Internet could replace the existing infrastructure of hospitals\nand healthcare providers, deliver on old promises of telemedicine\nand teleradiology, and, above all, level a playing field systemati-\ncally distorted and angled by corporate and government institu-\ntions that sought secrecy and private control, and stymied progress.\nIn healthcare, as Adrian repeatedly explained to me, this skewed\nplaying field was not only unfair but malicious and irresponsible.\nIt was costing lives. It slowed the creation and deployment of tech-\nnologies and solutions that could lower costs and thus provide more\nhealthcare for more people. The Internet was not part of the prob-\nlem; it was part of the solution to the problems that ailed 1990s\nhealthcare.\nAt the end of our car trip, at the Indian restaurant in Winchester,\nI learned about their next scheme, a project called MedCommons,\nwhich would build on the ideals of Free Software and give individu-\nals a way to securely control and manage their own healthcare\ndata. The rhetoric of commons and the promise of the Internet as\nan infrastructure dominated our conversation, but the realities of\nfunding and the question of whether MedCommons could be pur-\nsued without starting another company remained unsettled. I tried\nto imagine what form a future confession might take.\nGeeks and Their Internets\nSean and Adrian are geeks. They are entrepreneurs and idealists\nin different ways, a sometimes paradoxical combination. They are\ncertainly obsessed with technology, but especially with the Inter-\nnet, and they clearly distinguish themselves from others who are\ngeeks and recursive publics 33 obsessed with technology of just any sort. They aren\u2019t quite rep-\nresentative\u2014they do not stand in for all geeks\u2014but the way they\nthink about the Internet and its possibilities might be. Among the\nrich story of their successes and failures, one might glimpse the\noutlines of a question: where do their sympathies lie? Who are they\nwith? Who do they recognize as being like them? What might draw\nthem together with other geeks if not a corporation, a nation, a\nlanguage, or a cause? What binds these two geeks to any others?\nSean worked for the Federal Reserve in the 1980s, where he was\nintroduced to uNIx, C programming, EMACS, Usenet, Free Soft-\nware, and the Free Software Foundation. But he was not a Free\nSoftware hacker; indeed, he resisted my attempts to call him a\nhacker at all. Nevertheless, he started a series of projects and com-\npanies with Adrian that drew on the repertoire of practices and\nideas familiar from Free Software, including their MedCommons\nproject, which was based more or less explicitly in the ideals of Free\nSoftware. Adrian has a degree in medicine and in engineering, and\nis a serial entrepreneur, with Amicas being his biggest success\u2014\nand throughout the last ten years has attended all manner of con-\nferences and meetings devoted to Free Software, Open Source, open\nstandards, and so on, almost always as the lone representative\nfrom healthcare. Both graduated from the MIT (Sean in econom-\nics, Adrian in engineering), one of the more heated cauldrons of the\nInternet and the storied home of hackerdom, but neither were MIT\nhackers, nor even computer-science majors.\nTheir goals in creating a start-up rested on their understanding\nof the Internet as an infrastructure: as a standardized infrastructure\nwith certain extremely powerful properties, not the least of which\nwas its flexibility. Sean and Adrian talked endlessly about open\nsystems, open standards, and the need for the Internet to remain\nopen and standardized. Adrian spoke in general terms about how it\nwould revolutionize healthcare; Sean spoke in specific terms about\nhow it structured the way Amicas\u2019s software was being designed\nand written. Both participated in standards committees and in the\nonline and offline discussions that are tantamount to policymaking\nin the Internet world. The company they created was a \u201cvirtual\u201d\ncompany, that is, built on tools that depended on the Internet and\nallowed employees to manage and work from a variety of loca-\ntions, though not without frustration, of course: Sean waited years\nfor broadband access in his home, and the hospitals they served\n34 geeks and recursive publics hemmed themselves in with virtual private networks, intranets, and\nsecurity firewalls that betrayed the promises of openness that Sean\nand Adrian heralded.\nThe Internet was not the object of their work and lives, but it\ndid represent in detail a kind of moral or social order embodied\nin a technical system and available to everyone to use as a plat-\nform whereby they might compete to improve and innovate in any\nrealm. To be sure, although not all Internet entrepreneurs of the\n1990s saw the Internet in the same way, Sean and Adrian were\nhardly alone in their vision. Something about the particular way in\nwhich they understood the Internet as representing a moral order\u2014\nsimultaneously a network, a market, a public, and a technology\u2014\nwas shared by a large group of people, those who I now refer to\nsimply as geeks.\nThe term geek is meant to be inclusive and to index the problem-\natic of a recursive public. Other terms may be equally useful, but\nperhaps semantically overdetermined, most notably hacker, which\nregardless of its definitional range, tends to connote someone sub-\nversive and\/or criminal and to exclude geek-sympathetic entrepre-\nneurs and lawyers and activists.1 Geek is meant to signal, like the\npublic in \u201crecursive public,\u201d that geeks stand outside power, at least\nin some aspects, and that they are not capitalists or technocrats,\neven if they start businesses or work in government or industry.2\nGeek is meant to signal a mode of thinking and working, not an\nidentity; it is a mode or quality that allows people to find each\nother, for reasons other than the fact that they share an office, a\ndegree, a language, or a nation.\nUntil the mid-1990s, hacker, geek, and computer nerd designated\na very specific type: programmers and lurkers on relatively under-\nground networks, usually college students, computer scientists, and\n\u201camateurs\u201d or \u201chobbyists.\u201d A classic mock self-diagnostic called the\nGeek Code, by Robert Hayden, accurately and humorously detailed\nthe various ways in which one could be a geek in 1996\u2014uNIx\/\nLinux skills, love\/hate of Star Trek, particular eating and clothing\nhabits\u2014but as Hayden himself points out, the geeks of the early\n1990s exist no longer. The elite subcultural, relatively homogenous\ngroup it once was has been overrun: \u201cThe Internet of 1996 was still\na wild untamed virgin paradise of geeks and eggheads unpopulated\nby script kiddies, and the denizens of AOL. When things changed,\nI seriously lost my way. I mean, all the \u2018geek\u2019 that was the Internet\ngeeks and recursive publics 35 was gone and replaced by xfiles buzzwords and politicians passing\nlaws about a technology they refused to comprehend.\u201d3\nFor the purists like Hayden, geeks were there first, and they un-\nderstood something, lived in a way, that simply cannot be compre-\nhended by \u201cscript kiddies\u201d (i.e., teenagers who perform the hacking\nequivalent of spray painting or cow tipping), crackers, or AOL users,\nall of whom are despised by Hayden-style geeks as unskilled users\nwho parade around the Internet as if they own it. While certainly\nelitist, Hayden captures the distinction between those who are legiti-\nmately allowed to call themselves geeks (or hackers) and those who\naren\u2019t, a distinction that is often formulated recursively, of course:\n\u201cYou are a hacker when another hacker calls you a hacker.\u201d\nHowever, since the explosive growth of the Internet, geek has\nbecome more common a designation, and my use of the term thus\nsuggests a role that is larger than programmer\/hacker, but not as\nlarge as \u201call Internet users.\u201d Despite Hayden\u2019s frustration, geeks are\nstill bound together as an elite and can be easily distinguished from\n\u201cAOL users.\u201d Some of the people I discuss would not call themselves\ngeeks, and some would. Not all are engineers or programmers: I\nhave met businessmen, lawyers, activists, bloggers, gastroenter-\nologists, anthropologists, lesbians, schizophrenics, scientists, poets,\npeople suffering from malaria, sea captains, drug dealers, and peo-\nple who keep lemurs, many of whom refer to themselves as geeks,\nsome of the time.4 There are also lawyers, politicians, sociologists,\nand economists who may not refer to themselves as geeks, but who\ncare about the Internet just as other geeks do. By contrast \u201cusers\u201d\nof the Internet, even those who use it eighteen out of twenty-four\nhours in a day to ship goods and play games, are not necessarily\ngeeks by this characterization.\nOperating Systems and Social Systems\nBerlin, November 1999. I am in a very hip club in Mitte called\nWMF. It\u2019s about eight o\u2019clock\u2014five hours too early for me to be a\nhipster, but the context is extremely cool. WMF is in a hard-to-find,\nabandoned building in the former East; it is partially converted,\nfilled with a mixture of new and old furnishings, video projectors,\nspeakers, makeshift bars, and dance-floor lighting. A crowd of\naround fifty people lingers amid smoke and Beck\u2019s beer bottles,\n36 geeks and recursive publics sitting on stools and chairs and sofas and the floor. We are listen-\ning to an academic read a paper about Claude Shannon, the MIT\nengineer credited with the creation of information theory. The au-\nthor is smoking and reading in German while the audience politely\nlistens. He speaks for about seventy minutes. There are questions\nand some perfunctory discussion. As the crowd breaks up, I find\nmyself, in halting German that quickly converts to English, having\na series of animated conversations about the GNu General Public\nLicense, the Debian Linux Distribution, open standards in net radio,\nand a variety of things for which Claude Shannon is the perfect\nghostly technopaterfamilias, even if his seventy-minute invocation\nhas clashed heavily with the surroundings.\nDespite my lame German, I still manage to jump deeply into is-\nsues that seem extremely familiar: Internet standards and open sys-\ntems and licensing issues and namespaces and patent law and so\non. These are not businesspeople, this is not a start-up company. As\nI would eventually learn, there was even a certain disdain for die\nKrawattenfaktor, the suit-and-tie factor, at these occasional, hybrid\nevents hosted by Mikro e.V., a nonprofit collective of journalists,\nacademics, activists, artists, and others interested in new media, the\nInternet, and related issues. Mikro\u2019s constituency included people\nfrom Germany, Holland, Austria, and points eastward. They took\nsome pride in describing Berlin as \u201cthe farthest East the West gets\u201d\nand arranged for a group photo in which, facing West, they stood\nbehind the statue of Marx and Lenin, who face East and look eter-\nnally at the iconic East German radio tower (Funkturm) in Alexan-\nderplatz. Mikro\u2019s members are resolutely activist and see the issues\naround the Internet-as-infrastructure not in terms of its potential\nfor business opportunities, but in urgently political and unrepen-\ntantly aesthetic terms\u2014terms that are nonetheless similar to those\nof Sean and Adrian, from whom I learned the language that allows\nme to mingle with the Mikro crowd at WMF. I am now a geek.\nBefore long, I am talking with Volker Grassmuck, founding mem-\nber of Mikro and organizer of the successful \u201cWizards of OS\u201d confer-\nence, held earlier in the year, which had the very intriguing subtitle\n\u201cOperating Systems and Social Systems.\u201d Grassmuck is inviting me\nto participate in a planning session for the next WOS, held at the\nChaos Computer Congress, a hacker gathering that occurs each\nyear in December in Berlin. In the following months I will meet a\nhuge number of people who seem, uncharacteristically for artists\ngeeks and recursive publics 37 and activists, strangely obsessed with configuring their Linux dis-\ntributions or hacking the http protocol or attending German Parlia-\nment hearings on copyright reform. The political lives of these folks\nhave indeed mixed up operating systems and social systems in ways\nthat are more than metaphorical.\nThe Idea of Order at the Keyboard\nIf intuition can lead one from geek to geek, from start-up to night-\nclub, and across countries, languages, and professional orienta-\ntions, it can only be due to a shared set of ideas of how things fit\ntogether in the world. These ideas might be \u201ccultural\u201d in the tra-\nditional sense of finding expression among a community of people\nwho share backgrounds, homes, nations, languages, idioms, eth-\nnos, norms, or other designators of belonging and co-presence. But\nbecause the Internet\u2014like colonialism, satellite broadcasting, and\nair travel, among other things\u2014crosses all these lines with aban-\ndon that the shared idea of order is better understood as part of a\npublic, or public sphere, a vast republic of letters and media and\nideas circulating in and through our thoughts and papers and let-\nters and conversations, at a planetary scope and scale.\n\u201cPublic sphere\u201d is an odd kind of thing, however. It is at once a\nconcept\u2014intended to make sense of a space that is not the here and\nnow, but one made up of writings, ideas, and discussions\u2014and a\nset of ideas that people have about themselves and their own par-\nticipation in such a space. I must be able to imagine myself speak-\ning and being spoken to in such a space and to imagine a great\nnumber of other people also doing so according to unwritten rules\nwe share. I don\u2019t need a complete theory, and I don\u2019t need to call\nit a public sphere, but I must somehow share an idea of order with\nall those other people who also imagine themselves participating in\nand subjecting themselves to that order. In fact, if the public sphere\nexists as more than just a theory, then it has no other basis than just\nsuch a shared imagination of order, an imagination which provides\na guide against which to make judgments and a map for chang-\ning or achieving that order. Without such a shared imagination, a\npublic sphere is otherwise nothing more than a cacophony of voices\nand information, nothing more than a stream of data, structured\nand formatted by and for machines, whether paper or electronic.\n38 geeks and recursive publics Charles Taylor, building on the work of J\u00fcrgen Habermas and\nMichael Warner, suggests that the public sphere (both idea and\nthing) that emerged in the eighteenth century was created through\npractices of communication and association that reflected a moral\norder in which the public stands outside power and guides or checks\nits operation through shared discourse and enlightened discussion.\nContrary to the experience of bodies coming together into a com-\nmon space (Taylor calls them \u201ctopical spaces,\u201d such as conversa-\ntion, ritual, assembly), the crucial component is that the public\nsphere \u201ctranscends such topical spaces. We might say that it knits a\nplurality of spaces into one larger space of non-assembly. The same\npublic discussion is deemed to pass through our debate today, and\nsomeone else\u2019s earnest conversation tomorrow, and the newspaper\ninterview Thursday and so on. . . . The public sphere that emerges\nin the eighteenth century is a meta-topical common space.\u201d5\nBecause of this, Taylor refers to his version of a public as a \u201cso-\ncial imaginary,\u201d a way of capturing a phenomena that wavers be-\ntween having concrete existence \u201cout there\u201d and imagined rational\nexistence \u201cin here.\u201d There are a handful of other such imagined\nspaces\u2014the economy, the self-governing people, civil society\u2014and\nin Taylor\u2019s philosophical history they are related to each through\nthe \u201cideas of moral and social order\u201d that have developed in the\nWest and around the world.6\nTaylor\u2019s social imaginary is intended to do something specific: to\nresist the \u201cspectre of idealism,\u201d the distinction between ideas and\npractices, between \u201cideologies\u201d and the so-called material world as\n\u201crival causal agents.\u201d Taylor suggests, \u201cBecause human practices\nare the kind of thing that makes sense, certain ideas are internal to\nthem; one cannot distinguish the two in order to ask the question\nWhich causes which?\u201d7 Even if materialist explanations of cause\nare satisfying, as they often are, Taylor suggests that they are so\n\u201cat the cost of being implausible as a universal principle,\u201d and he\noffers instead an analysis of the rise of the modern imaginaries of\nmoral order.8\nThe concept of recursive public, like that of Taylor\u2019s public sphere,\nis understood here as a kind of social imaginary. The primary rea-\nson is to bypass the dichotomy between ideas and material practice.\nBecause the creation of software, networks, and legal documents\nare precisely the kinds of activities that trouble this distinction\u2014\nthey are at once ideas and things that have material effects in the\ngeeks and recursive publics 39 world, both expressive and performative\u2014it is extremely difficult\nto identify the properly material materiality (source code? com-\nputer chips? semiconductor manufacturing plants?). This is the first\nof the reasons why a recursive public is to be distinguished from the\nclassic formulae of the public sphere, that is, that it requires a kind\nof imagination that includes the writing and publishing and speak-\ning and arguing we are familiar with, as well as the making of\nnew kinds of software infrastructures for the circulation, archiving,\nmovement, and modifiability of our enunciations.\nThe concept of a social imaginary also avoids the conundrums\ncreated by the concept of \u201cideology\u201d and its distinction from mate-\nrial practice. Ideology in its technical usage has been slowly and\nsurely overwhelmed by its pejorative meaning: \u201cThe ideological is\nnever one\u2019s own position; it is always the stance of someone else,\nalways their ideology.\u201d9 If one were to attempt an explanation of\nany particular ideology in nonpejorative terms, there is seemingly\nnothing that might rescue the explanation from itself becoming\nideological.\nThe problem is an old one. Clifford Geertz noted it in \u201cIdeology\nas a Cultural System,\u201d as did Karl Mannheim before him in Ideology\nand Utopia: it is the difficulty of employing a non-evaluative con-\ncept of ideology.10 Of all the versions of struggle over the concept\nof a scientific or objective sociology, it is the claim of exploring ide-\nology objectively that most rankles. As Geertz put it, \u201cMen do not\ncare to have beliefs to which they attach great moral significance\nexamined dispassionately, no matter for how pure a purpose; and\nif they are themselves highly ideologized, they may find it simply\nimpossible to believe that a disinterested approach to critical mat-\nters of social and political conviction can be other than a scholastic\nsham.\u201d11\nMannheim offered one response: a version of epistemological\nrelativism in which the analysis of ideology included the ideologi-\ncal position of the analyst. Geertz offered another: a science of\n\u201csymbolic action\u201d based in Kenneth Burke\u2019s work and drawing on\na host of philosophers and literary critics.12 Neither the concept\nof ideology, nor the methods of cultural anthropology have been\nthe same since. \u201cIdeology\u201d has become one of the most widely\ndeployed (some might say, most diffuse) tools of critique, where\ncritique is understood as the analysis of cultural patterns given\nin language and symbolic structures, for the purposes of bringing\n40 geeks and recursive publics to light systems of hegemony, domination, authority, resistance,\nand\/or misrecognition.13 However, the practices of critique are just\nas (if not more) likely to be turned on critical scholars themselves,\nto show how the processes of analysis, hidden assumptions, latent\nfunctions of the university, or other unrecognized features the ma-\nterial, non-ideological real world cause the analyst to fall into an\nideological trap.\nThe concept of ideology takes a turn toward \u201csocial imaginary\u201d\nin Paul Ricoeur\u2019s Lectures on Ideology and Utopia, where he proposes\nideological and utopian thought as two components of \u201csocial and\ncultural imagination.\u201d Ricoeur\u2019s overview divides approaches to\nthe concept of ideology into three basic types\u2014the distorting, the\nintegrating, and the legitimating\u2014according to how actors deal\nwith reality through (symbolic) imagination. Does the imagina-\ntion distort reality, integrate it, or legitimate it vis-\u00e0-vis the state?\nRicoeur defends the second, Geertzian flavor: ideologies integrate\nthe symbolic structure of the world into a meaningful whole, and\n\u201conly because the structure of social life is already symbolic can it\nbe distorted.\u201d14\nFor Ricoeur, the very substance of life begins in the interpreta-\ntion of reality, and therefore ideologies (as well as utopias\u2014and\nperhaps conspiracies) could well be treated as systems that inte-\ngrate those interpretations into the meaningful wholes of political\nlife. Ricoeur\u2019s analysis of the integration of reality though social\nimagination, however, does not explicitly address how imagina-\ntion functions: what exactly is the nature of this symbolic action or\ninterpretation, or imagination? Can one know it from the outside,\nand does it resist the distinction between ideology and material\npractice? Both Ricoeur and Geertz harbor hope that ideology can\nbe made scientific, that the integration of reality through symbolic\naction requires only the development of concepts adequate to the\njob.\nRe-enter Charles Taylor. In Modern Social Imaginaries the con-\ncept of social imaginary is distinctive in that it attempts to capture\nthe specific integrative imaginations of modern moral and social\norder. Taylor stresses that they are imaginations\u2014not necessarily\ntheories\u2014of modern moral and social order: \u201cBy social imaginary,\nI mean something much broader and deeper than the intellectual\nschemes people may entertain when they think about social real-\nity in a disengaged mode. I am thinking, rather, of the ways in\ngeeks and recursive publics 41 which people imagine their social existence, how they fit together\nwith others, how things go on between them and their fellows, the\nexpectations that are normally met, and the deeper normative no-\ntions and images that underlie these expectations.\u201d15 Social imagi-\nnaries develop historically and result in both new institutions and\nnew subjectivities; the concepts of public, market, and civil society\n(among others) are located in the imaginative faculties of actors\nwho recognize the shared, common existence of these ideas, even if\nthey differ on the details, and the practices of those actors reflect a\ncommitment to working out these shared concepts.\nSocial imaginaries are an extension of \u201cbackground\u201d in the philo-\nsophical sense: \u201ca wider grasp of our whole predicament.\u201d16 The\nexample Taylor uses is that of marching in a demonstration: the ac-\ntion is in our imaginative repertory and has a meaning that cannot\nbe reduced to the local context: \u201cWe know how to assemble, pick\nup banners and march. . . . [W]e understand the ritual. . . . [T]he\nimmediate sense of what we are doing, getting the message to our\ngovernment and our fellow citizens that the cuts must stop, say,\nmakes sense in a wider context, in which we see ourselves standing\nin a continuing relation with others, in which it is appropriate to\naddress them in this manner.\u201d17 But we also stand \u201cinternationally\u201d\nand \u201cin history\u201d against a background of stories, images, legends,\nsymbols, and theories. \u201cThe background that makes sense of any\ngiven act is wide and deep. It doesn\u2019t include everything in our\nworld, but the relevant sense-giving features can\u2019t be circumscribed.\n. . . [It] draws on our whole world, that is, our sense of our whole\npredicament in time and space, among others and in history.\u201d18\nThe social imaginary is not simply the norms that structure our\nactions; it is also a sense of what makes norms achievable or \u201creal-\nizable,\u201d as Taylor says. This is the idea of a \u201cmoral order,\u201d one that\nwe expect to exist, and if it doesn\u2019t, one that provides a plan for\nachieving it. For Taylor, there is such a thing as a \u201cmodern idea of\norder,\u201d which includes, among other things, ideas of what it means\nto be an individual, ideas of how individual passions and desires\nare related to collective association, and, most important, ideas\nabout living in time together (he stresses a radically secular con-\nception of time\u2014secular in a sense that means more than simply\n\u201coutside religion\u201d). He by no means insists that this is the only such\ndefinition of modernity (the door is wide open to understanding\nalternative modernities), but that the modern idea of moral order is\n42 geeks and recursive publics one that dominates and structures a very wide array of institutions\nand individuals around the world.\nThe \u201cmodern idea of moral order\u201d is a good place to return to the\nquestion of geeks and their recursive publics. Are the ideas of order\nshared by geeks different from those Taylor outlines? Do geeks like\nSean and Adrian, or activists in Berlin, possess a distinctive social\nimaginary? Or do they (despite their planetary dispersal) participate\nin this common modern idea of moral order? Do the stories and nar-\nratives, the tools and technologies, the theories and imaginations\nthey follow and build on have something distinctive about them?\nSean\u2019s and Adrian\u2019s commitment to transforming healthcare seems\nto be, for instance, motivated by a notion of moral order in which\nthe means of allocation of healthcare might become more just, but\nit is also shot through with technical ideas about the role of stan-\ndards, the Internet, and the problems with current technical solu-\ntions; so while they may seem to be simply advocating for better\nhealthcare, they do so through a technical language and practice\nthat are probably quite alien to policymakers, upper management,\nand healthcare advocacy groups that might otherwise be in com-\nplete sympathy.\nThe affinity of geeks for each other is processed through and by\nideas of order that are both moral and technical\u2014ideas of order that\ndo indeed mix up \u201coperating systems and social systems.\u201d These\nsystems include the technical means (the infrastructure) through\nwhich geeks meet, assemble, collaborate, and plan, as well as how\nthey talk and think about those activities. The infrastructure\u2014the\nInternet\u2014allows for a remarkably wide and diverse array of people\nto encounter and engage with each other. That is to say, the idea\nof order shared by geeks is shared because they are geeks, because\nthey \u201cget it,\u201d because the Internet\u2019s structure and software have\ntaken a particular form through which geeks come to understand\nthe moral order that gives the fabric of their political lives warp\nand weft.\nInternet Silk Road\nBangalore, March 2000. I am at another bar, this time on one of\nBangalore\u2019s trendiest streets. The bar is called Purple Haze, and I\nhave been taken there, the day after my arrival, by Udhay Shankar\ngeeks and recursive publics 43 N. Inside it is dark and smoky, purple, filled with men between\neighteen and thirty, and decorated with posters of Jimi Hendrix,\nBlack Sabbath, Jim Morrison (Udhay: \u201cI hate that band\u201d), Led Zep-\npelin, and a somewhat out of place Frank Zappa (Udhay: \u201cOne of\nmy political and musical heroes\u201d). All of the men, it appears, are\nsinging along with the music, which is almost without exception\nheavy metal.\nI engage in some stilted conversation with Udhay and his cousin\nKirti about the difference between Karnatic music and rock-and-\nroll, which seems to boil down to the following: Karnatic music\ndecreases metabolism and heart rate, leading to a relaxed state of\nmind; rock music does the opposite. Given my aim of focusing on\nthe Internet and questions of openness, I have already decided not\nto pay attention to this talk of music. In retrospect, I understand\nthis to have been a grave methodological error: I underestimated\nthe extent to which the subject of music has been one of the pri-\nmary routes into precisely the questions about the \u201creorientation of\nknowledge and power\u201d I was interested in. Over the course of the\nevening and the following days, Udhay introduced me, as prom-\nised, to a range of people he either knew or worked with in some\ncapacity. Almost all of the people I met appeared to sincerely love\nheavy-metal music.\nI met udhay Shankar N. in 1999 through a newsletter, distributed\nvia e-mail, called Tasty Bits from the Technology Front. It was one of\na handful of sources I watched closely while in Berlin, looking for\nsuch connections to geek culture. The newsletter described a start-up\ncompany in Bangalore, one that was devoted to creating a gateway\nbetween the Internet and mobile phones, and which was, according\nto the newsletter, an entirely Indian operation, though presumably\nwith u.S. venture funds. I wanted to find a company to compare to\nAmicas: a start-up, run by geeks, with a similar approach to the In-\nternet, but halfway around the world and in a \u201cculture\u201d that might\nbe presumed to occupy a very different kind of moral order. udhay\ninvited me to visit and promised to introduce me to everyone he\nknew. He described himself as a \u201crandom networker\u201d; he was not\nreally a programmer or a designer or a Free Software geek, despite\nhis extensive knowledge of software, devices, operating systems,\nand so on, including Free and Open Source Software. Neither was\nhe a businessman, but rather described himself as the guy who\n\u201ctranslates between the suits and the techs.\u201d\n44 geeks and recursive publics Udhay \u201ccollects interesting people,\u201d and it was primarily through\nhis zest for collecting that I met all the people I did. I met cosmopoli-\ntan activists and elite lawyers and venture capitalists and engineers\nand cousins and brothers and sisters of engineers. I met advertising\nexecutives and airline flight attendants and consultants in Bombay.\nI met journalists and gastroenterologists, computer-science profes-\nsors and musicians, and one mother of a robot scientist in Banga-\nlore. Among them were Muslims, Hindus, Jains, Jews, Parsis, and\nChristians, but most of them considered themselves more secular\nand scientific than religious. Many were self-educated, or like their\nU.S. counterparts, had dropped out of university at some point, but\ncontinued to teach themselves about computers and networks. Some\nwere graduates or employees of the Indian Institute of Science in\nBangalore, an institution that was among the most important for In-\ndian geeks (as Stanford University is to Silicon Valley, many would\nsay). Among the geeks to whom Udhay introduced me, there were\nonly two commonalities: the geeks were, for the most part, male,\nand they all loved heavy-metal music.19\nWhile I was in Bangalore, I was invited to join a mailing list\nrun by Udhay called Silk-list, an irregular, unmoderated list de-\nvoted to \u201cintelligent conversation.\u201d The list has no particular fo-\ncus: long, meandering conversations about Indian politics, religion,\neconomics, and history erupt regularly; topics range from food to\nscience fiction to movie reviews to discussions on Kashmir, Harry\nPotter, the singularity, or nanotechnology. Udhay started Silk-list\nin 1997 with Bharath Chari and Ram Sundaram, and the recipients\nhave included hundreds of people around the world, some very\nwell-known ones, programmers, lawyers, a Bombay advertising\nexecutive, science-fiction authors, entrepreneurs, one member of\nthe start-up Amicas, at least two transhumanists, one (diagnosed)\nschizophrenic, and myself. Active participants usually numbered\nabout ten to fifteen, while many more lurked in the background.\nSilk-list is an excellent index of the relationship between the net-\nwork of people in Bangalore and their connection to a worldwide\ncommunity on the Internet\u2014a fascinating story of the power of\nheterogeneously connected networks and media. Udhay explained\nthat in the early 1990s he first participated in and then taught\nhimself to configure and run a modem-based networking system\nknown as a Bulletin Board Service (BBS) in Bangalore. In 1994\nhe heard about a book by Howard Rheingold called The Virtual\ngeeks and recursive publics 45 Community, which was his first introduction to the Internet. A cou-\nple of years later when he finally had access to the Internet, he im-\nmediately e-mailed John Perry Barlow, whose work he knew from\nWired magazine, to ask for Rheingold\u2019s e-mail address in order to\nconnect with him. Rheingold and Barlow exist, in some ways, at\nthe center of a certain kind of geek world: Rheingold\u2019s books are\nwidely read popular accounts of the social and community aspects\nof new technologies that have often had considerable impact inter-\nnationally; Barlow helped found the Electronic Frontier Foundation\nand is responsible for popularizing the phrase \u201cinformation wants\nto be free.\u201d20 Both men had a profound influence on udhay and ul-\ntimately provided him with the ideas central to running an online\ncommunity. A series of other connections of similar sorts\u2014some\npersonal, some precipitated out of other media and other chan-\nnels, some entirely random\u2014are what make up the membership\nof Silk-list.21\nLike many similar communities of \u201cdigerati\u201d during and after\nthe dot.com boom, Silk-list constituted itself more or less organi-\ncally around people who \u201cgot it,\u201d that is, people who claimed to\nunderstand the Internet, its transformative potential, and who had\nthe technical skills to participate in its expansion. Silk-list was not\nthe only list of its kind. Others such as the Tasty Bits newsletter,\nthe FoRK (Friends of Rohit Khare) mailing list (both based in Bos-\nton), and the Nettime and Syndicate mailing lists (both based in\nthe Netherlands) ostensibly had different reasons for existence, but\nmany had the same subscribers and overlapping communities of\ngeeks. Subscription was open to anyone, and occasionally someone\nwould stumble on the list and join in, but most were either invited\nby members or friends of friends, or they were connected by virtue\nof cross-posting from any number of other mailing lists to which\nmembers were subscribed.\n\/pub\nSilk-list is public in many senses of the word. Practically speak-\ning, one need not be invited to join, and the material that passes\nthrough the list is publicly archived and can be found easily on the\nInternet. Udhay does his best to encourage everyone to speak and\nto participate, and to discourage forms of discourse that he thinks\n46 geeks and recursive publics might silence participants into lurking. Silk-list is not a government,\ncorporate, or nongovernmental list, but is constituted only through\nthe activity of geeks finding each other and speaking to each other\non this list (which can happen in all manner of ways: through work,\nthrough school, through conferences, through fame, through ran-\ndom association, etc.). Recall Charles Taylor\u2019s distinction between\na topical and a metatopical space. Silk-list is not a conventionally\ntopical space: at no point do all of its members meet face-to-face\n(though there are regular meet-ups in cities around the world), and\nthey are not all online at the same time (though the volume and\ntempo of messages often reflect who is online \u201cspeaking\u201d to each\nother at any given moment). It is a topical space, however, if one\nconsiders it from the perspective of the machine: the list of names\non the mailing list are all assembled together in a database, or in\na file, on the server that manages the mailing list. It is a stretch\nto call this an \u201cassembly,\u201d however, because it assembles only the\navatars of the mailing-list readers, many of whom probably ignore\nor delete most of the messages.\nSilk-list is certainly, on the other hand, a \u201cmetatopical\u201d public.\nIt \u201cknits together\u201d a variety of topical spaces: my discussion with\nfriends in Houston, and other members\u2019 discussions with people\naround the world, as well as the sources of multiple discussions like\nnewspaper and magazine articles, films, events, and so on that are\nreported and discussed online. But Silk-list is not \u201cThe\u201d public\u2014it is\nfar from being the only forum in which the public sphere is knitted\ntogether. Many, many such lists exist.\nIn Publics and Counterpublics Michael Warner offers a further dis-\ntinction. \u201cThe\u201d public is a social imaginary, one operative in the\nterms laid out by Taylor: as a kind of vision of order evidenced\nthrough stories, images, narratives, and so on that constitute the\nimagination of what it means to be part of the public, as well as\nplans necessary for creating the public, if necessary. Warner dis-\ntinguishes, however, between a concrete, embodied audience, like\nthat at a play, a demonstration, or a riot (a topical public in Tay-\nlor\u2019s terms), and an audience brought into being by discourse and\nits circulation, an audience that is not metatopical so much as it\nis a public that is concrete in a different way; it is concrete not in\nthe face-to-face temporality of the speech act, but in the sense of\ncalling a public into being through an address that has a differ-\nent temporality. It is a public that is concrete in a media-specific\ngeeks and recursive publics 47 manner: it depends on the structures of creation, circulation, use,\nperformance, and reuse of particular kinds of discourse, particular\nobjects or instances of discourse.\nWarner\u2019s distinction has a number of implications. The first, as\nWarner is careful to note, is that the existence of particular media\nis not sufficient for a public to come into existence. Just because a\nbook is printed does not mean that a public exists; it requires also\nthat the public take corresponding action, that is, that they read\nit. To be part of a particular public is to choose to pay attention\nto those who choose to address those who choose to pay attention\n. . . and so on. Or as Warner puts it, \u201cThe circularity is essential\nto the phenomenon. A public might be real and efficacious, but its\nreality lies in just this reflexivity by which an addressable object is\nconjured into being in order to enable the very discourse that gives\nit existence.\u201d22\nThis \u201cautotelic\u201d feature of a public is crucial if one is to under-\nstand the function of a public as standing outside of power. It simply\ncannot be organized by the state, by a corporation, or by any other\nsocial totality if it is to have the legitimacy of an independently\nfunctioning public. As Warner puts it, \u201cA public organizes itself\nindependently of state institutions, law, formal frameworks of citi-\nzenship, or preexisting institutions such as the church. If it were not\npossible to think of the public as organized independently of the\nstate or other frameworks, the public could not be sovereign with\nrespect to the state. . . . Speaking, writing, and thinking involve\nus\u2014actively and immediately\u2014in a public, and thus in the being\nof the sovereign.\u201d23\nWarner\u2019s description makes no claim that any public or even The\nPublic actually takes this form in the present: it is a description\nof a social imaginary or a \u201cfaith\u201d that allows individuals to make\nsense of their actions according to a modern idea of social order. As\nWarner (and Habermas before him) suggests, the existence of such\nautonomous publics\u2014and certainly the idea of \u201cpublic opinion\u201d\u2014\ndoes not always conform to this idea of order. Often such publics\nturn out to have been controlled all along by states, corporations,\ncapitalism, and other forms of social totality that determine the\nnature of discourse in insidious ways. A public whose participants\nhave no faith that it is autotelic and autonomous is little more than\na charade meant to assuage opposition to authority, to transform\n48 geeks and recursive publics political power and equality into the negotiation between unequal\nparties.\nIs Silk-list a public? More important, is it a sovereign one? War-\nner\u2019s distinction between different media-specific forms of assembly\nis crucial to answering this question. If one wants to know whether\na mailing list on the Internet is more or less likely to be a sovereign\npublic than a book-reading public or the nightly-news-hearing one,\nthen one needs to approach it from the specificity of the form of\ndiscourse. This specificity not only includes whether the form is\ntext or video and audio, or whether the text is ASCII or Unicode, or\nthe video PAL or NTSC, but it also includes the means of creation,\ncirculation, and reuse of that discourse as well.\nFor example, consider the differences between a book published\nin a conventional fashion, by a conventional, corporate press, dis-\ntributed to bookstores or via Amazon.com, and a book published\nby an Internet start-up which makes an electronic copy freely\navailable with a copyleft license, yet charges (a lower price) for\na print-on-demand hardcopy. Both books might easily enter the\nmetatopical space of The Public: discussed in homes, schools, on\nmailing lists, glowingly reviewed or pilloried, perhaps having ef-\nfects on corporate behavior, state, or public policy. The former,\nhowever, is highly constrained in terms of who will author such\na book, how it will be distributed, marketed, edited, and revised,\nand so on. Copyright law will restrict what readers can do with it,\nincluding how they might read it or subsequently circulate it or\nmake derivative use of it. However, a traditionally published book\nis also enriched by its association with a reputable corporation: it\nis treated more or less immediately as authoritative, perhaps as\nmeeting some standard of accuracy, precision, or even truth, and\nits quality is measured primarily by sales.\nThe on-demand, Internet-mediated book, by contrast, will have a\nmuch different temporality of circulation: it might languish in ob-\nscurity due to lack of marketing or reputable authority, or it might\nget mentioned somewhere like the New York Times and suddenly\nbecome a sensation. For such a book, copyright law (in the form\nof a copyleft license) might allow a much wider range of uses and\nreuses, but it will restrict certain forms of commercialization of the\ntext. The two publics might therefore end up looking quite differ-\nent, overlapping, to be sure, but varying in terms of their control\ngeeks and recursive publics 49 and the terms of admittance. What is at stake is the power of one\nor the other such public to appear as an independent and sovereign\nentity\u2014free from suspect constraints and control\u2014whose function\nis to argue with other constituted forms of power.\nThe conventionally published book may well satisfy all the cri-\nteria of being a public, at least in the colloquial sense of making\na set of ideas and a discourse widely available and expecting to\ninfluence, or receive a response from, constituted forms of sover-\neign power. However, it is only the latter \u201con-demand\u201d scheme for\npublishing that satisfies the criteria of being a recursive public. The\ndifferences in this example offer a crude indication of why the In-\nternet is so crucially important to geeks, so important that it draws\nthem together, in its defense, as an infrastructure that enables the\ncreation of publics that are thought to be autonomous, indepen-\ndent, and autotelic. Geeks share an idea of moral and technical\norder when it comes to the Internet; not only this, but they share\na commitment to maintaining that order because it is what al-\nlows them to associate as a recursive public in the first place. They\ndiscover, or rediscover, through their association, the power and\npossibility of occupying the position of independent public\u2014one\nnot controlled by states, corporations, or other organizations, but\nopen (they claim) through and through\u2014and develop a desire to\ndefend it from encroachment, destruction, or refeudalization (to\nuse Habermas\u2019s term for the fragmentation of the public sphere).\nThe recursive public is thus not only the book and the discourse\naround the book. It is not even \u201ccontent\u201d expanded to include all\nkinds of media. It is also the technical structure of the Internet\nas well: its software, its protocols and standards, its applications\nand software, its legal status and the licenses and regulations that\ngovern it. This captures both of the reasons why recursive publics\nare distinctive: (1) they include not only the discourses of a public,\nbut the ability to make, maintain, and manipulate the infrastruc-\ntures of those discourses as well; and (2) they are \u201clayered\u201d and\ninclude both discourses and infrastructures, to a specific technical\nextent (i.e., not all the way down). The meaning of which layers\nare important develops more or less immediately from direct en-\ngagement with the medium. In the following example, for instance,\nNapster represents the potential of the Internet in miniature\u2014as an\napplication\u2014but it also connects immediately to concerns about\nthe core protocols that govern the Internet and the process of stan-\n50 geeks and recursive publics dardization that governs the development of these protocols: hence\nrecursion through the layers of an infrastructure.\nThese two aspects of the recursive public also relate to a concern\nabout the fragmentation or refeudalization of the public sphere:\nthere is only one Internet. Its singularity is not technically determined\nor by any means necessary, but it is what makes the Internet so\nvaluable to geeks. It is a contest, the goal of which is to main-\ntain the Internet as an infrastructure for autonomous and autotelic\npublics to emerge as part of The Public, understood as part of an\nimaginary of moral and technical order: operating systems and\nsocial systems.\nFrom Napster to the Internet\nOn 27 July 2000 Eugen Leitl cross-posted to Silk-list a message with\nthe subject line \u201cPrelude to the Singularity.\u201d The message\u2019s original\nauthor, Jeff Bone (not at the time a member of Silk-list), had posted\nthe \u201cop-ed piece\u201d initially to the FoRK mailing list as a response\nto the Recording Industry Association of America\u2019s (RIAA) actions\nagainst Napster. The RIAA had just succeeded in getting U.S. dis-\ntrict judge Marilyn Hall Patel, Ninth Circuit Court of Appeals, to\nissue an injunction to Napster to stop downloads of copyrighted\nmusic. Bone\u2019s op-ed said,\nPopular folklore has it that the Internet was designed with decentral-\nized routing protocols in order to withstand a nuclear attack. That\nis, the Internet \u201csenses damage\u201d and \u201croutes around it.\u201d It has been\nsaid that, on the \u2019Net, censorship is perceived as damage and is sub-\nsequently routed around. The RIAA, in a sense, has cast itself in a\ncensor\u2019s role. Consequently, the music industry will be perceived as\ndamage\u2014and it will be routed around. There is no doubt that this will\nhappen, and that technology will evolve more quickly than businesses\nand social institutions can; there are numerous highly-visible projects\nalready underway that attempt to create technology that is invulner-\nable to legal challenges of various kinds. Julian Morrison, the origina-\ntor of a project (called Fling) to build a fully anonymous\/untraceable\nsuite of network protocols, expresses this particularly eloquently.24\nBone\u2019s message is replete with details that illustrate the meaning\nand value of the Internet to geeks, and that help clarify the concept\ngeeks and recursive publics 51 of a recursive public. While it is only one message, it nonetheless\ncondenses and expresses a variety of stories, images, folklore, and\ntechnical details that I elaborate herein.\nThe Napster shutdown in 2000 soured music fans and geeks alike,\nand it didn\u2019t really help the record labels who perpetrated it either.\nFor many geeks, Napster represented the Internet in miniature,\nan innovation that both demonstrated something on a scope and\nscale never seen before, and that also connected people around\nsomething they cared deeply about\u2014their shared interest in music.\nNapster raised interesting questions about its own success: Was it\nsuccessful because it allowed people to develop new musical interests\non a scope and scale they had never experienced before? Or was\nit successful because it gave people with already existing musical\ninterests a way to share music on a scope and scale they had never\nexperienced before? That is to say, was it an innovation in mar-\nketing or in distribution? The music industry experienced it as the\nlatter and hence as direct competition with their own means of dis-\ntribution. Many music fans experienced it as the former, what Cory\nDoctorow nicely labeled \u201crisk-free grazing,\u201d meaning the ability to\ntry out an almost unimaginable diversity of music before choosing\nwhat to invest one\u2019s interests (and money) in. To a large extent,\nNapster was therefore a recapitulation of what the Internet already\nmeant to geeks.\nBone\u2019s message, the event of the Napster shutdown, and the vari-\nous responses to it nicely illustrate the two key aspects of the re-\ncursive public: first, the way in which geeks argue not only about\nrights and ideas (e.g., is it legal to share music?) but also about\nthe infrastructures that allow such arguing and sharing; second,\nthe \u201clayers\u201d of a recursive public are evidenced in the immediate\nconnection of Napster (an application familiar to millions) to the\n\u201cdecentralized routing protocols\u201d (TCP\/IP, DNS, and others) that\nmade it possible for Napster to work the way it did.\nBone\u2019s message contains four interrelated points. The first con-\ncerns the concept of autonomous technical progress. The title \u201cPre-\nlude to the Singularity\u201d refers to a 1993 article by Vernor Vinge\nabout the notion of a \u201csingularity,\u201d a point in time when the speed\nof autonomous technological development outstrips the human ca-\npacity to control it.25 The notion of singularity has the status of a\nkind of colloquial \u201claw\u201d similar to Moore\u2019s Law or Metcalfe\u2019s Law,\nas well as signaling links to a more general literature with roots in\n52 geeks and recursive publics libertarian or classically liberal ideas of social order ranging from\nJohn Locke and John Stuart Mill to Ayn Rand and David Brin.26\nBone\u2019s affinity for transhumanist stories of evolutionary theory,\neconomic theory, and rapid innovation sets the stage for the rest\nof his message. The crucial rhetorical gambit here is the appeal\nto inevitability (as in the emphatic \u201cthere is no doubt that this will\nhappen\u201d): Bone establishes that he is speaking to an audience that\nis accustomed to hearing about the inevitability of technical prog-\nress and the impossibility of legal maneuvering to change it, but\nhis audience may not necessarily agree with these assumptions.\nGeeks occupy a spectrum from \u201cpolymath\u201d to \u201ctranshumanist,\u201d a\nspectrum that includes their understandings of technological prog-\nress and its relation to human intervention. Bone\u2019s message clearly\nlands on the far transhumanist side.\nA second point concerns censorship and the locus of power: ac-\ncording to Bone, power does not primarily reside with the govern-\nment or the church, but comes instead from the private sector, in\nthis case the coalition of corporations represented by the RIAA.\nThe significance of this has to do with the fact that a \u201cpublic\u201d is\nexpected to be its own sovereign entity, distinct from church, state,\nor corporation, and while censorship by the church or the state is a\nfamiliar form of aggression against publics, censorship by corpora-\ntions (or consortia representing them), as it strikes Bone and others,\nis a novel development. Whether the blocking of file-sharing can\nlegitimately be called censorship is also controversial, and many\nSilk-list respondents found the accusation of censorship untenable.\nProving Bone\u2019s contention, over the course of the subsequent\nyears and court cases, the RIAA and the Motion Picture Associa-\ntion of America (MPAA) have been given considerably more police\nauthority than even many federal agencies\u2014especially with regard\nto policing networks themselves (an issue which, given its technical\nabstruseness, has rarely been mentioned in the mainstream mass\nmedia). Both organizations have not only sought to prosecute file-\nsharers but have been granted rights to obtain information from\nInternet Service Providers about customer activities and have con-\nsistently sought the right to secretly disable (hack into, disable,\nor destroy) private computers suspected of illegal activity. Even if\nthese practices may not be defined as censorship per se, they are\nnonetheless fine examples of the issues that most exercise geeks: the\nuse of legal means by a few (in this case, private corporations) to\ngeeks and recursive publics 53 suppress or transform technologies in wide use by the many. They\nalso index the problems of monopoly, antitrust, and technical con-\ntrol that are not obvious and often find expression, for example, in\nallegories of reformation and the control of the music-sharing laity\nby papal authorities.\nThird, Bone\u2019s message can itself be understood in terms of the\nreorientation of knowledge and power. Although what it means\nto call his message an \u201cop-ed\u201d piece may seem obvious, Bone\u2019s\nmessage was not published anywhere in any conventional sense. It\ndoesn\u2019t appear to have been widely cited or linked to. However, for\none day at least, it was a heated discussion topic on three mailing\nlists, including Silk-list. \u201cPublication\u201d in this instance is a different\nkind of event than getting an op-ed in the New York Times.\nThe material on Silk-list rests somewhere between private con-\nversation (in a public place, perhaps) and published opinion. No\neditor made a decision to \u201cpublish\u201d the message\u2014Bone just clicked\n\u201csend.\u201d However, as with any print publication, his piece was theo-\nretically accessible by anyone, and what\u2019s more, a potentially huge\nnumber of copies may be archived in many different places (the\ncomputers of all the participants, the server that hosts the list, the\nYahoo! Groups servers that archive it, Google\u2019s search databases,\netc.). Bone\u2019s message exemplifies the recursive nature of the recur-\nsive public: it is a public statement about the openness of the In-\nternet, and it is an example of the new forms of publicness it makes\npossible through its openness.\nThe constraints on who speaks in a public sphere (such as the\npower of printers and publishers, the requirements of licensing, or\nissues of cost and accessibility) are much looser in the Internet era\nthan in any previous one. The Internet gives a previously unknown\nJeff Bone the power to dash off a manifesto without so much as a\nsecond thought. On the other hand, the ease of distribution belies\nthe difficulty of actually being heard: the multitudes of other Jeff\nBones make it much harder to get an audience. In terms of publics,\nBone\u2019s message can constitute a public in the same sense that a New\nYork Times op-ed can, but its impact and meaning will be different.\nHis message is openly and freely available for as long as there are\ngeeks and laws and machines that maintain it, but the New York\nTimes piece will have more authority, will be less accessible, and,\nmost important, will not be available to just anyone. Geeks imagine\na space where anyone can speak with similar reach and staying\n54 geeks and recursive publics power\u2014even if that does not automatically imply authority\u2014and\nthey imagine that it should remain open at all costs. Bone is there-\nfore interested precisely in a technical infrastructure that ensures\nhis right to speak about that infrastructure and offer critique and\nguidance concerning it.\nThe ability to create and to maintain such a recursive public,\nhowever, raises the fourth and most substantial point that Bone\u2019s\nmessage makes clear. The leap to speaking about the \u201cdecentralized\nrouting protocols\u201d represents clearly the shared moral and technical\norder of geeks, derived in this case from the specific details of the\nInternet. Bone\u2019s post begins with a series of statements that are part\nof the common repertoire of technical stories and images among\ngeeks. Bone begins by making reference to the \u201cfolklore\u201d of the In-\nternet, in which routing protocols are commonly believed to have\nbeen created to withstand a nuclear attack. In calling it folklore he\nsuggests that this is not a precise description of the Internet, but an\nimage that captures its design goals. Bone collapses it into a more\nrecent bit of folklore: \u201cThe Internet treats censorship as damage\nand routes around it.\u201d27 Both bits of folklore are widely circulated\nand cited; they encapsulate one of the core intellectual ideas about\nthe architecture of the Internet, that is, its open and distributed in-\nterconnectivity. There is certainly a specific technical backdrop for\nthis suggestion: the TCP\/IP \u201cinternetting\u201d protocols were designed\nto link up multiple networks without making them sacrifice their\nautonomy and control. However, Bone uses this technical argument\nmore in the manner of a social imaginary than of a theory, that is,\nas a way of thinking about the technical (and moral) order of the\nInternet, of what the Internet is supposed to be like.\nIn the early 1990s this version of the technical order of the In-\nternet was part of a vibrant libertarian dogma asserting that the\nInternet simply could not be governed by any land-based sovereign\nand that it was fundamentally a place of liberty and freedom. This\nwas the central message of people such as John Perry Barlow, John\nGilmore, Howard Rheingold, Esther Dyson, and a host of others\nwho populated both the pre-1993 Internet (that is, before the World\nWide Web became widely available) and the pages of magazines\nsuch as Wired and Mondo 2000\u2014the same group of people, inciden-\ntally, whose ideas were visible and meaningful to Udhay Shankar\nand his friends in India even prior to Internet access there, not to\nmention to Sean and Adrian in Boston, and artists and activists in\ngeeks and recursive publics 55 Europe, all of whom often reacted more strongly against this liber-\ntarian aesthetic.\nFor Jeff Bone (and a great many geeks), the folkloric notion that\n\u201cthe net treats censorship as damage\u201d is a very powerful one: it\nsuggests that censorship is impossible because there is no central\npoint of control. A related and oft-cited sentiment is that \u201ctrying to\ntake something off of the Internet is like trying to take pee out of\na pool.\u201d This is perceived by geeks as a virtue, not a drawback, of\nthe Internet.\nThe argument is quite complex, however: on one side of a spec-\ntrum, there is the belief that the structure of the Internet ensures\nthat censorship cannot happen, technically speaking, so long as the\nInternet\u2019s protocols and software remain open. Furthermore, that\nstructure ensures that all attempts to regulate the Internet will also\nfail (e.g., the related sentiment that \u201cthe Internet treats Congress as\ndamage and routes around it\u201d).\nOn the other side of the spectrum, however, this view of the un-\nregulatable nature of the Internet has been roundly criticized, most\nprominently by Lawrence Lessig, who is otherwise often in sympa-\nthy with geek culture. Lessig suggests that just because the Internet\nhas a particular structure does not mean that it must always be\nthat way.28 His argument has two prongs: first, that the Internet is\nstructured the way it is because it is made of code that people write,\nand thus it could have been and will be otherwise, given that there\nare changes and innovations occurring all the time; second, that\nthe particular structure of the Internet therefore governs or regu-\nlates behavior in particular ways: Code is Law. So while it may be\ntrue that no one can make the Internet \u201cclosed\u201d by passing a law,\nit is also true that the Internet could become closed if the technol-\nogy were to be altered for that purpose, a process that may well be\nnudged and guided by laws, regulations, and norms.\nLessig\u2019s critique is actually at the heart of Bone\u2019s concern, and\nthe concern of recursive publics generally: the Internet is a contest\nand one that needs to be repeatedly and constantly replayed in\norder to maintain it as the legitimate infrastructure through which\ngeeks associate with one another. Geeks argue in detail about what\ndistinguishes technical factors from legal or social ones. Openness\non the Internet is complexly intertwined with issues of availability,\nprice, legal restriction, usability, elegance of design, censorship,\ntrade secrecy, and so on.\n56 geeks and recursive publics However, even where openness is presented as a natural tendency\nfor technology (in oft-made analogies with reproductive fitness and\nbiodiversity, for example), it is only a partial claim in that it rep-\nresents only one of the \u201clayers\u201d of a recursive public. For instance,\nwhen Bone suggests that the net is \u201cinvulnerable to legal attack\u201d\nbecause \u201ctechnology will evolve more quickly than businesses and\nsocial institutions can,\u201d he is not only referring to the fact that the\nInternet\u2019s novel technical configuration has few central points of\ncontrol, which makes it difficult for a single institution to control it,\nbut also talking about the distributed, loosely connected networks\nof people who have the right to write and rewrite software and deal\nregularly with the underlying protocols of the Internet\u2014in other\nwords, of geeks themselves.\nOperating systems and social systems: the imagination of order\nshared by geeks is both moral and technical. It is not only about the\ntechnical structure of the Internet, however innovative that is, but\nalso about the legal and social structure that has emerged with it,\nthe kind of order that has made it possible for geeks to associate in\na planetary public and to become aware of the value of the space\nthey have made.\nMany geeks, perhaps including Bone, discover the nature of this\norder by coming to understand how the Internet works\u2014how it\nworks technically, but also who created it and how. Some have\ncome to this understanding through participation in Free Software\n(an exemplary \u201crecursive public\u201d), others through stories and tech-\nnologies and projects and histories that illuminate the process of\ncreating, growing, and evolving the Internet. The story of the pro-\ncess by which the Internet is standardized is perhaps the most well\nknown: it is the story of the Internet Engineering Task Force and its\nRequests for Comments system.\nRequests for Comments\nFor many geeks, the Internet Engineering Task Force (IETF) and its\nRequests for Comments (RFC) system exemplify key features of the\nmoral and technical order they share, the \u201cstories and practices\u201d\nthat make up a social imaginary, according to Charles Taylor. The\nIETF is a longstanding association of Internet engineers who try to\nhelp disseminate some of the core standards of the Internet through\ngeeks and recursive publics 57 the RFC process. Membership is open to individuals, and the as-\nsociation has very little real control over the structure or growth\nof the Internet\u2014only over the key process of Internet standardiza-\ntion. Its standards rarely have the kind of political legitimacy that\none associates with international treaties and the standards bodies\nof Geneva, but they are nonetheless de facto legitimate. The RFC\nprocess is an unusual standards process that allows modifications\nto existing technologies to be made before the standard is final-\nized. Together Internet standards and the RFC process form the\nbackground of the Napster debate and of Jeff Bone\u2019s claims about\n\u201cinternet routing protocols.\u201d\nA famous bit of Internet-governance folklore expresses succinctly\nthe combination of moral and technical order that geeks share (at-\ntributed to IETF member David Clark): \u201cWe reject kings, presidents,\nand voting. We believe in rough consensus and running code.\u201d29\nThis quote emphasizes the necessity of arguing with and through\ntechnology, the first aspect of a recursive public; the only argu-\nment that convinces is working code. If it works, then it can be\nimplemented; if it is implemented, it will \u201croute around\u201d the legal\ndamage done by the RIAA. The notion of \u201crunning code\u201d is cen-\ntral to an understanding of the relationship between argument-\nby-technology and argument-by-talk for geeks. Very commonly,\nthe response by geeks to people who argued about Napster that\nsummer\u2014and the courts\u2019 decisions regarding it\u2014was to dismiss\ntheir complaints as mere talk. Many suggested that if Napster were\nshut down, thousands more programs like it would spring up in its\nwake. As one mailing-list participant, Ashish \u201cHash\u201d Gulhati, put\nit, \u201cIt is precisely these totally unenforceable and mindless judicial\ndecisions that will start to look like self-satisfied wanking when\nthere\u2019s code out there which will make the laws worth less than the\npaper they\u2019re written on. When it comes to fighting this shit in a\nway that counts, everything that isn\u2019t code is just talk.\u201d30\nSuch powerful rhetoric often collapses the process itself, for some-\none has to write the code. It can even be somewhat paradoxical:\nthere is a need to talk forcefully about the need for less talk and\nmore code, as demonstrated by Eugen Leitl when I objected that\nSilk-listers were \u201cjust talking\u201d: \u201cOf course we should talk. Did my\nlast post consist of some kickass Python code adding sore-missed\nfunctionality to Mojonation? Nope. Just more meta-level waffle\nabout the importance of waffling less, coding more. I lack the\n58 geeks and recursive publics proper mental equipment upstairs for being a good coder, hence I\nattempt to corrupt young impressionable innocents into contribut-\ning to the cause. Unashamedly so. So sue me.\u201d31\nEugen\u2019s flippancy reveals a recognition that there is a political\ncomponent to coding, even if, in the end, talk disappears and only\ncode remains. Though Eugen and others might like to adopt a rhet-\noric that suggests \u201cit will just happen,\u201d in practice none of them re-\nally act that way. Rather, the activities of coding, writing software,\nor improving and diversifying the software that exists are not inevi-\ntable or automatic but have specific characteristics. They require\ntime and \u201cthe proper mental equipment.\u201d The inevitability they\nrefer to consists not in some fantasy of machine intelligence, but in\na social imaginary shared by many people in loosely connected net-\nworks who spend all their free time building, downloading, hack-\ning, testing, installing, patching, coding, arguing, blogging, and\nproselytizing\u2014in short, creating a recursive public enabled by the\nInternet.\nJeff Bone\u2019s op-ed piece, which is typically enthusiastic about the\ninevitability of new technologies, still takes time to reference one\nof thousands (perhaps tens of thousands) of projects as worthy of\nattention and support, a project called Fling, which is an attempt\nto rewrite the core protocols of the Internet.32 The goal of the proj-\nect is to write a software implementation of these protocols with\nthe explicit goal of making them \u201canonymous, untraceable, and\nuntappable.\u201d Fling is not a corporation, a start-up, or a university\nresearch project (though some such projects are); it is only a Web\nsite. The core protocols of the Internet, contained in the RFCs, are\nlittle more than documents describing how computers should inter-\nact with each other. They are standards, but of an unusual kind.33\nBone\u2019s leap from a discussion about Napster to one about the core\nprotocols of the Internet is not unusual. It represents the second\naspect of a recursive public: the importance of understanding the\nInternet as a set of \u201clayers,\u201d each enabling the next and each re-\nquiring an openness that both prevents central control and leads to\nmaximum creativity.\nRFCs have developed from an informal system of memos into a\nformal standardization process over the life of the Internet, as the\nIETF and the Internet Society (ISOC) have become more bureau-\ncratic entities. The process of writing and maintaining these docu-\nments is particular to the Internet, precisely because the Internet\ngeeks and recursive publics 59 is the kind of network experiment that facilitates the sharing of\nresources across administratively bounded networks. It is a process\nthat has allowed all the experimenters to both share the network\nand to propose changes to it, in a common space. RFCs are primar-\nily suggestions, not demands. They are \u201cpublic domain\u201d documents\nand thus available to everyone with access to the Internet. As David\nClark\u2019s reference to \u201cconsensus and running code\u201d demonstrates,\nthe essential component of setting Internet standards is a good,\nworking implementation of the protocols. Someone must write soft-\nware that behaves in the ways specified by the RFC, which is, after\nall, only a document, not a piece of software. Different implemen-\ntations of, for example, the TCP\/IP protocol or the File Transfer\nProtocol (ftp) depend initially on individuals, groups, and\/or cor-\nporations building them into an operating-system kernel or a piece\nof user software and subsequently on the existence of a large num-\nber of people using the same operating system or application.\nIn many cases, subsequent to an implementation that has been\ndisseminated and adopted, the RFCs have been amended to reflect\nthese working implementations and to ordain them as standards.\nSo the current standards are actually bootstrapped, through a pro-\ncess of writing RFCs, followed by a process of creating implemen-\ntations that adhere loosely to the rules in the RFC, then observing\nthe progress of implementations, and then rewriting RFCs so that\nthe process begins all over again. The fact that geeks can have a\ndiscussion via e-mail depends on the very existence of both an RFC\nto define the e-mail protocol and implementations of software to\nsend the e-mails.\nThis standardization process essentially inverts the process of\nplanning. Instead of planning a system, which is then standard-\nized, refined, and finally built according to specification, the RFC\nprocess allows plans to be proposed, implemented, refined, repro-\nposed, rebuilt, and so on until they are adopted by users and be-\ncome the standard approved of by the IETF. The implication for\nmost geeks is that this process is permanently and fundamentally\nopen: changes to it can be proposed, implemented, and adopted\nwithout end, and the better a technology becomes, the more diffi-\ncult it becomes to improve on it, and therefore the less reason there\nis to subvert it or reinvent it. Counterexamples, in which a standard\nemerges but no one adopts it, are also plentiful, and they suggest\nthat the standardization process extends beyond the proposal-\n60 geeks and recursive publics implementation-proposal-standard circle to include the problem of\nactually convincing users to switch from one working technology\nto a better one. However, such failures of adoption are also seen as\na kind of confirmation of the quality or ease of use of the current\nsolution, and they are all the more likely to be resisted when some\norganization or political entity tries to force users to switch to the\nnew standard\u2014something the IETF has refrained from doing for\nthe most part.\nConclusion: Recursive Public\nNapster was a familiar and widely discussed instance of the \u201cre-\norientation of power and knowledge\u201d (or in this case, power and\nmusic) wrought by the Internet and the practices of geeks. Napster\nwas not, however, a recursive public or a Free Software project, but\na dot-com-inspired business plan in which proprietary software was\ngiven away for free in the hopes that revenue would flow from the\nstock market, from advertising, or from enhanced versions of the\nsoftware. Therefore, geeks did not defend Napster as much as they\nexperienced its legal restriction as a wake-up call: the Internet en-\nables Napster and will enable many other things, but laws, corpo-\nrations, lobbyists, money, and governments can destroy all of it.\nI started this chapter by asking what draws geeks together: what\nconstitutes the chain that binds geeks like Sean and Adrian to hip-\nsters in Berlin and to entrepreneurs and programmers in Bangalore?\nWhat constitutes their affinity if it is not any of the conventional\ncandidates like culture, nation, corporation, or language? A collo-\nquial answer might be that it is simply the Internet that brings them\ntogether: cyberspace, virtual communities, online culture. But this\ndoesn\u2019t answer the question of why? Because they can? Because\nCommunity Is Good? If mere association is the goal, why not AOL\nor a vast private network provided by Microsoft?\nMy answer, by contrast, is that geeks\u2019 affinity with one another is\nstructured by shared moral and technical understandings of order.\nThey are a public, an independent public that has the ability to\nbuild, maintain, and modify itself, that is not restricted to the activi-\nties of speaking, writing, arguing, or protesting. Recursive publics\nform through their experience with the Internet precisely because\nthe Internet is the kind of thing they can inhabit and transform. Two\ngeeks and recursive publics 61 things make recursive publics distinctive: the ability to include the\npractice of creating this infrastructure as part of the activity of be-\ning public or contesting control; and the ability to \u201crecurse\u201d through\nthe layers of that infrastructure, maintaining its publicness at each\nlevel without making it into an unchanging, static, unmodifiable\nthing.\nThe affinity constituted by a recursive public, through the me-\ndium of the Internet, creates geeks who understand clearly what\nassociation through the Internet means. This affinity structures\ntheir imagination of what the Internet is and enables: creation,\ndistribution, modification of knowledge, music, science, software.\nThe infrastructure\u2014this-infrastructure-here, the Internet\u2014must be\nunderstood as part of this imaginary (in addition to being a pulsat-\ning tangle of computers, wires, waves, and electrons).\nThe Internet is not the only medium for such association. A cor-\nporation, for example, is also based on a shared imaginary of the\neconomy, of how markets, exchanges, and business cycles are sup-\nposed to work; it is the creation of a concrete set of relations and\npractices, one that is generally inflexible\u2014even in this age of so-\ncalled flexible capitalism\u2014because it requires a commitment of\ntime, humans, and capital. Even in fast capitalism one needs to rent\noffice space, buy toilet paper, install payroll software, and so on.\nSoftware and networks can be equally concrete\u2014connecting\npeople, capital, and other resources over time and thus creating an\ninfrastructure\u2014but they are arguably more flexible, more change-\nable, and more reprogrammable\u2014than a corporation, a sewage\nsystem, or a stock exchange. The Internet, in particular, especially\nin the stories of the IETF and the RFC process, represents a radi-\ncalization of this flexibility: not only can one create an application\nlike Napster that takes clever advantage of the layers (protocols,\nrouters, and routes) of the Internet, but one can actually rewrite\nthe layers themselves, rendering possible a new class of Napsters.\nThe difficulty of doing so increases with ever deeper layers, but the\npossibility is not (yet) arbitrarily restricted by any organization,\nperson, law, or government. Affinity\u2014membership in a recursive\npublic\u2014depends on adopting the moral and technical imaginations\nof this kind of order.\nThe urgency evidenced in the case of Napster (and repeated in\nnumerous other instances, such as the debate over net neutrality)\nis linked to a moral idea of order in which there is a shared imagi-\n62 geeks and recursive publics nary of The Public, and not only a vast multiplicity of competing\npublics. It is an urgency linked directly to the fact that the Internet\nprovides geeks with a platform, an environment, an infrastructure\nthrough which they not only associate, but create, and do so in a\nmanner that is widely felt to be autonomous, autotelic, and inde-\npendent of at least the most conventional forms of power: states\nand corporations\u2014independent enough, in fact, that both states\nand corporations can make widespread use of this infrastructure\n(can become geeks themselves) without necessarily endangering\nits independence.\ngeeks and recursive publics 63 2.\nProtestant Reformers,\nPolymaths, Transhumanists\nGeeks talk a lot. They don\u2019t talk about recursive publics. They don\u2019t\noften talk about imaginations, infrastructures, moral or technical\norders. But they do talk a lot. A great deal of time and typing is\nnecessary to create software and networks: learning and talking,\nteaching and arguing, telling stories and reading polemics, reflect-\ning on the world in and about the infrastructure one inhabits. In\nthis chapter I linger on the stories geeks tell, and especially on\nstories and reflections that mark out contemporary problems of\nknowledge and power\u2014stories about grand issues like progress,\nenlightenment, liberty, and freedom.\nIssues of enlightenment, progress, and freedom are quite obvi-\nously still part of a \u201csocial imaginary,\u201d especially imaginations of\nthe relationship of knowledge and enlightenment to freedom and\nautonomy so clearly at stake in the notion of a public or public sphere. And while the example of Free Software illuminates how\nissues of enlightenment, progress, and freedom are proposed, con-\ntested, and implemented in and through software and networks,\nthis chapter contains stories that are better understood as \u201cusable\npasts\u201d\u2014less technical and more accessible narratives that make\nsense of the contemporary world by reflecting on the past and its\ndifference from today.\nUsable pasts is a more charitable term for what might be called\nmodern myths among geeks: stories that the tellers know to be a\ncombination of fact and fiction. They are told not in order to re-\nmember the past, but in order to make sense of the present and of\nthe future. They make sense of practices that are not questioned\nin the doing, but which are not easily understood in available in-\ntellectual or colloquial terms. The first set of stories I relate are\nthose about the Protestant Reformation: allegories that make use\nof Catholic and Protestant churches, laity, clergy, high priests, and\nreformation-era images of control and liberation. It might be sur-\nprising that geeks turn to the past (and especially to religious alle-\ngory) in order to make sense of the present, but the reason is quite\nsimple: there are no \u201cready-to-narrate\u201d stories that make sense of\nthe practices of geeks today. Precisely because geeks are \u201cfiguring\nout\u201d things that are not clear or obvious, they are of necessity bereft\nof effective ways of talking about it. The Protestant Reformation\nmakes for good allegory because it separates power from control;\nit draws on stories of catechism and ritual, alphabets, pamphlets\nand liturgies, indulgences and self-help in order to give geeks a\nway to make sense of the distinction between power and control,\nand how it relates to the technical and political economy they oc-\ncupy. The contemporary relationship among states, corporations,\nsmall businesses, and geeks is not captured by familiar oppositions\nlike commercial\/noncommercial, for\/against private property, or\ncapitalist\/socialist\u2014it is a relationship of reform and conversion,\nnot revolution or overthrow.\nUsable pasts are stories, but they are stories that reflect specific\nattitudes and specific ways of thinking about the relationship be-\ntween past, present, and future. Geeks think and talk a lot about\ntime, progress, and change, but their conclusions and attitudes are\nby no means uniform. Some geeks are much more aware of the\nspecific historical circumstances and contexts in which they op-\nerate, others less so. In this chapter I pose a question via Michel\nreformers, polymaths, transhumanists 65 Foucault\u2019s famous short piece \u201cWhat Is Enlightenment?\u201d Namely,\nare geeks modern? For Foucault, rereading Kant\u2019s eponymous\npiece from 1784, the problem of being modern (or of an age be-\ning \u201cenlightened\u201d) is not one of a period or epoch that people live\nthrough; rather, it involves a subjective relationship, an attitude.\nKant\u2019s explanation of enlightenment does not suggest that it is itself\na universal, but that it occurs through a form of reflection on what\ndifference the changes of one\u2019s immediate historical past make to\none\u2019s understanding of the supposed universals of a much longer\nhistory\u2014that is, one must ask why it is necessary to think the way\none does today about problems that have been confronted in ages\npast. For Foucault, such reflections must be rooted in the \u201chistori-\ncally unique forms in which the generalities of our relations . . .\nhave been problematized.\u201d1 Thus, I want to ask of geeks, how do\nthey connect the historically unique problems they confront\u2014from\nthe Internet to Napster to intellectual property to sharing and re-\nusing source code\u2014to the generalities of relations in which they\nnarrate them as problems of liberty, knowledge, power, and en-\nlightenment? Or, as Foucault puts it, are they modern in this sense?\nDo they \u201cdespise the present\u201d or not?\nThe attitudes that geeks take in responding to these questions\nfall along a spectrum that I have identified as ranging from \u201cpoly-\nmaths\u201d to \u201ctranshumanists.\u201d These monikers are drawn from real\ndiscussions with geeks, but they don\u2019t designate a kind of person.\nThey are \u201csubroutines,\u201d perhaps, called from within a larger pro-\ngram of moral and technical imaginations of order. It is possible\nfor the same person to be a polymath at work and a transhumanist\nat home, but generally speaking they are conflicting and opposite\nmantles. In polymath routines, technology is an intervention into a\ncomplicated, historically unique field of people, customs, organiza-\ntions, other technologies, and laws; in transhumanist routines, tech-\nnology is seen as an inevitable force\u2014a product of human action,\nbut not of human design\u2014that is impossible to control or resist\nthrough legal or customary means.\nProtestant Reformation\nGeeks love allegories about the Protestant Reformation; they relish\nstories of Luther and Calvin, of popery and iconoclasm, of reforma-\n66 reformers, polymaths, transhumanists tion over revolution. Allegories of Protestant revolt allow geeks to\nmake sense of the relationship between the state (the monarchy),\nlarge corporations (the Catholic Church), the small start-ups, in-\ndividual programmers, and adepts among whom they spend most\nof their time (Protestant reformers), and the laity (known as \u201clus-\ners\u201d and \u201csheeple\u201d). It gives them a way to assert that they prefer\nreformation (to save capitalism from the capitalists) over revolu-\ntion. Obviously, not all geeks tell stories of \u201creligious wars\u201d and the\nProtestant Reformation, but these images reappear often enough in\nconversations that most geeks will more or less instantly recognize\nthem as a way of making sense of modern corporate, state, and\npolitical power in the arena of information technology: the figures\nof Pope, the Catholic Church, the Vatican, the monarchs of various\nnations, the laity, the rebel adepts like Luther and Calvin, as well\nas models of sectarianism, iconoclasm (\u201cIn the beginning was the\nCommand Line\u201d), politicoreligious power, and arcane theological\nargumentation.2 The allegories that unfold provide geeks a way to\nmake sense of a similarly complex modern situation in which it is\nnot the Church and the State that struggle, but the Corporation and\nthe State; and what geeks struggle over are not matters of church\ndoctrine and organization, but matters of information technology\nand its organization as intellectual property and economic motor.\nI stress here that this is not an analogy that I myself am making\n(though I happily make use of it), but is one that is in wide circula-\ntion among the geeks I study. To the historian or religious critic,\nit may seem incomplete, or absurd, or bizarre, but it still serves a\nspecific function, and this is why I highlight it as one component of\nthe practical and technical ideas of order that geeks share.\nAt the first level are allegories of \u201creligious war\u201d or \u201choly war\u201d\n(and increasingly, of \u201cjihads\u201d). Such stories reveal a certain cyni-\ncism: they describe a technical war of details between two pieces of\nsoftware that accomplish the same thing through different means,\nso devotion to one or the other is seen as a kind of arbitrary theo-\nlogical commitment, at once reliant on a pure rationality and re-\nquiring aesthetic or political judgment. Such stories imply that two\ntechnologies are equally good and equally bad and that one\u2019s choice\nof sect is thus an entirely nonrational one based in the vicissitudes\nof background and belief. Some people are zealous proselytizers of\na technology, some are not. As one Usenet message explains: \u201cRe-\nligious \u2018wars\u2019 have tended to occur over theological and doctrinal\nreformers, polymaths, transhumanists 67 technicalities of one sort or another. The parallels between that\nand the computing technicalities that result in \u2018computing wars\u2019 are\npretty strong.\u201d3\nPerhaps the most familiar and famous of these wars is that be-\ntween Apple and Microsoft (formerly between Apple and IBM), a\nconflict that is often played out in dramatic and broad strokes that\nimply fundamental differences, when in fact the differences are\nextremely slight.4 Geeks are also familiar with a wealth of less well-\nknown \u201choly wars\u201d: EMACS versus vi; KDE versus Gnome; Linux\nversus BSD; Oracle versus all other databases.5\nOften the language of the Reformation creeps playfully into oth-\nerwise serious attempts to make aesthetic judgments about technol-\nogy, as in this analysis of the programming language tcl\/tk:\nIt\u2019s also not clear that the primary design criterion in tcl, perl, or Visual\nBASIC was visual beauty\u2014nor, probably, should it have been. Ouster-\nhout said people will vote with their feet. This is important. While the\nHigh Priests in their Ivory Towers design pristine languages of stark\nbeauty and balanced perfection for their own appreciation, the rest of\nthe mundane world will in blind and contented ignorance go plodding\nalong using nasty little languages like those enumerated above. These\npoor sots will be getting a great deal of work done, putting bread on\nthe table for their kids, and getting home at night to share it with\nthem. The difference is that the priests will shake their fingers at the\nlaity, and the laity won\u2019t care, because they\u2019ll be in bed asleep.6\nIn this instance, the \u201creligious war\u201d concerns the difference between\nacademic programming languages and regular programmers made\nequivalent to a distinction between the insularity of the Catholic\nChurch and the self-help of a protestant laity: the heroes (such as\ntcl\/tk, perl, and python\u2014all Free Software) are the \u201cnasty little\nlanguages\u201d of the laity; the High Priests design (presumably) Algol,\nLISP, and other \u201cacademic\u201d languages.\nAt a second level, however, the allegory makes precise use of\nProtestant Reformation details. For example, in a discussion about\nthe various fights over the Gnu C Compiler (gcc), a central compo-\nnent of the various UNIX operating systems, Christopher Browne\nposted this counter-reformation allegory to a Usenet group.\nThe EGCS project was started around two years ago when G++ (and\nGCC) development got pretty \u201cstuck.\u201d EGCS sought to integrate to-\n68 reformers, polymaths, transhumanists gether a number of the groups of patches that people were making to\nthe GCC \u201cfamily.\u201d In effect, there had been a \u201cProtestant Reforma-\ntion,\u201d with split-offs of:\na) The GNU FORTRAN Denomination;\nb) The Pentium Tuning Sect;\nc) The IBM Haifa Instruction Scheduler Denomination;\nd) The C++ Standard Acolytes.\nThese groups had been unable to integrate their efforts (for various\nreasons) with the Catholic Version, GCC 2.8. The Ecumenical GNU\nCompiler Society sought to draw these groups back into the Catholic\nflock. The project was fairly successful; GCC 2.8 was succeeded by\nGCC 2.9, which was not a direct upgrade from 2.8, but rather the\nresults of the EGCS project. EGCS is now GCC.7\nIn addition to the obvious pleasure with which they deploy the\nsectarian aspects of the Protestant Reformation, geeks also allow\nthemselves to see their struggles as those of Luther-like adepts, con-\nfronted by powerful worldly institutions that are distinct but inter-\ntwined: the Catholic Church and absolutist monarchs. Sometimes\nthese comparisons are meant to mock theological argument; some-\ntimes they are more straightforwardly hagiographic. For instance,\na 1998 article in Salon compares Martin Luther and Linus Torvalds\n(originator of the Linux kernel).\nIn Luther\u2019s Day, the Roman Catholic Church had a near-monopoly on\nthe cultural, intellectual and spiritual life of Europe. But the principal\nsource text informing that life\u2014the Bible\u2014was off limits to ordinary\npeople. . . . Linus Torvalds is an information-age reformer cut from the\nsame cloth. Like Luther, his journey began while studying for ordina-\ntion into the modern priesthood of computer scientists at the Univer-\nsity of Helsinki\u2014far from the seats of power in Redmond and Silicon\nValley. Also like Luther, he had a divine, slightly nutty idea to remove\nthe intervening bureaucracies and put ordinary folks in a direct rela-\ntionship to a higher power\u2014in this case, their computers. Dissolving\nthe programmer-user distinction, he encouraged ordinary people to\nparticipate in the development of their computing environment. And\njust as Luther sought to make the entire sacramental shebang\u2014the\nwine, the bread and the translated Word\u2014available to the hoi polloi,\nLinus seeks to revoke the developer\u2019s proprietary access to the OS,\ninsisting that the full operating system source code be delivered\u2014\nwithout cost\u2014to every ordinary Joe at the desktop.8\nreformers, polymaths, transhumanists 69 Adepts with strong convictions\u2014monks and priests whose initia-\ntion and mastery are evident\u2014make the allegory work. Other uses\nof Christian iconography are less, so to speak, faithful to the sources.\nAnother prominent personality, Richard Stallman, of the Free Soft-\nware Foundation, is prone to dressing as his alter-ego, St. IGNUcius,\npatron saint of the church of EMACS\u2014a church with no god, but\nintense devotion to a baroque text-processing program of undeni-\nable, nigh-miraculous power.9\nOften the appeal of Reformation-era rhetoric comes from a\nkind of indictment of the present: despite all this high tech, super-\nfabulous computronic wonderfulness, we are no less feudal, no less\nviolent, no less arbitrary and undemocratic; which is to say, geeks\nhave progressed, have seen the light and the way, but the rest of\nsociety\u2014and especially management and marketing\u2014have not. In\nthis sense, Reformation allegories are stories of how \u201cthings never\nchange.\u201d\nBut the most compelling use of the Protestant Reformation as\nusable past comes in the more detailed understandings geeks have\nof the political economy of information technology. The allego-\nrization of the Catholic Church with Microsoft, for instance, is a\nfrequent component, as in this brief message regarding start-up key\ncombinations in the Be operating system: \u201cThese secret handshakes\nare intended to reinforce a cabalistic high priesthood and should\nnot have been disclosed to the laity. Forget you ever saw this post\nand go by [sic] something from Microsoft.\u201d10\nMore generally, large corporations like IBM, Oracle, or Microsoft\nare made to stand in for Catholicism, while bureaucratic congresses\nand parliaments with their lobbyists take on the role of absolut-\nist monarchs and their cronies. Geeks can then see themselves as\nfighting to uphold Christianity (true capitalism) against the church\n(corporations) and to be reforming a way of life that is corrupted\nby church and monarchs, instead of overthrowing through revolu-\ntion a system they believe to be flawed. There is a historically and\ntechnically specific component of this political economy in which\nit is in the interest of corporations like IBM and Microsoft to keep\nusers \u201clocked as securely to Big Blue as an manacled wretch in a\nmedieval dungeon.\u201d11\nSuch stories appeal because they bypass the language of modern\nAmerican politics (liberal, conservative, Democrat, Republican) in\nwhich there are only two sides to any issue. They also bypass an\n70 reformers, polymaths, transhumanists argument between capitalism and socialism, in which if you are\nnot pro-capitalism you must be a communist. They are stories that\nallow the more pragmatist of the geeks to engage in intervention\nand reformation, rather than revolution. Though I\u2019ve rarely heard\nit articulated so bluntly, the allegory often implies that one must\n\u201csave capitalism from the capitalists,\u201d a sentiment that implies at\nleast some kind of human control over capitalism.\nIn fact, the allegorical use of the Reformation and the church\ngenerates all kinds of clever comparisons. A typical description of\nsuch comparisons might go like this: the Catholic Church stands in\nfor large, publicly traded corporations, especially those control-\nling large amounts of intellectual property (the granting of which\nmight roughly be equated with the ceremonies of communion and\nconfession) for which they depend on the assistance and support\nof national governments. Naturally, it is the storied excesses of the\nchurch\u2014indulgences, liturgical complexity, ritualistic ceremony,\nand corruption\u2014which make for easy allegory. Modern corpora-\ntions can be figured as a small, elite papal body with theologians\n(executives and their lawyers, boards of directors and their law-\nyers), who command a much larger clergy (employees), who serve\na laity (consumers) largely imagined to be sinful (underspending\non music and movies\u2014indeed, even \u201cstealing\u201d them) and thus in\nneed of elaborate and ritualistic cleansing (advertising and law-\nsuits) by the church. Access to grace (the American Dream) is medi-\nated only by the church and is given form through the holy acts of\nshopping and home improvement. The executives preach messages\nof damnation to the government, messages most government of-\nficials are all too willing to hear: do not tamper with our market\nshare, do not affect our pricing, do not limit our ability to expand\nthese markets. The executives also offer unaccountable promises of\nsalvation in the guise of deregulation and the American version of\n\u201creform\u201d\u2014the demolition of state and national social services. Gov-\nernment officials in turn have developed their own \u201cdivine right of\nkings,\u201d which justifies certain forms of manipulation (once called\n\u201celections\u201d) of succession. Indulgences are sold left and right by\nlobbyists or industry associations, and the decrees of the papacy\nevidence little but full disconnection from the miserable everyday\nexistence of the flock.\nIn fact, it is remarkable how easy such comparisons become the\nmore details of the political economy of information one learns. But\nreformers, polymaths, transhumanists 71 allegories of the Reformation and clerical power can lead easily to\ncynicism, which should perhaps be read in this instance as evidence\nof political disenfranchisement, rather than a lapse in faith. And\nyet the usable pasts of these reformation-minded modern monks\nand priests crop up regularly not only because they provide relief\nfrom technical chatter but because they explain a political, tech-\nnical, legal situation that does not have ready-to-narrate stories.\nGeeks live in a world finely controlled by corporate organizations,\nmass media, marketing departments, and lobbyists, yet they share\na profound distrust of government regulation\u2014they need another\nset of just-so stories to make sense of it. The standard unusable\npasts of the freeing of markets, the inevitability of capitalism and\ndemocracy, or more lately, the necessity of security don\u2019t do justice\nto their experience.\nAllegories of Reformation are stories that make sense of the po-\nlitical economy of information. But they also have a more precise\nuse: to make sense of the distinction between power and control.\nBecause geeks are \u201ccloser to the machine\u201d than the rest of the laity,\none might reasonably expect them to be the ones in power. This is\nclearly not the case, however, and it is the frustrations and mys-\nteries by which states, corporations, and individuals manipulate\ntechnical details in order to shift power that often earns the deepest\nire of geeks. Control, therefore, includes the detailed methods and\nactual practices by which corporations, government agencies, or\nindividuals attempt to manipulate people (or enroll them to ma-\nnipulate themselves and others) into making technical choices that\nserve power, rather than rationality, liberty, elegance, or any other\ngeekly concern.\nConsider the subject of evil. During my conversations with Sean\nDoyle in the late 1990s, as well as with a number of other geeks,\nthe term evil was regularly used to refer to some kind of design or\ntechnical problem. I asked Sean what he meant.\nSD: [Evil is] just a term I use to say that something\u2019s wrong, but usu-\nally it means something is wrong on purpose, there was agency be-\nhind it. I can\u2019t remember [the example you gave] but I think it may\nhave been some GE equipment, where it has this default where it\nlikes to send things in its own private format rather than in DICOM\n[the radiology industry standard for digital images], if you give it a\nchoice. I don\u2019t know why they would have done something like that,\n72 reformers, polymaths, transhumanists it doesn\u2019t solve any backward compatibility problem, it\u2019s really just\nan exclusionary sort of thing. So I guess there\u2019s Evil like that. . . .\nCK: one of the other examples that you had . . . was something with\nInternet Explorer 3.0?\nSD: Yes, oh yes, there are so many things with IE3 that are completely\nEvil. Like here\u2019s one of them: in the http protocol there\u2019s a thing\ncalled the \u201cuser agent field\u201d where a browser announces to the server\nwho it is. If you look at IE, it announces that it is Mozilla, which is\nthe [code-name for] Netscape. Why did they do this? Well because\na lot of the web servers were sending out certain code that said, if\nit were Mozilla they would serve the stuff down, [if not] they would\nsend out something very simple or stupid that would look very ugly.\nBut it turned out that [IE3, or maybe IE2] didn\u2019t support things\nwhen it first came out. Like, I don\u2019t think they supported tables, and\nlater on, their versions of Javascript were so different that there was\nno way it was compatible\u2014it just added tremendous complexity. It\nwas just a way of pissing on the Internet and saying there\u2019s no law\nthat says we have to follow these Internet standards. We can do as\nwe damn well please, and we\u2019re so big that you can\u2019t stop us. So I\nview it as Evil in that way. I mean they obviously have the talent to\ndo it. They obviously have the resources to do it. They\u2019ve obviously\ndone the work, it\u2019s just that they\u2019ll have this little twitch where they\nwon\u2019t support a certain MIME type or they\u2019ll support some things\ndifferently than others.\nCK: But these kinds of incompatibility issues can happen as a result\nof a lack of communication or coordination, which might involve\nagency at some level, right?\nSD: Well, I think of that more as Stupidity than Evil [laughter]. No,\nEvil is when there is an opportunity to do something, and an un-\nderstanding that there is an opportunity to, and resources and all\nthat\u2014and then you do something just to spite the other person. You\nknow I\u2019m sure it\u2019s like in messy divorces, where you would rather\nsell the property at half its value rather than have it go to the other\nperson.\nSean relates control to power by casting the decisions of a large\ncorporation in a moral light. Although the specific allegory of the\nProtestant Reformation does not operate here, the details do. Mi-\ncrosoft\u2019s decision to manipulate Internet Explorer\u2019s behavior stems\nnot from a lack of technical sophistication, nor is it an \u201caccident\u201d of\nreformers, polymaths, transhumanists 73 complexity, according to Sean, but is a deliberate assertion of eco-\nnomic and political power to corrupt the very details by which soft-\nware has been created and standardized and is expected to function.\nThe clear goal of this activity is conversion, the expansion of Micro-\nsoft\u2019s flock through a detailed control of the beliefs and practices\n(browsers and functionality) of computer users. Calling Microsoft\n\u201cEvil\u201d in this way has much the same meaning as questioning the\nCatholic Church\u2019s use of ritual, ceremony, literacy, and history\u2014the\ndetails of the \u201cimplementation\u201d of religion, so to speak.\nOr, in the terms of the Protestant Reformation itself, the practices\nof conversion as well as those of liberation, learning, and self-help\nare central to the story. It is not an accident that many historians\nof the Reformation themselves draw attention to the promises of\nliberation through reformation \u201cinformation technologies.\u201d12 Col-\nloquial (and often academic) assertions that the printing press was\ntechnologically necessary or sufficient to bring the Reformation\nabout appear constantly as a parable of this new age of informa-\ntion. Often the printing press is the only \u201ctechnological\u201d cause con-\nsidered, but scholars of the real, historical Reformation also pay\nclose attention to the fact of widespread literacy, to circulating de-\nvotional pamphlets, catechisms, and theological tracts, as well as\nto the range of transformations of political and legal relationships\nthat occurred simultaneously with the introduction of the printing\npress.\n@\n\u00a9\nOne final way to demonstrate the effectiveness of these allegories\u2014\ntheir ability to work on the minds of geeks\u2014is to demonstrate how\nthey have started to work on me, to demonstrate how much of a\ngeek I have become\u2014a form of participant allegorization, so to\nspeak. The longer one considers the problems that make up the con-\ntemporary political economy of information technology that geeks\ninhabit, the more likely it is that these allegories will start to pre-\nsent themselves almost automatically\u2014as, for instance, when I read\nThe Story of A, a delightful book having nothing to do with geeks,\na book about literacy in early America. The author, Patricia Crain,\nexplains that the Christ\u2019s cross (see above) was often used in the\ncreation of hornbooks or battledores, small leather-backed paddles\ninscribed with the Lord\u2019s Prayer and the alphabet, which were used\n74 reformers, polymaths, transhumanists to teach children their ABCs from as early as the fifteenth century\nuntil as late as the nineteenth: \u201cIn its early print manifestations,\nthe pedagogical alphabet is headed not by the letter A but by the\n@\n\u2018Christ\u2019s Cross\u2019: . . . . Because the alphabet is associated with Cath-\nolic Iconography, as if the two sets of signs were really part of one\nsemiological system, one of the struggles of the Reformation would\nbe to wrest the alphabet away from the Catholic Church.\u201d13\nHere, allegorically, the Catholic Church\u2019s control of the alphabet\n(like Microsoft\u2019s programming of Internet Explorer to blur public\nstandards for the Internet) is not simply ideological; it is not just\na fantasy of origin or ownership planted in the fallow mental soil\nof believers, but in fact a very specific, very nonsubjective, and\nvery media-specific normative tool of control. Crain explains fur-\n@\nther: \u201cToday represents the imprimatur of the Catholic Church on\ncopyright pages. In its connection to the early modern alphabet as\nwell, this cross carries an imprimatur or licensing effect. This \u2018let\nit be printed,\u2019 however, is directed not to the artisan printer but\nto the mind and memory of the young scholar. . . . Like modern\ncopyright, the cross authorizes the existence of the alphabet and as-\nsociates the letters with sacred authorship, especially since another\n@\nlong-lived function of in liturgical missals is to mark gospel pas-\nsages. The symbol both conveys information and generates ritual\nbehavior.\u201d14\nThe \u00a9 today carries as much if not more power, both ideologi-\ncally and legally, as the cross of the Catholic church. It is the very\nsymbol of authorship, even though in origin and in function it gov-\nerns only ownership and rights. Magical thinking about copyright\nabounds, but one important function of the symbol \u00a9, if not its\nlegal implications, is to achieve the same thing as the Christ\u2019s cross:\nto associate in the mind of the reader the ownership of a particular\ntext (or in this case, piece of software) with a particular organiza-\ntion or person. Furthermore, even though the symbol is an artifact\nof national and international law, it creates an association not be-\ntween a text and the state or government, but between a text and\nparticular corporations, publishers, printers, or authors.\nLike the Christ\u2019s cross, the copyright symbol carries both a licens-\ning effect (exclusive, limited or nonexclusive) and an imprimatur\non the minds of people: \u201clet it be imprinted in memory\u201d that this is\nthe work of such and such an author and that this is the property of\nsuch and such a corporation.\nreformers, polymaths, transhumanists 75 Without the allegory of the Protestant Reformation, the only\navailable narrative for such evil\u2014whether it be the behavior of\nMicrosoft or of some other corporation\u2014is that corporations are\n\u201ccompeting in the marketplace according to the rules of capital-\nism\u201d and thus when geeks decry such behavior, it\u2019s just sour grapes.\nIf corporations are not breaking any laws, why shouldn\u2019t they be\nallowed to achieve control in this manner? In this narrative there\nis no room for a moral evaluation of competition\u2014anything goes, it\nwould seem. Claiming for Microsoft that it is simply playing by the\nrules of capitalism puts everyone else into either the competitor\nbox or the noncompetitor box (the state and other noncompetitive\norganizations). Using the allegory of the Protestant Reformation,\non the other hand, gives geeks a way to make sense of an unequal\ndistribution among competing powers\u2014between large and small\ncorporations, and between market power and the details of control.\nIt provides an alternate imagination against which to judge the\ntechnically and legally specific actions that corporations and indi-\nviduals take, and to imagine forms of justified action in return.\nWithout such an allegory, geeks who oppose Microsoft are gen-\nerally forced into the position of being anticapitalist or are forced\nto adopt the stance that all standards should be publicly gener-\nated and controlled, a position few wish to take. Indeed, many\ngeeks would prefer a different kind of imaginary altogether\u2014a\nrecursive public, perhaps. Instead of an infrastructure subject to\nunequal distributions of power and shot through with \u201cevil\u201d distor-\ntions of technical control, there is, as geeks see it, the possibility\nfor a \u201cself-leveling\u201d level playing field, an autotelic system of rules,\nboth technical and legal, by which all participants are expected to\ncompete equally. Even if it remains an imaginary, the allegory of\nthe Protestant Reformation makes sense of (gives order to) the po-\nlitical economy of the contemporary information-technology world\nand allows geeks to conceive of their interests and actions accord-\ning to a narrative of reformation, rather than one of revolution or\nsubmission. In the Reformation the interpretation or truth of Chris-\ntian teaching was not primarily in question: it was not a doctrinal\nrevolution, but a bureaucratic one. Likewise, geeks do not question\nthe rightness of networks, software, or protocols and standards,\nnor are they against capitalism or intellectual property, but they\ndo wish to maintain a space for critique and the moral evaluation\nof contemporary capitalism and competition.\n76 reformers, polymaths, transhumanists Polymaths and Transhumanists\nUsable pasts articulate the conjunction of \u201coperating systems and\nsocial systems,\u201d giving narrative form to imaginations of moral\nand technical order. To say that there are no ready-to-narrate sto-\nries about contemporary political economy means only that the\nstandard colloquial explanations of the state of the modern world\ndo not do justice to the kinds of moral and technical imaginations\nof order that geeks possess by virtue of their practices. Geeks live\nin, and build, one kind of world\u2014a world of software, networks,\nand infrastructures\u2014but they are often confronted with stories\nand explanations that simply don\u2019t match up with their experi-\nence, whether in newspapers and on television, or among nongeek\nfriends. To many geeks, proselytization seems an obvious route:\nwhy not help friends and neighbors to understand the hidden world\nof networks and software, since, they are quite certain, it will come\nto structure their lives as well?\nGeeks gather through the Internet and, like a self-governing peo-\nple, possess nascent ideas of independence, contract, and constitu-\ntion by which they wish to govern themselves and resist governance\nby others.15 Conventional political philosophies like libertarianism,\nanarchism, and (neo)liberalism only partially capture these social\nimaginaries precisely because they make no reference to the op-\nerating systems, software, and networks within which geeks live,\nwork, and in turn seek to build and extend.\nGeeks live in specific ways in time and space. They are not just us-\ners of technology, or a \u201cnetwork society,\u201d or a \u201cvirtual community,\u201d\nbut embodied and imagining actors whose affinity for one another\nis enabled in new ways by the tools and technologies they have such\ndeep affective connections to. They live in this-network-here, a histor-\nically unique form grounded in particular social, moral, national,\nand historical specificities which nonetheless relates to generalities\nsuch as progress, technology, infrastructure, and liberty. Geeks are\nby no means of one mind about such generalities though, and they\noften have highly developed means of thinking about them.\nFoucault\u2019s article \u201cWhat Is Enlightenment?\u201d captures part of this\nproblematic. For Foucault, Kant\u2019s understanding of modernity was\nan attempt to rethink the relationship between the passage of his-\ntorical time and the subjective relationship that individuals have\ntoward it.\nreformers, polymaths, transhumanists 77 Thinking back on Kant\u2019s text, I wonder whether we may not envisage\nmodernity as an attitude rather than as a period of history. And by\n\u201cattitude,\u201d I mean a mode of relating to contemporary reality; a vol-\nuntary choice made by certain people; in the end, a way of thinking\nand feeling; a way, too, of acting and behaving that at one and the\nsame time marks a relation of belonging and presents itself as a task.\nNo doubt a bit like what the Greeks called an ethos. And consequently,\nrather than seeking to distinguish the \u201cmodern era\u201d from the \u201cpremod-\nern\u201d or \u201cpostmodern,\u201d I think it would be more useful to try to find\nout how the attitude of modernity, ever since its formation, has found\nitself struggling with attitudes of \u201ccountermodernity.\u201d16\nIn thinking through how geeks understand the present, the past,\nand the future, I pose the question of whether they are \u201cmodern\u201d in\nthis sense. Foucault makes use of Baudelaire as his foil for explain-\ning in what the attitude of modernity consists: \u201cFor [Baudelaire,]\nbeing modern . . . consists in recapturing something eternal that is\nnot beyond the present, or behind it, but within it.\u201d17 He suggests\nthat Baudelaire\u2019s understanding of modernity is \u201can attitude that\nmakes it possible to grasp the \u2018heroic\u2019 aspect of the present moment\n. . . the will to \u2018heroize\u2019 the present.\u201d18 Heroic here means some-\nthing like redescribing the seemingly fleeting events of the present\nin terms that conjure forth the universal or eternal character that\nanimates them. In Foucault\u2019s channeling of Baudelaire such an at-\ntitude is incommensurable with one that sees in the passage of\nthe present into the future some version of autonomous progress\n(whether absolute spirit or decadent degeneration), and the tag\nhe uses for this is \u201cyou have no right to despise the present.\u201d To be\nmodern is to confront the present as a problem that can be trans-\nformed by human action, not as an inevitable outcome of processes\nbeyond the scope of individual or collective human control, that\nis, \u201cattitudes of counter-modernity.\u201d When geeks tell stories of the\npast to make sense of the future, it is often precisely in order to\n\u201cheroize\u201d the present in this sense\u2014but not all geeks do so. Within\nthe spectrum from polymath to transhumanist, there are attitudes\nof both modernity and countermodernity.\nThe questions I raise here are also those of politics in a classical\nsense: Are the geeks I discuss bound by an attitude toward the pres-\nent that concerns such things as the relationship of the public to\nthe private and the social (\u00e0 la Hannah Arendt), the relationship\n78 reformers, polymaths, transhumanists of economics to liberty (\u00e0 la John Stuart Mill and John Dewey),\nor the possibilities for rational organization of society through the\napplication of scientific knowledge (\u00e0 la Friedrich Hayek or Fou-\ncault)? Are geeks \u201cenlightened\u201d? Are they Enlightenment rational-\nists? What might this mean so long after the Enlightenment and\nits vigorous, wide-ranging critiques? How is their enlightenment\nrelated to the technical and infrastructural commitments they have\nmade? Or, to put it differently, what makes enlightenment newly\nnecessary now, in the milieu of the Internet, Free Software, and re-\ncursive publics? What kinds of relationships become apparent when\none asks how these geeks relate their own conscious appreciation\nof the history and politics of their time to their everyday practices\nand commitments? Do geeks despise the present?\nPolymaths and transhumanists speak differently about concepts\nlike technology, infrastructure, networks, and software, and they\nhave different ideas about their temporality and relationship to\nprogress and liberty. Some geeks see technology as one kind of in-\ntervention into a constituted field of organizations, money, politics,\nand people. Some see it as an autonomous force made up of hu-\nmans and impersonal forces of evolution and complexity. Different\ngeeks speak about the role of technology and its relationship to the\npresent and future in different ways, and how they understand this\nrelationship is related to their own rich understandings of the com-\nplex technical and political environment they live and work in.\nPolymaths Polymathy is \u201cavowed dilettantism,\u201d not extreme intel-\nligence. It results from a curiosity that seems to grip a remarkable\nnumber of people who spend their time on the Internet and from\nthe basic necessity of being able to evaluate and incorporate some-\ntimes quite disparate fields of knowledge in order to build work-\nable software. Polymathy inevitably emerges in the context of large\nsoftware and networking projects; it is a creature of constraints,\na process bootstrapped by the complex sediment of technologies,\nbusinesses, people, money, and plans. It might also be posed in\nthe negative: bad software design is often the result of not enough\navowed dilettantism. Polymaths must know a very large and wide\nrange of things in order to intervene in an existing distribution of\nmachines, people, practices, and places. They must have a detailed\nsense of the present, and the project of the present, in order to imag-\nine how the future might be different.\nreformers, polymaths, transhumanists 79 My favorite polymath is Sean Doyle. Sean built the first versions\nof a piece of software that forms the centerpiece of the radiological-\nimage-management company Amicas. In order to build it Sean\nlearned the following: Java, to program it; the mathematics of\nwavelets, to encode the images; the workflow of hospital radiolo-\ngists and the manner in which they make diagnoses from images,\nto make the interface usable; several incompatible databases and\nthe SQL database language, to build the archive and repository;\nand manual after manual of technical standards, the largest and\nmost frightening of which was the Digital Imaging and Communi-\ncation (DICOM) standard for radiological images. Sean also read\nScience and Nature regularly, looking for inspiration about inter-\nface design; he read books and articles about imaging very small\nthings (mosquito knees), very large things (galaxies and interstellar\ndust), very old things (fossils), and very pretty things (butterfly-\nwing patterns as a function of developmental pathways). Sean also\nintroduced me to Tibetan food, to Jan Svankmeyer films, to Open\nSource Software, to cladistics and paleoherpetology, to Disney\u2019s\nscorched-earth policy with respect to culture, and to many other\nawesome things.\nSean is clearly an unusual character, but not that unusual. Over\nthe years I have met many people with a similar range and depth of\nknowledge (though rarely with Sean\u2019s humility, which does set him\napart). Polymathy is an occupational hazard for geeks. There is no\nsense in which a good programmer, software architect, or informa-\ntion architect simply specializes in code. Specialization is seen not\nas an end in itself, but rather as a kind of technical prerequisite\nbefore other work\u2014the real work\u2014can be accomplished. The real\nwork is the design, the process of inserting usable software into a\ncompletely unfamiliar amalgamation of people, organizations, ma-\nchines, and practices. Design is hard work, whereas the technical\nstuff\u2014like choosing the right language or adhering to a standard or\nfinding a ready-made piece of code to plug in somewhere\u2014is not.\nIt is possible for Internet geeks and software architects to think\nthis way in part due to the fact that so many of the technical issues\nthey face are both extremely well defined and very easy to address\nwith a quick search and download. It is easy to be an avowed dilet-\ntante in the age of mailing lists, newsgroups, and online scientific\npublishing. I myself have learned whole swaths of technical prac-\ntices in this manner, but I have designed no technology of note.\n80 reformers, polymaths, transhumanists Sean\u2019s partner in Amicas, Adrian Gropper, also fits the bill of\npolymath, though he is not a programmer. Adrian, a physician and\na graduate of MIT\u2019s engineering program, might be called a \u201chigh-\nfunctioning polymath.\u201d He scans the horizon of technical and sci-\nentific accomplishments, looking for ways to incorporate them into\nhis vision of medical technology qua intervention. Sean mockingly\ncalls these \u201cdelusions,\u201d but both agree that Amicas would be no-\nwhere without them. Adrian and Sean exemplify how the meanings\nof technology, intervention, design, and infrastructure are under-\nstood by polymaths as a particular form of pragmatic intervention,\na progress achieved through deliberate, piecemeal re-formation of\nexisting systems. As Adrian comments:\nI firmly believe that in the long run the only way you can save money\nand improve healthcare is to add technology. I believe that more\nstrongly than I believe, for instance, that if people invent better pesti-\ncides they\u2019ll be able to grow more rice, and it\u2019s for the universal good\nof the world to be able to support more people. I have some doubt\nas to whether I support people doing genetic engineering of crops\nand pesticides as being \u201cto the good.\u201d But I do, however, believe that\nhealthcare is different in that in the long run you can impact both the\ncost and quality of healthcare by adding technology. And you can call\nthat a religious belief if you want, it\u2019s not rational. But I guess what\nI\u2019m willing to say is that traditional healthcare that\u2019s not technology-\nbased has pretty much run out of steam.19\nIn this conversation, the \u201ctechnological\u201d is restricted to the novel\nthings that can make healthcare less costly (i.e., cost-reducing,\nnot cost-cutting), ease suffering, or extend life. Certain kinds of\ntechnological intervention are either superfluous or even pointless,\nand Adrian can\u2019t quite identify this \u201cclass\u201d\u2014it isn\u2019t \u201ctechnology\u201d in\ngeneral, but it includes some kinds of things that are technological.\nWhat is more important is that technology does not solve anything\nby itself; it does not obviate the political problems of healthcare\nrationing: \u201cNow, however, you get this other problem, which is\nthat the way that healthcare is rationed is through the fear of pain,\nfinancial pain to some extent, but physical pain; so if you have a\ntechnology that, for instance, makes it relatively painless to fix . . .\nI guess, bluntly put, it\u2019s cheaper to let people die in most cases,\nand that\u2019s just undeniable. So what I find interesting in all of this,\nis that most people who are dealing with the politics of healthcare\nreformers, polymaths, transhumanists 81 resource management don\u2019t want to have this discussion, nobody\nwants to talk about this, the doctors don\u2019t want to talk about it,\nbecause it\u2019s too depressing to talk about the value of. . . . And they\ndon\u2019t really have a mandate to talk about technology.\u201d20\nAdrian\u2019s self-defined role in this arena is as a nonpracticing phy-\nsician who is also an engineer and an entrepreneur\u2014hence, his\npolymathy has emerged from his attempts to translate between\ndoctors, engineers, and businesspeople. His goal is twofold: first,\ncreate technologies that save money and improve the allocation of\nhealthcare (and the great dream of telemedicine concerns precisely\nthis goal: the reallocation of the most valuable asset, individuals\nand their expertise); second, to raise the level of discussion in the\nbusiness-cum-medical world about the role of technology in man-\naging healthcare resources. Polymathy is essential, since Adrian\u2019s\ntwofold mission requires understanding the language and lives of\nat least three distinct groups who work elbow-to-elbow in health-\ncare: engineers and software architects; doctors and nurses; and\nbusinessmen.\nTechnology has two different meanings according to Adrian\u2019s two\ngoals: in the first case technology refers to the intervention by means\nof new technologies (from software, to materials, to electronics,\nto pharmaceuticals) in specific healthcare situations wherein high\ncosts or limited access to care can be affected. Sometimes technol-\nogy is allocated, sometimes it does the allocating. Adrian\u2019s goal is\nto match his knowledge of state-of-the-art technology\u2014in particu-\nlar, Internet technology\u2014with a specific healthcare situation and\nthereby effect a reorganization of practices, people, tools, and in-\nformation. The tool Amicas created was distinguished by its clever\nuse of compression, Internet standards, and cheap storage media to\ncompete with much larger, more expensive, much more entrenched\n\u201clegacy\u201d and \u201cturnkey\u201d systems. Whether Amicas invented some-\nthing \u201cnew\u201d is less interesting than the nature of this intervention\ninto an existing milieu. This intervention is what Adrian calls \u201ctech-\nnology.\u201d For Amicas, the relevant technology\u2014the important inter-\nvention\u2014was the Internet, which Amicas conceived as a tool for\nchanging the nature of the way healthcare was organized. Their\ngoal was to replace the infrastructure of the hospital radiology\ndepartment (and potentially the other departments as well) with\nthe Internet. Amicas was able to confront and reform the practices\nof powerful, entrenched entities, from the administration of large\n82 reformers, polymaths, transhumanists hospitals to their corporate bedfellows, like HBOC, Agfa, Siemens,\nand GE.\nWith regard to raising the level of discussion, however, technology\nrefers to a kind of political-rhetorical argument: technology does\nnot save the world (nor does it destroy it); it only saves lives\u2014and\nit does this only when one makes particular decisions about its al-\nlocation. Or, put differently, the means is technology, but the ends\nare still where the action is at. Thus, the hype surrounding infor-\nmation technology in healthcare is horrifying to Adrian: promises\nprecede technologies, and the promises suggest that the means can\nreplace the ends. Large corporations that promise \u201ctechnology,\u201d\nbut offer no real hard interventions (Adrian\u2019s first meaning of tech-\nnology) that can be concretely demonstrated to reduce costs or im-\nprove allocation are simply a waste of resources. Such companies\nare doubly frustrating because they use \u201ctechnology\u201d as a blinder\nthat allows people to not think about the hard problems (the ends)\nof allocation, equity, management, and organization; that is, they\ntreat \u201ctechnology\u201d (the means) as if it were a solution as such.\nAdrian routinely analyzes the rhetorical and practical uses of\ntechnology in healthcare with this kind of subtlety; clearly, such\nsubtlety of thought is rare, and it sets Adrian apart as someone who\nunderstands that intervention into, and reform of, modern organi-\nzations and styles of thought has to happen through reformation\u2014\nthrough the clever use of technology by people who understand it\nintimately\u2014not through revolution. Reformation through technical\ninnovation is opposed here to control through the consolidation of\nmoney and power.\nIn my observations, Adrian always made a point of making the\ntechnology\u2014the software tools and picture-archiving system\u2014\neasily accessible, easily demonstrable to customers. When talking\nto hospital purchasers, he often said something like \u201cI can show you\nthe software, and I can tell you the price, and I can demonstrate\nthe problem it will solve.\u201d In contrast, however, an array of enor-\nmous corporations with salesmen and women (usually called con-\nsultants) were probably saying something more like \u201cYour hospital\nneeds more technology, our corporation is big and stable\u2014give us\nthis much money and we will solve your problem.\u201d For Adrian, the\ndecision to \u201chold hands,\u201d as he put it, with the comfortably large\ncorporation was irrational if the hospital could instead purchase a\nspecific technology that did a specific thing, for a real price.\nreformers, polymaths, transhumanists 83 Adrian\u2019s reflections on technology are also reflections on the na-\nture of progress. Progress is limited intervention structured by goals\nthat are not set by the technology itself, even if entrepreneurial\nactivity is specifically focused on finding new uses and new ideas\nfor new technologies. But discussions about healthcare allocation\u2014\nwhich Adrian sees as a problem amenable to certain kinds of tech-\nnical solutions\u2014are instead structured as if technology did not\nmatter to the nature of the ends. It is a point Adrian resists: \u201cI firmly\nbelieve that in the long run the only way you can save money and\nimprove healthcare is to add technology.\u201d\nSean is similarly frustrated by the homogenization of the concept\nof technology, especially when it is used to suggest, for instance,\nthat hospitals \u201clag behind\u201d other industries with regard to com-\nputerization, a complaint usually made in order to either instigate\ninvestment or explain failures. Sean first objects to such a homog-\nenous notion of \u201ctechnological.\u201d\nI actually have no idea what that means, that it\u2019s lagging behind.\nBecause certainly in many ways in terms of image processing or some\nvery high-tech things it\u2019s probably way ahead. And if that means\nwhat\u2019s on people\u2019s desktops, ever since 19-maybe-84 or so when I\narrived at MGH [Massachusetts General Hospital] there\u2019s been a\ncomputer on pretty much everyone\u2019s desktop. . . . It seems like most\nhospitals that I have been to seem to have a serious commitment to\nnetworks and automation, etcetera. . . . I don\u2019t know about a lot of\nmanufacturing industries\u2014they might have computer consoles there,\nbut it\u2019s a different sort of animal. Farms probably lag really far behind,\nI won\u2019t even talk about amusement parks. In some sense, hospitals are\nvery complicated little communities, and so to say that this thing as a\nwhole is lagging behind doesn\u2019t make much sense.21\nHe also objects to the notion that such a lag results in failures\ncaused by technology, rather than by something like incompetence\nor bad management. In fact, it might be fair to say that, for the\npolymath, sometimes technology actually dissolves. Its boundar-\nies are not easily drawn, nor are its uses, nor are its purported\n\u201cunintended consequences.\u201d On one side there are rules, regula-\ntions, protocols, standards, norms, and forms of behavior; on the\nother there are organizational structures, business plans and logic,\nhuman skills, and other machines. This complex milieu requires\nreform from within: it cannot be replaced wholesale; it cannot leap-\n84 reformers, polymaths, transhumanists frog other industries in terms of computerization, as intervention is\nalways local and strategic; and it involves a more complex relation-\nship to the project of the present than simply \u201clagging behind\u201d or\n\u201cleaping ahead.\u201d\nPolymathy\u2014inasmuch as it is a polymathy of the lived experi-\nence of the necessity for multiple expertise to suit a situation\u2014turns\npeople into pragmatists. Technology is never simply a solution to a\nproblem, but always part of a series of factors. The polymath, un-\nlike the technophobe, can see when technology matters and when\nit doesn\u2019t. The polymath has a very this-worldly approach to tech-\nnology: there is neither mystery nor promise, only human ingenuity\nand error. In this manner, polymaths might better be described as\nFeyerabendians than as pragmatists (and, indeed, Sean turned out\nto be an avid reader of Feyerabend). The polymath feels there is\nno single method by which technology works its magic: it is highly\ndependent on rules, on patterned actions, and on the observation\nof contingent and contextual factors. Intervention into this already\ninstituted field of people, machines, tools, desires, and beliefs re-\nquires a kind of scientific-technical genius, but it is hardly single,\nor even autonomous. This version of pragmatism is, as Feyerabend\nsometimes refers to it, simply a kind of awareness: of standards, of\nrules, of history, of possibility.22 The polymath thus does not allow\nhimself or herself to despise the present, but insists on both reflect-\ning on it and intervening in it.\nSean and Adrian are avowedly scientific and technical people;\nlike Feyerabend, they assume that their interlocutors believe in\ngood science and the benefits of progress. They have little patience\nfor Luddites, for new-agers, for religious intolerance, or for any\nother non-Enlightenment-derived attitude. They do not despise the\npresent, because they have a well-developed sense of how provi-\nsional the conventions of modern technology and business are. Very\nlittle is sacred, and rules, when they exist, are fragile. Breaking\nthem pointlessly is immodest, but innovation is often itself seen as\na way of transforming a set of accepted rules or practices to other\nends. Progress is limited intervention.23\nHow ironic, and troubling, then, to realize that Sean\u2019s and Adrian\u2019s\ncompany would eventually become the kind of thing they started\nAmicas in order to reform. Outside of the limited intervention, cer-\ntain kinds of momentum seem irresistible: the demand for invest-\nment and funding rounds, the need for \u201cprofessional management,\u201d\nreformers, polymaths, transhumanists 85 and the inertia of already streamlined and highly conservative\npurchasing practices in healthcare. For Sean and Adrian, Amicas\nbecame a failure in its success. Nonetheless, they remain resolutely\nmodern polymaths: they do not despise the present. As described in\nKant\u2019s \u201cWhat Is Enlightenment?\u201d the duty of the citizen is broken\ninto public and private: on the one hand, a duty to carry out the\nresponsibilities of an office; on the other, a duty to offer criticism\nwhere criticism is due, as a \u201cscholar\u201d in a reading public. Sean\u2019s\nand Adrian\u2019s endeavor, in the form of a private start-up company,\nmight well be understood as the expression of the scholar\u2019s duty to\noffer criticism, through the creation of a particular kind of tech-\nnical critique of an existing (and by their assessment) ethically\nsuspect healthcare system. The mixture of private capital, public\ninstitutions, citizenship, and technology, however, is something\nKant could not have known\u2014and Sean and Adrian\u2019s technical pur-\nsuits must be understood as something more: a kind of modern civic\nduty, in the service of liberty and responding to the particularities\nof contemporary technical life.24\nTranshumanists Polymathy is born of practical and pragmatic en-\ngagement with specific situations, and in some ways is demanded\nby such exigencies. Opposite polymathy, however, and leaning more\ntoward a concern with the whole, with totality and the universal,\nare attitudes that I refer to by the label transhumanism, which con-\ncerns the mode of belief in the Timeline of Technical Progress.25\nTranshumanism, the movement and the philosophy, focuses on\nthe power of technology to transcend the limitations of the human\nbody as currently evolved. Subscribers believe\u2014but already this is\nthe wrong word\u2014in the possibility of downloading consciousness\nonto silicon, of cryobiological suspension, of the near emergence of\nstrong artificial intelligence and of various other forms of techni-\ncal augmentation of the human body for the purposes of achieving\nimmortality\u2014or at least, much more life.26\nVarious groups could be reasonably included under this label.\nThere are the most ardent purveyors of the vision, the Extropians;\nthere are a broad class of people who call themselves transhuman-\nists; there is a French-Canadian subclass, the Raelians, who are\nmore an alien-worshiping cult than a strictly scientific one and are\nbitterly denounced by the first two; there are also the variety of cos-\nmologists and engineers who do not formally consider themselves\n86 reformers, polymaths, transhumanists transhumanist, but whose beliefs participate in some way or an-\nother: Stephen Hawking, Frank Tipler and John Barrow (famous for\ntheir anthropic cosmological principle), Hans Moravic, Ray Kurz-\nweil, Danny Hillis, and down the line through those who embrace\nthe cognitive sciences, the philosophy of artificial intelligence, the\nphilosophy of mind, the philosophy of science, and so forth.\nHistorically speaking, the line of descent is diffuse. Teilhard de\nChardin is broadly influential, sometimes acknowledged, sometimes\nnot (depending on the amount of mysticism allowed). A more gen-\nerally recognized starting point is Julian Huxley\u2019s article \u201cTranshu-\nmanism\u201d in New Bottles for New Wine.27 Huxley\u2019s transhumanism,\nlike Teilhard\u2019s, has a strange whiff of Nietzsche about it, though\nit tends much more strongly in the direction of the evolutionary\nemergence of the superman than in the more properly moral sense\nNietzsche gave it. After Huxley, the notion of transhumanism is too\neasily identified with eugenics, and it has become one of a series of\nmidcentury subcultural currents which finds expression largely in\nsmall, non-mainstream places, from the libertarians to Esalen.28\nFor many observers, transhumanists are a lunatic fringe, bounded\non either side by alien abductees and Ayn Rand\u2013spouting objec-\ntivists. However, like so much of the fringe, it merely represents\nin crystalline form attitudes that seem to permeate discussions\nmore broadly, whether as beliefs professed or as beliefs attributed.\nTranshumanism, while probably anathema to most people, actu-\nally reveals a very specific attitude toward technical innovation,\ntechnical intervention, and political life that is widespread among\ntechnically adept individuals. It is a belief that has everything to do\nalso with the timeline of progress and the role of technology in it.\nThe transhumanist understanding of technological progress can\nbest be understood through the sometimes serious and sometimes\nplayful concept of the \u201csingularity,\u201d popularized by the science-fiction\nwriter and mathematician Vernor Vinge.29 The \u201csingularity\u201d is the\npoint at which the speed of technical progress is faster than hu-\nman comprehension of that progress (and, by implication, than\nhuman control over the course). It is a kind of cave-man parable,\nperhaps most beautifully rendered by Stanley Kubrik\u2019s film 2001:\nA Space Odyssey (in particular, in the jump-cut early in the film that\nturns a hurled bone into a spinning space station, recapitulating\nthe remarkable adventure of technology in two short seconds of an\notherwise seemingly endless film).\nreformers, polymaths, transhumanists 87 1. Illustration \u00a9 2005 Ray Kurzweil. Modifications \u00a9 2007 by C. Kelty.\nOriginal work licensed under a Creative Commons Attribution License: http:\/\/\nen.wikipedia.org\/wiki\/Image:PPTCountdowntoSingularityLog.jpg.\nIn figure 1, on the left hand of the timeline, there is history, or\nrather, there is a string of technological inventions (by which is\nimplied that previous inventions set the stage for later ones) spaced\nsuch that they produce a logarithmic curve that can look very much\nlike the doomsday population curves that started to appear in the\n1960s. Each invention is associated with a name or sometimes a\nnation. Beyond the edge of the graph to the right side is the future:\nhistory changes here from a series of inventions to an autonomous\nself-inventing technology associated not with individual inven-\ntors but with a complex system of evolutionary adaptation that\nincludes technological as well as biological forms. It is a future in\nwhich \u201chumans\u201d are no longer necessary to the progress of science\nand technology: technology-as-extension-of-humans on the left, a\nBorg-like autonomous technical intelligence on the right. The fun-\n88 reformers, polymaths, transhumanists damental operation in constructing the \u201csingularity\u201d is the \u201crea-\nsoned extrapolation\u201d familiar to the \u201chard science fiction\u201d writer\nor the futurist. One takes present technology as the initial condition\nfor future possibilities and extrapolates based on the (haphazardly\nhandled) evidence of past technical speed-up and change.\nThe position of the observer is always a bit uncertain, since he or\nshe is naturally projected at the highest (or lowest, depending on\nyour orientation) point of this curve, but one implication is clear:\nthat the function or necessity of human reflection on the present\nwill disappear at the same time that humans do, rendering enlight-\nenment a quaint, but necessary, step on the route to superrational,\ntranshuman immortality.\nStrangely, the notion that technical progress has acceleration\nseems to precede any sense of what the velocity of progress might\nmean in the first instance; technology is presumed to exist in abso-\nlute time\u2014from the Big Bang to the heat death of the universe\u2014\nand not in any relationship with human life or consciousness. The\nsingularity is always described from the point of view of a god who\nis not God. The fact of technological speed-up is generally treated\nas the most obvious thing in the world, reinforced by the constant\nrefrain in the media of the incredible pace of change in contempo-\nrary society.\nWhy is the singularity important? Because it always implies that\nthe absolute fact of technical acceleration\u2014this knowing glance\ninto the future\u2014should order the kinds of interventions that occur\nin the present. It is not mute waiting or eschatological certainty\nthat governs this attitude; rather, it is a mode of historical con-\nsciousness that privileges the inevitability of technological progress\nover the inevitability of human power. Only by looking into the\nfuture can one manipulate the present in a way that will be widely\nmeaningful, an attitude that could be expressed as something like\n\u201cThose who do not learn from the future are condemned to suffer\nin it.\u201d Since it is a philosophy based on the success of human ra-\ntionality and ingenuity, rationality and ingenuity are still clearly\nessential in the future. They lead, however, to a kind of posthuman\nstate of constant technological becoming which is inconceivable to\nthe individual human mind\u2014and can only be comprehended by a\ntranscendental intelligence that is not God.\nSuch is a fair description of some strands of transhumanism, and\nthe reason I highlight them is to characterize the kinds of attitudes\nreformers, polymaths, transhumanists 89 toward technology-as-intervention and the ideas of moral and tech-\nnical order that geeks can evince. On the far side of polymathy,\ngeeks are too close to the machine to see a big picture or to think\nabout imponderable philosophical issues; on the transhuman side,\nby contrast, one is constantly reassessing the arcane details of every-\nday technical change with respect to a vision of the whole\u2014a vision\nof the evolution of technology and its relationship to the humans\nthat (for the time being) must create and attempt to channel it.\nMy favorite transhumanist is Eugen Leitl (who is, in fact, an au-\nthentic transhumanist and has been vice-chair of the World Trans-\nhumanist Association). Eugen is Russian-born, lives in Munich, and\nonce worked in a cryobiology research lab. He is well versed in\nchemistry, nanotechnology, artificial-intelligence (AI) research,\ncomputational- and network-complexity research, artificial organs,\ncryobiology, materials engineering, and science fiction. He writes,\nfor example,\nIf you consider AI handcoded by humans, yes. However, given con-\nsiderable computational resources (~cubic meter of computronium),\nand using suitable start population, you can coevolve machine intelli-\ngence on a time scale of much less than a year. After it achieves about\na human level, it is potentially capable of entering an autofeedback\nloop. Given that even autoassembly-grade computronium is capable\nof running a human-grade intellect in a volume ranging from a sugar\ncube to an orange at a speed ranging from 10^4 . . . 10^6 it is easy to\nsee that the autofeedback loop has explosive dynamics.\n(I hope above is intelligible, I\u2019ve been exposed to weird memes for\nfar too long).30\nEugen is also a polymath (and an autodidact to boot), but in the\nconventional sense. Eugen\u2019s polymathy is an avocational necessity:\ntranshumanists need to keep up with all advances in technology and\nscience in order to better assess what kinds of human-augmenting\nor human-obsolescing technologies are out there. It is not for work\nin this world that the transhumanist expands his or her knowledge,\nnor quite for the next, but for a \u201cthis world\u201d yet to arrive.\nEugen and I were introduced during the Napster debates of 2001,\nwhich seemed at the time to be a knock-down, drag-out conflagra-\ntion, but Eugen has been involved in so many online flame wars\nthat he probably experienced it as a mere blip in an otherwise con-\nstant struggle with less-evolved intelligences like mine. Nonethe-\n90 reformers, polymaths, transhumanists less, it was one of the more clarifying examples of how geeks think,\nand think differently, about technology, infrastructure, networks,\nand software. Transhumanism has no truck with old-fashioned\nhumanism.\n> >From: Ramu Narayan . . .\n> >I don\u2019t like the\n> >notion of technology as an unstoppable force with a will of its\nown that\n> >has nothing to do with the needs of real people.\n[Eugen Leitl:] Emergent large-scale behaviour is nothing new. How\ndo you intend to control individual behaviour of a large population\nof only partially rational agents? They don\u2019t come with too many\nconvenient behaviour-modifying hooks (pheromones as in social in-\nsects, but notice menarche-synch in females sharing quarters), and\nfor a good reason. The few hooks we have (mob, war, politics, reli-\ngion) have been notoriously abused, already. Analogous to apoptosis,\nmetaindividuals may function using processes deletorious[sic] to its\ncomponents (us).31\nEugen\u2019s understanding of what \u201ctechnological progress\u201d means is\nsufficiently complex to confound most of his interlocutors. For one\nsurprising thing, it is not exactly inevitable. The manner in which\nLeitl argues with people is usually a kind of machine-gun prattle of\ncoevolutionary, game-theoretic, cryptographic sorites. Eugen piles\non the scientific and transhumanist reasoning, and his interlocutors\nslowly peel away from the discussion. But it isn\u2019t craziness, hype, or\nhalf-digested popular science\u2014Eugen generally knows his stuff\u2014it\njust fits together in a way that almost no one else can quite grasp.\nEugen sees the large-scale adoption and proliferation of technolo-\ngies (particularly self-replicating molecular devices and evolution-\nary software algorithms) as a danger that transcends all possibility\nof control at the individual or state level. Billions of individual deci-\nsions do not \u201caverage\u201d into one will, but instead produce complex\ndynamics and hang perilously on initial conditions. In discussing\nthe possibility of the singularity, Eugen suggests, \u201cIt could literally\nbe a science-fair project [that causes the singularity].\u201d If Francis\nBacon\u2019s understanding of the relation between Man and Nature\nwas that of master and possessor, Eugen\u2019s is its radicalization:\nMan is a powerful but ultimately arbitrary force in the progress of\nLife-Intelligence. Man is fully incorporated into Nature in this story,\nreformers, polymaths, transhumanists 91 so much so that he dissolves into it. Eugen writes, when \u201clife crosses\nover into this petri dish which is getting readied, things will become\na lot more lively. . . . I hope we\u2019ll make it.\u201d\nFor Eugen, the arguments about technology that the polymaths\ninvolve themselves in couldn\u2019t be more parochial. They are im-\nportant only insofar as they will set the \u201cinitial conditions\u201d for the\ngrand coevolutionary adventure of technology ahead of us. For the\ntranshumanist, technology does not dissolve. Instead, it is the so-\nlution within which humans are dissolved. Suffering, allocation,\ndecision making\u2014all these are inessential to the ultimate outcome\nof technological progress; they are worldly affairs, even if they con-\ncern life and death, and as such, they can be either denounced or\nsupported, but only with respect to fine-tuning the acceleration to-\nward the singularity. For the transhumanist, one can\u2019t fight the in-\nevitability of technical evolution, but one certainly can contribute to\nit. Technical progress is thus both law-like and subject to intelligent\nmanipulation; technical progress is inevitable, but only because of\nthe power of massively parallel human curiosity.\nConsidered as one of the modes of thought present in this-worldly\npolitical discussion, the transhumanist (like the polymath) turns\ntechnology into a rhetorical argument. Technology is the more\npowerful political argument because \u201cit works.\u201d It is pointless to\nargue \u201cabout\u201d technology, but not pointless to argue through and\nwith it. It is pointless to talk about whether stopping technology is\ngood or bad, because someone will simply build a technology that\nwill invalidate your argument.\nThere is still a role for technical invention, but it is strongly dis-\ntinguished from political, legal, cultural, or social interventions.\nFor most transhumanists, there is no rhetoric here, no sophistry,\njust the pure truth of \u201cit works\u201d: the pure, undeniable, unstoppable,\nand undeconstructable reality of technology. For the transhumanist\nattitude, the reality of \u201cworking code\u201d has a reality that other as-\nsertions about the world do not. Extreme transhumanism replaces\nthe life-world with the world of the computer, where bad (ethically\nbad) ideas won\u2019t compile. Less-staunch versions of transhumanism\nsimply allow the confusion to operate opportunistically: the prog-\nress of technology is unquestionable (omniscient), and only its ef-\nfects on humans are worth investigating.\nThe pure transhumanist, then, is a countermodern. The transhu-\nmanist despises the present for its intolerably slow descent into the\n92 reformers, polymaths, transhumanists future of immortality and superhuman self-improvement, and fears\ndestruction because of too much turbulent (and ignorant) human\nresistance. One need have no individual conception of the present,\nno reflection on or synthetic understanding of it. One only need\ncontribute to it correctly. One might even go so far as to suggest that\nforms of reflection on the present that do not contribute to techni-\ncal progress endanger the very future of life-intelligence. Curiosity\nand technical innovation are not historical features of Western sci-\nence, but natural features of a human animal that has created its\nown conditions for development. Thus, the transhumanists\u2019 histori-\ncal consciousness consists largely of a timeline that makes ordered\nsense of our place on the progress toward the Singularity.\nThe moral of the story is not just that technology determines his-\ntory, however. Transhumanism is a radically antihumanist position\nin which human agency or will\u2014if it even exists\u2014is not ontologi-\ncally distinct from the agency of machines and animals and life\nitself. Even if it is necessary to organize, do things, make choices,\nparticipate, build, hack, innovate, this does not amount to a belief\nin the ability of humans to control their destiny, individually or\ncollectively. In the end, the transhumanist cannot quite pinpoint\nexactly what part of this story is inevitable\u2014except perhaps the\nstory itself. Technology does not develop without millions of dis-\ntributed humans contributing to it; humans cannot evolve without\nthe explicit human adoption of life-altering and identity-altering\ntechnologies; evolution cannot become inevitable without the ma-\nnipulation of environments and struggles for fitness. As in the di-\nlemma of Calvinism (wherein one cannot know if one is saved by\none\u2019s good works), the transhumanist must still create technology\naccording to the particular and parochial demands of the day, but\nthis by no means determines the eventual outcome of technological\nprogress. It is a sentiment well articulated by Adam Ferguson and\nhighlighted repeatedly by Friederich Hayek with respect to human\nsociety: \u201cthe result of human action, but not the execution of any\nhuman design.\u201d32\nConclusion\nTo many observers, geeks exhibit a perhaps bewildering mix of\nliberalism, libertarianism, anarchism, idealism, and pragmatism,\nreformers, polymaths, transhumanists 93 yet tend to fall firmly into one or another constituted political cat-\negory (liberal, conservative, socialist, capitalist, neoliberal, etc.).\nBy showing how geeks make use of the Protestant Reformation as a\nusable past and how they occupy a spectrum of beliefs about prog-\nress, liberty, and intervention, I hope to resist this urge to classify.\nGeeks are an interesting case precisely because they are involved\nin the creation of new things that change the meaning of our consti-\ntuted political categories. Their politics are mixed up and combined\nwith the technical details of the Internet, Free Software, and the\nvarious and sundry organizations, laws, people, and practices that\nthey deal with on a regular basis: operating systems and social sys-\ntems. But such mixing does not make Geeks merely technoliberals\nor technoconservatives. Rather, it reveals how they think through\nthe specific, historically unique situation of the Internet to the gen-\neral problems of knowledge and power, liberty and enlightenment,\nprogress and intervention.\nGeeks are not a kind of person: geeks are geeks only insofar as\nthey come together in new, technically mediated forms of their own\ncreation and in ways that are not easy to identify (not language,\nnot culture, not markets, not nations, not telephone books or data-\nbases). While their affinity is very clearly constituted through the\nInternet, the Internet is not the only reason for that affinity. It is this\ncollective affinity that I refer to as a recursive public. Because it is\nimpossible to understand this affinity by trying to identify particu-\nlar types of people, it is necessary to turn to historically specific sets\nof practices that form the substance of their affinity. Free Software\nis an exemplary case\u2014perhaps the exemplar\u2014of a recursive pub-\nlic. To understand Free Software through its changing practices not\nonly gives better access to the life-world of the geek but also reveals\nhow the structure of a recursive public comes into being and man-\nages to persist and transform, how it can become a powerful form\nof life that extends its affinities beyond technophile geeks into the\nrealms of ordinary life.\n94 reformers, polymaths, transhumanists Part II free software  3.\nThe Movement\nPart II of Two Bits describes what Free Software is and where it\ncame from, with each of its five chapters detailing the historical\nnarrative of a particular kind of practice: creating a movement,\nsharing source code, conceptualizing openness or open systems,\nwriting copyright (and copyleft) licenses, and coordinating collabo-\nrations. Taken together, the stories describe Free Software. The sto-\nries have their endpoint (or starting point, genealogically speaking)\nin the years 1998\u201399, when Free Software burst onto the scene: on\nthe cover of Forbes magazine, as part of the dotcom boom, and in\nthe boardrooms of venture-capital firms and corporations like IBM\nand Netscape. While the chapters that make up part II can be read\ndiscretely to understand the practices that are the sine qua non of\nFree Software, they can also be read continuously, as a meandering\nstory of the history of software and networks stretching from the\nlate 1950s to the present. Rather than define what makes Free Software free or Open Source\nopen, Two Bits treats the five practices as parts of a collective tech-\nnical experimental system: each component has its own history, de-\nvelopment, and temporality, but they come together as a package\nand emerge as a recognizable thing around 1998\u201399. As with any\nexperimental system, changing the components changes the opera-\ntion and outcomes of the whole. Free Software so conceived is a\nkind of experimental system: its practices can be adopted, adapted,\nand modulated in new contexts and new places, but it is one whose\nrules are collectively determined and frequently modified. It is pos-\nsible to see in each of the five practices where choices about how to\ndo Free Software reached, or surpassed, certain limits, but nonethe-\nless remained part of a system whose identity finally firmed up in\nthe period 1998\u201399 and after.\nThe first of these practices\u2014the making of Free Software into a\nmovement\u2014is both the most immediately obvious and the most\ndifficult to grasp. By the term movement I refer to the practice,\namong geeks, of arguing about and discussing the structure and\nmeaning of Free Software: what it consists of, what it is for, and\nwhether or not it is a movement. Some geeks call Free Software\na movement, and some don\u2019t; some talk about the ideology and\ngoals of Free Software, and some don\u2019t; some call it Free Software,\nwhile others call it Open Source. Amid all this argument, however,\nFree Software geeks recognize that they are all doing the same\nthing: the practice of creating a movement is the practice of talk-\ning about the meaning and necessity of the other four practices.\nIt was in 1998\u201399 that geeks came to recognize that they were all\ndoing the same thing and, almost immediately, to argue about\nwhy.1\nOne way to understand the movement is through the story of\nNetscape and the Mozilla Web browser (now known as Firefox).\nNot only does this story provide some context for the stories of\ngeeks presented in part I\u2014and I move here from direct participant\nobservation to historical and archival research on a phenomenon\nthat was occurring at roughly the same time\u2014but it also contains\nall the elements necessary to understand Free Software. It is full\nof discussion and argument about the practices that make up Free\nSoftware: sharing source code, conceiving of openness, writing li-\ncenses, and coordinating collaborations.\n98 the movement Forking Free Software, 1997\u20132000\nFree Software forked in 1998 when the term Open Source suddenly\nappeared (a term previously used only by the CIA to refer to unclas-\nsified sources of intelligence). The two terms resulted in two sepa-\nrate kinds of narratives: the first, regarding Free Software, stretched\nback into the 1980s, promoting software freedom and resistance to\nproprietary software \u201choarding,\u201d as Richard Stallman, the head of\nthe Free Software Foundation, refers to it; the second, regarding\nOpen Source, was associated with the dotcom boom and the evan-\ngelism of the libertarian pro-business hacker Eric Raymond, who\nfocused on the economic value and cost savings that Open Source\nSoftware represented, including the pragmatic (and polymathic)\napproach that governed the everyday use of Free Software in some\nof the largest online start-ups (Amazon, Yahoo!, HotWired, and oth-\ners all \u201cpromoted\u201d Free Software by using it to run their shops).\nA critical point in the emergence of Free Software occurred in\n1998\u201399: new names, new narratives, but also new wealth and new\nstakes. \u201cOpen Source\u201d was premised on dotcom promises of cost-\ncutting and \u201cdisintermediation\u201d and various other schemes to make\nmoney on it (Cygnus Solutions, an early Free Software company,\nplayfully tagged itself as \u201cMaking Free Software More Affordable\u201d).\nVA Linux, for instance, which sold personal-computer systems pre-\ninstalled with Open Source operating systems, had the largest single\ninitial public offering (IPO) of the stock-market bubble, seeing a\n700 percent share-price increase in one day. \u201cFree Software\u201d by\ncontrast fanned kindling flames of worry over intellectual-property\nexpansionism and hitched itself to a nascent legal resistance to the\n1998 Digital Millennium Copyright Act and Sonny Bono Copyright\nTerm Extension Act. Prior to 1998, Free Software referred either to\nthe Free Software Foundation (and the watchful, micromanaging\neye of Stallman) or to one of thousands of different commercial,\navocational, or university-research projects, processes, licenses,\nand ideologies that had a variety of names: sourceware, freeware,\nshareware, open software, public domain software, and so on. The\nterm Open Source, by contrast, sought to encompass them all in one\nmovement.\nThe event that precipitated this attempted semantic coup d\u2019\u00e9tat\nwas the release of the source code for Netscape\u2019s Communicator\nthe movement 99 Web browser. It\u2019s tough to overestimate the importance of Netscape\nto the fortunes of Free Software. Netscape is justly famous for its\n1995 IPO and its decision to offer its core product, Netscape Navi-\ngator, for free (meaning a compiled, binary version could be down-\nloaded and installed \u201cfor zero dollars\u201d). But Netscape is far more\nfamous among geeks for giving away something else, in 1998: the\nsource code to Netscape Communicator (n\u00e9e Navigator). Giving\naway the Navigator application endeared Netscape to customers\nand confused investors. Giving away the Communicator source code\nin 1998 endeared Netscape to geeks and confused investors; it was\nignored by customers.\nNetscape is important from a number of perspectives. Business-\npeople and investors knew Netscape as the pet project of the suc-\ncessful businessman Jim Clarke, who had founded the specialty\ncomputer manufacturer, Silicon Graphics Incorporated (SGI). To\ncomputer scientists and engineers, especially in the small university\ntown of Champaign-Urbana, Illinois, Netscape was known as the\nhighest bidder for the WWW team at the National Center for Super-\ncomputing Applications (NCSA) at the University of Illinois. That\nteam\u2014Marc Andreessen, Rob McCool, Eric Bina, Jon Mittelhauser,\nAleks Totic, and Chris Houck\u2014had created Mosaic, the first and\nmost fondly remembered \u201cgraphical browser\u201d for surfing the World\nWide Web. Netscape was thus first known as Mosaic Communica-\ntions Corporation and switched its name only after legal threats\nfrom NCSA and a rival firm, Spyglass. Among geeks, Netscape was\nknown as home to a number of Free Software hackers and advo-\ncates, most notably Jamie Zawinski, who had rather flamboyantly\nbroken rank with the Free Software Foundation by forking the\nGNU EMACS code to create what was first known as Lucid Emacs\nand later as XEmacs. Zawinski would go on to lead the newly free\nNetscape browser project, now known as Mozilla.\nMeanwhile, most regular computer users remember Netscape\nboth as an emblem of the dotcom boom\u2019s venture-fed insanity and\nas yet another of Microsoft\u2019s victims. Although Netscape exploded\nonto the scene in 1995, offering a feature-rich browser that was\nan alternative to the bare-bones Mosaic browser, it soon began\nto lose ground to Microsoft, which relatively quickly adopted the\nstrategy of giving away its browser, Internet Explorer, as if it were\npart of the Windows operating system; this was a practice that the\nU.S. Department of Justice eventually found to be in violation of\n100 the movement antitrust laws and for which Microsoft was convicted, but never\npunished.\nThe nature of Netscape\u2019s decision to release the source code dif-\nfers based on which perspective it is seen from. It could appear to\nbe a business plan modeled on the original success: give away your\nproduct and make money in the stock market. It could appear to be\na strategic, last-gasp effort to outcompete Microsoft. It could also\nappear, and did appear to many geeks, to be an attempt to regain\nsome of that \u201chacker-cred\u201d it once had acquired by poaching the\nNCSA team, or even to be an attempt to \u201cdo the right thing\u201d by mak-\ning one of the world\u2019s most useful tools into Free Software. But why\nwould Netscape reach such a conclusion? By what reasoning would\nsuch a decision seem to be correct? The reasons for Netscape\u2019s deci-\nsion to \u201cfree the source\u201d recapitulate the five core practices of Free\nSoftware\u2014and provided key momentum for the new movement.\nSharing Source Code Netscape\u2019s decision to share its source code\ncould only seem surprising in the context of the widespread prac-\ntice of keeping source code secret; secrecy was a practice followed\nlargely in order to prevent competitors from copying a program\nand competing with it, but also as a means to control the market it-\nself. The World Wide Web that Andreessen\u2019s team at NCSA had cut\ntheir teeth on was itself designed to be \u201cplatform independent\u201d and\naccessible by any device on the network. In practice, however, this\nmeant that someone needed to create \u201cbrowsers\u201d for each different\ncomputer or device. Mosaic was initially created for UNIX, using\nthe Motif library of the X11 Window System\u2014in short, a very spe-\ncific kind of access. Netscape, by contrast, prided itself on \u201cporting\u201d\nNetscape Navigator to nearly all available computer architectures.\nIndeed, by 1997, plans were under way to create a version of the\nbrowser\u2014written in Java, the programming language created by\nSun Microsystems to \u201cwrite once, run anywhere\u201d\u2014that would be\ncompletely platform independent.\nThe Java-based Navigator (called Javagator, of course) created\na problem, however, with respect to the practice of keeping source\ncode secret. Whenever a program in Java was run, it created a set of\n\u201cbytecodes\u201d that were easy to reverse-engineer because they had to\nbe transmitted from the server to the machine that ran the program\nand were thus visible to anyone who might know how and where\nto look. Netscape engineers flirted with the idea of deliberately\nthe movement 101 obfuscating these bytecodes to deter competitors from copying them.\nHow can one compete, the logic goes, if anyone can copy your pro-\ngram and make their own ersatz version?\nZawinski, among others, suggested that this was a bad idea: why\nnot just share the source code and get people to help make it better?\nAs a longtime participant in Free Software, Zawinski understood\nthe potential benefits of receiving help from a huge pool of poten-\ntial contributors. He urged his peers at Netscape to see the light.\nHowever, although he told them stories and showed them successes,\nhe could never make the case that this was an intelligent business\nplan, only that it was an efficient software-engineering plan. From\nthe perspective of management and investors, such a move seemed\ntantamount to simply giving away the intellectual property of the\ncompany itself.\nFrank Hecker, a sales manager, made the link between the de-\nvelopers and management: \u201cIt was obvious to [developers] why it\nwas important. It wasn\u2019t really clear from a senior management\nlevel why releasing the source code could be of use because nobody\never made the business case.\u201d2 Hecker penned a document called\n\u201cNetscape Source Code as Netscape Product\u201d and circulated it to\nvarious people, including Andreessen and Netscape CEO Jim Barks-\ndale. As the title suggests, the business case was that the source\ncode could also be a product, and in the context of Netscape, whose\nbusiness model was \u201cgive it away and make it up on the stock mar-\nket,\u201d such a proposal seemed less insane than it otherwise might\nhave: \u201cWhen Netscape first made Navigator available for unre-\nstricted download over the Internet, many saw this as flying in the\nface of conventional wisdom for the commercial software business,\nand questioned how we could possibly make money \u2018giving our\nsoftware away.\u2019 Now of course this strategy is seen in retrospect as\na successful innovation that was a key factor in Netscape\u2019s rapid\ngrowth, and rare is the software company today that does not emu-\nlate our strategy in one way or another. Among other things, this\nprovokes the following question: What if we were to repeat this\nscenario, only this time with source code?\u201d3\nUnder the influence of Hecker, Zawinski, and CTO Eric Hahn\n(who had also written various internal \u201cheresy documents\u201d suggest-\ning similar approaches), Netscape eventually made the decision to\nshare their source code with the outside world, a decision that re-\nsulted in a famous January 1998 press release describing the aims\n102 the movement and benefits of doing so. The decision, at that particular point in\nNetscape\u2019s life, and in the midst of the dotcom boom, was certainly\nmomentous, but it did not lead either to a financial windfall or to\na suddenly superior product.4\nConceptualizing Open Systems Releasing the source code was, in\na way, an attempt to regain the trust of the people who had first\nimagined the www. Tim Berners-Lee, the initial architect of the\nwww, was always adamant that the protocol and all its imple-\nmentations should be freely available (meaning either \u201cin the pub-\nlic domain\u201d or \u201creleased as Free Software\u201d). Indeed, Berners-Lee\nhad done just that with his first bare-bones implementations of the\nwww, proudly declaring them to be in the public domain.\nOver the course of the 1990s, the \u201cbrowser wars\u201d caused both\nNetscape and Microsoft to stray far from this vision: each had im-\nplemented its own extensions and \u201cfeatures\u201d to the browsers and\nservers, extensions not present in the protocol that Berners-Lee\nhad created or in the subsequent standards created by the World\nWide Web Consortium (W3C). Included in the implementations\nwere various kinds of \u201cevil\u201d that could make browsers fail to work\non certain operating systems or with certain kinds of servers. The\n\u201cbrowser wars\u201d repeated an open-systems battle from the 1980s,\none in which the attempt to standardize a network operating sys-\ntem (UNIX) was stymied by competition and secrecy, at the same\ntime that consortiums devoted to \u201copenness\u201d were forming in order\nto try to prevent the spread of evil. Despite the fact that both Micro-\nsoft and Netscape were members of the W3C, the noncompatibility\nof their browsers clearly represented the manipulation of the stan-\ndards process in the name of competitive advantage.\nReleasing the source code for Communicator was thus widely\nseen as perhaps the only way to bypass the poisoned well of com-\npetitively tangled, nonstandard browser implementations. An Open\nSource browser could be made to comply with the standards\u2014if\nnot by the immediate members involved with its creation, then by\ncreating a \u201cfork\u201d of the program that was standards compliant\u2014\nbecause of the rights of redistribution associated with an Open\nSource license. Open Source would be the solution to an open-\nsystems problem that had never been solved because it had never\nconfronted the issue of intellectual property directly. Free Software,\nby contrast, had a well-developed solution in the GNU General\nthe movement 103 Public License, also known as copyleft license, that would allow\nthe software to remain free and revive hope for maintaining open\nstandards.\nWriting Licenses Herein lies the rub, however: Netscape was im-\nmediately embroiled in controversy among Free Software hackers\nbecause it chose to write its own bespoke licenses for distributing\nthe source code. Rather than rely on one of the existing licenses,\nsuch as the GNU GPL or the Berkeley Systems Distribution (BSD) or\nMIT licenses, they created their own: the Netscape Public License\n(NPL) and the Mozilla Public License. The immediate concerns of\nNetscape had to do with their existing network of contracts and\nagreements with other, third-party developers\u2014both those who\nhad in the past contributed parts of the existing source code that\nNetscape might not have the rights to redistribute as Free Software,\nand those who were expecting in the future to buy and redistribute\na commercial version. Existing Free Software licenses were either\ntoo permissive, giving to third parties rights that Netscape itself\nmight not have, or too restrictive, binding Netscape to make source\ncode freely available (the GPL) when it had already signed con-\ntracts with buyers of the nonfree code.\nIt was a complex and specific business situation\u2014a network of\nexisting contracts and licensed code\u2014that created the need for\nNetscape to write its own license. The NPL thus contained a clause\nthat allowed Netscape special permission to relicense any particu-\nlar contribution to the source code as a proprietary product in order\nto appease its third-party contracts; it essentially gave Netscape\nspecial rights that no other licensee would have. While this did not\nnecessarily undermine the Free Software licenses\u2014and it was cer-\ntainly Netscape\u2019s prerogative\u2014it was contrary to the spirit of Free\nSoftware: it broke the \u201crecursive public\u201d into two halves. In order\nto appease Free Software geeks, Netscape wrote one license for ex-\nisting code (the NPL) and a different license for new contributions:\nthe Mozilla Public License.\nNeither Stallman nor any other Free Software hacker was entirely\nhappy with this situation. Stallman pointed out three flaws: \u201cOne\nflaw sends a bad philosophical message, another puts the free soft-\nware community in a weak position, while the third creates a major\npractical problem within the free software community. Two of the\nflaws apply to the Mozilla Public License as well.\u201d He urged people\n104 the movement not to use the NPL. Similarly, Bruce Perens suggested, \u201cMany com-\npanies have adopted a variation of the MPL [sic] for their own\nprograms. This is unfortunate, because the NPL was designed for\nthe specific business situation that Netscape was in at the time it\nwas written, and is not necessarily appropriate for others to use.\nIt should remain the license of Netscape and Mozilla, and others\nshould use the GPL or the BSD or X licenses.\u201d5\nArguments about the fine details of licenses may seem scholastic,\nbut the decision had a huge impact on the structure of the new\nproduct. As Steven Weber has pointed out, the choice of license\ntracks the organization of a product and can determine who and\nwhat kinds of contributions can be made to a project.6 It is not an\nidle choice; every new license is scrutinized with the same intensity\nor denounced with the same urgency.\nCoordinating Collaborations One of the selling points of Free Soft-\nware, and especially of its marketing as Open Source, is that it\nleverages the work of thousands or hundreds of thousands of volun-\nteer contributors across the Internet. Such a claim almost inevitably\nleads to spurious talk of \u201cself-organizing\u201d systems and emergent\nproperties of distributed collaboration. The Netscape press release\npromised to \u201charness the creative power of thousands of program-\nmers on the Internet by incorporating their best enhancements,\u201d\nand it quoted CEO Jim Barksdale as saying, \u201cBy giving away the\nsource code for future versions, we can ignite the creative ener-\ngies of the entire Net community and fuel unprecedented levels of\ninnovation in the browser market.\u201d7 But as anyone who has ever\ntried to start or run a Free Software project knows, it never works\nout that way.\nSoftware engineering is a notoriously hard problem.8 The halls\nof the software industry are lined with the warning corpses of dead\nsoftware methodologies. Developing software in the dotcom boom\nwas no different, except that the speed of release cycles and the\nvelocity of funding (the \u201cburn rate\u201d) was faster than ever before.\nNetscape\u2019s in-house development methodologies were designed to\nmeet these pressures, and as many who work in this field will attest,\nthat method is some version of a semistructured, deadline-driven,\ncaffeine- and smart-drink\u2013fueled race to \u201cship.\u201d9\nReleasing the Mozilla code, therefore, required a system of co-\nordination that would differ from the normal practice of in-house\nthe movement 105 software development by paid programmers. It needed to incorpo-\nrate the contributions of outsiders\u2014developers who didn\u2019t work for\nNetscape. It also needed to entice people to contribute, since that\nwas the bargain on which the decision to free the source was based,\nand to allow them to track their contributions, so they could verify\nthat their contributions were included or rejected for legitimate\nreasons. In short, if any magical Open Source self-organization\nwere to take place, it would require a thoroughly transparent,\nInternet-based coordination system.\nAt the outset, this meant practical things: obtaining the domain\nname mozilla.org; setting up (and in turn releasing the source code\nfor) the version-control system (the Free Software standard cvs), the\nversion-control interface (Bonsai), the \u201cbuild system\u201d that managed\nand displayed the various trees and (broken) branches of a complex\nsoftware project (Tinderbox), and a bug-reporting system for track-\ning bugs submitted by users and developers (Bugzilla). It required\nan organizational system within the Mozilla project, in which paid\ndevelopers would be assigned to check submissions from inside and\noutside, and maintainers or editors would be designated to look at\nand verify that these contributions should be used.\nIn the end, the release of the Mozilla source code was both a\nsuccess and a failure. Its success was long in coming: by 2004,\nthe Firefox Web browser, based on Mozilla, had started to creep\nup the charts of most popular browsers, and it has become one of\nthe most visible and widely used Free Software applications. The\nfailure, however, was more immediate: Mozilla failed to reap the\nmassive benefits for Netscape that the 1995 give-away of Netscape\nNavigator had. Zawinski, in a public letter of resignation in April\n1999 (one year after the release), expressed this sense of failure.\nHe attributed Netscape\u2019s decline after 1996 to the fact that it had\n\u201cstopped innovating\u201d and become too large to be creative, and de-\nscribed the decision to free the Mozilla source code as a return to\nthis innovation: \u201c[The announcement] was a beacon of hope to me\n. . . . [I]t was so crazy, it just might work. I took my cue and ran with\nit, registering the domain that night, designing the structure of the\norganization, writing the first version of the web site, and, along\nwith my co-conspirators, explaining to room after room of Netscape\nemployees and managers how free software worked, and what we\nhad to do to make it work.\u201d10 For Zawinski, the decision was both\na chance for Netscape to return to its glory and an opportunity\n106 the movement to prove the power of Free Software: \u201cI saw it as a chance for the\ncode to actually prosper. By making it not be a Netscape project,\nbut rather, be a public project to which Netscape was merely a\ncontributor, the fact that Netscape was no longer capable of build-\ning products wouldn\u2019t matter: the outsiders would show Netscape\nhow it\u2019s done. By putting control of the web browser into the hands\nof anyone who cared to step up to the task, we would ensure that\nthose people would keep it going, out of their own self-interest.\u201d11\nBut this promise didn\u2019t come true\u2014or, at least, it didn\u2019t come\ntrue at the speed that Zawinski and others in the software world\nwere used to. Zawinski offered various reasons: the project was\nprimarily made up of Netscape employees and thus still appeared\nto be a Netscape thing; it was too large a project for outsiders to\ndive into and make small changes to; the code was too \u201ccrufty,\u201d\nthat is, too complicated, overwritten, and unclean. Perhaps most\nimportant, though, the source code was not actually working: \u201cWe\nnever distributed the source code to a working web browser, more\nimportantly, to the web browser that people were actually using.\u201d12\nNetscape failed to entice. As Zawinski put it, \u201cIf someone were\nrunning a web browser, then stopped, added a simple new com-\nmand to the source, recompiled, and had that same web browser\nplus their addition, they would be motivated to do this again, and\npossibly to tackle even larger projects.\u201d13 For Zawinski, the failure\nto \u201cship\u201d a working browser was the biggest failure, and he took\npains to suggest that this failure was not an indictment of Free\nSoftware as such: \u201cLet me assure you that whatever problems the\nMozilla project is having are not because open source doesn\u2019t work.\nOpen source does work, but it is most definitely not a panacea.\nIf there\u2019s a cautionary tale here, it is that you can\u2019t take a dying\nproject, sprinkle it with the magic pixie dust of \u2018open source,\u2019 and\nhave everything magically work out. Software is hard. The issues\naren\u2019t that simple.\u201d14\nFomenting Movements The period from 1 April 1998, when the\nMozilla source code was first released, to 1 April 1999, when Za-\nwinski announced its failure, couldn\u2019t have been a headier, more\nexciting time for participants in Free Software. Netscape\u2019s decision\nto release the source code was a tremendous opportunity for geeks\ninvolved in Free Software. It came in the midst of the rollicking dot-\ncom bubble. It also came in the midst of the widespread adoption of\nthe movement 107 key Free Software tools: the Linux operating system for servers, the\nApache Web server for Web pages, the perl and python scripting\nlanguages for building quick Internet applications, and a number\nof other lower-level tools like Bind (an implementation of the DNS\nprotocol) or sendmail for e-mail.\nPerhaps most important, Netscape\u2019s decision came in a period\nof fevered and intense self-reflection among people who had been\ninvolved in Free Software in some way, stretching back to the mid-\n1980s. Eric Raymond\u2019s article \u201cThe Cathedral and The Bazaar,\u201d\ndelivered at the Linux Kongress in 1997 and the O\u2019Reilly Perl Con-\nference the same year, had started a buzz among Free Software\nhackers. It was cited by Frank Hecker and Eric Hahn at Netscape\nas one of the sources for their thinking about the decision to free\nMozilla; Raymond and Bruce Perens had both been asked to consult\nwith Netscape on Free Software strategy. In April of the same year\nTim O\u2019Reilly, a publisher of handbooks for Free Software, orga-\nnized a conference called the Freeware Summit.\nThe Freeware Summit\u2019s very name indicated some of the con-\ncern about definition and direction. Stallman, despite his obvious\ncentrality, but also because of it, was not invited to the Freeware\nSummit, and the Free Software Foundation was not held up as the\ncore philosophical guide of this event. Rather, according to the\npress release distributed after the meeting, \u201cThe meeting\u2019s purpose\nwas to facilitate a high-level discussion of the successes and chal-\nlenges facing the developers. While this type of software has often\nbeen called \u2018freeware\u2019 or \u2018free software\u2019 in the past, the developers\nagreed that commercial development of the software is part of the\npicture, and that the terms \u2018open source\u2019 or \u2018sourceware\u2019 best de-\nscribe the development method they support.\u201d15\nIt was at this summit that Raymond\u2019s suggestion of \u201cOpen Source\u201d\nas an alternative name was first publicly debated.16 Shortly there-\nafter, Raymond and Perens created the Open Source Initiative and\npenned \u201cThe Open Source Definition.\u201d All of this self-reflection was\nintended to capitalize on the waves of attention being directed at\nFree Software in the wake of Netscape\u2019s announcement.\nThe motivations for these changes came from a variety of\nsources\u2014ranging from a desire to be included in the dotcom boom\nto a powerful (ideological) resistance to being ideological. Linus\nTorvalds loudly proclaimed that the reason to do Free Software\nwas because it was \u201cfun\u201d; others insisted that it made better busi-\n108 the movement ness sense or that the stability of infrastructures like the Internet\ndepended on a robust ability to improve them from any direction.\nBut none of them questioned how Free Software got done or pro-\nposed to change it.\nRaymond\u2019s paper \u201cThe Cathedral and the Bazaar\u201d quickly be-\ncame the most widely told story of how Open Source works and\nwhy it is important; it emphasizes the centrality of novel forms of\ncoordination over the role of novel copyright licenses or practices\nof sharing source code. \u201cThe Cathedral and the Bazaar\u201d reports\nRaymond\u2019s experiments with Free Software (the bazaar model) and\nreflects on the difference between it and methodologies adopted\nby industry (the cathedral model). The paper does not truck with\ntalk of freedom and has no denunciations of software hoarding\n\u00e0 la Stallman. Significantly, it also has no discussion of issues of\nlicensing. Being a hacker, however, Raymond did give his paper a\n\u201crevision-history,\u201d which proudly displays revision 1.29, 9 Febru-\nary 1998: \u201cChanged \u2018free software\u2019 to \u2018open source.\u2019 \u201d17\nRaymond was determined to reject the philosophy of liberty that\nStallman and the Free Software Foundation represented, but not in\norder to create a political movement of his own. Rather, Raymond\n(and the others at the Freeware Summit) sought to cash in on the\nrising tide of the Internet economy by turning the creation of Free\nSoftware into something that made more sense to investors, venture\ncapitalists, and the stock-buying public. To Raymond, Stallman and\nthe Free Software Foundation represented not freedom or liberty,\nbut a kind of dogmatic, impossible communism. As Raymond was\na committed libertarian, one might expect his core beliefs in the\nnecessity of strong property rights to conflict with the strange com-\nmunalism of Free Software\u2014and, indeed, his rhetoric was focused\non pragmatic, business-minded, profit-driven, and market-oriented\nuses of Free Software. For Raymond, the essentially interesting\ncomponent of Free Software was not its enhancement of human\nliberty, but the innovation in software production that it repre-\nsented (the \u201cdevelopment model\u201d). It was clear that Free Software\nachieved something amazing through a clever inversion of strong\nproperty rights, an inversion which could be expected to bring mas-\nsive revenue in some other form, either through cost-cutting or,\nNetscape-style, through the stock market.\nRaymond wanted the business world and the mainstream indus-\ntry to recognize Free Software\u2019s potential, but he felt that Stallman\u2019s\nthe movement 109 rhetoric was getting in the way. Stallman\u2019s insistence, for example,\non calling corporate intellectual-property protection of software\n\u201choarding\u201d was doing more damage than good in terms of Free\nSoftware\u2019s acceptance among businesses, as a practice, if not ex-\nactly a product.\nRaymond\u2019s papers channeled the frustration of an entire genera-\ntion of Free Software hackers who may or may not have shared\nStallman\u2019s dogmatic philosophical stance, but who nonetheless\nwanted to participate in the creation of Free Software. Raymond\u2019s\npaper, the Netscape announcement, and the Freeware Summit all\nplayed into a palpable anxiety: that in the midst of the single larg-\nest creation of paper wealth in U.S. history, those being enriched\nthrough Free Software and the Internet were not those who built it,\nwho maintained it, or who got it.\nThe Internet giveaway was a conflict of propriety: hackers and\ngeeks who had built the software that made it work, under the sign\nof making it free for all, were seeing that software generate un-\ntold wealth for people who had not built it (and furthermore, who\nhad no intention of keeping it free for all). Underlying the creation\nof wealth was a commitment to a kind of permanent technical\nfreedom\u2014a moral order\u2014not shared by those who were reaping\nthe most profit. This anxiety regarding the expropriation of work\n(even if it had been a labor of love) was ramified by Netscape\u2019s\nannouncement.\nAll through 1998 and 1999, buzz around Open Source built. Little-\nknown companies such as Red Hat, VA Linux, Cygnus, Slackware,\nand SuSe, which had been providing Free Software support and\nservices to customers, suddenly entered media and business con-\nsciousness. Articles in the mainstream press circulated throughout\nthe spring and summer of 1998, often attempting to make sense of\nthe name change and whether it meant a corresponding change in\npractice. A front-cover article in Forbes, which featured photos of\nStallman, Larry Wall, Brian Behlendorf, and Torvalds (figure 2),\nwas noncommittal, cycling between Free Software, Open Source,\nand Freeware.18\nBy early 1999, O\u2019Reilly Press published Open Sources: Voices\nfrom the Open Source Revolution, a hastily written but widely read\nbook. It included a number of articles\u2014this time including one by\nStallman\u2014that cobbled together the first widely available public\nhistory of Free Software, both the practice and the technologies\n110 the movement 2. \u201cPeace, Love and Software,\u201d cover of Forbes, 10 August 1998. Used\nwith permission of Forbes and Nathaniel Welch.\ninvolved. Kirk McKusick\u2019s article detailed the history of important\ntechnologies like the BSD version of UNIX, while an article by Brian\nBehlendorf, of Apache, detailed the practical challenges of running\nFree Software projects. Raymond provided a history of hackers and\na self-aggrandizing article about his own importance in creating\nthe movement, while Stallman\u2019s contribution told his own version\nof the rise of Free Software.\nBy December 1999, the buzz had reached a fever pitch. When\nVA Linux, a legitimate company which actually made something\nreal\u2014computers with Linux installed on them\u2014went public, its\nshares\u2019 value gained 700 percent in one day and was the single\nthe movement 111 most valuable initial public offering of the era. VA Linux took the\nunconventional step of allowing contributors to the Linux kernel to\nbuy into the stock before the IPO, thus bringing at least a partial set\nof these contributors into the mainstream Ponzi scheme of the Inter-\nnet dotcom economy. Those who managed to sell their stock ended\nup benefiting from the boom, whether or not their contributions\nto Free Software truly merited it. In a roundabout way, Raymond,\nO\u2019Reilly, Perens, and others behind the name change had achieved\nrecognition for the central role of Free Software in the success of the\nInternet\u2014and now its true name could be known: Open Source.\nYet nothing much changed in terms of the way things actually\ngot done. Sharing source code, conceiving openness, writing li-\ncenses, coordinating projects\u2014all these continued as before with\nno significant differences between those flashing the heroic mantle\nof freedom and those donning the pragmatic tunic of methodology.\nNow, however, stories proliferated; definitions, distinctions, details,\nand detractions filled the ether of the Internet, ranging from the\nphilosophical commitments of Free Software to the parables of sci-\nence as the \u201coriginal open source\u201d software. Free Software propo-\nnents refined their message concerning rights, while Open Source\nadvocates refined their claims of political agnosticism or nonideo-\nlogical commitments to \u201cfun.\u201d All these stories served to create\nmovements, to evangelize and advocate and, as Eugen Leitl would\nsay, to \u201ccorrupt young minds\u201d and convert them to the cause. The\nfact that there are different narratives for identical practices is an\nadvantageous fact: regardless of why people think they are doing\nwhat they are doing, they are all nonetheless contributing to the\nsame mysterious thing.\nA Movement?\nTo most onlookers, Free Software and Open Source seem to be over-\nwhelmed with frenzied argument; the flame wars and disputes, on-\nline and off, seem to dominate everything. To attend a conference\nwhere geeks\u2014especially high-profile geeks like Raymond, Stallman,\nand Torvalds\u2014are present, one might suspect that the very de-\ntailed practices of Free Software are overseen by the brow-beating,\nhistrionic antics of a few charismatic leaders and that ideological\ncommitments result in divergent, incompatible, and affect-laden\n112 the movement opposition which must of necessity take specific and incompatible\nforms. Strangely, this is far from the case: all this sound and fury\ndoesn\u2019t much change what people do, even if it is a requirement of\napprenticeship. It truly is all over but for the shouting.\nAccording to most of the scholarly literature, the function of a\nmovement is to narrate the shared goals and to recruit new mem-\nbers. But is this what happens in Free Software or Open Source?19\nTo begin with, movement is an awkward word; not all participants\nwould define their participation this way. Richard Stallman sug-\ngests that Free Software is social movement, while Open Source is a\ndevelopment methodology. Similarly some Open Source proponents\nsee it as a pragmatic methodology and Free Software as a dogmatic\nphilosophy. While there are specific entities like the Free Software\nFoundation and the Open Source Initiative, they do not comprise\nall Free Software or Open Source. Free Software and Open Source\nare neither corporations nor organizations nor consortia (for there\nare no organizations to consort); they are neither national, sub-\nnational, nor international; they are not \u201ccollectives\u201d because no\nmembership is required or assumed\u2014indeed to hear someone as-\nsert \u201cI belong\u201d to Free Software or Open Source would sound ab-\nsurd to anyone who does. Neither are they shady bands of hackers,\ncrackers, or thieves meeting in the dead of night, which is to say\nthat they are not an \u201cinformal\u201d organization, because there is no\nformal equivalent to mimic or annul. Nor are they quite a crowd,\nfor a crowd can attract participants who have no idea what the\ngoal of the crowd is; also, crowds are temporary, while movements\nextend over time. It may be that movement is the best term of the\nlot, but unlike social movements, whose organization and momen-\ntum are fueled by shared causes or broken by ideological dispute,\nFree Software and Open Source share practices first, and ideologies\nsecond. It is this fact that is the strongest confirmation that they\nare a recursive public, a form of public that is as concerned with\nthe material practical means of becoming public as it is with any\ngiven public debate.\nThe movement, as a practice of argument and discussion, is thus\ncentered around core agreements about the other four kinds of\npractices. The discussion and argument have a specific function:\nto tie together divergent practices according to a wide consensus\nwhich tries to capture the why of Free Software. Why is it differ-\nent from normal software development? Why is it necessary? Why\nthe movement 113 now? Why do people do it? Why do people use it? Can it be pre-\nserved and enhanced? None of these questions address the how:\nhow should source code circulate? How should a license be written?\nWho should be in charge? All of these \u201chows\u201d change slowly and\nexperimentally through the careful modulation of the practices,\nbut the \u201cwhys\u201d are turbulent and often distracting. Nonetheless,\npeople engaged in Free Software\u2014users, developers, supporters,\nand observers\u2014could hardly remain silent on this point, despite the\nfrequent demand to just \u201cshut up and show me the code.\u201d \u201cFiguring\nout\u201d Free Software also requires a practice of reflecting on what is\ncentral to it and what is outside of it.\nThe movement, as a practice of discussion and argument, is made\nup of stories. It is a practice of storytelling: affect- and intellect-laden\nlore that orients existing participants toward a particular problem,\ncontests other histories, parries attacks from outside, and draws\nin new recruits.20 This includes proselytism and evangelism (and\nthe usable pasts of protestant reformations, singularities, rebellion\nand iconoclasm are often salient here), whether for the reform of\nintellectual-property law or for the adoption of Linux in the trenches\nof corporate America. It includes both heartfelt allegiance in the\nname of social justice as well as political agnosticism stripped of\nall ideology.21 Every time Free Software is introduced to someone,\ndiscussed in the media, analyzed in a scholarly work, or installed\nin a workplace, a story of either Free Software or Open Source is\nused to explain its purpose, its momentum, and its temporality. At\nthe extremes are the prophets and proselytes themselves: Eric Ray-\nmond describes Open Source as an evolutionarily necessary out-\ncome of the natural tendency of human societies toward economies\nof abundance, while Richard Stallman describes it as a defense of\nthe fundamental freedoms of creativity and speech, using a variety\nof philosophical theories of liberty, justice, and the defense of free-\ndom.22 Even scholarly analyses must begin with a potted history\ndrawn from the self-narration of geeks who make or advocate free\nsoftware.23 Indeed, as a methodological aside, one reason it is so easy\nto track such stories and narratives is because geeks like to tell and,\nmore important, like to archive such stories\u2014to create Web pages,\ndefinitions, encyclopedia entries, dictionaries, and mini-histories\nand to save every scrap of correspondence, every fight, and every\nresolution related to their activities. This \u201carchival hubris\u201d yields\na very peculiar and specific kind of fieldsite: one in which a kind\n114 the movement of \u201cas-it-happens\u201d ethnographic observation is possible not only\nthrough \u201cbeing there\u201d in the moment but also by being there in the\nmassive, proliferating archives of moments past. Understanding the\nmovement as a changing entity requires constantly glancing back\nat its future promises and the conditions of their making.\nStories of the movement are also stories of a recursive public. The\nfact that movement isn\u2019t quite the right word is evidence of a kind\nof grasping, a figuring out of why these practices make sense to\nall these geeks, in this place and time; it is a practice that is not so\ndifferent from my own ethnographic engagement with it. Note that\nboth Free Software and Open Source tell stories of movement(s):\nthey are not divided by a commercial-noncommercial line, even\nif they are divided by ill-defined and hazy notions of their ulti-\nmate goals. The problem of a recursive public (or, in an alternate\nlanguage, a recursive market) as a social imaginary of moral and\ntechnical order is common to both of them as part of their prac-\ntices. Thus, stories about \u201cthe movement\u201d are detailed stories about\nthe technical and moral order that geeks inhabit, and they are\nbound up with the functions and fates of the Internet. Often these\nstories are themselves practices of inclusion and exclusion (e.g.,\n\u201cthis license is not a Free Software license\u201d or \u201cthat software is not\nan open system\u201d); sometimes the stories are normative definitions\nabout how Free Software should look. But they are, always, stories\nthat reveal the shared moral and technical imaginations that make\nup Free Software as a recursive public.\nConclusion\nBefore 1998, there was no movement. There was the Free Soft-\nware Foundation, with its peculiar goals, and a very wide array\nof other projects, people, software, and ideas. Then, all of a sud-\nden, in the heat of the dotcom boom, Free Software was a move-\nment. Suddenly, it was a problem, a danger, a job, a calling, a\ndogma, a solution, a philosophy, a liberation, a methodology, a\nbusiness plan, a success, and an alternative. Suddenly, it was Open\nSource or Free Software, and it became necessary to choose sides.\nAfter 1998, debates about definition exploded; denunciations and\nmanifestos and journalistic hagiography proliferated. Ironically,\nthe creation of two names allowed people to identify one thing, for\nthe movement 115 these two names referred to identical practices, licenses, tools, and\norganizations. Free Software and Open Source shared everything\n\u201cmaterial,\u201d but differed vocally and at great length with respect\nto ideology. Stallman was denounced as a kook, a communist, an\nidealist, and a dogmatic holding back the successful adoption of\nOpen Source by business; Raymond and users of \u201copen source\u201d were\ncharged with selling out the ideals of freedom and autonomy, with\nthe dilution of the principles and the promise of Free Software, as\nwell as with being stooges of capitalist domination. Meanwhile,\nboth groups proceeded to create objects\u2014principally software\u2014\nusing tools that they agreed on, concepts of openness that they\nagreed on, licenses that they agreed on, and organizational schemes\nthat they agreed on. Yet never was there fiercer debate about the\ndefinition of Free Software.\nOn the one hand, the Free Software Foundation privileges the lib-\nerty and creativity of individual geeks, geeks engaged in practices\nof self-fashioning through the creation of software. It gives prece-\ndence to the liberal claim that without freedom of expression, in-\ndividuals are robbed of their ability to self-determine. On the other\nhand, Open Source privileges organizations and processes, that is,\ngeeks who are engaged in building businesses, nonprofit organiza-\ntions, or governmental and public organizations of some form or\nanother. It gives precedence to the pragmatist (or polymathic) view\nthat getting things done requires flexible principles and negotia-\ntion, and that the public practice of building and running things\nshould be separate from the private practice of ethical and politi-\ncal beliefs. Both narratives give geeks ways of making sense of a\npractice that they share in almost all of its details; both narratives\ngive geeks a way to understand how Free Software or Open Source\nSoftware is different from the mainstream, proprietary software\ndevelopment that dominates their horizons. The narratives turn\nthe haphazard participation and sharing that existed before 1998\ninto meaningful, goal-directed practices in the present, turning a\nclass-in-itself into a class-for-itself, to use a terminology for the most\npart unwelcome among geeks.\nIf two radically opposed ideologies can support people engaged\nin identical practices, then it seems obvious that the real space of\npolitics and contestation is at the level of these practices and their\nemergence. These practices emerge as a response to a reorientation\nof power and knowledge, a reorientation somewhat impervious to\n116 the movement conventional narratives of freedom and liberty, or to pragmatic\nclaims of methodological necessity or market-driven innovation.\nWere these conventional narratives sufficient, the practices would\nbe merely bureaucratic affairs, rather than the radical transforma-\ntions they are.\nthe movement 117 4.\nSharing Source Code\nFree Software would be nothing without shared source code. The\nidea is built into the very name \u201cOpen Source,\u201d and it is a require-\nment of all Free Software licenses that source code be open to view,\nnot \u201cwelded shut.\u201d Perhaps ironically, source code is the most ma-\nterial of the five components of Free Software; it is both an ex-\npressive medium, like writing or speech, and a tool that performs\nconcrete actions. It is a mnemonic that translates between the illeg-\nible electron-speed doings of our machines and our lingering ability\nto partially understand and control them as human agents. Many\nFree Software programmers and advocates suggest that \u201cinforma-\ntion wants to be free\u201d and that sharing is a natural condition of\nhuman life, but I argue something contrary: sharing produces its\nown kind of moral and technical order, that is, \u201cinformation makes\npeople want freedom\u201d and how they want it is related to how that\ninformation is created and circulated. In this chapter I explore the twisted and contingent history of how source code and its sharing\nhave come to take the technical, legal, and pedagogical forms they\nhave today, and how the norms of sharing have come to seem so\nnatural to geeks.\nSource code is essential to Free Software because of the histori-\ncally specific ways in which it has come to be shared, \u201cported,\u201d\nand \u201cforked.\u201d Nothing about the nature of source code requires\nthat it be shared, either by corporations for whom secrecy and jeal-\nous protection are the norm or by academics and geeks for whom\nsource code is usually only one expression, or implementation, of\na greater idea worth sharing. However, in the last thirty years,\nnorms of sharing source code\u2014technical, legal, and pedagogical\nnorms\u2014have developed into a seemingly natural practice. They\nemerged through attempts to make software into a product, such\nas IBM\u2019s 1968 \u201cunbundling\u201d of software and hardware, through\nattempts to define and control it legally through trade secret, copy-\nright, and patent law, and through attempts to teach engineers how\nto understand and to create more software.\nThe story of the norms of sharing source code is, not by accident,\nalso the history of the UNIX operating system.1 The UNIX operating\nsystem is a monstrous academic-corporate hybrid, an experiment\nin portability and sharing whose impact is widely and reverently\nacknowledged by geeks, but underappreciated more generally. The\nstory of UNIX demonstrates the details of how source code has come\nto be shared, technically, legally, and pedagogically. In technical\nterms UNIX and the programming language C in which it was writ-\nten demonstrated several key ideas in operating-systems theory and\npractice, and they led to the widespread \u201cporting\u201d of UNIX to virtu-\nally every kind of hardware available in the 1970s, all around the\nworld. In legal terms UNIX\u2019s owner, AT&T, licensed it widely and\nliberally, in both binary and source-code form; the legal definition\nof UNIX as a product, however, was not the same as the technical\ndefinition of UNIX as an evolving experiment in portable operating\nsystems\u2014a tension that has continued throughout its lifetime. In\npedagogical terms UNIX became the very paradigm of an \u201coperating\nsystem\u201d and was thereby ported not only in the technical sense from\none machine to another, but from machines to minds, as computer-\nscience students learning the meaning of \u201coperating system\u201d studied\nthe details of the quasi-legally shared UNIX source code.2\nsharing source code 119 The proliferation of UNIX was also a hybrid commercial-academic\nundertaking: it was neither a \u201cpublic domain\u201d object shared solely\namong academics, nor was it a conventional commercial product.\nProliferation occurred through novel forms of academic sharing as\nwell as through licensing schemes constrained by the peculiar sta-\ntus of AT&T, a regulated monopoly forbidden to enter the computer\nand software industry before 1984. Thus proliferation was not mere\nreplication: it was not the sale of copies of UNIX, but a complex\nweb of shared and re-shared chunks of source code, and the re-\nimplementation of an elegant and simple conceptual scheme. As\nUNIX proliferated, it was stabilized in multiple ways: by academics\nseeking to keep it whole and self-compatible through contributions\nof source code; by lawyers at AT&T seeking to define boundaries\nthat mapped onto laws, licenses, versions, and regulations; and by\nprofessors seeking to define it as an exemplar of the core concepts\nof operating-system theory. In all these ways, UNIX was a kind\nof primal recursive public, drawing together people for whom the\nmeaning of their affiliation was the use, modification, and stabili-\nzation of UNIX.\nThe obverse of proliferation is differentiation: forking. UNIX is\nadmired for its integrity as a conceptual thing and despised (or\nmarveled at) for its truly tangled genealogical tree of ports and\nforks: new versions of UNIX, some based directly on the source\ncode, some not, some licensed directly from AT&T, some sublicensed\nor completely independent.\nForking, like random mutation, has had both good and bad ef-\nfects; on the one hand, it ultimately created versions of UNIX that\nwere not compatible with themselves (a kind of autoimmune re-\nsponse), but it also allowed the merger of UNIX and the Arpanet,\ncreating a situation wherein UNIX operating systems came to be\nnot only the paradigm of operating systems but also the paradigm\nof networked computers, through its intersection with the develop-\nment of the TCP\/IP protocols that are at the core of the Internet.3\nBy the mid-1980s, UNIX was a kind of obligatory passage point for\nanyone interested in networking, operating systems, the Internet,\nand especially, modes of creating, sharing, and modifying source\ncode\u2014so much so that UNIX has become known among geeks not\njust as an operating system but as a philosophy, an answer to a\nvery old question in new garb: how shall we live, among a new\nworld of machines, software, and networks?\n120 sharing source code Before Source\nIn the early days of computing machinery, there was no such thing\nas source code. Alan Turing purportedly liked to talk to the machine\nin binary. Grace Hopper, who invented an early compiler, worked\nas close to the Harvard Mark I as she could get: flipping switches\nand plugging and unplugging relays that made up the \u201ccode\u201d of\nwhat the machine would do. Such mechanical and meticulous work\nhardly merits the terms reading and writing; there were no GOTO\nstatements, no line numbers, only calculations that had to be trans-\nlated from the pseudo-mathematical writing of engineers and hu-\nman computers to a physical or mechanical configuration.4 Writing\nand reading source code and programming languages was a long,\nslow development that became relatively widespread only by the\nmid-1970s. So-called higher-level languages began to appear in the\nlate 1950s: FORTRAN, COBOL, Algol, and the \u201ccompilers\u201d which\nallowed for programs written in them to be transformed into the\nillegible mechanical and valvular representations of the machine.\nIt was in this era that the terms source language and target language\nemerged to designate the activity of translating higher to lower\nlevel languages.5\nThere is a certain irony about the computer, not often noted:\nthe unrivaled power of the computer, if the ubiquitous claims are\nbelieved, rests on its general programmability; it can be made to\ndo any calculation, in principle. The so-called universal Turing\nmachine provides the mathematical proof.6 Despite the abstract\npower of such certainty, however, we do not live in the world of\nThe Computer\u2014we live in a world of computers. The hardware sys-\ntems that manufacturers created from the 1950s onward were so\nspecific and idiosyncratic that it was inconceivable that one might\nwrite a program for one machine and then simply run it on another.\n\u201cProgramming\u201d became a bespoke practice, tailored to each new\nmachine, and while programmers of a particular machine may well\nhave shared programs with each other, they would not have seen\nmuch point in sharing with users of a different machine. Likewise,\ncomputer scientists shared mathematical descriptions of algorithms\nand ideas for automation with as much enthusiasm as corpora-\ntions jealously guarded theirs, but this sharing, or secrecy, did not\nextend to the sharing of the program itself. The need to \u201crewrite\u201d\na program for each machine was not just a historical accident, but\nsharing source code 121 was determined by the needs of designers and engineers and the\nvicissitudes of the market for such expensive machines.7\nIn the good old days of computers-the-size-of-rooms, the lan-\nguages that humans used to program computers were mnemonics;\nthey did not exist in the computer, but on a piece of paper or a\nspecially designed code sheet. The code sheet gave humans who\nwere not Alan Turing a way to keep track of, to share with other\nhumans, and to think systematically about the invisible light-speed\ncalculations of a complicated device. Such mnemonics needed to\nbe \u201ccoded\u201d on punch cards or tape; if engineers conferred, they\nconferred over sheets of paper that matched up with wires, relays,\nand switches\u2014or, later, printouts of the various machine-specific\ncodes that represented program and data.\nWith the introduction of programming languages, the distinction\nbetween a \u201csource\u201d language and a \u201ctarget\u201d language entered the\npractice: source languages were \u201ctranslated\u201d into the illegible tar-\nget language of the machine. Such higher-level source languages\nwere still mnemonics of sorts\u2014they were certainly easier for hu-\nmans to read and write, mostly on yellowing tablets of paper or\nspecial code sheets\u2014but they were also structured enough that a\nsource language could be input into a computer and translated into\na target language which the designers of the hardware had speci-\nfied. Inputting commands and cards and source code required a\nseries of actions specific to each machine: a particular card reader\nor, later, a keypunch with a particular \u201ceditor\u201d for entering the\ncommands. Properly input and translated source code provided the\nmachine with an assembled binary program that would, in fact,\nrun (calculate, operate, control). It was a separation, an abstrac-\ntion that allowed for a certain division of labor between the in-\ngenious human authors and the fast and mechanical translating\nmachines.\nEven after the invention of programming languages, program-\nming \u201con\u201d a computer\u2014sitting at a glowing screen and hacking\nthrough the night\u2014was still a long time in coming. For example,\nonly by about 1969 was it possible to sit at a keyboard, write source\ncode, instruct the computer to compile it, then run the program\u2014all\nwithout leaving the keyboard\u2014an activity that was all but unimag-\ninable in the early days of \u201cbatch processing.\u201d8 Very few program-\nmers worked in such a fashion before the mid-1970s, when text\neditors that allowed programmers to see the text on a screen rather\n122 sharing source code than on a piece of paper started to proliferate.9 We are, by now, so\nfamiliar with the image of the man or woman sitting at a screen\ninteracting with this device that it is nearly impossible to imagine\nhow such a seemingly obvious practice was achieved in the first\nplace\u2014through the slow accumulation of the tools and techniques\nfor working on a new kind of writing\u2014and how that practice ex-\nploded into a Babel of languages and machines that betrayed the\npromise of the general-purpose computing machine.\nThe proliferation of different machines with different architec-\ntures drove a desire, among academics especially, for the standard-\nization of programming languages, not so much because any single\nlanguage was better than another, but because it seemed necessary\nto most engineers and computer users to share an emerging cor-\npus of algorithms, solutions, and techniques of all kinds, necessary\nto avoid reinventing the wheel with each new machine. Algol, a\nstreamlined language suited to algorithmic and algebraic represen-\ntations, emerged in the early 1960s as a candidate for international\nstandardization. Other languages competed on different strengths:\nFORTRAN and COBOL for general business use; LISP for symbolic\nprocessing. At the same time, the desire for a standard \u201chigher-level\u201d\nlanguage necessitated a bestiary of translating programs: compil-\ners, parsers, lexical analyzers, and other tools designed to transform\nthe higher-level (human-readable) language into a machine-specific\nlower-level language, that is, machine language, assembly language,\nand ultimately the mystical zeroes and ones that course through our\nmachines. The idea of a standard language and the necessity of de-\nvising specific tools for translation are the origin of the problem of\nportability: the ability to move software\u2014not just good ideas, but\nactual programs, written in a standard language\u2014from one machine\nto another.\nA standard source language was seen as a way to counteract the\nproliferation of different machines with subtly different architec-\ntures. Portable source code would allow programmers to imagine\ntheir programs as ships, stopping in at ports of call, docking on dif-\nferent platforms, but remaining essentially mobile and unchanged\nby these port-calls. Portable source code became the Esperanto of\nhumans who had wrought their own Babel of tribal hardware ma-\nchines.\nMeanwhile, for the computer industry in the 1960s, portable\nsource code was largely a moot point. Software and hardware were\nsharing source code 123 two sides of single, extremely expensive coin\u2014no one, except engi-\nneers, cared what language the code was in, so long as it performed\nthe task at hand for the customer. Each new machine needed to\nbe different, faster, and, at first, bigger, and then smaller, than\nthe last. The urge to differentiate machines from each other was\nnot driven by academic experiment or aesthetic purity, but by a\ndemand for marketability, competitive advantage, and the trans-\nformation of machines and software into products. Each machine\nhad to do something really well, and it needed to be developed in\nsecret, in order to beat out the designs and innovations of competi-\ntors. In the 1950s and 1960s the software was a core component of\nthis marketable object; it was not something that in itself was dif-\nferentiated or separately distributed\u2014until IBM\u2019s famous decision\nin 1968 to \u201cunbundle\u201d software and hardware.\nBefore the 1970s, employees of a computer corporation wrote\nsoftware in-house. The machine was the product, and the software\nwas just an extra line-item on the invoice. IBM was not the first\nto conceive of software as an independent product with its own\nmarket, however. Two companies, Informatics and Applied Data\nResearch, had explored the possibilities of a separate market in\nsoftware.10 Informatics, in particular, developed the first commer-\ncially successful software product, a business-management system\ncalled Mark IV, which in 1967 cost $30,000. Informatics\u2019 president\nWalter Bauer \u201clater recalled that potential buyers were \u2018astounded\u2019\nby the price of Mark IV. In a world accustomed to free software the\nprice of $30,000 was indeed high.\u201d11\nIBM\u2019s unbundling decision marked a watershed, the point at\nwhich \u201cportable\u201d source code became a conceivable idea, if not a\npractical reality, to many in the industry.12 Rather than providing\na complete package of hardware and software, IBM decided to dif-\nferentiate its products: to sell software and hardware separately to\nconsumers.13 But portability was not simply a technical issue; it was\na political-economic one as well. IBM\u2019s decision was driven both by\nits desire to create IBM software that ran on all IBM machines (a\ncentral goal of the famous OS\/360 project overseen and diagnosed\nby Frederick Brooks) and as response to an antitrust suit filed by the\nU.S. Department of Justice.14 The antitrust suit included as part of\nits claims the suggestion that the close tying of software and hard-\nware represented a form of monopolistic behavior, and it prompted\nIBM to consider strategies to \u201cunbundle\u201d its product.\n124 sharing source code Portability in the business world meant something specific,\nhowever. Even if software could be made portable at a technical\nlevel\u2014transferable between two different IBM machines\u2014this was\ncertainly no guarantee that it would be portable between custom-\ners. One company\u2019s accounting program, for example, may not suit\nanother\u2019s practices. Portability was therefore hindered both by the\ndiversity of machine architectures and by the diversity of business\npractices and organization. IBM and other manufacturers therefore\nsaw no benefit to standardizing source code, as it could only pro-\nvide an advantage to competitors.15\nPortability was thus not simply a technical problem\u2014the problem\nof running one program on multiple architectures\u2014but also a kind\nof political-economic problem. The meaning of product was not al-\nways the same as the meaning of hardware or software, but was usu-\nally some combination of the two. At that early stage, the outlines\nof a contest over the meaning of portable or shareable source code\nare visible, both in the technical challenges of creating high-level\nlanguages and in the political-economic challenges that corpora-\ntions faced in creating distinctive proprietary products.\nThe UNIX Time-Sharing System\nSet against this backdrop, the invention, success, and proliferation\nof the UNIX operating system seems quite monstrous, an aberration\nof both academic and commercial practice that should have failed\nin both realms, instead of becoming the most widely used portable\noperating system in history and the very paradigm of an \u201coperating\nsystem\u201d in general. The story of UNIX demonstrates how portability\nbecame a reality and how the particular practice of sharing UNIX\nsource code became a kind of de facto standard in its wake.\nUNIX was first written in 1969 by Ken Thompson and Dennis\nRitchie at Bell Telephone Labs in Murray Hill, New Jersey. UNIX\nwas the d\u00e9nouement of the MIT project Multics, which Bell Labs\nhad funded in part and to which Ken Thompson had been assigned.\nMultics was one of the earliest complete time-sharing operating sys-\ntems, a demonstration platform for a number of early innovations\nin time-sharing (multiple simultaneous users on one computer).16 By\n1968, Bell Labs had pulled its support\u2014including Ken Thompson\u2014\nfrom the project and placed him back in Murray Hill, where he and\nsharing source code 125 Dennis Ritchie were stuck without a machine, without any money,\nand without a project. They were specialists in operating systems,\nlanguages, and machine architecture in a research group that had\nno funding or mandate to pursue these areas. Through the creative\nuse of some discarded equipment, and in relative isolation from the\nrest of the lab, Thompson and Ritchie created, in the space of about\ntwo years, a complete operating system, a programming language\ncalled C, and a host of tools that are still in extremely wide use\ntoday. The name UNIX (briefly, UNICS) was, among other things,\na puerile pun: a castrated Multics.\nThe absence of an economic or corporate mandate for Thomp-\nson\u2019s and Ritchie\u2019s creativity and labor was not unusual for Bell\nLabs; researchers were free to work on just about anything, so long\nas it possessed some kind of vague relation to the interests of AT&T.\nHowever, the lack of funding for a more powerful machine did\nrestrict the kind of work Thompson and Ritchie could accomplish.\nIn particular, it influenced the design of the system, which was ori-\nented toward a super-slim control unit (a kernel) that governed the\nbasic operation of the machine and an expandable suite of small,\nindependent tools, each of which did one thing well and which\ncould be strung together to accomplish more complex and powerful\ntasks.17 With the help of Joseph Ossana, Douglas McIlroy, and oth-\ners, Thompson and Ritchie eventually managed to agitate for a new\nPDP-11\/20 based not on the technical merits of the UNIX operating\nsystem itself, but on its potential applications, in particular, those\nof the text-preparation group, who were interested in developing\ntools for formatting, typesetting, and printing, primarily for the\npurpose of creating patent applications, which was, for Bell Labs,\nand for AT&T more generally, obviously a laudable goal.18\nUNIX was unique for many technical reasons, but also for a spe-\ncific economic reason: it was never quite academic and never quite\ncommercial. Martin Campbell-Kelly notes that UNIX was a \u201cnon-\nproprietary operating system of major significance.\u201d19 Kelly\u2019s use\nof \u201cnon-proprietary\u201d is not surprising, but it is incorrect. Although\nbusiness-speak regularly opposed open to proprietary throughout the\n1980s and early 1990s (and UNIX was definitely the former), Kelly\u2019s\nslip marks clearly the confusion between software ownership and\nsoftware distribution that permeates both popular and academic\nunderstandings. UNIX was indeed proprietary\u2014it was copyrighted\nand wholly owned by Bell Labs and in turn by Western Electric\n126 sharing source code and AT&T\u2014but it was not exactly commercialized or marketed by\nthem. Instead, AT&T allowed individuals and corporations to in-\nstall UNIX and to create UNIX-like derivatives for very low licensing\nfees. Until about 1982, UNIX was licensed to academics very widely\nfor a very small sum: usually royalty-free with a minimal service\ncharge (from about $150 to $800).20 The conditions of this license\nallowed researchers to do what they liked with the software so long\nas they kept it secret: they could not distribute or use it outside of\ntheir university labs (or use it to create any commercial product\nor process), nor publish any part of it. As a result, throughout the\n1970s UNIX was developed both by Thompson and Ritchie inside\nBell Labs and by users around the world in a relatively informal\nmanner. Bell Labs followed such a liberal policy both because it\nwas one of a small handful of industry-academic research and de-\nvelopment centers and because AT&T was a government monopoly\nthat provided phone service to the country and was therefore for-\nbidden to directly enter the computer software market.21\nBeing on the border of business and academia meant that UNIX\nwas, on the one hand, shielded from the demands of management\nand markets, allowing it to achieve the conceptual integrity that\nmade it so appealing to designers and academics. On the other,\nit also meant that AT&T treated it as a potential product in the\nemerging software industry, which included new legal questions\nfrom a changing intellectual-property regime, novel forms of mar-\nketing and distribution, and new methods of developing, support-\ning, and distributing software.\nDespite this borderline status, UNIX was a phenomenal success.\nThe reasons why UNIX was so popular are manifold; it was widely\nadmired aesthetically, for its size, and for its clever design and\ntools. But the fact that it spread so widely and quickly is testament\nalso to the existing community of eager computer scientists and en-\ngineers (and a few amateurs) onto which it was bootstrapped, users\nfor whom a powerful, flexible, low-cost, modifiable, and fast oper-\nating system was a revelation of sorts. It was an obvious alternative\nto the complex, poorly documented, buggy operating systems that\nroutinely shipped standard with the machines that universities and\nresearch organizations purchased. \u201cIt worked,\u201d in other words.\nA key feature of the popularity of UNIX was the inclusion of the\nsource code. When Bell Labs licensed UNIX, they usually provided\na tape that contained the documentation (i.e., documentation that\nsharing source code 127 was part of the system, not a paper technical manual external to\nit), a binary version of the software, and the source code for the\nsoftware. The practice of distributing the source code encouraged\npeople to maintain it, extend it, document it\u2014and to contribute\nthose changes to Thompson and Ritchie as well. By doing so, users\ndeveloped an interest in maintaining and supporting the project\nprecisely because it gave them an opportunity and the tools to use\ntheir computer creatively and flexibly. Such a globally distributed\ncommunity of users organized primarily by their interest in main-\ntaining an operating system is a precursor to the recursive public,\nalbeit confined to the world of computer scientists and researchers\nwith access to still relatively expensive machines. As such, UNIX\nwas not only a widely shared piece of quasi-commercial software\n(i.e., distributed in some form other than through a price-based re-\ntail market), but also the first to systematically include the source\ncode as part of that distribution as well, thus appealing more to\nacademics and engineers.22\nThroughout the 1970s, the low licensing fees, the inclusion of\nthe source code, and its conceptual integrity meant that UNIX was\nported to a remarkable number of other machines. In many ways,\nacademics found it just as appealing, if not more, to be involved in\nthe creation and improvement of a cutting-edge system by licens-\ning and porting the software themselves, rather than by having it\nprovided to them, without the source code, by a company. Peter\nSalus, for instance, suggests that people experienced the lack of\nsupport from Bell Labs as a kind of spur to develop and share their\nown fixes. The means by which source code was shared, and the\nnorms and practices of sharing, porting, forking, and modifying\nsource code were developed in this period as part of the develop-\nment of UNIX itself\u2014the technical design of the system facilitates\nand in some cases mirrors the norms and practices of sharing that\ndeveloped: operating systems and social systems.23\nSharing UNIX\nOver the course of 1974\u201377 the spread and porting of UNIX was\nphenomenal for an operating system that had no formal system of\ndistribution and no official support from the company that owned\nit, and that evolved in a piecemeal way through the contributions\n128 sharing source code of people from around the world. By 1975, a user\u2019s group had de-\nveloped: USENIX.24 UNIX had spread to Canada, Europe, Australia,\nand Japan, and a number of new tools and applications were being\nboth independently circulated and, significantly, included in the\nfrequent releases by Bell Labs itself. All during this time, AT&T\u2019s li-\ncensing department sought to find a balance between allowing this\ncirculation and innovation to continue, and attempting to maintain\ntrade-secret status for the software. UNIX was, by 1980, without a\ndoubt the most widely and deeply understood trade secret in com-\nputing history.\nThe manner in which the circulation of and contribution to UNIX\noccurred is not well documented, but it includes both technical and\npedagogical forms of sharing. On the technical side, distribution\ntook a number of forms, both in resistance to AT&T\u2019s attempts to\ncontrol it and facilitated by its unusually liberal licensing of the\nsoftware. On the pedagogical side, UNIX quickly became a para-\ndigmatic object for computer-science students precisely because it\nwas a working operating system that included the source code and\nthat was simple enough to explore in a semester or two.\nIn A Quarter Century of UNIX Salus provides a couple of key sto-\nries (from Ken Thompson and Lou Katz) about how exactly the\ntechnical sharing of UNIX worked, how sharing, porting, and fork-\ning can be distinguished, and how it was neither strictly legal nor\ndeliberately illegal in this context. First, from Ken Thompson: \u201cThe\nfirst thing to realize is that the outside world ran on releases of\nUNIX (V4, V5, V6, V7) but we did not. Our view was a continuum.\nV5 was what we had at some point in time and was probably out\nof date simply by the activity required to put it in shape to export.\nAfter V6, I was preparing to go to Berkeley to teach for a year. I\nwas putting together a system to take. Since it was almost a release,\nI made a diff with V6 [a tape containing only the differences be-\ntween the last release and the one Ken was taking with him]. On\nthe way to Berkeley I stopped by Urbana-Champaign to keep an eye\non Greg Chesson. . . . I left the diff tape there and I told him that I\nwouldn\u2019t mind if it got around.\u201d25\nThe need for a magnetic tape to \u201cget around\u201d marks the differ-\nence between the 1970s and the present: the distribution of soft-\nware involved both the material transport of media and the digital\ncopying of information. The desire to distribute bug fixes (the \u201cdiff \u201d\ntape) resonates with the future emergence of Free Software: the\nsharing source code 129 fact that others had fixed problems and contributed them back to\nThompson and Ritchie produced an obligation to see that the fixes\nwere shared as widely as possible, so that they in turn might be\nported to new machines. Bell Labs, on the other hand, would have\nseen this through the lens of software development, requiring a new\nrelease, contract renegotiation, and a new license fee for a new ver-\nsion. Thompson\u2019s notion of a \u201ccontinuum,\u201d rather than a series of\nreleases also marks the difference between the idea of an evolving\ncommon set of objects stewarded by multiple people in far-flung\nlocales and the idea of a shrink-wrapped \u201cproductized\u201d software\npackage that was gaining ascendance as an economic commodity\nat the same time. When Thompson says \u201cthe outside world,\u201d he is\nreferring not only to people outside of Bell Labs but to the way the\nworld was seen from within Bell Labs by the lawyers and marketers\nwho would create a new version. For the lawyers, the circulation of\nsource code was a problem because it needed to be stabilized, not\nso much for commercial reasons as for legal ones\u2014one license for\none piece of software. Distributing updates, fixes, and especially\nnew tools and additions written by people who were not employed\nby Bell Labs scrambled the legal clarity even while it strengthened\nthe technical quality. Lou Katz makes this explicit.\nA large number of bug fixes was collected, and rather than issue them\none at a time, a collection tape (\u201cthe 50 fixes\u201d) was put together by\nKen [the same \u201cdiff tape,\u201d presumably]. Some of the fixes were quite\nimportant, though I don\u2019t remember any in particular. I suspect that a\nsignificant fraction of the fixes were actually done by non-Bell people. Ken\ntried to send it out, but the lawyers kept stalling and stalling and stall-\ning. Finally, in complete disgust, someone \u201cfound a tape on Mountain\nAvenue\u201d [the location of Bell Labs] which had the fixes. When the\nlawyers found out about it, they called every licensee and threatened\nthem with dire consequences if they didn\u2019t destroy the tape, after try-\ning to find out how they got the tape. I would guess that no one would\nactually tell them how they came by the tape (I didn\u2019t).26\nDistributing the fixes involved not just a power struggle between\nthe engineers and management, but was in fact clearly motivated\nby the fact that, as Katz says, \u201ca significant fraction of the fixes\nwere done by non-Bell people.\u201d This meant two things: first, that\nthere was an obvious incentive to return the updated system to these\n130 sharing source code people and to others; second, that it was not obvious that AT&T\nactually owned or could claim rights over these fixes\u2014or, if they\ndid, they needed to cover their legal tracks, which perhaps in part\nexplains the stalling and threatening of the lawyers, who may have\nbeen buying time to make a \u201clegal\u201d version, with the proper per-\nmissions.\nThe struggle should be seen not as one between the rebel forces\nof UNIX development and the evil empire of lawyers and manag-\ners, but as a struggle between two modes of stabilizing the object\nknown as UNIX. For the lawyers, stability implied finding ways to\nmake UNIX look like a product that would meet the existing legal\nframework and the peculiar demands of being a regulated monop-\noly unable to freely compete with other computer manufacturers;\nthe ownership of bits and pieces, ideas and contributions had to be\nstrictly accountable. For the programmers, stability came through\nredistributing the most up-to-date operating system and sharing\nall innovations with all users so that new innovations might also\nbe portable. The lawyers saw urgency in making UNIX legally sta-\nble; the engineers saw urgency in making UNIX technically stable\nand compatible with itself, that is, to prevent the forking of UNIX,\nthe death knell for portability. The tension between achieving\nlegal stability of the object and promoting its technical portability\nand stability is one that has repeated throughout the life of UNIX\nand its derivatives\u2014and that has ramifications in other areas as\nwell.\nThe identity and boundaries of UNIX were thus intricately formed\nthrough its sharing and distribution. Sharing produced its own form\nof moral and technical order. Troubling questions emerged imme-\ndiately: were the versions that had been fixed, extended, and ex-\npanded still UNIX, and hence still under the control of AT&T? Or\nwere the differences great enough that something else (not-UNIX)\nwas emerging? If a tape full of fixes, contributed by non-Bell em-\nployees, was circulated to people who had licensed UNIX, and those\nfixes changed the system, was it still UNIX? Was it still UNIX in a\nlegal sense or in a technical sense or both? While these questions\nmight seem relatively scholastic, the history of the development\nof UNIX suggests something far more interesting: just about every\npossible modification has been made, legally and technically, but\nthe concept of UNIX has remained remarkably stable.\nsharing source code 131 Porting UNIX\nTechnical portability accounts for only part of UNIX\u2019s success. As\na pedagogical resource, UNIX quickly became an indispensable\ntool for academics around the world. As it was installed and im-\nproved, it was taught and learned. The fact that UNIX spread first\nto university computer-science departments, and not to businesses,\ngovernment, or nongovernmental organizations, meant that it also\nbecame part of the core pedagogical practice of a generation of\nprogrammers and computer scientists; over the course of the 1970s\nand 1980s, UNIX came to exemplify the very concept of an operat-\ning system, especially time-shared, multi-user operating systems.\nTwo stories describe the porting of UNIX from machines to minds\nand illustrate the practice as it developed and how it intersected\nwith the technical and legal attempts to stabilize UNIX as an ob-\nject: the story of John Lions\u2019s Commentary on Unix 6th Edition and\nthe story of Andrew Tanenbaum\u2019s Minix.\nThe development of a pedagogical UNIX lent a new stability to\nthe concept of UNIX as opposed to its stability as a body of source\ncode or as a legal entity. The porting of UNIX was so successful that\neven in cases where a ported version of UNIX shares none of the same\nsource code as the original, it is still considered UNIX. The monstrous\nand promiscuous nature of UNIX is most clear in the stories of Lions\nand Tanenbaum, especially when contrasted with the commercial,\nlegal, and technical integrity of something like Microsoft Windows,\nwhich generally exists in only a small number of forms (NT, ME,\nXP, 95, 98, etc.), possessing carefully controlled source code, im-\nmured in legal protection, and distributed only through sales and\nservice packs to customers or personal-computer manufacturers.\nWhile Windows is much more widely used than UNIX, it is far from\nhaving become a paradigmatic pedagogical object; its integrity is\npredominantly legal, not technical or pedagogical. Or, in pedagogi-\ncal terms, Windows is to fish as UNIX is to fishing lessons.\nLions\u2019s Commentary is also known as \u201cthe most photocopied doc-\nument in computer science.\u201d Lions was a researcher and senior\nlecturer at the University of New South Wales in the early 1970s;\nafter reading the first paper by Ritchie and Thompson on UNIX, he\nconvinced his colleagues to purchase a license from AT&T.27 Lions,\nlike many researchers, was impressed by the quality of the system,\nand he was, like all of the UNIX users of that period, intimately\n132 sharing source code familiar with the UNIX source code\u2014a necessity in order to install,\nrun, or repair it. Lions began using the system to teach his classes\non operating systems, and in the course of doing so he produced\na textbook of sorts, which consisted of the entire source code of\nUNIX version 6 (V6), along with elaborate, line-by-line commen-\ntary and explanation. The value of this textbook can hardly be\nunderestimated. Access to machines and software that could be\nused to understand how a real system worked was very limited:\n\u201cReal computers with real operating systems were locked up in\nmachine rooms and committed to processing twenty four hours a\nday. UNIX changed that.\u201d28 Berny Goodheart, in an appreciation of\nLions\u2019s Commentary, reiterated this sense of the practical usefulness\nof the source code and commentary: \u201cIt is important to understand\nthe significance of John\u2019s work at that time: for students study-\ning computer science in the 1970s, complex issues such as process\nscheduling, security, synchronization, file systems and other con-\ncepts were beyond normal comprehension and were extremely diffi-\ncult to teach\u2014there simply wasn\u2019t anything available with enough\naccessibility for students to use as a case study. Instead a student\u2019s\ndiscipline in computer science was earned by punching holes in\ncards, collecting fan-fold paper printouts, and so on. Basically, a\ncomputer operating system in that era was considered to be a huge\nchunk of inaccessible proprietary code.\u201d29\nLions\u2019s commentary was a unique document in the world of com-\nputer science, containing a kind of key to learning about a central\ncomponent of the computer, one that very few people would have\nhad access to in the 1970s. It shows how UNIX was ported not only\nto machines (which were scarce) but also to the minds of young\nresearchers and student programmers (which were plentiful). Sev-\neral generations of both academic computer scientists and students\nwho went on to work for computer or software corporations were\ntrained on photocopies of UNIX source code, with a whiff of toner\nand illicit circulation: a distributed operating system in the textual\nsense.\nUnfortunately, Commentary was also legally restricted in its dis-\ntribution. AT&T and Western Electric, in hopes that they could\nmaintain trade-secret status for UNIX, allowed only very limited\ncirculation of the book. At first, Lions was given permission to dis-\ntribute single copies only to people who already possessed a license\nfor UNIX V6; later Bell Labs itself would distribute Commentary\nsharing source code 133 briefly, but only to licensed users, and not for sale, distribution,\nor copying. Nonetheless, nearly everyone seems to have possessed\na dog-eared, nth-generation copy. Peter Reintjes writes, \u201cWe soon\ncame into possession of what looked like a fifth generation photo-\ncopy and someone who shall remain nameless spent all night in\nthe copier room spawning a sixth, an act expressly forbidden by\na carefully worded disclaimer on the first page. Four remarkable\nthings were happening at the same time. One, we had discovered\nthe first piece of software that would inspire rather than annoy\nus; two, we had acquired what amounted to a literary criticism\nof that computer software; three, we were making the single most\nsignificant advancement of our education in computer science by\nactually reading an entire operating system; and four, we were\nbreaking the law.\u201d30\nThus, these generations of computer-science students and aca-\ndemics shared a secret\u2014a trade secret become open secret. Every\nstudent who learned the essentials of the UNIX operating sys-\ntem from a photocopy of Lions\u2019s commentary, also learned about\nAT&T\u2019s attempt to control its legal distribution on the front cover\nof their textbook. The parallel development of photocopying has a\nnice resonance here; together with home cassette taping of music\nand the introduction of the video-cassette recorder, photocopying\nhelped drive the changes to copyright law adopted in 1976.\nThirty years later, and long after the source code in it had been\ncompletely replaced, Lions\u2019s Commentary is still widely admired by\ngeeks. Even though Free Software has come full circle in providing\nstudents with an actual operating system that can be legally stud-\nied, taught, copied, and implemented, the kind of \u201cliterary criticism\u201d\nthat Lions\u2019s work represents is still extremely rare; even reading ob-\nsolete code with clear commentary is one of the few ways to truly\nunderstand the design elements and clever implementations that\nmade the UNIX operating system so different from its predecessors\nand even many of its successors, few, if any of which have been so\nsuccessfully ported to the minds of so many students.\nLions\u2019s Commentary contributed to the creation of a worldwide\ncommunity of people whose connection to each other was formed\nby a body of source code, both in its implemented form and in its\ntextual, photocopied form. This nascent recursive public not only\nunderstood itself as belonging to a technical elite which was consti-\ntuted by its creation, understanding, and promotion of a particular\n134 sharing source code technical tool, but also recognized itself as \u201cbreaking the law,\u201d a\ncommunity constituted in opposition to forms of power that gov-\nerned the circulation, distribution, modification, and creation of\nthe very tools they were learning to make as part of their vocation.\nThe material connection shared around the world by UNIX-loving\ngeeks to their source code is not a mere technical experience, but a\nsocial and legal one as well.\nLions was not the only researcher to recognize that teaching the\nsource code was the swiftest route to comprehension. The other\nstory of the circulation of source code concerns Andrew Tanenbaum,\na well-respected computer scientist and an author of standard text-\nbooks on computer architecture, operating systems, and network-\ning.31 In the 1970s Tanenbaum had also used UNIX as a teaching\ntool in classes at the Vrije Universiteit, in Amsterdam. Because the\nsource code was distributed with the binary code, he could have his\nstudents explore directly the implementations of the system, and he\noften used the source code and the Lions book in his classes. But, ac-\ncording to his Operating Systems: Design and Implementation (1987),\n\u201cWhen AT&T released Version 7 [ca. 1979], it began to realize that\nUNIX was a valuable commercial product, so it issued Version 7\nwith a license that prohibited the source code from being studied in\ncourses, in order to avoid endangering its status as a trade secret.\nMany universities complied by simply dropping the study of UNIX,\nand teaching only theory\u201d (13). For Tanenbaum, this was an unac-\nceptable alternative\u2014but so, apparently, was continuing to break\nthe law by teaching UNIX in his courses. And so he proceeded to\ncreate a completely new UNIX-like operating system that used not\na single line of AT&T source code. He called his creation Minix. It\nwas a stripped-down version intended to run on personal computers\n(IBM PCs), and to be distributed along with the textbook Operating\nSystems, published by Prentice Hall.32\nMinix became as widely used in the 1980s as a teaching tool as\nLions\u2019s source code had been in the 1970s. According to Tanen-\nbaum, the Usenet group comp.os.minix had reached 40,000 mem-\nbers by the late 1980s, and he was receiving constant suggestions\nfor changes and improvements to the operating system. His own\ncommitment to teaching meant that he incorporated few of these\nsuggestions, an effort to keep the system simple enough to be\nprinted in a textbook and understood by undergraduates. Minix\nsharing source code 135 was freely available as source code, and it was a fully function-\ning operating system, even a potential alternative to UNIX that\nwould run on a personal computer. Here was a clear example of the\nconceptual integrity of UNIX being communicated to another gen-\neration of computer-science students: Tanenbaum\u2019s textbook is not\ncalled \u201cUNIX Operating Systems\u201d\u2014it is called Operating Systems.\nThe clear implication is that UNIX represented the clearest example\nof the principles that should guide the creation of any operating\nsystem: it was, for all intents and purposes, state of the art even\ntwenty years after it was first conceived.\nMinix was not commercial software, but nor was it Free Soft-\nware. It was copyrighted and controlled by Tanenbaum\u2019s publisher,\nPrentice Hall. Because it used no AT&T source code, Minix was also\nlegally independent, a legal object of its own. The fact that it was\nintended to be legally distinct from, yet conceptually true to UNIX\nis a clear indication of the kinds of tensions that govern the cre-\nation and sharing of source code. The ironic apotheosis of Minix as\nthe pedagogical gold standard for studying UNIX came in 1991\u201392,\nwhen a young Linus Torvalds created a \u201cfork\u201d of Minix, also rewrit-\nten from scratch, that would go on to become the paradigmatic\npiece of Free Software: Linux. Tanenbaum\u2019s purpose for Minix was\nthat it remain a pedagogically useful operating system\u2014small,\nconcise, and illustrative\u2014whereas Torvalds wanted to extend and\nexpand his version of Minix to take full advantage of the kinds of\nhardware being produced in the 1990s. Both, however, were com-\nmitted to source-code visibility and sharing as the swiftest route to\ncomplete comprehension of operating-systems principles.\nForking UNIX\nTanenbaum\u2019s need to produce Minix was driven by a desire to share\nthe source code of UNIX with students, a desire AT&T was mani-\nfestly uncomfortable with and which threatened the trade-secret\nstatus of their property. The fact that Minix might be called a fork\nof UNIX is a key aspect of the political economy of operating sys-\ntems and social systems. Forking generally refers to the creation of\nnew, modified source code from an original base of source code,\nresulting in two distinct programs with the same parent. Whereas\nthe modification of an engine results only in a modified engine, the\n136 sharing source code modification of source code implies differentiation and reproduc-\ntion, because of the ease with which it can be copied.\nHow could Minix\u2014a complete rewrite\u2014still be considered the\nsame object? Considered solely from the perspective of trade-secret\nlaw, the two objects were distinct, though from the perspective\nof copyright there was perhaps a case for infringement, although\nAT&T did not rely on copyright as much as on trade secret. From\na technical perspective, the functions and processes that the soft-\nware accomplishes are the same, but the means by which they are\ncoded to do so are different. And from a pedagogical standpoint,\nthe two are identical\u2014they exemplify certain core features of an\noperating system (file-system structure, memory paging, process\nmanagement)\u2014all the rest is optimization, or bells and whistles.\nUnderstanding the nature of forking requires also that UNIX be\nunderstood from a social perspective, that is, from the perspective\nof an operating system created and modified by user-developers\naround the world according to particular and partial demands. It\nforms the basis for the emergence of a robust recursive public.\nOne of the more important instances of the forking of UNIX\u2019s\nperambulatory source code and the developing community of UNIX\nco-developers is the story of the Berkeley Software Distribution and\nits incorporation of the TCP\/IP protocols. In 1975 Ken Thompson\ntook a sabbatical in his hometown of Berkeley, California, where\nhe helped members of the computer-science department with their\ninstallations of UNIX, arriving with V6 and the \u201c50 bug fixes\u201d diff\ntape. Ken had begun work on a compiler for the Pascal program-\nming language that would run on UNIX, and this work was taken\nup by two young graduate students: Bill Joy and Chuck Hartley.\n(Joy would later co-found Sun Microsystems, one of the most suc-\ncessful UNIX-based workstation companies in the history of the\nindustry.)\nJoy, above nearly all others, enthusiastically participated in the\ninformal distribution of source code. With a popular and well-built\nPascal system, and a new text editor called ex (later vi), he created\nthe Berkeley Software Distribution (BSD), a set of tools that could\nbe used in combination with the UNIX operating system. They were\nextensions to the original UNIX operating system, but not a com-\nplete, rewritten version that might replace it. By all accounts, Joy\nserved as a kind of one-man software-distribution house, making\ntapes and posting them, taking orders and cashing checks\u2014all in\nsharing source code 137 addition to creating software.33 UNIX users around the world soon\nlearned of this valuable set of extensions to the system, and be-\nfore long, many were differentiating between AT&T UNIX and BSD\nUNIX.\nAccording to Don Libes, Bell Labs allowed Berkeley to distribute its\nextensions to UNIX so long as the recipients also had a license from\nBell Labs for the original UNIX (an arrangement similar to the one\nthat governed Lions\u2019s Commentary).34 From about 1976 until about\n1981, BSD slowly became an independent distribution\u2014indeed, a\ncomplete version of UNIX\u2014well-known for the vi editor and the\nPascal compiler, but also for the addition of virtual memory and its\nimplementation on DEC\u2019s VAX machines.35 It should be clear that\nthe unusual quasi-commercial status of AT&T\u2019s UNIX allowed for\nthis situation in a way that a fully commercial computer corpora-\ntion would never have allowed. Consider, for instance, the fact that\nmany UNIX users\u2014students at a university, for instance\u2014could\nnot essentially know whether they were using an AT&T product\nor something called BSD UNIX created at Berkeley. The operating\nsystem functioned in the same way and, except for the presence of\ncopyright notices that occasionally flashed on the screen, did not\nmake any show of asserting its brand identity (that would come\nlater, in the 1980s). Whereas a commercial computer manufacturer\nwould have allowed something like BSD only if it were incorpo-\nrated into and distributed as a single, marketable, and identifiable\nproduct with a clever name, AT&T turned something of a blind eye\nto the proliferation and spread of AT&T UNIX and the result were\nforks in the project: distinct bodies of source code, each an instance\nof something called UNIX.\nAs BSD developed, it gained different kinds of functionality than\nthe UNIX from which it was spawned. The most significant develop-\nment was the inclusion of code that allowed it to connect computers\nto the Arpanet, using the TCP\/IP protocols designed by Vinton Cerf\nand Robert Kahn. The TCP\/IP protocols were a key feature of the\nArpanet, overseen by the Information Processing and Techniques\nOffice (IPTO) of the Defense Advanced Research Projects Agency\n(DARPA) from its inception in 1967 until about 1977. The goal of\nthe protocols was to allow different networks, each with its own\nmachines and administrative boundaries, to be connected to each\nother.36 Although there is a common heritage\u2014in the form of J. C. R.\nLicklider\u2014which ties the imagination of the time-sharing operat-\n138 sharing source code ing system to the creation of the \u201cgalactic network,\u201d the Arpanet\ninitially developed completely independent of UNIX.37 As a time-\nsharing operating system, UNIX was meant to allow the sharing of\nresources on a single computer, whether mainframe or minicom-\nputer, but it was not initially intended to be connected to a network\nof other computers running UNIX, as is the case today.38 The goal\nof Arpanet, by contrast, was explicitly to achieve the sharing of\nresources located on diverse machines across diverse networks.\nTo achieve the benefits of TCP\/IP, the resources needed to be\nimplemented in all of the different operating systems that were con-\nnected to the Arpanet\u2014whatever operating system and machine\nhappened to be in use at each of the nodes. However, by 1977, the\noriginal machines used on the network were outdated and increas-\ningly difficult to maintain and, according to Kirk McKusick, the\ngreatest expense was that of porting the old protocol software to\nnew machines. Hence, IPTO decided to pursue in part a strategy\nof achieving coordination at the operating-system level, and they\nchose UNIX as one of the core platforms on which to standardize.\nIn short, they had seen the light of portability. In about 1978 IPTO\ngranted a contract to Bolt, Beranek, and Newman (BBN), one of the\noriginal Arpanet contractors, to integrate the TCP\/IP protocols into\nthe UNIX operating system.\nBut then something odd happened, according to Salus: \u201cAn initial\nprototype was done by BBN and given to Berkeley. Bill [Joy] im-\nmediately started hacking on it because it would only run an Eth-\nernet at about 56K\/sec utilizing 100% of the CPU on a 750. . . . Bill\nlobotomized the code and increased its performance to on the order\nof 700KB\/sec. This caused some consternation with BBN when they\ncame in with their \u2018finished\u2019 version, and Bill wouldn\u2019t accept it.\nThere were battles for years after, about which version would be in\nthe system. The Berkeley version ultimately won.\u201d39\nAlthough it is not clear, it appears BBN intended to give Joy\nthe code in order to include it in his BSD version of UNIX for dis-\ntribution, and that Joy and collaborators intended to cooperate\nwith Rob Gurwitz of BBN on a final implementation, but Berkeley\ninsisted on \u201cimproving\u201d the code to make it perform more to their\nneeds, and BBN apparently dissented from this.40 One result of this\nscuffle between BSD and BBN was a genuine fork: two bodies of\ncode that did the same thing, competing with each other to become\nthe standard UNIX implementation of TCP\/IP. Here, then, was a\nsharing source code 139 case of sharing source code that led to the creation of different ver-\nsions of software\u2014sharing without collaboration. Some sites used\nthe BBN code, some used the Berkeley code.\nForking, however, does not imply permanent divergence, and the\ncontinual improvement, porting, and sharing of software can have\nodd consequences when forks occur. On the one hand, there are par-\nticular pieces of source code: they must be identifiable and exact, and\nprepended with a copyright notice, as was the case of the Berkeley\ncode, which was famously and vigorously policed by the University\nof California regents, who allowed for a very liberal distribution of\nBSD code on the condition that the copyright notice was retained.\nOn the other hand, there are particular named collections of code\nthat work together (e.g., UNIX\u2122, or DARPA-approved UNIX, or\nlater, Certified Open Source [sm]) and are often identified by a\ntrademark symbol intended, legally speaking, to differentiate prod-\nucts, not to assert ownership of particular instances of a product.\nThe odd consequence is this: Bill Joy\u2019s specific TCP\/IP code was\nincorporated not only into BSD UNIX, but also into other versions\nof UNIX, including the UNIX distributed by AT&T (which had origi-\nnally licensed UNIX to Berkeley) with the Berkeley copyright notice\nremoved. This bizarre, tangled bank of licenses and code resulted\nin a famous suit and countersuit between AT&T and Berkeley, in\nwhich the intricacies of this situation were sorted out.41 An innocent\nbystander, expecting UNIX to be a single thing, might be surprised\nto find that it takes different forms for reasons that are all but\nimpossible to identify, but the cause of which is clear: different ver-\nsions of sharing in conflict with one another; different moral and\ntechnical imaginations of order that result in complex entangle-\nments of value and code.\nThe BSD fork of UNIX (and the subfork of TCP\/IP) was only one\nof many to come. By the early 1980s, a proliferation of UNIX forks\nhad emerged and would be followed shortly by a very robust com-\nmercialization. At the same time, the circulation of source code\nstarted to slow, as corporations began to compete by adding fea-\ntures and creating hardware specifically designed to run UNIX\n(such as the Sun Sparc workstation and the Solaris operating sys-\ntem, the result of Joy\u2019s commercialization of BSD in the 1980s). The\nquestion of how to make all of these versions work together eventu-\nally became the subject of the open-systems discussions that would\ndominate the workstation and networking sectors of the computer\n140 sharing source code market from the early 1980s to 1993, when the dual success of Win-\ndows NT and the arrival of the Internet into public consciousness\nchanged the fortunes of the UNIX industry.\nA second, and more important, effect of the struggle between\nBBN and BSD was simply the widespread adoption of the TCP\/\nIP protocols. An estimated 98 percent of computer-science depart-\nments in the United States and many such departments around the\nworld incorporated the TCP\/IP protocols into their UNIX systems\nand gained instant access to Arpanet.42 The fact that this occurred\nwhen it did is important: a few years later, during the era of the\ncommercialization of UNIX, these protocols might very well not\nhave been widely implemented (or more likely implemented in in-\ncompatible, nonstandard forms) by manufacturers, whereas before\n1983, university computer scientists saw every benefit in doing so\nif it meant they could easily connect to the largest single computer\nnetwork on the planet. The large, already functioning, relatively\nstandard implementation of TCP\/IP on UNIX (and the ability to\nlook at the source code) gave these protocols a tremendous advan-\ntage in terms of their survival and success as the basis of a global\nand singular network.\nConclusion\nThe UNIX operating system is not just a technical achievement; it is\nthe creation of a set of norms for sharing source code in an unusual\nenvironment: quasi-commercial, quasi-academic, networked, and\nplanetwide. Sharing UNIX source code has taken three basic forms:\nporting source code (transferring it from one machine to another);\nteaching source code, or \u201cporting\u201d it to students in a pedagogical\nsetting where the use of an actual working operating system vastly\nfacilitates the teaching of theory and concepts; and forking source\ncode (modifying the existing source code to do something new or\ndifferent). This play of proliferation and differentiation is essential\nto the remarkably stable identity of UNIX, but that identity exists\nin multiple forms: technical (as a functioning, self-compatible op-\nerating system), legal (as a license-circumscribed version subject to\nintellectual property and commercial law), and pedagogical (as a\nconceptual exemplar, the paradigm of an operating system). Source\ncode shared in this manner is essentially unlike any other kind of\nsharing source code 141 source code in the world of computers, whether academic or com-\nmercial. It raises troubling questions about standardization, about\ncontrol and audit, and about legitimacy that haunts not only UNIX\nbut the Internet and its various \u201copen\u201d protocols as well.\nSharing source code in Free Software looks the way it does today\nbecause of UNIX. But UNIX looks the way it does not because of\nthe inventive genius of Thompson and Ritchie, or the marketing\nand management brilliance of AT&T, but because sharing produces\nits own kind of order: operating systems and social systems. The fact\nthat geeks are wont to speak of \u201cthe UNIX philosophy\u201d means that\nUNIX is not just an operating system but a way of organizing the\ncomplex relations of life and work through technical means; a way\nof charting and breaching the boundaries between the academic,\nthe aesthetic, and the commercial; a way of implementing ideas of\na moral and technical order. What\u2019s more, as source code comes to\ninclude more and more of the activities of everyday communica-\ntion and creation\u2014as it comes to replace writing and supplement\nthinking\u2014the genealogy of its portability and the history of its\nforking will illuminate the kinds of order emerging in practices\nand technologies far removed from operating systems\u2014but tied in-\ntimately to the UNIX philosophy.\n142 sharing source code 5.\nConceiving Open Systems\nThe great thing about standards is that there are\nso many to choose from.1\nOpenness is an unruly concept. While free tends toward ambiguity\n(free as in speech, or free as in beer?), open tends toward obfusca\u00ad\ntion. Everyone claims to be open, everyone has something to share,\neveryone agrees that being open is the obvious thing to do\u2014after all,\nopenness is the other half of \u201copen source\u201d\u2014but for all its obvious\u00ad\nness, being \u201copen\u201d is perhaps the most complex component of Free\nSoftware. It is never quite clear whether being open is a means or an\nend. Worse, the opposite of open in this case (specifically, \u201copen sys\u00ad\ntems\u201d) is not closed, but \u201cproprietary\u201d\u2014signaling the complicated\nimbrication of the technical, the legal, and the commercial.\nIn this chapter I tell the story of the contest over the meaning\nof \u201copen systems\u201d from 1980 to 1993, a contest to create a simul\u00ad\ntaneously moral and technical infrastructure within the computer industry.2 The infrastructure in question includes technical\ncomponents\u2014the UNIX operating system and the TCP\/IP proto\u00ad\ncols of the Internet as open systems\u2014but it also includes \u201cmoral\u201d\ncomponents, including the demand for structures of fair and open\ncompetition, antimonopoly and open markets, and open\u00adstandards\nprocesses for high\u00adtech networked computers and software in the\n1980s.3 By moral, I mean imaginations of the proper order of collec\u00ad\ntive political and commercial action; referring to much more than\nsimply how individuals should act, moral signifies a vision of how\neconomy and society should be ordered collectively.\nThe open\u00adsystems story is also a story of the blind spot of open\nsystems\u2014in that blind spot is intellectual property. The story re\u00ad\nveals a tension between incompatible moral\u00adtechnical orders: on\nthe one hand, the promise of multiple manufacturers and corpora\u00ad\ntions creating interoperable components and selling them in an\nopen, heterogeneous market; on the other, an intellectual\u00adproperty\nsystem that encouraged jealous guarding and secrecy, and granted\nmonopoly status to source code, designs, and ideas in order to dif\u00ad\nferentiate products and promote competition. The tension proved\nirresolvable: without shared source code, for instance, interoperable\noperating systems are impossible. Without interoperable operating\nsystems, internetworking and portable applications are impossible.\nWithout portable applications that can run on any system, open\nmarkets are impossible. Without open markets, monopoly power\nreigns.\nStandardization was at the heart of the contest, but by whom and\nby what means was never resolved. The dream of open systems, pur\u00ad\nsued in an entirely unregulated industry, resulted in a complicated\nexperiment in novel forms of standardization and cooperation. The\ncreation of a \u201cstandard\u201d operating system based on UNIX is the\nstory of a failure, a kind of \u201cfiguring out\u201d gone haywire, which\nresulted in huge consortia of computer manufacturers attempting\nto work together and compete with each other at the same time.\nMeanwhile, the successful creation of a \u201cstandard\u201d networking\nprotocol\u2014known as the Open Systems Interconnection Reference\nModel (OSI)\u2014is a story of failure that hides a larger success; OSI\nwas eclipsed in the same period by the rapid and ad hoc adoption\nof the Transmission Control Protocol\/Internet Protocol (TCP\/IP),\nwhich used a radically different standardization process and which\nsucceeded for a number of surprising reasons, allowing the Internet\n144 conceiving open systems to take the form it did in the 1990s and ultimately exemplifying the\nmoral\u00adtechnical imaginary of a recursive public\u2014and one at the\nheart of the practices of Free Software.\nThe conceiving of openness, which is the central plot of these two\nstories, has become an essential component of the contemporary\npractice and power of Free Software. These early battles created a\nkind of widespread readiness for Free Software in the 1990s, a rec\u00ad\nognition of Free Software as a removal of open systems\u2019 blind spot,\nas much as an exploitation of its power. The geek ideal of openness\nand a moral\u00adtechnical order (the one that made Napster so sig\u00ad\nnificant an event) was forged in the era of open systems; without\nthis concrete historical conception of how to maintain openness\nin technical and moral terms, the recursive public of geeks would\nbe just another hierarchical closed organization\u2014a corporation\nmanqu\u00e9\u2014and not an independent public serving as a check on\nthe kinds of destructive power that dominated the open\u00adsystems\ncontest.\nHopelessly Plural\nBig iron, silos, legacy systems, turnkey systems, dinosaurs, main\u00ad\nframes: with the benefit of hindsight, the computer industry of the\n1960s to the 1980s appears to be backward and closed, to have\nliterally painted itself into a corner, as an early Intel advertisement\nsuggests (figure 3). Contemporary observers who show disgust and\nimpatience with the form that computers took in this era are with\u00ad\nout fail supporters of open systems and opponents of proprietary\nsystems that \u201clock in\u201d customers to specific vendors and create ar\u00ad\ntificial demands for support, integration, and management of re\u00ad\nsources. Open systems (were it allowed to flourish) would solve all\nthese problems.\nGiven the promise of a \u201cgeneral\u00adpurpose computer,\u201d it should\nseem ironic at best that open systems needed to be created. But\nthe general\u00adpurpose computer never came into being. We do not\nlive in the world of The Computer, but in a world of computers:\nmyriad, incompatible, specific machines. The design of specialized\nmachines (or \u201carchitectures\u201d) was, and still is, key to a competi\u00ad\ntive industry in computers. It required CPUs and components and\nassociated software that could be clearly qualified and marketed\nconceiving open systems 145 3. Open systems is the solution to painting yourself into a corner. Intel\nadvertisement, Wall Street Journal, 30 May 1984. as distinct products: the DEC PDP\u00ad11 or the IBM 360 or the CDC\n6600. On the Fordist model of automobile production, the computer\nindustry\u2019s mission was to render desired functions (scientific calcu\u00ad\nlation, bookkeeping, reservations management) in a large box with\na button on it (or a very large number of buttons on increasingly\nsmaller boxes). Despite the theoretical possibility, such computers\nwere not designed to do anything, but, rather, to do specific kinds\nof calculations exceedingly well. They were objects customized to\nparticular markets.\nThe marketing strategy was therefore extremely stable from\nabout 1955 to about 1980: identify customers with computing\nneeds, build a computer to serve them, provide them with all of the\nequipment, software, support, or peripherals they need to do the\njob\u2014and charge a large amount. Organizationally speaking, it was\nan industry dominated by \u201cIBM and the seven dwarfs\u201d: Hewlett\u00ad\nPackard, Honeywell, Control Data, General Electric, NCR, RCA,\nUnivac, and Burroughs, with a few upstarts like DEC in the wings.\nBy the 1980s, however, a certain inversion had happened. Com\u00ad\nputers had become smaller and faster; there were more and more\nof them, and it was becoming increasingly clear to the \u201cbig iron\u201d\nmanufacturers that what was most valuable to users was the in\u00ad\nformation they generated, not the machines that did the generat\u00ad\ning. Such a realization, so the story goes, leads to a demand for\ninterchangeability, interoperability, information sharing, and net\u00ad\nworking. It also presents the nightmarish problems of conversion\nbetween a bewildering, heterogeneous, and rapidly growing array\nof hardware, software, protocols, and systems. As one conference\npaper on the subject of evaluating open systems put it, \u201cAt some\npoint a large enterprise will look around and see a huge amount\nof equipment and software that will not work together. Most im\u00ad\nportantly, the information stored on these diverse platforms is not\nbeing shared, leading to unnecessary duplication and lost profit.\u201d4\nOpen systems emerged in the 1980s as the name of the solution\nto this problem: an approach to the design of systems that, if all\nparticipants were to adopt it, would lead to widely interoperable,\nintegrated machines that could send, store, process, and receive\nthe user\u2019s information. In marketing and public\u00adrelations terms, it\nwould provide \u201cseamless integration.\u201d\nIn theory, open systems was simply a question of standards adop\u00ad\ntion. For instance, if all the manufacturers of UNIX systems could\nconceiving open systems 147 be convinced to adopt the same basic standard for the operating\nsystem, then seamless integration would naturally follow as all the\nvarious applications could be written once to run on any variant\nUNIX system, regardless of which company made it. In reality, such\na standard was far from obvious, difficult to create, and even more\ndifficult to enforce. As such, the meaning of open systems was \u201chope\u00ad\nlessly plural,\u201d and the term came to mean an incredibly diverse\narray of things.\n\u201cOpenness\u201d is precisely the kind of concept that wavers between\nend and means. Is openness good in itself, or is openness a means\nto achieve something else\u2014and if so what? Who wants to achieve\nopenness, and for what purpose? Is openness a goal? Or is it a means\nby which a different goal\u2014say, \u201cinteroperability\u201d or \u201cintegration\u201d\u2014\nis achieved? Whose goals are these, and who sets them? Are the\ngoals of corporations different from or at odds with the goals of uni\u00ad\nversity researchers or government officials? Are there large central\nvisions to which the activities of all are ultimately subordinate?\nBetween 1980 and 1993, no person or company or computer in\u00ad\ndustry consortium explicitly set openness as the goal that organiza\u00ad\ntions, corporations, or programmers should aim at, but, by the same\ntoken, hardly anyone dissented from the demand for openness. As\nsuch, it appears clearly as a kind of cultural imperative, reflecting\na longstanding social imaginary with roots in liberal democratic\nnotions, versions of a free market and ideals of the free exchange\nof knowledge, but confronting changed technical conditions that\nbring the moral ideas of order into relief, and into question.\nIn the 1980s everyone seemed to want some kind of openness,\nwhether among manufacturers or customers, from General Motors\nto the armed forces.5 The debates, both rhetorical and technical,\nabout the meaning of open systems have produced a slough of\nwritings, largely directed at corporate IT managers and CIOs. For\ninstance, Terry A. Critchley and K. C. Batty, the authors of Open\nSystems: The Reality (1993), claim to have collected over a hundred\ndefinitions of open systems. The definitions stress different aspects\u2014\nfrom interoperability of heterogeneous machines, to compatibility\nof different applications, to portability of operating systems, to\nlegitimate standards with open-interface definitions\u2014including\nthose that privilege ideologies of a free market, as does Bill Gates\u2019s\ndefinition: \u201cThere\u2019s nothing more open than the PC market. . . .\n[U]sers can choose the latest and greatest software.\u201d The range\n148 conceiving open systems of meanings was huge and oriented along multiple axes: what, to\nwhom, how, and so on. Open systems could mean that source code\nwas open to view or that only the specifications or interfaces were;\nit could mean \u201cavailable to certain third parties\u201d or \u201cavailable to\neveryone, including competitors\u201d; it could mean self\u00adpublishing,\nwell-defined interfaces and application programming interfaces\n(APIs), or it could mean sticking to standards set by governments\nand professional societies. To cynics, it simply meant that the mar\u00ad\nketing department liked the word open and used it a lot.\nOne part of the definition, however, was both consistent and ex\u00ad\ntremely important: the opposite of an \u201copen system\u201d was not a\n\u201cclosed system\u201d but a \u201cproprietary system.\u201d In industries other than\nnetworking and computing the word proprietary will most likely\nhave a positive valence, as in \u201cour exclusive proprietary technol\u00ad\nogy.\u201d But in the context of computers and networks such a usage\nbecame anathema in the 1980s and 1990s; what customers report\u00ad\nedly wanted was a system that worked nicely with other systems,\nand that system had to be by definition open since no single com\u00ad\npany could provide all of the possible needs of a modern business\nor government agency. And even if it could, it shouldn\u2019t be allowed\nto. For instance, \u201cIn the beginning was the word and the word\nwas \u2018proprietary.\u2019 IBM showed the way, purveying machines that\nexisted in splendid isolation. They could not be operated using pro\u00ad\ngrams written for any other computer; they could not communicate\nwith the machines of competitors. If your company started out buy\u00ad\ning computers of various sizes from the International Business Ma\u00ad\nchines Corporation because it was the biggest and best, you soon\nfound yourself locked as securely to Big Blue as a manacled wretch\nin a medieval dungeon. When an IBM rival unveiled a technologi\u00ad\ncally advanced product, you could only sigh; it might be years be\u00ad\nfore the new technology showed up in the IBM line.\u201d6\nWith the exception of IBM (and to some extent its closest com\u00ad\npetitors: Hewlett\u00adPackard, Burroughs, and Unisys), computer cor\u00ad\nporations in the 1980s sought to distance themselves from such\n\u201cmedieval\u201d proprietary solutions (such talk also echoes that of us\u00ad\nable pasts of the Protestant Reformation often used by geeks). New\nfirms like Sun and Apollo deliberately berated the IBM model. Bill\nJoy reportedly called one of IBM\u2019s new releases in the 1980s a\n\u201cgrazing dinosaur \u2018with a truck outside pumping its bodily fluids\nthrough it.\u2019 \u201d7\nconceiving open systems 149 Open systems was never a simple solution though: all that com\u00ad\nplexity in hardware, software, components, and peripherals could\nonly be solved by pushing hard for standards\u2014even for a single\nstandard. Or, to put it differently, during the 1980s, everyone\nagreed that open systems was a great idea, but no one agreed on\nwhich open systems. As one of the anonymous speakers in Open Sys-\ntems: The Reality puts it, \u201cIt took me a long time to understand what\n(the industry) meant by open vs. proprietary, but I finally figured\nit out. From the perspective of any one supplier, open meant \u2018our\nproducts.\u2019 Proprietary meant \u2018everyone else\u2019s products.\u2019 \u201d8\nFor most supporters of open systems, the opposition between open\nand proprietary had a certain moral force: it indicated that corpo\u00ad\nrations providing the latter were dangerously close to being evil,\nimmoral, perhaps even criminal monopolists. Adrian Gropper and\nSean Doyle, the principals in Amicas, an Internet teleradiology\ncompany, for instance, routinely referred to the large proprietary\nhealthcare\u00adinformation systems they confronted in these terms:\nopen systems are the way of light, not dark. Although there are\nno doubt arguments for closed systems\u2014security, privacy, robust\u00ad\nness, control\u2014the demand for interoperability does not mean that\nsuch closure will be sacrificed.9 Closure was also a choice. That is,\nopen systems was an issue of sovereignty, involving the right, in\na moral sense, of a customer to control a technical order hemmed\nin by firm standards that allowed customers to combine a number\nof different pieces of hardware and software purchased in an open\nmarket and to control the configuration themselves\u2014not enforced\nopenness, but the right to decide oneself on whether and how to be\nopen or closed.\nThe open-systems idea of moral order conflicts, however, with an\nidea of moral order represented by intellectual property: the right,\nencoded in law, to assert ownership over and control particular\nbits of source code, software, and hardware. The call for and the\nmarket in open systems were never imagined as being opposed to\nintellectual property as such, even if the opposition between open\nand proprietary seemed to indicate a kind of subterranean recogni\u00ad\ntion of the role of intellectual property. The issue was never explic\u00ad\nitly broached. Of the hundred definitions in Open Systems, only one\ndefinition comes close to including legal issues: \u201cSpeaker at Interop\n\u201990 (paraphrased and maybe apocryphal): \u2018If you ask to gain access\nto a technology and the response you get back is a price list, then\n150 conceiving open systems that technology is \u201copen.\u201d If what you get back is a letter from a\nlawyer, then it\u2019s not \u201copen.\u201d \u2019 \u201d10\nOpenness here is not equated with freedom to copy and modify,\nbut with the freedom to buy access to any aspect of a system with\u00ad\nout signing a contract, a nondisclosure agreement, or any other\nlegal document besides a check. The ground rules of competition\nare unchallenged: the existing system of intellectual property\u2014a\nsystem that was expanded and strengthened in this period\u2014was a\nsine qua non of competition.\nOpenness understood in this manner means an open market in\nwhich it is possible to buy standardized things which are neither\nobscure nor secret, but can be examined and judged\u2014a \u201ccommod\u00ad\nity\u201d market, where products have functions, where quality is com\u00ad\nparable and forms the basis for vigorous competition. What this\nnotion implies is freedom from monopoly control by corporations over\nproducts, a freedom that is nearly impossible to maintain when\nthe entire industry is structured around the monopoly control of\nintellectual property through trade secret, patent, or copyright. The\nblind spot hides the contradiction between an industry imagined on\nthe model of manufacturing distinct and tangible products, and the\nreality of an industry that wavers somewhere between service and\nproduct, dealing in intangible intellectual property whose bound\u00ad\naries and identity are in fact defined by how they are exchanged,\ncirculated, and shared, as in the case of the proliferation and dif\u00ad\nferentiation of the UNIX operating system.\nThere was no disagreement about the necessity of intellectual\nproperty in the computer industry of the 1980s, and there was no\nperceived contradiction in the demands for openness. Indeed, open\u00ad\nness could only make sense if it were built on top of a stable system\nof intellectual property that allowed competitors to maintain clear\ndefinitions of the boundaries of their products. But the creation of\ninteroperable components seemed to demand a relaxation of the se\u00ad\ncrecy and guardedness necessary to \u201cprotect\u201d intellectual property.\nIndeed, for some observers, the problem of openness created the\nopportunity for the worst kinds of cynical logic, as in this example\nfrom Regis McKenna\u2019s Who\u2019s Afraid of Big Blue?\nUsers want open environments, so the vendors had better comply. In\nfact, it is a good idea to support new standards early. That way, you\ncan help control the development of standards. Moreover, you can\nconceiving open systems 151 take credit for driving the standard. Supporting standards is a way to\ndemonstrate that you\u2019re on the side of users. On the other hand, com\u00ad\npanies cannot compete on the basis of standards alone. Companies\nthat live by standards can die by standards. Other companies, adher\u00ad\ning to the same standards, could win on the basis of superior manufactur-\ning technology. If companies do nothing but adhere to standards, then\nall computers will become commodities, and nobody will be able to\nmake any money. Thus, companies must keep something proprietary,\nsomething to differentiate their products.11\nBy such an account, open systems would be tantamount to\neconomic regression, a state of pure competition on the basis of\nmanufacturing superiority, and not on the basis of the competitive\nadvantage granted by the monopoly of intellectual property, the\nclear hallmark of a high\u00adtech industry.12 It was an irresolvable ten\u00ad\nsion between the desire for a cooperative, market\u00adbased infrastruc\u00ad\nture and the structure of an intellectual\u00adproperty system ill\u00adsuited\nto the technical realities within which companies and customers\noperated\u2014a tension revealing the reorientation of knowledge and\npower with respect to creation, dissemination, and modification of\nknowledge.\nFrom the perspective of intellectual property, ideas, designs, and\nsource code are everything\u2014if a company were to release the source\ncode, and allow other vendors to build on it, then what exactly\nwould they be left to sell? Open systems did not mean anything\nlike free, open\u00adsource, or public\u00addomain computing. But the fact\nthat competition required some form of collaboration was obvious\nas well: standard software and network systems were needed; stan\u00ad\ndard markets were needed; standard norms of innovation within\nthe constraints of standards were needed. In short, the challenge\nwas not just the creation of competitive products but the creation\nof a standard infrastructure, dealing with the technical questions of\navailability, modifiability, and reusability of components, and the\nmoral questions of the proper organization of competition and col\u00ad\nlaboration across diverse domains: engineers, academics, the com\u00ad\nputer industry, and the industries it computerized. What follows\nis the story of how UNIX entered the open\u00adsystems fray, a story\nin which the tension between the conceiving of openness and the\ndemands of intellectual property is revealed.\n152 conceiving open systems Open Systems One: Operating Systems\nIn 1980 UNIX was by all accounts the most obvious choice for a\nstandard operating system for a reason that seemed simple at the\noutset: it ran on more than one kind of hardware. It had been in\u00ad\nstalled on DEC machines and IBM machines and Intel processors\nand Motorola processors\u2014a fact exciting to many professional pro\u00ad\ngrammers, university computer scientists, and system administra\u00ad\ntors, many of whom also considered UNIX to be the best designed\nof the available operating systems.\nThere was a problem, however (there always is): UNIX belonged\nto AT&T, and AT&T had licensed it to multiple manufacturers over\nthe years, in addition to allowing the source code to circulate more\nor less with abandon throughout the world and to be ported to a\nwide variety of different machine architectures. Such proliferation,\nalbeit haphazard, was a dream come true: a single, interoperable\noperating system running on all kinds of hardware. Unfortunately,\nproliferation would also undo that dream, because it meant that as\nthe markets for workstations and operating systems heated up, the\nexisting versions of UNIX hardened into distinct and incompatible\nversions with different features and interfaces. By the mid 1980s,\nthere were multiple competing efforts to standardize UNIX, an en\u00ad\ndeavour that eventually went haywire, resulting in the so\u00adcalled\nUNIX wars, in which \u201cgangs\u201d of vendors (some on both sides of\nthe battle) teamed up to promote competing standards. The story\nof how this happened is instructive, for it is a story that has been\nreiterated several times in the computer industry.13\nAs a hybrid commercial\u00adacademic system, UNIX never entered the\nmarket as a single thing. It was licensed in various ways to different\npeople, both academic and commercial, and contained additions\nand tools and other features that may or may not have originated\nat (or been returned to) Bell Labs. By the early 1980s, the Berkeley\nSoftware Distribution was in fact competing with the AT&T ver\u00ad\nsion, even though BSD was a sublicensee\u2014and it was not the only\none. By the late 1970s and early 1980s, a number of corporations\nhad licensed UNIX from AT&T for use on new machines. Micro\u00ad\nsoft licensed it (and called it Xenix, rather than licensing the name\nUNIX as well) to be installed on Intel\u00adbased machines. IBM, Uni\u00ad\nsys, Amdahl, Sun, DEC, and Hewlett\u00adPackard all followed suit and\nconceiving open systems 153 created their own versions and names: HP\u00adUX, A\/UX, AIX, Ultrix,\nand so on. Given the ground rules of trade secrecy and intellectual\nproperty, each of these licensed versions needed to be made legally\ndistinct\u2014if they were to compete with each other. Even if \u201cUNIX\u201d\nremained conceptually pure in an academic or pedagogical sense,\nevery manufacturer would nonetheless have to tweak, to extend,\nto optimize in order to differentiate. After all, \u201cif companies do\nnothing but adhere to standards, then all computers will become\ncommodities, and nobody will be able to make any money.\u201d14\nIt was thus unlikely that any of these corporations would con\u00ad\ntribute the changes they made to UNIX back into a common pool,\nand certainly not back to AT&T which subsequent to the 1984 di\u00ad\nvestiture finally released their own commercial version of UNIX,\ncalled UNIX System V. Very quickly, the promising \u201copen\u201d UNIX\nof the 1970s became a slough of alternative operating systems,\neach incompatible with the next thanks to the addition of market\u00ad\ndifferentiating features and hardware-specific tweaks. According\nto Pamela Gray, \u201cBy the mid\u00ad1980s, there were more than 100\nversions in active use\u201d centered around the three market leaders,\nAT&T\u2019s System V, Microsoft\/SCO Xenix, and the BSD.15 By 1984,\nthe differences in systems had become significant\u2014as in the case\nof the BSD additions of the TCP\/IP protocols, the vi editor, and the\nPascal compiler\u2014and created not only differentiation in terms of\nquality but also incompatibility at both the software and network\u00ad\ning levels.\nDifferent systems of course had different user communities, based\non who was the customer of whom. Eric Raymond suggests that\nin the mid\u00ad1980s, independent hackers, programmers, and com\u00ad\nputer scientists largely followed the fortunes of BSD: \u201cThe divide\nwas roughly between longhairs and shorthairs; programmers and\ntechnical people tended to line up with Berkeley and BSD, more\nbusiness\u00adoriented types with AT&T and System V. The longhairs,\nrepeating a theme from Unix\u2019s early days ten years before, liked\nto see themselves as rebels against a corporate empire; one of the\nsmall companies put out a poster showing an X\u00adwing\u00adlike space\nfighter marked \u201cBSD\u201d speeding away from a huge AT&T \u2018death\nstar\u2019 logo left broken and in flames.\u201d16\nSo even though UNIX had become the standard operating system\nof choice for time\u00adsharing, multi\u00aduser, high\u00adperformance computers\nby the mid\u00ad1980s, there was no such thing as UNIX. Competitors\n154 conceiving open systems in the UNIX market could hardly expect the owner of the system,\nAT&T, to standardize it and compete with them at the same time,\nand the rest of the systems were in some legal sense still derivations\nfrom the original AT&T system. Indeed, in its licensing pamphlets,\nAT&T even insisted that UNIX was not a noun, but an adjective, as\nin \u201cthe UNIX system.\u201d17\nThe dawning realization that the proliferation of systems was not\nonly spreading UNIX around the world but also spreading it thin\nand breaking it apart led to a series of increasingly startling and\nhigh-profile attempts to \u201cstandardize\u201d UNIX. Given that the three\nmajor branches (BSD, which would become the industry darling as\nSun\u2019s Solaris operating system; Microsoft, and later SCO Xenix; and\nAT&T\u2019s System V) all emerged from the same AT&T and Berkeley\nwork done largely by Thompson, Ritchie, and Joy, one would think\nthat standardization would be a snap. It was anything but.\nFiguring Out Goes Haywire\nFiguring out the moral and technical order of open systems went\nhaywire around 1986\u201388, when there were no fewer than four com\u00ad\npeting international standards, represented by huge consortia of\ncomputer manufacturers (many of whom belonged to multiple con\u00ad\nsortia): POSIX, the X\/Open consortium, the Open Software Foun\u00ad\ndation, and UNIX International. The blind spot of open systems\nhad much to do with this crazy outcome: academics, industry, and\ngovernment could not find ways to agree on standardization. One\ngoal of standardization was to afford customers choice; another\nwas to allow competition unconstrained by \u201cartificial\u201d means.\nA standard body of source code was impossible; a standard \u201cin\u00ad\nterface definition\u201d was open to too much interpretation; govern\u00ad\nment and academic standards were too complex and expensive; no\nparticular corporation\u2019s standard could be trusted (because they\ncould not be trusted to reveal it in advance of their own innova\u00ad\ntions); and worst of all, customers kept buying, and vendors kept\nshipping, and the world was increasingly filled with diversity, not\nstandardization.\nUNIX proliferated quickly because of porting, leading to multiple\ninstances of an operating system with substantially similar source\ncode shared by academics and licensed by AT&T. But it differentiated\nconceiving open systems 155 just as quickly because of forking, as particular features were added\nto different ports. Some features were reincorporated into the \u201cmain\u201d\nbranch\u2014the one Thompson and Ritchie worked on\u2014but the bulk\nof these mutations spread in a haphazard way, shared through us\u00ad\ners directly or implemented in newly formed commercial versions.\nSome features were just that, features, but others could extend the\nsystem in ways that might make an application possible on one ver\u00ad\nsion, but not on another.\nThe proliferation and differentiation of UNIX, the operating sys\u00ad\ntem, had peculiar effects on the emerging market for UNIX, the\nproduct: technical issues entailed design and organizational issues.\nThe original UNIX looked the way it did because of the very pecu\u00ad\nliar structure of the organization that created and sustained UNIX:\nBell Labs and the worldwide community of users and developers.\nThe newly formed competitors, conceiving of UNIX as a product\ndistinct from the original UNIX, adopted it precisely because of\nits portability and because of the promise of open systems as an\nalternative to \u201cbig iron\u201d mainframes. But as UNIX was funneled\ninto existing corporations with their own design and organizational\nstructures, it started to become incompatible with itself, and the\ndesire for competition in open systems necessitated efforts at UNIX\nstandardization.\nThe first step in the standardization of open systems and UNIX\nwas the creation of what was called an \u201cinterface definition,\u201d a\nstandard that enumerated the minimum set of functions that any\nversion of UNIX should support at the interface level, meaning that\nany programmer who wrote an application could expect to interact\nwith any version of UNIX on any machine in the same way and\nget the same response from the machine (regardless of the specific\nimplementation of the operating system or the source code that was\nused). Interface definitions, and extensions to them, were ideally to\nbe published and freely available.\nThe interface definition was a standard that emphasized portabil\u00ad\nity, not at the source\u00adcode or operating\u00adsystem level, but at the ap\u00ad\nplication level, allowing applications built on any version of UNIX\nto be installed and run on any other. The push for such a standard\ncame first from a UNIX user group founded in 1980 by Bob Marsh\nand called, after the convention of file hierarchies in the UNIX\ninterface, \u201c\/usr\/group\u201d (later renamed Uniforum). The 1984 \/usr\/\ngroup standard defined a set of system calls, which, however, \u201cwas\n156 conceiving open systems immediately ignored and, for all practical purposes, useless.\u201d18 It\nseemed the field was changing too fast and UNIX proliferating and\ninnovating too widely for such a standard to work.\nThe \/usr\/group standard nevertheless provided a starting point\nfor more traditional standards organizations\u2014the Institute of Elec\u00ad\ntrical and Electronics Engineers (IEEE) and the American National\nStandards Institute (ANSI)\u2014to take on the task. Both institutions\ntook the \/usr\/group standard as a basis for what would be called\nIEEE P1003 Portable Operating System Interface for Computer En\u00ad\nvironments (POSIX). Over the next three years, from 1984 to 1987,\nPOSIX would work diligently at providing a standard interface\ndefinition for UNIX.\nAlongside this development, the AT&T version of UNIX became\nthe basis for a different standard, the System V Interface Definition\n(SVID), which attempted to standardize a set of functions similar but\nnot identical to the \/usr\/group and POSIX standards. Thus emerged\ntwo competing definitions for a standard interface to a system that\nwas rapidly proliferating into hundreds of tiny operating\u00adsystem\nfiefdoms.19 The danger of AT&T setting the standard was not lost\non any of the competing manufacturers. Even if they created a thor\u00ad\noughly open standard-interface definition, AT&T\u2019s version of UNIX\nwould be the first to implement it, and they would continually have\nprivileged knowledge of any changes: if they sought to change the\nimplementation, they could change the standard; if they received\ndemands that the standard be changed, they could change their\nimplementation before releasing the new standard.\nIn response to this threat, a third entrant into the standards race\nemerged: X\/Open, which comprised a variety of European com\u00ad\nputer manufacturers (including AT&T!) and sought to develop a\nstandard that encompassed both SVID and POSIX. The X\/Open ini\u00ad\ntiative grew out of European concern about the dominance of IBM\nand originally included Bull, Ericsson, ICL, Nixdorf, Olivetti, Phil\u00ad\nips, and Siemens. In keeping with a certain 1980s taste for the in\u00ad\ntegration of European economic activity vis\u00ad\u00e0\u00advis the United States\nand Japan, these manufacturers banded together both to distribute\na unified UNIX operating system in Europe (based initially on the\nBSD and Sun versions of UNIX) and to attempt to standardize it at\nthe same time.\nX\/Open represented a subtle transformation of standardization\nefforts and of the organizational definition of open systems. While\nconceiving open systems 157 the \/usr\/group standard was developed by individuals who used\nUNIX, and the POSIX standard by an acknowledged professional\nsociety (IEEE), the X\/Open group was a collective of computer cor\u00ad\nporations that had banded together to fund an independent entity\nto help further the cause of a standard UNIX. This paradoxical\nsituation\u2014of a need to share a standard among all the competitors\nand the need to keep the details of that standardized product secret to\nmaintain an advantage\u2014was one that many manufacturers, espe\u00ad\ncially the Europeans with their long experience of IBM\u2019s monopoly,\nunderstood as mutually destructive. Hence, the solution was to en\u00ad\ngage in a kind of organizational innovation, to create a new form\nof metacorporate structure that could strategically position itself\nas at least temporarily interested in collaboration with other firms,\nrather than in competition. Thus did stories and promises of open\nsystems wend their way from the details of technical design to those\nof organizational design to the moral order of competition and\ncollaboration, power and strategy. \u201cStandards\u201d became products\nthat corporations sought to \u201csell\u201d to their own industry through the\nintermediary of the consortium.\nIn 1985 and 1986 the disarrayed state of UNIX was also frus\u00ad\ntrating to the major U.S. manufacturers, especially to Sun Micro\u00ad\nsystems, which had been founded on the creation of a market for\nUNIX\u00adbased \u201cworkstations,\u201d high\u00adpowered networked computers\nthat could compete with mainframes and personal computers at\nthe same time. Founded by Bill Joy, Vinod Khosla, and Andreas\nBechtolsheim, Sun had very quickly become an extraordinarily\nsuccessful computer company. The business pages and magazines\nwere keen to understand whether workstations were viable com\u00ad\npetitors to PCs, in particular to those of IBM and Microsoft, and\nthe de facto standard DOS operating system, for which a variety\nof extremely successful business\u00ad, personal\u00ad, and home\u00adcomputer\napplications were written.\nSun seized on the anxiety around open systems, as is evident in\nthe ad it ran during the summer of 1987 (figure 4). The ad plays\nsubtly on two anxieties: the first is directed at the consumer and sug\u00ad\ngests that only with Sun can one actually achieve interoperability\namong all of one business\u2019 computers, much less across a network\nor industry; the second is more subtle and plays to fears within the\ncomputer industry itself, the anxiety that Sun might merge with one\n158 conceiving open systems 4a and 4b. Open systems anxiety around mergers and compatibility. Sun\nMicrosystems advertisement, Wall Street Journal, 9 July 1987.\nof the big corporations, AT&T or Unisys, and corner the market in\nopen systems by producing the de facto standard.\nIn fact, in October 1987 Sun announced that it had made a deal\nwith AT&T. AT&T would distribute a workstation based on Sun\u2019s\nSPARC line of workstations and would acquire 20 percent of Sun.20\nAs part of this announcement, Sun and AT&T made clear that they\nintended to merge two of the dominant versions of UNIX on the\nmarket: AT&T\u2019s System V and the BSD\u00adderived Solaris. This move\nclearly frightened the rest of the manufacturers interested in UNIX\nand open systems, as it suggested a kind of super\u00adpower alignment\nthat would restructure (and potentially dominate) the market. A\n1988 article in the New York Times quotes an industry analyst who\ncharacterizes the merger as \u201ca matter of concern at the highest\nlevels of every major computer company in the United States, and\npossibly the world,\u201d and it suggests that competing manufacturers\n\u201calso fear that AT&T will gradually make Unix a proprietary prod\u00ad\nuct, usable only on AT&T or Sun machines.\u201d21 The industry anxiety\nwas great enough that in March Unisys (a computer manufacturer,\nformerly Burroughs\u00adSperry) announced that it would work with\nAT&T and Sun to bring UNIX to its mainframes and to make its\nconceiving open systems 159 business applications run on UNIX. Such a move was tantamount\nto Unisys admitting that there would be no future in proprietary\nhigh\u00adend computing\u2014the business on which it had hitherto built\nits reputation\u2014unless it could be part of the consortium that could\nown the standard.22\nIn response to this perceived collusion a group of U.S. and European\ncompanies banded together to form another rival organization\u2014\none that partially overlapped with X\/Open but now included\nIBM\u2014this one called the Open Software Foundation. A nonprofit\ncorporation, the foundation included IBM, Digital Equipment,\nHewlett\u00adPackard, Bull, Nixdorf, Siemens, and Apollo Computer\n(Sun\u2019s most direct competitor in the workstation market). Their\ngoal was explicitly to create a \u201ccompeting standard\u201d for UNIX\nthat would be available on the hardware they manufactured (and\nbased, according to some newspaper reports, on IBM\u2019s AIX, which\nwas to be called OSF\/1). AT&T appeared at first to support the\nfoundation, suggesting that if the Open Software Foundation could\ncome up with a standard, then AT&T would make System V com\u00ad\npatible with it. Thus, 1988 was the summer of open love. Every\nmajor computer manufacturer in the world was now part of some\nconsortium or another, and some were part of two\u2014each promot\u00ad\ning a separate standard.\nOf all the corporations, Sun did the most to brand itself as the\noriginator of the open\u00adsystems concept. They made very broad\nclaims for the success of open\u00adsystems standardization, as for in\u00ad\nstance in an ad from August 1988 (figure 5), which stated in part:\nBut what\u2019s more, those sales confirm a broad acceptance of the whole\nidea behind Sun.\nThe Open Systems idea.\nSystems based on standards so universally accepted that they allow\ncombinations of hardware and software from literally thousands of\nindependent vendors. . . .\nSo for the first time, you\u2019re no longer locked into the company who\nmade your computers. Even if it\u2019s us.\nThe ad goes on to suggest that \u201cin a free market, the best products\nwin out,\u201d even as Sun played both sides of every standardization\nbattle, cooperating with both AT&T and with the Open Software\nFoundation. But by October of that year, it was clear to Sun that\n160 conceiving open systems 5. It pays to be open: Sun\u2019s version of profitable and successful open\nsystems. Sun Microsystems advertisement, New York Times, 2 August\n1988. the idea hadn\u2019t really become \u201cso universal\u201d just yet. In that month\nAT&T and Sun banded together with seventeen other manufactur\u00ad\ners and formed a rival consortium: Unix International, a coalition\nof the willing that would back the AT&T UNIX System V version\nas the one true open standard. In a full\u00adpage advertisement from\nHalloween of 1988 (figure 6), run simultaneously in the New York\nTimes, the Washington Post, and the Wall Street Journal, the rhetoric\nof achieved success remained, but now instead of \u201cthe Open Sys\u00ad\ntems idea,\u201d it was \u201cyour demand for UNIX System V\u00adbased solu\u00ad\ntions that ushered in the era of open architecture.\u201d Instead of a\nstandard for all open systems, it was a war of all against all, a war\nto assure customers that they had made, not the right choice of\nhardware or software, but the right choice of standard.\nThe proliferation of standards and standards consortia is often\nreferred to as the UNIX wars of the late 1980s, but the creation of\nsuch consortia did not indicate clearly drawn lines. Another meta\u00ad\nphor that seems to have been very popular in the press at the time\nwas that of \u201cgang\u201d warfare (no doubt helped along by the creation\nof another industry consortia informally called the Gang of Nine,\nwhich were involved in a dispute over whether MicroChannel or\nEISA buses should be installed in PCs). The idea of a number of\ncompanies forming gangs to fight with each other, Bloods-and-Crips\nstyle\u2014or perhaps more Jets\u00adand\u00adSharks style, minus the singing\n\u2014was no doubt an appealing metaphor at the height of Los Ange\u00ad\nles\u2019s very real and high-profile gang warfare. But as one article in\nthe New York Times pointed out, these were strange gangs: \u201cSince\n\u2018openness\u2019 and \u2018cooperation\u2019 are the buzzwords behind these alli\u00ad\nances, the gang often asks its enemy to join. Often the enemy does\nso, either so that it will not seem to be opposed to openness or to\nkeep tabs on the group. IBM was invited to join the corporation\nfor Open Systems, even though the clear if unstated motive of the\ngroup was to dilute IBM\u2019s influence in the market. AT&T negotiated\nto join the Open Software Foundation, but the talks collapsed re\u00ad\ncently. Some companies find it completely consistent to be members\nof rival gangs. . . . About 10 companies are members of both the\nOpen Software Foundation and its archrival Unix International.\u201d23\nThe proliferation of these consortia can be understood in various\nways. One could argue that they emerged at a time\u2014during the\nReagan administration\u2014when antitrust policing had diminished to\n162 conceiving open systems 6. The UNIX Wars, Halloween 1988. UNIX International advertisement,\nWall Street Journal and New York Times, 31 October 1988. the point where computer corporations did not see such collusion as\na risky activity vis\u00ad\u00e0\u00advis antitrust policing. One could also argue that\nthese consortia represented a recognition that the focus on hard\u00ad\nware control (the meaning of proprietary) had been replaced with a\nfocus on the control of the \u201copen standard\u201d by one or several manu\u00ad\nfacturers, that is, that competition was no longer based on superior\nproducts, but on \u201cowning the standard.\u201d It is significant that the in\u00ad\ndustry consortia quickly overwhelmed national efforts, such as the\nIEEE POSIX standard, in the media, an indication that no one was\nlooking to government or nonprofits, or to university professional\nsocieties, to settle the dispute by declaring a standard, but rather\nto industry itself to hammer out a standard, de facto or otherwise.\nYet another way to understand the emergence of these consortia is\nas a kind of mutual policing of the market, a kind of paranoid strat\u00ad\negy of showing each other just enough to make sure that no one\nwould leapfrog ahead and kill the existing, fragile competition.\nWhat this proliferation of UNIX standards and consortia most\nclearly represents, however, is the blind spot of open systems: the\ndifficulty of having collaboration and competition at the same time\nin the context of intellectual\u00adproperty rules that incompletely cap\u00ad\nture the specific and unusual characteristics of software. For par\u00ad\nticipants in this market, the structure of intellectual property was\nunassailable\u2014without it, most participants assumed, innovation\nwould cease and incentives disappear. Despite the fact that secrecy\nhaunted the industry, its customers sought both openness and com\u00ad\npatibility. These conflicting demands proved irresolvable.\nDenouement\nIronically, the UNIX wars ended not with the emergence of a win\u00ad\nner, but with the reassertion of proprietary computing: Microsoft\nWindows and Windows NT. Rather than open systems emerging vic\u00ad\ntorious, ushering in the era of seamless integration of diverse com\u00ad\nponents, the reverse occurred: Microsoft managed to grab a huge\nshare of computer markets, both desktop and high\u00adperformance,\nby leveraging its brand, the ubiquity of DOS, and application\u00ad\nsoftware developers\u2019 dependence on the \u201cWintel\u201d monster (Win\u00ad\ndows plus Intel chips). Microsoft triumphed, largely for the same\nreasons the open\u00adsystems dream failed: the legal structure of intel\u00ad\n164 conceiving open systems lectual property favored a strong corporate monopoly on a single,\nbranded product over a weak array of \u201copen\u201d and competing com\u00ad\nponents. There was no large gain to investors, or to corporations,\nfrom an industry of nice guys sharing the source code and making\nthe components work together. Microsoft, on the other hand, had\ndecided to do so internal to itself; it did not necessarily need to form\nconsortia or standardize its operating systems, if it could leverage\nits dominance in the market to spread the operating system far and\nwide. It was, as standards observers like to say, the triumph of de\nfacto standardization over de jure. It was a return to the manacled\nwretches of IBM\u2019s monopoly\u2014but with a new dungeon master.\nThe denouement of the UNIX standards story was swift: AT&T\nsold its UNIX System Labs (including all of the original source and\nrights) to Novell in 1993, who sold it in turn to SCO two years later.\nNovell sold (or transferred) the trademark name UNIX\u2122 to the X\/\nOpen group, which continued to fight for standardization, includ\u00ad\ning a single universal UNIX specification. In 1996 X\/Open and the\nOpen Software Foundation merged to form the Open Group.24 The\nOpen Group eventually joined forces with IEEE to turn POSIX into\na single UNIX specification in 2001. They continue to push the\noriginal vision of open systems, though they carefully avoid using\nthe name or concept, referring instead to the trademarked mouth\u00ad\nful \u201cBoundaryless Information Flow\u201d and employing an updated\nand newly inscrutable rhetoric: \u201cBoundaryless Information Flow,\na shorthand representation of \u2018access to integrated information to\nsupport business process improvements\u2019 represents a desired state\nof an enterprise\u2019s infrastructure and is specific to the business needs\nof the organization.\u201d25\nThe Open Group, as well as many other participants in the his\u00ad\ntory of open systems, recognize the emergence of \u201copen source\u201d as\na return to the now one true path of boundaryless information flow.\nEric Raymond, of course, sees continuity and renewal (not least of\nwhich in his own participation in the Open Source movement) and\nin his Art of UNIX Programming says, \u201cThe Open Source movement\nis building on this stable foundation and is creating a resurgence\nof enthusiasm for the UNIX philosophy. In many ways Open Source\ncan be seen as the true delivery of Open Systems that will ensure it\ncontinues to go from strength to strength.\u201d26\nThis continuity, of course, deliberately disavows the centrality\nof the legal component, just as Raymond and the Open Source\nconceiving open systems 165 Initiative had in 1998. The distinction between a robust market in\nUNIX operating systems and a standard UNIX\u00adbased infrastructure\non which other markets and other activities can take place still\nremains unclear to even those closest to the money and machines.\nIt does not yet exist, and may well never come to.\nThe growth of Free Software in the 1980s and 1990s depended\non openness as a concept and component that was figured out dur\u00ad\ning the UNIX wars. It was during these wars that the Free Software\nFoundation (and other groups, in different ways) began to recog\u00ad\nnize the centrality of the issue of intellectual property to the goal\nof creating an infrastructure for the successful creation of open\nsystems.27 The GNU (GNU\u2019s Not Unix) project in particular, but\nalso the X Windows system at MIT, the Remote Procedure Call and\nNetwork File System (NFS) systems created by Sun, and tools like\nsendmail and BIND were each in their own way experiments with\nalternative licensing arrangements and were circulating widely on\na variety of the UNIX versions in the late 1980s. Thus, the experi\u00ad\nence of open systems, while technically a failure as far as UNIX was\nconcerned, was nonetheless a profound learning experience for an\nentire generation of engineers, hackers, geeks, and entrepreneurs.\nJust as the UNIX operating system had a pedagogic life of its own,\ninculcating itself into the minds of engineers as the paradigm of an\noperating system, open systems had much the same effect, realizing\nan inchoate philosophy of openness, interconnection, compatibility,\ninteroperability\u2014in short, availability and modifiability\u2014that was in\nconflict with intellectual-property structures as they existed. To put\nit in Freudian terms: the neurosis of open systems wasn\u2019t cured, but\nthe structure of its impossibility had become much clearer to ev\u00ad\neryone. UNIX, the operating system, did not disappear at all\u2014but\nUNIX, the market, did.\nOpen Systems Two: Networks\nThe struggle to standardize UNIX as a platform for open systems\nwas not the only open\u00adsystems struggle; alongside the UNIX wars,\nanother \u201creligious war\u201d was raging. The attempt to standardize\nnetworks\u2014in particular, protocols for the inter\u00adnetworking of mul\u00ad\ntiple, diverse, and autonomous networks of computers\u2014was also\na key aspect of the open\u00adsystems story of the 1980s.28 The war\n166 conceiving open systems between the TCP\/IP and OSI was also a story of failure and surpris\u00ad\ning success: the story of a successful standard with international\napproval (the OSI protocols) eclipsed by the experimental, military\u00ad\nfunded TCP\/IP, which exemplified an alternative and unusual stan\u00ad\ndards process. The moral\u00adtechnical orders expressed by OSI and\nTCP\/IP are, like that of UNIX, on the border between government,\nuniversity, and industry; they represent conflicting social imaginar\u00ad\nies in which power and legitimacy are organized differently and, as\na result, expressed differently in the technology.\nOSI and TCP\/IP started with different goals: OSI was intended\nto satisfy everyone, to be the complete and comprehensive model\nagainst which all competing implementations would be validated;\nTCP\/IP, by contrast, emphasized the easy and robust intercon\u00ad\nnection of diverse networks. TCP\/IP is a protocol developed by\nbootstrapping between standard and implementation, a mode\nexemplified by the Requests for Comments system that developed\nalongside them as part of the Arpanet project. OSI was a \u201cmodel\u201d\nor reference standard developed by internationally respected stan\u00ad\ndards organizations.\nIn the mid\u00ad1980s OSI was en route to being adopted internation\u00ad\nally, but by 1993 it had been almost completely eclipsed by TCP\/IP.\nThe success of TCP\/IP is significant for three reasons: (1) availability\n\u2014TCP\/IP was itself available via the network and development\nopen to anyone, whereas OSI was a bureaucratically confined and\nexpensive standard and participation was confined to state and\ncorporate representatives, organized through ISO in Geneva; (2)\nmodifiability\u2014TCP\/IP could be copied from an existing implemen\u00ad\ntation (such as the BSD version of UNIX) and improved, whereas\nOSI was a complex standard that had few existing implementations\navailable to copy; and (3) serendipity\u2014new uses that took advan\u00ad\ntage of availability and modifiability sprouted, including the \u201ckiller\napp\u201d that was the World Wide Web, which was built to function on\nexisting TCP\/IP\u00adbased networks, convincing many manufacturers\nto implement that protocol instead of, or in addition to, OSI.\nThe success of TCP\/IP over OSI was also significant because of\nthe difference in the standardization processes that it exemplified.\nThe OSI standard (like all official international standards) is con\u00ad\nceived and published as an aid to industrial growth: it was imag\u00ad\nined according to the ground rules of intellectual property and as\nan attempt to facilitate the expansion of markets in networking.\nconceiving open systems 167 OSI would be a \u201cvendor\u00adneutral\u201d standard: vendors would create\ntheir own, secret implementations that could be validated by OSI\nand thereby be expected to interoperate with other OSI\u00advalidated\nsystems. By stark contrast, the TCP\/IP protocols were not pub\u00ad\nlished (in any conventional sense), nor were the implementations\nvalidated by a legitimate international\u00adstandards organization; in\u00ad\nstead, the protocols are themselves represented by implementations\nthat allow connection to the network itself (where the TCP\/IP pro\u00ad\ntocols and implementations are themselves made available). The\nfact that one can only join the network if one possesses or makes\nan implementation of the protocol is generally seen as the ultimate\nin validation: it works.29 In this sense, the struggle between TCP\/IP\nand OSI is indicative of a very familiar twentieth\u00adcentury struggle\nover the role and extent of government planning and regulation\n(versus entrepreneurial activity and individual freedom), perhaps\nbest represented by the twin figures of Friedrich Hayek and May\u00ad\nnard Keynes. In this story, it is Hayek\u2019s aversion to planning and\nthe subsequent privileging of spontaneous order that eventually\ntriumphs, not Keynes\u2019s paternalistic view of the government as a\nneutral body that absorbs or encourages the swings of the market.\nBootstrapping Networks\nThe \u201creligious war\u201d between TCP\/IP and OSI occurred in the context\nof intense competition among computer manufacturers and during\na period of vibrant experimentation with computer networks world\u00ad\nwide. As with most developments in computing, IBM was one of the\nfirst manufacturers to introduce a networking system for its ma\u00ad\nchines in the early 1970s: the System Network Architecture (SNA).\nDEC followed suit with Digital Network Architecture (DECnet or\nDNA), as did Univac with Distributed Communications Architecture\n(DCA), Burroughs with Burroughs Network Architecture (BNA),\nand others. These architectures were, like the proprietary operat\u00ad\ning systems of the same era, considered closed networks, networks\nthat interconnected a centrally planned and specified number of\nmachines of the same type or made by the same manufacturer. The\ngoal of such networks was to make connections internal to a firm,\neven if that involved geographically widespread systems (e.g., from\nbranch to headquarters). Networks were also to be products.\n168 conceiving open systems The 1970s and 1980s saw extraordinarily vibrant experimenta\u00ad\ntion with academic, military, and commercial networks. Robert\nMetcalfe had developed Ethernet at Xerox PARC in the mid\u00ad1970s,\nand IBM later created a similar technology called \u201ctoken ring.\u201d\nIn the 1980s the military discovered that the Arpanet was being\nused predominantly by computer scientists and not just for military\napplications, and decided to break it into MILNET and CSNET.30\nBulletin Board Services, which connected PCs to each other via\nmodems to download files, appeared in the late 1970s. Out of this\ngrew Tom Jennings\u2019s very successful experiment called FidoNet.31\nIn the 1980s an existing social network of university faculty on\nthe East Coast of the United States started a relatively successful\nnetwork called BITNET (Because It\u2019s There Network) in the mid\u00ad\n1980s.32 The Unix to Unix Copy Protocol (uucp), which initially\nenabled the Usenet, was developed in the late 1970s and widely\nused until the mid\u00ad1980s to connect UNIX computers together. In\n1984 the NSF began a program to fund research in networking\nand created the first large backbones for NSFNet, successor to the\nCSNET and Arpanet.33\nIn the 1970s telecommunications companies and spin-off start-\nups experimented widely with what were called \u201cvideotex\u201d systems,\nof which the most widely implemented and well\u00adknown is Minitel\nin France.34 Such systems were designed for consumer users and\noften provided many of the now widespread services available on\nthe Internet in a kind of embryonic form (from comparison shop\u00ad\nping for cars, to directory services, to pornography).35 By the late\n1970s, videotex systems were in the process of being standardized\nby the Commit\u00e9 Consultative de Information, Technologie et T\u00e9l\u00e9\u00ad\ncommunications (CCITT) at the International Telecommunications\nUnion (ITU) in Geneva. These standards efforts would eventually\nbe combined with work of the International Organization for Stan\u00ad\ndardization (ISO) on OSI, which had originated from work done at\nHoneywell.36\nOne important feature united almost all of these experiments:\nthe networks of the computer manufacturers were generally pig\u00ad\ngybacked, or bootstrapped, onto existing telecommunications\ninfrastructures built by state\u00adrun or regulated monopoly telecom\u00ad\nmunications firms. This situation inevitably spelled grief, for tele\u00ad\ncommunications providers are highly regulated entities, while the\ncomputer industry has been almost totally unregulated from its\nconceiving open systems 169 inception. Since an increasingly core part of the computer industry\u2019s\nbusiness involved transporting signals through telecommunications\nsystems without being regulated to do so, the telecommunications\nindustry naturally felt themselves at a disadvantage.37 Telecom\u00ad\nmunications companies were not slow to respond to the need for\ndata communications, but their ability to experiment with products\nand practices outside the scope of telephony and telegraphy was\noften hindered by concerns about antitrust and monopoly.38 The\nunregulated computer industry, by contrast, saw the tentativeness\nof the telecommunications industry (or national PTTs) as either\nbureaucratic inertia or desperate attempts to maintain control and\npower over existing networks\u2014though no computer manufacturer\nrelished the idea of building their own physical network when so\nmany already existed.\nTCP\/IP and OSI have become emblematic of the split between the\nworlds of telecommunications and computing; the metaphors of re\u00ad\nligious wars or of blood feuds and cold wars were common.39 A par\u00ad\nticularly arch account from this period is Carl Malamud\u2019s Exploring\nthe Internet: A Technical Travelogue, which documents Malamud\u2019s\n(physical) visits to Internet sites around the globe, discussions (and\nbeer) with networking researchers on technical details of the net\u00ad\nworks they have created, and his own typically geeky, occasionally\noffensive takes on cultural difference.40 A subtheme of the story is\nthe religious war between Geneva (in particular the ITU) and the\nInternet: Malamud tells the story of asking the ITU to release its\n19,000\u00adpage \u201cblue book\u201d of standards on the Internet, to facilitate\nits adoption and spread.\nThe resistance of the ITU and Malamud\u2019s heroic if quixotic attempts\nare a parable of the moral\u00adtechnical imaginaries of openness\u2014\nand indeed, his story draws specifically on the usable past of Gior\u00ad\ndano Bruno.41 The \u201cbruno\u201d project demonstrates the gulf that exists\nbetween two models of legitimacy\u2014those of ISO and the ITU\u2014in\nwhich standards represent the legal and legitimate consensus of\na regulated industry, approved by member nations, paid for and\nenforced by governments, and implemented and adhered to by\ncorporations.\nOpposite ISO is the ad hoc, experimental style of Arpanet and\nInternet researchers, in which standards are freely available and\nimplementations represent the mode of achieving consensus, rather\nthan the outcome of the consensus. In reality, such a rhetorical\n170 conceiving open systems opposition is far from absolute: many ISO standards are used on\nthe Internet, and ISO remains a powerful, legitimate standards or\u00ad\nganization. But the clash of established (telecommunications) and\nemergent (computer\u00adnetworking) industries is an important context\nfor understanding the struggle between OSI and TCP\/IP.\nThe need for standard networking protocols is unquestioned: in\u00ad\nteroperability is the bread and butter of a network. Nonetheless,\nthe goals of the OSI and the TCP\/IP protocols differed in important\nways, with profound implications for the shape of that interoper\u00ad\nability. OSI\u2019s goals were completeness, control, and comprehen\u00ad\nsiveness. OSI grew out of the telecommunications industry, which\nhad a long history of confronting the vicissitudes of linking up net\u00ad\nworks and facilitating communication around the world, a problem\nthat required a strong process of consensus and negotiation among\nlarge, powerful, government\u00adrun entities, as well as among smaller\nmanufacturers and providers. OSI\u2019s feet were firmly planted in the\ninternational standardization organizations like OSI and the ITU\n(an organization as old as telecommunications itself, dating to the\n1860s).\nEven if they were oft\u00admocked as slow, bureaucratic, or cumber\u00ad\nsome, the processes of ISO and ITU\u2014based in consensus, inter\u00ad\nnational agreement, and thorough technical specification\u2014are\nprocesses of unquestioned legitimacy. The representatives of nations\nand corporations who attend ISO and ITU standards discussions,\nand who design, write, and vote on these standards, are usually not\nbureaucrats, but engineers and managers directly concerned with\nthe needs of their constituency. The consensus\u00adoriented process\nmeans that ISO and ITU standards attempt to satisfy all members\u2019\ngoals, and as such they tend to be very large, complex, and highly\nspecific documents. They are generally sold to corporations and\nothers who need to use them, rather than made freely available, a\nfact that until recently reflected their legitimacy, rather than lack\nthereof.\nTCP\/IP, on the other hand, emerged from very different condi\u00ad\ntions.42 These protocols were part of a Department of Defense\u2013\nfunded experimental research project: Arpanet. The initial Arpanet\nprotocols (the Network Control Protocol, or NCP) were insuffi\u00ad\ncient, and TCP\/IP was an experiment in interconnecting two dif\u00ad\nferent \u201cpacket\u00adswitched networks\u201d: the ground\u00adline\u2013based Arpanet\nnetwork and a radio\u00adwave network called Packet Radio.43 The\nconceiving open systems 171 problem facing the designers was not how to accommodate every\u00ad\none, but merely how to solve a specific problem: interconnecting\ntwo technically diverse networks, each with autonomous admin\u00ad\nistrative boundaries, but forcing neither of them to give up the\nsystem or the autonomy.\nUntil the mid\u00ad1980s, the TCP\/IP protocols were resolutely research\u00ad\noriented, and not the object of mainstream commercial interest.\nTheir development reflected a core set of goals shared by research\u00ad\ners and ultimately promoted by the central funding agency, the\nDepartment of Defense. The TCP\/IP protocols are often referred to\nas enabling packet\u00adswitched networks, but this is only partially cor\u00ad\nrect; the real innovation of this set of protocols was a design for an\n\u201cinter\u00adnetwork,\u201d a system that would interconnect several diverse\nand autonomous networks (packet\u00adswitched or circuit\u00adswitched),\nwithout requiring them to be transformed, redesigned, or standard\u00ad\nized\u2014in short, by requiring only standardization of the intercom\u00ad\nmunication between networks, not standardization of the network\nitself. In the first paper describing the protocol Robert Kahn and\nVint Cerf motivated the need for TCP\/IP thus: \u201cEven though many\ndifferent and complex problems must be solved in the design of\nan individual packet\u00adswitching network, these problems are mani\u00ad\nfestly compounded when dissimilar networks are interconnected.\nIssues arise which may have no direct counterpart in an individual\nnetwork and which strongly influence the way in which Internet\u00ad\nwork communication can take place.\u201d44\nThe explicit goal of TCP\/IP was thus to share computer resources,\nnot necessarily to connect two individuals or firms together, or to\ncreate a competitive market in networks or networking software.\nSharing between different kinds of networks implied allowing the\ndifferent networks to develop autonomously (as their creators and\nmaintainers saw best), but without sacrificing the ability to continue\nsharing. Years later, David Clark, chief Internet engineer for several\nyears in the 1980s, gave a much more explicit explanation of the\ngoals that led to the TCP\/IP protocols. In particular, he suggested\nthat the main overarching goal was not just to share resources but\n\u201cto develop an effective technique for multiplexed utilization of\nexisting interconnected networks,\u201d and he more explicitly stated\nthe issue of control that faced the designers: \u201cNetworks represent\nadministrative boundaries of control, and it was an ambition of this\nproject to come to grips with the problem of integrating a number\n172 conceiving open systems of separately administrated entities into a common utility.\u201d45 By\nplacing the goal of expandability first, the TCP\/IP protocols were\ndesigned with a specific kind of simplicity in mind: the test of the\nprotocols\u2019 success was simply the ability to connect.\nBy setting different goals, TCP\/IP and OSI thus differed in terms of\ntechnical details; but they also differed in terms of their context and\nlegitimacy, one being a product of international\u00adstandards bodies,\nthe other of military\u00adfunded research experiments. The technical\nand organizational differences imply different processes for stan\u00ad\ndardization, and it is the peculiar nature of the so\u00adcalled Requests\nfor Comments (RFC) process that gave TCP\/IP one of its most dis\u00ad\ntinctive features. The RFC system is widely recognized as a unique\nand serendipitous outcome of the research process of Arpanet.46 In\na thirty\u00adyear retrospective (published, naturally, as an RFC: RFC\n2555), Vint Cerf says, \u201cHiding in the history of the RFCs is the\nhistory of human institutions for achieving cooperative work.\u201d He\ngoes on to describe their evolution over the years: \u201cWhen the RFCs\nwere first produced, they had an almost 19th century character to\nthem\u2014letters exchanged in public debating the merits of various\ndesign choices for protocols in the ARPANET. As email and bulletin\nboards emerged from the fertile fabric of the network, the far-flung\nparticipants in this historic dialog began to make increasing use of\nthe online medium to carry out the discussion\u2014reducing the need\nfor documenting the debate in the RFCs and, in some respects, leav\u00ad\ning historians somewhat impoverished in the process. RFCs slowly\nbecame conclusions rather than debates.\u201d47\nIncreasingly, they also became part of a system of discussion and\nimplementation in which participants created working software as\npart of an experiment in developing the standard, after which there\nwas more discussion, then perhaps more implementation, and fi\u00ad\nnally, a standard. The RFC process was a way to condense the pro\u00ad\ncess of standardization and validation into implementation; which\nis to say, the proof of open systems was in the successful connection\nof diverse networks, and the creation of a standard became a kind\nof ex post facto rubber\u00adstamping of this demonstration. Any further\nimprovement of the standard hinged on an improvement on the\nstandard implementation because the standards that resulted were\nfreely and widely available: \u201cA user could request an RFC by email\nfrom his host computer and have it automatically delivered to his\nmailbox. . . . RFCs were also shared freely with official standards\nconceiving open systems 173 bodies, manufacturers and vendors, other working groups, and uni\u00ad\nversities. None of the RFCs were ever restricted or classified. This\nwas no mean feat when you consider that they were being funded\nby DoD during the height of the Cold War.\u201d48\nThe OSI protocols were not nearly so freely available. The ironic\nreversal\u2014the transparency of a military\u00adresearch program versus the\nopacity of a Geneva\u00adbased international\u00adstandards organization\u2014\ngoes a long way toward explaining the reasons why geeks might\nfind the story of TCP\/IP\u2019s success to be so appealing. It is not that\ngeeks are secretly militaristic, but that they delight in such sur\u00ad\nprising reversals, especially when those reversals exemplify the\nkind of ad hoc, clever solution to problems of coordination that\nthe RFC process does. The RFC process is not the only alternative\nto a consensus\u00adoriented model of standardization pioneered in the\ninternational organizations of Geneva, but it is a specific response\nto a reorientation of power and knowledge that was perhaps more\n\u201cintuitively obvious\u201d to the creators of Arpanet and the Internet,\nwith its unusual design goals and context, than it would have been\nto the purveyors of telecommunications systems with over a hun\u00ad\ndred years of experience in connecting people in very specific and\nestablished ways.\nSuccess as Failure\nBy 1985, OSI was an official standard, one with widespread accep\u00ad\ntance by engineers, by the government and military (the \u201cGOSIP\u201d\nstandard), and by a number of manufacturers, the most significant\nof which was General Motors, with its Manufacturing Automa\u00ad\ntion Protocol (MAP). In textbooks and handbooks of the late 1980s\nand early 1990s, OSI was routinely referred to as the inevitable\nstandard\u2014which is to say, it had widespread legitimacy as the\nstandard that everyone should be implementing\u2014but few imple\u00ad\nmentations existed. Many of the textbooks on networking from the\nlate 1980s, especially those slanted toward a theoretical introduc\u00ad\ntion, give elaborate detail of the OSI reference model\u2014a genera\u00ad\ntion of students in networking was no doubt trained to understand\nthe world in terms of OSI\u2014but the ambivalence continued. Indeed,\nthe most enduring legacy of the creation of the OSI protocols is\nnot the protocols themselves (some of which, like ASN.1, are still\n174 conceiving open systems widely used today), but the pedagogical model: the \u201c7 layer stack\u201d\nthat is as ubiquitous in networking classes and textbooks as UNIX\nis in operating\u00adsystems classes.49\nBut in the late 1980s, the ambivalence turned to confusion. With\nOSI widely recognized as the standard, TCP\/IP began to show up\nin more and more actually existing systems. For example, in Com-\nputer Network Architectures and Protocols, Carl Sunshine says, \u201cNow\nin the late 1980s, much of the battling seems over. CCITT and\nISO have aligned their efforts, and the research community seems\nlargely to have resigned itself to OSI.\u201d But immediately afterward\nhe adds: \u201cIt is ironic that while a consensus has developed that\nOSI is indeed inevitable, the TCP\/IP protocol suite has achieved\nwidespread deployment, and now serves as a de facto interoper\u00ad\nability standard. . . . It appears that the vendors were unable to\nbring OSI products to market quickly enough to satisfy the de\u00ad\nmand for interoperable systems, and TCP\/IP were there to fill the\nneed.\u201d50\nThe more implementations that appeared, the less secure the\nlegitimate standard seemed to be. By many accounts the OSI speci\u00ad\nfications were difficult to implement, and the yearly networking-\nindustry \u201cInterop\u201d conferences became a regular locale for the\nreligious war between TCP\/IP and OSI. The success of TCP\/IP over\nOSI reflects the reorientation of knowledge and power to which\nFree Software is also a response. The reasons for the success are no\ndoubt complex, but the significance of the success of TCP\/IP illus\u00ad\ntrates three issues: availability, modifiability, and serendipity.\nAvailability The TCP\/IP standards themselves were free to any\u00ad\none and available over TCP\/IP networks, exemplifying one of the\naspects of a recursive public: that the only test of participation in\na TCP\/IP\u00adbased internetwork is the fact that one possesses or has\ncreated a device that implements TCP\/IP. Access to the network is\ncontingent on the interoperability of the networks. The standards\nwere not \u201cpublished\u201d in a conventional sense, but made available\nthrough the network itself, without any explicit intellectual prop\u00ad\nerty restrictions, and without any fees or restrictions on who could\naccess them. By contrast, ISO standards are generally not circulated\nfreely, but sold for relatively high prices, as a source of revenue,\nand under the general theory that only legitimate corporations or\ngovernment agencies would need access to them.\nconceiving open systems 175 Related to the availability of the standards is the fact that the\nstandards process that governed TCP\/IP was itself open to anyone,\nwhether corporate, military or academic. The structure of gover\u00ad\nnance of the Internet Engineering Task Force (the IETF) and the In\u00ad\nternet Society (ISOC) allowed for anyone with the means available\nto attend the \u201cworking group\u201d meetings that would decide on the\nstandards that would be approved. Certainly this does not mean that\nthe engineers and defense contractors responsible actively sought\nout corporate stakeholders or imagined the system to be \u201cpublic\u201d\nin any dramatic fashion; however, compared to the system in place\nat most standards bodies (in which members are usually required\nto be the representatives of corporations or governments), the IETF\nallowed individuals to participate qua individuals.51\nModifiability Implementations of TCP\/IP were widely available,\nbootstrapped from machine to machine along with the UNIX op\u00ad\nerating system and other tools (e.g., the implementation of TCP\/\nIP in BSD 4.2, the BSD version of UNIX), generally including the\nsource code. An existing implementation is a much more expressive\nand usable object than a specification for an implementation, and\nthough ISO generally prepares reference implementations for such\nstandards, in the case of OSI there were many fewer implementa\u00ad\ntions to work with or build on. Because multiple implementations of\nTCP\/IP already existed, it was easy to validate: did your (modified)\nimplementation work with the other existing implementations? By\ncontrast, OSI would provide independent validation, but the in situ\nvalidation through connection to other OSI networks was much\nharder to achieve, there being too few of them, or access being re\u00ad\nstricted. It is far easier to build on an existing implementation and\nto improve on it piecemeal, or even to rewrite it completely, using its\nfaults as a template (so to speak), than it is to create an implementa\u00ad\ntion based solely on a standard. The existence of the TCP\/IP protocols\nin BSD 4.2 not only meant that people who installed that operating\nsystem could connect to the Internet easily, at a time when it was by\nno means standard to be able to do so, but it also meant that manu\u00ad\nfacturers or tinkerers could examine the implementation in BSD 4.2\nas the basis for a modified, or entirely new, implementation.\nSerendipity Perhaps most significant, the appearance of wide\u00ad\nspread and popular applications that were dependent on TCP\/IP\n176 conceiving open systems gave those protocols an inertia that OSI, with relatively few such\napplications, did not have. The most important of these by far was\nthe World Wide Web (the http protocol, the HTML mark\u00adup lan\u00ad\nguage, and implementations of both servers, such as libwww, and\nclients, such as Mosaic and Netscape). The basic components of the\nWeb were made to work on top of the TCP\/IP networks, like other\nservices that had already been designed (ftp, telnet, gopher, archie,\netc.); thus, Tim Berners\u00adLee, who co\u00adinvented the World Wide Web,\ncould also rely on the availability and openness of previous work\nfor his own protocols. In addition, Berners\u00adLee and CERN (the Eu\u00ad\nropean Organization for Nuclear Research) dedicated their work\nto the public domain more or less immediately, essentially allow\u00ad\ning anyone to do anything they wished with the system they had\ncobbled together.52 From the perspective of the tension between\nTCP\/IP and OSI, the World Wide Web was thus what engineers call\na \u201ckiller app,\u201d because its existence actually drove individuals and\ncorporations to make decisions (in favor of TCP\/IP) that it might\nnot have made otherwise.\nConclusion\nOpenness and open systems are key to understanding the prac\u00ad\ntices of Free Software: the open\u00adsystems battles of the 1980s set the\ncontext for Free Software, leaving in their wake a partially articu\u00ad\nlated infrastructure of operating systems, networks, and markets\nthat resulted from figuring out open systems. The failure to create\na standard UNIX operating system opened the door for Microsoft\nWindows NT, but it also set the stage for the emergence of the Linux\u00ad\noperating\u00adsystem kernel to emerge and spread. The success of the\nTCP\/IP protocols forced multiple competing networking schemes\ninto a single standard\u2014and a singular entity, the Internet\u2014which\ncarried with it a set of built\u00adin goals that mirror the moral\u00adtechnical\norder of Free Software.\nThis \u201cinfrastructure\u201d is at once technical (protocols and standards\nand implementations) and moral (expressing ideas about the proper\norder and organization of commercial efforts to provide high-tech\nsoftware, networks, and computing power). As with the invention\nof UNIX, the opposition commercial\u00adnoncommercial (or its dop\u00ad\npelgangers public-private, profit-nonprofit, capitalist-socialist, etc.)\nconceiving open systems 177 doesn\u2019t capture the context. Constraints on the ability to collab\u00ad\norate, compete, or withdraw are in the making here through the\ntechnical and moral imaginations of the actors involved: from the\ncorporate behemoths like IBM to (onetime) startups like Sun to\nthe independent academics and amateurs and geeks with stakes in\nthe new high\u00adtech world of networks and software.\nThe creation of a UNIX market failed. The creation of a legiti\u00ad\nmate international networking standard failed. But they were\nlocal failures only. They opened the doors to new forms of com\u00ad\nmercial practice (exemplified by Netscape and the dotcom boom)\nand new kinds of politicotechnical fractiousness (ICANN, IPv6, and\n\u201cnet neutrality\u201d). But the blind spot of open systems\u2014intellectual\nproperty\u2014at the heart of these failures also provided the impetus\nfor some geeks, entrepreneurs, and lawyers to start figuring out\nthe legal and economic aspects of Free Software, and it initiated a\nvibrant experimentation with copyright licensing and with forms\nof innovative coordination and collaboration built on top of the\nrapidly spreading protocols of the Internet.\n178 conceiving open systems 66..\nWriting Copyright Licenses\nTo protect your rights, we need to make restrictions\nthat forbid anyone to deny you these rights or to ask you\nto surrender the rights.\u2014Preamble to the GNU\nGeneral Public License\nThe use of novel, unconventional copyright licenses is, without a\ndoubt, the most widely recognized and exquisitely refined compo-\nnent of Free Software. The GNU General Public License (GPL), writ-\nten initially by Richard Stallman, is often referred to as a beautiful,\nclever, powerful \u201chack\u201d of intellectual-property law\u2014when it isn\u2019t\nbeing denounced as a viral, infectious object threatening the very\nfabric of economy and society. The very fact that something so bor-\ning, so arcane, and so legalistic as a copyright license can become\nan object of both devotional reverence and bilious scorn means\nthere is much more than fine print at stake. By the beginning of the twenty-first century, there were hundreds\nof different Free Software licenses, each with subtle legal and tech-\nnical differences, and an enormous legal literature to explain their\ndetails, motivation, and impact.1 Free Software licenses differ from\nconventional copyright licenses on software because they usually\nrestrict only the terms of distribution, while so-called End User\nLicense Agreements (EULAs) that accompany most proprietary\nsoftware restrict what users can do with the software. Ethnographi-\ncally speaking, licenses show up everywhere in the field, and contem-\nporary hackers are some of the most legally sophisticated non-lawyers\nin the world. Indeed, apprenticeship in the world of hacking is now\nimpossible, as Gabriella Coleman has shown, without a long, deep\nstudy of intellectual-property law.2\nBut how did it come to be this way? As with the example of shar-\ning UNIX source code, Free Software licenses are often explained as\na reaction to expanding intellectual-property laws and resistance\nto rapacious corporations. The text of the GPL itself begins deep in\nsuch assumptions: \u201cThe licenses for most software are designed to\ntake away your freedom to share and change it.\u201d3 But even if cor-\nporations are rapacious, sharing and modifying software are by no\nmeans natural human activities. The ideas of sharing and of com-\nmon property and its relation to freedom must always be produced\nthrough specific practices of sharing, before being defended. The\nGPL is a precise example of how geeks fit together the practices\nof sharing and modifying software with the moral and technical\norders\u2014the social imaginaries\u2014of freedom and autonomy. It is at\nonce an exquisitely precise legal document and the expression of\nan idea of how software should be made available, shareable, and\nmodifiable.\nIn this chapter I tell the story of the creation of the GPL, the first\nFree Software license, during a controversy over EMACS, a very\nwidely used and respected piece of software; the controversy con-\ncerned the reuse of bits of copyrighted source code in a version of\nEMACS ported to UNIX. There are two reasons to retell this story\ncarefully. The first is simply to articulate the details of the origin\nof the Free Software license itself, as a central component of Free\nSoftware, details that should be understood in the context of chang-\ning copyright law and the UNIX and open-systems struggles of the\n1980s. Second, although the story of the GPL is also an oft-told\nstory of the \u201chacker ethic,\u201d the GPL is not an \u201cexpression\u201d of this\n180 writing copyright licenses ethic, as if the ethic were genotype to a legal phenotype. Opposite\nthe familiar story of ethics, I explain how the GPL was \u201cfigured out\u201d\nin the controversy over EMACS, how it was formed in response to\na complicated state of affairs, both legal and technical, and in a\nmedium new to all the participants: the online mailing lists and\ndiscussion lists of Usenet and Arpanet.4\nThe story of the creation of the GNU General Public License ulti-\nmately affirms the hacker ethic, not as a story of the ethical hacker\ngenius, but as a historically specific event with a duration and a\ncontext, as something that emerges in response to the reorienta-\ntion of knowledge and power, and through the active modulation\nof existing practices among both human and nonhuman actors.\nWhile hackers themselves might understand the hacker ethic as\nan unchanging set of moral norms, their practices belie this belief\nand demonstrate how ethics and norms can emerge suddenly and\nsharply, undergo repeated transformations, and bifurcate into ideo-\nlogically distinct camps (Free Software vs. Open Source), even as\nthe practices remain stable relative to them. The hacker ethic does\nnot descend from the heights of philosophy like the categorical im-\nperative\u2014hackers have no Kant, nor do they want one. Rather, as\nManuel Delanda has suggested, the philosophy of Free Software is\nthe fact of Free Software itself, its practices and its things. If there\nis a hacker ethic, it is Free Software itself, it is the recursive public\nitself, which is much more than a list of norms.5 By understanding\nit in this way, it becomes possible to track the proliferation and\ndifferentiation of the hacker ethic into new and surprising realms,\ninstead of assuming its static universal persistence as a mere pro-\ncedure that hackers execute.\nFree Software Licenses, Once More with Feeling\nIn lecturing on liberalism in 1935, John Dewey said the following of\nJeremy Bentham: \u201cHe was, we might say, the first great muck-raker\nin the field of law . . . but he was more than that, whenever he saw\na defect, he proposed a remedy. He was an inventor in law and ad-\nministration, as much so as any contemporary in mechanical pro-\nduction.\u201d6 Dewey\u2019s point was that the liberal reforms attributed to\nBentham came not so much from his theories as from his direct in-\nvolvement in administrative and legal reform\u2014his experimentation.\nwriting copyright licenses 181 Whether or not Bentham\u2019s influence is best understood this way, it\nnonetheless captures an important component of liberal reform in\nEurope and America that is also a key component in the story of\nFree Software: that the route to achieving change is through direct\nexperiment with the system of law and administration.\nA similar story might be told of Richard Stallman, hacker hero\nand founder of the Free Software Foundation, creator of (among\nmany other things) the GNU C Compiler and GNU EMACS, two of\nthe most widely used and tested Free Software tools in the world.\nStallman is routinely abused for holding what many perceive to be\n\u201cdogmatic\u201d or \u201cintractable\u201d ideological positions about freedom\nand the right of individuals to do what they please with software.\nWhile it is no doubt quite true that his speeches and writings clearly\nbetray a certain fervor and fanaticism, it would be a mistake to\nassume that his speeches, ideas, or belligerent demands concern-\ning word choice constitute the real substance of his reform. In fact,\nit is the software he has created and the licenses he has written\nand rewritten which are the key to his Bentham-like inventiveness.\nUnlike Bentham, however, Stallman is not a creator of law and\nadministrative structure, but a hacker.\nStallman\u2019s GNU General Public License \u201chacks\u201d the federal copy-\nright law, as is often pointed out. It does this by taking advantage\nof the very strong rights granted by federal law to actually loosen\nthe restrictions normally associated with ownership. Because the\nstatutes grant owners strong powers to create restrictions, Stall-\nman\u2019s GPL contains the restriction that anybody can use the li-\ncensed material, for any purpose, so long as they subsequently offer\nthe same restriction. Hacks (after which hackers are named) are\nclever solutions to problems or shortcomings in technology. Hacks\nare work-arounds, clever, shortest-path solutions that take advan-\ntage of characteristics of the system that may or may not have been\nobvious to the people who designed it. Hacks range from purely\nutilitarian to mischievously pointless, but they always depend on\nan existing system or tool through which they achieve their point.\nTo call Free Software a hack is to point out that it would be noth-\ning without the existence of intellectual-property law: it relies on\nthe structure of U.S. copyright law (USC\u00a717) in order to subvert\nit. Free Software licenses are, in a sense, immanent to copyright\nlaws\u2014there is nothing illegal or even legally arcane about what\nthey accomplish\u2014but there is nonetheless a kind of lingering sense\n182 writing copyright licenses that this particular use of copyright was not how the law was in-\ntended to function.\nLike all software since the 1980 copyright amendments, Free\nSoftware is copyrightable\u2014and what\u2019s more, automatically copy-\nrighted as it is written (there is no longer any requirement to reg-\nister). Copyright law grants the author (or the employer of the\nauthor) a number of strong rights over the dispensation of what has\nbeen written: rights to copy, distribute, and change the work.7 Free\nSoftware\u2019s hack is to immediately make use of these rights in order\nto abrogate the rights the programmer has been given, thus grant-\ning all subsequent licensees rights to copy, distribute, modify, and\nuse the copyrighted software. Some licenses, like the GPL, add the\nfurther restriction that every licensee must offer the same terms to\nany subsequent licensee, others make no such restriction on subse-\nquent uses. Thus, while statutory law suggests that individuals need\nstrong rights and grants them, Free Software licenses effectively\nannul them in favor of other activities, such as sharing, porting,\nand forking software. It is for this reason that they have earned the\nname \u201ccopyleft.\u201d8\nThis is a convenient ex post facto description, however. Neither\nStallman nor anyone else started out with the intention of hack-\ning copyright law. The hack of the Free Software licenses was a\nresponse to a complicated controversy over a very important inven-\ntion, a tool that in turn enabled an invention called EMACS. The\nstory of the controversy is well-known among hackers and geeks,\nbut not often told, and not in any rich detail, outside of these small\ncircles.9\nEMACS, the Extensible, Customizable,\nSelf-documenting, Real-time Display Editor\nEMACS is a text editor; it is also something like a religion. As one\nof the two most famous text editors, it is frequently lauded by its\ndevoted users and attacked by detractors who prefer its competitor\n(Bill Joy\u2019s vi, also created in the late 1970s). EMACS is more than\njust a tool for writing text; for many programmers, it was (and still\nis) the principal interface to the operating system. For instance,\nit allows a programmer not only to write a program but also to\ndebug it, to compile it, to run it, and to e-mail it to another user,\nwriting copyright licenses 183 all from within the same interface. What\u2019s more, it allows users to\nquickly and easily write extensions to EMACS itself, extensions that\nautomate frequent tasks and in turn become core features of the\nsoftware. It can do almost anything, but it can also frustrate almost\nanyone. The name itself is taken from its much admired extensibil-\nity: EMACS stands for \u201cediting macros\u201d because it allows program-\nmers to quickly record a series of commands and bundle them into\na macro that can be called with a simple key combination. In fact,\nit was one of the first editors (if not the first) to take advantage of\nkeys like ctrl and meta, as in the now ubiquitous ctrl-S familiar to\nusers of non-free word processors like Microsoft Word\u2122.\nAppreciate the innovation represented by EMACS: before the\nUNIX-dominated minicomputer era, there were very few programs\nfor directly manipulating text on a display. To conceive of source\ncode as independent of a program running on a machine meant\nfirst conceiving of it as typed, printed, or hand-scrawled code which\nprogrammers would scrutinize in its more tangible, paper-based\nform. Editors that allowed programmers to display the code in front\nof them on a screen, to manipulate it directly, and to save changes\nto those files were an innovation of the mid- to late 1960s and were\nnot widespread until the mid-1970s (and this only for bleeding-\nedge academics and computer corporations). Along with a few\nearly editors, such as QED (originally created by Butler Lampson\nand Peter Deutsch, and rewritten for UNIX by Ken Thompson), one\nof the most famous of these was TECO (text editor and correc-\ntor), written by Dan Murphy for DEC\u2019s PDP-1 computer in 1962\u201363.\nOver the years, TECO was transformed (ported and extended) to\na wide variety of machines, including machines at Berkeley and\nMIT, and to other DEC hardware and operating systems. By the\nearly 1970s, there was a version of TECO running on the Incompat-\nible Time-sharing System (ITS), the system in use at MIT\u2019s Artificial\nIntelligence (AI) Lab, and it formed the basis for EMACS. (Thus,\nEMACS was itself conceived of as a series of macros for a separate\neditor: Editing MACroS for TECO.)\nLike all projects on ITS at the AI Lab, many people contributed\nto the extension and maintenance of EMACS (including Guy Steele,\nDave Moon, Richard Greenblatt, and Charles Frankston), but there\nis a clear recognition that Stallman made it what it was. The earli-\nest AI Lab memo on EMACS, by Eugene Ciccarelli, says: \u201cFinally, of\nall the people who have contributed to the development of EMACS,\n184 writing copyright licenses and the TECO behind it, special mention and appreciation go to\nRichard M. Stallman. He not only gave TECO the power and gen-\nerality it has, but brought together the good ideas of many different\nTeco-function packages, added a tremendous amount of new ideas\nand environment, and created EMACS. Personally one of the joys\nof my avocational life has been writing Teco\/EMACS functions;\nwhat makes this fun and not painful is the rich set of tools to work\nwith, all but a few of which have an \u2018RMS\u2019 chiseled somewhere on\nthem.\u201d10\nAt this point, in 1978, EMACS lived largely on ITS, but its repu-\ntation soon spread, and it was ported to DEC\u2019s TOPS-20 (Twenex)\noperating system and rewritten for Multics and the MIT\u2019s LISP ma-\nchine, on which it was called EINE (Eine Is Not EMACS), and which\nwas followed by ZWEI (Zwei Was Eine Initially).\nThe proliferation of EMACS was both pleasing and frustrating to\nStallman, since it meant that the work fragmented into different\nprojects, each of them EMACS-like, rather than building on one\ncore project, and in a 1981 report he said, \u201cThe proliferation of\nsuch superficial facsimiles of EMACS has an unfortunate confusing\neffect: their users, not knowing that they are using an imitation of\nEMACS and never having seen EMACS itself, are led to believe they\nare enjoying all the advantages of EMACS. Since any real-time dis-\nplay editor is a tremendous improvement over what they probably\nhad before, they believe this readily. To prevent such confusion, we\nurge everyone to refer to a nonextensible imitation of EMACS as an\n\u2018ersatz EMACS.\u2019 \u201d11\nThus, while EMACS in its specific form on ITS was a creation of\nStallman, the idea of EMACS or of any \u201creal-time display editor\u201d\nwas proliferating in different forms and on different machines. The\nporting of EMACS, like the porting of UNIX, was facilitated by both\nits conceptual design integrity and its widespread availability.\nThe phrase \u201cnonextensible imitation\u201d captures the combination\nof design philosophy and moral philosophy that EMACS repre-\nsented. Extensibility was not just a useful feature for the individual\ncomputer user; it was a way to make the improvements of each new\nuser easily available equally to all by providing a standard way for\nusers to add extensions and to learn how to use new extensions that\nwere added (the \u201cself-documenting\u201d feature of the system). The\nprogram had a conceptual integrity that was compromised when it\nwas copied imperfectly. EMACS has a modular, extensible design\nwriting copyright licenses 185 that by its very nature invites users to contribute to it, to extend it,\nand to make it perform all manner of tasks\u2014to literally copy and\nmodify it, instead of imitating it. For Stallman, this was not only\na fantastic design for a text editor, but an expression of the way\nhe had always done things in the small-scale setting of the AI Lab.\nThe story of Stallman\u2019s moral commitments stresses his resistance\nto secrecy in software production, and EMACS is, both in its design\nand in Stallman\u2019s distribution of it an example of this resistance.\nNot everyone shared Stallman\u2019s sense of communal order, how-\never. In order to facilitate the extension of EMACS through sharing,\nStallman started something he called the \u201cEMACS commune.\u201d At\nthe end of the 1981 report\u2014\u201cEMACS: The Extensible, Customizable\nSelf-documenting Display Editor,\u201d dated 26 March\u2014he explained\nthe terms of distribution for EMACS: \u201cIt is distributed on a basis of\ncommunal sharing, which means that all improvements must be\ngiven back to me to be incorporated and distributed. Those who\nare interested should contact me. Further information about how\nEMACS works is available in the same way.\u201d12\nIn another report, intended as a user\u2019s manual for EMACS, Stall-\nman gave more detailed and slightly more colorful instructions:\nEMACS does not cost anything; instead, you are joining the EMACS\nsoftware-sharing commune. The conditions of membership are that\nyou must send back any improvements you make to EMACS, includ-\ning any libraries you write, and that you must not redistribute the\nsystem except exactly as you got it, complete. (You can also distribute\nyour customizations, separately.) Please do not attempt to get a copy\nof EMACS, for yourself or anyone else, by dumping it off of your lo-\ncal system. It is almost certain to be incomplete or inconsistent. It is\npathetic to hear from sites that received incomplete copies lacking\nthe sources [source code], asking me years later whether sources are\navailable. (All sources are distributed, and should be on line at every\nsite so that users can read them and copy code from them). If you wish\nto give away a copy of EMACS, copy a distribution tape from MIT, or\nmail me a tape and get a new one.13\nBecause EMACS was so widely admired and respected, Stallman\nhad a certain amount of power over this commune. If it had been\nan obscure, nonextensible tool, useful for a single purpose, no one\nwould have heeded such demands, but because EMACS was by na-\nture the kind of tool that was both useful for all kinds of tasks and\n186 writing copyright licenses customizable for specific ones, Stallman was not the only person\nwho benefited from this communal arrangement. Two disparate\nsites may well have needed the same macro extension, and there-\nfore many could easily see the social benefit in returning exten-\nsions for inclusion, as well as in becoming a kind of co-developer\nof such a powerful system. As a result, the demands of the EMACS\ncommune, while unusual and autocratic, were of obvious value to\nthe flock. In terms of the concept of recursive public, EMACS was\nitself the tool through which it was possible for users to extend\nEMACS, the medium of their affinity; users had a natural incentive\nto share their contributions so that all might receive the maximum\nbenefit.\nThe terms of the EMACS distribution agreement were not quite\nlegally binding; nothing compelled participation except Stallman\u2019s\nreputation, his hectoring, or a user\u2019s desire to reciprocate. On the\none hand, Stallman had not yet chosen to, or been forced to, under-\nstand the details of the legal system, and so the EMACS commune\nwas the next best thing. On the other hand, the state of intellectual-\nproperty law was in great flux at the time, and it was not clear to\nanyone, whether corporate or academic, exactly what kind of legal\narrangements would be legitimate (the 1976 changes to copyright\nlaw were some of the most drastic in seventy years, and a 1980\namendment made software copyrightable, but no court cases had\nyet tested these changes). Stallman\u2019s \u201cagreement\u201d was a set of in-\nformal rules that expressed the general sense of mutual aid that\nwas a feature of both the design of the system and Stallman\u2019s own\nexperience at the AI Lab. It was an expression of the way Stallman\nexpected others to behave, and his attempts to punish or shame\npeople amounted to informal enforcement of these expectations.\nThe small scale of the community worked in Stallman\u2019s favor.\nAt its small scale, Stallman\u2019s commune was confronting many of\nthe same issues that haunted the open-systems debates emerging\nat the same time, issues of interoperability, source-code sharing,\nstandardization, portability, and forking. In particular, Stallman\nwas acutely aware of the blind spot of open systems: the conflict of\nmoral-technical orders represented by intellectual property. While\nUNIX vendors left intellectual-property rules unchallenged and\nsimply assumed that they were the essential ground rules of debate,\nStallman made them the substance of his experiment and, like Ben-\ntham, became something of a legal muckraker as a result.\nwriting copyright licenses 187 Stallman\u2019s communal model could not completely prevent the\nporting and forking of software. Despite Stallman\u2019s request that\nimitators refer to their versions of EMACS as ersatz EMACS, few\ndid. In the absence of legal threats over a trademarked term there\nwas not much to stop people from calling their ports and forks\nEMACS, a problem of success not unlike that of Kleenex or Xerox.\nFew people took the core ideas of EMACS, implemented them in an\nimitation, and then called it something else (EINE and ZWEI were\nexceptions). In the case of UNIX the proliferation of forked versions\nof the software did not render them any less UNIX, even when\nAT&T insisted on ownership of the trademarked name. But as time\nwent on, EMACS was ported, forked, rewritten, copied, or imitated\non different operating systems and different computer architectures\nin universities and corporations around the world; within five or six\nyears, many versions of EMACS were in wide use. It was this situ-\nation of successful adoption that would provide the context for the\ncontroversy that occurred between 1983 and 1985.\nThe Controversy\nIn brief the controversy was this: in 1983 James Gosling decided to\nsell his version of EMACS\u2014a version written in C for UNIX called\nGOSMACS\u2014to a commercial software vendor called Unipress.\nGOSMACS, the second most famous implementation of EMACS\n(after Stallman\u2019s itself ), was written when Gosling was a graduate\nstudent at Carnegie Mellon University. For years, Gosling had dis-\ntributed GOSMACS by himself and had run a mailing list on Usenet,\non which he answered queries and discussed extensions. Gosling\nhad explicitly asked people not to redistribute the program, but to\ncome back to him (or send interested parties to him directly) for\nnew versions, making GOSMACS more of a benevolent dictatorship\nthan a commune. Gosling maintained his authority, but graciously\naccepted revisions and bug-fixes and extensions from users, incor-\nporating them into new releases. Stallman\u2019s system, by contrast,\nallowed users to distribute their extensions themselves, as well\nas have them included in the \u201cofficial\u201d EMACS. By 1983, Gosling\nhad decided he was unable to effectively maintain and support\nGOSMACS\u2014a task he considered the proper role of a corporation.\n188 writing copyright licenses For Stallman, Gosling\u2019s decision to sell GOSMACS to Unipress\nwas \u201csoftware sabotage.\u201d Even though Gosling had been substan-\ntially responsible for writing GOSMACS, Stallman felt somewhat\nproprietorial toward this ersatz version\u2014or, at the very least, was\nirked that no noncommercial UNIX version of EMACS existed. So\nStallman wrote one himself (as part of a project he announced\naround the same time, called GNU [GNU\u2019s Not UNIX], to create a\ncomplete non-AT&T version of UNIX). He called his version GNU\nEMACS and released it under the same EMACS commune terms.\nThe crux of the debate hinged on the fact that Stallman used, albeit\nostensibly with permission, a small piece of Gosling\u2019s code in his\nnew version of EMACS, a fact that led numerous people, including\nthe new commercial suppliers of EMACS, to cry foul. Recrimina-\ntions and legal threats ensued and the controversy was eventually\nresolved when Stallman rewrote the offending code, thus creating\nan entirely \u201cGosling-free\u201d version that went on to become the stan-\ndard UNIX version of EMACS.\nThe story raises several questions with respect to the changing\nlegal context. In particular, it raises questions about the difference\nbetween \u201claw on the books\u201d and \u201claw in action,\u201d that is, the dif-\nference between the actions of hackers and commercial entities,\nadvised by lawyers and legally minded friends, and the text and\ninterpretation of statutes as they are written by legislators and in-\nterpreted by courts and lawyers. The legal issues span trade secret,\npatent, and trademark, but copyright is especially significant. Three\nissues were undecided at the time: the copyrightability of software,\nthe definition of what counts as software and what doesn\u2019t, and\nthe meaning of copyright infringement. While the controversy did\nnot resolve any of these issues (the first two would be resolved by\nCongress and the courts, the third remains somewhat murky), it did\nclarify the legal issues for Stallman sufficiently that he could leave\nbehind the informal EMACS commune and create the first version\nof a Free Software license, the GNU General Public License, which\nfirst started appearing in 1985.\nGosling\u2019s decision to sell GOSMACS, announced in April of 1983,\nplayed into a growing EMACS debate being carried out on the\nGOSMACS mailing list, a Usenet group called net.emacs. Since net\n.emacs was forwarded to the Arpanet via a gateway maintained\nby John Gilmore at Sun Microsystems, a fairly large community\nwriting copyright licenses 189 of EMACS users were privy to Gosling\u2019s announcement. Prior to\nhis declaration, there had been quite a bit of discussion regarding\ndifferent versions of EMACS, including an already \u201ccommercial\u201d\nversion called CCA EMACS, written by Steve Zimmerman, of Com-\nputer Corporation of America (CCA).14 Some readers wanted com-\nparisons between CCA EMACS and GOSMACS; others objected that\nit was improper to discuss a commercial version on the list: was\nsuch activity legitimate, or should it be carried out as part of the\ncommercial company\u2019s support activities? Gosling\u2019s announcement\nwas therefore a surprise, since it was already perceived to be the\n\u201cnoncommercial\u201d version.\nDate: Tue Apr 12 04:51:12 1983\nSubject: EMACS goes commercial\nThe version of EMACS that I wrote is now available commercially\nthrough a company called Unipress. . . . They will be doing develop-\nment, maintenance and will be producing a real manual. EMACS will\nbe available on many machines (it already runs on VAXen under Unix\nand VMS, SUNs, codatas, and Microsoft Xenix). Along with this, I\nregret to say that I will no longer be distributing it.\nThis is a hard step to take, but I feel that it is necessary. I can no\nlonger look after it properly, there are too many demands on my time.\nEMACS has grown to be completely unmanageable. Its popularity has\nmade it impossible to distribute free: just the task of writing tapes and\nstuffing them into envelopes is more than I can handle.\nThe alternative of abandoning it to the public domain is unaccept-\nable. Too many other programs have been destroyed that way.\nPlease support these folks. The effort that they can afford to put into\nlooking after EMACS is directly related to the support they get. Their\nprices are reasonable.\nJames.15\nThe message is worth paying careful attention to: Gosling\u2019s work\nof distributing the tapes had become \u201cunmanageable,\u201d and the work\nof maintenance, upkeep, and porting (making it available on mul-\ntiple architectures) is something he clearly believes should be done\nby a commercial enterprise. Gosling, it is clear, did not understand\nhis effort in creating and maintaining EMACS to have emerged\nfrom a communal sharing of bits of code\u2014even if he had done\na Sisyphean amount of work to incorporate all the changes and\nsuggestions his users had made\u2014but he did long have a commit-\n190 writing copyright licenses ment to distributing it for free, a commitment that resulted in many\npeople contributing bits and pieces to GOSMACS.\n\u201cFree,\u201d however, did not mean \u201cpublic domain,\u201d as is clear from\nhis statement that \u201cabandoning it\u201d to the public domain would\ndestroy the program. The distinction is an important one that was,\nand continues to be, lost on many sophisticated members of net\n.emacs. Here, free means without charge, but Gosling had no in-\ntention of letting that word suggest that he was not the author,\nowner, maintainer, distributor, and sole beneficiary of whatever\nvalue GOSMACS had. Public domain, by contrast, implied giving\nup all these rights.16 His decision to sell GOSMACS to Unipress was\na decision to transfer these rights to a company that would then\ncharge for all the labor he had previously provided for no charge\n(for \u201cfree\u201d). Such a distinction was not clear to everyone; many\npeople considered the fact that GOSMACS was free to imply that\nit was in the public domain.17 Not least of these was Richard Stall-\nman, who referred to Gosling\u2019s act as \u201csoftware sabotage\u201d and\nurged people to avoid using the \u201csemi-ersatz\u201d Unipress version.18\nTo Stallman, the advancing commercialization of EMACS, both\nby CCA and by Unipress, was a frustrating state of affairs. The com-\nmercialization of CCA had been of little concern so long as GOS-\nMACS remained free, but with Gosling\u2019s announcement, there was\nno longer a UNIX version of EMACS available. To Stallman, how-\never, \u201cfree\u201d meant something more than either \u201cpublic domain\u201d or\n\u201cfor no cost.\u201d The EMACS commune was designed to keep EMACS\nalive and growing as well as to provide it for free\u2014it was an image\nof community stewardship, a community that had included Gosling\nuntil April 1983.\nThe disappearance of a UNIX version of EMACS, as well as the\nsudden commercial interest in making UNIX into a marketable op-\nerating system, fed into Stallman\u2019s nascent plan to create a com-\npletely new, noncommercial, non-AT&T UNIX operating system\nthat he would give away free to anyone who could use it. He an-\nnounced his intention on 27 September 1983:19\nFree Unix!\nStarting this Thanksgiving I am going to write a complete Unix-\ncompatible software system called GNU (for Gnu\u2019s Not Unix), and give\nit away free to everyone who can use it. Contributions of time, money,\nprograms and equipment are greatly needed.\nwriting copyright licenses 191 His justifications were simple.\nWhy I Must Write GNU\nI consider that the golden rule requires that if I like a program I must\nshare it with other people who like it. I cannot in good conscience sign\na nondisclosure agreement or a software license agreement.\nSo that I can continue to use computers without violating my prin-\nciples, I have decided to put together a sufficient body of free software\nso that I will be able to get along without any software that is not\nfree.20\nAt that point, it is clear, there was no \u201cfree software license.\u201d\nThere was the word free, but not the term public domain. There was\nthe \u201cgolden rule,\u201d and there was a resistance to nondisclosure and\nlicense arrangements in general, but certainly no articulated con-\nception of copyleft of Free Software as a legally distinct entity. And\nyet Stallman hardly intended to \u201cabandon it\u201d to the public domain,\nas Gosling suggested. Instead, Stallman likely intended to require\nthe same EMACS commune rules to apply to Free Software, rules\nthat he would be able to control largely by overseeing (in a non-\nlegal sense) who was sent or sold what and by demanding (in the\nform of messages attached to the software) that any modifications\nor improvements come in the form of donations. It was during the\nperiod 1983\u201385 that the EMACS commune morphed into the GPL,\nas Stallman began adding copyrights and appending messages that\nmade explicit what people could do with the software.21\nThe GNU project initially received little attention, however; scat-\ntered messages to net.unix-wizards over the course of 1983\u201384 pe-\nriodically ask about the status and how to contact them, often in\nthe context of discussions of AT&T UNIX licensing practices that\nwere unfolding as UNIX was divested and began to market its own\nversion of UNIX.22 Stallman\u2019s original plan for GNU was to start\nwith the core operating system, the kernel, but his extensive work\non EMACS and the sudden need for a free EMACS for UNIX led him\nto start with a UNIX version of EMACS. In 1984 and into 1985, he\nand others began work on a UNIX version of GNU EMACS. The two\ncommercial versions of UNIX EMACS (CCA EMACS and Unipress\nEMACS) continued to circulate and improve in parallel. DEC us-\ners meanwhile used the original free version created by Stallman.\nAnd, as often happens, life went on: Zimmerman left CCA in Au-\n192 writing copyright licenses gust 1984, and Gosling moved to Sun, neither of them remaining\nclosely involved in the software they had created, but leaving the\nnew owners to do so.\nBy March 1985, Stallman had a complete version (version 15) of\nGNU EMACS running on the BSD 4.2 version of UNIX (the version\nBill Joy had helped create and had taken with him to form the core\nof Sun\u2019s version of UNIX), running on DEC\u2019s VAX computers. Stall-\nman announced this software in a characteristically flamboyant\nmanner, publishing in the computer programmers\u2019 monthly maga-\nzine Dr. Dobbs an article entitled \u201cThe GNU Manifesto.\u201d23\nStallman\u2019s announcement that a free version of UNIX EMACS\nwas available caused some concern among commercial distribu-\ntors. The main such concern was that GNU EMACS 15.34 contained\ncode marked \u201cCopyright (c) James Gosling,\u201d code used to make\nEMACS display on screen.24 The \u201cdiscovery\u201d (not so difficult, since\nStallman always distributed the source code along with the binary)\nthat this code had been reused by Stallman led to extensive dis-\ncussion among EMACS users of issues such as the mechanics of\ncopyright, the nature of infringement, the definition of software,\nthe meaning of public domain, the difference between patent, copy-\nright, and trade secret, and the mechanics of permission and its\ngranting\u2014in short, a discussion that would be repeatedly recapitu-\nlated in nearly every software and intellectual property controversy\nin the future.\nThe story of the controversy reveals the structure of rumor on\nthe Usenet to be a bit like the child\u2019s game of Chinese Whispers,\nexcept that the translations are all archived. GNU EMACS 15.34\nwas released in March 1985. Between March and early June there\nwas no mention of its legal status, but around June 3 messages\non the subject began to proliferate. The earliest mention of the is-\nsue appeared not on net.emacs, but on fa.info-vax\u2014a newsgroup\ndevoted to discussions of VAX computer systems (\u201cfa\u201d stands for\n\u201cfrom Arpanet\u201d)\u2014and it included a dialogue, between Ron Natalie\nand Marty Sasaki, labeled \u201cGNU EMACS: How Public Domain?\u201d:\n\u201cFOO, don\u2019t expect that GNU EMACS is really in the public domain.\nUNIPRESS seems rather annoyed that there are large portions of it\nthat are marked copyright James Gosling.\u201d25 This message was re-\nprinted on 4 June 1985 on net.emacs, with the addendum: \u201cRMS\u2019s\nwork is based on a version of Gosling code that existed before Uni-\npress got it. Gosling had put that code into the public domain. Any\nwriting copyright licenses 193 work taking off from the early Gosling code is therefore also public\ndomain.\u201d26\nThe addendum was then followed by an extensive reply from\nZimmerman, whose CCA EMACS had been based on Warren Mont-\ngomery\u2019s Bell Labs EMACS but rewritten to avoid reusing the code,\nwhich may account for why his understanding of the issue seems to\nhave been both deep and troubling for him.\nThis is completely contrary to Gosling\u2019s public statements. Before he\nmade his arrangements with Unipress, Gosling\u2019s policy was that he\nwould send a free copy of his EMACS to anyone who asked, but he did\nnot (publicly, at least) give anyone else permission to make copies.\nOnce Unipress started selling Gosling\u2019s EMACS, Gosling stopped dis-\ntributing free copies and still did not grant anyone else permission\nto make them; instead, he suggested that people buy EMACS from\nUnipress. All versions of Gosling\u2019s EMACS distributed by him carry\nhis copyright notice, and therefore none of them are in the public\ndomain. Removing copyright notices without the author\u2019s permission\nis, of course, illegal. Now, a quick check of my GNU EMACS sources\nshows that sure enough, a number of files have Gosling\u2019s copyright\nnotice in them. What this all means is that unless RMS got written per-\nmission from Gosling to distribute his code, all copies of GNU EMACS\nconstitute violations of the copyright law. All those people making\nsuch copies, including those people who allow them to be copied off\ntheir machines, could each be liable for large sums of money. I think\nthat RMS had better tell us if he has Gosling\u2019s written permission to\nmake these copies. If so, why has he not stated this earlier (preferably\nin the distribution itself ) and thereby cleared up a potentially major\npoint of confusion? If not, why has he gone ahead and made many,\nmany people liable for criminal prosecution by recommending that\nthey distribute this code without even warning them of their liability?\n(People who distribute this code would be liable even if they claim\nthat they didn\u2019t see Gosling\u2019s notices; the fact that the notices are there\nis sufficient. \u201cIgnorance of the law is no excuse.\u201d)\nNow, I have nothing against free software; it\u2019s a free country and\npeople can do what they want. It\u2019s just that people who do distribute\nfree software had better be sure that they have the legal right to do so,\nor be prepared to face the consequences. (Jun 9, 1985).27\nStallman replied the next day.\n194 writing copyright licenses Nobody has any reason to be afraid to use or distribute GNU EMACS.\nIt is well known that I do not believe any software is anyone\u2019s prop-\nerty. However, for the GNU project, I decided it was necessary to obey\nthe law. I have refused to look at code I did not have permission to\ndistribute. About 5% of GNU EMACS is close to (though quite a bit\nchanged from) an old version of Gosling EMACS. I am distributing\nit for Fen Labalme, who received permission from Gosling to distrib-\nute it. It is therefore legal for me to do so. To be scrupulously legal,\nI put statements at the front of the files concerned, describing this\nsituation.\nI don\u2019t see anything I should warn people about\u2014except that Zim-\nmerman is going to try to browbeat them.28\nStallman\u2019s original defense for using Gosling\u2019s code was that he\nhad permission to do so. According to him, Fen Labalme had received\nwritten permission\u2014whether to make use of or to redistribute is not\nclear\u2014the display code that was included in GNU EMACS 15.34.\nAccording to Stallman, versions of Labalme\u2019s version of Gosling\u2019s\nversion of EMACS were in use in various places (including at La-\nbalme\u2019s employer, Megatest), and Stallman and Labalme consid-\nered this a legally defensible position.29\nOver the next two weeks, a slew of messages attempted to pick\napart and understand the issues of copyright, ownership, distri-\nbution, and authorship. Gosling wrote to clarify that GOSMACS\nhad never been in the public domain, but that \u201cunfortunately, two\nmoves have left my records in a shambles,\u201d and he is therefore silent\non the question of whether he granted permission.30 Gosling\u2019s claim\ncould well be strategic: giving permission, had he done so, might\nhave angered Unipress, which expected exclusive control over the\nversion he had sold; by the same token, he may well have approved\nof Stallman\u2019s re-creation, but not have wanted to affirm this in any\nlegally actionable way. Meanwhile, Zimmerman relayed an anony-\nmous message suggesting that some lawyers somewhere found the\n\u201cthird hand redistribution\u201d argument was legally \u201call wet.\u201d31\nStallman\u2019s biggest concern was not so much the legality of his\nown actions as the possibility that people would choose not to use\nthe software because of legal threats (even if such threats were is-\nsued only as rumors by former employees of companies that distrib-\nuted software they had written). Stallman wanted users not only\nwriting copyright licenses 195 to feel safe using his software but to adopt his view that software\nexists to be shared and improved and that anything that hinders this\nis a loss for everyone, which necessitates an EMACS commune.\nStallman\u2019s legal grounds for using Gosling\u2019s code may or may not\nhave been sound. Zimmerman did his best throughout to explain in\ndetail what kind of permission Stallman and Labalme would have\nneeded, drawing on his own experience with the CCA lawyers and\nAT&T Bell Labs, all the while berating Stallman for not creating\nthe display code himself. Meanwhile, Unipress posted an official\nmessage that said, \u201cUniPress wants to inform the community that\nportions of the GNU EMACS program are most definitely not pub-\nlic domain, and that use and\/or distribution of the GNU EMACS\nprogram is not necessarily proper.\u201d32 The admittedly vague tone\nof the message left most people wondering what that meant\u2014and\nwhether Unipress intended to sue anyone. Strategically speaking,\nthe company may have wished to maintain good will among hack-\ners and readers of net.emacs, an audience likely composed of many\npotential customers. Furthermore, if Gosling had given permission\nto Stallman, then Unipress would themselves have been on uncer-\ntain legal ground, unable to firmly and definitively threaten users\nof GNU EMACS with legal action. In either case, the question of\nwhether or not permission was needed was not in question\u2014only\nthe question of whether it had been granted.33\nHowever, a more complicated legal issue also arose as a result,\none concerning the status of code contributed to Gosling by others.\nFen Labalme wrote a message to net.emacs, which, although it did\nnot clarify the legal status of Gosling\u2019s code (Labalme was also\nunable to find his \u201cpermission\u201d from Gosling), did raise a related\nissue: the fact that he and others had made significant contribu-\ntions to GOSMACS, which Gosling had incorporated into his ver-\nsion, then sold to Unipress without their permission: \u201cAs one of the\n\u2018others\u2019 who helped to bring EMACS [GOSMACS] up to speed, I was\ndistressed when Jim sold the editor to UniPress. This seemed to be\na direct violation of the trust that I and others had placed in Jim\nas we sent him our improvements, modifications, and bug fixes. I\nam especially bothered by the general mercenary attitude surround-\ning EMACS which has taken over from the once proud \u2018hacker\u2019 ethic\n\u2014EMACS is a tool that can make all of our lives better. Let\u2019s help it\nto grow!\u201d34\n196 writing copyright licenses Labalme\u2019s implication, though he may not even have realized\nthis himself, is that Gosling may have infringed on the rights of\nothers in selling the code to Unipress, as a separate message from\nJoaquim Martillo makes clear: \u201cThe differences between current\nversion of Unipress EMACS and Gnu EMACS display.c (a 19 page\nmodule) is about 80%. For all the modules which Fen LeBalme [sic]\ngave RMS permission to use, the differences are similar. Unipress is\nnot even using the disputed software anymore! Now, these modules\ncontain code people like Chris Torek and others contributed when\nGosling\u2019s emacs was in the public domain. I must wonder whether\nthese people would have contributed had they known their freely-\ngiven code was going to become part of someone\u2019s product.\u201d35\nIndeed, the general irony of this complicated situation was cer-\ntainly not as evident as it might have been given the emotional\ntone of the debates: Stallman was using code from Gosling based on\npermission Gosling had given to Labalme, but Labalme had written\ncode for Gosling which Gosling had commercialized without telling\nLabalme\u2014conceivably, but not likely, the same code. Furthermore,\nall of them were creating software that had been originally con-\nceived in large part by Stallman (but based on ideas and work on\nTECO, an editor written twenty years before EMACS), who was\nnow busy rewriting the very software Gosling had rewritten for\nUNIX. The \u201conce proud hacker ethic\u201d that Labalme mentions would\nthus amount not so much to an explicit belief in sharing so much as\na fast-and-loose practice of making contributions and fixes without\ndocumenting them, giving oral permission to use and reuse, and\n\u201closing\u201d records that may or may not have existed\u2014hardly a noble\nenterprise.\nBut by 27 June 1985, all of the legal discussion was rendered\nmoot when Stallman announced that he would completely rewrite\nthe display code in EMACS.\nI have decided to replace the Gosling code in GNU EMACS, even\nthough I still believe Fen and I have permission to distribute that code,\nin order to keep people\u2019s confidence in the GNU project.\nI came to this decision when I found, this night, that I saw how to\nrewrite the parts that had seemed hard. I expect to have the job done\nby the weekend.36\nOn 5 July, Stallman sent out a message that said:\nwriting copyright licenses 197 Celebrate our independence from Unipress!\nEMACS version 16, 100% Gosling-free, is now being tested at sev-\neral places. It appears to work solidly on Vaxes, but some other ma-\nchines have not been tested yet.37\nThe fact that it only took one week to create the code is a testa-\nment to Stallman\u2019s widely recognized skills in creating great soft-\nware\u2014it doesn\u2019t appear to have indicated any (legal) threat or\nurgency. Indeed, even though Unipress seems also to have been\nconcerned about their own reputation, and despite the implication\nmade by Stallman that they had forced this issue to happen, they\ntook a month to respond. At that point, the Unipress employee Mike\nGallaher wrote to insist, somewhat after the fact, that Unipress\nhad no intention of suing anyone\u2014as long as they were using the\nGosling-free EMACS version 16 and higher.\nUniPress has no quarrel with the Gnu project. It bothers me that peo-\nple seem to think we are trying to hinder it. In fact, we hardly did or\nsaid much at all, except to point out that the Gnumacs code had James\nGosling\u2019s copyright in it. We have not done anything to keep anyone\nfrom using Gnumacs, nor do we intend to now that it is \u201cGosling-free\u201d\n(version 16.56).\nYou can consider this to be an official statement from UniPress:\nThere is nothing in Gnumacs version 16.56 that could possibly cause\nUniPress to get upset. If you were afraid to use Gnumacs because\nyou thought we would hassle you, don\u2019t be, on the basis of version\n16.56.38\nBoth Stallman and Unipress received various attacks and de-\nfenses from observers of the controversy. Many people pointed out\nthat Stallman should get credit for \u201cinventing\u201d EMACS and that the\nissue of him infringing on his own invention was therefore ironic.\nOthers proclaimed the innocence and moral character of Unipress,\nwhich, it was claimed, was providing more of a service (support for\nEMACS) than the program itself.\nSome readers interpreted the fact that Stallman had rewritten the\ndisplay code, whether under pressure from Unipress or not, as con-\nfirmation of the ideas expressed in \u201cThe GNU Manifesto,\u201d namely,\nthat commercial software stifles innovation. According to this logic,\nprecisely because Stallman was forced to rewrite the code, rather\nthan build on something that he himself assumed he had permis-\n198 writing copyright licenses sion to do, there was no innovation, only fear-induced caution.39\nOn the other hand, latent within this discussion is a deep sense of\npropriety about what people had created; many people, not only\nStallman and Gosling and Zimmerman, had contributed to making\nEMACS what it was, and most had done so under the assumption,\nlegally correct or not, that it would not be taken away from them\nor, worse, that others might profit by it.\nGosling\u2019s sale of EMACS is thus of a different order from his par-\nticipation in the common stewardship of EMACS. The distinction\nbetween creating software and maintaining it is a commercial fic-\ntion driven in large part by the structure of intellectual property. It\nmirrors the experience of open systems. Maintaining software can\nmean improving it, and improving it can mean incorporating the\noriginal work and ideas of others. To do so by the rules of a chang-\ning intellectual-property structure forces different choices than to\ndo so according to an informal hacker ethic or an experimental\n\u201ccommune.\u201d One programmer\u2019s minor improvement is another\nprogrammer\u2019s original contribution.\nThe Context of Copyright\nThe EMACS controversy occurred in a period just after some of the\nlargest changes to U.S. intellectual-property law in seventy years.\nTwo aspects of this context are worth emphasizing: (1) practices\nand knowledge about the law change slowly and do not immedi-\nately reflect the change in either the law or the strategies of actors;\n(2) U.S. law creates a structural form of uncertainty in which the\ninterplay between legislation and case law is never entirely cer-\ntain. In the former aspect, programmers who grew up in the 1970s\nsaw a commercial practice entirely dominated by trade secret and\npatent protection, and very rarely by copyright; thus, the shift to\nwidespread use of copyright law (facilitated by the 1976 and 1980\nchanges to the law) to protect software was a shift in thinking that\nonly slowly dawned on many participants, even the most legally\nastute, since it was a general shift in strategy as well as a statu-\ntory change. In the latter aspect, the 1976 and 1980 changes to the\ncopyright law contained a number of uncertainties that would take\nover a decade to be worked out in case law, issues such as the copy-\nrightability of software, the definition of software, and the meaning\nwriting copyright licenses 199 of infringement in software copyright, to say nothing of the impact\nof the codification of fair use and the removal of the requirement to\nregister (issues that arguably went unnoticed until the turn of the\nmillennium). Both aspects set the stage for the EMACS controversy\nand Stallman\u2019s creation of the GPL.\nLegally speaking, the EMACS controversy was about copyright,\npermission, and the meanings of a public domain and the reuse of\nsoftware (and, though never explicitly mentioned, fair use). Soft-\nware patenting and trade-secret law are not directly concerned,\nbut they nonetheless form a background to the controversy. Many\nof the participants expressed a legal and conventional orthodoxy\nthat software was not patentable, that is, that algorithms, ideas,\nor fundamental equations fell outside the scope of patent, even\nthough the 1981 case Diamond v. Diehr is generally seen as the first\nstrong support by the courts for forcing the United States Patent\nand Trademark Office to grant patents on software.40 Software,\nthis orthodoxy went, was better protected by trade-secret law (a\nstate-by-state law, not a federal statute), which provided protec-\ntion for any intellectual property that an owner reasonably tried to\nmaintain as a secret. The trade-secret status of UNIX, for example,\nmeant that all the educational licensees who were given the source\ncode of UNIX had agreed to keep it secret, even though it was\nmanifestly circulating the world over; one could therefore run afoul\nof trade-secret rules if one looked at the source code (e.g., signed a\nnondisclosure license or was shown the code by an employee) and\nthen implemented something similar.\nBy contrast, copyright law was rarely deployed in matters of\nsoftware production. The first copyright registration of software\noccurred in 1964, but the desirability of relying on copyright over\ntrade secret was uncertain well into the 1970s.41 Some corpora-\ntions, like IBM, routinely marked all source code with a copyright\nsymbol. Others asserted it only on the binaries they distributed or\nin the license agreements. The case of software on the UNIX op-\nerating system and its derivatives is particularly haphazard, and\nthe existence of copyright notices by the authors varies widely. An\ninformal survey by Barry Gold singled out only James Gosling, Wal-\nter Tichy (author of rcs), and the RAND Corporation as having ad-\nequately labeled source code with copyright notices.42 Gosling was\nalso the first to register EMACS as copyrighted software in 1983,\n200 writing copyright licenses while Stallman registered GNU EMACS just after version 15.34 was\nreleased in May 1985.43\nThe uncertainty of the change from reliance on trade secret to\nreliance on copyright is clear in some of the statements made by\nStallman around the reuse of Gosling\u2019s code. Since neither Stallman\nnor Gosling sought to keep the program secret in any form\u2014either\nby licensing it or by requiring users to keep it secret\u2014there could\nbe no claims of trade-secret status on either program. Nonetheless,\nthere was frequent concern about whether one had seen any code\n(especially code from a UNIX operating system, which is covered\nby trade secret) and whether code that someone else had seen,\nrewritten, or distributed publicly was therefore \u201cin the public do-\nmain.\u201d44 But, at the same time, Stallman was concerned that rewrit-\ning Gosling\u2019s display code would be too difficult: \u201cAny display code\nwould have a considerable resemblance to that display code, just\nby virtue of doing the same job. Without any clear idea of exactly\nhow much difference there would have to be to reassure you users,\nI cannot tell whether the rewrite would accomplish that. The law is\nnot any guidance here. . . . Writing display code that is significantly\ndifferent is not easy.\u201d45\nStallman\u2019s strategy for rewriting software, including his plan for\nthe GNU operating system, also involved \u201cnot looking at\u201d anyone\nelse\u2019s code, so as to ensure that no trade-secret violations would\noccur. Although it was clear that Gosling\u2019s code was not a trade\nsecret, it was also not obvious that it was \u201cin the public domain,\u201d\nan assumption that might be made about other kinds of software\nprotected by trade secret. Under trade-secret rules, Gosling\u2019s public\ndistribution of GOSMACS appears to give the green light for its\nreuse, but under copyright law, a law of strict liability, any unau-\nthorized use is a violation, regardless of how public the software\nmay have been.46\nThe fact of trade-secret protection was nonetheless an important\naspect of the EMACS controversy: the version of EMACS that War-\nren Montgomery had created at Bell Labs (and on which Zimmer-\nman\u2019s CCA version would be based) was the subject of trade-secret\nprotection by AT&T, by virtue of being distributed with UNIX and\nunder a nondisclosure agreement. AT&T was at the time still a year\naway from divestiture and thus unable to engage in commercial\nexploitation of the software. When CCA sought to commercialize\nwriting copyright licenses 201 the version of UNIX Zimmerman had based on Montgomery\u2019s, it\nwas necessary to remove any AT&T code in order to avoid violating\ntheir trade-secret status. CCA in turn distributed their EMACS as ei-\nther binary or as source (the former costing about $1,000, the latter\nas much as $7,000) and relied on copyright rather than trade-secret\nprotection to prevent unauthorized uses of their software.47\nThe uncertainty over copyright was thus in part a reflection of a\nchanging strategy in the computer-software industry, a kind of un-\neven development in which copyright slowly and haphazardly came\nto replace trade secret as the main form of intellectual-property\nprotection. This switch had consequences for how noncommercial\nprogrammers, researchers, and amateurs might interpret their own\nwork, as well as for the companies whose lawyers were struggling\nwith the same issues. Of course, copyright and trade-secret protec-\ntion are not mutually exclusive, but they structure the need for\nsecrecy in different ways, and they make different claims on issues\nlike similarity, reuse, and modification.\nThe 1976 changes to copyright law were therefore extremely sig-\nnificant in setting out a new set of boundaries and possibilities for\nintellectual-property arguments, arguments that created a different\nkind of uncertainty from that of a changing commercial strategy: a\nstructural uncertainty created by the need for a case law to develop\naround the statutory changes implemented by Congress.\nThe Copyright Act of 1976 introduced a number of changes that\nhad been some ten years in the making, largely organized around\nnew technologies like photocopier machines, home audiotaping,\nand the new videocassette recorders. It codified fair-use rights, it\nremoved the requirement to register, and it expanded the scope of\ncopyrightable materials considerably. It did not, however, explic-\nitly address software, an oversight that frustrated many in the com-\nputer industry, in particular the young software industry. Pursuant\nto this oversight, the National Commission on New Technological\nUses of Copyright (CONTU) was charged with making suggestions\nfor changes to the law with respect to software. It was therefore\nonly in 1980 that Congress implemented these changes, adding\nsoftware to title 17 of the U.S. copyright statute as something that\ncould be considered copyrightable by law.48\nThe 1980 amendment to the copyright law answered one of three\nlingering questions about the copyrightability of software: the sim-\nple question of whether it was copyrightable material at all. Con-\n202 writing copyright licenses gress answered yes. It did not, however, designate what constituted\n\u201csoftware.\u201d During the 1980s, a series of court cases helped specify\nwhat counted as software, including source code, object code (bina-\nries), screen display and output, look and feel, and microcode and\nfirmware.49 The final question, which the courts are still faced with\nadjudicating, concerns how much similarity constitutes an infringe-\nment in each of these cases. The implications of the codification\nof fair use and the requirement to register continue to unfold even\ninto the present.\nThe EMACS controversy confronts all three of these questions.\nStallman\u2019s initial creation of EMACS was accomplished under con-\nditions in which it was unclear whether copyright would apply (i.e.,\nbefore 1980). Stallman, of course, did not attempt to copyright the\nearliest versions of EMACS, but the 1976 amendments removed\nthe requirement to register, thus rendering everything written af-\nter 1978 automatically copyrighted. Registration represented only\nan additional effort to assert ownership in cases of suspected in-\nfringement.\nThroughout this period, the question of whether software was\ncopyrightable\u2014or copyrighted\u2014was being answered differently\nin different cases: AT&T was relying on trade-secret status; Gos-\nling, Unipress, and CCA negotiated over copyrighted material; and\nStallman was experimenting with his \u201ccommune.\u201d Although the\nuncertainty was answered statutorily by the 1980 amendment,\nnot everyone instantly grasped this new fact or changed practices\nbased on it. There is ample evidence throughout the Usenet archive\nthat the 1976 changes were poorly understood, especially by com-\nparison with the legal sophistication of hackers in the 1990s and\n2000s. Although the law changed in 1980, practices changed more\nslowly, and justifications crystallized in the context of experiments\nlike that of GNU EMACS.\nFurther, a tension emerged between the meaning of source code\nand the meaning of software. On the one hand was the question of\nwhether the source code or the binary code was copyrightable, and\non the other was the question of defining the boundaries of software\nin a context wherein all software relies on other software in order\nto run at all. For instance, EMACS was originally built on top of\nTECO, which was referred to both as an editor and as a program-\nming language; even seemingly obvious distinctions (e.g., applica-\ntion vs. programming language) were not necessarily always clear.\nwriting copyright licenses 203 If EMACS was an application written in TECO qua programming\nlanguage, then it would seem that EMACS should have its own\ncopyright, distinct from any other program written in TECO. But\nif EMACS was an extension or modification of TECO qua editor,\nthen it would seem that EMACS was a derivative work and would\nrequire the explicit permission of the copyright holder.\nFurther, each version of EMACS, in order to be EMACS, needed\na LISP interpreter in order to make the extensible interface similar\nacross all versions. But not all versions used the same LISP inter-\npreter. Gosling\u2019s used an interpreter called MOCKLISP (mlisp in\nthe trademarked Unipress version), for instance. The question of\nwhether the LISP interpreter was a core component of the software\nor an \u201cenvironment\u201d needed in order to extend the application was\nthus also uncertain and unspecified in the law. While both might\nbe treated as software suitable for copyright protection, both might\nalso be understood as necessary components out of which copy-\nrightable software would be built.50\nWhat\u2019s more, both the 1976 and 1980 amendments are silent\non the copyright status of source code vs. binary code. While all\nthe versions of EMACS were distributed in binary, Stallman and\nGosling both included the source to allow users to modify it and\nextend it, but they differed on the proper form of redistribution. The\nthreshold between modifying software for oneself and copyright\ninfringement was not yet clear, and it hung on the meaning of\nredistribution. Changing the software for use on a single computer\nmight be necessary to get it to run, but by the early days of the\nArpanet, innocently placing that code in a public directory on one\ncomputer could look like mass distribution.51\nFinally, the question of what constitutes infringement was at the\nheart of this controversy and was not resolved by law or by legal\nadjudication, but simply by rewriting the code to avoid the ques-\ntion. Stallman\u2019s use of Gosling\u2019s code, his claim of third-hand per-\nmission, the presence or absence of written permission, the sale\nof GOSMACS to Unipress when it most likely contained code not\nwritten by Gosling but copyrighted in his name\u2014all of these is-\nsues complicated the question of infringement to the point where\nStallman\u2019s only feasible option for continuing to create software\nwas to avoid using anyone else\u2019s code at all. Indeed, Stallman\u2019s\ndecision to use Gosling\u2019s code (which he claims to have changed in\nsignificant portions) might have come to nothing if he had unethi-\n204 writing copyright licenses cally and illegally chosen not to include the copyright notice at all\n(under the theory that the code was original to Stallman, or an\nimitation, rather than a portion of Gosling\u2019s work). Indeed, Chris\nTorek received Gosling\u2019s permission to remove Gosling\u2019s name and\ncopyright from the version of display.c he had heavily modified,\nbut he chose not to omit them: \u201cThe only reason I didn\u2019t do so is\nthat I feel that he should certainly be credited as the inspiration (at\nthe very least) for the code.\u201d52 Likewise, Stallman was most likely\nconcerned to obey the law and to give credit where credit was due,\nand therefore left the copyright notice attached\u2014a clear case of\nblurred meanings of authorship and ownership.\nIn short, the interplay between new statutes and their settlement\nin court or in practice was a structural uncertainty that set novel\nconstraints on the meaning of copyright, and especially on the\nnorms and forms of permission and reuse. GNU EMACS 15.34 was\nthe safest option\u2014a completely new version that performed the\nsame tasks, but in a different manner, using different algorithms\nand code.\nEven as it resolved the controversy, however, GNU EMACS posed\nnew problems for Stallman: how would the EMACS commune sur-\nvive if it wasn\u2019t clear whether one could legally use another person\u2019s\ncode, even if freely contributed? Was Gosling\u2019s action in selling\nwork by others to Unipress legitimate? Would Stallman be able to\nenforce its opposite, namely, prevent people from commercializing\nEMACS code they contributed to him? How would Stallman avoid\nthe future possibility of his own volunteers and contributors later\nasserting that he had infringed on their copyright?\nBy 1986, Stallman was sending out a letter that recorded the for-\nmal transfer of copyright to the Free Software Foundation (which\nhe had founded in late 1985), with equal rights to nonexclusive\nuse of the software.53 While such a demand for the expropriation\nof copyright might seem contrary to the aims of the GNU project,\nin the context of the unfolding copyright law and the GOSMACS\ncontroversy it made perfect sense. Having been accused himself of\nnot having proper permission to use someone else\u2019s copyrighted\nmaterial in his free version of GNU EMACS, Stallman took steps to\nforestall such an event in the future.\nThe interplay between technical and legal issues and \u201cethical\u201d\nconcerns was reflected in the practical issues of fear, intimidation,\nand common-sense (mis)understandings of intellectual-property\nwriting copyright licenses 205 law. Zimmerman\u2019s veiled threats of legal liability were directed\nnot only at Stallman but at anyone who was using the program\nStallman had written; breaking the law was, for Zimmerman, an\nethical lapse, not a problem of uncertainty and change. Whether or\nnot such an interpretation of the law was correct, it did reveal the\nmechanisms whereby a low level of detailed knowledge about the\nlaw\u2014and a law in flux, at that (not to mention the litigious reputa-\ntion of the U.S. legal system worldwide)\u2014often seemed to justify\na sense that buying software was simply a less risky option than\nacquiring it for free. Businesses, not customers, it was assumed,\nwould be liable for such infringements. By the same token, the sud-\nden concern of software programmers (rather than lawyers) with\nthe detailed mechanics of copyright law meant that a very large\nnumber of people found themselves asserting common-sense no-\ntions, only to be involved in a flame war over what the copyright\nlaw \u201cactually says.\u201d\nSuch discussion has continued and grown exponentially over the\nlast twenty years, to the point that Free Software hackers are now\nnearly as deeply educated about intellectual property law as they\nare about software code.54 Far from representing the triumph of the\nhacker ethic, the GNU General Public License represents the con-\ncrete, tangible outcome of a relatively wide-ranging cultural con-\nversation hemmed in by changing laws, court decisions, practices\nboth commercial and academic, and experiments with the limits\nand forms of new media and new technology.\nConclusion\nThe rest of the story is quickly told: Stallman resigned from the AI\nLab at MIT and started the Free Software Foundation in 1985; he\ncreated a raft of new tools, but ultimately no full UNIX operating\nsystem, and issued General Public License 1.0 in 1989. In 1990\nhe was awarded a MacArthur \u201cgenius grant.\u201d During the 1990s,\nhe was involved in various high-profile battles among a new gen-\neration of hackers; those controversies included the debate around\nLinus Torvalds\u2019s creation of Linux (which Stallman insisted be re-\nferred to as GNU\/Linux), the forking of EMACS into Xemacs, and\nStallman\u2019s own participation in\u2014and exclusion from\u2014conferences\nand events devoted to Free Software.\n206 writing copyright licenses Between 1986 and 1990, the Free Software Foundation and its\nsoftware became extremely well known among geeks. Much of this\nhad to do with the wealth of software that they produced and dis-\ntributed via Usenet and Arpanet. And as the software circulated\nand was refined, so were the new legal constraints and the process\nof teaching users to understand what they could and could not do\nwith the software\u2014and why it was not in the public domain.\nEach time a new piece of software was released, it was accompa-\nnied by one or more text files which explained what its legal status\nwas. At first, there was a file called DISTRIB, which contained an\nexplanation of the rights the new owner had to modify and redis-\ntribute the software.55 DISTRIB referenced a file called COPYING,\nwhich contained the \u201cGNU EMACS copying permission notice,\u201d\nalso known as the GNU EMACS GPL. The first of these licenses\nlisted the copyright holder as Richard Stallman (in 1985), but by\n1986 all licenses referred to the Free Software Foundation as the\ncopyright holder.\nAs the Free Software Foundation released other pieces of soft-\nware, the license was renamed\u2014GNU CC GPL, a GNU Bison GPL,\na GNU GDB GPL, and so on, all of which were essentially the same\nterms\u2014in a file called COPYING, which was meant to be distrib-\nuted along with the software. In 1988, after the software and the\nlicenses had become considerably more widely available, Stallman\nmade a few changes to the license that relaxed some of the terms\nand specified others.56 This new version would become the GNU\nGPL 1.0. By the time Free Software emerged into the public con-\nsciousness in the late 1990s, the GPL had reached version 2.0, and\nthe Free Software Foundation had its own legal staff.\nThe creation of the GPL and the Free Software Foundation are\noften understood as expressions of the hacker ethic, or of Stallman\u2019s\nideological commitment to freedom. But the story of EMACS and\nthe complex technical and legal details that structure it illustrate\nhow the GPL is more than just a hack: it was a novel, privately\nordered legal \u201ccommune.\u201d It was a space thoroughly independent\nof, but insinuated into the existing bedrock of rules and practices of\nthe world of corporate and university software, and carved out of\nthe slippery, changing substance of intellectual-property statutes.\nAt a time when the giants of the software industry were fighting to\ncreate a different kind of openness\u2014one that preserved and would\neven strengthen existing relations of intellectual property\u2014this\nwriting copyright licenses 207 hack was a radical alternative that emphasized the sovereignty\nnot of a national or corporate status quo, but of self-fashioning\nindividuals who sought to opt out of that national-corporate unity.\nThe creation of the GNU GPL was not a return to a golden age of\nsmall-scale communities freed from the dominating structures of\nbureaucratic modernity, but the creation of something new out\nof those structures. It relied on and emphasized, not their destruction,\nbut their stability\u2014at least until they are no longer necessary.\nThe significance of the GPL is due to its embedding within and\nemergence from the legal and technical infrastructure. Such a prac-\ntice of situated reworking is what gives Free Software\u2014and per-\nhaps all forms of engineering and creative practice\u2014its warp and\nweft. Stallman\u2019s decision to resign from the AI Lab and start the\nFree Software Foundation is a good example; it allowed Stallman\nno only to devote energy to Free Software but also to formally dif-\nferentiate the organizations, to forestall at least the potential threat\nthat MIT (which still provided him with office space, equipment,\nand network connection) might decide to claim ownership over his\nwork. One might think that the hacker ethic and the image of self-\ndetermining free individuals would demand the total absence of\norganizations, but it requires instead their proliferation and modu-\nlation. Stallman himself was never so purely free: he relied on the\nlargesse of MIT\u2019s AI Lab, without which he would have had no\noffice, no computer, no connection to the network, and indeed, for\na while, no home.\nThe Free Software Foundation represents a recognition on his\npart that individual and communal independence would come at\nthe price of a legally and bureaucratically recognizable entity, set\napart from MIT and responsible only to itself. The Free Software\nFoundation took a classic form: a nonprofit organization with a\nhierarchy. But by the early 1990s, a new set of experiments would\nbegin that questioned the look of such an entity. The stories of\nLinux and Apache reveal how these ventures both depended on the\nwork of the Free Software Foundation and departed from the hier-\narchical tradition it represented, in order to innovate new similarly\nembedded sociotechnical forms of coordination.\nThe EMACS text editor is still widely used, in version 22.1 as of\n2007, and ported to just about every conceivable operating system.\nThe controversy with Unipress has faded into the distance, as newer\nand more intense controversies have faced Stallman and Free Soft-\n208 writing copyright licenses ware, but the GPL has become the most widely used and most finely\nscrutinized of the legal licenses. More important, the EMACS con-\ntroversy was by no means the only one to have erupted in the lives\nof software programmers; indeed, it has become virtually a rite of\npassage for young geeks to be involved in such debates, because it\nis the only way in which the technical details and the legal details\nthat confront geeks can be explored in the requisite detail. Not all\nsuch arguments end in the complete rewriting of source code, and\ntoday many of them concern the attempt to convince or evangelize\nfor the release of source code under a Free Software license. The\nEMACS controversy was in some ways a primal scene\u2014a traumatic\none, for sure\u2014that determined the outcome of many subsequent\nfights by giving form to the Free Software license and its uses.\nwriting copyright licenses 209 7.\nCoordinating Collaborations\nThe final component of Free Software is coordination. For many\nparticipants and observers, this is the central innovation and es-\nsential significance of Open Source: the possibility of enticing po-\ntentially huge numbers of volunteers to work freely on a software\nproject, leveraging the law of large numbers, \u201cpeer production,\u201d\n\u201cgift economies,\u201d and \u201cself-organizing social economies.\u201d1 Coordi-\nnation in Free Software is of a distinct kind that emerged in the\n1990s, directly out of the issues of sharing source code, conceiving\nopen systems, and writing copyright licenses\u2014all necessary precur-\nsors to the practices of coordination. The stories surrounding these\nissues find continuation in those of the Linux operating-system ker-\nnel, of the Apache Web server, and of Source Code Management\ntools (SCMs); together these stories reveal how coordination worked\nand what it looked like in the 1990s.\nCoordination is important because it collapses and resolves the\ndistinction between technical and social forms into a meaningful whole for participants. On the one hand, there is the coordination\nand management of people; on the other, there is the coordination of\nsource code, patches, fixes, bug reports, versions, and distributions\u2014\nbut together there is a meaningful technosocial practice of manag-\ning, decision-making, and accounting that leads to the collaborative\nproduction of complex software and networks. Such coordination\nwould be unexceptional, essentially mimicking long-familiar cor-\nporate practices of engineering, except for one key fact: it has no\ngoals. Coordination in Free Software privileges adaptability over\nplanning. This involves more than simply allowing any kind of mod-\nification; the structure of Free Software coordination actually gives\nprecedence to a generalized openness to change, rather than to the\nfollowing of shared plans, goals, or ideals dictated or controlled by\na hierarchy of individuals.2\nAdaptability does not mean randomness or anarchy, however;\nit is a very specific way of resolving the tension between the indi-\nvidual curiosity and virtuosity of hackers, and the collective coordi-\nnation necessary to create and use complex software and networks.\nNo man is an island, but no archipelago is a nation, so to speak.\nAdaptability preserves the \u201cjoy\u201d and \u201cfun\u201d of programming with-\nout sacrificing the careful engineering of a stable product. Linux\nand Apache should be understood as the results of this kind of co-\nordination: experiments with adaptability that have worked, to\nthe surprise of many who have insisted that complexity requires\nplanning and hierarchy. Goals and planning are the province of\ngovernance\u2014the practice of goal-setting, orientation, and defini-\ntion of control\u2014but adaptability is the province of critique, and this\nis why Free Software is a recursive public: it stands outside power\nand offers powerful criticism in the form of working alternatives.\nIt is not the domain of the new\u2014after all Linux is just a rewrite of\nUNIX\u2014but the domain of critical and responsive public direction\nof a collective undertaking.\nLinux and Apache are more than pieces of software; they are\norganizations of an unfamiliar kind. My claim that they are \u201cre-\ncursive publics\u201d is useful insofar as it gives a name to a practice\nthat is neither corporate nor academic, neither profit nor nonprofit,\nneither governmental nor nongovernmental. The concept of recur-\nsive public includes, within the spectrum of political activity, the\ncreation, modification, and maintenance of software, networks,\nand legal documents. While a \u201cpublic\u201d in most theories is a body of\ncoordinating collaborations 211 people and a discourse that give expressive form to some concern,\n\u201crecursive public\u201d is meant to suggest that geeks not only give ex-\npressive form to some set of concerns (e.g., that software should\nbe free or that intellectual property rights are too expansive) but\nalso give concrete infrastructural form to the means of expression\nitself. Linux and Apache are tools for creating networks by which\nexpression of new kinds can be guaranteed and by which further\ninfrastructural experimentation can be pursued. For geeks, hack-\ning and programming are variants of free speech and freedom of\nassembly.\nFrom UNIX to Minix to Linux\nLinux and Apache are the two paradigmatic cases of Free Soft-\nware in the 1990s, both for hackers and for scholars of Free Soft-\nware. Linux is a UNIX-like operating-system kernel, bootstrapped\nout of the Minix operating system created by Andrew Tanenbaum.3\nApache is the continuation of the original National Center for Su-\npercomputing Applications (NCSA) project to create a Web server\n(Rob McCool\u2019s original program, called httpd), bootstrapped out of\na distributed collection of people who were using and improving\nthat software.\nLinux and Apache are both experiments in coordination. Both\nprojects evolved decision-making systems through experiment: a vot-\ning system in Apache\u2019s case and a structured hierarchy of decision-\nmakers, with Linus Torvalds as benevolent dictator, in Linux\u2019s case.\nBoth projects also explored novel technical tools for coordination,\nespecially Source Code Management (SCM) tools such as Concur-\nrent Versioning System (cvs). Both are also cited as exemplars of\nhow \u201cfun,\u201d \u201cjoy,\u201d or interest determine individual participation and\nof how it is possible to maintain and encourage that participation\nand mutual aid instead of narrowing the focus or eliminating pos-\nsible routes for participation.\nBeyond these specific experiments, the stories of Linux and Apache\nare detailed here because both projects were actively central to the\nconstruction and expansion of the Internet of the 1990s by allow-\ning a massive number of both corporate and noncorporate sites to\ncheaply install and run servers on the Internet. Were Linux and\nApache nothing more than hobbyist projects with a few thousand\n212 coordinating collaborations interested tinkerers, rather than the core technical components of\nan emerging planetary network, they would probably not represent\nthe same kind of revolutionary transformation ultimately branded\na \u201cmovement\u201d in 1998\u201399.\nLinus Torvalds\u2019s creation of the Linux kernel is often cited as the\nfirst instance of the real \u201cOpen Source\u201d development model, and it\nhas quickly become the most studied of the Free Software projects.4\nFollowing its appearance in late 1991, Linux grew quickly from a\nsmall, barely working kernel to a fully functional replacement for\nthe various commercial UNIX systems that had resulted from the\nUNIX wars of the 1980s. It has become versatile enough to be used\non desktop PCs with very little memory and small CPUs, as well as\nin \u201cclusters\u201d that allow for massively parallel computing power.\nWhen Torvalds started, he was blessed with an eager audience of\nhackers keen on seeing a UNIX system run on desktop computers\nand a personal style of encouragement that produced enormous pos-\nitive feedback. Torvalds is often given credit for creating, through\nhis \u201cmanagement style,\u201d a \u201cnew generation\u201d of Free Software\u2014\na younger generation than that of Stallman and Raymond. Linus\nand Linux are not in fact the causes of this change, but the results\nof being at the right place at the right time and joining together\na number of existing components. Indeed, the title of Torvalds\u2019s\nsemi-autobiographical reflection on Linux\u2014Just for Fun: The Story\nof an Accidental Revolutionary\u2014captures some of the character of\nits genesis.\nThe \u201cfun\u201d referred to in the title reflects the privileging of adapt-\nability over planning. Projects, tools, people, and code that were\nfun were those that were not dictated by existing rules and ideas.\nFun, for geeks, was associated with the sudden availability, espe-\ncially for university students and amateur hackers, of a rapidly ex-\npanding underground world of networks and software\u2014Usenet and\nthe Internet especially, but also university-specific networks, online\nenvironments and games, and tools for navigating information of\nall kinds. Much of this activity occurred without the benefit of any\nexplicit theorization, with the possible exception of the discourse of\n\u201ccommunity\u201d (given print expression by Howard Rheingold in 1993\nand present in nascent form in the pages of Wired and Mondo 2000)\nthat took place through much of the 1990s.5 The late 1980s and\nearly 1990s gave rise to vast experimentation with the collaborative\npossibilities of the Internet as a medium. Particularly attractive was\ncoordinating collaborations 213 that this medium was built using freely available tools, and the tools\nthemselves were open to modification and creative reuse. It was\na style that reflected the quasi-academic and quasi-commercial\nenvironment, of which the UNIX operating system was an exemplar\u2014\nnot pure research divorced from commercial context, nor entirely\nthe domain of commercial rapacity and intellectual property.\nFun included the creation of mailing lists by the spread of software\nsuch as list-serv and majordomo; the collaborative maintenance\nand policing of Usenet; and the creation of Multi-User Dungeons\n(MUDs) and MUD Object Orienteds (MOOs), both of which gave\ngame players and Internet geeks a way to co-create software envi-\nronments and discover many of the problems of management and\npolicing that thereby emerged.6 It also included the increasing array\nof \u201cinformation services\u201d that were built on top of the Internet, like\narchie, gopher, Veronica, WAIS, ftp, IRC\u2014all of which were neces-\nsary to access the growing information wealth of the underground\ncommunity lurking on the Internet. The meaning and practice of\ncoordination in all of these projects was up for grabs: some were\norganized strictly as university research projects (gopher), while\nothers were more fluid and open to participation and even control\nby contributing members (MOOs and MUDs). Licensing issues were\nexplicit in some, unclear in some, and completely ignored in others.\nSome projects had autocratic leaders, while others experimented\nwith everything from representative democracy to anarchism.\nDuring this period (roughly 1987 to 1993), the Free Software\nFoundation attained a mythic cult status\u2014primarily among UNIX\nand EMACS users. Part of this status was due to the superiority of\nthe tools Stallman and his collaborators had already created: the\nGNU C Compiler (gcc), GNU EMACS, the GNU Debugger (gdb),\nGNU Bison, and loads of smaller utilities that replaced the original\nAT&T UNIX versions. The GNU GPL had also acquired a life of its\nown by this time, having reached maturity as a license and become\nthe de facto choice for those committed to Free Software and the\nFree Software Foundation. By 1991, however, the rumors of the\nimminent appearance of Stallman\u2019s replacement UNIX operating\nsystem had started to sound empty\u2014it had been six years since his\npublic announcement of his intention. Most hackers were skeptical\nof Stallman\u2019s operating-system project, even if they acknowledged\nthe success of all the other tools necessary to create a full-fledged\noperating system, and Stallman himself was stymied by the devel-\n214 coordinating collaborations opment of one particular component: the kernel itself, called GNU\nHurd.\nLinus Torvalds\u2019s project was not initially imagined as a contribu-\ntion to the Free Software Foundation: it was a Helsinki university\nstudent\u2019s late-night project in learning the ins and outs of the rela-\ntively new Intel 386\/486 microprocessor. Torvalds, along with tens\nof thousands of other computer-science students, was being schooled\nin UNIX through the pedagogy of Andrew Tanenbaum\u2019s Minix, Doug-\nlas Comer\u2019s Xinu-PC, and a handful of other such teaching versions\ndesigned to run on IBM PCs. Along with the classroom pedagogy\nin the 1980s came the inevitable connection to, lurking on, and\nposting to the Usenet and Arpanet mailing lists devoted to techni-\ncal (and nontechnical) topics of all sorts.7 Torvalds was subscribed,\nnaturally, to comp.os.minix, the newsgroup for users of Minix.\nThe fact of Linus Torvalds\u2019s pedagogical embedding in the world\nof UNIX, Minix, the Free Software Foundation, and the Usenet\nshould not be underestimated, as it often is in hagiographical ac-\ncounts of the Linux operating system. Without this relatively robust\nmoral-technical order or infrastructure within which it was possible\nto be at the right place at the right time, Torvalds\u2019s late-night dorm-\nroom project would have amounted to little more than that\u2014but\nthe pieces were all in place for his modest goals to be transformed\ninto something much more significant.\nConsider his announcement on 25 August 1991:\nHello everybody out there using minix\u2014I\u2019m doing a (free) operat-\ning system ( just a hobby, won\u2019t be big and professional like gnu) for\n386(486) AT clones. This has been brewing since april, and is starting\nto get ready. I\u2019d like any feedback on things people like\/dislike in\nminix, as my OS resembles it somewhat (same physical layout of the\nfile-system (due to practical reasons) among other things). I\u2019ve cur-\nrently ported bash(1.08) and gcc(1.40), and things seem to work. This\nimplies that I\u2019ll get something practical within a few months, and I\u2019d\nlike to know what features most people would want. Any suggestions\nare welcome, but I won\u2019t promise I\u2019ll implement them :-)\nLinus . . .\nPS. Yes\u2014it\u2019s free of any minix code, and it has a multi-threaded\nfs. It is NOT portable (uses 386 task switching etc), and it probably\nnever will support anything other than AT-harddisks, as that\u2019s all I\nhave :-(.8\ncoordinating collaborations 215 Torvalds\u2019s announcement is telling as to where his project fit into\nthe existing context: \u201cjust a hobby,\u201d not \u201cbig and professional like\ngnu\u201d (a comment that suggests the stature that Stallman and the\nFree Software Foundation had achieved, especially since they were\nin reality anything but \u201cbig and professional\u201d). The announcement\nwas posted to the Minix list and thus was essentially directed at\nMinix users; but Torvalds also makes a point of insisting that the\nsystem would be free of cost, and his postscript furthermore indi-\ncates that it would be free of Minix code, just as Minix had been\nfree of AT&T code.\nTorvalds also mentions that he has ported \u201cbash\u201d and \u201cgcc,\u201d\nsoftware created and distributed by the Free Software Foundation\nand tools essential for interacting with the computer and compiling\nnew versions of the kernel. Torvalds\u2019s decision to use these utili-\nties, rather than write his own, reflects both the boundaries of his\nproject (an operating-system kernel) and his satisfaction with the\navailability and reusability of software licensed under the GPL.\nSo the system is based on Minix, just as Minix had been based on\nUNIX\u2014piggy-backed or bootstrapped, rather than rewritten in an\nentirely different fashion, that is, rather than becoming a different\nkind of operating system. And yet there are clearly concerns about\nthe need to create something that is not Minix, rather than simply\nextending or \u201cdebugging\u201d Minix. This concern is key to understand-\ning what happened to Linux in 1991.\nTanenbaum\u2019s Minix, since its inception in 1984, was always\nintended to allow students to see and change the source code of\nMinix in order to learn how an operating system worked, but it\nwas not Free Software. It was copyrighted and owned by Pren-\ntice Hall, which distributed the textbooks. Tanenbaum made the\ncase\u2014similar to Gosling\u2019s case for Unipress\u2014that Prentice Hall was\ndistributing the system far wider than if it were available only on\nthe Internet: \u201cA point which I don\u2019t think everyone appreciates is\nthat making something available by FTP is not necessarily the way\nto provide the widest distribution. The Internet is still a highly elite\ngroup. Most computer users are NOT on it. . . . MINIX is also widely\nused in Eastern Europe, Japan, Israel, South America, etc. Most\nof these people would never have gotten it if there hadn\u2019t been a\ncompany selling it.\u201d9\nBy all accounts, Prentice Hall was not restrictive in its sublicensing\nof the operating system, if people wanted to create an \u201cenhanced\u201d\n216 coordinating collaborations version of Minix. Similarly, Tanenbaum\u2019s frequent presence on\ncomp.os.minix testified to his commitment to sharing his knowl-\nedge about the system with anyone who wanted it\u2014not just paying\ncustomers. Nonetheless, Torvalds\u2019s pointed use of the word free and\nhis decision not to reuse any of the code is a clear indication of his\ndesire to build a system completely unencumbered by restrictions,\nbased perhaps on a kind of intuitive folkloric sense of the dangers\nassociated with cases like that of EMACS.10\nThe most significant aspect of Torvalds\u2019s initial message, how-\never, is his request: \u201cI\u2019d like to know what features most people\nwould want. Any suggestions are welcome, but I won\u2019t promise\nI\u2019ll implement them.\u201d Torvalds\u2019s announcement and the subsequent\ninterest it generated clearly reveal the issues of coordination and\norganization that would come to be a feature of Linux. The reason\nTorvalds had so many eager contributors to Linux, from the very\nstart, was because he enthusiastically took them off of Tanenbaum\u2019s\nhands.\nDesign and Adaptability\nTanenbaum\u2019s role in the story of Linux is usually that of the straw\nman\u2014a crotchety old computer-science professor who opposes the\nrevolutionary young Torvalds. Tanenbaum did have a certain revo-\nlutionary reputation himself, since Minix was used in classrooms\naround the world and could be installed on IBM PCs (something\nno other commercial UNIX vendors had achieved), but he was also\na natural target for people like Torvalds: the tenured professor es-\npousing the textbook version of an operating system. So, despite the\nfact that a very large number of people were using or knew of Minix\nas a UNIX operating system (estimates of comp.os.minix subscrib-\ners were at 40,000), Tanenbaum was emphatically not interested in\ncollaboration or collaborative debugging, especially if debugging\nalso meant creating extensions and adding features that would\nmake the system bigger and harder to use as a stripped-down tool\nfor teaching. For Tanenbaum, this point was central: \u201cI\u2019ve been\nrepeatedly offered virtual memory, paging, symbolic links, window\nsystems, and all manner of features. I have usually declined be-\ncause I am still trying to keep the system simple enough for students\nto understand. You can put all this stuff in your version, but I won\u2019t\ncoordinating collaborations 217 put it in mine. I think it is this point which irks the people who say\n\u2018MINIX is not free,\u2019 not the $60.\u201d11\nSo while Tanenbaum was in sympathy with the Free Software\nFoundation\u2019s goals (insofar as he clearly wanted people to be able\nto use, update, enhance, and learn from software), he was not in\nsympathy with the idea of having 40,000 strangers make his soft-\nware \u201cbetter.\u201d Or, to put it differently, the goals of Minix remained\nthose of a researcher and a textbook author: to be useful in class-\nrooms and cheap enough to be widely available and usable on the\nlargest number of cheap computers.\nBy contrast, Torvalds\u2019s \u201cfun\u201d project had no goals. Being a cocky\nnineteen-year-old student with little better to do (no textbooks to\nwrite, no students, grants, research projects, or committee meet-\nings), Torvalds was keen to accept all the ready-made help he could\nfind to make his project better. And with 40,000 Minix users, he\nhad a more or less instant set of contributors. Stallman\u2019s audience\nfor EMACS in the early 1980s, by contrast, was limited to about a\nhundred distinct computers, which may have translated into thou-\nsands, but certainly not tens of thousands of users. Tanenbaum\u2019s\nwork in creating a generation of students who not only understood\nthe internals of an operating system but, more specifically, under-\nstood the internals of the UNIX operating system created a huge\npool of competent and eager UNIX hackers. It was the work of port-\ning UNIX not only to various machines but to a generation of minds\nas well that set the stage for this event\u2014and this is an essential,\nthough often overlooked component of the success of Linux.\nMany accounts of the Linux story focus on the fight between\nTorvalds and Tanenbaum, a fight carried out on comp.os.minix\nwith the subject line \u201cLinux is obsolete.\u201d12 Tanenbaum argued that\nTorvalds was reinventing the wheel, writing an operating system\nthat, as far as the state of the art was concerned, was now obsolete.\nTorvalds, by contrast, asserted that it was better to make something\nquick and dirty that worked, invite contributions, and worry about\nmaking it state of the art later. Far from illustrating some kind of\noutmoded conservatism on Tanenbaum\u2019s part, the debate highlights\nthe distinction between forms of coordination and the meanings\nof collaboration. For Tanenbaum, the goals of Minix were either\npedagogical or academic: to teach operating-system essentials or to\nexplore new possibilities in operating-system design. By this model,\nLinux could do neither; it couldn\u2019t be used in the classroom because\n218 coordinating collaborations it would quickly become too complex and feature-laden to teach,\nand it wasn\u2019t pushing the boundaries of research because it was an\nout-of-date operating system. Torvalds, by contrast, had no goals.\nWhat drove his progress was a commitment to fun and to a largely\ninarticulate notion of what interested him and others, defined at\nthe outset almost entirely against Minix and other free operating\nsystems, like FreeBSD. In this sense, it could only emerge out of the\ncontext\u2014which set the constraints on its design\u2014of UNIX, open\nsystems, Minix, GNU, and BSD.\nBoth Tanenbaum and Torvalds operated under a model of coordi-\nnation in which one person was ultimately responsible for the entire\nproject: Tanenbaum oversaw Minix and ensured that it remained\ntrue to its goals of serving a pedagogical audience; Torvalds would\noversee Linux, but he would incorporate as many different features\nas users wanted or could contribute. Very quickly\u2014with a pool of\n40,000 potential contributors\u2014Torvalds would be in the same po-\nsition Tanenbaum was in, that is, forced to make decisions about\nthe goals of Linux and about which enhancements would go into it\nand which would not. What makes the story of Linux so interesting\nto observers is that it appears that Torvalds made no decision: he\naccepted almost everything.\nTanenbaum\u2019s goals and plans for Minix were clear and auto-\ncratically formed. Control, hierarchy, and restriction are after all\nappropriate in the classroom. But Torvalds wanted to do more.\nHe wanted to go on learning and to try out alternatives, and with\nMinix as the only widely available way to do so, his decision to\npart ways starts to make sense; clearly he was not alone in his\ndesire to explore and extend what he had learned. Nonetheless,\nTorvalds faced the problem of coordinating a new project and mak-\ning similar decisions about its direction. On this point, Linux has\nbeen the subject of much reflection by both insiders and outsiders.\nDespite images of Linux as either an anarchic bazaar or an auto-\ncratic dictatorship, the reality is more subtle: it includes a hierar-\nchy of contributors, maintainers, and \u201ctrusted lieutenants\u201d and a\nsophisticated, informal, and intuitive sense of \u201cgood taste\u201d gained\nthrough reading and incorporating the work of co-developers.\nWhile it was possible for Torvalds to remain in charge as an\nindividual for the first few years of Linux (1991\u201395, roughly), he\neventually began to delegate some of that control to people who\nwould make decisions about different subcomponents of the kernel.\ncoordinating collaborations 219 It was thus possible to incorporate more of the \u201cpatches\u201d (pieces of\ncode) contributed by volunteers, by distributing some of the work of\nevaluating them to people other than Torvalds. This informal hier-\narchy slowly developed into a formal one, as Steven Weber points\nout: \u201cThe final de facto \u2018grant\u2019 of authority came when Torvalds\nbegan publicly to reroute relevant submissions to the lieutenants.\nIn 1996 the decision structure became more formal with an explicit\ndifferentiation between \u2018credited developers\u2019 and \u2018maintainers.\u2019 . . .\nIf this sounds very much like a hierarchical decision structure, that\nis because it is one\u2014albeit one in which participation is strictly\nvoluntary.\u201d13\nAlmost all of the decisions made by Torvalds and lieutenants were\nof a single kind: whether or not to incorporate a piece of code sub-\nmitted by a volunteer. Each such decision was technically complex:\ninsert the code, recompile the kernel, test to see if it works or if it\nproduces any bugs, decide whether it is worth keeping, issue a new\nversion with a log of the changes that were made. Although the var-\nious official leaders were given the authority to make such changes,\ncoordination was still technically informal. Since they were all work-\ning on the same complex technical object, one person (Torvalds)\nultimately needed to verify a final version, containing all the sub-\nparts, in order to make sure that it worked without breaking.\nSuch decisions had very little to do with any kind of design goals\nor plans, only with whether the submitted patch \u201cworked,\u201d a term\nthat reflects at once technical, aesthetic, legal, and design criteria\nthat are not explicitly recorded anywhere in the project\u2014hence,\nthe privileging of adaptability over planning. At no point were the\npatches assigned or solicited, although Torvalds is justly famous for\nencouraging people to work on particular problems, but only if they\nwanted to. As a result, the system morphed in subtle, unexpected\nways, diverging from its original, supposedly backwards \u201cmono-\nlithic\u201d design and into a novel configuration that reflected the in-\nterests of the volunteers and the implicit criteria of the leaders.\nBy 1995\u201396, Torvalds and lieutenants faced considerable chal-\nlenges with regard to hierarchy and decision-making, as the project\nhad grown in size and complexity. The first widely remembered\nresponse to the ongoing crisis of benevolent dictatorship in Linux\nwas the creation of \u201cloadable kernel modules,\u201d conceived as a way\nto release some of the constant pressure to decide which patches\nwould be incorporated into the kernel. The decision to modularize\n220 coordinating collaborations Linux was simultaneously technical and social: the software-code\nbase would be rewritten to allow for external loadable modules to\nbe inserted \u201con the fly,\u201d rather than all being compiled into one\nlarge binary chunk; at the same time, it meant that the responsi-\nbility to ensure that the modules worked devolved from Torvalds\nto the creator of the module. The decision repudiated Torvalds\u2019s\nearly opposition to Tanenbaum in the \u201cmonolithic vs. microkernel\u201d\ndebate by inviting contributors to separate core from peripheral\nfunctions of an operating system (though the Linux kernel remains\nmonolithic compared to classic microkernels). It also allowed for a\nsignificant proliferation of new ideas and related projects. It both\ncontracted and distributed the hierarchy; now Linus was in charge\nof a tighter project, but more people could work with him accord-\ning to structured technical and social rules of responsibility.\nCreating loadable modules changed the look of Linux, but not\nbecause of any planning or design decisions set out in advance. The\nchoice is an example of the privileged adaptability of the Linux, re-\nsolving the tension between the curiosity and virtuosity of individ-\nual contributors to the project and the need for hierarchical control\nin order to manage complexity. The commitment to adaptability\ndissolves the distinction between the technical means of coordina-\ntion and the social means of management. It is about producing a\nmeaningful whole by which both people and code can be coordi-\nnated\u2014an achievement vigorously defended by kernel hackers.\nThe adaptable organization and structure of Linux is often de-\nscribed in evolutionary terms, as something without teleological\npurpose, but responding to an environment. Indeed, Torvalds him-\nself has a weakness for this kind of explanation.\nLet\u2019s just be honest, and admit that it [Linux] wasn\u2019t designed.\nSure, there\u2019s design too\u2014the design of UNIX made a scaffolding for\nthe system, and more importantly it made it easier for people to com-\nmunicate because people had a mental model for what the system was\nlike, which means that it\u2019s much easier to discuss changes.\nBut that\u2019s like saying that you know that you\u2019re going to build a\ncar with four wheels and headlights\u2014it\u2019s true, but the real bitch is in\nthe details.\nAnd I know better than most that what I envisioned 10 years ago\nhas nothing in common with what Linux is today. There was certainly\nno premeditated design there.14\ncoordinating collaborations 221 Adaptability does not answer the questions of intelligent design.\nWhy, for example, does a car have four wheels and two headlights?\nOften these discussions are polarized: either technical objects are\ndesigned, or they are the result of random mutations. What this\nopposition overlooks is the fact that design and the coordination of\ncollaboration go hand in hand; one reveals the limits and possibili-\nties of the other. Linux represents a particular example of such a\nproblematic\u2014one that has become the paradigmatic case of Free\nSoftware\u2014but there have been many others, including UNIX, for\nwhich the engineers created a system that reflected the distributed\ncollaboration of users around the world even as the lawyers tried\nto make it conform to legal rules about licensing and practical con-\ncerns about bookkeeping and support.\nBecause it privileges adaptability over planning, Linux is a re-\ncursive public: operating systems and social systems. It privileges\nopenness to new directions, at every level. It privileges the right to\npropose changes by actually creating them and trying to convince\nothers to use and incorporate them. It privileges the right to fork\nthe software into new and different kinds of systems. Given what\nit privileges, Linux ends up evolving differently than do systems\nwhose life and design are constrained by corporate organization,\nor by strict engineering design principles, or by legal or marketing\ndefinitions of products\u2014in short, by clear goals. What makes this\ndistinction between the goal-oriented design principle and the prin-\nciple of adaptability important is its relationship to politics. Goals\nand planning are the subject of negotiation and consensus, or of\nautocratic decision-making; adaptability is the province of critique.\nIt should be remembered that Linux is by no means an attempt to\ncreate something radically new; it is a rewrite of a UNIX operating\nsystem, as Torvalds points out, but one that through adaptation can\nend up becoming something new.\nPatch and Vote\nThe Apache Web server and the Apache Group (now called the\nApache Software Foundation) provide a second illuminating ex-\nample of the how and why of coordination in Free Software of the\n1990s. As with the case of Linux, the development of the Apache\nproject illustrates how adaptability is privileged over planning\n222 coordinating collaborations and, in particular, how this privileging is intended to resolve the\ntensions between individual curiosity and virtuosity and collective\ncontrol and decision-making. It is also the story of the progres-\nsive evolution of coordination, the simultaneously technical and\nsocial mechanisms of coordinating people and code, patches and\nvotes.\nThe Apache project emerged out of a group of users of the origi-\nnal httpd (HyperText Transmission Protocol Daemon) Web server\ncreated by Rob McCool at NCSA, based on the work of Tim Berners-\nLee\u2019s World Wide Web project at CERN. Berners-Lee had written\na specification for the World Wide Web that included the mark-up\nlanguage HTML, the transmission protocol http, and a set of librar-\nies that implemented the code known as libwww, which he had\ndedicated to the public domain.15\nThe NCSA, at the University of Illinois, Urbana-Champaign,\npicked up both www projects, subsequently creating both the first\nwidely used browser, Mosaic, directed by Marc Andreessen, and\nhttpd. Httpd was public domain up until version 1.3. Development\nslowed when McCool was lured to Netscape, along with the team\nthat created Mosaic. By early 1994, when the World Wide Web\nhad started to spread, many individuals and groups ran Web serv-\ners that used httpd; some of them had created extensions and fixed\nbugs. They ranged from university researchers to corporations like\nWired Ventures, which launched the online version of its maga-\nzine (HotWired.com) in 1994. Most users communicated primar-\nily through Usenet, on the comp.infosystems.www.* newsgroups,\nsharing experiences, instructions, and updates in the same manner\nas other software projects stretching back to the beginning of the\nUsenet and Arpanet newsgroups.\nWhen NCSA failed to respond to most of the fixes and extensions\nbeing proposed, a group of several of the most active users of httpd\nbegan to communicate via a mailing list called new-httpd in 1995.\nThe list was maintained by Brian Behlendorf, the webmaster for\nHotWired, on a server he maintained called hyperreal; its partici-\npants were those who had debugged httpd, created extensions, or\nadded functionality. The list was the primary means of associa-\ntion and communication for a diverse group of people from vari-\nous locations around the world. During the next year, participants\nhashed out issues related to coordination, to the identity of and the\nprocesses involved in patching the \u201cnew\u201d httpd, version 1.3.16\ncoordinating collaborations 223 Patching a piece of software is a peculiar activity, akin to debug-\nging, but more like a form of ex post facto design. Patching covers\nthe spectrum of changes that can be made: from fixing security\nholes and bugs that prevent the software from compiling to feature\nand performance enhancements. A great number of the patches\nthat initially drew this group together grew out of needs that each\nindividual member had in making a Web server function. These\npatches were not due to any design or planning decisions by NCSA,\nMcCool, or the assembled group, but most were useful enough that\neveryone gained from using them, because they fixed problems that\neveryone would or could encounter. As a result, the need for a\ncoordinated new-httpd release was key to the group\u2019s work. This\nnew version of NCSA httpd had no name initially, but apache was a\npersistent candidate; the somewhat apocryphal origin of the name\nis that it was \u201ca patchy webserver.\u201d17\nAt the outset, in February and March 1995, the pace of work of\nthe various members of new-httpd differed a great deal, but was in\ngeneral extremely rapid. Even before there was an official release\nof a new httpd, process issues started to confront the group, as Roy\nFielding later explained: \u201cApache began with a conscious attempt\nto solve the process issues first, before development even started,\nbecause it was clear from the very beginning that a geographi-\ncally distributed set of volunteers, without any traditional organi-\nzational ties, would require a unique development process in order\nto make decisions.\u201d18\nThe need for process arose more or less organically, as the group\ndeveloped mechanisms for managing the various patches: assign-\ning them IDs, testing them, and incorporating them \u201cby hand\u201d\ninto the main source-code base. As this happened, members of\nthe list would occasionally find themselves lost, confused by the\nprocess or the efficiency of other members, as in this message from\nAndrew Wilson concerning Cliff Skolnick\u2019s management of the list\nof bugs:\nCliff, can you concentrate on getting an uptodate copy of the bug\/\nimprovement list please. I\u2019ve already lost track of just what the heck is\nmeant to be going on. Also what\u2019s the status of this pre-pre-pre release\nApache stuff. It\u2019s either a pre or it isn\u2019t surely? AND is the pre-pre-etc\nthing the same as the thing Cliff is meant to be working on?\nJust what the fsck is going on anyway? Ay, ay ay! Andrew Wilson.19\n224 coordinating collaborations To which Rob Harthill replied, \u201cIt is getting messy. I still think\nwe should all implement one patch at a time together. At the rate\n(and hours) some are working we can probably manage a couple\nof patches a day. . . . If this is acceptable to the rest of the group, I\nthink we should order the patches, and start a systematic processes\nof discussion, implementations and testing.\u201d20\nSome members found the pace of work exciting, while others ap-\npealed for slowing or stopping in order to take stock. Cliff Skolnick\ncreated a system for managing the patches and proposed that list-\nmembers vote in order to determine which patches be included.21\nRob Harthill voted first.\nHere are my votes for the current patch list shown at\nhttp:\/\/www.hyperreal.com\/httpd\/patchgen\/list.cgi\nI\u2019ll use a vote of\n-1 have a problem with it\n0 haven\u2019t tested it yet (failed to understand it or whatever)\n+1 tried it, liked it, have no problem with it.\n[Here Harthill provides a list of votes on each patch.]\nIf this voting scheme makes sense, lets use it to filter out the stuff\nwe\u2019re happy with. A \u201c-1\u201d vote should veto any patch. There seems to\nbe about 6 or 7 of us actively commenting on patches, so I\u2019d suggest\nthat once a patch gets a vote of +4 (with no vetos), we can add it to\nan alpha.22\nHarthill\u2019s votes immediately instigated discussion about various\npatches, further voting, and discussion about the process (i.e., how\nmany votes or vetoes were needed), all mixed together in a flurry\nof e-mail messages. The voting process was far from perfect, but\nit did allow some consensus on what \u201capache\u201d would be, that is,\nwhich patches would be incorporated into an \u201cofficial\u201d (though\nnot very public) release: Apache 0.2 on 18 March.23 Without a vot-\ning system, the group of contributors could have gone on applying\npatches individually, each in his own context, fixing the problems\nthat ailed each user, but ignoring those that were irrelevant or un-\nnecessary in that context. With a voting process, however, a con-\nvergence on a tested and approved new-httpd could emerge. As the\nprocess was refined, members sought a volunteer to take votes, to\nopen and close the voting once a week, and to build a new version\nof Apache when the voting was done. (Andrew Wilson was the first\nvolunteer, to which Cliff Skolnick replied, \u201cI guess the first vote is\ncoordinating collaborations 225 voting Andrew as the vote taker :-).\u201d)24 The patch-and-vote process\nthat emerged in the early stages of Apache was not entirely novel;\nmany contributors noted that the FreeBSD project used a similar\nprocess, and some suggested the need for a \u201cpatch coordinator\u201d and\nothers worried that \u201cusing patches gets very ugly, very quickly.\u201d25\nThe significance of the patch-and-vote system was that it clearly\nrepresented the tension between the virtuosity of individual devel-\nopers and a group process aimed at creating and maintaining a\ncommon piece of software. It was a way of balancing the ability\nof each separate individual\u2019s expertise against a common desire to\nship and promote a stable, bug-free, public-domain Web server. As\nRoy Fielding and others would describe it in hindsight, this tension\nwas part of Apache\u2019s advantage.\nAlthough the Apache Group makes decisions as a whole, all of the\nactual work of the project is done by individuals. The group does not\nwrite code, design solutions, document products, or provide support\nto our customers; individual people do that. The group provides an\nenvironment for collaboration and an excellent trial-by-fire for ideas\nand code, but the creative energy needed to solve a particular prob-\nlem, redesign a piece of the system, or fix a given bug is almost always\ncontributed by individual volunteers working on their own, for their\nown purposes, and not at the behest of the group. Competitors mistak-\nenly assume Apache will be unable to take on new or unusual tasks\nbecause of the perception that we act as a group rather than follow a\nsingle leader. What they fail to see is that, by remaining open to new\ncontributors, the group has an unlimited supply of innovative ideas,\nand it is the individuals who chose to pursue their own ideas who are\nthe real driving force for innovation.26\nAlthough openness is widely touted as the key to the innovations\nof Apache, the claim is somewhat disingenuous: patches are just\nthat, patches. Any large-scale changes to the code could not be ac-\ncomplished by applying patches, especially if each patch must be\nsubjected to a relatively harsh vote to be included. The only way to\nmake sweeping changes\u2014especially changes that require iteration\nand testing to get right\u2014is to engage in separate \u201cbranches\u201d of a\nproject or to differentiate between internal and external releases\u2014\nin short, to fork the project temporarily in hopes that it would soon\nrejoin its stable parent. Apache encountered this problem very early\non with the \u201cShambhala\u201d rewrite of httpd by Robert Thau.\n226 coordinating collaborations Shambhala was never quite official: Thau called it his \u201cnoodling\u201d\nserver, or a \u201cgarage\u201d project. It started as his attempt to rewrite\nhttpd as a server which could handle and process multiple requests\nat the same time. As an experiment, it was entirely his own project,\nwhich he occasionally referred to on the new-httpd list: \u201cStill hack-\ning Shambhala, and laying low until it works well enough to talk\nabout.\u201d27 By mid-June of 1995, he had a working version that he\nannounced, quite modestly, to the list as \u201ca garage project to ex-\nplore some possible new directions I thought *might* be useful for\nthe group to pursue.\u201d28 Another list member, Randy Terbush, tried\nit out and gave it rave reviews, and by the end of June there were\ntwo users exclaiming its virtues. But since it hadn\u2019t ever really been\nofficially identified as a fork, or an alternate development path-\nway, this led Rob Harthill to ask: \u201cSo what\u2019s the situation regarding\nShambhala and Apache, are those of you who have switched to it\ngiving up on Apache and this project? If so, do you need a separate\nlist to discuss Shambhala?\u201d29\nHarthill had assumed that the NCSA code-base was \u201ctried and\ntested\u201d and that Shambhala represented a split, a fork: \u201cThe ques-\ntion is, should we all go in one direction, continue as things stand\nor Shambahla [sic] goes off on its own?\u201d30 His query drew out the\nmiscommunication in detail: that Thau had planned it as a \u201cdrop-\nin\u201d replacement for the NCSA httpd, and that his intentions were\nto make it the core of the Apache server, if he could get it to work.\nHarthill, who had spent no small amount of time working hard at\npatching the existing server code, was not pleased, and made the\ncore issues explicit.\nMaybe it was rst\u2019s [Robert Thau\u2019s] choice of phrases, such as \u201cga-\nrage project\u201d and it having a different name, maybe I didn\u2019t read his\nmailings thoroughly enough, maybe they weren\u2019t explicit enough,\nwhatever. . . . It\u2019s a shame that nobody using Shambhala (who must\nhave realized what was going on) didn\u2019t raise these issues weeks\nago. I can only presume that rst was too modest to push Shamb-\nhala, or at least discussion of it, onto us more vigourously. I remem-\nber saying words to the effect of \u201cthis is what I plan to do, stop\nme if you think this isn\u2019t a good idea.\u201d Why the hell didn\u2019t anyone\nsay something? . . . [D]id others get the same impression about rst\u2019s\nwork as I did? Come on people, if you want to be part of this group,\ncollaborate!31\ncoordinating collaborations 227 Harthill\u2019s injunction to collaborate seems surprising in the con-\ntext of a mailing list and project created to facilitate collabora-\ntion, but the injunction is specific: collaborate by making plans\nand sharing goals. Implicit in his words is the tension between a\nproject with clear plans and goals, an overarching design to which\neveryone contributes, as opposed to a group platform without clear\ngoals that provides individuals with a setting to try out alterna-\ntives. Implicit in his words is the spectrum between debugging an\nexisting piece of software with a stable identity and rewriting the\nfundamental aspects of it to make it something new. The meaning\nof collaboration bifurcates here: on the one hand, the privileging of\nthe autonomous work of individuals which is submitted to a group\npeer review and then incorporated; on the other, the privileging of\na set of shared goals to which the actions and labor of individuals\nis subordinated.32\nIndeed, the very design of Shambhala reflects the former ap-\nproach of privileging individual work: like UNIX and EMACS before\nit, Shambhala was designed as a modular system, one that could\n\u201cmake some of that process [the patch-and-vote process] obsolete,\nby allowing stuff which is not universally applicable (e.g., database\nback-ends), controversial, or just half-baked, to be shipped anyway\nas optional modules.\u201d33 Such a design separates the core platform\nfrom the individual experiments that are conducted on it, rather\nthan creating a design that is modular in the hierarchical sense\nof each contributor working on an assigned section of a project.\nUndoubtedly, the core platform requires coordination, but exten-\nsions and modifications can happen without needing to transform\nthe whole project.34 Shambhala represents a certain triumph of the\n\u201cshut up and show me the code\u201d aesthetic: Thau\u2019s \u201cmodesty\u201d is\ninstead a recognition that he should be quiet until it \u201cworks well\nenough to talk about,\u201d whereas Harthill\u2019s response is frustration\nthat no one has talked about what Thau was planning to do before\nit was even attempted. The consequence was that Harthill\u2019s work\nseemed to be in vain, replaced by the work of a more virtuosic\nhacker\u2019s demonstration of a superior direction.\nIn the case of Apache one can see how coordination in Free Soft-\nware is not just an afterthought or a necessary feature of distributed\nwork, but is in fact at the core of software production itself, govern-\ning the norms and forms of life that determine what will count as\ngood software, how it will progress with respect to a context and\n228 coordinating collaborations background, and how people will be expected to interact around\nthe topic of design decisions. The privileging of adaptability brings\nwith it a choice in the mode of collaboration: it resolves the tension\nbetween the agonistic competitive creation of software, such as\nRobert Thau\u2019s creation of Shambhala, and the need for collective\ncoordination of complexity, such as Harthill\u2019s plea for collabora-\ntion to reduce duplicated or unnecessary work.\nCheck Out and Commit\nThe technical and social forms that Linux and Apache take are\nenabled by the tools they build and use, from bug-tracking tools\nand mailing lists to the Web servers and kernels themselves. One\nsuch tool plays a very special role in the emergence of these or-\nganizations: Source Code Management systems (SCMs). SCMs are\ntools for coordinating people and code; they allow multiple people\nin dispersed locales to work simultaneously on the same object,\nthe same source code, without the need for a central coordinating\noverseer and without the risk of stepping on each other\u2019s toes. The\nhistory of SCMs\u2014especially in the case of Linux\u2014also illustrates\nthe recursive-depth problem: namely, is Free Software still free if it\nis created with non-free tools?\nSCM tools, like the Concurrent Versioning System (cvs) and Sub-\nversion, have become extremely common tools for Free Software\nprogrammers; indeed, it is rare to find a project, even a project\nconducted by only one individual, which does not make use of these\ntools. Their basic function is to allow two or more programmers to\nwork on the same files at the same time and to provide feedback on\nwhere their edits conflict. When the number of programmers grows\nlarge, an SCM can become a tool for managing complexity. It keeps\ntrack of who has \u201cchecked out\u201d files; it enables users to lock files\nif they want to ensure that no one else makes changes at the same\ntime; it can keep track of and display the conflicting changes made\nby two users to the same file; it can be used to create \u201cinternal\u201d\nforks or \u201cbranches\u201d that may be incompatible with each other, but\nstill allows programmers to try out new things and, if all goes well,\nmerge the branches into the trunk later on. In sophisticated forms\nit can be used to \u201canimate\u201d successive changes to a piece of code,\nin order to visualize its evolution.\ncoordinating collaborations 229 Beyond mere coordination functions, SCMs are also used as a\nform of distribution; generally SCMs allow anyone to check out\nthe code, but restrict those who can check in or \u201ccommit\u201d the code.\nThe result is that users can get instant access to the most up-to-date\nversion of a piece of software, and programmers can differentiate\nbetween stable releases, which have few bugs, and \u201cunstable\u201d or\nexperimental versions that are under construction and will need the\nhelp of users willing to test and debug the latest versions. SCM tools\nautomate certain aspects of coordination, not only reducing the\nlabor involved but opening up new possibilities for coordination.\nThe genealogy of SCMs can be seen in the example of Ken\nThompson\u2019s creation of a diff tape, which he used to distribute\nchanges that had been contributed to UNIX. Where Thompson\nsaw UNIX as a spectrum of changes and the legal department at\nBell Labs saw a series of versions, SCM tools combine these two\napproaches by minutely managing the revisions, assigning each\nchange (each diff ) a new version number, and storing the history\nof all of those changes so that software changes might be precisely\nundone in order to discover which changes cause problems. Writ-\nten by Douglas McIlroy, \u201cdiff \u201d is itself a piece of software, one of\nthe famed small UNIX tools that do one thing well. The program\ndiff compares two files, line by line, and prints out the differences\nbetween them in a structured format (showing a series of lines with\ncodes that indicate changes, additions, or removals). Given two\nversions of a text, one could run diff to find the differences and\nmake the appropriate changes to synchronize them, a task that is\notherwise tedious and, given the exactitude of source code, prone\nto human error. A useful side-effect of diff (when combined with\nan editor like ed or EMACS) is that when someone makes a set of\nchanges to a file and runs diff on both the original and the changed\nfile, the output (i.e., the changes only) can be used to reconstruct\nthe original file from the changed file. Diff thus allows for a clever,\nspace-saving way to save all the changes ever made to a file, rather\nthan retaining full copies of every new version, one saves only the\nchanges. Ergo, version control. diff\u2014and programs like it\u2014became\nthe basis for managing the complexity of large numbers of pro-\ngrammers working on the same text at the same time.\nOne of the first attempts to formalize version control was Walter\nTichy\u2019s Revision Control System (RCS), from 1985.35 RCS kept track\nof the changes to different files using diff and allowed programmers\n230 coordinating collaborations to see all of the changes that had been made to that file. RCS, how-\never, could not really tell the difference between the work of one\nprogrammer and another. All changes were equal, in that sense,\nand any questions that might arise about why a change was made\ncould remain unanswered.\nIn order to add sophistication to RCS, Dick Grune, at the Vrije\nUniversiteit, Amsterdam, began writing scripts that used RCS as\na multi-user, Internet-accessible version-control system, a system\nthat eventually became the Concurrent Versioning System. cvs al-\nlowed multiple users to check out a copy, make changes, and then\ncommit those changes, and it would check for and either prevent or\nflag conflicting changes. Ultimately, cvs became most useful when\nprogrammers could use it remotely to check out source code from\nanywhere on the Internet. It allowed people to work at different\nspeeds, different times, and in different places, without needing a\ncentral person in charge of checking and comparing the changes.\ncvs created a form of decentralized version control for very-large-\nscale collaboration; developers could work offline on software, and\nalways on the most updated version, yet still be working on the\nsame object.\nBoth the Apache project and the Linux kernel project use SCMs.\nIn the case of Apache the original patch-and-vote system quickly\nbegan to strain the patience, time, and energy of participants as\nthe number of contributors and patches began to grow. From the\nvery beginning of the project, the contributor Paul Richards had\nurged the group to make use of cvs. He had extensive experience\nwith the system in the Free-BSD project and was convinced that\nit provided a superior alternative to the patch-and-vote system.\nFew other contributors had much experience with it, however, so\nit wasn\u2019t until over a year after Richards began his admonitions\nthat cvs was eventually adopted. However, cvs is not a simple re-\nplacement for a patch-and-vote system; it necessitates a different\nkind of organization. Richards recognized the trade-off. The patch-\nand-vote system created a very high level of quality assurance and\npeer review of the patches that people submitted, while the cvs\nsystem allowed individuals to make more changes that might not\nmeet the same level of quality assurance. The cvs system allowed\nbranches\u2014stable, testing, experimental\u2014with different levels of\nquality assurance, while the patch-and-vote system was inherently\ndirected at one final and stable version. As the case of Shambhala\ncoordinating collaborations 231 exhibited, under the patch-and-vote system experimental versions\nwould remain unofficial garage projects, rather than serve as of-\nficial branches with people responsible for committing changes.\nWhile SCMs are in general good for managing conflicting\nchanges, they can do so only up to a point. To allow anyone to\ncommit a change, however, could result in a chaotic mess, just as\ndifficult to disentangle as it would be without an SCM. In practice,\ntherefore, most projects designate a handful of people as having\nthe right to \u201ccommit\u201d changes. The Apache project retained its vot-\ning scheme, for instance, but it became a way of voting for \u201ccom-\nmitters\u201d instead for patches themselves. Trusted committers\u2014those\nwith the mysterious \u201cgood taste,\u201d or technical intuition\u2014became\nthe core members of the group.\nThe Linux kernel has also struggled with various issues surround-\ning SCMs and the management of responsibility they imply. The\nstory of the so-called VGER tree and the creation of a new SCM\ncalled Bitkeeper is exemplary in this respect.36 By 1997, Linux de-\nvelopers had begun to use cvs to manage changes to the source\ncode, though not without resistance. Torvalds was still in charge\nof the changes to the official stable tree, but as other \u201clieutenants\u201d\ncame on board, the complexity of the changes to the kernel grew.\nOne such lieutenant was Dave Miller, who maintained a \u201cmirror\u201d of\nthe stable Linux kernel tree, the VGER tree, on a server at Rutgers.\nIn September 1998 a fight broke out among Linux kernel develop-\ners over two related issues: one, the fact that Torvalds was failing\nto incorporate (patch) contributions that had been forwarded to\nhim by various people, including his lieutenants; and two, as a\nresult, the VGER cvs repository was no longer in synch with the\nstable tree maintained by Torvalds. Two different versions of Linux\nthreatened to emerge.\nA great deal of yelling ensued, as nicely captured in Moody\u2019s\nRebel Code, culminating in the famous phrase, uttered by Larry\nMcVoy: \u201cLinus does not scale.\u201d The meaning of this phrase is that\nthe ability of Linux to grow into an ever larger project with increas-\ning complexity, one which can handle myriad uses and functions\n(to \u201cscale\u201d up), is constrained by the fact that there is only one\nLinus Torvalds. By all accounts, Linus was and is excellent at what\nhe does\u2014but there is only one Linus. The danger of this situation\nis the danger of a fork. A fork would mean one or more new ver-\nsions would proliferate under new leadership, a situation much like\n232 coordinating collaborations the spread of UNIX. Both the licenses and the SCMs are designed to\nfacilitate this, but only as a last resort. Forking also implies dilution\nand confusion\u2014competing versions of the same thing and poten-\ntially unmanageable incompatibilities.\nThe fork never happened, however, but only because Linus went\non vacation, returning renewed and ready to continue and to be\nmore responsive. But the crisis had been real, and it drove devel-\nopers into considering new modes of coordination. Larry McVoy\noffered to create a new form of SCM, one that would allow a much\nmore flexible response to the problem that the VGER tree repre-\nsented. However, his proposed solution, called Bitkeeper, would\ncreate far more controversy than the one that precipitated it.\nMcVoy was well-known in geek circles before Linux. In the late\nstages of the open-systems era, as an employee of Sun, he had\npenned an important document called \u201cThe Sourceware Operating\nSystem Proposal.\u201d It was an internal Sun Microsystems document\nthat argued for the company to make its version of UNIX freely\navailable. It was a last-ditch effort to save the dream of open sys-\ntems. It was also the first such proposition within a company to \u201cgo\nopen source,\u201d much like the documents that would urge Netscape to\nOpen Source its software in 1998. Despite this early commitment,\nMcVoy chose not to create Bitkeeper as a Free Software project, but\nto make it quasi-proprietary, a decision that raised a very central\nquestion in ideological terms: can one, or should one, create Free\nSoftware using non-free tools?\nOn one side of this controversy, naturally, was Richard Stallman\nand those sharing his vision of Free Software. On the other were\npragmatists like Torvalds claiming no goals and no commitment to\n\u201cideology\u201d\u2014only a commitment to \u201cfun.\u201d The tension laid bare the\nway in which recursive publics negotiate and modulate the core\ncomponents of Free Software from within. Torvalds made a very\nstrong and vocal statement concerning this issue, responding to\nStallman\u2019s criticisms about the use of non-free software to create\nFree Software: \u201cQuite frankly, I don\u2019t _want_ people using Linux for\nideological reasons. I think ideology sucks. This world would be a\nmuch better place if people had less ideology, and a whole lot more\n\u2018I do this because it\u2019s FUN and because others might find it useful,\nnot because I got religion.\u2019 \u201d37\nTorvalds emphasizes pragmatism in terms of coordination: the\nright tool for the job is the right tool for the job. In terms of licenses,\ncoordinating collaborations 233 however, such pragmatism does not play, and Torvalds has always\nbeen strongly committed to the GPL, refusing to let non-GPL soft-\nware into the kernel. This strategic pragmatism is in fact a rec-\nognition of where experimental changes might be proposed, and\nwhere practices are settled. The GPL was a stable document, shar-\ning source code widely was a stable practice, but coordinating a\nproject using SCMs was, during this period, still in flux, and thus\nBitkeeper was a tool well worth using so long as it remained suit-\nable to Linux development. Torvalds was experimenting with the\nmeaning of coordination: could a non-free tool be used to create\nFree Software?\nMcVoy, on the other hand, was on thin ice. He was experiment-\ning with the meaning of Free Software licenses. He created three\nseparate licenses for Bitkeeper in an attempt to play both sides: a\ncommercial license for paying customers, a license for people who\nsell Bitkeeper, and a license for \u201cfree users.\u201d The free-user license\nallowed Linux developers to use the software for free\u2014though it\nrequired them to use the latest version\u2014and prohibited them from\nworking on a competing project at the same time. McVoy\u2019s attempt\nto have his cake and eat it, too, created enormous tension in the de-\nveloper community, a tension that built from 2002, when Torvalds\nbegan using Bitkeeper in earnest, to 2005, when he announced he\nwould stop.\nThe tension came from two sources: the first was debates among\ndevelopers addressing the moral question of using non-free soft-\nware to create Free Software. The moral question, as ever, was\nalso a technical one, as the second source of tension, the license\nrestrictions, would reveal.\nThe developer Andrew Trigdell, well known for his work on a\nproject called Samba and his reverse engineering of a Microsoft net-\nworking protocol, began a project to reverse engineer Bitkeeper by\nlooking at the metadata it produced in the course of being used for\nthe Linux project. By doing so, he crossed a line set up by McVoy\u2019s\nexperimental licensing arrangement: the \u201cfree as long as you don\u2019t\ncopy me\u201d license. Lawyers advised Trigdell to stay silent on the\ntopic while Torvalds publicly berated him for \u201cwillful destruction\u201d\nand a moral lapse of character in trying to reverse engineer Bit-\nkeeper. Bruce Perens defended Trigdell and censured Torvalds for\nhis seemingly contradictory ethics.38 McVoy never sued Trigdell,\nand Bitkeeper has limped along as a commercial project, because,\n234 coordinating collaborations much like the EMACS controversy of 1985, the Bitkeeper contro-\nversy of 2005 ended with Torvalds simply deciding to create his\nown SCM, called git.\nThe story of the VGER tree and Bitkeeper illustrate common ten-\nsions within recursive publics, specifically, the depth of the mean-\ning of free. On the one hand, there is Linux itself, an exemplary\nFree Software project made freely available; on the other hand,\nhowever, there is the ability to contribute to this process, a pro-\ncess that is potentially constrained by the use of Bitkeeper. So long\nas the function of Bitkeeper is completely circumscribed\u2014that is,\ncompletely planned\u2014there can be no problem. However, the mo-\nment one user sees a way to change or improve the process, and\nnot just the kernel itself, then the restrictions and constraints of\nBitkeeper can come into play. While it is not clear that Bitkeeper\nactually prevented anything, it is also clear that developers clearly\nrecognized it as a potential drag on a generalized commitment to\nadaptability. Or to put it in terms of recursive publics, only one\nlayer is properly open, that of the kernel itself; the layer beneath\nit, the process of its construction, is not free in the same sense. It is\nironic that Torvalds\u2014otherwise the spokesperson for antiplanning\nand adaptability\u2014willingly adopted this form of constraint, but not\nat all surprising that it was collectively rejected.\nThe Bitkeeper controversy can be understood as a kind of ex-\nperiment, a modulation on the one hand of the kinds of accept-\nable licenses (by McVoy) and on the other of acceptable forms of\ncoordination (Torvalds\u2019s decision to use Bitkeeper). The experiment\nwas a failure, but a productive one, as it identified one kind of non-\nfree software that is not safe to use in Free Software development:\nthe SCM that coordinates the people and the code they contribute.\nIn terms of recursive publics the experiment identified the proper\ndepth of recursion. Although it might be possible to create Free\nSoftware using some kinds of non-free tools, SCMs are not among\nthem; both the software created and the software used to create it\nneed to be free.39\nThe Bitkeeper controversy illustrates again that adaptability\nis not about radical invention, but about critique and response.\nWhereas controlled design and hierarchical planning represent the\ndomain of governance\u2014control through goal-setting and orienta-\ntion of a collective or a project\u2014adaptability privileges politics,\nproperly speaking, the ability to critique existing design and to\ncoordinating collaborations 235 propose alternatives without restriction. The tension between goal-\nsetting and adaptability is also part of the dominant ideology of\nintellectual property. According to this ideology, IP laws promote\ninvention of new products and ideas, but restrict the re-use or trans-\nformation of existing ones; defining where novelty begins is a core\ntest of the law. McVoy made this tension explicit in his justifica-\ntions for Bitkeeper: \u201cRichard [Stallman] might want to consider the\nfact that developing new software is extremely expensive. He\u2019s very\nproud of the collection of free software, but that\u2019s a collection of\nre-implementations, but no profoundly new ideas or products. . . .\nWhat if the free software model simply can\u2019t support the costs of\ndeveloping new ideas?\u201d40\nNovelty, both in the case of Linux and in intellectual property\nlaw more generally, is directly related to the interplay of social and\ntechnical coordination: goal direction vs. adaptability. The ideal of\nadaptability promoted by Torvalds suggests a radical alternative\nto the dominant ideology of creation embedded in contemporary\nintellectual-property systems. If Linux is \u201cnew,\u201d it is new through\nadaptation and the coordination of large numbers of creative con-\ntributors who challenge the \u201cdesign\u201d of an operating system from\nthe bottom up, not from the top down. By contrast, McVoy rep-\nresents a moral imagination of design in which it is impossible\nto achieve novelty without extremely expensive investment in top-\ndown, goal-directed, unpolitical design\u2014and it is this activity that\nthe intellectual-property system is designed to reward. Both are\nengaged, however, in an experiment; both are engaged in \u201cfiguring\nout\u201d what the limits of Free Software are.\nCoordination Is Design\nMany popular accounts of Free Software skip quickly over the de-\ntails of its mechanism to suggest that it is somehow inevitable or\nobvious that Free Software should work\u2014a self-organizing, emer-\ngent system that manages complexity through distributed contri-\nbutions by hundreds of thousands of people. In The Success of Open\nSource Steven Weber points out that when people refer to Open\nSource as a self-organizing system, they usually mean something\nmore like \u201cI don\u2019t understand how it works.\u201d41\n236 coordinating collaborations Eric Raymond, for instance, suggests that Free Software is essen-\ntially the emergent, self-organizing result of \u201ccollaborative debug-\nging\u201d: \u201cGiven enough eyeballs, all bugs are shallow.\u201d42 The phrase\nimplies that the core success of Free Software is the distributed,\nisolated, labor of debugging, and that design and planning happen\nelsewhere (when a developer \u201cscratches an itch\u201d or responds to a\npersonal need). On the surface, such a distinction seems quite obvi-\nous: designing is designing, and debugging is removing bugs from\nsoftware, and presto!\u2014Free Software. At the extreme end, it is an\nunderstanding by which only individual geniuses are capable of\nplanning and design, and if the initial conditions are properly set,\nthen collective wisdom will fill in the details.\nHowever, the actual practice and meaning of collective or col-\nlaborative debugging is incredibly elastic. Sometimes debugging\nmeans fixing an error; sometimes it means making the software do\nsomething different or new. (A common joke, often made at Micro-\nsoft\u2019s expense, captures some of this elasticity: whenever something\ndoesn\u2019t seem to work right, one says, \u201cThat\u2019s a feature, not a bug.\u201d)\nSome programmers see a design decision as a stupid mistake and\ntake action to correct it, whereas others simply learn to use the\nsoftware as designed. Debugging can mean something as simple as\nreading someone else\u2019s code and helping them understand why it\ndoes not work; it can mean finding bugs in someone else\u2019s software;\nit can mean reliably reproducing bugs; it can mean pinpointing\nthe cause of the bug in the source code; it can mean changing the\nsource to eliminate the bug; or it can, at the limit, mean changing\nor even re-creating the software to make it do something different\nor better.43 For academics, debugging can be a way to build a ca-\nreer: \u201cFind bug. Write paper. Fix bug. Write paper. Repeat.\u201d44 For\ncommercial software vendors, by contrast, debugging is part of a\nbattery of tests intended to streamline a product.\nCoordination in Free Software is about adaptability over planning.\nIt is a way of resolving the tension between individual virtuosity in\ncreation and the social benefit in shared labor. If all software were\ncreated, maintained, and distributed only by individuals, coordina-\ntion would be superfluous, and software would indeed be part of\nthe domain of poetry. But even the paradigmatic cases of virtuosic\ncreation\u2014EMACS by Richard Stallman, UNIX by Ken Thompson\nand Dennis Ritchie\u2014clearly represent the need for creative forms\ncoordinating collaborations 237 of coordination and the fundamental practice of reusing, rework-\ning, rewriting, and imitation. UNIX was not created de novo, but\nwas an attempt to streamline and rewrite Multics, itself a system\nthat evolved out of Project MAC and the early mists of time-sharing\nand computer hacking.45 EMACS was a reworking of the TECO\neditor. Both examples are useful for understanding the evolution of\nmodes of coordination and the spectrum of design and debugging.\nUNIX was initially ported and shared through mixed academic\nand commercial means, through the active participation of com-\nputer scientists who both received updates and contributed fixes\nback to Thompson and Ritchie. No formal system existed to man-\nage this process. When Thompson speaks of his understanding of\nUNIX as a \u201cspectrum\u201d and not as a series of releases (V1, V2, etc.),\nthe implication is that work on UNIX was continuous, both within\nBell Labs and among its widespread users. Thompson\u2019s use of the\ndiff tape encapsulates the core problem of coordination: how to col-\nlect and redistribute the changes made to the system by its users.\nSimilarly, Bill Joy\u2019s distribution of BSD and James Gosling\u2019s\ndistribution of GOSMACS were both ad hoc, noncorporate experi-\nments in \u201creleasing early and often.\u201d These distribution schemes\nhad a purpose (beyond satisfying demand for the software). The\nfrequent distribution of patches, fixes, and extensions eased the\npain of debugging software and satisfied users\u2019 demands for new\nfeatures and extensions (by allowing them to do both themselves).\nHad Thompson and Ritchie followed the conventional corporate\nmodel of software production, they would have been held respon-\nsible for thoroughly debugging and testing the software they dis-\ntributed, and AT&T or Bell Labs would have been responsible for\ncoming up with all innovations and extensions as well, based on\nmarketing and product research. Such an approach would have\nsacrificed adaptability in favor of planning. But Thompson\u2019s and\nRitchie\u2019s model was different: both the extension and the debug-\nging of software became shared responsibilities of the users and\nthe developers. Stallman\u2019s creation of EMACS followed a similar\npattern; since EMACS was by design extensible and intended to\nsatisfy myriad unforeseen needs, the responsibility rested on the\nusers to address those needs, and sharing their extensions and fixes\nhad obvious social benefit.\nThe ability to see development of software as a spectrum implies\nmore than just continuous work on a product; it means seeing the\n238 coordinating collaborations product itself as something fluid, built out of previous ideas and\nproducts and transforming, differentiating into new ones. Debug-\nging, from this perspective, is not separate from design. Both are\npart of a spectrum of changes and improvements whose goals and\ndirection are governed by the users and developers themselves, and\nthe patterns of coordination they adopt. It is in the space between\ndebugging and design that Free Software finds its niche.\nConclusion: Experiments and Modulations\nCoordination is a key component of Free Software, and is frequently\nidentified as the central component. Free Software is the result of\na complicated story of experimentation and construction, and the\nforms that coordination takes in Free Software are specific out-\ncomes of this longer story. Apache and Linux are both experiments\u2014\nnot scientific experiments per se but collective social experiments\nin which there are complex technologies and legal tools, systems\nof coordination and governance, and moral and technical orders\nalready present.\nFree Software is an experimental system, a practice that changes\nwith the results of new experiments. The privileging of adaptability\nmakes it a peculiar kind of experiment, however, one not directed\nby goals, plans, or hierarchical control, but more like what John\nDewey suggested throughout his work: the experimental praxis of\nscience extended to the social organization of governance in the\nservice of improving the conditions of freedom. What gives this ex-\nperimentation significance is the centrality of Free Software\u2014and\nspecifically of Linux and Apache\u2014to the experimental expansion\nof the Internet. As an infrastructure or a milieu, the Internet is\nchanging the conditions of social organization, changing the rela-\ntionship of knowledge to power, and changing the orientation of\ncollective life toward governance. Free Software is, arguably, the\nbest example of an attempt to make this transformation public, to\nensure that it uses the advantages of adaptability as critique to\ncounter the power of planning as control. Free Software, as a re-\ncursive public, proceeds by proposing and providing alternatives. It\nis a bit like Kant\u2019s version of enlightenment: insofar as geeks speak\n(or hack) as scholars, in a public realm, they have a right to pro-\npose criticisms and changes of any sort; as soon as they relinquish\ncoordinating collaborations 239 that commitment, they become private employees or servants of\nthe sovereign, bound by conscience and power to carry out the du-\nties of their given office. The constitution of a public realm is not\na universal activity, however, but a historically specific one: Free\nSoftware confronts the specific contemporary technical and legal\ninfrastructure by which it is possible to propose criticisms and offer\nalternatives. What results is a recursive public filled not only with\nindividuals who govern their own actions but also with code and\nconcepts and licenses and forms of coordination that turn these\nactions into viable, concrete technical forms of life useful to inhabi-\ntants of the present.\n240 coordinating collaborations Part III modulations\nThe question cannot be answered by argument. Experimental\nmethod means experiment, and the question can be answered only\nby trying, by organized effort. The reasons for making the trial are\nnot abstract or recondite. They are found in the confusion, uncer-\ntainty and conflict that mark the modern world. . . . The task is\nto go on, and not backward, until the method of intelligence and\nexperimental control is the rule in social relations and social direc-\ntion. \u2014john dewey, Li beralism and Social Act ion  8.\n\u201cIf We Succeed, We Will Disappear\u201d\nIn early 2002, after years of reading and learning about Open\nSource and Free Software, I finally had a chance to have dinner\nwith famed libertarian, gun-toting, Open Source\u2013founding impresa-\nrio Eric Raymond, author of The Cathedral and the Bazaar and other\namateur anthropological musings on the subject of Free Software.\nHe had come to Houston, to Rice University, to give a talk at the be-\nhest of the Computer and Information Technology Institute (CITI).\nVisions of a mortal confrontation between two anthropologists-\nmanqu\u00e9 filled my head. I imagined explaining point by point why\nhis references to self-organization and evolutionary psychology\nwere misguided, and how the long tradition of economic anthro-\npology contradicted basically everything he had to say about gift-\nexchange. Alas, two things conspired against this epic, if bathetic,\nshowdown.\nFirst, there was the fact that (as so often happens in meetings\namong geeks) there was only one woman present at dinner; she was young, perhaps unmarried, but not a student\u2014an interested female\nhacker. Raymond seated himself beside this woman, turned toward\nher, and with a few one-minute-long exceptions proceeded to lavish\nher with all of his available attention. The second reason was that\nI was seated next to Richard Baraniuk and Brent Hendricks. All at\nonce, Raymond looked like the past of Free Software, arguing the\nsame arguments, using the same rhetoric of his online publications,\nwhile Baraniuk and Hendricks looked like its future, posing ques-\ntions about the transformation\u2014the modulation\u2014of Free Software\ninto something surprising and new.\nBaraniuk, a professor of electrical engineering and a specialist\nin digital signal processing, and Hendricks, an accomplished pro-\ngrammer, had started a project called Connexions, an \u201copen con-\ntent repository of educational materials.\u201d Far more interesting to\nme than Raymond\u2019s amateur philosophizing was this extant project\nto extend the ideas of Free Software to the creation of educational\nmaterials\u2014textbooks, in particular.\nRich and Brent were, by the looks of it, equally excited to be\nseated next to me, perhaps because I was answering their ques-\ntions, whereas Raymond was not, or perhaps because I was a new\nhire at Rice University, which meant we could talk seriously about\ncollaboration. Rich and Brent (and Jan Odegard, who, as direc-\ntor of CITI, had organized the dinner) were keen to know what\nI could add to help them understand the \u201csocial\u201d aspects of what\nthey wanted to do with Connexions, and I, in return, was equally\neager to learn how they conceptualized their Free Software\u2013like\nproject: what had they kept the same and what had they changed\nin their own experiment? Whatever they meant by \u201csocial\u201d (and\nsometimes it meant ethical, sometimes legal, sometimes cultural,\nand so on), they were clear that there were domains of expertise in\nwhich they felt comfortable (programming, project management,\nteaching, and a particular kind of research in computer science\nand electrical engineering) and domains in which they did not\n(the \u201cnorms\u201d of academic life outside their disciplines, intellectual-\nproperty law, \u201cculture\u201d). Although I tried to explain the nature of\nmy own expertise in social theory, philosophy, history, and ethno-\ngraphic research, the academic distinctions were far less important\nthan the fact that I could ask detailed and pointed questions about\nthe project, questions that indicated to them that I must have some\nkind of stake in the domains that they needed filled\u2014in particular,\n244 \u201cif we succeed, we will disappear\u201d around the question of whether Connexions was the same thing as\nFree Software, and what the implications of that might be.\nRaymond courted and chattered on, then left, the event of his talk\nand dinner of fading significance, but over the following weeks, as\nI caught up with Brent and Rich, I became (surprisingly quickly)\npart of their novel experiment.\nAfter Free Software\nMy nonmeeting with Raymond is an allegory of sorts: an allegory\nof what comes after Free Software. The excitement around that\ntable was not so much about Free Software or Open Source, but\nabout a certain possibility, a kind of genotypic urge of which Free\nSoftware seemed a fossil phenotype and Connexions a live one.\nRich and Brent were people in the midst of figuring something out.\nThey were engaged in modulating the practices of Free Software.\nBy modulation I mean exploring in detail the concrete practices\u2014\nthe how\u2014of Free Software in order to ask what can be changed,\nand what cannot, in order to maintain something (openness?) that\nno one can quite put his finger on. What drew me immediately to\nConnexions was that it was related to Free Software, not meta-\nphorically or ideologically, but concretely, practically, and experi-\nmentally, a relationship that was more about emergence out of than\nit was about the reproduction of forms. But the opposition between\nemergence and reproduction immediately poses a question, not un-\nlike that of the identity of species in evolution: if Free Software is\nno longer software, what exactly is it?\nIn part III I confront this question directly. Indeed, it was this\nquestion that necessitated part II, the analytic decomposition of\nthe practices and histories of Free Software. In order to answer\nthe question \u201cIs Connexions Free Software?\u201d (or vice versa) it was\nnecessary to rethink Free Software as itself a collective, technical\nexperiment, rather than as an expression of any ideology or culture.\nTo answer yes, or no, however, merely begs the question \u201cWhat is\nFree Software?\u201d What is the cultural significance of these practices?\nThe concept of a recursive public is meant to reveal in part the\nsignificance of both Free Software and emergent projects like Con-\nnexions; it is meant to help chart when these emergent projects\nbranch off absolutely (cease to be public) and when they do not, by\n\u201cif we succeed, we will disappear\u201d 245 focusing on how they modulate the five components that give Free\nSoftware its contemporary identity.\nConnexions modulates all of the components except that of the\nmovement (there is, as of yet, no real \u201cFree Textbook\u201d movement,\nbut the \u201cOpen Access\u201d movement is a close second cousin).1 Perhaps\nthe most complex modulation concerns coordination\u2014changes to\nthe practice of coordination and collaboration in academic-textbook\ncreation in particular, and more generally to the nature of collabo-\nration and coordination of knowledge in science and scholarship\ngenerally.\nConnexions emerged out of Free Software, and not, as one might\nexpect, out of education, textbook writing, distance education, or\nany of those areas that are topically connected to pedagogy. That is\nto say, the people involved did not come to their project by attempt-\ning to deal with a problem salient to education and teaching as\nmuch as they did so through the problems raised by Free Software\nand the question of how those problems apply to university text-\nbooks. Similarly, a second project, Creative Commons, also emerged\nout of a direct engagement with and exploration of Free Software,\nand not out of any legal movement or scholarly commitment to\nthe critique of intellectual-property law or, more important, out of\nany desire to transform the entertainment industry. Both projects\nare resolutely committed to experimenting with the given practices\nof Free Software\u2014to testing their limits and changing them where\nthey can\u2014and this is what makes them vibrant, risky, and poten-\ntially illuminating as cases of a recursive public.\nWhile both initiatives are concerned with conventional subject\nareas (educational materials and cultural productions), they enter\nthe fray orthogonally, armed with anxiety about the social and\nmoral order in which they live, and an urge to transform it by\nmodulating Free Software. This binds such projects across substan-\ntive domains, in that they are forced to be oppositional, not because\nthey want to be (the movement comes last), but because they enter\nthe domains of education and the culture industry as outsiders.\nThey are in many ways intuitively troubled by the existing state\nof affairs, and their organizations, tools, legal licenses, and move-\nments are seen as alternative imaginations of social order, espe-\ncially concerning creative freedom and the continued existence of a\ncommons of scholarly knowledge. To the extent that these projects\n246 \u201cif we succeed, we will disappear\u201d remain in an orthogonal relationship, they are making a recursive\npublic appear\u2014something the textbook industry and the entertain-\nment industry are, by contrast, not at all interested in doing, for\nobvious financial and political reasons.\nStories of Connexion\nI\u2019m at dinner again. This time, a windowless hotel conference room\nin the basement maybe, or perhaps high up in the air. Lawyers, ac-\nademics, activists, policy experts, and foundation people are semi-\nexcitedly working their way through the hotel\u2019s steam-table fare.\nI\u2019m trying to tell a story to the assembled group\u2014a story that I have\nheard Rich Baraniuk tell a hundred times\u2014but I\u2019m screwing it up.\nRich always gets enthusiastic stares of wonder, light-bulbs going\noff everywhere, a subvocalized \u201cAha!\u201d or a vigorous nod. I, on the\nother hand, am clearly making it too complicated. Faces and fore-\nheads are squirmed up into lines of failed comprehension, people\nstare at the gravy-sodden food they\u2019re soldiering through, weighing\nthe option of taking another bite against listening to me complicate\nan already complicated world. I wouldn\u2019t be doing this, except that\nRich is on a plane, or in a taxi, delayed by snow or engineers or\nperhaps at an eponymous hotel in another city. Meanwhile, our\nco-organizer Laurie Racine, has somehow convinced herself that I\nhave the childlike enthusiasm necessary to channel Rich. I\u2019m flat-\ntered, but unconvinced. After about twenty minutes, so is she, and\nas I try to answer a question, she stops me and interjects, \u201cRich re-\nally needs to be here. He should really be telling this story.\u201d\nMiraculously, he shows up and, before he can even say hello,\nis conscripted into telling his story properly. I sigh in relief and\npray that I\u2019ve not done any irreparable damage and that I can go\nback to my role as straight man. I can let the superaltern speak for\nhimself. The downside of participant observation is being asked to\nparticipate in what one had hoped first of all to observe. I do know\nthe story\u2014I have heard it a hundred times. But somehow what I\nhear, ears tuned to academic questions and marveling at some of\nthe stranger claims he makes, somehow this is not the ear for en-\nlightenment that his practiced and boyish charm delivers to those\nhearing it for the first time; it is instead an ear tuned to questions\n\u201cif we succeed, we will disappear\u201d 247 of why: why this project? Why now? And even, somewhat convo-\nlutedly, why are people so fascinated when he tells the story? How\ncould I tell it like Rich?\nRich is an engineer, in particular, a specialist in Digital Signal\nProcessing (DSP). DSP is the science of signals. It is in everything,\nsays Rich: your cell phones, your cars, your CD players, all those\ndevices. It is a mathematical discipline, but it is also an intensely\npractical one, and it\u2019s connected to all kinds of neighboring fields\nof knowledge. It is the kind of discipline that can connect calculus,\nbioinformatics, physics, and music. The statistical and analytical\ntechniques come from all sorts of research and end up in all kinds\nof interesting devices. So Rich often finds himself trying to teach\nstudents to make these kinds of connections\u2014to understand that a\nFourier transform is not just another chapter in calculus but a tool\nfor manipulating signals, whether in bioinformatics or in music.\nAround 1998 or 1999, Rich decided that it was time for him to\nwrite a textbook on DSP, and he went to the dean of engineering,\nSidney Burris, to tell him about the idea. Burris, who is also a DSP\nman and longtime member of the Rice University community, said\nsomething like, \u201cRich, why don\u2019t you do something useful?\u201d By\nwhich he meant: there are a hundred DSP textbooks out there, so\nwhy do you want to write the hundred and first? Burris encouraged\nRich to do something bigger, something ambitious enough to put\nRice on the map. I mention this because it is important to note that\neven a university like Rice, with a faculty and graduate students\non par with the major engineering universities of the country, per-\nceives that it gets no respect. Burris was, and remains, an inveterate\nsupporter of Connexions, precisely because it might put Rice \u201cin the\nhistory books\u201d for having invented something truly novel.\nAt about the same time as his idea for a textbook, Rich\u2019s research\ngroup was switching over to Linux, and Rich was first learning\nabout Open Source and the emergence of a fully free operating\nsystem created entirely by volunteers. It isn\u2019t clear what Rich\u2019s aha!\nmoment was, other than simply when he came to an understand-\ning that such a thing as Linux was actually possible. Nonetheless,\nat some point, Rich had the idea that his textbook could be an\nOpen Source textbook, that is, a textbook created not just by him,\nbut by DSP researchers all over the world, and made available to\neveryone to make use of and modify and improve as they saw fit,\njust like Linux. Together with Brent Hendricks, Yan David Erlich,\n248 \u201cif we succeed, we will disappear\u201d and Ross Reedstrom, all of whom, as geeks, had a deep familiarity\nwith the history and practices of Free and Open Source Software,\nRich started to conceptualize a system; they started to think about\nmodulations of different components of Free and Open Source Soft-\nware. The idea of a Free Software textbook repository slowly took\nshape.\nThus, Connexions: an \u201copen content repository of high-quality ed-\nucational materials.\u201d These \u201ctextbooks\u201d very quickly evolved into\nsomething else: \u201cmodules\u201d of content, something that has never\nbeen sharply defined, but which corresponds more or less to a small\nchunk of teachable information, like two or three pages in a text-\nbook. Such modules are much easier to conceive of in sciences like\nmathematics or biology, in which textbooks are often multiauthored\ncollections, finely divided into short chapters with diagrams, exer-\ncises, theorems, or programs. Modules lend themselves much less\nwell to a model of humanities or social-science scholarship based in\nreading texts, discussion, critique, and comparison\u2014and this bias is\na clear reflection of what Brent, Ross, and Rich knew best in terms\nof teaching and writing. Indeed, the project\u2019s frequent recourse to\nthe image of an assembly-line model of knowledge production often\nconfirms the worst fears of humanists and educators when they first\nencounter Connexions. The image suggests that knowledge comes\nin prepackaged and colorfully branded tidbits for the delectation\nof undergrads, rather than characterizing knowledge as a state of\nbeing or as a process.\nThe factory image (figure 7) is a bit misleading. Rich\u2019s and Brent\u2019s\nimaginations are in fact much broader, which shows whenever they\ndemo the project, or give a talk, or chat at a party about it. Part of\nmy failure to communicate excitement when I tell the story of Con-\nnexions is that I skip the examples, which is where Rich starts: what\nif, he says, you are a student taking Calculus 101 and, at the same\ntime, Intro to Signals and Systems\u2014no one is going to explain to\nyou how Fourier transforms form a bridge, or connection, between\nthem. \u201cOur brains aren\u2019t organized in linear, chapter-by-chapter\nways,\u201d Rich always says, \u201cso why are our textbooks?\u201d How can we\ngive students a way to see the connection between statistics and\ngenetics, between architecture and biology, between intellectual-\nproperty law and chemical engineering? Rich is always looking for\nnew examples: a music class for kids that uses information from\nphysics, or vice versa, for instance. Rich\u2019s great hope is that the\n\u201cif we succeed, we will disappear\u201d 249 7. The Connexions textbook as a factory. Illustration by Jenn\nDrummond, Ross Reedstrom, Max Starkenberg, and others,\n1999\u20132004. Used with permission. more modules there are in the Connexions commons, the more fan-\ntastic and fascinating might be the possibilities for such novel\u2014and\nnatural\u2014connections.\nFree Software\u2014and, in particular, Open Source in the guise of\n\u201cself-organizing\u201d distributed systems of coordination\u2014provide a\nparticular promise of meeting the challenges of teaching and learn-\ning that Rich thinks we face. Rich\u2019s commitment is not to a certain\nkind of pedagogical practice, but to the \u201csocial\u201d or \u201ccommunity\u201d\nbenefits of thousands of people working \u201ctogether\u201d on a textbook.\nIndeed, Connexions did not emerge out of education or educational\ntechnology; it was not aligned with any particular theory of learn-\ning (though Rich eventually developed a rhetoric of linked, net-\nworked, connected knowledge\u2014hence the name Connexions\u2014that\nhe uses often to sell the project). There is no school of education\nat Rice, nor a particular constituency for such a project (teacher-\ntraining programs, say, or administrative requirements for online\neducation). What makes Rich\u2019s sell even harder is that the project\nemerged at about the same time as the high-profile failure of dot-\ncom bubble\u2013fueled schemes to expand university education into on-\nline education, distance education, and other systems of expanding\nthe paying student body without actually inviting them onto cam-\npus. The largest of these failed experiments by far was the project\nat Columbia, which had reached the stage of implementation at the\ntime the bubble burst in 2000.2\nThus, Rich styled Connexions as more than just a factory of\nknowledge\u2014it would be a community or culture developing richly\nassociative and novel kinds of textbooks\u2014and as much more than\njust distance education. Indeed, Connexions was not the only such\nproject busy differentiating itself from the perceived dangers of dis-\ntance education. In April 2001 MIT had announced that it would\nmake the content of all of its courses available for free online in\na project strategically called OpenCourseWare (OCW). Such news\ncould only bring attention to MIT, which explicitly positioned the\nannouncement as a kind of final death blow to the idea of distance\neducation, by saying that what students pay $35,000 and up for\nper year is not \u201cknowledge\u201d\u2014which is free\u2014but the experience of\nbeing at MIT. The announcement created pure profit from the per-\nspective of MIT\u2019s reputation as a generator and disseminator of sci-\nentific knowledge, but the project did not emerge directly out of an\ninterest in mimicking the success of Open Source. That angle was\n\u201cif we succeed, we will disappear\u201d 251 provided ultimately by the computer-science professor Hal Abel-\nson, whose deep understanding of the history and growth of Free\nSoftware came from his direct involvement in it as a long-standing\nmember of the computer-science community at MIT. OCW emerged\nmost proximately from the strange result of a committee report,\ncommissioned by the provost, on how MIT should position itself in\nthe \u201cdistance\/e-learning\u201d field. The surprising response: don\u2019t do it,\ngive the content away and add value to the campus teaching and\nresearch experience instead.3\nOCW, Connexions, and distance learning, therefore, while all os-\ntensibly interested in combining education with the networks and\nsoftware, emerged out of different demands and different places.\nWhile the profit-driven demand of distance learning fueled many\nattempts around the country, it stalled in the case of OCW, largely\nbecause the final MIT Council on Educational Technology report\nthat recommended OCW was issued at the same time as the first\nplunge in the stock market (April 2000). Such issues were not a core\nfactor in the development of Connexions, which is not to say that\nthe problems of funding and sustainability have not always been\nimportant concerns, only that genesis of the project was not at the\nadministrative level or due to concerns about distance education.\nFor Rich, Brent, and Ross the core commitment was to openness\nand to the success of Open Source as an experiment with massive,\ndistributed, Internet-based, collaborative production of software\u2014\ntheir commitment to this has been, from the beginning, completely\nand adamantly unwavering. Neverthless, the project has involved\nmodulations of the core features of Free Software. Such modula-\ntions depend, to a certain extent, on being a project that emerges\nout of the ideas and practices of Free Software, rather than, as in\nthe case of OCW, one founded as a result of conflicting goals (profit\nand academic freedom) and resulting in a strategic use of public\nrelations to increase the symbolic power of the university over its\nfiscal growth.\nWhen Rich recounts the story of Connexions, though, he doesn\u2019t\nmention any of this background. Instead, like a good storyteller,\nhe waits for the questions to pose themselves and lets his demon-\nstration answer them. Usually someone asks, \u201cHow is Connexions\ndifferent from OCW?\u201d And, every time, Rich says something simi-\nlar: Connexions is about \u201ccommunities,\u201d about changing the way\nscholars collaborate and create knowledge, whereas OCW is simply\n252 \u201cif we succeed, we will disappear\u201d an attempt to transfer existing courses to a Web format in order to\nmake the content of those courses widely available. Connexions is\na radical experiment in the collaborative creation of educational\nmaterials, one that builds on the insights of Open Source and that\nactually encompasses the OCW project. In retrospective terms, it is\nclear that OCW was interested only in modulating the meaning of\nsource code and the legal license, whereas Connexions seeks also\nto modulate the practice of coordination, with respect to academic\ntextbooks.\nRich\u2019s story of the origin of Connexions usually segues into a\ndemonstration of the system, in which he outlines the various tech-\nnical, legal, and educational concepts that distinguish it. Connex-\nions uses a standardized document format, the eXtensible Mark-up\nLanguage (XML), and a Creative Commons copyright license on\nevery module; the Creative Commons license allows people not only\nto copy and distribute the information but to modify it and even to\nuse it for commercial gain (an issue that causes repeated discussion\namong the team members). The material ranges from detailed ex-\nplanations of DSP concepts (naturally) to K-12 music education (the\nmost popular set of modules). Some contributors have added entire\ncourses; others have created a few modules here and there. Con-\ntributors can set up workgroups to manage the creation of modules,\nand they can invite other users to join. Connexions uses a version-\ncontrol system so that all of the changes are recorded; thus, if a\nmodule used in one class is changed, the person using it for an-\nother class can continue to use the older version if they wish. The\nnumber of detailed and clever solutions embodied in the system\nnever ceases to thoroughly impress anyone who takes the time to\nlook at it.\nBut what always animates people is the idea of random and flex-\nible connection, the idea that a textbook author might be able to\nbuild on the work of hundreds of others who have already contrib-\nuted, to create new classes, new modules, and creative connections\nbetween them, or surprising juxtapositions\u2014from the biologist\nteaching a class on bioinformatics who needs to remind students\nof certain parts of calculus without requiring a whole course; to\nthe architect who wants a studio to study biological form, not nec-\nessarily in order to do experiments in biology, but to understand\nbuildings differently; to the music teacher who wants students to\nunderstand just enough physics to get the concepts of pitch and\n\u201cif we succeed, we will disappear\u201d 253 timbre; to or the physicist who needs a concrete example for the\nexplanation of waves and oscillation.\nThe idea of such radical recombinations is shocking for some\n(more often for humanities and social-science scholars, rather than\nscientists or engineers, for reasons that clearly have to do with\nan ideology of authentic and individualized creative ability). The\nquestions that result\u2014regarding copyright, plagiarism, control, un-\nauthorized use, misuse, misconstrual, misreading, defamation, and\nso on\u2014generally emerge with surprising force and speed. If Rich\nwere trying to sell a version of \u201cdistance learning,\u201d skepticism and\nsuspicion would quickly overwhelm the project; but as it is, Connex-\nions inverts almost all of the expectations people have developed\nabout textbooks, classroom practice, collaboration, and copyright.\nMore often than not people leave the discussion converted\u2014no\ndoubt helped along by Rich\u2019s storytelling gift.\nModulations: From Free Software to Connexions\nConnexions surprises people for some of the same reasons as Free\nSoftware surprises people, emerging, as it does, directly out of the\nsame practices and the same components. Free Software provides\na template made up of the five components: shared source code,\na concept of openness, copyleft licenses, forms of coordination,\nand a movement or ideology. Connexions starts with the idea of\nmodulating a shared \u201csource code,\u201d one that is not software, but\neducational textbook modules that academics will share, port, and\nfork. The experiment that results has implications for the other four\ncomponents as well. The implications lead to new questions, new\nconstraints, and new ideas.\nThe modulation of source code introduces a specific and poten-\ntially confusing difference from Free Software projects: Connexions\nis both a conventional Free Software project and an unconventional\nexperiment based on Free Software. There is, of course, plenty of\nnormal source code, that is, a number of software components that\nneed to be combined in order to allow the creation of digital docu-\nments (the modules) and to display, store, transmit, archive, and\nmeasure the creation of modules. The creation and management\nof this software is expected to function more or less like all Free\nSoftware projects: it is licensed using Free Software licenses, it is\n254 \u201cif we succeed, we will disappear\u201d built on open standards of various kinds, and it is set up to take\ncontributions from other users and developers. The software system\nfor managing modules is itself built on a variety of other Free Soft-\nware components (and a commitment to using only Free Software).\nConnexions has created various components, which are either re-\nleased like conventional Free Software or contributed to another\nFree Software project. The economy of contribution and release is\na complex one; issues of support and maintenance, as well as of\nreputation and recognition, figure into each decision. Others are\ninvited to contribute, just as they are invited to contribute to any\nFree Software project.4\nAt the same time, there is \u201ccontent,\u201d the ubiquitous term for digi-\ntal creations that are not software. The creation of content modules,\non the other hand (which the software system makes technically\npossible), is intended to function like a Free Software project, in\nwhich, for instance, a group of engineering professors might get\ntogether to collaborate on pieces of a textbook on DSP. The Connex-\nions project does not encompass or initiate such collaborations, but,\nrather, proceeds from the assumption that such activity is already\nhappening and that Connexions can provide a kind of alternative\nplatform\u2014an alternative infrastructure even\u2014which textbook-\nwriting academics can make use of instead of the current infra-\nstructure of publishing. The current infrastructure and technical\nmodel of textbook writing, this implies, is one that both prevents\npeople from taking advantage of the Open Source model of col-\nlaborative development and makes academic work \u201cnon-free.\u201d The\nshared objects of content are not source code that can be compiled,\nlike source code in C, but documents marked up with XML and\nfilled with \u201ceducational\u201d content, then \u201cdisplayed\u201d either on paper\nor onscreen.\nThe modulated meaning of source code creates all kinds of new\nquestions\u2014specifically with respect to the other four components.\nIn terms of openness, for instance, Connexions modulates this\ncomponent very little; most of the actors involved are devoted to\nthe ideals of open systems and open standards, insofar as it is a\nFree Software project of a conventional type. It builds on UNIX\n(Linux) and the Internet, and the project leaders maintain a nearly\nfanatical devotion to openness at every level: applications, pro-\ngramming languages, standards, protocols, mark-up languages,\ninterface tools. Every place where there is an open (as opposed to a\n\u201cif we succeed, we will disappear\u201d 255 proprietary) solution\u2014that choice trumps all others (with one note-\nworthy exception).5 James Boyle recently stated it well: \u201cWherever\npossible, design the system to run with open content, on open pro-\ntocols, to be potentially available to the largest possible number\nof users, and to accept the widest possible range of experimental\nmodifications from users who can themselves determine the devel-\nopment of the technology.\u201d6\nWith respect to content, the devotion to openness is nearly identi-\ncal, because conventional textbook publishers \u201clock in\u201d customers\n(students) through the creation of new editions and useless \u201cen-\nhanced\u201d content, which jacks up prices and makes it difficult for\neducators to customize their own courses. \u201cOpenness\u201d in this sense\ntrades on the same reasoning as it did in the 1980s: the most im-\nportant aspect of the project is the information people create, and\nany proprietary system locks up content and prevents people from\ntaking it elsewhere or using it in a different context.\nIndeed, so firm is the commitment to openness that Rich and\nBrent often say something like, \u201cIf we are successful, we will dis-\nappear.\u201d They do not want to become a famous online textbook\npublisher; they want to become a famous publishing infrastructure.\nBeing radically open means that any other competitor can use your\nsystem\u2014but it means they are using your system, and this is the\ngoal. Being open means not only sharing the \u201csource code\u201d (content\nand modules), but devising ways to ensure the perpetual openness\nof that content, that is, to create a recursive public devoted to the\nmaintenance and modifiability of the medium or infrastructure by\nwhich it communicates. Openness trumps \u201csustainability\u201d (i.e., the\nself-perpetuation of the financial feasibility of a particular organi-\nzation), and where it fails to, the commitment to openness has been\ncompromised.\nThe commitment to openness and the modulation of the mean-\ning of source code thus create implications for the meaning of Free\nSoftware licenses: do such licenses cover this kind of content? Are\nnew licenses necessary? What should they look like? Connexions\nwas by no means the first project to stimulate questions about the\napplicability of Free Software licenses to texts and documents. In\nthe case of EMACS and the GPL, for example, Richard Stallman\nhad faced the problem of licensing the manual at the same time as\nthe source code for the editor. Indeed, such issues would ultimately\nresult in a GNU Free Documentation License intended narrowly to\n256 \u201cif we succeed, we will disappear\u201d cover software manuals. Stallman, due to his concern, had clashed\nduring the 1990s with Tim O\u2019Reilly, publisher and head of O\u2019Reilly\nPress, which had long produced books and manuals for Free Soft-\nware programs. O\u2019Reilly argued that the principles reflected in Free\nSoftware licenses should not be applied to instructional books, be-\ncause such books provided a service, a way for more people to learn\nhow to use Free Software, and in turn created a larger audience.\nStallman argued the opposite: manuals, just like the software they\nserved, needed to be freely modifiable to remain useful.\nBy the late 1990s, after Free Software and Open Source had been\nsplashed across the headlines of the mainstream media, a num-\nber of attempts to create licenses modeled on Free Software, but\napplicable to other things, were under way. One of the earliest\nand most general was the Open Content License, written by the\neducational-technology researcher David Wiley. Wiley\u2019s license\nwas intended for use on any kind of content. Content could include\ntext, digital photos, movies, music, and so on. Such a license raises\nnew issues. For example, can one designate some parts of a text as\n\u201cinvariant\u201d in order to prevent them from being changed, while al-\nlowing other parts of the text to be changed (the model eventually\nadopted by the GNU Free Documentation License)? What might the\nrelationship between the \u201coriginal\u201d and the modified version be?\nCan one expect the original author to simply incorporate suggested\nchanges? What kinds of forking are possible? Where do the \u201cmoral\nrights\u201d of an author come into play (regarding the \u201cintegrity\u201d of a\nwork)?\nAt the same time, the modulation of source code to include aca-\ndemic textbooks has extremely complex implications for the mean-\ning and context of coordination: scholars do not write textbooks\nlike programmers write code, so should they coordinate in the same\nways? Coordination of a textbook or a course in Connexions re-\nquires novel experiments in textbook writing. Does it lend itself to\nacademic styles of work, and in which disciplines, for what kinds of\nprojects? In order to cash in on the promise of distributed, collab-\norative creation, it would be necessary to find ways to coordinate\nscholars.\nSo, when Rich and Brent recognized in me, at dinner, someone\nwho might know how to think about these issues, they were ac-\nknowledging that the experiment they had started had created a\ncertain turbulence in their understanding of Free Software and,\n\u201cif we succeed, we will disappear\u201d 257 in turn, a need to examine the kinds of legal, cultural, and social\npractices that would be at stake.7\nModulations: From Connexions to Creative Commons\nI\u2019m standing in a parking lot in 100 degree heat and 90 percent\nhumidity. It is spring in Houston. I am looking for my car, and\nI cannot find it. James Boyle, author of Shamans, Software, and\nSpleens and distinguished professor of law at Duke University, is\nstanding near me, staring at me, wearing a wool suit, sweating and\nwatching me search for my car under the blazing sun. His look says\nsimply, \u201cIf I don\u2019t disembowel you with my Palm Pilot stylus, I am\ngoing to relish telling this humiliating story to your friends at every\nopportunity I can.\u201d Boyle is a patient man, with the kind of arch\nScottish humor that can make you feel like his best friend, even as\nhis stories of the folly of man unfold with perfect comic pitch and\nturn out to be about you. Having laughed my way through many\nan uproarious tale of the foibles of my fellow creatures, I am aware\nthat I have just taken a seat among them in Boyle\u2019s theater of hu-\nman weakness. I repeatedly press the panic button on my key chain,\nin the hopes that I am near enough to my car that it will erupt in a\nfrenzy of honking and flashing that will end the humiliation.\nThe day had started well. Boyle had folded himself into my Volks-\nwagen (he is tall), and we had driven to campus, parked the car\nin what no doubt felt like a memorable space at 9 A.M., and hap-\npily gone to the scheduled meeting\u2014only to find that it had been\nmistakenly scheduled for the following day. Not my fault, though\nnow, certainly, my problem. The ostensible purpose of Boyle\u2019s visit\nwas to meet the Connexions team and learn about what they were\ndoing. Boyle had proposed the visit himself, as he was planning to\npass through Houston anyway. I had intended to pester him with\nquestions about the politics and possibilities of licensing the content\nin Connexions and with comparisons to MIT\u2019s OCW and other such\ncommons projects that Boyle knew of.\nInstead of attending the meeting, I took him back to my office,\nwhere I learned more about why he was interested in Connexions.\nBoyle\u2019s interest was not entirely altruistic (nor was it designed to\nspend valuable quarter hours standing in a scorched parking lot as\nI looked for my subcompact car). What interested Boyle was find-\n258 \u201cif we succeed, we will disappear\u201d ing a constituency of potential users for Creative Commons, the\nnonprofit organization he was establishing with Larry Lessig, Hal\nAbelson, Michael Carroll, Eric Eldred, and others\u2014largely because\nhe recognized the need for a ready constituency in order to make\nCreative Commons work. The constituency was needed both to give\nthe project legitimacy and to allow its founders to understand what\nexactly was needed, legally speaking, for the creation of a whole\nnew set of Free Software-like licenses.\nCreative Commons, as an organization and as a movement, had\nbeen building for several years. In some ways, Creative Commons\nrepresented a simple modulation of the Free Software license: a\nbroadening of the license\u2019s concept to cover other types of content.\nBut the impetus behind it was not simply a desire to copy and ex-\ntend Free Software. Rather, all of the people involved in Creative\nCommons were those who had been troubling issues of intellectual\nproperty, information technology, and notions of commons, public\ndomains, and freedom of information for many years. Boyle had\nmade his name with a book on the construction of the informa-\ntion society by its legal (especially intellectual property) structures.\nEldred was a publisher of public-domain works and the lead plain-\ntiff in a court case that went to the Supreme Court in 2002 to de-\ntermine whether the recent extension of copyright term limits was\nconstitutional. Abelson was a computer scientist with an active in-\nterest in issues of privacy, freedom, and law \u201con the electronic fron-\ntier.\u201d And Larry Lessig was originally interested in constitutional\nlaw, a clerk for Judge Richard Posner, and a self-styled cyberlaw\nscholar, who was, during the 1990s, a driving force for the explo-\nsion of interest in cyberlaw, much of it carried out at the Berkman\nCenter for Internet and Society at Harvard University.\nWith the exception of Abelson\u2014who, in addition to being a fa-\nmous computer scientist, worked for years in the same building\nthat Richard Stallman camped out in and chaired the committee\nthat wrote the report recommending OCW\u2014none of the members\nof Creative Commons cut their teeth on Free Software projects (they\nwere lawyers and activists, primarily) and yet the emergence of\nOpen Source into the public limelight in 1998 was an event that\nmade more or less instant and intuitive sense to all of them. Dur-\ning this time, Lessig and members of the Berkman Center began an\n\u201copen law\u201d project designed to mimic the Internet-based collabora-\ntion of the Open Source project among lawyers who might want to\n\u201cif we succeed, we will disappear\u201d 259 contribute to the Eldred case. Creative Commons was thus built as\nmuch on a commitment to a notion of collaborative creation\u2014the\nuse of the Internet especially\u2014but more generally on the ability of\nindividuals to work together to create new things, and especially to\ncoordinate the creation of these things by the use of novel licensing\nagreements.\nCreative Commons provided more than licenses, though. It was\npart of a social imaginary of a moral and technical order that ex-\ntended beyond software to include creation of all kinds; notions of\ntechnical and moral freedom to make use of one\u2019s own \u201cculture\u201d\nbecame more and more prominent as Larry Lessig became more\nand more involved in struggles with the entertainment industry\nover the \u201ccontrol of culture.\u201d But for Lessig, Creative Commons was\na fall-back option; the direct route to a transformation of the legal\nstructure of intellectual property was through the Eldred case, a\ncase that built huge momentum throughout 2001 and 2002, was\ngranted cert by the Supreme Court, and was heard in October of\n2002. One of the things that made the case remarkable was the\nseries of strange bedfellows it produced; among the economists\nand lawyers supporting the repeal of the 1998 \u201cSonny Bono\u201d Copy-\nright Term Extension Act were the arch free-marketeers and Nobel\nPrize winners Milton Friedman, James Buchanan, Kenneth Arrow,\nRonald Coase, and George Akerlof. As Boyle pointed out in print,\nconservatives and liberals and libertarians all have reasons to be\nin favor of scaling back copyright expansion.8 Lessig and his team\nlost the case, and the Supreme Court essentially affirmed Congress\u2019s\ninterpretation of the Constitution that \u201cfor limited times\u201d meant\nonly that the time period be limited, not that it be short.\nCreative Commons was thus a back-door approach: if the laws\ncould not be changed, then people should be given the tools they\nneeded to work around those laws. Understanding how Creative\nCommons was conceived requires seeing it as a modulation of\nboth the notion of \u201csource code\u201d and the modulation of \u201ccopy-\nright licenses.\u201d But the modulations take place in that context of a\nchanging legal system that was so unfamiliar to Stallman and his\nEMACS users, a legal system responding to new forms of software,\nnetworks, and devices. For instance, the changes to the Copyright\nAct of 1976 created an unintended effect that Creative Commons\nwould ultimately seize on. By eliminating the requirement to regis-\nter copyrighted works (essentially granting copyright as soon as the\n260 \u201cif we succeed, we will disappear\u201d work is \u201cfixed in a tangible medium\u201d), the copyright law created a\nsituation wherein there was no explicit way in which a work could\nbe intentionally placed in the public domain. Practically speaking\nan author could declare that a work was in the public domain, but\nlegally speaking the risk would be borne entirely by the person who\nsought to make use of that work: to copy it, transform it, sell it,\nand so on. With the explosion of interest in the Internet, the prob-\nlem ramified exponentially; it became impossible to know whether\nsomeone who had placed a text, an image, a song, or a video online\nintended for others to make use of it\u2014even if the author explicitly\ndeclared it \u201cin the public domain.\u201d Creative Commons licenses were\nthus conceived and rhetorically positioned as tools for making ex-\nplicit exactly what uses could be made of a specific work. They\nprotected the rights of people who sought to make use of \u201cculture\u201d\n(i.e., materials and ideas and works they had not authored), an\napproach that Lessig often summed up by saying, \u201cCulture always\nbuilds on the past.\u201d\nThe background to and context of the emergence of Creative\nCommons was of course much more complicated and fraught. Con-\ncerns ranged from the plights of university libraries with regard to\nhigh-priced journals, to the problem of documentary filmmakers\nunable to afford, or even find the owners of, rights to use images\nor snippets in films, to the high-profile fights over online music\ntrading, Napster, and the RIAA. Over the course of four years, Les-\nsig and the other founders of Creative Commons would address all\nof these issues in books, in countless talks and presentations and\nconferences around the world, online and off, among audiences\nranging from software developers to entrepreneurs to musicians to\nbloggers to scientists.\nOften, the argument for Creative Commons draws heavily on the\nconcept of culture besieged by the content industries. A story which\nLessig enjoys telling\u2014one that I heard on several occasions when\nI saw him speak at conferences\u2014was that of Mickey Mouse. An\ninteresting, quasi-conspiratorial feature of the twentieth-century\nexpansion of intellectual-property law is that term limits seem to\nhave been extended right around the time Mickey Mouse was about\nto become public property. True or not, the point Lessig likes to\nmake is that the Mouse is not the de novo creation of the mind of\nWalt Disney that intellectual-property law likes to pretend it is,\nbut built on the past of culture, in particular, on Steamboat Willie,\n\u201cif we succeed, we will disappear\u201d 261 Charlie Chaplin, Krazy Kat, and other such characters, some as\ninspiration, some as explicit material. The greatness in Disney\u2019s\ncreation comes not from the mind of Disney, but from the culture\nfrom which it emerged. Lessig will often illustrate this in videos and\nimages interspersed with black-typewriter-font\u2013bestrewn slides and\na machine-gun style that makes you think he\u2019s either a beat-poet\nmanqu\u00e9 or running for office, or maybe both.\nOther examples of intellectual-property issues fill the books and\ntalks of Creative Commons advocates, stories of blocked innova-\ntion, stifled creativity, and\u2014the scariest point of all (at least for\neconomist-lawyers)\u2014inefficiency due to over-expansive intellectual-\nproperty laws and overzealous corporate lawyer-hordes.9 Lessig\noften preaches to the converted (at venues like South by South-\nwest Interactive and the O\u2019Reilly Open Source conferences), and\nthe audiences are always outraged at the state of affairs and eager\nto learn what they can do. Often, getting involved in the Creative\nCommons is the answer. Indeed, within a couple of years, Creative\nCommons quickly became more of a movement (a modulation of\nthe Free\/Open Source movement) than an experiment in writing\nlicenses.\nOn that hot May day in 2002, however, Creative Commons was\nstill under development. Later in the day, Boyle did get a chance to\nmeet with the Connexions project team members. The Connexions\nteam had already realized that in pursuing an experimental proj-\nect in which Free Software was used as a template they created a\nneed for new kinds of licenses. They had already approached the\nRice University legal counsel, who, though well-meaning, were not\ngrounded at all in a deep understanding of Free Software and were\nthus naturally suspicious of it. Boyle\u2019s presence and his detailed\nquestions about the project were like a revelation\u2014a revelation\nthat there were already people out there thinking about the very\nproblem the Connexions team faced and that the team would not\nneed to solve the problem themselves or make the Rice University\nlegal counsel write new open-content licenses. What Boyle offered\nwas the possibility for Connexions, as well as for myself as interme-\ndiary, to be involved in the detailed planning and license writing\nthat was under way at Creative Commons. At the same time, it\ngave Creative Commons an extremely willing \u201cearly-adopter\u201d for\nthe license, and one from an important corner of the world: schol-\narly research and teaching.10 My task, after recovering from the\n262 \u201cif we succeed, we will disappear\u201d shame of being unable to find my car, was to organize a workshop\nin August at which members of Creative Commons, Connexions,\nMIT\u2019s OCW, and any other such projects would be invited to talk\nabout license issues.\nParticipant Figuring Out\nThe workshop I organized in August 2002 was intended to allow\nCreative Commons, Connexions, and MIT\u2019s OCW project to try to\narticulate what each might want from the other. It was clear what\nCreative Commons wanted: to convince as many people as pos-\nsible to use their licenses. But what Connexions and OCW might\nhave wanted, from each other as well as from Creative Commons,\nwas less clear. Given the different goals and trajectories of the two\nprojects, their needs for the licenses differed in substantial ways\u2014\nenough so that the very idea of using the same license was, at least\ntemporarily, rendered impossible by MIT. While OCW was primar-\nily concerned about obtaining permissions to place existing copy-\nrighted work on the Web, Connexions was more concerned about\nensuring that new work remain available and modifiable.\nIn retrospect, this workshop clarified the novel questions and\nproblems that emerged from the process of modulating the com-\nponents of Free Software for different domains, different kinds of\ncontent, and different practices of collaboration and sharing. Since\nthen, my own involvement in this activity has been aimed at re-\nsolving some of these issues in accordance with an imagination of\nopenness, an imagination of social order, that I had learned from\nmy long experience with geeks, and not from my putative expertise\nas an anthropologist or a science-studies scholar. The fiction that\nI had at first adopted\u2014that I was bringing scholarly knowledge to\nthe table\u2014became harder and harder to maintain the more I real-\nized that it was my understanding of Free Software, gained through\nongoing years of ethnographic apprenticeship, that was driving my\ninvolvement.\nIndeed, the research I describe here was just barely undertaken\nas a research project. I could not have conceived of it as a fundable\nactivity in advance of discovering it; I could not have imagined the\ncourse of events in any of the necessary detail to write a proper\nproposal for research. Instead, it was an outgrowth of thinking and\n\u201cif we succeed, we will disappear\u201d 263 participating that was already under way, participation that was\ndriven largely by intuition and a feeling for the problem repre-\nsented by Free Software. I wanted to help figure something out. I\nwanted to see how \u201cfiguring out\u201d happens. While I could have orga-\nnized a fundable research project in which I picked a mature Free\nSoftware project, articulated a number of questions, and spent time\nanswering them among this group, such a project would not have\nanswered the questions I was trying to form at the time: what is\nhappening to Free Software as it spreads beyond the world of hack-\ners and software? How is it being modulated? What kinds of limits\nare breached when software is no longer the central component?\nWhat other domains of thought and practice were or are \u201creadied\u201d\nto receive and understand Free Software and its implications?11\nMy experience\u2014my participant-observation\u2014with Creative Com-\nmons was therefore primarily done as an intermediary between\nthe Connexions project (and, by implication, similar projects under\nway elsewhere) and Creative Commons with respect to the writing\nof licenses. In many ways this detailed, specific practice was the\nmost challenging and illuminating aspect of my participation, but\nin retrospect it was something of a red herring. It was not only the\nmodulation of the meaning of source code and of legal licenses\nthat differentiated these projects, but, more important, the meaning\nof collaboration, reuse, coordination, and the cultural practice of\nsharing and building on knowledge that posed the trickiest of the\nproblems.\nMy contact at Creative Commons was not James Boyle or Larry\nLessig, but Glenn Otis Brown, the executive director of that orga-\nnization (as of summer 2002). I first met Glenn over the phone, as\nI tried to explain to him what Connexions was about and why he\nshould join us in Houston in August to discuss licensing issues re-\nlated to scholarly material. Convincing him to come to Texas was\nan easier sell than explaining Connexions (given my penchant for\ncomplicating it unnecessarily), as Glenn was an Austin native who\nhad been educated at the University of Texas before heading off to\nHarvard Law School and its corrupting influence at the hands of\nLessig, Charlie Nesson, and John Perry Barlow.\nGlenn galvanized the project. With his background as a lawyer,\nand especially his keen interest in intellectual-property law, and\nhis long-standing love of music of all kinds Glenn lent incredible\nenthusiasm to his work. Prior to joining Creative Commons, he had\n264 \u201cif we succeed, we will disappear\u201d clerked for the Hon. Stanley Marcus on the Eleventh Circuit Court\nof Appeals, in Miami, where he worked on the so-called Wind Done\nGone case.12 His participation in the workshop was an experiment\nof his own; he was working on a story that he would tell countless\ntimes and which would become one of the core examples of the\nkind of practice Creative Commons wanted to encourage.\nA New York Times story describes how the band the White Stripes\nhad allowed Steven McDonald, the bassist from Redd Kross, to lay a\nbass track onto the songs that made up the album White Blood Cells.\nIn a line that would eventually become a kind of mantra for Cre-\native Commons, the article stated: \u201cMr. McDonald began putting\nthese copyrighted songs online without permission from the White\nStripes or their record label; during the project, he bumped into\nJack White, who gave him spoken assent to continue. It can be that\neasy when you skip the intermediaries.\u201d13 The ease with which these\ntwo rockers could collaborate to create a modified work (called, of\ncourse, Redd Blood Cells) without entering a studio, or, more salient,\na law firm, was emblematic of the notion that \u201cculture builds on the\npast\u201d and that it need not be difficult to do so.\nGlenn told the story with obvious and animated enthusiasm, end-\ning with the assertion that the White Stripes didn\u2019t have to give\nup all their rights to do this, but they didn\u2019t have to keep them all\neither; instead of \u201cAll Rights Reserved,\u201d he suggested, they could\nsay \u201cSome Rights Reserved.\u201d The story not only manages to capture\nthe message and aims of Creative Commons, but is also a nice indi-\ncation of the kind of dual role that Glenn played, first as a lawyer,\nand second as a kind of marketing genius and message man. The\npossibility of there being more than a handful of people like Glenn\naround was not lost on anyone, and his ability to switch between\nthe language of law and that of nonprofit populist marketing was\nphenomenal.14\nAt the workshop, participants had a chance to hash out a num-\nber of different issues related to the creation of licenses that would\nbe appropriate to scholarly content: questions of attribution and\ncommercial use, modification and warranty; differences between\nfederal copyright law concerning licenses and state law concerning\ncommercial contracts. The starting point for most people was Free\nSoftware, but this was not the only starting point. There were at\nleast two other broad threads that fed into the discussion and into\nthe general understanding of the state of affairs facing projects like\n\u201cif we succeed, we will disappear\u201d 265 Connexions or OCW. The first thread was that of digital libraries,\nhypertext, human-computer interaction research, and educational\ntechnology. These disciplines and projects often make common ref-\nerence to two pioneers, Douglas Englebart and Theodore Nelson,\nand more proximately to things like Apple\u2019s HyperCard program\nand a variety of experiments in personal academic computing. The\ndebates and history that lead up to the possibility of Connexions\nare complex and detailed, but they generally lack attention to le-\ngal detail. With the exception of a handful of people in library\nand information science who have made \u201cdigital\u201d copyright into\na subspecialty, few such projects, over the last twenty-five years,\nhave made the effort to understand, much less incorporate, issues\nof intellectual property into their purview.\nThe other thread combines a number of more scholarly interests\nthat come out of the disciplines of economics and legal theory:\ninstitutional economics, critical legal realism, law and economics\u2014\nthese are the scholastic designations. Boyle and Lessig, for exam-\nple, are both academics; Boyle does not practice law, and Lessig\nhas tried few cases. Nonetheless, they are both inheritors of a legal\nand philosophical pragmatism in which value is measured by the\ntransformation of policy and politics, not by the mere extension\nor specification of conceptual issues. Although both have penned\na large number of complicated theoretical articles (and Boyle is\nwell known in several academic fields for his book Shamans, Soft-\nware, and Spleens and his work on authorship and the law), nei-\nther, I suspect, would ever sacrifice the chance to make a set of\nconcrete changes in legal or political practice given the choice.\nThis point was driven home for me in a conversation I had with\nBoyle and others at dinner on the night of the launch of Creative\nCommons, in December 2002. During that conversation, Boyle said\nsomething to the effect of, \u201cWe actually made something; we didn\u2019t\njust sit around writing articles and talking about the dangers that\nface us\u2014we made something.\u201d He was referring as much to the\norganization as to the legal licenses they had created, and in this\nsense Boyle qualifies very much as a polymathic geek whose un-\nderstanding of technology is that it is an intervention into an al-\nready constituted state of affairs, one that demonstrates its value\nby being created and installed, not by being assessed in the court\nof scholarly opinions.\n266 \u201cif we succeed, we will disappear\u201d Similarly, Lessig\u2019s approach to writing and speaking is unabash-\nedly aimed at transforming the way people approach intellectual-\nproperty law and, even more generally, the way they understand\nthe relationship between their rights and their culture.15 Lessig\u2019s\napproach, at a scholarly level, is steeped in the teachings of law\nand economics (although, as he has playfully pointed out, a \u201csec-\nond\u201d Chicago school) but is focused more on the understanding\nand manipulation of norms and customs (\u201cculture\u201d) than on law\nnarrowly conceived.16\nInforming both thinkers is a somewhat heterodox economic con-\nsensus drawn primarily from institutional economics, which is\nroutinely used to make policy arguments about the efficacy or effi-\nciency of the intellectual-property system. Both are also informed by\nan emerging consensus on treating the public domain in the same\nmanner in which environmentalists treated the environment in the\n1960s.17 These approaches begin with long-standing academic and\npolicy concerns about the status and nature of \u201cpublic goods,\u201d not\ndirectly with the problem of Free Software or the Internet. In some\nways, the concern with public goods, commons, the public domain,\nand collective action are part of the same \u201creorientation of power\nand knowledge\u201d I identify throughout Two Bits: namely, the legiti-\nmation of the media of knowledge creation, communication, and\ncirculation. Most scholars of institutional economics and public\npolicy are, however, just as surprised and bewildered by the fact\nof Free Software as the rest of the world has been, and they have\nsought to square the existing understanding of public goods and\ncollective action with this new phenomenon.18\nAll of these threads form the weft of the experiment to modulate\nthe components of Free Software to create different licenses that\ncover a broader range of objects and that deal with people and\norganizations that are not software developers. Rather than at-\ntempt to carry on arguments at the level of theory, however, my\naim in participating was to see how and what was argued in prac-\ntice by the people constructing these experiments, to observe what\nconstraints, arguments, surprises, or bafflements emerged in the\ncourse of thinking through the creation of both new licenses and a\nnew form of authorship of scholarly material. Like those who study\n\u201cscience in action\u201d or the distinction between \u201claw on the books\u201d\nand \u201claw in action,\u201d I sought to observe the realities of a practice\n\u201cif we succeed, we will disappear\u201d 267 heavily determined by textual and epistemological frameworks of\nvarious sorts.19\nIn my years with Connexions I eventually came to see it as some-\nthing in between a natural experiment and a thought experiment: it\nwas conducted in the open, and it invited participation from work-\ning scholars and teachers (a natural experiment, in that it was not\na closed, scholarly endeavor aimed at establishing specific results,\nbut an essentially unbounded, functioning system that people could\nand would come to depend on), and yet it proceeded by making a\nseries of strategic guesses (a thought experiment) about three re-\nlated things: (1) what it is (and will be) possible to do technically;\n(2) what it is (and will be) possible to do legally; and (3) what\nscholars and educators have done and now do in the normal course\nof their activities.\nAt the same time, this experiment gave shape to certain legal\nquestions that I channeled in the direction of Creative Commons,\nissues that ranged from technical questions about the structure of\ndigital documents, requirements of attribution, and URLs to ques-\ntions about moral rights, rights of disavowal, and the meaning of\n\u201cmodification.\u201d The story of the interplay between Connexions and\nCreative Commons was, for me, a lesson in a particular mode of le-\ngal thinking which has been described in more scholarly terms as the\ndifference between the Roman or, more proximately, the Napoleonic\ntradition of legal rationalism and the Anglo-American common-\nlaw tradition.20 It was a practical experience of what exactly the\ndifference is between legal code and software code, with respect to\nhow those two things can be made flexible or responsive.\n268 \u201cif we succeed, we will disappear\u201d 9.\nReuse, Modification, and\nthe Nonexistence of Norms\nThe Connexions project was an experiment in modulating the prac-\ntices of Free Software. It was not inspired by so much as it was\nbased on a kind of template drawn from the experience of people\nwho had some experience with Free Software, including myself. But\nhow exactly do such templates get used? What is traced and what\nis changed? In terms of the cultural significance of Free Software,\nwhat are the implications of these changes? Do they maintain the\norientation of a recursive public, or are they attempts to apply Free\nSoftware for other private concerns? And if they are successful,\nwhat are the implications for the domains they affect: education,\nscholarship, scientific knowledge, and cultural production? What\neffects do these changes have on the norms of work and the mean-\ning and shape of knowledge in these domains? In this chapter I explore in ethnographic detail how the modu-\nlations of Free Software undertaken by Connexions and Creative\nCommons are related to the problems of reuse, modification, and\nthe norms of scholarly production. I present these two projects as\nresponses to the contemporary reorientation of knowledge and\npower; they are recursive publics just as Free Software is, but they\nexpand the domain of practice in new directions, that is, into the\nscholarly world of textbooks and research and into the legal do-\nmains of cultural production more generally.\nIn the course of \u201cfiguring out\u201d what they are doing, these two\nprojects encounter a surprising phenomenon: the changing mean-\ning of the finality of a scholarly or creative work. Finality is not\ncertainty. While certainty is a problematic that is well and often\nstudied in the philosophy of science and in science studies, final-\nity is less so. What makes a work stay a work? What makes a fact\nstay a fact? How does something, certain or not, achieve stability\nand identity? Such finality, the very paradigm of which is the pub-\nlished book, implies stability. But Connexions and Creative Com-\nmons, through their experiments with Free Software, confront the\nproblem of how to stabilize a work in an unstable context: that of\nshareable source code, an open Internet, copyleft licenses, and new\nforms of coordination and collaboration.1 The meaning of finality\nwill have important effects on the ability to constitute a politics\naround any given work, whether a work of art or a work of scholar-\nship and science. The actors in Creative Commons and Connexions\nrealize this, and they therefore form yet another instance of a recur-\nsive public, precisely because they seek ways to define the mean-\ning of finality publicly and openly\u2014and to make modifiability an\nirreversible aspect of the process of stabilizing knowledge.\nThe modulations of Free Software performed by Connexions and\nCreative Commons reveal two significant issues. The first is the\ntroublesome matter of the meaning of reuse, as in the reuse of con-\ncepts, ideas, writings, articles, papers, books, and so on for the cre-\nation of new objects of knowledge. Just as software source code can\nbe shared, ported, and forked to create new versions with new func-\ntions, and just as software and people can be coordinated in new\nways using the Internet, so too can scholarly and scientific content.\nI explore the implications of this comparison in this chapter. The\ncentral gambit of both Connexions and Creative Commons (and\nmuch of scientific practice generally) is that new work builds on\n270 reuse, modification, norms previous work. In the sciences the notion that science is cumulative\nis not at issue, but exactly how scientific knowledge accumulates is\nfar from clear. Even if \u201cstanding on the shoulders of giants\u201d can be\nrevealed to hide machinations, secret dealings, and Machiavellian\nmaneuvering of the most craven sort, the very concept of cumula-\ntive knowledge is sound. Building a fact, a result, a machine, or\na theory out of other, previous works\u2014this kind of reuse as prog-\nress is not in question. But the actual material practice of writing,\npublication, and the reuse of other results and works is something\nthat, until very recently, has been hidden from view, or has been\nso naturalized that the norms of practice are nearly invisible to\npractitioners themselves.\nThis raises the other central concern of this chapter: that of the\nexistence or nonexistence of norms. For an anthropologist to query\nwhether or not norms exist might seem to theorize oneself out of\na job; one definition of anthropology is, after all, the making ex-\nplicit of cultural norms. But the turn to \u201cpractices\u201d in anthropology\nand science studies has in part been a turn away from \u201cnorms\u201d\nin their classic sociological and specifically Mertonian fashion.\nRobert Merton\u2019s suggestion that science has been governed by\nnorms\u2014disinterestedness, communalism, organized skepticism,\nobjectivity\u2014has been repeatedly and roundly criticized by a gen-\neration of scholars in the sociology of scientific knowledge who\nnote that even if such norms are asserted by actors, they are often\nsubverted in the doing.2 But a striking thing has happened recently;\nthose Mertonian norms of science have in fact become the more or\nless explicit goals in practice of scientists, engineers, and geeks in\nthe wake of Free Software. If Mertonian norms do not exist, then\nthey are being invented. This, of course, raises novel questions: can\none create norms? What exactly would this mean? How are norms\ndifferent from culture or from legal and technical constraints? Both\nConnexions and Creative Commons explicitly pose this question\nand search for ways to identify, change, or work with norms as\nthey understand them, in the context of reuse.\nWhiteboards: What Was Publication?\nMore than once, I have found myself in a room with Rich Baraniuk\nand Brent Hendricks and any number of other employees of the\nreuse, modification, norms 271 Connexions project, staring at a whiteboard on which a number\nof issues and notes have been scrawled. Usually, the notes have a\nkind of palimpsestic quality, on account of the array of previous\nconversations that are already there, rewritten in tiny precise script\nin a corner, or just barely erased beneath our discussion. These con-\nversations are often precipitated by a series of questions that Brent,\nRoss Reedstrom, and the development team have encountered as\nthey build and refine the system. They are never simple questions.\nA visitor staring at the whiteboard might catch a glimpse of the\npeculiar madness that afflicts the project: a mixture of legal terms,\ntechnical terms, and terms like scholarly culture or DSP communities.\nI\u2019m consulted whenever this mixture of terms starts to worry the\ndevelopers in terms of legality, culture, or the relationship between\nthe two. I\u2019m generally put in the position of speaking either as a\nlawyer (which, legally speaking, I am not supposed to do) or as an\nanthropologist (which I do mainly by virtue of holding a position\nin an anthropology department). Rarely are the things I say met\nwith assent: Brent and Ross, like most hackers, are insanely well\nversed in the details of intellectual-property law, and they routinely\ncorrect me when I make bold but not-quite-true assertions about it.\nNonetheless, they rarely feel well versed enough to make decisions\nabout legal issues on their own, and often I have been called\u2014on\nagain as a thoughtful sounding board, and off again as intermedi-\nary with Creative Commons.\nThis process, I have come to realize, is about figuring something\nout. It is not just a question of solving technical problems to which I\nmight have some specific domain knowledge. Figuring out is modu-\nlation; it is template-work. When Free Software functions as a tem-\nplate for projects like Connexions, it does so literally, by allowing\nus to trace a known form of practice (Free Software) onto a less\nwell known, seemingly chaotic background and to see where the\nforms match up and where they do not. One very good way to un-\nderstand what this means in a particular case\u2014that is, to see more\nclearly the modulations that Connexions has performed\u2014is to con-\nsider the practice and institution of scholarly publication through\nthe template of Free Software.\nConsider the ways scholars have understood the meaning and\nsignificance of print and publication in the past, prior to the Inter-\nnet and the contemporary reorientation of knowledge and power.\nThe list of ambitious historians and theorists of the relationship\n272 reuse, modification, norms of media to knowledge is long: Lucien Febvre, Walter Ong, Mar-\nshall McLuhan, Jack Goody, Roger Chartier, Friedrich Kittler, Eliza-\nbeth Eisenstein, Adrian Johns, to name a few.3 With the exception\nof Johns, however, the history of publication does not start with the\nconventional, legal, and formal practices of publication so much as\nit does with the material practices and structure of the media them-\nselves, which is to say the mechanics and technology of the printed\nbook.4 Ong\u2019s theories of literacy and orality, Kittler\u2019s re-theorization\nof the structure of media evolution, Goody\u2019s anthropology of the\nmedia of accounting and writing\u2014all are focused on the tangible\nmedia as the dependent variable of change. By contrast, Johns\u2019s The\nNature of the Book uncovers the contours of the massive endeavor\ninvolved in making the book a reliable and robust form for the cir-\nculation of knowledge in the seventeenth century and after.\nPrior to Johns\u2019s work, arguments about the relationship of print\nand power fell primarily into two camps: one could overestimate\nthe role of print and the printing press by suggesting that the \u201cfix-\nity\u201d of a text and the creation of multiple copies led automatically to\nthe spread of ideas and the rise of enlightenment. Alternately, one\ncould underestimate the role of the book by suggesting that it was\nmerely a transparent media form with no more or less effect on the\ncirculation or evaluation of ideas than manuscripts or television.\nJohns notes in particular the influence of Elizabeth Eisenstein\u2019s\nscholarship on the printing press (and Bruno Latour\u2019s dependence\non this in turn), which very strongly identified the characteristics of\nthe printed work with the cultural changes seen to follow, includ-\ning the success of the scientific revolution and the experimental\nmethod.5 For example, Eisenstein argued that fixity\u2014the fact that\na set of printed books can be exact copies of each other\u2014implied\nvarious transformations in knowledge. Johns, however, is at pains\nto show just how unreliable texts are often perceived to be. From\nwhich sources do they come? Are they legitimate? Do they have\nthe backing or support of scholars or the crown? In short, fixity\ncan imply sound knowledge only if there is a system of evaluation\nalready in place. Johns suggests a reversal of this now common-\nsense notion: \u201cWe may consider fixity not as an inherent quality, but\nas a transitive one. . . . We may adopt the principle that fixity exists\nonly inasmuch as it is recognized and acted upon by people\u2014and\nnot otherwise. The consequence of this change in perspective is that\nprint culture itself is immediately laid open to analysis. It becomes\nreuse, modification, norms 273 a result of manifold representations, practices and conflicts, rather\nthan just the manifold cause with which we are often presented.\nIn contrast to talk of a \u2018print logic\u2019 imposed on humanity, this ap-\nproach allows us to recover the construction of different print cul-\ntures in particular historical circumstances.\u201d6\nJohns\u2019s work focuses on the elaborate and difficult cultural, so-\ncial, and economic work involved, in the sixteenth and seventeenth\ncenturies, in transforming the European book into the kind of au-\nthority it is taken to be across the globe today. The creation and\nstandardization not just of books but of a publishing infrastructure\ninvolved the kind of careful social engineering, reputation man-\nagement, and skills of distinction, exclusion, and consensus that\nscience studies has effectively explored in science and engineering.\nHence, Johns focuses on \u201cprint-in-the-making\u201d and the relationship\nof the print culture of that period to the reliability of knowledge. In-\nstead of making broad claims for the transformation of knowledge\nby print (eerily similar in many respects to the broad claims made\nfor the Internet), Johns explores the clash of representations and\npractices necessary to create the sense, in the twentieth century,\nthat there really is or was only one print culture.\nThe problem of publication that Connexions confronts is thus not\nsimply caused by the invention or spread of the Internet, much\nless that of Free Software. Rather, it is a confrontation with the\nproblems of producing stability and finality under very different\ntechnical, legal, and social conditions\u2014a problem more complex\neven than the \u201cdifferent print cultures in particular historical cir-\ncumstances\u201d that Johns speaks of in regard to the book. Connexions\nfaces two challenges: that of figuring out the difference that today\nintroduces with respect to yesterday, and that of creating or modi-\nfying an infrastructure in order to satisfy the demands of a properly\nauthoritative knowledge. Connexions textbooks of necessity look\ndifferent from conventional textbooks; they consist of digital docu-\nments, or \u201cmodules,\u201d that are strung together and made available\nthrough the Web, under a Creative Commons license that allows\nfor free use, reuse, and modification. This version of \u201cpublication\u201d\nclearly has implications for the meaning of authorship, ownership,\nstewardship, editing, validation, collaboration, and verification.\nThe conventional appearance of a book\u2014in bookstores, through\nmail-order, in book clubs, libraries, or universities\u2014was an event\nthat signified, as the name suggests, its official public appearance\n274 reuse, modification, norms in the world. Prior to this event, the text circulated only privately,\nwhich is to say only among the relatively small network of people\nwho could make copies of it or who were involved in its writing,\nediting, proofreading, reviewing, typesetting, and so on. With the\nInternet, the same text can be made instantly available at each of\nthese stages to just as many or more potential readers. It effectively\nturns the event of publication into a notional event\u2014the click of a\nbutton\u2014rather than a highly organized, material event. Although\nit is clear that the practice of publication has become denaturalized\nor destabilized by the appearance of new information technolo-\ngies, this hardly implies that the work of stabilizing the meaning of\npublication\u2014and producing authoritative knowledge as a result\u2014\nhas ceased. The tricky part comes in understanding how Free Soft-\nware is used as a template by which the authority of publication\nin the Gutenberg Galaxy is being transformed into the authority of\npublication in the Turing Universe.\nPublication in Connexions\nIn the case of Connexions there are roughly three stages to the cre-\nation of content. The first, temporally speaking, is whatever hap-\npens before Connexions is involved, that is, the familiar practices\nof what I would call composition, rather than simply writing. Some\nproject must be already under way, perhaps started under the con-\nstraints of and in the era of the book, perhaps conceived as a digital\ntextbook or an online textbook, but still, as of yet, written on paper\nor saved in a Word document or in LaTeX, on a scholar\u2019s desktop.\nIt could be an individual project, as in the case of Rich\u2019s initial plan\nto write a DSP textbook, or it could be a large collaborative project\nto write a textbook.\nThe second stage is the one in which the document or set of\ndocuments is translated (\u201cConnexified\u201d) into the mark-up system\nused by Connexions. Connexions uses the eXtensible Mark-up Lan-\nguage (XML), in particular a subset of tags that are appropriate\nto textbooks. These \u201csemantic\u201d tags (e.g., <term>) refer only to\nthe meaning of the text they enclose, not to the \u201cpresentation\u201d or\nsyntactic look of what they enclose; they give the document the\nnecessary structure it needs to be transformed in a number of cre-\native ways. Because XML is related only to content, and not to\nreuse, modification, norms 275 presentation (it is sometimes referred to as \u201cagnostic\u201d), the same\ndocument in Connexions can be automatically made to look a num-\nber of different ways, as an onscreen presentation in a browser, as\na pdf document, or as an on-demand published work that can be\nprinted out as a book, complete with continuous page numbering,\nfootnotes (instead of links), front and back matter, and an index.\nTherein lies much of Connexions\u2019s technical wizardry.\nDuring the second stage, that of being marked up in XML, the\ndocument is not quite public, although it is on the Internet; it is in\nwhat is called a workgroup, where only those people with access\nto the particular workgroup (and those have been invited to col-\nlaborate) can see the document. It is only when the document is\nfinished, ready to be distributed, that it will enter the third, \u201cpub-\nlished\u201d stage\u2014the stage at which anyone on the Internet can ask\nfor the XML document and the software will display it, using style\nsheets or software converters, as an HTML page, a pdf document\nfor printing, or as a section of a larger course. However, publication\ndoes not here signify finality; indeed, one of the core advantages\nof Connexions is that the document is rendered less stable than the\nbook-object it mimics: it can be updated, changed, corrected, de-\nleted, copied, and so on, all without any of the rigmarole associated\nwith changing a published book or article. Indeed, the very pow-\nerful notion of fixity theorized by McLuhan and Eisenstein is ren-\ndered moot here. The fact that a document has been printed (and\nprinted as a book) no longer means that all copies will be the same;\nindeed, it may well change from hour to hour, depending on how\nmany people contribute (as in the case of Free Software, which can\ngo through revisions and updates as fast, or faster, than one can\ndownload and install new versions). With Wikipedia entries that\nare extremely politicized or active, for example, a \u201cfinal\u201d text is\nimpossible, although the dynamics of revision and counter-revision\ndo suggest outlines for the emergence of some kinds of stability.\nBut Connexions differs from Wikipedia with respect to this finality\nas well, because of the insertion of the second stage, during which\na self-defined group of people can work on a nonpublic text before\ncommitting changes that a public can see.\nIt should be clear, given the example of Connexions, or any simi-\nlar project such as Wikipedia, that the changing meaning of \u201cpub-\nlication\u201d in the era of the Internet has significant implications,\nboth practical (they affect the way people can both write and pub-\n276 reuse, modification, norms lish their works) and legal (they fit uneasily into the categories\nestablished for previous media). The tangibility of a textbook is\nquite obviously transformed by these changes, but so too is the\ncultural significance of the practice of writing a textbook. And if\ntextbooks are written differently, using new forms of collabora-\ntion and allowing novel kinds of transformation, then the vali-\ndation, certification, and structure of authority of textbooks also\nchange, inviting new forms of open and democratic participation\nin writing, teaching, and learning. No longer are all of the settled\npractices of authorship, collaboration, and publication configured\naround the same institutional and temporal scheme (e.g., the book\nand its publishing infrastructure). In a colloquial sense, this is obvi-\nous, for instance, to any musician today: recording and releasing a\nsong to potentially millions of listeners is now technically possible\nfor anyone, but how that fact changes the cultural significance of\nmusic creation is not yet clear. For most musicians, creating music\nhasn\u2019t changed much with the introduction of digital tools, since\nnew recording and composition technologies largely mimic the\nrecording practices that preceded them (for example, a program\nlike Garage Band literally looks like a four-track recorder on the\nscreen). Similarly, much of the practice of digital publication has\nbeen concerned with recreating something that looks like tradi-\ntional publication.7\nPerhaps unsurprisingly, the Connexions team spent a great deal\nof time at the outset of the project creating a pdf-document-creation\nsystem that would essentially mimic the creation of a conventional\ntextbook, with the push of a button.8 But even this process causes\na subtle transformation: the concept of \u201cedition\u201d becomes much\nharder to track. While a conventional textbook is a stable entity\nthat goes through a series of printings and editions, each of which\nis marked on its publication page, a Connexions document can go\nthrough as many versions as an author wants to make changes, all\nthe while without necessarily changing editions. In this respect, the\nmodulation of the concept of source code translates the practices\nof updating and \u201cversioning\u201d into the realm of textbook writing.\nRecall the cases ranging from the \u201ccontinuum\u201d of UNIX versions\ndiscussed by Ken Thompson to the complex struggles over version\ncontrol in the Linux and Apache projects. In the case of writing\nsource code, exactitude demands that the change of even a single\ncharacter be tracked and labeled as a version change, whereas a\nreuse, modification, norms 277 conventional-textbook spelling correction or errata issuance would\nhardly create the need for a new edition.\nIn the Connexions repository all changes to a text are tracked\nand noted, but the identity of the module does not change. \u201cEdi-\ntions\u201d have thus become \u201cversions,\u201d whereas a substantially revised\nor changed module might require not reissuance but a forking of\nthat module to create one with a new identity. Editions in publish-\ning are not a feature of the medium per se; they are necessitated\nby the temporal and spatial practices of publication as an event,\nthough this process is obviously made visible only in the book it-\nself. In the same way, versioning is now used to manage a process,\nbut it results in a very different configuration of the medium and\nthe material available in that medium. Connexions traces the tem-\nplate of software production (sharing, porting, and forking and the\nnorms and forms of coordination in Free Software) directly onto\nolder forms of publication. Where the practices match, no change\noccurs, and where they don\u2019t, it is the reorientation of knowledge\nand power and the emergence of recursive publics that serves as a\nguide to the development of the system.\nLegally speaking, the change from editions to versions and forks\nraises troubling questions about the boundaries and status of a\ncopyrighted work. It is a peculiar feature of copyright law that it\nneeds to be updated regularly each time the media change, in order\nto bring certain old practices into line with new possibilities. Scat-\ntered throughout the copyright statutes is evidence of old new me-\ndia: gramophones, jukeboxes, cable TV, photocopiers, peer-to-peer\nfile-sharing programs, and so on. Each new form of communication\nshifts the assumptions of past media enough that they require a re-\nevaluation of the putative underlying balance of the constitutional\nmandate that gives (U.S.) intellectual-property law its inertia. Each\nnew device needs to be understood in terms of creation, storage,\ndistribution, production, consumption, and tangibility, in order to\nassess the dangers it poses to the rights of inventors and artists.\nBecause copyright law \u201chard codes\u201d the particular media into the\nstatutes, copyright law is comfortable with, for example, book edi-\ntions or musical recordings. But in Connexions, new questions arise:\nhow much change constitutes a new work, and thus demands a\nnew copyright license? If a licensee receives one copy of a work, to\nwhich versions will he or she retain rights after changes? Because\n278 reuse, modification, norms of the complexity of the software involved, there are also questions\nthat the law simply cannot deal with ( just as it had not been able\nto do in the late 1970s with respect to the definition of software): is\nthe XML document equivalent to the viewable document, or must\nthe style sheet also be included? Where does the \u201ccontent\u201d begin\nand the \u201csoftware\u201d end? Until the statutes either incorporate these\nnew technologies or are changed to govern a more general process,\nrather than a particular medium, these questions will continue to\nemerge as part of the practice of writing.\nThis denaturalization of the notion of \u201cpublication\u201d is responsible\nfor much of the surprise and concern that greets Connexions and\nprojects like it. Often, when I have shown the system to scholars,\nthey have displayed boredom mixed with fear and frustration: \u201cIt\ncan never replace the book.\u201d On the one hand, Connexions has made\nan enormous effort to make its output look as much like conven-\ntional books as possible; on the other hand, the anxiety evinced is\njustified, because what Connexions seeks to replace is not the book,\nwhich is merely ink and paper, but the entire publishing process. The\nfact that it is not replacing the book per se, but the entire process\nwhereby manuscripts are made into stable and tangible objects\ncalled books is too overwhelming for most scholars to contemplate\u2014\nespecially scholars who have already mastered the existing pro-\ncess of book writing and creation. The fact that the legal system is\nbuilt to safeguard something prior to and not fully continuous with\nthe practice of Connexions only adds to the concern that such a\ntransformation is immodest and risky, that it endangers a practice\nwith centuries of stability behind it. Connexions, however, is not\nthe cause of destabilization; rather, it is a response to or recogni-\ntion of a problem. It is not a new problem, but one that periodically\nreemerges: a reorientation of knowledge and power that includes\nquestions of enlightenment and rationality, democracy and self-\ngovernance, liberal values and problems of the authority and vali-\ndation of knowledge. The salient moments of correlation are not the\ninvention of the printing press and the Internet, but the struggle to\nmake published books into a source of authoritative knowledge in\nthe seventeenth and eighteenth centuries and the struggle to find\nways to do the same with the Internet today.9\nConnexions is, in many ways, understood by its practitioners to be\nboth a response to the changing relations of knowledge and power,\nreuse, modification, norms 279 one that reaffirms the fundamental values of academic freedom and\nthe circulation of knowledge, and also an experiment with, even a\nradicalization of, the ideals of both Free Software and Mertonian\nscience. The transformation of the meaning of publication implies a\nfundamental shift in the status, in the finality of knowledge. It seeks\nto make of knowledge (knowledge in print, not in minds) something\nliving and constantly changing, as opposed to something static and\nfinal. The fact that publication no longer signifies finality\u2014that\nis, no longer signifies a state of fixity that is assumed in theory\n(and frequently in practice) to account for a text\u2019s reliability\u2014has\nimplications for how the text is used, reused, interpreted, valued,\nand trusted.10 Whereas the traditional form of the book is the same\nacross all printed versions or else follows an explicit practice of ap-\npearing in editions (complete with new prefaces and forewords), a\nConnexions document might very well look different from week to\nweek or year to year.11 While a textbook might also change signifi-\ncantly to reflect the changing state of knowledge in a given field,\nit is an explicit goal of Connexions to allow this to happen \u201cin real\ntime,\u201d which is to say, to allow educators to update textbooks as\nfast as they do scientific knowledge.12\nThese implications are not lost on the Connexions team, but nei-\nther are they understood as goals or as having simple solutions.\nThere is a certain immodest, perhaps even reckless, enthusiasm sur-\nrounding these implications, an enthusiasm that can take both poly-\nmath and transhumanist forms. For instance, the destabilization\nof the contemporary textbook-publishing system that Connexions\nrepresents is (according to Rich) a more accurate way to represent\nthe connections between concepts than a linear textbook format.\nConnexions thus represents a use of technology as an intervention\ninto an existing context of practice. The fact that Connexions could\nalso render the reliability or trustworthiness of scholarly knowledge\nunstable is sometimes discussed as an inevitable outcome of tech-\nnical change\u2014something that the world at large, not Connexions,\nmust learn to deal with.\nTo put it differently, the \u201cgoal\u201d of Connexions was never to de-\nstroy publishing, but it has been structured by the same kind of\nimaginations of moral and technical order that pervade Free Soft-\nware and the construction of the Internet. In this sense Rich, Brent,\nand others are geeks in the same sense as Free Software geeks: they\n280 reuse, modification, norms share a recursive public devoted to achieving a moral and techni-\ncal order in which openness and modifiability are core values (\u201cIf\nwe are successful, we will disappear\u201d). The implication is that the\nexisting model and infrastructure for the publication of textbooks is\nof a different moral and technical order, and thus that Connexions\nneeds to innovate not only the technology (the source code or the\nopenness of the system) or the legal arrangements (licenses) but\nalso the very norms and forms of textbook writing itself (coordina-\ntion and, eventually, a movement). If publication once implied the\nappearance of reliable, final texts\u2014even if the knowledge therein\ncould be routinely contested by writing more texts and reviews and\ncritiques\u2014Connexions implies the denaturalization of not knowl-\nedge per se, but of the process whereby that knowledge is stabilized\nand rendered reliable, trustworthy.\nA keyword for the transformation of textbook writing is com-\nmunity, as in the tagline of the Connexions project: \u201cSharing\nKnowledge and Building Communities.\u201d Building implies that such\ncommunities do not yet exist and that the technology will enable\nthem; however, Connexions began with the assumption that there\nexist standard academic practices and norms of creating teach-\ning materials. As a result, Connexions both enables these practices\nand norms, by facilitating a digital version of the textbook, and\nintervenes in them, by creating a different process for creating a\ntextbook. Communities are both assumed and desired. Sometimes\nthey are real (a group of DSP engineers, networked around Rich\nand others who work in his subspecialty), and sometimes they are\nimagined (as when in the process of grant writing we claim that the\nmost important component of the success of the project is the \u201cseed-\ning\u201d of scholarly communities). Communities, furthermore, are not\naudiences or consumers, and sometimes not even students or learn-\ners. They are imagined to be active, creative producers and users of\nteaching materials, whether for teaching or for the further creation\nof such materials. The structure of the community has little to do\nwith issues of governance, solidarity, or pedagogy, and much more\nto do with a set of relationships that might obtain with respect\nto the creation of teaching materials\u2014a community of collabora-\ntive production or collaborative debugging, as in the modulation of\nforms of coordination, modulated to include the activity of creating\nteaching materials.\nreuse, modification, norms 281 Agency and Structure in Connexions\nOne of the most animated whiteboard conversations I remember\nhaving with Brent and Ross concerned difference between the pos-\nsible \u201croles\u201d that a Connexions user might occupy and the implica-\ntions this could have for both the technical features of the system\nand the social norms that Connexions attempts to maintain and\nreplicate. Most software systems are content to designate only \u201cus-\ners,\u201d a generic name-and-password account that can be given a set\nof permissions (and which has behind it a long and robust tradition\nin computer-operating-system and security research). Users are us-\ners, even if they may have access to different programs and files.\nWhat Connexions needed was a way to designate that the same\nperson might have two different exogenous roles: a user might be\nthe author, but not the owner of the content, and vice versa. For in-\nstance, perhaps Rice University maintains the copyright for a work,\nbut the author is credited for its creation. Such a situation\u2014known,\nin legal terms, as \u201cwork for hire\u201d\u2014is routine in some universities and\nmost corporations. So while the author is generally given the free-\ndom and authority to create and modify the text as he or she sees\nfit, the university asserts copyright ownership in order to retain the\nright to commercially exploit the work. Such a situation is far from\nsettled and is, of course, politically fraught, but the Connexions sys-\ntem, in order to be useful at all to anyone, needed to accommodate\nthis fact. Taking an oppositional political stand would render the\nsystem useless in too many cases or cause it to become precisely\nthe kind of authorless, creditless system as Wikipedia\u2014a route not\ndesired by many academics. In a perfectly open world all Connex-\nions modules might each have identical authors and owners, but\npragmatism demands that the two roles be kept separate.\nFurthermore, there are many people involved every day in the\ncreation of academic work who are neither the author nor the\nowner: graduate students and undergraduates, research scientists,\ntechnicians, and others in the grand, contested, complex academic\necology. In some disciplines, all contributors may get authorship\ncredit and some of them may even share ownership, but often many\nof those who do the work get mentioned only in acknowledgments,\nor not at all. Again, although the impulse of the creators of Connex-\nions might be to level the playing field and allow only one kind of\nuser, the fact of the matter is that academics simply would not use\n282 reuse, modification, norms such a system.13 The need for a role such as \u201cmaintainer\u201d (which\nmight also include \u201ceditor\u201d), which was different from author or\nowner, thus also presented itself.\nAs Brent, Ross, and I stared at the whiteboard, the discovery of\nthe need for multiple exogenous roles hit all of us in a kind of slow-\nmotion shockwave. It was not simply that the content needed to\nhave different labels attached to it to keep track of these people\nin a database\u2014something deeper was at work: the law and the\npractice of authorship actually dictated, to a certain extent, what\nthe software itself should look like. All of sudden, the questions\nwere preformatted, so to speak, by the law and by certain kinds\nof practices that had been normalized and thus were nearly invis-\nible: who should have permission to change what? Who will have\npermission to add or drop authors? Who will be allowed to make\nwhat changes, and who will have the legal right to do so and who\nthe moral or customary right? What implications follow from the\nchoices the designers make and the choices we present to authors\nor maintainers?\nThe Creative Commons licenses were key to revealing many of\nthese questions. The licenses were in themselves modulations of\nFree Software licenses, but created with people like artists, musi-\ncians, scholars, and filmmakers in mind. Without them, the content\nin Connexions would be unlicensed, perhaps intended to be in the\npublic domain, but ultimately governed by copyright statutes that\nprovided no clear answers to any of these questions, as those stat-\nutes were designed to deal with older media and a different publi-\ncation process. Using the Creative Commons licenses, on the other\nhand, meant that the situation of the content in Connexions became\nwell-defined enough, in a legal sense, to be used as a constraint\nin defining the structure of the software system. The license itself\nprovided the map of the territory by setting parameters for things\nsuch as distribution, modification, attribution, and even display,\nreading, or copying.\nFor instance, when the author and owner are different, it is not\nat all obvious who should be given credit. Authors, especially aca-\ndemic authors, expect to be given credit (which is often all they get)\nfor an article or a textbook they have written, yet universities often\nretain ownership of those textbooks, and ownership would seem\nto imply a legal right to be identified as both owner and author\n(e.g., Forrester Research reports or UNESCO reports, which hide the\nreuse, modification, norms 283 identity of authors). In the absence of any licenses, such a scenario\nhas no obvious solution or depends entirely on the specific context.\nHowever, the Creative Commons licenses specified the meaning of\nattribution and the requirement to maintain the copyright notice,\nthus outlining a procedure that gave the Connexions designers fixed\nconstraints against which to measure how they would implement\ntheir system.\nA positive result of such constraints is that they allow for a kind\nof institutional flexibility that would not otherwise be possible.\nWhether a university insists on expropriating copyright or allows\nscholars to keep their copyrights, both can use Connexions. Connex-\nions is more \u201copen\u201d than traditional textbook publishing because\nit allows a greater number of heterogeneous contributors to par-\nticipate, but it is also more \u201copen\u201d than something like Wikipedia,\nwhich is ideologically committed to a single definition of author-\nship and ownership (anonymous, reciprocally licensed collabora-\ntive creation by authors who are also the owners of their work).\nWhile Wikipedia makes such an ideological commitment, it cannot\nbe used by institutions that have made the decision to operate as\nexpropriators of content, or even in cases wherein authors willingly\nallow someone else to take credit. If authors and owners must be\nidentical, then either the author is identified as the owner, which\nis illegal in some cases, or the owner is identified as the author, a\nsituation no academic is willing to submit to.\nThe need for multiple roles also revealed other peculiar and trou-\nbling problems, such as the issue of giving an \u201cidentity\u201d to long-\ndead authors whose works are out of copyright. So, for instance, a\npiece by A. E. Housman was included as a module for a class, and\nwhile it is clear that Housman is the author, the work is no longer\nunder copyright, so Housman is no longer the copyright holder (nor\nis the society which published it in 1921). Yet Connexions requires\nthat a copyright be attached to each module to allow it to be li-\ncensed openly. This particular case, of a dead author, necessitated\ntwo interesting interventions. Someone has to actually create an\naccount for Housman and also issue the work as an \u201cedition\u201d or de-\nrivative under a new copyright. In this case, the two other authors\nare Scott McGill and Christopher Kelty. A curious question arose in\nthis context: should we be listed both as authors and owners (and\nmaintainers), or only as owners and maintainers? And if someone\nuses the module in a new context (as they have the right to do,\n284 reuse, modification, norms under the license), will they be required to give attribution only\nto Housman, or also to McGill and Kelty as well? What rights to\nownership do McGill and Kelty have over the digital version of the\npublic-domain text by Housman?14\nThe discussion of roles circulated fluidly across concepts like\nlaw (and legal licenses), norms, community, and identity. Brent\nand Ross and others involved had developed sophisticated imagi-\nnations of how Connexions would fit into the existing ecology of\nacademia, constrained all the while by both standard goals, like\nusability and efficiency, and by novel legal licenses and concerns\nabout the changing practices of authors and scholars. The ques-\ntion, for instance, of how a module can be used (technically, le-\ngally) is often confused with, or difficult to disentangle from, how\na module should be used (technically, legally, or, more generally,\n\u201csocially\u201d\u2014with usage shaped by the community who uses it). In\norder to make sense of this, Connexions programmers and partici-\npants like myself are prone to using the language of custom and\nnorm, and the figure of community, as in \u201cthe customary norms of\na scholarly community.\u201d\nFrom Law and Technology to Norm\nThe meaning of publication in Connexions and the questions about\nroles and their proper legal status emerged from the core concern\nwith reuse, which is the primary modulation of Free Software that\nConnexions carries out: the modulation of the meaning of source\ncode to include textbook writing. What makes source code such\na central component of Free Software is the manner in which it\nis shared and transformed, not the technical features of any par-\nticular language or program. So the modulation of source code to\ninclude textbooks is not just an attempt to make textbooks exact,\nalgorithmic, or digital, but an experiment in sharing textbook writ-\ning in a similar fashion.\nThis modulation also affects the other components: it creates a de-\nmand for openness in textbook creation and circulation; it demands\nnew kinds of copyright licenses (the Creative Commons licenses);\nand it affects the meaning of coordination among scholars, ranging\nfrom explicit forms of collaboration and co-creation to the entire\nspectrum of uses and reuses that scholars normally make of their\nreuse, modification, norms 285 peers\u2019 works. It is this modulation of coordination that leads to the\nsecond core concern of Connexions: that of the existence of \u201cnorms\u201d\nof scholarly creation, use, reuse, publication, and circulation.\nSince software programmers and engineers are prone to thinking\nabout things in concrete, practical, and detailed ways, discussions\nof creation, use, and circulation are rarely conducted at the level\nof philosophical abstraction. They are carried out on whiteboards,\nusing diagrams.\nThe whiteboard diagram transcribed in figure 8 was precipitated\nby a fairly precise question: \u201cWhen is the reuse of something in a\nmodule (or of an entire module) governed by \u2018academic norms\u2019\nand when is it subject to the legal constraints of the licenses?\u201d\nFor someone to quote a piece of text from one module in another\nis considered normal practice and thus shouldn\u2019t involve concerns\nabout legal rights and duties to fork the module (create a new\nmodified version, perhaps containing only the section cited, which\nis something legal licenses explicitly allow). But what if someone\nborrows, say, all of the equations in a module about information\ntheory and uses them to illustrate a very different point in a differ-\nent module. Does he or she have either a normal or a legal right\nto do so? Should the equations be cited? What should that citation\nlook like? What if the equations are particularly hard to mark-up\nin the MathML language and therefore represent a significant in-\nvestment in time on the part of the original author? Should the law\ngovern this activity, or should norms?\nThere is a natural tendency among geeks to answer these ques-\ntions solely with respect to the law; it is, after all, highly codified\nand seemingly authoritative on such issues. However, there is often\nno need to engage the law, because of the presumed consensus\n(\u201cacademic norms\u201d) about how to proceed, even if those norms con-\nflict with the law. But these norms are nowhere codified, and this\nmakes geeks (and, increasingly, academics themselves) uneasy. As\nin the case of a requirement of attribution, the constraints of a\nwritten license are perceived to be much more stable and reliable\nthan those of culture, precisely because culture is what remains\ncontested and contestable. So the idea of creating a new \u201cversion\u201d\nof a text is easier to understand when it is clearly circumscribed as\na legally defined \u201cderivative work.\u201d The Connexions software was\ntherefore implemented in such a way that the legal right to create\na derived work (to fork a module) could be done with the press of\n286 reuse, modification, norms 8. Whiteboard diagram: the cascade of reuse in Connexions. Conception\nby Ross Reedstrom, Brent Hendricks, and Christopher Kelty. Transcribed\nin the author\u2019s fieldnotes, 2003.\na button: a distinct module is automatically created, and it retains\nthe name of the original author and the original owner, but now\nalso includes the new author\u2019s name as author and maintainer.\nThat new author can proceed to make any number of changes.\nBut is forking always necessary? What if the derivative work con-\ntains only a few spelling corrections and slightly updated informa-\ntion? Why not change the existing module (where such changes\nwould be more akin to issuing a new edition), rather than cre-\nate a legally defined derivative work? Why not simply suggest the\nchanges to the original author? Why not collaborate? While a legal\nlicense gives people the right to do all of these things without ever\nconsulting the person who licensed it, there may well be occasions\nreuse, modification, norms 287 when it makes much more sense to ignore those rights in favor of\nother norms. The answers to these questions depend a great deal\non the kind and the intent of the reuse. A refined version of the\nwhiteboard diagram, depicted in figure 9, attempts to capture the\nvarious kinds of reuse and their intersection with laws, norms, and\ntechnologies.\nThe center of the diagram contains a list of different kinds of\nimaginable reuses, arrayed from least interventionist at the top\nto most interventionist at the bottom, and it implies that as the\nintended transformations become more drastic, the likelihood of\ncollaboration with the original author decreases. The arrow on the\nleft indicates the legal path from cultural norms to protected fair\nuses; the arrow on the right indicates the technical path from built-\nin legal constraints based on the licenses to software tools that\nmake collaboration (according to presumed scholarly norms) easier\nthan the alternative (exercising the legal right to make a derivative\nwork). With the benefit of hindsight, it seems that the arrows on\neither side should actually be a circle that connect laws, technolo-\ngies, and norms in a chain of influence and constraint, since it is\nclear in retrospect that the norms of authorial practice have actu-\nally changed (or at least have been made explicit) based on the\nexistence of licenses and the types of tools available (such as blogs\nand Wikipedia).\nThe diagram can best be understood as a way of representing,\nto Connexions itself (and its funders), the experiment under way\nwith the components of Free Software. By modulating source code\nto include the writing of scholarly textbooks, Connexions made vis-\nible the need for new copyright licenses appropriate to this content;\nby making the system Internet-based and relying on open stan-\ndards such as XML and Open Source components, Connexions also\nmodulated the concept of openness to include textbook publication;\nand by making the system possible as an open repository of freely\nlicensed textbook modules, Connexions made visible the changed\nconditions of coordination, not just between two collaborating au-\nthors, but within the entire system of publication, citation, use,\nreuse, borrowing, building on, plagiarizing, copying, emulating,\nand so on. Such changes to coordination may or may not take\nhold. For many scholars, they pose an immodest challenge to a\nworking system that has developed over centuries, but for others\nthey represent the removal of arbitrary constraints that prevent\n288 reuse, modification, norms 9. Whiteboard diagram transformed: forms of reuse in Connexions.\nConception by Christopher Kelty, 2004.\nnovel and innovative forms of knowledge creation and association\nrendered possible in the last thirty to forty years (and especially in\nthe last ten). For some, these modulations might form the basis for\na final modulation\u2014a Free Textbooks movement\u2014but as yet no\nsuch movement exists.\nIn the case of shared software source code, one of the principal\nreasons for sharing it was to reuse it: to build on it, to link to it, to\nemploy it in ways that made building more complex objects into an\neasier task. The very design philosophy of UNIX well articulates the\nnecessity of modularity and reuse, and the idea is no less powerful\nin other areas, such as textbooks. But just as the reuse of software is\nnot simply a feature of software\u2019s technical characteristics, the idea\nof \u201creusing\u201d scholarly materials implies all kinds of questions that\nare not simply questions of recombining texts. The ability to share\nsource code\u2014and the ability to create complex software based on\nit\u2014requires modulations of both the legal meaning of software,\nas in the case of EMACS, and the organizational form, as in the\nreuse, modification, norms 289 emergence of Free Software projects other than the Free Software\nFoundation (the Linux kernel, Perl, Apache, etc.).\nIn the case of textbook reuse (but only after Free Software), the\ntechnical and the legal problems that Connexions addresses are\nrelatively well specified: what software to use, whether to use XML,\nthe need for an excellent user interface, and so on. However, the\norganizational, cultural, or practical meaning of reuse is not yet\nentirely clear (a point made by figures 8 and 9). In many ways,\nthe recognition that there are cultural norms among academics\nmirrors the (re)discovery of norms and ethics among Free Software\nhackers.15 But the label \u201ccultural norms\u201d is a mere catch-all for a\nproblem that is probably better understood as a mixture of con-\ncrete technical, organizational, and legal questions and as more\nor less abstract social imaginaries through which a particular kind\nof material order is understood and pursued\u2014the creation of a re-\ncursive public. How do programmers, lawyers, engineers, and Free\nSoftware advocates (and anthropologists) \u201cfigure out\u201d how norms\nwork? How do they figure out ways to operationalize or make use\nof them? How do they figure out how to change them? How do\nthey figure out how to create new norms? They do so through the\nmodulations of existing practices, guided by imaginaries of moral\nand technical order. Connexions does not tend toward becoming\nFree Software, but it does tend toward becoming a recursive public\nwith respect to textbooks, education, and the publication of peda-\ngogical techniques and knowledge. The problematic of creating an\nindependent, autonomous public is thus the subterranean ground\nof both Free Software and Connexions.\nTo some extent, then, the matter of reuse raises a host of questions\nabout the borders and boundaries in and of academia. Brent, Ross,\nand I assumed at the outset that communities have both borders\nand norms, and that the two are related. But, as it turns out, this is\nnot a safe assumption. At neither the technical nor the legal level\nis the use of the software restricted to academics\u2014indeed, there\nis no feasible way to do that and still offer it on the Internet\u2014nor\ndoes anyone involved wish it to be so restricted. However, there\nis an implicit sense that the people who will contribute content\nwill primarily be academics and educators ( just as Free Software\nparticipants are expected, but not required to be programmers). As\nfigure 9 makes clear, there may well be tremendous variation in\nthe kinds of reuse that people wish to make, even within academia.\n290 reuse, modification, norms Scholars in the humanities, for instance, are loath to even imagine\nothers creating derivative works with articles they have written\nand can envision their work being used only in the conventional\nmanner of being read, cited, and critiqued. Scholars in engineer-\ning, biology, or computer science, on the other hand, may well take\npleasure in the idea or act of reuse, if it is adequately understood\nto be a \u201cscientific result\u201d or a suitably stable concept on which to\nbuild.16 Reuse can have a range of different meanings depending\nnot only on whether it is used by scholars or academics, but within\nthat heterogeneous group itself.\nThe Connexions software does not, however, enforce disciplinary\ndifferences. If anything it makes very strong and troubling claims\nthat knowledge is knowledge and that disciplinary constraints are\narbitrary. Thus, for instance, if a biologist wishes to transform a lit-\nerary scholar\u2019s article on Darwin\u2019s tropes to make it reflect current\nevolutionary theory, he or she could do so; it is entirely possible,\nboth legally and technically. The literary scholar could react in a\nnumber of ways, including outrage that the biologist has misread\nor misunderstood the work or pleasure in seeing the work refined.\nConnexions adheres rigorously to its ideas of openness in this re-\ngard; it neither encourages nor censures such behavior.\nBy contrast, as figure 9 suggests, the relationship between these\ntwo scholars can be governed either by the legal specification of\nrights contained in the licenses (a privately ordered legal regime\ndependent on a national-cum-global statutory regime) or by the\ncustomary means of collaboration enabled, perhaps enhanced, by\nsoftware tools. The former is the domain of the state, the legal pro-\nfession, and a moral and technical order that, for lack of a better\nword, might be called modernity. The latter, however, is the do-\nmain of the cultural, the informal, the practical, the interpersonal;\nit is the domain of ethics (prior to its modernization, perhaps) and\nof tradition.\nIf figure 9 is a recapitulation of modernity and tradition (what\nbetter role for an anthropologist to play!), then the presumptive\nboundaries around \u201ccommunities\u201d define which groups possess\nwhich norms. But the very design of Connexions\u2014its technical and\nlegal exactitude\u2014immediately brings a potentially huge variety\nof traditions into conflict with one another. Can the biologist and\nthe literary scholar be expected to occupy the same universe of\nnorms? Does the fact of being academics, employees of a university,\nreuse, modification, norms 291 or readers of Darwin ensure this sharing of norms? How are the\nboundaries policed and the norms communicated and reinforced?\nThe problem of reuse therefore raises a much broader and more\ncomplex question: do norms actually exist? In particular, do they\nexist independent of the particular technical, legal, or organiza-\ntional practice in which groups of people exist\u2014outside the coordi-\nnated infrastructure of scholarship and science? And if Connexions\nraises this question, can the same question not also be asked of\nthe elaborate system of professions, disciplines, and organizations\nthat coordinate the scholarship of different communities? Are these\nnorms, or are they \u201ctechnical\u201d and \u201clegal\u201d practices? What differ-\nence does formalization make? What difference does bureaucrati-\nzation make?17\nThe question can also be posed this way: should norms be under-\nstood as historically changing constructs or as natural features of\nhuman behavior (regular patterns, or conventions, which emerge\ninevitably wherever human beings interact). Are they a feature of\nchanging institutions, laws, and technologies, or do they form and\npersist in the same way wherever people congregate? Are norms\nfeatures of a \u201ccalculative agency,\u201d as Michael Callon puts it, or\nare they features of the evolved human mind, as Marc Hauser ar-\ngues?18\nThe answer that my informants give, in practice, concerning the\nmode of existence of cultural norms is neither. On the one hand, in\nthe Connexions project the question of the mode of existence of aca-\ndemic norms is unanswered; the basic assumption is that certain\nactions are captured and constrained neither by legal constraints\nnor technical barriers, and that it takes people who know or study\n\u201ccommunities\u201d (i.e., nonlegal and nontechnical constraints) to fig-\nure out what those actions may be. On some days, the project is\nmodestly understood to enable academics to do what they do faster\nand better, but without fundamentally changing anything about\nthe practice, institutions, or legal relations; on other days, however,\nit is a radically transformative project, changing how people think\nabout creating scholarly work, a project that requires educating\npeople and potentially \u201cchanging the culture\u201d of scholarly work,\nincluding its technology, its legal relations, and its practices.\nIn stark contrast (despite the very large degree of simpatico), the\nprincipal members of Creative Commons answer the question of the\nexistence of norms quite differently than do those in Connexions:\n292 reuse, modification, norms they assert that norms not only change but are manipulated and\/or\nchanneled by the modulation of technical and legal practices (this\nis the novel version of law and economics that Creative Commons\nis founded on). Such an assertion leaves very little for norms or for\nculture; there may be a deep evolutionary role for rule following or\nfor choosing socially sanctioned behavior over socially unaccept-\nable behavior, but the real action happens in the legal and techni-\ncal domains. In Creative Commons the question of the existence\nof norms is answered firmly in the phrase coined by Glenn Brown:\n\u201cpunt to culture.\u201d For Creative Commons, norms are a prelegal and\npretechnical substrate upon which the licenses they create operate.\nNorms must exist for the strategy employed in the licenses to make\nsense\u2014as the following story illustrates.\nOn the Nonexistence of Norms in the\nCulture of No Culture\nMore than once, I have found myself on the telephone with Glenn\nBrown, staring at notes, a diagram, or some inscrutable collec-\ntion of legalese. Usually, the conversations wander from fine legal\npoints to music and Texas politics to Glenn\u2019s travels around the\nglobe. They are often precipitated by some previous conversation\nand by Glenn\u2019s need to remind himself (and me) what we are in\nthe middle of creating. Or destroying. His are never simple ques-\ntions. While the Connexions project started with a repository of\nscholarly content in need of a license, Creative Commons started\nwith licenses in need of particular kinds of content. But both proj-\nects required participants to delve into the details of both licenses\nand the structure of digital content, which qualified me, for both\nprojects, as the intermediary who could help explore these intersec-\ntions. My phone conversations with Glenn, then, were much like the\nwhiteboard conversations at Connexions: filled with a mix of tech-\nnical and legal terminology, and conducted largely in order to give\nGlenn the sense that he had cross-checked his plans with someone\npresumed to know better. I can\u2019t count the number of times I have\nhung up the phone or left the conference room wondering, \u201cHave\nI just sanctioned something mad?\u201d Yet rarely have I felt that my\ninterventions served to do more than confirm suspicions or derail\nalready unstable arguments.\nreuse, modification, norms 293 In one particular conversation\u2014the \u201cpunt to culture\u201d conversa-\ntion\u2014I found myself bewildered by a sudden understanding of the\nprocess of writing legal licenses and of the particular assumptions\nabout human behavior that need to be present in order to imagine\ncreating these licenses or ensuring that they will be beneficial to\nthe people who will use them.\nThese discussions (which often included other lawyers) happened\nin a kind of hypothetical space of legal imagination, a space highly\nstructured by legal concepts, statutes, and precedents, and one ex-\ntraordinarily carefully attuned to the fine details of semantics. A\ncore aspect of operating within this imagination is the distinction\nbetween law as an abstract semantic entity and law as a practical\nfact that people may or may not deal with. To be sure, not all law-\nyers operate this way, but the warrant for thinking this way comes\nfrom no less eminent an authority than Oliver Wendell Holmes, for\nwhom the \u201cPath of Law\u201d was always from practice to abstract rule,\nand not the reverse.19 The opposition is unstable, but I highlight it\nhere because it was frequently used as a strategy for constructing\nprecise legal language. The ability to imagine the difference be-\ntween an abstract rule designating legality and a rule encountered\nin practice was a first step toward seeing how the language of the\nrule should be constructed.\nI helped write, read, and think about the first of the Creative\nCommons licenses, and it was through this experience that I came\nto understand how the crafting of legal language works, and in par-\nticular how the mode of existence of cultural or social norms relates\nto the crafting of legal language. Creative Commons licenses are\nnot a familiar legal entity, however. They are modulations of the\nFree Software license, but they differ in important ways.\nThe Creative Commons licenses allow authors to grant the use\nof their work in about a dozen different ways\u2014that is, the license\nitself comes in versions. One can, for instance, require attribution,\nprohibit commercial exploitation, allow derivative or modified\nworks to be made and circulated, or some combination of all these.\nThese different combinations actually create different licenses, each\nof which grants intellectual-property rights under slightly different\nconditions. For example, say Marshall Sahlins decides to write a\npaper about how the Internet is cultural; he copyrights the paper\n(\u201c\u00a9 2004 Marshall Sahlins\u201d), he requires that any use of it or any\ncopies of it maintain the copyright notice and the attribution of\n294 reuse, modification, norms authorship (these can be different), and he furthermore allows for\ncommercial use of the paper. It would then be legal for a publishing\nhouse to take the paper off Sahlins\u2019s Linux-based Web server and\npublish it in a collection without having to ask permission, as long\nas the paper remains unchanged and he is clearly and unambigu-\nously listed as author of the paper. The publishing house would not\nget any rights to the work, and Sahlins would not get any royalties.\nIf he had specified noncommercial use, the publisher would instead\nhave needed to contact him and arrange for a separate license (Cre-\native Commons licenses are nonexclusive), under which he could\ndemand some share of revenue and his name on the cover of the\nbook.20 But say he was, instead, a young scholar seeking only peer\nrecognition and approbation\u2014then royalties would be secondary\nto maximum circulation. Creative Commons allows authors to as-\nsert, as its members put it, \u201csome rights reserved\u201d or even \u201cno rights\nreserved.\u201d\nBut what if Sahlins had chosen a license that allowed modifica-\ntion of his work. This would mean that I, Christopher Kelty, whether\nin agreement with or in objection to his work, could download the\npaper, rewrite large sections of it, add in my own baroque and id-\niosyncratic scholarship, and write a section that purports to debunk\n(or, what could amount to the same, augment) Sahlins\u2019s arguments.\nI would then be legally entitled to re-release the paper as \u201c\u00a9 2004\nMarshall Sahlins, with modifications \u00a9 2007 Christopher Kelty,\u201d so\nlong as Sahlins is identified as the author of the paper. The nature\nor extent of the modifications is not legally restricted, but both the\noriginal and the modified version would be legally attributed to\nSahlins (even though he would own only the first paper).\nIn the course of a number of e-mails, chat sessions, and phone\nconversations with Glenn, I raised this example and proposed that\nthe licenses needed a way to account for it, since it seemed to me\nentirely possible that were I to produce a modified work that so dis-\ntorted Sahlins\u2019s original argument that he did not want to be asso-\nciated with the modified paper, then he should have the right also\nto repudiate his identification as author. Sahlins should, legally\nspeaking, be able to ask me to remove his name from all subsequent\nversions of my misrepresentation, thus clearing his good name and\nproviding me the freedom to continue sullying mine into obscurity.\nAfter hashing it out with the expensive Palo Alto legal firm that\nwas officially drafting the licenses, we came up with text that said:\nreuse, modification, norms 295 \u201cIf You create a Derivative Work, upon notice from any Licensor\nYou must, to the extent practicable, remove from the Derivative\nWork any reference to such Licensor or the Original Author, as\nrequested.\u201d\nThe bulk of our discussion centered around the need for the\nphrase, \u201cto the extent practicable.\u201d Glenn asked me, \u201cHow is the\noriginal author supposed to monitor all the possible uses of her\nname? How will she enforce this clause? Isn\u2019t it going to be difficult\nto remove the name from every copy?\u201d Glenn was imagining a situ-\nation of strict adherence, one in which the presence of the name\non the paper was the same as the reputation of the individual,\nregardless of who actually read it. On this theory, until all traces\nof the author\u2019s name were expunged from each of these teratomata\ncirculating in the world, there could be no peace, and no rest for\nthe wronged.\nI paused, then gave the kind of sigh meant to imply that I had\ncome to my hard-won understandings of culture through arduous\ndissertation research: \u201cIt probably won\u2019t need to be strictly enforced\nin all cases\u2014only in the significant ones. Scholars tend to respond\nto each other only in very circumscribed cases, by writing letters to\nthe editor or by sending responses or rebuttals to the journal that\npublished the work. It takes a lot of work to really police a reputa-\ntion, and it differs from discipline to discipline. Sometimes, drastic\naction might be needed, usually not. There is so much misuse and\nabuse of people\u2019s arguments and work going on all the time that\npeople only react when they are directly confronted with serious\nabuses. And even so, it is only in cases of negative criticism or mis-\nuse that people need respond. When a scholar uses someone\u2019s work\napprovingly, but incorrectly, it is usually considered petulant (at\nbest) to correct them publicly.\u201d\n\u201cIn short,\u201d I said, leaning back in my chair and acting the part\nof expert, \u201cit\u2019s like, you know, c\u2019mon\u2014it isn\u2019t all law, there are a\nbunch of, you know, informal rules of civility and stuff that govern\nthat sort of thing.\u201d\nThen Glenn said., \u201cOh, okay, well that\u2019s when we punt to culture.\u201d\nWhen I heard this phrase, I leaned too far back and fell over,\njoyfully stunned. Glenn had managed to capture what no amount\nof fieldwork, with however many subjects, could have. Some com-\nbination of American football, a twist of Hobbes or Holmes, and a\nlived understanding of what exactly these copyright licenses are\n296 reuse, modification, norms meant to achieve gave this phrase a luminosity I usually associate\nonly with Balinese cock-fights. It encapsulated, almost as a slogan,\na very precise explanation of what Creative Commons had under-\ntaken. It was not a theory Glenn proposed with this phrase, but a\nstrategy in which a particular, if vague, theory of culture played a\nrole.\nFor those unfamiliar, a bit of background on U.S. football may\nhelp. When two teams square off on the football field, the offensive\nteam gets four attempts, called \u201cdowns,\u201d to move the ball either ten\nyards forward or into the end zone for a score. The first three downs\nusually involve one of two strategies: run or pass, run or pass. On\nthe fourth down, however, the offensive team must either \u201cgo for\nit\u201d (run or pass), kick a field goal (if close enough to the end zone),\nor \u201cpunt\u201d the ball to the other team. Punting is a somewhat disap-\npointing option, because it means giving up possession of the ball\nto the other team, but it has the advantage of putting the other\nteam as far back on the playing field as possible, thus decreasing\nits likelihood of scoring.\nTo \u201cpunt to culture,\u201d then, suggests that copyright licenses try\nthree times to legally restrict what a user or consumer of a work\ncan make of it. By using the existing federal intellectual-property\nlaws and the rules of license and contract writing, copyright li-\ncenses articulate to people what they can and cannot do with that\nwork according to law. While the licenses do not (they cannot)\nforce people, in any tangible sense, to do one thing or another,\nthey can use the language of law and contract to warn people, and\nperhaps obliquely, to threaten them. If the licenses end up silent\non a point\u2014if there is no \u201cscore,\u201d to continue the analogy\u2014then\nit\u2019s time to punt to culture. Rather than make more law, or call in\nthe police, the license strategy relies on culture to fill in the gaps\nwith people\u2019s own understandings of what is right and wrong, be-\nyond the law. It operationalizes a theory of culture, a theory that\nemphasizes the sovereignty of nonstate customs and the diversity\nof systems of cultural norms. Creative Commons would prefer that\nits licenses remain legally minimalist. It would much prefer to\nassume\u2014indeed, the licenses implicitly require\u2014the robust, pow-\nerful existence of this multifarious, hetero-physiognomic, and for-\nmidable opponent to the law with neither uniform nor mascot,\nhunched at the far end of the field, preparing to, so to speak, clean\nlaw\u2019s clock.\nreuse, modification, norms 297 Creative Commons\u2019s \u201cculture\u201d thus seems to be a somewhat vague\nmixture of many familiar theories. Culture is an unspecified but\nfinely articulated set of given, evolved, designed, informal, prac-\nticed, habitual, local, social, civil, or historical norms that are ex-\npected to govern the behavior of individuals in the absence of a\nstate, a court, a king, or a police force, at one of any number of\nscales. It is not monolithic (indeed, my self-assured explanation\nconcerned only the norms of \u201cacademia\u201d), but assumes a diversity\nbeyond enumeration. It employs elements of relativism\u2014any cul-\nture should be able to trump the legal rules. It is not a hereditary\nbiological theory, but one that assumes historical contingency and\narbitrary structures.\nCertainly, whatever culture is, it is separate from law. Law is, to\nborrow Sharon Traweek\u2019s famous phrase, \u201ca culture of no culture\u201d\nin this sense. It is not the cultural and normative practices of legal\nscholars, judges, lawyers, legislators, and lobbyists that determine\nwhat laws will look like, but their careful, expert, noncultural ra-\ntiocination. In this sense, punting to culture implies that laws are\nthe result of human design, whereas culture is the result of hu-\nman action, but not of human design. Law is systematic and trac-\ntable; culture may have a deep structure, but it is intractable to\nhuman design. It can, however, be channeled and tracked, nudged\nor guided, by law.\nThus, Lawrence Lessig, one of the founders of Creative Commons\nhas written extensively about the \u201cregulation of social meaning,\u201d\nusing cases such as those involving the use or nonuse of seatbelts\nor whether or not to allow smoking in public places. The decision\nnot to wear a seatbelt, for instance, may have much more to do\nwith the contextual meaning of putting on a seatbelt (don\u2019t you\ntrust the cab driver?) than with either the existence of the seatbelt\n(or automatic seatbelts, for that matter) or with laws demanding\ntheir use. According to Lessig, the best law can do in the face of\ncustom is to change the meaning of wearing the seatbelt: to give the\nrefusal a dishonorable rather than an honorable meaning. Creative\nCommons licenses are based on a similar assumption: the law is\nrelatively powerless in the face of entrenched academic or artistic\ncustoms, and so the best the licenses can do is channel the meaning\nof sharing and reuse, of copyright control or infringement. As Glenn\nexplained in the context of a discussion about a license that would\nallow music sampling.\n298 reuse, modification, norms We anticipate that the phrase \u201cas appropriate to the medium, genre,\nand market niche\u201d might prompt some anxiety, as it leaves things\nrelatively undefined. But there\u2019s more method here than you might\nexpect: The definition of \u201csampling\u201d or \u201ccollage\u201d varies across differ-\nent media. Rather than try to define all possible scenarios (including\nones that haven\u2019t happened yet)\u2014which would have the effect of re-\nstricting the types of re-uses to a limited set\u2014we took the more laissez\nfaire approach.\nThis sort of deference to community values\u2014think of it as \u201cpunting\nto culture\u201d\u2014is very common in everyday business and contract law.\nThe idea is that when lawyers have trouble defining the specialized\nterms of certain subcultures, they should get out of the way and let\nthose subcultures work them out. It\u2019s probably not a surprise Creative\nCommons likes this sort of notion a lot.21\nAs in the case of reuse in Connexions, sampling in the music world\ncan imply a number of different, perhaps overlapping, custom-\nary meanings of what is acceptable and what is not. For Connex-\nions, the trick was to differentiate the cases wherein collaboration\nshould be encouraged from the cases wherein the legal right to\n\u201csample\u201d\u2014to fork or to create a derived work\u2014was the appropri-\nate course of action. For Creative Commons, the very structure of\nthe licenses attempts to capture this distinction as such and to al-\nlow for individuals to make determinations about the meaning of\nsampling themselves.22\nAt stake, then, is the construction of both technologies and legal\nlicenses that, as Brent and Rich would assert, \u201cmake it easy for us-\ners to do the right thing.\u201d The \u201cright thing,\u201d however, is precisely\nwhat goes unstated: the moral and technical order that guides the\ndesign of both licenses and tools. Connexions users are given tools\nthat facilitate citation, acknowledgment, attribution, and certain\nkinds of reuse instead of tools that privilege anonymity or facili-\ntate proliferation or encourage nonreciprocal collaborations. By\nthe same token, Creative Commons licenses, while legally binding,\nare created with the aim of changing norms: they promote attri-\nbution and citation; they promote fair use and clearly designated\nuses; they are written to give users flexibility to decide what kinds\nof things should be allowed and what kinds shouldn\u2019t. Without a\ndoubt, the \u201cright thing\u201d is right for some people and not for oth-\ners\u2014and it is thus political. But the criteria for what is right are not\nreuse, modification, norms 299 merely political; the criteria are what constitute the affinity of these\ngeeks in the first place, what makes them a recursive public. They\nsee in these instruments the possibility for the creation of authentic\npublics whose role is to stand outside power, outside markets, and\nto participate in sovereignty, and through this participation to pro-\nduce liberty without sacrificing stability.\nConclusion\nWhat happens when geeks modulate the practices that make up\nFree Software? What is the intuition or the cultural significance of\nFree Software that makes people want to emulate and modulate it?\nCreative Commons and Connexions modulate the practices of Free\nSoftware and extend them in new ways. They change the meaning\nof shared source code to include shared nonsoftware, and they try\nto apply the practices of license writing, coordination, and open-\nness to new domains. At one level, such an activity is fascinating\nsimply because of what it reveals: in the case of Connexions, it\nreveals the problem of determining the finality of a work. How\nshould the authority, stability, and reliability of knowledge be as-\nsessed when work can be rendered permanently modifiable? It is an\nactivity that reveals the complexity of the system of authorization\nand evaluation that has been built in the past.\nThe intuition that Connexions and Creative Commons draw from\nFree Software is an intuition about the authority of knowledge,\nabout a reorientation of knowledge and power that demands a\nresponse. That response needs to be technical and legal, to be sure,\nbut it also needs to be public\u2014a response that defines the meaning\nof finality publicly and openly and makes modifiability an irrevers-\nible aspect of the process of stabilizing knowledge. Such a commit-\nment is incompatible with the provision of stable knowledge by\nunaccountable private parties, whether individuals or corporations\nor governments, or by technical fiat. There must always remain the\npossibility that someone can question, change, reuse, and modify\naccording to their needs.\n300 reuse, modification, norms Conclusion\nThe Cultural Consequences of Free Software\nFree Software is changing. In all aspects it looks very different from\nwhen I started, and in many ways the Free Software described herein\nis not the Free Software readers will encounter if they turn to the\nInternet to find it. But how could it be otherwise? If the argument I\nmake in Two Bits is at all correct, then modulation must constantly\nbe occurring, for experimentation never seeks its own conclusion.\nA question remains, though: in changing, does Free Software and\nits kin preserve the imagination of moral and technical order that\ncreated it? Is the recursive public something that survives, orders,\nor makes sense of these changes? Does Free Software exist for more\nthan its own sake?\nIn Two Bits I have explored not only the history of Free Software\nbut also the question of where such future changes will have come from. I argue for seeing continuity in certain practices of everyday\nlife precisely because the Internet and Free Software pervade ev-\neryday life to a remarkable, and growing, degree. Every day, from\nhere to there, new projects and ideas and tools and goals emerge\neverywhere out of the practices that I trace through Free Software:\nConnexions and Creative Commons, open access, Open Source syn-\nthetic biology, free culture, access to knowledge (a2k), open cola,\nopen movies, science commons, open business, Open Source yoga,\nOpen Source democracy, open educational resources, the One Lap-\ntop Per Child project, to say nothing of the proliferation of wiki-\neverything or the \u201cpeer production\u201d of scientific data or consumer\nservices\u2014all new responses to a widely felt reorientation of knowl-\nedge and power.1 How is one to know the difference between all\nthese things? How is one to understand the cultural significance\nand consequence of them? Can one distinguish between projects\nthat promote a form of public sphere that can direct the actions of\nour society versus those that favor corporate, individual, or hierar-\nchical control over decision making?\nOften the first response to such emerging projects is to focus on\nthe promises and ideology of the people involved. On the one hand,\nclaiming to be open or free or public or democratic is something\nnearly everyone does (including unlikely candidates such as the\ndefense intelligence agencies of the United States), and one should\ntherefore be suspicious and critical of all such claims.2 While such\narguments and ideological claims are important, it would be a grave\nmistake to focus only on these statements. The \u201cmovement\u201d\u2014the\nideological, critical, or promissory aspect\u2014is just one component\nof Free Software and, indeed, the one that has come last, after\nthe other practices were figured out and made legible, replicable,\nand modifiable. On the other hand, it is easy for geeks and Free\nSoftware advocates to denounce emerging projects, to say, \u201cBut\nthat isn\u2019t really Open Source or Free Software.\u201d And while it may\nbe tempting to fix the definition of Free Software once and for all\nin order to ensure a clear dividing line between the true sons and\nthe carpetbaggers, to do so would reduce Free Software to mere\nrepetition without difference, would sacrifice its most powerful and\ndistinctive attribute: its responsive, emergent, public character.\nBut what questions should one ask? Where should scholars or cu-\nrious onlookers focus their attention in order to see whether or not\na recursive public is at work? Many of these questions are simple,\n302 conclusion practical ones: are software and networks involved at any level?\nDo the participants claim to understand Free Software or Open\nSource, either in their details or as an inspiration? Is intellectual-\nproperty law a key problem? Are participants trying to coordinate\neach other through the Internet, and are they trying to take advan-\ntage of voluntary, self-directed contributions of some kind? More\nspecifically, are participants modulating one of these practices? Are\nthey thinking about something in terms of source code, or source\nand binary? Are they changing or creating new forms of licenses,\ncontracts, or privately ordered legal arrangements? Are they ex-\nperimenting with forms of coordinating the voluntary actions of\nlarge numbers of unevenly distributed people? Are the people who\nare contributing aware of or actively pursuing questions of ideol-\nogy, distinction, movement, or opposition? Are these practices rec-\nognized as something that creates the possibility for affinity, rather\nthan simply arcane \u201ctechnical\u201d practices that are too complex to\nunderstand or appreciate?\nIn the last few years, talk of \u201csocial software\u201d or \u201cWeb 2.0\u201d has\ndominated the circuit of geek and entrepreneur conferences and\ndiscussions: Wikipedia, MySpace, Flickr, and YouTube, for ex-\nample. For instance, there are scores and scores of \u201csocial\u201d music\nsites, with collaborative rating, music sharing, music discovery,\nand so forth. Many of these directly use or take inspiration from\nFree Software. For all of them, intellectual property is a central\nand dominating concern. Key to their novelty is the leveraging and\ncoordinating of massive numbers of people along restricted lines\n(i.e., music preferences that guide music discovery). Some even ad-\nvocate or lobby for free(er) access to digital music. But they are not\n(yet) what I would identify as recursive publics: most of them are\ncommercial entities whose structure and technical specifications\nare closely guarded and not open to modification. While some such\nentities may deal in freely licensed content (for instance, Creative\nCommons\u2013licensed music), few are interested in allowing strang-\ners to participate in, modulate, or modify the system as such; they\nare interested in allowing users to become consumers in more and\nmore sophisticated ways, and not necessarily in facilitating a pub-\nlic culture of music. They want information and knowledge to be\nfree, to be sure, but not necessarily the infrastructure that makes\nthat information available and knowledge possible. Such entities\nlack the \u201crecursive\u201d commitment.\nconclusion 303 By contrast, some corners of the open-access movement are\nmore likely to meet this criteria. As the appellation suggests, par-\nticipants see it as a movement, not a corporate or state entity,\na movement founded on practices of copyleft and the modula-\ntion of Free Software licensing ideas. The use of scientific data\nand the tools for making sense of open access are very often at\nthe heart of controversy in science (a point often reiterated by sci-\nence and technology studies), and so there is often an argument\nabout not only the availability of data but its reuse, modification,\nand modulation as well. Projects like the BioBricks Foundation\n(biobricks.org) and new organizations like the Public Library of\nScience (plos.org) are committed to both availability and certain\nforms of collective modification. The commitment to becoming a\nrecursive public, however, raises unprecedented issues about the\nnature of quality, reliability, and finality of scientific data and\nresults\u2014questions that will reverberate throughout the sciences\nas a result.\nFarther afield, questions of \u201ctraditional heritage\u201d claims, the com-\npulsory licensing of pharmaceuticals, or new forms of \u201ccrowdsourc-\ning\u201d in labor markets are also open to analysis in the terms I offer\nin Two Bits.3 Virtual worlds like Second Life, \u201ca 3D digital world\nimagined, created, and owned by its residents,\u201d are increasingly\nlaboratories for precisely the kinds of questions raised here: such\nworlds are far less virtual than most people realize, and the experi-\nments conducted there far more likely to migrate into the so-called\nreal world before we know it\u2014including both economic and demo-\ncratic experiments.4 How far will Second Life go in facilitating a\nrecursive public sphere? Can it survive both as a corporation and as\na \u201cworld\u201d? And of course, there is the question of the \u201cblogosphere\u201d\nas a public sphere, as a space of opinion and discussion that is radi-\ncally open to the voices of massive numbers of people. Blogging\ngives the lie to conventional journalism\u2019s self-image as the public\nsphere, but it is by no means immune to the same kinds of prob-\nlematic dynamics and polarizations, no more \u201crational-critical\u201d\nthan FOX News, and yet . . .\nSuch examples should indicate the degree to which Two Bits is\nfocused on a much longer time span than simply the last couple\nof years and on much broader issues of political legitimacy and\ncultural change. Rather than offer immediate policy prescriptions\nor seek to change the way people think about an issue, I have ap-\n304 conclusion proached Two Bits as a work of history and anthropology, making\nit less immediately applicable in the hopes that it is more lastingly\nusable. The stories I have told reach back at least forty years, if not\nlonger. While it is clear that the Internet as most people know it is\nonly ten to fifteen years old, it has been \u201cin preparation\u201d since at\nleast the late 1950s. Students in my classes\u2014especially hip geeks\ndeep in Free Software apprenticeship\u2014are bewildered to learn that\nthe arguments and usable pasts they are rehearsing are refinements\nand riffs on stories that are as old or older than their parents. This\ndeeper stability is where the cultural significance of Free Software\nlies: what difference does Free Software today introduce with re-\nspect to knowledge and power yesterday?\nFree Software is a response to a problem, in much the same way\nthat the Royal Society in the sixteenth century, the emergence of a\npublishing industry in the eighteenth century, and the institutions\nof the public sphere in the eighteenth and nineteenth centuries were\nresponses. They responded to the collective challenge of creating\nregimes of governance that required\u2014and encouraged\u2014reliable\nempirical knowledge as a basis for their political legitimacy. Such\npolitical legitimacy is not an eternal or theoretical problem; it is a\nproblem of constant real-world practice in creating the infrastruc-\ntures by which individuals come to inhabit and understand their\nown governance, whether by states, corporations, or machines. If\npower seeks consent of the governed\u2014and especially the consent\nof the democratic, self-governing kind that has become the global\ndominant ideal since the seventeenth century\u2014it must also seek to\nensure the stability and reliability of the knowledge on which that\nconsent is propped.\nDebates about the nature and history of publics and public spheres\nhave served as one of the main arenas for this kind of questioning,\nbut, as I hope I have shown here, it is a question not only of public\nspheres but of practices, technologies, laws, and movements, of\ngoing concerns which undergo modulation and experimentation in\naccord with a social imagination of order both moral and techni-\ncal. \u201cRecursive public\u201d as a concept is not meant to replace that\nof public sphere. I intend neither for actors nor really for many\nscholars to find it generally applicable. I would not want to see it\nsuddenly discovered everywhere, but principally in tracking the\ntransformation, proliferation, and differentiation of Free Software\nand its derivatives.\nconclusion 305 Several threads from the three parts of Two Bits can now be tied\ntogether. The detailed descriptions of Free Software and its modu-\nlations should make clear that (1) the reason the Internet looks the\nway it does is due to the work of figuring out Free Software, both\nbefore and after it was recognized as such; (2) neither the Inter-\nnet nor the computer is the cause of a reorientation of knowledge\nand power, but both are tools that render possible modulations of\nsettled practices, modulations that reveal a much older problem\nregarding the legitimacy of the means of circulation and produc-\ntion of knowledge; (3) Free Software is not an ethical stance, but\na practical response to the revelation of these older problems; and\n(4) the best way to understand this response is to see it as a kind of\npublic sphere, a recursive public that is specific to the technical and\nmoral imaginations of order in the contemporary world of geeks.\nIt is possible now to return to the practical and political meaning\nof the \u201csingularity\u201d of the Internet, that is, to the fact that there\nis only one Internet. This does not mean that there are no other\nnetworks, but only that the Internet is a singular entity and not an\ninstance of a general type. How is it that the Internet is open in the\nsame way to everyone, whether an individual or a corporate or a\nnational entity? How has it become extensible (and, by extension,\ndefensible) by and to everyone, regardless of their identity, locale,\ncontext, or degree of power?\nThe singularity of the Internet is both an ontological and an epis-\ntemological fact; it is a feature of the Internet\u2019s technical configura-\ntions and modes of ordering the actions of humans and machines\nby protocols and software. But it is also a feature of the technical\nand moral imaginations of the people who build, manage, inhabit,\nand expand the Internet. Ontologically, the creation and dissemina-\ntion of standardized protocols, and novel standard-setting processes\nare at the heart of the story. In the case of the Internet, differences\nin standards-setting processes are revealed clearly in the form of the\nfamous Request for Comments system of creating, distributing, and\nmodifying Internet protocols. The RFC system, just as much as the\nGeneva-based International Organization for Standards, reveal the\nfault lines of international legitimacy in complex societies depen-\ndent on networks, software, and other high-tech forms of knowl-\nedge production, organization, and governance. The legitimacy of\nstandards has massive significance for the abilities of individual\nactors to participate in their own recursive publics, whether they\n306 conclusion be publics that address software and networks or those that address\neducation and development. But like the relationship between \u201claw\non the books\u201d and \u201claw in action,\u201d standards depend on the coor-\ndinated action and order of human practices.\nWhat\u2019s more, the seemingly obvious line between a legitimate\nstandard and a marketable product based on these standards causes\nnothing but trouble. The case of open systems in the 1980s high-end\ncomputer industry demonstrates how the logic of standardization\nis not at all clearly distinguished from the logic of the market.\nThe open-systems battles resulted in novel forms of cooperation-\nwithin-competition that sought both standardization and competi-\ntive advantage at the same time. Open systems was an attempt to\nachieve a kind of \u201csingularity,\u201d not only for a network but for a\nmarket infrastructure as well. Open systems sought ways to reform\ntechnologies and markets in tandem. What it ignored was the legal\nstructure of intellectual property. The failure of open systems re-\nveals the centrality of the moral and technical order of intellectual\nproperty\u2014to both technology and markets\u2014and shows how a reli-\nance on this imagination of order literally renders impossible the\nstandardization of singular market infrastructure. By contrast, the\nsuccess of the Internet as a market infrastructure and as a singular\nentity comes in part because of the recognition of the limitations of\nthe intellectual-property system\u2014and Free Software in the 1990s\nwas the main experimental arena for trying out alternatives.\nThe singularity of the Internet rests in turn on a counterintui-\ntive multiplicity: the multiplicity of the UNIX operating system and\nits thousands of versions and imitations and reimplementations.\nUNIX is a great example of how novel, unexpected kinds of order\ncan emerge from high-tech practices. UNIX is neither an academic\n(gift) nor a market phenomenon; it is a hybrid model of sharing\nthat emerged from a very unusual technical and legal context.\nUNIX demonstrates how structured practices of sharing produce\ntheir own kind of order. Contrary to the current scholarly consen-\nsus that Free Software and its derivatives are a kind of \u201cshadow\neconomy\u201d (a \u201csharing\u201d economy, a \u201cpeer production\u201d economy,\na \u201cnoncommercial\u201d economy), UNIX was never entirely outside\nof the mainstream market. The meanings of sharing, distribution,\nand profitability are related to the specific technical, legal, and\norganizational context. Because AT&T was prevented from com-\nmercializing UNIX, because UNIX users were keen to expand and\nconclusion 307 adapt it for their own uses, and because its developers were keen\nto encourage and assist in such adaptations, UNIX proliferated and\ndifferentiated in ways that few commercial products could have.\nBut it was never \u201cfree\u201d in any sense. Rather, in combination with\nopen systems, it set the stage for what \u201cfree\u201d could come to mean in\nthe 1980s and 1990s. It was a nascent recursive public, confronting\nthe technical and legal challenges that would come to define the\npractices of Free Software. To suggest that it represents some kind\nof \u201coutside\u201d to a functioning economic market based in money is to\nmisperceive how transformative of markets UNIX and the Internet\n(and Free Software) have been. They have initiated an imagination\nof moral and technical order that is not at all opposed to ideolo-\ngies of market-based governance. Indeed, if anything, what UNIX\nand Free Software represent is an imagination of how to change an\nentire market-based governance structure\u2014not just specific markets in\nthings\u2014to include a form of public sphere, a check on the power\nof existing authority.\nUNIX and Open Systems should thus be seen as early stages of a\ncollective technical experiment in transforming our imaginations\nof order, especially of the moral order of publics, markets, and\nself-governing peoples. The continuities and the gradualness of the\nchange are more apparent in these events than any sudden rupture\nor discontinuity that the \u201cinvention of the Internet\u201d or the passing of\nnew intellectual-property laws might suggest. The \u201creorientation of\nknowledge and power\u201d is more dance than earthquake; it is strati-\nfied in time, complex in its movements, and takes an experimental\nform whose concrete traces are the networks, infrastructures, ma-\nchines, laws, and standards left in the wake of the experiments.\nAvailability, reusability, and modifiability are at the heart of this\nreorientation. The experiments of UNIX and open systems would\nhave come to nothing if they had not also prompted a concur-\nrent experimentation with intellectual-property law, of which the\ncopyleft license is the central and key variable. Richard Stallman\u2019s\ncreation of GNU EMACS and the controversy over propriety that\nit engendered was in many ways an attempt to deal with exactly\nthe same problem that UNIX vendors and open-systems advocates\nfaced: how to build extensibility into the software market\u2014except\nthat Stallman never saw it as a market. For him, software was and\nis part of the human itself, constitutive of our very freedom and,\nhence, inalienable. Extending software, through collective mutual\n308 conclusion aid, is thus tantamount to vitality, progress, and self-actualization.\nBut even for those who insist on seeing software as mere product,\nthe problem of extensibility remains. Standardization, standards\nprocesses, and market entry all appear as political problems as\nsoon as extensibility is denied\u2014and thus the legal solution rep-\nresented by copyleft appears as an option, even though it raises\nnew and troubling questions about the nature of competition and\nprofitability.\nNew questions about competition and profitability have emerged\nfrom the massive proliferation of hybrid commercial and academic\nforms, forms that bring with them different traditions of sharing,\ncredit, reputation, control, creation, and dissemination of knowl-\nedge and products that require it. The new economic demands\non the university\u2014all too easily labeled neoliberalization or\ncorporatization\u2014mirror changing demands on industry that it\ncome to look more like universities, that is, that it give away\nmore, circulate more, and cooperate more. The development of\nUNIX, in its details, is a symptom of these changes, and the suc-\ncess of Free Software is an unambiguous witness to them.\nThe proliferation of hybrid commercial-academic forms in an era\nof modifiability and reusability, among the debris of standards,\nstandards processes, and new experiments in intellectual property,\nresults in a playing field with a thousand different games, all of\nwhich revolve around renewed experimentation with coordination,\ncollaboration, adaptability, design, evolution, gaming, playing,\nworlds, and worlding. These games are indicative of the triumph\nof the American love of entrepreneurialism and experimentalism;\nthey relinquish the ideals of planning and hierarchy almost abso-\nlutely in favor of a kind of embedded, technically and legally com-\nplex anarchism. It is here that the idea of a public reemerges: the\nambivalence between relinquishing control absolutely and absolute\ndistrust of government by the few. A powerful public is a response,\nand a solution, so long as it remains fundamentally independent of\ncontrol by the few. Hence, a commitment, widespread and growing,\nto a recursive public, an attempt to maintain and extend the kinds\nof independent, authentic, autotelic public spheres that people en-\ncounter when they come to an understanding of how Free Software\nand the Internet have evolved.\nThe open-access movement, and examples like Connexions, are at-\ntempts at maintaining such publics. Some are conceived as bulwarks\nconclusion 309 against encroaching corporatization, while others see themselves\nas novel and innovative, but most share some of the practices\nhashed out in the evolution of Free Software and the Internet. In\nterms of scholarly publishing and open access, the movement has\nreignited discussions of ethics, norms, and method. The Mertonian\nideals are in place once more, this time less as facts of scientific\nmethod than as goals. The problem of stabilizing collective knowl-\nedge has moved from being an inherent feature of science to being\na problem that needs our attention. The reorientation of knowledge\nand power and the proliferation of hybrid commercial-academic\nentities in an era of massive dependence on scientific knowledge\nand information leads to a question about the stabilization of that\nknowledge.\nUnderstanding how Free Software works and how it has devel-\noped along with the Internet and certain practices of legal and\ncultural critique may be essential to understanding the reliable\nfoundation of knowledge production and circulation on which we\nstill seek to ground legitimate forms of governance. Without Free\nSoftware, the only response to the continuing forms of excess we\nassociate with illegitimate, unaccountable, unjust forms of gover-\nnance might just be mute cynicism. With it, we are in possession of\na range of practical tools, structured responses and clever ways of\nworking through our complexity toward the promises of a shared\nimagination of legitimate and just governance. There is no doubt\nroom for critique\u2014and many scholars will demand it\u2014but schol-\narly critique will have to learn how to sit, easily or uneasily, with\nFree Software as critique. Free Software can also exclude, just as any\npublic or public sphere can, but this is not, I think, cause for resis-\ntance, but cause for joining. The alternative would be to create no\nnew rules, no new practices, no new procedures\u2014that is, to have\nwhat we already have. Free Software does not belong to geeks, and\nit is not the only form of becoming public, but it is one that will\nhave a profound structuring effect on any forms that follow.\n310 conclusion "}