{"text":"CHAPTER 1: DEFINING A GEOGRAPHIC INFORMATION SYSTEM\nAlthough GIS has been around since the 1970s, the concepts surrounding GIS are old, and even the practice of\ndoing GIS began before computers. The difference today is that GIS is computerized. By computerizing GIS, we\nhave taken the processes away from our hand-drawn depictions, which tend to require extensive time, money,\ntraining, and energy. Computers process numbers and mathematical equations far quicker than people. Yet,\nbefore the concepts behind GIS were transferred to computers, people were doing manual GIS by combining\nspatial and attribute data on various types of media including hard-copy maps, hard-copy overlays (acetate or\nvellum), aerial photographs, written reports, field notebooks, and\u2014of course\u2014their eyes and minds.\nWith manual GIS, a large base map was often placed on a tabletop, and a series of transparent overlay maps,\ndrawn at the same scale, were placed on top of the base map. One would then look for relationships among the\nbase map and the features on the transparent overlays. Frequently, spatial data were copied from one map (or\naerial photograph) to another. This took time, and because of it, many great ideas about the relationships of the\nEarth\u2019s features (both physical and human) were not analyzed. These ideas were constrained by the amount of\ntime it took to do the analysis. Still, some impressive manual GIS projects did occur. The much-repeated exam-\nple of Dr. John Snow\u2019s Cholera map is a great example of manual GIS (Figure 1.1).\nFigure 1.1. Dr. John Snow\u2019s Cholera Map of London\u2019s Soho. Source: Wikipedia http:\/\/en.wikipedia.org\/wiki\/\nGeographic_information_system\nIn the 1840s, a cholera outbreak killed several hundred residents in London\u2019s Soho section. Snow, a physician,\nlocated the address of each fatality on a hand-drawn base map and soon a cluster of cases was visible. Then,\non the base map, over the streets and fatalities, he drew the locations of water wells. Familiar with the idea of\ndistance decay, he knew that people might go a far distance to purchase a product that was cheaper, but they\nwould go to the nearest well because water was free and heavy to carry. Snow could see that the fatalities clus-\ntered largely among those who lived near the Broad Street water well. He and his students took the handle off\nthe water pump, and new cholera cases dropped rapidly. By disabling the pump, Snow demonstrated the spatial\nrelationship between cholera fatalities and the Broad Street water well, and, more importantly, he established the\nrelationship between cholera and drinking water.\n2 Chapter 1: Defining a Geographic Information System\nEven with the advent of computers, GIS applications to several decades to transform to the personal computer\nthat we use today. Originally, the largest and most powerful computers were mainframes that were available to\nsome academics and government officials. In the 1980s, most GIS applications ran on workstation computers\ntied to mainframe computers because the early microcomputers (IBM, Apple, etc.) did not have enough mem-\nory, storage capacity, or processing ability. Today\u2019s personal computers, however, are fast, capable of storing and\nprocessing large datasets, and can process multiple tasks simultaneously. This enables many academics, govern-\nment agencies (from local to federal), organizations, and small and large businesses to use GIS. Computer-based\nGIS has its advantages, but requires trained users.\nGIS DATA MODELS\nIn order to visualize natural phenomena, one must first determine how to best represent geographic space. Data\nmodels are a set of rules and\/or constructs used to describe and represent aspects of the real world in a computer.\nTwo primary data models are available to complete this task: raster data models and vector data models.\nVECTOR DATA MODEL\nAn introductory GIS course often emphasizes the vector data model, since it is the more commonly used in the\nplanning professions. Vector data models use points and their associated [X and Y] coordinate pairs to represent\nthe vertices of spatial features, much as if they were being drawn on a map by hand (Aronoff 1989)1. The data\nattributes of these features are then stored in a separate database management system. The spatial information\nand the attribute information for these models are linked via a simple identification number that is given to each\nfeature on a map. Three fundamental vector types exist in GIS: points, lines, and polygons, each of which we\ndefine below (and illustrate in Figure 1.2):\n(cid:37)(cid:3) Points are zero-dimensional objects that contain only a single coordinate pair. Points are typically used\nto model singular, discrete features such as buildings, wells, power poles, sample locations, and so forth.\nPoints have only the property of location. Other types of point features include the node and the vertex.\nSpecifically, a point is a stand-alone feature, while a node is a topological junction representing a common\nX, Y coordinate pair between intersecting lines and\/or polygons. Vertices are defined as each bend along\na line or polygon feature that is not the intersection of lines or polygons. Points can be spatially linked to\nform more complex features.\n(cid:37)(cid:3) Lines are one-dimensional features composed of multiple, explicitly connected points. Lines are used to\nrepresent linear features such as roads, streams, faults, boundaries, and so forth. Lines have the property of\nlength. Lines that directly connect two nodes are sometimes referred to as chains, edges, segments, or arcs.\n(cid:37)(cid:3) Polygons are two-dimensional features created by multiple lines that loop back to create a \u201cclosed\u201d feature.\nIn the case of polygons, the first coordinate pair (point) on the first line segment is the same as the last\ncoordinate pair on the last line segment. Polygons are used to represent features such as city boundar-\nies, geologic formations, lakes, soil associations, vegetation communities, and so forth. Polygons have the\nproperties of area and perimeter. Polygons are also called areas.\n1 Aronoff, S. 1989. Geographic Information Systems: A Management Perspective. Ottawa, Canada: WDL Publications.\n3 Chapter 1: Defining a Geographic Information System\nFigure 1.2. A simple vector map, using each of the vector elements: points for wells, lines for rivers, and a poly-\ngon for the lake. Source: Wikipedia. http:\/\/en.wikipedia.org\/wiki\/GIS_file_formats\nRASTER DATA MODEL\nThe raster data model is widely used in applications ranging far beyond geographic information systems (GISs).\nMost likely, you are already very familiar with this data model if you have any experience with digital photographs.\nThe raster data model consists of rows and columns of equally sized pixels interconnected to form a planar surface.\nThese pixels are used as building blocks for creating points, lines, areas, networks, and surfaces. Although pixels\nmay be triangles, hexagons, or even octagons, square pixels represent the simplest geometric form with which to\nwork. Accordingly, the vast majority of available raster GIS data are built on the square pixel. The contrast between\nraster and vector model reflect the \u2018pixilization\u2019 of a raster, which would be points, lines and polygons in a vector\ndata model (Figure 1.3). The raster data model is a part of a later chapter.\nFigure 1.3. Visual depiction of the difference between a raster (left) and vector (right) data model. Source: GIS\nCommons. http:\/\/giscommons.org\/introduction-concepts\/\nVECTOR VS. RASTER\nWhich is better? Although GIS users have their own personal favorite data model, the question of which is \u201cbet-\nter\u201d is an incomplete question. There are advantages and disadvantages to both data models, so a better ques-\ntion is which is better for particular applications or datasets. Some in the GIS industry use the slogan \u201cRaster is\nfaster, but vector is corrector.\u201d While this is a good starting point, it conceals the details. Yes, your computer can\nprocess raster data quicker, but today computer processors are so fast the difference may be negligible. Yes, vec-\n4 Chapter 1: Defining a Geographic Information System\ntor output looks more accurate, but you can increase pixel resolution to something resembling vector resolution\n(this, however, greatly increases the database size). In the following we try to list the advantage and disadvantag-\nes of vector and raster file.\nVECTOR ADVANTAGES:\n1. Intuitive. In our minds, we picture features discreetly rather than made up of contiguous square cells.\n2. Resolution. If the locations of features are precise and accurate, you can maintain that spatial accuracy. The\nfeatures will not float somewhere within a cell.\n3. Topology. Although the raster data model preserves where features are located in relation to one another,\nthey do not represent how they are related to one another. This complex form of topology can be con-\nstructed in most vector systems, so you can track the connections in a municipal water network between\npipe and valve features and thus track the direction and flow of water.\n4. Storage. Vector points, lines, and simple polygons use little disk space in comparison to raster systems. This\nwas once a major consideration when hard-disk storage was limited and expensive.\nVECTOR DISADVANTAGES:\n1. Geometry is complex. The geometrical algorithms needed for geoprocessing, for example polygon overlay\nand the calculation of distances, depending on the projection\/coordinate system used, require experienced\nprogrammers. This is not usually a problem for most GIS users since most functions are directly coded in\nthe software.\n2. Slow response times. The vector data model can be slow to process complex datasets especially on low-end\ncomputers.\n3. Less innovation. Since the math is more complex, new analysis functions may not surface on vector sys-\ntems for a couple of years after they have debuted on raster system.\nRASTER ADVANTAGES:\n1. Easy to understand. Conceptually, the raster data model is easy to understand. It arranges data into col-\numns and rows. Each pixel represents a piece of territory.\n2. Processing speed. Raster\u2019s simple data structure and its uncomplicated math produce quick results. For\nexample, to calculate a polygon\u2019s area, the computer takes the area contained within a single cell (which\nremains consistent throughout the layer) and multiples it by the number of cells making up the poly-\ngon. Likewise, the speed of many analysis processes, like overlay and buffering, are faster than vector\nsystems that must use geometric equations.\n3. Data form. Remote sensing imagery is easily handled by raster-based systems because the imagery is pro-\nvided in a raster format.\n4. Some analysis functions (surface analysis and neighborhood functions) are only feasible in raster sys-\ntems. In addition, many new analysis functions appear in raster systems before migrating to vector sys-\ntems because the math is simpler.\n5 Chapter 1: Defining a Geographic Information System\nRASTER DISADVANTAGES:\n1. Appearance. Cells \u201cseem\u201d to sacrifice too much detail (Figure 1.9). This disadvantage is largely aesthetic\nand can be remedied by increasing the layer\u2019s resolution.\n2. Accuracy. Sometimes accuracy is a problem due to the pixel resolution. Imagine if you had a raster layer\nwith a 30 by 30 meter resolution, and you wanted to locate traffic stop signs in that layer. The entire 30 by\n30 meter pixel would represent the single stop sign. If you converted this raster layer to vector, it might\nplace the stop sign at what was the pixel\u2019s center. Sometimes problems of accuracy (and appearance) can\nbe resolved by selecting a smaller pixel resolution, but this has database consequences.\n3. Large database. As just described, accuracy and appearance can be enhanced by reducing pixel size (the\narea of the Earth\u2019s surface covered by each cell), but this increases your layer\u2019s file size. By making the res-\nolution 50 percent better (say from 30 to 15 meters), your layer grows four times. Improve the resolution\nagain by halving the pixel size (to 7.5 meters) and your layer will again increase by four times (16 times\nlarger than the original 30-meter layer). The layer quadruples because the resolution increases in both the\nx and y direction.\nFigure 1.4. Visual depiction of overlay analysis. Source: ESRI. http:\/\/www.esri.com\/news\/arcnews\/fall04articles\/\narcgis-raster-data-model.html\nMORE ON VECTOR DATA MODELS\nThe real world is too complex and unmanageable for direct analysis and understanding because of its countless\nvariability and diversity. It would be an impossible task to describe and locate each city, building, tree, blade of\n6 Chapter 1: Defining a Geographic Information System\ngrass, and grain of sand. How do we reduce the complexity of the Earth and its inhabitants, so we can portray\nthem in a GIS database and on a map? We do it by selecting the most relevant features (ignoring those we do not\nthink are necessary for our specific research or project) and then generalizing the features we have selected. The\nimage above shows the real world is selectively represented by different features that we are interested in. They are\nalso called map layers in GIS.\nFEATURES AND FEATURE CLASS\nIn ArcGIS, map layers are also called shape files, or feature classes. Conceptually, there are two parts of a shape\nfile: a spatial or map component and an attribute or database component. Features have these two components\nas well. They are represented spatially on the map and their attributes, describing the features, are found in a data\nfile. These two parts are linked. In other words, each map feature is linked to a record in a data file that describes\nthe feature. If you delete the feature\u2019s attributes in the data file, the feature disappears on the map. Conversely, if\nyou delete the feature from the map, its attributes will disappear too.\nFigure 1.5. Spatial and attribute data. GIS Commons. http:\/\/giscommons.org\/introduction-concepts\/\nFeatures are individual objects and events that are located (present, past or future) in space. In the above Fig-\nure, a single parcel is an example of a feature. Within the GIS industry, features have many synonyms including\nobjects, events, activities, forms, observations, entities, and facilities. Combined with other features of the same\ntype (like all of the parcels in Figure), they are arranged in data files often called layers, coverages, or themes.\nIn the Figure below, three features\u2014parcels, buildings, and street centerlines\u2014of a typical city block are visi-\nble. Every feature has a spatial location and a set of attributes. Its spatial location describes not only its location\nbut its extent.\n7 Chapter 1: Defining a Geographic Information System\nFigure 1.6. Each feature in the layers above has a spatial location and attribute data, which describes the indi-\nvidual feature. GIS Commons. http:\/\/giscommons.org\/introduction-concepts\/\nBesides location, each feature usually has a set of descriptive attributes, which characterize the individual fea-\nture. Each attribute takes the form of numbers or text (characters), and these values can be qualitative (i.e. low,\nmedium, or high income) or quantitative (actual measurements). Sometimes, features may also have a temporal\ndimension; a period in which the feature\u2019s spatial or attribute data may change. As an example of a feature class,\nthink of a streetlight. Now imagine a map with the locations of all the streetlights in your neighborhood. In\nFigure 1.5, streetlights most are depicted as small circles. Now think of all of the different characteristics that you\ncould collect relating to each streetlight. It could be a long list. Streetlight attributes could include height, mate-\nrial, basement material, presence of a light globe, globe material, color of pole, style, wattage and lumens of bulb,\nbulb type, bulb color, date of installation, maintenance report, and many others. The necessary streetlight attri-\nbutes depends on how you intend to use them. For example, if you are solely interested in knowing the location\nof streetlights for personal safety reasons, you need to know location, pole heights, and bulb strength. On the\nother hand, if you are interested in historic preservation, you are concerned with the streetlight\u2019s location, style,\nand color.\nNow continue thinking about feature attributes, by imagining the trees planted around your campus or of-\nfice. What attributes would a gardener want versus a botanist? There would be differences because they have\ndifferent needs. You determine your study\u2019s features and the attributes that define the features.\nATTRIBUTE DATA TABLE\nOnce you have decided on the features and their attributes, determine how they will be coded in the GIS data-\nbase. There are multiple ways to code features in different scale and circumstance. For example, schools can be\ncoded as a point in large scale maps, and a polygon of their campus in small scale maps. You can decide whether\nto code each feature type as a point, line, or polygon. Together you also need to define the format and storage\nrequirements for each of the feature\u2019s attributes.\nWhile thinking about your attribute values, consider where it fits in the \u201clevels of measurement\u201d scale with its\nfour different data values: nominal, ordinal, interval, and ratio. Stanley S. Stevens, an American psychologist,\ndeveloped these categories in 1946. For our purposes, these categories are useful way to conceptualize how data\nvalues differ, and it is an important reminder that only some types of variables can be used for certain mathe-\nmatical operations and statistical tests, including many GIS functions. The different \u201clevels\u201d are depicted in the\n8 Chapter 1: Defining a Geographic Information System\nfollowing table and demonstrated using an example of a marathon race:\nNominal Ordinal Interval Ratio\nRunner ID Order finished Time of day finished Total race time\n238 1 10:10am 2:30\n143 2 10:11am 2:31\n14 3 10:13am 2:33\n. . . .\n. . . .\n. . . .\n. . . .\n301 450 18:10pm 10:30\nFigure 1.7. Levels of Measurement. Adopted from GIS Commons. http:\/\/giscommons.org\/chapter-2-input\/\nNominal data use characters or numbers to establish identity or categories within a series. In a marathon race,\nthe numbers pinned to the runners\u2019 jerseys are nominal numbers (first column in the figure above). They iden-\ntify runners, but the numbers do not indicate the order or even a predicted race outcome. Besides races, tele-\nphone numbers are a good example. It signifies the unique identity of a telephone. The phone number 961-8224\nis not more than 961-8049. Place names (and those of people) are nominal too. You may prefer the sound of\none name, but they serve only to distinguish themselves from each other. Nominal characters and numbers do\nnot suggest a rank order or relative value; they identify and categorize. Nominal data are usually coded as char-\nacter (string) data in a GIS database.\nAlthough census data originate as individual counts, much of what is counted is individuals\u2019 membership in\nnominal categories. Race, ethnicity, marital status, mode of transportation to work (car, bus, subway, railroad...),\nand type of heating fuel (gas, fuel oil, coal, electricity...) are measured as numbers of observations assigned to\nunranked categories. Using nominal data we can use the Census Bureau\u2019s first atlas to depict the minority groups\nwith the largest percentage of population in each U.S. state (Figure 1.8). Colors were chosen to differentiate the\ngroups through a qualitative color scheme to show differences between the classes, but not to imply any quanti-\ntative ordering. Thus, although numerical data were used to determine which category each state is in, the map\ndepicts the resulting nominal categories rather than the underlying numerical data.\nFigure 1.8. Highest percent minority group by state. Source: US Census (2000).\n9 Chapter 1: Defining a Geographic Information System\nOrdinal datasets establish rank order. In the race, the order they finished (i.e. 1st, 2nd, and 3rd place) are mea-\nsured on an ordinal scale (second column in Figure 2.5). While order is known, how much better one runner is\nthan the other is not. The ranks \u2018high\u2019, \u2018medium\u2019, and \u2018low\u2019 are also ordinal. So while we know the rank order, we\ndo not know the interval. Usually both numeric and character ordinal data are coded with characters because\nordinal data cannot be added, subtracted, multiplied, or divided in a meaningful way. The middle value, the\n\u201cmedian\u201d, in a string of ordinal values, however, is a good substitute for a mean (average) value.\nExamples of ordinal data often seen on reference maps include political boundaries that are classified hierarchi-\ncally (national, state, county, etc.) and transportation routes (primary highway, secondary highway, light-duty\nroad, unimproved road). Ordinal data measured by the Census Bureau include how well individuals speak En-\nglish (very well, well, not well, not at all), and level of educational attainment (high school graduate, some college\nno degree, etc.). Social surveys of preferences and perceptions are also usually scaled ordinally.\nIndividual observations measured at the ordinal level are not numerical, thus should not be added, subtracted,\nmultiplied, or divided. For example, suppose two 600-acre grid cells within your county are being evaluated\nas potential sites for a hazardous waste dump. Say the two areas are evaluated on three suitability criteria, each\nranked on a 0 to 3 ordinal scale, such that 0 = completely unsuitable, 1 = marginally unsuitable, 2 = marginally\nsuitable, and 3 = suitable. Now say Area A is ranked 0, 3, and 3 on the three criteria, while Area B is ranked 2, 2,\nand 2. If the Siting Commission was to simply add the three criteria, the two areas would seem equally suitable\n(0 + 3 + 3 = 6 = 2 + 2 + 2), even though a ranking of 0 on one criteria ought to disqualify Area A.\nThe Interval scale, like we will discuss with ratio data, pertains only to numbers; there is no use of character\ndata. With interval data the difference\u2014the \u201cinterval\u201d\u2014between numbers is meaningful. Interval data, unlike\nratio data, however, do not have a starting point at a true zero. Thus, while interval numbers can be added and\nsubtracted, division and multiplication do not make mathematical sense. In the marathon race, the time of the\nday each runner finished is measured on an interval scale. If the runners finished at 10:10 a.m., 10:20 a.m. and\n10:25 a.m., then the first runner finished 10 minutes before the second runner and the difference between the\nfirst two runners is twice that of the difference between the second and third place runners (see third column 3\nFigure 2.5). The runner finishing at 10:10 a.m., however, did not finish twice as fast as the runner finishing at\n20:20 (8:20 p.m.) did. A good non-race example is temperature. It makes sense to say that 20\u00b0 C is 10\u00b0 warmer\nthan 10\u00b0 C. Celsius temperatures (like Fahrenheit) are measured as interval data, but 20\u00b0 C is not twice as warm\nas 10\u00b0 C because 0\u00b0 C is not the lack of temperature, it is an arbitrary point that conveys when water freezes. Re-\nturning to phone numbers, it does not make sense to say that 968-0244 is 62195 more than 961-8049, so they are\nnot interval values.\nRatio is similar to interval. The difference is that ratio values have an absolute or natural zero point. In our race,\nthe first place runner finished in a time of 2 hours and 30 minutes, the second place runner in a time of 2 hours\nand 40 minutes, and the 450th place runner took 10 hours. The 450th place finisher took over five times longer\nthan the first place runner did. With ratio data, it makes sense to say that a 100 lb woman weighs half as much as\na 200 lb man, so weight in pounds is ratio. The zero point of weight is absolute. Addition, subtraction, multipli-\ncation, and division of ratio values make statistical sense.\nThe main reason that it\u2019s important to recognize levels of measurement is that different analytical operations are\npossible with data at different levels of measurement (Chrisman 2002). Some of the most common operations\ninclude:\n(cid:37)(cid:3) Group: Categories of nominal and ordinal data can be grouped into fewer categories. For instance, group-\ning can be used to reduce the number of land use\/land cover classes from, for instance, four (residential,\ncommercial, industrial, parks) to one (urban).\n10 Chapter 1: Defining a Geographic Information System\n(cid:37)(cid:3) Isolate: One or more categories of nominal, ordinal, interval, or ratio data can be selected, and others set\naside. For example, consider a range of temperature readings taken over a large area. Only a subset of those\ntemperatures are suitable for mosquito survival, and health officials can select and isolate areas based upon\na specific temperature range that is likely there to take action in order to reduce the threat of a West Nile\nVirus or Dengue Fever outbreak from these mosquitoes.\n(cid:37)(cid:3) Difference: The difference of two interval level observations (such as two calendar years) can result in one\nratio level observation (such as one age). For example, in 2012 (a year is an interval level value), someone\nborn in 2000 (also interval level, of course) is 12 years old (age is ratio level, since it has a definite zero).\n(cid:37)(cid:3) Other arithmetic operations: Two or more compatible sets of interval or ratio level data can be added or\nsubtracted. Only ratio level data can be multiplied or divided. For example, the per capita (average) income\nof an area can be calculated by dividing the sum of the income (ratio level) of every individual in that area\n(ratio level), by the number of persons (ratio level) residing in that area (a second ratio level variable).\n(cid:37)(cid:3) Classification: Numerical data (at interval and ratio level) can be sorted into classes, typically defined as\nnon-overlapping numerical data ranges. These classes are frequently treated as ordinal level categories for\nthematic mapping with the symbolization on choropleth maps, for example, emphasizing rank order with-\nout attempting to represent the actual magnitudes.\nThis chapter text has been compiled from the following web links that holds information with CC copyrights:\nuse and share alike.\nhttp:\/\/giscommons.org\/introduction-concepts\/\nhttp:\/\/giscommons.org\/chapter-2-input\/\nhttp:\/\/2012books.lardbucket.org\/books\/geographic-information-system-basics\/s08-02-vector-data-models.html#\nhttps:\/\/www.e-education.psu.edu\/geog160\/c3_p8.html\nDiscussion Questions\n1. In what ways would John Snow\u2019s mapping process differ given the GIS technologies available today? How\nwould his results be different or the same?\n2. How do the differences between discrete and continuous attribute data impact the selection of using vector\nand\/or raster data models?\n3. Find an internet source that contains an interesting map that visualizes data from two of the different attri-\nbute measurement scales: nominal, ordinal, interval, and ratio. How do the different measurement scales\nreflect the type of data provided?\nContextual Applications of Chapter 1\nAll Cities Are Not Created Unequal (Brookings)\nAmerican Migration\n11 CHAPTER 2: COORDINATE SYSTEMS AND PROJECTING GIS DATA\nA coordinate system is a way to reference, or locate, everything on the Earth\u2019s surface in x and y space. The meth-\nod used to portray a part of the spherical Earth on a flat surface, whether a paper map or a computer screen, is\ncalled a map projection. Each map projection used on a paper map or in a GIS is associated with a coordinate\nsystem. To simplify the use of maps and to avoid pinpointing locations on curved latitude-longitude reference\nlines, cartographers superimpose a rectangular grid on maps. Such grids use coordinate systems to determine the\nx and y position of any spot on the map. Coordinate systems are often identified by the name of the particular\nprojection for which they are designed. Because no single map projection is suitable for all purposes, many dif-\nferent coordinate systems have been developed. Some are worldwide or nearly so, while others cover individual\ncountries (such as the United Kingdom\u2019s Ordnance Survey\u2019s coordinate system), and others cover states or parts\nof states in the U.S.\nThis chapter begins with concepts that define the geographical referencing standards of the Earth. Topics include\nlatitude and longitude, projections, coordinate systems, and datums.\nGEOGRAPHIC COORDINATE SYSTEM- LATITUDE AND LONGITUDE\nAny feature can be referenced by its latitude and longitude, which are angles measured in degrees from the\nEarth\u2019s center to a point on the Earth\u2019s surface (see Figure 2.1). Across the spherical Earth, latitude lines stretch\nhorizontally from east to west (left image in Figure 2.2), and they are parallel to each other, hence their alter-\nnative name, parallels. Longitude lines, also called meridians, stand vertically and stretch from the North Pole\nto the South Pole (center image in Figure 2.2). Together these \u201cnorth to south\u201d and \u201ceast to west\u201d lines meet at\nperpendicular angles to form a graticule, a grid that encompasses the Earth (right image in Figure 2.2).\nLatitude can be thought of as the lines that intersect the y-axis, and longitude as lines that intersect the x-axis.\nThink of the equator as the x axis; the y axis is the prime meridian, which is a line running from pole to pole\nthrough Greenwich, England. Just as the upper right quarter in the Cartesian coordinate system is positive\nfor both x and y, latitude and longitude east of the prime meridian and north of the equator are both positive.\nEurope, Asia, and part of Africa \u2013 which have positive latitudes and longitudes \u2013 correspond to the upper right\nquarter of the Cartesian coordinate system. With the exception of some U.S. territories in the Pacific and the\nwesternmost Aleutian islands, all of the United States is north of the equator and west of the prime meridian, so\nall latitudes in the U.S are positive (or north) while almost all longitudes are negative (or west).\nFigure 2.1: Latitude and longitude are angles measured in degrees from the Earth\u2019s center to a point on the\nEarth\u2019s surface. GIS Commons. http:\/\/giscommons.org\/earth-and-map-preprocessing\/\n12 Chapter 2: Coordinate Systems and Projecting GIS Data\nFigure 2.2: Latitude, longitude, and the Earth\u2019s graticule. GIS Commons. http:\/\/giscommons.org\/\nearth-and-map-preprocessing\/\nMidway between the poles, the equator stretches around the Earth, and it defines the line of zero degrees latitude\n(left image in Figure 2.2). Relative to the equator, latitude is measured from 90 degrees at the North Pole to -90\ndegrees at the South Pole. The Prime Meridian is the line of zero degrees longitude (center image in Figure 2.2),\nand in most coordinate systems, it passes through Greenwich, England. Longitude runs from -180 degrees west\nof the Prime Meridian to 180 degrees east of the same meridian. Because the globe is 360 degrees in circumfer-\nence, -180 and 180 degrees is the same location.\nPROJECTION - TRANSFORMATION OF GEOGRAPHICAL COORDINATES TO CAR-\nTESIAN COORDINATE SYSTEMS\nWhile the system of latitude and longitude provides a consistent referencing system for anywhere on the earth, in\norder to portray our information on maps or for making calculations, we need to transform these angular mea-\nsures to Cartesian coordinates. These transformations amount to a mapping of geometric relationships expressed\non the shell of a globe to a flatten-able surface -- a mathematical problem that is figuratively referred to as Projec-\ntion.\nGlobes do not need projections, and even though they are the best way to depict the Earth\u2019s shape and to un-\nderstand latitude and longitude, they are not practical for most applications that require maps. We need flat\nmaps. This requires a reshaping of the Earth\u2019s 3-dimensions into a 2-dimensional surface.\nTo illustrate the concept of a map projection, imagine that we place a light bulb in the center of a translucent\nglobe (Figure 2.3). On the globe are outlines of the continents and the lines of longitude and latitude called the\ngraticule. When we turn the light bulb on, the outline of the continents and the graticule will be \u201cprojected\u201d as\nshadows on the wall, ceiling, or any other nearby surface. This is what is meant by map \u201cprojection.\u201d The term\n\u201cprojection\u201d implies that the ball-shaped net of parallels and meridians is transformed by casting its shadow\nupon some flat, or flatten-able, surface. In fact, almost all map projection methods are mathematical equations.\n13 Chapter 2: Coordinate Systems and Projecting GIS Data\nFigure 2.3. The concept of Map Projection as illustrated using a spherical globe and a flat map. http:\/\/\n2012books.lardbucket.org\/books\/geographic-information-system-basics\/s06-02-map-scale-coordinate-sys-\ntems-a.html\nWithin the realm of maps and mapping, there are three surfaces used for map projections (i.e., surfaces on which\nwe project the shadows of the graticule). These surfaces are the plane, the cylinder, and the cone (Figure 2.4).\nFigure 2.4. Three types of \u201cflattenable\u201d surfaces to which the graticule can be projected: a plane, a cone, and a\ncylinder. https:\/\/www.e-education.psu.edu\/geog482fall2\/c2_p30.html\nAs you might imagine, the appearance of the projected grid will change quite a lot depending on the type of sur-\nface it is projected onto, and how that surface is aligned with the globe. The three surfaces shown above in Figure\n2.4 -- the disk-shaped plane, the cone, and the cylinder--represent categories that account for the majority of\nprojection equations that are encoded in GIS software. The plane often is centered upon a pole. The cone is typi-\ncally aligned with the globe such that its line of contact (tangency) coincides with a parallel in the mid-latitudes.\nAnd the cylinder is frequently positioned tangent to the equator (Figure 2.5).\n14 Chapter 2: Coordinate Systems and Projecting GIS Data\nFigure 2.5. The projected graticules produced by projection equations in each category \u2013 plane, cone, and\ncylinder. http:\/\/2012books.lardbucket.org\/books\/geographic-information-system-basics\/s06-02-map-scale-co-\nordinate-systems-a.html\nReferring again to the previous example of a light bulb in the center of a globe, note that during the projection\nprocess, we can situate each surface in any number of ways. For example, surfaces can be tangential to the globe\nalong the equator or poles, they can pass through or intersect the surface, and they can be oriented at any num-\nber of angles. The following figures shows how these projections can vary.\nPROJECTION AND DISTORTION\nFlattening the globe cannot be done without introducing some error, and some distortion is unavoidable. Any\nprojection has its area of least distortion. Projections can be shifted around in order to put this area of least dis-\ntortion over the topographer\u2019s area of interest. Thus any projection can have an unlimited number of variations\nor cases that determined by standard parallels or meridians that adjust the location of the high-accuracy part of\nthe projection.\nIf the geographic extent of your project area was small, like a neighborhood or a portion of a city, you could\nassume that the Earth is flat and use no projection. This is referred to as a planar surface or even a planar \u201cprojec-\ntion,\u201d but with the understanding that it does not use a projection. Planar representation does not significantly\naffect a map\u2019s accuracy when scales are larger than 1:10,000. In other words, small areas do not need a projection\nbecause the statistical differences between locations on a flat plane and a 3-dimensional surface are not signifi-\ncant.\nFor small-scale maps one must consider the Earth\u2019s shape. Our assumption that the Earth is round or spheri-\ncal does not accurately represent it. The Earth\u2019s constant spinning causes it to bulge slightly along the equator,\nruining its perfect spherical shape. The slightly oval nature of the Earth\u2019s geometric surface makes the terms ellip-\nsoid and spheroid more accurate in describing its shape, but they are not perfect terms either since differences\nin material weights (for instance iron is denser than sedimentary deposits) and the movement of tectonic plates\nmakes the Earth dynamic and constantly changing. The Earth is a geoid with a slight pear shape; it is a little larg-\ner in the southern hemisphere and includes other bulges. The difference, however, between the ellipsoid and the\ngeoid is minor enough that it does not affect most mapping. Until recently, projections based on geoids were rare\nbecause of the complexity and cost of collecting the necessary data to create the projection, but satellite imagery\n15 Chapter 2: Coordinate Systems and Projecting GIS Data\nhas helped with measurement and geoid projections are now more common.\nProjections are abstractions, and they introduce distortions to either the Earth\u2019s shape, area, distance, or direc-\ntion (and sometimes to all of these properties). Different map projections cause different map distortions.\nOne way to classify map projections is to describe them by the characteristic they do not distort. Usually only\none property is preserved in a projection. Map projections classified based on the preserved properties include:\n(cid:37)(cid:3) Conformal - Preserves: Shape, Distorts: Area\n(cid:37)(cid:3) Equal Area - Preserves: Area; Distorts: Shape, Scale or Angle (bearing)\n(cid:37)(cid:3) Equidistant - Preserves: Distances between certain points (but not all points); Distorts: Other distances\n(cid:37)(cid:3) Azimuthal (True Direction) - Preserves: Angles (bearings); Distorts: Area and shape\nMap projections that accurately represent distances are referred to as equidistant projections. Note that distances\nare only correct in one direction, usually running north\u2013south, and are not correct everywhere across the map.\nEquidistant maps are frequently used for small-scale maps that cover large areas because they do a good job of\npreserving the shape of geographic features such as continent.\nMaps that represent angles between locations, also referred to as bearings, are called conformal. Conformal map\nprojections are used for navigational purposes due to the importance of maintaining a bearing or heading when\ntraveling great distances. The cost of preserving bearings is that areas tend to be quite distorted in conformal\nmap projections. Though shapes are more or less preserved over small areas, at small scales areas become wildly\ndistorted. The Mercator projection is an example of a conformal projection and is famous for distorting Green-\nland.\nAs the name indicates, equal area or equivalent projections preserve the quality of area. Such projections are of\nparticular use when accurate measures or comparisons of geographical distributions are necessary (e.g., defor-\nestation, wetlands). In an effort to maintain true proportions in the surface of the earth, features sometimes be-\ncome compressed or stretched depending on the orientation of the projection. Moreover, such projections distort\ndistances as well as angular relationships.\nAs noted earlier, there are theoretically an infinite number of map projections to choose from. One of the key\nconsiderations behind the choice of map projection is to reduce the amount of distortion. The geographical\nobject being mapped and the respective scale at which the map will be constructed are also important factors to\nthink about. For instance, maps of the North and South Poles usually use planar or azimuthal projections, and\nconical projections are best suited for the middle latitude areas of the earth. Features that stretch east\u2013west, such\nas the country of Russia, are represented well with the standard cylindrical projection, while countries oriented\nnorth\u2013south (e.g., Chile, Norway) are better represented using a transverse projection.\nIf a map projection is unknown, sometimes it can be identified by working backward and examining closely\nthe nature and orientation of the graticule (i.e., grid of latitude and longitude), as well as the varying degrees of\ndistortion. Clearly, there are trade-offs made with regard to distortion on every map. There are no hard-and-fast\nrules as to which distortions are more preferred over others. Therefore, the selection of map projection largely\ndepends on the purpose of the map.\nWithin the scope of GISs, knowing and understanding map projections are critical. For instance, in order to per-\nform an overlay analysis, all map layers need to be in the same projection. If they are not, geographical features\nwill not be aligned properly, and any analyses performed will be inaccurate and incorrect. If you want to con-\n16 Chapter 2: Coordinate Systems and Projecting GIS Data\nduct a measurement of land parcel size, you need to use a projection that does not distort area space. Most GISs\ninclude functions to assist in the identification of map projections, as well as to transform between projections in\norder to synchronize spatial data. Despite the capabilities of technology, an awareness of the potential and pitfalls\nthat surround map projections is essential.\nON-THE-FLY PROJECTION\nCreating map projections was extremely challenging, even just 30 years ago. And now we can project and un-\nproject massive quantities of coordinates, transforming them backward and forward from Latitude and Longi-\ntude (assuming this or that earth model) to overlay precisely with data that are stored in some other coordinate\nspace. It is truly amazing that humans have perfected a rich library of open-source software that can Forward\nProject geographic coordinates (latitude and longitude, + earth model) to any projected system; and also back-\nward project) from any well described projected coordinates back to geographic coordinates -- all in the wink of\nan eye. We can be thankful for that. But there are still some details that we have to understand.\nAutomatic transformation of coordinate systems requires that datasets include machine-readable metadata. In\nabout 2002, the makers of ArcMap added one more file to the schema of a shape file. The .prj file contains the\ndescription of the projection of a shape file, and if it exists, it is always copied with the shape file or dlements\nthat are exported from it. This is the machine-readable metadata that allows ArcMap to know how to handle\nthe dataset if any transformation (reprojection) is required. There are plenty of datasets that do not include such\nmachine readable metadata. This includes data that are not created with ArcMap since 2002 and even some that\nare. So we should get used to understanding map projections and their properties. If you need to learn to set the\ncoordinate system for a dataset, use ArcCatalog - as explained in the The ArcMap Projections Tutorial.\nUNIVERSAL TRANSVERSE MERCATOR\nThe first coordinate system we want to introduce here is the Universal Transverse Mercator grid, commonly\nreferred to as UTM and based on the Transverse Mercator projection. Universal Transverse Mercator (UTM) is\na coordinate system that largely covers the globe. The system reaches from 84 degrees north to 84 degrees south\nlatitude, and it divides the Earth into 60 north-south oriented zones that are 6 degrees of longitude wide (Figure\n2.6). Each individual zone uses a defined transverse Mercator projection (See Figure). The UTM system is not a\nsingle map projection. The system instead has 60 projections, and each uses a secant transverse Mercator projec-\ntion in each zone.\nThe contiguous U.S. consists of 10 zones (Figure 2.7). In the Northern hemisphere, the equator is the zero base-\nline for Northings (Southern hemisphere uses a 10,000 km false Northing). Each zone has an arbitrary central\nmeridian of 500 km west of each zone\u2019s central meridian (called a false Easting) to insure positive Easting values\nand a central bisecting meridian. In UTM, the CSUS Geography Department is located at 4,269,000 meters\nnorth; 637,200 meters east; zone 10, northern hemisphere. UTM zones are numbered consecutively beginning\nwith Zone 1. Zone 1 covers 180 degrees west longitude to 174 degrees west longitude (6 degrees of longitude),\nand includes the westernmost point of Alaska. Maine falls within Zone 16 because it lies between 84 degrees west\nand 90 degrees west. In each zone, coordinates are measured as northings and eastings in meters. The northing\nvalues are measured from zero at the equator in a northerly direction (in the southern hemisphere, the equator\nis assigned a false northing value of 10,000,000 meters). The central meridian in each zone is assigned an east-\ning value of 500,000 meters. In Zone 16, the central meridian is 87 degrees west. One meter east of that central\nmeridian is 500,001 meters easting.\n17 Chapter 2: Coordinate Systems and Projecting GIS Data\nFigure 2.6. Visual depiction of the Universal Transverse Mercator project system. http:\/\/en.wikipedia.org\/wiki\/\nUniversal_Transverse_Mercator_coordinate_system\nFigure 2.7. The zones of the Universal Transverse Mercator system as displayed over the United States. http:\/\/\nen.wikipedia.org\/wiki\/Universal_Transverse_Mercator_coordinate_system\nSTATE PLANE COORDINATE SYSTEM\nA second coordinate system is the State Plane Coordinate System. This system is actually a series of separate sys-\ntems, each covering a state, or a part of a state, and is only used in the United States. It is popular with some state\nand local governments due to its high accuracy, achieved through the use of relatively small zones. State Plane\nbegan in 1933 with the North Carolina Coordinate System and in less than a year it had been copied in all of the\nother states. The system is designed to have a maximum linear error of 1 in 10,000 and is four times as accurate\nas the UTM system.\nLike the UTM system, the State Plane system is based on zones. However, the 120 State Plane zones generally\nfollow county boundaries (except in Alaska). Given the State Plane system\u2019s desired level of accuracy, larger states\nare divided into multiple zones, such as the \u201cColorado North Zone.\u201d States with a long north-south axis (such as\nIdaho and Illinois) are mapped using a Transverse Mercator projection, while states with a long east-west axis\n(such as Washington and Pennsylvania) are mapped using a Lambert Conformal projection. In either case, the\nprojection\u2019s central meridian is generally run down the approximate center of the zone.\n18 Chapter 2: Coordinate Systems and Projecting GIS Data\nA Cartesian coordinate system is created for each zone by establishing an origin some distance (usually 2,000,000\nfeet) to the west of the zone\u2019s central meridian and some distance to the south of the zone\u2019s southernmost point.\nThis ensures that all coordinates within the zone will be positive. The X-axis running through this origin runs\neast-west, and the Y-axis runs north-south. Distances from the origin are generally measured in feet, but some-\ntimes are in meters. X distances are typically called eastings (because they measure distances east of the origin)\nand Y distances are typically called northings (because they measure distances north of the origin).\nFigure 2.8. Visual depiction of the State Plane project system as displayed over the United States. http:\/\/gis.\ndepaul.edu\/shwang\/teaching\/geog258\/Grid_files\/image002.jpg\nDATUMS\nAll coordinate systems are tied to a datum. A datum defines the starting point from which coordinates are mea-\nsured. Latitude and longitude coordinates, for example, are determined by their distance from the equator and\nthe prime meridian that runs through Greenwich, England. But where exactly is the equator? And where exactly\nis the Prime Meridian? And how does the irregular shape of the Earth figure into our measurements? All of these\nissues are defined by the datum.\nMany different datums exist, but in the United States only three datums are commonly used. The North Ameri-\ncan Datum of 1927 (NAD27) uses a starting point at a base station in Meades Ranch, Kansas and the Clarke El-\nlipsoid to calculate the shape of the Earth. Thanks to the advent of satellites, a better model later became available\nand resulted in the development of the North American Datum of 1983 (NAD83). Depending on one\u2019s location,\ncoordinates obtained using NAD83 could be hundreds of meters away from coordinates obtained using NAD27.\nA third datum, the World Geodetic System of 1984 (WGS84) is identical to NAD83 for most practical purposes\nwithin the United States. The differences are only important when an extremely high degree of precision is need-\ned. WGS84 is the default datum setting for almost all GPS devices. But most USGS topographic maps published\nup to 2009 use NAD27.\nThis chapter material has been collected from the following web links that holds information with CC copy-\nrights: use and share alike.\nhttp:\/\/giscommons.org\/earth-and-map-preprocessing\/\n19 Chapter 2: Coordinate Systems and Projecting GIS Data\nhttp:\/\/2012books.lardbucket.org\/books\/geographic-information-system-basics\/s06-02-map-scale-coordinate-\nsystems-a.html\nhttps:\/\/www.e-education.psu.edu\/geog482fall2\/c2_p30.html\nhttp:\/\/www.gsd.harvard.edu\/gis\/manual\/projection_fundamentals\/\nhttp:\/\/gis.depaul.edu\/shwang\/teaching\/geog258\/Grid.htm\nhttp:\/\/en.wikipedia.org\/wiki\/Universal_Transverse_Mercator_coordinate_system\nhttp:\/\/resources.arcgis.com\/en\/help\/main\/10.1\/index.html#\/\/003r0000000r000000\nDiscussion Questions\n1. Describe the general properties of the following projections: Universe Transverse Mercator (UTM), State\nplane system.\n2. Discuss the following concepts: Geographic Coordinate System, Projected coordinate system, Datums.\nContextual Applications of Chapter 2\nAn Anti-Poverty Policy that Works for Working Families (Brookings)\nU.S. Women Are Dying Younger Than Their Mothers, and No One Knows Why\n20 CHAPTER 3: TOPOLOGY AND CREATING DATA\nGEOMETRIC PRIMITIVES\nTopology is the subfield of mathematics that deals with the relationship between geometric entities, specifically\nwith properties of objects that are preserved under continuous deformation. A GIS topology is a set of rules and\nbehaviors that model how points, lines, and polygons share coincident geometry. The concepts of topology are\nvery useful for geographers, surveyors, transportation specialists, and others interested in how places and loca-\ntions relate to one another. We have learned that a location is a zero-dimensional entity (it has no length, width,\nheight, or volume), locations alone are not sufficient for representing the complexity of the real world. Locations\nare frequently composed into one or more geometric primitives, which include the set of entities more common-\nly referred to as:\n1. Points;\n2. Lines; and\n3. Polygons (or Areas).\nIn the field of Topology, we can expand them to:\n1. Nodes: zero-dimensional entities represented by coordinate pairs. Coordinates for nodes may be x,y values\nlike those in Euclidean geometry or longitude and latitude coordinates that represent places on Earth\u2019s sur-\nface. In both cases, a third z value is sometimes added to specify a location in three dimensions;\n2. Edges: one-dimensional entities created by connecting two nodes. The nodes at either end of an edge are\ncalled connecting nodes and can be referred to more specifically as a start node or end node, depending on\nthe direction of the edge, which is indicated by arrowheads. Edges in TIGER have direction so that the left\nand right side of the street can be determined for use in address matching. Nodes that are not associated\nwith an edge and exist by themselves are called isolated nodes. Edges can also contain vertices, which are\noptional intermediate points along an edge that can define the shape of an edge with more specificity than\nstart and end nodes alone. Examples of edges encoded in TIGER are streets, railroads, pipelines, and rivers;\nand\n3. Faces: two-dimensional (length and width) entities that are bounded by edges. Blocks, counties, and vot-\ning districts are examples of faces. Since faces are bounded by edges and edges have direction, faces can be\ndesignated as right faces or left faces.\nFigure below shows an example of these geometric primitives in a realistic arrangement. In this example, note\nthat:\n1. Nodes N14 and N17 are isolated nodes;\n2. N7 and N6 are the start and end nodes of edge E1; and\n3. Due to the directionality of edges, face F2 is on the left of edge E8.\n21 Chapter 3: Topology and Creating Data\nFigure 3.1. The geographic primitives include nodes, edges, and faces. Source: Department of Geography, The\nPennsylvania State University. Adapted from DiBiase (1997).\nThe following illustration shows how a layer of polygons can be described and used:\n(cid:37)(cid:3) As collections of geographic features (points, lines, and polygons)\n(cid:37)(cid:3) As a graph of topological elements (nodes, edges, faces, and their relationships)\nFigure 3.2. Relationship between geographic feature and the topological elements. Source: ESRI Help: http:\/\/\nwebhelp.esri.com\/arcgisserver\/9.3\/java\/index.htm#geodatabases\/topology_basics.htm\nTOPOLOGICAL RELATIONSHIPS\nWe have learned how coordinates, both geometric and geographic, can define points and nodes, how nodes can\nbuild edges, and how edges create faces. We will now consider how nodes, edges, and faces can relate to one\nanother through the concepts of containment, connectedness, and adjacency. A fundamental property of all\ntopological relations is that they are constant under continuous deformation: re-projecting a map will not alter\n22 Chapter 3: Topology and Creating Data\ntopology, nor will any amount of rubber-sheeting or other data transformations change relations from one form\nto another.\nContainment is the property that defines one entity as being within another. For example, if an isolated node\n(representing a household) is located inside a face (representing a congressional district) in the database, you can\ncount on it remaining inside that face no matter how you transform the data. Topology is vitally important to the\nCensus Bureau, whose constitutional mandate is to accurately associate population counts and characteristics\nwith political districts and other geographic areas.\nConnectedness refers to the property of two or more entities being connected. In Figure 2.1, Topologically, node\nN14 is not connected to any other nodes. Nodes N9 and N21 are connected because they are joined by edges\nE10, E1, and E10. In other words, nodes can be considered connected if and only if they are reachable through a\nset of nodes that are also connected; if a node is a destination, we must have a path to reach it.\nConnectedness is not immediately as intuitive as it may seem. A famous problem related to topology is the\nK\u00f6nigsberg bridge puzzle (Figure 3.3).\nFigure 3.3. The seven bridges of K\u00f6nigsberg bridge puzzle. Source: Euler, L. \u201cSolutio problematis ad geome-\ntriam situs pertinentis.\u201d Comment. Acad. Sci. U. Petrop. 8, 128-140, 1736. Reprinted in Opera Omnia Series\nPrima, Vol. 7. pp. 1-10, 1766.\nThe challenge of the puzzle is to find a route that crosses all seven bridges, while respecting the following criteria:\n1. Each bridge must be crossed;\n2. A bridge is a directional edge and can only be crossed once (no backtracking);\n3. Bridges must be fully crossed in one attempt (you cannot turn around halfway, and then do the same on\nthe other side to consider it \u201ccrossed\u201d).\n4. Optional: You must start and end at the same location. (It has been said that this was a traditional require-\nment of the problem, though it turns out that it doesn\u2019t actually matter \u2013 try it with and without this re-\nquirement to see if you can discover why.)\nThe right answer is, there is no such route. Euler proved, in 1736, that there was no solution to this problem.\n23 Chapter 3: Topology and Creating Data\nWAYS THAT FEATURES SHARE GEOMETRY IN A TOPOLOGY\nFeatures can share geometry within a topology. Here are some examples among adjacent features. Source: ESRI\nHelp http:\/\/webhelp.esri.com\/arcgisserver\/9.3\/java\/index.htm#geodatabases\/topology_basics.htm\n(cid:37)(cid:3) Area features can share boundaries (polygon topology).\n(cid:37)(cid:3) Line features can share endpoints (edge\u2013node topology).\nIn addition, shared geometry can be managed between feature classes using a geodatabase topology. For example:\n(cid:37)(cid:3) Line features can share segments with other line features. For example, parcels can nest within blocks:\n(cid:37)(cid:3) Area features can be coincident with other area features.\n(cid:37)(cid:3) Line features can share endpoint vertices with other point features (node topology).\n(cid:37)(cid:3) Point features can be coincident with line features (point events).\nSource: ESRI Help: http:\/\/webhelp.esri.com\/arcgisserver\/9.3\/java\/index.htm#geodatabases\/topology_basics.\nhtm\n24 Chapter 3: Topology and Creating Data\nSource: ESRI Help: http:\/\/webhelp.esri.com\/arcgisserver\/9.3\/java\/index.htm#geodatabases\/topology_basics.\nhtm\nGaldi (2005) describes the very specific rules that define the relations of entities in the vector database:\n1. Every edge must be bounded by two nodes (start and end nodes).\n2. Every edge has a left and right face.\n3. Every face has a closed boundary consisting of an alternating sequence of nodes and edges.\n4. There is an alternating closed sequence of edges and faces around every node.\n5. Edges do not intersect each other, except at nodes.\nCompliance with these topological rules is an aspect of data quality called logical consistency. In addition, the\nboundaries of geographic areas that are related hierarchically \u2014 such as blocks, block groups, tracts, and coun-\nties - are represented with common, non-redundant edges. Features that do not conform to the topological rules\ncan be identified automatically, and corrected.\nTopology is fundamentally used to ensure data quality of the spatial relationships and to aid in data compilation.\nTopology is also used for analyzing spatial relationships in many situations such as dissolving the boundaries\nbetween adjacent polygons with the same attribute values or traversing along a network of the elements in a\ntopology graph. Topology can also be used to model how the geometry from a number of feature classes can be\nintegrated. Some refer to this as vertical integration of feature classes. Generally, topology is employed to do the\nfollowing:\n(cid:37)(cid:3) Manage coincident geometry (constrain how features share geometry). For example, adjacent polygons,\nsuch as parcels, have shared edges; street centerlines and the boundaries of census blocks have coincident\ngeometry; adjacent soil polygons share edges; etc.\n(cid:37)(cid:3) Define and enforce data integrity rules (such as no gaps should exist between parcel features, parcels should\nnot overlap, road centerlines should connect at their endpoints).\n25 Chapter 3: Topology and Creating Data\n(cid:37)(cid:3) Support topological relationship queries and navigation (for example, to provide the ability to identify\nadjacent and connected features, find the shared edges, and navigate along a series of connected edges).\n(cid:37)(cid:3) Support sophisticated editing tools that enforce the topological constraints of the data model (such as the\nability to edit a shared edge and update all the features that share the common edge).\n(cid:37)(cid:3) Construct features from unstructured geometry (e.g., the ability to construct polygons from lines some-\ntimes referred to as \u201cspaghetti\u201d).\nThis chapter material has been collected from the following web links that holds information with CC copy-\nrights: use and share alike.\nhttps:\/\/www.e-education.psu.edu\/geog160\/node\/1948\nhttp:\/\/webhelp.esri.com\/arcgisserver\/9.3\/java\/index.htm#geodatabases\/topology_basics.htm\nDiscussion Questions\n1. How does the topology change as you change the features of a map \u2013 for example, when you introduce a\nroad into a landscape?\n2. Consider the role of the planner in understanding topology. In what ways does the concept of topology\napply to the practice of planning?\n3. What are examples of topological errors that may be present in a dataset, perhaps one that you received\nsecond-hand?\nContextual Applications of Chapter 3\nMetropolitan Jobs Recovery? Not Yet (Brookings)\nJob growth\n26 CHAPTER 4: MAPPING PEOPLE WITH CENSUS DATA\nWHY CENSUS?\nSome of the richest sources of attribute data for thematic mapping, particularly for choropleth maps, are national\ncensuses. In the United States, a periodic count of the entire population is required by the U.S. Constitution. Ar-\nticle 1, Section 2, ratified in 1787, states (in the last paragraph of the section shown below) that \u201cRepresentatives\nand direct taxes shall be apportioned among the several states which may be included within this union, accord-\ning to their respective numbers ... The actual Enumeration shall be made [every] ten years, in such manner as\n[the Congress] shall by law direct.\u201d The U.S. Census Bureau is the government agency charged with carrying out\nthe decennial census.\nFigure 4.1: A portion of the Constitution of the United States of America (preamble and first three paragraphs\nof Article 1). Credit: Obtained from: http:\/\/www.archives.gov\/exhibits\/charters\/charters_downloads.html\nThe results of the U.S. decennial census determine states\u2019 portions of the 435 total seats in the U.S. House of Rep-\nresentatives. Thematic maps can show states that lost and gained seats as a result of the reapportionment that fol-\nlowed the 2000 census (Figure 4.2). By focusing on the U.S. state-by-state, we develop a variant on a choropleth\nmap. Rather than using color fill to depict quantity, color depicts only change and its direction, red for a loss in\nnumber of Congressional seats, gray for no change, and blue for a gain in number of Congressional seats. Num-\nbers are then used as symbols to indicate amount of change (small -1 or +1 for a change of 1 seat and larger -2 or\n+2 for a change of two seats). This scaling of numbers is an example of the more general application of \u201csize\u201d as a\ngraphic variable to produce \u201cproportional symbols\u201d \u2013 the topic we cover in detail in the section on proportional\nsymbol mapping below.\n27 Chapter 4: Mapping People with Census Data\nFigure 4.2: Reapportionment of the U.S. House of Representatives as a result of the 2000 census. Source: Smith,\nJM., 2012. Department of Geography, The Pennsylvania State University; After figure in Chapter 3, DiBiase.\nCongressional voting district boundaries must be redrawn within the states that gained and lost seats, a process\ncalled redistricting. Constitutional rules and legal precedents require that voting districts contain equal popula-\ntions (within about 1 percent). In addition, districts must be drawn so as to provide equal opportunities for rep-\nresentation of racial and ethnic groups that have been discriminated against in the past. Further, each state is al-\nlowed to create its own parameters for meeting the equal opportunities constraint. Whether districts determined\neach decade actually meet these guidelines is typically a contentious issue and often results in legal challenges.\nBeyond the role of the census of population in determining the number of representatives per state (thus in\nproviding the data input to reapportionment and redistricting), the Census Bureau\u2019s mandate is to provide the\npopulation data needed to support governmental operations, more broadly including decisions on allocation of\nfederal expenditures. Its broader mission includes being \u201cthe preeminent collector and provider of timely, rel-\nevant, and quality data about the people and economy of the United States\u201d. To fulfill this mission, the Census\nBureau needs to count more than just numbers of people, and it does.\nTHEMATIC MAPPING\nUnlike reference maps, thematic maps are usually made with a single purpose in mind. Typically, that purpose\nhas to do with revealing the spatial distribution of one or two attribute data sets. In this section, we will consider\ndistinctions among three types of ratio level data, counts, rates, and densities. We will also explore several dif-\nferent types of thematic maps, and consider which type of map is conventionally used to represent the different\ntypes of data. We will focus on what is perhaps the most prevalent type of thematic map, the choropleth map.\nChoropleth maps tend to display ratio level data which have been transformed into ordinal level classes. Finally,\nyou will learn two common data classification procedures, quantiles and equal intervals.\nMAPPING COUNTS\nThe simplest thematic mapping technique for count data is to show one symbol for every individual counted. If\n28 Chapter 4: Mapping People with Census Data\nthe location of every individual is known, this method often works fine. If not, the solution is not as simple as it\nseems. Unfortunately, individual locations are often unknown, or they may be confidential. Software like ESRI\u2019s\nArcMap, for example, is happy to overlook this shortcoming. Its \u201cDot Density\u201d option causes point symbols to\nbe positioned randomly within the geographic areas in which the counts were conducted (Figure 4.3). The size\nof dots, and number of individuals represented by each dot, are also optional. Random dot placement may be\nacceptable if the scale of the map is small, so that the areas in which the dots are placed are small. Often, howev-\ner, this is not the case.\nFigure 4.3. A \u201cdot density\u201d map that depicts count data. Source: G. Hatchard. https:\/\/www.e-education.psu.edu\/\ngeog482fall2\/c3_p17.html\nAn alternative for mapping counts that lack individual locations is to use a single symbol, a circle, square, or some\nother shape, to represent the total count for each area. ArcMap calls the result of this approach a Proportional\nSymbol map. When the size of each symbol varies in direct proportion to the data value it represents we have a\nproportional symbol map (Figure 4.4). In other words, the area of a symbol used to represent the value \u201c1,000,000\u201d\nis exactly twice as great as a symbol that represents \u201c500,000.\u201d To compensate for the fact that map readers typi-\ncally underestimate symbol size, some cartographers recommend that symbol sizes be adjusted. ArcMap calls this\noption \u201cFlannery Compensation\u201d after James Flannery, a research cartographer who conducted psychophysical\nstudies of map symbol perception in the 1950s, 60s, and 70s. A variant on the Proportional Symbol approach is\nthe Graduated Symbol map type, in which different symbol sizes represent categories of data values rather than\nunique values. In both of these map types, symbols are usually placed at the mean locations, or centroids, of the\nareas they represent.\n29 Chapter 4: Mapping People with Census Data\n`\nFigure 4.4. A \u201cproportional circle\u201d map that depicts count data. Source: G. Hatchard. https:\/\/www.e-education.\npsu.edu\/geog482fall2\/c3_p17.html\nMAPPING RATES AND DENSITIES\nA rate is a proportion between two counts, such as Hispanic population as a percentage of total population. One\nway to display the proportional relationship between two counts is with what ArcMap calls its Pie Chart option.\nLike the Proportional Symbol map, the Pie Chart map plots a single symbol at the centroid of each geographic\narea by default, though users can opt to place pie symbols such that they won\u2019t overlap each other (This option\ncan result in symbols being placed far away from the centroid of a geographic area.) Each pie symbol varies in\nsize in proportion to the data value it represents. In addition, however, the Pie Chart symbol is divided into piec-\nes that represent proportions of a whole (Figure 4.5).\nFigure 4.5. A \u201cpie chart\u201d map that depicts rate data. Source: G. Hatchard. https:\/\/www.e-education.psu.edu\/\ngeog482fall2\/c3_p16.html\n30 Chapter 4: Mapping People with Census Data\nSome perceptual experiments have suggested that human beings are more adept at judging the relative lengths\nof bars than they are at estimating the relative sizes of pie pieces (although it helps to have the bars aligned along\na common horizontal base line). You can judge for yourself by comparing the effect of ArcMap\u2019s Bar\/Column\nChart option (Figure 4.6).\nFigure 4.6. A \u201cbar\/column chart\u201d map that depicts rate data. Source: G. Hatchard. https:\/\/www.e-education.\npsu.edu\/geog482fall2\/c3_p16.html\nLike rates, densities are produced by dividing one count by another, but the divisor of a density is the magnitude\nof a geographic area. Both rates and densities hold true for entire areas, but not for any particular point location.\nFor this reason, it is conventional not to use point symbols to symbolize rate and density data on thematic maps.\nInstead, cartography textbooks recommend a technique that ArcMap calls \u201cGraduated Colors.\u201d Maps produced\nby this method, properly called choropleth maps, fill geographic areas with colors that represent attribute data\nvalues (Figure 4.7).\nFigure 4.7. A \u201cgraduated color\u201d (choropleth) map that depicts density data. Source: G. Hatchard. https:\/\/ww-\nw.e-education.psu.edu\/geog482fall2\/c3_p16.html\nBecause our ability to discriminate among colors is limited, attribute data values at the ratio or interval level are\n31 Chapter 4: Mapping People with Census Data\nusually sorted into four to eight ordinal level categories. ArcMap calls these categories classes. Users can adjust\nthe number of classes, the class break values that separate the classes, and the colors used to symbolize the class-\nes. Users may choose a group of predefined colors, known as a color ramp, or they may specify their own custom\ncolors. Color ramps are sequences of colors that vary from light to dark, where the darkest color is used to rep-\nresent the highest value range. Most textbook cartographers would approve of this, since they have long argued\nthat it is the lightness and darkness of colors, not different color hues, that most logically represent quantitative\ndata.\nLogically or not, people prefer colorful maps. For this reason some might be tempted to choose ArcMap\u2019s Unique\nValues option to map rates, densities, or even counts. This option assigns a unique color to each data value.\nColors vary in hue as well as lightness. This symbolization strategy is designed for use with a small number of\nnominal level data categories. As illustrated in the map below (Figure 4.8), the use of an unlimited set of color\nhues to symbolize unique data values leads to a confusing thematic map.\nFigure 4.8. A \u201cunique values\u201d map that depicts density data. Note that the legend, which in the original shows\none category for each state, is trimmed off. Source: G Htchard. https:\/\/www.e-education.psu.edu\/geog482fall2\/\nc3_p16.html\nDATA CLASSIFICATION\nAs discussed earlier, all maps are abstractions. This means that they depict only selected information, but also\nthat the information selected must be generalized due to the limits of display resolution, comparable limits of\nhuman visual acuity, and especially the limits imposed by the costs of collecting and processing detailed data.\nWhat we have not previously considered is that generalization is not only necessary, it is sometimes beneficial; it\ncan make complex information understandable. Consider a simple example. The graph below (Figure 4.9) shows\nthe percent of people who prefer the term \u201cpop\u201d (not soda or coke) for each state. Categories along the x axis of\nthe graph represent each of the 50 unique percentage values (two of the states had exactly the same rate). Catego-\nries along the y axis are the numbers of states associated with each rate. As you can see, it\u2019s difficult to discern a\npattern in these data; it appears that there is no pattern.\n32 Chapter 4: Mapping People with Census Data\nFigure 4.9: Unique percentage values for people who use the term \u201cpop\u201d by state. Source: JM Smith, Depart-\nment of Geography, The Pennsylvania State University.\nThe following graph (Figure 4.10) shows exactly the same data set, only grouped into 10 classes with equal 10%\nranges). It\u2019s much easier to discern patterns and outliers in the classified data than in the unclassified data. Notice\nthat people in a large number of states (23) do not really prefer the term \u201cpop\u201d as they are distributed around 0\nto 10 percent of users who favor that term. There are no states at the other extreme (91-100%), but a few states\nwhose vast majority (81-90% of their population) prefer the term pop. Ignoring the many 0-10% states where\npop is rarely used, the most common states are ones in which about 2\/3 favor the term; looking back to `Figure\n3.13, these are primarily northern states, including Pennsylvania. All of these variations in the information are\nobscured in the unclassified data.\nFigure 4.10: Classed percentages of people who use the term \u201cpop\u201d by state. Source: JM Smith, Department of\nGeography, The Pennsylvania State University.\nAs shown above, data classification is a generalization process that can make data easier to interpret. Classifica-\ntion into a small number of ranges, however, gives up some details in exchange for the clearer picture, and there\nare multiple choices of methods to classify data for mapping. If a classification scheme is chosen and applied\nskillfully, it can help reveal patterns and anomalies that otherwise might be obscured (as shown above). By the\nsame token, a poorly-chosen classification scheme may hide meaningful patterns. The appearance of a themat-\nic map, and sometimes conclusions drawn from it, may vary substantially depending on the data classification\nscheme used. Thus, it is important to understand the choices that might be made, whether you are creating a\n33 Chapter 4: Mapping People with Census Data\nmap or interpreting one created by someone else.\nMany different systematic classification schemes have been developed. Some produce mathematically \u201coptimal\u201d\nclasses for unique data sets, maximizing the difference between classes and minimizing differences within classes.\nSince optimizing schemes produce unique solutions, however, they are not the best choice when several maps need\nto be compared. For this, data classification schemes that treat every data set alike are preferred.\nFigure 4.11. Portion of the ArcMap classification dialog box highlighting the schemes supported in ArcMap\n8.2. Source: Department of Geography, The Pennsylvania State University.\nTwo commonly used classification schemes are quantiles and equal intervals. The following two graphs illustrate\nthe differences.\nFigure 4.12. County population change rates divided into five quantile categories. Source: Department of Geog-\nraphy, The Pennsylvania State University.\nThe graph above groups the Pennsylvania county population change data into five classes, each of which contains\nthe same number of counties (in this case, approximately 20 percent of the total in each). The quantiles scheme\naccomplishes this by varying the width, or range, of each class. Quantile is a general label for any grouping of\nrank ordered data into an equal number of entities; quantiles with specific numbers of groups go by their own\nunique labels (\u201cquartiles\u201d and \u201cquintiles,\u201d for example, are instances of quantile classifications that group data\ninto four and five classes respectively). The figure below, then, is an example of quintiles.\n34 Chapter 4: Mapping People with Census Data\nFigure 4.13. County population change rates divided into five equal interval categories. Source: Department of\nGeography, The Pennsylvania State University.\nIn the second graph, the data range of each class is equivalent (8.5 percentage points). Consequently, the number\nof counties in each equal interval class varies.\nFigure 4.14. The five quantile classes mapped. Source: Department of Geography, The Pennsylvania State Uni-\nversity.\nFigure 4.15. The five equal interval classes mapped. Department of Geography, The Pennsylvania State Univer-\nsity.\n35 Chapter 4: Mapping People with Census Data\nAs you can see, the effect of the two different classification schemes on the appearance of the two choropleth\nmaps above is dramatic. The quantiles scheme is often preferred because it prevents the clumping of observations\ninto a few categories shown in the equal intervals map. Conversely, the equal interval map reveals two outlier\ncounties that are obscured in the quantiles map. Due to the potentially extreme differences in visual appearance,\nit is often useful to compare the maps produced by several different map classifications. Patterns that persist\nthrough changes in classification schemes are likely to be more conclusive evidence than patterns that shift. Pat-\nterns that show up with only one scheme may be important, but require special scrutiny (and an understanding\nof how the scheme works) to evaluate.\nAGGREGATED DATA: ENUMERATION VERSUS SAMPLES\nQuantitative data of the kinds depicted by the maps detailed in the previous section come from a diverse array of\nsources. In the U.S., one of the most important sources is the U.S. Bureau of the Census (discussed briefly above).\nHere we focus in on one important distinction in data collected by the Census and by other organizations, a dis-\ntinction between complete enumeration (counting every entity) and sampling.\nSixteen U.S. Marshals and 650 assistants conducted the first U.S. census in 1791. They counted some 3.9 million\nindividuals, although as then-Secretary of State Thomas Jefferson reported to President George Washington, the\nofficial number understated the actual population by at least 2.5 percent (Roberts, 1994). By 1960, when the U.S.\npopulation had reached 179 million, it was no longer practical to have a census taker visit every household. The\nCensus Bureau then began to distribute questionnaires by mail. Of the 116 million households to which ques-\ntionnaires were sent in 2000, 72 percent responded by mail. A mostly-temporary staff of over 800,000 was need-\ned to visit the remaining households, and to produce the final count of 281,421,906. Using statistically reliable\nestimates produced from exhaustive follow-up surveys, the Bureau\u2019s permanent staff determined that the final\ncount was accurate to within 1.6 percent of the actual number (although the count was less accurate for young\nand minority residences than it was for older and white residents). It was the largest and most accurate census\nto that time. (Interestingly, Congress insists that the original enumeration or \u201chead count\u201d be used as the official\npopulation count, even though the estimate calculated from samples by Census Bureau statisticians is demon-\nstrably more accurate.) As of this writing, some aspects of reporting from the decennial census of 2010 are still\nunderway. Like 2000, the mail-in response rate was 72 percent. The official 2010 census count, by state, was de-\nlivered to the U.S. Congress on December 21, 2010 (10 days prior to the mandated deadline). The total count for\nthe U.S. was 308,745,538, a 9.7% increase over 2000.\nIn the first census, in 1791, census takers asked relatively few questions. They wanted to know the numbers of\nfree persons, slaves, and free males over age 16, as well as the sex and race of each individual. (You can view\nreplicas of historical census survey forms at Ancestry.com) As the U.S. population has grown, and as its econo-\nmy and government have expanded, the amount and variety of data collected has expanded accordingly. In the\n2000 census, all 116 million U.S. households were asked six population questions (names, telephone numbers,\nsex, age and date of birth, Hispanic origin, and race), and one housing question (whether the residence is owned\nor rented). In addition, a statistical sample of one in six households received a \u201clong form\u201d that asked 46 more\nquestions, including detailed housing characteristics, expenses, citizenship, military service, health problems,\nemployment status, place of work, commuting, and income. From the sampled data the Census Bureau produced\nestimated data on all these variables for the entire population.\nIn the parlance of the Census Bureau, data associated with questions asked of all households are called 100%\ndata and data estimated from samples are called sample data. Both types of data are aggregated by various enu-\nmeration areas, including census block, block group, tract, place, county, and state (see the illustration below).\nThrough 2000, the Census Bureau distributes the 100% data in a package called the \u201cSummary File 1\u201d (SF1) and\nthe sample data as \u201cSummary File 3\u201d (SF3). In 2005, the Bureau launched a new project called American Com-\n36 Chapter 4: Mapping People with Census Data\nmunity Survey that surveys a representative sample of households on an ongoing basis. Every month, one house-\nhold out of every 480 in each county or equivalent area receives a survey similar to the old \u201clong form.\u201d Annual\nor semi-annual estimates produced from American Community Survey samples replaced the SF3 data product\nin 2010.\nTo protect respondents\u2019 confidentiality, as well as to make the data most useful to legislators, the Census Bureau\naggregates the data it collects from household surveys to several different types of geographic areas. SF1 data, for\ninstance, are reported at the block or tract level. There were about 8.5 million census blocks in 2000. By defini-\ntion, census blocks are bounded on all sides by streets, streams, or political boundaries. Census tracts are larger\nareas that have between 2,500 and 8,000 residents. When first delineated, tracts were relatively homogeneous\nwith respect to population characteristics, economic status, and living conditions. A typical census tract consists\nof about five or six sub-areas called block groups. As the name implies, block groups are composed of several\ncensus blocks. American Community Survey estimates, like the SF3 data that preceded them, are reported at the\nblock group level or higher. The unit types are organized with each higher type composed of some number of the\nlower type as outlined above for blocks, block groups, and census tracts (Figure 4.16).\nFigure 4.16 Relationships among the various census geographies. Source: U.S. Census Bureau, American Fact-\nFinder. http:\/\/factfinder2.census.gov\/faces\/nav\/jsf\/pages\/using_factfinder5.xhtml\nThis chapter material has been collected from the following web links that holds information with CC copy-\nrights: use and share alike.\nhttps:\/\/www.e-education.psu.edu\/geog160\/c3_p14.html\nhttps:\/\/www.e-education.psu.edu\/geog482fall2\/c3_p16.html\nhttps:\/\/www.e-education.psu.edu\/geog482fall2\/c3_p17.html\nDiscussion Questions\n1. In what ways do local planners rely on US Census data?\n2. How does a pie chart and a bar\/column chart differ? What are their visual advantages and disadvantages\n37 Chapter 4: Mapping People with Census Data\nwhen depicting quantitative data?\n3. How could you use longitudinal US Census data to address a pressing challenge in the field of urban and\nregional planning?\nContextual Applications of Chapter 4\nThe Metropolitan Geography of Low-Wage Work (Brookings)\nNew Data Illustrate Local Impact of Tax Credits for Working Families\n38 CHAPTER 5: LYING WITH MAPS\nWhen you understand the technique of making maps in general, it is time to realize that how maps finally look\nlike to a great extent depends on how you present your data. Especially in choropleth map, how do you define the\ndata breaking points, and varied choices on symbology lead to different looking maps which might hide part of the\ninformation what the real data truly present.\nThis piece by Mark Monmonier warn us not only to be careful in designing maps, but also to be critical in reading\nmaps, and promoting a healthy skepticism about these easy-to-manipulate models of reality. Monmonier shows\nthat, despite their immense value, maps lie. Statistics of any kind can be manipulated. For professionals working in\nthe planning field, who rely on lots of data to make public decisions, it is especially important to be skeptical about\nwhat you are presented.\nTo show how maps distort, Monmonier introduces basic principles of mapmaking, gives entertaining examples\nof the misuse of maps in situations through progressively more subtle treatments of data, each misleading, some\ninnocent and others malicious, until you begin to question all map abstractions entirely. It covers all the typical\nkinds of distortions from deliberate oversimplifications to the misleading use of color.\nRead the book chapter\nMark Monmonier. 1996. \u2018Data Maps: Making Nonsense of the Census\u2019. in Mark Monmonier (1996) 2nd edition.\nHow to Lie with Maps? Chapter 10. University Of Chicago Press\nDiscussion Questions\n1. What are ways that a map-maker (cartographer) has control of the displaying spatial data?\n2. What are examples of ways that planners can use maps to persuade city council members with a decision\nabout a proposed change in zoning or land use designation?\n3. How can a planner ensure that they are not being seduced by the spatial information they review?\nContextual Applications of Chapter 5\nThe 5 U.S. Counties Where Racial Diversity Is Highest\u2014and Lowest\nExposed: America\u2019s Totally Inconsistent Minimum Parking Requirements\n39 CHAPTER 6: TO STANDARDIZE OR NOT TO STANDARDIZE?\nThe idea of data quality and standards are important especially in the urban planning field because we make\ndecisions based on data collected from different institutions. If there is not a common standard to follow, it will\nbe frustrating to work if these data came with different quality. Besides, using low quality data might lead public\nofficials and researchers to wrong conclusions, affecting the decision-making process. This chapter by Yeung ex-\nplains the importance of data quality and data standards, and their inter-relationships. There are ways to quan-\ntitatively assess the positional and attribute accuracy of geo-spatial data. If you are interested, you can explore\ncensus TIGER file to see how the1990 files differ from thee 2000 dataset.\nThe chapter starts discussing concepts of geospatial data quality such as accuracy (degree to which data agree\nwith the description of the real world that they represent); precision (how exactly are measured and stored);\nerror (a measure of the deviation between the measured value and the true value), and uncertainty (lack of\nconfidence in the use of the data due to incomplete knowledge of the data). All these concepts are related to the\ndescription and evaluation of data quality. Yeung also discusses the sources and types of errors in geospatial data\n(inherent and operational errors), which are almost impossible to avoid.\nIn sum, Yeung describes seven dimensions of geospatial data quality: (i) lineage (document the sources from\nwhich data is derived), (ii) positional accuracy (it is defined as the closeness of values in the database to the\ntrue positions of the real world), (iii) attribute accuracy (closeness of descriptive data to the assumed real world\nvalues), (iv) logical consistency (describes the fidelity of the relationship between real world and encoded data),\n(v) completeness (refers to whether the data exhausts the universe of all possible items), (vi) temporal accuracy\n(refers to the representation of time in geospatial data), and (vii) semantic accuracy (measures how correctly spa-\ntial objects are labeled in the data set). The positional and attribute accuracy are the most relevant.\nThe presence of errors is a norm rather than an exception. Thus spatial errors need to be managed to reduce\nuncertainty. There are three perspectives to effect the management of spatial data errors: (i) data production\n(control de data quality during the data acquisition), (ii) data use (related to errors when data is used), and (iii)\ncommunication between data producer and data user (evaluating the quality of the data so that users are aware\nof the level of uncertainty).\nTo make sure the quality assurance and quality control of geospatial data is the expected the process of data col-\nlection needs to be monitor because is the greatest source of errors in digital geospatial data. During the process\nof geographic analysis there might be an accumulation of the effects of errors. This is known as the error prop-\nagation. Managing errors requires a pragmatic approach through, for example, sensitivity analysis, which is a\nmodeling technique to assess the subjectivity and variability in the parameters of spatial problem-solving model.\nThe purpose of the sensitivity analysis is to test the model for output over a range of legitimate uncertainties. An-\nother relevant aspect is the reporting data quality: information need to be effectively communicated in the form\nof \u2018accuracy indices\u2019 and maps to all potential users. Geospatial data standards can provide a yardstick -- created\nby consensus by a recognized organization -- against which quality can be evaluated, through the provision of\nrules for common and repeated use. Yueng offers four categories of standards: (i) application standards, (ii) data\nstandards \u2013the most important-, (iii) technology standards, and (iv) professional practice standards. In general\ngeospatial data standards provide the means of communication between suppliers and users. These are made up\nof one or more of these four components: (a) standard data products, (b) data transfer standards, (c) data quality\nstandards, and (d) metadata standards. The development and acceptance of data standards was crucial not only\nfor allowing sharing data but most important, it helped to develop \u2018open GIS\u2019.\nRead the book chapter\nAlbert Yeung. 2007. \u201cGeospatial Data Quality and Standards\u201d in Albert Yeung 2007 Concepts and Techniques in\nGeographic Information Systems. pp 108-42.\n40 Chapter 6: To Standardize or Not to Standardize\nDiscussion Questions\n1. Why does standardization provide a means for assessing data quality?\n2. What problems can standardization of spatial data create?\n3. IN what ways might you evaluate a dataset for possible errors?\nContextual Applications of Chapter 6\nNine cities that love their trees\nThe Map That Reveals 5,900 Natural Gas Leaks Under Washington, D.C.\n41 CHAPTER 7: GEOGRAPHIC CONSIDERATIONS IN PLANNING\nPRACTICE\nThis chapter is composed of two sections, a book chapter by O\u2019Sullivan and Unwin on the Pitfalls and Potential\nof Spatial Data, and a small compiled session on geo-coding. The book chapter identifies major problems in\nthe analysis of geographic information and statistical analysis of spatial data, related to spatial autocorrelation,\nmodifiable area units, the ecological fallacy, scale, and non-uniformity of space and edge effects. It also discusses\nrelevant geographic concepts central to spatial analysis such as, of distance, adjacency, interaction (first law of ge-\nography), and neighborhood. Finally, it discusses proximity polygons and shows how variogram clouds are used\nto analyze relationships between data attributes and their spatial location, through matrices.\nSpatial data require special analytic techniques thus standard statistic methods have significant problems to\nanalysis spatial distributions. There are five major problems. First, spatial data tend to violate assumption that\nsamples are random because in geography phenomena do not vary randomly through space, leading to the\ngiven problem of spatial autocorrelation (data from locations near to one another in space are more likely to be\nsimilar than data from locations remote from one another), which introduces the problem of redundancy due\nto biased samples. Second, the modifiable areal unit problem (when aggregation units used are arbitrary with\nrespect to the phenomena under investigation) tends to affect the \u2018coefficient of determination, R square. Third,\nthe ecological fallacy (when statistical relations observed at one level of aggregation are assumed to hold because\nthe same relationship holds when we look at more detail level). In this case the thread is that statistical relations\nmay change at different levels of aggregation. The fourth problem is related to scale, which might affect spatial\nanalysis based on the geographic scale at which the phenomenon of interest is analyzed. Lastly, another problem\ndistinguishing spatial analysis from conventional statistics is the non-uniformity of space. This issue refers to the\nfact that analysis might find patterns \u2013thus clusters-, simply as a result of where people live and work. An exam-\nple is the \u201cedge effect\u201d (it emerges when artificial boundary is imposed on a study).\nAlthough geospatial referencing provides ways to look at data. There are four useful concepts to analyze the\nspatial distribution of associated entities and spatial relationships: (i) distance (it can be measure as the simple\ncrow\u2019s flight distance between the spatial entities of interest, though it can be measured in more complex ways).\n(ii) Adjacency (it is of the thought as the nominal, or binary, equivalent of distance. It is argued that two spatial\nentities are either adjacent or they are not: there is not a middle ground). (iii) Interaction (it is considered a com-\nbination of distance and adjacency and rests on the ideas that nearer things are more related than distant things:\nfirst law of geography). And (iv) neighborhood (there are many ways to conceptualize it (e.g., with respect to\nsets of adjacent entities, a region of space defined by distance from an associated entity, etc.). One way of pulling\nthese four concepts together is to represent them in matrices.\nLastly, the chapter discusses the proximity polygons, a tool used to specify the spatial properties of a set of\nobjects through partitioning a study region into proximity polygons. The proximity polygon of an entity is the\nclosest region to the entity. The variogram cloud is an exploratory tool (though difficult to interpret) that offers a\ngeneral picture of relationships between the spatial locations of objects and the other data attributes. It does it by\nplotting the differences in attribute values for pairs of entities against the differences in their location.\nRead the book chapter\nO\u2019Sullivan, David, and David John Unwin. Geographic information analysis. John Wiley & Sons, 2003. The Pit-\nfalls and Potential of Spatial Data. Chapter 2\nCREATING DATA THROUGH GEOCODING\nGeocoding is the process used to convert location codes, such as street addresses or postal codes, into geographic\n(or other) coordinates. The terms \u201caddress geocoding\u201d and \u201caddress mapping\u201d refer to the same process. Geoc-\n42 Chapter 7: Geographic Considerations in Planning Practice\noding address-referenced population data is one of the Census Bureau\u2019s key responsibilities. However, as you\nmay know, it is also a very popular capability of online mapping and routing services. In addition, geocoding is\nan essential element of a suite of techniques that are becoming known as \u201cbusiness intelligence.\u201d We will look\nat applications like these later in this chapter, but, first, let\u2019s consider how the Census Bureau performs address\ngeocoding.\nADDRESS GEOCODING AT THE US CENSUS: PRE-MODERNIZATION\nPrior to the MAF\/TIGER modernization project that led up to the decennial census of 2010, the TIGER data-\nbase did not include a complete set of point locations for U.S. households. Lacking point locations, TIGER was\ndesigned to support address geocoding by approximation. As illustrated below, the pre-modernization TIGER\ndatabase included address range attributes for the edges that represent streets. Address range attributes were also\nincluded in the TIGER\/Line files extracted from TIGER. Coupled with the Start and End nodes bounding each\nedge, address ranges enable users to estimate locations of household addresses (Figure 7.1).\nFigure 7.1: How address range attributes were encoded in TIGER\/Line files. Address ranges in contemporary\nTIGER\/Line Shapefiles are similar, except that \u201cFrom\u201d (FR) and \u201cTo\u201d nodes are now called \u201cStart\u201d and \u201cEnd\u201d.\nSouce: U.S. Census Bureau 1997.\nHere\u2019s how it works. Figure 7.1 highlights an edge that represents a one-block segment of Oak Avenue. The edge\nis bounded by two nodes, labeled \u201cStart\u201d and \u201cEnd.\u201d A corresponding record in an attribute table includes the\nunique ID number (0007654320) that identifies the edge, along with starting and ending addresses for the left\n(FRADDL, TOADDL) and right (FRADDR, TOADDR) sides of Oak Avenue. Note also that the address ranges\ninclude potential addresses, not just existing ones. This is done in order to future-proof the records, ensuring that\nthe data will still be valid as new buildings and addresses are added to the street.\nThis chapter text has been compiled from the following web links that holds information with CC copyrights:\nuse and share alike.\n43 Chapter 7: Geographic Considerations in Planning Practice\nhttps:\/\/www.e-education.psu.edu\/geog160\/node\/1941\nDiscussion Questions\n1. In what ways can spatial autocorrelation impact your interpretation of spatial patterns, including changes\nin demographics or land use patterns?\n2. If land use practice occurs at a parcel (or tax lot) scale, then how and why are other scales relevant?\n3. What are pitfalls of the computerized geocoding process that may lead to a misinterpretation of spatial\nphenomena?\nContextual Applications of Chapter 7\nPeople of Color Are Disproportionately Hurt by Air Pollution\nSeattle\u2019s Hilly Neighborhoods Could Slide Into the Water During the Next Earthquake\n44 CHAPTER 8: MANIPULATING GIS DATA\nBefore today, we have been mainly working on data that we downloaded from certain sources, or creating new\nGIS shapefiles. We basically present whatever we have, and we haven\u2019t taken advantage of the greatest strength of\na geographic information system (GIS), notably the explicit spatial relationships. Spatial analysis is a fundamen-\ntal component of a GIS that allows for an in-depth study of the topological and geometric properties of a dataset\nor datasets. Geoprocessing is to provide tools and a framework for performing spatial analysis and managing\nyour geographic data. This chapter by Longley et al (2001) first discusses what is spatial analysis, and continued\ntwo major spatial analysis, one based on location, and the other based on distance.\nWhile knowing geo-processing refers to tools that allow one to perform GIS tasks that range from simple buffers\nand polygon overlays to complex regression analysis and image classification, spatial analysis refers to efforts that\nturns data into information: making what is implicit explicit, and what is invisible visible. This is especially true\nfor urban planners, whose needs require decisions to be based in data and from different disciplines. The kinds\nof tasks to be automated can be mundane\u2014for example, to collect different data and transform it from one for-\nmat to another. Or the tasks can be quite creative, using a sequence of operations to model and analyze complex\nspatial relationships\u2014for example, calculating optimum paths through a transportation network, predicting the\npath of wildfire, analyzing and finding patterns in crime locations, predicting which areas are prone to landslides,\nor predicting flooding effects of a storm event.\nOne point to be highlighted is that, spatial analysis open the door for a lot of more sophisticated GIS tools to us.\nThis does not underestimate the power of map making. The design of a map can be very sophisticated and that\nmaps provide a means of conveying geographic information and knowledge by revealing patterns and processes.\nMap making itself, can be one way of conducting spatial analysis.\nRead the book chapter\nLongley, Goodchild; Maguire, and Rhind. Geographic Information Systems and Science. Hoboken, NJ: John\nWiley & Sons, 2001. Spatial Data Analysis. Chapter 14.\nDiscussion Questions\n1. What geo-processing tasks may be most useful for GIS analysis in your planning interests (e.g. environ-\nment, transportation, community development, etc.)?\n2. How might we use geo-processing to find patterns of development that have unintended consequences\non the quality of life for residents?\n3. As spatial analysis becomes more complex, what are ways for communicating your spatial analysis to\nnon-technical audiences?\nContextual Applications of Chapter 8\nThe 5 U.S. Counties Where Racial Diversity Is Highest\u2014and Lowest\nThe Number of Americans Living in High-Poverty Neighborhoods Is Still on the Rise\n45 CHAPTER 9: RASTER DATA MODELS\nWe have learned that there are two major ways how GIS model the real world. Both the vector and raster ap-\nproaches accomplish the same thing: they allow us to represent the Earth\u2019s surface with a limited number of\nlocations. What distinguish the two is the sampling strategies they embody. The vector approach is like creating a\npicture of a landscape with shards of stained glass cut to various shapes and sizes. The raster approach, by con-\ntrast, is more like creating a mosaic with tiles of uniform size. Neither is well suited to all applications, however.\nSeveral variations on the vector and raster themes are in use for specialized applications, and the development of\nnew object-oriented approaches is underway.\nAlthough our course has mainly focused on the vector data model, raster data analysis presents the final power-\nful data mining tool available. Raster data are particularly suited to certain types of analyses, such as basic geo-\nprocessing, surface analysis, and terrain mapping. Some of them are very closely related to planning needs, such\nas terrain analysis to identify buildable land in a county. While not always true, raster data can simplify many\ntypes of spatial analyses that would otherwise be overly cumbersome to perform on vector datasets. Some of the\nmost common of these techniques are presented in this chapter. First, we want to summarize the advantages and\ndisadvantages of the raster model.\nADVANTAGES\/DISADVANTAGES OF THE RASTER MODEL\nThe use of a raster data model confers many advantages. First, the technology required to create raster graphics\nis inexpensive and ubiquitous. Nearly everyone currently owns some sort of raster image generator, namely a\ndigital camera, and few cellular phones are sold today that don\u2019t include such functionality. Similarly, a plethora\nof satellites are constantly beaming up-to-the-minute raster graphics to scientific facilities across the globe. These\ngraphics are often posted online for private and\/or public use, occasionally at no cost to the user.\nAdditional advantages of raster graphics are the relative simplicity of the underlying data structure. Each grid\nlocation represented in the raster image correlates to a single value (or series of values if attributes tables are\nincluded). This simple data structure may also help explain why it is relatively easy to perform overlay analyses\non raster data. This simplicity also lends itself to easy interpretation and maintenance of the graphics, relative to\nits vector counterpart.\nDespite the advantages, there are also several disadvantages to using the raster data model. The first disadvan-\ntage is that raster files are typically very large. Particularly in the case of raster images built from the cell-by-cell\nencoding methodology, the sheer number of values stored for a given dataset result in potentially enormous files.\nAny raster file that covers a large area and has somewhat finely resolved pixels will quickly reach hundreds of\nmegabytes in size or more. These large files are only getting larger as the quantity and quality of raster datasets\ncontinues to keep pace with quantity and quality of computer resources and raster data collectors (e.g., digital\ncameras, satellites).\nA second disadvantage of the raster model is that the output images are less \u201cpretty\u201d than their vector counter-\nparts. This is particularly noticeable when the raster images are enlarged or zoomed. Depending on how far one\nzooms into a raster image, the details and coherence of that image will quickly be lost amid a pixilated sea of\nseemingly randomly colored grid cells.\nThe geometric transformations that arise during map reprojection efforts can cause problems for raster graphics\nand represent a third disadvantage to using the raster data model. We know that changing map projections will\nalter the size and shape of the original input layer and frequently result in the loss or addition of pixels (White\n2006)2. These alterations will result in the perfect square pixels of the input layer taking on some alternate rhom-\n2 White, D. 2006. \u201cDisplay of Pixel Loss and Replication in Reprojecting Raster Data from the Sinusoidal Projection.\u201d Geo-\ncarto International 21 (2): 19\u201322.\n46 Chapter 9: Raster Data Models\nboidal dimensions. However, the problem is larger than a simple reformation of the square pixel. Indeed, the\nreprojection of a raster image dataset from one projection to another brings change to pixel values that may, in\nturn, significantly alter the output information (Seong 2003)3.\nThe final disadvantage of using the raster data model is that it is not suitable for some types of spatial analy-\nses. For example, difficulties arise when attempting to overlay and analyze multiple raster graphics produced at\ndiffering scales and pixel resolutions. Combining information from a raster image with 10 m spatial resolution\nwith a raster image with 1 km spatial resolution will most likely produce nonsensical output information as the\nscales of analysis are far too disparate to result in meaningful and\/or interpretable conclusions. In addition, some\nnetwork and spatial analyses (i.e., determining directionality or geocoding) can be problematic to perform on\nraster data.\nSINGLE LAYER ANALYSIS\nReclassifying, or recoding, a dataset is commonly one of the first steps undertaken during raster analysis. Re-\nclassification is basically the single layer process of assigning a new class or range value to all pixels in the dataset\nbased on their original values. For example, an elevation grid commonly contains a different value for nearly ev-\nery cell within its extent. These values could be simplified by aggregating each pixel value in a few discrete classes\n(i.e., 0\u2013100 = \u201c1,\u201d 101\u2013200 = \u201c2,\u201d 201\u2013300 = \u201c3,\u201d etc.). This simplification allows for fewer unique values and\ncheaper storage requirements. In addition, these reclassified layers are often used as inputs in secondary analyses.\nIn vector analysis, buffering is the process of creating an output dataset that contains a zone (or zones) of a spec-\nified width around an input feature. In the case of raster datasets, these input features are given as a grid cell or a\ngroup of grid cells containing a uniform value (e.g., buffer all cells whose value = 1). Buffers are particularly suited\nfor determining the area of influence around features of interest. Whereas buffering vector data results in a precise\narea of influence at a specified distance from the target feature, raster buffers tend to be approximations represent-\ning those cells that are within the specified distance range of the target (Figure 9.2).\nFigure 9.1 Raster Reclassification. http:\/\/2012books.lardbucket.org\/books\/geographic-information-system-ba-\nsics\/s12-geospatial-analysis-ii-raster-.html\n3 Seong, J. C. 2003. \u201cModeling the Accuracy of Image Data Reprojection.\u201d International Journal of Remote Sensing 24 (11):\n2309\u201321.\n47 Chapter 9: Raster Data Models\nFigure 9.2 Raster Buffer around a Target Cell(s). http:\/\/2012books.lardbucket.org\/books\/geographic-informa-\ntion-system-basics\/s12-geospatial-analysis-ii-raster-.html\nMULTIPLE LAYER ANALYSIS\nA raster dataset can also be clipped similar to a vector dataset. Here, the input raster is overlain by a vector polygon\nclip layer. The raster clip process results in a single raster that is identical to the input raster but shares the extent\nof the polygon clip layer.\nFigure 9.3 Clipping a Raster to a Vector Polygon Layer. http:\/\/2012books.lardbucket.org\/books\/geographic-in-\nformation-system-basics\/s12-geospatial-analysis-ii-raster-.html\nRASTER OVERLAYS\nRaster overlays are relatively simple compared to their vector counterparts and require much less computation-\nal power (Burroughs 1983)4. Raster overlay superimposes at least two input raster layers to produce an output\nlayer. Each cell in the output layer is calculated from the corresponding pixels in the input layers. To do this, the\nlayers must line up perfectly; they must have the same pixel resolution and spatial extent. Once preprocessed,\nraster overlay is flexible, efficient, quick, and offers more overlay possibilities than vector overlay.\nDespite their simplicity, it is important to ensure that all overlain rasters are coregistered (i.e., spatially aligned),\ncover identical areas, and maintain equal resolution (i.e., cell size). If these assumptions are violated, the analysis\nwill either fail or the resulting output layer will be flawed. With this in mind, there are several different method-\n4 Burroughs, P. 1983. Geographical Information Systems for Natural Resources Assessment. New York: Oxford University\nPress.\n48 Chapter 9: Raster Data Models\nologies for performing a raster overlay (Chrisman 2002)5.\nRaster overlay, frequently called map algebra, is based on calculations which include arithmetic expressions and\nset and Boolean algebraic operators to process the input layers to create an output layer (Figure 9.4). It is often\nused in risk assessment studies where various layers are combined to produce an outcome map showing areas of\nhigh risk\/reward. The most common operators are addition, subtraction, multiplication, and division. In short,\nraster overlay simply uses arithmetic operators to compute the corresponding cells of two or more input layers\ntogether, uses Boolean algebra like AND or OR to find the pixels that fit a particular query statement, or executes\nstatistical tests like correlation and regression on the input layers.\nThe Boolean connectors AND, OR, and XOR can be employed to combine the information of two overlying\ninput raster datasets into a single output raster. Similarly, the relational raster overlay method utilizes relational\noperators (<, <=, =, <>, >, and =>) to evaluate conditions of the input raster datasets. In both the Boolean and re-\nlational overlay methods, cells that meet the evaluation criteria are typically coded in the output raster layer with\na 1, while those evaluated as false receive a value of 0.\nThe simplicity of this methodology, however, can also lead to easily overlooked errors in interpretation if the\noverlay is not designed properly. Assume that a natural resource manager has two input raster datasets she plans\nto overlay; one showing the location of trees (\u201c0\u201d = no tree; \u201c1\u201d = tree) and one showing the location of urban\nareas (\u201c0\u201d = not urban; \u201c1\u201d = urban). If she hopes to find the location of trees in urban areas, a simple mathemat-\nical sum of these datasets will yield a \u201c2\u201d in all pixels containing a tree in an urban area. Similarly, if she hopes\nto find the location of all treeless (or \u201cnon-tree,\u201d nonurban areas, she can examine the summed output raster\nfor all \u201c0\u201d entries. Finally, if she hopes to locate urban, treeless areas, she will look for all cells containing a \u201c1.\u201d\nUnfortunately, the cell value \u201c1\u201d also is coded into each pixel for nonurban, tree cells. Indeed, the choice of input\npixel values and overlay equation in this example will yield confounding results due to the poorly devised overlay\nscheme.\nFigure 9.4 Mathematical Raster Overlay. http:\/\/2012books.lardbucket.org\/books\/geographic-information-sys-\ntem-basics\/s12-geospatial-analysis-ii-raster-.html - Two input raster layers are overlain to produce an output\nraster with summed cell values.\nTHE DIGITAL ELEVATION MODEL (DEM)\nThe United States Geologic Survey\u2019s DEM is a popular raster file format due to widespread availability, the\nsimplicity of the model, and its extensive software support. Each pixel value in these grid-based DEMs denotes\n5 Chrisman, N. 2002. Exploring Geographic Information Systems. 2nd ed. New York: John Wiley and Sons.\n49 Chapter 9: Raster Data Models\nspot elevations on the ground, usually in feet or meters. Care must be taken when using grid-based DEMs due\nto the enormous volume of data that accompanies these files as the spatial extent covered in the image begins to\nincrease. DEMs are referred to as digital terrain models (DTMs) when they represent a simple, bare-earth model\nand as digital surface models (DSMs) when they include the heights of landscape features such as buildings and\ntrees.\nFrom the elevation data in each pixel of the raster DEM layer, you are able to produce output layers to portray\nslope (inclination), aspect (direction), and hillshading (Figure 9.5). These topographic functions are typical\nneighborhood processes; each pixel in the resultant layer is a product of its own elevation value as well as those\nof its surrounding neighbors.\n(cid:37)(cid:3) Slope layers exhibit the incline or steepness of the land. It is the change in elevation over a defined distance.\n(cid:37)(cid:3) Aspect is the compass direction in which a slope faces. From north, it is usually expressed clockwise from\n0 to 360 degrees.\n(cid:37)(cid:3) Hillshading, which is cartographically called shaded relief, is a lighting effect which mimics the sun to high-\nlight hills and valleys. Some areas appear to be illuminated while others lie in shadows.\nFigure 9.5. Topographic Functions. The DEM creates the slope, aspect, and hillshading layers.\nWhile these functions are raster processes, most can be mimicked in a vector environment by Triangulated\nIrregular Networks (TIN). In addition, topographic functions can derive vector isolines (contours). Source: GIS\nCommons (http:\/\/giscommons.org\/analysis\/ )\nCONNECTIVITY ANALYSIS\nConnectivity analyses use functions that accumulate values over an area traveled. Most often, these include the\nanalysis of surfaces and networks. Connectivity analyses include network analysis, spread functions, and vis-\nibility analysis. This group of analytical functions is the least developed in commercial GIS software, but this\nsituation is changing as commercial demand for these functions is increasing. Vector-based systems generally\nfocus on network analysis capabilities. Raster-based systems provide visibility analysis and sophisticated spread\nfunction capabilities.\nSPREAD FUNCTIONS (SURFACE ANALYSIS)\nSpread functions are raster analysis techniques that determine paths through space by considering how phe-\nnomena (including features) spread over an area in all directions but with different resistances. You begin with\nan origin or starting layer (a point where the path begins) and a friction layer, which represents how difficult\u2014\nhow much resistance\u2014it is for the phenomenon to pass through each cell. From these two layers, a new layer\n50 Chapter 9: Raster Data Models\nis formed that indicates how much resistance the phenomenon encounters as it spreads in all directions (Figure\n9.6).\nAdd a destination layer, and you can determine the \u201cleast cost\u201d path between the origin and the destina-\ntion. \u201cLeast cost\u201d can be a monetary cost, but it can also represent the time it takes to go from one point to\nanother, the environmental cost of using a route, or even the amount of effort (calories) that is spent.\nFigure 9.6. Spread Functions. This example shows that the shortest distance is not always the least cost distance.\nSource: GIS Commons. http:\/\/giscommons.org\/analysis\/\nVIEWSHED MODELING (INTERVISIBILITY ANALYSIS)\nViewshed modeling uses elevation layers to indicate areas on the map that can and cannot be viewed from a\nspecific vantage point. The non-obscured area is the viewshed. Viewsheds are developed from DEMs in ras-\nter-based systems and from TINs in vector systems. The ability to determine viewshed (and how they can be\naltered) is particularly useful to national and state park planners and landscape architects (Figure 9.7).\n51 Chapter 9: Raster Data Models\nFigure 9.7 Viewshed Analysis depicting the areas within a park where a proposed radio antenna can be seen.\nMap courtesy of the National Park Service, Department of Interior, 2007. Source: GIS Commons. http:\/\/gis-\ncommons.org\/analysis\/\nCORRELATION AND REGRESSION\nCorrelation and Regression are two ways to compute the degree of association between two (or sometimes more)\nlayers. With correlation, you do not assume a causal relationship. In other words, one layer is not affecting the\nspatial pattern of the other layer. The patterns may be similar, but no cause and effect is implied. Regression is\ndifferent; you make the assumption that one layer (and its variable) influences the other. You specify an inde-\npendent variable layer (sometimes more than one) that affects the dependent variable layer (Figure 9.9).\n52 Chapter 9: Raster Data Models\nFigure 9.9. Is there a spatial relationship between these two layers? Source: GIS Commons. http:\/\/giscommons.\norg\/analysis\/ - Correlation and regression tests allow you to overlay layers to test their spatial relationship.\nWith both statistical tests, you compute a correlation coefficient, which ranges from -1 to +1. Positive coeffi-\ncients indicate that the two layer\u2019s variables are associated in the same direction. As one variable increases, the\nother variable increases (both can simultaneously decrease too). The values closer to +1 describe a stronger\nassociation than those closer to zero. A negative coefficient depicts two layer\u2019s variables that are associated but\nin opposite directions. As one variable increases, the other variable decreases. Values closer to -1 have a strong\nnegative association. If the correlation coefficient is near zero, there is little to no association. Both of these pro-\ncesses are raster based.\nThis chapter material has been collected from the following web links that holds information with CC copy-\nrights: use and share alike.\nhttp:\/\/giscommons.org\/analysis\/\nhttp:\/\/2012books.lardbucket.org\/books\/geographic-information-system-basics\/s08-01-raster-data-models.html\nhttps:\/\/www.e-education.psu.edu\/geog160\/node\/1935\nhttp:\/\/2012books.lardbucket.org\/books\/geographic-information-system-basics\/s12-geospatial-analysis-ii-raster-.\nhtml\nDiscussion Questions\n1. In what ways does a raster analysis provide insights that may not be available through the vector data\nmodel?\n2. What is an urban and regional planning application where map algebra could provide insights helpful for\ncharacterizing a location?\n3. As a practicing planner, how might you integrate the advantages of vector and raster data models when\nengaging the public?\n53 Chapter 9: Raster Data Models\nContextual Applications of Chapter 9\nWhy Drivers Should Pay to Park on Residential Streets\nThe Difficulty of Mapping Transit \u2018Deserts\u2019\n54 CHAPTER 10: THE FUTURE OF GIS\nPlanning support systems (PSS) emerged in the 1980\u2019s to include a widely set of computer-based tools provid-\ning \u2018strategic support\u2019 to urban planners. By the 1990\u2019s with the availability of GIS PSS displaced the more rigid\n\u201csystems planning approach\u201d and were widely used in most of the stages of the technical planning processes.\nCurrently PPS are applied to several and diverse planning proposes mostly because of three aspects related to\nthe transformation of the urban planning field: (i) into a more fragmented and pluralistic field; (ii) from a rigid\nprofessionalism to collective negotiation, where the processes of communication to inform has become crucial;\nand (iii) widely access to a diverse and constantly evolving computer technologies, through the internet and the\nopen source movement.\nAmong these new generic (GIS software, build in modules) and specialized \u2018planning tools\u2019 technologies are: (i)\nhardware able to process increasing amounts of data; (ii) convergence of computers and communications; (iii)\nnew powerful microprocessors; (iv) computer simulation models (agent-based, disaggregated) with three dimen-\nsional visualization displays; (v) ability to communicate and interact among computers and participants using\nvisualization technologies (e.g. virtual reality theaters allowing public interaction).\nVisualization and communication technologies revolve around interactivity using the Web. The Web is orga-\nnized into four general styles: (i) vanilla-style Web pages that present information to users with no interactivity\nother than hyperlinking; (ii) Web pages that enable users to download data and software to their desktops; (iii)\nWeb-pages enabling users to run software within their own Web; and (iv) Web-pages enabling users to import\ntheir own data and run software remotely. There are also \u2018collaboratories\u2019 (online systems remotely linked that\nenable users to communicate with one another and run software jointly), which are growing in popularity.\nAlthough we are in the midst of a fragmentation of PSS tools, we can classify them into: (i) those serving the\ntechnical planning process (e.g., problem identification, goal setting, etc.); (ii) processes focus on providing\nopportunities for public participation (e.g. PP-GIS, 3-D virtual city models); (iii) those related to tasks (observ-\ning, measuring, predicting, etc.) related to how the city system is represented and manipulated (e.g., modeling\nand simulation). Among the computer packages developed to do it are: GIS, land use transportation models\n(LUTM), multi-criteria analysis (MCA), What if\/; (iv) fine scale disaggregated models (agent based); (v) tools\nfocus on either spatial\/non-spatial analysis or general\/specialist tasks; (vi) GIS toolbox (e.g., free mapping and\nvisualization software on the Web).\nAuthors used three examples to illustrate many of the features and characteristics of the PSS: (i) long term fore-\ncasting: visualizing land use and transportation scenarios in the Greater London, through modeling (ii) Immedi-\nate forecasting at the local level: visualizing the impact of air pollution using a virtual city model for the Greater\nLondon; and (iii) Describing and exploring spatial data: tools to enhance the understanding of urban problems.\nPlanning support systems (PSS) usually refers to a computer-based system that can integrate spatial mapping,\nanalysis and visualization, and further lead to operational and meaningful public decision making. This intro-\nduction chapter in the book, which originated from a conference on PSS organized by Lincoln Institute of Land\nPolicy, provide an overview of planning and decision support system. This chapter highlights the movement\nof urban planning over the past decades from a top-down, \u2018professionals know best\u2019 attitude to a participatory\napproach involving a broad spectrum of citizens, interest groups, and public officials. Today planners and public\nofficials interact with multiple communities and increasingly do so with digital technology. Therefore, the visual-\nization of models and processes becomes the central part of planners\u2019 toolbox.\nRead the book chapter\n55 "}