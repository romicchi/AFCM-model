{"text":"Computer Organization and Architecture\nVARDHAMAN COLLEGE OF ENGINEERING\n(AUTONOMOUS)\nShamshabad \u2013 501 218, Hyderabad\nDEPARTMENT OF INFORMATION TECHNOLOGY\nCOMPUTER\nORGANIZATION AND\nARCHITECTURE\nUnit \u2013 I (10 Lectures)\nPage 1 Computer Organization and Architecture\nSTRUCTURE OF COMPUTERS: Computer types, functional units, basic operational concepts,\nVon\u2010Neumann architecture, bus structures, software, performance, multiprocessors and\nmulticomputer\nBook: Carl Hamacher, Zvonks Vranesic, SafeaZaky (2002), Computer Organization, 5th\nedition, McGraw Hill: Unit-1 Pages: 1-23\nData representation, fixed and floating point and error detecting codes.\nBook: M. Moris Mano (2006), Computer System Architecture, 3rd edition, Pearson\/PHI,\nIndia: Unit-3 Pages: 67-91\nREGISTER TRANSFER AND MICRO\u2010OPERATIONS: Register transfer language, register\ntransfer, bus and memory transfers, arithmetic micro\u2010operations, logic micro\u2010operations,\nshift micro\u2010operations, arithmetic logic shift unit.\nBook: M. Moris Mano (2006), Computer System Architecture, 3rd edition, Pearson\/PHI,\nIndia: Unit-3 Pages: 93-118\nComputer Architecture:\nComputer Architecture deals with giving operational attributes of the computer or Processor\nto be specific. It deals with details like physical memory, ISA (Instruction Set Architecture) of\nthe processor, the number of bits used to represent the data types, Input Output mechanism\nand technique for addressing memories.\nComputer Organization:\nComputer Organization is realization of what is specified by the computer architecture .It\ndeals with how operational attributes are linked together to meet the requirements specified\nby computer architecture. Some organizational attributes are hardware details, control\nsignals, peripherals.\nEXAMPLE:\nSay you are in a company that manufactures cars, design and all low-level details of the car\ncome under computer architecture (abstract, programmers view), while making it\u2019s parts\npiece by piece and connecting together the different components of that car by keeping the\nbasic design in mind comes under computer organization (physical and visible).\nComputer Organization Computer Architecture\nPage 2 Computer Organization and Architecture\nComputer architecture (a\nOften called microarchitecture (low level)\nbit higher level)\nProgrammer view (i.e.\nTransparent from programmer (ex. a programmer does\nProgrammer has to be\nnot worry much how addition is implemented in\naware of which instruction\nhardware)\nset used)\nLogic (Instruction set,\nPhysical components (Circuit design, Adders, Signals,\nAddressing modes, Data\nPeripherals)\ntypes, Cache optimization)\nWhat to do ? (Instruction\nHow to do ? (implementation of the architecture)\nset)\nGENERATIONS OF A COMPUTER\nGeneration in computer terminology is a change in technology a computer is\/was being\nused. Initially, the generation term was used to distinguish between varying hardware\ntechnologies. But nowadays, generation includes both hardware and software, which\ntogether make up an entire computer system.\nThere are totally five computer generations known till date. Each generation has been\ndiscussed in detail along with their time period and characteristics. Here approximate dates\nagainst each generations have been mentioned which are normally accepted.\nFollowing are the main five generations of computers\nS.N. Generation & Description\nFirst Generation\n1\nThe period of first generation: 1946-1959. Vacuum tube based.\nSecond Generation\n2\nThe period of second generation: 1959-1965. Transistor based.\nThird Generation\n3\nThe period of third generation: 1965-1971. Integrated Circuit based.\nFourth Generation\n4\nThe period of fourth generation: 1971-1980. VLSI microprocessor based.\nFifth Generation\n5\nThe period of fifth generation: 1980-onwards. ULSI microprocessor based\nFirst generation\nPage 3 Computer Organization and Architecture\nThe period of first generation was 1946-1959. The computers of first generation used\nvacuum tubes as the basic components for memory and circuitry for CPU (Central Processing\nUnit). These tubes, like electric bulbs, produced a lot of heat and were prone to frequent\nfusing of the installations, therefore, were very expensive and could be afforded only by very\nlarge organizations. In this generation mainly batch processing operating system were used.\nPunched cards, paper tape, and magnetic tape were used as input and output devices. The\ncomputers in this generation used machine code as programming language.\nThe main features of first generation are:\n\uf0b7 Vacuum tube technology\n\uf0b7 Unreliable\n\uf0b7 Supported machine language only\n\uf0b7 Very costly\n\uf0b7 Generated lot of heat\n\uf0b7 Slow input and output devices\n\uf0b7 Huge size\n\uf0b7 Need of A.C.\n\uf0b7 Non-portable\n\uf0b7 Consumed lot of electricity\nSome computers of this generation were:\n\uf0b7 ENIAC\n\uf0b7 EDVAC\n\uf0b7 UNIVAC\n\uf0b7 IBM-701\n\uf0b7 IBM-650\nPage 4 Computer Organization and Architecture\nSecond generation\nThe period of second generation was 1959-1965. In this generation transistors were used\nthat were cheaper, consumed less power, more compact in size, more reliable and faster\nthan the first generation machines made of vacuum tubes. In this generation, magnetic cores\nwere used as primary memory and magnetic tape and magnetic disks as secondary storage\ndevices. In this generation assembly language and high-level programming languages like\nFORTRAN, COBOL were used. The computers used batch processing and multiprogramming\noperating system.\nThe main features of second generation are:\n\uf0b7 Use of transistors\n\uf0b7 Reliable in comparison to first generation computers\n\uf0b7 Smaller size as compared to first generation computers\n\uf0b7 Generated less heat as compared to first generation computers\n\uf0b7 Consumed less electricity as compared to first generation computers\n\uf0b7 Faster than first generation computers\n\uf0b7 Still very costly\n\uf0b7 A.C. needed\n\uf0b7 Supported machine and assembly languages\nSome computers of this generation were:\n\uf0b7 IBM 1620\n\uf0b7 IBM 7094\n\uf0b7 CDC 1604\n\uf0b7 CDC 3600\n\uf0b7 UNIVAC 1108\nThird generation\nPage 5 Computer Organization and Architecture\nThe period of third generation was 1965-1971. The computers of third generation used\nintegrated circuits (IC's) in place of transistors. A single IC has many transistors, resistors\nand capacitors along with the associated circuitry. The IC was invented by Jack Kilby. This\ndevelopment made computers smaller in size, reliable and efficient. In this generation\nremote processing, time-sharing, multi-programming operating system were used. High-\nlevel languages (FORTRAN-II TO IV, COBOL, PASCAL PL\/1, BASIC, ALGOL-68 etc.) were used\nduring this generation.\nThe main features of third generation are:\n\uf0b7 IC used\n\uf0b7 More reliable in comparison to previous two generations\n\uf0b7 Smaller size\n\uf0b7 Generated less heat\n\uf0b7 Faster\n\uf0b7 Lesser maintenance\n\uf0b7 Still costly\n\uf0b7 A.C needed\n\uf0b7 Consumed lesser electricity\n\uf0b7 Supported high-level language\nSome computers of this generation were:\n\uf0b7 IBM-360 series\n\uf0b7 Honeywell-6000 series\n\uf0b7 PDP(Personal Data Processor)\n\uf0b7 IBM-370\/168\n\uf0b7 TDC-316\nFourth generation\nPage 6 Computer Organization and Architecture\nThe period of fourth generation was 1971-1980. The computers of fourth generation used\nVery Large Scale Integrated (VLSI) circuits. VLSI circuits having about 5000 transistors and\nother circuit elements and their associated circuits on a single chip made it possible to have\nmicrocomputers of fourth generation. Fourth generation computers became more powerful,\ncompact, reliable, and affordable. As a result, it gave rise to personal computer (PC)\nrevolution. In this generation time sharing, real time, networks, distributed operating system\nwere used. All the high-level languages like C, C++, DBASE etc., were used in this generation.\nThe main features of fourth generation are:\n\uf0b7 VLSI technology used\n\uf0b7 Very cheap\n\uf0b7 Portable and reliable\n\uf0b7 Use of PC's\n\uf0b7 Very small size\n\uf0b7 Pipeline processing\n\uf0b7 No A.C. needed\n\uf0b7 Concept of internet was introduced\n\uf0b7 Great developments in the fields of networks\n\uf0b7 Computers became easily available\nSome computers of this generation were:\n\uf0b7 DEC 10\n\uf0b7 STAR 1000\n\uf0b7 PDP 11\n\uf0b7 CRAY-1(Super Computer)\n\uf0b7 CRAY-X-MP(Super Computer)\nFifth generation\nPage 7 Computer Organization and Architecture\nThe period of fifth generation is 1980-till date. In the fifth generation, the VLSI technology\nbecame ULSI (Ultra Large Scale Integration) technology, resulting in the production of\nmicroprocessor chips having ten million electronic components. This generation is based on\nparallel processing hardware and AI (Artificial Intelligence) software. AI is an emerging\nbranch in computer science, which interprets means and method of making computers think\nlike human beings. All the high-level languages like C and C++, Java, .Net etc., are used in this\ngeneration.\nAI includes:\n\uf0b7 Robotics\n\uf0b7 Neural Networks\n\uf0b7 Game Playing\n\uf0b7 Development of expert systems to make decisions in real life situations.\n\uf0b7 Natural language understanding and generation.\nThe main features of fifth generation are:\n\uf0b7 ULSI technology\n\uf0b7 Development of true artificial intelligence\n\uf0b7 Development of Natural language processing\n\uf0b7 Advancement in Parallel Processing\n\uf0b7 Advancement in Superconductor technology\n\uf0b7 More user friendly interfaces with multimedia features\n\uf0b7 Availability of very powerful and compact computers at cheaper rates\nSome computer types of this generation are:\n\uf0b7 Desktop\n\uf0b7 Laptop\n\uf0b7 NoteBook\n\uf0b7 UltraBook\nPage 8 Computer Organization and Architecture\n\uf0b7 ChromeBook\nCOMPUTER TYPES\nClassification based on Operating Principles\nBased on the operating principles, computers can be classified into one of the following types:\n-\n1) Digital Computers\n2) Analog Computers\n3) Hybrid Computers\nDigital Computers: - Operate essentially by counting. All quantities are expressed as\ndiscrete or numbers. Digital computers are useful for evaluating arithmetic expressions and\nmanipulations of data (such as preparation of bills, ledgers, solution of simultaneous\nequations etc).\nAnalog Computers:- An analog computer is a form of computer that uses the continuously\nchangeable aspects of physical phenomena such as electrical, mechanical, or\nhydraulic quantities to model the problem being solved. In contrast, digital\ncomputers represent varying quantities symbolically, as their numerical values change.\nHybrid Computers:- are computers that exhibit features of analog\ncomputers and digital computers. The digital component normally serves as the controller\nPage 9 Computer Organization and Architecture\nand provides logical operations, while the analog component normally serves as a solver\nof differential equations.\nClassification digital Computer based on size and Capability\nBased on size and capability, computers are broadly classified into\nMicro Computers(Personal Computer)\nA microcomputer is the smallest general purpose processing system. The older pc started 8\nbit processor with speed of 3.7MB and current pc 64 bit processor with speed of 4.66 GB.\nExamples: - IBM PCs, APPLE computers\nMicrocomputer can be classified into 2 types:\n1. Desktops\n2. Portables\nThe difference is portables can be used while travelling whereas desktops computers cannot\nbe carried around.\nThe different portable computers are: -\n1) Laptop\n2) Notebooks\n3) Palmtop (hand held)\n4) Wearable computers\nLaptop: - this computer is similar to a desktop computers but the size is smaller. They are\nexpensive than desktop. The weight of laptop is around 3 to 5 kg.\nPage 10 Computer Organization and Architecture\nNotebook: - These computers are as powerful as desktop but size of these computers are\ncomparatively smaller than laptop and desktop. They weigh 2 to 3 kg. They are more costly\nthan laptop.\nPalmtop (Hand held): - They are also called as personal Digital Assistant (PDA). These\ncomputers are small in size. They can be held in hands. It is capable of doing word processing,\nspreadsheets and hand writing recognition, game playing, faxing and paging. These\ncomputers are not as powerful as desktop computers. Ex: - 3com palmV.\nWearable computer: - The size of this computer is very small so that it can be worn on the\nbody. It has smaller processing power. It is used in the field of medicine. For example pace\nmaker to correct the heart beats. Insulin meter to find the levels of insulin in the blood.\nWorkstations:- It is used in large, high-resolution graphics screen built in network support,\nEngineering applications(CAD\/CAM), software development desktop publishing\nPage 11 Computer Organization and Architecture\nEx: Unix and windows NT.\nb) Minicomputer: - A minicomputer is a medium-sized computer. That is more\npowerful than a microcomputer. These computers are usually designed to serve multiple\nusers simultaneously (Parallel Processing). They are more expensive than microcomputers.\nExamples: Digital Alpha, Sun Ultra.\nc) Mainframe (Enterprise) computers: - Computers with large storage capacities and\nvery high speed of processing (compared to mini- or microcomputers) are known as\nmainframe computers. They support a large number of terminals for simultaneous use by a\nnumber of users like ATM transactions. They are also used as central host computers in\ndistributed data processing system.\nExamples: - IBM 370, S\/390.\nd) Supercomputer: - Supercomputers have extremely large storage capacity and\ncomputing speeds which are many times faster than other computers. A supercomputer is\nmeasured in terms of tens of millions Instructions per second (mips), an operation is made up\nof numerous instructions. The supercomputer is mainly used for large scale numerical\nproblems in scientific and engineering disciplines such as Weather analysis.\nExamples: - IBM Deep Blue\nPage 12 Computer Organization and Architecture\nClassification based on number of microprocessors\nBased on the number of microprocessors, computers can be classified into\na) Sequential computers and\nb) Parallel computers\na) Sequential computers: - Any task complete in sequential computers is with one\nmicrocomputer only. Most of the computers (today) we see are sequential computers where\nin any task is completed sequentially instruction after instruction from the beginning to the\nend.\nb) Parallel computers: - The parallel computer is relatively fast. New types of computers\nthat use a large number of processors. The processors perform different tasks independently\nand simultaneously thus improving the speed of execution of complex programs dramatically.\nParallel computers match the speed of supercomputers at a fraction of the cost.\nClassification based on word-length\nA binary digit is called \u201cBIT\u201d. A word is a group of bits which is fixed for a computer.\nThe number of bits in a word (or word length) determines the representation of all characters\nin these many bits. Word length leis in the range from 16-bit to 64-bitsf or most computers of\ntoday.\nPage 13 Computer Organization and Architecture\nClassification based on number of users\nBased on number of users, computers are classified into: -\nSingle User: - Only one user can use the resource at any time.\nMulti User: - A single computer shared by a number of users at any time.\nNetwork: - A number of interconnected autonomous computers shared by a number\nof users at any time.\nPage 14 Computer Organization and Architecture\nCOMPUTER TYPES\nA computer can be defined as a fast electronic calculating machine that accepts the\n(data) digitized input information process it as per the list of internally stored instructions\nand produces the resulting information. List of instructions are called programs & internal\nstorage is called computer memory.\nThe different types of computers are\n1. Personal computers: - This is the most common type found in homes, schools,\nBusiness offices etc., It is the most common type of desk top computers with\nprocessing and storage units along with various input and output devices.\n2. Note book computers: - These are compact and portable versions of PC\n3. Work stations: - These have high resolution input\/output (I\/O) graphics capability,\nbut with same dimensions as that of desktop computer. These are used in engineering\napplications of interactive design work.\n4. Enterprise systems: - These are used for business data processing in medium to large\ncorporations that require much more computing power and storage capacity than\nwork stations. Internet associated with servers have become a dominant worldwide\nsource of all types of information.\n5. Super computers: - These are used for large scale numerical calculations required in\nthe applications like weather forecasting etc.,\nPage 15 Computer Organization and Architecture\nBASIC TERMINOLOGY\n\u2022Input: Whatever is put into a computer system.\n\u2022Data: Refers to the symbols that represent facts, objects, or ideas.\n\u2022Information: The results of the computer storing data as bits and bytes; the words, umbers,\nsounds, and graphics.\n\u2022Output: Consists of the processing results produced by a computer.\n\u2022Processing: Manipulation of the data in many ways.\n\u2022Memory: Area of the computer that temporarily holds data waiting to be processed, stored,\nor output.\n\u2022Storage: Area of the computer that holds data on a permanent basis when it is not\nimmediately needed for processing.\n\u2022Assembly language program (ALP) \u2013Programs are written using mnemonics\n\u2022Mnemonic \u2013Instruction will be in the form of English like form\n\u2022Assembler \u2013is a software which converts ALP to MLL (Machine Level Language)\n\u2022HLL (High Level Language) \u2013Programs are written using English like statements\n\u2022Compiler -Convert HLL to MLL, does this job by reading source program at once\n\u2022Interpreter \u2013Converts HLL to MLL, does this job statement by statement\n\u2022System software \u2013Program routines which aid the user in the execution of programs eg:\nAssemblers, Compilers\n\u2022Operating system \u2013Collection of routines responsible for controlling and coordinating all\nthe activities in a computer system\n# Computers has two kinds of components:\nHardware, consisting of its physical devices (CPU, memory, bus, storage devices, ...)\nSoftware, consisting of the programs it has (Operating system, applications, utilities, ...)\nFUNCTIONAL UNIT\nA computer consists of five functionally independent main parts input, memory,\narithmetic logic unit (ALU), output and control unit.\nPage 16 Computer Organization and Architecture\nFunctional units of computer\nInput device accepts the coded information as source program i.e. high level\nlanguage. This is either stored in the memory or immediately used by the processor to\nperform the desired operations. The program stored in the memory determines the\nprocessing steps. Basically the computer converts one source program to an object program.\ni.e. into machine language.\nFinally the results are sent to the outside world through output device. All of these\nactions are coordinated by the control unit.\nInput unit: -\nPage 17 Computer Organization and Architecture\nThe source program\/high level language program\/coded information\/simply data is\nfed to a computer through input devices keyboard is a most common type. Whenever a key is\npressed, one corresponding word or number is translated into its equivalent binary code\nover a cable & fed either to memory or processor.\nJoysticks, trackballs, mouse, scanners etc are other input devices.\nMemory unit: -\nIts function into store programs and data. It is basically to two types\n1. Primary memory\n2. Secondary memory\nWord:\nIn computer architecture, a word is a unit of data of a defined bit length that can be addressed\nand moved between storage and the computer processor. Usually, the defined bit length of a\nword is equivalent to the width of the computer's data bus so that a word can be moved in a\nsingle operation from storage to a processor register. For any computer architecture with an\neight-bit byte, the word will be some multiple of eight bits. In IBM's evolutionary\nSystem\/360 architecture, a word is 32 bits, or four contiguous eight-bit bytes. In Intel's PC\nprocessor architecture, a word is 16 bits, or two contiguous eight-bit bytes. A word can\ncontain a computer instruction, a storage address, or application data that is to be\nmanipulated (for example, added to the data in another word space).\nThe number of bits in each word is known as word length. Word length refers to the\nnumber of bits processed by the CPU in one go. With modern general purpose computers,\nword size can be 16 bits to 64 bits.\nThe time required to access one word is called the memory access time. The small, fast,\nRAM units are called caches. They are tightly coupled with the processor and are often\ncontained on the same IC chip to achieve high performance.\nPage 18 Computer Organization and Architecture\n1. Primary memory: - Is the one exclusively associated with the processor and operates at\nthe electronics speeds programs must be stored in this memory while they are being\nexecuted. The memory contains a large number of semiconductors storage cells. Each\ncapable of storing one bit of information. These are processed in a group of fixed site called\nword.\nTo provide easy access to a word in memory, a distinct address is associated with\neach word location. Addresses are numbers that identify memory location.\nNumber of bits in each word is called word length of the computer. Programs must\nreside in the memory during execution. Instructions and data can be written into the\nmemory or read out under the control of processor. Memory in which any location can be\nreached in a short and fixed amount of time after specifying its address is called random-\naccess memory (RAM).\nThe time required to access one word in called memory access time. Memory which is\nonly readable by the user and contents of which can\u2019t be altered is called read only memory\n(ROM) it contains operating system.\nPage 19 Computer Organization and Architecture\nCaches are the small fast RAM units, which are coupled with the processor and are\noften contained on the same IC chip to achieve high performance. Although primary storage\nis essential it tends to be expensive.\n2 Secondary memory: - Is used where large amounts of data & programs have to be stored,\nparticularly information that is accessed infrequently.\nExamples: - Magnetic disks & tapes, optical disks (ie CD-ROM\u2019s), floppies etc.,\nArithmetic logic unit (ALU):-\nMost of the computer operators are executed in ALU of the processor like addition,\nsubtraction, division, multiplication, etc. the operands are brought into the ALU from\nmemory and stored in high speed storage elements called register. Then according to the\ninstructions the operation is performed in the required sequence.\nThe control and the ALU are may times faster than other devices connected to a\ncomputer system. This enables a single processor to control a number of external devices\nsuch as key boards, displays, magnetic and optical disks, sensors and other mechanical\ncontrollers.\nOutput unit:-\nThese actually are the counterparts of input unit. Its basic function is to send the\nprocessed results to the outside world.\nExamples:- Printer, speakers, monitor etc.\nControl unit:-\nIt effectively is the nerve center that sends signals to other units and senses their\nstates. The actual timing signals that govern the transfer of data between input unit,\nprocessor, memory and output unit are generated by the control unit.\nBASIC OPERATIONAL CONCEPTS\nPage 20 Computer Organization and Architecture\nTo perform a given task an appropriate program consisting of a list of instructions is stored\nin the memory. Individual instructions are brought from the memory into the processor,\nwhich executes the specified operations. Data to be stored are also stored in the memory.\nExamples: - Add LOCA, R\n0\nThis instruction adds the operand at memory location LOCA, to operand in register R\n0\n& places the sum into register. This instruction requires the performance of several steps,\n1. First the instruction is fetched from the memory into the processor.\n2. The operand at LOCA is fetched and added to the contents of R\n0\n3. Finally the resulting sum is stored in the register R\n0\nThe preceding add instruction combines a memory access operation with an ALU\nOperations. In some other type of computers, these two types of operations are performed by\nseparate instructions for performance reasons.\nLoad LOCA, R1\nAdd R1, R0\nTransfers between the memory and the processor are started by sending the address\nof the memory location to be accessed to the memory unit and issuing the appropriate control\nsignals. The data are then transferred to or from the memory.\nThe fig shows how memory &\nPage 21 Computer Organization and Architecture\nthe processor can be connected. In addition to the ALU & the control circuitry, the processor\ncontains a number of registers used for several different purposes.\nRegister:\nIt is a special, high-speed storage area within the CPU. All data must be represented in\na register before it can be processed. For example, if two numbers are to be multiplied, both\nnumbers must be in registers, and the result is also placed in a register. (The register can\ncontain the address of a memory location where data is stored rather than the actual data\nitself.)\nThe number of registers that a CPU has and the size of each (number of bits) help\ndetermine the power and speed of a CPU. For example a 32-bit CPU is one in which each\nregister is 32 bits wide. Therefore, each CPU instruction can manipulate 32 bits of\ndata. In high-level languages, the compiler is responsible for translating high-level operations\ninto low-level operations that access registers.\nInstruction Format:\nComputer instructions are the basic components of a machine language program. They are\nalso known as macro operations, since each one is comprised of sequences of micro\noperations.\nEach instruction initiates a sequence of micro operations that fetch operands from registers\nor memory, possibly perform arithmetic, logic, or shift operations, and store results in\nregisters or memory.\nPage 22 Computer Organization and Architecture\nInstructions are encoded as binary instruction codes. Each instruction code contains of\na operation code, or opcode, which designates the overall purpose of the instruction (e.g. add,\nsubtract, move, input, etc.). The number of bits allocated for the opcode determined how\nmany different instructions the architecture supports.\nIn addition to the opcode, many instructions also contain one or more operands, which\nindicate where in registers or memory the data required for the operation is located. For\nexample, and add instruction requires two operands, and a not instruction requires one.\n15 12 11 6 5 0\n+-----------------------------------+\n| Opcode | Operand | Operand |\n+-----------------------------------+\nThe opcode and operands are most often encoded as unsigned binary numbers in order to\nminimize the number of bits used to store them. For example, a 4-bit opcode encoded as a\nbinary number could represent up to 16 different operations.\nThe control unit is responsible for decoding the opcode and operand bits in the instruction\nregister, and then generating the control signals necessary to drive all other hardware in the\nCPU to perform the sequence of micro operations that comprise the instruction.\nINSTRUCTION CYCLE:\nPage 23 Computer Organization and Architecture\nThe instruction register (IR):- Holds the instructions that are currently being executed. Its\noutput is available for the control circuits which generates the timing signals that control the\nvarious processing elements in one execution of instruction.\nThe program counter PC:-\nThis is another specialized register that keeps track of execution of a program. It\ncontains the memory address of the next instruction to be fetched and executed.\nBesides IR and PC, there are n-general purpose registers R0 through R .\nn-1\nThe other two registers which facilitate communication with memory are: -\n1. MAR \u2013 (Memory Address Register):- It holds the address of the location to be\naccessed.\n2. MDR \u2013 (Memory Data Register):- It contains the data to be written into or read out\nof the address location.\nOperating steps are\n1. Programs reside in the memory & usually get these through the I\/P unit.\n2. Execution of the program starts when the PC is set to point at the first instruction of\nthe program.\n3. Contents of PC are transferred to MAR and a Read Control Signal is sent to the\nmemory.\nPage 24 Computer Organization and Architecture\n4. After the time required to access the memory elapses, the address word is read out of\nthe memory and loaded into the MDR.\n5. Now contents of MDR are transferred to the IR & now the instruction is ready to be\ndecoded and executed.\n6. If the instruction involves an operation by the ALU, it is necessary to obtain the\nrequired operands.\n7. An operand in the memory is fetched by sending its address to MAR & Initiating a\nread cycle.\n8. When the operand has been read from the memory to the MDR, it is transferred from\nMDR to the ALU.\n9. After one or two such repeated cycles, the ALU can perform the desired operation.\n10. If the result of this operation is to be stored in the memory, the result is sent to MDR.\n11. Address of location where the result is stored is sent to MAR & a write cycle is\ninitiated.\n12. The contents of PC are incremented so that PC points to the next instruction that is to\nbe executed.\nNormal execution of a program may be preempted (temporarily interrupted) if some\ndevices require urgent servicing, to do this one device raises an Interrupt signal. An interrupt\nis a request signal from an I\/O device for service by the processor. The processor provides\nthe requested service by executing an appropriate interrupt service routine.\nThe Diversion may change the internal stage of the processor its state must be saved\nin the memory location before interruption. When the interrupt-routine service is completed\nthe state of the processor is restored so that the interrupted program may continue\nTHE VON NEUMANN ARCHITECTURE\nThe task of entering and altering programs for the ENIAC was extremely tedious. The\nprogramming process can be easy if the program could be represented in a form suitable for\nstoring in memory alongside the data. Then, a computer could get its instructions by reading\nthem from memory, and a program could be set or altered by setting the values of a portion of\nmemory. This idea is known a the stored-program concept. The first publication of the idea\nPage 25 Computer Organization and Architecture\nwas in a 1945 proposal by von Neumann for a new computer, the EDVAC (Electronic Discrete\nVariable Computer).\nIn 1946, von Neumann and his colleagues began the design of a new stored-program\ncomputer, referred to as the IAS computer, at the Princeton Institute for Advanced Studies.\nThe IAS computer, although not completed until 1952, is the prototype of all subsequent\ngeneral-purpose computers.\nIt consists of\n\uf076 A main memory, which stores both data and instruction\n\uf076 An arithmetic and logic unit (ALU) capable of operating on binary data\n\uf076 A control unit, which interprets the instructions in memory and causes them to be\nexecuted\n\uf076 Input and output (I\/O) equipment operated by the control unit\nBUS STRUCTURES:\nBus structure and multiple bus structures are types of bus or computing. A bus is basically a\nsubsystem which transfers data between the components of Computer components either\nwithin a computer or between two computers. It connects peripheral devices at the same\ntime.\nPage 26 Computer Organization and Architecture\n- A multiple Bus Structure has multiple inter connected service integration buses and for each\nbus the other buses are its foreign buses. A Single bus structure is very simple and consists of\na single server.\n- A bus cannot span multiple cells. And each cell can have more than one buses. - Published\nmessages are printed on it. There is no messaging engine on Single bus structure\nI) In single bus structure all units are connected in the same bus than connecting different\nbuses as multiple bus structure.\nII) Multiple bus structure's performance is better than single bus structure. Iii)single bus\nstructure's cost is cheap than multiple bus structure.\nGroup of lines that serve as connecting path for several devices is called a bus (one bit per\nline).\nIndividual parts must communicate over a communication line or path for exchanging\ndata, address and control information as shown in the diagram below. Printer example \u2013\nprocessor to printer. A common approach is to use the concept of buffer registers to hold the\ncontent during the transfer.\nBuffer registers hold the data during the data transfer temporarily. Ex: printing\nTypes of Buses:\n1. Data Bus:\nData bus is the most common type of bus. It is used to transfer data between different\ncomponents of computer. The number of lines in data bus affects the speed of data transfer\nbetween different components. The data bus consists of 8, 16, 32, or 64 lines. A 64-line data\nbus can transfer 64 bits of data at one time.\nPage 27 Computer Organization and Architecture\nThe data bus lines are bi-directional. It means that:\nCPU can read data from memory using these lines CPU can write data to memory locations\nusing these lines\n2. Address Bus:\nMany components are connected to one another through buses. Each component is assigned a\nunique ID. This ID is called the address of that component. It a component wants to\ncommunicate with another component, it uses address bus to specify the address of that\ncomponent. The address bus is a unidirectional bus. It can carry information only in one\ndirection. It carries address of memory location from microprocessor to the main memory.\n3. Control Bus:\nControl bus is used to transmit different commands or control signals from one component to\nanother component. Suppose CPU wants to read data from main memory. It will use control is\nalso used to transmit control signals like ASKS (Acknowledgement signals). A control signal\ncontains the following:\n1 Timing information: It specifies the time for which a device can use data and address bus.\n2 Command Signal: It specifies the type of operation to be performed.\nSuppose that CPU gives a command to the main memory to write data. The memory sends\nacknowledgement signal to CPU after writing the data successfully. CPU receives the signal\nand then moves to perform some other action.\nSOFTWARE\nIf a user wants to enter and run an application program, he\/she needs a System Software.\nSystem Software is a collection of programs that are executed as needed to perform functions\nsuch as:\n\u2022 Receiving and interpreting user commands\n\u2022 Entering and editing application programs and storing then as files in secondary storage\ndevices\n\u2022 Running standard application programs such as word processors, spread sheets, games\netc\u2026\nOperating system - is key system software component which helps the user to exploit the\nbelow underlying hardware with the programs.\nPage 28 Computer Organization and Architecture\nTypes of software\nA layer structure showing where Operating System is located on generally used software\nsystems on desktops\nSystem software\nSystem software helps run the computer hardware and computer system. It includes a\ncombination of the following:\n\uf0b7 device drivers\n\uf0b7 operating systems\n\uf0b7 servers\n\uf0b7 utilities\n\uf0b7 windowing systems\n\uf0b7 compilers\n\uf0b7 debuggers\n\uf0b7 interpreters\n\uf0b7 linkers\nThe purpose of systems software is to unburden the applications programmer from the often\ncomplex details of the particular computer being used, including such accessories as\ncommunications devices, printers, device readers, displays and keyboards, and also to\npartition the computer's resources such as memory and processor time in a safe and stable\nmanner. Examples are- Windows XP, Linux and Mac.\nApplication software\nApplication software allows end users to accomplish one or more specific (not directly\ncomputer development related) tasks. Typical applications include:\nonal software\nPage 29 Computer Organization and Architecture\nApplication software exists for and has impacted a wide variety of topics.\nPERFORMANCE\nThe most important measure of the performance of a computer is how quickly it can\nexecute programs. The speed with which a computer executes program is affected by the\ndesign of its hardware. For best performance, it is necessary to design the compiles, the\nmachine instruction set, and the hardware in a coordinated way.\nThe total time required to execute the program is elapsed time is a measure of the\nperformance of the entire computer system. It is affected by the speed of the processor, the\ndisk and the printer. The time needed to execute a instruction is called the processor time.\nJust as the elapsed time for the execution of a program depends on all units in a\ncomputer system, the processor time depends on the hardware involved in the execution of\nindividual machine instructions. This hardware comprises the processor and the memory\nwhich are usually connected by the bus as shown in the fig c.\nThe pertinent parts of the fig. c are repeated in fig. d which includes the cache\nmemory as part of the processor unit.\nPage 30 Computer Organization and Architecture\nLet us examine the flow of program instructions and data between the memory and\nthe processor. At the start of execution, all program instructions and the required data are\nstored in the main memory. As the execution proceeds, instructions are fetched one by one\nover the bus into the processor, and a copy is placed in the cache later if the same instruction\nor data item is needed a second time, it is read directly from the cache.\nThe processor and relatively small cache memory can be fabricated on a single IC\nchip. The internal speed of performing the basic steps of instruction processing on chip is\nvery high and is considerably faster than the speed at which the instruction and data can be\nfetched from the main memory. A program will be executed faster if the movement of\ninstructions and data between the main memory and the processor is minimized, which is\nachieved by using the cache.\nFor example:- Suppose a number of instructions are executed repeatedly over a short period\nof time as happens in a program loop. If these instructions are available in the cache, they can\nbe fetched quickly during the period of repeated use. The same applies to the data that are\nused repeatedly.\nProcessor clock: -\nProcessor circuits are controlled by a timing signal called clock. The clock designer\nthe regular time intervals called clock cycles. To execute a machine instruction the processor\ndivides the action to be performed into a sequence of basic steps that each step can be\ncompleted in one clock cycle. The length P of one clock cycle is an important parameter that\naffects the processor performance.\nProcessor used in today\u2019s personal computer and work station have a clock rates that\nrange from a few hundred million to over a billion cycles per second.\nBasic performance equation\nWe now focus our attention on the processor time component of the total elapsed\ntime. Let \u2018T\u2019 be the processor time required to execute a program that has been prepared in\nsome high-level language. The compiler generates a machine language object program that\nPage 31 Computer Organization and Architecture\ncorresponds to the source program. Assume that complete execution of the program requires\nthe execution of N machine cycle language instructions. The number N is the actual number\nof instruction execution and is not necessarily equal to the number of machine cycle\ninstructions in the object program. Some instruction may be executed more than once, which\nin the case for instructions inside a program loop others may not be executed all, depending\non the input data used.\nSuppose that the average number of basic steps needed to execute one machine cycle\ninstruction is S, where each basic step is completed in one clock cycle. If clock rate is \u2018R\u2019\ncycles per second, the program execution time is given by\nT=N*S\/R\nthis is often referred to as the basic performance equation.\nWe must emphasize that N, S & R are not independent parameters changing one may\naffect another. Introducing a new feature in the design of a processor will lead to improved\nperformance only if the overall result is to reduce the value of T.\nPipelining and super scalar operation: -\nWe assume that instructions are executed one after the other. Hence the value of S is\nthe total number of basic steps, or clock cycles, required to execute one instruction. A\nsubstantial improvement in performance can be achieved by overlapping the execution of\nsuccessive instructions using a technique called pipelining.\nConsider Add R R R\n1 2 3\nThis adds the contents of R & R and places the sum into R .\n1 2 3\nThe contents of R & R are first transferred to the inputs of ALU. After the addition\n1 2\noperation is performed, the sum is transferred to R . The processor can read the next\n3\ninstruction from the memory, while the addition operation is being performed. Then of that\ninstruction also uses, the ALU, its operand can be transferred to the ALU inputs at the same\ntime that the add instructions is being transferred to R .\n3\nIn the ideal case if all instructions are overlapped to the maximum degree possible\nthe execution proceeds at the rate of one instruction completed in each clock cycle.\nPage 32 Computer Organization and Architecture\nIndividual instructions still require several clock cycles to complete. But for the purpose of\ncomputing T, effective value of S is 1.\nA higher degree of concurrency can be achieved if multiple instructions pipelines are\nimplemented in the processor. This means that multiple functional units are used creating\nparallel paths through which different instructions can be executed in parallel with such an\narrangement, it becomes possible to start the execution of several instructions in every clock\ncycle. This mode of operation is called superscalar execution. If it can be sustained for a long\ntime during program execution the effective value of S can be reduced to less than one. But\nthe parallel execution must preserve logical correctness of programs that is the results\nproduced must be same as those produced by the serial execution of program instructions.\nNow days many processors are designed in this manner.\nClock rate\nThese are two possibilities for increasing the clock rate \u2018R\u2019.\n1. Improving the IC technology makes logical circuit faster, which reduces the time of\nexecution of basic steps. This allows the clock period P, to be reduced and the clock\nrate R to be increased.\n2. Reducing the amount of processing done in one basic step also makes it possible to\nreduce the clock period P. however if the actions that have to be performed by an\ninstructions remain the same, the number of basic steps needed may increase.\nIncrease in the value \u2018R\u2019 that are entirely caused by improvements in IC technology\naffects all aspects of the processor\u2019s operation equally with the exception of the time it takes\nto access the main memory. In the presence of cache the percentage of accesses to the main\nmemory is small. Hence much of the performance gain excepted from the use of faster\ntechnology can be realized.\nInstruction set CISC & RISC:-\nSimple instructions require a small number of basic steps to execute. Complex\ninstructions involve a large number of steps. For a processor that has only simple instruction\na large number of instructions may be needed to perform a given programming task. This\ncould lead to a large value of \u2018N\u2019 and a small value of \u2018S\u2019 on the other hand if individual\ninstructions perform more complex operations, a fewer instructions will be needed, leading\nPage 33 Computer Organization and Architecture\nto a lower value of N and a larger value of S. It is not obvious if one choice is better than the\nother.\nBut complex instructions combined with pipelining (effective value of S \u00bf 1) would\nachieve one best performance. However, it is much easier to implement efficient pipelining in\nprocessors with simple instruction sets.\nRISC and CISC are computing systems developed for computers. Instruction set or\ninstruction set architecture is the structure of the computer that provides commands to the\ncomputer to guide the computer for processing data manipulation. Instruction set consists of\ninstructions, addressing modes, native data types, registers, interrupt, exception handling and\nmemory architecture. Instruction set can be emulated in software by using an interpreter or\nbuilt into hardware of the processor. Instruction Set Architecture can be considered as a\nboundary between the software and hardware. Classification of microcontrollers and\nmicroprocessors can be done based on the RISC and CISC instruction set architecture.\nComparison between RISC and CISC:\nRISC CISC\nIt stands for \u2018Reduced It stands for \u2018Complex\nAcronym\nInstruction Set Computer\u2019. Instruction Set Computer\u2019.\nThe RISC processors have a The CISC processors have a\nDefinition smaller set of instructions with larger set of instructions with\nfew addressing nodes. many addressing nodes.\nIt has no memory unit and uses It has a memory unit to\nMemory unit a separate hardware to implement complex\nimplement instructions. instructions.\nIt has a hard-wired unit of It has a micro-programming\nProgram\nprogramming. unit.\nDesign It is a complex complier design. It is an easy complier design.\nThe calculations are faster and The calculations are slow and\nCalculations\nprecise. precise.\nDecoding of instructions is Decoding of instructions is\nDecoding\nsimple. complex.\nTime Execution time is very less. Execution time is very high.\nExternal It does not require external It requires external memory\nPage 34 Computer Organization and Architecture\nmemory memory for calculations. for calculations.\nPipelining does function Pipelining does not function\nPipelining\ncorrectly. correctly.\nStalling is mostly reduced in\nStalling The processors often stall.\nprocessors.\nCode expansion can be a Code expansion is not a\nCode expansion\nproblem. problem.\nDisc space The space is saved. The space is wasted.\nUsed in high end applications\nUsed in low end applications\nsuch as video processing,\nApplications such as security systems, home\ntelecommunications and image\nautomations, etc.\nprocessing.\nPage 35 Computer Organization and Architecture\n1.8 Performance measurements\nThe performance measure is the time taken by the computer to execute a given bench\nmark. Initially some attempts were made to create artificial programs that could be used as\nbench mark programs. But synthetic programs do not properly predict the performance\nobtained when real application programs are run.\nA non-profit organization called SPEC- system performance Evaluation Corporation\nselects and publishes bench marks.\nThe program selected range from game playing, compiler, and data base applications\nto numerically intensive programs in astrophysics and quantum chemistry. In each case, the\nprogram is compiled under test, and the running time on a real computer is measured. The\nsame program is also compiled and run on one computer selected as reference.\nThe \u2018SPEC\u2019 rating is computed as follows.\nSPEC rating = Running time on the reference computer\/ Running time on the computer under\ntest\nMULTIPROCESSORS AND MULTICOMPUTER\nPage 36 Computer Organization and Architecture\nmulticomputer multiprocessors\n1. A computer made up of several computers. 1. A computer that has more than one CPU on\n2. Distributed computing deals with hardware its motherboard.\nand software systems containing more than 2. Multiprocessing is the use of two or more\none processing element, multiple programs central processing units (CPUs) within a\n3. It can run faster single computer system.\n4. A multi-computer is multiple computers, 3. Speed depends on the all processors speed\neach of which can have multiple processors. 4. Single Computer with multiple processors\n5. Used for true parallel processing. 5. Used for true parallel processing.\n6. Processor can not share the memory. 6. Processors can share the memory.\n7. Called as message passing multi computers 7. Called as shared memory multi processors\n8. Cost is more 8. Cost is low\nData Representation:\nPage 37 Computer Organization and Architecture\nRegisters are made up of flip-flops and flip-flops are two-state devices that can store only 1\u2019s\nand 0\u2019s.\nThere are many methods or techniques which can be used to convert numbers from one\nbase to another. We'll demonstrate here the following \u2212\n\uf0b7 Decimal to Other Base System\n\uf0b7 Other Base System to Decimal\n\uf0b7 Other Base System to Non-Decimal\n\uf0b7 Shortcut method \u2212 Binary to Octal\n\uf0b7 Shortcut method \u2212 Octal to Binary\n\uf0b7 Shortcut method \u2212 Binary to Hexadecimal\n\uf0b7 Shortcut method \u2212 Hexadecimal to Binary\nDecimal to Other Base System\nSteps\n\uf0b7 Step 1 \u2212 Divide the decimal number to be converted by the value of the new base.\n\uf0b7 Step 2 \u2212 Get the remainder from Step 1 as the rightmost digit (least significant digit)\nof new base number.\n\uf0b7 Step 3 \u2212 Divide the quotient of the previous divide by the new base.\n\uf0b7 Step 4 \u2212 Record the remainder from Step 3 as the next digit (to the left) of the new\nbase number.\nRepeat Steps 3 and 4, getting remainders from right to left, until the quotient becomes zero\nin Step 3.\nThe last remainder thus obtained will be the Most Significant Digit (MSD) of the new base\nnumber.\nExample \u2212\nDecimal Number: 29\n10\nCalculating Binary Equivalent \u2212\nPage 38 Computer Organization and Architecture\nStep Operation Result Remainder\nStep 1 29 \/ 2 14 1\nStep 2 14 \/ 2 7 0\nStep 3 7 \/ 2 3 1\nStep 4 3 \/ 2 1 1\nStep 5 1 \/ 2 0 1\nAs mentioned in Steps 2 and 4, the remainders have to be arranged in the reverse order so\nthat the first remainder becomes the Least Significant Digit (LSD) and the last remainder\nbecomes the Most Significant Digit (MSD).\nDecimal Number \u2212 29 = Binary Number \u2212 11101 .\n10 2\nOther Base System to Decimal System\nSteps\n\uf0b7 Step 1 \u2212 Determine the column (positional) value of each digit (this depends on the\nposition of the digit and the base of the number system).\n\uf0b7 Step 2 \u2212 Multiply the obtained column values (in Step 1) by the digits in the\ncorresponding columns.\n\uf0b7 Step 3 \u2212 Sum the products calculated in Step 2. The total is the equivalent value in\ndecimal.\nStep Binary Number Decimal Number\nStep 1 11101 ((1 \u00d7 24) + (1 \u00d7 23) + (1 \u00d7 22) + (0 \u00d7 21) + (1 \u00d7 20))\n2 10\nStep 2 11101 (16 + 8 + 4 + 0 + 1)\n2 10\nStep 3 11101 29\n2 10\nExample\nBinary Number \u2212 11101\n2\nCalculating Decimal Equivalent \u2212\nBinary Number \u2212 11101 = Decimal Number \u2212 29\n2 10\nOther Base System to Non-Decimal System\nSteps\nPage 39 Computer Organization and Architecture\n\uf0b7 Step 1 \u2212 Convert the original number to a decimal number (base 10).\n\uf0b7 Step 2 \u2212 Convert the decimal number so obtained to the new base number.\nExample\nOctal Number \u2212 25\n8\nCalculating Binary Equivalent \u2212\nStep 1 \u2212 Convert to Decimal\nStep Octal Number Decimal Number\nStep 1 25 ((2 \u00d7 81) + (5 \u00d7 80))\n8 10\nStep 2 25 (16 + 5 )\n8 10\nStep 3 25 21\n8 10\nOctal Number \u2212 25 = Decimal Number \u2212 21\n8 10\nStep 2 \u2212 Convert Decimal to Binary\nStep Operation Result Remainder\nStep 1 21 \/ 2 10 1\nStep 2 10 \/ 2 5 0\nStep 3 5 \/ 2 2 1\nStep 4 2 \/ 2 1 0\nStep 5 1 \/ 2 0 1\nDecimal Number \u2212 21 = Binary Number \u2212 10101\n10 2\nOctal Number \u2212 25 = Binary Number \u2212 10101\n8 2\nShortcut method - Binary to Octal\nSteps\n\uf0b7 Step 1 \u2212 Divide the binary digits into groups of three (starting from the right).\n\uf0b7 Step 2 \u2212 Convert each group of three binary digits to one octal digit.\nExample\nBinary Number \u2212 10101\n2\nCalculating Octal Equivalent \u2212\nPage 40 Computer Organization and Architecture\nStep Binary Number Octal Number\nStep 1 10101 010 101\n2\nStep 2 10101 2 5\n2 8 8\nStep 3 10101 25\n2 8\nBinary Number \u2212 10101 = Octal Number \u2212 25\n2 8\nShortcut method - Octal to Binary\nSteps\n\uf0b7 Step 1 \u2212 Convert each octal digit to a 3 digit binary number (the octal digits may be\ntreated as decimal for this conversion).\n\uf0b7 Step 2 \u2212 Combine all the resulting binary groups (of 3 digits each) into a single binary\nnumber.\nExample\nOctal Number \u2212 25\n8\nCalculating Binary Equivalent \u2212\nStep Octal Number Binary Number\nStep 1 25 2 5\n8 10 10\nStep 2 25 010 101\n8 2 2\nStep 3 25 010101\n8 2\nOctal Number \u2212 25 = Binary Number \u2212 10101\n8 2\nShortcut method - Binary to Hexadecimal\nSteps\n\uf0b7 Step 1 \u2212 Divide the binary digits into groups of four (starting from the right).\n\uf0b7 Step 2 \u2212 Convert each group of four binary digits to one hexadecimal symbol.\nExample\nBinary Number \u2212 10101\n2\nCalculating hexadecimal Equivalent \u2212\nStep Binary Number Hexadecimal Number\nStep 1 10101 0001 0101\n2\nPage 41 Computer Organization and Architecture\nStep 2 10101 1 5\n2 10 10\nStep 3 10101 15\n2 16\nBinary Number \u2212 10101 = Hexadecimal Number \u2212 15\n2 16\nShortcut method - Hexadecimal to Binary\nSteps\n\uf0b7 Step 1 \u2212 Convert each hexadecimal digit to a 4 digit binary number (the hexadecimal\ndigits may be treated as decimal for this conversion).\n\uf0b7 Step 2 \u2212 Combine all the resulting binary groups (of 4 digits each) into a single binary\nnumber.\nExample\nHexadecimal Number \u2212 15\n16\nCalculating Binary Equivalent \u2212\nStep Hexadecimal Number Binary Number\nStep 1 15 1 5\n16 10 10\nStep 2 15 0001 0101\n16 2 2\nStep 3 15 00010101\n16 2\nHexadecimal Number \u2212 15 = Binary Number \u2212 10101\n16 2\nBinary Coded Decimal (BCD) code\nIn this code each decimal digit is represented by a 4-bit binary number. BCD is a way to\nexpress each of the decimal digits with a binary code. In the BCD, with four bits we can\nrepresent sixteen numbers (0000 to 1111). But in BCD code only first ten of these are used\n(0000 to 1001). The remaining six code combinations i.e. 1010 to 1111 are invalid in BCD.\nAdvantages of BCD Codes\n\uf0b7 It is very similar to decimal system.\nPage 42 Computer Organization and Architecture\n\uf0b7 We need to remember binary equivalent of decimal numbers 0 to 9 only.\nDisadvantages of BCD Codes\n\uf0b7 The addition and subtraction of BCD have different rules.\n\uf0b7 The BCD arithmetic is little more complicated.\n\uf0b7 BCD needs more number of bits than binary to represent the decimal number. So BCD\nis less efficient than binary.\nAlphanumeric codes\nA binary digit or bit can represent only two symbols as it has only two states '0' or '1'. But\nthis is not enough for communication between two computers because there we need many\nmore symbols for communication. These symbols are required to represent 26 alphabets\nwith capital and small letters, numbers from 0 to 9, punctuation marks and other symbols.\nThe alphanumeric codes are the codes that represent numbers and alphabetic\ncharacters. Mostly such codes also represent other characters such as symbol and various\ninstructions necessary for conveying information. An alphanumeric code should at least\nrepresent 10 digits and 26 letters of alphabet i.e. total 36 items. The following three\nalphanumeric codes are very commonly used for the data representation.\n\uf0b7 American Standard Code for Information Interchange (ASCII).\n\uf0b7 Extended Binary Coded Decimal Interchange Code (EBCDIC).\n\uf0b7 Five bit Baudot Code.\nASCII code is a 7-bit code whereas EBCDIC is an 8-bit code. ASCII code is more commonly\nused worldwide while EBCDIC is used primarily in large IBM computers.\nComplement Arithmetic\nComplements are used in the digital computers in order to simplify the subtraction\noperation and for the logical manipulations. For each radix-r system (radix r represents base\nof number system) there are two types of complements.\nS.N. Complement Description\n1 Radix Complement The radix complement is referred to as the r's\ncomplement\n2 Diminished Radix Complement The diminished radix complement is referred\nPage 43 Computer Organization and Architecture\nto as the (r-1)'s complement\nBinary system complements\nAs the binary system has base r = 2. So the two types of complements for the binary system\nare 2's complement and 1's complement.\n1's complement\nThe 1's complement of a number is found by changing all 1's to 0's and all 0's to 1's. This is\ncalled as taking complement or 1's complement. Example of 1's Complement is as follows.\n2's complement\nThe 2's complement of binary number is obtained by adding 1 to the Least Significant Bit\n(LSB) of 1's complement of the number.\n2's complement = 1's complement + 1\nExample of 2's Complement is as follows.\nBinary Arithmetic\nBinary arithmetic is essential part of all the digital computers and many other digital system.\nPage 44 Computer Organization and Architecture\nBinary Addition\nIt is a key for binary subtraction, multiplication, division. There are four rules of binary\naddition.\nIn fourth case, a binary addition is creating a sum of (1 + 1 = 10) i.e. 0 is written in the given\ncolumn and a carry of 1 over to the next column.\nExample \u2212 Addition\nBinary Subtraction\nSubtraction and Borrow, these two words will be used very frequently for the binary\nsubtraction. There are four rules of binary subtraction.\nExample \u2212 Subtraction\nBinary Multiplication\nPage 45 Computer Organization and Architecture\nBinary multiplication is similar to decimal multiplication. It is simpler than decimal\nmultiplication because only 0s and 1s are involved. There are four rules of binary\nmultiplication.\nExample \u2212 Multiplication\nBinary Division\nBinary division is similar to decimal division. It is called as the long division procedure.\nExample \u2212 Division\nSubtraction by 1\u2019s Complement\nIn subtraction by 1\u2019s complement we subtract two binary numbers using carried by 1\u2019s\ncomplement.\nPage 46 Computer Organization and Architecture\nThe steps to be followed in subtraction by 1\u2019s complement are:\ni) To write down 1\u2019s complement of the subtrahend.\nii) To add this with the minuend.\niii) If the result of addition has a carry over then it is dropped and an 1 is added in the last bit.\niv) If there is no carry over, then 1\u2019s complement of the result of addition is obtained to get the\nfinal result and it is negative.\nEvaluate:\n(i) 110101 \u2013 100101\nSolution:\n1\u2019s complement of 10011 is 011010. Hence\nMinued - 1 1 0 1 0 1\n1\u2019s complement of subtrahend - 0 1 1 0 1 0\nCarry over - 1 0 0 1 1 1 1\n1\n0 1 0 0 0 0\nThe required difference is 10000\n(ii) 101011 \u2013 111001\nSolution:\n1\u2019s complement of 111001 is 000110. Hence\nMinued - 1 0 1 0 1 1\n1\u2019s complement - 0 0 0 1 1 0\n1 1 0 0 0 1\nHence the difference is \u2013 1 1 1 0\n(iii) 1011.001 \u2013 110.10\nSolution:\n1\u2019s complement of 0110.100 is 1001.011 Hence\nMinued - 1 0 1 1 . 0 0 1\n1\u2019s complement of subtrahend - 1 0 0 1 . 0 1 1\nCarry over - 1 0 1 0 0 . 1 0 0\nPage 47 Computer Organization and Architecture\n1\n0 1 0 0 . 1 0 1\nHence the required difference is 100.101\n(iv) 10110.01 \u2013 11010.10\nSolution:\n1\u2019s complement of 11010.10 is 00101.01\n1 0 1 1 0 . 0 1\n0 0 1 0 1 . 0 1\n1 1 0 1 1 . 1 0\nHence the required difference is \u2013 00100.01 i.e. \u2013 100.01\nSubtraction by 2\u2019s Complement\nWith the help of subtraction by 2\u2019s complement method we can easily subtract two binary\nnumbers.\nThe operation is carried out by means of the following steps:\n(i) At first, 2\u2019s complement of the subtrahend is found.\n(ii) Then it is added to the minuend.\n(iii) If the final carry over of the sum is 1, it is dropped and the result is positive.\n(iv) If there is no carry over, the two\u2019s complement of the sum will be the result and it is\nnegative.\nThe following examples on subtraction by 2\u2019s complement will make the\nprocedure clear:\nEvaluate:\n(i) 110110 - 10110\nSolution:\nThe numbers of bits in the subtrahend is 5 while that of minuend is 6. We make the number of\nbits in the subtrahend equal to that of minuend by taking a `0\u2019 in the sixth place of the\nsubtrahend.\nNow, 2\u2019s complement of 010110 is (101101 + 1) i.e.101010. Adding this with the minuend.\n1 1 0 1 1 0 Minuend\n1 0 1 0 1 0 2\u2019s complement of subtrahend\nPage 48 Computer Organization and Architecture\nCarry over 1 1 0 0 0 0 0 Result of addition\nAfter dropping the carry over we get the result of subtraction to be 100000.\n(ii) 10110 \u2013 11010\nSolution:\n2\u2019s complement of 11010 is (00101 + 1) i.e. 00110. Hence\nMinued - 1 0 1 1 0\n2\u2019s complement of subtrahend - 0 0 1 1 0\nResult of addition - 1 1 1 0 0\nAs there is no carry over, the result of subtraction is negative and is obtained by writing the 2\u2019s\ncomplement of 11100 i.e.(00011 + 1) or 00100.\nHence the difference is \u2013 100.\n(iii) 1010.11 \u2013 1001.01\nSolution:\n2\u2019s complement of 1001.01 is 0110.11. Hence\nMinued - 1 0 1 0 . 1 1\n2\u2019s complement of subtrahend - 0 1 1 0 . 1 1\nCarry over 1 0 0 0 1 . 1 0\nAfter dropping the carry over we get the result of subtraction as 1.10.\n(iv) 10100.01 \u2013 11011.10\nSolution:\n2\u2019s complement of 11011.10 is 00100.10. Hence\nMinued - 1 0 1 0 0 . 0 1\n2\u2019s complement of subtrahend - 0 1 1 0 0 . 1 0\nResult of addition - 1 1 0 0 0 . 1 1\nAs there is no carry over the result of subtraction is negative and is obtained by writing the 2\u2019s\ncomplement of 11000.11.\nHence the required result is \u2013 00111.01.\nPage 49 Computer Organization and Architecture\nError Detection & Correction\nWhat is Error?\nError is a condition when the output information does not match with the input information.\nDuring transmission, digital signals suffer from noise that can introduce errors in the binary\nbits travelling from one system to other. That means a 0 bit may change to 1 or a 1 bit may\nchange to 0.\nError-Detecting codes\nWhenever a message is transmitted, it may get scrambled by noise or data may get\ncorrupted. To avoid this, we use error-detecting codes which are additional data added to a\ngiven digital message to help us detect if an error occurred during transmission of the\nmessage. A simple example of error-detecting code is parity check.\nError-Correcting codes\nAlong with error-detecting code, we can also pass some data to figure out the original\nmessage from the corrupt message that we received. This type of code is called an error-\ncorrecting code. Error-correcting codes also deploy the same strategy as error-detecting\ncodes but additionally, such codes also detect the exact location of the corrupt bit.\nIn error-correcting codes, parity check has a simple way to detect errors along with a\nsophisticated mechanism to determine the corrupt bit location. Once the corrupt bit is\nlocated, its value is reverted (from 0 to 1 or 1 to 0) to get the original message.\nHow to Detect and Correct Errors?\nTo detect and correct the errors, additional bits are added to the data bits at the time of\ntransmission.\n\uf0b7 The additional bits are called parity bits. They allow detection or correction of the\nerrors.\n\uf0b7 The data bits along with the parity bits form a code word.\nPage 50 Computer Organization and Architecture\nParity Checking of Error Detection\nIt is the simplest technique for detecting and correcting errors. The MSB of an 8-bits word is\nused as the parity bit and the remaining 7 bits are used as data or message bits. The parity of\n8-bits transmitted word can be either even parity or odd parity.\nEven parity -- Even parity means the number of 1's in the given word including the parity\nbit should be even (2,4,6,....).\nOdd parity -- Odd parity means the number of 1's in the given word including the parity bit\nshould be odd (1,3,5,....).\nUse of Parity Bit\nThe parity bit can be set to 0 and 1 depending on the type of the parity required.\n\uf0b7 For even parity, this bit is set to 1 or 0 such that the no. of \"1 bits\" in the entire word is\neven. Shown in fig. (a).\n\uf0b7 For odd parity, this bit is set to 1 or 0 such that the no. of \"1 bits\" in the entire word is\nodd. Shown in fig. (b).\nHow Does Error Detection Take Place?\nParity checking at the receiver can detect the presence of an error if the parity of the receiver\nsignal is different from the expected parity. That means, if it is known that the parity of the\ntransmitted signal is always going to be \"even\" and if the received signal has an odd parity,\nthen the receiver can conclude that the received signal is not correct. If an error is detected,\nPage 51 Computer Organization and Architecture\nthen the receiver will ignore the received byte and request for retransmission of the same\nbyte to the transmitter.\nPage 52 Computer Organization and Architecture\nUNIT \u2013 II (12 Lectures)\nBASIC COMPUTER ORGANIZATION AND DESIGN: Instruction codes, computer\nregisters, computer instructions, instruction cycle, timing and control,\nmemory\u2010reference instructions, input\u2010output and interrupt.\nBook: M. Moris Mano (2006), Computer System Architecture, 3rd edition, Pearson\/PHI,\nIndia: Unit-5 Pages: 123-157\nCentral processing unit: stack organization, instruction formats, addressing modes,\ndata transfer and manipulation, program control, reduced instruction set computer\n(RISC).\nBook: M. Moris Mano (2006), Computer System Architecture, 3rd edition, Pearson\/PHI,\nIndia: Unit-8 Pages: 241-297\nInstruction Codes\nComputer instructions are the basic components of a machine language program. They\nare also known as macro operations, since each one is comprised of sequences of\nmicro operations. Each instruction initiates a sequence of micro operations that fetch\noperands from registers or memory, possibly perform arithmetic, logic, or shift\noperations, and store results in registers or memory.\nInstructions are encoded as binary instruction codes. Each instruction code\ncontains of a operation code, or opcode, which designates the overall purpose of the\ninstruction (e.g. add, subtract, move, input, etc.). The number of bits allocated for the\nopcode determined how many different instructions the architecture supports.\nIn addition to the opcode, many instructions also contain one or more operands,\nwhich indicate where in registers or memory the data required for the operation is\nlocated. For example, and add instruction requires two operands, and a not instruction\nrequires one.\n15 12 11 6 5 0\n+-----------------------------------+\nPage 53 Computer Organization and Architecture\n| Opcode | Operand | Operand |\n+-----------------------------------+\nThe opcode and operands are most often encoded as unsigned binary numbers in\norder to minimize the number of bits used to store them. For example, a 4-bit opcode\nencoded as a binary number could represent up to 16 different operations.\nThe control unit is responsible for decoding the opcode and operand bits in the\ninstruction register, and then generating the control signals necessary to drive all\nother hardware in the CPU to perform the sequence of microoperations that comprise\nthe instruction.\nBasic Computer Instruction Format:\nThe Basic Computer has a 16-bit instruction code similar to the examples described\nabove. It supports direct and indirect addressing modes.\nHow many bits are required to specify the addressing mode?\n15 14 12 11 0\n+------------------+\n| I | OP | ADDRESS |\n+------------------+\nI = 0: direct\nI = 1: indirect\nComputer Instructions\nAll Basic Computer instruction codes are 16 bits wide. There are 3 instruction code\nformats:\nMemory-reference instructions take a single memory address as an operand, and\nhave the format:\n15 14 12 11 0\n+-------------------+\n| I | OP | Address |\n+-------------------+\nPage 54 Computer Organization and Architecture\nIf I = 0, the instruction uses direct addressing. If I = 1, addressing in indirect.\nHow many memory-reference instructions can exist?\nRegister-reference instructions operate solely on the AC register, and have the\nfollowing format:\n15 14 12 11 0\n+------------------+\n| 0 | 111 | OP |\n+------------------+\nHow many register-reference instructions can exist? How many memory-\nreference instructions can coexist with register-reference instructions?\nInput\/output instructions have the following format:\n15 14 12 11 0\n+------------------+\n| 1 | 111 | OP |\n+------------------+\nHow many I\/O instructions can exist? How many memory-reference\ninstructions can coexist with register-reference and I\/O instructions?\nTiming and Control\nAll sequential circuits in the Basic Computer CPU are driven by a master clock, with\nthe exception of the INPR register. At each clock pulse, the control unit sends control\nsignals to control inputs of the bus, the registers, and the ALU.\nControl unit design and implementation can be done by two general methods:\n\uf0b7 A hardwired control unit is designed from scratch using traditional digital logic\ndesign techniques to produce a minimal, optimized circuit. In other words, the\ncontrol unit is like an ASIC (application-specific integrated circuit).\nPage 55 Computer Organization and Architecture\n\uf0b7 A micro-programmed control unit is built from some sort of ROM. The desired\ncontrol signals are simply stored in the ROM, and retrieved in sequence to drive\nthe micro operations needed by a particular instruction.\nMicro programmed control:\nMicro programmed control is a control mechanism to generate control signals by\nusing a memory called control storage (CS), which contains the control\nsignals. Although micro programmed control seems to be advantageous to CISC\nmachines, since CISC requires systematic development of sophisticated control\nsignals, there is no intrinsic difference between these 2 control mechanisms.\nHard-wired control:\nHardwired control is a control mechanism to generate control signals by using\nappropriate finite state machine (FSM). The pair of \"microinstruction-register\" and\n\"control storage address register\" can be regarded as a \"state register\" for the\nhardwired control. Note that the control storage can be regarded as a kind of\ncombinational logic circuit. We can assign any 0, 1 values to each output\ncorresponding to each address, which can be regarded as the input for a\ncombinational logic circuit. This is a truth table.\nPage 56 Computer Organization and Architecture\nInstruction Cycle\nIn this chapter, we examine the sequences of micro operations that the Basic\nComputer goes through for each instruction. Here, you should begin to understand\nhow the required control signals for each state of the CPU are determined, and how\nthey are generated by the control unit.\nThe CPU performs a sequence of micro operations for each instruction. The sequence\nfor each instruction of the Basic Computer can be refined into 4 abstract phases:\n1. Fetch instruction\n2. Decode\n3. Fetch operand\n4. Execute\nProgram execution can be represented as a top-down design:\n1. Program execution\na. Instruction 1\ni. Fetch instruction\nii. Decode\niii. Fetch operand\niv. Execute\nb. Instruction 2\ni. Fetch instruction\nii. Decode\niii. Fetch operand\niv. Execute\nc. Instruction 3 ...\nPage 57 Computer Organization and Architecture\nProgram execution begins with:\nPC \u2190 address of first instruction, SC \u2190 0\nAfter this, the SC is incremented at each clock cycle until an instruction is completed,\nand then it is cleared to begin the next instruction. This process repeats until a HLT\ninstruction is executed, or until the power is shut off.\nInstruction Fetch and Decode\nThe instruction fetch and decode phases are the same for all instructions, so the\ncontrol functions and micro operations will be independent of the instruction code.\nEverything that happens in this phase is driven entirely by timing variables T , T and\n0 1\nT . Hence, all control inputs in the CPU during fetch and decode are functions of these\n2\nthree variables alone.\nT : AR \u2190 PC\n0\nT : IR \u2190 M[AR], PC \u2190 PC + 1\n1\nT : D \u2190 decoded IR(12-14), AR \u2190 IR(0-11), I \u2190 IR(15)\n2 0-7\nFor every timing cycle, we assume SC \u2190 SC + 1 unless it is stated that SC \u2190 0.\nThe operation D \u2190 decoded IR(12-14) is not a register transfer like most of our\n0-7\nmicro operations, but is actually an inevitable consequence of loading a value into the\nIR register. Since the IR outputs 12-14 are directly connected to a decoder, the outputs\nof that decoder will change as soon as the new values of IR(12-14) propagate through\nthe decoder.\nNote that incrementing the PC at time T assumes that the next instruction is at\n1\nthe next address. This may not be the case if the current instruction is a branch\ninstruction. However, performing the increment here will save time if the next\ninstruction immediately follows, and will do no harm if it doesn't. The incremented PC\nvalue is simply overwritten by branch instructions.\nIn hardware development, unlike serial software development, it is often\nadvantageous to perform work that may not be necessary. Since we can perform\nmultiple micro operations at the same time, we might was well do everything\nthat might be useful at the earliest possible time. Likewise, loading AR with the\nPage 58 Computer Organization and Architecture\naddress field from IR at T is only useful if the instruction is a memory-reference\n2\ninstruction. We won't know this until T , but there is no reason to wait since there is\n3\nno harm in loading AR immediately.\nInput-Output and Interrupt\nHardware Summary\nThe Basic Computer I\/O consists of a simple terminal with a keyboard and a\nprinter\/monitor.\nThe keyboard is connected serially (1 data wire) to the INPR register. INPR is a shift\nregister capable of shifting in external data from the keyboard one bit at a time. INPR\noutputs are connected in parallel to the ALU.\nShift enable\n|\nv\n+-----------+ 1 +-------+\n| Keyboard |---\/-->| INPR <|--- serial I\/O clock\n+-----------+ +-------+\n|\n\/ 8\n| | |\nv v v\n+---------------+\n| ALU |\n+---------------+\n|\n\/ 16\n|\nv\n+---------------+\n| AC <|--- CPU master clock\n+---------------+\nPage 59 Computer Organization and Architecture\nHow many CPU clock cycles are needed to transfer a character from the keyboard to\nthe INPR register? (tricky)\nAre the clock pulses provided by the CPU master clock?\nRS232, USB, Firewire are serial interfaces with their own clock independent of the\nCPU. ( USB speed is independent of processor speed. )\n\uf0b7 RS232: 115,200 kbps (some faster)\n\uf0b7 USB: 11 mbps\n\uf0b7 USB2: 480 mbps\n\uf0b7 FW400: 400 mbps\n\uf0b7 FW800: 800 mbps\n\uf0b7 USB3: 4.8 gbps\nOUTR inputs are connected to the bus in parallel, and the output is connected serially\nto the terminal. OUTR is another shift register, and the printer\/monitor receives an\nend-bit during each clock pulse.\nI\/O Operations\nSince input and output devices are not under the full control of the CPU (I\/O events\nare asynchronous), the CPU must somehow be told when an input device has new\ninput ready to send, and an output device is ready to receive more output. The FGI flip-\nflop is set to 1 after a new character is shifted into INPR. This is done by the I\/O\ninterface, not by the control unit. This is an example of an asynchronous input event\n(not synchronized with or controlled by the CPU).\nThe FGI flip-flop must be cleared after transferring the INPR to AC. This must be\ndone as a micro operation controlled by the CU, so we must include it in the CU design.\nThe FGO flip-flop is set to 1 by the I\/O interface after the terminal has finished\ndisplaying the last character sent. It must be cleared by the CPU after transferring a\ncharacter into OUTR. Since the keyboard controller only sets FGI and the CPU only\nclears it, a JK flip-flop is convenient:\n+-------+\nKeyboard controller --->| J Q |----->\n| | |\nPage 60 Computer Organization and Architecture\n+--------\\-----\\ | |\n) or >----->|> FGI |\n+--------\/-----\/ | |\n| | |\nCPU-------------------->| K |\n+-------\nHow do we control the CK input on the FGI flip-flop? (Assume leading-edge\ntriggering.)\nThere are two common methods for detecting when I\/O devices are ready,\nnamely software polling and interrupts. These two methods are discussed in the\nfollowing sections.\nStack Organization\nStack is the storage method of the items in which the last item included is the first one\nto be removed\/taken from the stack. Generally a stack in the computer is\na memory unit with an address register and the register holding the address of the\nstack is known as the Stack Pointer (SP). A stack performs Insertion and Deletion\noperation, were the operation of inserting an item is known as Push and operation of\ndeleting an item is known as Pop. Both Push and Pop operation results in\nincrementing and decrementing the stack pointer respectively.\nRegister Stack\nRegister or memory words can be organized to form a stack. The stack pointer is\na register that holds the memory address of the top of the stack. When an item need\nto be deleted from the stack, item on the top of the stack is deleted and the stack\npointer is decremented. Similarly, when an item needs to be added, the stack pointer is\nincremented and writing the word at the position indicated by the stack pointer. There\nare two 1 bit register; FULL and EMTY that are used for describing the\nstack overflow and underflow conditions. Following micro-operations are performed\nduring inserting and deleting an item in\/from the stack.\nInsert:\nSP <- SP + 1 \/\/ Increment the stack pointer to point the next higher address\/\/\nPage 61 Computer Organization and Architecture\nM[SP] <- DR \/\/ Write the item on the top of the stack\/\/\nIf (SP = 0) then (Full <- 1) \/\/ Check overflow condition \/\/\nEMTY <- 0 \/\/ Mark that the stack is not empty \/\/\nDelete:\nDR <- M[SP] \/\/Read an item from the top of the stack\/\/\nSP <- SP 1 \/\/Decrement the stack pointer \/\/\nIf (SP = 0) then (EMTY <- 1) \/\/Check underflow condition \/\/\nFULL <- 0 \/\/Mark that the stack is not full \/\/\nGet all the resource regarding the homework help and assignment help at\nTranstutors.com. With our team of experts, we are capable of providing homework\nhelp and assignment help for all levels. With us you can be rest assured the all the\ncontent provided for homework help and assignment help will be original and\nplagiarism free.\nRegister Stack:-\nA stack can be placed in a portion of a large memory as it can be organized as a\ncollection of a finite number of memory words as register.\nPage 62 Computer Organization and Architecture\nIn a 64- word stack, the stack pointer contains 6 bits because 26 = 64.\nThe one bit register FULL is set to 1 when the stack is full, and the one-bit register\nEMTY is set to 1 when the stack is empty. DR is the data register that holes the binary\ndata to be written into on read out of the stack. Initially, SP is decide to O, EMTY is set\nto 1, FULL = 0, so that SP points to the word at address O and the stack is masked\nempty and not full.\nPUSH SP \u00ae SP + 1 increment stack pointer\nM [SP] \u00ae DR unit item on top of the Stack\nIt (SP = 0) then (FULL \u00ae 1) check it stack is full\nEMTY \u00ae 0 mask the stack not empty.\nPOP DR \u00ae [SP] read item trans the top of stack\nSP \u00ae SP \u20131 decrement SP\nIt (SP = 0) then (EMTY \u00ae 1) check it stack is empty\nFULL \u00ae 0 mark the stack not full.\nA stack can be placed in a portion of a large memory or it can be organized as\na collection of a finite number of memory words or registers. Figure X shows the\norganization of a 64-word register stack. The stack pointer register SP contains a\nbinary number whose value is equal to the address of the word that is currently on\ntop of the stack.\nThree items are placed in the stack: A, B, and C in the order. item C is\non the top of the stack so that the content of sp is now 3. To remove the top item, the\nstack is popped by reading the memory word at address 3 and decrementing the\ncontent of SP. Item B is now on top of the stack since SP holds address 2. To insert a\nnew item, the stack is pushed by incrementing SP and writing a word in the next\nhigher location in the stack. Note that item C has read out but not physically removed.\nThis does not matter because when the stack is pushed, a new item is written in its\nplace.\nIn a 64-word stack, the stack pointer contains 6 bits because 26=64. since SP\nhas only six bits, it cannot exceed a number greater than 63(111111 in binary). When\nPage 63 Computer Organization and Architecture\n63 is incremented by 1, the result is 0 since 111111 + 1 =1000000 in binary, but SP\ncan accommodate only the six least significant bits. Similarly, when 000000 is\ndecremented by 1, the result is 111111. The one bit register Full is set to 1 when the\nstack is full, and the one-bit register EMTY is set to 1 when the stack is empty of\nitems. DR is the data register that holds the binary data to be written in to or read out\nof the stack.\nInitially, SP is cleared to 0, Emty is set to 1, and Full is cleared to 0, so that SP\npoints to the word at address o and the stack is marked empty and not full. if the stack\nis not full , a new item is inserted with a push operation. the push operation is\nimplemented with the following sequence of micro-operation.\nSP \u2190SP + 1 (Increment stack pointer)\nM(SP) \u2190 DR (Write item on top of the stack)\nif (sp=0) then (Full \u2190 1) (Check if stack is full)\nEmty \u2190 0 ( Marked the stack not empty)\nThe stack pointer is incremented so that it points to the address of the next-higher\nword. A memory write operation inserts the word from DR into the top of the stack.\nNote that SP holds the address of the top of the stack and that M(SP) denotes the\nmemory word specified by the address presently available in SP, the first item stored\nin the stack is at address 1. The last item is stored at address 0, if SP reaches 0, the\nstack is full of item, so FULLL is set to 1.\nThis condition is reached if the top item prior to the last push was in location 63\nand after increment SP, the last item stored in location 0. Once an item is stored in\nlocation 0, there are no more empty register in the stack. If an item is written in the\nstack, obviously the stack cannot be empty, so EMTY is cleared to 0.\nDR\u2190 M[SP] Read item from the top of stack\nSP \u2190 SP-1 Decrement stack Pointer\nif( SP=0) then (Emty \u2190 1) Check if stack is empty\nFULL \u2190 0 Mark the stack not full\nPage 64 Computer Organization and Architecture\nThe top item is read from the stack into DR. The stack pointer is then decremented. if\nits value reaches zero, the stack is empty, so Empty is set to 1. This condition is\nreached if the item read was in location 1. Once this item is read out, SP is\ndecremented and reaches the value 0, which is the initial value of SP. Note that if a\npop operation reads the item from location 0 and then SP is decremented, SP changes\nto 111111, which is equal to decimal 63. In this configuration, the word in address 0\nreceives the last item in the stack. Note also that an erroneous operation will result if\nthe stack is pushed when FULL=1 or popped when EMTY =1.\nMemory Stack :\nA stack can exist as a stand-alone unit as in figure 4 or can be implemented in\na random access memory attached to CPU. The implementation of a stack in the CPU\nis done by assigning a portion of memory to a stack operation and using a processor\nregister as a stack pointer. Figure shows a portion of computer memory partitioned in\nto three segment program, data and stack. The program counter PC points at the\naddress of the next instruction in the program. The address register AR points at an\narray of data. The stack pointer SP points at the top of the stack. The three register\nare connected to a common address bus, and either one can provide an address for\nmemory. PC is used during the fetch phase to read an instruction. AR is used during\nthe execute phase to read an operand. SP is used to push or POP items into or from\nthe stack.\nAs show in figure :4 the initial value of SP is 4001 and the stack grows with\ndecreasing addresses. Thus the first item stored in the stack is at address 4000, the\nsecond item is stored at address 3999, and the last address hat can be used for the\nstack is 3000. No previous are available for stack limit checks.\nWe assume that the items in the stack communicate with a data register DR. A new\nitem is inserted with the push operation as follows.\nSP\u2190 SP-1\nM[SP] \u2190 DR\nPage 65 Computer Organization and Architecture\nThe stack pointer is decremented so that it points at the address of the next word. A\nMemory write operation insertion the word from DR into the top of the stack. A new\nitem is deleted with a pop operation as follows.\nDR\u2190 M[SP]\nSP\u2190SP + 1\nThe top item is read from the stack in to DR. The stack pointer is then incremented to\npoint at the next item in the stack. Most computers do not provide hardware to check\nfor stack overflow (FULL) or underflow (Empty). The stack limit can be checked by\nusing two processor register: one to hold upper limit and other hold the lower limit.\nAfter the pop or push operation SP is compared with lower or upper limit register.\nREVERSE POLISH NOTATION\nFor example: A x B + C x D is an arithmetical expression written in infix notation, here\nx (denotes multiplication). In this expression A and B are two operands and x is an\noperator, similarly C and D are two operands and x is an operator. In this expression +\nPage 66 Computer Organization and Architecture\nis another operator which is written between (A x B) and (C x D). Because of the\nprecedence of the operator multiplication is done first. The order of precedence is as:\n1. Exponentiation have precedence one.\n2. Multiplication and Division has precedence two.\n3. Addition and subtraction has precedence three.\nReverse polish notation is also known as postfix notation is defined as: In postfix\nnotation operator is written after the operands. Examples of postfix notation are AB+\nand CD-. Here A and B are two operands and the operator is written after these two\noperands. The conversion from infix expression into postfix expression is shown\nbelow.\n\uf0a7 Convert the infix notation A x B + C x D + E x F into postfix notation?\nSOLUTION\nA x B + C x D + E x F\n= [ABx] + [CDx] + [EFx]\n= [ABxCDx] + [EFx]\n= [ABxCDxEFx]\n= ABxCDxEFx\nSo the postfix notation is ABxCDxEFx.\n\uf0a7 Convert the infix notation {A \u2013 B + C x (D x E \u2013 F)} \/ G + H x K into postfix\nnotation?\n{A \u2013 B + C x (D x E \u2013 F)} \/ G + H x K\n= {A \u2013 B + C x ([DEx] \u2013 F)} \/ G + [HKx]\n= {A \u2013 B + C x [DExF-]} \/ [GHKx+]\n= {A \u2013 B + [CDExF-x]} \/ [GHKx+]\n= {[AB-] + [CDExF-x]} \/ [GHKx+]\n= [AB-CDExF-x+] \/ [GHKx+]\n= [AB-CDExF-x+GHKx+\/]\n= AB-CDExF-x+GHKx+\/\nSo the postfix notation is AB-CDExF-x+GHKx+\/.\nNow let\u2019s how to evaluate a postfix expression, the algorithm for the evaluation of\npostfix notation is shown below:\nPage 67 Computer Organization and Architecture\nALGORITHM:\n(Evaluation of Postfix notation) This algorithm finds the result of a postfix expression.\nStep1: Insert a symbol (say #) at the right end of the postfix expression.\nStep2: Scan the expression from left to right and follow the Step3 and Step4 for each of\nthe symbol encountered.\nStep3: if an element is encountered insert into stack.\nStep4: if an operator (say &) is encountered pop the top element A (say) and next to\ntop element B (say) perform the following operation x = B&A. Push x into the top of\nthe stack.\nStep5: if the symbol # is encountered then stop scanning.\n\uf0a7 Evaluate the post fix expression 50 4 3 x 2 \u2013 + 7 8 x 4 \/ -?\nSOLUTION\nPut symbol # at the right end of the expression: 50 4 3 x 2 \u2013 + 7 8 x 4 \/ \u2013 #.\nPostfix expression Symbol Stack\nscanned\n50 4 3 x 2 \u2013 + 7 8 x 4 \/ \u2013 # _ _\n4 3 x 2 \u2013 + 7 8 x 4 \/ \u2013 # 50 50\n3 x 2 \u2013 + 7 8 x 4 \/ \u2013 # 4 50, 4\nx 2 \u2013 + 7 8 x 4 \/ \u2013 # 3 50, 4, 3\n2 \u2013 + 7 8 x 4 \/ \u2013 # x 50, 12\n\u2013 + 7 8 x 4 \/ \u2013 # 2 50, 12, 2\n+ 7 8 x 4 \/ \u2013 # \u2013 50, 10\n7 8 x 4 \/ \u2013 # + 60\n8 x 4 \/ \u2013 # 7 60, 7\nx 4 \/ \u2013 # 8 60, 7, 8\n4 \/ \u2013 # x 60, 56\n\/ \u2013 # 4 60, 56, 4\n\u2013 # \/ 60, 14\nPage 68 Computer Organization and Architecture\n# \u2013 46\n_ # Result = 46\nINSTRUCTION FORMATS\nThe most common fields found in instruction format are:-\n(1) An operation code field that specified the operation to be performed\n(2) An address field that designates a memory address or a processor registers.\n(3) A mode field that specifies the way the operand or the effective address is\ndetermined.\nComputers may have instructions of several different lengths containing varying\nnumber of addresses. The number of address field in the instruction format of a\ncomputer depends on the internal organization of its registers. Most computers fall\ninto one of three types of CPU organization.\n(1) Single Accumulator organization ADD X AC \u00ae AC + M [\u00d7]\n(2) General Register Organization ADD R1, R2, R3 R \u00ae R2 + R3\n(3) Stack Organization PUSH X\nThree address Instruction\nComputer with three addresses instruction format can use each address field to\nspecify either processor register are memory operand.\nADD R1, A, B A1 \u00ae M [A] + M [B]\nADD R2, C, D R2 \u00ae M [C] + M [B] X = (A + B) * (C + A)\nMUL X, R1, R2 M [X] R1 * R2\nThe advantage of the three address formats is that it results in short program when\nevaluating arithmetic expression. The disadvantage is that the binary-coded\ninstructions require too many bits to specify three addresses.\nTwo Address Instruction\nPage 69 Computer Organization and Architecture\nMost common in commercial computers. Each address field can specify either a\nprocesses register on a memory word.\nMOV R1, A R1 \u00ae M [A]\nADD R1, B R1 \u00ae R1 + M [B]\nMOV R2, C R2 \u00ae M [C] X = (A + B) * ( C + D)\nADD R2, D R2 \u00ae R2 + M [D]\nMUL R1, R2 R1 \u00ae R1 * R2\nMOV X1 R1 M [X] \u00ae R1\nOne Address instruction\nIt used an implied accumulator (AC) register for all data manipulation. For\nmultiplication\/division, there is a need for a second register.\nLOAD A AC \u00ae M [A]\nADD B AC \u00ae AC + M [B]\nSTORE T M [T] \u00ae AC X = (A +B) \u00d7 (C + A)\nAll operations are done between the AC register and a memory operand. It\u2019s the\naddress of a temporary memory location required for storing the intermediate result.\nLOAD C AC \u00ae M (C)\nADD D AC \u00ae AC + M (D)\nML T AC \u00ae AC + M (T)\nSTORE X M [\u00d7]\u00ae AC\nZero \u2013 Address Instruction\nA stack organized computer does not use an address field for the instruction ADD and\nMUL. The PUSH & POP instruction, however, need an address field to specify the\noperand that communicates with the stack (TOS \u00ae top of the stack)\nPUSH A TOS \u00ae A\nPUSH B TOS \u00ae B\nADD TOS \u00ae (A + B)\nPUSH C TOS \u00ae C\nPUSH D TOS \u00ae D\nADD TOS \u00ae (C + D)\nPage 70 Computer Organization and Architecture\nMUL TOS \u00ae (C + D) * (A + B)\nPOP X M [X] TOS\nAddressing Modes\nThe operation field of an instruction specifies the operation to be performed. This\noperation must be executed on some data stored in computer register as memory\nwords. The way the operands are chosen during program execution is dependent on\nthe addressing mode of the instruction. The addressing mode specifies a rule for\ninterpreting or modifying the address field of the instruction between the operand is\nactivity referenced. Computer use addressing mode technique for the purpose of\naccommodating one or both of the following provisions.\n(1) To give programming versatility to the uses by providing such facilities as\npointer to memory, counters for top control, indexing of data, and program relocation.\n(2) To reduce the number of bits in the addressing fields of the instruction.\nAddressing Modes: The most common addressing techniques are\n\u2022 Immediate\n\u2022 Direct\n\u2022 Indirect\n\u2022 Register\n\u2022 Register Indirect\n\u2022 Displacement\n\u2022 Stack\nAll computer architectures provide more than one of these addressing modes.\nThe question arises as to how the control unit can determine which addressing mode\nis being used in a particular instruction. Several approaches are used. Often, different\nopcodes will use different addressing modes. Also, one or more bits in the instruction\nPage 71 Computer Organization and Architecture\nformat can be used as a mode field. The value of the mode field determines which\naddressing mode is to be used.\nWhat is the interpretation of effective address. In a system without virtual\nmemory, the effective address will be either a main memory address or a register. In a\nvirtual memory system, the effective address is a virtual address or a register. The\nactual mapping to a physical address is a function of the paging mechanism and is\ninvisible to the programmer.\nOpcode Mode Address\nImmediate Addressing:\nThe simplest form of addressing is immediate addressing, in which the\noperand is actually present in the instruction:\nOPERAND = A\nThis mode can be used to define and use constants or set initial values of\nvariables. The advantage of immediate addressing is that no memory reference other\nthan the instruction fetch is required to obtain the operand. The disadvantage is that\nthe size of the number is restricted to the size of the address field, which, in most\ninstruction sets, is small compared with the world length.\nDirect Addressing:\nA very simple form of addressing is direct addressing, in which the address field\ncontains the effective address of the operand:\nPage 72 Computer Organization and Architecture\nEA = A\nIt requires only one memory reference and no special calculation.\nIndirect Addressing:\nWith direct addressing, the length of the address field is usually less than the\nword length, thus limiting the address range. One solution is to have the address field\nrefer to the address of a word in memory, which in turn contains a full-length address\nof the operand. This is known as indirect addressing:\nEA = (A)\nRegister Addressing:\nRegister addressing is similar to direct addressing. The only difference is that\nthe address field refers to a register rather than a main memory address:\nEA = R\nPage 73 Computer Organization and Architecture\nThe advantages of register addressing are that only a small address field is\nneeded in the instruction and no memory reference is required. The disadvantage of\nregister addressing is that the address space is very limited.\nThe exact register location of the operand in case of Register Addressing\nMode is shown in the Figure 34.4. Here, 'R' indicates a register where the operand is\npresent.\nRegister Indirect Addressing:\nRegister indirect addressing is similar to indirect addressing, except that the\naddress field refers to a register instead of a memory location. It requires only one\nmemory reference and no special calculation.\nEA = (R)\nRegister indirect addressing uses one less memory reference than indirect\naddressing. Because, the first information is available in a register which is nothing\nbut a memory address. From that memory location, we use to get the data or\ninformation. In general, register access is much more faster than the memory access.\nDisplacement Addressing:\nPage 74 Computer Organization and Architecture\nA very powerful mode of addressing combines the capabilities of direct\naddressing and register indirect addressing, which is broadly categorized as\ndisplacement addressing:\nEA = A + (R)\nDisplacement addressing requires that the instruction have two address fields,\nat least one of which is explicit. The value contained in one address field (value = A)\nis used directly. The other address field, or an implicit reference based on opcode,\nrefers to a register whose contents are added to A to produce the effective address.\nThe general format of Displacement Addressing is shown in the Figure 4.6.\nThree of the most common use of displacement addressing are:\n\u2022 Relative addressing\n\u2022 Base-register addressing\n\u2022 Indexing\nRelative Addressing:\nFor relative addressing, the implicitly referenced register is the program\ncounter (PC). That is, the current instruction address is added to the address field to\nproduce the EA. Thus, the effective address is a displacement relative to the address\nof the instruction.\nBase-Register Addressing:\nPage 75 Computer Organization and Architecture\nThe reference register contains a memory address, and the address field\ncontains a displacement from that address. The register reference may be explicit or\nimplicit. In some implementation, a single segment\/base register is employed and is\nused implicitly. In others, the programmer may choose a register to hold the base\naddress of a segment, and the instruction must reference it explicitly.\nIndexing:\nThe address field references a main memory address, and the reference\nregister contains a positive displacement from that address. In this case also the\nregister reference is sometimes explicit and sometimes implicit.\nGenerally index register are used for iterative tasks, it is typical that there is a\nneed to increment or decrement the index register after each reference to it. Because\nthis is such a common operation, some system will automatically do this as part of the\nsame instruction cycle.\nThis is known as auto-indexing. We may get two types of auto-indexing: -one is\nauto-incrementing and the other one is -auto-decrementing. If certain registers are\ndevoted exclusively to indexing, then auto-indexing can be invoked implicitly and\nautomatically. If general purpose register are used, the auto index operation may need\nto be signaled by a bit in the instruction.\nAuto-indexing using increment can be depicted as follows:\nEA = A + (R)\nR = (R) + 1\nAuto-indexing using decrement can be depicted as follows:\nEA = A + (R)\nR = (R) - 1\nIn some machines, both indirect addressing and indexing are provided, and it is\npossible to employ both in the same instruction. There are two possibilities: The\nindexing is performed either before or after the indirection. If indexing is performed\nafter the indirection, it is termed post indexing\nEA = (A) + (R)\nPage 76 Computer Organization and Architecture\nFirst, the contents of the address field are used to access a memory location\ncontaining an address. This address is then indexed by the register value.\nWith pre indexing, the indexing is performed before the indirection:\nEA = ( A + (R)\nAn address is calculated, the calculated address contains not the operand, but the\naddress of the operand.\nStack Addressing:\nA stack is a linear array or list of locations. It is sometimes referred to as a\npushdown list or last-in-first-out queue. A stack is a reserved block of locations. Items\nare appended to the top of the stack so that, at any given time, the block is partially\nfilled. Associated with the stack is a pointer whose value is the address of the top of\nthe stack. The stack pointer is maintained in a register. Thus, references to stack\nlocations in memory are in fact register indirect addresses.\nThe stack mode of addressing is a form of implied addressing. The machine\ninstructions need not include a memory reference but implicitly operate on the top of\nthe stack.\nValue addition: A Quick View\nVarious Addressing Modes with Examples\nThe most common names for addressing modes (names may differ\namong architectures)\nAddressing Example\nMeaning When used\nmodes Instruction\nWhen a value is\nRegister Add R4,R3 R4 <- R4 + R3\nin a register\nImmediate Add R4, #3 R4 <- R4 + 3 For constants\nAdd R4, R4 <- R4 + Accessing local\nDisplacement\n100(R1) Mem[100+R1] variables\nAccessing using\nRegister a pointer or a\nAdd R4,(R1) R4 <- R4 + M[R1]\ndeffered computed\naddress\nUseful in array\naddressing:\nAdd R3, (R1 R3 <- R3 + R1 - base of\nIndexed\n+ R2) Mem[R1+R2] array\nR2 - index\namount\nPage 77 Computer Organization and Architecture\nUseful in\nAdd R1,\nDirect R1 <- R1 + Mem[1001] accessing static\n(1001)\ndata\nIf R3 is the\nMemory Add R1, R1 <- R1 + address of a\ndeferred @(R3) Mem[Mem[R3]] pointer p, then\nmode yields *p\nUseful for\nstepping\nthrough arrays\nAuto- Add R1, R1 <- R1 +Mem[R2] in a loop.\nincrement (R2)+ R2 <- R2 + d R2 - start of\narray\nd - size of an\nelement\nSame as\nautoincrement.\nBoth can also\nAuto- Add R1,- R2 <-R2-d\nbe used to\ndecrement (R2) R1 <- R1 + Mem[R2]\nimplement a\nstack as push\nand pop\nUsed to index\narrays. May be\napplied to any\nAdd R1, R1<-\nScaled base\n100(R2)[R3] R1+Mem[100+R2+R3*d]\naddressing\nmode in some\nmachines.\nNotation:\n<- - assignment\nMem - the name for memory:\nMem[R1] refers to contents of memory location whose address is given by the\ncontents of R1\nSource: Self\nData Transfer & Manipulation\nComputer provides an extensive set of instructions to give the user the flexibility to\ncarryout various computational task. Most computer instruction can be classified into\nthree categories.\n(1) Data transfer instruction\n(2) Data manipulation instruction\n(3) Program control instruction\nData transfer instruction cause transferred data from one location to another without\nchanging the binary instruction content. Data manipulation instructions are those that\nperform arithmetic logic, and shift operations. Program control instructions provide\nPage 78 Computer Organization and Architecture\ndecision-making capabilities and change the path taken by the program when\nexecuted in the computer.\n(1) Data Transfer Instruction\nData transfer instruction move data from one place in the computer to another\nwithout changing the data content. The most common transfers are between memory\nand processes registers, between processes register & input or output, and between\nprocesses register themselves\n(Typical data transfer instruction)\nName Mnemonic\nLoad LD\nStore ST\nMove MOV\nExchange XCH\nInput IN\nOutput OUT\nPush PUSH\nPop POP\n(2) Data Manipulation Instruction\nIt performs operations on data and provides the computational capabilities for the\ncomputer. The data manipulation instructions in a typical computer are usually\ndivided into three basic types.\n(a) Arithmetic Instruction\n(b) Logical bit manipulation Instruction\n(c) Shift Instruction.\n(a) Arithmetic Instruction\nName Mnemonic\nIncrement INC\nDecrement DEC\nAdd Add\nPage 79 Computer Organization and Architecture\nSubtract Sub\nMultiply MUL\nDivide DIV\nAdd with Carry ADDC\nSubtract with Basses SUBB\nNegate (2\u2019s Complement) NEG\n(b) Logical & Bit Manipulation Instruction\nName Mnemonic\nClear CLR\nComplement COM\nAND AND\nOR OR\nExclusive-Or XOR\nClear Carry CLRC\nSet Carry SETC\nComplement Carry COMC\nEnable Interrupt ET\nDisable Interrupt OI\n(c) Shift Instruction\nInstructions to shift the content of an operand are quite useful and one often provided\nin several variations. Shifts are operation in which the bits of a word are moved to the\nleft or right. The bit-shifted in at the and of the word determines the type of shift used.\nShift instruction may specify either logical shift, arithmetic shifts, or rotate type shifts.\nName Mnemonic\nLogical Shift right SHR\nLogical Shift left SHL\nArithmetic shift right SHRA\nArithmetic shift left SHLA\nRotate right ROR\nPage 80 Computer Organization and Architecture\nRotate left ROL\nRotate right through carry RORC\nRotate left through carry ROLC\nIntroduction About Program Control:-\nA program that enhances an operating system by creating an environment in which\nyou can run other programs. Control programs generally provide a graphical\ninterface and enable you to run several programs at once in different windows.\nControl programs are also called operating environments.\nThe program control functions are used when a series of conditional or\nunconditional jump and return instruction are required. These instructions allow the\nprogram to execute only certain sections of the control logic if a fixed set of logic\nconditions are met. The most common instructions for the program control available\nin most controllers are described in this section.\nIntroduction About status bit register:-\nA status register, flag register, or condition code register is a collection of\nstatus flag bits for a processor. An example is the FLAGS register of the computer\narchitecture. The flags might be part of a larger register, such as a program status\nword (PSW) register.\nThe status register is a hardware register which contains information about the\nstate of the processor. Individual bits are implicitly or explicitly read and\/or written\nby the machine code instructions executing on the processor. The status register in a\ntraditional processor design includes at least three central flags: Zero, Carry, and\nOverflow, which are set or cleared automatically as effects of arithmetic and bit\nmanipulation operations. One or more of the flags may then be read by a subsequent\nconditional jump instruction (including conditional calls, returns, etc. in some\nmachines) or by some arithmetic, shift\/rotate or bitwise operation, typically using the\ncarry flag as input in addition to any explicitly given operands. There are also\nprocessors where other classes of instructions may read or write the fundamental\nPage 81 Computer Organization and Architecture\nzero, carry or overflow flags, such as block-, string- or dedicated input\/output\ninstructions, for instance.\nSome CPU architectures, such as the MIPS and Alpha, do not use a dedicated flag\nregister. Others do not implicitly set and\/or read flags. Such machines either do not\npass implicit status information between instructions at all, or do they pass it in a\nexplicitly selected general purpose register.\nA status register may often have other fields as well, such as more specialized\nflags, interrupt enable bits, and similar types of information. During an interrupt, the\nstatus of the thread currently executing can be preserved (and later recalled) by\nstoring the current value of the status register along with the program counter and\nother active registers into the machine stack or some other reserved area of memory.\nCommon flags:-\nThis is a list of the most common CPU status register flags, implemented in almost all\nmodern processors.\nFlag Name Description\nIndicates that the result of arithmetic or logical\nZ Zero flag\noperation (or, sometimes, a load) was zero.\nEnables numbers larger than a single word to be\nadded\/subtracted by carrying a binary digit from a less\nsignificant word to the least significant bit of a more\nC Carry flag\nsignificant word as needed. It is also used to extend bit\nshifts and rotates in a similar manner on many\nprocessors (sometimes done via a dedicated X flag).\nIndicates that the result of a mathematical operation is\nnegative. In some processors, the N and S flags are\nSign flag\ndistinct with different meanings and usage: One\nS \/ N Negative\nindicates whether the last result was negative whereas\nflag\nthe other indicates whether a subtraction or addition\nhas taken place.\nPage 82 Computer Organization and Architecture\nIndicates that the signed result of an operation is too\nOverflow\nV \/ O \/ W large to fit in the register width using twos complement\nflag\nrepresentation.\nIntroduction About Conditional branch instruction:-\nConditional branch instruction:-\nConditional branch instruction is the branch instruction bit and BR instruction is the\nProgram control instruction.\nThe conditional Branch Instructions are listed as Bellow:-\nMnemonics Branch Instruction Tested control\nBZ Branch if Zero Z=1\nBNZ Branch if not Zero Z=0\nBC Branch if Carry C=1\nBNC Branch if not Carry C=0\nBP Branch if Plus S=0\nBM Branch if Minus S=1\nBV Branch if Overflow V=1\nBNV Branch if not Overflow V=0\nUnsigned Compare(A-B):-\nMnemonics Branch Instruction Tested control\nBHI Branch if Higher A > B\nBHE Branch if Higher or Equal A >= B\nBLO Branch if Lower A < B\nBLE Branch if Lower or Equal A <= B\nBE Branch if Equal A=B\nBNE Branch if not Equal A not = B\nSigned Compare(A-B):\nMnemonics Branch Instruction Tested control\nPage 83 Computer Organization and Architecture\nBGT Branch if Greater Than A > B\nBGE Branch if Greater Than or Equal A >= B\nBLT Branch if Less Than A < B\nBLE Branch if Less Than or Equal A <= B\nBE Branch if Equal A=B\nBNE Branch if not Equal A not = B\nIntroduction About program interrupt:-\nWhen a Process is executed by the CPU and when a user Request for another Process\nthen this will create disturbance for the Running Process. This is also called as\nthe Interrupt.\nInterrupts can be generated by User, Some Error Conditions and also by\nSoftware\u2019s and the hardware\u2019s. But CPU will handle all the Interrupts very carefully\nbecause when Interrupts are generated then the CPU must handle all the Interrupts\nVery carefully means the CPU will also Provide Response to the Various Interrupts\nthose are generated. So that When an interrupt has Occurred then the CPU will handle\nby using the Fetch, decode and Execute Operations.\nInterrupts allow the operating system to take notice of an external event, such\nas a mouse click. Software interrupts, better known as exceptions, allow the OS to\nhandle unusual events like divide-by-zero errors coming from code execution.\nThe sequence of events is usually like this:\nHardware signals an interrupt to the processor\nThe processor notices the interrupt and suspends the currently running software\nThe processor jumps to the matching interrupt handler function in the OS\nThe interrupt handler runs its course and returns from the interrupt\nThe processor resumes where it left off in the previously running software\nThe most important interrupt for the operating system is the timer tick interrupt. The\ntimer tic interrupt allows the OS to periodically regain control from the currently\nrunning user process. The OS can then decide to schedule another process, return back\nPage 84 Computer Organization and Architecture\nto the same process, do housekeeping, etc. The timer tick interrupt provides the\nfoundation for the concept of preemptive multitasking.\nTYPES OF INTERRUPTS\nGenerally there are three types of Interrupts those are Occurred For Example\n1) Internal Interrupt\n2) External Interrupt.\n3) Software Interrupt.\n1.Internal Interrupt:\n\u2022 When the hardware detects that the program is doing something wrong, it will\nusually generate an interrupt usually generate an interrupt.\n\u2013 Arithmetic error - Invalid Instruction\n\u2013 Addressing error - Hardware malfunction\n\u2013 Page fault \u2013 Debugging\n\u2022 A Page Fault interrupt is not the result of a program error, but it does require the\noperating system to get control.\nThe Internal Interrupts are those which are occurred due to Some Problem in\nthe Execution For Example When a user performing any Operation which contains any\nError and which contains any type of Error. So that Internal Interrupts are those\nwhich are occurred by the Some Operations or by Some Instructions and the\nOperations those are not Possible but a user is trying for that Operation. And The\nSoftware Interrupts are those which are made some call to the System for Example\nwhile we are Processing Some Instructions and when we wants to Execute one more\nApplication Programs.\n2.External Interrupt:\n\u2022 I\/O devices tell the CPU that an I\/O request has completed by sending an interrupt\nsignal to the processor.\n\u2022 I\/O errors may also generate an interrupt.\nPage 85 Computer Organization and Architecture\n\u2022 Most computers have a timer which interrupts the CPU every so many interrupts the\nCPU every so many milliseconds.\nThe External Interrupt occurs when any Input and Output Device request for any\nOperation and the CPU will Execute that instructions first For Example When a\nProgram is executed and when we move the Mouse on the Screen then the CPU will\nhandle this External interrupt first and after that he will resume with his Operation.\n3.Software interrupts:\nThese types if interrupts can occur only during the execution of an instruction. They\ncan be used by a programmer to cause interrupts if need be. The primary purpose of\nsuch interrupts is to switch from user mode to supervisor mode.\nA software interrupt occurs when the processor executes an INT instruction.\nWritten in the program, typically used to invoke a system service. A processor\ninterrupt is caused by an electrical signal on a processor pin. Typically used by devices\nto tell a driver that they require attention. The clock tick interrupt is very common; it\nwakes up the processor from a halt state and allows the scheduler to pick other work\nto perform.\nA processor fault like access violation is triggered by the processor itself when it\nencounters a condition that prevents it from executing code. Typically when it tries to\nread or write from unmapped memory or encounters an invalid instruction.\nCISC Characteristics\nA computer with large number of instructions is called complex instruction set\ncomputer or CISC. Complex instruction set computer is mostly used in scientific\ncomputing applications requiring lots of floating point arithmetic.\n\uf0d8 A large number of instructions - typically from 100 to 250 instructions.\n\uf0d8 Some instructions that perform specialized tasks and are used infrequently.\n\uf0d8 A large variety of addressing modes - typically 5 to 20 different modes.\n\uf0d8 Variable-length instruction formats\nPage 86 Computer Organization and Architecture\n\uf0d8 Instructions that manipulate operands in memory.\nRISC Characteristics\nA computer with few instructions and simple construction is called reduced\ninstruction set computer or RISC. RISC architecture is simple and efficient. The major\ncharacteristics of RISC architecture are,\n\uf0d8 Relatively few instructions\n\uf0d8 Relatively few addressing modes\n\uf0d8 Memory access limited to load and store instructions\n\uf0d8 All operations are done within the registers of the CPU\n\uf0d8 Fixed-length and easily-decoded instruction format.\n\uf0d8 Single cycle instruction execution\n\uf0d8 Hardwired and micro programmed control\nExample of RISC & CISC\nExamples of CISC instruction set architectures are PDP-11, VAX, Motorola 68k,\nand your desktop PCs on intel\u2019s x86 architecture based too .\nExamples of RISC families include DEC Alpha, AMD 29k, ARC, Atmel AVR,\nBlackfin, Intel i860 and i960, MIPS, Motorola 88000, PA-RISC, Power (including PowerPC),\nSuperH, SPARC and ARM too.\nPage 87 Computer Organization and Architecture\nWhich one is better ?\nWe cannot differentiate RISC and CISC technology because both are suitable at its specific\napplication. What counts is how fast a chip can execute the instructions it is given and how well it\nruns existing software. Today, both RISC and CISC manufacturers are doing everything to get an\nedge on the competition.\nhttp:\/\/www.laureateiit.com\/projects\/bacii2014\/projects\/coa_anil\/i_o_interface.html\nUNIT \u2013 III (12 Lectures)\nMICRO\u2010PROGRAMMED CONTROL: Control memory, address sequencing, micro\u2010program\nexample, design of control unit.\nBook: M. Moris Mano (2006), Computer System Architecture, 3rd edition, Pearson\/PHI,\nIndia: Unit-7 Pages: 213-238\nPage 88 Computer Organization and Architecture\nCOMPUTER ARITHMETIC: Addition and subtraction, multiplication and division algorithms,\nfloating\u2010point arithmetic operation, decimal arithmetic unit, decimal arithmetic operations.\nBook: M. Moris Mano (2006), Computer System Architecture, 3rd edition, Pearson\/PHI,\nIndia: Unit-10 Pages: 333-380\nControl Memory:\nControl memory is a random access memory(RAM) consisting of addressable storage\nregisters. It is primarily used in mini and mainframe computers. It is used as a temporary\nstorage for data. Access to control memory data requires less time than to main memory; this\nspeeds up CPU operation by reducing the number of memory references for data storage and\nretrieval. Access is performed as part of a control section sequence while the master clock\noscillator is running. The control memory addresses are divided into two groups: a task mode\nand an executive (interrupt) mode.\nAddressing words stored in control memory is via the address select logic for each of\nthe register groups. There can be up to five register groups in control memory. These groups\nselect a register for fetching data for programmed CPU operation or for maintenance console\nor equivalent display or storage of data via maintenance console or equivalent. During\nprogrammed CPU operations, these registers are accessed directly by the CPU logic. Data\nrouting circuits are used by control memory to interconnect the registers used in control\nmemory. Some of the registers contained in a control memory that operate in the task and\nthe executive modes include the following: Accumulators Indexes Monitor clock status\nindicating registers Interrupt data registers\n\u2022 The control unit in a digital computer initiates sequences of micro operations\n\u2022 The complexity of the digital system is derived form the number of sequences that are\nperformed\n\u2022 When the control signals are generated by hardware, it is hardwired\n\u2022 In a bus-oriented system, the control signals that specify micro operations are groups of bits\nthat select the paths in multiplexers, decoders, and ALUs.\nPage 89 Computer Organization and Architecture\n\u2022 The control unit initiates a series of sequential steps of micro operations\n\u2022 The control variables can be represented by a string of 1\u2019s and 0\u2019s called a control word\n\u2022 A micro programmed control unit is a control unit whose binary control variables are stored\nin memory\n\u2022 A sequence of microinstructions constitutes a micro program\n\u2022 The control memory can be a read-only memory\n\u2022 Dynamic microprogramming permits a micro program to be loaded and uses a writable\ncontrol memory\n\u2022 A computer with a micro programmed control unit will have two separate memories: a\nmain memory and a control memory\n\u2022 The micro program consists of microinstructions that specify various internal control\nsignals for execution of register micro operations\n\u2022 These microinstructions generate the micro operations to:\n\uf0d8 fetch the instruction from main memory\n\uf0d8 evaluate the effective address\n\uf0d8 execute the operation\n\uf0d8 return control to the fetch phase for the next instruction\n\u2022 The control memory address register specifies the address of the microinstruction\n\u2022 The control data register holds the microinstruction read from memory\n\u2022 The microinstruction contains a control word that specifies one or more micro operations\nfor the data processor\n\u2022 The location for the next micro instruction may, or may not be the next in sequence\n\u2022 Some bits of the present micro instruction control the generation of the address of the next\nmicro instruction\n\u2022 The next address may also be a function of external input conditions\n\u2022 While the micro operations are being executed, the next address is computed in the next\naddress generator circuit (sequencer) and then transferred into the CAR to read the next\nmicro instructions\n\u2022 Typical functions of a sequencer are: o incrementing the CAR by one\n\uf0d8 loading into the CAR and address from control memory\n\uf0d8 transferring an external address\n\uf0d8 loading an initial address to start the control operations\nPage 90 Computer Organization and Architecture\n\u2022 A clock is applied to the CAR and the control word and next-address information are taken\ndirectly from the control memory\n\u2022 The address value is the input for the ROM and the control work is the output\n\u2022 No read signal is required for the ROM as in a RAM\n\u2022 The main advantage of the micro programmed control is that once the\nhardware configuration is established, there should be no need for h\/w or wiring changes\n\u2022 To establish a different control sequence, specify a different set of microinstructions for\ncontrol memory\nAddressing Sequencing:\nEach machine instruction is executed through the application of a sequence of\nmicroinstructions. Clearly, we must be able to sequence these; the collection of\nmicroinstructions which implements a particular machine instruction is called a routine.\nThe MCU typically determines the address of the first microinstruction which\nimplements a machine instruction based on that instruction's opcode. Upon machine power-\nup, the CAR should contain the address of the first microinstruction to be executed.\nThe MCU must be able to execute microinstructions sequentially (e.g., within routines), but\nmust also be able to ``branch'' to other microinstructions as required; hence, the need for a\nsequencer.\nThe microinstructions executed in sequence can be found sequentially in the CM, or\ncan be found by branching to another location within the CM. Sequential retrieval of\nmicroinstructions can be done by simply incrementing the current CAR contents; branching\nrequires determining the desired CW address, and loading that into the CAR.\nCAR\nControl Address Register\nPage 91 Computer Organization and Architecture\ncontrol ROM\ncontrol memory (CM); holds CWs\nopcode\nopcode field from machine instruction\nmapping logic\nhardware which maps opcode into microinstruction address\nbranch logic\ndetermines how the next CAR value will be determined from all the various possibilities\nmultiplexors\nimplements choice of branch logic for next CAR value\nincrementer\ngenerates CAR + 1 as a possible next CAR value\nSBR\nused to hold return address for subroutine-call branch operations\nConditional branches are necessary in the micro program. We must be able to perform\nsome sequences of micro-ops only when certain situations or conditions exist (e.g., for\nconditional branching at the machine instruction level); to implement these, we need to be\nable to conditional execute or avoid certain microinstructions within routines.\nSubroutine branches are helpful to have at the micro program level. Many routines\ncontain identical sequences of microinstructions; putting them into subroutines allows those\nroutines to be shorter, thus saving memory. Mapping of opcodes to microinstruction\naddresses can be done very simply. When the CM is designed, a ``required'' length is\ndetermine for the machine instruction routines (i.e., the length of the longest one). This is\nrounded up to the next power of 2, yielding a value k such that 2 k microinstructions will be\nsufficient to implement any routine.\nThe first instruction of each routine will be located in the CM at multiples of this\n``required'' length. Say this is N. The first routine is at 0; the next, at N; the next, at 2*N; etc.\nThis can be accomplished very easily. For instance, with a four-bit opcode and routine length\nof four microinstructions, k is two; generate the microinstruction address by appending two\nzero bits to the opcode:\nPage 92 Computer Organization and Architecture\nAlternately, the n-bit opcode value can be used as the ``address'' input of a 2n x M ROM; the\ncontents of the selected ``word'' in the ROM will be the desired M-bit CAR address for the\nbeginning of the routine implementing that instruction. (This technique allows for variable-\nlength routines in the CM.) >pp We choose between all the possible ways of generating CAR\nvalues by feeding them all into a multiplexor bank, and implementing special branch logic\nwhich will determine how the muxes will pass on the next address to the CAR.\nAs there are four possible ways of determining the next address, the multiplexor bank\nis made up of N 4x1 muxes, where N is the number of bits in the address of a CW. The branch\nlogic is used to determine which of the four possible ``next address'' values is to be passed on\nto the CAR; its two output lines are the select inputs for the muxes.\nPage 93 Computer Organization and Architecture\nAddition and Subtraction\nFour basic computer arithmetic operations are addition, subtraction, division and\nmultiplication. The arithmetic operation in the digital computer manipulate data to produce\nresults. It is necessary to design arithmetic procedures and circuits to program arithmetic\noperations using algorithm. The algorithm is a solution to any problem and it is stated by a\nfinite number of well-defined procedural steps. The algorithms can be developed for the\nfollowing types of data.\n1. Fixed point binary data in signed magnitude representation\n2. Fixed point binary data in signed 2\u2019s complement representation.\n3. Floating point representation\n4. Binary Coded Decimal (BCD) data\nAddition and Subtraction with signed magnitude\nConsider two numbers having magnitude A and B. When the signed numbers are added or\nsubtracted, there can be 8 different conditions depending on the sign and the operation\nperformed as shown in the table below:\nOperation Add magnitude When A > B When A < B When A = B\n(+A) + (+B) +(A + B) -- -- --\n(+A) + (-B) -- +(A - B) -(B - A) +(A - B)\n(-A) + (+B) -- -(A - B) +(B - A) +(A - B)\n(-A) + (-B) -(A + B) -- -- --\n(+A) - (+B) -- +(A - B) -(B - A) +(A - B)\n(+A) - (-B) +(A + B) -- -- --\n(-A) - (+B) -(A + B) -- -- --\n(-A) - (-B) -- -(A - B) +(B - A) +(A - B)\nFrom the table, we can derive an algorithm for addition and subtraction as follows:\nAddition (Subtraction) Algorithm:\n\uf0b7 When the signs of A & B are identical, add the two magnitudes and attach the sign of A to\nthe result.\n\uf0b7 When the sign of A & B are different, compare the magnitude and subtract the smaller\nnumber from the large number. Choose the sign of the result to be same as A if A > B, or the\ncomplement of the sign of A if A < B. If the two numbers are equal, subtract B from A and\nmake the sign of the result positive.\nPage 94 Computer Organization and Architecture\nHardware Implementation\nfig: Hardware for signed magnitude addition and subtraction\nThe hardware consists of two registers A and B to store the magnitudes, and two flip-\nflops As and Bs to store the corresponding signs. The results can be stored in the register A\nand As which acts as an accumulator. The subtraction is performed by adding A to the 2\u2019s\ncomplement of B. The output carry is transferred to the flip-flop E. The overflow may occur\nduring the add operation which is stored in the flip-flop A F. When m = 0, the output of E is\n\u00cb\u2026\ntransferred to the adder without any change along with the input carry of \u20180\".\nThe output of the parallel adder is equal to A + B which is an add operation. When m =\n1, the content of register B is complemented and transferred to parallel adder along with the\ninput carry of 1. Therefore, the output of parallel is equal to A + B\u2019 + 1 = A \u2013 B which is a\nsubtract operation.\nPage 95 Computer Organization and Architecture\nHardware Algorithm\nfig: flowchart for add and subtract operations\nAs and Bs are compared by an exclusive-OR gate. If output=0, signs are identical, if 1 signs are\ndifferent.\n\uf0b7 For Add operation, identical signs dictate addition of magnitudes and for operation\nidentical signs dictate addition of magnitudes and for subtraction, different magnitudes\ndictate magnitudes be added. Magnitudes are added with a micro operation EA\n\uf0b7 Two magnitudes are subtracted if signs are different for add operation and identical for\nsubtract operation. Magnitudes are subtracted with a micro operation EA = B and number\n(this number is checked again for 0 to make positive 0 [As=0]) in A is correct result. E = 0\nindicates A < B, so we take 2\u2019s complement of A.\nMultiplication\nHardware Implementation and Algorithm\nGenerally, the multiplication of two final point binary number in signed magnitude\nrepresentation is performed by a process of successive shift and ADD operation. The process\nconsists of looking at the successive bits of the multiplier (least significant bit first). If the\nmultiplier is 1, then the multiplicand is copied down otherwise, 0\u2019s are copied. The numbers\nPage 96 Computer Organization and Architecture\ncopied down in successive lines are shifted one position to the left and finally, all the numbers\nare added to get the product.\nBut, in digital computers, an adder for the summation (\u2211) of only two binary numbers are\nused and the partial product is accumulated in register. Similarly, instead of shifting the\nmultiplicand to the left, the partial product is shifted to the right. The hardware for the\nmultiplication of signed magnitude data is shown in the figure below.\nHardware for multiply operation\nInitially, the multiplier is stored q register and the multiplicand in the B register. A register is\nused to store the partial product and the sequence counter (SC) is set to a number equal to\nthe number of bits in the multiplier. The sum of A and B form the partial product and both\nshifted to the right using a statement \u201cShr EAQ\u201d as shown in the hardware algorithm. The flip\nflops As, Bs & Qs store the sign of A, B & Q respectively. A binary \u20180\u201d inserted into the flip-flop\nE during the shift right.\nHardware Algorithm\nflowchart for multiply algorithm\nPage 97 Computer Organization and Architecture\nExample: Multiply 23 by 19 using multiply algorithm.\nmultiplicand E A Q SC\nInitially, 0 00000 10011 101(5)\nIteration1(Qn=1), 00000\nadd B 0 +10111\nfirst partial product 10111\nshrEAQ, 0 01011 11001 100(4)\nIteration2(Qn=1) 01011\nAdd B 1 +10111 11001\nSecond partial product 00010\nshrEAQ, 0 10001 01100 011(3)\nIteration3(Qn=0)\n0 01000 10110 010(2)\nshrEAQ,\nIteration4(Qn=0)\n0 00100 01011 001(1)\nshrEAQ,\nIteration5(Qn=1 00100\nAdd B 0 +10111 01011\nFifth partial product 11011\nshrEAQ, 0 01101 10101 000\nFinalProductinAQ 0110110101\nThe final product is in register A & Q. therefore, the product is 0110110101.\nBooth Algorithm\nThe algorithm that is used to multiply binary integers in signed 2\u2019s complement form is called\nbooth multiplication algorithm. It works on the principle that the string 0\u2019s in the multiplier\ndoesn\u2019t need addition but just the shifting and the sting of 1\u2019s from bit weight 2k to 2m can be\ntreated as 2k+1 \u2013 2m (Example, +14 = 001110 = 23=1 \u2013 21 = 14). The product can be obtained by\nshifting the binary multiplication to the left and subtraction the multiplier shifted left once.\nAccording to booth algorithm, the rule for multiplication of binary integers in signed 2\u2019s\ncomplement form are:\n\uf0b7 The multiplicand is subtracted from the partial product of the first least significant bit is 1\nin a string of 1\u2019s in the multiplicand.\nPage 98 Computer Organization and Architecture\n\uf0b7 The multiplicand is added to the partial product if the first least significant bit is 0\n(provided that there was a previous 1) in a string of 0\u2019s in the multiplier.\n\uf0b7 The partial product doesn\u2019t change when the multiplier bit is identical to the previous\nmultiplier bit.\nThis algorithm is used for both the positive and negative numbers in signed 2\u2019s complement\nform. The hardware implementation of this algorithm is in figure below:\nThe flowchart for booth multiplication algorithm is given below:\nflowchart for booth multiplication algorithm\nNumerical Example: Booth algorithm\nBR=10111(Multiplicand)\nQR=10011(Multiplier)\nArray Multiplier\nThe multiplication algorithm first check the bits of the multiplier one at time and form partial\nproduct. This is a sequential process that requires a sequence of add and shift micro\noperation. This method is complicated and time consuming. The multiplication of 2 binary\nPage 99 Computer Organization and Architecture\nnumbers can also be done with one micro operation by using combinational circuit that\nprovides the product all at once.\nExample.\nConsider that the multiplicand bits are b1 and b0 and the multiplier bits are a1 and a0. The\npartial product is c3c2c1c0. The multiplication two bits a0 and a1 produces a binary 1 if both\nthe bits are 1, otherwise it produces a binary 0. This is identical to the AND operation and can\nbe implemented with the AND gates as shown in figure.\n2-bit by 2-bit array multiplier\nDivision Algorithm\nThe division of two fixed point signed numbers can be done by a process of successive\ncompare shift and subtraction. When it is implemented in digital computers, instead of\nshifting the divisor to the right, the dividend or the partial remainder is shifted to the left. The\nsubtraction can be obtained by adding the number A to the 2\u2019s complement of number B. The\ninformation about the relative magnitudes of the information about the relative magnitudes\nof numbers can be obtained from the end carry,\nHardware Implementation\nThe hardware implementation for the division signed numbers is shown id the figure.\nPage 100 Computer Organization and Architecture\nDivision Algorithm\nThe divisor is stored in register B and a double length dividend is stored in register A and Q.\nthe dividend is shifted to the left and the divider is subtracted by adding twice complement of\nthe value. If E = 1, then A >= B. In this case, a quotient bit 1 is inserted into Qn and the partial\nremainder is shifted to the left to repeat the process. If E = 0, then A > B. In this case, the\nquotient bit Qn remains zero and the value of B is added to restore the partial remainder in A\nto the previous value. The partial remainder is shifted to the left and approaches continues\nuntil the sequence counter reaches to 0. The registers E, A & Q are shifted to the left with 0\ninserted into Qn and the previous value of E is lost as shown in the flow chart for division\nalgorithm.\nflowchart for division algorithm\nThis algorithm can be explained with the help of an example.\nConsider that the divisor is 10001 and the dividend is 01110.\nPage 101 Computer Organization and Architecture\nbinary division with digital hardware\nRestoring method\nMethod described above is restoring method in which partial remainder is restored by\nadding the divisor to the negative result. Other methods:\nComparison method: A and B are compared prior to subtraction. Then if A >= B, B is\nsubtracted from A. if A < B nothing is done. The partial remainder is then shifted left and\nnumbers are compared again. Comparison inspects end carry out of the parallel adder before\ntransferring to E.\nNon-restoring method: In contrast to restoring method, when A -B is negative, B is not\nadded to restore A but instead, negative difference is shifted left and then B is added. How is it\npossible? Let\u2019s argue:\n\uf0b7 In flowchart for restoring method, when A < B, we restore A by operation A - B + B. Next\ntime in a loop,\nthis number is shifted left (multiplied by 2) and B subtracted again, which gives: 2 (A - B +\nB) \u2013 B = 2 A - B.\n\uf0b7 In Non-restoring method, we leave A - B as it is. Next time around the loop, the number is\nshifted left and B is added: 2 (A - B) + B = 2 A - B (same as above).\nPage 102 Computer Organization and Architecture\nDivide Overflow\nThe division algorithm may produce a quotient overflow called dividend overflow. The\noverflow can occur of the number of bits in the quotient are more than the storage capacity of\nthe register. The overflow flip-flop DVF is set to 1 if the overflow occurs.\nThe division overflow can occur if the value of the half most significant bits of the dividend is\nequal to or greater than the value of the divisor. Similarly, the overflow can occue=r if the\ndividend is divided by a 0. The overflow may cause an error in the result or sometimes it may\nstop the operation. When the overflow stops the operation of the system, then it is called\ndivide stop.\nArithmetic Operations on Floating-Point Numbers\nThe rules apply to the single-precision IEEE standard format. These rules\nspecify only the major steps needed to perform the four operations. Intermediate\nresults for both mantissas and exponents might require more than 24 and 8 bits,\nrespectively & overflow or an underflow may occur. These and other aspects of the\noperations must be carefully considered in designing an arithmetic unit that meets the\nstandard. If their exponents differ, the mantissas of floating-point numbers must be\nshifted with respect to each other before they are added or subtracted. Consider a\ndecimal example in which we wish to add 2.9400 x to 4.3100 x . We rewrite\n2.9400 x as 0.0294 x and then perform addition of the mantissas to get 4.3394\nx . The rule for addition and subtraction can be stated as follows:\nAdd\/Subtract Rule\nThe steps in addition (FA) or subtraction (FS) of floating-point numbers (s1, e\u02c6 , f1) fad{s2, e\u02c6\n2, f2) are as follows.\n1. Unpack sign, exponent, and fraction fields. Handle special operands such as zero,\ninfinity, or NaN(not a number).\n2. Shift the significand of the number with the smaller exponent right by bits.\n3. Set the result exponent er to max(e1,e2).\n4. If the instruction is FA and s1= s2 or if the instruction is FS and s1 \u2260 s2 then add the\nsignificands; otherwise subtract them.\nPage 103 Computer Organization and Architecture\n5. Count the number z of leading zeros. A carry can make z = -1. Shift the result\nsignificand left z bits or right 1 bit if z = -1.\n6. Round the result significand, and shift right and adjust z if there is rounding overflow,\nwhich is a carry-out of the leftmost digit upon rounding.\n7. Adjust the result exponent by er = er - z, check for overflow or underflow, and pack\nthe result sign, biased exponent, and fraction bits into the result word.\nMultiplication and division are somewhat easier than addition and subtraction, in that\nno alignment of mantissas is needed.\nBCD Adder:\nBCD adder A 4-bit binary adder that is capable of adding two 4-bit words having a BCD\n(binary-coded decimal) format. The result of the addition is a BCD-format 4-bit output word,\nrepresenting the decimal sum of the addend and augend, and a carry that is generated if this\nsum exceeds a decimal value of 9. Decimal addition is thus possible using these devices.\nPage 104 Computer Organization and Architecture\nUNIT \u2013 IV (10 Lectures)\nTHE MEMORY SYSTEM: Basic concepts, semiconductor RAM types of read \u2010 only memory\n(ROM), cache memory, performance considerations, virtual memory, secondary storage, raid,\ndirect memory access (DMA).\nBook: Carl Hamacher, Zvonks Vranesic, SafeaZaky (2002), Computer Organization, 5th\nedition, McGraw Hill: Unit-5 Pages: 292-366\nBASIC CONCEPTS OF MEMORY SYSTEM\nThe maximum size of the Main Memory (MM) that can be used in any computer is\ndetermined by its addressing scheme. For example, a 16-bit computer that generates 16-bit\naddresses is capable of addressing upto 216 =64K memory locations. If a machine generates\n32-bit addresses, it can access upto 232 = 4G memory locations. This number represents the\nsize of address space of the computer.\nIf the smallest addressable unit of information is a memory word, the machine is called\nword-addressable. If individual memory bytes are assigned distinct addresses, the computer\nis called byte-addressable. Most of the commercial machines are byte addressable. For\nexample in a byte-addressable 32-bit computer, each memory word contains 4 bytes. A\npossible word-address assignment would be:\nWord Address Byte Address\n0 0 1 2 3\n4 4 5 6 7\n8 8 9 10 11\n. \u2026.. . \u2026.. . \u2026..\nWith the above structure a READ or WRITE may involve an entire memory word or it may\ninvolve only a byte. In the case of byte read, other bytes can also be read but ignored by the\nCPU. However, during a write cycle, the control circuitry of the MM must ensure that only the\nspecified byte is altered. In this case, the higher-order 30 bits can specify the word and the\nlower-order 2 bits can specify the byte within the word.\nCPU-Main Memory Connection \u2013 A block schematic: -\nFrom the system standpoint, the Main Memory (MM) unit can be viewed as a \u201cblock box\u201d.\nData transfer between CPU and MM takes place through the use of two CPU registers, usually\ncalled MAR (Memory Address Register) and MDR (Memory Data Register). If MAR is K bits\nlong and MDR is \u2018n\u2019 bits long, then the MM unit may contain upto 2k addressable locations and\nPage 105 Computer Organization and Architecture\neach location will be \u2018n\u2019 bits wide, while the word length is equal to \u2018n\u2019 bits. During a \u201cmemory\ncycle\u201d, n bits of data may be transferred between the MM and CPU.\nThis transfer takes place over the processor bus, which has k address lines (address\nbus), n data lines (data bus) and control lines like Read, Write, Memory Function completed\n(MFC), Bytes specifiers etc (control bus). For a read operation, the CPU loads the address into\nMAR, set READ to 1 and sets other control signals if required. The data from the MM is loaded\ninto MDR and MFC is set to 1. For a write operation, MAR, MDR are suitably loaded by the\nCPU, write is set to 1 and other control signals are set suitably. The MM control circuitry loads\nthe data into appropriate locations and sets MFC to 1. This organization is shown in the\nfollowing block schematic.\nAddress Bus (k bits) Main Memory upto 2k addressable locations Word length = n bits Data\nbus (n bits) Control Bus (Read, Write, MFC, Byte Specifier etc) MAR MDR CPU\nSome Basic Concepts\nMemory Access Times: - It is a useful measure of the speed of the memory unit. It is the time\nthat elapses between the initiation of an operation and the completion of that operation (for\nexample, the time between READ and MFC).\nMemory Cycle Time :- It is an important measure of the memory system. It is the minimum\ntime delay required between the initiations of two successive memory operations (for\nexample, the time between two successive READ operations). The cycle time is usually\nslightly longer than the access time.\nRandom Access Memory (RAM):\nPage 106 Computer Organization and Architecture\nA memory unit is called a Random Access Memory if any location can be accessed for a READ\nor WRITE operation in some fixed amount of time that is independent of the location\u2019s\naddress. Main memory units are of this type. This distinguishes them from serial or partly\nserial access storage devices such as magnetic tapes and disks which are used as the\nsecondary storage device.\nCache Memory:-\nThe CPU of a computer can usually process instructions and data faster than they can be\nfetched from compatibly priced main memory unit. Thus the memory cycle time become the\nbottleneck in the system. One way to reduce the memory access time is to use cache memory.\nThis is a small and fast memory that is inserted between the larger, slower main memory and\nthe CPU. This holds the currently active segments of a program and its data. Because of the\nlocality of address references, the CPU can, most of the time, find the relevant information in\nthe cache memory itself (cache hit) and infrequently needs access to the main memory (cache\nmiss) with suitable size of the cache memory, cache hit rates of over 90% are possible leading\nto a cost-effective increase in the performance of the system.\nMemory Interleaving: -\nThis technique divides the memory system into a number of memory modules and arranges\naddressing so that successive words in the address space are placed in different modules.\nWhen requests for memory access involve consecutive addresses, the access will be to\ndifferent modules. Since parallel access to these modules is possible, the\naverage rate of fetching words from the Main Memory can be increased.\nVirtual Memory: -\nIn a virtual memory System, the address generated by the CPU is referred to as a virtual or\nlogical address. The corresponding physical address can be different and the required\nmapping is implemented by a special memory control unit, often called the memory\nmanagement unit. The mapping function itself may be changed during program execution\naccording to system requirements.\nBecause of the distinction made between the logical (virtual) address space and the\nphysical address space; while the former can be as large as the addressing capability of the\nCPU, the actual physical memory can be much smaller. Only the active portion of the virtual\naddress space is mapped onto the physical memory and the rest of the virtual address space\nPage 107 Computer Organization and Architecture\nis mapped onto the bulk storage device used. If the addressed information is in the Main\nMemory (MM), it is accessed and execution proceeds.\nOtherwise, an exception is generated, in response to which the memory management\nunit transfers a contiguous block of words containing the desired word from the bulk storage\nunit to the MM, displacing some block that is currently inactive. If the memory is managed in\nsuch a way that, such transfers are required relatively infrequency (ie the CPU will generally\nfind the required information in the MM), the virtual memory system can provide a\nreasonably good performance and succeed in creating an illusion of a large memory with a\nsmall, in expensive MM.\nInternal Organization of Semiconductor Memory Chips:-\nMemory chips are usually organized in the form of an array of cells, in which each cell is\ncapable of storing one bit of information. A row of cells constitutes a memory word, and the\ncells of a row are connected to a common line referred to as the word line, and this line is\ndriven by the address decoder on the chip. The cells in each column are\nconnected to a sense\/write circuit by two lines known as bit lines. The sense\/write circuits\nare connected to the data input\/output lines of the chip. During a READ operation, the\nSense\/Write circuits sense, or read, the information stored in the cells selected by a word line\nand transmit this information to the output lines. During a write operation, they receive input\ninformation and store it in the cells of the selected word.\nThe following figure shows such an organization of a memory chip consisting of 16 words of 8\nbits each, which is usually referred to as a 16 x 8 organization.\nThe data input and the data output of each Sense\/Write circuit are connected to a single bi-\ndirectional data line in order to reduce the number of pins required. One control line, the\nR\/W (Read\/Write) input is used a specify the required operation and another control line, the\nCS (Chip Select) input is used to select a given chip in a multichip\nmemory system. This circuit requires 14 external connections, and allowing 2 pins for power\nsupply and ground connections, can be manufactured in the form of a 16-pin chip. It can store\n16 x 8 = 128 bits. Another type of organization for 1k x 1 format is shown below:\nPage 108 Computer Organization and Architecture\nThe 10-bit address is divided into two groups of 5 bits each to form the row and column\naddresses for the cell array. A row address selects a row of 32 cells, all of which are accessed\nin parallel. One of these, selected by the column address, is connected to the external data\nlines by the input and output multiplexers. This structure can store 1024 bits, can be\nimplemented in a 16-pin chip.\nA Typical Memory Cell\nSemiconductor memories may be divided into bipolar and MOS types. They may be compared\nas follows:\nPage 109 Computer Organization and Architecture\nTwo transistor inverters connected to implement a basic flip-flop. The cell is connected to one\nword line and two bits lines as shown. Normally, the bit lines are kept at about 1.6V, and the\nword line is kept at a slightly higher voltage of about 2.5V. Under these conditions, the two\ndiodes D1 and D2 are reverse biased. Thus, because no current flows through the diodes, the\ncell is isolated from the bit lines.\nRead Operation:\nLet us assume the Q1 on and Q2 off represents a 1 to read the contents of a given cell, the\nvoltage on the corresponding word line is reduced from 2.5 V to approximately 0.3 V. This\ncauses one of the diodes D1 or D2 to become forward-biased, depending on whether the\ntransistor Q1 or Q2 is conducting. As a result, current flows from bit line b when the cell is in\nthe 1 state and from bit line b when the cell is in the 0 state. The Sense\/Write circuit at the\nend of each pair of bit lines monitors the current on lines b and b\u2019 and sets the output bit line\naccordingly.\nWrite Operation:\nWhile a given row of bits is selected, that is, while the voltage on the corresponding word line\nis 0.3V, the cells can be individually forced to either the 1 state by applying a positive voltage\nof about 3V to line b\u2019 or to the 0 state by driving line b. This function is performed by the\nSense\/Write circuit.\nMOS Memory Cell:\nMOS technology is used extensively in Main Memory Units. As in the case of bipolar\nmemories, many MOS cell configurations are possible. The simplest of these is a flip-flop\ncircuit. Two transistors T1 and T2 are connected to implement a flip-flop. Active pull-up to\nVCC is provided through T3 and T4. Transistors T5 and T6 act as switches that can be opened\nor closed under control of the word line. For a read operation, when the cell is selected, T5 or\nT6 is closed and the corresponding flow of current through b or b\u2019 is sensed by the\nsense\/write circuits to set the output bit line accordingly. For a write operation, the bit is\nselected and a positive voltage is applied on the appropriate bit line, to store a 0 or 1. This\nconfiguration is shown below:\nPage 110 Computer Organization and Architecture\nStatic Memories Vs Dynamic Memories:-\nBipolar as well as MOS memory cells using a flip-flop like structure to store information can\nmaintain the information as long as current flow to the cell is maintained. Such memories are\ncalled static memories. In contracts, Dynamic memories require not only the maintaining of a\npower supply, but also a periodic \u201crefresh\u201d to maintain the information stored in them.\nDynamic memories can have very high bit densities and very lower power consumption\nrelative to static memories and are thus generally used to realize the main memory unit.\nDynamic Memories:-\nThe basic idea of dynamic memory is that information is stored in the form of a charge on the\ncapacitor. An example of a dynamic memory cell is shown below:\nWhen the transistor T is turned on and an appropriate voltage is applied to the bit line,\ninformation is stored in the cell, in the form of a known amount of charge stored on the\ncapacitor. After the transistor is turned off, the capacitor begins to discharge. This is caused\nby the capacitor\u2019s own leakage resistance and the very small amount of current that still flows\nthrough the transistor. Hence the data is read correctly only if is read before the charge on the\ncapacitor drops below some threshold value. During a Read\noperation, the bit line is placed in a high-impedance state, the transistor is turned on and a\nsense circuit connected to the bit line is used to determine whether the charge on the\ncapacitor is above or below the threshold value. During such a Read, the charge on the\ncapacitor is restored to its original value and thus the cell is refreshed with every read\noperation.\nPage 111 Computer Organization and Architecture\nTypical Organization of a Dynamic Memory Chip:-\nThe cells are organized in the form of a square array such that the high-and lower-order 8 bits\nof the 16-bit address constitute the row and column addresses of a cell, respectively. In order\nto reduce the number of pins needed for external connections, the row and column address\nare multiplexed on 8 pins.\nTo access a cell, the row address is applied first. It is loaded into the row address latch\nin response to a single pulse on the Row Address Strobe (RAS) input. This selects a row of\ncells. Now, the column address is applied to the address pins and is loaded into the column\naddress latch under the control of the Column Address Strobe (CAS) input and this address\nselects the appropriate sense\/write circuit. If the R\/W signal indicates a Read operation, the\noutput of the selected circuit is transferred to the data output. Do. For a write operation, the\ndata on the DI line is used to overwrite the cell selected.\nIt is important to note that the application of a row address causes all the cells on the\ncorresponding row to be read and refreshed during both Read and Write operations. To\nensure that the contents of a dynamic memory are maintained, each row of cells must be\naddressed periodically, typically once every two milliseconds. A Refresh circuit performs this\nfunction. Some dynamic memory chips in-corporate a refresh facility the chips themselves\nand hence they appear as static memories to the user! such chips are often referred to as\nPseudostatic.\nAnother feature available on many dynamic memory chips is that once the row\naddress is loaded, successive locations can be accessed by loading only column addresses.\nPage 112 Computer Organization and Architecture\nSuch block transfers can be carried out typically at a rate that is double that for transfers\ninvolving random addresses. Such a feature is useful when memory access follow a regular\npattern, for example, in a graphics terminal Because of their high density and low cost,\ndynamic memories are widely used in the main memory units of computers. Commercially\navailable chips range in size from 1k to 4M bits or more, and are available in various\norganizations like 64k x 1, 16k x 4, 1MB x 1 etc.\nRAID (Redundant Array of Independent Disks)\nRAID (redundant array of independent disks; originally redundant array of inexpensive\ndisks) provides a way of storing the same data in different places (thus, redundantly) on\nmultiple hard disks (though not all RAID levels provide redundancy). By placing data on\nmultiple disks, input\/output (I\/O) operations can overlap in a balanced way, improving\nperformance. Since multiple disks increase the mean time between failures (MTBF), storing\ndata redundantly also increases fault tolerance.\nRAID arrays appear to the operating system (OS) as a single logical hard disk. RAID\nemploys the technique of disk mirroring or disk striping, which involves partitioning each\ndrive's storage space into units ranging from a sector (512 bytes) up to several megabytes.\nThe stripes of all the disks are interleaved and addressed in order.\nIn a single-user system where large records, such as medical or other scientific images,\nare stored, the stripes are typically set up to be small (perhaps 512 bytes) so that a single\nrecord spans all disks and can be accessed quickly by reading all disks at the same time.\nIn a multi-user system, better performance requires establishing a stripe wide enough to hold\nthe typical or maximum size record. This allows overlapped disk I\/O across drives.\nStandard RAID levels\nRAID 0: This configuration has striping but no redundancy of data. It offers the best\nperformance but no fault-tolerance.\nPage 113 Computer Organization and Architecture\nRAID 1: Also known as disk mirroring, this configuration consists of at least two drives that\nduplicate the storage of data. There is no striping. Read performance is improved since either\ndisk can be read at the same time. Write performance is the same as for single disk storage.\nRAID 2: This configuration uses striping across disks with some disks storing error checking\nand correcting (ECC) information. It has no advantage over RAID 3 and is no longer used.\nRAID 3: This technique uses striping and dedicates one drive to storing parity information.\nThe embedded ECC information is used to detect errors. Data recovery is accomplished by\ncalculating the exclusive OR (XOR) of the information recorded on the other drives. Since an\nI\/O operation addresses all drives at the same time, RAID 3 cannot overlap I\/O. For this\nreason, RAID 3 is best for single-user systems with long record applications.\nPage 114 Computer Organization and Architecture\nRAID 4: This level uses large stripes, which means you can read records from any single\ndrive. This allows you to use overlapped I\/O for read operations. Since all write operations\nhave to update the parity drive, no I\/O overlapping is possible. RAID 4 offers no advantage\nover RAID 5.\nRAID 5: This level is based on block-level striping with parity. The parity information is\nstriped across each drive, allowing the array to function even if one drive were to fail. The\narray\u2019s architecture allows read and write operations to span multiple drives. This results in\nperformance that is usually better than that of a single drive, but not as high as that of a RAID\n0 array. RAID 5 requires at least three disks, but it is often recommended to use at least five\ndisks for performance reasons.\nRAID 5 arrays are generally considered to be a poor choice for use on write-intensive\nsystems because of the performance impact associated with writing parity information. When\na disk does fail, it can take a long time to rebuild a RAID 5 array. Performance is usually\ndegraded during the rebuild time and the array is vulnerable to an additional disk failure until\nthe rebuild is complete.\nPage 115 Computer Organization and Architecture\nRAID 6: This technique is similar to RAID 5 but includes a second parity scheme that is\ndistributed across the drives in the array. The use of additional parity allows the array to\ncontinue to function even if two disks fail simultaneously. However, this extra protection\ncomes at a cost. RAID 6 arrays have a higher cost per gigabyte (GB) and often have slower\nwrite performance than RAID 5 arrays.\nDirect Memory Access (DMA)\nDMA stands for \"Direct Memory Access\" and is a method of transferring data from\nthe computer's RAM to another part of the computer without processing it using the CPU.\nWhile most data that is input or output from your computer is processed by the CPU, some\ndata does not require processing, or can be processed by another device.\nIn these situations, DMA can save processing time and is a more efficient way to move\ndata from the computer's memory to other devices. In order for devices to use direct memory\naccess, they must be assigned to a DMA channel. Each type of port on a computer has a set of\nDMA channels that can be assigned to each connected device. For example, a PCI controller\nand a hard drive controller each have their own set of DMA channels.\nPage 116 Computer Organization and Architecture\nFor example, a sound card may need to access data stored in the computer's RAM, but since it\ncan process the data itself, it may use DMA to bypass the CPU. Video cards that support DMA\ncan also access the system memory and process graphics without needing the CPU. Ultra DMA\nhard drives use DMA to transfer data faster than previous hard drives that required the data\nto first be run through the CPU.\nAn alternative to DMA is the Programmed Input\/Output (PIO) interface in which all\ndata transmitted between devices goes through the processor. A newer protocol for the\nATAIIDE interface is Ultra DMA, which provides a burst data transfer rate up to 33 mbps.\nHard drives that come with Ultra DMAl33 also support PIO modes 1, 3, and 4, and multiword\nDMA mode 2 at 16.6 mbps.\nDMA Transfer Types\nMemory To Memory Transfer\nIn this mode block of data from one memory address is moved to another memory address.\nIn this mode current address register of channel 0 is used to point the source address and the\ncurrent address register of channel is used to point the destination address in the first\ntransfer cycle, data byte from the source address is loaded in the temporary register of the\nDMA controller and in the next transfer cycle the data from the temporary register is stored\nin the memory pointed by destination address.\nAfter each data transfer current address registers are decremented or incremented\naccording to current settings. The channel 1 current word count register is also decremented\nby 1 after each data transfer. When the word count of channel 1 goes to FFFFH, a TC is\ngenerated which activates EOP output terminating the DMA service.\nAuto initialize\nIn this mode, during the initialization the base address and word count registers are loaded\nsimultaneously with the current address and word count registers by the microprocessor.\nThe address and the count in the base registers remain unchanged throughout the DMA\nservice.\nAfter the first block transfer i.e. after the activation of the EOP signal, the original\nvalues of the current address and current word count registers are automatically restored\nfrom the base address and base word count register of that channel. After auto initialization\nthe channel is ready to perform another DMA service, without CPU intervention.\nPage 117 Computer Organization and Architecture\nDMA Controller\nThe controller is integrated into the processor board and manages all DMA data transfers.\nTransferring data between system memory and an 110 device requires two steps. Data goes\nfrom the sending device to the DMA controller and then to the receiving device. The\nmicroprocessor gives the DMA controller the location, destination, and amount of data that is\nto be transferred. Then the DMA controller transfers the data, allowing the microprocessor to\ncontinue with other processing tasks.\nWhen a device needs to use the Micro Channel bus to send or receive data, it competes\nwith all the other devices that are trying to gain control of the bus. This process is known as\narbitration. The DMA controller does not arbitrate for control of the BUS instead; the I\/O\ndevice that is sending or receiving data (the DMA slave) participates in arbitration. It is the\nDMA controller, however, that takes control of the bus when the central arbitration control\npoint grants the DMA slave's request.\nDMA vs. interrupts vs. polling\nA diagram showing the position of the DMA in relation to peripheral devices, the CPU and\ninternal memory\n\uf0b7 Works in the background without CPU intervention\n\uf0b7 This speed up data transfer and CPU speed\n\uf0b7 The DMA is used for moving large files since it would take too much of CPU capacity\nInterrupt Systems\n\uf0b7 Interrupts take up time of the CPU\nPage 118 Computer Organization and Architecture\n\uf0b7 they work by asking for the use of the CPU by sending the interrupt to which the CPU\nresponds\no Note: In order to save time the CPU does not check if it has to respond\n\uf0b7 Interrupts are used when a task has to be performed immediately\nPolling\nPolling requires the CPU to actively monitor the process\n\uf0b7 The major advantage is that the polling can be adjusted to the needs of the device\n\uf0b7 polling is a low level process since the peripheral device is not in need of a quick response\nUNIT \u2013 V (10 Lectures)\nPage 119 Computer Organization and Architecture\nMULTIPROCESSORS: Characteristics of multiprocessors, interconnection structures, inter\nprocessor arbitration, inter processor communication and synchronization, cache coherence,\nshared memory multiprocessors.\nBook: M. Moris Mano (2006), Computer System Architecture, 3rd edition, Pearson\/PHI,\nIndia: Unit-13 Pages: 489-514\nCharacteristics of Multiprocessors\nA multiprocessor system is an interconnection of two or more CPU, with memory and\ninput-output equipment. As defined earlier, multiprocessors can be put under MIMD\ncategory. The term multiprocessor is sometimes confused with the term multi computers.\nThough both support concurrent operations, there is an important difference between a\nsystem with multiple computers and a system with multiple processors.\nIn a multi computers system, there are multiple computers, with their own operating\nsystems, which communicate with each other, if needed, through communication links. A\nmultiprocessor system, on the other hand, is controlled by a single operating system, which\ncoordinate the activities of the various processors, either through shared memory or inter\nprocessor messages.\nThe advantages of multiprocessor systems are:\n\u00b7 Increased reliability because of redundancy in processors\n\u00b7 Increased throughput because of execution of multiple jobs in parallel portions of the\nsame job in parallel\nA single job can be divided into independent tasks, either manually by the programmer, or by\nthe compiler, which finds the portions of the program that are data independent, and can be\nexecuted in parallel. The multiprocessors are further classified into two groups depending on\nthe way their memory is organized. The processors with shared memory are called tightly\ncoupled or shared memory processors.\nThe information in these processors is shared through the common memory. Each of\nthe processors can also have their local memory too. The other class of multiprocessors is\nloosely coupled or distributed memory multi-processors. In this, each processor has their\nown private memory, and they share information with each other through interconnection\nswitching scheme or message passing.\nPage 120 Computer Organization and Architecture\nThe principal characteristic of a multiprocessor is its ability to share a set of main\nmemory and some I\/O devices. This sharing is possible through some physical connections\nbetween them called the interconnection structures.\nInter processor Arbitration\nComputer system needs buses to facilitate the transfer of information between its\nvarious components. For example, even in a uniprocessor system, if the CPU has to access a\nmemory location, it sends the address of the memory location on the address bus. This\naddress activates a memory chip. The CPU then sends a red signal through the control bus, in\nthe response of which the memory puts the data on the address bus.\nThis address activates a memory chip. The CPU then sends a read signal through the\ncontrol bus, in the response of which the memory puts the data on the data bus. Similarly, in a\nmultiprocessor system, if any processor has to read a memory location from the shared areas,\nit follows the similar routine.\nThere are buses that transfer data between the CPUs and memory. These are called\nmemory buses. An I\/O bus is used to transfer data to and from input and output devices. A\nbus that connects major components in a multiprocessor system, such as CPUs, I\/Os, and\nmemory is called system bus. A processor, in a multiprocessor system, requests the access of\na component through the system bus.\nIn case there is no processor accessing the bus at that time, it is given then control of\nthe bus immediately. If there is a second processor utilizing the bus, then this processor has\nto wait for the bus to be freed. If at any time, there is request for the services of the bus by\nmore than one processor, then the arbitration is performed to resolve the conflict. A bus\ncontroller is placed between the local bus and the system bus to handle this.\nInter processor Communication and Synchronization\nIn a multiprocessor system, it becomes very necessary, that there be proper communication\nprotocol between the various processors. In a shared memory multiprocessor system, a\ncommon area in the memory is provided, in which all the messages that need to be\ncommunicated to other processors are written.\nPage 121 Computer Organization and Architecture\nA proper synchronization is also needed whenever there is a race of two or more\nprocessors for shared resources like I\/O resources. The operating system in this case is given\nthe task of allocating the resources to the various processors in a way, that at any time not\nmore than one processor use the resource.\nA very common problem that can occur when two or more resources are trying to\naccess a resource which can be modified. For example processor 1 and 2 are simultaneously\ntrying to access memory location 100. Say the processor 1 is writing on to the location while\nprocessor 2 is reading it. The chances are that processor 2 will end up reading erroneous\ndata. Such kind of resources which need to be protected from simultaneous access of more\nthan one processors are called critical sections. The following assumptions are made\nregarding the critical sections:\n- Mutual exclusion: At most one processor can be in a critical section at a time\n-Termination : The critical section is executed in a finite time\n- Fair scheduling: A process attempting to enter the critical section will eventually do so in a\nfinite time.\nA binary value called a semaphore is usually used to indicate whether a processor is currently\nExecuting the critical section.\nCache Coherence\nAs discussed in unit 2, cache memories are high speed buffers which are inserted\nbetween the processor and the main memory to capture those portions of the contents of\nmain memory which are currently in use. These memories are five to ten times faster than\nmain memories, and therefore, reduce the overall access time. In a multiprocessor system,\nwith shared memory, each processor has its own set of private cache.\nMultiple copies of the cache are provided with each processor to reduce the access\ntime. Each processor, whenever accesses the shared memory, also updates its private cache.\nThis introduced the problem of cache coherence, which may result in data inconsistency. That\nis, several copies of the same data may exist in different caches at any given time.\nFor example, let us assume there are two processors x and y. Both have the same copy of the\ncache. Processor x, produces data 'a' which is to be consumed by processor y. Processor\nupdate the value of 'a' in its own private copy of the cache. As it does not have any access to\nPage 122 Computer Organization and Architecture\nthe private copy of cache of processor y, the processor y continues to use the variable 'a' with\nold value, unless it is informed of the change.\nThus, in such kind of situations if the system is to perform correctly, every updation in\nthe cache should be informed to all the processors, so that they can make necessary changes\nin their private copies of the cache.\nPage 123 "}