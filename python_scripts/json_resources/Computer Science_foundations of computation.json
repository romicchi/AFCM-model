{"text":"Chapter 1\nLogic and Proof\nI\nn a sense, we know a lot more than we realize, because everything\nthat we know has consequences\u2014logical consequences\u2014that follow auto-\nmatically. If you know that all humans are mortal, and you know that\nyou are human, then in a sense you know that you are mortal, whether or\nnot you have ever considered or wanted to consider that fact. This is an\nexample of logical deduction: From the premises that \u201cAll humans are\nmortal\u201d and \u201cI am human,\u201d the conclusion that \u201cI am mortal\u201d can be\ndeduced by logic.\nLogical deduction is a kind of computation. By applying rules of logic\nto a given set of premises, conclusions that follow from those premises can\nbe generated automatically, by a computational process which could be\ncarried out by a computer. Once you know the premises, or are willing to\naccept them for the sake of argument, you are forced\u2014by logic\u2014to accept\nthe conclusions. Still, to say that you \u201cknow\u201d those conclusions would be\nmisleading. The problem is that there are too many of them (infinitely\nmany),and,ingeneral,mostofthemarenotparticularlyinteresting. Until\nyouhaveactuallymadethededuction,youdon\u2019treally knowtheconclusion,\nandknowingwhichofthepossiblechainsofdeductiontofollowisnoteasy.\nThe art of logic is to find an interesting conclusion and a chain of logical\ndeductions that leads from the premises to that conclusion. Checking that\nthe deductions are valid is the mechanical, computational side of logic.\nThischapterismostlyaboutthemechanicsoflogic. Wewillinvestigate\nlogicasabranchofmathematics,withitsownsymbols,formulas,andrules\nof computation. Your object is to learn the rules of logic, to understand\nwhy they are valid, and to develop skill in applying them. As with any\nbranch of mathematics, there is a certain beauty to the symbols and for-\n1 2 CHAPTER 1. LOGIC AND PROOF\nmulas themselves. But it is the applications that bring the subject to life\nformostpeople. Wewill,ofcourse,coversomeapplicationsaswegoalong.\nIn a sense, though, the real applications of logic include much of computer\nscience and of mathematics itself.\nAmongthefundamentalelementsofthought,andthereforeoflogic,are\npropositions. A proposition is a statement that has a truth value: It is\neither true or false. \u201cGrass is green\u201d and \u201c2+2=5\u201d are propositions. In\nthe first part of this chapter, we will study propositional logic, which\ntakes propositions as basic and considers how they can be combined and\nmanipulated. This branch of logic has surprising application to the design\nof the electronic circuits that make up computers.\nLogic gets more interesting when we consider the internal structure of\npropositions. In English, a proposition is expressed as a sentence, and, as\nyouknowfromstudyinggrammar,sentenceshaveparts. Asimplesentence\nlike \u201cGrass is green\u201d has a subject and a predicate. The sentence says\nsomething about its subject. The subject of \u201cGrass is green\u201d is grass. The\nsentence says something about grass. The something that the sentence\nsaysabout its subjectis thepredicate. Intheexample, the predicate isthe\nphrase \u201cis green.\u201d Once we start working with predicates, we can create\npropositions using quantifiers like \u201call,\u201d \u201csome,\u201d and \u201cno.\u201d For example,\nworking with the predicate \u201cis above average,\u201d we can move from simple\npropositions like \u201cJohnny is above average\u201d to \u201cAll children are above av-\nerage\u201d or to \u201cNo child is above average\u201d or to the rather more realistic\n\u201cSome children are above average.\u201d Logical deduction usually deals with\nquantified statements, as shown by the basic example of human mortality\nwith which we began this chapter. Logical deduction will be a major topic\nof this chapter; under the name of proof, it will be the last major topic of\nthis chapter, and a major tool for the rest of this book.\n1.1 Propositional Logic\nA proposition is a statement which is either true or false. In propositional\nlogic, we take propositions as basic and see what we can do with them.\nSince this is mathematics, we need to be able to talk about propositions\nwithout saying which particular propositions we are talking about, so we\nusesymbolicnamestorepresentthem. Wewillalwaysuselowercaseletters\nsuch as p, q, and r to represent propositions. A letter used in this way is\ncalled a propositional variable. Remember that when I say something\nlike\u201cLetpbeaproposition,\u201dImean\u201cFortherestofthisdiscussion,letthe\nsymbol p stand for some particular statement, which is either true or false 1.1. PROPOSITIONAL LOGIC 3\n(although I am not at the moment making any assumption about which it\nis).\u201d Thediscussionhasmathematicalgenerality inthatpcanrepresent\nany statement, and the discussion will be valid no matter which statement\nit represents.\nWhatwedowithpropositionsiscombinethemwithlogicaloperators.\nA logical operator can be applied to one or more propositions to produce\na new proposition. The truth value of the new proposition is completely\ndetermined by the operator and by the truth values of the propositions to\nwhich it is applied.1 In English, logical operators are represented by words\nsuch as \u201cand,\u201d \u201cor,\u201d and \u201cnot.\u201d For example, the proposition \u201cI wanted\nto leave and I left\u201d is formed from two simpler propositions joined by the\nword\u201cand.\u201d Addingtheword\u201cnot\u201dtotheproposition\u201cIleft\u201dgives\u201cIdid\nnot leave\u201d (after a bit of necessary grammatical adjustment).\nBut English is a little too rich for mathematical logic. When you read\nthesentence\u201cIwantedtoleaveandIleft,\u201dyouprobablyseeaconnotationof\ncausality: I left because I wanted to leave. This implication does not follow\nfrom the logical combination of the truth values of the two propositions\n\u201cI wanted to leave\u201d and \u201cI left.\u201d Or consider the proposition \u201cI wanted\nto leave but I did not leave.\u201d Here, the word \u201cbut\u201d has the same logical\nmeaning as the word \u201cand,\u201d but the connotation is very different. So, in\nmathematical logic, we use symbols to represent logical operators. These\nsymbolsdonotcarryanyconnotationbeyondtheirdefinedlogicalmeaning.\nThe logical operators corresponding to the English words \u201cand,\u201d \u201cor,\u201dand\n\u201cnot\u201d are , , and .\n\u2227 \u2228 \u00ac\nDefinition 1.1. Letpandq bepropositions. Thenp q, p q, and pare\n\u2228 \u2227 \u00ac\npropositions, whose truth values are given by the rules:\np q is true when both p is true and q is true, and in no other case.\n\u2022 \u2227\np q is true when either p is true, or q is true, or both p and q are\n\u2022 \u2228\ntrue, and in no other case.\np is true when p is false, and in no other case.\n\u2022 \u00ac\nThe operators , , and are referred to as conjunction, disjunction,\n\u2227 \u2228 \u00ac\nand negation, respectively. (Note that p q is read as \u201cp and q,\u201d p q is\n\u2227 \u2228\nread as \u201cp or q,\u201d and p is read as \u201cnot p.\u201d)\n\u00ac\n1It is not always true that the truth value of a sentence can be determined from\nthetruthvaluesofitscomponentparts. Forexample,ifpisaproposition,then\u201cSarah\nPalinbelievesp\u201disalsoaproposition,so\u201cSarahPalinbelieves\u201dissomekindofoperator.\nHowever, itdoesnotcountasalogical operatorbecausejustfromknowingwhetheror\nnot p is true, we get no information at all about whether \u201cSarah Palin believes p\u201d is\ntrue. 4 CHAPTER 1. LOGIC AND PROOF\nThese operators can be used in more complicated expressions, such as\np ( q) or (p q) (q r). A proposition made up of simpler propositions\n\u2227 \u00ac \u2228 \u2227 \u2228\nand logical operators is called a compound proposition. Parentheses\ncan be used in compound expressions to indicate the order in which the\noperators are to be evaluated. In the absence of parentheses, the order of\nevaluation is determined by precedence rules. For the logical operators\ndefinedabove,therulesarethat hashigherprecedencethat ,and has\n\u00ac \u2227 \u2227\nprecedence over . This means that in the absence of parentheses, any\n\u2228 \u00ac\noperators are evaluated first, followed by any operators, followed by any\n\u2227\noperators.\n\u2228\nFor example, the expression p q r is equivalent to the expression\n\u00ac \u2228 \u2227\n( p) (q r),whilep q q r isequivalenttop (q q) r. Asapractical\n\u00ac \u2228 \u2227 \u2228 \u2227 \u2228 \u2228 \u2227 \u2228\nmatter, when you make up your own expressions, it is usually better to\nput in parentheses to make your meaning clear. Remember that even if\nyou leave out parentheses, your expression has an unambiguous meaning.\nIf you say \u201c p q\u201d when what you meant was \u201c (p q),\u201d you\u2019ve got it\n\u00ac \u2227 \u00ac \u2227\nwrong!\nThis still leaves open the question of which of the operators in the\n\u2227\nexpression p q r is evaluated first. This is settled by the following\n\u2227 \u2227\nrule: When several operators of equal precedence occur in the absence of\nparentheses, they are evaluated from left to right. Thus, the expression\np q r is equivalent to (p q) r rather than to p (q r). In this\n\u2227 \u2227 \u2227 \u2227 \u2227 \u2227\nparticularcase,asamatteroffact,itdoesn\u2019treallymatterwhich operator\n\u2227\nisevaluatedfirst,sincethetwocompoundpropositions(p q) randp (q\n\u2227 \u2227 \u2227 \u2227\nr)alwayshavethesamevalue,nomatterwhatlogicalvaluesthecomponent\npropositions p, q, and r have. We say that is an associative operation.\n\u2227\nWe\u2019llseemoreaboutassociativityandotherpropertiesofoperationsinthe\nnext section.\nSuppose we want to verify that, in fact, (p q) r and p (q r) do\n\u2227 \u2227 \u2227 \u2227\nalways have the same value. To do so, we have to consider all possible\ncombinations of values of p, q, and r, and check that for all such combina-\ntions, the two compound expressions do indeed have the same value. It is\nconvenienttoorganizethiscomputationintoatruthtable. Atruth table\nis a table that shows the value of one or more compound propositions for\neachpossiblecombinationofvaluesofthepropositionalvariablesthatthey\ncontain. Figure1.1isatruthtablethatcomparesthevalueof(p q) r to\n\u2227 \u2227\nthe value of p (q r) for all possible values of p, q, and r. There are eight\n\u2227 \u2227\nrows in the table because there are exactly eight different ways in which\ntruth values can be assigned to p, q, and r.2 In this table, we see that the\n2Ingeneral,iftherearenvariables,thenthereare2n differentwaystoassigntruth\nvalues to the variables. This might become clear to you if you try to come up with a 1.1. PROPOSITIONAL LOGIC 5\np q r p q q r (p q) r p (q r)\n\u2227 \u2227 \u2227 \u2227 \u2227 \u2227\nfalse false false false false false false\nfalse false true false false false false\nfalse true false false false false false\nfalse true true false true false false\ntrue false false false false false false\ntrue false true false false false false\ntrue true false true false false false\ntrue true true true true true true\nFigure 1.1: A truth table that demonstrates the logical equivalence\nof (p q) r and p (q r). The fact that the last two columns\n\u2227 \u2227 \u2227 \u2227\nof this table are identical shows that these two expressions have the\nsame value for all eight possible combinations of values of p, q, and\nr.\nlast two columns, representing the values of (p q) r and p (q r), are\n\u2227 \u2227 \u2227 \u2227\nidentical.\nMore generally, we say that two compound propositions are logically\nequivalentiftheyalwayshavethesamevalue,nomatterwhattruthvalues\nareassignedtothepropositionalvariablesthattheycontain. Ifthenumber\nof propositional variables is small, it is easy to use a truth table to check\nwhether or not two propositions are logically equivalent.\nThere are other logical operators besides , , and . We will consider\n\u2227 \u2228 \u00ac\nthe conditional operator, , the biconditional operator, , and the\n\u2192 \u2194\nexclusive or operator, .3 These operators can be completely defined\n\u2295\nby a truth table that shows their values for the four possible combinations\nof truth values of p and q.\nDefinition 1.2. For any propositions p and q, we define the propositions\np q, p q, and p q according to the truth table:\n\u2192 \u2194 \u2295\nschemeforsystematicallylistingallpossiblesetsofvalues. Ifnot,you\u2019llfindarigorous\nproofofthefactlaterinthischapter.\n3Notethatthesymbolsusedinthisbookforthelogicaloperatorsarenotuniversal.\nWhile \u2227, \u2228, and \u2192 are fairly standard, \u00ac is often replaced by \u223c and \u2194 is sometimes\nrepresentedby\u2261or\u21d4. Thereisevenlessstandardizationoftheexclusiveoroperator,\nbutthatoperatorisgenerallynotsoimportantastheothers. 6 CHAPTER 1. LOGIC AND PROOF\np q p q p q p q\n\u2192 \u2194 \u2295\nfalse false true true false\nfalse true true false true\ntrue false false false true\ntrue true true true false\nWhen these operators are used in expressions, in the absence of paren-\nthesestoindicateorderofevaluation,weusethefollowingprecedencerules:\nThe exclusive or operator, , has the same precedence as . The condi-\n\u2295 \u2228\ntional operator, , has lower precedence than , , , and , and is\n\u2192 \u2227 \u2228 \u00ac \u2295\ntherefore evaluated after them. Finally, the biconditional operator, ,\n\u2194\nhas the lowest precedence and is therefore evaluated last. For example,\nthe expression \u201cp q r p s\u201d is evaluated as if it were written\n\u2192 \u2227 \u2194 \u00ac \u2295\n\u201c(p (q r)) (( p) s).\u201d\n\u2192 \u2227 \u2194 \u00ac \u2295\nInordertoworkeffectivelywiththelogicaloperators,youneedtoknow\nmore about their meaning and how they relate to ordinary English expres-\nsions.\nThe proposition p q is called an implication or a conditional. It\n\u2192\nis usually read as \u201cp implies q.\u201d In English, p q is often expressed as \u201cif\n\u2192\npthenq.\u201d Forexample, ifprepresentstheproposition\u201cBillGatesispoor\u201d\nand q represents \u201cthe moon is made of green cheese,\u201d then p q could be\n\u2192\nexpressed in English as \u201cIf Bill Gates is poor, then the moon is made of\ngreen cheese.\u201d In this example, p is false and q is also false. Checking the\ndefinition of p q, we see that p q is a true statement. Most people\n\u2192 \u2192\nwould agree with this. It\u2019s worth looking at a similar example in more\ndetail. Suppose that I assert that \u201cIf the Mets are a great team, then I\u2019m\nthe king of France.\u201d This statement has the form m k where m is the\n\u2192\nproposition \u201cthe Mets are a great team\u201d and k is the proposition \u201cI\u2019m the\nking of France.\u201d Now, demonstrably I am not the king of France, so k is\nfalse. Sincek isfalse,theonlywayform k tobetrueisformtobefalse\n\u2192\nas well. (Check the definition of in the table!) So, by asserting m k,\n\u2192 \u2192\nI am really asserting that the Mets are not a great team.\nOr consider the statement, \u201cIf the party is on Tuesday, then I\u2019ll be\nthere.\u201d What am I trying to say if I assert this statement? I am asserting\nthat p q is true, where p represents \u201cThe party is on Tuesday\u201d and q\n\u2192\nrepresents\u201cIwillbeattheparty.\u201d Supposethatpistrue,thatis,theparty\ndoes in fact take place on Tuesday. Checking the definition of , we see\n\u2192\nthat in the only case where p is true and p q is true, q is also true. So\n\u2192\nfrom the truth of \u201cIf the party is on Tuesday, then I will be at the party\u201d\nand \u201cThe party is in fact on Tuesday,\u201d you can deduce that \u201cI will be at\nthe party\u201d is also true. But suppose, on the other hand, that the party is 1.1. PROPOSITIONAL LOGIC 7\nactually on Wednesday. Then p is false. When p is false and p q is true,\n\u2192\nthe definition of p q allows q to be either true or false. So, in this case,\n\u2192\nyou can\u2019t make any deduction about whether or not I will be at the party.\nThestatement\u201cIfthepartyisonTuesday,thenI\u2019llbethere\u201ddoesn\u2019tassert\nanything about what will happen if the party is on some other day than\nTuesday.\nTheimplication( q) ( p)iscalledthecontrapositive ofp q. An\n\u00ac \u2192 \u00ac \u2192\nimplication is logically equivalent to its contrapositive. The contrapositive\nof \u201cIf this is Tuesday, then we are in Belgium\u201d is \u201cIf we aren\u2019t in Belgium,\nthenthisisn\u2019tTuesday.\u201d Thesetwosentencesassertexactlythesamething.\nNote that p q is not logically equivalent to q p. The implication\n\u2192 \u2192\nq p is called the converse of p q. The converse of \u201cIf this is Tuesday,\n\u2192 \u2192\nthenweareinBelgium\u201dis\u201cIfweareinBelgium,thenthisisTuesday.\u201d Note\nthatitispossibleforeitheroneofthesestatementstobetruewhiletheother\nisfalse. InEnglish,Imightexpressthefactthatbothstatementsaretrueby\nsaying\u201cIfthisisTuesday,thenweareinBelgium,andconversely.\u201d Inlogic,\nthis would be expressed with a proposition of the form (p q) (q p).\n\u2192 \u2227 \u2192\nThebiconditionaloperatoriscloselyrelatedtotheconditionaloperator.\nIn fact, p q is logically equivalent to (p q) (q p). The proposition\n\u2194 \u2192 \u2227 \u2192\np q is usually read as \u201cp if and only if q.\u201d (The \u201cp if q\u201d part represents\n\u2194\nq p, while \u201cp only if q\u201d is another way of asserting that p q.) It\n\u2192 \u2192\ncould also be expressed as \u201cif p then q, and conversely.\u201d Occasionally in\nEnglish, \u201cif... then\u201d is used when what is really meant is \u201cif and only if.\u201d\nForexample,ifaparenttellsachild,\u201cIfyouaregood,Santawillbringyou\ntoys,\u201d the parent probably really means to say \u201cSanta will bring you toys\nif and only if you are good.\u201d (The parent would probably not respond well\ntothechild\u2019sperfectlylogicalplea\u201cButyouneversaidwhatwouldhappen\nif I wasn\u2019t good!\u201d)\nFinally, we turn to the exclusive or operator. The English word \u201cor\u201d\nis actually somewhat ambiguous. The two operators and express the\n\u2295 \u2228\ntwopossiblemeaningsofthisword. Thepropositionp q canbeexpressed\n\u2228\nunambiguously as \u201cp or q, or both,\u201d while p q stands for \u201cp or q, but not\n\u2295\nboth.\u201d If a menu says that you can choose soup or salad, it doesn\u2019t mean\nthat you can have both. In this case, \u201cor\u201d is an exclusive or. On the other\nhand, in \u201cYou are at risk of heart disease if you smoke or drink,\u201d the or is\ninclusive since you certainly don\u2019t get off the hook if you both smoke and\ndrink. Inmathematics, theword\u201cor\u201disalwaystakenintheinclusivesense\nof p q.\n\u2228\nNow, any compound proposition that uses any of the operators , ,\n\u2192 \u2194\nand can be rewritten as a logically equivalent proposition that uses only\n\u2295\n, ,and . Itiseasytocheckthatp qislogicallyequivalentto( p) q.\n\u2227 \u2228 \u00ac \u2192 \u00ac \u2228 8 CHAPTER 1. LOGIC AND PROOF\n(Just make a truth table for ( p) q.) Similarly, p q can be expressed\n\u00ac \u2228 \u2194\nas (( p) q) (( q) p), So, in a strict logical sense, , , and are\n\u00ac \u2228 \u2227 \u00ac \u2228 \u2192 \u2194 \u2295\nunnecessary. (Nevertheless, they are useful and important, and we won\u2019t\ngive them up.)\nEven more is true: In a strict logical sense, we could do without the\nconjunction operator . It\u2019s easy to check that p q is logically equivalent\n\u2227 \u2227\nto ( p q),soanyexpressionthatuses canberewrittenasonethatuses\n\u00ac \u00ac \u2228\u00ac \u2227\nonly and . Alternatively, we could do without and write everything\n\u00ac \u2228 \u2228\nin terms of and .\n\u00ac \u2227\nCertain types of proposition will play a special role in our further work\nwithlogic. Inparticular,wedefinetautologiesandcontradictionsasfollows:\nDefinition 1.3. A compound proposition is said to be a tautology if\nand only if it is true for all possible combinations of truth values of the\npropositional variables which it contains. A compound proposition is said\ntobeacontradictionifandonlyifitisfalse forallpossiblecombinations\nof truth values of the propositional variables which it contains.\nForexample, theproposition((p q) q) pisatautology. Thiscan\n\u2228 \u2227\u00ac \u2192\nbe checked with a truth table:\np q p q q (p q) q ((p q) q) p\n\u2228 \u00ac \u2228 \u2227\u00ac \u2228 \u2227\u00ac \u2192\nfalse false false true false true\nfalse true true false false true\ntrue false true true true true\ntrue true true false false true\nThe fact that all entries in the last column are true tells us that this\nexpression is a tautology. Note that for any compound proposition P, P is\na tautology if and only if P is a contradiction. (Here and in the future,\n\u00ac\nI use uppercase letters to represent compound propositions. P stands for\nany formula made up of simple propositions, propositional variables, and\nlogicaloperators.) Logicalequivalencecanbedefinedintermsoftautology:\nDefinition 1.4. Two compound propositions, P and Q, are said to be\nlogically equivalent if and only if the proposition P Q is a tautology.\n\u2194\nThe assertion that P is logically equivalent to Q will be expressed sym-\nbolically as \u201cP Q.\u201d For example, (p q) ( p q), and p q\n\u2261 \u2192 \u2261 \u00ac \u2228 \u2295 \u2261\n(p q) (p q).\n\u2228 \u2227\u00ac \u2227 1.1. PROPOSITIONAL LOGIC 9\nExercises\n1. Give the three truth tables that define the logical operators , , and .\n\u2227 \u2228 \u00ac\n2. Insertparenthesesintothefollowingcompoundpropositionstoshowtheorder\nin which the operators are evaluated:\na) p q b) p q p c) p q r d) p q r\n\u00ac \u2228 \u2227 \u2228\u00ac \u2228 \u2227 \u2227\u00ac \u2228\n3. List the 16 possible combinations of truth values for the four propositional\nvariabless,p,q,r. Trytofindasystematicwaytolistthevalues. (Hint: Start\nwith the eight combinations of values for p, q, and r, as given in the truth\ntable in Figure 1.1. Now, explain why there are 32 possible combinations of\nvaluesforfivevariables,anddescribehowtheycouldbelistedsystematically.)\n4. Some of the following compound propositions are tautologies, some are con-\ntradictions,andsomeareneither. Ineachcase,useatruthtabletodecideto\nwhich of these categories the proposition belongs:\na) (p (p q)) q b) ((p q) (q r)) (p r)\n\u2227 \u2192 \u2192 \u2192 \u2227 \u2192 \u2192 \u2192\nc) p ( p) d) (p q) (p q)\n\u2227 \u00ac \u2228 \u2192 \u2227\ne) p ( p) f) (p q) (p q)\n\u2228 \u00ac \u2227 \u2192 \u2228\n5. Use truth tables to show that each of the following propositions is logically\nequivalent to p q.\n\u2194\na) (p q) (q p) b) ( p) ( q)\n\u2192 \u2227 \u2192 \u00ac \u2194 \u00ac\nc) (p q) (( p) ( q)) d) (p q)\n\u2192 \u2227 \u00ac \u2192 \u00ac \u00ac \u2295\n6. Is anassociativeoperation? Thisis,is(p q) r logicallyequivalentto\n\u2192 \u2192 \u2192\np (q r)? Is associative?\n\u2192 \u2192 \u2194\n7. Letprepresenttheproposition\u201cYouleave\u201dandletqrepresenttheproposition\n\u201cI leave.\u201d Express the following sentences as compound propositions using p\nand q, and show that they are logically equivalent:\na) Either you leave or I do. (Or both!)\nb) If you don\u2019t leave, I will.\n8. Suppose that m represents the proposition \u201cThe Earth moves,\u201d c represents\n\u201cTheEarthisthecenteroftheuniverse,\u201dandg represents\u201cGalileowasrail-\nroaded.\u201d TranslateeachofthefollowingcompoundpropositionsintoEnglish:\na) g c b) m c\n\u00ac \u2227 \u2192\u00ac\nc) m c d) (m g) (c g)\n\u2194\u00ac \u2192 \u2227 \u2192\u00ac\n9. Give the converse and the contrapositive of each of the following English\nsentences:\na) If you are good, Santa brings you toys.\nb) Ifthepackageweighsmorethanoneounce,thenyouneedextrapostage.\nc) If I have a choice, I don\u2019t eat eggplant.\n10. In an ordinary deck of fifty-two playing cards, for how many cards is it true\na) that \u201cThis card is a ten and this card is a heart\u201d?\nb) that \u201cThis card is a ten or this card is a heart\u201d?\nc) that \u201cIf this card is a ten, then this card is a heart\u201d? 10 CHAPTER 1. LOGIC AND PROOF\nd) that \u201cThis card is a ten if and only if this card is a heart\u201d?\n11. Define a logical operator so that p q is logically equivalent to (p q).\n\u2193 \u2193 \u00ac \u2228\n(Thisoperatorisusuallyreferredtoas\u201cnor,\u201dshortfor\u201cnotor\u201d). Showthat\neach of the propositions p, p q, p q, p q, p q, and p q can be\n\u00ac \u2227 \u2228 \u2192 \u2194 \u2295\nrewrittenasalogicallyequivalentpropositionthatuses asitsonlyoperator.\n\u2193\n1.2 Boolean Algebra\nSo far we have discussed how to write and interpret propositions. This\nsectiondealswithmanipulating them. Forthis, weneedalgebra. Ordinary\nalgebra, of the sort taught in high school, is about manipulating numbers,\nvariables that represent numbers, and operators such as + and that\n\u00d7\napply to numbers. Now, we need an algebra that applies to logical values,\npropositional variables, and logical operators. The first person to think\nof logic in terms of algebra was the mathematician, George Boole, who\nintroduced the idea in a book that he published in 1854. The algebra of\nlogic is now called Boolean algebra in his honor.\nThe algebra of numbers includes a large number of rules for manipu-\nlating expressions. The distributive law, for example, says that x(y+z)=\nxy +xz, where x, y, and z are variables that stand for any numbers or\nnumerical expressions. This law means that whenever you see something\nof the form xy+xz in a numerical expression, you can substitute x(y+z)\nwithoutchangingthevalueoftheexpression,andvice versa. Notethatthe\nequals sign in x(y+z)=xy+xz means \u201chas the same value as, no matter\nwhat numerical values x, y, and z have.\u201d\nIn Boolean algebra, we work with logical values instead of numerical\nvalues. There are only two logical values, true and false. We will write\nthesevaluesasTandF. ThesymbolsTandFplayasimilarroleinBoolean\nalgebra to the role that constant numbers such as 1 and 3.14159 play in\nordinary algebra. Instead of the equals sign, Boolean algebra uses logical\nequivalence, , which has essentially the same meaning.4 For example, for\n\u2261\npropositions p, q, and r, the operator in p (q r) (p q) r means\n\u2261 \u2227 \u2227 \u2261 \u2227 \u2227\n\u201chas the same value as, no matter what logical values p, q, and r have.\u201d\nMany of the rules of Boolean algebra are fairly obvious, if you think a\nbitaboutwhattheymean. Eventhosethatarenotobviouscanbeverified\neasily by using a truth table. Figure 1.2 lists the most important of these\n4In ordinary algebra, it\u2019s easy to be confused by the equals sign, because it has two\nverydifferentroles. Inanidentitysuchasthedistributivelaw,itmeans\u201cisalwaysequal\nto.\u201d On the other hand, an equation such as x2+3x=4 is a statement that might or\nmight not be true, depending on the value of x. Boolean algebra has two operators, \u2261\nand\u2194,thatplayrolessimilartothetworolesoftheequalssign. 1.2. BOOLEAN ALGEBRA 11\nDouble negation ( p) p\n\u00ac \u00ac \u2261\nExcluded middle p p T\n\u2228\u00ac \u2261\nContradiction p p F\n\u2227\u00ac \u2261\nIdentity laws T p p\n\u2227 \u2261\nF p p\n\u2228 \u2261\nIdempotent laws p p p\n\u2227 \u2261\np p p\n\u2228 \u2261\nCommutative laws p q q p\n\u2227 \u2261 \u2227\np q q p\n\u2228 \u2261 \u2228\nAssociative laws (p q) r p (q r)\n\u2227 \u2227 \u2261 \u2227 \u2227\n(p q) r p (q r)\n\u2228 \u2228 \u2261 \u2228 \u2228\nDistributive laws p (q r) (p q) (p r)\n\u2227 \u2228 \u2261 \u2227 \u2228 \u2227\np (q r) (p q) (p r)\n\u2228 \u2227 \u2261 \u2228 \u2227 \u2228\nDeMorgan\u2019s laws (p q) ( p) ( q)\n\u00ac \u2227 \u2261 \u00ac \u2228 \u00ac\n(p q) ( p) ( q)\n\u00ac \u2228 \u2261 \u00ac \u2227 \u00ac\nFigure1.2: LawsofBooleanAlgebra. Theselawsholdforanypropo-\nsitions p, q, and r.\nlaws. You will notice that all these laws, except the first, come in pairs:\nEach law in the pair can be obtained from the other by interchanging\n\u2227\nwith and T with F. This cuts down on the number of facts you have to\n\u2228\nremember.5\nJust as an example, let\u2019s verify the first rule in the table, the Law of\nDouble Negation. This law is just the old, basic grammar rule that two\nnegatives make a positive. Although this rule is questionable as it applies\nto English as it is actually used\u2014no matter what the grammarian says,\n\u201cI can\u2019t get no satisfaction\u201d doesn\u2019t really mean \u201cI can get satisfaction\u201d\u2014\nthe validity of the rule in logic can be verified just by computing the two\npossible cases: when p is true and when p is false. When p is true, then\nby the definition of the operator, p is false. But then, again by the\n\u00ac \u00ac\n5It is also an example of a more general fact known as duality, which asserts that\ngivenanytautologythatusesonlytheoperators\u2227,\u2228,and\u00ac,anothertautologycanbe\nobtained from it by interchanging \u2227 with \u2228 and T with F. We won\u2019t attempt to prove\nthishere. 12 CHAPTER 1. LOGIC AND PROOF\ndefinition of , the value of ( p) is true, which is the same as the value\n\u00ac \u00ac \u00ac\nof p. Similarly, in the case where p is false, ( p) is also false. Organized\n\u00ac \u00ac\ninto a truth table, this argument takes the rather simple form\np p ( p)\n\u00ac \u00ac \u00ac\ntrue false true\nfalse true false\nThe fact that the first and last columns are identical shows the logical\nequivalence of p and ( p). The point here is not just that ( p) p,\n\u00ac \u00ac \u00ac \u00ac \u2261\nbut also that this logical equivalence is valid because it can be verified\ncomputationally based just on the relevant definitions. Its validity does\nnot follow from the fact that \u201cit\u2019s obvious\u201d or \u201cit\u2019s a well-known rule of\ngrammar.\u201d Students often ask \u201cWhy do I have to prove something when\nit\u2019sobvious.\u201d Thepointisthatlogic\u2014andmathematicsmoregenerally\u2014is\nitsownlittle worldwithitsownsetofrules. Although thisworldisrelated\nsomehow to the real world, when you say that something is obvious (in the\nreal world), you aren\u2019t playing by the rules of the world of logic. The real\nmagic ofmathematicsisthatbyplayingbyitsrules,youcancomeupwith\nthings that are decidedly not obvious, but that still say something about\nthe real world\u2014often, something interesting and important.\nEach of the rules in Figure 1.2 can be verified in the same way, by\nmaking a truth table to check all the possible cases.\nIt\u2019simportanttounderstandthatthepropositionalvariablesinthelaws\nof Boolean algebra can stand for any propositions, including compound\npropositions. It is not just true, as the Double Negation Law states, that\n( p) p. It is also true that ( q) q, that ( (p q)) (p q),\n\u00ac \u00ac \u2261 \u00ac \u00ac \u2261 \u00ac \u00ac \u2227 \u2261 \u2227\nthat ( (p (q p))) (p (q p)), and an infinite number of\n\u00ac \u00ac \u2192 \u2227 \u00ac \u2261 \u2192 \u2227 \u00ac\nother statements of the same form. Here, a \u201cstatement of the same form\u201d\nis one that can be obtained by substituting something for p in both places\nwhere it occurs in ( p) p. How can I be sure that all these infinitely\n\u00ac \u00ac \u2261\nmany statements are valid when all that I\u2019ve checked is one little two-line\ntruth table? The answer is that any given proposition, Q, no matter how\ncomplicated, has a particular truth value, either true or false. So, the\nquestion of the validity of ( Q) Q always reduces to one of the two\n\u00ac \u00ac \u2261\ncases I already checked in the truth table. (Note that for this argument to\nbe valid, the same Q must be substituted for p in every position where it\noccurs.) While this argument may be \u201cobvious,\u201d it is not exactly a proof,\nbut for now we will just accept the validity of the following theorem:\nTheorem1.1(FirstSubstitutionLaw). SupposethatQisanyproposition,\nand that p is a propositional variable. Consider any tautology. If (Q) is 1.2. BOOLEAN ALGEBRA 13\nsubstitutedforpinallplaceswherepoccursinthetautology, thentheresult\nis also a tautology.\nSince logical equivalence is defined in terms of tautology, it is also true\nthat when (Q) is substituted for p in a logical equivalence, the result is\nagain a logical equivalence.6\nThe First Substitution Law lets you do algebra! For example, you can\nsubstitute p q for p in the law of double negation, ( p) p. This al-\n\u2192 \u00ac \u00ac \u2261\nlowsyouto\u201csimplify\u201dtheexpression ( (p q))top qwithconfidence\n\u00ac \u00ac \u2192 \u2192\nthat the resulting expression has the same logical value as the expression\nyou started with. (That\u2019s what it means for ( (p q)) and p q to be\n\u00ac \u00ac \u2192 \u2192\nlogically equivalent.) You can play similar tricks with all the laws in Fig-\nure 1.2. Even more important is the Second Substitution Law, which says\nthat you can substitute an expression for a logically equivalent expression,\nwherever it occurs. Once again, we will accept this as a theorem without\ntrying to prove it here. It is surprisingly hard to put this law into words:\nTheorem 1.2 (Second Substitution Law). Suppose that P and Q are any\npropositions such that P Q. Suppose that R is any compound proposition\n\u2261\nin which (P) occurs as a subproposition. Let R be the proposition that is\n\u2032\nobtained by substituting (Q) for that occurrence of (P) in R. Then R R.\n\u2032\n\u2261\nNote that in this case, the theorem does not require (Q) to be sub-\nstituted for every occurrence of (P) in R. You are free to substitute for\none, two, or as many occurrences of (P) as you like, and the result is still\nlogically equivalent to R.\nThe Second Substitution Law allows us to use the logical equivalence\n( p) p to \u201csimplify\u201d the expression q ( ( p)) by substituting (p)\n\u00ac \u00ac \u2261 \u2192 \u00ac \u00ac\nfor ( ( p)). The resulting expression, q (p), or just q p without\n\u00ac \u00ac \u2192 \u2192\nthe parentheses, is logically equivalent to the original q ( ( p)). Once\n\u2192 \u00ac \u00ac\nagain, we have to be careful about parentheses: The fact that p p p\n\u2228 \u2261\ndoes not allow us to rewrite q p p r as q p r. The problem is that\n\u2227 \u2228 \u2227 \u2227 \u2227\nq p p r means (q p) (p r), so that (p p) is not a subexpression.\n\u2227 \u2228 \u2227 \u2227 \u2228 \u2227 \u2228\nSo even though in practice we won\u2019t always write all the parentheses, you\nalways have to be aware of where the parentheses belong.\nThe final piece of algebra in Boolean algebra is the observation that we\ncan chain logical equivalences together. That is, from P Q and Q R,\n\u2261 \u2261\nit follows that P R. This is really just a consequence of the Second\n\u2261\n6I\u2019veaddedparenthesesaroundQherefortechnicalreasons. Sometimes,theparen-\nthesesarenecessarytomakesurethatQisevaluatedasawhole,sothatitsfinalvalue\nis used in place of p. As an example of what can go wrong, consider q\u2227r. If this is\nsubstituted literally for p in \u00ac(\u00acp), without parentheses, the result is \u00ac(\u00acq\u2227r). But\nthisexpressionmeans\u00ac((\u00acq)\u2227r),whichisnot equivalenttoq\u2227r. 14 CHAPTER 1. LOGIC AND PROOF\nSubstitution Law: The equivalence Q R allows us to substitute R for Q\n\u2261\ninthestatementP Q,givingP R. (Rememberthat,byDefinition1.4,\n\u2261 \u2261\nlogical equivalence is defined in terms of a proposition.) This means that\nwe can show that two compound propositions are logically equivalent by\nfinding a chain of logical equivalences that lead from one to the other. For\nexample:\np (p q) p ( p q) definition of p q, Theorem 1.2\n\u2227 \u2192 \u2261 \u2227 \u00ac \u2228 \u2192\n(p p) (p q) Distributive Law\n\u2261 \u2227\u00ac \u2228 \u2227\nF (p q) Law of Contradiction, Theorem 1.2\n\u2261 \u2228 \u2227\n(p q) Identity Law\n\u2261 \u2227\nEachstepinthechainhasitsownjustification. Inseveralcases,asubstitu-\ntion law is used without stating as much. In the first line, for example, the\ndefinition of p q is that p q p q. The Second Substitution Law\n\u2192 \u2192 \u2261 \u00ac \u2228\nallows us to substitute ( p q) for (p q). In the last line, we implicitly\n\u00ac \u2228 \u2192\nappliedtheFirstSubstitutionLawtotheIdentityLaw,F p p,toobtain\n\u2228 \u2261\nF (p q) (p q).\n\u2228 \u2227 \u2261 \u2227\nThe chain of equivalences in the above example allows us to conclude\nthat p (p q) is logically equivalent to p q. This means that if you\n\u2227 \u2192 \u2227\nwere to make a truth table for these two expressions, the truth values in\nthe column for p (p q) would be identical to those in the column for\n\u2227 \u2192\np q. We know this without actually making the table. In this case, the\n\u2227\ntable would only be four lines long and easy enough to make. But Boolean\nalgebracanbeappliedincaseswherethenumberofpropositionalvariables\nis too large for a truth table to be practical.\nLet\u2019s do another example. Recall that a compound proposition is a\ntautology if it is true for all possible combinations of truth values of the\npropositionalvariablesthatitcontains. Butanotherwayofsayingthesame\nthing is that P is a tautology if P T. So, we can prove that a compound\n\u2261\nproposition, P, is a tautology by finding a chain of logical equivalences 1.2. BOOLEAN ALGEBRA 15\nleading from P to T. For example:\n((p q) p) q\n\u2228 \u2227\u00ac \u2192\n( ((p q) p)) q definition of\n\u2261 \u00ac \u2228 \u2227\u00ac \u2228 \u2192\n( (p q) ( p)) q DeMorgan\u2019s Law, Theorem 1.2\n\u2261 \u00ac \u2228 \u2228\u00ac \u00ac \u2228\n( (p q) p) q Double Negation, Theorem 1.2\n\u2261 \u00ac \u2228 \u2228 \u2228\n( (p q)) (p q) Associative Law for\n\u2261 \u00ac \u2228 \u2228 \u2228 \u2228\nT Law of Excluded Middle\n\u2261\nFrom this chain of equivalences, we can conclude that ((p q) p) q is\n\u2228 \u2227\u00ac \u2192\na tautology.\nNow,ittakessomepracticetolookatanexpressionandseewhichrules\ncan be applied to it; to see ( (p q)) (p q) as an application of the law\n\u00ac \u2228 \u2228 \u2228\noftheexcludedmiddleforexample, youneedtomentallysubstitute(p q)\n\u2228\nfor p in the law as it is stated in Figure 1.2. Often, there are several rules\nthatapply,andtherearenodefiniteguidelinesaboutwhichoneyoushould\ntry. This is what makes algebra something of an art.\nIt is certainly not true that all possible rules of Boolean algebra are\ngiven in Figure 1.2. For one thing, there are many rules that are easy\nconsequences of the rules that are listed there. For example, although the\ntable asserts only that F p p, it is also true that p F p. This can be\n\u2228 \u2261 \u2228 \u2261\nchecked directly or by a simple calculation:\np F F p Commutative Law\n\u2228 \u2261 \u2228\np Identity Law as given in the table\n\u2261\nAdditional rules can be obtained by applying the Commutative Law to\nother rules in the table, and we will use such rules freely in the future.\nAnother sort of easy extension can be applied to the Associative Law,\n(p q) r p (q r). The law is stated for the operator applied to\n\u2228 \u2228 \u2261 \u2228 \u2228 \u2228\nthree terms, but it generalizes to four or more terms. For example\n((p q) r) s\n\u2228 \u2228 \u2228\n(p q) (r s) by the Associative Law for three terms\n\u2261 \u2228 \u2228 \u2228\np (q (r s)) by the Associative Law for three terms\n\u2261 \u2228 \u2228 \u2228 16 CHAPTER 1. LOGIC AND PROOF\nWe will, of course, often write this expression as p q r s, with no\n\u2228 \u2228 \u2228\nparenthesesatall, knowingthatwhereverweputtheparenthesesthevalue\nis the same.\nOne other thing that you should keep in mind is that rules can be\napplied in either direction. The Distributive Law, for example, allows you\nto distribute the p in p (q p) to get (p q) (p p). But it can\n\u2228 \u2227\u00ac \u2228 \u2227 \u2228\u00ac\nalso be used in reverse to \u201cfactor out\u201d a term, as when you start with\n(q (p q)) (q (q p))andfactorouttheqtogetq ((p q) (q p)).\n\u2228 \u2192 \u2227 \u2228 \u2192 \u2228 \u2192 \u2227 \u2192\nSofarinthissection,IhavebeenworkingwiththelawsofBooleanalge-\nbrawithoutsayingmuchaboutwhattheymeanorwhytheyarereasonable.\nOf course, you can apply the laws in calculations without understanding\nthem. Butifyouwanttofigureoutwhich calculationstodo,youneedsome\nunderstanding. Mostofthelawsareclearenoughwithalittlethought. For\nexample, if we already know that q is false, then p q will be true when p\n\u2228\nis true and false when p is false. That is, p F has the same logical value\n\u2228\nas p. But that\u2019s just what the Identity Law for says. A few of the laws\n\u2228\nneed more discussion.\nThe Law of the Excluded Middle, p p T, says that given any\n\u2228 \u00ac \u2261\npropositionp, atleastoneofpor pmustbetrue. Since pistrueexactly\n\u00ac \u00ac\nwhen p is false, this is the same as saying that p must be either true or\nfalse. There is no middle ground.7 The Law of Contradiction, p p F,\n\u2227\u00ac \u2261\nsaysthatitisnotpossibleforboth pand ptobetrue. Bothoftheserules\n\u00ac\nare obvious.\nTheDistributiveLawscannotbecalledobvious,butafewexamplescan\nshow that they are reasonable. Consider the statement, \u201cThis card is the\nace of spades or clubs.\u201d Clearly, this is equivalent to \u201cThis card is the ace\nofspacesorthiscardistheaceofclubs.\u201d Butthisisjustanexampleofthe\nfirst distributive law! For, let a represent the proposition \u201cThis card is an\nace,\u201d let s represent \u201cThis card is a spade,\u201d and let c represent \u201cThis card\nisaclub.\u201d Then\u201cThiscardistheaceofspadesorclubs\u201dcanbetranslated\ninto logic as a (s c), while \u201cThis card is the ace of spades or this card is\n\u2227 \u2228\ntheaceofclubs\u201dbecomes(a s) (a c). Andthedistributivelawassures\n\u2227 \u2228 \u2227\nus that a (s c) (a s) (a c). The second distributive law tells us,\n\u2227 \u2228 \u2261 \u2227 \u2228 \u2227\nfor example, that \u201cThis card is either a joker or is the ten of diamonds\u201d is\nlogicallyequivalentto\u201cThiscardiseitherajokeroraten,anditiseithera\n7In propositional logic, this is easily verified with a small truth table. But there is\na surprising amount of argument about whether this law is valid in all situations. In\nthe real world, there often seems to be a gray area between truth and falsity. Even in\nmathematics,therearesomepeoplewhothinkthereshouldbeathirdtruthvalue,one\nthat means something like \u201cunknown\u201d or \u201cnot proven.\u201d But the mathematicians who\nthinkthiswaytendtobeconsideredabitoddbymostothermathematicians. 1.2. BOOLEAN ALGEBRA 17\njokeroradiamond.\u201d Thatis, j (t d) (j t) (j d). Thedistributive\n\u2228 \u2227 \u2261 \u2228 \u2227 \u2228\nlaws are powerful tools and you should keep them in mind whenever you\nare faced with a mixture of and operators.\n\u2227 \u2228\nDeMorgan\u2019sLawsmustalsobelessthanobvious, sincepeopleoftenget\nthem wrong. But they do make sense. When considering (p q), you\n\u00ac \u2227\nshouldaskyourself,howcan\u201cpandq\u201dfail tobetrue. Itwillfailtobetrue\nifeitherpisfalseor ifq isfalse(orboth). Thatis, (p q)isequivalentto\n\u00ac \u2227\n( p) ( q). Considerthesentence\u201cAravenislargeandblack.\u201d Ifabirdis\n\u00ac \u2228 \u00ac\nnot large and black, then it is not a raven. But what exactly does it mean\ntobe\u201cnot (large and black)\u201d? Howcanyoutellwhethertheassertion\u201cnot\n(large and black)\u201d is true of something? This will be true if it is either\nnot large or not black. (It doesn\u2019t have to be both\u2014it could be large and\nwhite,itcouldbesmallandblack.) Similarly,for\u201cporq\u201dtofailtobetrue,\nboth p and q must be false. That is, (p q) is equivalent to ( p) ( q).\n\u00ac \u2228 \u00ac \u2227 \u00ac\nThis is DeMorgan\u2019s second law.\nRecallingthatp q isequivalentto( p) q,wecanapplyDeMorgan\u2019s\n\u2192 \u00ac \u2228\nlaw to obtain a formula for the negation an implication:\n(p q) (( p) q)\n\u00ac \u2192 \u2261\u00ac \u00ac \u2228\n( ( p)) ( q)\n\u2261 \u00ac \u00ac \u2227 \u00ac\np q\n\u2261 \u2227\u00ac\nThat is, p q is false exactly when both p is true and q is false. For\n\u2192\nexample, the negation of \u201cIf you have an ace, you win\u201d is \u201cYou have an\nace, and you don\u2019t win.\u201d Think of it this way: if you had an ace and you\ndidn\u2019t win, then the statement \u201cIf you have an ace, you win\u201d was not true.\nExercises\n1. Constructtruthtablestodemonstratethevalidityofeachofthedistributive\nlaws.\n2. Construct the following truth tables:\na) Construct truth tables to demonstrate that (p q) is not logically\n\u00ac \u2227\nequivalent to ( p) ( q).\n\u00ac \u2227 \u00ac\nb) Construct truth tables to demonstrate that (p q) is not logically\n\u00ac \u2228\nequivalent to ( p) ( q).\n\u00ac \u2228 \u00ac\nc) ConstructtruthtablestodemonstratethevalidityofbothDeMorgan\u2019s\nLaws.\n3. Constructtruthtablestodemonstratethat (p q)isnotlogicallyequiva-\n\u00ac \u2192\nlent to any of the following. 18 CHAPTER 1. LOGIC AND PROOF\na) ( p) ( q)\n\u00ac \u2192 \u00ac\nb) ( p) q\n\u00ac \u2192\nc) p ( q)\n\u2192 \u00ac\nReferbacktothissectionforaformulathatislogicallyequivalentto (p q).\n\u00ac \u2192\n4. Is (p q) logically equivalent to ( p) ( q)?\n\u00ac \u2194 \u00ac \u2194 \u00ac\n5. In the algebra of numbers, there is a distributive law of multiplication over\naddition: x(y+z)=xy+xz. Whatwouldadistributivelawofadditionover\nmultiplication look like? Is it a valid law in the algebra of numbers?\n6. ThedistributivelawsgiveninFigure1.2aresometimescalledtheleft distribu-\ntivelaws. Theright distributive laws saythat(p q) r (p r) (q r)\n\u2228 \u2227 \u2261 \u2227 \u2228 \u2227\nand that (p q) r (p r) (q r). Show that the right distributive laws\n\u2227 \u2228 \u2261 \u2228 \u2227 \u2228\nare also valid laws of Boolean algebra. (Note: In practice, both the left and\ntherightdistributivelawsarereferredtosimplyasthedistributivelaws,and\nboth can be used freely in proofs.)\n7. Showthatp (q r s) (p q) (p r) (p s)foranypropositionsp,q,\n\u2227 \u2228 \u2228 \u2261 \u2227 \u2228 \u2227 \u2228 \u2227\nr,ands. Inwords,wecansaythatconjunctiondistributesoveradisjunction\nof three terms. (Recall that the operator is called conjunction and is\n\u2227 \u2228\ncalled disjunction.) Translate into logic and verify the fact that conjunction\ndistributes over a disjunction of four terms. Argue that, in fact, conjunction\ndistributes over a disjunction of any number of terms.\n8. Therearetwoadditionalbasiclawsoflogic,involvingthetwoexpressionp F\n\u2227\nand p T. What are the missing laws? Show that your answers are, in fact,\n\u2228\nlaws.\n9. Foreachofthefollowingpairsofpropositions,showthatthetwopropositions\nare logically equivalent by finding a chain of equivalences from one to the\nother. State which definition or law of logic justifies each equivalence in the\nchain.\na) p (q p), p q b) ( p) q, p q\n\u2227 \u2227 \u2227 \u00ac \u2192 \u2228\nc) (p q) q, p q d) p (q r), (p q) r\n\u2228 \u2227\u00ac \u2227\u00ac \u2192 \u2192 \u2227 \u2192\ne) (p r) (q r), (p q) r f) p (p q), p q\n\u2192 \u2227 \u2192 \u2228 \u2192 \u2192 \u2227 \u2192\n10. For each of the following compound propositions, find a simpler proposition\nthat is logically equivalent. Try to find a proposition that is as simple as\npossible.\na) (p q) q b) (p q) p c) p p\n\u2227 \u2228\u00ac \u00ac \u2228 \u2227 \u2192\u00ac\nd) p (p q) e) (q p) q f) (p q) ( p q)\n\u00ac \u2227 \u2228 \u2227 \u2192 \u2192 \u2227 \u00ac \u2192\n11. Express the negation of each of the following sentences in natural English:\na) It is sunny and cold.\nb) I will have cake or I will have pie.\nc) If today is Tuesday, this is Belgium.\nd) If you pass the final exam, you pass the course.\n12. Apply one of the laws of logic to each of the following sentences, and rewrite\nit as an equivalent sentence. State which law you are applying. 1.3. APPLICATION: LOGIC CIRCUITS 19\na) I will have coffee and cake or pie.\nb) He has neither talent nor ambition.\nc) You can have spam, or you can have spam.\n1.3 Application: Logic Circuits\nComputers have a reputation\u2014not always deserved\u2014for being \u201clogical.\u201d\nBut fundamentally, deep down, they are made of logic in a very real sense.\nThe building blocks of computers are logic gates, which are electronic\ncomponents that compute the values of simple propositions such as p q\n\u2227\nand p. (Each gate is in turn built of even smaller electronic components\n\u00ac\ncalled transistors, but this needn\u2019t concern us here.)\nA wire in a computer can be in one of two states, which we can think\nof as being on and off. These two states can be naturally associated with\nthe Boolean values T and F. When a computer computes, the multitude\nof wires inside it are turned on and off in patterns that are determined by\ncertain rules. The rules involved can be most naturally expressed in terms\nof logic. A simple rule might be, \u201cturn wire C on whenever wire A is on\nand wire B is on.\u201d This rule can be implemented in hardware as an AND\ngate. An and gate is an electronic component with two input wires and\none output wire, whose job is to turn its output on when both of its inputs\nare on and to turn its output off in any other case. If we associate \u201con\u201d\nwithTand\u201coff\u201dwithF,andifwegivethenamesAandB totheinputsof\nthe gate, then the gate computes the value of the logical expression A B.\n\u2227\nIneffect,Aisapropositionwiththemeaning\u201cthefirstinputison,\u201dandB\nis a proposition with the meaning \u201cthe second input is on.\u201d The and gate\nfunctions to ensure that the output is described by the proposition A B.\n\u2227\nThat is, the output is on if and only if the first input is on and the second\ninput is on.\nAnORgateisanelectroniccomponentwithtwoinputsandoneoutput\nwhichturnsitsoutputonifeither(orboth)ofitsinputsison. Iftheinputs\nare given names A and B, then the or gate computes the logical value of\nA B. ANOT gate hasoneinputandoneoutput,anditturnsitsoutput\n\u2228\noffwhentheinputisonandonwhentheinputisoff. Iftheinputisnamed\nA, then the not gate computes the value of A.\n\u00ac\nOther types of logic gates are, of course, possible. Gates could be made\ntocomputeA B orA B, forexample. However, anycomputationthat\n\u2192 \u2295\ncan be performed by logic gates can be done using only and, or, and not\ngates, as we will see below. (In practice, however, nand gates and nor\ngates, which compute the values of (A B) and (A B) respectively,\n\u00ac \u2227 \u00ac \u2228 20 CHAPTER 1. LOGIC AND PROOF\nAND gate OR gate NOT gate\nA\noutput\nB\nC\nFigure 1.3: The standard symbols for the three basic logic gates,\nand a logic circuit that computes the value of the logical expression\n( A) (B (A C)). The input wires to each logic gate are on\n\u00ac \u2227 \u2228\u00ac \u2227\nthe left, with the output wire on the right. Note that when wires\ncross each other in a diagram such as this, the wires don\u2019t actually\nintersect unless there is a black circle at the point where they cross.\nare often used because they are easier to build from transistors than and\nand or gates.)\nThe three types of logic gates are represented by standard symbols,\nas shown in Figure 1.3. Since the inputs and outputs of logic gates are\njust wires carrying on\/off signals, logic gates can be wired together by\nconnecting outputs from some gates to inputs of other gates. The result is\na logic circuit. An example is also shown in Figure 1.3.\nThelogiccircuitinthefigurehasthreeinputs,labeledA,B,andC. The\ncircuitcomputesthevalueofthecompoundproposition( A) (B (A\n\u00ac \u2227 \u2228\u00ac \u2227\nC)). That is, when A represents the proposition \u201cthe input wire labeled A\nis on,\u201d and similarly for B and C, then the output of the circuit is on if\nand only if the value of the compound proposition ( A) (B (A C))\n\u00ac \u2227 \u2228\u00ac \u2227\nis true.\nGiven any compound proposition made from the operators , , and\n\u2227 \u2228\n, it is possible to build a logic circuit that computes the value of that\n\u00ac\nproposition. The proposition itself is a blueprint for the circuit. As noted\nin Section 1.1, every logical operator that we have encountered can be\nexpressed in terms of , , and , so in fact every compound proposition\n\u2227 \u2228 \u00ac\nthat we know how to write can be computed by a logic circuit.\nGiven a proposition constructed from , , and operators, it is easy\n\u2227 \u2228 \u00ac\nto build a circuit to compute it. First, identify the main operator in the 1.3. APPLICATION: LOGIC CIRCUITS 21\nproposition\u2014the one whose value will be computed last. Consider (A\n\u2228\nB) (A B). This circuit has two input values, A and B, which are\n\u2227\u00ac \u2227\nrepresented by wires coming into the circuit. The circuit has an output\nwire that represents the computed value of the proposition. The main\noperator in (A B) (A B), is the first , which computes the value\n\u2228 \u2227\u00ac \u2227 \u2227\nof the expression as a whole by combining the values of the subexpressions\nA B and (A B). This operator corresponds to an and gate in the\n\u2228 \u00ac \u2227 \u2227\ncircuit that computes the final output of the circuit.\nOnce the main operator has been identified and represented as a logic\ngate, you just have to build circuits to compute the input or inputs to that\noperator. In the example, the inputs to the main and gate come from two\nsubcircuits. One subcircuit computes the value of A B and the other\n\u2228\ncomputes the value of (A B). Building each subcircuit is a separate\n\u00ac \u2227\nproblem,butsmallerthantheproblemyoustartedwith. Eventually,you\u2019ll\ncome to a gate whose input comes directly from one of the input wires\u2014A\nor B in this case\u2014instead of from a subcircuit.\nSo,everycompoundpropositioniscomputedbyalogiccircuitwithone\noutput wire. Is the reverse true? That is, given a logic circuit with one\noutput, is there a proposition that expresses the value of the output in\ntermsofthevaluesoftheinputs? Notquite. Whenyouwiretogethersome\nlogic gates to make a circuit, there is nothing to stop you from introducing\nfeedback loops. A feedback loop occurs when the output from a gate is\nconnected\u2014possibly through one or more intermediate gates\u2014back to an\ninput of the same gate. Figure 1.5 shows an example of a circuit with a\nfeedback loop. Feedback loops cannot be described by compound propo-\nsitions, basically because there is no place to start, no input to associate\nwith a propositional variable. But feedback loops are the only thing that\ncan go wrong. A logic circuit that does not contain any feedback loops is\ncalled a combinatorial logic circuit. Every combinatorial logic circuit\nwith just one output computes the value of some compound proposition.\nThe propositional variables in the compound proposition are just names\nassociated with the input wires of the circuit. (Of course, if the circuit has\nmore than one output, you can simply use a different proposition for each\noutput.)\nThe key to understanding why this is true is to note that each wire in\nthe circuit\u2014not just the final output wire\u2014represents the value of some\nproposition. Furthermore, once you know which proposition is represented\nbyeachinputwiretoagate,it\u2019sobviouswhatpropositionisrepresentedby\nthe output: You just combine the input propositions with the appropriate\n, , or operator, depending on what type of gate it is. To find the\n\u2227 \u2228 \u00ac 22 CHAPTER 1. LOGIC AND PROOF\n1. We know that the final\noutput of the circuit is (A B)\ncomputed by an AND gate, (A B)\nwhose inputs are as shown.\nA (A B) 2. These inputs, in\nB turn come from an\nOR gate and a NOT\ngate, with inputs as\n(A B)\n(A B) shown.\n3. The circuit is completed by adding an AND gate\nto compute the input for the NOT gate, and and connecting\nthe circuit inputs, A and B, to the apropriate gate inputs.\nA (A B)\nB\n(A B) (A B)\nFigure 1.4: Stages in the construction of a circuit that computes the\ncompound proposition (A B) (A B).\n\u2228 \u2227\u00ac \u2227\nproposition associated with the final output, you just have to start from\nthe inputs and move through the circuit, labeling the output wire of each\ngate with the proposition that it represents. Figure 1.6 illustrates this\nprocess.\nSo, compound propositions correspond naturally with combinatorial\nlogic circuits. But we have still not quite settled the question of just how\npowerful these circuits and propositions are. We\u2019ve looked at a number of\nlogicaloperatorsandnotedthattheycanallbeexpressedintermsof , ,\n\u2227 \u2228\nand . But might there be other operators that cannot be so expressed?\n\u00ac\nEquivalently,mighttherebeothertypesoflogicgates\u2014possiblywithsome\nlarge number of inputs\u2014whose computations cannot be duplicated with\nand, or, and not gates? Any logical operator or logic gate computes a 1.3. APPLICATION: LOGIC CIRCUITS 23\nFigure 1.5: This circuit contains a feedback loop, so it is not a\ncombinatorial logic circuit. The feedback loop includes the and gate\nand the or gate on the right. This circuit does not compute the\nvalue of a compound proposition. This circuit does, however, play\nan important role in computer memories, since it can be used to\nstore a logical value.\nvalueforeachpossiblecombinationoflogicalvaluesofitsinputs. Wecould\nalways make a truth table showing the output for each possible combina-\ntion of inputs. As it turns out, given any such truth table, it is possible to\nfind a proposition, containing only the , , and operators, whose value\n\u2227 \u2228 \u00ac\nfor each combination of inputs is given precisely by that table.\nTo see why this is true, it is useful to introduce a particular type of\ncompound proposition. Define a simple term to be either a propositional\nvariableorthenegationofapropositionalvariable. Aconjunctionofsimple\nterms would then consist of one or more simple terms put together with\noperators. (A \u201cconjunction of one simple term\u201d is just a single simple\n\u2227\nterm by itself. This might not make grammatical sense, but it\u2019s the way\nmathematicians think.) Some examples of conjunctions of simple terms\nwould be p q, p, q, and p r w s t. Finally, we can take one or\n\u2227 \u00ac \u2227\u00ac \u2227\u00ac \u2227 \u2227\nmoresuchconjunctionsandjointhemintoa\u201cdisjunctionofconjunctionsof\nsimple terms.\u201d This is the type of compound proposition we need. We can\navoid some redundancy by assuming that no propositional variable occurs\nmorethanonceinasingleconjunction(sincep pcanbereplacedbyp,and\n\u2227\nif p and p both occur in a conjunction, then the value of the conjuction\n\u00ac\nis false, and it can be eliminated.) We can also assume that the same\nconjunction does not occur twice in the disjunction.\nDefinition 1.5. A compound proposition is said to be in disjunctive\nnormalform,orDNF,ifitisadisjunctionofconjunctionsofsimpleterms,\nandif,furthermore,eachpropositionalvariableoccursatmostonceineach\nconjunction and each conjunction occurs at most once in the disjunction.\nUsing p, q, r, s, A, and B as propositional variables, here are a few 24 CHAPTER 1. LOGIC AND PROOF\n1 2\nA A B (A B)\n5\n(A B)\nB\n(B C)\nC C B C\n4\n3\nFigure 1.6: Finding the proposition whose value is computed by a\ncombinatorial logic circuit. Each wire in the circuit is labeled with\nthepropositionthatitrepresents. Thenumberingofthelabelsshows\noneoftheordersinwhichtheycanbeassociatedwiththewires. The\ncircuit as a whole computes the value of (A B) (B C).\n\u00ac \u2227 \u2227 \u2228\u00ac\nexamples of propositions that are in disjunctive normal form:\n(p q r) (p q r s) ( p q)\n\u2227 \u2227 \u2228 \u2227\u00ac \u2227 \u2227 \u2228 \u00ac \u2227\u00ac\n(p q)\n\u2227\u00ac\n(A B) ( A B)\n\u2227\u00ac \u2228 \u00ac \u2227\np ( p q) ( p q r) ( p q r w)\n\u2228 \u00ac \u2227 \u2228 \u00ac \u2227\u00ac \u2227 \u2228 \u00ac \u2227\u00ac \u2227\u00ac \u2227\nPropositionsinDNFarejustwhatweneedtodealwithinput\/outputtables\nof the type that we have been discussing. Any such table can be computed\nbyapropositionindisjunctivenormalform. Itfollowsthatitispossibleto\nbuild a circuit to compute that table using only and, or, and not gates.\nTheorem 1.3. Consider a table that lists a logical output value for every\ncombination of values of several propositional variables. Assume that at\nleast one of the output values is true. Then there is a proposition contain-\ning those variables such that the value of the proposition for each possible\ncombination of the values of the variables is precisely the value specified in\nthe table. It is possible to choose the proposition to be in disjunctive normal\nform.\nProof. ConsideranyrowinthetableforwhichtheoutputvalueisT. Form\na conjunction of simple terms as follows: For each variable, p, whose value\nis T in that row, include p itself in the conjunction; for each variable, q,\nwhose value is F in the row, include q in the conjunction. The value of\n\u00ac\nthis conjunction is T for the combination of variable values given in that 1.3. APPLICATION: LOGIC CIRCUITS 25\nrow of the table, since each of the terms in the conjuction is true for that\ncombination of variables. Furthermore, for any other possible combination\nof variable values, the value of the conjunction will be F, since at least one\nof the simple terms in the conjunction will be false.\nTake the disjunction of all such conjunctions constructed in this way,\nfor each row in the table where the output value is true. This disjunction\nhas the value T if and only if one of the conjunctions that make it up has\nthe value T\u2014and that is precisely when the output value specified by the\ntable is T. So, this disjunction of conjunctions satisfies the requirements of\nthe theorem.\nAs an example, consider the table in Figure 1.7. This table speci-\nfies a desired output value for each possible combination of values for the\npropositional variables p, q, and r. Look at the second row of the table,\nwhere the output value is true. According to the proof of the theorem,\nthis row corresponds to the conjunction ( p q r). This conjunction\n\u00ac \u2227\u00ac \u2227\nis true when p is false, q is false, and r is true; in all other cases it is\nfalse, since in any other case at least one of the terms p, q, or r is\n\u00ac \u00ac\nfalse. The other two rows where the output is true give two more conjunc-\ntions. The three conjunctions are combined to produce the DNF proposi-\ntion( p q r) ( p q r) (p q r). Thispropositioncomputesallthe\n\u00ac \u2227\u00ac \u2227 \u2228 \u00ac \u2227 \u2227 \u2228 \u2227 \u2227\noutput values specified in the table. Using this proposition as a blueprint,\nwe get a logic circuit whose outputs match those given in the table.\nNow, given any combinatorial logic circuit, there are many other cir-\ncuits that have the same input\/output behavior. When two circuits have\nthe same input\/output table, the compound propositions associated with\nthe two circuits are logically equivalent. To put this another way, propo-\nsitions that are logically equivalent produce circuits that have the same\ninput\/output behavior. As a practical matter, we will usually prefer the\ncircuit that is simpler. The correspondence between circuits and proposi-\ntions allows us to apply Boolean algebra to the simplification of circuits.\nFor example, consider the DNF proposition corresponding to the table\ninFigure1.7. In( p q r) ( p q r) (p q r),wecanfactor(q r)\n\u00ac \u2227\u00ac \u2227 \u2228 \u00ac \u2227 \u2227 \u2228 \u2227 \u2227 \u2227\nfrom the last two terms, giving ( p q r) (( p p) (q r)). Since\n\u00ac \u2227\u00ac \u2227 \u2228 \u00ac \u2228 \u2227 \u2227\np p T, and T Q Q for any proposition Q, this can be simplified\n\u00ac \u2228 \u2261 \u2227 \u2261\nto ( p q r) (q r). Again, we can apply the distributive law to this\n\u00ac \u2227\u00ac \u2227 \u2228 \u2227\nto factor out an r, giving (( p q) q) r). This compound proposition\n\u00ac \u2227\u00ac \u2228 \u2227\nis logically equivalent to the one we started with, but implementing it in\na circuit requires only five logic gates, instead of the ten required by the\noriginal proposition.8\n8No,Ididn\u2019tcountwrong. Thereareelevenlogicaloperatorsintheoriginalexpres- 26 CHAPTER 1. LOGIC AND PROOF\np q r output\nF F F F\nF F T T ( p q r)\n\u00ac \u2227\u00ac \u2227\nF T F F\nF T T T ( p q r)\n\u00ac \u2227 \u2227\nT F F F\nT F T F\nT T F F\nT T T T p q r\n\u2227 \u2227\nFigure 1.7: An input\/output table specifying a desired output for\neachcombinationofvaluesofthepropositionalvariablesp, q, andr.\nEachrowwheretheoutputisTcorrespondstoaconjunction, shown\nnext to that row in the table. The disjunction of these conjunctions\nis a proposition whose output values are precisely those specified by\nthe table.\nIf you start with a circuit instead of a proposition, it is often possible\nto find the associated proposition, simplify it using Boolean algebra, and\nuse the simplified proposition to build an equivalent circuit that is simpler\nthan the original.\nAll this explains nicely the relationship between logic and circuits, but\nitdoesn\u2019texplainwhylogiccircuitsshouldbeusedincomputersinthefirst\nplace. Partoftheexplanationisfoundinthefactthatcomputersusebinary\nnumbers. A binary number is a string of zeros and ones. Binary numbers\nareeasytorepresentinanelectronicdevicelikeacomputer: Eachposition\nin the number corresponds to a wire. When the wire is on, it represents\none;whenthewireisoff,itrepresentszero. Whenwearethinkinginterms\nof logic, the same states of the wire represent true and false, but either\nrepresentation is just an interpretation of the reality, which is a wire that\nis on or off. The question is whether the interpretation is fruitful.\nOnce wires are thought of as representing zeros and ones, we can build\ncircuits to do computations with binary numbers. Which computations?\nsion,butyoucangetbywithtengatesinthecircuit: Useasinglenotgatetocompute\n\u00acp,andconnecttheoutputofthatgatetotwodifferentandgates. Reusingtheoutput\nof a logic gate is an obvious way to simplify circuits that does not correspond to any\noperationonpropositions. 1.3. APPLICATION: LOGIC CIRCUITS 27\nA B C output A B C output\n0 0 0 0 0 0 0 0\n0 0 1 1 0 0 1 0\n0 1 0 1 0 1 0 0\n0 1 1 0 0 1 1 1\n1 0 0 1 1 0 0 0\n1 0 1 0 1 0 1 1\n1 1 0 0 1 1 0 1\n1 1 1 1 1 1 1 1\nFigure 1.8: Input\/output tables for the addition of three binary dig-\nits, A, B, and C.\nAny that we want! If we know what the answer should be for each combi-\nnation of inputs, then by Theorem 1.3 we can build a circuit to compute\nthat answer. Of course, the procedure described in that theorem is only\npracticalforsmallcircuits,butsmallcircuitscanbeusedasbuildingblocks\nto make all the calculating circuits in a computer.\nForexample,let\u2019slookatbinaryaddition. Toaddtwoordinary,decimal\nnumbers, you line them up one on top of the other, and add the digits in\neachcolumn. Ineachcolumn,theremightalsobeacarryfromtheprevious\ncolumn. To add up a column, you only need to remember a small number\nof rules, such as 7+6+1=14 and 3+5+0=8. For binary addition, it\u2019s\neven easier, since the only digits are 0 and 1. There are only eight rules:\n0+0+0=00 1+0+0=01\n0+0+1=01 1+0+1=10\n0+1+0=01 1+1+0=10\n0+1+1=10 1+1+1=11\nHere, I\u2019ve written each sum using two digits. In a multi-column addition,\none of these digits is carried over to the next column. Here, we have a\ncalculation that has three inputs and two outputs. We can make an in-\nput\/output table for each of the two outputs. The tables are shown in\nFigure 1.8. We know that these tables can be implemented as combina-\ntorial circuits, so we know that circuits can add binary numbers. To add\nmulti-digit binary numbers, we just need one copy of the basic addition\ncircuit for each column in the sum. 28 CHAPTER 1. LOGIC AND PROOF\nExercises\n1. Using only and, or, and not gates, draw circuits that compute the value of\neach of the propositions A B, A B, and A B.\n\u2192 \u2194 \u2295\n2. For each of the following propositions, find a combinatorial logic circuit that\ncomputes that proposition:\na) A (B C) b) (p q) (p q)\n\u2227 \u2228\u00ac \u2227 \u2227\u00ac \u2227\u00ac\nc) (p q r) ( p q r) d) (A (B C)) (B A)\n\u2228 \u2228 \u2227 \u00ac \u2228\u00ac \u2228\u00ac \u00ac \u2227 \u2228 \u2228 \u2227\u00ac\n3. Find the compound proposition computed by each of the following circuits:\nA\nB\nC\nA\nB\nC\n4. This section describes a method for finding the compound proposition com-\nputed by any combinatorial logic circuit. This method fails if you try to\napply it to a circuit that contains a feedback loop. What goes wrong? Give\nan example.\n5. Showthateverycompoundpropositionwhichisnotacontradictionisequiva-\nlenttoapropositionindisjunctivenormalform. (Note: Wecaneliminatethe\nrestriction that the compound proposition is not a contradiction by agreeing\nthat \u201cF\u201d counts as a proposition in disjunctive normal form. F is logically\nequivalent to any contradiction.)\n6. A proposition in conjunctive normal form (CNF) is a conjunction of dis-\njunctions of simple terms (with the proviso, as in the definition of DNF that\na single item counts as a conjunction or disjunction). Show that every com-\npound proposition which is not a tautology is logically equivalent to a com-\npound proposition in conjunctive normal form. (Hint: What happens if you\ntake the negation of a DNF proposition and apply DeMorgan\u2019s Laws?)\n7. Use the laws of Boolean algebra to simplify each of the following circuits: 1.4. PREDICATES AND QUANTIFIERS 29\nA\nA\nB\nB\nC\nA\nB\nC\n8. Design circuits to implement the input\/output tables for addition, as given\nin Figure 1.8. Try to make your circuits as simple as possible. (The circuits\nthatareusedinrealcomputersforthispurposearemoresimplifiedthanthe\nonesyouwillprobablycomeupwith,butthegeneralapproachofusinglogic\nto design computer circuits is valid.)\n1.4 Predicates and Quantifiers\nInpropositionallogic,wecanletpstandfor\u201cRosesarered\u201dandqstandfor\n\u201cViolets are blue.\u201d Then p q will stand for \u201cRoses are red and violets are\n\u2227\nblue.\u201d But we lose a lot in the translation into logic. Since propositional\nlogic only deals with truth values, there\u2019s nothing we can do with p and q\nin propositional logic that has anything to do with roses, violets, or color.\nTo apply logic to such things, we need predicates. The type of logic that\nuses predicates is called predicate logic, or, when the emphasis is on\nmanipulating and reasoning with predicates, predicate calculus.\nApredicateisakindofincompleteproposition,whichbecomesapropo-\nsition when it is applied to some entity (or, as we\u2019ll see later, to several\nentities). Intheproposition\u201ctheroseisred,\u201d thepredicateisis red. Byit-\nself, \u201cisred\u201disnotaproposition. Thinkofitashavinganemptyslot, that\nneeds to be filled in to make a proposition: \u201c\u2014 is red.\u201d In the proposition\n\u201cthe roseisred,\u201d theslot isfilled bythe entity \u201cthe rose,\u201d but it could just\nas well be filled by other entities: \u201cthe barn is red\u201d; \u201cthe wine is red\u201d; \u201cthe\nbanana is red.\u201d Each of these propositions uses the same predicate, but\nthey are different propositions and they can have different truth values.\nIfP isapredicateandaisanentity,thenP(a)standsfortheproposition\nthat is formed when P is applied to a. If P represents \u201cis red\u201d and a\nstands for \u201cthe rose,\u201d then P(a) is \u201cthe rose is red.\u201d If M is the predicate\n\u201cis mortal\u201d and s is \u201cSocrates,\u201d then M(s) is the proposition \u201cSocrates is 30 CHAPTER 1. LOGIC AND PROOF\nmortal.\u201d\nNow, you might be asking, just what is an entity anyway? I am using\nthetermheretomeansomespecific,identifiablethingtowhichapredicate\ncan beapplied. Generally, it doesn\u2019t make sensetoapply agiven predicate\nto every possible entity, but only to entities in a certain category. For\nexample, itprobablydoesn\u2019tmakesensetoapplythepredicate\u201cismortal\u201d\nto your living room sofa. This predicate only applies to entities in the\ncategory of living things, since there is no way something can be mortal\nunless it is alive. This category is called the domain of discourse for the\npredicate.9\nWe are now ready for a formal definition of one-place predicates. A\none-place predicate, like all the examples we have seen so far, has a single\nslot which can be filled in with one entity:\nDefinition 1.6. A one-place predicate associates a proposition with\neach entity in some collection of entities. This collection is called the do-\nmain of discourse forthepredicate. IfP isapredicateandaisanentity\nin the domain of discourse for P, then P(a) denotes the proposition that\nis associated with a by P. We say that P(a) is the result of applying P\nto a.\nWe can obviously extend this to predicates that can be applied to two\normoreentities. Intheproposition\u201cJohnlovesMary,\u201dloves isatwo-place\npredicate. Besides John and Mary, it could be applied to other pairs of\nentities: \u201cJohnlovesJane,\u201d\u201cBilllovesMary,\u201d\u201cJohnlovesBill,\u201d\u201cJohnloves\nJohn.\u201d If Q is a two-place predicate, then Q(a,b) denotes the proposition\nthatisobtainedwhenQisappliedtotheentitiesaandb. Notethateachof\nthe \u201cslots\u201d in a two-place predicate can have its own domain of discourse.\nFor example, if Q represents the predicate \u201cowns,\u201d then Q(a,b) will only\nmakesensewhenaisapersonandbisaninanimateobject. Anexampleof\na three-place predicate is \u201ca gave b to c,\u201d and a four-place predicate would\nbe \u201ca bought b from c for d dollars.\u201d But keep in mind that not every\npredicate has to correspond to an English sentence.\nWhen predicates are applied to entities, the results are propositions,\nandalltheoperatorsofpropositionallogiccanbeappliedtotheseproposi-\ntions just as they can to any propositions. Let R be the predicate \u201cis red,\u201d\nand let L be the two-place predicate \u201cloves.\u201d If a, b, j, m, and b are enti-\nties belonging to the appropriate categories, then we can form compound\n9Inthelanguageofsettheory,whichwillbeintroducedinthenextchapter,wewould\nsaythatadomainofdiscourseisaset,U,andapredicateisafunctionfromU totheset\noftruthvalues. Thedefinitionshouldbeclearenoughwithouttheformallanguageofset\ntheory,andinfactyoushouldthinkofthisdefinition\u2014andmanyothers\u2014asmotivation\nforthatlanguage. 1.4. PREDICATES AND QUANTIFIERS 31\npropositions such as:\nR(a) R(b) a is red and b is red\n\u2227\nR(a) a is not red\n\u00ac\nL(j,m) L(m,j) j loves m, and m does not love j\n\u2227\u00ac\nL(j,m) L(b,m) if j loves m then b loves m\n\u2192\nR(a) L(j,j) a is red if and only if j loves j\n\u2194\nLet\u2019s go back to the proposition with which we started this section:\n\u201cRoses are red.\u201d This sentence is more difficult to handle than it might\nappear. We still can\u2019t express it properly in logic. The problem is that\nthis proposition is not saying something about some particular entity. It\nreally says that all roses are red (which happens to be a false statement,\nbut that\u2019s what it means). Predicates can only be applied to individual\nentities.\nMany other sentences raise similar difficulties: \u201cAll persons are mor-\ntal.\u201d \u201cSome roses are red, but no roses are black.\u201d \u201cAll math courses are\ninteresting.\u201d \u201cEvery prime number greater than two is odd.\u201d Words like\nall, no, some, and every are called quantifiers. We need to be able to\nexpress similar concepts in logic.\nSuppose that P is a predicate, and we want to express the proposition\nthat P is true when applied to any entity in the domain of discourse. That\nis, we want to say \u201cfor any entity x in the domain of discourse, P(x) is\ntrue.\u201d In predicate logic, we write this in symbols as x(P(x)). The\n\u2200 \u2200\nsymbol, which looks like an upside-down A, is usually read \u201cfor all,\u201d so\nthat x(P(x)) is read as \u201cfor all x, P(x).\u201d (It is understood that this\n\u2200\nmeans for all x in the domain of discourse for P.) For example, if R is the\npredicate \u201cis red\u201d and the domain of discourse consists of all roses, then\nx(R(x))expressestheproposition\u201cAllrosesarered.\u201d Notethatthesame\n\u2200\nproposition could be expressed in English as \u201cEvery rose is red\u201d or \u201cAny\nrose is red.\u201d\nNow, suppose we want to say that a predicate, P, is true for some\nentity in its domain of discourse. This is expressed in predicate logic as\nx(P(x)). The symbol, which looks like a backwards E, is usually read\n\u2203 \u2203\n\u201cthere exists,\u201d but a more exact reading would be \u201cthere is at least one.\u201d\nThus, x(P(x))isreadas\u201cThereexistsanxsuchthatP(x),\u201danditmeans\n\u2203\n\u201cthere is at least one x in the domain of discourse for P for which P(x)\nis true.\u201d If, once again, R stands for \u201cis red\u201d and the domain of discourse\nis \u201croses,\u201d then x(R(x)) could be expressed in English as \u201cThere is a red\n\u2203\nrose\u201d or \u201cAt least one rose is red\u201d or \u201cSome rose is red.\u201d It might also be\nexpressed as \u201cSome roses are red,\u201d but the plural is a bit misleading since\nx(R(x)) is true even if there is only one red rose. We can now give the\n\u2203 32 CHAPTER 1. LOGIC AND PROOF\nformal definitions:\nDefinition 1.7. Suppose that P is a one-place predicate. Then x(P(x))\n\u2200\nisaproposition,whichistrueifandonlyifP(a)istrueforeveryentityain\nthedomainofdiscourseforP. And x(P(x))isapropositionwhichistrue\n\u2203\nifandonlyifthereisatleastoneentity,a,inthedomainofdiscourseforP\nfor which P(a) is true. The symbol is called the universal quantifier,\n\u2200\nand is called the existential quantifier.\n\u2203\nThe x in x(P(x)) and x(P(x)) is a variable. (More precisely, it is an\n\u2200 \u2203\nentity variable, since its value can only be an entity.) Note that a plain\nP(x)\u2014without the x or x\u2014is not a proposition. P(x) is neither true\n\u2200 \u2203\nnor false because x is not some particular entity, but just a placeholder in\na slot that can be filled in with an entity. P(x) would stand for something\nlike the statement \u201cx is red,\u201d which is not really a statement in English at\nall. But it becomes a statement when the x is replaced by some particular\nentity, such as \u201cthe rose.\u201d Similarly, P(x) becomes a proposition if some\nentity a is substituted for the x, giving P(a).10\nAn open statement is an expression that contains one or more entity\nvariables, which becomes a proposition when entities are substituted for\nthe variables. (An open statement has open \u201cslots\u201d that need to be filled\nin.) P(x) and \u201cx is red\u201d are examples of open statements that contain\none variable. If L is a two-place predicate and x and y are variables,\nthen L(x,y) is an open statement containing two variables. An example in\nEnglishwouldbe\u201cxlovesy.\u201d Thevariablesinanopenstatementarecalled\nfree variables. An open statement that contains x as a free variable can\nbe quantified with x or x. The variable x is then said to be bound. For\n\u2200 \u2203\nexample, x is free in P(x) and is bound in x(P(x)) and x(P(x)). The\n\u2200 \u2203\nfree variable y in L(x,y) becomes bound in y(L(x,y)) and in y(L(x,y)).\n\u2200 \u2203\nNote that y(L(x,y)) is still an open statement, since it contains x as\n\u2200\na free variable. Therefore, it is possible to apply the quantifier x or x\n\u2200 \u2203\nto y(L(x,y)), giving x y(L(x,y)) and x y(L(x,y)) . Since all the\n\u2200 \u2200 \u2200 \u2203 \u2200\nvariables are bound in these expressions, they are propositions. If L(x,y)\n(cid:0) (cid:1) (cid:0) (cid:1)\nrepresents \u201cx loves y,\u201d then y(L(x,y)) is something like \u201cx loves every-\n\u2200\none,\u201d and x y(L(x,y)) is the proposition, \u201cThere is someone who loves\n\u2203 \u2200\neveryone.\u201d Of course, we could also have started with x(L(x,y)): \u201cThere\n(cid:0) (cid:1) \u2203\nis someone who loves y.\u201d Applying y to this gives y x(L(x,y)) , which\n\u2200 \u2200 \u2203\nmeans \u201cFor every person, there is someone who loves that person.\u201d Note\n(cid:0) (cid:1)\n10There is certainly room for confusion about names here. In this discussion, x is a\nvariableandaisanentity. Butthat\u2019sonlybecauseIsaidso. Anylettercouldbeused\nin either role, and you haveto pay attention to thecontext to figureoutwhat is going\non. Usually,x,y,andz willbevariables. 1.4. PREDICATES AND QUANTIFIERS 33\nin particular that x y(L(x,y)) and y x(L(x,y)) do not mean the\n\u2203 \u2200 \u2200 \u2203\nsame thing. Altogether, there are eight different propositions that can be\n(cid:0) (cid:1) (cid:0) (cid:1)\nobtained from L(x,y) by applying quantifiers, with six distinct meanings\namong them.\n(From now on, I will leave out parentheses when there is no ambiguity.\nFor example, I will write xP(x) instead of x(P(x)) and x yL(x,y)\n\u2200 \u2200 \u2203 \u2203\ninstead of x y(L(x,y)) . Furthermore, I will sometimes give predicates\n\u2203 \u2203\nand entities names that are complete words instead of just letters, as in\n(cid:0) (cid:1)\nRed(x) and Loves(john,mary). This might help to make examples more\nreadable.)\nInpredicatelogic,theoperatorsandlawsofBooleanalgebrastillapply.\nFor example, if P and Q are one-place predicates and a is an entity in the\ndomain of discourse, then P(a) Q(a) is a proposition, and it is logically\n\u2192\nequivalent to P(a) Q(a). Furthermore, if x is a variable, then P(x)\n\u00ac \u2228 \u2192\nQ(x) is an open statement, and x(P(x) Q(x)) is a proposition. So are\n\u2200 \u2192\nP(a) ( xQ(x)) and ( xP(x)) ( xP(x)). Obviously, predicate logic\n\u2227 \u2203 \u2200 \u2192 \u2203\ncan be very expressive. Unfortunately, the translation between predicate\nlogic and English sentences is not always obvious.\nLet\u2019s look one more time at the proposition \u201cRoses are red.\u201d If the\ndomain of discourse consists of roses, this translates into predicate logic\nas xRed(x). However, the sentence makes more sense if the domain of\n\u2200\ndiscourse is larger\u2014for example if it consists of all flowers. Then \u201cRoses\nare red\u201d has to be read as \u201cAll flowers which are roses are red,\u201d or \u201cFor\nany flower, if that flower is a rose, then it is red.\u201d The last form trans-\nlatesdirectlyintologicas x Rose(x) Red(x) . Supposewewanttosay\n\u2200 \u2192\nthat all red roses are pretty. The phrase \u201cred rose\u201d is saying both that the\n(cid:0) (cid:1)\nflower is a rose and that it is red, and it must be translated as a conjunc-\ntion, Rose(x) Red(x). So, \u201cAll red roses are pretty\u201d can be rendered as\n\u2227\nx (Rose(x) Red(x)) Pretty(x) .\n\u2200 \u2227 \u2192\nHere are a few more examples of translations from predicate logic to\n(cid:0) (cid:1)\nEnglish. Let H(x) represent \u201cx is happy,\u201d let C(y) represent \u201cy is a com-\nputer,\u201d and let O(x,y) represent \u201cx owns y.\u201d (The domain of discourse for\nx consists of people, and the domain for y consists of inanimate objects.)\nThen we have the following translations:\nJack owns a computer: x O(jack,x) C(x) . (That is, there is at\n\u2022 \u2203 \u2227\nleast one thing such that Jack owns that thing and that thing is a\n(cid:0) (cid:1)\ncomputer.)\nEverything Jack owns is a computer: x O(jack,x) C(x) .\n\u2022 \u2200 \u2192\n(cid:0) (cid:1) 34 CHAPTER 1. LOGIC AND PROOF\nIf Jack owns a computer, then he\u2019s happy:\n\u2022\ny(O(jack,y) C(y)) H(jack).\n\u2203 \u2227 \u2192\nEveryone(cid:0)who owns a computer(cid:1)is happy:\n\u2022\nx y(O(x,y) C(y) H(x) .\n\u2200 \u2203 \u2227 \u2192\nEveryone ow(cid:0)n(cid:0)s a computer: x(cid:1)y C(y) (cid:1)(cid:1)O(x,y) . (Note that this\n\u2022 \u2200 \u2203 \u2227\nallows each person to own a different computer. The proposition\n(cid:0) (cid:1)\ny x C(y) O(x,y) would mean that there is a single computer\n\u2203 \u2200 \u2227\nwhich is owned by everyone.)\n(cid:0) (cid:1)\nEveryone is happy: xH(x).\n\u2022 \u2200\nEveryone is unhappy: x( H(x)).\n\u2022 \u2200 \u00ac\nSomeone is unhappy: x( H(x)).\n\u2022 \u2203 \u00ac\nAt least two people are happy: x y H(x) H(y) (x = y) . (The\n\u2022 \u2203 \u2203 \u2227 \u2227 6\nstipulationthatx=y isnecessarybecausetwodifferentvariablescan\n6 (cid:0) (cid:1)\nrefer to the same entity. The proposition x y(H(x) H(y)) is true\n\u2203 \u2203 \u2227\neven if there is only one happy person.)\nThere is exactly one happy person:\n\u2022\nxH(x) ) y z((H(y) H(z)) (y = z)) . (The first\n\u2203 \u2227 \u2200 \u2200 \u2227 \u2192\npart of this conjunction says that there is at least one happy person.\n(cid:0) (cid:1) (cid:0) (cid:1)\nThe second part says that if y and z are both happy people, then\nthey are actually the same person. That is, it\u2019s not possible to find\ntwo different people who are happy.)\nTo calculate in predicate logic, we need a notion of logical equivalence.\nClearly,therearepairsofpropositionsinpredicatelogicthatmeanthesame\nthing. Consider the propositions ( xH(x)) and x( H(x)), where H(x)\n\u00ac \u2200 \u2203 \u00ac\nrepresents \u201cx is happy.\u201d The first of these propositions means \u201cNot every-\noneishappy,\u201dandthesecondmeans\u201cSomeoneisnothappy.\u201d Thesestate-\nments have the same truth value: If not everyone is happy, then someone\nis unhappy and vice versa. But logical equivalence is much stronger than\njusthavingthesametruthvalue. Inpropositionallogic,logicalequivalence\nis defined in terms of propositional variables: two compound propositions\nare logically equivalent if they have the same truth values for all possible\ntruth values of the propositional variables they contain. In predicate logic,\ntwo formulas are logically equivalent if they have the same truth value for\nall possible predicates.\nConsider ( xP(x)) and x( P(x)). These formulas make sense for\n\u00ac \u2200 \u2203 \u00ac\nany predicate P, and for any predicate P they have the same truth value.\nUnfortunately, we can\u2019t\u2014as we did in propositional logic\u2014just check this 1.4. PREDICATES AND QUANTIFIERS 35\n( xP(x)) x( P(x))\n\u00ac \u2200 \u2261\u2203 \u00ac\n( xP(x)) x( P(x))\n\u00ac \u2203 \u2261\u2200 \u00ac\nx yQ(x,y) y xQ(x,y)\n\u2200 \u2200 \u2261\u2200 \u2200\nx yQ(x,y) y xQ(x,y)\n\u2203 \u2203 \u2261\u2203 \u2203\nFigure 1.9: Four important rules of predicate logic. P can be any\none-place predicate, and Q can be any two-place predicate. The first\ntwo rules are called DeMorgan\u2019s Laws for predicate logic.\nfact with a truth table: there are no subpropositions, connected by , ,\n\u2227 \u2228\netc,outofwhichtobuildatable. So,let\u2019sreasonitout: Tosay ( xP(x))\n\u00ac \u2200\nis true is just to say that it is not the case that P(x) is true for all possible\nentities x. So, there must be some entity a for which P(a) is false. Since\nP(a) is false, P(a) is true. But saying that there is an a for which P(a)\n\u00ac \u00ac\nis true is just saying that x( P(x)) is true. So, the truth of ( xP(x))\n\u2203 \u00ac \u00ac \u2200\nimplies the truth of x( P(x)). On the other hand, if ( xP(x)) is false,\n\u2203 \u00ac \u00ac \u2200\nthen xP(x) is true. Since P(x) is true for every x, P(x) is false for\n\u2200 \u00ac\nevery x; that is, there is no entity a for which the statement P(a) is true.\n\u00ac\nBut this just means that the statement x( P(x)) is false. In any case,\n\u2203 \u00ac\nthen, the truth values of ( xP(x)) and x( P(x)) are the same. Since\n\u00ac \u2200 \u2203 \u00ac\nthis is true for any predicate P, we will say that these two formulas are\nlogically equivalent and write ( xP(x)) x( P(x)).\n\u00ac \u2200 \u2261\u2203 \u00ac\nA similar argument would show that ( xP(x)) x( P(x)). These\n\u00ac \u2203 \u2261 \u2200 \u00ac\ntwo equivalences, which explicate the relation between negation and quan-\ntification, are known as DeMorgan\u2019s Laws for predicate logic. (They are\nclosely related to DeMorgan\u2019s Laws for propositional logic; see the exer-\ncises.) These laws can be used to help simplify expressions. For example,\ny(R(y) Q(y)) y( (R(y) Q(y)))\n\u00ac\u2200 \u2228 \u2261\u2203 \u00ac \u2228\ny(( R(y)) ( Q(y))\n\u2261\u2203 \u00ac \u2227 \u00ac\nIt might not be clear exactly why this qualifies as a \u201csimplification,\u201d but\nit\u2019s generally considered simpler to have the negation operator applied to\nbasic propositions such as R(y), rather than to quantified expressions such 36 CHAPTER 1. LOGIC AND PROOF\nas y(R(y) Q(y)). For a more complicated example:\n\u2200 \u2228\nx P(x) ( y(Q(y) Q(x)))\n\u00ac\u2203 \u2227 \u2200 \u2192\n(cid:0) x P(x) ( y(Q(cid:1)(y) Q(x)))\n\u2261\u2200 \u00ac \u2227 \u2200 \u2192\nx(cid:0)((cid:0)P(x)) ( y(Q(y) Q(x)(cid:1)))\n\u2261\u2200 \u00ac \u2228 \u00ac\u2200 \u2192\nx(cid:0)( P(x)) ( y( (Q(y) Q(x))(cid:1)))\n\u2261\u2200 \u00ac \u2228 \u2203 \u00ac \u2192\nx(cid:0)( P(x)) ( y( ( Q(y) Q(x))))(cid:1)\n\u2261\u2200 \u00ac \u2228 \u2203 \u00ac \u00ac \u2228\nx(cid:0)( P(x)) ( y( Q(y) Q(x)))(cid:1)\n\u2261\u2200 \u00ac \u2228 \u2203 \u00ac\u00ac \u2227\u00ac\nx(cid:0)( P(x)) ( y(Q(y) Q(x))) (cid:1)\n\u2261\u2200 \u00ac \u2228 \u2203 \u2227\u00ac\n(cid:0) (cid:1)\nDeMorgan\u2019s Laws are listed in Figure 1.9 along with two other laws of\npredicate logic. The other laws allow you to interchange the order of the\nvariableswhentwoquantifiersofthesametype(both or )occurtogether.\n\u2203 \u2200\nTo define logical equivalence in predicate logic more formally, we need\nto talk about formulas that contain predicate variables, that is, variables\nthat act as place-holders for arbitrary predicates in the same way that\npropositional variables are place-holders for propositions and entity vari-\nablesareplace-holdersforentities. Withthisinmind,wecandefinelogical\nequivalenceandthecloselyrelatedconceptoftautologyforpredicatelogic.\nDefinition1.8. LetPbeaformulaofpredicatelogicwhichcontainsoneor\nmorepredicatevariables. Pissaidtobeatautology ifitistruewhenever\nallthepredicatevariablesthatitcontainsarereplacedbyactualpredicates.\nTwo formulas P and Q are said to be logically equivalent if P Q is\na tautology, that is if P and Q always have the same truth value\u2194 when\nthe predicate variables they contain arereplaced by actual predicates. The\nnotation P Q asserts that P is logically equivalent to Q.\n\u2261\nExercises\n1. Simplify each of the following propositions. In your answer, the operator\n\u00ac\nshould be applied only to individual predicates.\na) x( P(x)) b) x(P(x) Q(x))\n\u00ac\u2200 \u00ac \u00ac\u2203 \u2227\nc) z(P(z) Q(z)) d) ( xP(x)) y(Q(y))\n\u00ac\u2200 \u2192 \u00ac \u2200 \u2227\u2200\ne) x yP(x,y) f) (cid:0)x(R(x) yS(x,y))(cid:1)\n\u00ac\u2200 \u2203 \u00ac\u2203 \u2227\u2200\ng) y(P(y) Q(y)) h) x(P(x) ( yQ(x,y)))\n\u00ac\u2203 \u2194 \u00ac \u2200 \u2192 \u2203\n(cid:0) (cid:1)\n2. Give a careful argument to show that the second of DeMorgan\u2019s laws for\npredicate calculus, ( xP(x)) x( P(x)), is valid.\n\u00ac \u2200 \u2261\u2203 \u00ac 1.4. PREDICATES AND QUANTIFIERS 37\n3. Findthenegationofeachofthefollowingpropositions. Simplifytheresult;in\nyour answer, the operator should be applied only to individual predicates.\n\u00ac\na) n( sC(s,n))\n\u00ac\u2203 \u2200\nb) n( s(L(s,n) P(s)))\n\u00ac\u2203 \u2200 \u2192\nc) n( s(L(s,n) ( x y zQ(x,y,z)))).\n\u00ac\u2203 \u2200 \u2192 \u2203 \u2203 \u2203\nd) n( s(L(s,n) ( x y z(s=xyz R(x,y) T(y) U(x,y,z)))).\n\u00ac\u2203 \u2200 \u2192 \u2203 \u2203 \u2203 \u2227 \u2227 \u2227\n4. SupposethatthedomainofdiscourseforapredicateP containsonlytwoen-\ntities. Show that xP(x) is equivalent to a conjunction of two simple propo-\n\u2200\nsitions, and xP(x) is equivalent to a disjunction. Show that in this case,\n\u2203\nDeMorgan\u2019sLawsforpropositionallogicandDeMorgan\u2019sLawsforpredicate\nlogic actually say exactly the same thing. Extend the results to a domain of\ndiscourse that contains exactly three entities.\n5. Let H(x) stand for \u201cx is happy,\u201d where the domain of discourse consists of\npeople. Express the proposition \u201cThere are exactly three happy people\u201d in\npredicate logic.\n6. Let T(x,y) stand for \u201cx has taken y,\u201d where the domain of discourse for\nx consists of students and the domain of discourse for y consists of math\ncourses(at yourschool). Translateeach ofthefollowingpropositions intoan\nunambiguous English sentence:\na) x yT(x,y) b) x yT(x,y) c) y xT(x,y)\n\u2200 \u2200 \u2200 \u2203 \u2200 \u2203\nd) x yT(x,y) e) x yT(x,y) f) y xT(x,y)\n\u2203 \u2203 \u2203 \u2200 \u2203 \u2200\n7. LetF(x,t)standfor\u201cYoucanfoolpersonxattimet.\u201d Translatethefollowing\nsentenceintopredicatelogic: \u201cYoucanfoolsomeofthepeopleallofthetime,\nand you can fool all of the people some of the time, but you can\u2019t fool all of\nthe people all of the time.\u201d\n8. Translate each of the following sentences into a proposition using predicate\nlogic. Make up any predicates you need. State what each predicate means\nand what its domain of discourse is.\na) All crows are black.\nb) Any white bird is not a crow.\nc) Not all politicians are honest.\nd) All green elephants have purple feet.\ne) There is no one who does not like pizza.\nf) Anyone who passes the final exam will pass the course.\ng) Ifxisanypositivenumber,thenthereisanumbery suchthaty2 =x.\n9. Thesentence\u201cSomeonehastheanswertoeveryquestion\u201disambiguous. Give\ntwotranslationsofthissentenceintopredicatelogic,andexplainthedifference\nin meaning.\n10. The sentence \u201cJane is looking for a dog\u201d is ambiguous. One meaning is that\nthereissomeparticulardog\u2014maybetheoneshelost\u2014thatJaneislookingfor.\nTheothermeaningisthatJaneislookingforanyolddog\u2014maybebecauseshe\nwants to buy one. Express the first meaning in predicate logic. Explain why\nthesecondmeaningisnot expressedby x(Dog(x) LooksFor(jane,x)). In\n\u2200 \u2192 38 CHAPTER 1. LOGIC AND PROOF\nfact,thesecondmeaningcannotbeexpressedinpredicatelogic. Philosophers\nof language spend a lot of time thinking about things like this. They are\nespecially fond of the sentence \u201cJane is looking for a unicorn,\u201d which is not\nambiguous when applied to the real world. Why is that?\n1.5 Deduction\nLogiccanbeappliedtodrawconclusionsfromasetofpremises. Apremise\nis just a proposition that is known to be true or that has been accepted to\nbetrueforthesakeofargument,andaconclusionisapropositionthatcan\nbededucedlogicallyfromthepremises. Theideaisthatifyoubelievethat\nthe premises are true, then logic forces you to accept that the conclusion\nis true. An \u201cargument\u201d is a claim that a certain conclusion follows from a\ngiven set of premises. Here is an argument laid out in a traditional format:\nIf today is Tuesday, then this is Belgium\nToday is Tuesday\n\u2234 This is Belgium\nThepremisesoftheargumentareshownabovetheline,andtheconclusion\nbelow. Thesymbol\u2234isread\u201ctherefore.\u201d Theclaimisthattheconclusion,\n\u201cThis is Belgium,\u201d can be deduced logically from the two premises, \u201cIf\ntoday is Tuesday, then this is Belgium\u201d and \u201cToday is Tuesday.\u201d In fact,\nthis claim is true. Logic forces you to accept this argument. Why is that?\nLet p stand for the proposition \u201cToday is Tuesday,\u201d and let q stand for\nthe proposition \u201cThis is Belgium.\u201d Then the above argument has the form\np q\n\u2192\np\n\u2234 q\nNow, for any propositions p and q\u2014not just the ones in this particular\nargument\u2014if p q is true and p is true, then q must also be true. This is\n\u2192\neasy to check in a truth table:\np q p q\n\u2192\nfalse false true\nfalse true true\ntrue false false\ntrue true true\nThe only case where both p q and p are true is on the last line of the\n\u2192\ntable, and in this case, q is also true. If you believe p q and p, you have\n\u2192 1.5. DEDUCTION 39\nno logical choice but to believe q. This applies no matter what p and q\nrepresent. For example, if you believe \u201cIf Jill is breathing, then Jill pays\ntaxes,\u201d and you believe that \u201cJill is breathing,\u201d logic forces you to believe\nthat\u201cJillpaystaxes.\u201d Notethatwecan\u2019tsayforsurethattheconclusionis\ntrue, only that if the premises are true, then the conclusion must be true.\nThis fact can be rephrased by saying that (p q) p q is a\ntautology. More generally, for any compound\nproposi\u2192\ntions\nP\u2227 and\u2192Q,\nsaying\n\u201cP Q is a tautology\u201d is the same as saying th(cid:0) at \u201cin all cas(cid:1) es where P\nis tr\u2192 ue, Q is also true\u201d.11 We will use the notation P = Q to mean that\nP Q is a tautology. Think of P as being the premise\u21d2 of an argument or\nthe\u2192 conjunctionofseveralpremises. TosayP= QistosaythatQfollows\nlogically from P. We will use the same notation\u21d2 in both propositional logic\nand predicate logic. (Note that the relation of = to is the same as the\n\u21d2 \u2192\nrelation of to .)\n\u2261 \u2194\nDefinition 1.9. Let P and Q be any formulas in either propositional logic\nor predicate logic. The notation P= Q is used to mean that P Q is a\ntautology. That is, in all cases where\u21d2P is true, Q is also true. We\u2192 then say\nthat Q can be logically deduced from P or that P logically implies Q.\nAnargumentinwhichtheconclusionfollowslogicallyfromthepremises\nis said to be a valid argument. To test whether an argument is valid,\nyou have to replace the particular propositions or predicates that it con-\ntainswithvariables, andthentestwhethertheconjunctionofthepremises\nlogically implies the conclusion. We have seen that any argument of the\nform\np q\n\u2192\np\n\u2234 q\nis valid, since (p q) p q is a tautology. This rule of deduction is\n\u2192 \u2227 \u2192\ncalled modus ponens. It plays a central role in logic. Another, closely\n(cid:0) (cid:1)\nrelated rule is modus tollens, which applies to arguments of the form\np q\n\u2192\nq\n\u00ac\n\u2234 p\n\u00ac\n11Here, \u201cin all cases\u201d means for all combinations of truth values of the propositional\nvariablesinPandQ. SayingP\u2192Qisatautologymeansitistrueinallcases. Butby\ndefinition of \u2192, it is automatically true in cases where P is false. In cases where P is\ntrue,P\u2192QwillbetrueifandonlyifQistrue. 40 CHAPTER 1. LOGIC AND PROOF\nTo verify that this is a valid argument, just check that (p q) q =\n\u2192 \u2227\u00ac \u21d2\np, that is, that (p q) q p is a tautology. As an example, the\n\u00ac \u2192 \u2227\u00ac \u2192 \u00ac (cid:0) (cid:1)\nfollowing argument has the form of modus tollens and is therefore a valid\n(cid:0) (cid:1)\nargument:\nIf Keanu Reeves is a good actor, then I\u2019m the king of France\nI am not the king of France\n\u2234 Keanu Reeves in not a good actor\nYou should note carefully that the validity of this argument has nothing to\ndo with whether or not Keanu Reeves can act well. The argument forces\nyou to accept the conclusion only if you accept the premises. You can\nlogically believe that the conclusion is false, as long as you believe that at\nleast one of the premises is false.\nAnother named rule of deduction is the Law of Syllogism, which has\nthe form\np q\n\u2192\nq r\n\u2192\n\u2234 p r\n\u2192\nFor example:\nIf you study hard, you do well in school\nIf you do well in school, you get a good job\n\u2234 If you study hard, you get a good job\nThere are many other rules. Here are a few that might prove useful.\nSome of them might look trivial, but don\u2019t underestimate the power of a\nsimple rule when it is combined with other rules.\np q p p q p\n\u2228 \u2227\np q \u2234 p \u2234 p q\n\u00ac \u2228\n\u2234 q \u2234 p q\n\u2227\nLogical deduction is related to logical equivalence. We defined P and\nQ to be logically equivalent if P Q is a tautology. Since P Q is\nequivalent to (P Q) (Q P),\u2194 we see that P Q if and only\u2194 if both\nQ= PandP=\u2192Q. T\u2227 hus,\u2192 wecanshowthattwo\u2261 statementsarelogically\n\u21d2 \u21d2\nequivalent if we can show that each of them can be logically deduced from\nthe other. Also, we get a lot of rules about logical deduction for free\u2014two\nrulesofdeductionforeachlogicalequivalenceweknow. Forexample, since\n(p q) ( p q), we get that (p q) = ( p q). For example,\n\u00ac \u2227 \u2261 \u00ac \u2228\u00ac \u00ac \u2227 \u21d2 \u00ac \u2228\u00ac 1.5. DEDUCTION 41\nif we know \u201cIt is not both sunny and warm,\u201d then we can logically deduce\n\u201cEither it\u2019s not sunny or it\u2019s not warm.\u201d (And vice versa.)\nIngeneral,argumentsaremorecomplicatedthatthosewe\u2019veconsidered\nso far. Here, for example, is an argument that has five premises:\n(p r) s\n\u2227 \u2192\nq p\n\u2192\nt r\n\u2192\nq\nt\n\u2234 s\nIs this argument valid? Of course, you could use a truth table to check\nwhether the conjunction of the premises logically implies the conclusion.\nBut with five propositional variables, the table would have 32 lines, and\nthe size of the table grows quickly when more propositional variables are\nused. So, in general, truth tables are not practical.\nFortunately, there is another way to proceed, based on the fact that it\nis possible to chain several logical deductions together. That is, if P= Q\nand Q= R, it follows that P= R. This means we can demonstrate\u21d2 the\n\u21d2 \u21d2\nvalidity of an argument by deducing the conclusion from the premises in a\nsequence of steps. These steps can be presented in the form of a proof:\nDefinition 1.10. A formal proof that an argument is valid consists of\na sequence of propositions such that the last proposition in the sequence\nis the conclusion of the argument, and every proposition in the sequence\nis either a premise of the argument or follows by logical deduction from\npropositions that precede it in the list.\nTheexistenceofsuchaproofshowsthattheconclusionfollowslogically\nfrom the premises, and therefore that the argument is valid. Here is a\nformal proof that the argument given above is valid. The propositions in\nthe proof are numbered, and each proposition has a justification.\n1. q p premise\n\u2192\n2. q premise\n3. p from 1 and 2 (modus ponens)\n4. t r premise\n\u2192\n5. t premise\n6. r from 4 and 5 (modus ponens)\n7. p r from 3 and 6\n\u2227\n8. (p r) s premise\n\u2227 \u2192\n9. s from 7 and 8 (modus ponens) 42 CHAPTER 1. LOGIC AND PROOF\nOnce a formal proof has been constructed, it is convincing. Unfortunately,\nit\u2019s not necessarily easy to come up with the proof. Usually, the best\nmethod is a combination of working forward (\u201cHere\u2019s what I know, what\ncan I deduce from that?\u201d) and working backwards (\u201cHere\u2019s what I need\nto prove, what other things would imply that?\u201d). For this proof, I might\nhave thought: I want to prove s. I know that p r implies s, so if I can\n\u2227\nprove p r, I\u2019m OK. But to prove p r, it\u2019ll be enough to prove p and r\n\u2227 \u2227\nseparately....\nOf course, not every argument is valid, so the question also arises, how\ncan we show that an argument is invalid? Let\u2019s assume that the argument\nhas been put into general form, with all the specific propositions replaced\nby propositional variables. The argument is valid if in all cases where all\nthe premises are true, the conclusion is also true. The argument is invalid\nif there is even one case where all the premises are true and the conclusion\nisfalse. Wecanprovethatanargumentisinvalidbyfindinganassignment\nof truth values to the propositional variables which makes all the premises\ntrue but makes the conclusion false. For example, consider an argument of\nthe form:\np q\n\u2192\nq (p r)\n\u2192 \u2227\nr\n\u2234 p\nIn the case where p is false, q is false, and r is true, the three premises of\nthis argument are all true, but the conclusion is false. This shows that the\nargument is invalid.\nTo apply all this to arguments stated in English, we have to introduce\npropositional variables to represent all the propositions in the argument.\nFor example, consider:\nJohn will be at the party if Mary is there and Bill is not there.\nMary will be at the party if it\u2019s on Friday or Saturday. If Bill\nis at the party, Tom will be there. Tom won\u2019t be at the party if\nit\u2019s on Friday. The party is on Friday. Therefore, John will be\nat the party.\nLet j stand for \u201cJohn will be at the party,\u201d m for \u201cMary will be there,\u201d b\nfor \u201cBill will be there,\u201d t for \u201cTom will be there,\u201d f for \u201cThe party is on\nFriday,\u201d andsfor\u201cThepartyisonSaturday.\u201d Thenthisargumenthasthe\nform 1.5. DEDUCTION 43\n(m b) j\n\u2227\u00ac \u2192\n(f s) m\n\u2228 \u2192\nb t\n\u2192\nf t\n\u2192\u00ac\nf\n\u2234 j\nThis is a valid argument, as the following proof shows:\n1. f t premise\n\u2192\u00ac\n2. f premise\n3. t from 1 and 2 (modus ponens)\n\u00ac\n4. b t premise\n\u2192\n5. b from 4 and 3 (modus tollens)\n\u00ac\n6. f s from 2\n\u2228\n7. (f s) m premise\n\u2228 \u2192\n8. m from 6 and 7 (modus ponens)\n9. m b from 8 and 5\n\u2227\u00ac\n10. (m b) j premise\n\u2227\u00ac \u2192\n11. j from 10 and 9 (modus ponens)\nSo far in this section, we have been working mostly with propositional\nlogic. But the definitions of valid argument and logical deduction apply\nto predicate logic as well. One of the most basic rules of deduction in\npredicate logic says that ( xP(x))= P(a) for any entity a in the domain\n\u2200 \u21d2\nofdiscourseofthepredicateP. Thatis, ifapredicateistrueofallentities,\nthen it is true of any given particular entity. This rule can be combined\nwith rules of deduction for propositional logic to give the following valid\narguments\nx(P(x) Q(x)) x(P(x) Q(x))\n\u2200 \u2192 \u2200 \u2192\nP(a) Q(a)\n\u00ac\n\u2234 Q(a) \u2234 P(a)\n\u00ac\nThesevalidargumentsgobythenamesofmodus ponens andmodus tollens\nfor predicate logic. Note that from the premise x(P(x) Q(x)) we can\n\u2200 \u2192\ndeduce P(a) Q(a). From this and from the premise that P(a), we can\n\u2192\ndeduce Q(a) by modus ponens. So the first argument above is valid. The\nsecond argument is similar, using modus tollens.\nThemostfamouslogicaldeductionofthemallisanapplicationofmodus\nponens for predicate logic: 44 CHAPTER 1. LOGIC AND PROOF\nAll humans are mortal\nSocrates is human\n\u2234 Socrates is mortal\nThis has the form of modus ponens with P(x) standing for \u201cx is human,\u201d\nQ(x) standing for \u201cx is mortal,\u201d and a standing for the noted entity,\nSocrates.\nThereisalotmoretosayaboutlogicaldeductionandproofinpredicate\nlogic, and we\u2019ll spend the rest of this chapter on the subject.\nExercises\n1. Verify the validity of modus tollens and the Law of Syllogism.\n2. Each of the following is a valid rule of deduction. For each one, give an\nexample of a valid argument in English that uses that rule.\np q p p q p\n\u2228 \u2227\np q \u2234 p \u2234 p q\n\u00ac \u2228\n\u2234 q \u2234 p q\n\u2227\n3. There are two notorious invalid arguments that look deceptively like modus\nponens and modus tollens:\np q p q\n\u2192 \u2192\nq p\n\u00ac\n\u2234 p \u2234 q\n\u00ac\nShow that each of these arguments is invalid. Give an English example that\nuses each of these arguments.\n4. Decide whether each of the following arguments is valid. If it is valid, give a\nformalproof. Ifitisinvalid,showthatitisinvalidbyfindinganappropriate\nassignment of truth values to propositional variables.\na) p q b) p q c) p q\n\u2192 \u2227 \u2228\nq s q (r s) q (r s)\n\u2192 \u2192 \u2228 \u2192 \u2227\ns r p\n\u00ac \u00ac\n\u2234 p \u2234 s \u2234 s\nd) ( p) t e) p f) q t\n\u00ac \u2192 \u2192\nq s s r p (t s)\n\u2192 \u2192 \u2192 \u2192\nr q q r p\n\u2192 \u2228\n(q t) q p \u2234 q s\n\u00ac \u2228 \u2192\u00ac \u2192\n\u2234 p \u2234 s\n\u00ac\n5. For each of the following English arguments, express the argument in terms\nofpropositionallogicanddeterminewhethertheargumentisvalidorinvalid. 1.6. PROOF 45\na) If it is Sunday, it rains or snows. Today, it is Sunday and it\u2019s not\nraining. Therefore, it must be snowing.\nb) If there are anchovies on the pizza, Jack won\u2019t eat it. If Jack doesn\u2019t\neatpizza,hegetsangry. Jackisangry. Therefore,therewereanchovies\non the pizza.\nc) At 8:00, Jane studies in the library or works at home. It\u2019s 8:00 and\nJane is not studying in the library. So she must be working at home.\n1.6 Proof\nMathematics is unique in that it claims a certainty that is beyond all pos-\nsible doubt or argument. A mathematical proof shows how some result\nfollows by logic alone from a given set of assumptions, and once the result\nhas been proven, it is as solid as the foundations of logic themselves. Of\ncourse, mathematics achieves this certainty by restricting itself to an arti-\nficial, mathematical world, and its application to the real world does not\ncarry the same degree of certainty.\nWithintheworldofmathematics,consequencesfollowfromassumptions\nwith the force of logic, and a proof is just a way of pointing out logical\nconsequences. There is an old mathematical joke about this:\nThis mathematics professor walks into class one day and says \u201cWe\u2019ll\nstart today with this result, which is obvious,\u201d and he writes it on the\nboard. Then, he steps back and looks at the board for a while. He walks\naroundthefrontoftheroom,staresintospaceandbackattheboard. This\ngoesontilltheendofclass, andhewalksoutwithoutsayinganythingelse.\nThe next class period, the professor walks into the room with a big smile,\nwrites the same result on the board, turns to the class and says, \u201cI was\nright. It is obvious.\u201d\nFor of course, the fact that mathematical results follow logically does\nnotmeanthattheyareobviousinanynormalsense. Proofsareconvincing\nonce they are discovered, but finding them is often very difficult. They are\nwritten in a language and style that can seem obscure to the uninitiated.\nOften, a proof builds on a long series of definitions and previous results,\nand while each step along the way might be \u201cobvious,\u201d the end result\ncan be surprising and powerful. This is what makes the search for proofs\nworthwhile.\nIntherestofthischapter,we\u2019lllookatsomeapproachesandtechniques\nthatcanbeusedforprovingmathematicalresults,includingtwoimportant\nprooftechniquesknownasproofbycontradictionandmathematicalinduc-\ntion. Along the way, we\u2019ll encounter a few new definitions and notations.\nHopefully, you will be left with a higher level of confidence for exploring 46 CHAPTER 1. LOGIC AND PROOF\nthe mathematical world on your own.\nThe mathematical world and the real world weren\u2019t always quite so\nseparate. Until some time near the middle of the nineteenth century, the\nstatements of mathematics were regarded as statements about the world.\nA proof was simply a convincing argument, rather than a chain forged of\nabsolute logic. It was something closer to the original meaning of the word\n\u201cproof\u201d, as a test or trial: To prove something was to test its truth by\nputting it to the trial of logical argument.\nThefirstrumbleoftroublecameintheformofnon-Euclidean geom-\netry. For two thousand years, the geometry of the Greek mathematician\nEuclid had been accepted, simply, as the geometry of the world. In the\nmiddle of the nineteenth century, it was discovered that there are other\nsystems of geometry, which are at least as valid and self-consistent as Eu-\nclid\u2019s system. Mathematicians can work in any of these systems, but they\ncannot all claim to be working in the real world.\nNeartheendofthenineteenthcenturycameanothershock,intheform\nofcracksintheveryfoundationofmathematics. Atthattime, mathemati-\ncian Gottlieb Frege was finishing a book on set theory that represented his\nlife\u2019s work. In Frege\u2019s set theory, a set could be defined by any property.\nYou could have, for example, the set consisting of all sets that contain\nthree objects. As he was finishing his book, Frege received a letter from\na young mathematician named Bertrand Russell which described what be-\ncameknownasRussell\u2019s Paradox. Russellpointedoutthatthesetofall\nsets\u2014that is, the set that contains every entity that satisfies the property\nof being a set\u2014cannot logically exist. We\u2019ll see Russell\u2019s reasoning in the\nfollowing chapter. Fregecouldonlyincludeapostscriptinhisbookstating\nthat the basis of the work had been swept away.\nMathematicians responded to these problems by banishing appeals to\nfacts about the real world from mathematical proof. Mathematics was\nto be its own world, built on its own secure foundation. The foundation\nwould be a basic set of assumptions or \u201caxioms\u201d from which everything\nelse would follow by logic. It would only be necessary to show that the\naxioms themselves were logically consistent and complete, and the world\nof mathematics would be secure. Unfortunately, even this was not to be.\nIn the 1930s, Kurt G\u00a8odel showed that there is no consistent, finite set\nof axioms that completely describes even the corner of the mathematical\nworld known as arithmetic. G\u00a8odel showed that given any finite, consistent\nsetofaxioms,therearetruestatementsaboutarithmeticthatdonotfollow\nlogically from those axioms.\nWeareleftwithamathematical worldinwhichironchainsoflogicstill\nbindconclusionstoassumptions. Buttheassumptionsarenolongerrooted 1.6. PROOF 47\nintherealworld. Noristhereanyfinitecoreofaxiomstowhichtherestof\nthe mathematical world can be chained. In this world, axioms are set up\nas signposts in a void, and then structures of logic are built around them.\nForexample, insteadoftalkingaboutthe settheorythatdescribesthereal\nworld,wehavea settheory,basedonagivensetofaxioms. Thatsettheory\nis necessarily incomplete, and it might differ from other set theories which\nare based on other sets of axioms.\nUnderstandably, mathematicians are very picky about getting their\nproofs right. It\u2019s how they construct their world. Students sometimes\nobject that mathematicians are too picky about proving things that are\n\u201cobvious.\u201d But the fact that something is obvious in the real world counts\nfor very little in the constructed world of mathematics. Too many ob-\nvious things have turned out to be dead wrong. (For that matter, even\nthingsintherealworldthatseem\u201cobviously\u201dtruearenotnecessarilytrue\nat all. For example, consider the quantity f(n) = n2 + n + 41. When\nn = 0, f(n) = 41 which is prime; when n = 1, f(n) = 43 which is prime;\nwhen n = 2, f(n) = 47, which is prime. By the time you had calculated\nf(3),f(4),...,f(10) and found that they were all prime, you might con-\ncludethatitis\u201cobvious\u201dthatf(n)isprimeforalln 0. Butthisisnotin\n\u2265\nfact the case! (See exercises.) Similarly, those of you who are baseball fans\nmight consider it \u201cobvious\u201d that if player A has a higher batting average\nagainst left-handers than player B, and player A has a higher batting aver-\nage against right-handers than player B, then player A must have a higher\nbatting average than player B. Again, this is not true!)\nAs we saw in Section 1.5, a formal proof consists of a sequence of state-\nments where each statement is either an assumption or follows by a rule of\nlogic from previous statements. The examples in that section all worked\nwith unspecified generic propositions (p, q, etc). Let us now look at how\nonemightusethesametechniquestoproveaspecificpropositionaboutthe\nmathematical world. We will prove that for all integers n, if n is even then\nn2 is even. (Definition: an integer n is even iff n = 2k for some integer k.\nFor example, 2 is even since 2=2 1; 66 is even since 66=2 33; 0 is even\n\u00b7 \u00b7\nsince 0=2 0.)\n\u00b7\nProof. This is a proposition of the form n(P(n) Q(n)) where P(n) is\n\u2200 \u2192\n\u201cn is even\u201d and Q(n) is \u201cn2 is even.\u201d We need to show that P(n) Q(n)\n\u2192\nis true for all values of n. In the language of Section 1.5, we need to show\nthat for any n, P(n) logically implies Q(n); or, equivalently, that Q(n) can\nbe logically deduced from P(n); or, equivalently, that\nn is even\n\u2234 n2 is even 48 CHAPTER 1. LOGIC AND PROOF\nis a valid argument. Here is a formal proof that\nn is even\n\u2234 n2 is even\nis in fact a valid argument for any value of n:\nLet n be an arbitrary integer.\n1. n is even premise\n2. if n is even, then n=2k\nfor some integer k definition of even\n3. n=2k for some integer k from 1, 2 (modus ponens)\n4. if n=2k for some integer k,\nthen n2 =4k2 for that integer k basic algebra\n5. n2 =4k2 for some integer k from 3, 4 (modus ponens)\n6. if n2 =4k2 for some integer k,\nthen n2 =2(2k2) for that k basic algebra\n7. n2 =2(2k2) for some integer k from 5, 6 (modus ponens)\n8. if n2 =2(2k2) for some integer k,\nthen n2 =2k for some integer k basic fact about integers\n\u2032 \u2032\n9. n2 =2k for some integer k from 7, 8 (modus ponens)\n\u2032 \u2032\n10. if n2 =2k for some integer k ,\n\u2032 \u2032\nthen n2 is even definition of even\n11. n2 is even from 9, 10 (modus ponens)\n(The \u201cbasic fact about integers\u201d referred to above is that the product of\nintegers is again an integer.) Since n could be replaced by any integer\nthroughout this argument, we have proved the statement \u201cif n is even then\nn2 iseven\u201distrueforallintegersn. (Youmightworrythattheargumentis\nonlyvalidforevenn; seethedisclaimeraboutKeanuReeves\u2019actingability\non page 34, or remind yourself that P(n) Q(n) is automatically true if\n\u2192\nP(n) is false.)\nMathematical proofs are rarely presented with this degree of detail and\nformality. Aslightlylessformalproofofourpropositionmightleaveoutthe\nexplicit implications and instances of modus ponens and appear as follows:\nProof. Let n be an arbitrary integer. 1.6. PROOF 49\n1. n is even premise\n2. n=2k for some integer k definition of even\n3. n2 =4k2 for that integer k basic algebra\n4. n2 =2(2k2) for that k basic algebra\n5. n2 =2k for some integer k substituting k =2k2\n\u2032 \u2032 \u2032\n6. n2 is even definition of even\nSince n was an arbitrary integer, the statement is true for all integers.\nA more typical proof would take the argument above and present it in\nprose rather than list form:\nProof. Let n be an arbitrary integer and assume n is even. Then n = 2k\nfor some integer k by the definition of even, and n2 =4k2 =2(2k2). Since\nthe product of integers is an integer, we have n2 =2k for some integer k .\n\u2032 \u2032\nTherefore n2 is even. Since n was an arbitrary integer, the statement is\ntrue for all integers.\nTypically, in a \u201cformal\u201d proof, it is this kind of (relatively) informal\ndiscussion that is given, with enough details to convince the reader that a\ncomplete, formal proof could be constructed. Of course, how many details\nthe reader can be expected to fill in depends on the reader, and reading\nproofs is a skill that must be developed and practiced. Writing a proof is\nevenmoredifficult. Everyproofinvolvesacreativeactofdiscovery,inwhich\na chain of logic that leads from assumptions to conclusion is discovered. It\nalso involves a creative act of expression, in which that logic is presented\ninaclearandconvincingway. Thereisnoalgorithmforproducingcorrect,\ncoherentproofs. Thereare,however,somegeneralguidelinesfordiscovering\nand writing proofs.\nOneofthemostimportantpiecesofadvicetokeepinmindis, \u201cUsethe\ndefinition.\u201d Intheworldofmathematics,termsmeanexactlywhattheyare\ndefined to mean and nothing more. Definitions allow very complex ideas\nto be summarized as single terms. When you are trying to prove things\nabout those terms, you generally need to \u201cunwind\u201d the definitions. In our\nexampleabove,weusedthedefinitionofeventowriten=2k,andthenwe\nworkedwiththatequation. Whenyouaretryingtoprovesomethingabout\nequivalence relations in Chapter 2, you can be pretty sure that you will\nneedtousethefactthatequivalencerelations,bydefinition,aresymmetric,\nreflexive,andtransitive. (And,ofcourse,you\u2019llneedtoknowhowtheterm\n\u201crelation\u201d is defined in the first place! You\u2019ll get nowhere if you work from\nthe idea that \u201crelations\u201d are something like your aunt and uncle.) 50 CHAPTER 1. LOGIC AND PROOF\nMore advice along the same line is to check whether you are using the\nassumptions of the theorem. An assumption that is made in a theorem\nis called an hypothesis. The hypotheses of the theorem state conditions\nwhose truth will guarantee the conclusion of the theorem. To prove the\ntheoremmeanstoassumethatthehypothesesaretrue,andtoshow,under\nthat assumption, that the conclusion must be true. It\u2019s likely (though not\nguaranteed) that you will need to use the hypotheses explicitly at some\npoint in the proof, as we did in our example above.12 Also, you should\nkeep in mind that any result that has already been proved is available to\nbe used in your proof.\nA proof is a logical argument, based on the rules of logic. Since there\nare really not all that many basic rules of logic, the same patterns keep\nshowing up over and over. Let\u2019s look at some of the patterns.\nThemostcommonpatternarisesintheattempttoprovethatsomething\nis true \u201cfor all\u201d or \u201cfor every\u201d or \u201cfor any\u201d entity in a given category. In\ntermsoflogic,thestatementyouaretryingtoproveisoftheform xP(x).\n\u2200\nIn this case, the most likely way to begin the proof is by saying something\nlike, \u201cLet x be an arbitrary entity in the domain of discourse. We want\nto show that P(x).\u201d In the rest of the proof, x refers to some unspecified\nbut definite entity in the domain of discourse. Since x is arbitrary, proving\nP(x) amounts to proving xP(x). You only have to be careful that you\n\u2200\ndon\u2019t use any facts about x beyond what you have assumed. For example,\nin our proof above, we cannot make any assumptions about the integer n\nexcept that it is even; if we had made such assumptions, then the proof\nwould have been incorrect, or at least incomplete.\nSometimes, youhavetoprovethatanentityexiststhatsatisfiescertain\nstatedproperties. Suchaproofiscalledanexistence proof. Inthiscase,\nyouareattemptingtoproveastatementoftheform xP(x). Thewaytodo\n\u2203\nthis is to find an example, that is, to find a specific entity a for which P(a)\nistrue. Onewaytoprovethestatement\u201cThereisanevenprimenumber\u201dis\ntofindaspecificnumberthatsatisfiesthisdescription. Thesamestatement\ncould also be expressed \u201cNot every prime number is odd.\u201d This statement\nhas the form ( xP(x)), which is equivalent to the statement x( P(x)).\n\u00ac \u2200 \u2203 \u00ac\nAn example that proves the statement x( P(x)) also proves ( xP(x)).\n\u2203 \u00ac \u00ac \u2200\nSuch an example is called a counterexample to the statement xP(x):\n\u2200\nA counterexample proves that the statement xP(x) is false. The number\n\u2200\n2 is a counterexample to the statement \u201cAll prime numbers are odd.\u201d In\n12Of course, if you set out to discover new theorems on your own, you aren\u2019t given\nthehypothesesandconclusioninadvance,whichmakesthingsquiteabitharder\u2014and\nmoreinteresting. 1.6. PROOF 51\nfact,2istheonlycounterexampletothisstatement; manystatementshave\nmultiple counterexamples.\nNote that we have now discussed how to prove and disprove universally\nquantifiedstatements,andhowtoproveexistentiallyquantifiedstatements.\nHow do you disprove xP(x)? Recall that xP(x) is logically equivalent\n\u2203 \u00ac\u2203\nto x( P(x)), so to disprove xP(x) you need to prove x( P(x)).\n\u2200 \u00ac \u2203 \u2200 \u00ac\nMany statements, like that in our example above, have the logical\nform of an implication, p q. (More accurately, they are of the form\n\u2192\n\u201c x(P(x) Q(x))\u201d, but as discussed above the strategy for proving such\n\u2200 \u2192\na statement is to prove P(x) Q(x) for an arbitrary element x of the\n\u2192\ndomain of discourse.) The statement might be \u201cFor all natural numbers n,\nif n is even then n2 is even,\u201d or \u201cFor all strings x, if x is in the language\nL then x is generated by the grammar G,\u201d or \u201cFor all elements s, if s A\n\u2208\nthens B.\u201d Sometimestheimplicationisimplicitratherthanexplicit: for\n\u2208\nexample, \u201cThe sumof two rationals isrational\u201d isreally short for\u201cForany\nnumbers x and y, if x and y are rational then x+y is rational.\u201d A proof\nof such a statement often begins something like this: \u201cAssume that p. We\nwanttoshowthatq.\u201d Intherestoftheproof,pistreatedasanassumption\nthat is known to be true. As discussed above, the logical reasoning behind\nthis is that you are essentially proving that\np\n\u2234 q\nis a valid argument. Another way of thinking about it is to remember that\np q is automatically true in the case where p is false, so there is no need\n\u2192\nto handle that case explicitly. In the remaining case, when p is true, we\ncan show that p q is true by showing that the truth of q follows from\n\u2192\nthe truth of p.\nA statement of the form p q can be proven by proving p and q sepa-\n\u2227\nrately. Astatementoftheformp q canbeprovedbyprovingthelogically\n\u2228\nequivalent statement ( p) q: to prove p q, you can assume that p\n\u00ac \u2192 \u2228\nis false and prove, under that assumption, that q is true. For example,\nthe statement \u201cEvery integer is either even or odd\u201d is equivalent to the\nstatement \u201cEvery integer that is not even is odd.\u201d\nSince p q is equivalent to (p q) (q p), a statement of the form\n\u2194 \u2192 \u2227 \u2192\np q is often proved by giving two proofs, one of p q and one of q p.\n\u2194 \u2192 \u2192\nIn English, p q can be stated in several forms such as \u201cp if and only if\n\u2194\nq\u201d, \u201cif p then q and conversely,\u201d and \u201cp is necessary and sufficient for q.\u201d\nThe phrase \u201cif and only if\u201d is so common in mathematics that it is often\nabbreviated iff. 52 CHAPTER 1. LOGIC AND PROOF\nYou should also keep in mind that you can prove p q by displaying a\n\u2192\nchain of valid implications p r s q. Similarly, p q can be\n\u2192 \u2192 \u2192 \u00b7\u00b7\u00b7 \u2192 \u2194\nproved with a chain of valid biconditionals p r s q.\n\u2194 \u2194 \u2194\u00b7\u00b7\u00b7\u2194\nWe\u2019ll turn to a few examples, but first here is some terminology that\nwe will use throughout our sample proofs:\nThenatural numbers (denotedN)arethenumbers0,1,2,.... Note\n\u2022\nthat the sum and product of natural numbers are natural numbers.\nThe integers (denoted Z) are the numbers 0, 1,1, 2,2, 3,3,....\n\u2022 \u2212 \u2212 \u2212\nNote that the sum, product, and difference of integers are integers.\nThe rational numbers (denoted Q) are all numbers that can be\n\u2022 written in the form m where m and n are integers and n = 0. So 1\nn 6 3\nand 65 are rationals; so, less obviously, are 6 and \u221a27 since 6 = 6\n\u22127 \u221a12 1\n(or, for that matter, 6 = 12), and \u221a27 = 27 = 9 = 3. Note\n\u22122 \u221a12 12 4 2\n\u2212\nthe restriction that the number in the denomqinator caqnnot be 0: 3 is\n0\nnotanumberatall,rationalorotherwise;itisanundefinedquantity.\nNote also that the sum, product, difference, and quotient of rational\nnumbers are rational numbers (provided you don\u2019t attempt to divide\nby 0.)\nThe real numbers (denoted R) are numbers that can be written\n\u2022\nin decimal form, possibly with an infinite number of digits after the\ndecimalpoint. Notethatthesum,product,difference,andquotientof\nreal numbers are real numbers (provided you don\u2019t attempt to divide\nby 0.)\nThe irrational numbers are real numbers that are not rational, i.e.\n\u2022\nthat cannot be written as a ratio of integers. Such numbers include\n\u221a3 (which we will prove is not rational) and \u03c0 (if anyone ever told\nyou that \u03c0 = 22, they lied\u201422 is only an approximation of the value\n7 7\nof \u03c0).\nAn integer n is divisible by m iff n=mk for some integer k. (This\n\u2022\ncan also be expressed by saying that m evenly divides n.) So for\nexample,nisdivisibleby2iffn=2k forsomeintegerk;nisdivisible\nby 3 iff n = 3k for some integer k, and so on. Note that if n is not\ndivisibleby2,thennmustbe1morethanamultipleof2son=2k+1\nfor some integer k. Similarly, if n is not divisible by 3 then n must\nbe 1 or 2 more than a multiple of 3, so n=2k+1 or n=2k+2 for\nsome integer k. 1.6. PROOF 53\nAn integer is even iff it is divisible by 2 and odd iff it is not.\n\u2022\nAn integer n > 1 is prime if it is divisible by exactly two positive\n\u2022\nintegers, namely 1 and itself. Note that a number must be greater\nthan 1 to even have a chance of being termed \u201cprime\u201d. In particular,\nneither 0 nor 1 is prime.\nLet\u2019s look now at another example: prove that the sum of any two\nrational numbers is rational.\nProof. We start by assuming that x and y are arbitrary rational numbers.\nHere\u2019s a formal proof that the inference rule\nx is rational\ny is rational\n\u2234 x+y is rational\nis a valid rule of inference: 54 CHAPTER 1. LOGIC AND PROOF\n1. x is rational premise\n2. if x is rational, then x= a\nb\nfor some integers a and b=0 definition of rationals\n3. x= a for some integers a an6 d b=0 from 1,2 (modus ponens)\nb 6\n4. y is rational premise\n5. if y is rational, then y = c for\nd\nsome integers c and d=0 definition of rational\n6. y = c for some c and d=6 0 from 4,5 (modus ponens)\n7. x= d a for some a and b6 =0 and\ny =b c for some c and6 d=0 from 3,6\n8. if x=\nad\nfor some a and\nb=6\n0 and\ny = b c for c and d=0 th6 en\nd 6\nx+y = ad+bcwhere a,b,c,d\nbd\nare integers and b,d=0 basic algebra\n6\n9. x+y = ad+bc for some a,b,c,d\nbd\nwhere b,d=0 from 7,8 (modus ponens)\n6\n10. if x+y = ad+bc for some a,b,c,d\nbd\nwhere b,d=0 then x+y = m\n6 n\nwhere m,n are integers and n=0 properties of integers\n11. x+y = m where m and n 6\nn\nare integers and n=0 from 9,10 (modus ponens)\n12. if x+y = m where m6 and n are\nn\nintegers and n=0\n6\nthen x+y is rational definition of rational\n13. x+y is rational from 11,12 (modus ponens)\nSo the rule of inference given above is valid. Since x and y are arbitrary\nrationals, we have proved that the rule is valid for all rationals, and hence\nthe sum of any two rationals is rational.\nAgain, a more informal presentation would look like:\nProof. Let x and y be arbitrary rational numbers. By the definition of\nrational, there are integers a,b = 0,c,d = 0 such that x = a and y = c.\n6 6 b d\nThenx+y = ad+bc; weknowad+bcandbdareintegerssincethesumand\nbd\nproductofintegersareintegers,andwealsoknowbd=0sinceneitherbnor\n6\ndis0. Sowehavewrittenx+yastheratiooftwointegers,thedenominator\nbeing non-zero. Therefore, by the definition of rational numbers, x + y\nis rational. Since x and y were arbitrary rationals, the sum of any two\nrationals is rational. 1.6. PROOF 55\nAndonemoreexample: wewillprovethatany4-digitnumberd d d d\n1 2 3 4\nis divisible by 3 iff the sum of the four digits is divisible by 3.\nProof. This statement is of the form p q; recall that p q is logically\n\u2194 \u2194\nequivalent to (p q) (q p). So we need to prove for any 4-digit\n\u2192 \u2227 \u2192\nnumberd d d d that(1)ifd d d d isdivisibleby3thend +d +d +d\n1 2 3 4 1 2 3 4 1 2 3 4\nis divisible by 3, and (2) if d +d +d +d is divisible by 3 then d d d d\n1 2 3 4 1 2 3 4\nis divisible by 3. So let d d d d be an arbitrary 4-digit number.\n1 2 3 4\n(1) Assume d d d d is divisible by 3, i.e. d d d d = 3k for some\n1 2 3 4 1 2 3 4\nintegerk. Thenumberd d d d isactuallyd 1000+d 100+d 10+d ,\n1 2 3 4 1 2 3 4\n\u00d7 \u00d7 \u00d7\nso we have the equation\nd 1000+d 100+d 10+d =3k.\n1 2 3 4\n\u00d7 \u00d7 \u00d7\nSince 1000 = 999+1, 100 = 99+1, and 10 = 9+1, this equation can be\nrewritten\n999d +d +99d +d +9d +d +d =3k.\n1 1 2 2 3 3 4\nRearranging gives\nd +d +d +d =3k 999d 99d 9d\n1 2 3 4 1 2 3\n\u2212 \u2212 \u2212\n=3k 3(333d ) 3(33d ) 3(3d ).\n1 2 3\n\u2212 \u2212 \u2212\nWe can now factor a 3 from the right side to get\nd +d +d +d =3(k 333d 33d d ).\n1 2 3 4 1 2 3\n\u2212 \u2212 \u2212\nSince(k 333d 33d d )isaninteger,wehaveshownthatd +d +d +d\n1 2 3 1 2 3 4\n\u2212 \u2212 \u2212\nis divisible by 3.\n(2) Assume d +d +d +d is divisible by 3. Consider the number\n1 2 3 4\nd d d d . As remarked above,\n1 2 3 4\nd d d d =d 1000+d 100+d 10+d\n1 2 3 4 1 2 3 4\n\u00d7 \u00d7 \u00d7\nso\nd d d d =999d +d +99d +d +9d +d +d\n1 2 3 4 1 1 2 2 3 3 4\n=999d +99d +9d +(d +d +d +d ).\n1 2 3 1 2 3 4\nWe assumed that d +d +d +d = 3k for some integer k, so we can\n1 2 3 4\nsubstitute into the last equation to get\nd d d d =999d +99d +9d +3k =3(333d +33d +3d +k).\n1 2 3 4 1 2 3 1 2 3 56 CHAPTER 1. LOGIC AND PROOF\nSincethequantityinparenthesesisaninteger,wehaveprovedthatd d d d\n1 2 3 4\nis divisible by 3.\nIn (1) and (2) above, the number d d d d was an arbitrary 4-digit\n1 2 3 4\ninteger, so we have proved that for all 4-digit integers, d d d d is divisible\n1 2 3 4\nby 3 iff the sum of the four digits is divisible by 3.\nNow suppose we wanted to prove the statement \u201cFor all integers n,\nn2 is even if and only if n is even.\u201d We have already proved half of this\nstatement (\u201cFor all integers n, if n is even then n2 is even\u201d), so all we\nneed to do is prove the statement \u201cFor all integers n, if n2 is even then n\nis even\u201d and we\u2019ll be done. Unfortunately, this is not as straightforward\nas it seems: suppose we started in our standard manner and let n be an\narbitrary integer and assumed that n2 =2k for some integer k. Then we\u2019d\nbe stuck! Taking the square root of both sides would give us n on the left\nbut would leave a \u221a2k on the right. This quantity is not of the form 2k\n\u2032\nforanyintegerk ; multiplyingitby \u221a2 wouldgive2\u221ak butthereisnoway\n\u2032 \u221a2 \u221a2\nfor us to prove that \u221ak is an integer. So we\u2019ve hit a dead end. What do\n\u221a2\nwe do now?\nThe answer is that we need a different proof technique. The proofs we\nhave written so far are what are called direct proofs: to prove p q you\n\u2192\nassume p is true and prove that the truth of q follows. Sometimes, when a\ndirect proof of p q fails, an indirect proof will work. Recall that the\n\u2192\ncontrapositive of the implication p q is the implication q p, and\n\u2192 \u00ac \u2192 \u00ac\nthat this proposition is logically equivalent to p q. An indirect proof\n\u2192\nof p q, then, is a direct proof of the contrapositive q p. In our\n\u2192 \u00ac \u2192 \u00ac\ncurrent example, instead of proving \u201cif n2 is even then n is even\u201d directly,\nwe can prove its contrapositive \u201cif n is not even (i.e. n is odd) then n2\nis not even (i.e. n2 is odd.)\u201d The proof of this contrapositive is a routine\ndirect argument which we leave to the exercises.\nExercises\n1. Find a natural number n for which n2+n+41 is not prime.\n2. Show that the propositions p q and ( p) q are logically equivalent.\n\u2228 \u00ac \u2192\n3. Show that the proposition (p q) r is equivalent to (p r) (q r).\n\u2228 \u2192 \u2192 \u2227 \u2192\n4. Determine whether each of the following statements is true. If it true, prove\nit. If it is false, give a counterexample.\na) Every prime number is odd.\nb) Every prime number greater than 2 is odd. 1.7. PROOF BY CONTRADICTION 57\nc) If x and y are integers with x<y, then there is an integer z such that\nx<z<y.\nd) If x and y are real numbers with x<y, then there is a real number z\nsuch that x<z<y.\n5. Supposethatr,s,andtareintegers,suchthatrevenlydividessandsevenly\ndivides t. Prove that r evenly divides t.\n6. Prove that for all integers n, if n is odd then n2 is odd.\n7. Prove that an integer n is divisible by 3 iff n2 is divisible by 3. (Hint: give\nan indirect proof of \u201cif n2 is divisible by 3 then n is divisible by 3.\u201d)\n8. Prove or disprove each of the following statements.\na) The product of two even integers is even.\nb) The product of two integers is even only if both integers are even.\nc) The product of two rational numbers is rational.\nd) The product of two irrational numbers is irrational.\ne) For all integers n, if n is divisible by 4 then n2 is divisible by 4.\nf) For all integers n, if n2 is divisible by 4 then n is divisible by 4.\n1.7 Proof by Contradiction\nSuppose that we start with some set of assumptions and apply rules of\nlogic to derive a sequence of statements that can be proved from those\nassumptions, and suppose that we derive a statement that we know to be\nfalse. Whenthelawsoflogicareappliedtotruestatements,thestatements\nthatarederivedwillalsobetrue. Ifwederiveafalsestatementbyapplying\nrules of logic to a set of assumptions, then at least one of the assumptions\nmust be false. This observation leads to a powerful proof technique, which\nis known as proof by contradiction.\nSupposethatyouwanttoprovesomeproposition, p. Toapplyproofby\ncontradiction, assumethat pistrue, andapplytherulesoflogictoderive\n\u00ac\nconclusionsbasedonthisassumption. Ifitispossibletoderiveastatement\nthatisknowntobefalse, itfollowsthattheassumption, p, mustbefalse.\n\u00ac\n(Ofcourse, ifthederivationisbasedonseveralassumptions, thenyouonly\nknow that at least one of the assumptions must be false.) The fact that\np is false proves that p is true. Essentially, you are arguing that p must\n\u00ac\nbe true, because if it weren\u2019t, then some statement that is known to be\nfalse could be proved to be true. Generally, the false statement that is\nderived in a proof by contradiction is of the form q q. This statement\n\u2227\u00ac\nis a contradiction in the sense that it is false no matter what the value of\nq. Note that deriving the contradiction q q is the same as showing that\n\u2227\u00ac\nthe two statements, q and q, both follow from the assumption that p.\n\u00ac \u00ac 58 CHAPTER 1. LOGIC AND PROOF\nAs a first example of proof by contradiction, consider the following the-\norem:\nTheorem 1.4. The number \u221a3 is irrational.\nProof. Assume for the sake of contradiction that \u221a3 is rational. Then \u221a3\ncan be written as the ratio of two integers, \u221a3 =\nm\u2032\nfor some integers m\nn\u2032 \u2032\nand n. Furthermore, the fraction\nm\u2032\ncan be reduced to lowest terms by\n\u2032 n\u2032\ncancelingallcommonfactorsofm andn. So\u221a3= m forsomeintegersm\n\u2032 \u2032 n\nand n which have no common factors. Squaring both sides of this equation\ngives 3 = m2 and re-arranging gives 3n2 = m2. From this equation we see\nn2\nthat m2 is divisible by 3; you proved in the previous section (Exercise 6)\nthat m2 is divisible by 3 iff m is divisible by 3. Therefore m is divisible\nby 3 and we can write m = 3k for some integer k. Substituting m = 3k\ninto the last equation above gives 3n2 =(3k)2 or 3n2 =9k2, which in turn\nbecomes n2 =3k2. From this we see that n2 is divisible by 3, and again we\nknow that this implies that n is divisible by 3. But now we have (i) m and\nnhavenocommonfactors,and(ii)mandnhaveacommonfactor,namely\n3. It is impossible for both these things to be true, yet our argument has\nbeen logically correct. Therefore our original assumption, namely that \u221a3\nis rational, must be incorrect. Therefore \u221a3 must be irrational.\nOne of the oldest mathematical proofs, which goes all the way back to\nEuclid,isaproofbycontradiction. Recallthataprimenumberisaninteger\nn, greater than 1, such that the only positive integers that evenly divide n\nare 1 and n. We will show that there are infinitely many primes. Before\nwe get to the theorem, we need a lemma. (A lemma is a theorem that\nis introduced only because it is needed in the proof of another theorem.\nLemmas help to organize the proof of a major theorem into manageable\nchunks.)\nLemma 1.5. If N is an integer and N >1, then there is a prime number\nwhich evenly divides N.\nProof. Let D be the smallest integer which is greater than 1 and which\nevenly divides N. (D exists since there is at least one number, namely N\nitself, which is greater than 1 and which evenly divides N. We use the fact\nthat any non-empty subset of N has a smallest member.) I claim that D is\nprime, so that D is a prime number that evenly divides N.\nSuppose that D is not prime. We show that this assumption leads to a\ncontradiction. Since D is not prime, then, by definition, there is a number\nk between 2 and D 1, inclusive, such that k evenly divides D. But since\n\u2212\nD evenly divides N, we also have that k evenly divides N (by exercise 5 1.7. PROOF BY CONTRADICTION 59\nin the previous section). That is, k is an integer greater than one which\nevenly divides N. But since k is less than D, this contradicts the fact that\nDisthesmallest suchnumber. ThiscontradictionprovesthatDisaprime\nnumber.\nTheorem 1.6. There are infinitely many prime numbers.\nProof. Suppose that there are only finitely many prime numbers. We will\nshow that this assumption leads to a contradiction.\nLet p , p , ..., p be a complete list of all prime numbers (which exists\n1 2 n\nunder the assumption that there are only finitely many prime numbers).\nConsider the number N obtained by multiplying all the prime numbers\ntogether and adding one. That is,\nN =(p p p p )+1.\n1 2 3 n\n\u00b7 \u00b7 \u00b7\u00b7\u00b7\nNow, since N is larger than any of the prime numbers p , and since p ,\ni 1\np , ..., p is a complete list of prime numbers, we know that N cannot\n2 n\nbe prime. By the lemma, there is a prime number p which evenly divides\nN. Now, p must be one of the numbers p , p , ..., p . But in fact, none\n1 2 n\nof these numbers evenly divides N, since dividing N by any p leaves a\ni\nremainder of 1. This contradiction proves that the assumption that there\nare only finitely many primes is false.\nThis proof demonstrates the power of proof by contradiction. The fact\nthat is proved here is not at all obvious, and yet it can be proved in just a\nfew paragraphs.\nExercises\n1. Suppose that a1, a2, ..., a10 are real numbers, and suppose that a1+a2+\n+a10 >100. Useaproofbycontradictiontoconcludethatatleastoneof\n\u00b7\u00b7\u00b7\nthe numbers a i must be greater than 10.\n2. Prove that each of the following statements is true. In each case, use a proof\nby contradiction. Remember that the negation of p q is p q.\na) Let nbe aninteger. Ifn2 is an even integer, t\u2192 hen nis\u2227 an\u00ac even integer.\nb) \u221a2 is irrational.\nc) If r is a rational number and x is an irrational number, then r+x is\nan irrational number. (That is, the sum of a rational number and an\nirrational number is irrational.)\nd) If r is a non-zero rational number and x is an irrational number, then\nrx is an irrational number.\ne) If r and r+x are both rational, then x is rational. 60 CHAPTER 1. LOGIC AND PROOF\n3. The pigeonhole principle is the following obvious observation: If you have\nnpigeonsink pigeonholesandifn>k,thenthereisatleastonepigeonhole\nthat contains more than one pigeon. Even though this observation seems\nobvious, it\u2019s a good idea to prove it. Prove the pigeonhole principle using a\nproof by contradiction.\n1.8 Mathematical Induction\nThestructureofthenaturalnumbers\u20140,1,2,3,andontoinfinity\u2014makes\npossibleapowerfulprooftechniqueknownasinductionormathematical\ninduction. The idea behind induction is simple. Let P be a one-place\npredicatewhosedomainofdiscourseincludesthenaturalnumbers. Suppose\nthat we can prove that P(0) is true. Suppose that we can also prove the\nstatements P(0) P(1), P(1) P(2), P(2) P(3), and so on. The\n\u2192 \u2192 \u2192\nprincipal of mathematical induction is the observation that we can then\nconcludethatP(n)istrueforall natural numbersn. Thisshouldbeclear.\nSince P(0) and P(0) P(1) are true, we can apply the rule of modus\n\u2192\nponens to conclude that P(1) is true. Then, since P(1) and P(1) P(2)\n\u2192\nare true, we can conclude by modus ponens that P(2) is true. From P(2)\nandP(2) P(3),weconcludethatP(3)istrue. Foranygivennintheset\n\u2192\nN, we can continue this chain of deduction for n steps to prove that P(n)\nis true.\nWhen applying induction, we don\u2019t actually prove each of the implica-\ntions P(0) P(1), P(1) P(2), and so on, individually. That would\n\u2192 \u2192\nrequire an infinite amount of work. The whole point of induction is to\navoid any infinitely long process. Instead, we prove k(P(k) P(k+1))\n\u2200 \u2192\n(where the domain of discourse for the predicate P is N). The statement\nk(P(k) P(k+1)) summarizes all the infinitely many implications in a\n\u2200 \u2192\nsingle statement. Stated formally, the principle of mathematical induction\nsaysthatifwecanprovethestatementP(0) k(P(k) P(k+1) ,then\n\u2227 \u2200 \u2192\nwe can deduce that nP(n) (again, with N as the domain of discourse).\n\u2200 (cid:0) (cid:1)\nIt should be intuitively clear that the principle of induction is valid. It\nfollows from the fact that the list 0, 1, 2, 3, ..., if extended long enough,\nwilleventuallyincludeanygivennaturalnumber. IfwestartfromP(0)and\ntake enough steps of the form P(k) P(k+1), we can get P(n) for any\n\u2192\ngiven natural number n. However, whenever we deal with infinity, we are\ncourtingthepossibilityofparadox. Wewillprovetheprincipleofinduction\nrigorously in the next chapter (see Theorem 2.3), but for now we just state\nit as a theorem:\nTheorem 1.7. Let P be a one-place predicate whose domain of discourse 1.8. MATHEMATICAL INDUCTION 61\nincludes the natural numbers. Suppose that P(0) k N(P(k) P(k+\n\u2227 \u2200 \u2208 \u2192\n1)) . Then P(n) is true for all natural numbers n. (That is, the statement\n(cid:0)\nnP(n) is true, where the domain of discourse for P is the set of natural\n\u2200 (cid:1)\nnumbers.)\nMathematical induction can be applied in many situations: you can\nprovethingsaboutstringsofcharactersbydoinginductiononthelengthof\nthe string, things about graphs by doing induction on the number of nodes\nin the graph, things about grammars by doing induction on the number of\nproductions in the grammar, and soon. We\u2019ll be looking at applications of\ninduction for the rest of this chapter, and throughout the remainder of the\ntext. Althoughproofsbyinductioncanbeverydifferentfromoneanother,\nthey all follow just a few basic structures. A proof based on the preceding\ntheorem always has two parts. First, P(0) is proved. This is called the\nbase case of the induction. Then the statement k(P(k) P(k+1)) is\n\u2200 \u2192\nproved. This statement can be proved by letting k be an arbitrary element\nofNandprovingP(k) P(k+1). Thisinturncanbeprovedbyassuming\n\u2192\nthat P(k) is true and proving that the truth of P(k+1) follows from that\nassumption. Thiscaseiscalledtheinductive case, andP(k)iscalledthe\ninductive hypothesis ortheinduction hypothesis. Notethatthebase\ncase is just as important as the inductive case. By itself, the truth of the\nstatement k(P(k) P(k+1)) says nothing at all about the truth of any\n\u2200 \u2192\noftheindividualstatementsP(n). ThechainofimplicationsP(0) P(1),\n\u2192\nP(1) P(2), ..., P(n 1) P(n) says nothing about P(n) unless the\n\u2192 \u2212 \u2192\nchainisanchoredattheotherendbythetruthofP(0). Let\u2019slookatafew\nexamples.\nTheorem 1.8. Thenumber 22n 1is divisible by3 for allnatural numbers\n\u2212\nn.\nProof. Here, P(n) is the statement that 22n 1 is divisible by 3.\n\u2212\nBase case: When n=0, 22n 1=20 1=1 1=0 and 0 is divisible\n\u2212 \u2212 \u2212\nby 3 (since 0=3 0.) Therefore the statement holds when n=0.\n\u00b7\nInductive case: We want to show that if the statement is true for n=k\n(where k is an arbitrary natural number), then it is true for n = k +1\nalso. That is, we must prove the implication P(k) P(k +1). So we\n\u2192\nassumeP(k),thatis,weassumethat22k isdivisibleby3. Thismeansthat\n22k 1=3m for some integer m. We want to prove P(k+1), that is, that\n\u2212 62 CHAPTER 1. LOGIC AND PROOF\n22(k+1) 1 is also divisible by 3:\n\u2212\n22(k+1) 1=22k+2 1\n\u2212 \u2212\n=22k 22 1 properties of exponents\n\u00b7 \u2212\n=4 22k 1\n\u00b7 \u2212\n=4 22k 4+4 1\n\u00b7 \u2212 \u2212\n=4(22k 1)+3 algebra\n\u2212\n=4(3m)+3 the inductive hypothesis\n=3(4m+1) algebra\nand from the last line we see that 22k+1 is in fact divisible by 3. (The\nthird step\u2014subtracting and adding 4\u2014was done to enable us to use our\ninductive hypothesis.)\nAltogether, we have proved that P(0) holds and that, for all k, P(k)\n\u2192\nP(k+1) is true. Therefore, by the principle of induction, P(n) is true for\nall n in N, i.e. 22n 1 is divisible by 3 for all n in N.\n\u2212\nThe principal of mathematical induction gives a method for proving\nP(n) for all n in the set N. It should be clear that if M is any natural\nnumber, a similar method can be used to show that P(n) is true for all\nnatural numbers n that satisfy n M. Just start the induction with a\n\u2265\nbase case of n=M instead of with a base case of n=0. I leave the proof\nofthisextensionoftheprincipleofinductionasanexercise. Wecanusethe\nextended principle of induction to prove a result that was first mentioned\nin Section 1.1.\nTheorem 1.9. Suppose that a compound proposition contains exactly n\npropositional variables, where n 1. Then there are exactly 2n different\n\u2265\nways of assigning truth values to the n variables.\nProof. Let P(n) be the statement \u201cThere are exactly 2n different ways of\nassigning truth values to n propositional variables.\u201d We will use induction\nto prove the P(n) is true for all n 1.\n\u2265\nBase case: First, we prove the statement P(1). If there is exactly one\nvariable, then there are exactly two ways of assigning a truth value to that\nvariable. Namely, the variable can be either true or false. Since 2 = 21,\nP(1) is true.\nInductivecase: SupposethatP(k)isalreadyknowntobetrue. Wewant\nto prove that, under this assumption, P(k+1) is also true. Suppose that\np , p , ..., p are k+1 propositional variables. Since we are assuming\n1 2 k+1 1.8. MATHEMATICAL INDUCTION 63\nthat P(k) is true, we know that there are 2k ways of assigning truth values\nto p , p , ..., p . But each assignment of truth values to p , p , ..., p\n1 2 k 1 2 k\ncan be extended to the complete list p , p , ..., p , p in two ways.\n1 2 k k+1\nNamely, p can be assigned the value true or the value false. It follows\nk+1\nthatthereare2 2k waysofassigningtruthvaluestop ,p ,...,p . Since\n1 2 k+1\n\u00b7\n2 2k =2k+1, this finishes the proof.\n\u00b7\nThe sum of an arbitrary number of terms is written using the symbol\n. (ThissymbolistheGreeklettersigma,whichisequivalenttotheLatin\nletter S and stands for \u201csum.\u201d) Thus, we have\nP\n5\ni2 =12+22+32+42+52\ni=1\nX\n7\na =a +a +a +a +a\nk 3 4 5 6 7\nk=3\nX\nN\n1 1 1 1 1\n= + + + +\nn+1 0+1 1+1 2+1 \u00b7\u00b7\u00b7 N +1\nn=0\nX\nThis notation for a sum, using the operator, is called summation no-\ntation. A similar notation for products uses the symbol . (This is the\nP\nGreek letter pi, which is equivalent to the Latin letter P and stands for\nQ\n\u201cproduct.\u201d) For example,\n5\n(3k+2)=(3 2+2)(3 3+2)(3 4+2)(3 5+2)\n\u00b7 \u00b7 \u00b7 \u00b7\nk=2\nY\nn\n1 1 1 1\n=\ni 1 \u00b7 2\u00b7\u00b7\u00b7n\u00b7\ni=1\nY\nInduction can be used to prove many formulas that use these notations.\nHere are two examples:\nn\nn(n+1)\nTheorem 1.10. i= for any integer n greater than zero.\n2\ni=1\nX\nn\nn(n+1)\nProof. Let P(n) be the statement i = . We use induction to\n2\ni=1\nX\nshow that P(n) is true for all n 1.\n\u2265 64 CHAPTER 1. LOGIC AND PROOF\n1\nBase case: Consider the case n=1. P(1) is the statement that i=\ni=1\nX\n1\n1(1+1) 1(1+1)\n. Since i=1 and =1, P(1) is true.\n2 2\ni=1\nX\nInductive case: Let k > 1 be arbitrary, and assume that P(k) is true.\nk+1\nWe want to show that P(k+1) is true. P(k+1) is the statement i=\ni=1\nX\n(k+1)(k+2)\n. But\n2\nk+1 k\ni= i +(k+1)\n!\ni=1 i=1\nX X\nk(k+1)\n= +(k+1) (inductive hypothesis)\n2\nk(k+1) 2(k+1)\n= +\n2 2\nk(k+1)+2(k+1)\n=\n2\n(k+2)(k+1)\n=\n2\n(k+1)(k+2)\n=\n2\nwhich is what we wanted to show. This computation completes the induc-\ntion.\nn\nTheorem 1.11. i2i 1 =(n 1) 2n+1 for any natural number n>0.\n\u2212\n\u2212 \u00b7\ni=1\nX\nn\nProof. Let P(n) be the statement i2i 1 = (n 1) 2n +1. We use\n\u2212\n\u2212 \u00b7\ni=1\nX\ninduction to show that P(n) is true for all n>0\nBase case: Consider the case n = 1. P(1) is the statement that\n1\ni2i 1 = (1 1) 21 + 1. Since each side of this equation is equal\n\u2212\n\u2212 \u00b7\ni=1\nX\nto one, this is true.\nInductive case: Let k > 1 be arbitrary, and assume that P(k) is true. 1.8. MATHEMATICAL INDUCTION 65\nk+1\nWewanttoshowthatP(k+1)istrue. P(k+1)isthestatement i2i 1 =\n\u2212\ni=1\n((k+1) 1) 2k+1+1. But, we can compute that X\n\u2212 \u00b7\nk+1 k\ni2i 1 = i2i 1 +(k+1)2(k+1) 1\n\u2212 \u2212 \u2212\n!\ni=1 i=1\nX X\n= (k 1) 2k+1 +(k+1)2k (inductive hypothesis)\n\u2212 \u00b7\n=(cid:0)(k 1)+(k+1(cid:1)) 2k+1\n\u2212\n=(cid:0)(k 2) 2k+1 (cid:1)\n\u00b7 \u00b7\n=k2k+1+1\nwhich is what we wanted to show. This completes the induction.\n100\nForexample,thesetheoremsshowthat i=1+2+3+4+ +100=\n\u00b7\u00b7\u00b7\ni=1\nX\n100(100+1)\n=5050andthat1 20+2 21+3 22+4 23+5 24 =(5 1)25+1=\n2 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u2212\n129, as well as infinitely many other such sums.\nThereisasecondformoftheprincipleofmathematicalinductionwhich\nis useful in some cases. To apply the first form of induction, we assume\nP(k) for an arbitrary natural number k and show that P(k +1) follows\nfrom that assumption. In the second form of induction, the assumption\nis that P(x) holds for all x between 0 and k inclusive, and we show that\nP(k +1) follows from this. This gives us a lot more to work with when\ndeducing P(k+1). We will need this second form of induction in the next\ntwo sections. A proof will be given in the next chapter.\nTheorem 1.12. Let P be a one-place predicate whose domain of discourse\nincludes the natural numbers. Suppose that P(0) is true and that\n(P(0) P(1) P(k)) P(k+1)\n\u2227 \u2227\u00b7\u00b7\u00b7\u2227 \u2192\nis true for each natural number k 0. Then P(n) is true for every natural\n\u2265\nnumber n.\nForexample,wecanusethistheoremtoprovethateveryintegergreater\nthan one can be written as a product of prime numbers (where a number\nthat is itself prime is considered to be a product of one prime number).\nTheproofillustratesanimportantpointaboutapplicationsofthistheorem: 66 CHAPTER 1. LOGIC AND PROOF\nWhen proving P(k+1), you don\u2019t necessarily have to use the assumptions\nthat P(0), P(1), ..., and P(k) are true. If P(k + 1) is proved by any\nmeans\u2014possibly including the assumptions\u2014then the statement (P(0)\n\u2227\nP(1) P(k)) P(k+1) has been shown to be true. It follows from\n\u2227\u00b7\u00b7\u00b7\u2227 \u2192\nthisobservationthatseveralnumbers, notjustzero, canbe\u201cbasecases\u201din\nthesensethatP(x+1)canbeprovedindependentlyofP(0)throughP(x).\nIn this sense, 0, 1, and every prime number are base cases in the following\ntheorem.\nTheorem 1.13. Every natural number greater than one can be written as\na product of prime numbers.\nProof. Let P(n) be the statement \u201cif n > 1, then n can be written as a\nproduct of prime numbers.\u201d We will prove that P(n) is true for all n by\napplying the second form of the principle of induction.\nNote that P(0) and P(1) are both automatically true, since n=0 and\nn = 1 do not satisfy the condition that n > 1, and P(2) is true since 2 is\nthe product of the single prime number 2. Suppose that k is an arbitrary\nnatural number with k > 1, and suppose that P(0), P(1), ..., P(k) are\nalready known to be true; we want to show that P(k+1) is true. In the\ncase where k+1 is a prime number, then k+1 is a product of one prime\nnumber, so P(k+1) is true.\nConsider the case where k +1 is not prime. Then, according to the\ndefinition of prime number, it is possible to write k+1 = ab where a and\nb are numbers in the range from 2 to k inclusive. Since P(0) through P(k)\nare known to be true, a and b can each be written as a product of prime\nnumbers. Since k+1=ab, k+1 can also be written as a product of prime\nnumbers. WehaveshownthatP(k+1)followsfromP(0) P(1) P(k),\n\u2227 \u2227\u00b7\u00b7\u00b7\u2227\nand this completes the induction.\nExercises\n1. Use induction to prove that n3 +3n2 +2n is divisible by 3 for all natural\nnumbers n.\n2. Use induction to prove that\nn 1 rn+1\nri = \u2212\n1 r\nXi=0 \u2212\nfor any natural number n and for any real number r such that r=1.\n6\n3. Use induction to prove that for any natural number n,\nn\n1 1\n=2\n2i \u2212 2n\nXi=0 1.9. APPLICATION: RECURSION AND INDUCTION 67\nInadditiontoprovingthisbyinduction,showthatitfollowsasacorollaryof\nExercise 2.\n4. Use induction to prove that for any natural number n,\nn\n2i =2n+1 1\n\u2212\nXi=0\nInadditiontoprovingthisbyinduction,showthatitfollowsasacorollaryof\nExercise 2.\n5. Use induction to prove that for any positive integer n,\nn\nn(n+1)(2n+1)\ni2 =\n6\nXi=1\n6. Use induction to prove that for any positive integer n,\nn\n(2i 1)=n2\n\u2212\nXi=1\n7. Evaluate the following sums, using results proved in this section and in the\nprevious exercises:\na) 1+3+5+7+9+11+13+15+17+19\n1 1 1 1 1 1\nb) 1+ + + + + +\n3 32 33 34 35 36\nc) 50+51+52+53+ +99+100\n\u00b7\u00b7\u00b7\nd) 1+4+9+16+25+36+49+81+100\n1 1 1\ne) + + +\n22 23 \u00b7\u00b7\u00b7 299\n8. Write each of the sums in the preceding problem using summation notation.\n9. RewritetheproofsofTheorem1.10andTheorem1.11withoutusingsumma-\ntion notation.\n10. Use induction to prove the following generalized distributive laws for propo-\nsitional logic: For any natural number n>1 and any propositions q, p1, p2,\n..., p n,\na) q (p1 p2 p n)=(q p1) (q p2) (q p n)\n\u2227 \u2228 \u2228\u00b7\u00b7\u00b7\u2228 \u2227 \u2228 \u2227 \u2228\u00b7\u00b7\u00b7\u2228 \u2227\nb) q (p1 p2 p n)=(q p1) (q p2) (q p n)\n\u2228 \u2227 \u2227\u00b7\u00b7\u00b7\u2227 \u2228 \u2227 \u2228 \u2227\u00b7\u00b7\u00b7\u2227 \u2228\n1.9 Application: Recursion and Induction\nIn computer programming, there is a technique called recursion that is\nclosely related to induction. In a computer program, a subroutine is a\nnamed sequence of instructions for performing a certain task. When that\ntask needs to be performed in a program, the subroutine can be called by 68 CHAPTER 1. LOGIC AND PROOF\nname. A typical way to organize a program is to break down a large task\nintosmaller,simplersubtasksbycallingsubroutinestoperformeachofthe\nsubtasks. Asubroutinecanperformitstaskbycallingothersubroutinesto\nperformsubtasksoftheoveralltask. Asubroutinecanalsocallitself. That\nis, in the process of performing some large task, a subroutine can call itself\nto perform a subtask. This is known as recursion, and a subroutine that\ndoes this is said to be a recursive subroutine. Recursion is appropriate\nwhen a large task can be broken into subtasks where some or all of the\nsubtasks are smaller, simpler versions of the main task.\nLike induction, recursion is often considered to be a \u201chard\u201d topic by\nstudents. Professors, on the other hand, often say that they can\u2019t see what\nall the fuss is about, since induction and recursion are elegant methods\nwhich \u201cobviously\u201d work. In fairness, students have apoint, sinceinduction\nand recursion both manage to pull infinite rabbits out of very finite hats.\nBut the magic is indeed elegant, and learning the trick is very worthwhile.\nA simple example of a recursive subroutine is a function that computes\nn! for a non-negative integer n. n!, which is read \u201cn factorial,\u201d is defined\nas follows:\n0!=1\nn\nn!= i for n>0\ni=1\nY\nFor example, 5!=1 2 3 4 5=120. Note that for n>1,\n\u00b7 \u00b7 \u00b7 \u00b7\nn n 1\n\u2212\nn!= i= i n= (n 1)! n\n!\u00b7 \u2212 \u00b7\ni=1 i=1\nY Y (cid:0) (cid:1)\nItisalsotruethatn!= (n 1)! nwhenn=1. Thisobservationmakesit\n\u2212 \u00b7\npossibletowritearecursivefunctiontocomputen!. (Alltheprogramming\n(cid:0) (cid:1)\nexamples in this section are written in the Java programming language.)\nint factorial( int n ) {\n\/\/ Compute n!. Assume that n >= 0.\nint answer;\nif ( n == 0 ) {\nanswer = 1;\n}\nelse {\nanswer = factorial( n-1 ) * n; 1.9. APPLICATION: RECURSION AND INDUCTION 69\n}\nreturn answer;\n}\nIn order to compute factorial(n) for n > 0, this function first computes\nfactorial(n 1) by calling itself recursively. The answer from that compu-\n\u2212\ntation is then multiplied by n to give the value of n!. The recursion has a\nbase case, namely the case when n = 0. For the base case, the answer is\ncomputed directly rather than by using recursion. The base case prevents\ntherecursionfromcontinuingforever, inaninfinitechainofrecursivecalls.\nNow, as it happens, recursion is not the best way to compute n!. It\ncan be computed more efficiently using a loop. Furthermore, except for\nsmall values of n, the value of n! is outside the range of numbers that\ncan be represented as 32-bit ints. However, ignoring these problems, the\nfactorial function provides a nice first example of the interplay between\nrecursion and induction. We can use induction to prove that factorial(n)\ndoes indeed compute n! for n 0. (In the proof, we pretend that the\n\u2265\ndata type int is not limited to 32 bits. In reality, the function only gives\nthe correct answer when the answer can be represented as a 32-bit binary\nnumber.)\nTheorem 1.14. Assume that the data type int can represent arbitrarily\nlarge integers. Under this assumption, the factorial function defined above\ncorrectly computes n! for any natural number n.\nProof. LetP(n)bethestatement\u201cfactorial(n)correctlycomputesn!.\u201d We\nuse induction to prove that P(n) is true for all natural numbers n.\nBase case: In the case n = 0, the if statement in the function assigns\nthe value 1 to the answer. Since 1 is the correct value of 0!, factorial(0)\ncorrectly computes 0!.\nInductive case: Let k be an arbitrary natural number, and assume that\nP(k) is true. From this assumption, we must show that P(k+1) is true.\nThe assumption is that factorial(k) correctly computes k!, and we want to\nshow that factorial(k+1) correctly computes (k+1)!.\nWhenthefunctioncomputesfactorial(k+1),thevalueoftheparameter\nn is k+1. Since k+1>0, the if statement in the function computes the\nvalue of factorial(k +1) by applying the computation factorial(k) (k +\n\u2217\n1). We know, by the induction hypothesis, that the value computed by\nfactorial(k) is k!. It follows that the value computed by factorial(k+1) is\n(k!) (k+1). Asweobservedabove,foranyk+1>0,(k!) (k+1)=(k+1)!.\n\u00b7 \u00b7\nWe see that factorial(k+1) correctly computes (k+1)!. This completes\nthe induction. 70 CHAPTER 1. LOGIC AND PROOF\nIn this proof, we see that the base case of the induction corresponds\nto the base case of the recursion, while the inductive case corresponds to a\nrecursivesubroutinecall. Arecursivesubroutinecall,liketheinductivecase\nof an induction, reduces a problem to a \u201csimpler\u201d or \u201csmaller\u201d problem,\nwhich is closer to the base case.\nAnotherstandardexampleofrecursionistheTowersofHanoiproblem.\nLetnbeapositiveinteger. Imagineasetofndisksofdecreasingsize,piled\nup in order of size, with the largest disk on the bottom and the smallest\ndisk on top. The problem is to move this tower of disks to a second pile,\nfollowing certain rules: Only one disk can be moved at a time, and a disk\ncan only be placed on top of another disk if the disk on top is smaller.\nWhilethedisksarebeingmovedfromthefirstpiletothesecondpile,disks\ncan be kept in a third, spare pile. All the disks must at all times be in one\nof the three piles. For example, if there are two disks, the problem can be\nsolved by the following sequence of moves:\nMove disk 1 from pile 1 to pile 3\nMove disk 2 from pile 1 to pile 2\nMove disk 1 from pile 3 to pile 2\nA simple recursive subroutine can be used to write out the list of moves\nto solve the problem for any value of n. The recursion is based on the\nobservation that for n > 1, the problem can be solved as follows: Move\nn 1 disks from pile number 1 to pile number 3 (using pile number 2 as\n\u2212\na spare). Then move the largest disk, disk number n, from pile number\n1 to pile number 2. Finally, move the n 1 disks from pile number 3 to\n\u2212\npile number 2, putting them on top of the nth disk (using pile number 1\nas a spare). In both cases, the problem of moving n 1 disks is a smaller\n\u2212\nversion of the original problem and so can be done by recursion. Here is\nthe subroutine, written in Java:\nvoid Hanoi(int n, int A, int B, int C) {\n\/\/ List the moves for moving n disks from\n\/\/ pile number A to pile number B, using\n\/\/ pile number C as a spare. Assume n > 0.\nif ( n == 1 ) {\nSystem.out.println(\"Move disk 1 from pile \"\n+ A + \" to pile \" + B);\n}\nelse {\nHanoi( n-1, A, C, B );\nSystem.out.println(\"Move disk \" + n 1.9. APPLICATION: RECURSION AND INDUCTION 71\n+ \" from pile \" + A + \" to pile \" + B);\nHanoi( n-1, C, B, A );\n}\n}\nWe can use induction to prove that this subroutine does in fact solve the\nTowers of Hanoi problem.\nTheorem 1.15. The sequence of moves printed by the Hanoi subroutine\nas given above correctly solves the Towers of Hanoi problem for any integer\nn 1.\n\u2265\nProof. We prove by induction that whenever n is a positive integer and A,\nB, and C are the numbers 1, 2, and 3 in some order, the subroutine call\nHanoi(n,A,B,C) prints a sequence of moves that will move n disks from\npile A to pile B, following all the rules of the Towers of Hanoi problem.\nInthebasecase,n=1,thesubroutinecallHanoi(1,A,B,C)printsout\nthe single step \u201cMove disk 1 from pile A to pile B,\u201d and this move does\nsolve the problem for 1 disk.\nLetkbeanarbitrarypositiveinteger,andsupposethatHanoi(k,A,B,C)\ncorrectly solves the problem of moving the k disks from pile A to pile B\nusing pile C as the spare, whenever A, B, and C are the numbers 1, 2,\nand 3 in some order. We need toshow that Hanoi(k+1,A,B,C) correctly\nsolves the problem for k+1 disks. Since k+1 > 1, Hanoi(k+1,A,B,C)\nbegins by calling Hanoi(k,A,C,B). By the induction hypothesis, this cor-\nrectlymovesk disksfrompileAtopileC. Disknumberk+1isnotmoved\nduringthisprocess. Atthatpoint, pileC containsthek smallestdisksand\npile A still contains the (k+1)st disk, which has not yet been moved. So\nthe next move printed by the subroutine, \u201cMove disk (k+1) from pile A\nto pile B,\u201d is legal because pile B is empty. Finally, the subroutine calls\nHanoi(k,C,B,A), which, by the induction hypothesis, correctly moves the\nk smallest disks from pile C to pile B, putting them on top of the (k+1)st\ndisk, which does not move during this process. At that point, all (k+1)\ndisks are on pile B, so the problem for k + 1 disks has been correctly\nsolved.\nRecursion is often used with linked data structures, which are data\nstructures that are constructed by linking several objects of the same type\ntogetherwithpointers. (Ifyoudon\u2019talreadyknowaboutobjectsandpoint-\ners, you will not be able to follow the rest of this section.) For an example,\nwe\u2019ll look at the data structure known as a binary tree. A binary tree\nconsists of nodes linked together in a tree-like structure. The nodes can\ncontain any type of data, but we will consider binary trees in which each 72 CHAPTER 1. LOGIC AND PROOF\nnodecontainsaninteger. Abinarytreecanbeempty, oritcanconsistofa\nnode (called the root of the tree) and two smaller binary trees (called the\nleft subtree and the right subtree of the tree). You can already see the\nrecursive structure: A tree can contain smaller trees. In Java, the nodes of\na tree can be represented by objects belonging to the class\nclass BinaryTreeNode {\nint item; \/\/ An integer value stored in the node.\nBinaryTreeNode left; \/\/ Pointer to left subtree.\nBinaryTreeNode right; \/\/ Pointer to right subtree.\n}\nAnemptytreeisrepresentedbyapointerthathasthespecialvaluenull. If\nroot isapointer totheroot nodeof atree, thenroot.left isapointer tothe\nleftsubtreeandroot.right isapointertotherightsubtree. Ofcourse, both\nroot.left and root.right can be null if the corresponding subtree is empty.\nSimilarly, root.item is a name for the integer in the root node.\nLet\u2019s say that we want a function that will find the sum of all the\nintegers in all the nodes of a binary tree. We can do this with a simple\nrecursive function. The base case of the recursion is an empty tree. Since\nthere are no integers in an empty tree, the sum of the integers in an empty\ntree is zero. For a non-empty tree, we can use recursion to find the sums\nof the integers in the left and right subtrees, and then add those sums to\nthe integer in the root node of the tree. In Java, this can be expressed as\nfollows:\nint TreeSum( BinaryTreeNode root ) {\n\/\/ Find the sum of all the integers in the\n\/\/ tree that has the given root.\nint answer;\nif ( root == null ) { \/\/ The tree is empty.\nanswer = 0;\n}\nelse {\nanswer = TreeSum( root.left );\nanswer = answer + TreeSum( root.right );\nanswer = answer + root.item;\n}\nreturn answer;\n}\nWe can use the second form of the principle of mathematical induction to\nprove that this function is correct. 1.9. APPLICATION: RECURSION AND INDUCTION 73\nTheorem 1.16. The function TreeSum, defined above, correctly computes\nthe sum of all the integers in a binary tree.\nProof. We use induction on the number of nodes in the tree. Let P(n) be\nthe statement \u201cTreeSum correctly computes the sum of the nodes in any\nbinary tree that contains exactly n nodes.\u201d We show that P(n) is true for\nevery natural number n.\nConsiderthecasen=0. Atreewithzeronodesisempty,andanempty\ntree is represented by a null pointer. In this case, the if statement in the\ndefinition of TreeSum assigns the value 0 to the answer, and this is the\ncorrect sum for an empty tree. So, P(0) is true.\nLet k be an arbitrary natural number, with k >0. Suppose we already\nknow P(x) for each natural number x with 0 x < k. That is, TreeSum\n\u2264\ncorrectly computes the sum of all the integers in any tree that has fewer\nthan k nodes. We must show that it follows that P(k) is true, that is, that\nTreeSum works for a tree with k nodes. Suppose that root is a pointer to\nthe root node of a tree that has a total of k nodes. Since the root node\ncounts as a node, that leaves a total of k 1 nodes for the left and right\n\u2212\nsubtrees,soeachsubtreemustcontainfewerthanknodes. Bytheinduction\nhypothesis, we know that TreeSum(root.left) correctly computes the sum\nof all the integers in the left subtree, and TreeSum(root.right) correctly\ncomputes the sum of all the integers in the right subtree. The sum of all\nthe integers in the tree is root.item plus the sums of the integers in the\nsubtrees, and this is the value computed by TreeSum. So, TreeSum does\nwork for a tree with k nodes. This completes the induction.\nNote how closely the structure of the inductive proof follows the struc-\nture of the recursive function. In particular, the second principle of math-\nematical induction is very natural here, since the size of subtree could be\nanythinguptoonelessthanthesizeofthecompletetree. Itwouldbevery\ndifficulttousethefirstprincipleofinductioninaproofaboutbinarytrees.\nExercises\n1. The Hanoi subroutine given in this section does not just solve the Towers of\nHanoiproblem. Itsolvestheproblemusingtheminimumpossiblenumberof\nmoves. Use induction to prove this fact.\n2. UseinductiontoprovethattheHanoi subroutineuses2n 1movestosolve\n\u2212\nthe Towers of Hanoi problem for n disks. (There is a story that goes along\nwith the Towers of Hanoi problem. It is said that on the day the world was\ncreated, a group of monks in Hanoi were set the task of solving the problem\nfor 64 disks. They can move just onedisk each day. On the day the problem 74 CHAPTER 1. LOGIC AND PROOF\nis solved, the world will end. However, we shouldn\u2019t worry too much, since\n264 1 days is a very long time\u2014about 50 million billion years.)\n\u2212\n3. Consider the following recursive function:\nint power( int x, int n ) {\n\/\/ Compute x raised to the power n.\n\/\/ Assume that n >= 0.\nint answer;\nif ( n == 0 ) {\nanswer = 1;\n}\nelse if (n % 2 == 0) {\nanswer = power( x * x, n \/ 2);\n}\nelse {\nanswer = x * power( x * x, (n-1) \/ 2);\n}\nreturn answer;\n}\nShow that for any integer x and any non-negative integer n, the function\npower(x,n) correctly computes the value of xn. (Assume that the int data\ntype can represent arbitrarily large integers.) Note that the test \u201cif (n % 2\n== 0)\u201d tests whether n is evenly divisible by 2. That is, the test is true if n\nisanevennumber. (Thisfunctionisactuallyaveryefficientwaytocompute\nxn.)\n4. A leaf node in a binary tree is a node in which both the left and the right\nsubtrees are empty. Prove that the following recursive function correctly\ncounts the number of leaves in a binary tree:\nint LeafCount( BinaryTreeNode root ) {\n\/\/ Counts the number of leaf nodes in\n\/\/ the tree with the specified root.\nint count;\nif ( root == null ) {\ncount = 0;\n}\nelse if ( root.left == null && root.right == null ) {\ncount = 1;\n}\nelse {\ncount = LeafCount( root.left );\ncount = count + LeafCount( root.right );\n}\nreturn count;\n} 1.10. RECURSIVE DEFINITIONS 75\n5. A binary sort tree satisfies the following property: If node is a pointer to\nany node in the tree, then all the integers in the left subtree of node are less\nthan node.item and all the integers in the right subtree of node are greater\nthan or equal to node.item. Prove that the following recursive subroutine\nprints all the integers in a binary sort tree in non-decreasing order:\nvoid SortPrint( BinaryTreeNode root ) {\n\/\/ Assume that root is a pointer to the\n\/\/ root node of a binary sort tree. This\n\/\/ subroutine prints the integers in the\n\/\/ tree in non-decreasing order.\nif ( root == null ) {\n\/\/ There is nothing to print.\n}\nelse {\nSortPrint( root.left );\nSystem.out.println( root.item );\nSortPrint( root.right );\n}\n}\n1.10 Recursive Definitions\nRecursion occurs in programming when a subroutine is defined\u2014partially,\nat least\u2014in terms of itself. But recursion also occurs outside of program-\nming. A recursive definition is a definition that includes a reference to\nthe term that is being defined. A recursive definition defines something at\nleast partially in terms of itself. As in the case of recursive subroutines,\nmathematical induction can often be used to prove facts about things that\nare defined recursively.\nAsalreadynoted,thereisarecursivedefinitionforn!,forninN. Wecan\ndefine0!=1andn!=n (n 1)!forn>0. Othersequencesofnumberscan\n\u00b7 \u2212\nalsobedefinedrecursively. Forexample,thefamousFibonacci sequence\nis the sequence of numbers f , f , f , ..., defined recursively by\n0 1 2\nf =0\n0\nf =1\n1\nf =f +f for n>1\nn n 1 n 2\n\u2212 \u2212 76 CHAPTER 1. LOGIC AND PROOF\nUsing this definition, we compute that\nf =f +f =0+1=1\n2 1 0\nf =f +f =1+1=2\n3 2 1\nf =f +f =2+1=3\n4 3 2\nf =f +f =3+2=5\n5 4 3\nf =f +f =5+3=8\n6 5 4\nf =f +f =8+5=13\n7 6 5\nand so on. Based on this definition, we can use induction to prove facts\nabout the Fibonacci sequence. We can prove, for example, that f grows\nn\nexponentially with n, even without finding an exact formula for f :\nn\nTheorem 1.17. The Fibonacci sequence, f , f , f , ..., satisfies f >\n0 1 2 n\n3 n \u22121 , for n 6.\n2 \u2265\n(cid:0)Pro(cid:1)of. We prove this by induction on n. For n = 6, we have that f = 8\nn\nwhile 1.5n 1 = 1.55, which is about 7.6. So f > 1.5n 1 for n = 6.\n\u2212 n \u2212\nSimilarly, for n = 7, we have f = 13 and 1.5n 1 = 1.56, which is about\nn \u2212\n11.4. So f >1.5n 1 for n=7.\nn \u2212\nNow suppose that k is an arbitrary integer with k > 7. Suppose that\nwe already know that f > 1.5n 1 for n = k 1 and for n = k 2. We\nn \u2212\n\u2212 \u2212\nwant to show that the inequality then holds for n=k as well. But\nf =f +f\nk k 1 k 2\n\u2212 \u2212\n>1.5(k 1) 1+1.5(k 2) 1 (by the induction hypothesis)\n\u2212 \u2212 \u2212 \u2212\n=1.5k 2+1.5k 3\n\u2212 \u2212\n=(1.5) (1.5k 3)+(1.5k 3)\n\u2212 \u2212\n\u00b7\n=(2.5) (1.5k 3)\n\u2212\n\u00b7\n>(1.52) (1.5k 3) (since 1.52 =2.25)\n\u2212\n\u00b7\n=1.5k 1\n\u2212\nThis string of equalities and inequalities shows that f > 1.5k 1. This\nk \u2212\ncompletes the induction and proves the theorem.\nExercises\n1. Prove that the Fibonacci sequence, f0, f1, f2, ..., satisfies f n < 2n for all\nnatural numbers n. 1.10. RECURSIVE DEFINITIONS 77\n2. Supposethata1,a2,a3,...,isasequenceofnumberswhichisdefinedrecur-\nsively by a1 = 1 and a n = 2a n\u22121+2n\u22121 for n > 1. Prove that a n = n2n\u22121\nfor every positive integer n. 78 CHAPTER 1. LOGIC AND PROOF Chapter 2\nSets, Functions, and\nRelations\nW\ne deal with the complexity of the world by putting things into\ncategories. There are not just hordes of individual creatures. There are\ndogs, cats, elephants, and mice. There are mammals, insects, and fish.\nAnimals, vegetables, and minerals. Solids, liquids, and gases. Things that\nare red. Big cities. Pleasant memories.... Categories build on categories.\nThey are the subject and the substance of thought.\nIn mathematics, which operates in its own abstract and rigorous world,\ncategoriesaremodeledbysets. Asetisjustacollectionofelements. Along\nwith logic, sets form the \u201cfoundation\u201d of mathematics, just as categories\narepartofthefoundationofday-to-daythought. Inthischapter, westudy\nsets and relationships among sets.\n2.1 Basic Concepts\nAset isacollection ofelements. Asetisdefinedentirelybytheelements\nthat it contains. An element can be anything, including another set. You\nwill notice that this is not a precise mathematical definition. Instead, it is\nan intuitive description of what the word \u201cset\u201d is supposed to mean: Any\ntimeyouhaveabunchofentitiesandyouconsiderthemasaunit,youhave\naset. Mathematically, setsarereallydefinedbytheoperationsthatcanbe\nperformed on them. These operations model things that can be done with\ncollections of objects in the real world. These operations are the subject of\nthe branch of mathematics known as set theory.\nThemostbasicoperationinsettheoryisformingasetfromagivenlist\n79 80 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS\nofspecificentities. Thesetthatisformedinthiswayisdenotedbyenclosing\nthe list of entities between a left brace, \u201c \u201d, and a right brace, \u201c \u201d. The\n{ }\nentities in the list are separated by commas. For example, the set denoted\nby\n17, \u03c0, New York City, Barack Obama, Big Ben\n{ }\nis the set that contains the entities 17, \u03c0, New York City, Barack Obama,\nand Big Ben. These entities are the elements of the set. Since we assume\nthat a set is completely defined by the elements that it contains, the set\nis well-defined. Of course, we still haven\u2019t said what it means to be an\n\u201centity.\u201d Something as definite as \u201cNew York City\u201d should qualify, except\nthat it doesn\u2019t seem like New York City really belongs in the world of\nMathematics. The problem is that mathematics is supposed to be its own\nself-contained world, but it is supposed to model the real world. When we\nuse mathematics to model the real world, we admit entities such as New\nYork City and even Big Ben. But when we are doing mathematics per se,\nwe\u2019ll generally stick to obviously mathematical entities such as the integer\n17 or the real number \u03c0. We will also use letters such as a and bto refer to\nentities. Forexample,whenIsaysomethinglike\u201cLetAbetheset a,b,c ,\u201d\n{ }\nI mean a, b, and c to be particular, but unspecified, entities.\nIt\u2019s important to understand that a set is defined by the elements that\nit contains, and not by the order in which those elements might be listed.\nFor example, the notations a,b,c,d and b,c,a,d define the same set.\n{ } { }\nFurthermore, a set can only contain one copy of a given element, even\nif the notation that specifies the set lists the element twice. This means\nthat a,b,a,a,b,c,a and a,b,c specify exactly the same set. Note in\n{ } { }\nparticular that it\u2019s incorrect to say that the set a,b,a,a,b,c,a contains\n{ }\nseven elements, since some of the elements in the list are identical. The\nnotation a,b,c can lead to some confusion, since it might not be clear\n{ }\nwhetherthelettersa,b,andcareassumedtorefertothreedifferent entities.\nAmathematicianwouldgenerallynot makethisassumptionwithoutstating\nitexplicitly,sothatthesetdenotedby a,b,c couldactuallycontaineither\n{ }\none,two,orthreeelements. Whenitisimportantthatdifferentlettersrefer\nto different entities, I will say so explicitly, as in \u201cConsider the set a,b,c ,\n{ }\nwhere a, b, and c are distinct.\u201d\nThe symbol is used to express the relation \u201cis an element of.\u201d That\n\u2208\nis, if a is an entity and A is a set, then a A is a statement that is true\n\u2208\nif and only if a is one of the elements of A. In that case, we also say that\na is a member of the set A. The assertion that a is not an element of A\nis expressed by the notation a A. Note that both a A and a A are\n6\u2208 \u2208 6\u2208\nstatements in the sense of propositional logic. That is, they are assertions\nwhich can be either true or false. The statement a A is equivalent to\n6\u2208 2.1. BASIC CONCEPTS 81\n(a A).\n\u00ac \u2208\nIt is possible for a set to be empty, that is, to contain no elements\nwhatsoever. Since a set is completely determined by the elements that it\ncontains, there is only one set that contains no elements. This set is called\nthe empty set, and it is denoted by the symbol . Note that for any\n\u2205\nelement a, the statement a is false. The empty set, , can also be\n\u2208 \u2205 \u2205\ndenoted by an empty pair of braces, .\n{ }\nIf A and B are sets, then, by definition, A is equal to B if and only\nif they contain exactly the same elements. In this case, we write A = B.\nUsing the notation of predicate logic, we can say that A=B if and only if\nx(x A x B).\n\u2200 \u2208 \u2194 \u2208\nSuppose now that A and B are sets such that every element of A is an\nelement of B. In that case, we say that A is a subset of B, i.e. A is a\nsubset of B if and only if x(x A x B). The fact that A is a subset\n\u2200 \u2208 \u2192 \u2208\nof B is denoted by A B. Note that is a subset of every set B: x is\n\u2286 \u2205 \u2208\u2205\nfalse for any x, and so given any B, (x x B) is true for all x.\n\u2208\u2205\u2192 \u2208\nIf A = B, then it is automatically true that A B and that B A.\n\u2286 \u2286\nThe converse is also true: If A B and B A, then A=B. This follows\n\u2286 \u2286\nfrom the fact that for any x, the statement (x A x B) is logically\n\u2208 \u2194 \u2208\nequivalent to the statement (x A x B) (x B x A). This\n\u2208 \u2192 \u2208 \u2227 \u2208 \u2192 \u2208\nfact is important enough to state as a theorem.\nTheorem 2.1. Let A and B be sets. Then A = B if and only if both\nA B and B A.\n\u2286 \u2286\nThis theorem expresses the following advice: If you want to check that\ntwo sets, A and B, are equal, you can do so in two steps. First check\nthat every element of A is also an element of B, and then check that every\nelement of B is also an element of A.\nIf A B but A = B, we say that A is a proper subset of B. We\n\u2286 6\nuse the notation A B to mean that A is a proper subset of B. That is,\nA B if and only if A B A=B. We will sometimes use A B as an\n\u2286 \u2227 6 \u2287\nequivalent notation for B A, and A!B as an equivalent for B A.\n\u2286\nA set can contain an infinite number of elements. In such a case, it is\nnot possible to list all the elements in the set. Sometimes the ellipsis \u201c...\u201d\nis used to indicate a list that continues on infinitely. For example, N, the\nset of natural numbers, can be specified as\nN= 0,1,2,3,...\n{ }\nHowever, this is an informal notation, which is not really well-defined, and\nit should only be used in cases where it is clear what it means. It\u2019s not 82 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS\nvery useful to say that \u201cthe set of prime numbers is 2,3,5,7,11,13,... ,\u201d\n{ }\nand it is completely meaningless to talk about \u201cthe set 17,42,105,... .\u201d\n{ }\nClearly, we need another way to specify sets besides listing their elements.\nThe need is fulfilled by predicates.\nIfP(x)isapredicate,thenwecanformthesetthatcontainsallentities\na such that a is in the domain of discourse for P and P(a) is true. The\nnotation x P(x) is used to denote this set. The name of the variable,\n{ | }\nx, is arbitrary, so the same set could equally well be denoted as z P(z)\n{ | }\nor r P(r) . The notation x P(x) can be read \u201cthe set of x such that\n{ | } { | }\nP(x).\u201d For example, if E(x) is the predicate \u201cx is an even number,\u201d and\nif the domain of discourse for E is the set N of natural numbers, then the\nnotation x E(x) specifies the set of even natural numbers. That is,\n{ | }\nx E(x) = 0,2,4,6,8,...\n{ | } { }\nItturnsout,fordeepandsurprisingreasonsthatwewilldiscusslaterinthis\nsection,thatwehavetobealittlecarefulaboutwhatcountsasapredicate.\nInorderforthenotation x P(x) tobevalid, wehavetoassumethatthe\n{ | }\ndomain of discourse of P is in fact a set. (You might wonder how it could\nbe anything else. That\u2019s the surprise!) Often, it is useful to specify the\ndomain of discourse explicitly in the notation that defines a set. In the\naboveexample,tomakeitclearthatxmustbeanaturalnumber,wecould\nwrite thesetas x N E(x) . Thisnotation canbereadas\u201cthe setof all\n{ \u2208 | }\nx in N such that E(x).\u201d More generally, if X is a set and P is a predicate\nwhosedomainofdiscourseincludesalltheelementsofX,thenthenotation\nx X P(x)\n{ \u2208 | }\nis the set that consists of all entities a that are members of the set X and\nfor which P(a) is true. In this notation, we don\u2019t have to assume that\nthe domain of discourse for P is a set, since we are effectively limiting the\ndomainofdiscoursetothesetX. Thesetdenotedby x X P(x) could\n{ \u2208 | }\nalso be written as x x X P(x) .\n{ | \u2208 \u2227 }\nWecanusethisnotationtodefinethesetofprimenumbersinarigorous\nway. A prime number is a natural number n which is greater than 1 and\nwhich satisfies the property that for any factorization n = xy, where x\nand y are natural numbers, either x or y must be n. We can express this\ndefinition as a predicate and define the set of prime numbers as\nn N (n>1) x y (x N y N n=xy) (x=n y =n) .\n{ \u2208 | \u2227\u2200 \u2200 \u2208 \u2227 \u2208 \u2227 \u2192 \u2228 }\nAdmittedly, thisdefinitio(cid:0)nishardtotakeininonegulp. Butthisexam(cid:1)ple\nshows that it is possible to define complex sets using predicates. 2.1. BASIC CONCEPTS 83\nNow that we have a way to express a wide variety of sets, we turn to\noperations that can be performed on sets. The most basic operations on\nsets are union and intersection. If A and B are sets, then we define the\nunionofAandB tobethesetthatcontainsalltheelementsofAtogether\nwith all the elements of B. The union of A and B is denoted by A B.\n\u222a\nThe union can be defined formally as\nA B = x x A x B .\n\u222a { | \u2208 \u2228 \u2208 }\nThe intersection of A and B is defined to be the set that contains every\nentity that is both a member of A and a member of B. The intersection of\nA and B is denoted by A B. Formally,\n\u2229\nA B = x x A x B .\n\u2229 { | \u2208 \u2227 \u2208 }\nAn entity gets into A B if it is in either A or B. It gets into A B if it\n\u222a \u2229\nis in both A and B. Note that the symbol for the logical \u201cor\u201d operator, ,\n\u2228\nis similar to the symbol for the union operator, , while the logical \u201cand\u201d\n\u222a\noperator, , is similar to the intersection operator, .\n\u2227 \u2229\nThe set difference of two sets, A and B, is defined to be the set of\nall entities that are members of A but are not members of B. The set\ndifference of A and B is denoted ArB. The idea is that ArB is formed\nby starting with A and then removing any element that is also found in B.\nFormally,\nArB = x x A x B .\n{ | \u2208 \u2227 6\u2208 }\nUnion and intersection are clearly commutative operations. That is, A\n\u222a\nB =B AandA B =B AforanysetsAandB. However,setdifference\n\u222a \u2229 \u2229\nis not commutative. In general, ArB =BrA.\n6\nSuppose that A = a,b,c , that B = b,d , and that C = d,e,f .\n{ } { } { }\nThen we can apply the definitions of union, intersection, and set difference\nto compute, for example, that:\nA B = a,b,c,d A B = b ArB = a,c\n\u222a { } \u2229 { } { }\nA C = a,b,c,d,e,f A C = ArC = a,b,c\n\u222a { } \u2229 \u2205 { }\nIn this example, the sets A and C have no elements in common, so that\nA C = . There is a term for this: Two sets are said to be disjoint if\n\u2229 \u2205\nthey have no elements in common. That is, for any sets A and B, A and\nB are said to be disjoint if and only if A B = .\n\u2229 \u2205\nOfcourse,thesetoperationscanalsobeappliedtosetsthataredefined\nby predicates. For example, let L(x) be the predicate \u201cx is lucky,\u201d and let\nW(x) be the predicate \u201cx is wise,\u201d where the domain of discourse for each 84 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS\nNotation Definition\na A a is a member (or element) of A\n\u2208\na A (a A), a is not a member of A\n6\u2208 \u00ac \u2208\nthe empty set, which contains no elements\n\u2205\nA B A is a subset of B, x(x A x B)\n\u2286 \u2200 \u2208 \u2192 \u2208\nA B A is a proper subset of B, A B A=B\n\u2286 \u2227 6\nA B A is a superset of B, same as B A\n\u2287 \u2286\nA!B A is a proper superset of B, same as B !A\nA=B A and B have the same members, A B B A\n\u2286 \u2227 \u2286\nA B union of A and B, x x A x B\n\u222a { | \u2208 \u2228 \u2208 }\nA B intersection of A and B, x x A x B\n\u2229 { | \u2208 \u2227 \u2208 }\nArB set difference of A and B, x x A x B\n{ | \u2208 \u2227 6\u2208 }\nP(A) power set of A, X X A\n{ | \u2286 }\nFigure 2.1: Some of the notations that are defined in this section.\nA and B are sets, and a is an entity.\npredicate is the set of people. Let X = x L(x) , and let Y = x W(x) .\n{ | } { | }\nThen\nX Y = x L(x) W(x) = people who are lucky or wise\n\u222a { | \u2228 } { }\nX Y = x L(x) W(x) = people who are lucky and wise\n\u2229 { | \u2227 } { }\nX rY = x L(x) W(x) = people who are lucky but not wise\n{ | \u2227\u00ac } { }\nY rX = x W(x) L(x) = people who are wise but not lucky\n{ | \u2227\u00ac } { }\nYou have to be a little careful with the English word \u201cand.\u201d We might\nsay that the set X Y contains people who are lucky and people who are\n\u222a\nwise. But what this means is that a person gets into the set X Y either\n\u222a\nby being lucky or by being wise, so X Y is defined using the logical \u201cor\u201d\n\u222a\noperator, .\n\u2228\nSets can contain other sets as elements. For example, the notation\na, b defines a set that contains two elements, the entity a and the\n{ { }}\nset b . Since the set b is a member of the set a, b , we have that\n{ } { } { { }}\nb a, b . On the other hand, provided that a = b, the statement\n{ } \u2208 { { }} 6\nb a, b is false, since saying b a, b is equivalent to saying\n{ } \u2286 { { }} { } \u2286 { { }} 2.1. BASIC CONCEPTS 85\nthatb a, b ,andtheentitybisnotoneofthetwomembersof a, b .\n\u2208{ { }} { { }}\nFor the entity a, it is true that a a, b .\n{ }\u2286{ { }}\nGiven a set A, we can construct the set that contains all the subsets of\nA. This set is called the power set of A, and is denoted P(A). Formally,\nwe define\nP(A)= X X A .\n{ | \u2286 }\nFor example, if A = a,b , then the subsets of A are the empty set, a ,\n{ } { }\nb , and a,b , so the power set of A is set given by\n{ } { }\nP(A)= , a , b , a,b .\n{\u2205 { } { } { }}\nNote that since the empty set is a subset of any set, the empty set is an\nelement of the power set of any set. That is, for any set A, A and\nP(A). Since the empty set is a subset of itself, and is its o\u2205 nl\u2286 y subset,\nw\u2205 e\u2208 havethatP( )= . Theset isnotempty. Itcontainsoneelement,\n\u2205 {\u2205} {\u2205}\nnamely .\n\u2205\nWe remarked earlier in this section that the notation x P(x) is only\n{ | }\nvalidifthedomainofdiscourseofP isaset. Thismightseemaratherpuz-\nzling thing to say\u2014after all, why and how would the domain of discourse\nbe anything else? The answer is related to Russell\u2019s Paradox, which we\nmentioned briefly in Chapter 1 and which shows that it is logically impos-\nsible for the set of all sets to exist. This impossibility can be demonstrated\nusing a proof by contradiction. In the proof, we use the existence of the\nset of all sets to define another set which cannot exist because its existence\nwould lead to a logical contradiction.\nTheorem 2.2. There is no set of all sets.\nProof. Suppose that the set of all sets exists. We will show that this as-\nsumption leads to a contradiction. Let V be the set of all sets. We can\nthen define the set R to be the set which contains every set that does not\ncontain itself. That is,\nR= X V X X\n{ \u2208 | 6\u2208 }\nNow, we must have either R R or R R. We will show that either case\n\u2208 6\u2208\nleads to a contradiction.\nConsider the case where R R. Since R R, R must satisfy the\n\u2208 \u2208\ncondition for membership in R. A set X is in R iff X X. To say that\n6\u2208\nR satisfies this condition means that R R. That is, from the fact that\n6\u2208\nR R, we deduce the contradiction that R R.\n\u2208 6\u2208 86 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS\nNow consider the remaining case, where R R. Since R R, R does\n6\u2208 6\u2208\nnot satisfy the condition for membership in R. Since the condition for\nmembershipisthatR R,andthisconditionisfalse,thestatementR R\n6\u2208 6\u2208\nmust be false. But this means that the statement R R is true. From the\n\u2208\nfact that R R, we deduce the contradiction that R R.\n6\u2208 \u2208\nSince both possible cases, R R and R R, lead to contradictions, we\n\u2208 6\u2208\nsee that it is not possible for R to exist. Since the existence of R follows\nfrom the existence of V, we see that V also cannot exist.\nToavoidRussell\u2019sparadox,wemustputlimitationsontheconstruction\nofnewsets. Wecan\u2019tforcethesetofallsetsintoexistencesimplybythink-\ningofit. Wecan\u2019tformtheset x P(x) unlessthedomainofdiscourseof\n{ | }\nP is a set. Any predicate Q can be used to form a set x X Q(x) , but\n{ \u2208 | }\nthis notation requires a pre-existing set X. Predicates can be used to form\nsubsetsofexistingsets,buttheycan\u2019tbeusedtoformnewsetscompletely\nfrom scratch.\nThe notation x A P(x) is a convenient way to effectively limit the\n{ \u2208 | }\ndomain of discourse of a predicate, P, to members of a set, A, that we are\nactually interested in. We will use a similar notation with the quantifiers\n\u2200\nand . The proposition ( x A)(P(x)) is true if and only if P(a) is true\n\u2203 \u2200 \u2208\nfor every element a of the set A. And the proposition ( x A)(P(x)) is\n\u2203 \u2208\ntrue if and only if there is some element a of the set A for which P(a) is\ntrue. These notations are valid only when A is contained in the domain\nof discourse for P. As usual, we can leave out parentheses when doing so\nintroduces no ambiguity. So, for example, we might write x AP(x).\n\u2200 \u2208\nWeendthissectionwithproofsofthetwoformsoftheprincipleofmath-\nematical induction. These proofs were omitted from the previous chapter,\nbutonlyforthelack ofabitofsetnotation. Infact, theprincipleofmath-\nematical induction is valid only because it follows from one of the basic\naxioms that define the natural numbers, namely the fact that any non-\nempty set of natural numbers has a smallest element. Given this axiom,\nwe can use it to prove the following two theorems:\nTheorem 2.3. Let P be a one-place predicate whose domain of discourse\nincludes the natural numbers. Suppose that P(0) k N(P(k) P(k+\n\u2227 \u2200 \u2208 \u2192\n1)) . Then n N, P(n).\n\u2200 \u2208 (cid:0)\n(cid:1)\nProof. Suppose that both P(0) and k N(P(k) P(k+1)) are true,\n\u2200 \u2208 \u2192\nbut that n N, P(n) is false. We show that this assumption leads to a\n\u2200 \u2208 (cid:0) (cid:1)\ncontradiction.\n(cid:0) (cid:1) 2.1. BASIC CONCEPTS 87\nSincethestatement n N, P(n)isfalse,itsnegation, ( n N, P(n)),\n\u2200 \u2208 \u00ac \u2200 \u2208\nis true. The negation is equivalent to n N, P(n). Let X = n\n\u2203 \u2208 \u00ac { \u2208\nN P(n) . Since n N, P(n) is true, we know that X is not empty.\n|\u00ac } \u2203 \u2208 \u00ac\nSince X is a non-empty set of natural numbers, it has a smallest element.\nLet x be the smallest element of X. That is, x is the smallest natural\nnumber such that P(x) is false. Since we know that P(0) is true, x cannot\nbe 0. Let y = x 1. Since x = 0, y is a natural number. Since y < x,\n\u2212 6\nwe know, by the definition of x, that P(y) is true. We also know that\nk N(P(k) P(k+1)) is true. In particular, taking k = y, we know\n\u2200 \u2208 \u2192\nthat P(y) P(y +1). Since P(y) and P(y) P(y +1), we deduce by\n\u2192 \u2192\nmodusponens thatP(y+1)istrue. Buty+1=x,sowehavededucedthat\nP(x)istrue. ThiscontradictsthefactthatP(x)isfalse. Thiscontradiction\nproves the theorem.\nTheorem 2.4. Let P be a one-place predicate whose domain of discourse\nincludes the natural numbers. Suppose that P(0) is true and that\n(P(0) P(1) P(k)) P(k+1)\n\u2227 \u2227\u00b7\u00b7\u00b7\u2227 \u2192\nis true for each natural number k 0. Then it is true that n N, P(n).\n\u2265 \u2200 \u2208\nProof. Suppose that P is a predicate that satisfies the hypotheses of the\ntheorem, and suppose that the statement n N, P(n) is false. We show\n\u2200 \u2208\nthat this assumption leads to a contradiction.\nLet X = n N P(n) . Because of the assumption that n\n{ \u2208 |\u00ac } \u2200 \u2208\nN, P(n) is false, X is non-empty. It follows that X has a smallest element.\nLetxbethesmallestelementofX. TheassumptionthatP(0)istruemeans\nthat 0 X, sowe must have x>0. Since xis the smallest natural number\n6\u2208\nfor which P(x) is false, we know that P(0), P(1), ..., and P(x 1) are all\n\u2212\ntrue. Fromthisandthefactthat(P(0) P(1) P(x 1)) P(x),we\n\u2227 \u2227\u00b7\u00b7\u00b7\u2227 \u2212 \u2192\ndeduce that P(x) is true. But this contradicts the fact that P(x) is false.\nThis contradiction proves the theorem.\nExercises\n1. If we don\u2019t make the assumption that a, b, and c are distinct, then the set\ndenoted by a,b,c might actually contain either 1, 2, or 3 elements. How\n{ }\nmany different elements might the set a, b, a , a,c , a,b,c contain?\n{ { } { } { }}\nExplain your answer.\n2. Compute A B, A B, and ArB for each of the following pairs of sets\n\u222a \u2229\na) A= a,b,c , B =\n{ } \u2205\nb) A= 1,2,3,4,5 , B = 2,4,6,8,10\n{ } { } 88 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS\nc) A= a,b , B = a,b,c,d\n{ } { }\nd) A= a,b, a,b , B = a , a,b\n{ { }} {{ } { }}\n3. RecallthatNrepresentsthesetofnaturalnumbers. Thatis,N= 0,1,2,3,... .\n{ }\nLet X = n N n 5 , let Y = n N n 10 , and let Z = n\n{ \u2208 | \u2265 } { \u2208 | \u2264 } { \u2208\nN n is an even number . Find each of the following sets:\n| }\na) X Y b) X Y c) XrY d) NrZ\n\u2229 \u222a\ne) X Z f) Y Z g) Y Z h) ZrN\n\u2229 \u2229 \u222a\n4. Find P 1,2,3 . (It has eight elements.)\n{ }\n5. Assume(cid:0)thataa(cid:1)ndbareentitiesandthata=b. LetAandB bethesetsde-\n6\nfined by A= a, b , a,b and B = a, b, a, b . Determine whether\n{ { } { }} { { { }}}\neach of the following statements is true or false. Explain your answers.\na) b A b) a,b A c) a,b B\n\u2208 { }\u2286 { }\u2286\nd) a,b B e) a, b A f) a, b B\n{ }\u2208 { { }}\u2208 { { }}\u2208\n6. SinceP(A)isaset,itispossibletoformthesetP P(A) . WhatisP P( ) ?\nWhat is P P( a,b ) ? (It has sixteen elements.) (cid:0) (cid:1) (cid:0) \u2205 (cid:1)\n{ }\n7. In the Eng(cid:0)lish sente(cid:1)nce, \u201cShe likes men who are tall, dark, and handsome,\u201d\ndoes she like an intersection or a union of sets of men? How about in the\nsentence, \u201cShe likes men who are tall, men who are dark, and men who are\nhandsome?\u201d\n8. IfAisanyset,whatcanyousayaboutA A? AboutA A? AboutArA?\n\u222a \u2229\nWhy?\n9. Suppose that A and B are sets such that A B. What can you say about\n\u2286\nA B? About A B? About ArB? Why?\n\u222a \u2229\n10. Suppose that A, B, and C are sets. Show that C A B if and only if\n\u2286 \u2229\n(C A) (C B).\n\u2286 \u2227 \u2286\n11. Suppose that A, B, and C are sets, and that A B and B C. Show that\n\u2286 \u2286\nA C.\n\u2286\n12. Suppose that A and B are sets such that A B. Is it necessarily true that\nP(A) P(B)? Why or why not? \u2286\n\u2286\n13. Let M be any natural number, and let P(n) be a predicate whose domain of\ndiscourse includes all natural numbers greater than or equal to M. Suppose\nthat P(M) is true, and suppose that P(k) P(k+1) for all k M. Show\n\u2192 \u2265\nthat P(n) is true for all n M.\n\u2265\n2.2 The Boolean Algebra of Sets\nIt is clear that set theory is closely related to logic. The intersection and\nunion of sets can be defined in terms of the logical \u201cand\u201d and logical \u201cor\u201d\noperators. The notation x P(x) makes it possible to use predicates to\n{ | }\nspecifysets. AndifAisanyset,thentheformulax Adefinesaoneplace\n\u2208\npredicate that is true for an entity x if and only if x is a member of A. So 2.2. THE BOOLEAN ALGEBRA OF SETS 89\nit should not be a surprise that many of the rules of logic have analogs in\nset theory.\nFor example, we have already noted that and are commutative\n\u222a \u2229\noperations. This fact can be verified using the rules of logic. Let A and B\nbe sets. According to the definition of equality of sets, we can show that\nA B =B A by showing that x (x A B) (x B A) . But for\n\u222a \u222a \u2200 \u2208 \u222a \u2194 \u2208 \u222a\nany x,\n(cid:0) (cid:1)\nx A B x A x B (definition of )\n\u2208 \u222a \u2194 \u2208 \u2228 \u2208 \u222a\nx B x A (commutativity of )\n\u2194 \u2208 \u2228 \u2208 \u2228\nx B A (definition of )\n\u2194 \u2208 \u222a \u222a\nThe commutativity of follows in the same way from the definition of\n\u2229 \u2229\nin terms of and the commutativity of , and a similar argument shows\n\u2227 \u2227\nthat union and intersection are associative operations.\nThedistributivelawsforpropositionallogicgiverisetotwosimilarrules\nin set theory. Let A, B, and C be any sets. Then\nA (B C)=(A B) (A C)\n\u222a \u2229 \u222a \u2229 \u222a\nand\nA (B C)=(A B) (A C)\n\u2229 \u222a \u2229 \u222a \u2229\nThese rules are called the distributive laws for set theory. To verify the\nfirst of these laws, we just have to note that for any x,\nx A (B C)\n\u2208 \u222a \u2229\n(x A) ((x B) (x C)) (definition of , )\n\u2194 \u2208 \u2228 \u2208 \u2227 \u2208 \u222a \u2229\n((x A) (x B)) ((x A) (x C)) (distributivity of )\n\u2194 \u2208 \u2228 \u2208 \u2227 \u2208 \u2228 \u2208 \u2228\n(x A B) (x A C) (definition of )\n\u2194 \u2208 \u222a \u2227 \u2208 \u222a \u222a\nx ((A B) (A C)) (definition of )\n\u2194 \u2208 \u222a \u2229 \u222a \u2229\nThe second distributive law for sets follows in exactly the same way.\nWhile isanalogousto and isanalogousto ,wehavenotyetseen\n\u222a \u2228 \u2229 \u2227\nanyoperationissettheorythatisanalogoustothelogical\u201cnot\u201doperator, .\n\u00ac\nGiven a set A, it is tempting to try to define x (x A) , the set that\n{ |\u00ac \u2208 }\ncontains everything that does not belong to A. Unfortunately, the rules of\nset theory do not allow us to define such a set. The notation x P(x)\n{ | }\ncan only be used when the domain of discourse of P is a set, so there 90 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS\nmust be an underlying set from which the elements that are\/are not in A\nare chosen, i.e. some underlying set of which A is a subset. We can get\naround this problem by restricting the discussion to subsets of some fixed\nset. This set will be known as the universal set. Keep in mind that the\nuniversal set is only universal for some particular discussion. It is simply\nsome set that is large enough to contain all the sets under discussion as\nsubsets. Given a universal set U and any subset A of U, we can define the\nset x U (x A) .\n{ \u2208 |\u00ac \u2208 }\nDefinition 2.1. LetU beagivenuniversalset, andletAbeanysubsetof\nU. We define the complement of A in U to be the set A that is defined\nby A= x U x A .\n{ \u2208 | 6\u2208 }\nUsually, we will refer to the complement of A in U simply as the com-\nplementofA,butyoushouldrememberthatwhenevercomplementsofsets\nare used, there must be some universal set in the background.\nGiventhecomplementoperationonsets, wecanlookforanalogstothe\nrules of logic that involve negation. For example, we know that p p=F\n\u2227\u00ac\nfor any proposition p. It follows that for any subset A of U,\nA A= x U (x A) (x A) (definition of )\n\u2229 { \u2208 | \u2208 \u2227 \u2208 } \u2229\n= x U (x A) (x A) (definition of complement)\n{ \u2208 | \u2208 \u2227 6\u2208 }\n= x U (x A) (x A) (definition of )\n{ \u2208 | \u2208 \u2227\u00ac \u2208 } 6\u2208\n=\n\u2205\nthe last equality following because the proposition (x A) (x A) is\n\u2208 \u2227\u00ac \u2208\nfalse for any x. Similarly, we can show that A A = U and that A =\n\u222a\nA (where A is the complement of the complement of A, that is, the set\nobtained by taking the complement of A.)\nThe most important laws for working with complements of sets are De-\nMorgan\u2019sLawsforsets. Theselaws,whichfollowdirectlyfromDeMorgan\u2019s\nLaws for logic, state that for any subsets A and B of a universal set U,\nA B =A B\n\u222a \u2229\nand\nA B =A B\n\u2229 \u222a 2.2. THE BOOLEAN ALGEBRA OF SETS 91\nDouble complement A=A\nMiscellaneous laws A A=U\n\u222a\nA A=\n\u2229 \u2205\nA=A\n\u2205\u222a\nA=\n\u2205\u2229 \u2205\nIdempotent laws A A=A\n\u2229\nA A=A\n\u222a\nCommutative laws A B =B A\n\u2229 \u2229\nA B =B A\n\u222a \u222a\nAssociative laws A (B C)=(A B) C\n\u2229 \u2229 \u2229 \u2229\nA (B C)=(A B) C\n\u222a \u222a \u222a \u222a\nDistributive laws A (B C)=(A B) (A C)\n\u2229 \u222a \u2229 \u222a \u2229\nA (B C)=(A B) (A C)\n\u222a \u2229 \u222a \u2229 \u222a\nDeMorgan\u2019s laws A B =A B\n\u2229 \u222a\nA B =A B\n\u222a \u2229\nFigure 2.2: Some Laws of Boolean Algebra for sets. A, B, and C\nare sets. For the laws that involve the complement operator, they\nare assumed to be subsets of some universal set, U. For the most\npart, these laws correspond directly to laws of Boolean Algebra for\npropositional logic as given in Figure 1.2.\nFor example, we can verify the first of these laws with the calculation\nA B = x U x (A B) (definition of complement)\n\u222a { \u2208 | 6\u2208 \u222a }\n= x U (x A B) (definition of )\n{ \u2208 |\u00ac \u2208 \u222a } 6\u2208\n= x U (x A x B) (definition of )\n{ \u2208 |\u00ac \u2208 \u2228 \u2208 } \u222a\n= x U ( (x A)) ( (x B)) (DeMorgan\u2019s Law for logic)\n{ \u2208 | \u00ac \u2208 \u2227 \u00ac \u2208 }\n= x U (x A) (x B) (definition of )\n{ \u2208 | 6\u2208 \u2227 6\u2208 } 6\u2208\n= x U (x A) (x B) (definition of complement)\n{ \u2208 | \u2208 \u2227 \u2208 }\n=A B (definition of )\n\u2229 \u2229\nAn easy inductive proof can be used to verify generalized versions of\nDeMorgan\u2019s Laws for set theory. (In this context, all sets are assumed to 92 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS\nbe subsets of some unnamed universal set.) A simple calculation verifies\nDeMorgan\u2019s Law for three sets:\nA B C =(A B) C\n\u222a \u222a \u222a \u222a\n=(A B) C (by DeMorgan\u2019s Law for two sets)\n\u222a \u2229\n=(A B) C (by DeMorgan\u2019s Law for two sets)\n\u2229 \u2229\n=A B C\n\u2229 \u2229\nFrom there, we can derive similar laws for four sets, five sets, and so on.\nHowever, just saying \u201cand so on\u201d is not a rigorous proof of this fact. Here\nis a rigorous inductive proof of a generalized DeMorgan\u2019s Law:\nTheorem 2.5. For any natural number n 2 and for any sets X , X ,\n1 2\n\u2265\n..., X ,\nn\nX X X =X X X\n1 2 n 1 2 2\n\u222a \u222a\u00b7\u00b7\u00b7\u222a \u2229 \u2229\u00b7\u00b7\u00b7\u2229\nProof. Wegiveaproofbyinduction. Inthebasecase,n=2,thestatement\nis that X X = X X . This is true since it is just an application of\n1 2 1 n\n\u222a \u2229\nDeMorgan\u2019s law for two sets.\nFor the inductive case, suppose that the statement is true for n = k.\nWe want to show that it is true for n = k+1. Let X , X , ..., X be\n1 2 k+1\nany k sets. Then we have:\nX X X =(X X X ) X\n1 2 k+1 1 2 k k+1\n\u222a \u222a\u00b7\u00b7\u00b7\u222a \u222a \u222a\u00b7\u00b7\u00b7\u222a \u222a\n=(X X X ) X\n1 2 k k+1\n\u222a \u222a\u00b7\u00b7\u00b7\u222a \u2229\n=(X X X ) X\n1 2 k k+1\n\u2229 \u2229\u00b7\u00b7\u00b7\u2229 \u2229\n=X X X\n1 2 k+1\n\u2229 \u2229\u00b7\u00b7\u00b7\u2229\nIn this computation, the second step follows by DeMorgan\u2019s Law for two\nsets, while the third step follows from the induction hypothesis.\nJust as the laws of logic allow us to do algebra with logical formulas,\nthelawsofsettheoryallowustodoalgebrawithsets. Becauseoftheclose\nrelationship between logic and set theory, their algebras are very similar.\nThe algebra of sets, like the algebra of logic, is Boolean algebra. When\nGeorge Boole wrote his 1854 book about logic, it was really as much about\nset theory as logic. In fact, Boole did not make a clear distinction between\na predicate and the set of objects for which that predicate is true. His\nalgebraic laws and formulas apply equally to both cases. More exactly, if\nweconsideronlysubsetsofsomegivenuniversalsetU,thenthereisadirect\ncorrespondence between the basic symbols and operations of propositional 2.2. THE BOOLEAN ALGEBRA OF SETS 93\nlogic and certain symbols and operations in set theory, as shown in this\ntable:\nLogic Set Theory\nT U\nF\n\u2205\np q A B\n\u2227 \u2229\np q A B\n\u2228 \u222a\np A\n\u00ac\nAny valid logical formula or computation involving propositional variables\nand the symbols T, F, , , and can be transformed into a valid formula\n\u2227 \u2228 \u00ac\nor computation in set theory by replacing the propositions in the formula\nwith subsets of U and replacing the logical symbols with U, , , , and\n\u2205 \u2229 \u222a\nthe complement operator.\nJust as in logic, the operations of set theory can be combined to form\ncomplexexpressionssuchas(A C) (B C D). Parenthesescanalways\n\u222a \u2229 \u222a \u222a\nbe used in such expressions to specify the order in which the operations\nare to be performed. In the absence of parentheses, we need precedence\nrules to determine the order of operation. The precedence rules for the\nBoolean algebra of sets are carried over directly from the Boolean algebra\nof propositions. When union and intersection are used together without\nparentheses, intersection has precedence over union. Furthermore, when\nseveral operators of the same type are used without parentheses, then they\nare evaluated in order from left to right. (Of course, since and are\n\u222a \u2229\nboth associative operations, it really doesn\u2019t matter whether the order of\nevaluation is left-to-right or right-to-left.) For example, A B C D is\n\u222a \u2229 \u222a\nevaluated as (A ((B C)) D. The complement operation is a special\n\u222a \u2229 \u222a\ncase. Since it is denoted by drawing a line over its operand, there is never\nany ambiguity about which part of a formula it applies to.\nThe laws of set theory can be used to simplify complex expressions\ninvolving sets. (As usual, of course, the meaning of \u201csimplification\u201d is\npartly in the eye of the beholder.) For example, for any sets X and Y,\n(X Y) (Y X)=(X Y) (X Y) (Commutative Law)\n\u222a \u2229 \u222a \u222a \u2229 \u222a\n=(X Y) (Idempotent Law)\n\u222a\nwhereinthesecondstep, theIdempotentLaw, whichsaysthatA A=A,\n\u2229\nis applied with A = X Y. For expressions that use the complement\n\u222a\noperation,itisusuallyconsideredtobesimplertoapplytheoperationtoan 94 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS\nindividual set, as in A, rather than to a formula, as in A B. DeMorgan\u2019s\n\u2229\nLawscanalwaysbeusedtosimplifyanexpressioninwhichthecomplement\noperation is applied to a formula. For example,\nA B A=A (B A) (DeMorgan\u2019s Law)\n\u2229 \u222a \u2229 \u2229\n=A (B A) (Double Complement)\n\u2229 \u2229\n=A (A B) (Commutative Law)\n\u2229 \u2229\n=(A A) B) (Associative Law)\n\u2229 \u2229\n=A B (Idempotent Law)\n\u2229\nAs a final example of the relationship between set theory and logic,\nconsider the set-theoretical expression A (A B) and the corresponding\n\u2229 \u222a\ncompound proposition p (p q). (These correspond since for any x,\n\u2227 \u2228\nx A (A B) (x A) ((x A) (x B)).) You might find it\n\u2208 \u2229 \u222a \u2261 \u2208 \u2227 \u2208 \u2228 \u2208\nintuitively clear that A (A B)=A. Formally, this follows from the fact\n\u2229 \u222a\nthat p (p q) p, which might be less intuitively clear and is surprising\n\u2227 \u2228 \u2261\ndifficult to prove algebraically from the laws of logic. However, there is\nanother way to check that a logical equivalence is valid: Make a truth\ntable. Consider a truth table for p (p q):\n\u2227 \u2228\np q p q p (p q)\n\u2228 \u2227 \u2228\nfalse false false false\nfalse true true false\ntrue false true true\ntrue true true true\nThefactthatthefirstcolumnandthelastcolumnofthistableareidentical\nshows that p (p q) p. Taking p to be the proposition x A and q to\n\u2227 \u2228 \u2261 \u2208\nbe the proposition x B, it follows that the sets A and A (A B) have\n\u2208 \u2229 \u222a\nthe same members and therefore are equal.\nExercises\n1. Usethelawsoflogictoverifytheassociativelawsforunionandintersection.\nThat is, show that if A, B, and C are sets, then A (B C)=(A B) C\n\u222a \u222a \u222a \u222a\nand A (B C)=(A B) C.\n\u2229 \u2229 \u2229 \u2229\n2. Show that for any sets A and B, A A B and A B A.\n\u2286 \u222a \u2229 \u2286\n3. Recallthatthesymbol denotesthelogicalexclusiveoroperation. IfAand\n\u2295\nB sets, define the set A B by A B = x (x A) (x B) . Show that\n\u25b3 \u25b3 { | \u2208 \u2295 \u2208 }\nA B =(ArB) (BrA). (A B isknownasthesymmetric difference\n\u25b3 \u222a \u25b3\nof A and B.) 2.3. APPLICATION: PROGRAMMING WITH SETS 95\n4. LetAbeasubsetofsomegivenuniversalsetU. VerifythatA=Aandthat\nA A=U.\n\u222a\n5. VerifythesecondofDeMorgan\u2019sLawsforsets,A B =A B. Foreachstep\n\u2229 \u222a\nin your verification, state why that step is valid.\n6. The subset operator, , is defined in terms of the logical implication opera-\n\u2286\ntor, . However, differs from the and operators in that A B and\n\u2192 \u2286 \u2229 \u222a \u2229\nA B are sets, while A B is a statement. So the relationship between\n\u222a \u2286 \u2286\nand isn\u2019t quite the same as the relationship between and or between\n\u2192 \u222a \u2228\nand . Nevertheless, and do share some similar properties. This\n\u2229 \u2227 \u2286 \u2192\nproblem shows one example.\na) Showthatthefollowingthreecompoundpropositionsarelogicallyequiv-\nalent: p q, (p q) p, and (p q) q.\n\u2192 \u2227 \u2194 \u2228 \u2194\nb) Show that for any sets A and B, the following three statements are\nequivalent: A B, A B =A, and A B =B.\n\u2286 \u2229 \u222a\n7. DeMorgan\u2019s Laws apply to subsets of some given universal set U. Show that\nfor a subset X of U, X = U rX. It follows that DeMorgan\u2019s Laws can be\nwrittenasUr(A B)=(UrA) (UrB)andUr(A B)=(UrA) (UrB).\n\u222a \u2229 \u2229 \u222a\nShowthattheselawsholdwhetherornotAandB aresubsetsofU. Thatis,\nshow that for any sets A, B, and C, Cr(A B)=(CrA) (CrB) and\n\u222a \u2229\nCr(A B)=(CrA) (CrB).\n\u2229 \u222a\n8. Show that A (A B)=A for any sets A and B.\n\u222a \u2229\n9. LetX andY besets. Simplifyeachofthefollowingexpressions. Justifyeach\nstep in the simplification with one of the rules of set theory.\na) X (Y X) b) (X Y) X\n\u222a \u222a \u2229 \u2229\nc) (X Y) Y d) (X Y) (X Y)\n\u222a \u2229 \u222a \u222a \u2229\n10. Let A, B, and C be sets. Simplify each of the following expressions. In your\nanswer, the complement operator should only be applied to the individual\nsets A, B, and C.\na) A B C b) A B C c) A B\n\u222a \u222a \u222a \u2229 \u222a\nd) B C e) A B C f) A A B\n\u2229 \u2229 \u2229 \u2229 \u222a\n11. Use induction to prove the following generalized DeMorgan\u2019s Law for set\ntheory: For any natural number n 2 and for any sets X1, X2, ..., X n,\n\u2265\nX1 X2 X\nn\n=X1 X2 X\nn\n\u2229 \u2229\u00b7\u00b7\u00b7\u2229 \u222a \u222a\u00b7\u00b7\u00b7\u222a\n12. State and prove generalized distributive laws for set theory.\n2.3 Application: Programming with Sets\nOn a computer, all data are represented, ultimately, as strings of zeros\nand ones. At times, computers need to work with sets. How can sets be\nrepresented as strings of zeros and ones? 96 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS\nA set is determined by its elements. Given a set A and an entity x, the\nfundamentalquestionis, doesxbelongtoAornot? Ifweknowtheanswer\nto this question for each possible x, then we know the set. For a given x,\nthe answer to the question, \u201cIs x a member of A,\u201d is either yes or no. The\nanswer can be encoded by letting 1 stand for yes and 0 stand for no. The\nanswer, then, is a single bit, that is, a value that can be either zero or one.\nTo represent the set A as a string of zeros and ones, we could use one bit\nfor each possible member of A. If a possible member x is in the set, then\nthe corresponding bit has the value one. If x is not in the set, then the\ncorresponding bit has the value zero.\nNow, in cases where the number of possible elements of the set is very\nlarge or infinite, it is not practical to represent the set in this way. It\nwould require too many bits, perhaps an infinite number. In such cases,\nsome other representation for the set can be used. However, suppose we\nare only interested in subsets of some specified small set. Since this set\nplays the role of a universal set, let\u2019s call it U. To represent a subset of\nU, we just need one bit for each member of U. If the number of members\nof U is n, then a subset of U is represented by a string of n zeros and\nones. Furthermore, every string of n zeros and ones determines a subset\nof U, namely that subset that contains exactly the elements of U that\ncorrespond to ones in the string. A string of n zeros and ones is called an\nn-bit binary number. So, we see that if U is a set with n elements, then\nthe subsets of U correspond to n-bit binary numbers.\nTo make things more definite, let U be the set 0,1,2,...,31 . This\n{ }\nset consists of the 32 integers between 0 and 31, inclusive. Then each\nsubset of U can be represented by a 32-bit binary number. We use 32 bits\nbecause most computer languages can work directly with 32-bit numbers.\nFor example, the programming languages Java, C, and C++ have a data\ntype named int. A value of type int is a 32-bit binary number.1 Before we\ngetadefinitecorrespondencebetweensubsetsofU and32-bitnumbers, we\nhave to decide which bit in the number will correspond to each member of\nU. Following tradition, we assume that the bits are numbered from right\nto left. That is, the rightmost bit corresponds to the element 0 in U, the\nsecond bit from the right corresponds to 1, the third bit from the right to\n2, and so on. For example, the 32-bit number\n1000000000000000000001001110110\ncorresponds to the subset 1,2,4,5,6,9,31 . Since the leftmost bit of the\n{ }\n1Actually,insomeversionsofCandC++,avalueoftypeint isa16-bitnumber. A\n16-bitnumbercanbeusedtorepresentasubsetoftheset{0,1,2,...,15}. Theprinciple,\nofcourse,isthesame. 2.3. APPLICATION: PROGRAMMING WITH SETS 97\nHex. Binary Hex. Binary\n0 0000 8 1000\n2 2\n1 0001 9 1001\n2 2\n2 0010 A 1010\n2 2\n3 0011 B 1011\n2 2\n4 0100 C 1100\n2 2\n5 0101 D 1101\n2 2\n6 0110 E 1110\n2 2\n7 0111 F 1111\n2 2\nFigure 2.3: The 16 hexadecimal digits and the corresponding bi-\nnary numbers. Each hexadecimal digit corresponds to a 4-bit binary\nnumber. Longer binary numbers can be written using two or more\nhexadecimal digits. For example, 101000011111 =0xA1F.\n2\nnumberis1,thenumber31isintheset; sincethenextbitis0,thenumber\n30 is not in the set; and so on.\nFrom now on, I will write binary numbers with a subscript of 2 to\navoid confusion with ordinary numbers. Furthermore, I will often leave\nout leading zeros. For example, 1101 is the binary number that would be\n2\nwritten out in full as\n00000000000000000000000000001101\nand which corresponds to the set 0,2,3 . On the other hand 1101 repre-\n{ }\nsents the ordinary number one thousand one hundred and one.\nEven with this notation, it can be very annoying to write out long bi-\nnary numbers\u2014and almost impossible to read them. So binary numbers\nareneverwrittenoutassequencesofzerosandonesincomputerprograms.\nAn alternative is to use hexadecimal numbers. Hexadecimal numbers\nare written using the sixteen symbols 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C,\nD, E, and F. These symbols are knows as the hexadecimal digits. Each\nhexadecimal digit corresponds to a 4-bit binary number, as shown in Fig-\nure 2.3. To represent a longer binary number, several hexadecimal digits\ncan be strung together. For example, the hexadecimal number C7 repre-\nsents the binary number 11000111 . In Java and many related languages,\n2\na hexadecimal number is written with the prefix \u201c0x\u201d. Thus, the hexadec-\nimal number C7 would appear in the program as 0xC7. I will follow the 98 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS\nsameconventionhere. Any32-bitbinarynumbercanbewrittenusingeight\nhexadecimal digits (or fewer if leading zeros are omitted). Thus, subsets of\n0,1,2,...,31 correspond to 8-digit hexadecimal numbers. For example,\n{ }\nthe subset 1,2,4,5,6,9,31 corresponds to 0x80000276, which represents\n{ }\nthe binary number 1000000000000000000001001110110 . Similarly, 0xFF\n2\ncorresponds to 0,1,2,3,4,5,6,7 and 0x1101 corresponds to the binary\n{ }\nnumber 0001000100000001 and to the set 0,8,12 .\n2\n{ }\nNow, if you have worked with binary numbers or with hexadecimal\nnumbers, you know that they have another, more common interpretation.\nThey represent ordinary integers. Just as 342 represents the integer 3\n\u00b7\n102 + 4 101 + 2 100, the binary number 1101 represents the integer\n2\n\u00b7 \u00b7\n1 23+1 22+0 21+1 20,or13. Whenusedinthisway,binarynumbersare\n\u00b7 \u00b7 \u00b7 \u00b7\nknownasbase-2numbers,justasordinarynumbersarebase-10numbers.\nHexadecimalnumberscanbeinterpretedasbase-16numbers. Forexample,\n0x3C7 represents the integer 3 162 +12 161 +7 160, or 874. So, does\n\u00b7 \u00b7 \u00b7\n1101 really represent the integer 13, or does it represent the set 0,2,3 ?\n2\n{ }\nThe answer is that to a person, 1101 can represent either. Both are valid\n2\ninterpretations, and the only real question is which interpretation is useful\ninagivencircumstance. Ontheotherhand,tothecomputer,1101 doesn\u2019t\n2\nrepresentanything. It\u2019sjustastringofbits, andthecomputermanipulates\nthe bits according to its program, without regard to their interpretation.\nOf course, we still have to answer the question of whether it is ever\nuseful to interpret strings of bits in a computer as representing sets.\nIf all we could do with sets were to \u201crepresent\u201d them, it wouldn\u2019t be\nvery useful. We need to be able to compute with sets. That is, we need to\nbe able to perform set operations such as union and complement.\nMany programming languages provideoperators that performsetoper-\nations. In Java and related languages, the operators that perform union,\nintersection, and complement are written as , &, and ~. For exam-\n|\nple, if x and y are 32-bit integers representing two subsets, X and Y, of\n0,1,2,...,31 , then x y is a 32-bit integer that represents the set X Y.\n{ } | \u222a\nSimilarly,x&yrepresentsthesetX Y,and~xrepresentsthecomplement,\n\u2229\nX.\nThe operators , &, and ~ are called bitwise logical operators be-\n|\ncause of the way they operate on the individual bits of the numbers to\nwhichtheyareapplied. If0and1areinterpretedasthelogicalvaluesfalse\nand true, then the bitwise logical operators perform the logical operations\n, , and on individual bits. To see why this is true, let\u2019s look at the\n\u2228 \u2227 \u00ac\ncomputations that these operators have to perform.\nLet k be one of the members of 0,1,2,...,31 . In the binary numbers\n{ }\nx,y,x y,x&y,and~x,thenumberk correspondstothebitinpositionk.\n| 2.3. APPLICATION: PROGRAMMING WITH SETS 99\nThatis,k isinthesetrepresentedbyabinarynumberifandonlyifthebit\nin position k in that binary number is 1. Considered as sets, x&y is the\nintersection of x and y, so k is a member of the set represented by x&y if\nand only if k is a member of both of the sets represented by x and y. That\nis, bitk is1inthebinarynumberx&y ifandonlyifbitk is1inxandbit\nk is 1 in y. When we interpret 1 as true and 0 as false, we see that bit k of\nx&y is computed by applying the logical \u201cand\u201d operator, , to bit k of x\n\u2227\nand bit k of y. Similarly, bit k of x y is computed by applying the logical\n|\n\u201cor\u201d operator, , to bit k of x and bit k of y. And bit k of ~x is computed\n\u2228\nby applying the logical \u201cnot\u201d operator, , to bit k of x. In each case, the\n\u00ac\nlogical operator is applied to each bit position separately. (Of course, this\ndiscussion is just a translation to the language of bits of the definitions of\nthesetoperationsintermsoflogicaloperators: A B = x x A x B ,\n\u2229 { | \u2208 \u2227 \u2208 }\nA B = x x A x B , and A= x U (x A) .)\n\u222a { | \u2208 \u2228 \u2208 } { \u2208 |\u00ac \u2208 }\nFor example, consider the binary numbers 1011010 and 10111 , which\n2 2\nrepresent the sets 1,3,4,6 and 0,1,2,4 . Then 1011010 & 10111 is\n2 2\n{ } { }\n10010 . This binary number represents the set 1,4 , which is the inter-\n2\n{ }\nsection 1,3,4,6 0,1,2,4 . It\u2019seasiertoseewhat\u2019sgoingonifwewrite\n{ }\u2229{ }\nout the computation in columns, the way you probably first learned to do\naddition:\n1 0 1 1 0 1 0 6, 4, 3, 1\n{ }\n& 0 0 1 0 1 1 1 4, 2, 1, 0\n{ }\n0 0 1 0 0 1 0 4, 1\n{ }\nNotethatineachcolumninthebinarynumbers,thebitinthebottomrow\nis computed as the logical \u201cand\u201d of the two bits that lie above it in the\ncolumn. I\u2019ve written out the sets that correspond to the binary numbers\ntoshowhowthebitsinthenumberscorrespondtothepresenceorabsence\nof elements in the sets. Similarly, we can see how the union of two sets is\ncomputed as a bitwise \u201cor\u201d of the corresponding binary numbers.\n1 0 1 1 0 1 0 6, 4, 3, 1\n{ }\n0 0 1 0 1 1 1 4, 2, 1, 0\n| { }\n1 0 1 1 1 1 1 6, 4, 3, 2, 1, 0\n{ }\nThecomplementofasetiscomputedusingabitwise\u201cnot\u201doperation. Since\nwe are working with 32-bit binary numbers, the complement is taken with\nrespect to the universal set 0,1,2,...,31 . So, for example,\n{ }\n~1011010 = 11111111111111111111111110100101\n2 2\nOf course, we can apply the operators &, , and ~ to numbers written in\n|\nhexadecimal form, or even in ordinary, base-10 form. When doing such 100 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS\ncalculations by hand, it is probably best to translate the numbers into\nbinary form. For example,\n0xAB7 & 0x168E =101010110111 & 1011010001110\n2 2\n=0001010000110\n2\n=0x286\nWhen computing with sets, it is sometimes necessary to work with in-\ndividual elements. Typical operations include adding an element to a set,\nremovinganelementfromaset,andtestingwhetheranelementisinaset.\nHowever, instead of working with an element itself, it\u2019s convenient to work\nwith the set that contains that element as its only member. For example,\ntesting whether 5 A is the same as testing whether 5 A = . The\n\u2208 { }\u2229 6 \u2205\nset 5 is represented by the binary number 100000 or by the hexadeci-\n2\n{ }\nmal number 0x20. Suppose that the set A is represented by the number x.\nThen,testingwhether5 Aisequivalenttotestingwhether0x20&x=0.\n\u2208 6\nSimilarly, the set A 5 , which is obtained by adding 5 to A, can be com-\n\u222a{ }\nputed as x 0x20. The set Ar 5 , which is the set obtained by removing\n| { }\n5 from A if it occurs in A, is represented by x & ~0x20.\nThe sets 0 , 1 , 2 , 3 , 4 , 5 , 6 , ..., 31 are represented by\n{ } { } { } { } { } { } { } { }\nthe hexadecimal numbers 0x1, 0x2, 0x4, 0x8, 0x10, 0x20, ..., 0x80000000.\nIn typical computer applications, some of these numbers are given names,\nand these names are thought of as names for the possible elements of a\nset (although, properly speaking, they are names for sets containing those\nelements). Suppose, for example, that a, b, c, and d are names for four of\nthe numbers from the above list. Then a c is the set that contains the two\n|\nelements corresponding to the numbers a and c. If x is a set, then x&~d\nis the set obtained by removing d from x. And we can test whether b is in\nx by testing if x&b=0.\n6\nHere is an actual example, which is used in the Macintosh operating\nsystem. Characterscanbeprintedordisplayedonthescreeninvarioussizes\nand styles. A font is a collection of pictures of characters in a particular\nsizeandstyle. OntheMacintosh,abasicfontcanbemodifiedbyspecifying\nanyofthefollowingstyleattributes: bold,italic,underline,outline,shadow,\ncondense,andextend. Thestyleofafontisasubsetofthissetofattributes.\nA style set can be specified by or-ing together individual attributes. For\nexample, an underlined, bold, italic font has style set underline bold\n| |\nitalic. For a plain font, with none of the style attributes set, the style set\nis the empty set, which is represented by the number zero.\nThe Java programming language uses a similar scheme to specify style\nattributes for fonts, but currently there are only two basic attributes, 2.3. APPLICATION: PROGRAMMING WITH SETS 101\nFont.BOLD and Font.ITALIC. A more interesting example in Java is pro-\nvidedbyeventtypes. AneventinJavarepresentssomekindofuseraction,\nsuch as pressing a key on the keyboard. Events are associated with \u201ccom-\nponents\u201d such as windows, push buttons, and scroll bars. Components can\nbe set to ignore a given type of event. We then say that that event type\nis disabled for that component. If a component is set to process events\nof a given type, then that event type is said to be enabled. Each compo-\nnent keeps track of the set of event types that are currently enabled. It\nwill ignore any event whose type is not in that set. Each event type has\nan associated constant with a name such as AWTEvent.MOUSE EVENT MASK.\nThese constants represent the possible elements of a set of event types. A\nsetofeventtypescanbespecifiedbyor-ingtogetheranumberofsuchcon-\nstants. If c is a component and x is a number representing a set of event\ntypes,thenthecommand\u201cc.enableEvents(x)\u201denablestheeventsintheset\nx for the component c. If y represents the set of event types that were\nalready enabled for c, then the effect of this command is to replace y with\nthe union, y x. Another command, \u201cc.disableEvents(x)\u201d, will disable the\n|\nevent types in x for the component c. It does this by replacing the current\nset, y, with y&~x.\nExercises\n1. Suppose that the numbers x and y represent the sets A and B. Show that\nthe set ArB is represented by x&(~y).\n2. Write each of the following binary numbers in hexadecimal:\na) 101101102 b) 102 c) 1111000011112 d) 1010012\n3. Write each of the following hexadecimal numbers in binary:\na) 0x123 b) 0xFADE c) 0x137F d) 0xFF11\n4. Givethevalueofeachofthefollowingexpressionsasahexadecimalnumber:\na) 0x73 0x56A b) ~0x3FF0A2FF\n|\nc) (0x44 0x95) & 0xE7 d) 0x5C35A7 & 0xFF00\n|\ne) 0x5C35A7 & ~0xFF00 f) ~(0x1234 & 0x4321)\n5. Find a calculator (or a calculator program on a computer) that can work\nwithhexadecimalnumbers. Writeashortreportexplaininghowtoworkwith\nhexadecimal numbers on that calculator. You should explain, in particular,\nhow the calculator can be used to do the previous problem.\n6. Thisquestionassumesthatyouknowhowtoaddbinarynumbers. Supposex\nandyarebinarynumbers. Underwhatcircumstanceswillthebinarynumbers\nx+y and x y be the same?\n|\n7. In addition to hexadecimal numbers, the programming languages Java, C,\nand C++ support octal numbers. Look up and report on octal numbers in 102 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS\nJava,C,orC++. Explainwhatoctalnumbersare,howtheyarewritten,and\nhow they are used.\n8. In the UNIX (or Linux) operating system, every file has an associated set of\npermissions, which determine who can use the file and how it can be used.\nThe set of permissions for a given file is represented by a nine-bit binary\nnumber. This number is sometimes written as an octal number. Research\nand report on the UNIX systems of permissions. What set of permissions\nis represented by the octal number 752? by the octal number 622? Explain\nwhat is done by the UNIX commands \u201cchmod g+rw filename\u201d and \u201cchmod\no-w filename\u201d in terms of sets. (Hint: Look at the man page for the chmod\ncommand. To see the page, use the UNIX command \u201cman chmod\u201d. If you\ndon\u2019t know what this means, you probably don\u2019t know enough about UNIX\nto do this exercise.)\n9. Java,C,andC++eachhaveabooleandatatypethathasthevaluestrue and\nfalse. Theusuallogicaland,or,andnotoperatorsonbooleanvaluesarerep-\nresented by the operators &&, , and !. C and C++ allow integer values to\n||\nbeusedinplaceswherebooleanvaluesareexpected. Inthiscase,theinteger\nzero represents the boolean value false while any non-zero integer represents\nthe boolean value true. This means that if x and y are integers, then both\nx&y and x&&y are valid expressions, and both can be considered to rep-\nresent boolean values. Do the expressions x&y and x&&y always represent\nthe same boolean value, for any integers x and y? Do the expressions x y\n|\nand x y always represent the same boolean values? Explain your answers.\n||\n10. Supposethatyou,asaprogrammer,wanttowriteasubroutinethatwillopen\nawindowonthecomputer\u2019sscreen. Thewindowcanhaveanyofthefollowing\noptions: a close box, a zoom box, a resize box, a minimize box, a vertical\nscroll bar, a horizontal scroll bar. Design a scheme whereby the options\nfor the window can be specified by a single parameter to the subroutine.\nThe parameter should represent a set of options. How would you use your\nsubroutine to open a window that has a close box and both scroll bars and\nno other options? Inside your subroutine, how would you determine which\noptions have been specified for the window?\n2.4 Functions\nBoththerealworldandtheworldofmathematicsarefullofwhatarecalled,\nin mathematics, \u201cfunctional relationships.\u201d A functional relationship is a\nrelationship between two sets, which associates exactly one element from\nthe second set to each element of the first set.\nFor example, each item for sale in a store has a price. The first set in\nthis relationship is the set of items in the store. For each item in the store,\nthereisanassociatedprice,sothesecondsetintherelationshipisthesetof 2.4. FUNCTIONS 103\npossible prices. The relationship is a functional relationship because each\nitem has a price. That is, the question \u201cWhat is the price of this item?\u201d\nhas a single, definite answer for each item in the store.\nSimilarly, thequestion\u201cWhoisthe(biological)motherofthisperson?\u201d\nhas a single, definite answer for each person. So, the relationship \u201cmother\nof\u201d defines a functional relationship. In this case, the two sets in the rela-\ntionship are the same set, namely the set of people.2 On the other hand,\nthe relationship \u201cchild of\u201d is not a functional relationship. The question\n\u201cWho is the child of this person?\u201d does not have a single, definite answer\nfor each person. A given person might not have any child at all. And a\ngiven person might have more than one child. Either of these cases\u2014a\nperson with no child or a person with more than one child\u2014is enough to\nshow that the relationship \u201cchild of\u201d is not a functional relationship.\nOr consider an ordinary map, such as a map of New York State or a\nstreet map of Rome. The whole point of the map, if it is accurate, is that\nthere is a functional relationship between points on the map and points on\nthe surface of the Earth. Perhaps because of this example, a functional\nrelationship is sometimes called a mapping.\nThere are also many natural examples of functional relationships in\nmathematics. For example, every rectangle has an associated area. This\nfactexpressesafunctionalrelationshipbetweenthesetofrectanglesandthe\nsetofnumbers. Everynaturalnumbernhasasquare,n2. Therelationship\n\u201csquare of\u201d is a functional relationship from the set of natural numbers to\nitself.\nInmathematics,ofcourse,weneedtoworkwithfunctionalrelationships\nin the abstract. To dothis, we introduce the idea of function. You should\nthink of a function as a mathematical object that expresses a functional\nrelationship between two sets. The notation f: A B expresses the fact\n\u2192\nthat f is a function from the set A to the set B. That is, f is a name for a\nmathematical object that expresses a functional relationship from the set\nA to the set B. The notation f: A B is read as \u201cf is a function from A\n\u2192\nto B\u201d or more simply as \u201cf maps A to B.\u201d\nIf f: A B and if a A, the fact that f is a functional relationship\n\u2192 \u2208\nfromAtoB meansthatf associatessomeelementofB toa. Thatelement\nis denoted f(a). That is, for each a A, f(a) B and f(a) is the single,\n\u2208 \u2208\ndefinite answer to the question \u201cWhat element of B is associated to a by\nthe function f?\u201d The fact that f is a function from A to B means that\nthis question has a single, well-defined answer. Given a A, f(a) is called\n\u2208\nthe value of the function f at a.\n2I\u2019mavoidingherethequestionofAdamandEveorofpre-humanape-likeancestors.\n(Takeyourpick.) 104 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS\nForexample,ifI isthesetofitemsforsaleinagivenstoreandM isthe\nset of possible prices, then there is function c: I M which is defined by\n\u2192\nthe fact that for each x I, c(x) is the price of the item x. Similarly, if P\n\u2208\nis the set of people, then there is a function m: P P such that for each\n\u2192\nperson p, m(p) is the mother of p. And if N is the set of natural numbers,\nthen the formula s(n) = n2 specifies a function s: N N. It is in the\n\u2192\nform of formulas such as s(n)=n2 or f(x)=x3 3x+7 that most people\n\u2212\nfirst encounter functions. But you should note that a formula by itself is\nnotafunction, althoughitmightwellspecifyafunctionbetweentwogiven\nsetsofnumbers. Functionsaremuchmoregeneralthanformulas, andthey\napply to all kinds of sets, not just to sets of numbers.\nSuppose that f: A B and g: B C are functions. Given a A,\n\u2192 \u2192 \u2208\nthereisanassociatedelementf(a) B. Sinceg isafunctionfromB toC,\n\u2208\nand since f(a) B, g associates some element of C to f(a). That element\n\u2208\nisg(f(a)). StartingwithanelementaofA,wehaveproducedanassociated\nelementg(f(a))ofC. Thismeansthatwehavedefinedanewfunctionfrom\nthe set A to the set C. This function is called the composition of g with\nf, and it is denoted by g f. That is, if f: A B and g: B C are\n\u25e6 \u2192 \u2192\nfunctions, then g f: A C is the function which is defined by\n\u25e6 \u2192\n(g f)(a)=g(f(a))\n\u25e6\nfor each a A. For example, suppose that p is the function that associates\n\u2208\ntoeachiteminastorethepriceoftheitem,andsupposethattisafunction\nthat associates the amount of tax on a price to each possible price. The\ncomposition, t p, is the function that associates to each item the amount\n\u25e6\nof tax on that item. Or suppose that s: N N and r: N N are the\n\u2192 \u2192\nfunctions defined by the formulas s(n) = n2 and r(n) = 3n+1, for each\nn N. Then r s is a function from N to N, and for n N, (r s)(n) =\n\u2208 \u25e6 \u2208 \u25e6\nr(s(n)) = r(n2) = 3n2 +1. In this case, we also have the function s r,\n\u25e6\nwhich satisfies (s r)(n)=s(r(n))=s(3n+1)=(3n+1)2 =9n2+6n+1.\n\u25e6\nNote in particular that r s and s r are not the same function. The\n\u25e6 \u25e6\noperation is not commutative.\n\u25e6\nIf A is a set and f: A A, then f f, the composition of f with itself,\n\u2192 \u25e6\nis defined. For example, using the function s from the preceding example,\ns sisthefunctionfromNtoNgivenbytheformula(s s)(n)=s(s(n))=\n\u25e6 \u25e6\ns(n2) = (n2)2 = n4. If m is the function from the set of people to itself\nwhich associates to each person that person\u2019s mother, then m m is the\n\u25e6\nfunctionthatassociatestoeachpersonthatperson\u2019smaternalgrandmother.\nIfaandbareentities,then(a,b)denotestheorderedpair containinga\nandb. Theorderedpair(a,b)differsfromtheset a,b becauseasetisnot\n{ } 2.4. FUNCTIONS 105\nordered. That is, a,b and b,a denote the same set, but if a = b, then\n{ } { } 6\n(a,b) and (b,a) are different ordered pairs. More generally, two ordered\npairs (a,b) and (c,d) are equal if and only if both a=c and b=d. If (a,b)\nis an ordered pair, then a and b are referred to as the coordinates of the\nordered pair. In particular, a is the first coordinate and b is the second\ncoordinate.\nIfAandB aresets, thenwecanformthesetA B whichisdefinedby\n\u00d7\nA B = (a,b) a A and b B .\n\u00d7 { | \u2208 \u2208 }\nThis set is called the cross product or Cartesian product of the sets\nA and B. The set A B contains every ordered pair whose first co-\n\u00d7\nordinate is an element of A and whose second coordinate is an element\nof B. For example, if X = c,d and Y = 1,2,3 , then X Y =\n{ } { } \u00d7\n(c,1),(c,2),(c,3),(d,1),(d,2),(d,3) . It is possible to extend this idea\n{ }\nto the cross product of more than two sets. The cross product of the three\nsets A, B, and C is denoted A B C. It consists of all ordered triples\n\u00d7 \u00d7\n(a,b,c) where a A, b B, and c C. The definition for four or more\n\u2208 \u2208 \u2208\nsets is similar. The general term for a member of a cross product is tuple\nor, more specifically, ordered n-tuple. For example, (a,b,c,d,e) is an\nordered 5-tuple.\nGiven a function f: A B, consider the set (a,b) A B a\n\u2192 { \u2208 \u00d7 | \u2208\nA and b = f(a) . This set of ordered pairs consists of all pairs (a,b) such\n}\nthata AandbistheelementofBthatisassociatedtoabythefunctionf.\n\u2208\nThe set (a,b) A B a A and b = f(a) is called the graph of the\n{ \u2208 \u00d7 | \u2208 }\nfunction f. Since f is a function, each element a A occurs once and only\n\u2208\nonceasafirstcoordinateamongtheorderedpairsinthegraphoff. Given\na A, we can determine f(a) by finding that ordered pair and looking at\n\u2208\nthesecondcoordinate. Infact,itisconvenienttoconsiderthefunctionand\nits graph to be the same thing, and to use this as our official mathematical\ndefinition.3\nDefinition 2.2. LetAandB besets. AfunctionfromAtoB isasubset\nof A B which has the property that for each a A, the set contains one\n\u00d7 \u2208\nandonlyoneorderedpairwhosefirstcoordinateisa. If(a,b)isthatordered\npair, then b is called the value of the function at a and is denoted f(a). If\nb=f(a), then we also say that the function f maps a to b. The fact that\nf is a function from A to B is indicated by the notation f: A B.\n\u2192\n3Thisis aconvenientdefinition for themathematical world, butasisoften thecase\nin mathematics, it leaves out an awful lot of the real world. Functional relationships\nin therealworld aremeaningful, butwemodelthemin mathematics withmeaningless\nsets of ordered pairs. We do this for the usual reason: to have something precise and\nrigorousenoughthatwecanmakelogicaldeductionsandprovethingsaboutit. 106 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS\nForexample, ifX = a,b andY = 1,2,3 , thentheset (a,2),(b,1)\n{ } { } { }\nis a function from X to Y, and (1,a),(2,a),(3,b) is a function from Y\n{ }\nto X. On the other hand, (1,a),(2,b) is not a function from Y to X,\n{ }\nsince it does not specify any value for 3. And (a,1),(a,2),(b,3) is not\n{ }\na function from X to Y because it specifies two different values, 1 and 2,\nassociated with the same element, a, of X.\nEven though the technical definition of a function is a set of ordered\npairs, it\u2019s usually better to think of a function from A to B as something\nthatassociatessomeelementofB toeveryelementofA. Thesetofordered\npairsisonewayofexpressingthisassociation. Iftheassociationisexpressed\nin some other way, it\u2019s easy to write down the set of ordered pairs. For\nexample,thefunctions: N Nwhichisspecifiedbytheformulas(n)=n2\n\u2192\ncan be written as the set of ordered pairs (n,n2) n N .\n{ | \u2208 }\nSuppose that f: A B is a function from the set A to the set B.\n\u2192\nWe say that A is the domain of the function f and that B is the range\nof the function. We define the image of the function f to be the set\nb B a A(b = f(a)) . Put more simply, the image of f is the set\n{ \u2208 |\u2203 \u2208 }\nf(a) a A . That is, the image is the set of all values, f(a), of the\n{ | \u2208 }\nfunction, for all a A. (You should note that in some cases\u2014particularly\n\u2208\nin calculus courses\u2014the term \u201crange\u201d is used to refer to what I am calling\nthe image.) For example, for the function s: N N that is specified by\n\u2192\ns(n)=n2, both the domain and the range are N, and the image is the set\nn2 n N , or 0,1,4,9,16,... .\n{ | \u2208 } { }\nNote that the image of a function is a subset of its range. It can be\na proper subset, as in the above example, but it is also possible for the\nimage of a function to be equal to the range. In that case, the function is\nsaid to be onto. Sometimes, the fancier term surjective is used instead.\nFormally, a function f: A B is said to be onto (or surjective) if every\n\u2192\nelement of B is equal to f(a) for some element of A. In terms of logic, f is\nonto if and only if\nb B a A(b=f(a)) .\n\u2200 \u2208 \u2203 \u2208\nFor example, let X = a,b an(cid:0)d Y = 1,2,3 , a(cid:1)nd consider the function\n{ } { }\nfromY toX specifiedbythesetoforderedpairs (1,a),(2,a),(3,b) . This\n{ }\nfunctionisontobecauseitsimage, a,b ,isequaltotherange,X. However,\n{ }\nthe function from X to Y given by (a,1),(b,3) is not onto, because its\n{ }\nimage, 1,3 , is a proper subset of its range, Y. As a further example,\n{ }\nconsider the function f from Z to Z given by f(n)=n 52. To show that\n\u2212\nf is onto, we need to pick an arbitrary b in the range Z and show that\nthere is some number a in the domain Z such that f(a)=b. So let b be an\narbitrary integer; we want to find an a such that a 52 = b. Clearly this\n\u2212 2.4. FUNCTIONS 107\nequation will be true when a = b+52. So every element b is the image of\nthe number a = b+52, and f is therefore onto. Note that if f had been\nspecified to have domain N, then f would not be onto, as for some b Z\n\u2208\nthe number a = b+52 is not in the domain N (for example, the integer\n73 is not in the image of f, since 21 is not in N.)\n\u2212 \u2212\nIf f: A B and if a A, then a is associated to only one element of\n\u2192 \u2208\nB. Thisispartofthedefinitionofafunction. However, nosuchrestriction\nholdsforelementsofB. Ifb B,itispossibleforbtobeassociatedtozero,\n\u2208\none, two, three, ..., or even to an infinite number of elements of A. In the\ncase where each element of the range is associated to at most one element\nofthedomain,thefunctionissaidtobeone-to-one. Sometimes,theterm\ninjective is used instead. The function f is one-to-one (or injective) if for\nany two distinct elements x and y in the domain of f, f(x) and f(y) are\nalso distinct. In terms of logic, f: A B is one-to-one if and only if\n\u2192\nx A y A x=y f(x)=f(y) .\n\u2200 \u2208 \u2200 \u2208 6 \u2192 6\nSince a proposition is equivalent(cid:0)to its contrapositive,(cid:1)we can write this\ncondition equivalently as\nx A y A f(x)=f(y) x=y .\n\u2200 \u2208 \u2200 \u2208 \u2192\nSometimes, it is easier to work w(cid:0)ith the definition of on(cid:1)e-to-one when it is\nexpressed in this form. The function that associates every person to his or\nher mother is not one-to-one because it is possible for two different people\nto have the same mother. The function s: N N specified by s(n) = n2\n\u2192\nis one-to-one. However, we can define a function r: Z Z by the same\n\u2192\nformula: r(n) = n2, for n Z. The function r is not one-to-one since two\n\u2208\ndifferent integers can have the same square. For example, r( 2)=r(2).\n\u2212\nA function that is both one-to-one and onto is said to be bijective.\nThe function that associates each point in a map of New York State to a\npointinthestateitselfispresumablybijective. Foreachpointonthemap,\nthere is a corresponding point in the state, and vice versa. If we specify\nthe function f from the set 1,2,3 to the set a,b,c as the set of ordered\n{ } { }\npairs (1,b),(2,a),(3,c) , then f is a bijective function. Or consider the\n{ }\nfunction from Z to Z given by f(n)=n 52. We have already shown that\n\u2212\nf is onto. We can show that it is also one-to-one: pick an arbitrary x and\ny in Z and assume that f(x)=f(y). This means that x 52=y 52, and\n\u2212 \u2212\nadding 52 to both sides of the equation gives x = y. Since x and y were\narbitrary, we have proved x Z y Z(f(x) = f(y) x = y), that is,\n\u2200 \u2208 \u2200 \u2208 \u2192\nthat f is one-to-one. Altogether, then, f is a bijection.\nOne difficulty that people sometimes have with mathematics is its gen-\nerality. A set is a collection of entities, but an \u201centity\u201d can be anything at 108 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS\nall, including other sets. Once we have defined ordered pairs, we can use\norderedpairsaselementsofsets. Wecouldalsomakeorderedpairsofsets.\nNow that we have defined functions, every function is itself an entity. This\nmeans that we can have sets that contain functions. We can even have a\nfunction whose domain and range are sets of functions. Similarly, the do-\nmainorrangeofafunctionmight beasetofsets, orasetoforderedpairs.\nComputer scientists have a good name for this. They would say that sets,\nordered pairs, and functions are first-class objects. Once a set, ordered\npair, or function has been defined, it can be used just like any other entity.\nIf they were not first-class objects, there could be restrictions on the way\nthey can be used. For example, it might not be possible to use functions\nas members of sets. (This would make them \u201csecond class.\u201d)\nFor example, suppose that A, B, and C are sets. Then since A B is\n\u00d7\na set, we might have a function f: A B C. If (a,b) A B, then the\n\u00d7 \u2192 \u2208 \u00d7\nvalue of f at (a,b) would be denoted f((a,b)). In practice, though, one set\nof parentheses is usually dropped, and the value of f at (a,b) is denoted\nf(a,b). As a particular example, we might define a function p: N N N\n\u00d7 \u2192\nwith the formula p(n,m) = nm+1. Similarly, we might define a function\nq: N N N N N by q(n,m,k)=(nm k,nk n).\n\u00d7 \u00d7 \u2192 \u00d7 \u2212 \u2212\nSuppose that A and B are sets. There are, in general, many functions\nthat map A to B. We can gather all those functions into a set. This\nset, whose elements are all the functions from A to B, is denoted BA.\n(We\u2019llseelaterwhythisnotationisreasonable.) Usingthisnotation,saying\nf: A B is exactly the same as saying f BA. Both of these notations\n\u2192 \u2208\nassert that f is a function from A to B. Of course, we can also form\nan unlimited number of other sets, such as the power set P BA , the cross\nproductBA A,orthesetAA A,whichcontainsallthefunctionsfromthe\n\u00d7\n\u00d7 (cid:0) (cid:1)\nset A A to the set A. And of course, any of these sets can be the domain\nor ran\u00d7 ge of a function. An example of this is the function E: BA A B\ndefined by the formula E(f,a) = f(a). Let\u2019s see if we can make\u00d7 sen\u2192 se of\nthis notation. Since the domain of E is BA A, an element in the domain\n\u00d7\nis an ordered pair in which the first coordinate is a function from A to B\nand the second coordinate is an element of A. Thus, E(f,a) is defined for\na function f: A B and an element a A. Given such an f and a, the\nnotation f(a) spe\u2192 cifies an element of B, s\u2208 o the definition of E(f,a) as f(a)\nmakes sense. The function E is called the \u201cevaluation function\u201d since it\ncaptures the idea of evaluating a function at an element of its domain.\nExercises\n1. Let A= 1,2,3,4 and let B = a,b,c . Find the sets A B and B A.\n{ } { } \u00d7 \u00d7 2.5. APPLICATION: PROGRAMMING WITH FUNCTIONS 109\n2. Let A be the set a,b,c,d . Let f be the function from A to A given by the\n{ }\nsetoforderedpairs (a,b),(b,b),(c,a),(d,c) ,andletg bethefunctiongiven\n{ }\nbythesetoforderedpairs (a,b),(b,c),(c,d),(d,d) . Findthesetofordered\n{ }\npairs for the composition g f.\n\u25e6\n3. Let A = a,b,c and let B = 0,1 . Find all possible functions from A to\n{ } { }\nB. Give each function as a set of ordered pairs. (Hint: Every such function\ncorresponds to one of the subsets of A.)\n4. ConsiderthefunctionsfromZtoZwhicharedefinedbythefollowingformu-\nlas. Decidewhethereachfunctionisontoandwhetheritisone-to-one;prove\nyour answers.\na) f(n)=2n b) g(n)=n+1 c) h(n)=n2+n+1\nn\/2, if n is even\nd) s(n)=\n(cid:26) (n+1)\/2, if n is odd\n5. Provethatcompositionoffunctionsisanassociativeoperation. Thatis,prove\nthat for functions f: A B, g: B C, and h: C D, the compositions\n\u2192 \u2192 \u2192\n(h g) f and h (g f) are equal.\n\u25e6 \u25e6 \u25e6 \u25e6\n6. Suppose that f: A B and g: B C are functions and that g f is one-\n\u2192 \u2192 \u25e6\nto-one.\na) Prove that f is one-to-one. (Hint: use a proof by contradiction.)\nb) Findaspecificexamplethatshowsthatg isnotnecessarilyone-to-one.\n7. Suppose that f: A B and g: B C, and suppose that the composition\n\u2192 \u2192\ng f is an onto function.\n\u25e6\na) Prove that g is an onto function.\nb) Find a specific example that shows that f is not necessarily onto.\n2.5 Application: Programming with Functions\nFunctions are fundamental in computer programming, although not every-\nthing in programming that goes by the name of \u201cfunction\u201d is a function\naccording to the mathematical definition.\nIn computer programming, a function is a routine that is given some\ndata as input and that will calculate and return an answer based on that\ndata. For example, in the C++ programming language, a function that\ncalculates the square of an integer could be written\nint square(int n) {\nreturn n*n;\n}\nIn C++, int is a data type. From the mathematical point of view, a\ndata type is a set. The data type int is the set of all integers that can\nbe represented as 32-bit binary numbers. Mathematically, then, int Z.\n\u2286 110 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS\n(You should get used to the fact that sets and functions can have names\nthat consist of more than one character, since it\u2019s done all the time in\ncomputer programming.) The first line of the above function definition,\n\u201cint square(int n)\u201d, says that we are defining a function named square\nwhoserangeisint andwhosedomainisint. Intheusualnotationforfunc-\ntions, we would express this as square: int int, or possibly as square\nint int \u2192 \u2208\nint , whereint isthesetofallfunctionsthatmapthesetint totheset\nint.\nThe first line of the function, int square(int n), is called the proto-\ntype of the function. The prototype specifies the name, the domain, and\nthe range of the function and so carries exactly the same information as\nthe notation \u201cf: A B\u201d. The \u201cn\u201d in \u201cint square(int n)\u201d is a name\n\u2192\nfor an arbitrary element of the data type int. In computer jargon, n is\ncalled a parameter of the function. The rest of the definition of square\ntells the computer to calculate the value of square(n) for any n int by\n\u2208\nmultiplying n times n. The statement \u201creturn n*n\u201d says that n n is the\n\u2217\nvalue that is computed, or \u201creturned,\u201d by the function. (The stands for\n\u2217\nmultiplication.)\nC++ has many data types in addition to int. There is a boolean data\ntype named bool. The values of type bool are true and false. Mathemati-\ncally, bool isanamefortheset true, false . Thetypefloat consistsofreal\n{ }\nnumbers,whichcanincludeadecimalpoint. Ofcourse,onacomputer,it\u2019s\nnotpossibletorepresenttheentireinfinitesetofrealnumbers,sofloat rep-\nresents some subset of the mathematical set of real numbers. There is also\na data type whose values are strings of characters, such as \u201cHello world\u201d\nor \u201cxyz152QQZ\u201d. The name for this data type in C++ is string. All these\ntypes, and many others, can be used in functions. For example, in C++,\nm%n is the remainder when the integer m is divided by the integer n. We\ncan define a function to test whether an integer is even as follows:\nbool even(int k) {\nif ( k % 2 == 1 )\nreturn false;\nelse\nreturn true;\n}\nYou don\u2019t need to worry about all the details here, but you should under-\nstand that the prototype, bool even(int k), says that even is a function\nfromthesetint tothesetbool. Thatis, even: int bool. Givenaninteger\n\u2192\nN, even(N) has the value true if N is an even integer, and it has the value\nfalse if N is an odd integer. 2.5. APPLICATION: PROGRAMMING WITH FUNCTIONS 111\nA function can have more than one parameter. For example, we might\ndefine a function with prototype int index(string str, string sub).\nIf s and t are strings, then index(s,t) would be the int that is the value of\nthefunctionattheorderedpair(s,t). Weseethatthedomainofindexisthe\ncross product string string, and we can write index: string string int\nor, equivalently, inde\u00d7 x intstring string. \u00d7 \u2192\n\u00d7\n\u2208\nNot every C++ function is actually a function in the mathematical\nsense. Inmathematics, afunctionmustassociateasinglevalueinitsrange\nto each value in its domain. There are two things that can go wrong: The\nvalueofthefunctionmightnotbedefinedforeveryelementofthedomain,\nandthefunctionmightassociateseveraldifferentvaluestothesameelement\nof the domain. Both of these things can happen with C++ functions.\nIn computer programming, it is very common for a \u201cfunction\u201d to be\nundefined for some values of its parameter. In mathematics, a partial\nfunction from a set A to a set B is defined to be a function from a subset\nof A to B. A partial function from A to B can be undefined for some\nelements of A, but when it is defined for some a A, it associates just one\n\u2208\nelement of B to a. Many functions in computer programs are actually par-\ntial functions. (When dealing with partial functions, an ordinary function,\nwhichisdefinedforeveryelementofitsdomain,issometimesreferredtoas\na total function. Note that\u2014with the mind-boggling logic that is typical\nof mathematicians\u2014a total function is a type of partial function, because\na set is a subset of itself.)\nIt\u2019s also very common for a \u201cfunction\u201d in a computer program to pro-\nduce a variety of values for the same value of its parameter. A common\nexample is a function with prototype int random(int N), which returns\na random integer between 1 and N. The value of random(5) could be 1, 2,\n3, 4, or 5. This is not the behavior of a mathematical function!\nEventhoughmanyfunctionsincomputerprogramsarenotreallymath-\nematical functions, I will continue to refer to them as functions in this sec-\ntion. Mathematicians will just have to stretch their definitions a bit to\naccommodate the realities of computer programming.\nIn most programming languages, functions are not first-class objects.\nThat is, a function cannot be treated as a data value in the same way as\na string or an int. However, C++ does take a step in this direction. It is\npossibleforafunctiontobeaparametertoanotherfunction. Forexample,\nconsider the function prototype\nfloat sumten( float f(int) )\nThis is a prototype for a function named sumten whose parameter is a\nfunction. The parameter is specified by the prototype \u201cfloat f(int)\u201d. 112 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS\nThis means that the parameter must be a function from int to float. The\nparametername, f, standsforanarbitrarysuchfunction. Mathematically,\nf floatint, and so sumten: floatint float.\n\u2208 \u2192\nMy idea is that sumten(f) would compute f(1)+f(2)+ +f(10). A\n\u00b7\u00b7\u00b7\nmore useful function would be able to compute f(a)+f(a+1)+ +f(b)\n\u00b7\u00b7\u00b7\nforanyintegersaandb. Thisjustmeansthataandbshouldbeparameters\nto the function. The prototype for the improved function would look like\nfloat sum( float f(int), int a, int b )\nTheparameterstosum formanorderedtripleinwhichthefirstcoordinate\nis a function and the second and third coordinates are integers. So, we\ncould write\nsum: floatint int int float\n\u00d7 \u00d7 \u2192\nIt\u2019s interesting that computer programmers deal routinely with such com-\nplex objects.\nOne thing you can\u2019t do in C++ is write a function that creates new\nfunctions from scratch. The only functions that exist are those that are\ncoded into the source code of the program. There are programming lan-\nguages that do allow new functions to be created from scratch while a\nprogram is running. In such languages, functions are first-class objects.\nThese languages support what is called functional programming.\nOneofthemostaccessiblelanguagesthatsupportsfunctionalprogram-\nming is JavaScript, a language that is used on Web pages. (Although\nthe names are similar, JavaScript and Java are only distantly related.) In\nJavaScript, the function that computes the square of its parameter could\nbe defined as\nfunction square(n) {\nreturn n*n;\n}\nThisissimilartotheC++definitionofthesamefunction,butyou\u2019llnotice\nthat no type is specified for the parameter n or for the value computed by\nthe function. Given this definition of square, square(x) would be legal for\nany x of any type. (Of course, the value of square(x) would be undefined\nfor most types, so square is a very partial function, like most functions in\nJavaScript.) In effect, all possible data values in JavaScript are bundled\ntogether into one set, which I will call data. We then have square: data\n\u2192\ndata.4\n4Not all functional programming languages lump data types together in this way.\nThereisafunctionalprogramminglanguagenamedHaskell,forexample,thatisasstrict\nabouttypesasC++. ForinformationaboutHaskell,seehttp:\/\/www.Haskell.org\/. 2.5. APPLICATION: PROGRAMMING WITH FUNCTIONS 113\nIn JavaScript, a function really is a first-class object. We can begin to\nsee this by looking at an alternative definition of the function square:\nsquare = function(n) { return n*n; }\nHere, the notation \u201cfunction(n) { return n*n; }\u201d creates a function\nthat computes the square of its parameter, but it doesn\u2019t give any name\nto this function. This function object is then assigned to a variable named\nsquare. The value of square can be changed later, with another assignment\nstatement, to a different function or even to a different type of value. This\nnotationforcreatingfunctionobjectscanbeusedinotherplacesbesidesas-\nsignmentstatements. Suppose,forexample,thatafunctionwithprototype\nfunction sum(f,a,b) has been defined in a JavaScript program to com-\nputef(a)+f(a+1)+ +f(b). Thenwecouldcompute12+22+ +1002\n\u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7\nby saying\nsum( function(n) { return n*n; }, 1, 100 )\nHere, the first parameter is the function that computes squares. We have\ncreated and used this function without ever giving it a name.\nItisevenpossibleinJavaScriptforafunctiontoreturnanotherfunction\nas its value. For example,\nfunction monomial(a, n) {\nreturn ( function(x) { a*Math.pow(x,n); } );\n}\nHere, Math.pow(x,n) computes xn, so for any numbers a and n, the value\nof monomial(a,n) is a function that computes axn. Thus,\nf = monomial(2,3);\nwould define f to be the function that satisfies f(x) = 2x3, and if sum is\nthe function described above, then\nsum( monomial(8,4), 3, 6 )\nwould compute 8 34 +8 44 +8 54 +8 64. In fact, monomial can\n\u2217 \u2217 \u2217 \u2217\nbe used to create an unlimited number of new functions from scratch. It\nis even possible to write monomial(2,3)(5) to indicate the result of apply-\ning the function monomial(2,3) to the value 5. The value represented by\nmonomial(2,3)(5)is2 53,or250. Thisisrealfunctionalprogrammingand\n\u2217\nmight give you some idea of its power. 114 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS\nExercises\n1. For each of the following C++ function prototypes, translate the prototype\nintoastandardmathematicalfunctionspecification,suchasfunc: float int.\n\u2192\na) int strlen(string s)\nb) float pythag(float x, float y)\nc) int round(float x)\nd) string sub(string s, int n, int m)\ne) string unlikely( int f(string) )\nf) int h( int f(int), int g(int) )\n2. Write a C++ function prototype for a function that belongs to each of the\nfollowing sets.\na) stringstring\nb) boolfloat\u00d7float\nc)\nfloatintint\n3. It is possible to define new types in C++. For example, the definition\nstruct point {\nfloat x;\nfloat y;\n}\ndefinesanewtypenamedpoint. Avalueoftypepoint containstwovaluesof\ntype float. What mathematical operation corresponds to the construction of\nthis data type? Why?\n4. Let square, sum and monomial be the JavaScript functions described in this\nsection. What is the value of each of the following?\na) sum(square, 2, 4)\nb) sum(monomial(5,2), 1, 3)\nc) monomial(square(2), 7)\nd) sum(function(n) return 2 n; , 1, 5)\n{ \u2217 }\ne) square(sum(monomial(2,3), 1, 2))\n5. Write a JavaScript function named compose that computes the composition\noftwofunctions. Thatis,compose(f,g)isf g,wheref andgarefunctionsof\n\u25e6\noneparameter. Recallthatf gisthefunctiondefinedby(f g)(x)=f(g(x)).\n\u25e6 \u25e6\n2.6 Counting Past Infinity\nAs children, we all learned toanswer the question \u201cHow many?\u201d by count-\ning with numbers: 1, 2, 3, 4, .... But the question of \u201cHow many?\u201d was\nasked and answered long before the abstract concept of number was in-\nvented. The answer can be given in terms of \u201cas many as.\u201d How many 2.6. COUNTING PAST INFINITY 115\ncousins do you have? As many cousins as I have fingers on both hands.\nHow many sheep do you own? As many sheep as there are notches on this\nstick. How many baskets of wheat must I pay in taxes? As many baskets\nastherearestonesinthisbox. Thequestionofhowmanythingsareinone\ncollection of objects is answered by exhibiting another, more convenient,\ncollection of objects that has just as many members.\nInsettheory,theideaofonesethavingjustasmanymembersasanother\nset is expressed in terms of one-to-one correspondence. A one-to-one\ncorrespondence between two sets A and B pairs each element of A with an\nelement of B in such a way that every element of B is paired with one and\nonlyoneelementofA. Theprocessofcounting,asitislearnedbychildren,\nestablishes a one-to-one correspondence between a set of n objects and the\nsetofnumbersfrom1ton. Therulesofcountingaretherulesofone-to-one\ncorrespondence: Make sure you count every object, make sure you don\u2019t\ncountthesameobjectmorethanonce. Thatis,makesurethateachobject\ncorresponds to one and only one number. Earlier in this chapter, we used\nthe fancy name \u201cbijective function\u201d to refer to this idea, but we can now\nsee it as as an old, intuitive way of answering the question \u201cHow many?\u201d\nIn counting, as it is learned in childhood, the set 1,2,3,...,n is used\n{ }\nas a typical set that contains n elements. In mathematics and computer\nscience, it has become more common to start counting with zero instead of\nwith one, so we define the following sets to use as our basis for counting:\nN = , a set with 0 elements\n0\n\u2205\nN = 0 , a set with 1 element\n1\n{ }\nN = 0,1 , a set with 2 elements\n2\n{ }\nN = 0,1,2 , a set with 3 elements\n3\n{ }\nN = 0,1,2,3 , a set with 4 elements\n4\n{ }\nand so on. In general, N = 0,1,2,...,n 1 for each n N. For each\nn\n{ \u2212 } \u2208\nnatural number n, N is a set with n elements. Note that if n = m, then\nn\n6\nthereisnoone-to-onecorrespondencebetweenN andN . Thisisobvious,\nn m\nbut like many obvious things is not all that easy to prove rigorously, and\nwe omit the argument here.\nTheorem 2.6. For each n N, let N be the set N = 0,1,...,n 1 .\nn n\n\u2208 { \u2212 }\nIf n=m, then there is no bijective function from N to N .\nm n\n6\nWe can now make the following definitions:\nDefinition 2.3. A set A is said to be finite if there is a one-to-one cor-\nrespondence between A and N for some natural number n. We then say\nn\nthat n is the cardinality of A. The notation A is used to indicate the\n| | 116 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS\ncardinalityofA. Thatis, ifAisafiniteset, then A isthenaturalnumber\n| |\nn such that there is a one-to-one correspondence between A and N . A set\nn\nthat is not finite is said to be infinite. That is, a set B is infinite if for\nevery n N, there is no one-to-one correspondence between B and N .\nn\n\u2208\nFortunately, we don\u2019t always have to count every element in a set indi-\nvidually todetermine its cardinality. Consider, for example, theset A B,\n\u00d7\nwhereAandB arefinitesets. Ifwealreadyknow A and B , thenwecan\n| | | |\ndetermine A B by computation, without explicit counting of elements.\n| \u00d7 |\nIn fact, A B = A B . The cardinality of the cross product A B\n| \u00d7 | | |\u00b7| | \u00d7\ncan be computed by multiplying the cardinality of A by the cardinality of\nB. To see why this is true, think of how you might count the elements of\nA B. You could put the elements into piles, where all the ordered pairs\n\u00d7\nin a pile have the same first coordinate. There are as many piles as there\nare elements of A, and each pile contains as many ordered pairs as there\nare elements of B. That is, there are A piles, with B items in each. By\n| | | |\nthe definition of multiplication, the total number of items in all the piles\nis A B . A similar result holds for the cross product of more that two\n| |\u00b7| |\nfinite sets. For example, A B C = A B C .\n| \u00d7 \u00d7 | | |\u00b7| |\u00b7| |\nIt\u2019salsoeasytocompute A B inthecasewhereAandB aredisjoint\n| \u222a |\nfinite sets. (Recall that two sets A and B are said to be disjoint if they\nhave no members in common, that is, if A B = .) Suppose A = n\n\u2229 \u2205 | |\nand B = m. If we wanted to count the elements of A B, we could use\n| | \u222a\nthe n numbers from 0 to n 1 to count the elements of A and then use\n\u2212\nthe m numbers from n to n+m 1 to count the elements of B. This\n\u2212\namounts toaone-to-one correspondencebetweenA B andthesetN .\nn+m\n\u222a\nWe see that A B = n+m. That is, for disjoint finite sets A and B,\n| \u222a |\nA B = A + B .\n| \u222a | | | | |\nWhat about A B, where A and B are not disjoint? We have to\n\u222a\nbe careful not to count the elements of A B twice. After counting the\n\u2229\nelementsofA,thereareonly B A B newelementsinB thatstillneed\n| |\u2212| \u2229 |\nto be counted. So we see that for any two finite sets A and B, A B =\n| \u222a |\nA + B A B .\n| | | |\u2212| \u2229 |\nWhat about the number of subsets of a finite set A? What is the rela-\ntionship between A and P(A)? The answer is provided by the following\n| | | |\ntheorem.\nTheorem 2.7. A finite set with cardinality n has 2n subsets.\nProof. Let P(n) be the statement \u201cAny set with cardinality n has 2n sub-\nsets.\u201d We will use induction to show that P(n) is true for all n N.\n\u2208\nBase case: For n=0, P(n) is the statement that a set with cardinality 2.6. COUNTING PAST INFINITY 117\n0has20 subsets. Theonlysetwith0elementsistheemptyset. Theempty\nset has exactly 1 subset, namely itself. Since 20 =1, P(0) is true.\nInductive case: Let k be an arbitrary element of N, and assume that\nP(k) is true. That is, assume that any set with cardinality k has 2k ele-\nments. (This is the induction hypothesis.) We must show that P(k+1)\nfollows from this assumption. That is, using the assumption that any set\nwith cardinality k has 2k subsets, we must show that any set with cardi-\nnality k+1 has 2k+1 subsets.\nLet A be an arbitrary set with cardinality k+1. We must show that\nP(A) = 2k+1. Since A > 0, A contains at least one element. Let x be\n| | | |\nsome element of A, and let B =Ar x . The cardinality of B is k, so we\nhavebytheinductionhypothesisthat{P}\n(B) =2k. Now,wecandividethe\n| |\nsubsetsofAintotwoclasses: subsetsofAthatdonotcontainxandsubsets\nof A that do contain x. Let Y be the collection of subsets of A that do not\ncontain x, and let X be the collection of subsets of A that do contain x.\nX and Y are disjoint, since it is impossible for a given subset of A both to\ncontain and to not contain x. It follows that P(A) = X Y = X + Y .\n| | | \u222a | | | | |\nNow, a member of Y is a subset of A that does not contain x. But that\nis exactly the same as saying that a member of Y is a subset of B. So Y =\nP(B),whichweknowcontains2k members. AsforX,thereisaone-to-one\ncorrespondence between P(B) and X. Namely, the function f: P(B) X\n\u2192\ndefined by f(C)=C x is a bijective function. (The proof of this is left\nasanexercise.) From\u222a th{ is,} itfollowsthat X = P(B) =2k. Puttingthese\nfacts together, we see that P(A) = X +| | Y =| 2k +| 2k = 2 2k = 2k+1.\n| | | | | | \u00b7\nThis completes the proof that P(k) P(k+1).\n\u2192\nWe have seen that the notation AB represents the set of all functions\nfrom B to A. Suppose A and B are finite, and that A =n and B =m.\n| | | |\nThen AB =nm = A B . (Thisfactisoneofthereasonswhythenotation\n| |\n| |\nAB is reasonable.) One way to see this is to note that there is a one-to-one\n(cid:12) (cid:12)\ncorres(cid:12)pond(cid:12)ence between AB and a cross product A A A, where the\n\u00d7 \u00d7\u00b7\u00b7\u00b7\nnumberoftermsinthecrossproductism. (Thiswillbeshowninoneofthe\nexercisesattheendofthissection.) Itfollowsthat AB = A A A =\n| |\u00b7| |\u00b7\u00b7\u00b7| |\nn n n, where the factor n occurs m times in the product. This product\n\u00b7 \u00b7\u00b7\u00b7 (cid:12) (cid:12)\nis, by definition, nm. (cid:12) (cid:12)\nThis discussion about computing cardinalities is summarized in the fol-\nlowing theorem:\nTheorem 2.8. Let A and B be finite sets. Then\nA B = A B .\n\u2022 | \u00d7 | | |\u00b7| |\nA B = A + B A B .\n\u2022 | \u222a | | | | |\u2212| \u2229 | 118 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS\nIf A and B are disjoint then A B = A + B .\n\u2022 | \u222a | | | | |\nAB = A B .\n| |\n\u2022 | |\n(cid:12) (cid:12)\n(cid:12)P(A(cid:12)) =2 |A |.\n\u2022 | |\nWhen it comes to counting and computing cardinalities, this theorem\nisonlythebeginningofthestory. Thereisanentirelargeanddeepbranch\nof mathematics known as combinatorics that is devoted mostly to the\nproblem of counting. But the theorem is already enough to answer many\nquestions about cardinalities.\nFor example, suppose that A = n and B = m. We can form the set\nP(A B),whichconsistsofalls| ub| setsofA | B| . Usingthetheorem,wecan\ncomp\u00d7 ute that P(A B) = 2A B = 2A \u00d7 B = 2nm. If we assume that A\n| \u00d7 | | |\u00b7| |\n| \u00d7 |\nand B are disjoint, then we can compute that AA B = A A B =nn+m.\n\u222a | \u222a |\n| |\nTo be more concrete, let X = a,b,c,d,e and let Y = c,d,e,f\n{ (cid:12)} (cid:12) { }\nwhere a, b, c, d, e, and f are distinct. Then(cid:12)X Y(cid:12) = 5 4 = 20 while\n| \u00d7 | \u00b7\nX Y =5+4 c,d,e =6 and XY =54 =625.\n| \u222a | \u2212|{ }|\nWe can also answer some simple practical questions. Suppose that in a\n(cid:12) (cid:12)\nrestaurant you can choose one appeti(cid:12)zer a(cid:12)nd one main course. What is the\nnumberofpossiblemeals? IfAisthesetofpossibleappetizersandC isthe\nset of possible main courses, then your meal is an ordered pair belonging\nto the set A C. The number of possible meals is A C , which is the\n\u00d7 | \u00d7 |\nproduct of the number of appetizers and the number of main courses.\nOrsupposethatfourdifferentprizesaretobeawarded,andthattheset\nof people who are eligible for the prizes is A. Suppose that A = n. How\n| |\nmany different ways are there to award the prizes? One way to answer\nthis question is to view a way of awarding the prizes as a function from\nthe set of prizes to the set of people. Then, if P is the set of prizes, the\nnumber of different ways of awarding the prizes is AP . Since P =4 and\n| |\nA = n, this is n4. Another way to look at it is to note that the people\n| | (cid:12) (cid:12)\nwho win the prizes form an ordered tuple (a,b,c,d)(cid:12), wh(cid:12)ich is an element of\nA A A A. So the number of different ways of awarding the prizes is\n\u00d7 \u00d7 \u00d7\nA A A A, which is A A A A. This is A4, or n4, the same\n| \u00d7 \u00d7 \u00d7 | | |\u00b7| |\u00b7| |\u00b7| | | |\nanswer we got before.5\n5This discussion assumes that one person can receive any number of prizes. What\nif the prizes have to go to four different people? This question takes us a little farther\nintocombinatoricsthanIwouldliketogo,buttheanswerisnothard. Thefirstaward\ncan be given to any of n people. The second prize goes to one of the remaining n\u22121\npeople. There are n\u22122 choices for the third prize and n\u22123 for the fourth. The\nnumber of different ways of awarding the prizes to four different people is the product\nn(n\u22121)(n\u22122)(n\u22123). 2.6. COUNTING PAST INFINITY 119\nSo far, we have only discussed finite sets. N, the set of natural numbers\n0,1,2,3,... , is an example of an infinite set. There is no one-to-one\n{ }\ncorrespondence between N and any of the finite sets N . Another example\nn\nof an infinite set is the set of even natural numbers, E = 0,2,4,6,8,... .\n{ }\nThere is a natural sense in which the sets N and E have the same number\nof elements. That is, there is a one-to-one correspondence between them.\nThe function f: N E defined by f(n)=2n is bijective. We will say that\n\u2192\nN and E have the same cardinality, even though that cardinality is not a\nfinitenumber. NotethatE isapropersubsetofN. Thatis,Nhasaproper\nsubset that has the same cardinality as N.\nWe will see that not all infinite sets have the same cardinality. When\nit comes to infinite sets, intuition is not always a good guide. Most people\nseemtobetornbetweentwoconflictingideas. Ontheonehand,theythink,\nit seems that a proper subset of a set should have fewer elements than the\nsetitself. Ontheotherhand,itseemsthatanytwoinfinitesetsshouldhave\nthe same number of elements. Neither of these is true, at least if we define\nhavingthesamenumberofelementsintermsofone-to-onecorrespondence.\nA set A is said to be countably infinite if there is a one-to-one corre-\nspondence between N and A. A set is said to be countable if it is either\nfinite or countably infinite. An infinite set that is not countably infinite\nis said to be uncountable. If X is an uncountable set, then there is no\none-to-one correspondence between N and X.\nThe idea of \u201ccountable infinity\u201d is that even though a countably infi-\nnite set cannot be counted in a finite time, we can imagine counting all\nthe elements of A, one-by-one, in an infinite process. A bijective func-\ntionf: N Aprovidessuchaninfinitelisting: (f(0),f(1),f(2),f(3),...).\n\u2192\nSince f is onto, this infinite list includes all the elements of A. In fact,\nmaking such a list effectively shows that A is countably infinite, since the\nlist amounts to a bijective function from N to A. For an uncountable set,\nit is impossible to make a list, even an infinite list, that contains all the\nelements of the set.\nBefore you start believing in uncountable sets, you should ask for an\nexample. InChapter1,weworkedwiththeinfinitesetsZ(theintegers),Q\n(therationals), R(thereals), andRrQ(theirrationals). Intuitively, these\nare all \u201cbigger\u201d than N, but as we have already mentioned, intuition is a\npoor guide when it comes to infinite sets. Are any of Z, Q, R, and RrQ\nin fact uncountable?\nIt turns out that both Z and Q are only countably infinite. The proof\nthat Z is countable is left as an exercise; we will show here that the set\nof non-negative rational numbers is countable. (The fact that Q itself is\ncountablefollowseasilyfromthis.) Thereasonisthatit\u2019spossibletomake 120 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS\nan infinite list containing all the non-negative rational numbers. Start the\nlist with all the non-negative rational numbers n\/m such that n+m = 1.\nThere is only one such number, namely 0\/1. Next come numbers with\nn+m = 2. They are 0\/2 and 1\/1, but we leave out 0\/2 since it\u2019s just\nanother way of writing 0\/1, which is already in the list. Now, we add the\nnumbers with n+m = 3, namely 0\/3, 1\/2, and 2\/1. Again, we leave out\n0\/3, since it\u2019s equal to a number already in the list. Next come numbers\nwith n+m=4. Leaving out 0\/4and 2\/2since they arealready in thelist,\nwe add 1\/3 and 3\/1 to the list. We continue in this way, adding numbers\nwith n+m = 5, then numbers with n+m = 6, and so on. The list looks\nlike\n0 1 1 2 1 3 1 2 3 4 1 5 1 2\n, , , , , , , , , , , , , ,...\n1 1 2 1 3 1 4 3 2 1 5 1 6 5\n(cid:18) (cid:19)\nThis process can be continued indefinitely, and every non-negative rational\nnumberwilleventuallyshowupinthelist. Sowegetacomplete,infinitelist\nof non-negative rational numbers. This shows that the set of non-negative\nrational numbers is in fact countable.\nOn the other hand, R is uncountable. It is not possible to make an\ninfinitelistthatcontainseveryrealnumber. Itisnotevenpossibletomake\na list that contains every real number between zero and one. Another way\nof saying this is that every infinite list of real numbers between zero and\none,nomatterhowitisconstructed,leavessomethingout. Toseewhythis\nis true, imagine such a list, displayed in an infinitely long column. Each\nrow contains one number, which has an infinite number of digits after the\ndecimal point. Since it is a number between zero and one, the only digit\nbefore the decimal point is zero. For example, the list might look like this:\n0.90398937249879561297927654857945...\n0.12349342094059875980239230834549...\n0.22400043298436234709323279989579...\n0.50000000000000000000000000000000...\n0.77743449234234876990120909480009...\n0.77755555588888889498888980000111...\n0.12345678888888888888888800000000...\n0.34835440009848712712123940320577...\n0.93473244447900498340999990948900...\n.\n.\n.\nThis is only (a small part of) one possible list. How can we be certain that\nevery such list leaves out some real number between zero and one? The\ntrick is to look at the digits shown in bold face. We can use these digits to 2.6. COUNTING PAST INFINITY 121\nbuild a number that is not in the list. Since the first number in the list has\na 9 in the first position after the decimal point, we know that this number\ncannotequalanynumberof,forexample,theform0.4.... Sincethesecond\nnumber has a 2 in the second position after the decimal point, neither of\nthe first two numbers in the list is equal to any number that begins with\n0.44.... Since the third number has a 4 in the third position after the\ndecimal point, none of the first three numbers in the list is equal to any\nnumber that begins 0.445.... We can continue to construct a number in\nthisway, andweendupwithanumberthatisdifferentfromeverynumber\nin the list. The nth digit of the number we are building must differ from\nthe nth digit of the nth number in the list. These are the digits shown in\nboldfaceintheabovelist. Tobedefinite,Iusea5whenthecorresponding\nboldfacenumberis4,andotherwiseIusea4. Forthelistshownabove,this\ngivesanumberthatbegins0.44544445.... Thenumberconstructedinthis\nwayisnotinthegivenlist,sothelistisincomplete. Thesameconstruction\nclearly works for any list of real numbers between zero and one. No such\nlist can be a complete listing of the real numbers between zero and one,\nand so there can be no complete listing of all real numbers. We conclude\nthat the set R is uncountable.\nThe technique used in this argument is called diagonalization. It is\nnamed after the fact that the bold face digits in the above list lie along a\ndiagonal line. This proof was discovered by a mathematician named Georg\nCantor,whocausedquiteafussinthenineteenthcenturywhenhecameup\nwiththeideathattherearedifferentkindsofinfinity. Sincethen,hisnotion\nofusingone-to-onecorrespondencetodefinethecardinalitiesofinfinitesets\nhasbeenaccepted. MathematiciansnowconsideritalmostintuitivethatN,\nZ,andQhavethesamecardinalitywhileRhasastrictlylargercardinality.\nTheorem 2.9. Suppose that X is an uncountable set, and that K is a\ncountable subset of X. Then the set X rK is uncountable.\nProof. Let X be an uncountable set. Let K X, and suppose that K\n\u2286\nis countable. Let L = X rK. We want to show that L is uncountable.\nSuppose that L is countable. We will show that this assumption leads to a\ncontradiction.\nNote that X = K (X rK) = K L. You will show in Exercise 11\n\u222a \u222a\nof this section that the union of two countable sets is countable. Since X\nis the union of the countable sets K and L, it follows that X is countable.\nBut this contradicts the fact that X is uncountable. This contradiction\nproves the theorem.\nIn the proof, both q and q are shown to follow from the assumptions,\n\u00ac 122 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS\nwhere q is the statement \u201cX is countable.\u201d The statement q is shown to\nfollow from the assumption that XrK is countable. The statement q is\n\u00ac\ntrue by assumption. Since q and q cannot both be true, at least one of\n\u00ac\nthe assumptions must be false. The only assumption that can be false is\nthe assumption that XrK is countable.\nThis theorem, by the way, has the following easy corollary. (A corol-\nlary is a theorem that follows easily from another, previously proved the-\norem.)\nCorollary 2.10. The set of irrational real numbers is uncountable.\nProof. LetI bethesetofirrationalrealnumbers. Bydefinition,I =RrQ.\nWe have already shown that R is uncountable and that Q is countable, so\nthe result follows immediately from the previous theorem.\nYou might still think that R is as big as things get, that is, that any\ninfinite set is in one-to-one correspondence with R or with some subset\nofR. Infact,though,ifX isanysetthenit\u2019spossibletofindasetthathas\nstrictly larger cardinality than X. In fact, P(X) is such a set. A variation\nof the diagonalization technique can be used to show that there is no one-\nto-one correspondence between X and P(X). Note that this is obvious for\nfinite sets, since for a finite set X, P(X) =2X , which is larger than X .\n| |\n| | | |\nThe point of the theorem is that it is true even for infinite sets.\nTheorem 2.11. Let X be any set. Then there is no one-to-one correspon-\ndence between X and P(X).\nProof. Given an arbitrary function f: X P(X), we can show that f is\n\u2192\nnot onto. Since a one-to-one correspondence is both one-to-one and onto,\nthis shows that f is not a one-to-one correspondence.\nRecall that P(X) is the set of subsets of X. So, for each x X, f(x) is\n\u2208\na subset of X. We have to show that no matter how f is defined, there is\nsome subset of X that is not in the image of f.\nGiven f, we define A to be the set A = x X x f(x) . The test\n{ \u2208 | 6\u2208 }\n\u201cx f(x)\u201d makes sense because f(x) is a set. Since A X, we have that\nA\n6\u2208P(X).\nHowever, A is not in the image of f. That\ni\u2286\ns, for every y X,\n\u2208 \u2208\nA = f(y).6 To see why this is true, let y be any element of X. There are\n6\ntwocasestoconsider. Eithery f(y)ory f(y). Weshowthatwhichever\n\u2208 6\u2208\ncaseholds,A=f(y). Ifitistruethaty f(y),thenbythedefinitionofA,\n6 \u2208\ny A. Sincey f(y)buty A,f(y)andAdonothavethesameelements\n6\u2208 \u2208 6\u2208\nand therefore are not equal. On the other hand, suppose that y f(y).\n6\u2208\n6In fact, we have constructed A so that the sets A and f(y) differ in at least one\nelement,namelyy itself. Thisiswherethe\u201cdiagonalization\u201dcomesin. 2.6. COUNTING PAST INFINITY 123\nAgain, by the definition of A, this implies that y A. Since y f(y)\n\u2208 6\u2208\nbut y A, f(y) and A do not have the same elements and therefore are\n\u2208\nnot equal. In either case, A = f(y). Since this is true for any y X, we\n6 \u2208\nconcludethatAisnotintheimageoff andthereforef isnotaone-to-one\ncorrespondence.\nFromthistheorem,itfollowsthatthereisnoone-to-onecorrespondence\nbetween R and P(R). The cardinality of P(R) is strictly bigger than the\ncardinality of R. But it doesn\u2019t stop there. P(P(R)) has an even bigger\ncardinality, and the cardinality of P(P(P(R))) is bigger still. We could\ngo on like this forever, and we still won\u2019t have exhausted all the possible\ncardinalities. If we let X be the infinite union R P(R) P(P(R)) ,\n\u222a \u222a \u222a\u00b7\u00b7\u00b7\nthen X has larger cardinality than any of the sets in the union. And then\nthere\u2019s P(X), P(P(X)), X P(X) P(P(X)) . There is no end to this.\n\u222a \u222a \u222a\u00b7\u00b7\u00b7\nThere is no upper limit on possible cardinalities, not even an infinite one!\nWe have counted past infinity.\nWe have seen that R is strictly larger than N. We end this section\n| | | |\nwith what might look like a simple question: Is there a subset of R that\nis neither in one-to-one correspondence with N nor with R? That is, is\nthe cardinality of R the next largest cardinality after the cardinality of N,\nor are there other cardinalities intermediate between them? This problem\nwasunsolvedforquiteawhile,andthesolution,whenitwasfound,proved\nto be completely unexpected. It was shown that both \u201cyes\u201d and \u201cno\u201d are\nconsistent answers to this question! That is, the logical structure built on\nthe system of axioms that had been accepted as the basis of set theory was\nnot extensive enough to answer the question. It is possible to extend the\nsystem in various ways. In some extensions, the answer is yes. In others,\nthe answer is no. You might object, \u201cYes, but which answer is true for the\nreal realnumbers?\u201d Unfortunately,it\u2019snotevenclearwhetherthisquestion\nmakes sense, since in the world of mathematics, the real numbers are just\npart of a structure built from a system of axioms. And it\u2019s not at all clear\nwhetherthe\u201crealnumbers\u201dexistinsomesenseintherealworld. Ifallthis\nsounds like it\u2019s a bit of a philosophical muddle, it is. That\u2019s the state of\nthings today at the foundation of mathematics.\nExercises\n1. SupposethatA,B,andC arefinitesetswhicharepairwisedisjoint. (Thatis,\nA B =A C =B C = .) Expressthecardinalityofeachofthefollowing\n\u2229 \u2229 \u2229 \u2205\nsets in terms of A, B , and C . Which of your answers depend on the fact\n| | | | | |\nthat the sets are pairwise disjoint?\na) P(A B) b) A (BC) c) P(A) P(C)\n\u222a \u00d7 \u00d7 124 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS\nd) AB\u00d7C e) (A B)C f) P(AB)\ng) (A B)C h) (A\u00d7 B) A i) A A B B\n\u222a \u222a \u00d7 \u00d7 \u00d7 \u00d7\n2. SupposethatAandB arefinitesetswhicharenotnecessarilydisjoint. What\nare all the possible values for A B ?\n| \u222a |\n3. Let\u2019s say that an \u201cidentifier\u201d consists of one or two characters. The fist\ncharacterisoneofthetwenty-sixletters(A,B,...,C).Thesecondcharacter,\nif there is one, is either a letter or one of the ten digits (0, 1, ..., 9). How\nmany different identifiers are there? Explain your answer in terms of unions\nand cross products.\n4. Supposethattherearefivebooksthatyoumightbringalongtoreadonyour\nvacation. In how many different ways can you decide which books to bring,\nassuming that you want to bring at least one? Why?\n5. Show that the cardinality of a finite set is well-defined. That is, show that if\nf is a bijective function from a set A to N n, and if g is a bijective function\nfrom A to N m, then n=m.\n6. FinishtheproofofTheorem2.7byprovingthefollowingstatement: LetAbe\nanon-emptyset,andletx A. LetB =Ar x . LetX = C A x C .\nDefine f: P(B) X by t\u2208 he formula f(C) ={ } C x . S{ how\u2286 tha| t f\u2208 is} a\n\u2192 \u222a{ }\nbijective function.\n7. Use induction on the cardinality of B to show that for any finite sets A and\nB, AB = A|B|. (Hint: ForthecasewhereB = ,choosex B,anddivide\n| | 6 \u2205 \u2208\nAB(cid:12)into(cid:12)classes according to the value of f(x).)\n(cid:12) (cid:12)\n8. LetAandBbefinitesetswith A =nand B =m. ListtheelementsofBas\nB = b0,b1,...,b m\u22121 . Define| th| efunction|F:| AB A A A,whereA\noccur{ s m times in the} cross product, by F(f)= f\u2192 (b0),\u00d7 f(b\u00d7 1)\u00b7 ,\u00b7 .\u00b7 .\u00d7 ., f(b m\u22121) .\nShow that F is a one-to-one correspondence. (cid:0) (cid:1)\n9. Show that Z, the set of integers, is countable by finding a one-to-one corre-\nspondence between N and Z.\n10. Show that the set N N is countable.\n\u00d7\n11. Complete the proof of Theorem 2.9 as follows:\na) Suppose that A and B are countably infinite sets. Show that A B is\n\u222a\ncountably infinite.\nb) SupposethatAandBarecountablesets. ShowthatA Biscountable.\n\u222a\n12. Prove that each of the following statements is true. In each case, use a proof\nby contradiction.\na) Let X be a countably infinite set, and let N be a finite subset of X.\nThen XrN is countably infinite.\nb) LetAbeaninfiniteset,andletX beasubsetofA. Thenatleastone\nof the sets X and ArX is infinite.\nc) Every subset of a finite set is finite.\n13. LetAandB besetsandlet beanentitythatisnot amemberofB. Show\n\u22a5\nthat there is a one-to-one correspondence between the set of functions from 2.7. RELATIONS 125\nAtoB andthesetofpartialfunctionsfromAtoB. (Partialfunctions\n\u222a{\u22a5}\nweredefinedinSection2.5. Thesymbol\u201c \u201dissometimesusedintheoretical\n\u22a5\ncomputer science to represent the value \u201cundefined.\u201d)\n2.7 Relations\nInSection2.4,wesawthat\u201cmotherof\u201disafunctionalrelationshipbecause\nevery person has one and only one mother, but that \u201cchild of\u201d is not a\nfunctionalrelationship,becauseapersoncanhavenochildrenormorethan\nonechild. However,therelationshipexpressedby\u201cchildof\u201discertainlyone\nthat we have a right to be interested in and one that we should be able to\ndeal with mathematically.\nThere are many examples of relationships that are not functional re-\nlationships. The relationship that holds between two natural numbers n\nand m when n m is an example in mathematics. The relationship be-\n\u2264\ntween a person and a book that that person has on loan from the library\nis another. Some relationships involve more than two entities, such as the\nrelationship that associates a name, an address, and a phone number in an\naddressbookortherelationshipthatholdsamongthreerealnumbersx, y,\nand z if x2+y2+z2 = 1. Each of these relationships can be represented\nmathematically by what is called a \u201crelation.\u201d\nA relation on two sets, A and B, is defined to be a subset of A B.\n\u00d7\nSinceafunctionfromAtoB isdefined, formally, asasubsetofA B that\n\u00d7\nsatisfies certain properties, a function is a relation. However, relations are\nmore general than functions, since any subset of A B is a relation. We\n\u00d7\nalso define a relation among three or more sets to be a subset of the cross\nproduct of those sets. In particular, a relation on A, B, and C is a subset\nof A B C.\n\u00d7 \u00d7\nFor example, if P is the set of people and B is the set of books owned\nby a library, then we can define a relation R on the sets P and B to be the\nset R = (p,b) P B p has b out on loan . The fact that a particular\n(p,b)\nR{\nis a\nf\u2208\nact\na\u00d7 bout|\nthe world that the\nl}\nibrary will certainly want to\n\u2208\nkeep track of. When a collection of facts about the world is stored on a\ncomputer,itiscalledadatabase. We\u2019llseeinthenextsectionthatrelations\nare the most common means of representing data in databases.\nIf A is a set and R is a relation on the sets A and A (that is, on two\ncopies of A), then R is said to be a binary relation on A. That is, a\nbinary relation on the set A is a subset of A A. The relation consisting\n\u00d7\nof all ordered pairs (c,p) of people such that c is a child of p is a binary\nrelation on the set of people. The set (n,m) N N n m is a binary\n{ \u2208 \u00d7 | \u2264 }\nrelation on N. Similarly, we define a ternary relation on a set A to be a 126 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS\nsubset of A A A. The set (x,y,z) R R R x2+y2+z2 = 1 is\n\u00d7 \u00d7 { \u2208 \u00d7 \u00d7 | }\na ternary relation on R. For complete generality, we can define an n-ary\nrelationonA,foranypositiveintegern, tobeasubsetofA A A,\n\u00d7 \u00d7\u00b7\u00b7\u00b7\u00d7\nwhere A occurs n times in the cross product.\nFor the rest of this section, we will be working exclusively with binary\nrelations. Suppose that R A A. That is, suppose that R is a binary\nrelation on a set A. If (a,b\u2286 ) R\u00d7 , then we say that a is related to b by R.\nInstead of writing \u201c(a,b) R\u2208 \u201d, we will often write \u201caRb\u201d. This notation\n\u2208\nis used in analogy to the notation n m to express the relation that n is\nless than or equal to m. Remember t\u2264 hat aRb is just an alternative way of\nwriting (a,b) R. In fact, we could consider the relation to be a set of\n\u2208 \u2264\nordered pairs and write (n,m) in place of the notation n m.\n\u2208\u2264 \u2264\nIn many applications, attention is restricted to relations that satisfy\nsome property or set of properties. (This is, of course, just what we do\nwhen we study functions.) We begin our discussion of binary relations by\nconsidering several important properties. In this discussion, let A be a set\nand let R be a binary relation on A, that is, a subset of A A.\n\u00d7\nR is said to be reflexive if a A(aRa). That is, a binary relation\n\u2200 \u2208\non a set is reflexive if every element of the set is related to itself. This is\ntrue, for example, for the relation on the set N, since n n for every\n\u2264 \u2264\nn N. On the other hand, it is not true for the relation < on N, since, for\n\u2208\nexample, the statement 17<17 is false.7\nR is called transitive if a A, b A, c A (aRb bRc)\n\u2200 \u2208 \u2200 \u2208 \u2200 \u2208 \u2227 \u2192\n(aRc) . Transitivity allows us to \u201cchain together\u201d two true statements\n(cid:0)\naRb and bRc, which are \u201clinked\u201d by the b that occurs in each statement,\nto ded(cid:1) uce that aRc. For example, suppose P is the set of people, and\ndefine the relation C on P such that xPy if and only if x is a child of y.\nThe relation P is not transitive because the child of a child of a person\nis not a child of that person. Suppose, on the other hand, that we define\na relation D on P such that xDy if and only if x is a descendent of y.\nThen D is a transitive relation on the set of people, since a descendent of\na descendent of a person is a descendent of that person. That is, from the\nfactsthatElizabethisadescendentofVictoriaandVictoriaisadescendent\nof James, we can deduce that Elizabeth is a descendent of James. In the\nmathematicalworld,therelations and<onthesetNarebothtransitive.\n\u2264\nR is said to be symmetric if a A, b B(aRb bRa). That is,\n\u2200 \u2208 \u2200 \u2208 \u2192\nwhenever a is related to b, it follows that b is related to a. The relation \u201cis\n7NotethattoshowthattherelationRisnotreflexive,youonlyneedtofindoneasuch\nthataRaisfalse. Thisfollowsfromthefactthat\u00ac(cid:0)\u2200a\u2208A(aRa)(cid:1)\u2261\u2203a\u2208A(cid:0)\u00ac(aRa)(cid:1).\nAsimilarremarkholdsforeachofthepropertiesofrelationsthatarediscussedhere. 2.7. RELATIONS 127\na first cousin of\u201d on the set of people is symmetric, since whenever x is a\nfirst cousin of y, we have automatically that y is a first cousin of x. On the\notherhand,the\u201cchildof\u201drelationiscertainlynotsymmetric. Therelation\non N is not symmetric. From the fact that n m, we cannot conclude\n\u2264 \u2264\nthat m n. It is true for some n and m in N that n m m n, but\n\u2264 \u2264 \u2192 \u2264\nit is not true for all n and m in N.\nFinally,Risantisymmetricif a A, b B (aRb bRa) a=b .\nThe relation R is antisymmetric if f\u2200 or\u2208 any t\u2200 wo\u2208 distinct ele\u2227 ments x\u2192 and y of\nA,wecan\u2019thavebothxRyandyRx. Therelation(cid:0) onNisantisymmetr(cid:1) ic\n\u2264\nbecause from the facts that n m and m n, we can deduce that n=m.\n\u2264 \u2264\nThe relation \u201cchild of\u201d on the set of people is antisymmetric since it\u2019s\nimpossible to have both that x is a child of y and y is a child of x.\nThere are a few combinations of properties that define particularly use-\nful types of binary relations. The relation on the set N is reflexive,\n\u2264\nantisymmetric, and transitive. These properties define what is called a\npartial order: A partial order on a set A is a binary relation on A that is\nreflexive, antisymmetric, and transitive.\nAnother example of a partial order is the subset relation, , on the\npower set of any set. If X is a set, then of course P(X) is a set\u2286 in its own\nright, and can be considered to be a binary relation on this set. Two\nelements A\u2286 and B of P(X) are related by if and only if A B. This\n\u2286 \u2286\nrelation is reflexive since every set is a subset of itself. The fact that it is\nantisymmetric follows from Theorem 2.1. The fact that it is transitive was\nExercise 11 in Section 2.1.\nThe ordering imposed on N by has one important property that the\n\u2264\nordering of subsets by does not share. If n and m are natural numbers,\n\u2286\nthen at least one of the statements n m and m n must be true.\n\u2264 \u2264\nHowever, if A and B are subsets of a set X, it is certainly possible that\nboth A B and B A are false. A binary relation R on a set A is said\n\u2286 \u2286\nto be a total order if it is a partial order and furthermore for any two\nelements a and b of A, either aRb or bRa. The relation on the set N is\natotal order. Therelation onP(X) isnot. (Note once\u2264 again theslightly\n\u2286\nodd mathematical language: A total order is a kind of partial order\u2014not,\nas you might expect, the opposite of a partial order.)\nFor another example of ordering, let L be the set of strings that can be\nmade from lowercase letters. L contains both English words and nonsense\nstrings such as \u201csxjja\u201d. There is a commonly used total order on the set L,\nnamely alphabetical order.\nWe\u2019ll approach another important kind of binary relation indirectly,\nthrough what might at first appear to be an unrelated idea. Let A be a 128 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS\nset. A partition of A is defined to be a collection of non-empty subsets\nof A such that each pair of distinct subsets in the collection is disjoint and\nthe union of all the subsets in the collection is A. A partition of A is just\na division of all the elements of A into non-overlapping subsets. For exam-\nple, the sets 1,2,6 , 3,7 , 4,5,8,10 , and 9 form a partition of the\n{ } { } { } { }\nset 1,2,...,10 . Each element of 1,2,...,10 occurs in exactly one of\n{ } { }\nthe sets that make up the partition. As another example, we can partition\nthe set of all people into two sets, the set of males and the set of females.\nBiologists try to partition the set of all organisms into different species.\nLibrarians try to partition books into various categories such as fiction, bi-\nography, and poetry. In the real world, classifying things into categories is\nanessentialactivity,althoughtheboundariesbetweencategoriesarenotal-\nwayswell-defined. Theabstractmathematicalnotionofapartitionofaset\nmodels the real-world notion of classification. In the mathematical world,\nthough, the categories are sets and the boundary between two categories is\nsharp.\nIntherealworld, itemsareclassifiedinthesamecategorybecausethey\nare related in some way. This leads us from partitions back to relations.\nSuppose that we have a partition of a set A. We can define a relation R\non A by declaring that for any a and b in A, aRb if and only if a and b\nare members of the same subset in the partition. That is, two elements of\nA are related if they are in the same category. It is clear that the relation\ndefined in this way is reflexive, symmetric, and transitive.\nAn equivalence relation is defined to be a binary relation that is\nreflexive, symmetric, and transitive. Any relation defined, as above, from\na partition is an equivalence relation. Conversely, we can show that any\nequivalence relation defines a partition. Suppose that R is an equivalence\nrelation on a set A. Let a A. We define the equivalence class of a\nunder the equivalence relatio\u2208 n R to be the subset [a]R defined as [a]R =\nb A bRa . That is, the equivalence class of a is the set of all elements\n{ \u2208 | }\nof A that are related to a. In most cases, we\u2019ll assume that the relation in\nquestion is understood, and we\u2019ll write [a] instead of [a]R. Note that each\nequivalence class is a subset of A. The following theorem shows that the\ncollection of equivalence classes form a partition of A.\nTheorem 2.12. Let A be a set and let R be an equivalence relation on A.\nThen the collection of all equivalence classes under R is a partition of A.\nProof. To show that a collection of subsets of A is a partition, we must\nshow that each subset is non-empty, that the intersection of two distinct\nsubsets is empty, and that the union of all the subsets is A.\nIf [a] is one of the equivalence classes, it is certainly non-empty, since 2.7. RELATIONS 129\na [a]. (This follows from the fact that R is reflexive, and hence aRa.)\n\u2208\nTo show that A is the union of all the equivalence classes, we just have to\nshow that each element of A is a member of one of the equivalence classes.\nAgain, the fact that a [a] for each a A shows that this is true.\n\u2208 \u2208\nFinally,wehavetoshowthattheintersectionoftwodistinctequivalence\nclasses is empty. Suppose that a and b are elements of A and consider the\nequivalence classes [a] and [b]. We have to show that if [a] = [b], then\n6\n[a] [b] = . Equivalently, we can show the converse: If [a] [b] = then\n\u2229 \u2205 \u2229 6 \u2205\n[a]=[b]. So, assume that [a] [b]= . Saying that a set is not empty just\n\u2229 6 \u2205\nmeans that the set contains some element, so there must be an x A such\nthat x [a] [b]. Since x [a], xRa. Since R is symmetric, we a\u2208 lso have\naRx. S\u2208 ince\u2229 x [b], xRb.\u2208 Since R is transitive and since (aRx) (xRb),\nit follows that\u2208 aRb. \u2227\nOur object is to deduce that [a] = [b]. Since [a] and [b] are sets, they\nare equal if and only if [a] [b] and [b] [a]. To show that [a] [b], let c\n\u2286 \u2286 \u2286\nbe an arbitrary element of [a]. We must show that c [b]. Since c [a],\nwe have that cRa. And we have already shown that\u2208 aRb. From\u2208 these\ntwo facts and the transitivity of R, it follows that cRb. By definition, this\nmeans that c [b]. We have shown that any member of [a] is a member of\n\u2208\n[b] and therefore that [a] [b]. The fact that [b] [a] can be shown in the\n\u2286 \u2286\nsame way. We deduce that [a]=[b], which proves the theorem.\nThe point of this theorem is that if we can find a binary relation that\nsatisfiescertainproperties,namelythepropertiesofanequivalencerelation,\nthen we can classify things into categories, where the categories are the\nequivalence classes.\nFor example, suppose that U is a possibly infinite set. Define a binary\nrelation on P(U) as follows: For X and Y in P(U), X Y if and\n\u223c \u223c\nonly if there is a bijective function from the set X to the set Y. In other\nwords, X Y means that X and Y have the same cardinality. Then\nis an eq\u223c uivalence relation on P(U). (The symbol is often used to\n\u223c \u223c\ndenote equivalence relations. It is usually read \u201cis equivalent to.\u201d) If X\nP(U), then the equivalence class [X] consists of all the subsets of U tha\u2208 t\n\u223c\nhave the same cardinality as X. We have classified all the subsets of U\naccording to their cardinality\u2014even though we have never said what an\ninfinite cardinality is. (We have only said what it means to have the same\ncardinality.)\nYoumightrememberapopularpuzzlecalledRubic\u2019sCube,acubemade\nof smaller cubes with colored sides that could be manipulated by twisting\nlayers of little cubes. The object was to manipulate the cube so that the\ncolors of the little cubes formed a certain configuration. Define two con- 130 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS\nfigurations of the cube to be equivalent if it\u2019s possible to manipulate one\nconfiguration into the other by a sequence of twists. This is, in fact, an\nequivalence relation on the set of possible configurations. (Symmetry fol-\nlowsfromthefactthateachmoveisreversible.) Ithasbeenshownthatthis\nequivalence relation has exactly twelve equivalence classes. The interesting\nfactisthatithasmorethanoneequivalenceclass: Iftheconfigurationthat\nthecubeisinandtheconfigurationthatyouwanttoachievearenotinthe\nsame equivalence class, then you are doomed to failure.\nSuppose that R is a binary relation on a set A. Even though R might\nnotbetransitive,itisalwayspossibletoconstructatransitiverelationfrom\nRinanaturalway. IfwethinkofaRbasmeaningthataisrelatedbyRto\nb \u201cin one step,\u201d then we consider the relationship that holds between two\nelements x and y when x is related by R to y \u201cin one or more steps.\u201d This\nrelationship defines a binary relation on A that is called the transitive\nclosure of R. The transitive closure of R is denoted R . Formally, R is\n\u2217 \u2217\ndefinedasfollows: ForaandbinA,aR bifthereisasequencex ,x ,...x\n\u2217 0 1 n\nof elements of A, where n > 0 and x = a and x = b, such that x Rx ,\n0 n 0 1\nx Rx , ..., and x Rx .\n1 2 n 1 n\nFor example, if a\u2212Rc, cRd, and dRb, then we would have that aR b.\n\u2217\nOf course, we would also have that aR c, and aR d.\n\u2217 \u2217\nFor a practical example, suppose that C is the set of all cities and let\nA be the binary relation on C such that for x and y in C, xAy if there is\na regularly scheduled airline flight from x to y. Then the transitive closure\nA hasanaturalinterpretation: xA y ifit\u2019spossibletogetfromxtoy by\n\u2217 \u2217\na sequence of one or more regularly scheduled airline flights. You\u2019ll find a\nfew more examples of transitive closures in the exercises.\nExercises\n1. For a finite set, it is possible to define a binary relation on the set by listing\ntheelementsoftherelation,consideredasasetoforderedpairs. LetAbethe\nset a,b,c,d ,wherea,b,c,anddaredistinct. Considereachofthefollowing\n{ }\nbinaryrelationsonA. Istherelationreflexive? Symmetric? Antisymmetric?\nTransitive? Is it a partial order? An equivalence relation?\na) R= (a,b), (a,c), (a,d) .\nb) S= { (a,a), (b,b), (c,c),} (d,d), (a,b), (b,a) .\nc)\nT={\n(b,b), (c,c), (d,d) .\n}\nd)\nC={\n(a,b), (b,c),\n(a,c),}\n(d,d) .\ne)\nD={\n(a,b), (b,a), (c,d),\n(d,c)}\n.\n{ }\n2. Let A be the set 1,2,3,4,5,6 . Consider thepartition of A into the subsets\n{ }\n1,4,5 , 3 , and 2,6 . Write out the associated equivalence relation on A\n{ } { } { }\nas a set of ordered pairs. 2.7. RELATIONS 131\n3. Consider each of the following relations on the set of people. Is the relation\nreflexive? Symmetric? Transitive? Is it an equivalence relation?\na) x is related to y if x and y have the same biological parents.\nb) xisrelatedtoyifxandyhaveatleastonebiologicalparentincommon.\nc) x is related to y if x and y were born in the same year.\nd) x is related to y if x is taller than y.\ne) x is related to y if x and y have both visited Honolulu.\n4. It is possible for a relation to be both symmetric and antisymmetric. For\nexample, the equality relation, =, is a relation on any set which is both\nsymmetricandantisymmetric. SupposethatAisasetandRisarelationon\nA that is both symmetric and antisymmetric. Show that R is a subset of =\n(when both relations are considered as sets of ordered pairs). That is, show\nthat for any a and b in A, (aRb) (a=b).\n\u2192\n5. Let be the relation on R, the set of real numbers, such that for x and y in\n\u223c\nR, x y if and only if x y Z. For example, \u221a2 1 \u221a2 +17 because\n\u223c \u2212 \u2208 \u2212 \u223c\nthe difference, (\u221a2 1) (\u221a2 +17), is 18, which is an integer. Show that\n\u2212 \u2212 \u2212\nis an equivalence relation. Show that each equivalence class [x] contains\n\u223c\n\u223c\nexactlyonenumberawhichsatisfies0 a<1. (Thus,thesetofequivalence\n\u2264\nclasses under is in one-to-one correspondence with the half-open interval\n\u223c\n[0,1).)\n6. Let A and B be any sets, and suppose f: A B. Define a relation on B\n\u2192 \u223c\nsuch that for any x and y in A, x y if and only if f(x)=f(y). Show that\n\u223c\nis an equivalence relation on A.\n\u223c\n7. Let Z+ be the set of positive integers 1,2,3,... . Define a binary relation\nDonZ+ suchthatfornandminZ+,{ nDmifn} dividesevenlyintom,with\nno remainder. Equivalently, nDm if n is a factor of m, that is, if there is a\nk in Z+ such that m=nk. Show that D is a partial order.\n8. ConsiderthesetN N,whichconsistsofallorderedpairsofnaturalnumbers.\n\u00d7\nSince N N is a set, it is possible to have binary relations on N N. Such a\n\u00d7 \u00d7\nrelation would be a subset of (N N) (N N). Define a binary relation\n\u00d7 \u00d7 \u00d7 (cid:22)\non N N such that for (m,n) and (k,\u2113) in N N, (m,n) (k,\u2113) if and only\n\u00d7 \u00d7 (cid:22)\nif either m<k or ((m=k) (n \u2113)). Which of the following are true?\n\u2227 \u2264\na) (2,7) (5,1) b) (8,5) (8,0)\n(cid:22) (cid:22)\nc) (0,1) (0,2) d) (17,17) (17,17)\n(cid:22) (cid:22)\nShow that is a total order on N N.\n(cid:22) \u00d7\n9. Let be the relation defined on N N such that (n,m) (k,\u2113) if and only\n\u223c \u00d7 \u223c\nif n+\u2113=m+k. Show that is an equivalence relation.\n\u223c\n10. Let P be the set of people and let C be the \u201cchild of\u201d relation. That is xCy\nmeans that x is a child of y. What is the meaning of the transitive closure\nC\u2217? Explain your answer.\n11. Let R be the binary relation on N such that xRy if and only if y = x+1.\nIdentifythetransitiveclosureR\u2217. (Itisawell-knownrelation.) Explainyour\nanswer. 132 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS\n12. Suppose that R is a reflexive, symmetric binary relation on a set A. Show\nthat the transitive closure R\u2217 is an equivalence relation.\n2.8 Application: Relational Databases\nOne of the major uses of computer systems is to store and manipulate\ncollections of data. A database is a collection of data that has been orga-\nnizedsothatitispossibletoaddanddeleteinformation,toupdatethedata\nthat it contains, and to retrieve specified parts of the data. A Database\nManagement System, or DBMS, is a computer program that makes it\npossible to create and manipulate databases. A DBMS must be able to\naccept and process commands that manipulate the data in the databases\nthat it manages. These commands are called queries, and the languages\nin which they are written are called query languages. A query language\nis a kind of specialized programming language.\nTherearemanydifferentwaysthatthedatainadatabasecouldberep-\nresented. Different DBMS\u2019s use various data representations and various\nquery languages. However, data is most commonly stored in relations. A\nrelation in a database is a relation in the mathematical sense. That is, it\nis a subset of a cross product of sets. A database that stores its data in\nrelationsiscalledarelational database. Thequerylanguageformostre-\nlationaldatabasemanagementsystemsissomeformofthelanguageknown\nas Structured Query Language, or SQL. In this section, we\u2019ll take a\nvery brief look at SQL, relational databases, and how they use relations.\nA relation is just a subset of a cross product of sets. Since we are\ndiscussing computer representation of data, the sets are data types. As in\nSection 2.5, we\u2019ll use data type names such as int and string to refer to\nthese sets. A relation that is a subset of the cross product int int string\n\u00d7 \u00d7\nwould consist of ordered 3-tuples such as (17, 42, \u201chike\u201d). In a relational\ndatabase, the data is stored in the form of one or more such relations. The\nrelations are called tables, and the tuples that they contain are called rows\nor records.\nAsanexample,consideralendinglibrarythatwantstostoredataabout\nits members, the books that it owns, and which books the members have\nout on loan. This data could be represented in three tables, as illustrated\nin Figure 2.4. The relations are shown as tables rather than as sets of\nordered tuples, but each table is, in fact, a relation. The rows of the table\narethetuples. TheMemberstable,forexample, isasubsetofint string\n\u00d7 \u00d7\nstring string, and one of the tuples is (1782, \u201cSmith, John\u201d, \u201c107 Main\n\u00d7\nSt\u201d,\u201cNewYork,NY\u201d).Atabledoeshaveonethingthatordinaryrelations 2.8. APPLICATION: RELATIONAL DATABASES 133\nin mathematics do not have. Each column in the table has a name. These\nnamesareusedinthequerylanguagetomanipulatethedatainthetables.\nThe data in the Members table is the basic information that the library\nneeds in order to keep track of its members, namely the name and address\nof each member. A member also has a MemberID number, which is pre-\nsumably assigned by the library. Two different members can\u2019t have the\nsame MemberID, even though they might have the same name or the same\naddress. The MemberID acts as a primary key for the Members table. A\ngiven value of the primary key uniquely identifies one of the rows of the\ntable. Similarly, the BookID in the Books table is a primary key for that\ntable. In the Loans table, which holds information about which books are\nout on loan to which members, a MemberID unambiguously identifies the\nmemberwhohasagivenbookonloan,andtheBookIDsaysunambiguously\nwhich book that is. Every table has a primary key, but the key can consist\nof more than one column. The DBMS enforces the uniqueness of primary\nkeys. Thatis,itwon\u2019tletusersmakeamodificationtothetableifitwould\nresult in two rows having the same primary key.\nThe fact that a relation is a set\u2014a set of tuples\u2014means that it can\u2019t\ncontainthesametuplemorethanonce. Intermsoftables, thismeansthat\na table shouldn\u2019t contain two identical rows. But since no two rows can\ncontain the same primary key, it\u2019s impossible for two rows to be identical.\nSo tables are in fact relations in the mathematical sense.\nThe library must have a way to add and delete members and books\nand to make a record when a book is borrowed or returned. It should\nalso have a way to change the address of a member or the due date of a\nborrowed book. Operations suchas these areperformed usingthe DBMS\u2019s\nquery language. SQL has commands named INSERT, DELETE, and UPDATE\nfor performing these operations. The command for adding Barack Obama\nas a member of the library with MemberID 999 would be\nINSERT INTO Members\nVALUES (999, \"Barack Obama\",\n\"1600 Pennsylvania Ave\", \"Washington, DC\")\nWhen it comes to deleting and modifying rows, things become more inter-\nesting because it\u2019s necessary to specify which row or rows will be affected.\nThis is done by specifying a condition that the rows must fulfill. For ex-\nample, this command will delete the member with ID 4277:\nDELETE FROM Members\nWHERE MemberID = 4277\nIt\u2019s possible for a command to affect multiple rows. For example, 134 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS\nDELETE FROM Members\nWHERE Name = \"Smith, John\"\nwould delete every row in which the name is \u201cSmith, John.\u201d The update\ncommand also specifies what changes are to be made to the row:\nUPDATE Members\nSET Address=\"19 South St\", City=\"Hartford, CT\"\nWHERE MemberID = 4277\nOf course, the library also needs a way of retrieving information from\nthe database. SQL provides the SELECT command for this purpose. For\nexample, the query\nSELECT Name, Address\nFROM Members\nWHERE City = \"New York, NY\"\nasksforthenameandaddressofeverymemberwholivesinNewYorkCity.\nThe last line of the query is a condition that picks out certain rows of the\n\u201cMembers\u201d relation, namely all the rows in which the City is \u201cNew York,\nNY\u201d.Thefirstlinespecifieswhichdatafromthoserowsshouldberetrieved.\nThe data is actually returned in the form of a table. For example, given\nthe data in Figure 2.4, the query would return this table:\nSmith, John 107 Main St\nJones, Mary 1515 Center Ave\nLee, Joseph 90 Park Ave\nO\u2019Neil, Sally 89 Main St\nThe table returned by a SELECT query can even be used to construct more\ncomplex queries. For example, if the table returned by SELECT has only\none column, then it can be used with the IN operator to specify any value\nlisted in that column. The following query will find the BookID of every\nbook that is out on loan to a member who lives in New York City:\nSELECT BookID\nFROM Loans\nWHERE MemberID IN (SELECT MemberID\nFROM Members\nWHERE City = \"New York, NY\")\nMorethanonetablecanbelistedintheFROMpartofaquery. Thetables\nthat are listed are joined into one large table, which is then used for the 2.8. APPLICATION: RELATIONAL DATABASES 135\nquery. The large table is essentially the cross product of the joined tables,\nwhenthetablesareunderstoodassetsoftuples. Forexample,supposethat\nwe want the titles of all the books that are out on loan to members who\nlive in New York City. The titles are in the Books table, while information\nabout loans is in the Loans table. To get the desired data, we can join the\ntables and extract the answer from the joined table:\nSELECT Title\nFROM Books, Loans\nWHERE MemberID IN (SELECT MemberID\nFROM Members\nWHERE City = \"New York, NY\")\nIn fact, we can do the same query without using the nested SELECT. We\nneed one more bit of notation: If two tables have columns that have the\nsame name, the columns can be named unambiguously by combining the\ntable name with the column name. For example, if the Members table and\nLoans table are both under discussion, then the MemberID columns in the\ntwo tables can be referred to as Members.MemberID and Loans.MemberID.\nSo, we can say:\nSELECT Title\nFROM Books, Loans\nWHERE City =\"New York, NY\"\nAND Members.MemberID = Loans.MemberID\nThis is just a sample of what can be done with SQL and relational\ndatabases. The conditions in WHERE clauses can get very complicated, and\nthere are other operations besides the cross product for combining tables.\nThe database operations that are needed to complete a given query can\nbe complex and time-consuming. Before carrying out a query, the DBMS\ntries to optimize it. That is, it manipulates the query into a form that can\nbe carried out most efficiently. The rules for manipulating and simplifying\nqueries form an algebra of relations, and the theoretical study of relational\ndatabases is in large part the study of the algebra of relations.\nExercises\n1. Using the library database from Figure 2.4, what is the result of each of the\nfollowing SQL commands?\na) SELECT Name, Address\nFROM Members 136 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS\nWHERE Name = \"Smith, John\"\nb) DELETE FROM Books\nWHERE Author = \"Isaac Asimov\"\nc) UPDATE Loans\nSET DueDate = \"November 20\"\nWHERE BookID = 221\nd) SELECT Title\nFROM Books, Loans\nWHERE Books.BookID = Loans.BookID\ne) DELETE FROM Loans\nWHERE MemberID IN (SELECT MemberID\nFROM Members\nWHERE Name = \"Lee, Joseph\")\n2. Using the library database from Figure 2.4, write an SQL command to do\neach of the following database manipulations:\na) Find the BookID of every book that is due on November 1, 2010.\nb) ChangetheDueDateofthebookwithBookID221toNovember15,2010.\nc) Change the DueDate of the book with title \u201cSummer Lightning\u201d to\nNovember 14, 2010. Use a nested SELECT.\nd) Findthenameofeverymemberwhohasabookoutonloan. Usejoined\ntables in the FROM clause of a SELECT command.\n3. Suppose that a college wants to use a database to store information about\nitsstudents,thecoursesthatareofferedinagiventerm, andwhichstudents\nare taking which courses. Design tables that could be used in a relational\ndatabase for representing this data. Then write SQL commands to do each\nof the following database manipulations. (You should design your tables so\nthat they can support all these commands.)\na) Enroll the student with ID number 1928882900 in \u201cEnglish 260\u201d.\nb) Remove \u201cJohn Smith\u201d from \u201cBiology 110\u201d.\nc) Remove the student with ID number 2099299001 from every course in\nwhich that student is enrolled.\nd) Find the names and addresses of the students who are taking \u201cCom-\nputer Science 229\u201d.\ne) Cancel the course \u201cHistory 101\u201d. 2.8. APPLICATION: RELATIONAL DATABASES 137\nMembers\nMemberID Name Address City\n1782 Smith, John 107 Main St New York, NY\n2889 Jones, Mary 1515 Center Ave New York, NY\n378 Lee, Joseph 90 Park Ave New York, NY\n4277 Smith, John 2390 River St Newark, NJ\n5704 O\u2019Neil, Sally 89 Main St New York, NY\nBooks\nBookID Title Author\n182 I, Robot Isaac Asimov\n221 The Sound and the Fury William Faulkner\n38 Summer Lightning P.G. Wodehouse\n437 Pride and Prejudice Jane Austen\n598 Left Hand of Darkness Ursula LeGuin\n629 Foundation Trilogy Isaac Asimov\n720 Mirror Dance Lois McMaster Bujold\nLoans\nMemberID BookID DueDate\n378 221 October 8, 2010\n2889 182 November 1, 2010\n4277 221 November 1, 2010\n1782 38 October 30, 2010\nFigure 2.4: Tables that could be part of a relational database. Each\ntable has a name, shown above the table. Each column in the table\nalso has a name, shown in the top row of the table. The remaining\nrows hold the data. 138 CHAPTER 2. SETS, FUNCTIONS, AND RELATIONS Chapter 3\nRegular Expressions and\nFinite-State Automata\nW\nith the set of mathematical tools from the first two chapters,\nwe are now ready to study languages and formal language theory.\nOur intent is to examine the question of how, and which, languages can be\nmechanically generated and recognized; and, ultimately, to see what this\ntells us about what computers can and can\u2019t do.\n3.1 Languages\nIn formal language theory, an alphabet is a finite, non-empty set. The\nelements of the set are called symbols. A finite sequence of symbols\na a ...a from an alphabet is called a string over that alphabet.\n1 2 n\nExample 3.1. \u03a3 = 0,1 is an alphabet, and 011, 1010, and 1 are all\n{ }\nstrings over \u03a3.\nNote that strings really are sequences of symbols, which implies that\norder matters. Thus 011, 101, and 110 are all different strings, though\nthey are made up of the same symbols. The strings x = a a ...a and\n1 2 n\ny = b b ...b are equal only if m = n (i.e. the strings contain the same\n1 2 m\nnumber of symbols) and a =b for all 1 i n.\ni i\n\u2264 \u2264\nJustasthereareoperations definedonnumbers, truthvalues, sets, and\nother mathematical entities, there are operations defined on strings. Some\nimportant operations are:\n139 140 CHAPTER 3. REGULAR EXPRESSIONS AND FSA\u2019S\n1. length: the length of a string x is the number of symbols in it. The\nnotation for the length of x is x. Note that this is consistent with\n| |\nother uses of , all of which involve some notion of size: number\n| | | |\nmeasures how big a number is (in terms of its distance from 0); set\n| |\nmeasures the size of a set (in terms of the number of elements).\nWe will occasionally refer to a length-n string. This is a slightly\nawkward, but concise, shorthand for \u201ca string whose length is n\u201d.\n2. concatenation: theconcatenationoftwostringsx=a a ...a and\n1 2 m\ny = b b ...b is the sequence of symbols a ...a b ...b . Some-\n1 2 n 1 m 1 n\ntimes is used to denote concatenation, but it is far more usual\n\u00b7\nto see the concatenation of x and y denoted by xy than by x y.\n\u00b7\nYoucaneasilyconvinceyourselfthatconcatenationisassociative(i.e.\n(xy)z = x(yz) for all strings x,y and z.) Concatenation is not com-\nmutative(i.e.itisnotalwaystruethatxy =yx: forexample,ifx=a\nand y =b then xy =ab while yx=ba and, as discussed above, these\nstrings are not equal.)\n3. reversal: the reverse of a string x = a a ...a is the string xR =\n1 2 n\na a ...a a .\nn n 1 2 1\n\u2212\nExample 3.2. Let \u03a3 = a,b , x = a, y = abaa, and z = bab. Then\n{ }\nx = 1, y = 4, and z = 3. Also, xx = aa, xy = aabaa, xz = abab, and\n| | | | | |\nzx=baba. Finally, xR =a, yR =aaba, and zR =bab.\nBy the way, the previous example illustrates a naming convention stan-\ndard throughout language theory texts: if a letter is intended to represent\na single symbol in an alphabet, the convention is to use a letter from the\nbeginning of the English alphabet (a, b, c, d ); if a letter is intended to\nrepresent a string, the convention is to use a letter from the end of the\nEnglish alphabet (u, v, etc).\nInsettheory,wehaveaspecialsymboltodesignatethesetthatcontains\nnoelements. Similarly,languagetheoryhasaspecialsymbol\u03b5whichisused\nto represent the empty string, the string with no symbols in it. (Some\ntextsusethesymbol\u03bbinstead.) Itisworthnotingthat \u03b5 =0,that\u03b5R =\u03b5,\n| |\nand that \u03b5 x=x \u03b5=x for all strings x. (This last fact may appear a bit\n\u00b7 \u00b7\nconfusing. Remember that \u03b5 is not a symbol in a string with length 1, but\nrather the name given to the string made up of 0 symbols. Pasting those 0\nsymbols onto the front or back of a string x still produces x.)\nThe set of all strings over an alphabet \u03a3 is denoted \u03a3 . (In language\n\u2217\ntheory,thesymbol istypicallyusedtodenote\u201czeroormore\u201d,so\u03a3 isthe\n\u2217 \u2217 3.1. LANGUAGES 141\nset of strings made up of zero or more symbols from \u03a3.) Note that while\nan alphabet \u03a3 is by definition a finite set of symbols, and strings are by\ndefinition finite sequences of those symbols, the set \u03a3 is always infinite.\n\u2217\nWhyisthis? Suppose\u03a3containsnelements. Thenthereisonestringover\n\u03a3with0symbols,nstringswith1symbol,n2 stringswith2symbols(since\nthere are n choices for the first symbol and n choices for the second), n3\nstrings with 3 symbols, etc.\nExample 3.3. If \u03a3 = 1 , then \u03a3 = \u03b5,1,11,111,... . If \u03a3 = a,b ,\n\u2217\n{ } { } { }\nthen \u03a3 = \u03b5,a,b,aa,ab,ba,bb,aaa,aab,... .\n\u2217\n{ }\nNote that \u03a3 is countably infinite: if we list the strings as in the pre-\n\u2217\nceding example (length-0 strings, length-1 strings in \u201calphabetical\u201d order,\nlength-2 strings similarly ordered, etc) then any string over \u03a3 will eventu-\nally appear. (In fact, if \u03a3 = n 2 and x \u03a3 has length k, then x will\n\u2217\nappear on the list within| th| e firs\u2265 t nk+1 1 en\u2208 tries.)\nn 1\u2212\n\u2212\nWe now come to the definition of a language in the formal language\ntheoretical sense.\nDefinition 3.1. A language over an alphabet \u03a3 is a subset of \u03a3 . Thus,\n\u2217\na language over \u03a3 is an element of (\u03a3 ), the power set of \u03a3 .\n\u2217 \u2217\nP\nIn other words, any set of strings (over alphabet \u03a3) constitutes a lan-\nguage (over alphabet \u03a3).\nExample 3.4. Let \u03a3 = 0,1 . Then the following are all languages over\n{ }\n\u03a3:\nL = 011,1010,111\n1\n{ }\nL = 0,10,110,1110,11110,...\n2\n{ }\nL = x \u03a3 n (x) = n (x) , where the notation n (x) stands for\n3 \u2217 0 1 0\n{ \u2208 | }\nthe\nnumber of 0\u2019s in the string x, and similarly for n (x).\n1\nL = x x represents a multiple of 5 in binary\n4\n{ | }\nNote that languages can be either finite or infinite. Because \u03a3 is infi-\n\u2217\nnite,itclearlyhasaninfinitenumberofsubsets,andsothereareaninfinite\nnumberoflanguagesover\u03a3. Butaretherecountablyoruncountablymany\nsuch languages?\nTheorem 3.1. For any alphabet \u03a3, the number of languages over \u03a3 is\nuncountable. 142 CHAPTER 3. REGULAR EXPRESSIONS AND FSA\u2019S\nThis fact is an immediate consequence of the result, proved in a previ-\nous chapter, that the power set of a countably infinite set is uncountable.\nSince the elements of (\u03a3) are exactly the languages over \u03a3, there are\nP\nuncountably many such languages.\nLanguages are sets and therefore, as for any sets, it makes sense to talk\nabouttheunion,intersection,andcomplementoflanguages. (Whentaking\nthe complement of a language over an alphabet \u03a3, we always consider the\nuniveral set to be \u03a3 , the set of all strings over \u03a3.) Because languages\n\u2217\nare sets of strings, there are additional operations that can be defined on\nlanguages, operations that would be meaningless on more general sets. For\nexample, the idea of concatenation can be extended from strings to lan-\nguages.\nFor two sets of strings S and T, we define the concatenation of S\nand T (denoted S T or just ST) to be the set ST = st s S t\n\u00b7 { | \u2208 \u2227 \u2208\nT . For example, if S = ab,aab and T = \u03b5,110,1010 , then ST =\n} { } { }\nab,ab110,ab1010,aab,aab110,aab1010 . Note in particular that ab ST,\n{ } \u2208\nbecause ab S, \u03b5 T, and ab \u03b5 = ab. Because concatenation of sets is\n\u2208 \u2208 \u00b7\ndefined in terms of the concatenation of the strings that the sets contain,\nconcatenation of sets is associative and not commutative. (This can easily\nbe verified.)\nWhen a set S is concatenated with itself, the notation SS is usually\nscrapped in favour of S2; if S2 is concatenated with S, we write S3 for the\nresulting set, etc. So S2 is the set of all strings formed by concatenating\ntwo (possibly different, possibly identical) strings from S, S3 is the set of\nstrings formed by concatenating three strings from S, etc. Extending this\nnotation, we take S1 to be the set of strings formed from one string in S\n(i.e. S1 is S itself), and S0 to be the set of strings formed from zero strings\nin S (i.e. S0 = \u03b5 ). If we take the union S0 S1 S2 ..., then the\n{ } \u222a \u222a \u222a\nresulting set is the set of all strings formed by concatenating zero or more\nstringsfromS,andisdenotedS . ThesetS iscalledtheKleeneclosure\n\u2217 \u2217\nof S, and the operator is called the Kleene star operator.\n\u2217\nExample 3.5. Let S = 01,ba . Then\n{ }\nS0 = \u03b5\n{ }\nS1 = 01,ba\n{ }\nS2 = 0101,01ba,ba01,baba\n{ }\nS3 = 010101,0101ba,01ba01,01baba,ba0101,ba01ba,baba01,bababa\n{ }\netc, so\nS = \u03b5,01,ba,0101,01ba,ba01,baba,010101,0101ba,... .\n\u2217\n{ }\nNotethatthisisthesecondtimewehaveseenthenotationsomething .\n\u2217\nWehavepreviouslyseenthatforanalphabet\u03a3, \u03a3 isdefinedtobetheset\n\u2217 3.2. REGULAR EXPRESSIONS 143\nof all strings over \u03a3. If you think of \u03a3 as being a set of length-1 strings,\nand take its Kleene closure, the result is once again the set of all strings\nover \u03a3, and so the two notions of coincide.\n\u2217\nExample 3.6. Let \u03a3= a,b . Then\n{ }\n\u03a30 = \u03b5\n{ }\n\u03a31 = a,b\n{ }\n\u03a32 = aa,ab,ba,bb\n{ }\n\u03a33 = aaa,aab,aba,abb,baa,bab,bba,bbb\n{ }\netc, so\n\u03a3 = \u03b5,a,b,aa,ab,ba,bb,aaa,aab,aba,abb,baa,bab,... .\n\u2217\n{ }\nExercises\n1. Let S = \u03b5,ab,abab and T = aa,aba,abba,abbba,... . Find the following:\na) S2{ b)} S3 { c) S\u2217 d) ST } e) TS\n2. The reverse of a language L is defined to be LR = xR x L . Find SR\nand TR for the S and T in the preceding problem. { | \u2208 }\n3. Give an example of a language L such that L=L\u2217.\n3.2 Regular Expressions\nThough we have used the term string throughout to refer to a sequence of\nsymbols from an alphabet, an alternative term that is frequently used is\nword. The analogy seems fairly obvious: strings are made up of \u201cletters\u201d\nfromanalphabet,justaswordsareinhumanlanguageslikeEnglish. InEn-\nglish, however, there are no particular rules specifying which sequences of\nletterscanbeusedtoformlegalEnglishwords\u2014evenunlikelycombinations\nlikeghthandckstrhavetheirplace. Whilesomeformallanguagesmaysim-\nply be random collections of arbitrary strings, more interesting languages\nare those where the strings in the language all share some common struc-\nture: L 1 = x a,b \u2217 n a(x) = n b(x) ; L 2 = legal Java identifiers ;\n{ \u2208 { } | } { }\nL = legal C++ programs . Inalloftheselanguages,therearestructural\n3\n{ }\nrules which determine which sequences of symbols are in the language and\nwhich aren\u2019t. So despite the terminology of \u201calphabet\u201d and \u201cword\u201d in for-\nmal language theory, the concepts don\u2019t necessarily match \u201calphabet\u201d and\n\u201cword\u201d for human languages. A better parallel is to think of the alphabet\nin a formal language as corresponding to the words in a human language;\nthewordsinaformallanguagecorrespondtothesentencesinahumanlan-\nguage, as there are rules (grammar rules) which determine how they can\nlegally be constructed. 144 CHAPTER 3. REGULAR EXPRESSIONS AND FSA\u2019S\nOnewayofdescribingthegrammaticalstructureofthestringsinalan-\nguage is to use a mathematical formalism called a regular expression. A\nregular expression is a pattern that \u201cmatches\u201d strings that have a partic-\nular form. For example, consider the language (over alphabet \u03a3 = a,b )\n{ }\nL = x x starts and ends with a . What is the symbol-by-symbol struc-\n{ | }\nture of strings in this language? Well, they start with an a, followed by\nzero or more a\u2019s or b\u2019s or both, followed by an a. The regular expression\na (a b) aisapatternthatcapturesthisstructureandmatchesanystring\n\u2217\n\u00b7 | \u00b7\ninL( and havetheirusualmeanings, and designatesor.1) Conversely,\n\u2217\n\u00b7 |\nconsider the regular expression (a (a b) ) ((a b) a). This is a pattern\n\u2217 \u2217\n\u00b7 | | | \u00b7\nthat matches any string that either has the form \u201ca followed by zero or\nmore a\u2019s or b\u2019s or both\u201d (i.e. any string that starts with an a) or has the\nform\u201czeroor morea\u2019s or b\u2019sor both followed by an a\u201d(i.e. any stringthat\nends with an a). Thus the regular expression generates the language of all\nstrings that start or end (or both) in an a: this is the set of strings that\nmatch the regular expression.\nHerearetheformaldefinitionsofaregularexpressionandthelanguage\ngenerated by a regular expression:\nDefinition 3.2. Let \u03a3 be an alphabet. Then the following patterns are\nregular expressions over \u03a3:\n1. \u03a6 and \u03b5 are regular expressions;\n2. a is a regular expression, for each a \u03a3;\n\u2208\n3. if r and r are regular expressions, then so are r r , r r , r\n1 2 1\n|\n2 1\n\u00b7\n2 1\u2217\nand (r ) (and of course, r and (r )). As in concatenation of strings,\n1 2\u2217 2\nthe is often left out of the second expression. (Note: the order of\n\u00b7\nprecedence of operators, from lowest to highest, is , , .)\n| \u00b7 \u2217\nNo other patterns are regular expressions.\nDefinition 3.3. The language generated by a regular expression r,\ndenoted L(r), is defined as follows:\n1. L(\u03a6)= , i.e. no strings match \u03a6;\n\u2205\n2. L(\u03b5)= \u03b5 , i.e. \u03b5 matches only the empty string;\n{ }\n3. L(a)= a , i.e. a matches only the string a;\n{ }\n1Various symbols have been used to represent the \u201cor\u201d operation in regular expres-\nsions. Both+and\u222ahavebeenusedforthispurpose. Inthisbook,weusethesymbol|\nbecauseitiscommonlyusedincomputerimplementationsofregularexpressions. 3.2. REGULAR EXPRESSIONS 145\n4. L(r r )=L(r ) L(r ), i.e. r r matches stringsthat match r or\n1 2 1 2 1 2 1\n| \u222a |\nr or both;\n2\n5. L(r r ) = L(r )L(r ), i.e. r r matches strings of the form \u201csome-\n1 2 1 2 1 2\nthing that matches r followed by something that matches r \u201d;\n1 2\n6. L(r )=(L(r )) , i.e. r matches sequences of 0 or more strings each\n1\u2217 1 \u2217 1\u2217\nof which matches r .\n1\n7. L((r )) = L(r ), i.e. (r ) matches exactly those strings matched by\n1 1 1\nr .\n1\nExample 3.7. Let \u03a3 = a,b , and consider the regular expression r =\n{ }\na b . WhatisL(r)? Well,L(a)= a soL(a )=(L(a)) = a ,and a\n\u2217 \u2217 \u2217 \u2217 \u2217 \u2217\n{ } { } { }\nis the set of all strings of zero or more a\u2019s, so L(a ) = \u03b5,a,aa,aaa,... .\n\u2217\n{ }\nSimilarly,L(b )= \u03b5,b,bb,bbb,... . SinceL(a b )=L(a )L(b )= xy x\n\u2217 \u2217 \u2217 \u2217 \u2217\n{ } { | \u2208\nL(a ) y L(b ) ,wehaveL(a b )= \u03b5,a,b,aa,ab,bb,aaa,aab,abb,bbb,... ,\n\u2217 \u2217 \u2217 \u2217\n\u2227 \u2208 } { }\nwhich is the set of all strings of the form \u201czeroor more a\u2019s followed by zero\nor more b\u2019s\u201d.\nExample 3.8. Let \u03a3 = a,b , and consider the regular expression r =\n{ }\n(a aa aaa)(bb) . Since L(a) = a , L(aa) = L(a)L(a) = aa . Simi-\n\u2217\n| | { } { }\nlarly, L(aaa) = aaa and L(bb) = bb . Now L(a aa aaa) = L(a)\n{ } { } | | \u222a\nL(aa) L(aaa) = a,aa,aaa , and L((bb) ) = (L((bb))) = (L(bb)) (the\n\u2217 \u2217 \u2217\n\u222a { }\nlast equality is from clause 7 of Definition 3.3), and (L(bb)) = bb =\n\u2217 \u2217\n{ }\n\u03b5,bb,bbbb,... . So L(r) is the set of strings formed by concatenating a or\n{ }\naa or aaa with zero or more pairs of b\u2019s.\nDefinition 3.4. A language is regular if it is generated by a regular ex-\npression.\nClearly the union of two regular languages is regular; likewise, the con-\ncatenation of regular languages is regular; and the Kleene closure of a reg-\nular language is regular. It is less clear whether the intersection of regular\nlanguages is always regular; nor is it clear whether the complement of a\nregular language is guaranteed to be regular. These are questions that will\nbe taken up in Section 3.6.\nRegular languages, then, are languages whose strings\u2019 structure can be\ndescribed in a very formal, mathematical way. The fact that a language\ncan be \u201cmechanically\u201d described or generated means that we are likely to\nbe able to get a computer to recognize strings in that language. We will\npursue the question of mechanical language recognition in Section 3.4, and\nsubsequently will see that our first attempt to model mechanical language\nrecognition does in fact produce a family of \u201cmachines\u201d that recognize 146 CHAPTER 3. REGULAR EXPRESSIONS AND FSA\u2019S\nexactly the regular languages. But first, in the next section, we will look\nat some practical applications of regular expressions.\nExercises\n1. Give English-language descriptions of the languages generated by the follow-\ning regular expressions.\na) (a b)\u2217 b) a\u2217 b\u2217 c) b\u2217(ab\u2217ab\u2217)\u2217 d) b\u2217(abb\u2217)\n| |\n2. Giveregularexpressionsover\u03a3= a,b thatgeneratethefollowinglanguages.\n{ }\na) L1 = x x contains 3 consecutive a\u2019s\n{ | }\nb) L2 = x x has even length\n{ | }\nc) L3 = x n b(x)=2mod3\n{ | }\nd) L4 = x x contains the substring aaba\n{ | }\ne) L5 = x n b(x)<2\n{ | }\nf) L6 = x x doesn\u2019t end in aa\n{ | }\n3. Prove that all finite languages are regular.\n3.3 Application: Using Regular Expressions\nA common operation when editing text is to search for a given string of\ncharacters, sometimes with the purpose of replacing it with another string.\nMany\u201csearchandreplace\u201dfacilitieshavetheoptionofusingregularexpres-\nsionsinsteadofsimplestringsofcharacters. Aregularexpressiondescribes\na language, that is, a set of strings. We can think of a regular expression\nas a pattern that matches certain strings, namely all the strings in the\nlanguage described by the regular expression. When a regular expression\nis used in a search operation, the goal is to find a string that matches the\nexpression. This type of pattern matching is very useful.\nTheability todopatternmatchingwithregularexpressionsisprovided\nin many text editors, including jedit and kwrite. Programming languages\noften come with libraries for working with regular expressions. Java (as of\nversion 1.4) provides regular expression handling though a package named\njava.util.regexp. C++ typically provides a header file named regexp.h for\nthesamepurpose. Inall theseapplications, many newnotations areadded\nto the syntax to make it more convenient to use. The syntax can vary\nfrom one implementation to another, but most implementations include\nthe capabilities discussed in this section.\nIn applications of regular expressions, the alphabet usually includes all\nthe characters on the keyboard. This leads to a problem, because regular\nexpressions actually use two types of symbols: symbols that are members 3.3. APPLICATION: USING REGULAR EXPRESSIONS 147\nof the alphabet and special symbols such a \u201c*\u201d and \u201c)\u201d that are used to\nconstruct expressions. These special symbols, which are not part of the\nlanguage being described but are used in the description, are called meta-\ncharacters. The problem is, when the alphabet includes all the available\ncharacters, what do we do about meta-characters? If the language that we\nare describing uses the \u201c*\u201d character, for example, how can we represent\nthe Kleene star operation?\nThe solution is to use a so-called \u201cescape character,\u201d which is usually\nthe backslash, \\. We agree, for example, that the notation \\* refers to the\nsymbol * that is a member of the alphabet, while * by itself is the meta-\ncharacter that represents the Kleene star operation. Similarly, ( and ) are\nthe meta-characters that are used for grouping, while the corresponding\ncharactersinthelanguagearewrittenas\\(and\\). Forexample,aregular\nexpressionthatmatchesthestringa*brepeatedanynumberoftimeswould\nbe written: (a\\*b)*. The backslash is also used to represent certain non-\nprinting characters. For example, a tab is represented as \\t and a new line\ncharacter is \\n.\nWe introduce two new common operations on regular expressions and\ntwo new meta-characters to represent them. The first operation is rep-\nresented by the meta-character +: If r is a regular expression, then r+\nrepresents the occurrence of r one or more times. The second operation\nis represented by ?: The notation r? represents an occurrence of r zero\nor one times. That is to say, r? represents an optional occurrence of r.\nNote that these operations are introduced for convenience only and do not\nrepresent any real increase in the power. In fact, r+ is exactly equivalent\nto rr*, and r? is equivalent to (r|\u03b5) (except that in applications there is\ngenerally no equivalent to \u03b5).\nTo make it easier to deal with the large number of characters in the\nalphabet, character classes are introduced. A character class consists of\na list of characters enclosed between brackets, [ and ]. (The brackets are\nmeta-characters.) A character class matches a single character, which can\nbe any of the characters in the list. For example, [0123456789] matches\nany one of the digits 0 through 9. The same thing could be expressed as\n(0|1|2|3|4|5|6|7|8|9), so once again we have added only convenience,\nnot new representational power. For even more convenience, a hyphen can\nbe included in a character class to indicate a range of characters. This\nmeans that [0123456789] could also be written as [0-9] and that the\nregularexpression[a-z]willmatchanysinglelowercaseletter. Acharacter\nclass can include multiple ranges, so that [a-zA-Z] will match any letter,\nlower-oruppercase. Theperiod(.) isameta-characterthatwillmatchany\nsinglecharacter,except(inmostimplementations)foranend-of-line. These 148 CHAPTER 3. REGULAR EXPRESSIONS AND FSA\u2019S\nnotations can, of course, be used in more complex regular expressions. For\nexample, [A-Z][a-zA-Z]* will match any capitalized word, and \\(.*\\)\nmatches any string of characters enclosed in parentheses.\nInmostimplementations, themeta-character^canbeusedinaregular\nexpression to match the beginning of a line of text, so that the expression\n^[a-zA-Z]+ will only match a word that occurs at the start of a line.\nSimilarly, $ is used as a meta-character to match the end of a line. Some\nimplementationsalsohaveawayofmatchingbeginningsandendsofwords.\nTypically, \\b will match such \u201cword boundaries.\u201d Using this notation, the\npattern \\band\\b will match the string \u201cand\u201d when it occurs as a word,\nbut will not match the a-n-d in the word \u201crandom.\u201d We are going a bit\nbeyond basic regular expressions here: Previously, we only thought of a\nregular expression as something that either will match or will not match\na given string in its entirety. When we use a regular expression for a\nsearch operation, however, we want to find a substring of a given string\nthat matches the expression. The notations ^, $ and \\b put a restrictions\non where the matching substring can be located in the string.\nWhen regular expressions are used in search-and-replace operations, a\nregular expression is used for the search pattern. A search is made in a\n(typicallylong)stringforasubstringthatmatchesthepattern,andthenthe\nsubstring is replaced by a specified replacement pattern. The replacement\npattern is not used for matching and is not a regular expression. However,\nitcanbemorethanjustasimplestring. It\u2019spossibletoincludepartsofthe\nsubstring that is being replaced in the replacement string. The notations\n\\0, \\1, ..., \\9 are used for this purpose. The first of these, \\0, stands for\nthe entire substring that is being replaced. The others are only available\nwhen parentheses are used in the search pattern. The notation \\1 stands\nfor \u201cthe part of the substring that matched the part of the search pattern\nbeginning with the first ( in the pattern and ending with the matching ).\u201d\nSimilarly, \\2 represents whatever matched the part of the search pattern\nbetween the second pair of parentheses, and so on.\nSuppose, for example, that you would like to search for a name in the\nform last-name, first-name and replace it with the same name in the form\nfirst-name last-name. For example, \u201cReeves, Keanu\u201d should be converted\nto \u201cKeanu Reeves\u201d. Assuming that names contain only letters, this could\nbe done using the search pattern ([A-Za-z]+), ([A-Za-z]+) and the re-\nplacement pattern\\2 \\1. Whenthematchismade, thefirst([A-Za-z]+)\nwill match \u201cReeves,\u201d so that in the replacement pattern, \\1 represents the\nsubstring \u201cReeves\u201d. Similarly, \\2 will represent \u201cKeanu\u201d. Note that the\nparentheses are included in the search pattern only to specify what parts\nof the string are represented by \\1 and \\2. In practice, you might use 3.3. APPLICATION: USING REGULAR EXPRESSIONS 149\n^([A-Za-z]+), ([A-Za-z])$ as the search pattern to constrain it so that\nit will only match a complete line of text. By using a \u201cglobal\u201d search-and-\nreplace, you could convert an entire file of names from one format to the\nother in a single operation.\nRegular expressions are a powerful and useful technique that should be\npartofanycomputerscientist\u2019stoolbox. Thissectionhasgivenyouataste\nofwhattheycando,butyoushouldcheckoutthespecificcapabilitiesofthe\nregularexpressionimplementationinthetoolsandprogramminglanguages\nthat you use.\nExercises\n1. The backslash is itself a meta-character. Suppose that you want to match a\nstring that contains a backslash character. How do you suppose you would\nrepresent the backslash in the regular expression?\n2. Usingthenotationintroducedinthissection,writearegularexpressionthat\ncould be used to match each of the following:\na) Anysequenceofletters(upper-orlowercase)thatincludestheletterZ\n(in uppercase).\nb) Anyeleven-digittelephonenumberwrittenintheform(xxx)xxx-xxxx.\nc) Any eleven-digit telephone number either in the form (xxx)xxx-xxxx\nor xxx-xxx-xxxx.\nd) A non-negative real number with an optional decimal part. The ex-\npression should match numbers such as 17, 183.9999, 182., 0, 0.001,\nand 21333.2.\ne) A complete line of text that contains only letters.\nf) A C++ style one-line comment consisting of \/\/ and all the following\ncharacters up to the end-of-line.\n3. Give a search pattern and a replace pattern that could be used to perform\nthe following conversions:\na) Convertastringthatisenclosedinapairofdoublequotestothesame\nstring with the double quotes replaced by single quotes.\nb) Convert seven-digit telephone numbers in the format xxx-xxx-xxxx to\nthe format (xxx)xxx-xxxx.\nc) Convert C++ one-line comments, consisting of characters between \/\/\nand end-of-line, to C style comments enclosed between \/* and *\/.\nd) Convert any number of consecutive spaces and tabs to a single space.\n4. In some implementations of \u201cregular expressions,\u201d the notations \\1, \\2, and\nsooncanoccurinasearchpattern. Forexample,considerthesearchpattern\n^([a-zA-Z]).*\\1$. Here, \\1 represents a recurrence of the same substring\nthat matched [a-zA-Z], the part of the pattern between the first pair of\nparentheses. The entire pattern, therefore, will match a line of text that 150 CHAPTER 3. REGULAR EXPRESSIONS AND FSA\u2019S\nbegins and ends with the same letter. Using this notation, write a pattern\nthat matches all strings in the language L= anban n 0 . (Later in this\n{ | \u2265 }\nchapter, we will see that L is not a regular language, so allowing the use of\n\\1ina\u201cregularexpression\u201dmeansthatit\u2019snotreallyaregularexpressionat\nall! Thisnotationcanaddarealincreaseinexpressivepowertothepatterns\nthat contain it.)\n3.4 Finite-State Automata\nWe have seen how regular expressions can be used to generate languages\nmechanically. Howmightlanguagesberecognizedmechanically? Theques-\ntion is of interest because if we can mechanically recognize languages like\nL = all legal C++ programs that will not go into infinite loops on any\n{\ninput ,thenitwouldbepossibletowriteu\u00a8ber-compilersthatcandoseman-\n}\ntic error-checking like testing for infinite loops, in addition to the syntactic\nerror-checking they currently do.\nWhat formalism might we use to model what it means to recognize a\nlanguage \u201cmechanically\u201d? We look for inspiration to a language-recognizer\nwith which we are all familiar, and which we\u2019ve already in fact mentioned:\nacompiler. ConsiderhowaC++compilermighthandlerecognizingalegal\nif statement. Having seen the word if, the compiler will be in a state or\nphase of its execution where it expects to see a \u2018(\u2019; in this state, any other\ncharacter will put the compiler in a \u201cfailure\u201d state. If the compiler does in\nfactseea\u2018(\u2019next,itwillthenbeinan\u201cexpectingabooleancondition\u201dstate;\nif it sees a sequence of symbols that make up a legal boolean condition, it\nwill then be in an \u201cexpecting a \u2018)\u2019\u201d state; and then \u201cexpecting a \u2018 \u2019 or a\n{\nlegalstatement\u201d;andsoon. Thusonecanthinkofthecompilerasbeingin\naseriesofstates; onseeinganewinputsymbol,itmovesontoanewstate;\nand this sequence of transitions eventually leads to either a \u201cfailure\u201d state\n(iftheifstatementisnotsyntacticallycorrect)ora\u201csuccess\u201dstate(iftheif\nstatement is legal). We isolate these three concepts\u2014states, input-inspired\ntransitionsfromstatetostate,and\u201caccepting\u201dvs\u201cnon-accepting\u201dstates\u2014\nas the key features of a mechanical language-recognizer, and capture them\nin a model called a finite-state automaton. (Whether this is a successful\ndistillationoftheessenceofmechanicallanguagerecognitionremainstobe\nseen; the question will be taken up later in this chapter.)\nA finite-state automaton (FSA), then, is a machine which takes,\nas input, a finite string of symbols from some alphabet \u03a3. There is a\nfinite set of states in which the machine can find itself. The state it is\nin before consuming any input is called the start state. Some of the\nstates are accepting or final. If the machine ends in such a state after 3.4. FINITE-STATE AUTOMATA 151\ncompletely consuming an input string, the string is said to be accepted\nby the machine. The actual functioning of the machine is described by\nsomething called a transition function, which specifies what happens if\nthemachineisinaparticularstateandlookingataparticularinputsymbol.\n(\u201cWhat happens\u201d means \u201cin which state does the machine end up\u201d.)\nExample 3.9. Below is a table that describes the transition function of a\nfinite-state automaton with states p, q, and r, on inputs 0 and 1.\np q r\n0 p q r\n1 q r r\nThe table indicates, for example, that if the FSA were in state p and\nconsumed a 1, it would move to state q.\nFSAs actually come in two flavours depending on what properties you\nrequire of the transition function. We will look first at a class of FSAs\ncalled deterministic finite-state automata (DFAs). In these machines, the\ncurrent state of the machine and the current input symbol together deter-\nmineexactlywhichstatethemachineendsupin: forevery<currentstate,\ncurrent input symbol> pair, there is exactly one possible next state for the\nmachine.\nDefinition 3.5. Formally, a deterministic finite-state automaton M\nis specified by 5 components: M =(Q,\u03a3,q ,\u03b4,F) where\n0\nQ is a finite set of states;\n\u2022\n\u03a3 is an alphabet called the input alphabet;\n\u2022\nq Q is a state which is designated as the start state;\n0\n\u2022 \u2208\nF is a subset of Q; the states in F are states designated as final or\n\u2022\naccepting states;\n\u03b4 is a transition function that takes <state, input symbol> pairs and\n\u2022\nmaps each one to a state: \u03b4 : Q \u03a3 Q. To say \u03b4(q,a) = q\n\u2032\n\u00d7 \u2192\nmeans that if the machine is in state q and the input symbol a is\nconsumed, then the machine will move into state q . The function\n\u2032\n\u03b4 must be a total function, meaning that \u03b4(q,a) must be defined for\nevery state q and every input symbol a. (Recall also that, according\nto the definition of a function, there can be only one output for any\nparticular input. This means that for any given q and a, \u03b4(q,a) can 152 CHAPTER 3. REGULAR EXPRESSIONS AND FSA\u2019S\nhave only one value. This is what makes the finite-state automaton\ndeterministic: giventhecurrentstateandinputsymbol, thereisonly\none possible move the machine can make.)\nExample 3.10. The transition function described by the table in the pre-\nceding example is that of a DFA. If we take p to be the start state and r\nto be a final state, then the formal description of the resulting machine is\nM =( p,q,r , 0,1 ,p,\u03b4, r ), where \u03b4 is given by\n{ } { } { }\n\u03b4(p,0)=p \u03b4(p,1)=q\n\u03b4(q,0)=q \u03b4(q,1)=r\n\u03b4(r,0)=r \u03b4(r,1)=r\nThetransitionfunction\u03b4 describesonlyindividualstepsofthemachine\nas individual input symbols are consumed. However, we will often want\nto refer to \u201cthe state the automaton will be in if it starts in state q and\nconsumesinputstringw\u201d,wherew isastringofinputsymbolsratherthan\na single symbol. Following the usual practice of using to designate \u201c0 or\n\u2217\nmore\u201d, we define \u03b4 (q,w) as a convenient shorthand for \u201cthe state that the\n\u2217\nautomaton will be in if it starts in state q and consumes the input string\nw\u201d. For any string, it is easy to see, based on \u03b4, what steps the machine\nwillmakeasthosesymbolsareconsumed,andwhat\u03b4 (q,w)willbeforany\n\u2217\nq and w. Note that if no input is consumed, a DFAmakes no move, and so\n\u03b4 (q,\u03b5)=q for any state q.2\n\u2217\nExample 3.11. LetM betheautomatonintheprecedingexample. Then,\nfor example:\n\u03b4 (p,001)=q, since \u03b4(p,0)=p, \u03b4(p,0)=p, and \u03b4(p,1)=q;\n\u2217\n\u03b4 (p,01000)=q;\n\u2217\n\u03b4 (p,1111)=r;\n\u2217\n\u03b4 (q,0010)=r.\n\u2217\nWe have divided the states of a DFA into accepting and non-accepting\nstates, with the idea that some strings will be recognized as \u201clegal\u201d by the\nautomaton, and some not. Formally:\nDefinition 3.6. Let M = (Q,\u03a3,q ,\u03b4,F). A string w \u03a3 is accepted\n0 \u2217\n\u2208\nby M iff \u03b4 (q ,w) F. (Don\u2019t get confused by the notation. Remember,\n\u2217 0\n\u2208\nit\u2019s just a shorter and neater way of saying \u201cw \u03a3 is accepted by M if\n\u2217\n\u2208\n2\u03b4\u2217 can be defined formally by saying that \u03b4\u2217(q,\u03b5) = q for every state q, and\n\u03b4\u2217(q,ax) = \u03b4\u2217(\u03b4(q,a),x) for any state q, a \u2208 \u03a3 and x \u2208 \u03a3\u2217. Note that this is a re-\ncursivedefinition. 3.4. FINITE-STATE AUTOMATA 153\nand only if the state that M will end up in if it starts in q and consumes\n0\nw is one of the states in F.\u201d)\nThelanguage accepted by M, denotedL(M), isthesetofallstrings\nw \u03a3 that are accepted by M: L(M)= w \u03a3 \u03b4 (q ,w) F .\n\u2217 \u2217 \u2217 0\n\u2208 { \u2208 | \u2208 }\nNote that we sometimes use a slightly different phrasing and say that a\nlanguage L is accepted by some machine M. We don\u2019t mean by this that\nL and maybe some other strings are accepted by M; we mean L = L(M),\ni.e. L is exactly the set of strings accepted by M.\nIt may not be easy, looking at a formal specification of a DFA, to de-\ntermine what language that automaton accepts. Fortunately, the mathe-\nmatical description of the automaton M = (Q,\u03a3,q ,\u03b4,F) can be neatly\n0\nand helpfully captured in a picture called a transition diagram. Con-\nsider again the DFA of the two preceding examples. It can be represented\npictorially as:\n0 0 0,1\n1 1\nM:\np q r\nThe arrow on the left indicates that p is the start state; double circles\nindicate that a state is accepting. Looking at this picture, it should be\nfairly easy to see that the language accepted by the DFA M is L(M) =\nx 0,1 n (x) 2 .\n\u2217 1\n{ \u2208{ } | \u2265 }\nExample3.12. FindthelanguageacceptedbytheDFAshownbelow(and\ndescribe it using a regular expression!)\nb\na\nM:\nq 3 a q b q b q 2\n0 1\nb a\na\na,b q\n4\nThe start state of M is accepting, which means \u03b5 L(M). If M is in\n\u2208\nstate q , a sequence of two a\u2019s or three b\u2019s will move M back to q and\n0 0\nhence be accepted. So L(M)=L((aa bbb) ).\n\u2217\n| 154 CHAPTER 3. REGULAR EXPRESSIONS AND FSA\u2019S\nThe state q in the preceding example is often called a garbage or trap\n4\nstate: it is a non-accepting state which, once reached by the machine,\ncannot be escaped. It is fairly common to omit such states from transition\ndiagrams. For example, one is likely to see the diagram:\na\nb\nNotethatthiscannotbeacompleteDFA,becauseaDFAisrequiredto\nhave a transition defined for every state-input pair. The diagram is \u201cshort\nfor\u201d the full diagram:\na a\na,b\nb\nb\nAs well as recognizing what language is accepted by a given DFA, we\noften want to do the reverse and come up with a DFA that accepts a\ngiven language. Building DFAs for specified languages is an art, not a sci-\nence. There is no algorithm that you can apply to produce a DFA from an\nEnglish-language description of the set of strings the DFA should accept.\nOn the other hand, it is not generally successful, either, to simply write\ndown a half-dozen strings that are in the language and design a DFA to\naccept those strings\u2014invariably there are strings that are in the language\nthat aren\u2019t accepted, and other strings that aren\u2019t in the language that\nare accepted. So how do you go about building DFAs that accept all and\nonly the strings they\u2019re supposed to accept? The best advice I can give\nis to think about relevant characteristics that determine whether a string\nis in the language or not, and to think about what the possible values or\n\u201cstates\u201d of those characteristics are; then build a machine that has a state\ncorresponding to each possible combination of values of relevant character-\nistics, and determine how the consumption of inputs affects those values.\nI\u2019ll illustrate what I mean with a couple of examples.\nExample 3.13. Find a DFA with input alphabet \u03a3= a,b that accepts\n{ }\nthe language L= w \u03a3 n (w) and n (w) are both even .\n\u2217 a b\n{ \u2208 | }\nThe characteristics that determine whether or not a string w is in L\naretheparity ofn (w)andn (w). Therearefourpossiblecombinations of\na b\n\u201cvalues\u201d for these characteristics: both numbers could be even, both could\nbeodd,thefirstcouldbeoddandthesecondeven,orthefirstcouldbeeven 3.4. FINITE-STATE AUTOMATA 155\nand the second odd. So we build a machine with four states q ,q ,q ,q\n1 2 3 4\ncorresponding to the four cases. We want to set up \u03b4 so that the machine\nwill be in state q exactly when it has consumed a string with an even\n1\nnumber of a\u2019s and an even number of b\u2019s, in state q exactly when it has\n2\nconsumed a string with an odd number of a\u2019s and an odd number of b\u2019s,\nand so on.\nTo do this, we first make the state q into our start state, because the\n1\nDFA will be in the start state after consuming the empty string \u03b5, and \u03b5\nhas an even number (zero) of both a\u2019s and b\u2019s. Now we add transitions\nby reasoning about how the parity of a\u2019s and b\u2019s is changed by additional\ninput. Forinstance, ifthemachineisinq (meaninganevennumberofa\u2019s\n1\nand an even number of b\u2019s have been seen) and a further a is consumed,\nthen we want the machine to move to state q , since the machine has now\n3\nconsumed an odd number of a\u2019s and still an even number of b\u2019s. So we\nadd the transition \u03b4(q ,a)=q to the machine. Similarly, if the machine is\n1 3\nin q (meaning an odd number of a\u2019s and an odd number of b\u2019s have been\n2\nseen) and a further b is consumed, then we want the machine to move to\nstate q again, since the machine has still consumed an odd number of a\u2019s,\n3\nand now an even number of b\u2019s. So we add the transition \u03b4(q ,b) = q to\n2 3\nthe machine. Similar reasoning produces a total of eight transitions, one\nfor each state-input pair. Finally, we have to decide which states should\nbe final states. The only state that corresponds to the desired criteria for\nthe language L is q , so we make q a final state. The complete machine is\n1 1\nshown below.\nq\na q\n1\n3\na\nb b b b\na\nq a q\n4 2\nExample 3.14. Find a DFA with input alphabet \u03a3= a,b that accepts\n{ }\nthe language L = w \u03a3 n (w) is divisible by 3 .\n\u2217 a\n{ \u2208 | }\nThe relevant characteristic here is of course whether or not the number\nof a\u2019s in a string is divisible by 3, perhaps suggesting a two-state machine.\nBut in fact, there is more than one way for a number to not be divisible\nby 3: dividing the number by 3 could produce a remainder of either 1 or 2\n(a remainder of 0 corresponds to the number in fact being divisible by 3). 156 CHAPTER 3. REGULAR EXPRESSIONS AND FSA\u2019S\nSo we build a machine with three states q , q , q , and add transitions so\n0 1 2\nthat the machine will be in state q exactly when the number of a\u2019s it has\n0\nconsumed is evenly divisible by 3, in state q exactly when the number of\n1\na\u2019s it has consumed is equivalent to 1mod3, and similarly for q . State q\n2 0\nwillbethestartstate, as\u03b5has0a\u2019sand0isdivisibleby3. Thecompleted\nmachine is shown below. Notice that because the consumption of a b does\nnotaffecttheonlyrelevantcharacteristic,b\u2019sdonotcausechangesofstate.\na\na a\nq q q\n0 1 2 b\nb\nb\nExample 3.15. Find a DFA with input alphabet \u03a3= a,b that accepts\n{ }\nthe language L = w \u03a3 w contains three consecutive a\u2019s .\n\u2217\n{ \u2208 | }\nAgain, it is not quite so simple as making a two-state machine where\nthe states correspond to \u201chave seen aaa\u201d and \u201chave not seen aaa\u201d. Think\ndynamically: as you move through the input string, how do you arrive at\nthe goal of having seen three consecutive a\u2019s? You might have seen two\nconsecutive a\u2019s and still need a third, or you might just have seen one a\nand be looking for two more to come immediately, or you might just have\nseen a b and be right back at the beginning as far as seeing 3 consecutive\na\u2019sgoes. Soonceagaintherewillbethreestates,withthe\u201clastsymbolwas\nnot an a\u201d state being the start state. The complete automaton is shown\nbelow.\nb\na,b\nb\na a a\nb\nExercises\n1. Give DFAs that accept the following languages over \u03a3= a,b .\n{ }\na) L1 = x x contains the substring aba\n{ | }\nb) L2 =L(a\u2217b\u2217)\nc) L3 = x n a(x)+n b(x) is even\n{ | }\nd) L4 = x n a(x) is a multiple of 5\n{ | }\ne) L5 = x x does not contain the substring abb\n{ | } 3.5. NONDETERMINISTIC FINITE-STATE AUTOMATA 157\nf) L6 = x x has no a\u2019s in the even positions\n{ | }\ng) L7 =L(aa\u2217 aba\u2217b\u2217)\n|\n2. What languages do the following DFAs accept?\na) a a,b\nb b b\na\na\na\nb) b a,b\na\na\nb\nb\n3. Let \u03a3= 0,1 . Give a DFA that accepts the language\n{ }\nL= x \u03a3\u2217 x is the binary representation of an integer divisible by 3 .\n{ \u2208 | }\n3.5 Nondeterministic Finite-State Automata\nAs mentioned briefly above, there is an alternative school of though as to\nwhat properties should be required of a finite-state automaton\u2019s transition\nfunction. Recall our motivating example of a C++ compiler and a legal if\nstatement. In our description, we had the compiler in an \u201cexpecting a \u2018)\u2019\u201d\nstate; onseeinga\u2018)\u2019, thecompilermovedintoan\u201cexpectinga\u2018 \u2019oralegal\n{\nstatement\u201d state. An alternative way to view this would be to say that\nthe compiler, on seeing a \u2018)\u2019, could move into one of two different states: it\ncould move to an \u201cexpecting a \u2018 \u2019\u201d state or move to an \u201cexpecting a legal\n{\nstatement\u201d state. Thus, from a single state, on input \u2018)\u2019, the compiler has\nmultiple moves. This alternative interpretation is not allowed by the DFA\nmodel. A second point on which one might question the DFA model is the\nfact that input must be consumed for the machine to change state. Think\nofthesyntaxforC++functiondeclarations. Thereturntypeofafunction\nneednotbespecified(thedefaultistakentobeint). Thestartstateofthe\ncompilerwhenparsingafunctiondeclarationmightbe\u201cexpectingareturn\ntype\u201d; then with no input consumed, the compiler can move to the state\n\u201cexpectingalegalfunctionname\u201d. Tomodelthis,itmightseemreasonable\nto allow transitions that do not require the consumption of input (such 158 CHAPTER 3. REGULAR EXPRESSIONS AND FSA\u2019S\ntransitions are called \u03b5-transitions). Again, this is not supported by the\nDFAabstraction. Thereis,therefore,asecondclassoffinite-stateautomata\nthat people study, the class of nondeterministic finite-state automata.\nA nondeterministic finite-state automaton (NFA) is the same as\na deterministic finite-state automaton except that the transition function\nis no longer a function that maps a state-input pair to a state; rather, it\nmaps a state-input pair or a state-\u03b5 pair to a set of states. No longer do\nwehave\u03b4(q,a)=q ,meaningthatthemachinemustchangetostateq ifit\n\u2032 \u2032\nis in state q and consumes an a. Rather, we have \u2202(q,a)= q ,q ,...,q ,\n1 2 n\n{ }\nmeaningthatifthemachineisinstateq andconsumesana,itmightmove\ndirectly to any one of the states q ,...,q . Note that the set of next states\n1 n\n\u2202(q,a) is defined for every state q and every input symbol a, but for some\nq\u2019s and a\u2019s it could be empty, or contain just one state (there don\u2019t have\nto be multiple next states). The function \u2202 must also specify whether it is\npossibleforthemachinetomakeanymoveswithoutinputbeingconsumed,\ni.e. \u2202(q,\u03b5) must be specified for every state q. Again, it is quite possible\nthat\u2202(q,\u03b5)maybeemptyforsomestatesq: thereneednotbe\u03b5-transitions\nout of q.\nDefinition 3.7. Formally, a nondeterministic finite-state automaton M is\nspecified by 5 components: M =(Q,\u03a3,q ,\u2202,F) where\n0\nQ, \u03a3, q and F are as in the definition of DFAs;\n0\n\u2022\n\u2202 isatransitionfunctionthattakes<state, inputsymbol>pairsand\n\u2022\nmaps each one to a set of states. To say \u2202(q,a) = q ,q ,...,q\n1 2 n\n{ }\nmeans that if the machine is in state q and the input symbol a is\nconsumed,thenthemachinemaymovedirectlyintoanyoneofstates\nq ,q ,...,q . Thefunction\u2202mustalsobedefinedforevery<state,\u03b5>\n1 2 n\npair. To say \u2202(q,\u03b5) = q ,q ,...,q means that there are direct \u03b5-\n1 2 n\n{ }\ntransitions from state q to each of q ,q ,...,q .\n1 2 n\nTheformaldescriptionofthefunction\u2202 is\u2202 :Q (\u03a3 \u03b5 ) P(Q).\n\u00d7 \u222a{ } \u2192\nThe function \u2202 describes how the machine functions on zero or one\ninput symbol. As with DFAs, we will often want to refer to the behavior\nof the machine on a string of inputs, and so we use the notation \u2202 (q,w)\n\u2217\nas shorthand for \u201cthe set of states in which the machine might be if it\nstarts in state q and consumes input string w\u201d. As with DFAs, \u2202 (q,w) is\n\u2217\ndetermined by the specification of \u2202. Note that for every state q, \u2202 (q,\u03b5)\n\u2217\ncontainsatleastq,andmaycontainadditionalstatesifthereare(sequences\nof) \u03b5-transitions out of q. 3.5. NONDETERMINISTIC FINITE-STATE AUTOMATA 159\nWe do have to think a bit carefully about what it means for an NFA\nto accept a string w. Suppose \u2202 (q ,w) contains both accepting and non-\n\u2217 0\nacceptingstates, i.e.themachinecouldendinanacceptingstateaftercon-\nsuming w, but it might also end in a non-accepting state. Should we con-\nsiderthemachinetoacceptw,orshouldwerequireeverystatein\u2202 (q ,w)\n\u2217 0\nto be accepting before we admit w to the ranks of the accepted? Think\nof the C++ compiler again: provided that an if statement fits one of the\nlegal syntax specifications, the compiler will accept it. So we take as the\ndefinition of acceptance by an NFA: A string w is accepted by an NFA\nprovided that at least one of the states in \u2202 (q ,w) is an accepting state.\n\u2217 0\nThat is, if there is some sequence of steps of the machine that consumes w\nand leaves the machine in an accepting state, then the machine accepts w.\nFormally:\nDefinition 3.8. LetM =(Q,\u03a3,q ,\u2202,F)beanondeterministicfinite-state\n0\nautomaton. The string w \u03a3 is accepted by M iff \u2202 (q ,w) contains at\n\u2217 \u2217 0\n\u2208\nleast one state q F.\nF\n\u2208\nThelanguage accepted by M, denotedL(M), isthesetofallstrings\nw \u03a3 that are accepted by M: L(M)= w \u03a3 \u2202 (q ,w) F = .\n\u2217 \u2217 \u2217 0\n\u2208 { \u2208 | \u2229 6 \u2205}\nExample 3.16. The NFA shown below accepts all strings of a\u2019s and b\u2019s in\nwhich the second-to-last symbol is a.\na,b\na a,b\nItshouldbefairlyclearthateverylanguagethatisacceptedbyaDFAis\nalso accepted by an NFA. Pictorially, a DFA looks exactly like an NFA (an\nNFA that doesn\u2019t happen to have any \u03b5-transitions or multiple same-label\ntransitions from any state), though there is slightly more going on behind\nthe scenes. Formally, given the DFA M =(Q,\u03a3,q ,\u03b4,F), you can build an\n0\nNFA M = (Q,\u03a3,q ,\u2202,F) where 4 of the 5 components are the same and\n\u2032 0\nwhere every transition \u03b4(q,a)=q has been replaced by \u2202(q,a)= q .\n\u2032 \u2032\n{ }\nBut is the reverse true? Can any NFA-recognized language be recog-\nnizedby aDFA?Look, for example, at the language in Example 3.16. Can\nyou come up with a DFA that accepts this language? Try it. It\u2019s pretty\ndifficulttodo. ButdoesthatmeanthattherereallyisnoDFAthataccepts\nthe language, or only that we haven\u2019t been clever enough to find one?\nIt turns out that the limitation is in fact in our cleverness, and not in\nthe power of DFAs. 160 CHAPTER 3. REGULAR EXPRESSIONS AND FSA\u2019S\nTheorem 3.2. Every language that is accepted by an NFA is accepted by\na DFA.\nProof. Suppose we are given an NFA N = (P,\u03a3,p ,\u2202,F ), and we want\n0 p\nto build a DFA D = (Q,\u03a3,q ,\u03b4,F ) that accepts the same language. The\n0 q\nidea is to make the states in D correspond to subsets of N\u2019s states, and\nthen to set up D\u2019s transition function \u03b4 so that for any string w, \u03b4 (q ,w)\n\u2217 0\ncorresponds to \u2202 (p ,w); i.e. the single state that w gets you to in D\n\u2217 0\ncorresponds to the set of states that w could get you to in N. If any\nof those states is accepting in N, w would be accepted by N, and so the\ncorresponding state in D would be made accepting as well.\nSo how do we make this work? The first thing to do is to deal with\na start state q for D. If we\u2019re going to make this state correspond to a\n0\nsubset of N\u2019s states, what subset should it be? Well, remember (1) that in\nany DFA, \u03b4 (q ,\u03b5)=q ; and (2) we want to make \u03b4 (q ,w) correspond to\n\u2217 0 0 \u2217 0\n\u2202 (p ,w) for every w. Putting these two limitations together tells us that\n\u2217 0\nweshouldmakeq correspondto\u2202 (p ,\u03b5). Soq correspondstothesubset\n0 \u2217 0 0\nof all of N\u2019s states that can be reached with no input.\nNow we progressively set up D\u2019s transition function \u03b4 by repeatedly\ndoing the following:\n\u2013findastateqthathasbeenaddedtoDbutwhoseout-transitionshave\nnotyetbeenadded. (Notethatq initiallyfitsthisdescription.) Remember\n0\nthat the state q corresponds to some subset p ,...,p of N\u2019s states.\n1 n\n{ }\n\u2013foreachinputsymbola,lookatallN\u2019sstatesthatcanbereachedfrom\nany one of p ,...,p by consuming a (perhaps making some \u03b5-transitions\n1 n\nas well). That is, look at \u2202 (p ,a) ... \u2202 (p ,a). If there is not already\n\u2217 1 \u2217 n\n\u222a \u222a\na DFA state q that corresponds to this subset of N\u2019s states, then add one,\n\u2032\nand add the transition \u03b4(q,a)=q to D\u2019s transitions.\n\u2032\nTheaboveprocessmusthalteventually,asthereareonlyafinitenumber\nof states n in the NFA, and therefore there can be at most 2n states in the\nDFA,as that is the numberof subsetsof the NFA\u2019s states. The final states\nof the new DFA are those where at least one of the associated NFA states\nis an accepting state of the NFA.\nCan we now argue that L(D) = L(N)? We can, if we can argue that\n\u03b4 (q ,w) corresponds to \u2202 (p ,w) for all w \u03a3 : if this latter property\n\u2217 0 \u2217 0 \u2217\n\u2208\nholds, then w L(D) iff \u03b4 (q ,w) is accepting, which we made be so iff\n\u2217 0\n\u2208\n\u2202 (p ,w) contains an accepting state of N, which happens iff N accepts w\n\u2217 0\ni.e. iff w L(N).\n\u2208\nSo can we argue that \u03b4 (q ,w) does in fact correspond to \u2202 (p ,w) for\n\u2217 0 \u2217 0\nall w? We can, using induction on the length of w.\nFirst, a preliminary observation. Suppose w =xa, i.e. w is the string x\nfollowed by the single symbol a. How are \u2202 (p ,x) and \u2202 (p ,w) related?\n\u2217 0 \u2217 0 3.5. NONDETERMINISTIC FINITE-STATE AUTOMATA 161\nWell, recall that \u2202 (p ,x) is the set of all states that N can reach when\n\u2217 0\nit starts in p and consumes x: \u2202 (p ,x) = p ,...,p for some states\n0 \u2217 0 1 n\n{ }\np ,...,p . Now, w is just x with an additional a, so where might N end\n1 n\nup if it starts in p and consumes w? We know that x gets N to p or ...\n0 1\nor p , so xa gets N to any state that can be reached from p with an a\nn 1\n(andmaybesome\u03b5-transitions), andtoanystatethatcanbereachedfrom\np with an a (and maybe some \u03b5-transitions), etc. Thus, our relationship\n2\nbetween \u2202 (p ,x) and \u2202 (p ,w) is that if \u2202 (p ,x) = p ,...,p , then\n\u2217 0 \u2217 0 \u2217 0 1 n\n{ }\n\u2202 (p ,w)=\u2202 (p ,a) ... \u2202 (p ,a). With this observation in hand, let\u2019s\n\u2217 0 \u2217 1 \u2217 n\n\u222a \u222a\nproceed to our proof by induction.\nWewanttoprovethat\u03b4 (q ,w)correspondsto\u2202 (p ,w)forallw \u03a3 .\n\u2217 0 \u2217 0 \u2217\n\u2208\nWe use induction on the length of w.\n1. Base case: Suppose w has length 0. The only string w with length\n0 is \u03b5, so we want to show that \u03b4 (q ,\u03b5) corresponds to \u2202 (p ,\u03b5).\n\u2217 0 \u2217 0\nWell, \u03b4 (q ,\u03b5) =q , since in a DFA, \u03b4 (q,\u03b5) =q for any state q. We\n\u2217 0 0 \u2217\nexplicitly made q correspond to \u2202 (p ,\u03b5), and so the property holds\n0 \u2217 0\nfor w with length 0.\n2. Inductivecase: Assumethatthedesiredpropertyholdsforsomenum-\nber n, i.e. that \u03b4 (q ,x) corresponds to \u2202 (p ,x) for all x with length\n\u2217 0 \u2217 0\nn. Look at an arbitrary stringw with length n+1. Wewant to show\nthat \u03b4 (q ,w) corresponds to \u2202 (p ,w). Well, the string w must look\n\u2217 0 \u2217 0\nlike xa for some string x (whose length is n) and some symbol a. By\nour inductive hypothesis, we know \u03b4 (q ,x) corresponds to \u2202 (p ,x).\n\u2217 0 \u2217 0\nWeknow\u2202 (p ,x)isasetofN\u2019sstates, say\u2202 (p ,x)= p ,...,p .\n\u2217 0 \u2217 0 1 n\n{ }\nAt this point, our subsequent reasoning might be a bit clearer if we\ngive explicit names to \u03b4 (q ,w) (the state D reaches on input w) and\n\u2217 0\n\u03b4 (q ,x) (the state D reaches on input x). Call \u03b4 (q ,w) q , and\n\u2217 0 \u2217 0 w\ncall \u03b4 (q ,x) q . We know, because w = xa, there must be an a-\n\u2217 0 x\ntransition from q to q . Look at how we added transitions to \u03b4:\nx w\nthe fact that there is an a-transition from q to q means that q\nx w w\ncorrespondstotheset\u2202 (p ,a) ... \u2202 (p ,a)ofN\u2019sstates. Byour\n\u2217 1 \u2217 n\n\u222a \u222a\npreliminaryobservation,\u2202 (p ,a) ... \u2202 (p ,a)isjust\u2202 (p ,w). So\n\u2217 1 \u2217 n \u2217 0\n\u222a \u222a\nq (or \u03b4 (q ,w)) corresponds to \u2202 (p ,w), which is what we wanted\nw \u2217 0 \u2217 0\nto prove. Since w was an arbitrary string of length n+1, we have\nshown that the property holds for n+1.\nAltogether, we have shown by induction that \u03b4 (q ,w) corresponds to\n\u2217 0\n\u2202 (p ,w) for all w \u03a3 . As indicated at the very beginning of this proof,\n\u2217 0 \u2217\n\u2208\nthatisenoughtoprovethatL(D)=L(N). SoforanyNFAN,wecanfind\na DFA D that accepts the same language. 162 CHAPTER 3. REGULAR EXPRESSIONS AND FSA\u2019S\nExample 3.17. Consider the NFA shown below.\na,b\nb a,b a,b\np p p p\n0 1 2 3\nWe start by looking at \u2202 (p ,\u03b5), and then add transitions and states as\n\u2217 0\ndescribed above.\n\u2202 (p ,\u03b5)= p so q = p .\n\u2217 0 0 0 0\n\u2022 { } { }\n\u03b4(q ,a) will be \u2202 (p ,a), which is p , so \u03b4(q ,a)=q .\n0 \u2217 0 0 0 0\n\u2022 { }\n\u03b4(q ,b) will be \u2202 (p ,b), which is p ,p , so we need to add a new\n0 \u2217 0 0 1\n\u2022 { }\nstate q = p ,p to the DFA; and add \u03b4(q ,b) = q to the DFA\u2019s\n1 0 1 0 1\n{ }\ntransition function.\n\u03b4(q ,a) will be \u2202 (p ,a) unioned with \u2202 (p ,a) since q = p ,p .\n1 \u2217 0 \u2217 1 1 0 1\n\u2022 { }\nSince \u2202 (p ,a) \u2202 (p ,a)= p p = p ,p , we need to add a\n\u2217 0 \u2217 1 0 2 0 2\n\u222a { }\u222a{ } { }\nnew state q = p ,p to the DFA, and a transition \u03b4(q ,a)=q .\n2 0 2 1 2\n{ }\n\u03b4(q ,b)willbe\u2202 (p ,b)unionedwith\u2202 (p ,b), whichgives p ,p\n1 \u2217 0 \u2217 1 0 1\n\u2022 { }\u222a\np ,whichagaingivesusanewstateq toaddtotheDFA,together\n2 3\n{ }\nwith the transition \u03b4(q ,b)=q .\n1 3\nAt this point, our partially-constructed DFA looks as shown below:\nq q\n1 b 3\na {p 0 ,p 1 } {p 0 ,p 1 ,p 2 }\na\nq\nb 2\n{p } {p ,p }\n0 0 2\nq\n0\nTheconstructioncontinuesaslongastherearenewstatesbeingadded,\nand new transitions from those states that have to be computed. The final\nDFA is shown below.\nb\na {p 0 ,p 1 } {p 0 ,p 1 ,p 2} b b\na\nb b\nb a\n{p } {p ,p } {p ,p ,p } {p ,p ,p ,p }\n0 0 2 0 1 3 0 1 2 3\nb a a\nb a\na\n{p 0 ,p 3 } aa {p 0 ,p 2 ,p 3} 3.6. FINITE-STATE AUTOMATA AND REGULAR LANGUAGES 163\nExercises\n1. What language does the NFA in Example 3.17 accept?\n2. Give a DFA that accepts the language accepted by the following NFA.\na,b\na,b a\na\nb a,b\nb\n3. Give a DFA that accepts the language accepted by the following NFA. (Be\nsure to note that, for example, it is possible to reach both q1 and q3 from q0\non consumption of an a, because of the \u03b5-transition.)\nb a\nq q\na 1 3\na\nq b\n0\nb\nq q\n2 a 4\nb\na\n3.6 Finite-State Automata and Regular Lan-\nguages\nWeknownowthatourtwomodelsformechanical languagerecognition ac-\ntuallyrecognizethesameclassoflanguages. Thequestionstillremains: do\nthey recognize the same class of languages as the class generated mechan-\nically by regular expressions? The answer turns out to be \u201cyes\u201d. There\nare two parts to proving this: first that every language generated can be\nrecognized, and second that every language recognized can be generated.\nTheorem 3.3. Every language generated by a regular expression can be\nrecognized by an NFA.\nProof. The proof of this theorem is a nice example of a proof by induction\nonthestructureofregularexpressions. Thedefinitionofregularexpression\nisinductive: \u03a6,\u03b5,andaarethesimplestregularexpressions,andthenmore 164 CHAPTER 3. REGULAR EXPRESSIONS AND FSA\u2019S\ncomplicatedregularexpressionscanbebuiltfromthese. Wewillshowthat\nthereareNFAsthatacceptthelanguagesgeneratedbythesimplestregular\nexpressions,andthenshowhowthosemachinescanbeputtogethertoform\nmachines that accept languages generated by more complicated regular\nexpressions.\nConsider the regular expression \u03a6. L(\u03a6) = . Here is a machine that\n{}\naccepts :\n{}\nConsider the regular expression \u03b5. L(\u03b5) = \u03b5 . Here is a machine that\n{ }\naccepts \u03b5 :\n{ }\nConsider the regular expression a. L(a)= a . Here is a machine that\n{ }\naccepts a :\n{ }\na\nNow suppose that you have NFAs that accept the languages generated\nby the regular expressions r and r . Building a machine that accepts\n1 2\nL(r r )isfairlystraightforward: takeanNFAM thatacceptsL(r )and\n1 2 1 1\n|\nan NFA M that accepts L(r ). Introduce a new state q , connect it to\n2 2 new\nthe start states of M and M via \u03b5-transitions, and designate it as the\n1 2\nstart state of the new machine. No other transitions are added. The final\nstatesofM togetherwiththefinalstatesofM aredesignatedasthefinal\n1 2\nstates of the new machine. It should be fairly clear that this new machine\naccepts exactly those strings accepted by M together with those strings\n1\naccepted by M : any string w that was accepted by M will be accepted\n2 1\nby the new NFA by starting with an \u03b5-transition to the old start state of\nM andthenfollowingtheacceptingpaththroughM ;similarly,anystring\n1 1\naccepted by M will be accepted by the new machine; these are the only\n2\nstrings that will be accepted by the new machine, as on any input w all\nthe new machine can do is make an \u03b5-move to M \u2019s (or M \u2019s) start state,\n1 2\nandfromtherew willonlybeacceptedbythenewmachineifitisaccepted\nby M (or M ). Thus, the new machine accepts L(M ) L(M ), which is\n1 2 1 2\n\u222a\nL(r ) L(r ), which is exactly the definition of L(r r ).\n1 2 1 2\n\u222a | 3.6. FINITE-STATE AUTOMATA AND REGULAR LANGUAGES 165\nM\n1\nM :\nnew\nq\nnew\nM\n2\n(A pause before we continue: note that for the simplest regular expres-\nsions, the machines that we created to accept the languages generated by\nthe regular expressions were in fact DFAs. In our last case above, however,\nwe needed \u03b5-transitions to build the new machine, and so if we were trying\ntoprovethateveryregularlanguagecouldbeacceptedbyaDFA,ourproof\nwould be in trouble. THIS DOES NOT MEAN that the statement \u201cevery\nregular language can be accepted by a DFA\u201d is false, just that we can\u2019t\nprove it using this kind of argument, and would have to find an alternative\nproof.)\nSuppose you have machines M and M that accept L(r ) and L(r )\n1 2 1 2\nrespectively. TobuildamachinethatacceptsL(r )L(r )proceedasfollows.\n1 2\nMakethestartstateq ofM bethestartstateofthenewmachine. Make\n01 1\nthe final states of M be the final states of the new machine. Add \u03b5-\n2\ntransitions from the final states of M to the start state q of M .\n1 02 2\nq q\nM : 01 02\nnew\nM M\n1 final states 2 final states\nof M , of M ,\n1 2\nnot of M and of M\nnew new\nIt should be fairly clear that this new machine accepts exactly those\nstrings of the form xy where x L(r ) and y L(r ): first of all, any\n1 2\n\u2208 \u2208\nstring of this form will be accepted because x L(r ) implies there is a\n1\n\u2208\npath that consumes x from q to a final state of M ; a \u03b5-transition moves\n01 1\ntoq ; theny L(r )impliesthereisapaththatconsumesy fromq toa\n02 2 02\n\u2208\nfinal state of M ; and the final states of M are the final states of the new\n2 2\nmachine, so xy will be accepted. Conversely, suppose z is accepted by the\nnew machine. Since the only final states of the new machine are in the old\nM , and the only way to get into M is to take a \u03b5-transition from a final\n2 2\nstate of M , this means that z = xy where x takes the machine from its\n1 166 CHAPTER 3. REGULAR EXPRESSIONS AND FSA\u2019S\nstartstatetoafinalstateofM ,a\u03b5-transitionoccurs,andtheny takesthe\n1\nmachine from q to a final state of M . Clearly, x L(r ) and y L(r ).\n02 2 1 2\n\u2208 \u2208\nWe leave the construction of an NFA that accepts L(r ) from an NFA\n\u2217\nthat accepts L(r) as an exercise.\nTheorem 3.4. Every language that is accepted by a DFA or an NFA is\ngenerated by a regular expression.\nProving this result is actually fairly involved and not very illuminating.\nBefore presenting a proof, we will give an illustrative example of how one\nmight actually go about extracting a regular expression from an NFA or a\nDFA. You can go on to read the proof if you are interested.\nExample 3.18. Consider the DFA shown below:\na\nq a b\n0\na b\nq q\n1 2\nb\nNote that there is a loop from state q back to state q : any number of\n2 2\na\u2019s will keep the machine in state q , and so we label the transition with\n2\ntheregularexpressiona . Wedothesamethingtothetransitionlabeledb\n\u2217\nfromq . (NotethattheresultisnolongeraDFA,butthatdoesn\u2019tconcern\n0\nus, we\u2019re just interested in developing a regular expression.)\na*\nq a b\n0\na b\nq q\n1 2\nb*\nNext we note that there is in fact a loop from q to q via q . A regular\n1 1 0\nexpression that matches the strings that would move around the loop is\nab a. So we add a transition labeled ab a from q to q , and remove the\n\u2217 \u2217 1 1\nnow-irrelevant a-transition from q to q . (It is irrelevant because it is not\n1 0\npart of any other loop from q to q .)\n1 1\nq a q b a*\n0 1\nb\nq\n2\nb* ab* a 3.6. FINITE-STATE AUTOMATA AND REGULAR LANGUAGES 167\nNext we note that there is also a loop from q to q via q . A regular\n1 1 2\nexpression that matches the strings that would move around the loop is\nba b. Since the transitions in the loop are the only transitions to or from\n\u2217\nq , we simply remove q and replace it with a transition from q to q .\n2 2 1 1\nba* b\na\nb*\nab* a\nIt is now clear from the diagram that strings of the form b a get you to\n\u2217\nstate q , and any number of repetitions of strings that match ab a or ba b\n1 \u2217 \u2217\nwill keep you there. So the machine accepts L(b a(ab a ba b) ).\n\u2217 \u2217 \u2217 \u2217\n|\nProof of Theorem 3.4. We prove that the language accepted by a DFA is\nregular. The proof for NFAs follows from the equivalence between DFAs\nand NFAs.\nSuppose that M is a DFA, where M = (Q,\u03a3,q ,\u03b4,F). Let n be the\n0\nnumber of states in M, and write Q = q ,q ,...,q . We want to\n0 1 n 1\n{ \u2212 }\nconsidercomputationsinwhichM startsinsomestateq ,readsastringw,\ni\nand ends in state q . In such a computation, M might go through a series\nk\nof intermediates states between q and q :\ni k\nq p p p q\ni 1 2 r k\n\u2212\u2192 \u2212\u2192 \u00b7\u00b7\u00b7\u2212\u2192 \u2212\u2192\nWeareinterestedincomputationsinwhichalloftheintermediatestates\u2014\np ,p ,...,p \u2014are in the set q ,q ,...,q , for some number j. We\n1 2 r 0 1 j 1\n{ \u2212 }\ndefine R to be the set of all strings w in \u03a3 that are consumed by such\ni,j,k \u2217\na computation. That is, w R if and only if when M starts in state\ni,j,k\n\u2208\nq and reads w, it ends in state q , and all the intermediate states between\ni k\nq and q are in the set q ,q ,...,q . R is a language over \u03a3. We\ni k 0 1 j 1 i,j,k\n{ \u2212 }\nshow that R for 0 i<n, 0 j n, 0 k <n.\ni,j,k\n\u2264 \u2264 \u2264 \u2264\nConsider the language R . For w R , the set of allowable inter-\ni,0,k i,0,k\n\u2208\nmediatestatesisempty. Sincetherecanbenointermediatestates,itfollows\nthattherecanbeatmostonestepinthecomputationthatstartsinstateq ,\ni\nreadsw, andendsinstateq . So, w canbeatmostone. Thismeansthat\nk\n| |\nR isfinite,andhenceisregular. (Infact,R = a \u03a3 \u03b4(q ,a)=q ,\ni,0,k i,0,k i k\n{ \u2208 | }\nfor i = k, and R = \u03b5 a \u03a3 \u03b4(q ,a) = q . Note that in many\ni,0,i i i\n6 { }\u222a{ \u2208 | }\ncases, R will be the empty set.)\ni,0,k\nWe now proceed by induction on j to show that R is regular for all\ni,j,k\ni and k. We have proved the base case, j = 0. Suppose that 0 j < n\n\u2264 168 CHAPTER 3. REGULAR EXPRESSIONS AND FSA\u2019S\nwe already know that R is regular for all i and all k. We need to show\ni,j,k\nthat R is regular for all i and k. In fact,\ni,j+1,k\nR =R R R R\ni,j+1,k i,j,k\n\u222a\ni,j,j j\u2217,j,j j,j,k\nwhich is regular because R is regu(cid:0)lar for all i and(cid:1)k, and because the\ni,j,k\nunion, concatenation, and Kleene star of regular languages are regular.\nTo see that the above equation holds, consider a string w \u03a3 . Now,\n\u2217\n\u2208\nw R if and only if when M starts in state q and reads w, it\ni,j+1,k i\n\u2208\nends in state q , with all intermediate states in the computation in the\nk\nset q ,q ,...,q . Consider such a computation. There are two cases:\n0 1 j\n{ }\nEither q occurs as an intermediate state in the computation, or it does\nj\nnot. If it does not occur, then all the intermediate states are in the set\nq ,q ,...,q , which means that in fact w R . If q does occur\n0 1 j 1 i,j,k j\n{ \u2212 } \u2208\nas an intermediate state in the computation, then we can break the com-\nputation into phases, by dividing it at each point where q occurs as an\nj\nintermediate state. This breaks w into a concatenation w = xy y y z.\n1 2 r\n\u00b7\u00b7\u00b7\nThe string x is consumed in the first phase of the computation, during\nwhich M goes from state q to the first occurrence of q ; since the interme-\ni j\ndiate states in this computation are in the set q ,q ,...,q , x R .\n0 1 j 1 i,j,j\n{ \u2212 } \u2208\nThe string z is consumed by the last phase of the computation, in which\nM goes from the final occurrence of q to q , so that z R . And each\nj k j,j,k\n\u2208\nstring y is consumed in a phase of the computation in which M goes from\nt\none occurrence of q to the next occurrence of q , so that y R . This\nj j r j,j,j\n\u2208\nmeans that w =xy y y z R R R .\n1 2\n\u00b7\u00b7\u00b7\nr\n\u2208\ni,j,j j\u2217,j,j j,j,k\nWe now know, in particular, that R is a regular language for all\n0,n,k\nk. But R consists of all strings w \u03a3 such that when M starts\n0,n,k \u2217\n\u2208\nin state q and reads w, it ends in state q (with no restriction on the\n0 k\nintermediate states in the computation, since every state of M is in the\nset q ,q ,...,q ). To finish the proof that L(M) is regular, it is only\n0 1 n 1\n{ \u2212 }\nnecessary to note that\nL(M)= R\n0,n,k\nqk[\u2208F\nwhich is regular since it is a union of regular languages. This equation is\ntrue since a string w is in L(M) if and only if when M starts in state q\n0\nand reads w, in ends in some accepting state q F. This is the same as\nk\n\u2208\nsaying w R for some k with q F.\n0,n,k k\n\u2208 \u2208\nWe have already seen that if two languages L and L are regular, then\n1 2\nso are L L , L L , and L (and of course L ). We have not yet seen,\n1\n\u222a\n2 1 2 \u22171 \u22172\nhowever,howthecommonsetoperationsintersectionandcomplementation 3.6. FINITE-STATE AUTOMATA AND REGULAR LANGUAGES 169\naffect regularity. Is the complement of a regular language regular? How\nabout the intersection of two regular languages?\nBothofthesequestionscanbeansweredbythinkingofregularlanguages\nin terms of their acceptance by DFAs. Let\u2019s consider first the question of\ncomplementation. Suppose we have an arbitrary regular language L. We\nknow there is a DFA M that accepts L. Pause a moment and try to think\nof a modification that you could make to M that would produce a new\nmachine M that accepts L.... Okay, the obvious thing to try is to make\n\u2032\nM be a copy of M with all final states of M becoming non-final states of\n\u2032\nM and vice versa. This is in fact exactly right: M does in fact accept L.\n\u2032 \u2032\nTo verify this, consider an arbitrary string w. The transition functions for\nthe two machines M and M are identical, so \u03b4 (q ,w) is the same state\n\u2032 \u2217 0\nin both M and M ; if that state is accepting in M then it is non-accepting\n\u2032\nin M , so if w is accepted by M it is not accepted by M ; if the state is\n\u2032 \u2032\nnon-accepting in M then it is accepting in M , so if w is not accepted by\n\u2032\nM then it is accepted by M . Thus M accepts exactly those strings that\n\u2032 \u2032\nM does not, and hence accepts L.\nIt is worth pausing for a moment and looking at the above argument\na bit longer. Would the argument have worked if we had looked at an\narbitrary language L and an arbitrary NFA M that accepted L? That is,\nif we had built a new machine M in which the final and non-final states\n\u2032\nhad been switched, would the new NFA M accept the complement of the\n\u2032\nlanguageacceptedbyM? Theansweris\u201cnotnecessarily\u201d. Rememberthat\nacceptance in an NFA is determined based on whether or not at least one\nof the states reached by a string is accepting. So any string w with the\nproperty that \u2202 (q ,w) contains both accepting and non-accepting states\n\u2217 0\nof M would be accepted both by M and by M .\n\u2032\nNow let\u2019s turn to the question of intersection. Given two regular lan-\nguages L and L , is L L regular? Again, it is useful to think in terms\n1 2 1 2\n\u2229\nof DFAs: given machines M and M that accept L and L , can you use\n1 2 1 2\nthem to build a new machine that accepts L L ? The answer is yes, and\n1 2\n\u2229\nthe idea behind the construction bears some resemblance to that behind\nthe NFA-to-DFA construction. We want a new machine where transitions\nreflect the transitions of both M and M simultaneously, and we want to\n1 2\naccept a string w only if that those sequences of transitions lead to final\nstates in both M and M . So we associate the states of our new ma-\n1 2\nchine M with pairs of states from M and M . For each state (q ,q ) in\n1 2 1 2\nthe new machine and input symbol a, define \u03b4((q ,q ),a) to be the state\n1 2\n(\u03b4 (q ,a),\u03b4 (q ,a)). The start state q of M is (q ,q ), where q is the\n1 1 2 2 0 01 02 0i\nstart state of M . The final states of M are the the states of the form\ni\n(q ,q ) where q is an accepting state of M and q is an accepting\nf1 f2 f1 1 f2 170 CHAPTER 3. REGULAR EXPRESSIONS AND FSA\u2019S\nstate of M . You should convince yourself that M accepts a string x iff x\n2\nis accepted by both M and M .\n1 2\nThe results of the previous section and the preceding discussion are\nsummarized by the following theorem:\nTheorem 3.5. The intersection of two regular languages is a regular lan-\nguage.\nThe union of two regular languages is a regular language.\nThe concatenation of two regular languages is a regular language.\nThe complement of a regular language is a regular language.\nThe Kleene closure of a regular language is a regular language.\nExercises\n1. Give a DFA that accepts the intersection of the languages accepted by the\nmachines shown below. (Suggestion: use the construction discussed in the\nchapter just before Theorem 3.5.)\nb a b b b\na a a\nq 0 q 1 p 0 p 1 p 2\nb\na\n2. CompletetheproofofTheorem3.3byshowinghowtomodifyamachinethat\naccepts L(r) into a machine that accepts L(r\u2217).\n3. Using theconstruction described in Theorem3.3, build an NFA that accepts\nL((ab a)\u2217(bb)).\n|\n4. Prove that the reverse of a regular language is regular.\n5. ShowthatforanyDFAorNFA,thereisanNFAwithexactlyonefinalstate\nthat accepts the same language.\n6. Suppose we change the model of NFAs to allow NFAs to have multiple start\nstates. Show that for any \u201cNFA\u201d with multiple start states, there is an NFA\nwith exactly one start state that accepts the same language.\n7. Suppose that M1 = (Q1,\u03a3,q1,\u03b41,F1) and M2 = (Q2,\u03a3,q2,\u03b42,F2) are DFAs\nover the alphabet \u03a3. It is possible to construct a DFA that accepts the\nlangauge L(M1) L(M2) in a single step. Define the DFA\n\u2229\nM =(Q1 Q2,\u03a3,(q1,q2),\u03b4,F1 F2)\n\u00d7 \u00d7\nwhere \u03b4 is the function from (Q1 Q2) \u03a3 to Q1 Q2 that is defined by:\n\u00d7 \u00d7 \u00d7\n\u03b4((p1,p2),\u03c3)) = (\u03b41(p1,\u03c3),\u03b42(p2,\u03c3)). Convince yourself that this definition\nmakessense. (Forexample, notethatstatesinM arepairs(p1,p2)ofstates,\nwhere p1 Q1 and p2 Q2, and note that the start state (q1,q2) in M is\n\u2208 \u2208 3.7. NON-REGULAR LANGUAGES 171\nin fact a state in M.) Prove that L(M)=L(M1) L(M2), and explain why\n\u2229\nthisshowsthattheintersectionofanytworegularlanguagesisregular. This\nproof\u2014ifyoucangetpastthenotation\u2014ismoredirectthantheoneoutlined\nabove.\n3.7 Non-regular Languages\nThe fact that our models for mechanical language-recognition accept ex-\nactly the same languages as those generated by our mechanical language-\ngenerationsystemwouldseemtobeaverypositiveindicationthatin\u201creg-\nular\u201d we have in fact managed to isolate whatever characteristic it is that\nmakes a language \u201cmechanical\u201d. Unfortunately, there are languages that\nwe intuitively think of as being mechanically-recognizable (and which we\ncould write C++ programs to recognize) that are not in fact regular.\nHowdoesoneprovethatalanguageisnotregular? Wecouldtryproving\nthat there is no DFA or NFA that accepts it, or no regular expression\nthat generates it, but this kind of argument is generally rather difficult\nto make. It is hard to rule out all possible automata and all possible\nregular expressions. Instead, we will look at a property that all regular\nlanguages have; proving that a given language does not have this property\nthen becomes a way of proving that that language is not regular.\nConsider the language L = w a,b n (w) = 2mod3, n (w) =\n\u2217 a b\n{ \u2208 { } |\n2mod3 . BelowisaDFAthatacceptsthislanguage,withstatesnumbered\n}\n1 through 9.\na\na a\n1 2 3\nb b b\na\n4 a 5 a 6\nb b b\nb b b\na\n7 a 8 a 9\nConsider the sequence of states that the machine passes through while\nprocessing the string abbbabb. Note that there is a repeated state (state\n2). We say that abbbabb \u201cgoes through the state 2 twice\u201d, meaning that in\nthe course of the string being processed, the machine is in state 2 twice (at 172 CHAPTER 3. REGULAR EXPRESSIONS AND FSA\u2019S\nleast). Call the section of the string that takes you around the loop y, the\npreceding section x, and the rest z. Then xz is accepted, xyyz is accepted,\nxyyyz is accepted, etc. Note that the string aabb cannot be divided this\nway, because it does not go through the same state twice. Which strings\ncan be divided this way? Any string that goes through the same state\ntwice. Thismayincludesomerelativelyshortstringsandmustincludeany\nstringwithlengthgreaterthanorequalto9,becausethereareonly9states\nin the machine, and so repetition must occur after 9 input symbols at the\nlatest.\nMore generally, consider an arbitrary DFA M, and let the number of\nstates in M be n. Then any string w that is accepted by M and has n or\nmore symbols must go through the same state twice, and can therefore be\nbroken up into three pieces x,y,z (where y contains at least one symbol)\nso that w =xyz and\nxz is accepted by M\nxyz is accepted by M (after all, we started with w in L(M))\nxyyz is accepted by M\netc.\nNote that you can actually say even more: within the first n characters\nofwyoumustalreadygetarepeatedstate,soyoucanalwaysfindanx,y,z\nasdescribedabovewhere,inaddition,thexyportionofw(theportionofw\nthattakesyoutoandbacktoarepeatedstate)containsatmostnsymbols.\nSo altogether, if M is an n-state DFA that accepts L, and w is a string\ninLwhoselengthisatleastn,thenwcanbebrokendownintothreepieces\nx, y, and z, w =xyz, such that\n(i) x and y together contain no more than n symbols;\n(ii) y contains at least one symbol;\n(iii) xz is accepted by M\n(xyz is accepted by M)\nxyyz is accepted by M\netc.\nThe usually-stated form of this result is the Pumping Lemma:\nTheorem 3.6. If L is a regular language, then there is some number n>0\nsuch that any string w in L whose length is greater than or equal to n can\nbe broken down into three pieces x, y, and z, w =xyz, such that\n(i) x and y together contain no more than n symbols;\n(ii) y contains at least one symbol;\n(iii) xz is accepted by M 3.7. NON-REGULAR LANGUAGES 173\n(xyz is accepted by M)\nxyyz is accepted by M\netc.\nThough the Pumping Lemma says something about regular languages,\nit is not used to prove that languages are regular. It says \u201cif a language is\nregular, then certain things happen\u201d, not \u201cif certain things happen, then\nyou can conclude that the language is regular.\u201d However, the Pumping\nLemma is useful for proving that languages are not regular, since the con-\ntrapositive of \u201cif a language is regular then certain things happen\u201d is \u201cif\ncertainthingsdon\u2019thappenthenyoucanconcludethatthelanguageisnot\nregular.\u201dSowhatarethe\u201ccertainthings\u201d? Basically,theP.L.saysthatifa\nlanguage is regular, there is some \u201cthreshold\u201d length for strings, and every\nstring that goes over that threshold can be broken down in a certain way.\nTherefore, if we can show that \u201cthere is some threshold length for strings\nsuch that every string that goes over that threshold can be broken down in\na certain way\u201d is a false assertion about a language, we can conclude that\nthe language is not regular. How do you show that there is no threshold\nlength? Saying a number is a threshold length for a language means that\nevery string in the language that is at least that long can be broken down\nin the ways described. So to show that a number is not a threshold value,\nwe have to show that there is some string in the language that is at least\nthat long that cannot be broken down in the appropriate way.\nTheorem 3.7. anbn n 0 is not regular.\n{ | \u2265 }\nProof. We do this by showing that there is no threshold value for the lan-\nguage. Let N be an arbitrary candidate for threshold value. We want to\nshow that it is not in fact a threshold value, so we want to find a string in\nthe language whose length is at least N and which can\u2019t be broken down\nin the way described by the Pumping Lemma. What string should we try\nto prove unbreakable? We can\u2019t pick strings like a100b100 because we\u2019re\nworking with an arbitrary N i.e. making no assumptions about N\u2019s value;\npicking a100b100 is implicitly assuming that N is no bigger than 200 \u2014 for\nlarger values of N, a100b100 would not be \u201ca string whose length is at least\nN\u201d. Whateverstringwepick, wehavetobesurethatitslengthisatleast\nN,nomatterwhatnumberN is. Sowepick,forinstance,w =aNbN. This\nstringisinthelanguage, anditslengthisatleastN, nomatterwhatnum-\nberN is. Ifwecanshowthatthisstringcan\u2019tbebrokendownasdescribed\nby the Pumping Lemma, then we\u2019ll have shown that N doesn\u2019t work as a\nthreshold value, and since N was an arbitrary number, we will have shown 174 CHAPTER 3. REGULAR EXPRESSIONS AND FSA\u2019S\nthat there is no threshold value for L and hence L is not regular. So let\u2019s\nshow that w =aNbN can\u2019t be broken down appropriately.\nWe need to show that you can\u2019t write w = aNbN as w = xyz where\nx and y together contain at most N symbols, y isn\u2019t empty, and all the\nstrings xz, xyyz, xyyyz, etc. are still in L, i.e. of the form anbn for some\nnumbern. Thebestwaytodothisistoshowthatanychoicefory (withx\nbeing whatever precedes it and z being whatever follows) that satisfies the\nfirst two requirements fails to satisfy the third. So what are our possible\nchoicesfory? Well, sincexandy togethercancontainatmostN symbols,\nand w starts with N a\u2019s, both x and y must be made up entirely of a\u2019s;\nsinceycan\u2019tbeempty,itmustcontainatleastoneaand(from(i))nomore\nthan N a\u2019s. So the possible choices for y are y = ak for some 1 k N.\n\u2264 \u2264\nWe want to show now that none of these choices will satisfy the third\nrequirement by showing that for any value of k, at least one of the strings\nxz, xyyz, xyyyz, etc will not be in L. No matter what value we try for k,\nwedon\u2019thavetolookfarforourroguestring: thestringxz,whichisaNbN\nwith k a\u2019s deleted from it, looks like aN kbN, which is clearly not of the\n\u2212\nform anbn. So the only y\u2019s that satisfy (i) and (ii) don\u2019t satisfy (iii); so w\ncan\u2019t be broken down as required; so N is not a threshold value for L; and\nsince N was an arbitrary number, there is no threshold value for L; so L is\nnot regular.\nThe fact that languages like anbn n 0 and ap p is prime are\n{ | \u2265 } { | }\nnot regular is a severe blow to any idea that regular expressions or finite-\nstate automata capture the language-generation or language-recognition\ncapabilities of a computer: They are both languages that we could easily\nwrite programs to recognize. It is not clear how the expressive power of\nregular expressions could be increased, nor how one might modify the FSA\nmodel to obtain a more powerful one. However, in the next chapter you\nwill be introduced to the concept of a grammar as a tool for generating\nlanguages. Thesimplestgrammarsstillonlyproduceregularlanguages,but\nyou will see that more complicated grammars have the power to generate\nlanguages far beyond the realm of the regular.\nExercises\n1. UsethePumpingLemmatoshowthatthefollowinglanguagesover a,b are\n{ }\nnot regular.\na) L1 = x n a(x)=n b(x)\nb) L2 ={ xx| x a,b \u2217 }\nc) L3\n={ xxR| x\u2208{ a,} b}\u2217\nd) L4\n={ anbm| n\u2208 <{\nm\n} }\n{ | } Chapter 4\nGrammars\nB\noth natural languages, such as English and the artificial languages\nused for programming have a structure known as grammar or syntax. In\norder to form legal sentences or programs, the parts of the language must\nbe fit together according to certain rules. For natural languages, the rules\nare somewhat informal (although high-school English teachers might have\nus believe differently). For programming languages, the rules are absolute,\nand programs that violate the rules will be rejected by a compiler.\nInthischapter,wewillstudyformalgrammarsandlanguagesdefinedby\nthem. Thelanguageswelookatwill,forthemostpart,be\u201ctoy\u201dlanguages,\ncompared to natural languages or even to programming languages, but the\nideas and techniques are basic to any study of language. In fact, many of\nthe ideas arose almost simultaneously in the 1950s in the work of linguists\nwho were studying natural language and programmers who were looking\nfor ways to specify the syntax of programming languages.\nThegrammarsinthischapteraregenerativegrammars. Agenerative\ngrammar is a set of rules that can be used to generate all the legal strings\nin a language. We will also consider the closely related idea of parsing.\nTo parse a string means to determine how that string can be generated\naccording to the rules.\nThis chapter is a continuation of the preceding chapter. Like a regular\nexpression, a grammar is a way to specify a possibly infinite language with\na finite amount of information. In fact, we will see that every regular\nlanguage can be specified by a certain simple type of grammar. We will\nalso see that some languages that can be specified by grammars are not\nregular.\n175 176 CHAPTER 4. GRAMMARS\n4.1 Context-free Grammars\nInitsmostgeneralform, agrammarisasetofrewriting rules. Arewrit-\ning rule specifies that a certain string of symbols can be substituted for all\nor part of another string. If w and u are strings, then w u is a rewrit-\n\u2212\u2192\ning rule that specifies that the string w can be replaced by the string u.\nThe symbol \u201c \u201d is read \u201ccan be rewritten as.\u201d Rewriting rules are also\n\u2212\u2192\ncalled production rules or productions, and \u201c \u201d can also be read as\n\u2212\u2192\n\u201cproduces.\u201d For example, if we consider strings over the alphabet a,b,c ,\n{ }\nthen the production rule aba cc can be applied to the string abbabac\n\u2212\u2192\nto give the string abbccc. The substring aba in the string abbabac has been\nreplaced with cc.\nIn a context-free grammar, every rewriting rule has the form A\n\u2212\u2192\nw,whereAissinglesymbolandwisastringofzeroormoresymbols. (The\ngrammaris\u201ccontext-free\u201dinthesensethatwcanbesubstitutedforAwher-\never A occurs in a string, regardless of the surrounding context in which A\noccurs.) The symbols that occur on the left-hand sides of production rules\nin a context-free grammar are called non-terminal symbols. By conven-\ntion, the non-terminal symbols are usually uppercase letters. The strings\non the right-hand sides of the production rules can include non-terminal\nsymbolsaswellasothersymbols,whicharecalledterminal symbols. By\nconvention, the terminal symbols are usually lowercase letters. Here are\nsome typical production rules that might occur in context-free grammars:\nA aAbB\n\u2212\u2192\nS SS\n\u2212\u2192\nC Acc\n\u2212\u2192\nB b\n\u2212\u2192\nA \u03b5\n\u2212\u2192\nIn the last rule in this list, \u03b5 represents the empty string, as usual. For\nexample, this rule could be applied to the string aBaAcA to produce the\nstring aBacA. The first occurrence of the symbol A in aBaAcA has been\nreplaced bythe empty string\u2014which isjust another way of saying that the\nsymbol has been dropped from the string.\nIn every context-free grammar, one of the non-terminal symbols is des-\nignated as the start symbol of the grammar. The start symbol is often,\nthough not always, denoted by S. When the grammar is used to generate\nstringsinalanguage,theideaistostartwithastringconsistingofnothing\nbutthestartsymbol. Thenasequenceofproductionrulesisapplied. Each\napplication of a production rule to the string transforms the string to a 4.1. CONTEXT-FREE GRAMMARS 177\nnew string. If and when this process produces a string that consists purely\nof terminal symbols, the process ends. That string of terminal symbols is\none of the strings in the language generated by the grammar. In fact, the\nlanguage consists precisely of all strings of terminal symbols that can be\nproduced in this way.\nAs a simple example, consider a grammar that has three production\nrules: S aS, S bS, and S b. In this example, S is the only\n\u2212\u2192 \u2212\u2192 \u2212\u2192\nnon-terminal symbol, and the terminal symbols are a and b. Starting from\nthestringS,wecanapplyanyofthethreerulesofthegrammartoproduce\neither aS, bS, or b. Since the string b contains no non-terminals, we see\nthatbisoneofthestringsinthelanguagegeneratedbythisgrammar. The\nstrings aS and bS are not in that language, since they contain the non-\nterminal symbol S, but we can continue to apply production rules to these\nstrings. From aS, for example, we can obtain aaS, abS, or ab. From abS,\nwe go on to obtain abaS, abbS, or abb. The strings ab and abb are in the\nlanguage generated by the grammar. It\u2019s not hard to see that any string\nof a\u2019s and b\u2019s that ends with a b can be generated by this grammar, and\nthatthesearetheonlystringsthatcanbegenerated. Thatis,thelanguage\ngenerated by this grammar is the regular language specified by the regular\nexpression (a b) b.\n\u2217\n|\nIt\u2019s time to give some formal definitions of the concepts which we have\nbeen discussing.\nDefinition4.1. Acontext-freegrammarisa4-tuple(V,\u03a3,P,S),where:\n1. V is a finite set of symbols. The elements of V are the non-terminal\nsymbols of the grammar.\n2. \u03a3 is a finite set of symbols such that V \u03a3= . The elements of \u03a3 are\n\u2229 \u2205\nthe terminal symbols of the grammar.\n3. P is a set of production rules. Each rule is of the form A w where\n\u2212\u2192\nAisoneofthesymbolsinV andw isastringinthelanguage(V \u03a3) .\n\u2217\n\u222a\n4. S V. S is the start symbol of the grammar.\n\u2208\nEven though this is the formal definition, grammars are often specified\ninformally simply by listing the set of production rules. When this is done\nitisassumed,unlessotherwisespecified,thatthenon-terminalsymbolsare\njust the symbols that occur on the left-hand sides of production rules of\nthe grammar. The terminal symbols are all the other symbols that occur\nontheright-handsidesofproductionrules. Thestartsymbolisthesymbol\nthat occurs on the left-hand side of the first production rule in the list. 178 CHAPTER 4. GRAMMARS\nThus, the list of production rules\nT TT\n\u2212\u2192\nT A\n\u2212\u2192\nA aAa\n\u2212\u2192\nA bB\n\u2212\u2192\nB bB\n\u2212\u2192\nB \u03b5\n\u2212\u2192\nspecifies a grammar G=(V,\u03a3,P,T) where V is T,A,B , \u03a3 is a,b , and\n{ } { }\nT is the start symbol. P, of course, is a set containing the six production\nrules in the list.\nLet G = (V,\u03a3,P,S) be a context-free grammar. Suppose that x and\ny are strings in the language (V \u03a3) . The notation x = y is used\n\u2217 G\n\u222a \u21d2\nto express the fact that y can be obtained from x by applying one of the\nproduction rules in P. To be more exact, we say that x= y if and only\nG\n\u21d2\nif there is a production rule A w in the grammar and two strings u\n\u2212\u2192\nand v in the language (V \u03a3) such that x=uAv and y =uwv. The fact\n\u2217\n\u222a\nthat x=uAv is just a way of saying that A occurs somewhere in x. When\nthe production rule A w is applied to substitute w for A in uAv, the\n\u2212\u2192\nresult is uwv, which is y. Note that either u or v or both can be the empty\nstring.\nIf a string y can be obtained from a string x by applying a sequence of\nzeroormoreproductionrules,wewritex= y. Inmostcases,the\u201cG\u201din\n\u21d2\u2217G\nthe notations = and = will be omitted, assuming that the grammar\n\u21d2G \u21d2\u2217G\nin question is understood. Note that = is a relation on the set (V \u03a3) .\n\u2217\n\u21d2 \u222a\nThe relation = is the reflexive, transitive closure of that relation. (This\n\u2217\n\u21d2\nexplains the use of \u201c \u201d, which is usually used to denote the transitive, but\n\u2217\nnotnecessarilyreflexive, closureofarelation. Inthiscase, = isreflexive\n\u2217\n\u21d2\nas well as transitive since x = x is true for any string x.) For example,\n\u2217\n\u21d2\nusing the grammar that is defined by the above list of production rules, we\nhave\naTB = aTTB\n\u21d2\n= aTAB\n\u21d2\n= aTAbB\n\u21d2\n= aTbBbB\n\u21d2\n= aTbbB\n\u21d2\nFrom this, it follows that aTB = aTbbB. The relation = is read\n\u2217\n\u21d2 \u21d2\n\u201cyields\u201d or \u201cproduces\u201d while = can be read \u201cyields in zero or more\n\u2217\n\u21d2 4.1. CONTEXT-FREE GRAMMARS 179\nsteps\u201d or \u201cproduces in zero or more steps.\u201d The following theorem states\nsome simple facts about the relations = and = :\n\u2217\n\u21d2 \u21d2\nTheorem 4.1. Let G be the context-free grammar (V,\u03a3,P,S). Then:\n1. If x and y are strings in (V \u03a3) such that x= y, then x= y.\n\u2217 \u2217\n\u222a \u21d2 \u21d2\n2. If x, y, and z are strings in (V \u03a3) such that x = y and y = z,\n\u2217 \u2217 \u2217\n\u222a \u21d2 \u21d2\nthen x= z.\n\u2217\n\u21d2\n3. If x and y are strings in (V \u03a3) such that x= y, and if s and t are\n\u2217\n\u222a \u21d2\nany strings in (V \u03a3) , then sxt= syt.\n\u2217\n\u222a \u21d2\n4. If x and y are strings in (V \u03a3) such that x= y, and if s and t are\n\u2217 \u2217\n\u222a \u21d2\nany strings in (V \u03a3) , then sxt= syt.\n\u2217 \u2217\n\u222a \u21d2\nProof. Parts1and2followfromthefactthat= isthetransitiveclosure\n\u2217\n\u21d2\nof = . Part 4 follows easily from Part 3. (I leave this as an exercise.) To\n\u21d2\nprove Part 3, suppose that x, y, s, and t are strings such that x= y. By\n\u21d2\ndefinition,thismeansthatthereexiststringsuandv andaproductionrule\nA w such that x = uAv and y = uwv. But then we also have sxt =\n\u2212\u2192\nsuAvt and syt = suwvt. These two equations, along with the existence of\nthe production rule A w show, by definition, that sxt= syt.\n\u2212\u2192 \u21d2\nWe can use = to give a formal definition of the language generated\n\u2217\n\u21d2\nby a context-free grammar:\nDefinition 4.2. Suppose that G=(V,\u03a3,P,S) is a context-free grammar.\nThen the language generated by G is the language L(G) over the alphabet\n\u03a3 defined by\nL(G)= w \u03a3 S = w\n{ \u2208\n\u2217\n|\n\u21d2\u2217G\n}\nThatis,L(G)containsanystringofterminalsymbolsthatcanbeobtained\nby starting with the string consisting of the start symbol, S, and applying\na sequence of production rules.\nA language L is said to be a context-free language if there is a\ncontext-free grammar G such that L(G) is L. Note that there might be\nmany different context-free grammars that generate the same context-free\nlanguage. Twocontext-freegrammarsthatgeneratethesamelanguageare\nsaid to be equivalent.\nSuppose G is a context-free grammar with start symbol S and suppose\nw L(G). Bydefinition,thismeansthatthereisasequenceofoneormore\n\u2208\napplications of production rules which produces the string w from S. This\nsequence has the form S = x = x = = w. Such a sequence\n1 2\n\u21d2 \u21d2 \u21d2 \u00b7\u00b7\u00b7 \u21d2\nis called a derivation of w (in the grammar G). Note that w might have\nmore than one derivation. That is, it might be possible to produce w in\nseveral different ways. 180 CHAPTER 4. GRAMMARS\nConsider the language L = anbn n N . We already know that L\n{ | \u2208 }\nis not a regular language. However, it is a context-free language. That is,\nthere is a context-free grammar such that L is the language generated by\nG. This gives us our first theorem about grammars:\nTheorem 4.2. Let L be the language L = anbn n N . Let G be\n{ | \u2208 }\nthe context-free grammar (V,\u03a3,P,S) where V = S , \u03a3 = a,b and P\n{ } { }\nconsists of the productions\nS aSb\n\u2212\u2192\nS \u03b5\n\u2212\u2192\nThen L = L(G), so that L is a context-free language. In particular, there\nexist context-free languages which are not regular.\nProof. To show that L = L(G), we must show both that L L(G) and\n\u2286\nthat L(G) L. To show that L L(G), let w be an arbitrary element\n\u2286 \u2286\nof L. By definition of L, w = anbn for some n N. We show that\n\u2208\nw L(G) by induction on n. In the case where n = 0, we have w = \u03b5.\n\u2208\nNow, \u03b5 L(G) since \u03b5 can be produced from the start symbol S by an\n\u2208\napplication of the rule S \u03b5, so our claim is true for n = 0. Now,\n\u2212\u2192\nsuppose that k N and that we already know that akbk L(G). We must\n\u2208 \u2208\nshow that ak+1bk+1 L(G). Since S = akbk, we also have, by Theorem\n\u2217\n\u2208 \u21d2\n4.1, that aSb = aakbkb. That is, aSb = ak+1bk+1. Combining this\n\u2217 \u2217\n\u21d2 \u21d2\nwith the production rule S aSb, we see that S = ak+1bk+1. This\n\u2217\n\u2212\u2192 \u21d2\nmeans that ak+1bk+1 L(G), as we wanted to show. This completes the\n\u2208\nproof that L L(G).\n\u2286\nToshowthatL(G) L,supposethatw L(G). Thatis,S = w. We\n\u2217\n\u2286 \u2208 \u21d2\nmust show that w =anbn for some n. Since S = w, there is a derivation\n\u2217\n\u21d2\nS = x = x = = x ,wherew =x . Wefirstprovebyinduction\n0 1 n n\n\u21d2 \u21d2 \u21d2\u00b7\u00b7\u00b7 \u21d2\non n that in any derivation S = x = x = = x , we must have\n0 1 n\n\u21d2 \u21d2 \u21d2\u00b7\u00b7\u00b7 \u21d2\neither x = anbn or x = an+1Sbn+1. Consider the case n = 0. Suppose\nn n\nS = x . Then,wemusthavethatS x isaruleinthegrammar,sox\n0 0 0\n\u21d2 \u2212\u2192\nmust be either \u03b5 or aSb. Since \u03b5=a0b0 and aSb=a0+1Sb0+1, x is of the\n0\nrequired form. Next, consider the inductive case. Suppose that k > 1 and\nwe already know that in any derivation S = x = x = = x ,\n0 1 k\n\u21d2 \u21d2 \u21d2 \u00b7\u00b7\u00b7 \u21d2\nwe must have x = akbk or x = ak+1Sbk+1. Suppose that S = x =\nk 0\n\u21d2 \u21d2\nx = = x = x . We know by induction that x = akbk or\n1 k k+1 k\n\u21d2 \u00b7\u00b7\u00b7 \u21d2 \u21d2\nx = ak+1Sbk+1, but since x = x and akbk contains no non-terminal\nk k+1\n\u21d2\nsymbols,wemusthavex =ak+1Sbk+1. Sincex isobtainedbyapplying\nk k+1\none of the production rules S \u03b5 or S aSb to x , x is either\nk k+1\n\u2212\u2192 \u2212\u2192\nak+1\u03b5bk+1 or ak+1aSbbk+1. That is, x is either ak+1bk+1 or ak+2Sbk+2,\nk+1 4.1. CONTEXT-FREE GRAMMARS 181\nas we wanted to show. This completes the induction. Turning back to w,\nwe see that w must be of the form anbn or of the form anSbn. But since\nw L(G),itcancontainnonon-terminalsymbols,sowmustbeoftheform\n\u2208\nanbn, as we wanted to show. This completes the proof that L(G) L.\n\u2286\nI have given a very formal and detailed proof of this theorem, to show\nhow it can be done and to show how induction plays a role in many proofs\nabout grammars. However, a more informal proof of the theorem would\nprobably be acceptable and might even be more convincing. To show that\nL L(G), wecouldjustnotethatthederivationS = aSb= a2Sb2 =\n\u2286 \u21d2 \u21d2 \u21d2\n= anSbn = anbn demonstrates that anbn L. On the other hand,\n\u00b7\u00b7\u00b7 \u21d2 \u21d2 \u2208\nit is clear that every derivation for this grammar must be of this form, so\nevery string in L(G) is of the form anbn.\nFor another example, consider the language anbm n m 0 . Let\u2019s\n{ | \u2265 \u2265 }\ntrytodesignagrammarthatgeneratesthislanguage. Thisissimilartothe\npreviousexample,butnowwewanttoincludestringsthatcontainmorea\u2019s\nthanb\u2019s. TheproductionruleS aSbalwaysproducesthesamenumber\n\u2212\u2192\nof a\u2019s and b\u2019s. Can we modify this idea to produce more a\u2019s than b\u2019s?\nOne approach would be to produce a string containing just as many\na\u2019s as b\u2019s, and then to add some extra a\u2019s. A rule that can generate any\nnumber of a\u2019s is A aA. After applying the rule S aSb for a while,\n\u2212\u2192 \u2212\u2192\nwe want to move to a new state in which we apply the rule A aA. We\n\u2212\u2192\ncan get to the new state by applying a rule S A that changes the S\n\u2212\u2192\ninto an A. We still need a way to finish the process, which means getting\nrid of all non-terminal symbols in the string. For this, we can use the rule\nA \u03b5. Putting these rules together, we get the grammar\n\u2212\u2192\nS aSb\n\u2212\u2192\nS A\n\u2212\u2192\nA aA\n\u2212\u2192\nA \u03b5\n\u2212\u2192\nThis grammar does indeed generate the language anbm n m 0 .\n{ | \u2265 \u2265 }\nWith slight variations on this grammar, we can produce other related lan-\nguages. For example, if we replace the rule A \u03b5 with A a, we get\n\u2212\u2192 \u2212\u2192\nthe language anbm n>m 0 .\n{ | \u2265 }\nThereareotherwaystogeneratethelanguage anbm n m 0 . For\n{ | \u2265 \u2265 }\nexample, the extra non-terminal symbol, A, is not really necessary, if we\nallow S to sometimes produce a single a without a b. This leads to the 182 CHAPTER 4. GRAMMARS\ngrammar\nS aSb\n\u2212\u2192\nS aS\n\u2212\u2192\nS \u03b5\n\u2212\u2192\n(But note that the rule S Sa would not work in place of S aS,\n\u2212\u2192 \u2212\u2192\nsince it would allow the production of strings in which an a can follow a b,\nand there are no such strings in the language anbm n m 0 .) And\n{ | \u2265 \u2265 }\nhere are two more grammars that generate this language:\nS AB S ASb\n\u2212\u2192 \u2212\u2192\nA aA A aA\n\u2212\u2192 \u2212\u2192\nB aBb S \u03b5\n\u2212\u2192 \u2212\u2192\nA \u03b5 A \u03b5\n\u2212\u2192 \u2212\u2192\nB \u03b5\n\u2212\u2192\nConsideranothervariationonthelanguage anbn n N ,inwhichthe\n{ | \u2208 }\na\u2019sandb\u2019scanoccurinanyorder,butthenumberofa\u2019sisstillequaltothe\nnumber of b\u2019s. This language can be defined as L= w a,b n (w)=\n\u2217 a\n{ \u2208{ } |\nn (w) . This language includes strings such as abbaab, baab, and bbbaaa.\nb\n}\nLet\u2019sstartwiththegrammarcontainingtherulesS aSbandS\n\u2212\u2192 \u2212\u2192\n\u03b5. We can try adding the rule S bSa. Every string that can be\n\u2212\u2192\ngenerated using these three rules is in the language L. However, not every\nstring in L can be generated. A derivation that starts with S = aSb can\n\u21d2\nonly produce strings that begin with a and end with b. A derivation that\nstarts with S = bSa can only generate strings that begin with b and end\n\u21d2\nwith a. There is no way to generate the strings baab or abbbabaaba, which\nare in the language L. But we shall see that any string in L that begins\nand endswith thesame letter canbewritten in the formxy where xand y\nare shorter strings in L. To produce strings of this form, we need one more\nrule, S SS. The complete set of production rules for the language L is\n\u2212\u2192\nS aSb\n\u2212\u2192\nS bSa\n\u2212\u2192\nS SS\n\u2212\u2192\nS \u03b5\n\u2212\u2192\nIt\u2019s easy to see that every string that can be generated using these rules\nis in L, since each rule introduces the same number of a\u2019s as b\u2019s. But we\nalsoneedtocheckthateverystringw inLcanbegeneratedbytheserules. 4.1. CONTEXT-FREE GRAMMARS 183\nThis can be done by induction on the length of w, using the second form\nof the principle of mathematical induction. In the base case, w = 0 and\n| |\nw = \u03b5. In this case, w L since S = \u03b5 in one step. Suppose w = k,\n\u2208 \u21d2 | |\nwhere k > 0, and suppose that we already know that for any x L with\n\u2208\nx < k, S = x. To finish the induction we must show, based on this\n\u2217\n| | \u21d2\ninduction hypothesis, that S = w.\n\u2217\n\u21d2\nSuppose that the first and last characters of w are different. Then w is\neither of the form axb or of the form bxa, for some string x. Let\u2019s assume\nthat w is of the form axb. (The case where w is of the form bxa is handled\nin a similar way.) Since w has the same number of a\u2019s and b\u2019s and since\nx has one fewer a than w and one fewer b than w, x must also have the\nsame number of a\u2019s as b\u2019s. That is x L. But x = w 2<k, so by the\n\u2208 | | | |\u2212\ninduction hypothesis, x L(G). So we have S = x. By Theorem 4.1, we\n\u2217\n\u2208 \u21d2\nget then aSb = axb. Combining this with the fact that S = aSb, we\n\u2217\n\u21d2 \u21d2\nget that S = axb, that is, S = w. This proves that w L(G).\n\u2217 \u2217\n\u21d2 \u21d2 \u2208\nFinally, suppose that the first and last characters of w are the same.\nLet\u2019s say that w begins and ends with a. (The case where w begins and\nends with b is handled in a similar way.) I claim that w can be written\nin the form xy where x L(G) and y L(G) and neither x nor y is the\n\u2208 \u2208\nempty string. This will finish the induction, since we will then have by the\ninductionhypothesisthatS = xandS = y,andwecanderivexy from\n\u2217 \u2217\n\u21d2 \u21d2\nS by first applying the rule S SS and then using the first S on the\n\u2212\u2192\nright-hand side to derive x and the second to derive y.\nIt only remains to figure out how to divide w into two strings x and y\nwhich are both in L(G). The technique that is used is one that is more\ngenerally useful. Suppose that w = c c c , where each c is either\n1 2 k i\n\u00b7\u00b7\u00b7\na or b. Consider the sequence of integers r , r , ..., r where for each\n1 2 k\ni=1,2,...,k, r is the number of a\u2019s in c c c minus the number of b\u2019s\ni 1 2 i\n\u00b7\u00b7\u00b7\nin c c c . Since c =a, r =1. Since w L, r =0. And since c =a,\n1 2 i 1 1 k k\n\u00b7\u00b7\u00b7 \u2208\nwe must have r = r 1 = 1. Furthermore the difference between\nk 1 k\n\u2212 \u2212 \u2212\nr and r is either 1 or 1, for i=1,2,...,k 1.\ni+1 i\n\u2212 \u2212\nSince r = 1 and r = 1 and the value of r goes up or down by\n1 k 1 i\n\u2212 \u2212\n1 when i increases by 1, r must be zero for some i between 1 and k 1.\ni\n\u2212\nThat is, r cannot get from 1 to 1 unless it passes through zero. Let i be\ni\n\u2212\na number between 1 and k 1 such that r =0. Let x=c c c and let\ni 1 2 i\n\u2212 \u00b7\u00b7\u00b7\ny = c c c . Note that xy = w. The fact that r = 0 means that\ni+1 i+2 k i\n\u00b7\u00b7\u00b7\nthe string c c c has the same number of a\u2019s and b\u2019s, so x L(G). It\n1 2 i\n\u00b7\u00b7\u00b7 \u2208\nfollows automatically that y L(G) also. Since i is strictly between 1 and\n\u2208\nk 1, neither x nor y is the empty string. This is all that we needed to\n\u2212\nshow to finish the proof that L=L(G).\nThebasicideaofthisproofisthatifw containsthesamenumberofa\u2019s 184 CHAPTER 4. GRAMMARS\nasb\u2019s,thenanaatthebeginningofwmusthavea\u201cmatching\u201dbsomewhere\nin w. This b matches the a in the sense that the corresponding r is zero,\ni\nand the b marks the end of a string x which contains the same number of\na\u2019s as b\u2019s. For example, in the string aababbabba, the a at the beginning of\nthe string is matched by the third b, since aababb is the shortest prefix of\naababbabba that has an equal number of a\u2019s and b\u2019s.\nClosely related to this idea of matching a\u2019s and b\u2019s is the idea of bal-\nanced parentheses. Consider a string made up of parentheses, such as\n(()(()))(()). Theparenthesesinthissamplestringarebalancedbecause\neach left parenthesis has a matching right parenthesis, and the matching\npairs are properly nested. A careful definition uses the sort of integer se-\nquence introduced in the above proof. Let w be a string of parentheses.\nWrite w = c c c , where each c is either ( or ). Define a sequence\n1 2 n i\n\u00b7\u00b7\u00b7\nof integers r , r , ..., r , where r is the number of left parentheses in\n1 2 n i\nc c c minus the number of right parentheses. We say that the paren-\n1 2 i\n\u00b7\u00b7\u00b7\ntheses in w are balanced if r = 0 and r 0 for all i = 1,2,...,n. The\nn i\n\u2265\nfact that r = 0 says that w contains the same number of left parenthe-\nn\nses as right parentheses. The fact the r 0 means that the nesting of\ni\n\u2265\npairs of parentheses is correct: You can\u2019t have a right parenthesis unless it\nis balanced by a left parenthesis in the preceding part of the string. The\nlanguagethatconsistsofallbalancedstringsofparenthesesiscontext-free.\nIt is generated by the grammar\nS (S)\n\u2212\u2192\nS SS\n\u2212\u2192\nS \u03b5\n\u2212\u2192\nThe proof is similar to the preceding proof about strings of a\u2019s and b\u2019s.\n(It might seem that I\u2019ve made an awfully big fuss about matching and\nbalancing. The reason is that this is one of the few things that we can do\nwith context-free languages that we can\u2019t do with regular languages.)\nBefore leaving this section, we should look at a few more general re-\nsults. Since we know that most operations on regular languages produce\nlanguages that are also regular, we can ask whether a similar result holds\nfor context-free languages. We will see later that the intersection of two\ncontext-free languages is not necessarily context-free. Also, the comple-\nment of a context-free language is not necessarily context-free. However,\nsome other operations on context-free languages do produce context-free\nlanguages.\nTheorem 4.3. Suppose that L and M are context-free languages. Then\nthe languages L M, LM, and L are also context-free.\n\u2217\n\u222a 4.1. CONTEXT-FREE GRAMMARS 185\nProof. Iwillproveonlythefirstclaimofthetheorem,thatL M iscontext-\n\u222a\nfree. In the exercises for this section, you are asked to construct grammars\nforLM andL (withoutgivingformalproofsthatyouranswersarecorrect).\n\u2217\nLet G = (V,\u03a3,P,S) and H = (W,\u0393,Q,T) be context-free grammars\nsuch that L = L(G) and M = L(H). We can assume that W V = ,\n\u2229 \u2205\nsince otherwise we could simply rename the non-terminal symbols in W.\nThe idea of the proof is that to generate a string in L M, we first decide\n\u222a\nwhetherwewantastringinLorastringinM. Oncethatdecisionismade,\nto make a string in L, we use production rules from G, while to make a\nstring in M, we use rules from H. We have to design a grammar, K, to\nrepresent this process.\nLet R be a symbol that is not in any of the alphabets V, W, \u03a3, or \u0393.\nR will be the start symbol of K. The production rules for K consist of all\nthe production rules from G and H together with two new rules:\nR S\n\u2212\u2192\nR T\n\u2212\u2192\nFormally, K is defined to be the grammar\n(V W R ,P Q R S,R T ,\u03a3 \u0393,R)\n\u222a \u222a{ } \u222a \u222a{ \u2212\u2192 \u2212\u2192 } \u222a\nSuppose that w L. That is w L(G), so there is a derivation S = w.\n\u2208 \u2208\n\u21d2\u2217G\nSince every rule from G is also a rule in K, if follows that S = w.\n\u21d2\u2217K\nCombining this with the fact that R= S, we have that R= w, and\n\u21d2K \u21d2\u2217K\nw L(K). This shows that L L(K). In an exactly similar way, we can\n\u2208 \u2286\nshow that M L(K). Thus, L M L(K).\n\u2286 \u222a \u2286\nItremainstoshowthatL(K) L M. Supposew L(K). Thenthere\n\u2286 \u222a \u2208\nis a derivation R = w. This derivation must begin with an application\n\u21d2\u2217K\nof one of the rules R S or R T, since these are the only rules in\n\u2212\u2192 \u2212\u2192\nwhich R appears. If the first rule applied in the derivation is R S,\n\u2212\u2192\nthen the remainder of the derivation shows that S = w. Starting from\n\u21d2\u2217K\nS, the only rules that can be applied are rules from G, so in fact we have\nS = w. Thisshowsthat w L. Similarly, if thefirstruleapplied in the\n\u21d2\u2217G\n\u2208\nderivation R = w is R T, then w M. In any case, w L M.\n\u21d2\u2217K\n\u2212\u2192 \u2208 \u2208 \u222a\nThis proves that L(K) L M.\n\u2286 \u222a\nFinally, we should clarify the relationship between context-free lan-\nguagesandregularlanguages. Wehavealreadyseenthattherearecontext-\nfree languages which are not regular. On the other hand, it turns out that\neveryregularlanguageiscontext-free. Thatis, givenanyregularlanguage,\nthere is a context-free grammar that generates that language. This means 186 CHAPTER 4. GRAMMARS\nthat any syntax that can be expressed by a regular expression, by a DFA,\nor by an NFA could also be expressed by a context-free grammar. In fact,\nweonlyneedacertainrestrictedtypeofcontext-freegrammartoduplicate\nthe power of regular expressions.\nDefinition 4.3. Aright-regular grammar isacontext-freegrammarin\nwhich the right-hand side of every production rule has one of the following\nforms: theemptystring;astringconsistingofasinglenon-terminalsymbol;\nor a string consisting of a single terminal symbol followed by a single non-\nterminal symbol.\nExamples of the types of production rule that are allowed in a right-\nregular grammar are A \u03b5, B C, and D aE. The idea of\n\u2212\u2192 \u2212\u2192 \u2212\u2192\nthe proof is that given a right-regular grammar, we can build a corre-\nsponding NFA and vice-versa. The states of the NFA correspond to the\nnon-terminal symbols of the grammar. The start symbol of the grammar\ncorrespondstothestartingstateoftheNFA.Aproductionruleoftheform\nA bC corresponds to a transition in the NFA from state A to state\n\u2212\u2192\nC while reading the symbol b. A production rule of the form A B\n\u2212\u2192\ncorresponds to an \u03b5-transition from state A to state B in the NFA. And\na production rule of the form A \u03b5 exists in the grammar if and only\n\u2212\u2192\nif A is a final state in the NFA. With this correspondence, a derivation of\na string w in the grammar corresponds to an execution path through the\nNFA as it accepts the string w. I won\u2019t give a complete proof here. You\nare welcome to work through the details if you want. But the important\nfact is:\nTheorem4.4. AlanguageLisregularifandonlyifthereisaright-regular\ngrammar G such that L = L(G). In particular, every regular language is\ncontext-free.\nExercises\n1. Show that Part 4 of Theorem 4.1 follows from Part 3.\n2. Give a careful proof that the language anbm n m 0 is generated by\n{ | \u2265 \u2265 }\nthe context-free grammar\nS aSb\n\u2212\u2192\nS A\n\u2212\u2192\nA aA\n\u2212\u2192\nA \u03b5\n\u2212\u2192 4.1. CONTEXT-FREE GRAMMARS 187\n3. Identify the language generated by each of the following context-free gram-\nmars.\na) S aaSb b) S aSb\n\u2212\u2192 \u2212\u2192\nS \u03b5 S aaSb\n\u2212\u2192 \u2212\u2192\nS \u03b5\n\u2212\u2192\nc) S TS d) S ABA\n\u2212\u2192 \u2212\u2192\nS \u03b5 A aA\n\u2212\u2192 \u2212\u2192\nT aTb A a\n\u2212\u2192 \u2212\u2192\nT \u03b5 B bB\n\u2212\u2192 \u2212\u2192\nB cB\n\u2212\u2192\nB \u03b5\n\u2212\u2192\n4. Foreachofthefollowinglanguagesfindacontext-freegrammarthatgenerates\nthe language:\na) anbm n m>0 b) anbm n,m N\nc)\n{ anbm| n\u2265\n0\nm}\n=n+1 d)\n{ anbmc|n n,m\u2208 }\nN\ne) { anbmc|k \u2265 n=\u2227 m+k } f) { anbm n| =m \u2208 }\ng)\n{ anbmcrd|t n+m=}\nr+t h)\n{ anbmc|k 6 n=}\nm+k\n{ | } { | 6 }\n5. Findacontext-freegrammarthatgeneratesthelanguage w a,b \u2217 n a(w)>\n{ \u2208{ } |\nn b(w) .\n}\n6. Findacontext-freegrammarthatgeneratesthelanguage w a,b,c \u2217 n a(w)=\n{ \u2208{ } |\nn b(w) .\n}\n7. Apalindrome isastringthatreadsthesamebackwardsandforwards,such\nas\u201cmom\u201d,\u201cradar\u201d,or\u201caabccbccbaa\u201d. Thatis,wisapalindromeifw=wR.\nLet L= w a,b,c \u2217 w is a palindrome . Show that L is a context-free\n{ \u2208{ } | }\nlanguage by finding a context-free grammar that generates L.\n8. Let \u03a3= (, ), [, ] . That is, \u03a3 is the alphabet consisting of the four sym-\n{ }\nbols (, ), [, and ]. Let L be the language over \u03a3 consisting of strings in\nwhich both parentheses and brackets are balanced. For example, the string\n([][()()])([]) is in L but [(]) is not. Find a context-free grammar that\ngenerates the language L.\n9. SupposethatGandH arecontext-freegrammars. LetL=L(G)andletM =\nL(H). Explainhowtoconstructacontext-freegrammarforthelanguageLM.\nYou do not need to give a formal proof that your grammar is correct.\n10. Suppose that G is a context-free grammar. Let L = L(G). Explain how to\nconstruct a context-free grammar for the language L\u2217. You do not need to\ngive a formal proof that your grammar is correct.\n11. Suppose that L is a context-free language. Prove that LR is a context-free\nlanguage. (Hint: Given a context-free grammar G for L, make a new gram-\nmar, GR, by reversing the right-hand side of each of the production rules in\nG. That is, A w is a production rule in G if and only if A wR is a\nproduction rule\u2212 i\u2192 n GR.) \u2212\u2192\n12. Define a left-regular grammar to be a context-free grammar in which the\nright-hand side of every production rule is of one of the following forms: the 188 CHAPTER 4. GRAMMARS\nemptystring;asinglenon-terminalsymbol;oranon-terminalsymbolfollowed\nbyaterminalsymbol. Showthatalanguageisregularifandonlyifitcanbe\ngenerated by a left-regular grammar. (Hint: Use the preceding exercise and\nTheorem 4.4.)\n4.2 Application: BNF\nContext-free grammars are used to describe some aspects of the syntax of\nprogramming languages. However, the notation that is used for grammars\nin the context of programming languages is somewhat different from the\nnotation introduced in the preceding section. The notation that is used is\ncalled Backus-Naur Form or BNF. It is named after computer scientists\nJohnBackusandPeterNaur,whodevelopedthenotation. Actually,several\nvariationsofBNFexist. Iwilldiscussoneofthemhere. BNFcanbeusedto\ndescribethesyntaxofnaturallanguages,aswellasprogramminglanguages,\nandsomeoftheexamplesinthissectionwilldealwiththesyntaxofEnglish.\nLike context-free grammars, BNF grammars make use of production\nrules, non-terminals, and terminals. The non-terminals are usually given\nmeaningful, multi-character names. Here, I will follow a common practice\nof enclosing non-terminals in angle brackets, so that they can be easily\ndistinguished. For example, noun and sentence could be non-terminals\nh i h i\nin a BNF grammar for English, while program and if-statement might\nh i h i\nbeusedinaBNFgrammarforaprogramminglanguage. NotethataBNF\nnon-terminal usually represents a meaningful syntactic category, that\nis, a certain type of building block in the syntax of the language that is\nbeing described, such as an adverb, a prepositional phrase, or a variable\ndeclaration statement. The terminals of a BNF grammar are the things\nthat actually appear in the language that is being described. In the case of\nnatural language, the terminals are individual words.\nIn BNF production rules, I will use the symbol \u201c::=\u201d in place of the\n\u201c \u201d that is used in context-free grammars. BNF production rules are\n\u2212\u2192\nmore powerful than the production rules in context-free grammars. That\nis,oneBNFrulemightbeequivalenttoseveralcontext-freegrammarrules.\nAs for context-free grammars, the left-hand side of a BNF production rule\nis a single non-terminal symbol. The right hand side can include terminals\nand non-terminals, and can also use the following notations, which should\nremind you of notations used in regular expressions:\nA vertical bar, , indicates a choice of alternatives. For example,\n\u2022 |\ndigit ::= 0 1 2 3 4 5 6 7 8 9\nh i | | | | | | | | |\nindicatesthatthenon-terminal digit canbereplacedbyanyone\nh i 4.2. APPLICATION: BNF 189\nof the terminal symbols 0, 1, ..., 9.\nItems enclosed in brackets are optional. For example,\n\u2022\ndeclaration ::= type variable [ = expression ] ;\nh i h i h i h i\nsaysthat declaration canbereplacedeitherby\u201c type variable ;\u201d\nh i h ih i\norby\u201c type variable = expression ;\u201d. (Thesymbols\u201c=\u201dand\nh ih i h i\n\u201c;\u201d are terminal symbols in this rule.)\nItems enclosed between \u201c[\u201d and \u201c]...\u201d can be repeated zero or\n\u2022\nmoretimes. (Thishasthesameeffectasa\u201c \u201dinaregularexpres-\n\u2217\nsion.) For example,\ninteger ::= digit [ digit ]...\nh i h i h i\nsays that an integer consists of a digit followed optionally by\nh i h i\nany number of additional digit \u2019s. That is, the non-terminal\nh i\ninteger can be replaced by digit or by digit digit or by\nh i h i h ih i\ndigit digit digit , and so on.\nh ih ih i\nParentheses can be used as usual, for grouping.\n\u2022\nAllthesenotationscanbeexpressedinacontext-freegrammarbyintro-\nducingadditionalproductionrules. Forexample,theBNFrule\u201c sign ::=\nh i\n+ \u201d is equivalent to the two rules, \u201c sign ::= +\u201d and \u201c sign ::= \u201d.\n| \u2212 h i h i \u2212\nA rule that contains an optional item can also be replaced by two rules.\nFor example,\ndeclaration ::= type variable [ = expression ] ;\nh i h i h i h i\ncan be replaced by the two rules\ndeclaration ::= type variable ;\nh i h i h i\ndeclaration ::= type variable = expression ;\nh i h i h i h i\nIn context-free grammars, repetition can be expressed by using a recursive\nrule such as \u201cS aS\u201d, in which the same non-terminal symbol appears\n\u2212\u2192\nbothontheleft-handsideandontheright-handsideoftherule. BNF-style\nnotation using \u201c[\u201d and \u201c]...\u201d can be eliminated by replacing it with a new\nnon-terminal symbol and adding a recursive rule to allow that symbol to\nrepeat zero or more times. For example, the production rule\ninteger ::= digit [ digit ]...\nh i h i h i\ncan be replaced by three rules using a new non-terminal symbol digit-list\nh i\nto represent a string of zero or more digit \u2019s:\nh i\ninteger ::= digit digit-list\nh i h i h i\ndigit-list ::= digit digit-list\nh i h i h i\ndigit-list ::= \u03b5\nh i\nAs an example of a complete BNF grammar, let\u2019s look at a BNF gram-\nmarforaverysmallsubsetofEnglish. Thestartsymbolforthegrammaris 190 CHAPTER 4. GRAMMARS\nsentence , and the terminal symbols are English words. All the sentences\nh i\nthat can be produced from this grammar are syntactically correct English\nsentences, although you wouldn\u2019t encounter many of them in conversation.\nHere is the grammar:\nsentence ::= simple-sentence [ and simple-sentence ]...\nh i h i h i\nsimple-sentence ::= nout-part verb-part\nh i h i h i\nnoun-part ::= article noun [ who verb-part ]...\nh i h i h i h i\nverb-part ::= intransitive-verb ( transitive-verb noun-part )\nh i h i | h i h i\narticle ::= the a\nh i |\nnoun ::= man woman dog cat computer\nh i | | | |\nintransitive-verb ::= runs jumps hides\nh i | |\ntransitive-verb ::= knows loves chases owns\nh i | | |\nThisgrammarcangeneratesentencessuchas\u201cAdogchasesthecatandthe\ncat hides\u201d and \u201cThe man loves a woman who runs.\u201d The second sentence,\nfor example, is generated by the derivation\nsentence = simple-sentence\nh i \u21d2 h i\n= noun-part verb-part\n\u21d2 h i h i\n= article noun verb-part\n\u21d2 h i h i h i\n= the noun verb-part\n\u21d2 h i h i\n= the man verb-part\n\u21d2 h i\n= the man transitive-verb noun-part\n\u21d2 h i h i\n= the man loves noun-part\n\u21d2 h i\n= the man loves article noun who verb-part\n\u21d2 h i h i h i\n= the man loves a noun who verb-part\n\u21d2 h i h i\n= the man loves a woman who verb-part\n\u21d2 h i\n= the man loves a woman who intransitive-verb\n\u21d2 h i\n= the man loves a woman who runs\n\u21d2\nBNFismostoftenusedtospecifythesyntaxofprogramminglanguages.\nMost programming languages are not, in fact, context-free languages, and\nBNF is not capable of expressing all aspects of their syntax. For example,\nBNF cannot express the fact that a variable must be declared before it is\nused or the fact that the number of actual parameters in a subroutine call\nstatement must match the number of formal parameters in the declaration\nofthesubroutine. SoBNFisusedtoexpressthecontext-freeaspectsofthe 4.2. APPLICATION: BNF 191\nsyntax of a programming language, and other restrictions on the syntax\u2014\nsuch as the rule about declaring a variable before it is used\u2014are expressed\nusing informal English descriptions.\nWhen BNF is applied to programming languages, the terminal symbols\nare generally \u201ctokens,\u201d which are the minimal meaningful units in a pro-\ngram. Forexample,thepairofsymbols<=constituteasingletoken,asdoes\na string such as \"Hello World\". Every number is represented by a single\ntoken. (The actual value of the number is stored as a so-called \u201cattribute\u201d\nof the token, but the value plays no role in the context-free syntax of the\nlanguage.) I will use the symbol number to represent a numerical token.\nSimilarly, every variable name, subroutine name, or other identifier in the\nprogram is represented by the same token, which I will denote as ident.\nOne final complication: Some symbols used in programs, such as \u201c]\u201d and\n\u201c(\u201d, are also used with a special meaning in BNF grammars. When such a\nsymbol occurs as a terminal symbol, I will enclose it in double quotes. For\nexample, in the BNF production rule\narray-reference ::= ident \u201c[\u201d expression \u201c]\u201d\nh i h i\nthe \u201c[\u201d and \u201c]\u201d are terminal symbols in the language that is being de-\nscribed, rather than the BNF notation for an optional item. With this\nnotation, here is part of a BNF grammar that describes statements in the\nJava programming language:\nstatement ::= block-statement if-statement while-statement\nh i h i | h i | h i\nassignment-statement null-statement\n| h i | h i\nblock-statement ::= [ statement ]...\nh i { h i }\nif-statement ::=if\u201c(\u201d condition \u201c)\u201d statement [else statement ]\nh i h i h i h i\nwhile-statement ::= while \u201c(\u201d condition \u201c)\u201d statement\nh i h i h i\nassignment-statement ::= variable = expression ;\nh i h i h i\nnull-statement ::= \u03b5\nh i\nThenon-terminals condition , variable ,and expression would,ofcourse,\nh i h i h i\nhave to be defined by other production rules in the grammar. Here is a set\nof rules that define simple expressions, made up of numbers, identifiers,\nparentheses and the arithmetic operators +, , and \/:\n\u2212 \u2217\nexpression ::= term [ [ + ] term ]...\nh i h i | \u2212 h i\nterm ::= factor [ [ \/ ] factor ]...\nh i h i \u2217 | h i\nfactor ::= ident number \u201c(\u201d expression \u201c)\u201d\nh i | | h i\nThefirstrulesaysthatan expression isasequenceofoneormore term \u2019s,\nh i h i\nseparated by plus or minus signs. The second rule defines a term to be a\nh i 192 CHAPTER 4. GRAMMARS\nsequence of one or more factors , separated by multiplication or division\nh i\noperators. The last rule says that a factor can be either an identifier\nh i\nor a number or an expression enclosed in parentheses. This small BNF\nh i\ngrammar can generate expressions such as \u201c3 5\u201d and \u201cx (x+1) 3\/(z+\n\u2217 \u2217 \u2212\n2 (3 x))+7\u201d. Thelatterexpressionismadeupofthreeterms: x (x+1),\n\u2217 \u2212 \u2217\n3\/(z+2 (3 x)),and7. Thefirstofthesetermsismadeupoftwofactors,\n\u2217 \u2212\nx and (x+1). The factor (x+1) consists of the expression x+1 inside a\npair of parentheses.\nThe nice thing about this grammar is that the precedence rules for\nthe operators are implicit in the grammar. For example, according to the\ngrammar, the expression 3+5 7 is seen as term + term where the\n\u2217 h i h i\nfirst term is 3 and the second term is 5 7. The 5 7 occurs as a group,\n\u2217 \u2217\nwhich must be evaluated before the result is added to 3. Parentheses can\nchange the order of evaluation. For example, (3+5) 7 is generated by the\n\u2217\ngrammarasasingle term oftheform factor factor . Thefirst factor\nh i h i\u2217h i h i\nis (3+5). When (3+5) 7 is evaluated, the value of (3+5) is computed\n\u2217\nfirst and then multiplied by 7. This is an example of how a grammar that\ndescribes the syntax of a language can also reflect its meaning.\nAlthough this section has not introduced any really new ideas or theo-\nretical results, I hope it has demonstrated how context-free grammars can\nbe applied in practice.\nExercises\n1. One of the examples in this section was a grammar for a subset of English.\nGive five more examples of sentences that can be generated from that gram-\nmar. Your examples should, collectively, use all the rules of the grammar.\n2. Rewrite the example BNF grammar for a subset of English as a context-free\ngrammar.\n3. WriteasingleBNFproductionrulethatisequivalenttothefollowingcontext-\nfree grammar:\nS aSa\n\u2212\u2192\nS bB\n\u2212\u2192\nB bB\n\u2212\u2192\nB \u03b5\n\u2212\u2192\n4. Write a BNF production rule that specifies the syntax of real numbers, as\nthey appear in programming languages such as Java and C. Real numbers\ncan include a sign, a decimal point and an exponential part. Some examples\nare: 17.3, .73, 23.1e67, 1.34E 12, +0.2, 100E+100\n\u2212 \u2212 4.3. PARSING AND PARSE TREES 193\n5. Variable references in the Java programming language can be rather compli-\ncated. Someexamplesinclude: x,list.next,A[7],a.b.c,S[i+1].grid[r][c].red,\n.... Write a BNF production rule for Java variables. You can use the token\nident and the non-terminal expression in your rule.\nh i\n6. Use BNF to express the syntax of the try...catch statement in the Java\nprogramming language.\n7. Give a BNF grammar for compound propositions made up of propositional\nvariables, parentheses, and the logical operators , , and . Use the non-\n\u2227 \u2228 \u00ac\nterminal symbol pv to represent a propositional variable. You do not have\nh i\nto give a definition of pv .\nh i\n4.3 Parsing and Parse Trees\nSuppose that G is a grammar for the language L. That is, L=L(G). The\ngrammar G can be used to generate strings in the language L. In practice,\nthough, weoftenstartwithastringwhichmight ormightnotbeinL, and\nthe problem is to determine whether the string is in the language and, if\nso, how it can be generated by G. The goal is to find a derivation of the\nstring, using the production rules of the grammar, or to show that no such\nderivation exists. This is known as parsing the string. When the string is\nacomputerprogramorasentenceinanaturallanguage, parsingthestring\nis an essential step in determining its meaning.\nAs an example that we will use throughout this section, consider the\nlanguagethatconsistsofarithmeticexpressionscontainingparentheses,the\nbinary operators + and , and the variables x, y, and z. Strings in this\n\u2217\nlanguageincludex, x+y z, and((x+y) y)+z z. Hereisacontext-free\n\u2217 \u2217 \u2217\ngrammar that generates this language:\nE E+E\n\u2212\u2192\nE E E\n\u2212\u2192 \u2217\nE (E)\n\u2212\u2192\nE x\n\u2212\u2192\nE y\n\u2212\u2192\nE z\n\u2212\u2192\nCallthegrammardescribedbytheseproductionrulesG . ThegrammarG\n1 1\nsaysthatx,y,andzareexpressions,andthatyoucanmakenewexpressions\nbyaddingtwoexpressions,bymultiplyingtwoexpressions,andbyenclosing\nan expression in parentheses. (Later, we\u2019ll look at other grammars for the\nsame language\u2014ones that turn out to have certain advantages over G .)\n1 194 CHAPTER 4. GRAMMARS\nConsiderthestringx+y z. Toshowthatthisstringisinthelanguage\n\u2217\nL(G ), we can exhibit a derivation of the string from the start symbol E.\n1\nFor example:\nE = E+E\n\u21d2\n= E+E E\n\u21d2 \u2217\n= E+y E\n\u21d2 \u2217\n= x+y E\n\u21d2 \u2217\n= x+y z\n\u21d2 \u2217\nThisderivationshowsthatthestringx+y z isinfactinL(G ). Now,this\n1\n\u2217\nstringhasmanyotherderivations. Ateachstepinthederivation,therecan\nbe a lot of freedom about which rule in the grammar to apply next. Some\nof this freedom is clearly not very meaningful. When faced with the string\nE +E E in the above example, the order in which we replace the E\u2019s\n\u2217\nwiththevariablesx,y,andz doesn\u2019tmuchmatter. Tocutoutsomeofthis\nmeaningless freedom, we could agree that in each step of a derivation, the\nnon-terminal symbol that is replaced is the leftmost non-terminal symbol\ninthestring. Aderivationinwhichthisistrueiscalledaleft derivation.\nThefollowingleftderivationofthestringx+y z usesthesameproduction\n\u2217\nrules as the previous derivation, but it applies them in a different order:\nE = E+E\n\u21d2\n= x+E\n\u21d2\n= x+E E\n\u21d2 \u2217\n= x+y E\n\u21d2 \u2217\n= x+y z\n\u21d2 \u2217\nIt shouldn\u2019t be too hard to convince yourself that any string that has a\nderivation has a left derivation (which can be obtained by changing the\norder in which production rules are applied).\nWe have seen that the same string might have several different deriva-\ntions. We might ask whether it can have several different left derivations.\nThe answer is that it depends on the grammar. A context-free grammar\nG is said to be ambiguous if there is a string w L(G) such that w has\n\u2208\nmore than one left derivation according to the grammar G.\nOur example grammar G is ambiguous. In fact, in addition to the left\n1 4.3. PARSING AND PARSE TREES 195\nderivationgivenabove,thestringx+y z hasthealternativeleftderivation\n\u2217\nE = E E\n\u21d2 \u2217\n= E+E E\n\u21d2 \u2217\n= x+E E\n\u21d2 \u2217\n= x+y E\n\u21d2 \u2217\n= x+y z\n\u21d2 \u2217\nInthisleftderivationofthestringx+y z,thefirstproductionrulethatis\n\u2217\nappliedisE E E. ThefirstE ontheright-handsideeventuallyyields\n\u2212\u2192 \u2217\n\u201cx+y\u201d while the second yields \u201cz\u201d. In the previous left derivation, the\nfirst production rule that was applied was E E +E, with the first E\n\u2212\u2192\non the right yielding \u201cx\u201d and the second E yielding \u201cy z\u201d. If we think in\n\u2217\ntermsofarithmeticexpressions,thetwoleftderivationsleadtotwodifferent\ninterpretations of the expression x+y z. In one interpretation, the x+y\n\u2217\nis a unit that is multiplied by z. In the second interpretation, the y z is a\n\u2217\nunitthatisaddedtox. Thesecondinterpretationistheonethatiscorrect\naccording to the usual rules of arithmetic. However, the grammar allows\neither interpretation. The ambiguity of the grammar allows the string to\nbe parsed in two essentially different ways, and only one of the parsings\nis consistent with the meaning of the string. Of course, the grammar for\nEnglish is also ambiguous. In a famous example, it\u2019s impossible to tell\nwhether a \u201cpretty girls\u2019 camp\u201d is meant to describe a pretty camp for girls\nor a camp for pretty girls.\nWhendealingwithartificiallanguagessuchasprogramminglanguages,\nit\u2019s bettertoavoid ambiguity. The grammar G isperfectlycorrectin that\n1\nit generates the correct set of strings, but in a practical situation where we\nare interested in the meaning of the strings, G is not the right grammar\n1\nfor the job. There are other grammars that generate the same language\nas G . Some of them are unambiguous grammars that better reflect the\n1\nmeaning of the strings in the language. For example, the language L(G )\n1\nis also generated by the BNF grammar\nE ::= T [ + T ]...\nT ::= F [ F ]...\n\u2217\nF ::= \u201c(\u201d E \u201c)\u201d x y z\n| | |\nThis grammar can be translated into a standard context-free grammar, 196 CHAPTER 4. GRAMMARS\nwhich I will call G :\n2\nE TA\n\u2212\u2192\nA +TA\n\u2212\u2192\nA \u03b5\n\u2212\u2192\nT FB\n\u2212\u2192\nB FB\n\u2212\u2192\u2217\nB \u03b5\n\u2212\u2192\nF (E)\n\u2212\u2192\nF x\n\u2212\u2192\nF y\n\u2212\u2192\nF z\n\u2212\u2192\nThe language generated by G consists of all legal arithmetic expressions\n2\nmadeupofparentheses,theoperators+and ,andthevariablesx,y,and\n\u2212\nz. That is, L(G ) = L(G ). However, G is an unambiguous grammar.\n2 1 2\nConsider,forexample,thestringx+y z. UsingthegrammarG ,theonly\n2\n\u2217\nleft derivation for this string is:\nE = TA\n\u21d2\n= FBA\n\u21d2\n= xBA\n\u21d2\n= xA\n\u21d2\n= x+TA\n\u21d2\n= x+FBA\n\u21d2\n= x+yBA\n\u21d2\n= x+y FBA\n\u21d2 \u2217\n= x+y zBA\n\u21d2 \u2217\n= x+y zA\n\u21d2 \u2217\n= x+y z\n\u21d2 \u2217\nThere is no choice about the first step in this derivation, since the only\nproduction rule with E on the left-hand side is E TA. Similarly, the\n\u2212\u2192\nsecond step is forced by the fact that there is only one rule for rewriting a\nT. In the third step, we must replace an F. There are four ways to rewrite\nF, but only one way to produce the x that begins the string x+y z, so\n\u2217\nwe apply the rule F x. Now, we have to decide what to do with the\n\u2212\u2192\nB in xBA. There two rules for rewriting B, B FB and B \u03b5.\n\u2212\u2192 \u2217 \u2212\u2192 4.3. PARSING AND PARSE TREES 197\nHowever, the first of these rules introduces a non-terminal, , which does\n\u2217\nnot match the string we are trying to parse. So, the only choice is to apply\nthe production rule B \u03b5. In the next step of the derivation, we must\n\u2212\u2192\napply the rule A +TA in order to account for the + in the string\n\u2212\u2192\nx+y z. Similarly, each of the remaining steps in the left derivation is\n\u2217\nforced.\nThe fact that G is an unambiguous grammar means that at each step\n2\nin a left derivation for a string w, there is only one production rule that\ncan be applied which will lead ultimately to a correct derivation of w.\nHowever, G actually satisfies a much stronger property: at each step in\n2\ntheleftderivationofw,wecantellwhichproductionrulehastobeapplied\nby looking ahead at the next symbol in w. We say that G is an LL(1)\n2\ngrammar. (This notation means that we can read a string from Left to\nrightandconstructaLeftderivationofthestringbylookingaheadatmost\n1 character in the string.) Given an LL(1) grammar for a language, it is\nfairly straightforward to write a computer program that can parse strings\nin that language. If the language is a programming language, then parsing\nisoneoftheessentialstepsintranslatingacomputerprogramintomachine\nlanguage. LL(1) grammars and parsing programs that use them are often\nstudied in courses in programming languages and the theory of compilers.\nNot every unambiguous context-free grammar is an LL(1) grammar.\nConsider, for example, the following grammar, which I will call G :\n3\nE E+T\n\u2212\u2192\nE T\n\u2212\u2192\nT T F\n\u2212\u2192 \u2217\nT F\n\u2212\u2192\nF (E)\n\u2212\u2192\nF x\n\u2212\u2192\nF y\n\u2212\u2192\nF z\n\u2212\u2192\nThis grammar generates the same language as G and G , and it is un-\n1 2\nambiguous. However, it is not possible to construct a left derivation for\na string according to the grammar G by looking ahead one character in\n3\nthe string at each step. The first step in any left derivation must be either\nE = E +T or E = T. But how can we decide which of these is the\n\u21d2 \u21d2\ncorrect first step? Consider the strings (x+y) z and (x+y) z+z x,\n\u2217 \u2217 \u2217\nwhich are both in the language L(G ). For the string (x+y) z, the first\n3\n\u2217\nstep in a left derivation must be E = T, while the first step in a left\n\u21d2 198 CHAPTER 4. GRAMMARS\nderivation of (x+y) z+z x must be E = E+T. However, the first\n\u2217 \u2217 \u21d2\nseven characters of the strings are identical, so clearly looking even seven\ncharacters ahead is not enough to tell us which production rule to apply.\nIn fact, similar examples show that looking ahead any given finite number\nof characters is not enough.\nHowever,thereisanalternativeparsingprocedurethatwillworkforG .\n3\nThis alternative method of parsing a string produces a right derivation\nof the string, that is, a derivation in which at each step, the non-terminal\nsymbol that is replaced is the rightmost non-terminal symbol in the string.\nHere, for example, is a right derivation of the string (x+y) z according\n\u2217\nto the grammar G :\n3\nE = T\n\u21d2\n= T F\n\u21d2 \u2217\n= T z\n\u21d2 \u2217\n= F z\n\u21d2 \u2217\n= (E) z\n\u21d2 \u2217\n= (E+T) z\n\u21d2 \u2217\n= (E+F) z\n\u21d2 \u2217\n= (E+y) z\n\u21d2 \u2217\n= (T +y) z\n\u21d2 \u2217\n= (F +y) z\n\u21d2 \u2217\n= (x+y) z\n\u21d2 \u2217\nThe parsing method that produces this right derivation produces it from\n\u201cbottom to top.\u201d That is, it begins with the string (x+y) z and works\n\u2217\nbackwardtothestartsymbolE,generatingthestepsoftherightderivation\ninreverseorder. ThemethodworksbecauseG iswhatiscalledanLR(1)\n3\ngrammar. That is, roughly, it is possible to read a string from Left to\nright and produce a Right derivation of the string, by looking ahead at\nmost1symbolateachstep. AlthoughLL(1)grammarsareeasierforpeople\nto work with, LR(1) grammars turn out to be very suitable for machine\nprocessing, and they are used as the basis for the parsing process in many\ncompilers.\nLR(1) parsing uses a shift\/reduce algorithm. Imagine a cursor or\ncurrent position that moves through the string that is being parsed. We\ncan visualize the cursor as a vertical bar, so for the string (x+y) z, we\n\u2217\nstart with the configuration (x+y) z. A shift operation simply moves\n| \u2217\nthe cursor one symbol to the right. For example, a shift operation would 4.3. PARSING AND PARSE TREES 199\nconvert (x + y) z to (x + y) z, and a second shift operation would\n| \u2217 | \u2217\nconvert that to (x +y) z. In a reduce operation, one or more symbols\n| \u2217\nimmediately to the left of the cursor are recognized as the right-hand side\nof oneof theproduction rulesin thegrammar. Thesesymbols areremoved\nand replaced by the left-hand side of the production rule. For example,\nin the configuration (x + y) z, the x to the left of the cursor is the\n| \u2217\nright-hand side of the production rule F x, so we can apply a reduce\n\u2212\u2192\noperation and replace the x with F, giving (F +y) z. This first reduce\n| \u2217\noperation corresponds to the last step in the right derivation of the string,\n(F +y) z = (x+y) z. Now the F can be recognized as the right-hand\n\u2217 \u21d2 \u2217\nsideoftheproductionruleT F,sowecanreplacetheF withT,giving\n\u2212\u2192\n(T +y) z. Thiscorrespondstothenext-to-laststepintherightderivation,\n| \u2217\n(T +y) z = (F +y) z.\n\u2217 \u21d2 \u2217\nAt this point, we have the configuration (T +y) z. The T could be\n| \u2217\nthe right-hand side of the production rule E T. However, it could also\n\u2212\u2192\nconceivably come from the rule T T F. How do we know whether to\n\u2212\u2192 \u2217\nreducetheT toE atthispointortowaitfora F tocomealongsothatwe\n\u2217\ncan reduce T F ? We can decide by looking ahead at the next character\n\u2217\nafter the cursor. Since this character is a + rather than a , we should\n\u2217\nchoose the reduce operation that replaces T with E, giving (E +y) z.\n| \u2217\nWhat makes G an LR(1) grammar is the fact that we can always decide\n3\nwhat operation to apply by looking ahead at most one symbol past the\ncursor.\nAfterafewmoreshiftandreduceoperations,theconfigurationbecomes\n(E) z, which we can reduce to T z by applying the production rules\n|\u2217 |\u2217\nF (E) and T F. Now, faced with T z, we must once again\n\u2212\u2192 \u2212\u2192 |\u2217\ndecide between a shift operation and a reduce operation that applies the\nrule E T. In this case, since the next character is a rather than a +,\n\u2212\u2192 \u2217\nweapplytheshiftoperation,givingT z. Fromthereweget,insuccession,\n\u2217|\nT z , T F , T , and finally E . At this point, we have reduced the entire\n\u2217 | \u2217 | | |\nstring (x+y) z to the start symbol of the grammar. The very last step,\n\u2217\nthereductionofT toE correspondstothefirststepoftherightderivation,\nE = T.\n\u21d2\nIn summary, LR(1) parsing transforms a string into the start symbol\nof the grammar by a sequence of shift and reduce operations. Each reduce\noperationcorrespondstoastepinarightderivationofthestring,andthese\nstepsaregeneratedinreverseorder. Becausethestepsinthederivationare\ngenerated from \u201cbottom to top,\u201d LR(1) parsing is a type of bottom-up\nparsing. LL(1) parsing, on the other hand, generates the steps in a left\nderivation from \u201ctop to bottom\u201d and so is a type of top-down parsing. 200 CHAPTER 4. GRAMMARS\nAlthough the language generated by a context-free grammar is defined\nintermsofderivations,thereisanotherwayofpresentingthegenerationof\na string that is often more useful. A parse tree displays the generation of\nastringfromthestartsymbolofagrammarasatwodimensionaldiagram.\nHere are two parse trees that show two derivations of the string x+y*z ac-\ncordingtothegrammarG ,whichwasgivenatthebeginningofthissection:\n1\nE E\nE + E E * E\nx E * E E + E z\ny z x y\nA parse tree is made up of terminal and non-terminal symbols, connected\nby lines. The start symbol is at the top, or \u201croot,\u201d of the tree. Terminal\nsymbols are at the lowest level, or \u201cleaves,\u201d of the tree. (For some reason,\ncomputer scientists traditionally draw trees with leaves at the bottom and\nroot at the top.) A production rule A w is represented in a parse tree\n\u2212\u2192\nby the symbol A lying above all the symbols in w, with a line joining A to\neach of the symbols in w. For example, in the left parse tree above, the\nroot, E, is connected to the symbols E, +, and E, and this corresponds to\nan application of the production rule E E+E.\n\u2212\u2192\nIt is customary to draw a parse tree with the string of non-terminals in\na row across the bottom, and with the rest of the tree built on top of that\nbase. Thus, the two parse trees shown above might be drawn as:\nE E\nE E\nE E E E E E\nx + y * z x + y * z\nGiven any derivation of a string, it is possible to construct a parse tree\nthat shows each of the steps in that derivation. However, two different\nderivations can give rise to the same parse tree, since the parse tree does\nnotshowtheorderinwhichproductionrulesareapplied. Forexample,the\nparse tree on the left, above, does not show whether the production rule\nE xisappliedbeforeoraftertheproductionruleE y. However, if\n\u2212\u2192 \u2212\u2192\nwerestrictourattentiontoleftderivations,thenwefindthateachparsetree\ncorresponds to a unique left derivation and vice versa. I will state this fact\nas a theorem, without proof. A similar result holds for right derivations. 4.3. PARSING AND PARSE TREES 201\nTheorem 4.5. Let G be a context-free grammar. There is a one-to-one\ncorrespondence between parse trees and left derivations based on the gram-\nmar G.\nBased on this theorem, we can say that a context-free grammar G is\nambiguous if and only if there is a string w L(G) which has two parse\n\u2208\ntrees.\nExercises\n1. Show that each of the following grammars is ambiguous by finding a string\nthat has two left derivations according to the grammar:\na) S SS b) S ASb\n\u2212\u2192 \u2212\u2192\nS aSb S \u03b5\n\u2212\u2192 \u2212\u2192\nS bSa A aA\n\u2212\u2192 \u2212\u2192\nS \u03b5 A a\n\u2212\u2192 \u2212\u2192\n2. Considerthestringz+(x+y) x. Findaleftderivationofthisstringaccording\n\u2217\nto each of the grammars G1, G2, and G3, as given in this section.\n3. Drawaparsetreeforthestring(x+y) z xaccordingtoeachofthegrammars\n\u2217 \u2217\nG1, G2, and G3, as given in this section.\n4. Drawthreedifferentparsetreesforthestringababbaabbasedonthegrammar\ngiven in part a) of exercise 1.\n5. Suppose that the string abbcabac has the following parse tree, according to\nsome grammar G:\nS\nC C\nA C\nA A C\nA A A C\na b b c a b a c\na) List five production rules that must be rules in the grammar G, given\nthat this is a valid parse tree.\nb) Givealeftderivationforthestringabbcabacaccordingtothegrammar\nG.\nc) Givearightderivationforthestringabbcabacaccordingtothegrammar\nG. 202 CHAPTER 4. GRAMMARS\n6. Show the full sequence of shift and reduce operations that are used in the\nLR(1)parsingofthestringx+(y) z accordingtothegrammarG3,andgive\n\u2217\nthe corresponding right derivation of the string.\n7. ThissectionshowedhowtouseLL(1)andLR(1)parsingtofindaderivation\nof a string in the language L(G) generated by some grammar G. How is it\npossibletouseLL(1)orLR(1)parsingtodetermineforanarbitrarystringw\nwhether w L(G)? Give an example.\n\u2208\n4.4 Pushdown Automata\nInthepreviouschapter,wesawthatthereisaneatcorrespondencebetween\nregular expressions and finite automata. That is, a language is generated\nby a regular expression if and only if that language is accepted by a finite\nautomaton. Finite automata come in two types, deterministic and nonde-\nterministic, but the two types of finite automata are equivalent in terms of\ntheir ability to recognize languages. So, the class of regular languages can\nbedefinedintwoways: eitherasthesetoflanguagesthatcanbegenerated\nby regular expressions or as the set of languages that can be recognized by\nfinite automata (either deterministic or nondeterministic).\nIn this chapter, we have introduced the class of context-free languages,\nandwehaveconsideredhowcontext-freegrammarscanbeusedtogenerate\ncontext-free languages. You might wonder whether there is any type of\nautomaton that can be used to recognize context-free languages. In fact,\nthere is: The abstract machines known as pushdown automata can be\nused to define context-free languages. That is, a language is context-free if\nand only if there is a pushdown automaton that accepts that language.\nA pushdown automaton is essentially a finite automaton with an aux-\niliary data structure known as a stack. A stack consists of a finite list of\nsymbols. Symbols can be added to and removed from the list, but only\nat one end of the list. The end of the list where items can be added and\nremoved is called the top of the stack. The list is usually visualized as a\nvertical \u201cstack\u201d of symbols, with items being added and removed at the\ntop. Adding a symbol at the top of the stack is referred to as pushing a\nsymbol onto the stack, and removing a symbol is referred to as popping\nan item from the stack. During each step of its computation, a pushdown\nautomatoniscapableofdoingseveralpushandpopoperationsonitsstack\n(this in addition to possibly reading a symbol from the input string that is\nbeing processed by the automaton).\nBefore giving a formal definition of pushdown automata, we will look\nat how they can be represented by transition diagrams. A diagram of a 4.4. PUSHDOWN AUTOMATA 203\npushdown automaton is similar to a diagram for an NFA, except that each\ntransition in the diagram can involve stack operations. We will use a label\nof the form \u03c3,x\/y on a transition to mean that the automaton consumes \u03c3\nfrom its input string, pops x from the stack, and pushes y onto the stack.\n\u03c3 can be either \u03b5 or a single symbol. x and y are strings, possibly empty.\n(When a string x = a a ...a is pushed onto a stack, the symbols are\n1 2 k\npushed in the order a ,...,a , so that a ends up on the top of the stack;\nk 1 1\nfor y =b b ...b to be popped from the stack, b must be the top symbol\n1 2 n 1\non the stack, followed by b , etc.) For example, consider the following\n2\ntransition diagram for a pushdown automaton:\nThis pushdown automaton has start state q and one accepting state,\n0\nq . It can read strings over the alphabet \u03a3 = a,b . The transition from\n1\n{ }\nq to q , labeled with a,\u03b5\/1, means that if the machine is in state q , then\n0 0 0\nit can read an a from its input string, pop nothing from the stack, push 1\nonto the stack, and remain in state q . Similarly, the transition from q to\n0 1\nq means that if the machine is in state q , it can read a b from its input\n1 1\nstring,popa1fromthestack,andpushnothingontothestack. Finally,the\ntransition from state q to q , labeled with \u03b5,\u03b5\/\u03b5, means that the machine\n0 1\ncantransitionfromstateq tostateq withoutreading,pushing,orpopping\n0 1\nanything.\nNote that the automation can follow transition b,1\/\u03b5 only if the next\nsymbol in the input string is b and if 1 is on the top of the stack. When\nit makes the transition, it consumes the b from input and pops the 1 from\nthe stack. Since in this case, the automaton pushes \u03b5 (that is, no symbols\nat all) onto the stack, the net change in the stack is simply to pop the 1.\nWehavetosaywhatitmeansforthispushdownautomatontoaccepta\nstring. For w a,b , we say that the pushdown automaton accepts w if\n\u2217\n\u2208{ }\nand only if it is possible for the machine to start in its start state, q , read\n0\nall of w, and finish in the accepting state, q , with an empty stack. Note\n1\nin particular that it is not enough for the machine to finish in an accepting\nstate\u2014it must also empty the stack.1\nIt\u2019s not difficult to see that with this definition, the language accepted\n1We could relax this restriction and require only that the machine finish in an ac-\ncepting state after reading the string w, without requiring that the stack be empty. In\nfact,usingthisdefinitionofacceptingwouldnotchangetheclassoflanguagesthatare\nacceptedbypushdownautomata. 204 CHAPTER 4. GRAMMARS\nby our pushdown automaton is anbn n N . In fact, given the string\n{ | \u2208 }\nw = akbk, the machine can process this string by following the transition\nfrom q to q k times. This will consume all the a\u2019s and will push k 1\u2019s\n0 0\nonto the stack. The machine can then jump to state q and follow the\n1\ntransition from q to q k times. Each time it does so, it consumes one\n1 1\nb from the input and pops one 1 from the stack. At the end, the input\nhas been completely consumed and the stack is empty. So, the string w\nis accepted by the automaton. Conversely, this pushdown automaton only\naccepts strings of the form akbk, since the only way that the automaton\ncanfinishintheacceptingstate,q ,istofollowthetransitionfromq toq\n1 0 0\nsome number of times, reading a\u2019s as it does so, then jump at some point\nto q , and then follow the transition from q to q some number of times,\n1 1 1\nreadingb\u2019sasitdoesso. Thismeansthatanacceptedstringmustbeofthe\nformakb\u2113 forsomek,\u2113 N. However,inreadingthisstring,theautomaton\n\u2208\npushes k 1\u2019s onto the stack and pops \u2113 1\u2019s from the stack. For the stack to\nend up empty, \u2113 must equal k, which means that in fact the string is of the\nform akbk, as claimed.\nHere are two more examples. These pushdown automata use the capa-\nbility to push or pop more than one symbol at a time:\nThe atomaton on the left accepts the language anbm n m 2 n .\n{ | \u2264 \u2264 \u2217 }\nEach time it reads an a, it pushes either one or two 1\u2019s onto the stack, so\nthat after reading n a\u2019s, the number of 1\u2019s on the stack is between n and\n2 n. Ifthemachinethenjumpstostateq ,itmustbeabletoreadexactly\n1\n\u2217\nenoughb\u2019stoemptythestack,soanystringacceptedbythismachinemust\nbe of the form anbm with n m 2 n. Conversely, any such string can\n\u2264 \u2264 \u2217\nbe accepted by the machine. Similarly, the automaton on the right above\naccepts the language anbm n\/2 m n . To accept anbm, it must push\n{ | \u2264 \u2264 }\nn1\u2019aontothestackandthenpoponeortwo1\u2019sforeachb;thiscansucceed\nonly if the number of b\u2019s is between n\/2 and n.\nNotethatanNFAcanbeconsideredtobeapushdownautomaton that\ndoesnotmake anyuseofitsstack. Thismeansthat anylanguage that can\nbeacceptedbyanNFA(thatis,anyregularlanguage)canbeacceptedbya\npushdownautomaton. Sincethelanguage anbn n N iscontext-freebut\n{ | \u2208 } 4.4. PUSHDOWN AUTOMATA 205\nnot regular, and since it is accepted by the above pushdown automaton,\nwe see that pushdown automata are capable of recognizing context-free\nlanguages that are not regular and so that pushdown automata are strictly\nmore powerful than finite automata.\nAlthough it is not particularly illuminating, we can give a formal defi-\nnition of pushdown automaton. The definition does at least make it clear\nthat the set of symbols that can be used on the stack is not necessarily the\nsame as the set of symbols that can be used as input.\nDefinition 4.4. ApushdownautomatonM isspecifiedbysixcomponents\nM =(Q,\u03a3,\u039b,q ,\u2202,F) where\n0\nQ is a finite set of states.\n\u2022\n\u03a3 is an alphabet. \u03a3 is the input alphabet for M.\n\u2022\n\u039b is an alphabet. \u039b is the stack alphabet for M.\n\u2022\nq Q is the start state of M.\n0\n\u2022 \u2208\nF Q is the set of final or accepting states in M.\n\u2022 \u2286\n\u2202 isthesetoftransitionsinM. \u2202 canbetakentobeafinitesubsetof\n\u2022\ntheset(Q (\u03a3 \u03b5 ) \u039b Q \u039b . Anelement (q ,\u03c3,x),(q ,y)\n\u2217 \u2217 1 2\n\u00d7 \u222a{ } \u00d7 \u00d7 \u00d7\nof\u2202 representsatransitionfromstateq tostateq inwhichM reads\n1 2\n(cid:1) (cid:0) (cid:1) (cid:0) (cid:1)\n\u03c3 from its input string, pops x from the stack, and pushes y onto the\nstack.\nWecanthendefinethelanguageL(M)acceptedbyapushdownautoma-\nton M = (Q,\u03a3,\u039b,q ,\u2202,F) to be the set L(M) = w \u03a3 starting from\n0 \u2217\n{ \u2208 |\nstate q , it is possible for M to read all of w and finish in some state in F\n0\nwithanemptystack . Withthisdefinition,theclassoflanguagesaccepted\n}\nby pushdown automata is the same as the class of languages generated by\ncontext-free grammars.\nTheorem 4.6. Let \u03a3 be an alphabet, and let L be a language over L. Then\nL is context-free if and only if there is a pushdown automaton whose input\nalphabet is \u03a3 such that L=L(M).\nWe will not prove this theorem, but we do discuss how one direction\ncan be proved. Suppose that L is a context-free language over an alphabet\n\u03a3. Let G = (V,\u03a3,P,S) be a context-free grammar for L. Then we can\nconstruct a pushdown automaton M that accepts L. In fact, we can take\nM = (Q,\u03a3,\u039b,q ,\u2202,F) where Q = q ,q , \u039b = \u03a3 V, F = q , and \u2202\n0 0 1 1\n{ } \u222a { }\ncontains transitions of the forms 206 CHAPTER 4. GRAMMARS\n1. (q ,\u03b5,\u03b5),(q ,S) ;\n0 1\n(cid:0) (cid:1)\n2. (q ,\u03c3,\u03c3),(q ,\u03b5) , for \u03c3 \u03a3; and\n1 1\n\u2208\n(cid:0) (cid:1)\n3. (q ,\u03b5,A),(q ,x) , for each production A x in G.\n1 1\n\u2212\u2192\n(cid:0) (cid:1)\nThe transition (q ,\u03b5,\u03b5),(q ,S) lets M move from the start state q\n0 1 0\nto the accepting state q while reading no input and pushing S onto the\n1\n(cid:0) (cid:1)\nstack. This is the only possible first move by M.\nA transition of the form (q ,\u03c3,\u03c3),(q ,\u03b5) , for \u03c3 \u03a3 allows M to read\n1 1\n\u2208\n\u03c3 from its input string, provided there is a \u03c3 on the top of the stack. Note\n(cid:0) (cid:1)\nthat if \u03c3 is at the top of the stack, then this transition is only transition\nthat applies. Effectively, any terminal symbol that appears at the top of\nthestackmustbematchedbythesamesymbolintheinputstring,andthe\ntransition rule allows M to consume the symbol from the input string and\nremove it from the stack at the same time.\nA transition of the third form, (q ,\u03b5,A),(q ,x) can be applied if and\n1 1\nonly if the non-terminal symbol A is at the top of the stack. M consumes\n(cid:0) (cid:1)\nnoinputwhenthisruleisapplied,butAisreplacedonthetopofthestack\nby the string on the right-hand side of the production rule A x. Since\n\u2212\u2192\nthe grammar G can contain several production rules that have A as their\nleft-hand side, there can be several transition rules in M that apply when\nA is on the top of the stack. This is the only source of nondeterminism in\nM; note that is also the source of nondeterminism in G.\nThe proof that L(M)=L(G) follows from the fact that a computation\nof M that accepts a string w \u03a3 corresponds in a natural way to a left\n\u2217\n\u2208\nderivation of w from G\u2019s start symbol, S. Instead of giving a proof of this\nfact, we look at an example. Consider the following context-free grammar:\nS AB\n\u2212\u2192\nA aAb\n\u2212\u2192\nA \u03b5\n\u2212\u2192\nB bB\n\u2212\u2192\nB b\n\u2212\u2192\nThis grammar generates the language anbm m > n . The pushdown\n{ | }\nautomaton constructed from this grammar by the procedure given above 4.4. PUSHDOWN AUTOMATA 207\nhas the following set of transition rules:\n(q ,\u03b5,\u03b5),(q ,S)\n0 1\n(cid:0)(q 1,a,a),(q 1,\u03b5)(cid:1)\n(cid:0)(q 1,b,b),(q 1,\u03b5)(cid:1)\n(cid:0)(q 1,\u03b5,S),(q 1,A(cid:1)B)\n(cid:0)(q 1,\u03b5,A),(q 1,aAb)(cid:1)\n(cid:0)(q 1,\u03b5,A),(q 1,\u03b5) (cid:1)\n(cid:0)(q 1,\u03b5,B),(q 1,bB(cid:1))\n(cid:0)(q 1,\u03b5,B),(q 1,b) (cid:1)\n(cid:0) (cid:1)\nSuppose that the automaton is run on the input aabbbb. We can trace the\nsequence of transitions that are applied in a computation that accepts this\ninput, and we can compare that computation to a left derivation of the\nstring:\nInput\nTransition Consumed Stack Derivation\n(q ,\u03b5,\u03b5),(q ,S) S\n0 1\n(q ,\u03b5,S),(q ,AB) AB S = AB\n(cid:0) 1 1 (cid:1) \u21d2\n(q ,\u03b5,A),(q ,aAb) aAbB = aAbB\n(cid:0) 1 1 (cid:1) \u21d2\n(q ,a,a),(q ,\u03b5) a AbB\n(cid:0) 1 1 (cid:1)\n(q ,\u03b5,A),(q ,aAb) a aAbbB = aaAbbB\n(cid:0) 1 1 (cid:1) \u21d2\n(q ,a,a),(q ,\u03b5) aa AbbB\n(cid:0) 1 1 (cid:1)\n(q ,\u03b5,A),(q ,\u03b5) aa bbB = aabbB\n(cid:0) 1 1 (cid:1) \u21d2\n(q ,b,b),(q ,\u03b5) aab bB\n(cid:0) 1 1 (cid:1)\n(q ,b,b),(q ,\u03b5) aabb B\n(cid:0) 1 1 (cid:1)\n(q ,\u03b5,B),(q ,bB) aabb bB = aabbbB\n(cid:0) 1 1 (cid:1) \u21d2\n(q ,b),(q ,b,\u03b5) aabbb B\n(cid:0) 1 1 (cid:1)\n(q ,\u03b5,B),(q ,b) aabbb b = aabbbb\n(cid:0) 1 1 (cid:1) \u21d2\n(q ,b,b),(q ,\u03b5) aabbbb\n(cid:0) 1 1 (cid:1)\n(cid:0) (cid:1)\nNote that at all times during this computation, the concatenation of\nthe input that has been consumed so far with the contents of the stack is\nequal to one of the strings in the left derivation. Application of a rule of\nthe form (q ,\u03c3,\u03c3),(q ,\u03b5) has the effect of removing one terminal symbol\n1 1\nfrom the \u201cStack\u201d column to the \u201cInput Consumed\u201d column. Application\n(cid:0) (cid:1) 208 CHAPTER 4. GRAMMARS\nof a rule of the form (q ,\u03b5,A),(q ,x) has the effect of applying the next\n1 1\nstep in the left derivation to the non-terminal symbol on the top of the\n(cid:0) (cid:1)\nstack. (In the \u201cStack\u201d column, the pushdown automaton\u2019s stack is shown\nwith its top on the left.) In the end, the entire input string has been\nconsumed and the stack is empty, which means that the string has been\nacceptedbythepushdownautomaton. Itshouldbeeasytoseethatforany\ncontextfreegrammarG,thesamecorrespondencewillalwaysholdbetween\nleft derivations and computations performed by the pushdown automaton\nconstructed from G.\nThe computation of a pushdown automaton can involve nondetermin-\nism. That is, at some point in the computation, there might be more than\none transition rule that apply. When this is not the case\u2014that is, when\nthere is no circumstance in which two different transition rules apply\u2014\nthen we say that the pushdown automaton is deterministic. Note that\na deterministic pushdown automaton can have transition rules of the form\n(q ,\u03b5,x),(q ,y) (or even (q ,\u03b5,\u03b5),(q ,y) if that is the only transition\ni j i j\nfrom state q ). Note also that is is possible for a deterministic pushdown\ni\n(cid:0) (cid:1) (cid:0) (cid:1)\nautomatontoget\u201cstuck\u201d; thatis,itispossiblethatnorulesapplyinsome\ncircumstanceseventhoughtheinputhasnotbeencompletelyconsumedor\nthe stack is not empty. If a deterministic pushdown automaton gets stuck\nwhile reading a string x, then x is not accepted by the automaton.\nThe automaton given at the beginning of this section, which accepts\nthe language anbn n N , is not deterministic. However, it is easy to\n{ | \u2208 }\nconstruct a deterministic pushdown automaton for this language:\nHowever, consider the language wwR w a,b . Here is a pushdown\n\u2217\n{ | \u2208 { } }\nautomaton that accepts this language:\nIn state q , this machine copies the first part of its input string onto the\n0\nstack. In state q , it tries to match the remainder of the input against the\n1 4.4. PUSHDOWN AUTOMATA 209\ncontents of the stack. In order for this to work, it must \u201cguess\u201d where the\nmiddleofthestringoccursbyfollowingthetransitionfromstateq tostate\n0\nq . In this case, it is by no means clear that it is possible to construct a\n1\ndeterministic pushdown automaton that accepts the same language.\nAtthispoint,itmightbetemptingtodefineadeterministiccontext-free\nlanguageasoneforwhichthereexistsadeterministicpushdownautomaton\nwhich accepts that language. However, there is a technical problem with\nthis definition: we need to make it possible for the pushdown automaton\nto detect the end of the input string. Consider the language w w\n{ | \u2208\na,b n (w)=n (w) , which consists of strings over the alphabet a,b\n\u2217 a b\n{ } \u2227 } { }\nin which the number of a\u2019s is equal to the number of b\u2019s. This language is\naccepted by the following pushdown automaton:\nIn this automaton, a c is first pushed onto the stack, and it remains on\nthe bottom of the stack until the computation ends. During the process\nof reading an input string, if the machine is in state q , then the number\n3\nof a\u2019s that have been read is greater than or equal to the number of b\u2019s\nthat have been read, and the stack contains (copies of) the excess a\u2019s that\nhave been read. Similarly, if the machine is in state q , then the number\n4\nof b\u2019s that have been read is greater than or equal to the number of a\u2019s\nthat have been read, and the stack contains (copies of) the excess b\u2019s that\nhavebeenread. Asthecomputationproceeds,ifthestackcontainsnothing\nbut a c, then the number of a\u2019s that have been consumed by the machine\nis equal to the number of b\u2019s that have been consumed; in such cases, the\nmachinecanpopthecfromthestack\u2014leavingthestackempty\u2014andjump\nto state q . If the entire string has been read at that time, then the string\n2 210 CHAPTER 4. GRAMMARS\nis accepted. This involves nondeterminism because the automaton has to\n\u201cguess\u201d when to jump to state q ; it has no way of knowing whether it has\n2\nactually reached the end of the string.\nAlthoughthispushdownautomatonisnotdeterministic,wecanmodify\nit easily to get a deterministic pushdown automaton that accepts a closely\nrelatedlanguage. Wejusthavetoaddaspecialend-of-stringsymboltothe\nlanguage. Weusethesymbol$forthispurpose. Thefollowingdeterministic\nautomaton accepts the language w$ w a,b n (w)=n (w) :\n\u2217 a b\n{ | \u2208{ } \u2227 }\nIn this modified automaton, it is only possible for the machine to reach\nthe accepting state q by reading the end-of-string symbol at a time when\n2\nthe number of a\u2019s that have been consumed is equal to the number of b\u2019s.\nTaking our cue from this example, we define what it means for a language\nto be deterministic context-free as follows:\nDefinition 4.5. Let L be a language over an alphabet \u03a3, and let $ be a\nsymbol that is not in \u03a3. We say that L is a deterministic context-free\nlanguage ifthereisadeterministicpushdownautomatonthatacceptsthe\nlanguage L$ (which is equal to w$ w L ).\n{ | \u2208 }\nTherearecontext-freelanguagesthatarenotdeterministiccontext-free.\nThismeansthatforpushdownautomata,nondeterminismaddsrealpower.\nThis contrasts with the case of finite automata, where deterministic finite\nautomata and nondeterministic finite automata are equivalent in power in\nthe sense that they accept the same class of languages.\nA deterministic context-free language can be parsed efficiently. LL(1)\nparsing and LR(1) parsing can both be defined in terms of deterministic\npushdown automata, although we have not pursued that approach here. 4.4. PUSHDOWN AUTOMATA 211\nExercises\n1. Identify the context-free language that is accepted by each of the following\npushdown automata. Explain your answers.\na)\nb)\nc)\nd)\n2. Let B be the language over the alphabet (,) that consists of strings of\n{ }\nparentheses that are balanced in the sense that every left parenthesis has a\nmatching right parenthesis. Examples include (), (())(), ((())())()(()),\nandtheemptystring. Findadeterministicpushdownautomatonwithasingle\nstate that accepts the language B. Explain how your automaton works, and\nexplain the circumstances in which it will fail to accept a given string of\nparentheses.\n3. SupposethatLislanguageoveranalphabet\u03a3. Supposethatthereisadeter-\nministic pushdown automaton that accepts L. Show that L is deterministic\ncontext-free. That is, show how to construct a deterministic pushdown au-\ntomaton that accepts the language L$. (Assume that the symbol $ is not in\n\u03a3.)\n4. Findadeterministicpushdownautomatonthatacceptsthelanguage wcwR w\n{ | \u2208\na,b \u2217 .\n{ } }\n5. Show that the language anbm n=m is deterministic context-free.\n{ | 6 }\n6. Show that the language L = w a,b \u2217 n a(w) > n b(w) is deterministic\n{ \u2208 { } | }\ncontext-free. 212 CHAPTER 4. GRAMMARS\n7. Let M = (Q,\u03a3,\u039b,q0,\u2202,F) be a pushdown automaton. Define L\u2032(M) to be\nthelanguageL\u2032(M)= w \u03a3\u2217 itispossibleforM tostartinstateq0,read\n{ \u2208 |\nall of w, and end in an accepting state . L\u2032(M) differs from L(M) in that\n}\nfor w L\u2032(M), we do not require that the stack be empty at the end of the\n\u2208\ncomputation.\na) Show that there is a pushdown automaton M\u2032 such that L(M\u2032) =\nL\u2032(M).\nb) ShowthatalanguageLiscontext-freeifandonlyifthereisapushdown\nautomaton M such that L=L\u2032(M).\nc) Identify the language L\u2032(M) for each of the automata in Exercise 1.\n8. Let L be a regular language over an alphabet \u03a3, and let K be a context-free\nlanguage over the same alphabet. Let M = (Q,\u03a3,q0,\u03b4,F) be a DFA that\naccepts L, and let N = (P,\u03a3,\u039b,p0,\u2202,E)) be a pushdown automaton that\naccepts K. Show that the language L K is context-free by constructing a\n\u2229\npushdown automaton that accepts L K. The pushdown automaton can be\n\u2229\nconstructed as a \u201ccross product\u201d of M and N in which the set of states is\nQ P. The construction is analogous to the proof that the intersection of\n\u00d7\ntwo regular languages is regular, as outlined in Exercise 3.6.7.\n4.5 Non-context-free Languages\nWe have seen that there are context-free languages that are not regular.\nThe natural question arises, are there languages that are not context-free?\nIt\u2019s easy to answer this question in the abstract: For a given alphabet \u03a3,\nthereareuncountablymanylanguagesover\u03a3,butthereareonlycountably\nmanycontext-freelanguagesover\u03a3. Itfollowsthatmostlanguagesarenot\ncontext-free. However, this answer is not very satisfying since it doesn\u2019t\ngive us any example of a specific language that is not context-free.\nAs in the case of regular languages, one way to show that a given lan-\nguage L is not context-free is to find some property that is shared by all\ncontext-free languages and then to show that L does not have that prop-\nerty. For regular languages, the Pumping Lemma gave us such a property.\nIt turns out that there is a similar Pumping Lemma for context-free lan-\nguages. Theproofofthislemmausesparsetrees. Intheproof,wewillneed\na way of representing abstract parse trees, without showing all the details\nof the tree. The picture\nA\nx 4.5. NON-CONTEXT-FREE LANGUAGES 213\nrepresents a parse tree which has the non-terminal symbol A at its root\nand the string x along the \u201cbottom\u201d of the tree. (That is, x is the string\nmade up of all the symbols at the endpoints of the tree\u2019s branches, read\nfrom left toright.) Note that this could bea partial parsetree\u2014something\nthat could be a part of a larger tree. That is, we do not require A to be\nthe start symbol of the grammar and we allow x to contain both terminal\nand non-terminal symbols. The string x, which is along the bottom of the\ntree, is referred to as the yield of the parse tree. Sometimes, we need to\nshow more explicit detail in the tree. For example, the picture\nA\nB\nx y z\nrepresents a parse tree in which the yield is the string xyz. The string y is\ntheyieldofasmallertree,withrootB,whichiscontainedwithinthelarger\ntree. Note that any of the strings x, y, or z could be the empty string.\nWe will also need the concept of the height of a parse tree. The height\nof a parse tree is the length of the longest path from the root of the tree to\nthe tip of one of its branches.\nLiketheversionforregularlanguages,thePumpingLemmaforcontext-\nfree languages shows that any sufficiently long string in a context-free lan-\nguage contains a pattern that can be repeated to produce new strings that\nare also in the language. However, the pattern in this case is more com-\nplicated. For regular languages, the pattern arises because any sufficiently\nlong path through a given DFA must contain a loop. For context-free lan-\nguages, the pattern arises because in a sufficiently large parse tree, along a\npath from the root of the tree to the tip of one of its branches, there must\nbe some non-terminal symbol that occurs more than once.\nTheorem 4.7 (Pumping Lemma for Context-free Languages). Suppose\nthat L is a context-free language. Then there is an integer K such that any\nstring w L(G) with w K has the property that w can be written in the\n\u2208 | |\u2265\nform w =uxyzv where\nx and z are not both equal to the empty string;\n\u2022\nxyz <K; and\n\u2022| |\nFor any n N, the string uxnyznv is in L.\n\u2022 \u2208\nProof. Let G = (V,\u03a3,P,S) be a context-free grammar for the language\nL. Let N be the number of non-terminal symbols in G, plus 1. That is, 214 CHAPTER 4. GRAMMARS\nN = V +1. Consider all possible parse trees for the grammar G with\n| |\nheight less than or equal to N. (Include parse trees with any non-terminal\nsymbol as root, not just parse trees with root S.) There are only finitely\nmany such parse trees, and therefore there are only finitely many different\nstrings that are the yields of such parse trees. Let K be an integer which\nis greater than the length of any such string.\nNow suppose that w is any string in L whose length is greater than\nor equal to K. Then any parse tree for w must have height greater than\nN. (This follows since w K and the yield of any parse tree of height\n| | \u2265\nN has length less than K.) Consider a parse tree for w of minimal size,\n\u2264\nthat is one that contains the smallest possible number of nodes. Since the\nheight of this parse tree is greater than N, there is at least one path from\nthe root of the tree to tip of a branch of the tree that has length greater\nthan N. Consider the longest such path. The symbol at the tip of this\npath is a terminal symbol, but all the other symbols on the path are non-\nterminal symbols. There are at least N such non-terminal symbols on the\npath. Since the number of different non-terminal symbols is V and since\n| |\nN = V +1, some non-terminal symbol must occur twice on the path. In\n| |\nfact, some non-terminal symbol must occur twice among the bottommost\nN non-terminal symbols on the path. Call this symbol A. Then we see\nthat the parse tree for w has the form shown here:\nS\nA\nA\nu x y z v\nThestructureofthistreebreaksthestringw intofivesubstrings, asshown\nin the above diagram. We then have w = uxyzv. It only remains to show\nthat x, y, and z satisfy the three requirements stated in the theorem.\nLet T refer to the entire parse tree, let T refer to the parse tree whose\n1\nrootistheupperAinthediagram, andletT betheparsetreewhoseroot\n2\nis the lower A in the diagram. Note that the height of T is less than or\n1 4.5. NON-CONTEXT-FREE LANGUAGES 215\nequal to N. (This follows from two facts: The path shown in T from its\n1\nroot to its base has length less than or equal to N, because we chose the\ntwooccurrencesofAtobeamongtheN bottommostnon-terminalsymbols\nalong the path in T from its root to its base. We know that there is no\nlonger path from the root of T to its base, since we chose the path in T\n1\nto be the longest possible path from the root of T to its base.) Since any\nparse tree with height less than or equal to N has yield of length less than\nK, we see that xyz <K.\n| |\nIf we remove T from T and replace it with a copy of T , the result is a\n1 2\nparse tree with yield uyv, so we see that the string uyv is in the language\nL. Now, suppose that both x and z are equal to the empty string. In that\ncase, w =uyv, so the tree we have created would be another parse tree for\nw. But this tree is smaller than T, so this would contradict the fact that\nT is the smallest parse tree for w. We see that x and z cannot both be the\nempty string.\nIf we remove T from T and replace it with a copy of T , the result is a\n2 1\nparse tree with yield ux2yz2v, so we see that ux2yz2v L. The two parse\n\u2208\ntrees that we have created look like this:\nS S\nA A\ny\nA\nu v u x A z v\nx y z\nFurthermore, we can apply the process of replacing T with a copy of T\n2 1\nto the tree on the right above to create a parse tree with yield ux3yz3v.\nContinuing in this way, we see that uxnyznv L for all n N. This\n\u2208 \u2208\ncompletes the proof of the theorem.\nSince this theorem guarantees that all context-free languages have a\ncertain property, it can be used to show that specific languages are not\ncontext-free. The method is to show that the language in question does 216 CHAPTER 4. GRAMMARS\nnot have the property that is guaranteed by the theorem. We give two\nexamples.\nCorollary 4.8. Let L be the language anbncn n N . Then L is not a\n{ | \u2208 }\ncontext-free language.\nProof. We give a proof by contradiction. Suppose that L is context-free.\nThen, by the Pumping Lemma for Context-free Languages, there is an\ninteger K such that every string w L with w K can be written in\n\u2208 | | \u2265\nthe form w = uxyzv where x and z are not both empty, xyz < K, and\n| |\nuxnyznv L for every n N.\n\u2208 \u2208\nConsider the string w =aKbKcK, which is in L, and write w =uxyzv,\nwhere u, x, y, z, and v satisfy the stated conditions. Since xyz < K,\n| |\nwe see that if xyz contains an a, then it cannot contain a c. And if it\ncontains a c, then it cannot contain an a. It is also possible that xyz is\nmadeupentirelyofb\u2019s. Inanyofthesecases,thestringux2yz2v cannotbe\nin L, since it does not contain equal numbers of a\u2019s, b\u2019s, and c\u2019s. But this\ncontradicts the fact that uxnyznv L for all n N. This contradiction\n\u2208 \u2208\nshows that the assumption that L is context-free is incorrect.\nCorollary 4.9. Let \u03a3 be any alphabet that contains at least two symbols.\nLet L be the language over \u03a3 defined by L = ss s \u03a3 . Then L is not\n\u2217\n{ | \u2208 }\ncontext-free.\nProof. Suppose, forthesakeofcontradiction, thatLiscontext-free. Then,\nby the Pumping Lemma for Context-free Languages, there is an integer\nK such that every string w L with w K can be written in the form\n\u2208 | | \u2265\nw =uxyzvwherexandzarenotbothempty, xyz <K,anduxnyznv L\n| | \u2208\nfor every n N.\n\u2208\nLet a and b represent distinct symbols in \u03a3. Let s = aKbaKb and let\nw =ss=aKbaKbaKbaKb, which is in L. Write w =uxyzv, where u, x, y,\nz, and v satisfy the stated conditions.\nSince xyz < K, x and z can, together, contain no more than one b.\n| |\nIf either x or y contains a b, then ux2yz2v contains exactly five b\u2019s. But\nany string in L is of the form rr for some string r and so contains an even\nnumber of b\u2019s. The fact that ux2yz2z contains five b\u2019s contradicts the fact\nthat ux2yz2v L. So, we get a contradiction in the case where x or y\n\u2208\ncontains a b.\nNow, consider the case where x and y consist entirely of a\u2019s. Again\nsince xyz < K, we must have either that x and y are both contained in\n| |\nthe same group of a\u2019s in the string aKbaKbaKbaKb, or that x is contained\nin one group of a\u2019s and y is contained in the next. In either case, it is easy 4.5. NON-CONTEXT-FREE LANGUAGES 217\nto check that the string ux2yz2v is no longer of the form rr for any string\nr, which contradicts the fact that ux2yz2v L.\n\u2208\nSince we are led to a contradiction in every case, we see that the as-\nsumption that L is context-free must be incorrect.\nNow that we have some examples of languages that are not context-\nfree, we can settle some other questions about context-free languages. In\nparticular, we can show that the intersection of two context-free languages\nis not necessarily context-free and that the complement of a context-free\nlanguage is not necessarily context-free.\nTheorem 4.10. The intersection of two context-free languages is not nec-\nessarily a context-free language.\nProof. To prove this, it is only necessary to produce an example of two\ncontext-free languages L and M such that L M is not a context-free\n\u2229\nlanguages. Consider the following languages, defined over the alphabet\n\u03a3= a,b,c :\n{ }\nL= anbncm n N and m N\n{ | \u2208 \u2208 }\nM = anbmcm n N and m N\n{ | \u2208 \u2208 }\nNote that strings in L have equal numbers of a\u2019s and b\u2019s while strings in\nM have equal numbers of b\u2019s and c\u2019s. It follows that strings in L M have\n\u2229\nequal numbers of a\u2019s, b\u2019s, and c\u2019s. That is,\nL M = anbncn n N\n\u2229 { | \u2208 }\nWeknowfromtheabovetheoremthatL M isnotcontext-free. However,\n\u2229\nbothLandM arecontext-free. ThelanguageLisgeneratedbythecontext-\nfree grammar\nS TC\n\u2212\u2192\nC cC\n\u2212\u2192\nC \u03b5\n\u2212\u2192\nT aTb\n\u2212\u2192\nT \u03b5\n\u2212\u2192\nand M is generated by a similar context-free grammar.\nCorollary 4.11. The complement of a context-free language is not neces-\nsarily context-free. 218 CHAPTER 4. GRAMMARS\nProof. Suppose for the sake of contradiction that the complement of every\ncontext-free language is context-free.\nLetLandM betwocontext-freelanguagesoverthealphabet\u03a3. Byour\nassumption, the complements L and M are context-free. By Theorem 4.3,\nit follows that L M is context-free. Applying our assumption once again,\n\u222a\nwe have that L M is context-free. But L M =L M, so we have that\n\u222a \u222a \u2229\nL M is context-free.\n\u2229\nWe have shown, based on our assumption that the complement of any\ncontext-freelanguageiscontext-free,thattheintersectionofanytwocontext-\nfree languages is context-free. But this contradicts the previous theorem,\nsoweseethattheassumptioncannotbetrue. Thisprovesthetheorem.\nNote that the preceding theorem and corollary say only that L M is\n\u2229\nnotcontext-freeforsome context-freelanguagesLandM andthatLisnot\ncontext-free for some context-free language L. There are, of course, many\nexamples of context-free languages L and M for which L M and L are in\n\u2229\nfact context-free.\nEven though the intersection of two context-free languages is not nec-\nessarily context-free, it happens that the intersection of a context-free lan-\nguage with a regular language is always context-free. This is not difficult\nto show, and a proof is outlined in Exercise 4.4.8. I state it here without\nproof:\nTheorem 4.12. Suppose that L is a context-free language and that M is\na regular language. Then L M is a context-free language.\n\u2229\nFor example, let L and M be the languages defined by L = w\n{ \u2208\na,b w = wR and M = w a,b the length of w is a multiple\n\u2217 \u2217\n{ } | } { \u2208 { } |\nof 5 . Since L is context-free and M is regular, we know that L M is\n} \u2229\ncontext-free. The language L M consists of every palindrome over the\n\u2229\nalphabet a,b whose length is a multiple of five.\n{ }\nThis theorem can also be used to show that certain languages are not\ncontext-free. Forexample,considerthelanguageL= w a,b,c n (w)\n\u2217 a\n{ \u2208{ } |\n= n (w) = n (w) . (Recall that n (w) is the number of times that the\nb c x\n}\nsymbol x occurs in the string w.) We can use a proof by contradiction to\nshow that L is not context-free. Let M be the regular language defined by\nthe regular expression a b c . It is clear that L M = anbncn n N . If\n\u2217 \u2217 \u2217\n\u2229 { | \u2208 }\nLwerecontext-free,then,bytheprevioustheorem,L M wouldbecontext-\n\u2229\nfree. However, we know from Theorem 4.8 that L M is not context-free.\n\u2229\nSo we can conclude that L is not context-free. 4.6. GENERAL GRAMMARS 219\nExercises\n1. Show that the following languages are not context-free:\na) anbmck n>m>k\n{ | }\nb) w a,b,c \u2217 n a(w)>n b(w)>n c(w)\n{ \u2208{ } | }\nc) www w a,b \u2217\n{ | \u2208{ } }\nd) anbmck n,m N and k=m n\n{ | \u2208 \u2217 }\ne) anbm m=n2\n{ | }\n2. Show that the languages an n is a prime number and an2 n N are\n{ | } { | \u2208 }\nnot context-free. (In fact, it can be shown that a language over the alphabet\na is context-free if and only if it is regular.)\n{ }\n3. Show that the language w a,b \u2217 n a(w) = n b(w) and w contains the\n{ \u2208 { } |\nstring baaab as a substring is context-free.\n}\n4. SupposethatM isanyfinitelanguageandthatLisanycontext-freelanguage.\nShow that the language LrM is context-free. (Hint: Any finite language is\na regular language.)\n4.6 General Grammars\nAt the beginning of this chapter the general idea of a grammar as a set of\nrewritingorproductionruleswasintroduced. Formostofthechapter,how-\never, we have restricted our attention to context-free grammars, in which\nproduction rules must be of the form A x where A is a non-terminal\n\u2212\u2192\nsymbol. In this section, we will consider general grammars, that is, gram-\nmars in which there is no such restriction on the form of production rules.\nFor a general grammar, a production rule has the form u x, where u is\n\u2212\u2192\nstring that can contain both terminal and non-terminal symbols. For con-\nvenience, we will assume that u contains at least one non-terminal symbol,\nalthough even this restriction could be lifted without changing the class of\nlanguages that can be generated by grammars. Note that a context-free\ngrammar is, in fact, an example of a general grammar, since production\nrules in a general grammar are allowed to be of the form A x. They\n\u2212\u2192\njustdon\u2019thavetobeofthisform. Iwillusetheunmodifiedtermgrammar\ntorefertogeneralgrammars.2 Thedefinitionofgrammarisidenticaltothe\ndefinition of context-free grammar, except for the form of the production\nrules:\n2There is another special type of grammar that is intermediate between context-\nfreegrammarsandgeneralgrammars. Inaso-calledcontext-sensitivegrammar,every\nproductionruleisoftheformu\u2212\u2192xwhere|x|\u2265|u|. Wewillnotcovercontext-sensitive\ngrammarsinthistext. 220 CHAPTER 4. GRAMMARS\nDefinition 4.6. A grammar is a 4-tuple (V,\u03a3,P,S), where:\n1. V is a finite set of symbols. The elements of V are the non-terminal\nsymbols of the grammar.\n2. \u03a3 is a finite set of symbols such that V \u03a3= . The elements of \u03a3 are\n\u2229 \u2205\nthe terminal symbols of the grammar.\n3. P is a set of production rules. Each rule is of the form u x where u\n\u2212\u2192\nand x are strings in (V \u03a3) and u contains at least one symbol from\n\u2217\n\u222a\nV.\n4. S V. S is the start symbol of the grammar.\n\u2208\nSuppose G is a grammar. Just as in the context-free case, the lan-\nguage generated by G is denoted by L(G) and is defined as L(G) = x\n{ \u2208\n\u03a3 S = x . That is, a string x is in L(G) if and only if x is a string of\n\u2217\n|\n\u21d2\u2217G\n}\nterminal symbols and there is a derivation that produces x from the start\nsymbol, S, in one or more steps.\nThe natural question is whether there are languages that can be gen-\nerated by general grammars but that cannot be generated by context-free\nlanguages. We can answer this question immediately by giving an example\nof such a language. Let L be the language L = w a,b,c n (w) =\n\u2217 a\n{ \u2208 { } |\nn (w) = n (w) . We saw at the end of the last section that L is not\nb c\n}\ncontext-free. However, L is generated by the following grammar:\nS SABC\n\u2212\u2192\nS \u03b5\n\u2212\u2192\nAB BA\n\u2212\u2192\nBA AB\n\u2212\u2192\nAC CA\n\u2212\u2192\nCA AC\n\u2212\u2192\nBC CB\n\u2212\u2192\nCB BC\n\u2212\u2192\nA a\n\u2212\u2192\nB b\n\u2212\u2192\nC c\n\u2212\u2192\nForthisgrammar,thesetofnon-terminalsis S,A,B,C andthesetofter-\n{ }\nminal symbols is a,b,c . Since both terminals and non-terminal symbols\n{ }\ncanoccurontheleft-handsideofaproductionruleinageneralgrammar,it\nisnotpossible,ingeneral,todeterminewhichsymbolsarenon-terminaland\nwhichareterminaljustbylookingatthelistofproductionrules. However,\nI will follow the convention that uppercase letters are always non-terminal 4.6. GENERAL GRAMMARS 221\nsymbols. Withthisconvention,Icancontinuetospecifyagrammarsimply\nby listing production rules.\nThe first two rules in the above grammar make it possible to produce\nthe strings \u03b5, ABC, ABCABC, ABCABCABC, and so on. Each of these\nstrings contains equal numbers of A\u2019s, B\u2019s, and C\u2019s. The next six rules\nallow the order of the non-terminal symbols in the string to be changed.\nThey make it possible to arrange the A\u2019s, B\u2019s, and C\u2019s into any arbitrary\norder. Note that these rules could not occur in a context-free grammar.\nThe last three rules convert the non-terminal symbols A, B, and C into\nthe corresponding terminal symbols a, b, and c. Remember that all the\nnon-terminals must be eliminated in order to produce a string in L(G).\nHere, for example, is a derivation of the string baabcc using this grammar.\nIn each line, the string that will be replaced on the next line is underlined.\nS = SABC\n\u21d2\n= SABCABC\n\u21d2\n= ABCABC\n\u21d2\n= BACABC\n\u21d2\n= BAACBC\n\u21d2\n= BAABCC\n\u21d2\n= bAABCC\n\u21d2\n= baABCC\n\u21d2\n= baaBCC\n\u21d2\n= baabCC\n\u21d2\n= baabcC\n\u21d2\n= baabcc\n\u21d2\nWe could produce any string in L in a similar way. Of course, this only\nshows that L L(G). To show that L(G) L, we can observe that\n\u2286 \u2286\nfor any string w such that S = w, n (w)+n (w) = n (w)+n (w) =\n\u2217 A a B b\n\u21d2\nn (w)+n (w). This follows since the rule S = SABC produces strings\nC c\n\u21d2\nin which n (w) = n (w) = n (w), and no other rule changes any of the\nA B C\nquantitiesn (w)+n (w),n (w)+n (w),orn (w)+n (w). Afterapplying\nA a B b C c\nthese rules to produce a string x L(G), we must have that n (x), n (x),\nA B\n\u2208\nandn (x)arezero. Thefactthatn (x)=n (x)=n (x)thenfollowsfrom\nC a b c\nthe fact that n (x)+n (x) = n (x)+n (x) = n (x)+n (x). That is,\nA a B b C c\nx L.\n\u2208\nOur first example of a non-context-free language was anbncn n N .\n{ | \u2208 }\nThis language can be generated by a general grammar similar to the pre- 222 CHAPTER 4. GRAMMARS\nvious example. However, it requires some cleverness to force the a\u2019s, b\u2019s,\nandc\u2019sintothecorrectorder. Todothis, insteadofallowingA\u2019s, B\u2019s, and\nC\u2019s to transform themselves spontaneously into a\u2019s, b\u2019s, and c\u2019s, we use\nadditional non-terminal symbols to transform them only after they are in\nthe correct position. Here is a grammar that does this:\nS SABC\n\u2212\u2192\nS X\n\u2212\u2192\nBA AB\n\u2212\u2192\nCA AC\n\u2212\u2192\nCB BC\n\u2212\u2192\nXA aX\n\u2212\u2192\nX Y\n\u2212\u2192\nYB bY\n\u2212\u2192\nY Z\n\u2212\u2192\nZC cZ\n\u2212\u2192\nZ \u03b5\n\u2212\u2192\nHere,thefirsttworulesproduceoneofthestringsX,XABC,XABCABC,\nXABCABCABC,andsoon. ThenextthreerulesallowA\u2019stomovetothe\nleftandC\u2019stomovetotheright,producingastringoftheformXAnBnCn,\nfor some n N. The rule XA aX allows the X to move through the\n\u2208 \u2212\u2192\nA\u2019sfromlefttoright, convertingA\u2019stoa\u2019sasitgoes. Afterconvertingthe\nA\u2019s,theX canbetransformedintoaY. TheY willthenmovethroughthe\nB\u2019s, converting them to b\u2019s. Then, the Y is transformed into a Z, which\nis responsible for converting C\u2019s to c\u2019s. Finally, an application of the rule\nZ \u03b5 removes the Z, leaving the string anbncn.\n\u2212\u2192\nNote that if the rule X Y is applied before all the A\u2019s have been\n\u2212\u2192\nconvertedtoa\u2019s,thenthereisnowayfortheremainingA\u2019stobeconverted\ntoa\u2019sorotherwiseremovedfromthestring. Thismeansthatthederivation\nhas entered a dead end, which can never produce a string that consists of\nterminalsymbolsonly. Theonlyderivationsthatcanproducestringsinthe\nlanguage generated by the grammar are derivations in which the X moves\npast all the A\u2019s, converting them all toa\u2019s. At this point in the derivation,\nthestringisoftheformanXuwhereuisastringconsistingentirelyofB\u2019s\nand C\u2019s. At this point, the rule X Y can be applied, producing the\n\u2212\u2192\nstring anYu. Then, if a string of terminal symbols is ever to be produced,\ntheY mustmovepastalltheB\u2019s,producingthestringanbnYCn. Youcan\nsee that the use of three separate non-terminals, X, Y, and Z, is essential\nfor forcing the symbols in anbncn into the correct order. 4.6. GENERAL GRAMMARS 223\nFor one more example, consider the language\nan2\nn N . Like the\n{ | \u2208 }\nother languages we have considered in this section, this language is not\ncontext-free. However, it can be generated by a grammar. Consider the\ngrammar\nS DTE\n\u2212\u2192\nT BTA\n\u2212\u2192\nT \u03b5\n\u2212\u2192\nBA AaB\n\u2212\u2192\nBa aB\n\u2212\u2192\nBE E\n\u2212\u2192\nDA D\n\u2212\u2192\nDa aD\n\u2212\u2192\nDE \u03b5\n\u2212\u2192\nThe first three rules produce all strings of the form DBnAnE, for n N.\n\u2208\nLet\u2019s considerwhat happensto the stringDBnAnE asthe remaining rules\nare applied. The next two rules allow a B to move to the right until it\nreachestheE. EachtimetheB passesanA, anewaisgenerated, butaB\nwill simply move past an a without generating any other characters. Once\nthe B reaches the E, the rule BE E makes the B disappear. Each\n\u2212\u2192\nB from the string DBnAnE moves past n A\u2019s and generates n a\u2019s. Since\nthere are n B\u2019s, a total of n2 a\u2019s are generated. Now, the only way to get\nridoftheD atthebeginningofthestringisforittomoverightthroughall\nthe A\u2019s and a\u2019s until it reaches the E at the end of the string. As it does\nthis, the rule DA D eliminates all the A\u2019s from the string, leaving the\nstring\nan2\nDE.\nAp\u2212 p\u2192\nlying the rule DE \u03b5 to this gives\nan2\n. This string\n\u2212\u2192\ncontains no non-terminal symbols and so is in the language generated by\nthe grammar. We see that every string of the form\nan2\nis generated by the\nabove grammar. Furthermore, only strings of this form can be generated\nby the grammar.\nGiven a fixed alphabet \u03a3, there are only countably many different lan-\nguages over \u03a3 that can be generated by grammars. Since there are un-\ncountably many different languages over \u03a3, we know that there are many\nlanguages that cannot be generated by grammars. However, it is surpris-\ningly difficult to find an actual example of such a language.\nAs a first guess, you might suspect that just as anbn n N is an\n{ | \u2208 }\nexample of a language that is not regular and anbncn n N is an ex-\n{ | \u2208 }\nample of a language that is not context-free, so anbncndn n N might\n{ | \u2208 } 224 CHAPTER 4. GRAMMARS\nbe an example of a language that cannot be generated by any grammar.\nHowever,thisisnotthecase. Thesametechniquethatwasusedtoproduce\na grammar that generates anbncn n N can also be used to produce a\n{ | \u2208 }\ngrammar for anbncndn n N . In fact, the technique extends to similar\n{ | \u2208 }\nlanguages based on any number of symbols.\nOr you might guess that there is no grammar for the language an n\n{ |\nisaprimenumber . Certainly,producingprimenumbersdoesn\u2019tseemlike\n}\nthekindofthingthatwewouldordinarilydowithagrammar. Nevertheless,\nthereisagrammarthatgeneratesthislanguage. Wewillnotactuallywrite\ndownthegrammar,butwewilleventuallyhaveawaytoprovethatitexists.\nThe language\nan2\nn N really doesn\u2019t seem all that \u201cgrammatical\u201d\n{ | \u2208 }\neither, but we produced a grammar for it above. If you think about how\nthis grammar works, you might get the feeling that its operation is more\nlike \u201ccomputation\u201d than \u201cgrammar.\u201d This is our clue. A grammar can\nbe thought of as a kind of program, albeit one that is executed in a non-\ndeterministic fashion. It turns out that general grammars are precisely as\npowerfulasanyothergeneral-purposeprogramminglanguage,suchasJava\nor C++. More exactly, a language can be generated by a grammar if and\nonlyifthereisacomputerprogramwhoseoutputconsistsofalistcontain-\ning all the strings and only the strings in that language. Languages that\nhave this property are said to be recursively enumerable languages.\n(This term as used here is not closely related to the idea of a recursive\nsubroutine.) The languages that can be generated by general grammars\nare precisely the recursively enumerable languages. We will return to this\ntopic in the next chapter.\nItturnsoutthattherearemanyformsofcomputationthatareprecisely\nequivalent in power to grammars and to computer programs, and no one\nhas ever found any form of computation that is more powerful. This is one\nof the great discoveries of the twentieth century, and we will investigate it\nfurther in the next chapter.\nExercises\n1. Findaderivationforthestringcaabcb,accordingtothefirstexamplegrammar\nin this section. Find a derivation for the string aabbcc, according to the\nsecond example grammar in this section. Find a derivation for the string\naaaa, according to the third example grammar in this section.\n2. Consider the third sample grammar from this section, which generates the\nlanguage\nan2\nn N . Isthenon-terminalsymbolDnecessaryinthisgram-\n{ | \u2208 }\nmar? What if the first rule of the grammar were replaced by S TE\n\u2212\u2192\nand the last three rules were replaced by A \u03b5 and E \u03b5? Would the\n\u2212\u2192 \u2212\u2192\nresulting grammar still generate the same language? Why or why not? 4.6. GENERAL GRAMMARS 225\n3. Find a grammar that generates the language L= w a,b,c,d \u2217 n a(w)=\n{ \u2208{ } |\nn b(w) = n c(w) = n d(w) . Let \u03a3 be any alphabet. Argue that the language\n}\nw \u03a3\u2217 all symbols in \u03a3 occur equally often in w can be generated by a\n{ \u2208 | }\ngrammar.\n4. For each of the following languages, find a grammar that generates the lan-\nguage. In each case, explain how your grammar works.\na) anbncndn n N b) anbmcnm n N and m N\n{ | \u2208 } { | \u2208 \u2208 }\nc) ww w a,b \u2217 d) www w a,b \u2217\ne)\n{ a2n|\nn\n\u2208 N{ } }\nf)\n{\nw\n| a,b\u2208 ,c{\n\u2217\nn} a(}\nw)>n b(w)>n c(w)\n{ | \u2208 } { \u2208{ } | } 226 CHAPTER 4. GRAMMARS Chapter 5\nTuring Machines and\nComputability\nW\ne saw hints at the end of the previous chapter that \u201ccomputation\u201d is\na more general concept than we might have thought. General grammars,\nwhich at first encounter don\u2019t seem to have much to do with algorithms or\ncomputing, turn out to be able to do things that are similar to the tasks\ncarriedoutbycomputerprograms. Inthischapter,wewillseethatgeneral\ngrammars are precisely equivalent to computer programs in terms of their\ncomputationalpower,andthatbothareequivalenttoaparticularlysimple\nmodel of computation known as a Turing machine. We shall also see\nthat there are limits to what can be done by computing.\n5.1 Turing Machines\nHistorically,thetheoreticalstudyofcomputingbeganbeforecomputersex-\nisted. One of the early models of computation was developed in the 1930s\nby the British mathematician, Alan Turing, who was interested in study-\ning the theoretical abilities and limitations of computation. His model for\ncomputation is a very simple abstract computing machine which has come\nto be known as a Turing machine. While Turing machines are not appli-\ncable in the same way that regular expressions, finite-state automata, and\ngrammarsareapplicable,theiruseasafundamentalmodelforcomputation\nmeans that every computer scientist should be familiar with them, at least\nin a general way.\nA Turing machine is really not much more complicated than a finite-\n227 228 CHAPTER 5. TURING MACHINES AND COMPUTABILITY\nstateautomatonorapushdownautomaton.1 LikeaFSA,aTuringmachine\nhas a finite number of possible states, and it changes from state to state\nas it computes. However, a Turing machine also has an infinitely long\ntape that it can use for input and output. The tape extends to infinity\nin both directions. The tape is divided into cells, which are in one-to-one\ncorrespondencewiththeintegers,Z. Eachcellcaneitherbeblankoritcan\nhold a symbol from a specified alphabet. The Turing machine can move\nback and forth along this tape, reading and writing symbols and changing\nstate. Itcanreadonlyonecellatatime, andpossiblywriteanewvaluein\nthat cell. After doing this, it can change state and it can move by one cell\neithertotheleftortotheright. ThisishowtheTuringmachinecomputes.\nTouseaTuringmachine,youwouldwritesomeinputonitstape,startthe\nmachine, andletitcomputeuntilithalts. Whateveriswrittenonthetape\nat that time is the output of the computation.\nAlthough the tape is infinite, only a finite number of cells can be non-\nblank at any given time. If you don\u2019t like the idea of an infinite tape, you\ncan think of a finite tape that can be extended to an arbitrarily large size\nas the Turing machine computes: If the Turing machine gets to either end\nof the tape, it will pause and wait politely until you add a new section\nof tape. In other words, it\u2019s not important that the Turing machine have\nan infinite amount of memory, only that it can use as much memory as it\nneedsforagivencomputation, uptoanyarbitrarilylargesize. Inthisway,\na Turing machine is like a computer that can ask you to buy it a new disk\ndrive whenever it needs more storage space to continue a computation.2\nA given Turing machine has a fixed, finite set of states. One of these\nstatesisdesignatedasthestartstate. ThisisthestateinwhichtheTuring\nmachine begins a computation. Another special state is the halt state.\nThe Turing machine\u2019s computation ends when it enters its halt state. It\nis possible that a computation might never end because the machine never\nenters the halt state. This is analogous to an infinite loop in a computer\nprogram.\n1Infact,Turingmachinescanbeshowntobeequivalentintheircomputationalpower\ntopushdownautomatawithtwoindependentstacks.\n2The tape of a Turing machine can be used to store arbitrarily large amounts of\ninformation in a straightforward way. Although we can imagine using an arbitrary\namount of memory with a computer, it\u2019s not so easy. Computers aren\u2019t set up to keep\ntrack of unlimited amounts of data. If you think about how it might be done, you\nprobably won\u2019t come with anything better than an infinite tape. (The problem is that\ncomputersuseinteger-valuedaddressestokeeptrackofdatalocations. Ifalimitisput\non the number of bits in an address, then only a fixed, finite amount of data can be\naddressed. If no limit is put on the number of bits in an address, then we are right\nback to the problem of storing an arbitrarily large piece of data\u2014just to represent an\naddress!) 5.1. TURING MACHINES 229\nAt each step in its computation, the Turing machine reads the contents\nof the tape cell where it is located. Depending on its state and the symbol\nthat it reads, the machine writes a symbol (possibly the same symbol) to\nthe cell, moves one cell either to the left or to the right, and (possibly)\nchanges its state. The output symbol, direction of motion, and new state\naredeterminedbythecurrentstateandtheinputsymbol. Notethateither\nthe input symbol, the output symbol, or both, can be blank. A Turing\nmachine has a fixed set of rules that tell it how to compute. Each rule\nspecifies the output symbol, direction of motion, and new state for some\ncombinationofcurrentstateandinputsymbol. Themachinehasarulefor\nevery possible combination of current state and input symbol, except that\ntherearenorulesforwhathappensifthecurrentstateisthehaltstate. Of\ncourse, once the machine enters the halt state, its computation is complete\nand the machine simply stops.\nI will use the character # to represent a blank in a way that makes\nit visible. I will always use h to represent the halt state. I will indicate\nthe directions, left and right, with L and R, so that L,R is the set of\n{ }\npossible directions of motion. With these conventions, we can give the\nformal definition of a Turing machine as follows:\nDefinition 5.1. A Turing machine is a 4-tuple (Q,\u039b,q ,\u03b4), where:\n0\n1. Q is a finite set of states, including the halt state, h.\n2. \u039b is an alphabet which includes the blank symbol, #.\n3. q Q is the start state.\n0\n\u2208\n4. \u03b4: (Qr h ) \u039b \u039b L,R Qisthetransitionfunction. The\n{ } \u00d7 \u2192 \u00d7{ }\u00d7\nfact that \u03b4(q,\u03c3) = (\u03c4,d,r) means that when the Turing machine\nisinstateq andreadsthesymbol\u03c3,itwritesthesymbol\u03c4,moves\none cell in the direction d, and enters state r.\nEventhoughthisistheformaldefinition,it\u2019seasiertoworkwithatran-\nsition diagram representation of Turing machines. The transition diagram\nfor a Turing machine is similar to the transition diagram for a DFA. How-\never,thereareno\u201caccepting\u201dstates(onlyahaltstate). Furthermore,there\nmustbeawaytospecifytheoutputsymbolandthedirectionofmotionfor\neachstepofthecomputation. Wedothisbylabelingarrowswithnotations\nof the form (\u03c3,\u03c4,L) and (\u03c3,\u03c4,R), where \u03c3 and \u03c4 are symbols in the Turing\nmachine\u2019s alphabet. For example,\n(a,b,L)\nq h\n0 230 CHAPTER 5. TURING MACHINES AND COMPUTABILITY\nindicates that when the machine is in state q and reads an a, it writes a\n0\nb, moves left, and enters state h.\nHere, for example, is a transition diagram for a simple Turing machine\nthat moves to the right, changing a\u2019s to b\u2019s and vice versa, until it finds a\nc. It leaves blanks (#\u2019s) unchanged. When and if the machine encounters a\nc, it moves to the left and halts:\n(a,b,R)\n(c,c,L)\n(b,a,R) q h\n0\n(#,#,R)\nTo simplify the diagrams, I will leave out any transitions that are not\nrelevant to the computation that I want the machine to perform. You\ncan assume that the action for any omitted transition is to write the same\nsymbol that was read, move right, and halt.\nFor example, shown below is a transition diagram for a Turing machine\nthat makes a copy of a string of a\u2019s and b\u2019s. To use this machine, you\nwould write a string of a\u2019s and b\u2019s on its tape, place the machine on the\nfirst character of the string, and start the machine in its start state, q .\n0\nWhen the machine halts, there will be two copies of the string on the tape,\nseparatedbyablank. Themachinewillbepositionedonthefirstcharacter\noftheleftmostcopyofthestring. Notethatthismachineusesc\u2019sandd\u2019sin\naddition to a\u2019s and b\u2019s. While it is copying the input string, it temporarily\nchanges the a\u2019s and b\u2019s that it has copied to c\u2019s and d\u2019s, respectively. In\nthis way it can keep track of which characters it has already copied. After\nthe string has been copied, the machine changes the c\u2019s and d\u2019s back to a\u2019s\nand b\u2019s before halting. 5.1. TURING MACHINES 231\n(a,a,R)\n(a,a,R)\nq (#,#,R) q (#,a,L)\n1 2\n(a,c,R)\n(b,b,R) (b,b,R)\n(a,a,L) (a,a,L)\nq\n0 (c,c,R)\n(d,d,R) q q\n(#,#,L) 6 (#,#,L) 5\n(b,b,L) (b,b,L)\n(b,d,R)\nq\n7 (a,a,R)\n(a,a,R)\n(c,a,L) (d,b,L)\nq (#,#,R) q (#,b,L)\n(#,#,R) 3 4\n(b,b,R) (b,b,R)\nh\nIn this machine, state q checks whether the next character is an a, a\n0\nb, or a # (indicating the end of the string). States q and q add an a to\n1 2\nthe end of the new string, and states q and q do the same thing with a\n3 4\nb. States q and q return the machine to the next character in the input\n5 6\nstring. When the end of the input string is reached, state q will move the\n7\nmachine back to the start of the input string, changing c\u2019s and d\u2019s back to\na\u2019s and b\u2019s as it goes. Finally, when the machine hits the # that precedes\nthe input string, it moves to the right and halts. This leave it back at the\nfirstcharacteroftheinputstring. Itwouldbeagoodideatoworkthrough\nthe execution of this machine for a few sample input strings. You should\nalso check that it works even for an input string of length zero.\nOurprimaryinterestinTuringmachinesisaslanguageprocessors. Sup-\npose that w is a string over an alphabet \u03a3. We will assume that \u03a3 does\nnot contain the blank symbol. We can use w as input to a Turing ma-\nchine M = (Q,\u039b,q ,\u03b4) provided that \u03a3 \u039b. To use w as input for M,\n0\n\u2286\nwe will write w on M\u2019s tape and assume that the remainder of the tape\nis blank. We place the machine on the cell containing the first character\nof the string, except that if w = \u03b5 then we simply place the machine on a\ncompletely blank tape. Then we start the machine in its initial state, q ,\n0\nand see what computation it performs. We refer to this setup as \u201crunning\nM with input w.\u201d 232 CHAPTER 5. TURING MACHINES AND COMPUTABILITY\nWhenM isrunwithinputw,itispossiblethatitwilljustkeeprunning\nforeverwithouthalting. Inthatcase,itdoesn\u2019tmakesensetoaskaboutthe\noutput of the computation. Suppose however that M does halt on input\nw. Suppose, furthermore, thatwhenM halts, itstapeisblankexceptfora\nstring x of non-blank symbols, and that the machine is located on the first\ncharacter of x. In this case, we will say that \u201cM halts with output x.\u201d In\naddition,ifM haltswithanentirelyblanktape,wesaythat\u201cM haltswith\noutput \u03b5.\u201d Note that when we run M with input w, one of three things\ncan happen: (1) M might halt with some string as output; (1) M might\nfail to halt; or (3) M might halt in some configuration that doesn\u2019t count\nas outputting any string.\nThe fact that a Turing machine can produce an output value allows\nus for the first time to deal with computation of functions. A function\nf: A B takes an input value in the set A and produces an output value\n\u2192\nin the set B. If the sets are sets of strings, we can now ask whether the\nvalues of the function can be computed by a Turing machine. That is, is\nthere a Turing machine M such that, given any string w in the domain of\nf as input, M will compute as its output the string f(w). If this is that\ncase, then we say that f is a Turing-computable function.\nDefinition 5.2. Suppose that \u03a3 and \u0393 are alphabets that do not contain\n# and that f is a function from \u03a3 to \u0393 . We say that f is Turing-\n\u2217 \u2217\ncomputable if there is a Turing machine M = (Q,\u039b,q ,\u03b4) such that\n0\n\u03a3 \u039b and \u0393 \u039b and for each string w \u03a3 , when M is run with input\n\u2217\n\u2286 \u2286 \u2208\nw, it halts with output f(w). In this case, we say that M computes the\nfunction f.\nFor example, let \u03a3 = a and define f: \u03a3 \u03a3 by f(an) = a2n, for\n\u2217 \u2217\n{ } \u2192\nn N. Then f is Turing-computable since it is computed by this Turing\n\u2208\nmachine: 5.1. TURING MACHINES 233\n(c,c,R) (c,a,L)\n(#,#,R)\nq (#,#,L) q h\n0 2\n(a,c,L)\n(#,c,R)\n(c,c,L)\nq\n1\nWe can also use Turing machines to define \u201ccomputable languages.\u201d\nThere are actually two different notions of Turing-computability for lan-\nguages. One is based on the idea of Turing-computability for functions.\nSuppose that \u03a3 is an alphabet and that L \u03a3 . The characteristic\n\u2217\n\u2286\nfunction of L is the function \u03c7 : \u03a3 0,1 defined by the fact that\nL \u2217\n\u2192 { }\n\u03c7 (w)=1 if w L and \u03c7 (w)=0 if w L. Note that given the function\nL L\n\u2208 6\u2208\n\u03c7 , L can be obtained as the set L = w \u03a3 \u03c7 (w) = 1 . Given a\nL \u2217 L\n{ \u2208 | }\nlanguage L, we can ask whether the corresponding function \u03c7 is Turing-\nL\ncomputable. If so, then we can use a Turing machine to decide whether or\nnot a given string w is in L. Just run the machine with input w. It will\nhaltwithoutput\u03c7 (w). (Thatis,itwillhaltandwhenitdoesso,thetape\nL\nwill be blank except for a 0 or a 1, and the machine will be positioned on\nthe0or1.) Ifthemachinehaltswithoutput1,thenw L. Ifthemachine\n\u2208\nhalts with output 0, then w L.\n6\u2208\nDefinition 5.3. Let \u03a3 be an alphabet that does not contain # and let L\nbe a language over \u03a3. We say that L is Turing-decidable if there is a\nTuring machine M = (Q,\u039b,q ,\u03b4) such that \u03a3 \u039b, 0,1 \u039b, and for\n0\n\u2286 { } \u2286\neach w \u03a3 , when M is run with input w, it halts with output \u03c7 (w).\n\u2217 L\n\u2208\n(That is, it halts with output 0 or 1, and the output is 0 if w L and is 1\n6\u2208\nif w L.) In this case, we say that M decides the language L.\n\u2208\nThe second notion of computability for languages is based on the inter-\nesting fact that it is possible for a Turing machine to run forever, without\never halting. Whenever we run a Turing machine M with input w, we can\naskthequestion,willM everhaltorwillitrunforever? IfM haltsoninput\nw, we will say that M \u201caccepts\u201d w. We can then look at all the strings\nover a given alphabet that are accepted by a given Turing machine. This\nleads to the notion of Turing-acceptable languages. 234 CHAPTER 5. TURING MACHINES AND COMPUTABILITY\nDefinition 5.4. Let \u03a3 be an alphabet that does not contain #, and let L\nbe a language over \u03a3. We say that L is Turing-acceptable if there is a\nTuring machine M = (Q,\u039b,q ,\u03b4) such that \u03a3 \u039b, and for each w \u03a3 ,\n0 \u2217\n\u2286 \u2208\nM halts on input w if and only if w L. In this case, we say that M\n\u2208\naccepts the language L.\nItshouldbeclearthatanyTuring-decidablelanguageisTuring-acceptable.\nInfact,ifLisalanguageoveranalphabet\u03a3,andifM isaTuringmachine\nthat decides L, then it is easy to modify M to produce a Turing machine\nthat accepts L. At the point where M enters the halt state with output 0,\nthe new machine should enter a new state in which it simply moves to the\nright forever, without ever halting. Given an input w \u03a3 , the modified\n\u2217\n\u2208\nmachine will halt if and only if M halts with output 1, that is, if and only\nif w L.\n\u2208\nExercises\n1. Let\u03a3= a . DrawatransitiondiagramforaTuringmachinethatcomputes\nthe funct{ ion} f: \u03a3\u2217 \u03a3\u2217 where f(an) = a3n, for n N. Draw a transition\n\u2192 \u2208\ndiagramforaTuringmachinethatcomputesthefunctionf: \u03a3\u2217 \u03a3\u2217 where\nf(an)=a3n+1, for n N. \u2192\n\u2208\n2. Let \u03a3 = a,b . Draw a transition diagram for a Turing machine that com-\nputes the{ func} tion f: \u03a3\u2217 \u03a3\u2217 where f(w)=wR.\n\u2192\n3. Supposethat\u03a3,\u0393,and\u039earealphabetsandthatf: \u03a3\u2217 \u0393\u2217andg: \u0393\u2217 \u039e\u2217\n\u2192 \u2192\nare Turing-computable functions. Show that g f is Turing-computable.\n\u25e6\n4. We have defined computability for functions f: \u03a3\u2217 \u0393\u2217, where \u03a3 and \u0393\n\u2192\nare alphabets. How could Turing machines be used to define computable\nfunctions from N to N? (Hint: Consider the alphabet \u03a3= a .)\n{ }\n5. Let\u03a3beanalphabetandletLbealanguageover\u03a3. ShowthatLisTuring-\ndecidable if and only if its complement, L, is Turing-decidable.\n6. Draw a transition diagram for a Turing machine which decides the language\nanbn n N . (Hint: Change the a\u2019s and b\u2019s to $\u2019s in pairs.) Explain\n{ | \u2208 }\nin general terms how to make a Turing machine that decides the language\nanbncn n N .\n{ | \u2208 }\n7. Draw a transition diagram for a Turing machine which decides the language\nanbm n>0 and m is a multiple of n . (Hint: Erase n b\u2019s at a time.)\n{ | }\n8. Basedonyouranswertothepreviousproblemandthecopyingmachinepre-\nsentedinthissection,describeingeneraltermshowyouwouldbuildaTuring\nmachine to decide the language ap p is a prime number .\n{ | }\n9. Let g: a \u2217 0,1 \u2217 be the function such that for each n N, g(an) is\n{ } \u2192 { } \u2208\nthe representation of n as a binary number. Draw a transition diagram for a\nTuring machine that computes g. 5.2. COMPUTABILITY 235\n5.2 Computability\nAt this point, it would be useful to look at increasingly complex Turing\nmachines, which compute increasingly complex functions and languages.\nAlthough Turing machines are very simple devices, it turns out that they\ncan perform very sophisticated computations. In fact, any computation\nthat can be carried out by a modern digital computer\u2014even one with an\nunlimited amount of memory\u2014can be carried out by a Turing machine.\nAlthough it is not something that can be proved, it is widely believed that\nanything that can reasonably be called \u201ccomputation\u201d can be done by a\nTuring machine. This claim is known as the Church-Turing Thesis.\nWe do not have time to look at enough examples to convince you that\nTuring machines are as powerful as computers, but the proof reduces to\nthe fact that computers are actually fairly simple in their basic operation.\nEverything that a computer does comes down to copying data from one\nplace to another, making simple comparisons between two pieces of data,\nand performing some basic arithmetic operations. It\u2019s possible for Turing\nmachines to do all these things. In fact, it\u2019s possible to build a Turing ma-\nchine to simulate the step-by-step operation of a given computer. Doing so\nprovesthattheTuringmachinecandoanycomputationthatthecomputer\ncould do, although it will, of course, work much, much more slowly.\nWecan,however,lookbrieflyatsomeothermodelsofcomputationand\nseehowtheycomparewithTuringmachines. Forexample,therearevarious\nwaysinwhichwemighttrytoincreasethepowerofaTuringmachine. For\nexample, consider a two-tape Turing machine that has two tapes, with\na read\/write head on each tape. In each step of its computation, a two-\ntapeTuringmachinereadsthesymbolsunderitsread\/writeheadsonboth\ntapes. Based on these symbols and on its current state, it can write a new\nsymbol onto each tape, independently move the read\/write head on each\ntape one cell to the left or right, and change state.\nIt might seem that with two tapes available, two-tape Turing machines\nmightbeabletodocomputationsthatareimpossibleforordinaryone-tape\nmachines. Infact,though,thisisnotthecase. Thereason,again,issimula-\ntion: Givenanytwo-tapeTuringmachine, itispossibletobuildaone-tape\nTuring machine that simulates the step-by-step computation of the two-\ntape machine. Let M be a two-tape Turing machine. To simulate M with\naone-tapemachine,K,wemuststorethecontentsofbothofM\u2019stapeson\nonetape,andwemustkeeptrackofthepositionsofbothofM\u2019sread\/write\nheads. Let @ and $ be symbols that are not in the alphabet of M. The @\nwill be used to mark the position of a read\/write head, and the $ will be 236 CHAPTER 5. TURING MACHINES AND COMPUTABILITY\nusedtodelimitthepartsofK\u2019stapethatrepresentthetwotapesofM. For\nexample, suppose that one of M\u2019s tapes contains the symbols \u201cabb##cca\u201d\nwith the read\/write head on the first b, and that the other tape contains\n\u201c01#111#001\u201d with the read\/write head on the final 1. This configuration\nwould be represented on K\u2019s tape as \u201c$a@bb##cca$01#111#00@1$\u201d. To\nsimulateonestepofM\u2019scomputation,K mustscanitsentiretape,looking\nfor the @\u2019s and noting the symbol to the right of each @. Based on this\ninformation, K can update its tape and its own state to reflect M\u2019s new\nconfiguration after one step of computation. Obviously, K will take more\nsteps than M and it will operate much more slowly, but this argument\nmakes it clear that one-tape Turing machines can do anything that can be\ndone by two-tape machines.\nWe needn\u2019t stop there. We can imagine n-tape Turing machines, for\nn > 2. We might allow a Turing machine to have multiple read\/write\nheads that move independently on each tape. We could even allow two\nor three-dimensional tapes. None of this makes any difference as far as\ncomputational power goes, since each type of Turing machine can simulate\nany of the other types.3\nWe have used Turing machines to define Turing-acceptable languages\nandTuring-decidablelanguages. Thedefinitionsseemtodependverymuch\non the peculiarities of Turing machines. But the same classes of languages\ncan be defined in other ways. For example, we could use programs running\non an idealized computer, with an unlimited amount of memory, to accept\nordecidelanguages. Orwecouldusen-tapeTuringmachines. Theresulting\nclassesoflanguageswouldbeexactlythesameastheTuring-acceptableand\nTuring-decidable languages.\nWecouldlookatotherwaysofspecifyinglanguages\u201ccomputationally.\u201d\nOne of the most natural is to imagine a Turing machine or computer pro-\ngram that runs forever and outputs an infinite list of strings over some\nalphabet \u03a3. In the case of Turing machines, it\u2019s convenient to think of\na two-tape Turing machine that lists the strings on its second tape. The\nstrings in the list form a language over \u03a3. A language that can be listed\nin this way is said to be recursively enumerable. Note that we make no\nassumption that the strings must be listed in any particular order, and we\n3Wecanalsodefinenon-deterministicTuringmachinesthatcanhaveseveralpossi-\nbleactionsateachstep. Non-deterministicTuringmachinescannotbeusedtocompute\nfunctions,sinceafunctioncanhaveonlyonepossibleoutputforanygiveninput. How-\never, they can be used to accept languages. We say that a non-deterministic Turing\nmachineacceptsalanguageLisitispossible forthemachinetohaltoninputw ifand\nonly if w \u2208 L. The class of languages accepted by non-deterministic Turing machines\nis the same as the class of languages accepted by deterministic Turing machines. So,\nnon-determinismdoesnotaddanycomputationalpower. 5.2. COMPUTABILITY 237\nallowthesamestringtoappearintheoutputanynumberoftimes. Clearly,\narecursivelyenumerablelanguage is\u201ccomputable\u201d insomesense. Perhaps\nwe have found a new type of computable language. But no\u2014it turns out\nthat we have just found another way of describing the Turing-acceptable\nlanguages. The following theorem makes this fact official and adds one\nmore way of describing the same class of languages:\nTheorem 5.1. Let \u03a3 be an alphabet and let L be a language over \u03a3. Then\nthe following are equivalent:\n1. There is a Turing machine that accepts L.\n2. There is a two-tape Turing machine that runs forever, making a\nlist of strings on its second tape, such that a string w is in the list\nif and only if w L.\n\u2208\n3. There is a Turing-computable function f: a \u03a3 such that L\n\u2217 \u2217\n{ } \u2192\nis the range of the function f.\nWhile I will not give a complete, formal proof of this theorem, it\u2019s not\ntoohardtoseewhyitistrue. Consideralanguagethatsatisfiesproperty3.\nWecanusethefactthatListherangeofaTuring-computablefunction,f,\ntobuildatwo-tapeTuringmachinethatlistsL. Themachinewillconsider\neachofthestringsan,forn N,inturn,anditwillcomputef(an)foreach\n\u2208\nn. Once the value of f(an) has been computed, it can be copied onto the\nmachine\u2019s second tape, and the machine can move on to do the same with\nan+1. This machine writes all the elements of L (the range of f) onto its\nsecond tape, so L satisfies property 2. Conversely, suppose that there is a\ntwo-tape Turing machine, M, that lists L. Define a function g: a \u03a3\n\u2217 \u2217\n{ } \u2192\nsuch that for n N, g(an) is the (n+1)th item in the list produced by\n\u2208\nM. Then the range of g is L, and g is Turing-computable since it can be\ncomputed as follows: On input an, simulate the computation of M until it\nhasproducedn+1strings, thenhalt, givingthe(n+1)th stringasoutput.\nThis shows that property 2 implies property 3, so these properties are in\nfact equivalent.\nWe can also check that property 2 is equivalent to property 1. Suppose\nthat L satisfies property 2. Consider a two-tape Turing machine, T, that\nliststheelementsofL. WemustbuildaTuringmachine,M,whichaccepts\nL. We do this as follows: Given an input w \u03a3 , M will simulate the\n\u2217\n\u2208\ncomputation of T. Every time the simulated T produces a string in the\nlist, M compares that string to w. If they are the same, M halts. If\nw L, eventually it will be produced by T, so M will eventually halt. If\n\u2208\nw L,thenitwillneverturnupinthelistproducedbyT,soM willnever\n6\u2208\nhalt. Thus, M accepts the language L. This shows that property 2 implies\nproperty 1. 238 CHAPTER 5. TURING MACHINES AND COMPUTABILITY\nThe fact that property 1 implies property 2 is somewhat harder to see.\nFirst, we note that it is possible for a Turing machine to generate every\npossible string in \u03a3 , one-by-one, in some definite order (such as order of\n\u2217\nincreasing length, with something like alphabetical order for strings of the\nsame length). Now, suppose that L is Turing-acceptable and that M is\na Turing machine that accepts L. We need a two-tape Turing machine,\nT that makes a list of all the elements of L. Unfortunately, the following\nidea does not work: Generate each of the elements in \u03a3 in turn, and see\n\u2217\nwhether M accepts it. If so, then add it to the list on the second tape. It\nlookslikewehaveamachinethatlistsalltheelementsofL. Theproblemis\nthat the only way for T to \u201csee whether M accepts\u201d a string is to simulate\nthe computation of M. Unfortunately, assoon aswe try this forany string\nw that is not in L, the computation never ends! T will get stuck in the\nsimulation and will never even move on to the next string. To avoid this\nproblem, T must simulate multiple computations of M at the same time.\nT can keep track of these computations in different regions of its first tape\n(separated by $\u2019s). Let the list of all strings in \u03a3 be x , x , x , .... Then\n\u2217 1 2 3\nT should operate as follows:\n1. Set up the simulation of M on input x , and simulate one step of\n1\nthe computation for x\n1\n2. Set up the simulation of M on input x , and simulate one step of\n2\nthe computation for x and one step of the computation for x .\n1 2\n3. Set up the simulation of M on input x , and simulate one step of\n3\neach of the computations, for x , x , and x .\n1 2 3\n...\nn. Set up the simulation of M on input x , and simulate one step of\nn\neach of the computations, for x , x , ..., x .\n1 2 n\nand so on. Each time one of the computations halts, T should write the\ncorrespondingx ontoitssecondtape. Overthecourseoftime,T simulates\ni\nthe computation of M for each input w \u03a3 for an arbitrary number of\n\u2217\n\u2208\nsteps. If w L, the simulated computation will eventually end and w will\n\u2208\nappearonT\u2019ssecondtape. Ontheotherhand,ifw L,thenthesimulated\n6\u2208\ncomputationwillneverend, sow willnotappearinthelist. Soweseethat\nT does in fact make a list of all the elements, and only the elements of L.\nThis completes an outline of the proof of the theorem.\nNext, we compare Turing machines to a completely different method\nof specifying languages: general grammars. Suppose G = (V,\u03a3,P,S) is a\ngeneral grammar and that L is the language generated by G. Then there\nis a Turing machine, M, that accepts the same language, L. The alphabet 5.2. COMPUTABILITY 239\nfor M will be V \u03a3 $,# , where $ is a symbol that is not in V \u03a3. (We\n\u222a \u222a{ } \u222a\nalso assume that # is not in V \u03a3.) Suppose that M is started with input\n\u222a\nw, where w \u03a3 . We have to design M so that it will halt if and only if\n\u2217\n\u2208\nw L. The idea is to have M find each string that can be derived from\n\u2208\nthe start symbol S. The strings will be written to M\u2019s tape and separated\nby $\u2019s. M can begin by writing the start symbol, S, on its tape, separated\nfrom w by a $. Then it repeats the following process indefinitely: For each\nstring on the tape and for each production rule, x y, of G, search the\n\u2212\u2192\nstring for occurrences of x. When one is found, add a $ to the end of the\ntape and copy the string to the end of the tape, replacing the occurrence\nofxbyy. Thenewstringrepresentstheresultsofapplyingtheproduction\nrulex y tothestring. EachtimeM producesanewstring,itcompares\n\u2212\u2192\nthat string to w. If they are equal, then M halts. If w is in fact in L, then\neventuallyM willproducethestringwandwillhalt. Conversely,ifwisnot\nin L, then M will go on producing strings forever without ever finding w,\nso M will never halt. This shows that, in fact, the language L is accepted\nby M.\nConversely,supposethatLisalanguageoveranalphabet\u03a3,andthatL\nisTuring-acceptable. ThenitispossibletofindagrammarGthatgenerates\nL. Todothis,it\u2019sconvenienttousethefactthat,asdiscussedabove,there\nisaTuring-computablefunctionf: a \u03a3suchthatListherangeoff.\n\u2217\n{ } \u2192\nLet M = (Q,\u039b,q ,\u03b4) be a Turing machine that computes the function f.\n0\nWe can build a grammar, G, that imitates the computations performed by\nM. The idea is that most of the production rules of G will imitate steps\nin the computation of M. Some additional rules are added to get things\nstarted, to clean up, and to otherwise bridge the conceptual gap between\ngrammars and Turing machines.\nThe terminal symbols of G will be the symbols from the alphabet, \u03a3.\nFor the non-terminal symbols, we use: the states of M, every member of \u039b\nthat is not in \u03a3, two special symbols < and >, and two additional symbols\nS and A. (We can assume that all these symbols are distinct.) S will be\nthestartsymbolofG. Asforproductionrules, webeginwiththefollowing\nthree rules:\nS <q A>\n0\n\u2212\u2192\nA aA\n\u2212\u2192\nA \u03b5\n\u2212\u2192\nThese rules make it possible to produce any string of the form <q an>.\n0\nThisistheonlyrolethatS andAplayinthegrammar. Oncewe\u2019vegotten\nridofS andA,stringsoftheremainingterminalandnon-terminalsymbols 240 CHAPTER 5. TURING MACHINES AND COMPUTABILITY\nrepresent configurations of the Turing machine M. The string will contain\nexactlyoneofthestatesofM (whichis,remember,oneofthenon-terminal\nsymbolsofG). ThistellsuswhichstateM isin. Thepositionofthestate-\nsymbol tells us where M is positioned on the tape: the state-symbol is\nlocated in the string to the left of the symbol on which M is positioned.\nAndthespecialsymbols<and>justrepresentthebeginningandtheend\nof a portion of the tape of M. So, the initial string <q an> represents a\n0\nconfiguration in which M is in its start state, and is positioned on the first\nainastringofna\u2019s. ThisisthestartingconfigurationofM whenitisrun\nwith input an.\nNow, we need some production rules that will allow the grammar to\nsimulate the computations performed by M. For each state q and each\ni\nsymbol \u03c3 \u039b, we need a production rule that imitates the transition rule\n\u2208\n\u03b4(q ,\u03c3) = (\u03c4,d,q ). If d = R, that is if the machine moves to the right,\ni j\nthen all we need is the rule\nq \u03c3 \u03c4q\ni j\n\u2212\u2192\nThis represents that fact that M converts the \u03c3 to a \u03c4, moves to the right,\nand changes to state q . If d=L, that is if the machine moves to the left,\nj\nthen we will need several rules\u2014one rule for each \u03bb \u039b, namely\n\u2208\n\u03bbq \u03c3 q \u03bb\u03c4\ni j\n\u2212\u2192\nThis rule says that M changes the \u03c3 to a \u03c4, moves left, and changes to\nstate q . The \u03bb doesn\u2019t affect the application of the rule, but is necessary\nj\nto represent the fact that M moves left.\nEachapplicationofoneoftheserulesrepresentsonestepinthecompu-\ntation of M. There is one remaining requirement for correctly simulating\nM. Since M\u2019s tape contains an infinite number of cells and we are only\nrepresentingafiniteportionofthattape,weneedawaytoaddandremove\n#\u2019sattheendsofthestring. Wecanusethefollowingfourrulestodothis:\n< <#\n\u2212\u2192\n<# <\n\u2212\u2192\n> #>\n\u2212\u2192\n#> >\n\u2212\u2192\nThese rules allow blank symbols to appear at the ends of the string when\nthey are needed to continue the computation, and to disappear from the\nends of the string whenever we like. 5.2. COMPUTABILITY 241\nNow, suppose that w is some element of L. Then w = f(an) for some\nn N. We know that on input an, M halts with output w. If we translate\n\u2208\nthe computation of M into the corresponding sequence of production rules\nin G, we see that for the grammar G, <q an> = <hw>, where h is the\n0 \u2217\n\u21d2\nhalt state of M. Since we already know that S = <q an>, for every\n\u2217 0\n\u21d2\nn N, we see that in fact S = <hw> for each w L. We almost have\n\u2217\n\u2208 \u21d2 \u2208\nit! We want to show that S = w. If we can just get rid of the <, the h,\n\u2217\n\u21d2\nand the >, we will have that <hw> = w and we can then deduce that\n\u2217\n\u21d2\nS = w for each w L, as desired. We can do this by adding just a few\n\u2217\n\u21d2 \u2208\nmore rules to G. We want to let the h eliminate the <, move through the\nw, and then eliminate the > along with itself. We need the rules\n<h h\n\u2212\u2192\nh> \u03b5\n\u2212\u2192\nand, for each \u03c3 \u03a3,\n\u2208\nh\u03c3 \u03c3h\n\u2212\u2192\nWe have constructed G so that it generates every string in L. It is not\ndifficult to see that the strings in L are in fact the only strings that are\ngenerated by G. That is, L is precisely L(G).\nWehavenowshown,somewhatinformally,thatalanguageLisTuring-\nacceptable if and only if there is a grammar G that generates L. Even\nthough Turing machines and grammars are very different things, they are\nequivalent in terms of their ability to describe languages. We state this as\na theorem:\nTheorem5.2. AlanguageLisTuringacceptable(equivalently, recursively\nenumerable) if and only if there is a general grammar that generates L.\nIn this section, we have been talking mostly about recursively enumer-\nable languages (also known as the Turing-acceptable languages). What\nabout the Turing-decidable languages? We already know that if a lan-\nguage L is Turing-decidable, then it is Turing-acceptable. The converse is\nnot true (although we won\u2019t be able to prove this until the next section).\nHowever, suppose that L is a language over the alphabet \u03a3 and that both\nL and its complement, L = \u03a3 r L, are Turing-acceptable. Then L is\n\u2217\nTuring-decidable.\nFor suppose that M is a Turing machine that accepts the language L\nand that M is a Turing machine that accepts L. We must show that L\n\u2032\nis Turing-decidable. That is, we have to build a Turing machine T that 242 CHAPTER 5. TURING MACHINES AND COMPUTABILITY\ndecides L. For each w \u03a3 , when T is run with input w, it should halt\n\u2217\n\u2208\nwith output 1 if w L and with output 0 if w L. To do this, T will\n\u2208 6\u2208\nsimulate the computation of both M and M on input w. (It will simulate\n\u2032\none step in the computation of M, then one step in the computation of\nM , then one step of M, then one step of M , and so on.) If and when the\n\u2032 \u2032\nsimulatedcomputationofM halts,thenT willhaltwithoutput1; sinceM\nacceptsL, thiswillhappenifandonlyifw L. Ifandwhenthesimulated\n\u2208\ncomputation of M halts, then T will halt with output 0; since M accepts\n\u2032\nL, this will happen if and only if w L. So, for any w \u03a3 , T halts with\n\u2217\n6\u2208 \u2208\nthe desired output. This means that T does in fact decide the language L.\nIt is easy to prove the converse, and the proof is left as an exercise.\nSo we see that a language is Turing-decidable if and only if both it and\nits complement are Turing-acceptable. Since Turing-acceptability can be\ndefined using other forms of computation besides Turing machines, so can\nTuring-decidability. For example, a language is Turing-decidable if and\nonly if both it and its complement can be generated by general gram-\nmars. We introduced the term \u201crecursively enumerable\u201d as a synonym for\nTuring-acceptable, to get away from the association with a particular form\nofcomputation. Similarly, wedefinetheterm\u201crecursive\u201dasasynonymfor\nTuring-decidable. Thatis,alanguageLissaidtoberecursive ifandonly\nif it is Turing-decidable. We then have the theorem:\nTheorem5.3. Let\u03a3beanalphabetandletLbealanguageover\u03a3. ThenL\nisrecursiveifandonlyifbothLanditscomplement, \u03a3 rL, arerecursively\n\u2217\nenumerable.\nExercises\n1. The language L = am m > 0 is the range of the function f(an) = an+1.\n{ | }\nDesignaTuringmachinethatcomputesthisfunction,andfindthegrammar\nthatgeneratesthelanguageLbyimitatingthecomputationofthatmachine.\n2. Complete the proof of Theorem 5.3 by proving the following: If L is a recur-\nsive language over an alphabet \u03a3, then both L and \u03a3\u2217 rL are recursively\nenumerable.\n3. Show that a language L over an alphabet \u03a3 is recursive if and only if there\nare grammars G and H such that the language generated by G is L and the\nlanguage generated by H is \u03a3\u2217rL.\n4. This section discusses recursive languages and recursively enumerable lan-\nguages. How could one define recursive subsets of N and recursively enumer-\nable subsets of N?\n5. Give an informal argument to show that a subset X N is recursive if and\n\u2286\nonly if there is a computer program that prints out the elements of X in\nincreasing order. 5.3. THE LIMITS OF COMPUTATION 243\n5.3 The Limits of Computation\nRecursively enumerable languages are languages that can be defined by\ncomputation. Wehaveseenthattherearemanydifferentmodelsofcompu-\ntation\u2014Turing machines, two-tape Turing machines, grammars, computer\nprograms\u2014but they all lead to the same class of languages. In fact, every\ncomputational method for specifying languages that has ever been devel-\noped produces only recursively enumerable languages. There is something\nabouttheselanguages\u2014somepatternorproperty\u2014thatmakesthem\u201ccom-\nputable,\u201d anditissomeintrinsicpropertyofthelanguagesthemselves, not\nsome peculiarity of any given model of computation.\nThis is especially interesting since most languages are not recursively\nenumerable. Given an alphabet \u03a3, there are uncountably many languages\nover \u03a3, but only countably many of them are recursively enumerable. The\nrest\u2014the vast majority\u2014are not recursively enumerable. What can we\nsay about all these non-recursively-enumerable languages? If the language\nL is not recursively enumerable, then there is no algorithm for listing the\nmembersofL. ItmightbepossibletodefineLbyspecifyingsomeproperty\nthat all its members satisfy, but that property can\u2019t be computable. That\nis,therecanbenocomputerprogramorTuringmachinethattestswhether\na given string w has the property, since if there were, then we could write\na program that lists the members of L.\nSo, even though almost every language is non-recursively-enumerable,\nit\u2019sdifficulttofindaparticularlanguagethatisnotrecursivelyenumerable.\nNevertheless, in this section we will find one such language. At that same\ntime, we will find an example of a language that is recursively enumerable\nbut not recursive. And we will discover some interesting limitations to the\npower of computation.\nTheexamplesthatwewilllookatinthissectioninvolveTuringmachines\nthat work with other Turing machines as data. For this to work, we need\na symbolic representation of Turing machines\u2014a representation that can\nbe written on the tape of another Turing machine. This will let us create\ntwo machines: First, a Turing machine that can generate Turing machines\non demand by writing their symbolic representations on its tape. We will\ndesign a Turing machine G to do this. And second, a Turing machine that\ncan simulate the computation of other Turingmachines whose descriptions\nare written on its tape.\nIn order to do all this, we must put some limitations on the states\nand alphabetic symbols that can be used in the Turing machines that we\nconsider. Clearly, given any Turing machine, we can change the names of 244 CHAPTER 5. TURING MACHINES AND COMPUTABILITY\nthe states without changing the behavior of the machine. So, without any\nloss of generality, we can assume that all states have names chosen from\nthe list: h, q, q , q , q , q , .... We assume that h is the halt state and q\n\u2032 \u2032\u2032 \u2032\u2032\u2032 \u2032\u2032\u2032\u2032\nis the start state. Note that there is an infinite number of possible states,\nbut any given Turing machine will only use finitely many states from this\nlist.\nAs for the alphabets of the Turing machines, I want to look at Turing\nmachines whose alphabets include the symbols 0, 1, a, and of course #.\nThese are the symbols that the machines will use for input and output.\nThe alphabets can also include other symbols. We will assume that these\nauxiliary symbols are chosen from the list: a, a , a , a , .... Given a\n\u2032 \u2032\u2032 \u2032\u2032\u2032 \u2032\u2032\u2032\u2032\nTuringmachinewhosealphabetincludesthesymbols0,1,a,and#,wecan\nrename any other symbols in its alphabet using names from this list. This\nrenaming will not affect any of the behavior that we are interested in.\nNow suppose we have one of these standard Turing machines\u2014one\nwhose states are chosen from the list h, q, q , q , q , ..., whose start\n\u2032 \u2032\u2032 \u2032\u2032\u2032\nstate is q, and whose symbols are chosen from the list #, 0, 1, a, a, a ,\n\u2032 \u2032\u2032\na , .... Such a machine can be completely encoded as a string of sym-\n\u2032\u2032\u2032\nbols over the alphabet h,q,L,R,#,0,1,a, ,$ . A transition rule such as\n\u2032\n{ }\n\u03b4(q ,0) = (a ,L,q) can be encoded as a string q 0a Lq. To encode a\n\u2032\u2032 \u2032\u2032\u2032 \u2032\u2032 \u2032\u2032\u2032\ncomplete machine, simply encode each of its transition rules in this way\nand join them together in a string, separated by $\u2019s. We now have the\nsymbolic representation for Turing machines that we need.\nNote that a string over the alphabet h,q,L,R,#,0,1,a, ,$ might or\n\u2032\n{ }\nmight not encode a Turing machine. However, it is a simple matter to\ncheck whether such a string is the code for a Turing machine. We can\nimagine the following process: Generate all the strings over the alphabet\nh,q,L,R,#,0,1,a, ,$ . Check each string to see whether it encodes a\n\u2032\n{ }\nTuring machine. If so, add the string to an output list. In this way, we\ncan generate a list of all strings that encode standard Turing machines. In\neffect, the standard Turing machines, or at least their symbolic representa-\ntions, form a recursively enumerable set. Let T be the machine encoded\n0\nby the first string in this list of standard Turing machines; let T be the\n1\nmachine encoded by the second string; let T be the machine encoded by\n2\nthe third string; and so on. The list T , T , T , ..., includes every stan-\n0 1 2\ndard Turing machine. Furthermore, given n N, we can find the symbolic\n\u2208\nrepresentation for T by generating strings in the list until we have n+1\nn\nstrings. Furthermore\u2014and this is the essential point\u2014we can use a Turing\nmachine to do all these calculations. In fact, there is a Turing machine\nthat, when run with input an, will halt with the string representation of\nT written on its tape as output. The Turing machine that does this is G,\nn 5.3. THE LIMITS OF COMPUTATION 245\nthe first of the two machines that we need.\nThe second machine that we need will be called U. It is a so-called\nUniversal Turing Machine. The single Turing machine U can simulate\nthe computation of any standard Turing machine, T, on any input. Both\nthe symbolic representation of T and that of the input string are written\ntoU\u2019stape, separatedbyaspace. AsU simulatesthecomputationofT, it\nwill need some way to keep track of what state T is in and of the position\nof T on its (simulated) tape. It does this by writing the current state of\nT on its tape, following T\u2019s input string, and by adding a special symbol,\nsuchas@,totheinputstringtomarkT\u2019sposition. WhenU isfirststarted,\nit begins by adding the @ to the beginning of the input string and writing\na q after the string to represent the start state of T. It is then relatively\nstraightforward for U to simulate the computation of T. For each step in\nthe computation of T, it can determine the current state of T (which is\nrecorded on U\u2019s tape) and the symbol which T is currently reading (which\nis on U\u2019s tape, after the @). U searches the symbolic representation of\nT for the rule that tells T what to do in this situation. Using this rule,\nU can update its representation of T\u2019s state, position, and tape to reflect\nthe result of applying the rule. If the new state of T is the halt state,\nthen U also halts. Otherwise, it goes on to simulate the next step in T\u2019s\ncomputation. Note that whenU isgiven T andaninput stringw asinput,\nU will halt if and only if T halts on input w. (Obviously, this is a very\ninefficient simulation, but we are not concerned with efficiency here.)\nSo, we have our two machines, G and U. After all this setup, we are\nfinallyinapositiontolookatthemajortheoremthatwehavebeenworking\ntowards.\nTheorem 5.4. Let T , T , T , ..., be the standard Turing machines, as\n0 1 2\ndescribed above. Let K be the language over the alphabet a defined by\n{ }\nK = an T halts when run with input an .\nn\n{ | }\nThen K is a recursively enumerable language, but K is not recursive. The\ncomplement\nK = an T does not halt when run with input an .\nn\n{ | }\nis a language that is not recursively enumerable.\nFirst note that if both K and K were recursively enumerable, then K\nwouldberecursive,byTheorem5.3. So,onceweshowthatK isrecursively\nenumerable but not recursive, it follows immediately that K cannot be 246 CHAPTER 5. TURING MACHINES AND COMPUTABILITY\nrecursively enumerable. That is, the second part of the theorem follows\nfrom the first.\nTo show that K is recursively enumerable, it suffices to find a Turing\nmachine, M, that accepts K. That is, when run with input an, for n N,\n\u2208\nM should halt if and only if an K. We can build M from the Turing\n\u2208\nmachines G and U which were introduced above. When started with input\nan, M should proceed as follows: First copy the input. Run G on the\nfirst copy of an. This will produce a symbolic description of the Turing\nmachine T . Now run U to simulate the computation of T on input an.\nn n\nThis simulation will end if and only if T halts when run with input an,\nn\nthat is, if and only if an K. The Turing machine M that performs the\n\u2208\ncomputation we have described accepts the language K. This proves that\nK is recursively enumerable.\nToshowthatK isnotrecursive,weneedtoshowthatthereisno Turing\nmachinethatdecidesK. LetH beanyTuringmachine. Wemustshowthat\nnomatterwhatH does,itdoesnotdecidethelanguageK. Wemustdothis\nwithout knowing anything more about H that the fact that is it a Turing\nmachine. TosaythatH decidesK wouldmeanthatforanyn N,whenH\n\u2208\nisrunwithinputan,H willhaltwithoutput1ifan K andwillhaltwith\n\u2208\noutput 0 if an K. To show that H does not decide K we need to show\n6\u2208\nthat there is some n N such that when H is run with input an, H either\n\u2208\nfails to halt or else halts but gives the wrong output. Note in particular\nthatweonlyneedtofindone nforwhichH doesnotgivethecorrectresult.\nAs we try to find n, we have nothing much to work with but H itself.\nTo find n, we construct a Turing machine M that is a simple variation\nonH. WhenM isrunonanyinput,itduplicatesthebehaviorofH onthat\ninput until H halts (if it ever does). At that point, M should check H\u2019s\noutput. If H has halted with output 1, then M should go into an infinite\nloop,sothatM neverhaltsinthiscase. Otherwise,iftheoutputofH isnot\n1, then M should halt. Now, we can assume that M is one of the standard\nTuring machines, say M =T . (If M is not already one of these machines,\nn\nit is because it uses different names for its states and symbols. Renaming\nthe states and symbols will produce an equivalent machine with the same\nbehavior as M, and we can replace M with this standard machine.)\nWe now have a Turing machine T = M which has the following be-\nn\nhavior when it is run with input an (note that the n here is the same n as\nin T ): If H halts with output 1 on input an, then T will fail to halt on\nn n\ninput an. If H halts with output 0 on input an, then T fails to halt on\nn\ninput an. (What T might do in other cases is not relevant here.)\nn\nRemember that we are trying to show that H does not decide the lan-\nguage K. I claim that, in fact, H does not give the correct answer for 5.3. THE LIMITS OF COMPUTATION 247\nan. When H is run with input an, it is supposed to halt with output 1 if\nan K, and it is supposed to halt with output 0 if an K. Recall that\n\u2208 6\u2208\nan K if and only if T halts when run with input an.\nn\n\u2208\nSupposethatwerunH withinputan. IfH doesnothaltwithoutput0\nor1,thenithascertainlynotgiventhecorrectanswerforan. Now,suppose\nthat H halts with output 1 on input an. In this case, by the properties\nof T given above, we know that T does not halt on input an. But that\nn n\nmeans, by definition of K, that an K. By halting with output 1 in this\n6\u2208\ncase, H has given the wrong answer for an. Finally, suppose that H halts\nwith output 0 on input an. We then know that T halts on input an. But\nn\nthat means that an K. Again, by halting with output 0 in this case, H\n\u2208\nhas given the wrong answer for an. So, in no case will H give the correct\nanswerforan. ThismeansthatH doesnotdecidethelanguageK,because\nH givesanincorrectanswerwhenitisrunwiththeparticularinputan. H\ndoes not decide K, and since H was an arbitrary Turing machine, we see\nthat there is no Turing machine at all that decides the language K. Thus,\nK is not a recursive language, as the theorem claims.\nTo decide the language K would be to solve the following problem:\nGiven a Turing machine T , decide whether or not T will halt when it is\nn n\nrunwithinputan. ThisproblemiscalledtheHalting Problem. Wehave\nshown that there is no Turing machine that solves this problem. Given the\nequivalence of Turing machines and computer programs, we can also say\nthatthereisnocomputerprogramthatsolvesthehaltingproblem. Wesay\nthat the halting problem is computationally unsolvable.\nThehaltingproblemisjustoneofmanyproblemsthatcannotbesolved\nby Turing machines or computer programs. In fact, almost any interesting\nyes\/no question that can be asked about Turing machines or programs is\nin this class: Does this Turing machine halt for all possible inputs in \u03a3 ?\n\u2217\nGiven this input, will this program ever halt? Do these two programs (or\nTuring machines) have the same output for each possible input? Will this\nTuringmachineeverhaltifitisstartedonablanktape? Alltheseproblems\narecomputationallyunsolvableinthesensethatthereisnoTuringmachine\nor computer program that will answer them correctly in all cases. The\nexistenceofsuchproblemsisareallimitationonthepowerofcomputation. Index\n\u03b4 (q,w), 152 and logic circuits, 25\n\u2217\n\u03b5-transitions, 158 in predicate logic, 33\nn-ary relation, 126 in set theory, 92\nn (x), 141 bottom-up parsing, 199\n0\nbound variable, 32\naccepted, 151, 152, 159\naccepting, 150 calling a subroutine, 67\naddition, binary, 27 cardinality, 115, 119, 129\nalgebra, 10, 92, 135 Cartesian product, 105\nalphabet, 139 cell, 228\nambiguous grammar, 194 character classes, 147\nand (logical operator), 3, 88 characteristic function, 233\nAND gate, 19 Church-Turing Thesis, 235\nantisymmetric relation, 127 CNF, see conjunctive normal form\nargument, 38 combinatorial logic circuit, 21\nassociative operator, 4 combinatorics, 118\ncomplement, 90\nBackus-Naur Form, 188 composition, 104\nbalanced parentheses, 184 compound proposition, 4\nbase case, 61, 66, 69 computational unsolvability, 247\nbase-2 number, 98 concatenation, 140, 142\nbiconditional operator, 5 conclusion, 1, 38\nbijective, 107 conditional, 6\nbijective function, 115 conditional operator, 5\nbinary number, 26, 96 conjunction, 3\nbinary relation, 125 conjunctive normal form, 28\nbinary sort tree, 75 context-free grammar, 176, 177\nbinary tree, 71 context-free language, 179\nbit, 96 deterministic, 210\nbitwise logical operator, 98 context-sensitive grammar, 219\nBoole, George, 10 contradiction, 8, 57\nBoolean algebra, 10, 13 contrapositive, 7\n248 "}