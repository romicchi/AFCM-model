{"text":"Chapter 1\nIntroduction\n1.1 What this book is, and what it isn\u2019t\nThis book provides implementations of common and uncommon algorithms in\npseudocodewhichislanguageindependentandprovidesforeasyportingtomost\nimperative programming languages. It is not a definitive book on the theory of\ndata structures and algorithms.\nForthemostpartthisbookpresentsimplementationsdevisedbytheauthors\nthemselves based on the concepts by which the respective algorithms are based\nupon so it is more than possible that our implementations differ from those\nconsidered the norm.\nYou should use this book alongside another on the same subject, but one\nthat contains formal proofs of the algorithms in question. In this book we use\nthe abstract big Oh notation to depict the run time complexity of algorithms\nso that the book appeals to a larger audience.\n1.2 Assumed knowledge\nWe have written this book with few assumptions of the reader, but some have\nbeennecessaryinordertokeepthebookasconciseandapproachableaspossible.\nWe assume that the reader is familiar with the following:\n1. Big Oh notation\n2. An imperative programming language\n3. Object oriented concepts\n1.2.1 Big Oh notation\nForruntimecomplexityanalysisweusebigOhnotationextensivelysoitisvital\nthat you are familiar with the general concepts to determine which is the best\nalgorithm for you in certain scenarios. We have chosen to use big Oh notation\nfor a few reasons, the most important of which is that it provides an abstract\nmeasurement by which we can judge the performance of algorithms without\nusing mathematical proofs.\n1 CHAPTER 1. INTRODUCTION 2\nFigure 1.1: Algorithmic run time expansion\nFigure1.1showssomeoftheruntimestodemonstratehowimportantitisto\nchooseanefficientalgorithm. Forthesanityofourgraphwehaveomittedcubic\nO(n3), and exponential O(2n) run times. Cubic and exponential algorithms\nshouldonlyeverbeusedforverysmallproblems(ifever!);avoidthemiffeasibly\npossible.\nThe following list explains some of the most common big Oh notations:\nO(1) constant: theoperationdoesn\u2019tdependonthesizeofitsinput,e.g. adding\na node to the tail of a linked list where we always maintain a pointer to\nthe tail node.\nO(n) linear: the run time complexity is proportionate to the size of n.\nO(log n) logarithmic: normally associated with algorithms that break the problem\ninto smaller chunks per each invocation, e.g. searching a binary search\ntree.\nO(n log n) justnlogn: usuallyassociatedwithanalgorithmthatbreakstheproblem\nintosmallerchunkspereachinvocation,andthentakestheresultsofthese\nsmaller chunks and stitches them back together, e.g. quick sort.\nO(n2) quadratic: e.g. bubble sort.\nO(n3) cubic: very rare.\nO(2n) exponential: incredibly rare.\nIfyouencountereitherofthelattertwoitems(cubicandexponential)thisis\nreally a signal for you to review the design of your algorithm. While prototyp-\ning algorithm designs you may just have the intention of solving the problem\nirrespective of how fast it works. We would strongly advise that you always\nreview your algorithm design and optimise where possible\u2014particularly loops CHAPTER 1. INTRODUCTION 3\nand recursive calls\u2014so that you can get the most efficient run times for your\nalgorithms.\nThe biggest asset that big Oh notation gives us is that it allows us to es-\nsentially discard things like hardware. If you have two sorting algorithms, one\nwith a quadratic run time, and the other with a logarithmic run time then the\nlogarithmic algorithm will always be faster than the quadratic one when the\ndata set becomes suitably large. This applies even if the former is ran on a ma-\nchine that is far faster than the latter. Why? Because big Oh notation isolates\na key factor in algorithm analysis: growth. An algorithm with a quadratic run\ntime grows faster than one with a logarithmic run time. It is generally said at\nsome point as n \u2192 \u221e the logarithmic algorithm will become faster than the\nquadratic algorithm.\nBig Oh notation also acts as a communication tool. Picture the scene: you\nare having a meeting with some fellow developers within your product group.\nYouarediscussingprototypealgorithmsfornodediscoveryinmassivenetworks.\nSeveral minutes elapse after you and two others have discussed your respective\nalgorithms and how they work. Does this give you a good idea of how fast each\nrespective algorithm is? No. The result of such a discussion will tell you more\naboutthehighlevelalgorithmdesignratherthanitsefficiency. Replaythescene\nback in your head, but this time as well as talking about algorithm design each\nrespective developer states the asymptotic run time of their algorithm. Using\nthe latter approach you not only get a good general idea about the algorithm\ndesign, but also key efficiency data which allows you to make better choices\nwhen it comes to selecting an algorithm fit for purpose.\nSome readers may actually work in a product group where they are given\nbudgets per feature. Each feature holds with it a budget that represents its up-\npermosttimebound. Ifyousavesometimeinonefeatureitdoesn\u2019tnecessarily\ngive you a buffer for the remaining features. Imagine you are working on an\napplication, and you are in the team that is developing the routines that will\nessentially spin up everything that is required when the application is started.\nEverything is great until your boss comes in and tells you that the start up\ntime should not exceed n ms. The efficiency of every algorithm that is invoked\nduring start up in this example is absolutely key to a successful product. Even\nif you don\u2019t have these budgets you should still strive for optimal solutions.\nTaking a quantitative approach for many software development properties\nwill make you a far superior programmer - measuring one\u2019s work is critical to\nsuccess.\n1.2.2 Imperative programming language\nAll examples are given in a pseudo-imperative coding format and so the reader\nmust know the basics of some imperative mainstream programming language\nto port the examples effectively, we have written this book with the following\ntarget languages in mind:\n1. C++\n2. C#\n3. Java CHAPTER 1. INTRODUCTION 4\nThe reason that we are explicit in this requirement is simple\u2014all our imple-\nmentations are based on an imperative thinking style. If you are a functional\nprogrammeryouwillneedtoapplyvariousaspectsfromthefunctionalparadigm\nto produce efficient solutions with respect to your functional language whether\nit be Haskell, F#, OCaml, etc.\nTwo of the languages that we have listed (C# and Java) target virtual\nmachines which provide various things like security sand boxing, and memory\nmanagement via garbage collection algorithms. It is trivial to port our imple-\nmentations to these languages. When porting to C++ you must remember to\nuse pointers for certain things. For example, when we describe a linked list\nnode as having a reference to the next node, this description is in the context\nof a managed environment. In C++ you should interpret the reference as a\npointer to the next node and so on. For programmers who have a fair amount\nof experience with their respective language these subtleties will present no is-\nsue, which is why we really do emphasise that the reader must be comfortable\nwith at least one imperative language in order to successfully port the pseudo-\nimplementations in this book.\nIt is essential that the user is familiar with primitive imperative language\nconstructs before reading this book otherwise you will just get lost. Some algo-\nrithms presented in this book can be confusing to follow even for experienced\nprogrammers!\n1.2.3 Object oriented concepts\nFor the most part this book does not use features that are specific to any one\nlanguage. In particular, we never provide data structures or algorithms that\nwork on generic types\u2014this is in order to make the samples as easy to follow\nas possible. However, to appreciate the designs of our data structures you will\nneed to be familiar with the following object oriented (OO) concepts:\n1. Inheritance\n2. Encapsulation\n3. Polymorphism\nThisisespeciallyimportantifyouareplanningonlookingattheC#target\nthat we have implemented (more on that in \u00a71.7) which makes extensive use\nof the OO concepts listed above. As a final note it is also desirable that the\nreader is familiar with interfaces as the C# target uses interfaces throughout\nthe sorting algorithms.\n1.3 Pseudocode\nThroughout this book we use pseudocode to describe our solutions. For the\nmost part interpreting the pseudocode is trivial as it looks very much like a\nmore abstract C++, or C#, but there are a few things to point out:\n1. Pre-conditions should always be enforced\n2. Post-conditionsrepresenttheresultofapplyingalgorithmatodatastruc-\nture d CHAPTER 1. INTRODUCTION 5\n3. The type of parameters is inferred\n4. All primitive language constructs are explicitly begun and ended\nIf an algorithm has a return type it will often be presented in the post-\ncondition, but where the return type is sufficiently obvious it may be omitted\nfor the sake of brevity.\nMost algorithms in this book require parameters, and because we assign no\nexplicittypetothoseparametersthetypeisinferredfromthecontextsinwhich\nit is used, and the operations performed upon it. Additionally, the name of\nthe parameter usually acts as the biggest clue to its type. For instance n is a\npseudo-name for a number and so you can assume unless otherwise stated that\nn translates to an integer that has the same number of bits as a WORD on a\n32bitmachine,similarlyl isapseudo-nameforalistwherealistisaresizeable\narray (e.g. a vector).\nThelastmajorpointofreferenceisthatwealwaysexplicitlyendalanguage\nconstruct. For instance if we wish to close the scope of a for loop we will\nexplicitly state end for rather than leaving the interpretation of when scopes\nareclosedtothereader. Whileimplicitscopeclosureworkswellinsimplecode,\nin complex cases it can lead to ambiguity.\nThepseudocodestylethatweusewithinthisbookisratherstraightforward.\nAll algorithms start with a simple algorithm signature, e.g.\n1) algorithm AlgorithmName(arg1, arg2, ..., argN)\n2) ...\nn) end AlgorithmName\nImmediately after the algorithm signature we list any Pre or Post condi-\ntions.\n1) algorithm AlgorithmName(n)\n2) Pre: n is the value to compute the factorial of\n3) n\u22650\n4) Post: the factorial of n has been computed\n5) \/\/ ...\nn) end AlgorithmName\nThe example above describes an algorithm by the name of AlgorithmName,\nwhich takes a single numeric parameter n. The pre and post conditions follow\nthe algorithm signature; you should always enforce the pre-conditions of an\nalgorithm when porting them to your language of choice.\nNormallywhatislistedasapre-coniditioniscriticaltothealgorithmsopera-\ntion. Thismaycoverthingsliketheactualparameternotbeingnull,orthatthe\ncollection passed in must contain at least n items. The post-condition mainly\ndescribestheeffectofthealgorithmsoperation. Anexampleofapost-condition\nmight be \u201cThe list has been sorted in ascending order\u201d\nBecause everything we describe is language independent you will need to\nmake your own mind up on how to best handle pre-conditions. For example,\nin the C# target we have implemented, we consider non-conformance to pre-\nconditions to be exceptional cases. We provide a message in the exception to\ntell the caller why the algorithm has failed to execute normally. CHAPTER 1. INTRODUCTION 6\n1.4 Tips for working through the examples\nAs with most books you get out what you put in and so we recommend that in\norder to get the most out of this book you work through each algorithm with a\npen and paper to track things like variable names, recursive calls etc.\nThe best way to work through algorithms is to set up a table, and in that\ntablegiveeachvariableitsowncolumnandcontinuouslyupdatethesecolumns.\nThis will help you keep track of and visualise the mutations that are occurring\nthroughout the algorithm. Often while working through algorithms in such\na way you can intuitively map relationships between data structures rather\nthan trying to work out a few values on paper and the rest in your head. We\nsuggest you put everything on paper irrespective of how trivial some variables\nand calculations may be so that you always have a point of reference.\nWhen dealing with recursive algorithm traces we recommend you do the\nsame as the above, but also have a table that records function calls and who\ntheyreturnto. Thisapproachisafarcleanerwaythandrawingoutanelaborate\nmap of function calls with arrows to one another, which gets large quickly and\nsimply makes things more complex to follow. Track everything in a simple and\nsystematic way to make your time studying the implementations far easier.\n1.5 Book outline\nWe have split this book into two parts:\nPart 1: Provides discussion and pseudo-implementations of common and uncom-\nmon data structures; and\nPart 2: Providesalgorithmsofvaryingpurposesfromsortingtostringoperations.\nThe reader doesn\u2019t have to read the book sequentially from beginning to\nend: chapters can be read independently from one another. We suggest that\nin part 1 you read each chapter in its entirety, but in part 2 you can get away\nwith just reading the section of a chapter that describes the algorithm you are\ninterested in.\nEachofthechaptersondatastructurespresentinitiallythealgorithmscon-\ncerned with:\n1. Insertion\n2. Deletion\n3. Searching\nThe previous list represents what we believe in the vast majority of cases to\nbe the most important for each respective data structure.\nFor all readers we recommend that before looking at any algorithm you\nquickly look at Appendix E which contains a table listing the various symbols\nusedwithinouralgorithmsandtheirmeaning. Onekeywordthatwewouldlike\nto point out here is yield. You can think of yield in the same light as return.\nThereturnkeywordcausesthemethodtoexitandreturnscontroltothecaller,\nwhereas yield returns each value to the caller. With yield control only returns\nto the caller when all values to return to the caller have been exhausted. CHAPTER 1. INTRODUCTION 7\n1.6 Testing\nAll the data structures and algorithms have been tested using a minimised test\ndriven development style on paper to flesh out the pseudocode algorithm. We\nthen transcribe these tests into unit tests satisfying them one by one. When\nall the test cases have been progressively satisfied we consider that algorithm\nsuitably tested.\nFor the most part algorithms have fairly obvious cases which need to be\nsatisfied. Some however have many areas which can prove to be more complex\ntosatisfy. Withsuchalgorithmswewillpointoutthetestcaseswhicharetricky\nandthecorrespondingportionsofpseudocodewithinthealgorithmthatsatisfy\nthat respective case.\nAs you become more familiar with the actual problem you will be able to\nintuitively identify areas which may cause problems for your algorithms imple-\nmentation. Thisinsomecaseswillyieldanoverwhelminglistofconcernswhich\nwill hinder your ability to design an algorithm greatly. When you are bom-\nbarded with such a vast amount of concerns look at the overall problem again\nandsub-dividetheproblemintosmallerproblems. Solvingthesmallerproblems\nand then composing them is a far easier task than clouding your mind with too\nmany little details.\nThe only type of testing that we use in the implementation of all that is\nprovided in this book are unit tests. Because unit tests contribute such a core\npiece of creating somewhat more stable software we invite the reader to view\nAppendix D which describes testing in more depth.\n1.7 Where can I get the code?\nThis book doesn\u2019t provide any code specifically aligned with it, however we do\nactively maintain an open source project1 that houses a C# implementation of\nallthepseudocodelisted. TheprojectisnamedDataStructuresandAlgorithms\n(DSA) and can be found at http:\/\/codeplex.com\/dsa.\n1.8 Final messages\nWe have just a few final messages to the reader that we hope you digest before\nyou embark on reading this book:\n1. Understand how the algorithm works first in an abstract sense; and\n2. Always work through the algorithms on paper to understand how they\nachieve their outcome\nIfyoualwaysfollowthesekeypoints, youwillgetthemostoutofthisbook.\n1All readers are encouraged to provide suggestions, feature requests, and bugs so we can\nfurtherimproveourimplementations. Part I\nData Structures\n8 Chapter 2\nLinked Lists\nLinked lists can be thought of from a high level perspective as being a series\nof nodes. Each node has at least a single pointer to the next node, and in the\nlast node\u2019s case a null pointer representing that there are no more nodes in the\nlinked list.\nIn DSA our implementations of linked lists always maintain head and tail\npointers so that insertion at either the head or tail of the list is a constant\ntime operation. Random insertion is excluded from this and will be a linear\noperation. As such, linked lists in DSA have the following characteristics:\n1. Insertion is O(1)\n2. Deletion is O(n)\n3. Searching is O(n)\nOut of the three operations the one that stands out is that of insertion. In\nDSA we chose to always maintain pointers (or more aptly references) to the\nnode(s) at the head and tail of the linked list and so performing a traditional\ninsertion to either the front or back of the linked list is an O(1) operation. An\nexception to this rule is performing an insertion before a node that is neither\nthe head nor tail in a singly linked list. When the node we are inserting before\nis somewhere in the middle of the linked list (known as random insertion) the\ncomplexity is O(n). In order to add before the designated node we need to\ntraverse the linked list to find that node\u2019s current predecessor. This traversal\nyields an O(n) run time.\nThisdatastructureistrivial, butlinkedlistshaveafewkeypointswhichat\ntimes make them very attractive:\n1. thelistisdynamicallyresized,thusitincursnocopypenaltylikeanarray\nor vector would eventually incur; and\n2. insertion is O(1).\n2.1 Singly Linked List\nSingly linked lists are one of the most primitive data structures you will find in\nthis book. Each node that makes up a singly linked list consists of a value, and\na reference to the next node (if any) in the list.\n9 CHAPTER 2. LINKED LISTS 10\nFigure 2.1: Singly linked list node\nFigure 2.2: A singly linked list populated with integers\n2.1.1 Insertion\nIn general when people talk about insertion with respect to linked lists of any\nform they implicitly refer to the adding of a node to the tail of the list. When\nyou use an API like that of DSA and you see a general purpose method that\naddsanodetothelist,youcanassumethatyouareaddingthenodetothetail\nof the list not the head.\nAdding a node to a singly linked list has only two cases:\n1. head=\u2205 in which case the node we are adding is now both the head and\ntail of the list; or\n2. we simply need to append our node onto the end of the list updating the\ntail reference appropriately.\n1) algorithm Add(value)\n2) Pre: value is the value to add to the list\n3) Post: value has been placed at the tail of the list\n4) n\u2190 node(value)\n5) if head=\u2205\n6) head\u2190n\n7) tail\u2190n\n8) else\n9) tail.Next \u2190n\n10) tail\u2190n\n11) end if\n12) end Add\nAs an example of the previous algorithm consider adding the following se-\nquence of integers to the list: 1, 45, 60, and 12, the resulting list is that of\nFigure 2.2.\n2.1.2 Searching\nSearching a linked list is straightforward: we simply traverse the list checking\nthe value we are looking for with the value of each node in the linked list. The\nalgorithmlistedinthissectionisverysimilartothatusedfortraversalin\u00a72.1.4. CHAPTER 2. LINKED LISTS 11\n1) algorithm Contains(head, value)\n2) Pre: head is the head node in the list\n3) value is the value to search for\n4) Post: the item is either in the linked list, true; otherwise false\n5) n\u2190head\n6) while n (cid:54)=\u2205 and n.Value (cid:54)=value\n7) n\u2190n.Next\n8) end while\n9) if n=\u2205\n10) return false\n11) end if\n12) return true\n13) end Contains\n2.1.3 Deletion\nDeleting a node from a linked list is straightforward but there are a few cases\nwe need to account for:\n1. the list is empty; or\n2. the node to remove is the only node in the linked list; or\n3. we are removing the head node; or\n4. we are removing the tail node; or\n5. the node to remove is somewhere in between the head and tail; or\n6. the item to remove doesn\u2019t exist in the linked list\nThe algorithm whose cases we have described will remove a node from any-\nwherewithinalistirrespectiveofwhetherthenodeistheheadetc. Ifyouknow\nthat items will only ever be removed from the head or tail of the list then you\ncan create much more concise algorithms. In the case of always removing from\nthe front of the linked list deletion becomes an O(1) operation. CHAPTER 2. LINKED LISTS 12\n1) algorithm Remove(head, value)\n2) Pre: head is the head node in the list\n3) value is the value to remove from the list\n4) Post: value is removed from the list, true; otherwise false\n5) if head=\u2205\n6) \/\/ case 1\n7) return false\n8) end if\n9) n\u2190head\n10) if n.Value =value\n11) if head=tail\n12) \/\/ case 2\n13) head\u2190\u2205\n14) tail\u2190\u2205\n15) else\n16) \/\/ case 3\n17) head\u2190head.Next\n18) end if\n19) return true\n20) end if\n21) while n.Next (cid:54)=\u2205 and n.Next.Value (cid:54)=value\n22) n\u2190n.Next\n23) end while\n24) if n.Next (cid:54)=\u2205\n25) if n.Next =tail\n26) \/\/ case 4\n27) tail\u2190n\n28) end if\n29) \/\/ this is only case 5 if the conditional on line 25 was false\n30) n.Next \u2190n.Next.Next\n31) return true\n32) end if\n33) \/\/ case 6\n34) return false\n35) end Remove\n2.1.4 Traversing the list\nTraversing a singly linked list is the same as that of traversing a doubly linked\nlist (defined in \u00a72.2). You start at the head of the list and continue until you\ncome across a node that is \u2205. The two cases are as follows:\n1. node=\u2205, we have exhausted all nodes in the linked list; or\n2. we must update the node reference to be node.Next.\nThe algorithm described is a very simple one that makes use of a simple\nwhile loop to check the first case. CHAPTER 2. LINKED LISTS 13\n1) algorithm Traverse(head)\n2) Pre: head is the head node in the list\n3) Post: the items in the list have been traversed\n4) n\u2190head\n5) while n (cid:54)=0\n6) yield n.Value\n7) n\u2190n.Next\n8) end while\n9) end Traverse\n2.1.5 Traversing the list in reverse order\nTraversing a singly linked list in a forward manner (i.e. left to right) is simple\nasdemonstratedin\u00a72.1.4. However, whatifwewantedtotraversethenodesin\nthe linked list in reverse order for some reason? The algorithm to perform such\na traversal is very simple, and just like demonstrated in \u00a72.1.3 we will need to\nacquire a reference to the predecessor of a node, even though the fundamental\ncharacteristics of the nodes that make up a singly linked list make this an\nexpensiveoperation. Foreachnode,findingitspredecessorisanO(n)operation,\nsooverthecourseoftraversingthewholelistbackwardsthecostbecomesO(n2).\nFigure2.3depictsthefollowingalgorithmbeingappliedtoalinkedlistwith\nthe integers 5, 10, 1, and 40.\n1) algorithm ReverseTraversal(head, tail)\n2) Pre: head and tail belong to the same list\n3) Post: the items in the list have been traversed in reverse order\n4) if tail (cid:54)=\u2205\n5) curr \u2190tail\n6) while curr (cid:54)=head\n7) prev \u2190head\n8) while prev.Next (cid:54)=curr\n9) prev \u2190prev.Next\n10) end while\n11) yield curr.Value\n12) curr \u2190prev\n13) end while\n14) yield curr.Value\n15) end if\n16) end ReverseTraversal\nThis algorithm is only of real interest when we are using singly linked lists,\nas you will soon see that doubly linked lists (defined in \u00a72.2) make reverse list\ntraversal simple and efficient, as shown in \u00a72.2.3.\n2.2 Doubly Linked List\nDoubly linked lists are very similar to singly linked lists. The only difference is\nthat each node has a reference to both the next and previous nodes in the list. CHAPTER 2. LINKED LISTS 14\nFigure 2.3: Reverse traveral of a singly linked list\nFigure 2.4: Doubly linked list node CHAPTER 2. LINKED LISTS 15\nThe following algorithms for the doubly linked list are exactly the same as\nthose listed previously for the singly linked list:\n1. Searching (defined in \u00a72.1.2)\n2. Traversal (defined in \u00a72.1.4)\n2.2.1 Insertion\nThe only major difference between the algorithm in \u00a72.1.1 is that we need to\nremember to bind the previous pointer of n to the previous tail node if n was\nnot the first node to be inserted into the list.\n1) algorithm Add(value)\n2) Pre: value is the value to add to the list\n3) Post: value has been placed at the tail of the list\n4) n\u2190 node(value)\n5) if head=\u2205\n6) head\u2190n\n7) tail\u2190n\n8) else\n9) n.Previous \u2190tail\n10) tail.Next \u2190n\n11) tail\u2190n\n12) end if\n13) end Add\nFigure 2.5 shows the doubly linked list after adding the sequence of integers\ndefined in \u00a72.1.1.\nFigure 2.5: Doubly linked list populated with integers\n2.2.2 Deletion\nAs you may of guessed the cases that we use for deletion in a doubly linked\nlist are exactly the same as those defined in \u00a72.1.3. Like insertion we have the\nadded task of binding an additional reference (Previous) to the correct value. CHAPTER 2. LINKED LISTS 16\n1) algorithm Remove(head, value)\n2) Pre: head is the head node in the list\n3) value is the value to remove from the list\n4) Post: value is removed from the list, true; otherwise false\n5) if head=\u2205\n6) return false\n7) end if\n8) if value=head.Value\n9) if head=tail\n10) head\u2190\u2205\n11) tail\u2190\u2205\n12) else\n13) head\u2190head.Next\n14) head.Previous \u2190\u2205\n15) end if\n16) return true\n17) end if\n18) n\u2190head.Next\n19) while n (cid:54)=\u2205 and value (cid:54)=n.Value\n20) n\u2190n.Next\n21) end while\n22) if n=tail\n23) tail\u2190tail.Previous\n24) tail.Next \u2190\u2205\n25) return true\n26) else if n (cid:54)=\u2205\n27) n.Previous.Next \u2190n.Next\n28) n.Next.Previous \u2190n.Previous\n29) return true\n30) end if\n31) return false\n32) end Remove\n2.2.3 Reverse Traversal\nSinglylinkedlistshaveaforwardonlydesign,whichiswhythereversetraversal\nalgorithmdefinedin\u00a72.1.5requiredsomecreativeinvention. Doublylinkedlists\nmake reverse traversal as simple as forward traversal (defined in \u00a72.1.4) except\nthatwestartatthetailnodeandupdatethepointersintheoppositedirection.\nFigure 2.6 shows the reverse traversal algorithm in action. CHAPTER 2. LINKED LISTS 17\nFigure 2.6: Doubly linked list reverse traversal\n1) algorithm ReverseTraversal(tail)\n2) Pre: tail is the tail node of the list to traverse\n3) Post: the list has been traversed in reverse order\n4) n\u2190tail\n5) while n(cid:54)=\u2205\n6) yield n.Value\n7) n\u2190n.Previous\n8) end while\n9) end ReverseTraversal\n2.3 Summary\nLinked lists are good to use when you have an unknown number of items to\nstore. Using a data structure like an array would require you to specify the size\nup front; exceeding that size involves invoking a resizing algorithm which has\na linear run time. You should also use linked lists when you will only remove\nnodes at either the head or tail of the list to maintain a constant run time.\nThis requires maintaining pointers to the nodes at the head and tail of the list\nbut the memory overhead will pay for itself if this is an operation you will be\nperforming many times.\nWhat linked lists are not very good for is random insertion, accessing nodes\nby index, and searching. At the expense of a little memory (in most cases 4\nbytes would suffice), and a few more read\/writes you could maintain a count\nvariable that tracks how many items are contained in the list so that accessing\nsuch a primitive property is a constant operation - you just need to update\ncount during the insertion and deletion algorithms.\nSingly linked lists should be used when you are only performing basic in-\nsertions. In general doubly linked lists are more accommodating for non-trivial\noperations on a linked list.\nWe recommend the use of a doubly linked list when you require forwards\nand backwards traversal. For the most cases this requirement is present. For\nexample, consider a token stream that you want to parse in a recursive descent\nfashion. Sometimes you will have to backtrack in order to create the correct\nparse tree. In this scenario a doubly linked list is best as its design makes\nbi-directional traversal much simpler and quicker than that of a singly linked CHAPTER 2. LINKED LISTS 18\nlist. Chapter 3\nBinary Search Tree\nBinarysearchtrees(BSTs)areverysimpletounderstand. Westartwitharoot\nnode with value x, where the left subtree of x contains nodes with values < x\nand the right subtree contains nodes whose values are \u2265 x. Each node follows\nthe same rules with respect to nodes in their left and right subtrees.\nBSTsareofinterestbecausetheyhaveoperationswhicharefavourablyfast:\ninsertion,lookup,anddeletioncanallbedoneinO(log n)time. Itisimportant\nto note that the O(log n) times for these operations can only be attained if\nthe BST is reasonably balanced; for a tree data structure with self balancing\nproperties see AVL tree defined in \u00a77).\nIn the following examples you can assume, unless used as a parameter alias\nthat root is a reference to the root node of the tree.\n23\n14 31\n7 17\n9\nFigure 3.1: Simple unbalanced binary search tree\n19 CHAPTER 3. BINARY SEARCH TREE 20\n3.1 Insertion\nAs mentioned previously insertion is an O(log n) operation provided that the\ntree is moderately balanced.\n1) algorithm Insert(value)\n2) Pre: value has passed custom type checks for type T\n3) Post: value has been placed in the correct location in the tree\n4) if root =\u2205\n5) root\u2190 node(value)\n6) else\n7) InsertNode(root, value)\n8) end if\n9) end Insert\n1) algorithm InsertNode(current, value)\n2) Pre: current is the node to start from\n3) Post: value has been placed in the correct location in the tree\n4) if value<current.Value\n5) if current.Left =\u2205\n6) current.Left \u2190 node(value)\n7) else\n8) InsertNode(current.Left, value)\n9) end if\n10) else\n11) if current.Right =\u2205\n12) current.Right \u2190 node(value)\n13) else\n14) InsertNode(current.Right, value)\n15) end if\n16) end if\n17) end InsertNode\nThe insertion algorithm is split for a good reason. The first algorithm (non-\nrecursive) checks a very core base case - whether or not the tree is empty. If\nthe tree is empty then we simply create our root node and finish. In all other\ncases we invoke the recursive InsertNode algorithm which simply guides us to\nthe first appropriate place in the tree to put value. Note that at each stage we\nperform a binary chop: we either choose to recurse into the left subtree or the\nrightbycomparingthenewvaluewiththatofthecurrentnode. Foranytotally\nordered type, no value can simultaneously satisfy the conditions to place it in\nboth subtrees. CHAPTER 3. BINARY SEARCH TREE 21\n3.2 Searching\nSearchingaBSTisevensimplerthaninsertion. Thepseudocodeisself-explanatory\nbut we will look briefly at the premise of the algorithm nonetheless.\nWehavetalkedpreviouslyaboutinsertion,wegoeitherleftorrightwiththe\nright subtree containing values that are \u2265 x where x is the value of the node\nwe are inserting. When searching the rules are made a little more atomic and\nat any one time we have four cases to consider:\n1. the root=\u2205 in which case value is not in the BST; or\n2. root.Value =value in which case value is in the BST; or\n3. value<root.Value, we must inspect the left subtree of root for value; or\n4. value>root.Value, we must inspect the right subtree of root for value.\n1) algorithm Contains(root, value)\n2) Pre: root is the root node of the tree, value is what we would like to locate\n3) Post: value is either located or not\n4) if root=\u2205\n5) return false\n6) end if\n7) if root.Value =value\n8) return true\n9) else if value<root.Value\n10) return Contains(root.Left, value)\n11) else\n12) return Contains(root.Right, value)\n13) end if\n14) end Contains CHAPTER 3. BINARY SEARCH TREE 22\n3.3 Deletion\nRemoving a node from a BST is fairly straightforward, with four cases to con-\nsider:\n1. the value to remove is a leaf node; or\n2. the value to remove has a right subtree, but no left subtree; or\n3. the value to remove has a left subtree, but no right subtree; or\n4. the value to remove has both a left and right subtree in which case we\npromote the largest value in the left subtree.\nThere is also an implicit fifth case whereby the node to be removed is the\nonly node in the tree. This case is already covered by the first, but should be\nnoted as a possibility nonetheless.\nOf course in a BST a value may occur more than once. In such a case the\nfirst occurrence of that value in the BST will be removed.\n#4: Right subtree\n23\nand left subtree\n#3: Left subtree\n14 31\nno right subtree\n#2: Right subtree\n7\nno left subtree\n#1: Leaf Node 9\nFigure 3.2: binary search tree deletion cases\nThe Remove algorithm given below relies on two further helper algorithms\nnamed FindParent, and FindNode which are described in \u00a73.4 and \u00a73.5 re-\nspectively. CHAPTER 3. BINARY SEARCH TREE 23\n1) algorithm Remove(value)\n2) Pre: value is the value of the node to remove, root is the root node of the BST\n3) Count is the number of items in the BST\n3) Post: node with value is removed if found in which case yields true, otherwise false\n4) nodeToRemove\u2190 FindNode(value)\n5) if nodeToRemove=\u2205\n6) return false \/\/ value not in BST\n7) end if\n8) parent\u2190 FindParent(value)\n9) if Count =1\n10) root\u2190\u2205 \/\/ we are removing the only node in the BST\n11) else if nodeToRemove.Left =\u2205 and nodeToRemove.Right =null\n12) \/\/ case #1\n13) if nodeToRemove.Value <parent.Value\n14) parent.Left \u2190\u2205\n15) else\n16) parent.Right \u2190\u2205\n17) end if\n18) else if nodeToRemove.Left =\u2205 and nodeToRemove.Right (cid:54)=\u2205\n19) \/\/ case # 2\n20) if nodeToRemove.Value <parent.Value\n21) parent.Left \u2190nodeToRemove.Right\n22) else\n23) parent.Right \u2190nodeToRemove.Right\n24) end if\n25) else if nodeToRemove.Left (cid:54)=\u2205 and nodeToRemove.Right =\u2205\n26) \/\/ case #3\n27) if nodeToRemove.Value <parent.Value\n28) parent.Left \u2190nodeToRemove.Left\n29) else\n30) parent.Right \u2190nodeToRemove.Left\n31) end if\n32) else\n33) \/\/ case #4\n34) largestValue\u2190nodeToRemove.Left\n35) while largestValue.Right (cid:54)=\u2205\n36) \/\/ find the largest value in the left subtree of nodeToRemove\n37) largestValue\u2190largestValue.Right\n38) end while\n39) \/\/ set the parents\u2019 Right pointer of largestValue to \u2205\n40) FindParent(largestValue.Value).Right \u2190\u2205\n41) nodeToRemove.Value \u2190largestValue.Value\n42) end if\n43) Count \u2190 Count \u22121\n44) return true\n45) end Remove CHAPTER 3. BINARY SEARCH TREE 24\n3.4 Finding the parent of a given node\nThe purpose of this algorithm is simple - to return a reference (or pointer) to\nthe parent node of the one with the given value. We have found that such an\nalgorithm is very useful, especially when performing extensive tree transforma-\ntions.\n1) algorithm FindParent(value, root)\n2) Pre: value is the value of the node we want to find the parent of\n3) root is the root node of the BST and is !=\u2205\n4) Post: a reference to the parent node of value if found; otherwise \u2205\n5) if value=root.Value\n6) return \u2205\n7) end if\n8) if value<root.Value\n9) if root.Left =\u2205\n10) return \u2205\n11) else if root.Left.Value =value\n12) return root\n13) else\n14) return FindParent(value, root.Left)\n15) end if\n16) else\n17) if root.Right =\u2205\n18) return \u2205\n19) else if root.Right.Value =value\n20) return root\n21) else\n22) return FindParent(value, root.Right)\n23) end if\n24) end if\n25) end FindParent\nA special case in the above algorithm is when the specified value does not\nexistintheBST,inwhichcasewereturn\u2205. Callerstothisalgorithmmusttake\naccount of this possibility unless they are already certain that a node with the\nspecified value exists.\n3.5 Attaining a reference to a node\nThisalgorithmisverysimilarto\u00a73.4,butinsteadofreturningareferencetothe\nparent of the node with the specified value, it returns a reference to the node\nitself. Again, \u2205 is returned if the value isn\u2019t found. CHAPTER 3. BINARY SEARCH TREE 25\n1) algorithm FindNode(root, value)\n2) Pre: value is the value of the node we want to find the parent of\n3) root is the root node of the BST\n4) Post: a reference to the node of value if found; otherwise \u2205\n5) if root=\u2205\n6) return \u2205\n7) end if\n8) if root.Value =value\n9) return root\n10) else if value<root.Value\n11) return FindNode(root.Left, value)\n12) else\n13) return FindNode(root.Right, value)\n14) end if\n15) end FindNode\nAstute readers will have noticed that the FindNode algorithm is exactly the\nsame as the Contains algorithm (defined in \u00a73.2) with the modification that\nwe are returning a reference to a node not true or false. Given FindNode,\nthe easiest way of implementing Contains is to call FindNode and compare the\nreturn value with \u2205.\n3.6 Finding the smallest and largest values in\nthe binary search tree\nTo find the smallest value in a BST you simply traverse the nodes in the left\nsubtree of the BST always going left upon each encounter with a node, termi-\nnatingwhenyoufindanodewithnoleftsubtree. Theoppositeisthecasewhen\nfindingthelargestvalueintheBST.Bothalgorithmsareincrediblysimple,and\nare listed simply for completeness.\nThebasecaseinbothFindMin,andFindMaxalgorithmsiswhentheLeft\n(FindMin), or Right (FindMax) node references are \u2205 in which case we have\nreached the last node.\n1) algorithm FindMin(root)\n2) Pre: root is the root node of the BST\n3) root (cid:54)=\u2205\n4) Post: the smallest value in the BST is located\n5) if root.Left =\u2205\n6) return root.Value\n7) end if\n8) FindMin(root.Left)\n9) end FindMin CHAPTER 3. BINARY SEARCH TREE 26\n1) algorithm FindMax(root)\n2) Pre: root is the root node of the BST\n3) root (cid:54)=\u2205\n4) Post: the largest value in the BST is located\n5) if root.Right =\u2205\n6) return root.Value\n7) end if\n8) FindMax(root.Right)\n9) end FindMax\n3.7 Tree Traversals\nThere are various strategies which can be employed to traverse the items in a\ntree; the choice of strategy depends on which node visitation order you require.\nIn this section we will touch on the traversals that DSA provides on all data\nstructures that derive from BinarySearchTree.\n3.7.1 Preorder\nWhenusingthepreorderalgorithm,youvisittherootfirst,thentraversetheleft\nsubtree and finally traverse the right subtree. An example of preorder traversal\nis shown in Figure 3.3.\n1) algorithm Preorder(root)\n2) Pre: root is the root node of the BST\n3) Post: the nodes in the BST have been visited in preorder\n4) if root (cid:54)=\u2205\n5) yield root.Value\n6) Preorder(root.Left)\n7) Preorder(root.Right)\n8) end if\n9) end Preorder\n3.7.2 Postorder\nThis algorithm is very similar to that described in \u00a73.7.1, however the value\nof the node is yielded after traversing both subtrees. An example of postorder\ntraversal is shown in Figure 3.4.\n1) algorithm Postorder(root)\n2) Pre: root is the root node of the BST\n3) Post: the nodes in the BST have been visited in postorder\n4) if root (cid:54)=\u2205\n5) Postorder(root.Left)\n6) Postorder(root.Right)\n7) yield root.Value\n8) end if\n9) end Postorder CHAPTER 3. BINARY SEARCH TREE 27\n23 23 23\n14 31 14 31 14 31\n7 17 7 17 7 17\n9 9 9\n(a) (b) (c)\n23 23 23\n14 31 14 31 14 31\n7 17 7 17 7 17\n9 9 9\n(d) (e) (f)\nFigure 3.3: Preorder visit binary search tree example CHAPTER 3. BINARY SEARCH TREE 28\n23 23 23\n14 31 14 31 14 31\n7 17 7 17 7 17\n9 9 9\n(a) (b) (c)\n23 23 23\n14 31 14 31 14 31\n7 17 7 17 7 17\n9 9 9\n(d) (e) (f)\nFigure 3.4: Postorder visit binary search tree example CHAPTER 3. BINARY SEARCH TREE 29\n3.7.3 Inorder\nAnothervariationofthealgorithmsdefinedin\u00a73.7.1and\u00a73.7.2isthatofinorder\ntraversal where the value of the current node is yielded in between traversing\ntheleftsubtreeandtherightsubtree. Anexampleofinordertraversalisshown\nin Figure 3.5.\n23 23 23\n14 31 14 31 14 31\n7 17 7 17 7 17\n9 9 9\n(a) (b) (c)\n23 23 23\n14 31 14 31 14 31\n7 17 7 17 7 17\n9 9 9\n(d) (e) (f)\nFigure 3.5: Inorder visit binary search tree example\n1) algorithm Inorder(root)\n2) Pre: root is the root node of the BST\n3) Post: the nodes in the BST have been visited in inorder\n4) if root (cid:54)=\u2205\n5) Inorder(root.Left)\n6) yield root.Value\n7) Inorder(root.Right)\n8) end if\n9) end Inorder\nOne of the beauties of inorder traversal is that values are yielded in their\ncomparison order. In other words, when traversing a populated BST with the\ninorder strategy, the yielded sequence would have property x \u2264x \u2200i.\ni i+1 CHAPTER 3. BINARY SEARCH TREE 30\n3.7.4 Breadth First\nTraversing a tree in breadth first order yields the values of all nodes of a par-\nticular depth in the tree before any deeper ones. In other words, given a depth\nd we would visit the values of all nodes at d in a left to right fashion, then we\nwould proceed to d+1 and so on until we hade no more nodes to visit. An\nexample of breadth first traversal is shown in Figure 3.6.\nTraditionally breadth first traversal is implemented using a list (vector, re-\nsizeablearray, etc)tostorethevaluesofthenodesvisitedinbreadthfirstorder\nand then a queue to store those nodes that have yet to be visited.\n23 23 23\n14 31 14 31 14 31\n7 17 7 17 7 17\n9 9 9\n(a) (b) (c)\n23 23 23\n14 31 14 31 14 31\n7 17 7 17 7 17\n9 9 9\n(d) (e) (f)\nFigure 3.6: Breadth First visit binary search tree example CHAPTER 3. BINARY SEARCH TREE 31\n1) algorithm BreadthFirst(root)\n2) Pre: root is the root node of the BST\n3) Post: the nodes in the BST have been visited in breadth first order\n4) q \u2190 queue\n5) while root (cid:54)=\u2205\n6) yield root.Value\n7) if root.Left (cid:54)=\u2205\n8) q.Enqueue(root.Left)\n9) end if\n10) if root.Right (cid:54)=\u2205\n11) q.Enqueue(root.Right)\n12) end if\n13) if !q.IsEmpty()\n14) root\u2190q.Dequeue()\n15) else\n16) root\u2190\u2205\n17) end if\n18) end while\n19) end BreadthFirst\n3.8 Summary\nAbinarysearchtreeisagoodsolutionwhenyouneedtorepresenttypesthatare\norderedaccordingtosomecustomrulesinherenttothattype. Withlogarithmic\ninsertion, lookup, and deletion it is very effecient. Traversal remains linear, but\nthere are many ways in which you can visit the nodes of a tree. Trees are\nrecursive data structures, so typically you will find that many algorithms that\noperate on a tree are recursive.\nTheruntimespresentedinthischapterarebasedonaprettybigassumption\n- that the binary search tree\u2019s left and right subtrees are reasonably balanced.\nWe can only attain logarithmic run times for the algorithms presented earlier\nwhen this is true. A binary search tree does not enforce such a property, and\nthe run times for these operations on a pathologically unbalanced tree become\nlinear: such a tree is effectively just a linked list. Later in \u00a77 we will examine\nan AVL tree that enforces self-balancing properties to help attain logarithmic\nrun times. Chapter 4\nHeap\nAheapcanbethoughtofasasimpletreedatastructure,howeveraheapusually\nemploys one of two strategies:\n1. min heap; or\n2. max heap\nEach strategy determines the properties of the tree and its values. If you\nweretochoosetheminheapstrategytheneachparentnodewouldhaveavalue\nthat is \u2264 than its children. For example, the node at the root of the tree will\nhave the smallest value in the tree. The opposite is true for the max heap\nstrategy. In this book you should assume that a heap employs the min heap\nstrategy unless otherwise stated.\nUnlikeothertreedatastructuresliketheonedefinedin\u00a73aheapisgenerally\nimplemented as an array rather than a series of nodes which each have refer-\nences to other nodes. The nodes are conceptually the same, however, having at\nmost two children. Figure 4.1 shows how the tree (not a heap data structure)\n(127(32)6(9 ))wouldberepresentedasanarray. ThearrayinFigure4.1isa\nresult of simply adding values in a top-to-bottom, left-to-right fashion. Figure\n4.2 shows arrows to the direct left and right child of each value in the array.\nThischapterisverymuchcentredaroundthenotionofrepresentingatreeas\nan array and because this property is key to understanding this chapter Figure\n4.3 shows a step by step process to represent a tree data structure as an array.\nIn Figure 4.3 you can assume that the default capacity of our array is eight.\nUsingjustanarrayisoftennotsufficientaswehavetobeupfrontaboutthe\nsizeofthearraytousefortheheap. Oftentheruntimebehaviourofaprogram\ncan be unpredictable when it comes to the size of its internal data structures,\nsoweneedtochooseamoredynamicdatastructurethatcontainsthefollowing\nproperties:\n1. we can specify an initial size of the array for scenarios where we know the\nupper storage limit required; and\n2. the data structure encapsulates resizing algorithms to grow the array as\nrequired at run time\n32 CHAPTER 4. HEAP 33\nFigure 4.1: Array representation of a simple tree data structure\nFigure4.2: Directchildrenofthenodesinanarrayrepresentationofatreedata\nstructure\n1. Vector\n2. ArrayList\n3. List\nFigure 4.1 does not specify how we would handle adding null references to\nthe heap. This varies from case to case; sometimes null values are prohibited\nentirely; in other cases we may treat them as being smaller than any non-null\nvalue, or indeed greater than any non-null value. You will have to resolve this\nambiguityyourselfhavingstudiedyourrequirements. Forthesakeofclaritywe\nwill avoid the issue by prohibiting null values.\nBecause we are using an array we need some way to calculate the index of a\nparent node, and the children of a node. The required expressions for this are\ndefined as follows for a node at index:\n1. (index\u22121)\/2 (parent index)\n2. 2\u2217index+1 (left child)\n3. 2\u2217index+2 (right child)\nIn Figure 4.4 a) represents the calculation of the right child of 12 (2\u22170+2);\nand b) calculates the index of the parent of 3 ((3\u22121)\/2).\n4.1 Insertion\nDesigning an algorithm for heap insertion is simple, but we must ensure that\nheap order is preserved after each insertion. Generally this is a post-insertion\noperation. Insertingavalueintothenextfreeslotinanarrayissimple: wejust\nneedtokeeptrackofthenextfreeindexinthearrayasacounter,andincrement\nit after each insertion. Inserting our value into the heap is the first part of the\nalgorithm;thesecondisvalidatingheaporder. Inthecaseofmin-heapordering\nthis requires us to swap the values of a parent and its child if the value of the\nchild is < the value of its parent. We must do this for each subtree containing\nthe value we just inserted. CHAPTER 4. HEAP 34\nFigure 4.3: Converting a tree data structure to its array counterpart CHAPTER 4. HEAP 35\nFigure 4.4: Calculating node properties\nThe run time efficiency for heap insertion is O(log n). The run time is a\nby product of verifying heap order as the first part of the algorithm (the actual\ninsertion into the array) is O(1).\nFigure 4.5 shows the steps of inserting the values 3, 9, 12, 7, and 1 into a\nmin-heap. CHAPTER 4. HEAP 36\nFigure 4.5: Inserting values into a min-heap CHAPTER 4. HEAP 37\n1) algorithm Add(value)\n2) Pre: value is the value to add to the heap\n3) Count is the number of items in the heap\n4) Post: the value has been added to the heap\n5) heap[Count] \u2190value\n6) Count \u2190 Count +1\n7) MinHeapify()\n8) end Add\n1) algorithm MinHeapify()\n2) Pre: Count is the number of items in the heap\n3) heap is the array used to store the heap items\n4) Post: the heap has preserved min heap ordering\n5) i\u2190 Count \u22121\n6) while i>0 and heap[i] <heap[(i\u22121)\/2]\n7) Swap(heap[i], heap[(i\u22121)\/2]\n8) i\u2190 (i\u22121)\/2\n9) end while\n10) end MinHeapify\nThe design of the MaxHeapify algorithm is very similar to that of the Min-\nHeapify algorithm, the only difference is that the < operator in the second\ncondition of entering the while loop is changed to >.\n4.2 Deletion\nJust as for insertion, deleting an item involves ensuring that heap ordering is\npreserved. The algorithm for deletion has three steps:\n1. find the index of the value to delete\n2. put the last value in the heap at the index location of the item to delete\n3. verify heap ordering for each subtree which used to include the value CHAPTER 4. HEAP 38\n1) algorithm Remove(value)\n2) Pre: value is the value to remove from the heap\n3) left, and right are updated alias\u2019 for 2\u2217index+1, and 2\u2217index+2 respectively\n4) Count is the number of items in the heap\n5) heap is the array used to store the heap items\n6) Post: value is located in the heap and removed, true; otherwise false\n7) \/\/ step 1\n8) index\u2190 FindIndex(heap, value)\n9) if index<0\n10) return false\n11) end if\n12) Count \u2190 Count \u22121\n13) \/\/ step 2\n14) heap[index] \u2190heap[Count]\n15) \/\/ step 3\n16) while left< Count and heap[index] >heap[left] or heap[index] >heap[right]\n17) \/\/ promote smallest key from subtree\n18) if heap[left] <heap[right]\n19) Swap(heap, left, index)\n20) index\u2190left\n21) else\n22) Swap(heap, right, index)\n23) index\u2190right\n24) end if\n25) end while\n26) return true\n27) end Remove\nFigure 4.6 shows the Remove algorithm visually, removing 1 from a heap\ncontaining the values 1, 3, 9, 12, and 13. In Figure 4.6 you can assume that we\nhavespecifiedthatthebackingarrayoftheheapshouldhaveaninitialcapacity\nof eight.\nPleasenotethatinourdeletionalgorithmthatwedon\u2019tdefaulttheremoved\nvalue in the heap array. If you are using a heap for reference types, i.e. objects\nthatareallocatedonaheapyouwillwanttofreethatmemory. Thisisimportant\nin both unmanaged, and managed languages. In the latter we will want to null\nthat empty hole so that the garbage collector can reclaim that memory. If we\nwere to not null that hole then the object could still be reached and thus won\u2019t\nbe garbage collected.\n4.3 Searching\nSearching a heap is merely a matter of traversing the items in the heap array\nsequentially, so this operation has a run time complexity of O(n). The search\ncan be thought of as one that uses a breadth first traversal as defined in \u00a73.7.4\nto visit the nodes within the heap to check for the presence of a specified item. CHAPTER 4. HEAP 39\nFigure 4.6: Deleting an item from a heap CHAPTER 4. HEAP 40\n1) algorithm Contains(value)\n2) Pre: value is the value to search the heap for\n3) Count is the number of items in the heap\n4) heap is the array used to store the heap items\n5) Post: value is located in the heap, in which case true; otherwise false\n6) i\u21900\n7) while i< Count and heap[i] (cid:54)=value\n8) i\u2190i+1\n9) end while\n10) if i< Count\n11) return true\n12) else\n13) return false\n14) end if\n15) end Contains\nThe problem with the previous algorithm is that we don\u2019t take advantage\nof the properties in which all values of a heap hold, that is the property of the\nheap strategy being used. For instance if we had a heap that didn\u2019t contain the\nvalue4wewouldhavetoexhaustthewholebackingheaparraybeforewecould\ndetermine that it wasn\u2019t present in the heap. Factoring in what we know about\nthe heap we can optimise the search algorithm by including logic which makes\nuse of the properties presented by a certain heap strategy.\nOptimising to deterministically state that a value is in the heap is not that\nstraightforward, however the problem is a very interesting one. As an example\nconsideramin-heapthatdoesn\u2019tcontainthevalue5. Wecanonlyrulethatthe\nvalue is not in the heap if 5 > the parent of the current node being inspected\nand < the current node being inspected \u2200 nodes at the current level we are\ntraversing. If this is the case then 5 cannot be in the heap and so we can\nprovide an answer without traversing the rest of the heap. If this property is\nnot satisfied for any level of nodes that we are inspecting then the algorithm\nwill indeed fall back to inspecting all the nodes in the heap. The optimisation\nthat we present can be very common and so we feel that the extra logic within\nthe loop is justified to prevent the expensive worse case run time.\nThefollowingalgorithmisspecificallydesignedforamin-heap. Totailorthe\nalgorithmforamax-heapthetwocomparisonoperationsintheelseif condition\nwithin the inner while loop should be flipped. CHAPTER 4. HEAP 41\n1) algorithm Contains(value)\n2) Pre: value is the value to search the heap for\n3) Count is the number of items in the heap\n4) heap is the array used to store the heap items\n5) Post: value is located in the heap, in which case true; otherwise false\n6) start\u21900\n7) nodes\u21901\n8) while start< Count\n9) start\u2190nodes\u22121\n10) end\u2190nodes+start\n11) count\u21900\n12) while start< Count and start<end\n13) if value=heap[start]\n14) return true\n15) else if value> Parent(heap[start]) and value<heap[start]\n16) count\u2190count+1\n17) end if\n18) start\u2190start+1\n19) end while\n20) if count=nodes\n21) return false\n22) end if\n23) nodes\u2190nodes\u22172\n24) end while\n25) return false\n26) end Contains\nThe new Contains algorithm determines if the value is not in the heap by\nchecking whether count = nodes. In such an event where this is true then we\ncan confirm that \u2200 nodes n at level i:value> Parent(n), value<n thus there\nisnopossiblewaythatvalueisintheheap. AsanexampleconsiderFigure4.7.\nIf we are searching for the value 10 within the min-heap displayed it is obvious\nthat we don\u2019t need to search the whole heap to determine 9 is not present. We\ncan verify this after traversing the nodes in the second level of the heap as the\nprevious expression defined holds true.\n4.4 Traversal\nAs mentioned in \u00a74.3 traversal of a heap is usually done like that of any other\narray data structure which our heap implementation is based upon. As a result\nyou traverse the array starting at the initial array index (0 in most languages)\nand then visit each value within the array until you have reached the upper\nboundoftheheap. YouwillnotethatinthesearchalgorithmthatweuseCount\nas this upper bound rather than the actual physical bound of the allocated\narray. Count is used to partition the conceptual heap from the actual array\nimplementation of the heap: we only care about the items in the heap, not the\nwhole array\u2014the latter may contain various other bits of data as a result of\nheap mutation. CHAPTER 4. HEAP 42\nFigure4.7: Determining10isnotintheheapafterinspectingthenodesofLevel\n2\nFigure 4.8: Living and dead space in the heap backing array\nIf you have followed the advice we gave in the deletion algorithm then a\nheap that has been mutated several times will contain some form of default\nvalue for items no longer in the heap. Potentially you will have at most\nLengthOf(heapArray)\u2212Count garbage values in the backing heap array data\nstructure. The garbage values of course vary from platform to platform. To\nmake things simple the garbage value of a reference type will be simple \u2205 and 0\nfor a value type.\nFigure4.8showsaheapthatyoucanassumehasbeenmutatedmanytimes.\nForthisexamplewecanfurtherassumethatatsomepointtheitemsinindexes\n3 \u2212 5 actually contained references to live objects of type T. In Figure 4.8\nsubscript is used to disambiguate separate objects of T.\nFrom what you have read thus far you will most likely have picked up that\ntraversing the heap in any other order would be of little benefit. The heap\nproperty only holds for the subtree of each node and so traversing a heap in\nany other fashion requires some creative intervention. Heaps are not usually\ntraversed in any other way than the one prescribed previously.\n4.5 Summary\nHeaps are most commonly used to implement priority queues (see \u00a76.2 for a\nsample implementation) and to facilitate heap sort. As discussed in both the\ninsertion \u00a74.1 and deletion \u00a74.2 sections a heap maintains heap order according\nto the selected ordering strategy. These strategies are referred to as min-heap, CHAPTER 4. HEAP 43\nand max heap. The former strategy enforces that the value of a parent node is\nless than that of each of its children, the latter enforces that the value of the\nparent is greater than that of each of its children.\nWhenyoucomeacrossaheapandyouarenottoldwhatstrategyitenforces\nyou should assume that it uses the min-heap strategy. If the heap can be\nconfigured otherwise, e.g. to use max-heap then this will often require you to\nstate this explicitly. The heap abides progressively to a strategy during the\ninvocationoftheinsertion,anddeletionalgorithms. Thecostofsuchapolicyis\nthatuponeachinsertionanddeletionweinvokealgorithmsthathavelogarithmic\nrun time complexities. While the cost of maintaining the strategy might not\nseem overly expensive it does still come at a price. We will also have to factor\nin the cost of dynamic array expansion at some stage. This will occur if the\nnumber of items within the heap outgrows the space allocated in the heap\u2019s\nbackingarray. Itmaybeinyourbestinteresttoresearchagoodinitialstarting\nsize for your heap array. This will assist in minimising the impact of dynamic\narray resizing. Chapter 5\nSets\nA set contains a number of values, in no particular order. The values within\nthe set are distinct from one another.\nGenerally set implementations tend to check that a value is not in the set\nbefore adding it, avoiding the issue of repeated values from ever occurring.\nThissectiondoesnotcoversettheoryindepth;ratheritdemonstratesbriefly\nthewaysinwhichthevaluesofsetscanbedefined,andcommonoperationsthat\nmay be performed upon them.\nThenotationA={4,7,9,12,0}definesasetAwhosevaluesarelistedwithin\nthe curly braces.\nGiven the set A defined previously we can say that 4 is a member of A\ndenoted by 4\u2208A, and that 99 is not a member of A denoted by 99\u2208\/ A.\nOften defining a set by manually stating its members is tiresome, and more\nimportantly the set may contain a large number of values. A more concise way\nof defining a set and its members is by providing a series of properties that the\nvalues of the set must satisfy. For example, from the definition A = {x|x >\n0,x % 2 = 0} the set A contains only positive integers that are even. x is an\nalias to the current value we are inspecting and to the right hand side of | are\nthe properties that x must satisfy to be in the set A. In this example, x must\nbe>0,andtheremainderofthearithmeticexpressionx\/2mustbe0. Youwill\nbeabletonotefromthepreviousdefinitionofthesetAthatthesetcancontain\nan infinite number of values, and that the values of the set A will be all even\nintegersthatareamemberofthenaturalnumberssetN,whereN={1,2,3,...}.\nFinally in this brief introduction to sets we will cover set intersection and\nunion, both of which are very common operations (amongst many others) per-\nformed on sets. The union set can be defined as follows A\u222aB = {x | x \u2208\nA or x \u2208 B}, and intersection A\u2229B = {x | x \u2208 A and x \u2208 B}. Figure 5.1\ndemonstrates set intersection and union graphically.\nGiventhesetdefinitionsA={1,2,3},andB ={6,2,9}theunionofthetwo\nsetsisA\u222aB ={1,2,3,6,9},andtheintersectionofthetwosetsisA\u2229B ={2}.\nBoth set union and intersection are sometimes provided within the frame-\nwork associated with mainstream languages. This is the case in .NET 3.51\nwhere such algorithms exist as extension methods defined in the type Sys-\ntem.Linq.Enumerable2, as a result DSA does not provide implementations of\n1http:\/\/www.microsoft.com\/NET\/\n2http:\/\/msdn.microsoft.com\/en-us\/library\/system.linq.enumerable_members.aspx\n44 CHAPTER 5. SETS 45\nFigure 5.1: a) A\u2229B; b) A\u222aB\nthese algorithms. Most of the algorithms defined in System.Linq.Enumerable\ndeal mainly with sequences rather than sets exclusively.\nSetunioncanbeimplementedasasimpletraversalofbothsetsaddingeach\nitem of the two sets to a new union set.\n1) algorithm Union(set1, set2)\n2) Pre: set1, and set2(cid:54)=\u2205\n3) union is a set\n3) Post: A union of set1, and set2 has been created\n4) foreach item in set1\n5) union.Add(item)\n6) end foreach\n7) foreach item in set2\n8) union.Add(item)\n9) end foreach\n10) return union\n11) end Union\nThe run time of our Union algorithm is O(m+n) where m is the number\nof items in the first set and n is the number of items in the second set. This\nruntime applies only to sets that exhibit O(1) insertions.\nSet intersection is also trivial to implement. The only major thing worth\npointing out about our algorithm is that we traverse the set containing the\nfewest items. We can do this because if we have exhausted all the items in the\nsmaller of the two sets then there are no more items that are members of both\nsets, thus we have no more items to add to the intersection set. CHAPTER 5. SETS 46\n1) algorithm Intersection(set1, set2)\n2) Pre: set1, and set2(cid:54)=\u2205\n3) intersection, and smallerSet are sets\n3) Post: An intersection of set1, and set2 has been created\n4) if set1.Count <set2.Count\n5) smallerSet\u2190set1\n6) else\n7) smallerSet\u2190set2\n8) end if\n9) foreach item in smallerSet\n10) if set1.Contains(item) and set2.Contains(item)\n11) intersection.Add(item)\n12) end if\n13) end foreach\n14) return intersection\n15) end Intersection\nThe run time of our Intersection algorithm is O(n) where n is the number\nof items in the smaller of the two sets. Just like our Union algorithm a linear\nruntime can only be attained when operating on a set with O(1) insertion.\n5.1 Unordered\nSets in the general sense do not enforce the explicit ordering of their mem-\nbers. For example the members of B ={6,2,9} conform to no ordering scheme\nbecause it is not required.\nMost libraries provide implementations of unordered sets and so DSA does\nnot; we simply mention it here to disambiguate between an unordered set and\nordered set.\nWe will only look at insertion for an unordered set and cover briefly why a\nhash table is an efficient data structure to use for its implementation.\n5.1.1 Insertion\nAnunorderedsetcanbeefficientlyimplementedusingahashtableasitsbacking\ndata structure. As mentioned previously we only add an item to a set if that\nitem is not already in the set, so the backing data structure we use must have\na quick look up and insertion run time complexity.\nA hash map generally provides the following:\n1. O(1) for insertion\n2. approaching O(1) for look up\nThe above depends on how good the hashing algorithm of the hash table\nis, but most hash tables employ incredibly efficient general purpose hashing\nalgorithms and so the run time complexities for the hash table in your library\nof choice should be very similar in terms of efficiency. CHAPTER 5. SETS 47\n5.2 Ordered\nAn ordered set is similar to an unordered set in the sense that its members are\ndistinct, but an ordered set enforces some predefined comparison on each of its\nmembers to produce a set whose members are ordered appropriately.\nIn DSA 0.5 and earlier we used a binary search tree (defined in \u00a73) as the\ninternal backing data structure for our ordered set. From versions 0.6 onwards\nwe replaced the binary search tree with an AVL tree primarily because AVL is\nbalanced.\nThe ordered set has its order realised by performing an inorder traversal\nupon its backing tree data structure which yields the correct ordered sequence\nof set members.\nBecause an ordered set in DSA is simply a wrapper for an AVL tree that\nadditionally ensures that the tree contains unique items you should read \u00a77 to\nlearn more about the run time complexities associated with its operations.\n5.3 Summary\nSets provide a way of having a collection of unique objects, either ordered or\nunordered.\nWhen implementing a set (either ordered or unordered) it is key to select\nthe correct backing data structure. As we discussed in \u00a75.1.1 because we check\nfirst if the item is already contained within the set before adding it we need\nthis check to be as quick as possible. For unordered sets we can rely on the use\nof a hash table and use the key of an item to determine whether or not it is\nalreadycontainedwithintheset. Usingahashtablethischeckresultsinanear\nconstant run time complexity. Ordered sets cost a little more for this check,\nhowever the logarithmic growth that we incur by using a binary search tree as\nits backing data structure is acceptable.\nAnotherkeypropertyofsetsimplementedusingtheapproachwedescribeis\nthat both have favourably fast look-up times. Just like the check before inser-\ntion,forahashtablethisruntimecomplexityshouldbenearconstant. Ordered\nsets as described in 3 perform a binary chop at each stage when searching for\nthe existence of an item yielding a logarithmic run time.\nWecanusesetstofacilitatemanyalgorithmsthatwouldotherwisebealittle\nless clear in their implementation. For example in \u00a711.4 we use an unordered\nset to assist in the construction of an algorithm that determines the number of\nrepeated words within a string. Chapter 6\nQueues\nQueues are an essential data structure that are found in vast amounts of soft-\nware from user mode to kernel mode applications that are core to the system.\nFundamentally they honour a first in first out (FIFO) strategy, that is the item\nfirst put into the queue will be the first served, the second item added to the\nqueue will be the second to be served and so on.\nA traditional queue only allows you to access the item at the front of the\nqueue; when you add an item to the queue that item is placed at the back of\nthe queue.\nHistorically queues always have the following three core methods:\nEnqueue: places an item at the back of the queue;\nDequeue: retrievestheitematthefrontofthequeue,andremovesitfromthe\nqueue;\nPeek: 1 retrieves the item at the front of the queue without removing it from\nthe queue\nAsanexampletodemonstratethebehaviourofaqueuewewillwalkthrough\nascenariowherebyweinvokeeachofthepreviouslymentionedmethodsobserv-\ning the mutations upon the queue data structure. The following list describes\nthe operations performed upon the queue in Figure 6.1:\n1. Enqueue(10)\n2. Enqueue(12)\n3. Enqueue(9)\n4. Enqueue(8)\n5. Enqueue(3)\n6. Dequeue()\n7. Peek()\n1ThisoperationissometimesreferredtoasFront\n48 CHAPTER 6. QUEUES 49\n8. Enqueue(33)\n9. Peek()\n10. Dequeue()\n6.1 A standard queue\nA queue is implicitly like that described prior to this section. In DSA we don\u2019t\nprovide a standard queue because queues are so popular and such a core data\nstructure that you will find pretty much every mainstream library provides a\nqueue data structure that you can use with your language of choice. In this\nsection we will discuss how you can, if required, implement an efficient queue\ndata structure.\nThe main property of a queue is that we have access to the item at the\nfront of the queue. The queue data structure can be efficiently implemented\nusing a singly linked list (defined in \u00a72.1). A singly linked list provides O(1)\ninsertion and deletion run time complexities. The reason we have an O(1) run\ntimecomplexityfordeletionisbecauseweonlyeverremoveitemsfromthefront\nof queues (with the Dequeue operation). Since we always have a pointer to the\nitem at the head of a singly linked list, removal is simply a case of returning\nthe value of the old head node, and then modifying the head pointer to be the\nnext node of the old head node. The run time complexity for searching a queue\nremains the same as that of a singly linked list: O(n).\n6.2 Priority Queue\nUnlike a standard queue where items are ordered in terms of who arrived first,\na priority queue determines the order of its items by using a form of custom\ncomparer to see which item has the highest priority. Other than the items in a\npriorityqueuebeingorderedbypriorityitremainsthesameasanormalqueue:\nyou can only access the item at the front of the queue.\nAsensibleimplementationofapriorityqueueistouseaheapdatastructure\n(definedin\u00a74). Usingaheapwecanlookatthefirstiteminthequeuebysimply\nreturningtheitematindex0withintheheaparray. Aheapprovidesuswiththe\nability to construct a priority queue where the items with the highest priority\nare either those with the smallest value, or those with the largest.\n6.3 Double Ended Queue\nUnlike the queues we have talked about previously in this chapter a double\nended queue allows you to access the items at both the front, and back of the\nqueue. Adoubleendedqueueiscommonlyknownasadequewhichisthename\nwe will here on in refer to it as.\nA deque applies no prioritization strategy to its items like a priority queue\ndoes, items are added in order to either the front of back of the deque. The\nformerpropertiesofthedequearedenotedbytheprogrammerutilisingthedata\nstructures exposed interface. CHAPTER 6. QUEUES 50\nFigure 6.1: Queue mutations CHAPTER 6. QUEUES 51\nDeque\u2019sprovidefrontandbackspecificversionsofcommonqueueoperations,\ne.g. you may want to enqueue an item to the front of the queue rather than\nthe back in which case you would use a method with a name along the lines\nof EnqueueFront. The following list identifies operations that are commonly\nsupported by deque\u2019s:\n\u2022 EnqueueFront\n\u2022 EnqueueBack\n\u2022 DequeueFront\n\u2022 DequeueBack\n\u2022 PeekFront\n\u2022 PeekBack\nFigure 6.2 shows a deque after the invocation of the following methods (in-\norder):\n1. EnqueueBack(12)\n2. EnqueueFront(1)\n3. EnqueueBack(23)\n4. EnqueueFront(908)\n5. DequeueFront()\n6. DequeueBack()\nThe operations have a one-to-one translation in terms of behaviour with\nthose of a normal queue, or priority queue. In some cases the set of algorithms\nthat add an item to the back of the deque may be named as they are with\nnormalqueues,e.g. EnqueueBack maysimplybecalledEnqueue ansoon. Some\nframeworksalsospecifyexplicitbehaviour\u2019sthatdatastructuresmustadhereto.\nThisiscertainlythecasein.NETwheremostcollectionsimplementaninterface\nwhich requires the data structure to expose a standard Add method. In such\na scenario you can safely assume that the Add method will simply enqueue an\nitem to the back of the deque.\nWith respect to algorithmic run time complexities a deque is the same as\na normal queue. That is enqueueing an item to the back of a the queue is\nO(1), additionally enqueuing an item to the front of the queue is also an O(1)\noperation.\nA deque is a wrapper data structure that uses either an array, or a doubly\nlinked list. Using an array as the backing data structure would require the pro-\ngrammer to be explicit about the size of the array up front, this would provide\nanobviousadvantageiftheprogrammercoulddeterministicallystatethemaxi-\nmum number of items the deque would contain at any one time. Unfortunately\nin most cases this doesn\u2019t hold, as a result the backing array will inherently\nincur the expense of invoking a resizing algorithm which would most likely be\nan O(n) operation. Such an approach would also leave the library developer CHAPTER 6. QUEUES 52\nFigure 6.2: Deque data structure after several mutations CHAPTER 6. QUEUES 53\nto look at array minimization techniques as well, it could be that after several\ninvocations of the resizing algorithm and various mutations on the deque later\nthat we have an array taking up a considerable amount of memory yet we are\nonly using a few small percentage of that memory. An algorithm described\nwould also be O(n) yet its invocation would be harder to gauge strategically.\nTo bypass all the aforementioned issues a deque typically uses a doubly\nlinked list as its baking data structure. While a node that has two pointers\nconsumesmorememorythanitsarrayitemcounterpartitmakesredundantthe\nneed for expensive resizing algorithms as the data structure increases in size\ndynamically. With a language that targets a garbage collected virtual machine\nmemory reclamation is an opaque process as the nodes that are no longer ref-\nerenced become unreachable and are thus marked for collection upon the next\ninvocation of the garbage collection algorithm. With C++ or any other lan-\nguagethatusesexplicitmemoryallocationanddeallocationitwillbeuptothe\nprogrammer to decide when the memory that stores the object can be freed.\n6.4 Summary\nWithnormalqueueswehaveseenthatthosewhoarrivefirstaredealtwithfirst;\nthat is they are dealt with in a first-in-first-out (FIFO) order. Queues can be\never so useful; for example the Windows CPU scheduler uses a different queue\nfor each priority of process to determine which should be the next process to\nutilise the CPU for a specified time quantum. Normal queues have constant\ninsertion and deletion run times. Searching a queue is fairly unusual\u2014typically\nyou are only interested in the item at the front of the queue. Despite that,\nsearching is usually exposed on queues and typically the run time is linear.\nIn this chapter we have also seen priority queues where those at the front\nof the queue have the highest priority and those near the back have the lowest.\nOne implementation of a priority queue is to use a heap data structure as its\nbacking store, so the run times for insertion, deletion, and searching are the\nsame as those for a heap (defined in \u00a74).\nQueuesareaverynaturaldatastructure,andwhiletheyarefairlyprimitive\nthey can make many problems a lot simpler. For example the breadth first\nsearch defined in \u00a73.7.4 makes extensive use of queues. Chapter 7\nAVL Tree\nIn the early 60\u2019s G.M. Adelson-Velsky and E.M. Landis invented the first self-\nbalancing binary search tree data structure, calling it AVL Tree.\nAnAVLtreeisabinarysearchtree(BST,definedin\u00a73)withaself-balancing\ncondition stating that the difference between the height of the left and right\nsubtrees cannot be no more than one, see Figure 7.1. This condition, restored\nafter each tree modification, forces the general shape of an AVL tree. Before\ncontinuing, let us focus on why balance is so important. Consider a binary\nsearch tree obtained by starting with an empty tree and inserting some values\nin the following order 1,2,3,4,5.\nThe BST in Figure 7.2 represents the worst case scenario in which the run-\nning time of all common operations such as search, insertion and deletion are\nO(n). By applying a balance condition we ensure that the worst case running\ntime of each common operation is O(log n). The height of an AVL tree with n\nnodes is O(log n) regardless of the order in which values are inserted.\nTheAVLbalancecondition,knownalsoasthenodebalancefactorrepresents\nan additional piece of information stored for each node. This is combined with\na technique that efficiently restores the balance condition for the tree. In an\nAVLtreetheinventorsmakeuseofawell-knowntechniquecalledtreerotation.\nh\nh+1\nFigure 7.1: The left and right subtrees of an AVL tree differ in height by at\nmost 1\n54 CHAPTER 7. AVL TREE 55\n1\n2\n3\n4\n5\nFigure 7.2: Unbalanced binary search tree\n2 4\n1 4 2 5\n3 5 1 3\na) b)\nFigure 7.3: Avl trees, insertion order: -a)1,2,3,4,5 -b)1,5,4,3,2 CHAPTER 7. AVL TREE 56\n7.1 Tree Rotations\nAtreerotationisaconstanttimeoperationonabinarysearchtreethatchanges\ntheshapeofatreewhilepreservingstandardBSTproperties. Thereareleftand\nright rotations both of them decrease the height of a BST by moving smaller\nsubtrees down and larger subtrees up.\n14 8\nRightRotation\n8 24 2 14\nLeftRotation\n2 11 11 24\nFigure 7.4: Tree left and right rotations CHAPTER 7. AVL TREE 57\n1) algorithm LeftRotation(node)\n2) Pre: node.Right !=\u2205\n3) Post: node.Right is the new root of the subtree,\n4) node has become node.Right\u2019s left child and,\n5) BST properties are preserved\n6) RightNode \u2190 node.Right\n7) node.Right \u2190 RightNode.Left\n8) RightNode.Left \u2190 node\n9) end LeftRotation\n1) algorithm RightRotation(node)\n2) Pre: node.Left !=\u2205\n3) Post: node.Left is the new root of the subtree,\n4) node has become node.Left\u2019s right child and,\n5) BST properties are preserved\n6) LeftNode \u2190 node.Left\n7) node.Left \u2190 LeftNode.Right\n8) LeftNode.Right \u2190 node\n9) end RightRotation\nThe right and left rotation algorithms are symmetric. Only pointers are\nchanged by a rotation resulting in an O(1) runtime complexity; the other fields\npresent in the nodes are not changed.\n7.2 Tree Rebalancing\nThe algorithm that we present in this section verifies that the left and right\nsubtrees differ at most in height by 1. If this property is not present then we\nperform the correct rotation.\nNotice that we use two new algorithms that represent double rotations.\nThese algorithms are named LeftAndRightRotation, and RightAndLeftRotation.\nThe algorithms are self documenting in their names, e.g. LeftAndRightRotation\nfirst performs a left rotation and then subsequently a right rotation. CHAPTER 7. AVL TREE 58\n1) algorithm CheckBalance(current)\n2) Pre: current is the node to start from balancing\n3) Post: current height has been updated while tree balance is if needed\n4) restored through rotations\n5) if current.Left =\u2205 and current.Right =\u2205\n6) current.Height = -1;\n7) else\n8) current.Height = Max(Height(current.Left),Height(current.Right)) + 1\n9) end if\n10) if Height(current.Left) - Height(current.Right) >1\n11) if Height(current.Left.Left) - Height(current.Left.Right) >0\n12) RightRotation(current)\n13) else\n14) LeftAndRightRotation(current)\n15) end if\n16) else if Height(current.Left) - Height(current.Right) <\u22121\n17) if Height(current.Right.Left) - Height(current.Right.Right) <0\n18) LeftRotation(current)\n19) else\n20) RightAndLeftRotation(current)\n21) end if\n22) end if\n23) end CheckBalance\n7.3 Insertion\nAVL insertion operates first by inserting the given value the same way as BST\ninsertion and then by applying rebalancing techniques if necessary. The latter\nisonlyperformediftheAVLpropertynolongerholds, thatistheleftandright\nsubtrees height differ by more than 1. Each time we insert a node into an AVL\ntree:\n1. Wegodownthetreetofindthecorrectpointatwhichtoinsertthenode,\nin the same manner as for BST insertion; then\n2. we travel up the tree from the inserted node and check that the node\nbalancing property has not been violated; if the property hasn\u2019t been\nviolated then we need not rebalance the tree, the opposite is true if the\nbalancing property has been violated. CHAPTER 7. AVL TREE 59\n1) algorithm Insert(value)\n2) Pre: value has passed custom type checks for type T\n3) Post: value has been placed in the correct location in the tree\n4) if root =\u2205\n5) root\u2190 node(value)\n6) else\n7) InsertNode(root, value)\n8) end if\n9) end Insert\n1) algorithm InsertNode(current, value)\n2) Pre: current is the node to start from\n3) Post: value has been placed in the correct location in the tree while\n4) preserving tree balance\n5) if value<current.Value\n6) if current.Left =\u2205\n7) current.Left \u2190 node(value)\n8) else\n9) InsertNode(current.Left, value)\n10) end if\n11) else\n12) if current.Right =\u2205\n13) current.Right \u2190 node(value)\n14) else\n15) InsertNode(current.Right, value)\n16) end if\n17) end if\n18) CheckBalance(current)\n19) end InsertNode\n7.4 Deletion\nOurbalancingalgorithmisliketheonepresentedforourBST(definedin\u00a73.3).\nThe major difference is that we have to ensure that the tree still adheres to the\nAVL balance property after the removal of the node. If the tree doesn\u2019t need\nto be rebalanced and the value we are removing is contained within the tree\nthen no further step are required. However, when the value is in the tree and\nits removal upsets the AVL balance property then we must perform the correct\nrotation(s). CHAPTER 7. AVL TREE 60\n1) algorithm Remove(value)\n2) Pre: value is the value of the node to remove, root is the root node\n3) of the Avl\n4) Post: node with value is removed and tree rebalanced if found in which\n5) case yields true, otherwise false\n6) nodeToRemove\u2190root\n7) parent\u2190\u2205\n8) Stackpath\u2190 root\n9) while nodeToRemove(cid:54)=\u2205 and nodeToRemove.Value=Value\n10) parent = nodeToRemove\n11) if value<nodeToRemove.Value\n12) nodeToRemove\u2190 nodeToRemove.Left\n13) else\n14) nodeToRemove\u2190 nodeToRemove.Right\n15) end if\n16) path.Push(nodeToRemove)\n17) end while\n18) if nodeToRemove=\u2205\n19) return false \/\/ value not in Avl\n20) end if\n21) parent\u2190 FindParent(value)\n22) if count=1 \/\/ count keeps track of the # of nodes in the Avl\n23) root\u2190\u2205 \/\/ we are removing the only node in the Avl\n24) else if nodeToRemove.Left =\u2205 and nodeToRemove.Right =null\n25) \/\/ case #1\n26) if nodeToRemove.Value <parent.Value\n27) parent.Left \u2190\u2205\n28) else\n29) parent.Right \u2190\u2205\n30) end if\n31) else if nodeToRemove.Left =\u2205 and nodeToRemove.Right (cid:54)=\u2205\n32) \/\/ case # 2\n33) if nodeToRemove.Value <parent.Value\n34) parent.Left \u2190nodeToRemove.Right\n35) else\n36) parent.Right \u2190nodeToRemove.Right\n37) end if\n38) else if nodeToRemove.Left (cid:54)=\u2205 and nodeToRemove.Right =\u2205\n39) \/\/ case #3\n40) if nodeToRemove.Value <parent.Value\n41) parent.Left \u2190nodeToRemove.Left\n42) else\n43) parent.Right \u2190nodeToRemove.Left\n44) end if\n45) else\n46) \/\/ case #4\n47) largestValue\u2190nodeToRemove.Left\n48) while largestValue.Right (cid:54)=\u2205\n49) \/\/ find the largest value in the left subtree of nodeToRemove\n50) largestValue\u2190largestValue.Right CHAPTER 7. AVL TREE 61\n51) end while\n52) \/\/ set the parents\u2019 Right pointer of largestValue to \u2205\n53) FindParent(largestValue.Value).Right \u2190\u2205\n54) nodeToRemove.Value \u2190largestValue.Value\n55) end if\n56) while path.Count>0\n57) CheckBalance(path.Pop()) \/\/ we trackback to the root node check balance\n58) end while\n59) count\u2190count\u22121\n60) return true\n61) end Remove\n7.5 Summary\nThe AVL tree is a sophisticated self balancing tree. It can be thought of as\nthe smarter, younger brother of the binary search tree. Unlike its older brother\nthe AVL tree avoids worst case linear complexity runtimes for its operations.\nThe AVL tree guarantees via the enforcement of balancing algorithms that the\nleft and right subtrees differ in height by at most 1 which yields at most a\nlogarithmic runtime complexity. Part II\nAlgorithms\n62 Chapter 8\nSorting\nAll the sorting algorithms in this chapter use data structures of a specific type\nto demonstrate sorting, e.g. a 32 bit integer is often used as its associated\noperations (e.g. <, >, etc) are clear in their behaviour.\nThe algorithms discussed can easily be translated into generic sorting algo-\nrithms within your respective language of choice.\n8.1 Bubble Sort\nOne of the most simple forms of sorting is that of comparing each item with\nevery other item in some list, however as the description may imply this form\nof sorting is not particularly effecient O(n2). In it\u2019s most simple form bubble\nsort can be implemented as two loops.\n1) algorithm BubbleSort(list)\n2) Pre: list (cid:54)=\u2205\n3) Post: list has been sorted into values of ascending order\n4) for i\u21900 to list.Count\u22121\n5) for j \u21900 to list.Count\u22121\n6) if list[i]<list[j]\n7) Swap(list[i],list[j])\n8) end if\n9) end for\n10) end for\n11) return list\n12) end BubbleSort\n8.2 Merge Sort\nMerge sort is an algorithm that has a fairly efficient space time complexity -\nO(nlogn)andisfairlytrivialtoimplement. Thealgorithmisbasedonsplitting\nalist,intotwosimilarsizedlists(left,andright)andsortingeachlistandthen\nmerging the sorted lists back together.\nNote: the function MergeOrdered simply takes two ordered lists and makes\nthem one.\n63 CHAPTER 8. SORTING 64\n4 75 74 2 54 4 75 74 2 54 4 74 75 2 54 4 74 2 75 54 4 74 2 54 75\n0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4\n4 74 2 54 75 4 74 2 54 75 4 2 74 54 75 4 2 54 74 75\n0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4\n4 2 54 74 75 2 4 54 74 75 2 4 54 74 75\n0 1 2 3 4 0 1 2 3 4 0 1 2 3 4\n2 4 54 74 75 2 4 54 74 75\n0 1 2 3 4 0 1 2 3 4\n2 4 54 74 75\n0 1 2 3 4\nFigure 8.1: Bubble Sort Iterations\n1) algorithm Mergesort(list)\n2) Pre: list (cid:54)= \u2205\n3) Post: list has been sorted into values of ascending order\n4) if list.Count =1 \/\/ already sorted\n5) return list\n6) end if\n7) m\u2190list.Count \/ 2\n8) left\u2190 list(m)\n9) right\u2190 list(list.Count \u2212 m)\n10) for i\u21900 to left.Count\u22121\n11) left[i] \u2190 list[i]\n12) end for\n13) for i\u21900 to right.Count\u22121\n14) right[i] \u2190 list[i]\n15) end for\n16) left\u2190 Mergesort(left)\n17) right\u2190 Mergesort(right)\n18) return MergeOrdered(left, right)\n19) end Mergesort CHAPTER 8. SORTING 65\n4\n4 4\n2\n75 75\n4\n4\n75\n54\n75\n74\n74\n74 75\n2\n2\n74\n54\n54\n2 2\n74\n54 2 2\n54 54\n5\n4\nDivide Impera(Merge)\nFigure 8.2: Merge Sort Divide et Impera Approach\n8.3 Quick Sort\nQuick sort is one of the most popular sorting algorithms based on divide et\nimperastrategy,resultinginanO(nlog n)complexity. Thealgorithmstartsby\npicking an item, called pivot, and moving all smaller items before it, while all\ngreaterelementsafterit. Thisisthemainquicksortoperation,calledpartition,\nrecursively repeated on lesser and greater sub lists until their size is one or zero\n- in which case the list is implicitly sorted.\nChoosinganappropriatepivot, asforexamplethemedianelementisfunda-\nmental for avoiding the drastically reduced performance of O(n2). CHAPTER 8. SORTING 66\n4 75 74 2 54\nPivot\n4 75 74 2 54\nPivot\n4 54 74 2 75\nPivot\n4 2 74 54 75\nPivot\n4 2 54 74 75\nPivot\n4 2 74 75\nPivot Pivot\n2 4 74 75\nPivot Pivot\n2 4 54 74 75\nFigure 8.3: Quick Sort Example (pivot median strategy)\n1) algorithm QuickSort(list)\n2) Pre: list (cid:54)= \u2205\n3) Post: list has been sorted into values of ascending order\n4) if list.Count =1 \/\/ already sorted\n5) return list\n6) end if\n7) pivot\u2190MedianValue(list)\n8) for i\u21900 to list.Count\u22121\n9) if list[i]=pivot\n10) equal.Insert(list[i])\n11) end if\n12) if list[i]<pivot\n13) less.Insert(list[i])\n14) end if\n15) if list[i]>pivot\n16) greater.Insert(list[i])\n17) end if\n18) end for\n19) return Concatenate(QuickSort(less), equal, QuickSort(greater))\n20) end Quicksort CHAPTER 8. SORTING 67\n8.4 Insertion Sort\nInsertionsortisasomewhatinterestingalgorithmwithanexpensiveruntimeof\nO(n2). It can be best thought of as a sorting scheme similar to that of sorting\na hand of playing cards, i.e. you take one card and then look at the rest with\nthe intent of building up an ordered set of cards in your hand.\n4 75 74\n4 75 74 2 54 4 75 74 2 54 4 75 74 2 54\n2 54\n4 74 75 2 54 2 4 74 75 54 2 4 54 74 75\nFigure 8.4: Insertion Sort Iterations\n1) algorithm Insertionsort(list)\n2) Pre: list (cid:54)= \u2205\n3) Post: list has been sorted into values of ascending order\n4) unsorted\u21901\n5) while unsorted<list.Count\n6) hold\u2190list[unsorted]\n7) i\u2190unsorted\u22121\n8) while i\u22650 and hold<list[i]\n9) list[i+1] \u2190 list[i]\n10) i\u2190i\u22121\n11) end while\n12) list[i+1] \u2190hold\n13) unsorted\u2190unsorted+1\n14) end while\n15) return list\n16) end Insertionsort CHAPTER 8. SORTING 68\n8.5 Shell Sort\nPut simply shell sort can be thought of as a more efficient variation of insertion\nsort as described in \u00a78.4, it achieves this mainly by comparing items of varying\ndistances apart resulting in a run time complexity of O(n log2 n).\nShell sort is fairly straight forward but may seem somewhat confusing at\nfirst as it differs from other sorting algorithms in the way it selects items to\ncompare. Figure 8.5 shows shell sort being ran on an array of integers, the red\ncoloured square is the current value we are holding.\n1) algorithm ShellSort(list)\n2) Pre: list (cid:54)= \u2205\n3) Post: list has been sorted into values of ascending order\n4) increment\u2190list.Count \/ 2\n5) while increment (cid:54)=0\n6) current\u2190increment\n7) while current<list.Count\n8) hold\u2190list[current]\n9) i\u2190current\u2212increment\n10) while i\u22650 and hold<list[i]\n11) list[i+increment] \u2190list[i]\n12) i\u2212=increment\n13) end while\n14) list[i+increment] \u2190hold\n15) current\u2190current+1\n16) end while\n17) increment \/=2\n18) end while\n19) return list\n20) end ShellSort\n8.6 Radix Sort\nUnlike the sorting algorithms described previously radix sort uses buckets to\nsort items, each bucket holds items with a particular property called a key.\nNormally a bucket is a queue, each time radix sort is performed these buckets\nare emptied starting the smallest key bucket to the largest. When looking at\nitemswithinalisttosortwedosobyisolatingaspecifickey,e.g. intheexample\nwe are about to show we have a maximum of three keys for all items, that is\nthe highest key we need to look at is hundreds. Because we are dealing with, in\nthis example base 10 numbers we have at any one point 10 possible key values\n0..9 each of which has their own bucket. Before we show you this first simple\nversion of radix sort let us clarify what we mean by isolating keys. Given the\nnumber 102 if we look at the first key, the ones then we can see we have two of\nthem, progressing to the next key - tens we can see that the number has zero\nof them, finally we can see that the number has a single hundred. The number\nused as an example has in total three keys: CHAPTER 8. SORTING 69\nFigure 8.5: Shell sort CHAPTER 8. SORTING 70\n1. Ones\n2. Tens\n3. Hundreds\nForfurtherclarificationwhatifwewantedtodeterminehowmanythousands\nthe number 102 has? Clearly there are none, but often looking at a number as\nfinal like we often do it is not so obvious so when asked the question how many\nthousands does 102 have you should simply pad the number with a zero in that\nlocation, e.g. 0102 here it is more obvious that the key value at the thousands\nlocation is zero.\nThe last thing to identify before we actually show you a simple implemen-\ntation of radix sort that works on only positive integers, and requires you to\nspecify the maximum key size in the list is that we need a way to isolate a\nspecific key at any one time. The solution is actually very simple, but its not\noften you want to isolate a key in a number so we will spell it out clearly\nhere. A key can be accessed from any integer with the following expression:\nkey \u2190 (number \/ keyToAccess) % 10. As a simple example lets say that we\nwant to access the tens key of the number 1290, the tens column is key 10 and\nso after substitution yields key \u2190 (1290 \/ 10) % 10 = 9. The next key to\nlookatforanumbercanbeattainedbymultiplyingthelastkeybytenworking\nleft to right in a sequential manner. The value of key is used in the following\nalgorithmtoworkouttheindexofanarrayofqueuestoenqueuetheiteminto.\n1) algorithm Radix(list, maxKeySize)\n2) Pre: list (cid:54)=\u2205\n3) maxKeySize\u22650 and represents the largest key size in the list\n4) Post: list has been sorted\n5) queues\u2190 Queue[10]\n6) indexOfKey \u21901\n7) fori\u21900 to maxKeySize\u22121\n8) foreach item in list\n9) queues[GetQueueIndex(item, indexOfKey)].Enqueue(item)\n10) end foreach\n11) list\u2190 CollapseQueues(queues)\n12) ClearQueues(queues)\n13) indexOfKey \u2190indexOfKey\u221710\n14) end for\n15) return list\n16) end Radix\nFigure8.6showsthemembersofqueuesfromthealgorithmdescribedabove\noperating on the list whose members are 90,12,8,791,123, and 61, the key we\nare interested in for each number is highlighted. Omitted queues in Figure 8.6\nmean that they contain no items.\n8.7 Summary\nThroughout this chapter we have seen many different algorithms for sorting\nlists, some are very efficient (e.g. quick sort defined in \u00a78.3), some are not (e.g. CHAPTER 8. SORTING 71\nFigure 8.6: Radix sort base 10 algorithm\nbubble sort defined in \u00a78.1).\nSelectingthecorrectsortingalgorithmisusuallydenotedpurelybyefficiency,\ne.g. you would always choose merge sort over shell sort and so on. There are\nalso other factors to look at though and these are based on the actual imple-\nmentation. Some algorithms are very nicely expressed in a recursive fashion,\nhoweverthesealgorithmsoughttobeprettyefficient,e.g. implementingalinear,\nquadratic, or slower algorithm using recursion would be a very bad idea.\nIf you want to learn more about why you should be very, very careful when\nimplementing recursive algorithms see Appendix C. Chapter 9\nNumeric\nUnless stated otherwise the alias n denotes a standard 32 bit integer.\n9.1 Primality Test\nA simple algorithm that determines whether or not a given integer is a prime\nnumber, e.g. 2, 5, 7, and 13 are all prime numbers, however 6 is not as it can\nbe the result of the product of two numbers that are <6.\n\u221a\nIn an attempt to slow down the inner loop the n is used as the upper\nbound.\n1) algorithm IsPrime(n)\n2) Post: n is determined to be a prime or not\n3) for i\u21902 to n do\n4) for j \u21901 to sqrt(n) do\n5) if i\u2217j =n\n6) return false\n7) end if\n8) end for\n9) end for\n10) end IsPrime\n9.2 Base conversions\nDSA contains a number of algorithms that convert a base 10 number to its\nequivalent binary, octal or hexadecimal form. For example 78 has a binary\n10\nrepresentation of 1001110 .\n2\nTable 9.1 shows the algorithm trace when the number to convert to binary\nis 742 .\n10\n72 CHAPTER 9. NUMERIC 73\n1) algorithm ToBinary(n)\n2) Pre: n\u22650\n3) Post: n has been converted into its base 2 representation\n4) while n>0\n5) list.Add(n % 2)\n6) n\u2190n\/2\n7) end while\n8) return Reverse(list)\n9) end ToBinary\nn list\n742 { 0 }\n371 { 0,1 }\n185 { 0,1,1 }\n92 { 0,1,1,0 }\n46 { 0,1,1,0,1 }\n23 { 0,1,1,0,1,1 }\n11 { 0,1,1,0,1,1,1 }\n5 { 0,1,1,0,1,1,1,1 }\n2 { 0,1,1,0,1,1,1,1,0 }\n1 { 0,1,1,0,1,1,1,1,0,1 }\nTable 9.1: Algorithm trace of ToBinary\n9.3 Attaining the greatest common denomina-\ntor of two numbers\nAfairlyroutineprobleminmathematicsisthatoffindingthegreatestcommon\ndenominatoroftwointegers,whatweareessentiallyafteristhegreatestnumber\nwhich is a multiple of both, e.g. the greatest common denominator of 9, and\n15 is 3. One of the most elegant solutions to this problem is based on Euclid\u2019s\nalgorithm that has a run time complexity of O(n2).\n1) algorithm GreatestCommonDenominator(m, n)\n2) Pre: m and n are integers\n3) Post: the greatest common denominator of the two integers is calculated\n4) if n=0\n5) return m\n6) end if\n7) return GreatestCommonDenominator(n, m % n)\n8) end GreatestCommonDenominator CHAPTER 9. NUMERIC 74\n9.4 Computing the maximum value for a num-\nber of a specific base consisting of N digits\nThis algorithm computes the maximum value of a number for a given number\nof digits, e.g. using the base 10 system the maximum number we can have\nmade up of 4 digits is the number 9999 . Similarly the maximum number that\n10\nconsists of 4 digits for a base 2 number is 1111 which is 15 .\n2 10\nThe expression by which we can compute this maximum value for N digits\nis: BN \u22121. In the previous expression B is the number base, and N is the\nnumberofdigits. Asanexampleifwewantedtodeterminethemaximumvalue\nfor a hexadecimal number (base 16) consisting of 6 digits the expression would\nbe as follows: 166\u22121. The maximum value of the previous example would be\nrepresented as FFFFFF which yields 16777215 .\n16 10\nIn the following algorithm numberBase should be considered restricted to\nthe values of 2, 8, 9, and 16. For this reason in our actual implementation\nnumberBase has an enumeration type. The Base enumeration type is defined\nas:\nBase={Binary \u21902,Octal\u21908,Decimal\u219010,Hexadecimal\u219016}\nThereasonweprovidethedefinitionof Base istogiveyouanideahowthis\nalgorithmcanbemodelledinamorereadablemannerratherthanusingvarious\ncheckstodeterminethecorrectbasetouse. Forourimplementationwecastthe\nvalueofnumberBasetoaninteger,assuchweextractthevalueassociatedwith\nthe relevant option in the Base enumeration. As an example if we were to cast\ntheoptionOctal toanintegerwewouldgetthevalue8. Inthealgorithmlisted\nbelow the cast is implicit so we just use the actual argument numberBase.\n1) algorithm MaxValue(numberBase, n)\n2) Pre: numberBase is the number system to use, n is the number of digits\n3) Post: the maximum value for numberBase consisting of n digits is computed\n4) return Power(numberBase,n) \u22121\n5) end MaxValue\n9.5 Factorial of a number\nAttainingthefactorialofanumberisaprimitivemathematicaloperation. Many\nimplementations of the factorial algorithm are recursive as the problem is re-\ncursive in nature, however here we present an iterative solution. The iterative\nsolution is presented because it too is trivial to implement and doesn\u2019t suffer\nfrom the use of recursion (for more on recursion see \u00a7C).\nThefactorialof0and1is0. Theaforementionedactsasabasecasethatwe\nwill build upon. The factorial of 2 is 2\u2217 the factorial of 1, similarly the factorial\nof 3 is 3\u2217 the factorial of 2 and so on. We can indicate that we are after the\nfactorial of a number using the form N! where N is the number we wish to\nattain the factorial of. Our algorithm doesn\u2019t use such notation but it is handy\nto know. CHAPTER 9. NUMERIC 75\n1) algorithm Factorial(n)\n2) Pre: n\u22650, n is the number to compute the factorial of\n3) Post: the factorial of n is computed\n4) if n<2\n5) return 1\n6) end if\n7) factorial\u21901\n8) for i\u21902 to n\n9) factorial\u2190factorial\u2217i\n10) end for\n11) return factorial\n12) end Factorial\n9.6 Summary\nIn this chapter we have presented several numeric algorithms, most of which\nare simply here because they were fun to design. Perhaps the message that\nthe reader should gain from this chapter is that algorithms can be applied to\nseveral domains to make work in that respective domain attainable. Numeric\nalgorithmsinparticulardrivesomeofthemostadvancedsystemsontheplanet\ncomputing such data as weather forecasts. Chapter 10\nSearching\n10.1 Sequential Search\nA simple algorithm that search for a specific item inside a list. It operates\nlooping on each element O(n) until a match occurs or the end is reached.\n1) algorithm SequentialSearch(list, item)\n2) Pre: list (cid:54)= \u2205\n3) Post: return index of item if found, otherwise \u22121\n4) index\u21900\n5) while index<list.Count and list[index] (cid:54)= item\n6) index\u2190index+1\n7) end while\n8) if index<list.Count and list[index] = item\n9) return index\n10) end if\n11) return \u22121\n12) end SequentialSearch\n10.2 Probability Search\nProbability search is a statistical sequential searching algorithm. In addition to\nsearching for an item, it takes into account its frequency by swapping it with\nit\u2019s predecessor in the list. The algorithm complexity still remains at O(n) but\ninanon-uniformitemssearchthemorefrequentitemsareinthefirstpositions,\nreducing list scanning time.\nFigure 10.1 shows the resulting state of a list after searching for two items,\nnotice how the searched items have had their search probability increased after\neach search operation respectively.\n76 CHAPTER 10. SEARCHING 77\nFigure 10.1: a) Search(12), b) Search(101)\n1) algorithm ProbabilitySearch(list, item)\n2) Pre: list (cid:54)= \u2205\n3) Post: a boolean indicating where the item is found or not;\nin the former case swap founded item with its predecessor\n4) index\u21900\n5) while index<list.Count and list[index] (cid:54)= item\n6) index\u2190index+1\n7) end while\n8) if index\u2265list.Count or list[index] (cid:54)= item\n9) return false\n10) end if\n11) if index>0\n12) Swap(list[index],list[index\u22121])\n13) end if\n14) return true\n15) end ProbabilitySearch\n10.3 Summary\nIn this chapter we have presented a few novel searching algorithms. We have\npresented more efficient searching algorithms earlier on, like for instance the\nlogarithmic searching algorithm that AVL and BST tree\u2019s use (defined in \u00a73.2).\nWe decided not to cover a searching algorithm known as binary chop (another\nname for binary search, binary chop usually refers to its array counterpart) as CHAPTER 10. SEARCHING 78\nthe reader has already seen such an algorithm in \u00a73.\nSearching algorithms and their efficiency largely depends on the underlying\ndata structure being used to store the data. For instance it is quicker to deter-\nminewhetheranitemisinahashtablethanitisanarray,similarlyitisquicker\ntosearchaBSTthanitisalinkedlist. Ifyouaregoingtosearchfordatafairly\noftenthenwestronglyadvisethatyousitdownandresearchthedatastructures\navailable to you. In most cases using a list or any other primarily linear data\nstructure is down to lack of knowledge. Model your data and then research the\ndata structures that best fit your scenario. Chapter 11\nStrings\nStrings have their own chapter in this text purely because string operations\nand transformations are incredibly frequent within programs. The algorithms\npresented are based on problems the authors have come across previously, or\nwere formulated to satisfy curiosity.\n11.1 Reversing the order of words in a sentence\nDefining algorithms for primitive string operations is simple, e.g. extracting a\nsub-string of a string, however some algorithms that require more inventiveness\ncan be a little more tricky.\nThe algorithm presented here does not simply reverse the characters in a\nstring, rather it reverses the order of words within a string. This algorithm\nworks on the principal that words are all delimited by white space, and using a\nfew markers to define where words start and end we can easily reverse them.\n79 CHAPTER 11. STRINGS 80\n1) algorithm ReverseWords(value)\n2) Pre: value (cid:54)=\u2205, sb is a string buffer\n3) Post: the words in value have been reversed\n4) last\u2190value.Length \u2212 1\n5) start\u2190last\n6) while last\u22650\n7) \/\/ skip whitespace\n8) while start\u22650 and value[start] = whitespace\n9) start\u2190start\u22121\n10) end while\n11) last\u2190start\n12) \/\/ march down to the index before the beginning of the word\n13) while start\u22650 and start (cid:54)= whitespace\n14) start\u2190start\u22121\n15) end while\n16) \/\/ append chars from start+1 to length+1 to string buffer sb\n17) for i\u2190start+1 to last\n18) sb.Append(value[i])\n19) end for\n20) \/\/ if this isn\u2019t the last word in the string add some whitespace after the word in the buffer\n21) if start>0\n22) sb.Append(\u2018 \u2019)\n23) end if\n24) last\u2190start\u22121\n25) start\u2190last\n26) end while\n27) \/\/ check if we have added one too many whitespace to sb\n28) if sb[sb.Length \u22121] = whitespace\n29) \/\/ cut the whitespace\n30) sb.Length \u2190sb.Length \u22121\n31) end if\n32) return sb\n33) end ReverseWords\n11.2 Detecting a palindrome\nAlthough not a frequent algorithm that will be applied in real-life scenarios\ndetecting a palindrome is a fun, and as it turns out pretty trivial algorithm to\ndesign.\nThe algorithm that we present has a O(n) run time complexity. Our algo-\nrithmusestwopointersatoppositeendsofstringwearecheckingisapalindrome\nor not. These pointers march in towards each other always checking that each\ncharactertheypointtoisthesamewithrespecttovalue. Figure11.1showsthe\nIsPalindrome algorithminoperationonthestring\u201cWasitEliot\u2019stoiletIsaw?\u201d\nIf you remove all punctuation, and white space from the aforementioned string\nyou will find that it is a valid palindrome. CHAPTER 11. STRINGS 81\nFigure 11.1: left and right pointers marching in towards one another\n1) algorithm IsPalindrome(value)\n2) Pre: value (cid:54)=\u2205\n3) Post: value is determined to be a palindrome or not\n4) word\u2190value.Strip().ToUpperCase()\n5) left\u21900\n6) right\u2190word.Length \u22121\n7) while word[left] =word[right] and left<right\n8) left\u2190left+1\n9) right\u2190right\u22121\n10) end while\n11) return word[left] =word[right]\n12) end IsPalindrome\nIn the IsPalindrome algorithm we call a method by the name of Strip. This\nalgorithmdiscardspunctuationinthestring,includingwhitespace. Asaresult\nword contains a heavily compacted representation of the original string, each\ncharacter of which is in its uppercase representation.\nPalindromesdiscardwhitespace,punctuation,andcasemakingthesechanges\nallowsustodesignasimplealgorithmwhilemakingouralgorithmfairlyrobust\nwith respect to the palindromes it will detect.\n11.3 Counting the number of words in a string\nCountingthenumberofwordsinastringcanseemprettytrivialatfirst,however\nthere are a few cases that we need to be aware of:\n1. tracking when we are in a string\n2. updating the word count at the correct place\n3. skipping white space that delimits the words\nAsanexampleconsiderthestring\u201cBenatehay\u201dClearlythisstringcontains\nthree words, each of which distinguished via white space. All of the previously\nlisted points can be managed by using three variables:\n1. index\n2. wordCount\n3. inWord CHAPTER 11. STRINGS 82\nFigure 11.2: String with three words\nFigure 11.3: String with varying number of white space delimiting the words\nOf the previously listed index keeps track of the current index we are at in\nthestring, wordCountisanintegerthatkeepstrackofthenumberofwordswe\nhave encountered, and finally inWord is a Boolean flag that denotes whether\nor not at the present time we are within a word. If we are not currently hitting\nwhite space we are in a word, the opposite is true if at the present index we are\nhitting white space.\nWhat denotes a word? In our algorithm each word is separated by one or\nmore occurrences of white space. We don\u2019t take into account any particular\nsplitting symbols you may use, e.g. in .NET String.Split1 can take a char (or\narray of characters) that determines a delimiter to use to split the characters\nwithin the string into chunks of strings, resulting in an array of sub-strings.\nInFigure11.2wepresentastringindexedasanarray. Typicallythepattern\nis the same for most words, delimited by a single occurrence of white space.\nFigure 11.3 shows the same string, with the same number of words but with\nvarying white space splitting them.\n1http:\/\/msdn.microsoft.com\/en-us\/library\/system.string.split.aspx CHAPTER 11. STRINGS 83\n1) algorithm WordCount(value)\n2) Pre: value (cid:54)=\u2205\n3) Post: the number of words contained within value is determined\n4) inWord\u2190true\n5) wordCount\u21900\n6) index\u21900\n7) \/\/ skip initial white space\n8) while value[index] = whitespace and index<value.Length \u22121\n9) index\u2190index+1\n10) end while\n11) \/\/ was the string just whitespace?\n12) if index=value.Length and value[index] = whitespace\n13) return 0\n14) end if\n15) while index<value.Length\n16) if value[index] = whitespace\n17) \/\/ skip all whitespace\n18) while value[index] = whitespace and index<value.Length \u22121\n19) index\u2190index+1\n20) end while\n21) inWord\u2190false\n22) wordCount\u2190wordCount+1\n23) else\n24) inWord\u2190true\n25) end if\n26) index\u2190index+1\n27) end while\n28) \/\/ last word may have not been followed by whitespace\n29) if inWord\n30) wordCount\u2190wordCount+1\n31) end if\n32) return wordCount\n33) end WordCount\n11.4 Determining the number of repeated words\nwithin a string\nWith the help of an unordered set, and an algorithm that can split the words\nwithin a string using a specified delimiter this algorithm is straightforward to\nimplement. If we split all the words using a single occurrence of white space\nas our delimiter we get all the words within the string back as elements of\nan array. Then if we iterate through these words adding them to a set which\ncontainsonlyuniquestringswecanattainthenumberofuniquewordsfromthe\nstring. All that is left to do is subtract the unique word count from the total\nnumber of stings contained in the array returned from the split operation. The\nsplit operation that we refer to is the same as that mentioned in \u00a711.3. CHAPTER 11. STRINGS 84\nFigure 11.4: a) Undesired uniques set; b) desired uniques set\n1) algorithm RepeatedWordCount(value)\n2) Pre: value (cid:54)=\u2205\n3) Post: the number of repeated words in value is returned\n4) words\u2190value.Split(\u2019 \u2019)\n5) uniques\u2190 Set\n6) foreach word in words\n7) uniques.Add(word.Strip())\n8) end foreach\n9) return words.Length \u2212uniques.Count\n10) end RepeatedWordCount\nYou will notice in the RepeatedWordCount algorithm that we use the Strip\nmethod we referred to earlier in \u00a711.1. This simply removes any punctuation\nfrom a word. The reason we perform this operation on each word is so that\nwe can build a more accurate unique string collection, e.g. \u201ctest\u201d, and \u201ctest!\u201d\narethesamewordminusthepunctuation. Figure11.4showstheundesiredand\ndesired sets for the unique set respectively.\n11.5 Determining the first matching character\nbetween two strings\nThealgorithmtodeterminewhetheranycharacterofastringmatchesanyofthe\ncharactersinanotherstringisprettytrivial. Putsimply,wecanparsethestrings\nconsidered using a double loop and check, discarding punctuation, the equality\nbetween any characters thus returning a non-negative index that represents the\nlocation of the first character in the match (Figure 11.5); otherwise we return\n-1 if no match occurs. This approach exhibit a run time complexity of O(n2). CHAPTER 11. STRINGS 85\ni i i\nWord t e s t t e s t t e s t\n0 1 2 3 4 0 1 2 3 4 0 1 2 3 4\nindex index index\nMatch p t e r s p t e r s p t e r s\n0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6\na) b) c)\nFigure 11.5: a) First Step; b) Second Step c) Match Occurred\n1) algorithm Any(word,match)\n2) Pre: word,match (cid:54)=\u2205\n3) Post: index representing match location if occured, \u22121 otherwise\n4) for i\u21900 to word.Length\u22121\n5) while word[i] = whitespace\n6) i\u2190i+1\n7) end while\n8) for index\u21900 to match.Length\u22121\n9) while match[index] = whitespace\n10) index\u2190index+1\n11) end while\n12) if match[index] =word[i]\n13) return index\n14) end if\n15) end for\n16) end for\n17) return \u22121\n18) end Any\n11.6 Summary\nWe hope that the reader has seen how fun algorithms on string data types\nare. Strings are probably the most common data type (and data structure -\nrememberwearedealingwithanarray)thatyouwillworkwithsoitsimportant\nthat you learn to be creative with them. We for one find strings fascinating. A\nsimple Google search on string nuances between languages and encodings will\nprovide you with a great number of problems. Now that we have spurred you\nalongalittlewithourintroductoryalgorithmsyoucandevisesomeofyourown. "}