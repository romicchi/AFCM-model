{"text":"Part 1\nGeospatial Data and GPS\n1\nIt\u2019s a Geospatial World Out There\nAn Introduction to Geospatial Technology, Geospatial Data,\nGeospatial Jobs, and Google Earth\nHave you ever done any of the following?\na Used an online mapping service like MapQuest, Google Maps, or Bing\nMaps to find directions (and the best route) to a destination or to print a\nmap of an area?\na Used an in-car navigation system (like a device from companies like\nGarmin, Magellan, or Tom-Tom) to navigate to or from a destination?\na Used a Global Positioning System (GPS) receiver while hiking, jogging,\nhunting, fishing, golfing, or geocaching?\na Used an online resource to find a map of your neighborhood, compare\nnearby housing values, or to see where your property line ends and your\nneighbor\u2019s begins?\na Used a virtual globe program (like Google Earth) or an online map to look\nat photos or images of your home, street, school, or workplace?\nIf so, then congratulations\u2014you\u2019ve used geospatial technology applica-\ntions. Anytime you\u2019re using any sort of technology-assisted information con-\ncerning maps, locations, directions, imagery, or analysis, you\u2019re putting this\nconcept to use. Geospatial technology has become extremely widespread in\nsociety with a multitude of uses in both the private and public sectors. How-\never, more often than not, if you tell someone you\u2019re using geospatial technol-\nogy, you\u2019ll be asked \u201cwhat\u2019s that?\u201d\nWhat Is Geospatial Technology?\nAlthough geospatial technology is being used in numerous fields today, the\nterm \u201cgeospatial technology\u201d doesn\u2019t appear to have penetrated into everyday\nusage, despite the prevalence of the technology itself. Words like \u201csatellite\n1 2\nChapter 1 It\u2019s a Geospatial World Out There\nimages\u201d or \u201cGoogle Earth\u201d and acronyms like \u201cGIS\u201d or \u201cGPS\u201d are growing in-\ngeospatial\ncreasingly commonplace in today\u2019s society, yet the phrase \u201cgeospatial tech-\ntechnology a number\nof different high-tech nology\u201d seems relatively unknown, while incorporating all of these things\nsystems that acquire, and more. Geospatial technology describes the use of a number of different\nanalyze, manage, store, high-tech systems and tools that acquire, analyze, manage, store, or visual-\nor visualize various\nize various types of location-based data. The field of geospatial technology\ntypes of location-based\nencompasses several fields and techniques including:\ndata.\nGeographic a Geographic Information System (GIS): Computer-based mapping,\nInformation System analysis, and retrieval of location-based data.\n(GIS) computer-based\na Remote sensing: Acquisition of data and imagery from the use of satel-\nmapping, analysis, and\nlites (satellite imagery) or aircraft (aerial photography).\nretrieval of location-\nbased data. a Global Positioning System (GPS): Acquiring real-time location infor-\nmation from a series of satellites in Earth\u2019s orbit.\nremote sensing\nacquisition of data and\nThere are numerous related fields that utilize one or more of these types\nimagery from the use\nof technologies. For instance, an in-car navigation system already contains\nof satellites or aircraft.\nextensive road-network data mapped out and ready to use, which includes\nsatellite imagery\ninformation about address ranges, speed limits, road connections, and spe-\ndigital images of\ncial features of roads (such as one-way streets). It also requires the mapping\nthe Earth acquired\nby sensors onboard of points of interest (such as the locations of gas stations or restaurants) and\norbiting spaceborne having a means of referencing new user-defined destinations. It also has to\nplatforms. be able to plot the car\u2019s real-time position in relation to these maps and may\naerial photography even have a feature that shows a representation of the surrounding land-\nthe acquisition of scape as taken from an overhead viewpoint. As such, many of these types of\nimagery of the ground systems combine different geospatial technologies to work together in one\ntaken from an airborne\napplication.\nplatform.\nGlobal Positioning\nSystem (GPS) Who Uses Geospatial Technology?\nacquiring real-time\nlocation information\nGeospatial technology is used in a wide variety of fields (Figure 1.1), includ-\nfrom a series of\nsatellites in Earth orbit. ing federal, state, and local government, forestry, law enforcement, public\nhealth, biology, and environmental studies (see Hands-on Application 1.1:\nI ndustries Using Geospatial Technology for a look at industries employing\npeople in these fields). As long as the job field involves utilizing some sort of\ninformation or data that has a location associated with it, chances are that\nsome sort of geospatial technology is being used. Geospatial technology has\nbeen heralded by the U.S. Department of Labor as one of the main emerging\nand evolving job fields in the United States with enormous growth potential.\nFor instance, the O*NET utility from the Department of Labor contains job\ndescriptions for fields such as \u201cGeospatial Information Scientists and Tech-\nnologists,\u201d \u201cRemote Sensing Scientists and Technologists,\u201d and \u201cPrecision Ag-\nriculture Technicians\u201d (see Hands-on Application 1.2: Jobs in the Geospatial\nField on page 4 for more information about types of jobs). 3\nWho Uses Geospatial Technology?\nFIGURE 1.1 Examples of\ngeospatial technology in\naction on the job. (Source:\n(a) AP Photo\/Wilfredo Lee\n(b) AP Photo\/The Daily Press\/\nAllison Williams (c) AP Photo\/\nGordon Hamilton\/University\nof Maine (d) Bob Nichols\/\nUSDA NRCS (e) AP Photo\/U.S.\nGeological Survey, Dr. Dan\nDzurisin (F) Justin Sullivan\/\nGetty Images)\nHands-on Application 1.1\nIndustries Using Geospatial Technology\nGeospatial technology is being used in a variety of and describes how GIS (and Esri products) are being\ndifferent applications in numerous different fields utilized in them. Examine a few of them that are con-\ntoday. For a deeper look at some of these applica- nected to fields of interest of yours. For instance, if\ntions, open your Web browser and go to http:\/\/www. your interest is in criminal justice, examine some of\nesri.com\/industries.html, this is part of the Esri the \u201cPublic Safety\u201d applications. If you\u2019re involved in\ncompany\u2019s Website (we\u2019ll discuss more about Esri in public or community health, examine some of the\nChapter 5, but the short version is that they\u2019re the \u201cHealth and Human Services\u201d applications, then de-\nmarket leader in GIS). This Website lists dozens of scribe how GIS is being utilized in some real-world,\ndifferent fields that are using geospatial technology on-the-job applications. 4\nChapter 1 It\u2019s a Geospatial World Out There\nHands-on Application 1.2\nJobs in the Geospatial Field\nBusinesses are hiring in the geospatial field. For 6. GIS Lounge: http:\/\/jobs.gislounge.com\nexamples of current job openings, open your Web 7. GIS Connection: http:\/\/www.gisconnection.com\nbrowser and visit some of the following Websites:\nThese are just a sampling of Websites where em-\n1. The GIS Jobs Clearinghouse: http:\/\/www.gjc.org ployers post job openings worldwide. Examine sev-\n2. GIS Jobs.com: http:\/\/www.gisjobs.com eral jobs from areas near where you are (or where\n3. Geosearch: http:\/\/www.geosearch.com you\u2019d like to go to). Describe the various types of\njobs that are advertised, qualifications and skills\n4. Geocommunity (GIS Jobs and Careers): http:\/\/\nemployers are looking for, and salary ranges being\ncareers.geocomm.com\noffered.\n5. GIS Careers: http:\/\/giscareers.com\nThe following are just a handful of examples of fields that utilize geospatial\ntechnology.\nArcheology\nBeing able to pinpoint the location of artifacts uncovered on a dig, construct a\nmap of the area, and then search for patterns on the site are all archeological\ntechniques that can be rendered quickly and efficiently with geospatial tech-\nnology (Figure 1.2). Archeologists can utilize historic maps, current aerial\nphotography or satellite imagery, and location information obtained on the\nsite throughout the course of their work.\nFIGURE 1.2 Using GIS\nfor archeological mapping\nof Clay Tobacco Pipestems\nin Virginia. (Source: Courtesy\nof Colonial National Historical\nPark, National Park Service) 5\nWho Uses Geospatial Technology?\nCity Planning\nUtilities, waste water, green space, traffic, roads, zoning, and housing are all\ntopics that urban planners deal with. Geospatial technology provides a means\nof working with all of these entities together for planning purposes. Strategies\nfor smart urban growth and managing and updating city resources can be ex-\namined through a variety of different applications.\nEnvironmental Monitoring\nProcesses that affect the Earth\u2019s environment in a wide variety of ways can be\ntracked and assessed using geospatial technology. Information about land-\nuse change, pollution, air quality, water quality, and global temperature lev-\nels is vital to environmental research ranging from monitoring harmful algae\nblooms to studies in climate change (see Figure 1.3).\nForestry\nAll manner of forest monitoring, management, and protection can be aided\nthrough the use of geospatial technology. Modeling animal habitats and the\npressures placed upon them, examining the spatial dimensions of forest frag-\nmentation, and managing fires are among the many different ways that geo-\nspatial technology is utilized within the field of forestry (Figure 1.4, page 6).\nHomeland Security\nGeospatial technology is a key component of examining vulnerable areas\nwith regard to homeland security. Risk assessment of everything from\nFIGURE 1.3 Global\ncarbon monoxide\nconcentrations as\nmonitored by NASA\nremote sensing satellites.\n(Source: NASA GSFC Scientific\nVisualization Studio, based on\ndata from MOPITT (Canadian\nSpace Agency and University\nof Toronto)) 6\nChapter 1 It\u2019s a Geospatial World Out There\nFIGURE 1.4 Geospatial\ntechnology used for\nassessing the risk of\nwildfires in Virginia.\n(Source: Courtesy Virginia\nDepartment of Forestry)\nevacuation plans to smoke-plume modeling can be examined using geo-\nspatial technology. Disaster mitigation and recovery efforts can be greatly\nenhanced through the use of current satellite imagery and location-based\ncapabilities.\nLaw Enforcement\nThe locations of various types of crimes can be plotted using GIS. Law enforce-\nment officials can use this information for analysis of patterns (Figure 1.5)\nas well as determining potential new crime areas. Geospatial technology\ncan be used in several other ways beyond mapping and analysis; for in-\nstance, high-resolution aerial photography of locations where police patrol\ncan be gathered to provide further information about potentially danger-\nous areas.\nHealth and Human Services\nGeospatial technology is used for a variety of health-related services. For\nexample, monitoring of diseases, tracking sources of diseases, and mapping\nhealth-related issues (such as the spread of H1N1 or other influenza; see\nFigure 1.6) are all tasks that can be completed using geospatial technology\napplications.\nReal Estate\nThrough geospatial technology, realtors and appraisers (as well as home buy-\ners and sellers) can create and examine maps of homes and quickly compare\nhousing prices and values of nearby or similar properties. Other features of a 7\nWho Uses Geospatial Technology?\nFIGURE 1.5 A GIS map\nshowing crime locations\nin Minneapolis. (Source:\nCourtesy Minneapolis Police\nCrime Analysis Unit)\nFIGURE 1.6 A CDC map\nexamining the spatial\ndistribution of influenza\nactivity. (Source: Center\nfor Disease Control and\nPrevention) 8\nChapter 1 It\u2019s a Geospatial World Out There\nproperty can be examined by viewing high-resolution aerial images, the to-\npography, the terrain, and even where it sits on a floodplain. You can also\nexamine where a property is located in relation to schools, highways, waste-\nwater treatment plants, and other urban features.\nAll of these areas and many, many more are reliant on the capabilities\nof geospatial technology. No matter the field, what distinguishes geospatial\ntechnology from other types of computer systems or technologies is that it\nexplicitly handles geospatial data.\nWhat Is Geospatial Data?\ngeospatial data items Geospatial data (also often referred to as spatial data) refers to location-\nthat are tied to a based data, which is at the heart of geospatial technology applications.\nspecific real-world\nThis ability to assign a location to data is what makes geospatial technolo-\nlocation.\ngy different from other systems. You\u2019re using geospatial concepts anytime\nyou want to know \u201cwhere\u201d something is located. For instance, emergency\ndispatch systems can determine the location you\u2019re calling from if you\nmake a 911 call. They also have access to information concerning where\nthe local fire stations, ambulance facilities, and hospitals are. This type\nof information allows the closest emergency services to be routed to your\nlocation.\nWhen the data you\u2019re using has a location that it can be tied to, you\u2019re\nworking with geospatial data. This isn\u2019t just limited to point locations\u2014the\nlength and dimensions of a hiking trail (and the locations of comfort stations\nalong the trail) are examples of real-world data with a spatial dimension.\nO ther kinds of data, like the boundaries of housing parcels in a subdivision or\na satellite image of the extent of an area impacted by a natural disaster would\nfall under this category. Geospatial technology explicitly handles these types\nof location-based concepts.\nHowever, not all of the data in the world is geospatial data. For in-\nstance, data regarding the location of a residential parcel containing a\nhouse and the parcel\u2019s dimensions on the ground would be spatial informa-\ntion. However, other data such as the names of the occupants, the assessed\nvalue of the house, or the value of the land is not spatial information. A\nnon-spatial data data benefit of using geospatial technology is that this type of non-spatial data\nthat is not directly can be linked to a location. For instance, say you sell a lot of used books on\nlinked to a spatial eBay. You could make a map showing the destinations of each of the pur-\nlocation (such as\nchasers of your online sales\u2014this would be spatial information. You could\ntabular data).\nthen link related non-spatial data (such as the name of the book that was\nsold or the price it was purchased for) to those locations, creating a basic\ndatabase of sales.\nSpatial information can also be gathered about other characteristics, such\nas the landscape, terrain, or land use. Remote sensing provides images of the 9\nWhat Is Geospatial Data?\n1972 1986\n1992 2000\nFIGURE 1.7 Satellite\nimagery (from 1972,\n1986, 1992, and 2000)\nground that serve as a \u201csnapshot\u201d of a particular location at a specific time\nshowing the growth of\nthat can generate other forms of spatial information. For instance, a satellite\nLas Vegas, Nevada over\nimage of Las Vegas, Nevada, can help denote the locations of new housing time. (Source: USGS)\ndevelopments. Examining imagery from several dates can help track the loca-\ntion of where new houses are appearing (Figure 1.7). Similarly, geospatial\ntechnology can be used to determine where, spatially, new urban develop-\nments are most likely to occur, by analyzing different pressures of develop-\nment on these areas.\nMany types of decision making are reliant on useful geospatial infor-\nmation. For instance, when a new library is proposed to be built in an area,\nits choice of location is going to be important\u2014the new site should maxi-\nmize the number of people who can use it but minimize its impact on other\nnearby sites. The same type of spatial thinking applies to the placement of\nnew schools, fire stations, or fast-food chain restaurants. Spatial informa-\ntion fuels other decisions that are location dependent. For instance, historic 10\nChapter 1 It\u2019s a Geospatial World Out There\nHands-on Application 1.3\nMapping Data Layers Online\nOnline mapping applications that combine s everal can zoom to a major city (such as Morgantown), a\ntypes of spatial data into a series of map lay- county, or a particular place. Tools for zooming in\ners are becoming increasingly common. A good, and out of the map (a pair of magnifying glasses),\neasy-to-use example of viewing spatial informa- panning around the map (the hand), getting fur-\ntion online is MapWV, a utility for mapping vari- ther information about objects (the \u201ci\u201d in the circle),\nous layers of data in West Virginia. Open your Web or resetting the view back to the entire state (the\nbrowser and go to http:\/\/www.mapwv.gov and globe) are available on the toolbar in the middle\nthen select the option for Create an Advanced Map. of the screen. Examine various map layers for Mor-\nNumerous l ayers are available for you to create a gantown, West Virginia, and see how they fit to-\nmap from\u2014locations of cities and roads, tax district gether in a spatial context (such as the hydrology\nboundaries, historic places, shaded relief of the data and the shaded relief or the aerial photogra-\nt opography, and crisp aerial photographs. Select phy with the historic locations).\nthe layers you want to map in West Virginia. You\npreservation efforts seek to defend historic Civil War battlefield areas from\nurban development. Knowing the location of the parcels of land that are\nunder the greatest development pressure helps in channeling preservation\ne fforts to have their greatest impact. With more and more geospatial technol-\nogy applications becoming available, these types of geospatial problems are\nbeing taken into consideration (see Hands-on Application 1.3: Mapping Data\nLayers Online and Hands-on Application 1.4: Examining Real Estate Values On-\nline for examples online).\nHands-on Application 1.4\nExamining Real Estate Values Online\nZillow is an online tool used to examine hous- residential neighborhoods, and you\u2019ll see markers\ning prices in cities across the United States. You in yellow (recently sold homes) and red (homes for\ncan examine the prices of houses for sale in the sale). Selecting a marker will give you the sale (or\nneighborhood and see\u2014spatially\u2014where housing sold) price, along with the estimated value. When\nprices change. In other words, Zillow provides on- you\u2019re using this tool, you\u2019re selecting a location\nline mapping related to real estate prices and ap- and examining the attributes (items like housing\npraisal. To check it out, open your Web browser price or other characteristics) that accompany that\nand visit http:\/\/www.zillow.com. Start with the place. Check your own local area for housing and\n\u201cFind Homes\u201d search options, looking for houses examine local housing prices and their estimated\nin Virginia Beach, Virginia. Zoom in to some of the values. 11\nWhat Is Google Earth?\nWhat Is Google Earth?\nAn example of a widespread geospatial technology application is the popular\nGoogle Earth software program. Google Earth is a virtual globe\u2014a program Google Earth a freely\nthat provides an interactive three-dimensional (3D) representation of the available virtual globe\nEarth. Google Earth brings together several aspects of geospatial technology program first released\nin 2005 by Google.\ninto one easy-to-use program. You can examine satellite imagery or aerial pho-\ntography of the entire globe, zoom from Earth orbit to a single street address, virtual globe a\nsoftware program that\nnavigate through 3D cities or landscapes, measure global distances, and ex-\nprovides an interactive\namine multiple layers of spatial data together, all with an intuitive interface.\nthree-dimensional map\nIn many ways, the coolness and simplicity of use of Google Earth makes it feel\nof the Earth.\nlike a video game of geospatial technology, but it\u2019s far more than that. Google\nEarth is able to handle vast amounts of geospatial data swiftly and easily, and\nenables users to specify and create their own location-based spatial data (see\nFigure 1.8 for a look at Google Earth and some of its layers).\nBefore Google Earth, there was Keyhole\u2019s \u201cEarth Viewer,\u201d a program\nthat enabled you to fly across a virtual Earth, zooming in and out of loca-\ntions and imagery. Google purchased the Keyhole company and the first\nversion of Google Earth was released in 2005. Google Earth is freely avail-\nable for download off the Web and relies on a high-speed Internet connec-\ntion to send the data from Google servers to your computer. For instance,\nwhen you use Google Earth to fly from space to your house, the imagery you\nsee on the screen is being streamed across the Internet to you. The imagery\non Google Earth is \u201ccanned,\u201d that is to say, it\u2019s fixed from one moment in\ntime, rather than being a live view. For instance, if the imagery of your house\nused by Google was obtained in 2005, then that\u2019s the snapshot in time you\u2019ll\nbe viewing, not a current image of your house. However, Google Earth has\nFIGURE 1.8 A view\nof downtown Boston,\nMassachusetts from\nGoogle Earth. (Source: Gray\nBuildings \uf6d9 2008 Sanborn,\n\uf6d9 2010 Google) 12\nChapter 1 It\u2019s a Geospatial World Out There\nThinking Critically with Geospatial Technology 1.1\nWhat Happens to Privacy In a Geospatial World?\nThink about this type of scenario\u2014if you own proper- such as a close-up picture of your house taken from\nty, then that information is part of the public record the street. All of this is free, publicly available infor-\nwith the county auditor\u2019s office and likely available mation that can be had with just a few clicks of a\nonline. Someone can type your address into an on- mouse. Anonymity about where you live or work is\nline GIS (or mapping Website such as MapQuest or suddenly becoming a thing of the past. What does\nGoogle Maps) and get a map showing how to travel the widespread availability (and a ccess to) this\nto your property (house or business). In addition, type of spatial information mean about privacy? If\nthe dimensions, boundaries, and a high-resolution anyone with an Internet connection can obtain a\nimage of your property can be easily o btained. It detailed image of the house you live in, has your\nis possible that other information can be acquired, personal privacy been invaded?\na feature so that you can examine past images. For instance, if imagery of\nyour town was available in 2006, 2008, and today, you can choose which of\nthese \u201ccanned\u201d images you want to view. Many other data layers have been\nupd ated and are constantly being added, such as 3D buildings or other\nlocation-based application layers.\nThe Geospatial Lab Application: Introduction to Geospatial Concepts and\nGoogle Earth that accompanies this chapter walks you through several differ-\nent usages of Google Earth. It assumes no prior experience with Google Earth,\nbut even veteran users may find some new tricks or twists in how geospa-\ntial data is handled. You\u2019ll use a variety of Google Earth tools to investigate\nseveral of the avenues this book will explore, from aerial image interpreta-\ntion, to 3D terrain and objects, to determining the shortest path between two\ndestinations. Before you start using Google Earth, Hands-on Application 1.5:\nThe Google Earth Plug-In will show you some of its very cool features and ap-\nplications just using your Web browser.\nHands-on Application 1.5\nThe Google Earth Plug-In\nThe Google Earth Plug-in is a special add-on that The Google Earth Website also features several\nwill install into your Web browser and allow you examples of how the plug-in is used for a variety\nto utilize some Google Earth features via the Web. of geospatial Web applications. For instance, the\nOpen your Web browser and go to http:\/\/www. \u201cSports Stadiums\u201d link allows you to zoom from\ngoogle.com\/earth\/explore\/products\/plugin.html space to a variety of 3D versions of stadiums across\nand follow the instructions for installing the Google the United States, while the \u201cOil Spill\u201d link allows you\nEarth Plug-in for your Web browser. After installa- to interactively view the extent of the 2010 Deep-\ntion, you\u2019ll see a preview window that will allow water Horizon oil spill in the Gulf of Mexico. Check\nyou to rotate, zoom, and tilt a virtual globe of the out all of the above and also explore more examples\nEarth. of Google Earth to see its geospatial applications. 13\nChapter Wrapup\nWhat\u2019s All This Have to do with Geography?\nThe first sentence for the entry \u201cgeography\u201d in The Dictionary of Human Ge-\nography describes geography as \u201cthe study of the Earth\u2019s surface as the space\nwithin which the human population lives.\u201d To simplify a definition further into\nwords of one syllable, geography is not just \u201cwhere things are\u201d but the study of\n\u201cwhy things are where they are.\u201d Geography deals with concepts of the spatial\ncharacteristics of our planet and the spatial relationships and interactions of\nthe people and features that populate it. The notion of \u201cGeographic Informa-\ntion Science\u201d has been acclaimed for years as the study of the multiple con-\ncepts that surround the handling of spatial data, and today the use of geospa-\ntial technologies is tightly interwoven with the discipline of geography.\nAt the higher education level, courses in GIS, remote sensing, GPS, and\nvarious applications of these technologies are key components of a Geog-\nraphy curriculum. Certificate programs in Geographic Information Science\n(GISci) or Geospatial Technologies are also becoming increasingly common\nto be offered by colleges and universities. Geospatial degree programs at the\nbachelors, masters, or doctoral level have been developed at numerous uni-\nversities (see Chapter 15 for more information about geospatial technology in\neducation). Even in the basic usages of collecting data in the field for a class\nproject or producing a map of results, geospatial technologies have become an\nintegral part of the geography discipline.\nIn a 2004 article, Jack Dangermond, CEO of the Environmental Sys-\ntems Research Institute, Inc. (Esri; see Chapter 5), heralded GIS as a way of\n\u201cSpeaking the Language of Geography\u201d and in a 2009 article noted that GIS\nwas seen as \u201cGeography in Action.\u201d Both of these are great descriptions\u2014for\nexample, when you use an in-vehicle navigation system to navigate your way\nthrough unfamiliar territory or use an online tool to examine housing values\nfor your neighborhood, you\u2019re using basic geographic concepts of space and\nplace through technology. Geospatial technology gives us the tools to more\neasily apply location-based principles to real-world situations. For example,\nhow is the new freeway bypass going to affect the local community regarding\ntraffic flows, new commercial development, and individual properties? Or\nwhat are the potential impacts of the site chosen for a new casino develop-\nment in relation to the land use, economic and social effects, and the draw of\ngamers from far outside the local area? Throughout this book, you\u2019ll be deal-\ning with geospatial technology concepts and applications, and using some\nsort of geospatial data, or measuring spatial characteristics, or examining\nspatial relations.\nChapter Wrapup\nSo what are you waiting for? Move on to this chapter\u2019s lab (see Geospatial Lab\nApplication: Introduction to Geospatial Concepts and Google Earth) and start\ngetting to work. This chapter\u2019s lab will have you doing all sorts of things with 14\nChapter 1 It\u2019s a Geospatial World Out There\nGoogle Earth, while touching on aspects of all of this book\u2019s chapters. There\nare also questions throughout the lab for you to answer based on the tasks\nyou\u2019ll be doing.\nImportant note: The references for this chapter are part of the online com-\npanion for this book and can be found at http:\/\/www.whfreeman.com\/\nshellito1e.\nKey Terms\ngeospatial technology (p. 2) Global Positioning System (GPS)\nGeographic Information System (p. 2)\n(GIS) (p. 2) geospatial data (p. 8)\nremote sensing (p. 2) non-spatial data (p. 8)\nsatellite imagery (p. 2) Google Earth (p. 11)\naerial photography (p. 2) virtual globe (p. 11) 1.1\nGeospatial Lab Application\nIntroduction to Geospatial Concepts\nand Google Earth\nThis chapter\u2019s Geospatial Lab Application will introduce you to some basic\nconcepts of geospatial technology through the use of the Google Earth soft-\nware program. This lab also provides an introduction on how to use Google\nEarth and will help familiarize you with many of its features. Although the\nGeospatial Lab Applications in later chapters will use a variety of software\npackages, you\u2019ll also be using Google Earth in many of them. The investiga-\ntions in this introductory lab may seem pretty basic, but labs in later chapters\nwill be more in-depth and build on concepts learned here.\nObjectives\nThe goals for you to take away from this lab are:\na Familiarizing yourself with the Google Earth (GE) environment and basic\nfunctionality and navigation using the software.\na Using different GE layers and features.\nObtaining Software\nGoogle Earth (the current version, 6.0) is available for free download at\nhttp:\/\/earth.google.com.\nImportant note: Software and online resources sometimes change fast.\nThis lab was designed with the most recently available version of the soft-\nware at the time of writing. However, if the software or Websites have signifi-\ncantly changed between then and now, an updated version of this lab (using\nthe newest versions) is available online at http:\/\/www.whfreeman.com\/\nshellito1e.\nLab Data\nThere is no data to copy in this lab. All data comes as part of the GE data\nthat is installed with the software or is streamed across the Internet when\nusing GE.\nLocalizing This Lab\nAlthough this lab visits popular locations in a tour around South Dakota, the\ntechniques it uses can be easily adapted to any locale. Rather than using GE\nimagery and features of South Dakota, find nearby landmarks or popular\nspots and use GE to tour around your local area.\n15 16\nChapter 1 It\u2019s a Geospatial World Out There\n1.1 Starting to Use Google Earth\n1. Start GE (the default install folder is called Google Earth). GE will\nusually open with a startup tip box, which you can close. Also, make sure\nthe GE window is maximized, or else some features may not show up\nproperly.\nSearch box\nPlaces box\nLayers box\n(Source: \uf6d9 2011 Europa Technologies, US Dept of State Geographer, \uf6d9 East View Map Link, \uf6d9 Google)\nA common use of GE is to view the ground (whether physical landscapes\nor structures) from above by using the aerial photography or satellite\nimagery streamed in from Google\u2019s servers. The Fly To option in the\nSearch Box is used for this.\n2. Select the Fly To tab and then type in \u201cMitchell, SD.\u201d Next, click on\nthe Begin Search button (the magnifying glass next to where you\ntype).\n3. GE will rotate and zoom the view down to eastern South Dakota. One\nof the popular attractions in Mitchell is the Corn Palace, an event center\nthat, each year, remakes the fa\u00e7ade of the building into a new theme\nusing corn. More information about the Corn Palace is available at\nhttp:\/\/www.cornpalace.com\/index.php.\n4. In the Layers Box, place a checkmark in the Photos box. This enables\nlinking locations on the ground to photos that users have taken. The\nlocations that have linked photos appear as blue squares over top of the\nimagery. You\u2019ll see numerous photo locations clustered together in one\nplace\u2014that would be the Corn Palace. Double-click on some of them to\nsee some photos (taken from the ground) of the Corn Palace. 17\nIntroduction to Geospatial Concepts and Google Earth\n5. In GE, you can rotate and zoom your view to get a closer look at the\nimagery on the ground. Move the mouse to the upper right-hand side of\nthe screen and a set of controls will appear. These fade out when not in\nuse and reappear when the mouse rolls over them.\n6. There are five controls. The first one (the ring with the N at the top)\nallows you to change the direction being faced. Grab the N with the\nmouse (by placing the pointer on N and holding down the left mouse\nbutton) and drag the N (that is, slowly rotate it) around the ring. (This\nis commonly called \u201cgrab and drag.\u201d) You\u2019ll see the view change (north\nwill still be the direction in which the N is pointed). Double-clicking on\nthe N will return the imagery so that the north direction is facing the top\nof the screen.\n7. The second control is the Look control (the arrows surrounding the\neye). By selecting one of the directional arrows and pressing it with\nthe mouse, you can tilt your view around the terrain as if you were\nstanding still and looking about. You can also grab the control and move\nit (by holding down the left button on your mouse), simulating looking\naround in multiple directions.\n8. The third control is the Move control (the arrows surrounding the\nhand). By grabbing this control, you can pan the imagery around in\nwhichever direction you choose.\n9. The fourth control is the Street View icon (the yellow figure of a\nhuman\u2014referred to as the \u201cpegman\u201d\u2014standing on a green circle). This\nicon will appear when Google Street View imagery is available to see on\nthe ground. We\u2019ll spend more time on Google Street View in Chapter 8,\nas it\u2019s a utility that allows you to see what places look like as if you were\nstanding on the street in front of them. To use Street View, you would\ngrab the icon from the controls with the mouse and drag it to a street\nthat\u2019s visible in the view to enter the Street View mode. This control will\nonly be visible if there are streets in the view that you can enter Street\nView mode with (which is why you won\u2019t initially see it when you start\nGE and are looking at the entire planet).\n10. The last control is the Zoom Slider. By pushing forward (moving the\nbar toward the plus sign), you zoom closer, and by pulling backwards\n(moving the bar toward the minus sign), you zoom further out. By\ndefault, when GE zooms in, it tilts the terrain to give you a perspective\nview, rather than looking from the top directly down. To always use the\ntop-down option rather than tilting the ground when zooming, select\nthe Tools pull-down menu and choose Options. In the dialog box that\nappears, click on the Navigation tab, and then select the radio button\nthat says Do not automatically tilt when zooming. To have GE tilt\nwhile zooming, select one of the other radio buttons.\n11. You can also use the mouse for navigating around GE. Grab the imagery\nand drag it to pan across the map. To zoom in and out, roll the mouse 18\nChapter 1 It\u2019s a Geospatial World Out There\nwheel back and forth. Pan and zoom over to the location of the Corn\nPalace. To confirm that you\u2019re in the right place, type the address of\nthe Corn Palace into the Fly To box as follows: 604 North Main Street,\nMitchell, SD 57301. (GE will automatically adjust to that location.)\n12. For now, turn off the Photos options (remove the checkmark from the\nbox).\n13. Zoom in on the Corn Palace so that it fills most of the view and you can\nmake out some details about it.\n14. Grab the Street View icon from the controls and drag it to the street\nright in front of the Corn Palace. You\u2019ll see the street turn blue (this\nindicates that Street View imagery is available for this street). Release\nthe icon on the street and the view will shift from an aerial view to\nimagery that looks like you\u2019re actually standing on the road in front\nof the Corn Palace.\nGrab this\nStreet View\nicon\nPlace the\nicon here\n(Source: Image \uf6d9 2011 Digital Globe, \uf6d9 2011 Google)\n15. In this Street View, use the mouse to pan around the image (you can see\n360 degrees around your position) until you can see the entrance of the\nCorn Palace. You can pan and zoom with the mouse as necessary (and also\nclick on the road\u2014or elsewhere in the image\u2014to advance your view).\n16. Examine the Corn Palace and its surrounding area. Then answer\nQuestion 1.1. To return to the aerial view, click on the Exit Street View\ntext in the upper right corner of the Street View. We\u2019ll do more with\nStreet View (and street networks) in Chapter 8.\nQuestion 1.1 By being able to view the Corn Palace from above and from\nstanding on the ground in front of it, what details from the aerial view can 19\nIntroduction to Geospatial Concepts and Google Earth\nhelp identify just what the building is? (You may need to switch back and\nforth between the aerial view and the street view to answer this question.)\n17. With a number of remote sensing concepts, it\u2019s important to think of\nviewing the ground from above and that many features you can view from\na street level are difficult to perceive from an aerial view. Chapter 9 delves\nfurther into interpreting aerial images and some strategies and tips for\ndoing so, while Chapter 10 deals with how this type of remotely sensed\nimagery is acquired. Chapters 11 and 12 explore obtaining imagery from\nvarious satellites.\n18. Here\u2019s another way of looking at the Corn Palace\u2014a 3D representation.\nIn the GE Layers options, select 3D Buildings. A 3D representation of\nthe Corn Palace will appear over top of the imagery of the building. Use\nthe zoom and tilt controls and navigate around the building to get a\nbetter look at the 3D model of the Corn Palace, including its fa\u00e7ade and\nturrets. Turn off the option for 3D Buildings when you\u2019re done.\n19. There are numerous buildings and objects that have 3D representations\ndesigned for use in GE. In Chapter 14, you\u2019ll investigate more about\nworking with 3D versions of geospatial data, as well as constructing your\nown 3D buildings and viewing them in GE.\n1.2 Finding Your Way Around with Google Earth\n1. From the Layers Box, turn on the Roads layer. You\u2019ll see major roads\n(interstates, state highways) and their labels appear, as well as local\nroads when you\u2019re zoomed in close.\n2. We\u2019re going to leave the Corn Palace behind and continue west through\nSouth Dakota to Wall Drug, a famous tourist location. More information\non Wall Drug can be found at http:\/\/www.walldrug.com.\n3. In the Search Box, click on the Directions tab.\n4. In the From option, type in the Corn Palace\u2019s address as follows: 604 N\nMain St, Mitchell, SD 57301.\n5. In the To option, type in Wall Drug\u2019s address as follows: 510 Main Street,\nWall, SD 57790. 20\nChapter 1 It\u2019s a Geospatial World Out There\n6. Click the Begin Search button. GE will zoom out to show you the path\n(in purple) it has calculated for driving distance between the Corn\nPalace and Wall Drug, while the Search Box will fill with directions\nfeaturing stops and turns along the way. At the bottom of the Search Box\nwill be a checkbox marked \u201cRoute.\u201d By turning this on and off, you can\nview the route and the roads underneath it.\nQuestion 1.2 By viewing the expanse of South Dakota between Mitchell\nand Wall, there are many possible roads between the two. Why do you\nsuppose GE chose this particular path to go from the Corn Palace to Wall\nDrug?\nQuestion 1.3 Based on the route that GE calculated, what is the driving\ndistance (and approximate time equivalent) to get from the Corn Palace to\nWall Drug?\n7. This capability to take a bunch of letters and numbers and turn them\ninto a mapped location, as well as being able to calculate the shortest\ndriving distance between points is further developed in Chapter 8.\nMatching addresses and calculating shortest paths are key usages of\ngeospatial technologies, and the Chapter 8 lab will go into more depth\non how these types of operations are performed.\n8. Also at the bottom of the Search Box is the Play Tour button. Click the\nPlay Tour button. This will allow you to follow the path that GE has\ncalculated for you.\nPlay tour\n9. A new set of controls will appear in the lower left corner of the view\n(if they don\u2019t, move your mouse over to that part of the view). These\nallow you to pause, fast-forward, rewind, and save the path that\u2019s been\ncalculated.\n10. Follow the path from Mitchell to Wall. It\u2019ll take a long time to watch the\nwhole tour, so when you\u2019re ready, double-click on the final destination\nin the Search Box (that is, the \u201cArrive at: 510 Main Street\u201d option)\nand you\u2019ll jump ahead to the end of the path\u2014Wall Drug.\n1.3 Using Google Earth to Examine Landscapes\n1. There\u2019s a lot more to the South Dakota landscape than tourist\nattractions. Wall Drug is located in the town of Wall, named for the line\nof magnificent land formations nearby, and also lies directly adjacent\nto Badlands National Park. The Badlands are located to the south and\nsouthwest of Wall. For more information about the Badlands visit\nhttp:\/\/www.nps.gov\/badl. 21\nIntroduction to Geospatial Concepts and Google Earth\n2. To see the boundaries of Badlands National Park in Google Earth, go to\nthe Layers Box and expand the option for More (click on the plus button\nto the left of the name). In the options that appear under the expanded\nheading, put a checkmark in the Parks\/Recreation Areas option.\nZoom out from the boundaries of Wall and (to the south) you\u2019ll see the\nnorthernmost boundary of Badlands National Park highlighted in green.\n3. Zoom out so that you can see the entire park in the view. New icons for\nthe locations of Visitors Centers and other features will also appear.\n4. Pan over to the eastern edge of the park and you will see a large\nQuestion Mark icon indicating a park entrance as well as a green arrow\nindicating an overlook.\nOverlook\n(Source: Image USDA Farm Service Agency, Image \uf6d9 2011 Digital Globe, \uf6d9 2011 Google)\n5. Zoom in to the point representing the overlook. At the bottom of the\nview you\u2019ll see numbers representing the latitude and longitude of the\npoint, as well as the real-world elevation of that spot.\nQuestion 1.4 What is the elevation for this particular overlook in Badlands\nNational Park?\n6. The imagery in GE is placed over top of a model of the Earth\u2019s terrain\nand landscape (hills, valleys, ridges, and so on). It\u2019s very difficult to\nmake out the pseudo three-dimensional (3D) nature of the terrain\nmodel from above, so use the Zoom Slider to tilt the view down so\nthat you can look around as if you were standing on the overlook point\n(in a perspective view of the planet). Once you\u2019ve tilted all the way\ndown, use the Look controls to examine the landscape. From here, 22\nChapter 1 It\u2019s a Geospatial World Out There\nuse the Move controls to \u201cfly\u201d over the Badlands from this overlook\npoint. When you\u2019re down done cruising around the Badlands, answer\nQuestion 1.5.\nQuestion 1.5 How does the terrain modeling (with the tilt function) aid\nin the visualization of the Badlands?\n7. This ability to model the peaks and valleys of the landscape with aerial\nimagery \u201cdraped\u201d or \u201cstretched\u201d over the terrain for a more realistic\nappearance is often used with many aspects of geospatial technology.\nChapter 13 greatly expands on this by getting into how this is actually\nperformed as well as doing some hands-on terrain analysis.\n1.4 Using Google Earth to Save Imagery\nIt\u2019s time to continue on the next leg of our South Dakota journey by heading\nto Mount Rushmore. Carved out of the side of a mountain in the Black Hills,\nMount Rushmore is a monument featuring the faces of presidents George\nWashington, Thomas Jefferson, Theodore Roosevelt, and Abraham Lincoln.\nFor more information about Mount Rushmore visit http:\/\/www.nps.gov\/\nmoru.\n1. In GE\u2019s Fly To box, type \u201cMount Rushmore.\u201d GE will zoom around to an\noverhead view of the monument. Like the Badlands, Mount Rushmore is\na national park\u2014zoom out a little bit until you can see the extent of the\npark\u2019s boundaries (still outlined in green).\n2. To begin, you can save an image of what\u2019s being shown in the view. GE\nwill take a \u201csnapshot\u201d and save it as a JPEG graphic file (.jpg), which is\nlike a digital camera picture. Position the view to see the full outlined\nextent of Mount Rushmore, select the File pull-down menu, then select\nSave, and finally select Save Image. Name the image \u201cmoru\u201d (GE will\nautomatically add the .jpg file extension) and save it to your computer\u2019s\nhard drive.\n3. Minimize GE for now and go to the location on your computer where\nyou saved the image, and then open it to examine it (for instance, with\nMicrosoft Office Picture Manager).\nQuestion 1.6 Note that even though the graphic contains the locations\nof Mount Rushmore, the outline of the park, and latitude\/longitude and\nelevation information at the bottom, it doesn\u2019t have any spatial reference for\nmeasurements. Why is this?\n4. Close the image and return to Google Earth. Even though the saved\nimage doesn\u2019t have any spatial reference, Chapter 3 describes how to\ntake unreferenced imagery (or other unreferenced data) and transform\nit to match referenced data. In Chapter 3\u2019s lab, you do this in a hands-on\nfashion. You can also turn off the Parks\/Recreation layer for now. 23\nIntroduction to Geospatial Concepts and Google Earth\n1.5 Using Google Earth to Make Placemarks and\nMeasurements\nWhile you\u2019re examining Mount Rushmore imagery, you can set up some\npoints of reference to return to. GE allows you to create points of reference\nas Placemarks. There are three points to mark on the map\u2014the top of the\nmountain, the amphitheater, and the parking area (see the graphic below for\ntheir locations).\nTop of mountain\nAmphitheater\nParking area\n(Source: \uf8e9 2010 Google, Image \uf8e9 2010 Digital Globe)\n1. From the GE toolbar, select the Add Placemark button:\n2. A yellow pushpin (labeled \u201cUntitled Placemark\u201d) will appear on the\nscreen. Using your mouse, click on the pushpin and drag it to the rear\nof the amphitheater so that the pin of the placemark is where the path\nmeets the amphitheater.\n3. In the Google Earth New Placemark dialog box, type the name \u201cMount\nRushmore Amphitheater.\u201d\n4. By pressing the Placemark icon button next to where you typed the\nname, you can select a different icon rather than the yellow pushpin.\nChoose something more distinctive. 24\nChapter 1 It\u2019s a Geospatial World Out There\nChoose new\nicon\n5. Click OK to close the dialog box.\n6. Repeat the process by putting a new placemark at the top of the\nmountain where the monument is. Name this new placemark \u201cTop of\nthe Monument.\u201d\n7. Zoom GE\u2019s view in tightly, so you can prominently see the two\nplacemarks in the view (with the two placemarks in diagonally opposite\ncorners of the view).\n8. Next, select the Show Ruler button from the GE toolbar:\n9. In the Ruler dialog box that appears, select Feet from the Length pull-\ndown menu.\n10. Use the options under the Line tab. This will let you compute the\ndistance between two points. The Path option will let you find the total\naccumulated distance between more than two points.\n11. Click the mouse on the point of the placemark in the amphitheater and\ndrag it to the placemark on top of the mountain (your heading should\nbe about 310 degrees or so). Click the mouse at the top of the mountain\nplacemark.\n12. Answer Question 1.7. When you\u2019re done, click on Clear in the Ruler\ndialog box to remove the drawn line from the screen, and then close the\nRuler dialog box. 25\nIntroduction to Geospatial Concepts and Google Earth\nQuestion 1.7 What is the measured distance between the rear of the\namphitheater and the top of the monument (keep in mind this is the ground\ndistance, not a straight line between the two points)?\n13. This ability to create points of reference (as well as lines and area\nshapes) and then compute distance between them is a very basic tool\nof GIS and can be used as the basis for multiple types of analysis.\nChapter 5 introduces GIS concepts, Chapter 6 expands on how spatial\nanalysis can be performed with GIS, and Chapter 7 demonstrates how\nto take your data and create a professional quality map of the results.\n14. Use the tilt functions of GE to get a perspective view on Mount\nRushmore (as you did with the Badlands) and take a look at the\nmountain from a pseudo-3D view. While you can see the height and\ndimensions of the mountain, the famous four presidents\u2019 faces can\u2019t be\nseen, despite zooming or rotating the model.\nQuestion 1.8 Even with the terrain turned on and the view switching over\nto a perspective, why can\u2019t the presidents\u2019 faces on the side of the monument\nbe seen?\n1.6 Using Google Earth to Examine Coordinates\n1. You\u2019ll notice at the bottom of the GE screen a set of coordinates for\nlat (Latitude) and lon (Longitude) next to the elevation value. Move\nthe mouse around the screen and you\u2019ll see the coordinates change\nto reflect the latitude and longitude of whatever the mouse\u2019s current\nlocation is.\n2. Zoom in closely to the road that enters the parking structure area of\nMount Rushmore.\nQuestion 1.9 What are the latitude and longitude coordinates of the\nentrance to the Mount Rushmore parking area?\n3. You can also reference specific locations on the Earth\u2019s surface by their\ncoordinates instead of by name. In the Fly To box type the following\ncoordinates: 43.836584, -103.623403. GE will rotate and zoom to\nthis new location. Turn on the Photos to obtain more information on\nwhat you\u2019ve just flown to. You can also turn on the 3D Buildings layer\nto look at the location in a pseudo-3D view. Answer Questions 1.10 and\n1.11, and then turn off the Photos (and the 3D Buildings option) when\nyou\u2019re done.\nQuestion 1.10 What is located at the following geographic coordinates:\nlatitude 43.836584, longitude -103.623403?\nQuestion 1.11 What is located at the following geographic coordinates:\nlatitude 43.845709, longitude -103.563499? 26\nChapter 1 It\u2019s a Geospatial World Out There\nChapter 2 deals with numerous concepts related to coordinate systems, ref-\nerence systems, and projections of a three-dimensional Earth onto a two-\ndimensional surface.\n1.7 Other Features of Google Earth\nAs noted before, you will be using GE for several other labs in this book and\nexploring some of these applications of geospatial technology. GE has a mul-\ntitude of other features beyond those covered in this first lab. For instance, if\nyou had a GPS receiver (say, one from Garmin or Magellan) you could import\nthe data you\u2019ve collected directly into GE.\n1. From the Tools pull-down menu, select GPS. You\u2019ll see several options\nfor offloading information you collected in the field (such as waypoints)\nfrom the receiver into GE.\nChapter 4 will introduce you to several concepts related to GPS, as well as col-\nlecting data in the field and some pre-field work-planning options.\n2. Close the GPS Import dialog box now (unless you have a receiver you\nwant to plug into the computer on which you can view your data in GE).\nThere are many other GE layers and features to explore, especially in relation\nto how geospatial data is visualized. To look at one of these features, return to\nthe Badlands once more and do the following:\n3. In the Layers box, expand the options for Gallery, and then place a\ncheckmark in the box next to 360 Cities.\n4. Fly back to the Badlands and scroll to the western edge of the park\nand you\u2019ll see another symbol on the map\u2014a red circle marked \u201c360.\u201d\nHover your cursor over it and the text should read \u201cMako Sica aka The\nBadlands.\u201d\n5. Double-click on the \u201c360\u201d circle. You will zoom down to see a sphere on\nthe surface, and then the view should dive inside the sphere. The view\nwill change to a new view of the Badlands (use the mouse to move about\nand look around a full 360 degrees). In essence, this tool simulates what\nthe Badlands would look like from that position, all around you. You\ncan look in any direction, as well as zooming in or out of places. Answer\nQuestion 1.12. When you\u2019re done looking around, click on the \u201cExit\nPhoto\u201d button in the view to return to regular GE. Chapter 15\nwill present more information about some other cool visualization\ntechniques for geospatial data.\nQuestion 1.12 How does the 360-degree image of the Badlands aid in\nvisualizing the scenery (keep in mind this image has been tied to a particular\nlocation)?\nGeospatial technology is such a rapidly changing field that new advances\ncome quickly. Chapter 15 also explores some of these current developments\non the frontiers of geospatial technology. 27\nIntroduction to Geospatial Concepts and Google Earth\nGoogle Earth is always changing, and new versions or upgrades are con-\nstantly being made available by Google for this program. Some of the newer\noptions you may want to explore are Google Mars, Google Sky, and Flight\nSimulator.\nGoogle Mars\nFrom the View pull-down menu, select Explore, and then select Mars. The\nview will shift to the familiar-looking globe, but this time covered with imag-\nery (from NASA and the USGS) from the surface of Mars. The usual controls\nwork the same way as they do with GE and there\u2019s a lot of Martian territory\nto be explored. When you\u2019re ready to return to GE, from the View pull-down\nmenu, select Explore, and then select Earth.\nGoogle Sky\nFrom the View pull-down menu, select Explore, and then select Sky. GE\u2019s view\nwill change from instead of looking down on Earth, you\u2019re looking up to the\nstars\u2014and instead of remotely sensed aerial or satellite imagery, you\u2019re looking\nat space-telescope (including the Hubble) imagery. There\u2019s plenty of imagery\nto be explored in this new mode. When you want to get back \u201cdown to Earth\u201d\nagain, select the View pull-down menu, select Explore, and then select Earth.\nFlight Simulator\nFrom the Tools pull-down menu, select Enter Flight Simulator. A new dia-\nlog box will appear asking if you want to pilot an F-16 fighter jet or an SR-22\npropeller airplane. Select whichever plane you want to fly and GE will switch\nto the view as seen out the cockpit of your chosen plane and you can fly across\nSouth Dakota (and the rest of GE). Try not to crash (although you can easily\nreset if you do).\n(Source: Image \uf8e9 2011 Digital Globe, Image USDA Farm Service Agency) 28\nChapter 1 It\u2019s a Geospatial World Out There\nClosing Time\nThis introductory lab was pretty straightforward but served to introduce you\nto the basic functions of Google Earth, as well as providing examples of many\ngeospatial concepts you\u2019ll be working with over the course of this book. Chap-\nter 2 starts looking at location concepts (like various coordinate systems) and\nreturns to Google Earth to start applying some of the topics you\u2019ve worked\nwith in this chapter. You\u2019re all set, and now you can exit GE by selecting Exit\nfrom the File pull-down menu. There\u2019s no need to save any data (or Tempo-\nrary Places) in this lab. 2\nWhere in the Geospatial\nWorld Are You?\nLocations in a Digital World, Position Measurements, Datums,\nCoordinate Systems, GCS, Map Projections, UTM, and SPCS\nThink about this\u2014you\u2019re at the mall and your cell phone rings. It\u2019s a friend call-\ning you with a really basic question: \u201cWhere are you?\u201d For such a simple ques-\ntion, there\u2019s really a wide variety of answers. You could say \u201cI\u2019m at the mall,\u201d\nwhich gives the other person some basic location information, but only provid-\ning they know where the mall is. If they\u2019re calling from out of town and unfa-\nmiliar with the location of the mall, what you\u2019ve told them is useless for them to\nknow where you are. If you tell them \u201cI\u2019m in Columbus,\u201d then they have another\ntype of information, but Columbus is a big city, and they still don\u2019t know where\nyou are in the city. You could give them the name of the cross streets by the mall,\nthe address of the mall, or the name of the store you\u2019re in at the mall, and even\nthough all of the above contain different levels of information, your friend still\nwon\u2019t have the information needed to know precisely where you are.\nFor instance, the answer of \u201cI\u2019m at the mall\u201d effectively conveys your\nl ocation only if the person you\u2019re talking to has some sort of knowledge of\nthe mall\u2019s location. If she has a reference like \u201cthe mall\u2019s on the north side of\ntown\u201d or \u201cthe mall is just off exit 5 on the freeway,\u201d then she has some idea\nof your location. For the location information to make sense, she needs to\nhave something (perhaps a map of the city you\u2019re in) to reference the data to.\nIf you need specific information about your exact location, you\u2019d have to be\nmuch more specific than saying you\u2019re at the mall. For example, in the case\nof a medical emergency, Emergency Medical Technicians (EMTs) would need\nto know your exact location (they couldn\u2019t afford to waste time searching the\nentire mall to find you). In this particular case, you would need to provide\nprecise location information so that the EMTs would be able to find you, no\nmatter where you are.\nGeospatial technology works the same way. First, since it deals with\ngeospatial data (that is linked to non-spatial data), there has to be a way of\n2299 30\nChapter 2 Where in the Geospatial World Are You?\nassigning location information to the data being handled. Every location on\nEarth has to be referenced in some way. In other words, every location has to\nbe identified and measured. Second, these measurements require some sort\nof reference system so that locations at one point on Earth can be compared\nwith locations at another area. Third, since we\u2019re dealing with points on a\nthree-dimensional (3D) Earth but we only have a two-dimensional (2D) sur-\nface to examine (like a map), there has to be a way of translating real-world\ndata to an environment we can use. This chapter examines how these con-\ncepts are treated in all forms of geospatial technology. If you\u2019re going to be\nable to precisely pin down the coordinates of a location, you\u2019ll need to first\nhave a datum and a coordinate system.\nWhat Is a Datum?\ndatum a reference A datum is a reference surface, or model of Earth that is used for plotting\nsurface of Earth. l ocations across the globe. The datum represents the size and shape of Earth,\nellipsoid a model of which, contrary to popular belief, isn\u2019t perfectly round. Earth is actually an\nthe rounded shape ellipsoid (or spheroid) shape, meaning that it\u2019s larger at its center than it is\nof Earth. at its poles. Think of taking a basketball and squeezing the top and bottom of\ngeoid a model of Earth the ball to slightly compress it and you\u2019ll have a basic spheroid shape. How-\nusing mean sea level ever, the real form of Earth isn\u2019t a smooth ellipsoid\u2014g ravitational forces af-\nas a base. fect different parts of Earth in different ways, causing some areas to not be\ngeodesy the science in synch with the ellipsoid. Another model of Earth, called a geoid, places\nof measuring Earth\u2019s Earth\u2019s surface at mean sea level to try and account for these differences\nshape. (see Figure 2.1 for how the geoid and the ellipsoid stack up against each\nother as well as the variable topography of Earth). The science of measur-\ning Earth\u2019s shape to develop these kinds of models and reference surfaces is\nreferred to as geodesy.\nDeveloping a datum means creating a mathematical model to reference\nlocations and coordinates against. Thus, when you\u2019re plotting a point, you\nhave a reference to measure this location to. The difficulty with datums in\nFIGURE 2.1 How the ellipsoid geoid Earth\nreal-world Earth, the\nellipsoid, and the geoid\nmatch up with each other. 31\nWhat Is a Geographic Coordinate System?\nmapping arises because there isn\u2019t just one datum to use for all measurements\nof Earth\u2019s locations. In actuality, there are hundreds of datums being used.\nSome datums are global and measure the entire world, while some are more\nlocalized to a particular continent or region. Some of the more common da-\ntums you\u2019ll likely encounter with geospatial data are:\na NAD27: The North American Datum of 1927. This datum was developed NAD27 the North\nfor measurements of the United States and North America. It has its cen- American Datum of\n1927.\nter point positioned at Meades Ranch in Kansas.\na NAD83: The North American Datum of 1983. This datum was developed NAD83 the North\nAmerican Datum of\nby the National Geodetic Survey and Canadian agencies and is used as\n1983.\nthe datum for much data for the United States and the North American\ncontinent as a whole. WGS84 the World\nGeodetic System of\na WGS84: The World Geodetic System of 1984. This datum was developed\n1984 datum (used with\nby the U.S. Department of Defense and is used by the Global Positioning the Global Positioning\nSystem (GPS) for locating points worldwide on Earth\u2019s surface (see Chap- System).\nter 4 for more information about GPS).\nMeasurements made with one datum don\u2019t necessarily line up with mea-\nsurements made from a second datum. Problems would arise if you take one\nset of data measured in one datum and your friend takes a second set of data\nmeasured from a different datum and you place both sets of data together.\nYou have data of the same place, but your datasets wouldn\u2019t match up because\ndatum transformation\nyou\u2019re both using different forms of reference for measurement. For instance, changing\ndata measured from the NAD27 datum could be off by a couple hundred measurements\nmeters from data measured from the NAD83 datum. This problem is com- from one datum to\nmeasurements in\npounded with the availability of several local datums, or reference surfaces,\nanother datum.\ndesigned to better fit a smaller geographic area. With the measurement dif-\nferences that may arise, the best strategy to use when dealing with geospatial geographic\ncoordinate system\ndata is to keep everything in the same datum. A datum transformation is\n(GCS) a set of global\nrequired to alter the measurements from one datum to another (for instance,\nlatitude and longitude\nchanging the measurements made in NAD27 to NAD83). Datum transforma- measurements used as\ntion is a computational process, but a standard one in many geospatial soft- a reference system for\nware packages. A datum can be used to set up a geographic coordinate system finding locations.\n(GCS) for measuring coordinates. latitude imaginary\nlines on a globe north\nand south of the\nWhat Is a Geographic Coordinate System? Equator that serve as a\nbasis of measurement\nin GCS.\nA geographic coordinate system (GCS) is a global reference system for de-\nEquator the line of\ntermining the exact position of a point on Earth (see Figure 2.2 on page 32\nlatitude that runs\nfor an example of GCS). GCS consists of lines of latitude and longitude that\naround the center of\ncover the entire planet and are used to find a position. Lines of latitude (also Earth and serves as the\nreferred to as parallels) run in an east-to-west direction around the globe. The 0 degree line to make\nEquator serves as the starting point of zero, with measurements north of the latitude measurements\nfrom.\nEquator being numbers read as north latitude and measurements south of 32\nChapter 2 Where in the Geospatial World Are You?\nFIGURE 2.2 The\ngeographic coordinate\nsystem (GCS) covers the\nglobe and locations are\nmeasured using latitude\nand longitude. (Source:\n\u00a9 2010 Google. Data SIO,\nNOAA, U.S. Navy, NGA, GEBCO.\nImage IBCAO. Image \u00a9 2010\nDigital Globe. Image \u00a9 2010\nTerraMetrics.)\nthe Equator being numbers read as south latitude. Lines of longitude (also\nlongitude imaginary\ncalled meridians) run north-to-south from the North Pole to the South Pole.\nlines on a globe east\nand west of the The Prime Meridian (the line of longitude that runs through Greenwich,\nPrime Meridian that England) serves as the starting point of zero. Measurements made east of\nserve as a basis of the Prime Meridian are numbers read as east longitude, while measurements\nmeasurement in GCS.\nmade west of the Prime Meridian are numbers read as west longitude. The\nPrime Meridian the breaking point between east and west longitude is the 180th meridian, also\nline of longitude referred to as the International Date Line.\nthat runs through\nUsing this system (where measurements are made north or south of the\nGreenwich, England, and\nEquator and east or west of the Prime Meridian), any point on the globe can\nserves as the 0 degree\nline of longitude to base be located. GCS measurements are not made in feet or meters or miles, but\nmeasurements from. rather they are in degrees, minutes, and seconds (DMS). A degree is a unit\nof measurement that can be broken into 60 subsections (each one referred to\nInternational Date\nLine a line of longitude as a minute of distance). A minute can be subsequently broken down into 60\nthat follows a similar subsections (each one referred to as a second of distance). A single minute of\npath as the 180th latitude is equivalent to roughly one nautical mile (or about 1.15 miles). How-\nmeridian (but changes\never, the distance between degrees of longitude varies across the globe as lines\naway from a straight\nof longitude are closer together at the poles and further apart at the Equator.\nline to accommodate\ngeography). When GCS coordinates are being written for a location, they are in terms\nof how far the distance (that is, in terms of degrees, minutes, and seconds)\ndegrees, minutes, and\nis from the Equator and the Prime Meridian. Latitude values run from 0 to\nseconds (DMS) the\nmeasurement system 90 degrees north and south of the Equator, while longitude values run from\nused in GCS. 0 to 180 east and west of the Prime Meridian. For example, the GCS coordi-\nnates of a spot at the Mount Rushmore National Monument are 43 degrees,\n52 minutes, 53.38 seconds north of the Equator (43\u00b052\u203253.38\u2033 N latitude) 33\nWhat Is a Geographic Coordinate System?\nand 103 degrees, 27 minutes, 56.46 seconds west of the Prime Meridian\n(103\u00b027\u203256.46\u2033 W longitude).\nNegative values can also be used when expressing coordinates using lati-\ntude and longitude. When distance measurements are made west of the Prime\nMeridian, they are often listed with a negative sign to note the direction. For\ninstance, the location at Mount Rushmore, being west longitude, could also be\nwritten as (cid:2)103\u00b027\u203256.46\u2033 longitude. The same use of negative values holds\ntrue when making measurements of south latitude (below the Equator).\nGCS coordinates can also be expressed in their decimal equivalent, re-\nferred to as decimal degrees (DD). In the decimal degrees method, values decimal degrees\nfor minutes and seconds are converted to their fractional number of degrees. (DD) the fractional\ndecimal equivalent\nFor instance, 1 degree and 30 minutes of latitude (in DMS) would equate to\nto coordinates found\n1.5 degrees in DD (since 30 minutes is equal to 0.5 degrees). Using the Mount\nusing degrees, minutes,\nRushmore coordinates as an example, the latitude measurement in decimal\nand seconds.\ndegrees would be 43.881494 north latitude (since 52 minutes and 53.38 sec-\nonds is equal to 0.881494 of one degree) and 103.465683 west longitude.\nUsing this system, any point on Earth\u2019s surface can be precisely mea-\nsured and identified. Keep in mind that these locations are being made on a\n3D globe (in reference to a datum), so measurements between points have to\ntake this into account. When making measurements on a sphere, the shortest\ndistance between two points is referred to as the great circle distance. Thus, great circle distance\nif you wanted to find the shortest distance between the cities of New York and the shortest distance\nParis, you would have to calculate the great circle distance between two sets between two points on\na spherical surface.\nof coordinates (see Hands-on Application 2.1: Great Circle Distance Calcula-\ntions for more information about using the great circle distance). time zones a method\nThe latitude and longitude system of GCS also serves as the basis for break- of measuring time\naround the world, found\ning Earth up into multiple time zones to account for difference in time around\nby dividing the world\nthe world. Time zones set up the concept of a 24-hour day, with each zone hav-\ninto subdivisions and\ning a different time as it relates to the current time in Greenwich, England (also relating the time in that\ncalled Greenwich Mean Time or GMT). The Earth spans 360 degrees of lon- division to the time in\ngitude, and dividing that by 24 hours equals 15 degrees of longitude for each Greenwich, England.\nhour. Thus, every 15 degrees away from the time zone containing the Prime\nMeridian marks the start of a new time zone (Figure 2.3 on pages 34\u201335).\nHands-on Application 2.1\nGreat Circle Distance Calculations\nTo find the real-world distance around the planet find the real-world distance between New York and\nbetween two cities\u2014New York and Paris\u2014you\u2019ll Paris. Use it to find the city closest to you and cal-\nneed the great circle distance between the two. culate the great circle distance between sets of cit-\nTo quickly calculate great circle distances, use the ies. Also, use a program like Google Earth to obtain\nSurface Distance application online at http:\/\/www. the GCS coordinates (latitude and longitude) for a\nchemical-ecology.net\/java\/lat-long.htm. Use the nearby location and find the surface distance from\noption for Great Circle Distance Between Cities to there to a nearby city. 34\nChapter 2 Where in the Geospatial World Are You?\nFIGURE 2.3 The world\ntime zones, as separated\n-12 -11 -10 -9 -8 -7 -6 -5 -4 -3 -2 -1\nby lines of longitude (and\ngeographic boundaries).\n165\u00b0W 150\u00b0 135\u00b0 120\u00b0 105\u00b0 90\u00b0 75\u00b0 60\u00b0 45\u00b0 30\u00b0 15\u00b0W\nARCTIC OCEAN\nGreenland\n75\u00b0 (Denmark)\nI Dn ate tern La it ni eonal Beaufort Sea Bain Bay -3\nRUSSIA\nARCTIC C\nMo Sn ud na dy ay -9 ICELAND\n60\u00b0 Hudson\nAB le er uin tig\na\nnS Ie sla ands CANADA Bay -4Labrador Sea\nIRELAND\n-3.5\n-5 Newfoundland\n45\u00b0 -8 -7 -6\nNORTH PACIFIC UNITED STATES NORTH ATLANTIC PORTUGAL\nOCEAN OCEAN\nMORO\n30\u00b0\nGulf of\nTROPIC OF CANCER MEXICO Mexico BAHAMAS TROPIC OF CANCER W Se as ht ae rr an\nCUBA DOMINICAN (Mor.)\n-10 HAITI REPUBLIC MAURITAN\n15\u00b0N BEL. HJA OM N. D. Caribbean CAPE VERDE SENEGAL\nGUAT EE LM SA AL LA VADOR NICARAGUA Sea TRINIDAD AND TOBAGO GUINEA-BISG SA AM UBIA GUINEA\nCOSTA RICA VENEZUELA GUYANA SIERRA LEONE D\u2019C IV\u00d4 O\nKIRI IB nA DtT\ne\naI\nr tn ea Lti io nn\ne- a1\nl2M on daySu n\nday\nEQUATOR PANAMA\nECUAD\n-OC 5ROLOMBIA\nSUR BINA RME\nAZF G Ir ue i Lan nch a (Fr.) EQUATOR LIBER EI QA UATG OH RA I\nSAMOA PERU -4\n1\nF\nTI5\nOJI\nN\u00b0S\nGA\nCook\nIslands\nTROPIC OF CAPRICORN\nBOLIVI PA\nARAGUAY\n-3\nTROPIC OF CA\n-10\n30\u00b0\nURUGUAY SOUTH ATLA\nCHILE OCEAN\nSOUTH PACIFIC ARGENTINA\nOCEAN\n45\u00b0\nFalkland\nIslands (U.K.)\nScotia Sea\n60\u00b0\n165\u00b0W 150\u00b0 135\u00b0 120\u00b0 105\u00b0 90\u00b0 75\u00b0 60\u00b0 45\u00b0 30\u00b0 15\u00b0W\nSunday\n00:00 1:00 2:00 3:00 4:00 5:00 6:00 7:00 8:00 9:00 10:00 11:00\nSOUTHERN OCEAN\nyadnoM\nlanoitanretnI\nyadnuS\neniL\netaD 35\nWhat Is a Geographic Coordinate System?\n0 +1 +2 +3 +4 +5 +6 +7 +8 +9 +10 +11 +12\n0\u00b0 15\u00b0E 30\u00b0 45\u00b0 60\u00b0 75\u00b0 90\u00b0 105\u00b0 120\u00b0 135\u00b0 150\u00b0 165\u00b0E 180\u00b0\nARCTIC OCEAN\nFranz Josef Land Severnaya\n(Russia) Zemlya\nSvalbard (North Land)\n(Norway)\nBarents Sea\nNovaya\nZemlya\nK Sa er aa\nLaptev Sea\nNew Siberian Islands\nEast Siberian Sea\n75\u00b0\nNorwegian Sea\nCIRCLE +11\n+9 +10 +12\nSWEDENFINLAND RUSSIA\nNORWAY +3\n+5 +7 60\u00b0\nN So er ath LAE TS VT IO ANIA +4 +8\nSea of\nBe Sr ei ang\nUNITED DENMARK L RI UT SH SU IAANIA Okhotsk\nO IC\nA\nBK S\nC\nUIN P\nO\nRM0AG KID AN INAO LF AM LR IGALN U N EBE X S C RET W. EL BIH A. NI ETG. NIZE\nG\nITR . NEUM RC NIA TR IN A SMC OA ILY SZ AO.U Y LLE NS I.C B. CB +TP H .H A. HS YO H. 1L LU AL A.O BSNA DV .E GN . R. R M RD O B EAM . EB CB CA .E U U EL N\nS\n+EK LA M I UGA .RR\nC\n2O DT YU YA L AU PS PD LI\nI.\nSN NTEO R RBVE K ..A E EY\nJO\nRS IRY\nTA\nDR\nRR AG\nAI\nEAM SE\nN\nARI\nAO E\nR\nANR\nU\nDA+ I BG\nY\nJA\nD\nIQ\nEI BIA A4\nQAI\nM\nOZ\nA\nEK\nUE\nT\nNU\nTR\nA\nIWB R+A AI\nUT\nBI IRJ 3\nTAU\nA\n.AANR\nH. .N\nEK\n5\nR\n.\nOM\nA\nMU IE NZ +AN AB NIK S 4E rAT SA K\naF\n+AI eZ bS GN 3T\naH\niA\nP\naA\nA\nA.K nN\n5N\nKH\nI IS\nSTS A\nT\nTT K\nA\nAJIA Y K+\nN\nNR IN S IG6 TY A NZ N +ST 5DAN\n.5IANEP BBA BAL eaB\nN\nnyH\nG\ng\nU\nL\noT\naA\nfA\nM\nlDN\nE YASH\nNC\nMA\nTH\nR\nHM\n+\nA\nCIO\nLI\nL\nA8\nA\nAN\nMN\nO\nNG\nBS\nD\nOO\nA\nDL IVI AA\nIET\nS\nCN\no\nhA\nu\niM\nnt ah\nT PAI HW\nIC\nA\nLE S PNh\nIS aK\nPe\nhiO O\ns\nnN PK\na\nR\niU R\nt\nSlyOO\na\nIiT E\nNu\np\neRR\nkAH\ny\nEET\np\nau\nAH\nSI is n.\neJAPA TN\nROP +IC 1OF\n0\nCKu Aril\nN\nI Cs.\nER\nPN OAO\nC\nM\nIC\nA\nSER\nR\nLI\nASAFT\nH NA\nDI\nNH\nL\nSC\nL\n1534 \u00b005 N\u00b0\n\u00b0\nA\u00d4 IO NT A SI AE LR\n\u00c3\nE G\nO\nPU RTIN\n\u00cdO\nNE MT CAO\nI\u00c9\nP\nG\nA\nEO NN DIGE CR GA AMIA BE OR NO CO ON N RG\nCD\nOEO\nPE\nNC .M. GA\nO.\nO. FR.\nRW\nBASS UNUO RDDU U UAAT G TNH N A ADN NID ZE A AKT NEH IN AYIO APIA S +OM 3ALIA\nSEYCHELLES\nMALDIVESSRI LANKA MA SIL NA GAYB PSR OIU A RS N EEe Ia\nIND TO IMON\nR\nLF EE EP SD A T+E L ESR A 9A UT IED\nA\nSTATES GPE O\nUA\nNQF\nP\nI\nU EM\nNU\nWAI EC T AR AO O RNESI SA\nIO SLL AO NM DO\nSNNAUR+ U12\nKIR TI UB\nV0\nA\nA\u00b0\nT LUI\nAPRICORN\nNA AN MG IBO IL AA BOTZ SA WZM\nI\nAMB NBIA AABWM EA MOZL AMA BIW QI UE MAC DO AM GO AR SO CS\nAR\nMAURITIUSINDIAN OCEAN\nTROPIC OF CAPRICORN\nTimor Sea Coral Sea VANUATU 1 F5 IJI\u00b0S\nAUSTRALIA +11\nSOUTH SWAZILAND +9.5 30\u00b0\nNTIC AFRICA LESOTHO\nTasman NEW\nSea ZEALAND\n45\u00b0\n60\u00b0\n0\u00b0 15\u00b0E 30\u00b0 45\u00b0 60\u00b0 75\u00b0 90\u00b0 105\u00b0 120\u00b0 135\u00b0 150\u00b0 165\u00b0E 180\u00b0\nSunday\nSunday\n12:00 13:00 14:00 15:00 16:00 17:00 18:00 19:00 20:00 21:00 22:00 23:00 24:00\nNAIDIREM\nHCIWNEERG\nNAIDIREM\nHCIWNEERG lanoitanretnI\neniL\netaD 36\nChapter 2 Where in the Geospatial World Are You?\nThinking Critically with Geospatial Technology 2.1\nDo You Really Need Printed Maps in a Digital World?\nLet\u2019s face it, these days we\u2019re living in a digital where you want to go? In the same vein, are paper\nworld\u2014road maps, road directions, and property map collections at universities or libraries relics of\nmaps are all available with a few clicks of a mouse in the past or still useful resources? When would hav-\ndigital form. With everything available digitally (and ing access to such a collection be necessary?\nmore portable and accessible with wireless Internet When driving a long distance is it necessary to\naccess via laptops and phones), is there a need for have a printed map of the area you\u2019re traveling\nprinted maps? Do you really need to carry around a through when you\u2019ve got GPS, satellite maps, and\npaper map version of the road network in Los Angeles, Internet mapping capabilities? If yes, what\u2019s a situa-\nCalifornia, or will a mapping app on your smartphone tion in which it would be necessary, and if no, why\nsuffice to find where your location is and get you are p rinted maps still available?\nFrom 7.5 degrees west longitude to 7.5 degrees east longitude is one time zone,\n7.5 to 22.5 degrees longitude is a second time zone, and so on. Following these\nsections of longitude around the world, the east coast of the United States is five\ntime zones to the west of the Prime Meridian, thus making the time in places\nlike New York or Boston five hours behind the time in Greenwich. America\u2019s\nwest coast lies in the region eight time zones to the west of the Prime Meridian\n(and three to the west of the United States east coast), thus making the time in\nSan Francisco eight hours behind Greenwich (or three hours behind New York).\nHowever, take a closer look at Figure 2.3 and you\u2019ll see that time zones\ndon\u2019t exactly follow the lines of longitude. Some political or geographic\nboundaries cause some areas to entirely be in one time zone when the area\nmay straddle two zones, or some areas may be in one zone or another de-\nspite how the lines of longitude lie. Other countries, such as China, adopt one\ntime measurement for the entire country, despite China\u2019s geography crossing\nmultiple time zones. The International Date Line marks the division between\n24-hour periods and splits one day from another, but it also bends to accom-\nmodate some geography and islands to be in one time zone or another, rather\nthan strictly follow the line of the 180th meridian.\nHow Can Real-World Data Be Translated\nonto a Two-Dimensional Surface?\nAll of these measurements are being done on a three-dimensional world\nmap projection the\nbut are somehow being translated onto a two-dimensional surface (like a\ntranslation of locations\non a three-dimensional piece of paper or a computer screen) for ease of use. A map projection is a\n(3D) Earth to a two- translation of coordinates and locations from their places in the real world\ndimensional (2D) to a flat surface, and it is necessary to be able to use this geospatial data.\nsurface.\nThe biggest problem with a map projection is that not everything is going 37\nHow Can Real-World Data Be Translated onto a Two-Dimensional Surface?\nFIGURE 2.4 The\nMercator projection, which\nretains the shapes of\ncountries and continents,\nbut greatly warps their\nsizes.\nto perfectly translate from a three-dimensional object to a two-dimensional\nsurface. In fact, the only representation of the world that would accurately\ncapture all features would be a globe. Since it\u2019s hard to keep a globe in the\nglove compartment of the car, we\u2019re going to have to make due with a flat\nsurface representation of the world (like a folded map) and realize that the\nmap is going to have some sort of distortions built into it, simply because it\u2019s\na two-dimensional version of a three-dimensional construct. One or more of\nthe following will be distorted in a map projection: the shape of objects, the\nsize of objects, the distance between objects, or the direction that objects are\nin. Some map projections may retain the shape of continents but warp their\nsize. Other projections may keep the land area correct but throw the shapes\nout of place.\nAn example of this is the Mercator projection (Figure 2.4), a common\ntype of map that was developed in 1569 for use as a navigation aid\u2014it was\ndesigned so that every straight line on the map was a line of constant di-\nrection. The Mercator projection encompasses the whole globe and while it\nkeeps the shapes of areas intact (that is, the continents look the way they\nshould), the sizes (that is, the areas of land masses) are completely distorted,\nparticularly the further you go from the Equator. For example, take a look at\nGreenland and Africa\u2014they look to be about the same size on the map, even\nthough Africa is really more than 13 times larger in area than Greenland.\nSimilar problems can be seen with Antarctica or Russia which (while they\nhave the right shape) look overwhelming in land area compared to their ac-\ntual size.\nThink of a projection like this\u2014you have a see-through globe of Earth\nwith the various landforms and lines of latitude and longitude painted on it.\nIf you could place a light bulb inside the globe and turn it on, you\u2019d see the\nshadows of the latitude, longitude, and continents projected onto the walls\nof the room you\u2019re in. In essence, you\u2019ve \u201cprojected\u201d the three-dimensional 38\nChapter 2 Where in the Geospatial World Are You?\nFIGURE 2.5 The three\ntypes of developable\nsurfaces in map\nprojections.\nCylindrical Conical Azimuthal\nworld onto the surfaces of the walls. To make a map projection, you\u2019d have\nto wrap a piece of paper around the globe and \u201cproject\u201d the lines onto that\npaper. Then, when you unroll the paper, you\u2019d have the globe translated onto\nthe map.\nThe real process is a much more complicated mathematical translation\nof coordinates, but the basic concept is similar. A map projection is created\nthrough the use of a developable surface, or a flat area onto which the coor-\ndinates from Earth can be placed. In this way, the locations on Earth\u2019s sur-\nface get translated onto the flat surface. The three main developable surfaces\n(Figure 2.5) that are used in this fashion are a cylinder (which creates a\ncylindrical projection), a cone (which creates a conical projection), and a flat\nplane (which creates an azimuthal surface). The place where Earth \u201ctouches\u201d\nthe surface is called the point of tangency, which is where the map\u2019s distor-\ntion is minimized.\nLike datums, there are numerous map projections available for use, each\nhaving its own different uses and built-in distortions (see Hands-on Applica-\ntion 2.2: Examining the Effects of Different Map Projections for more informa-\ntion). Two of the more common projections that you may encounter with geo-\nspatial data are Lambert Conformal Conic and Transverse Mercator.\nHands-on Application 2.2\nExamining the Effects of Different Map Projections\nDifferent kinds of map projections will alter the map the projection to view the world map as Mercator,\nin various ways, depending on which parts are b eing Lambert Conformal Conic, and Transverse Mercator,\ndistorted and which parts are being maintained. To and then select the option to redraw the map. How\ninteractively examine the effects of different pro- does each of these map projections alter the map\njections on a map, open your Web browser and go as drawn? Try some of the other options and exam-\nto http:\/\/projections.mgis.psu.edu. The Website ine what types of distortions and changes enter the\nenables you to select different projections and map with each projection.\nview how the map looks with each of them. Switch 39\nWhat Is UTM?\nLambert Conformal Conic\nThis is a conic projection, in which a cone intersects Earth at two parallels.\nThe choice of these tangency lines is important as the distortion on the map\nis minimized closer to them and maximized the farther you go from them.\nThe Lambert Conformal Conic projection is commonly used for United States\ngeospatial data (and other east-west trending areas).\nTransverse Mercator\nThis is a cylindrical projection, with the tangency being the intersection be-\ntween the cylinder and Earth. Measurements are most true at that point and\ndistortions get worse the further east or west the areas displayed on the map\nare. It\u2019s considered \u201ctransverse\u201d because the cylinder is wrapped around the\npoles, rather than the Equator. Transverse Mercator is used for more north-\nsouth trending areas.\nGCS coordinates can find locations across the entire globe, down to pre-\ncise measurements of degrees, minutes, and seconds. However, latitude and\nlongitude is not the only coordinate system used today\u2014there are several co-\nordinate systems that use a map projection to translate locations on Earth\u2019s\nsurface to a Cartesian grid system using x and y measurements. Two of the\ncommon grid systems used with geospatial data are UTM and SPCS.\nWhat Is UTM?\nThe Universal Transverse Mercator (UTM) grid system works by divid- Universal Transverse\ning the world into a series of zones, then determining the x and y coor- Mercator (UTM) the\ndinates for a location in that zone. The UTM grid is set up by translating grid system of locating\ncoordinates across the\nreal-world locations to where their corresponding places would be on a two-\nglobe.\ndimensional surface (using the Transverse Mercator projection). However,\nUTM only covers Earth from between 84\u00b0 N latitude and 80\u00b0 S latitude (and\nthus couldn\u2019t be used for mapping the polar regions\u2014a different system,\nthe U niversal P olar Stereographic Grid System, is used to find coordinates\non the poles).\nThe first step in determining UTM coordinates is to find which UTM UTM zone one of the\nzone the location is in. UTM divides the world into 60 zones, with each zone 60 divisions of the\nworld set up by the\nbeing 6\u00b0 of longitude wide (Figure 2.6 on page 40), and each zone is set\nUTM system, with each\nup as its own cylindrical projection. These zones are numbered 1 through\nzone being 6 degrees\n60 beginning at the 180th meridian and moving east. For instance, Mount\nof longitude wide.\nRushmore, outside of Keystone, South Dakota, is located in Zone 13. Beijing,\nChina, is located in Zone 50. Sydney, Australia is in Zone 56. So, the first step\nin using UTM is to determine which UTM zone the point you want to locate\nis in.\nThe next step is to determine the x and y grid coordinates of the loca-\ntion. Unlike GCS, which uses degrees, minutes, and seconds, UTM utilizes 40\nChapter 2 Where in the Geospatial World Are You?\n31 33 35 37\n32\n1 2 3 4 5 6 7 8 9101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960\n1 2 3 4 5 6 7 8 9101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960\nFIGURE 2.6 The 60\nzones that comprise the\nUTM system. meters as its measurement unit. UTM\u2019s x and y coordinates are referred to\nas an easting and a northing. The UTM northing is found by counting the\nnumber of meters the point is north of the Equator. For instance, a particular\neasting a location at Mount Rushmore is located 4,859,580 meters to the north of the\nmeasurement of so Equator.\nmany units east (or Locations in the southern hemisphere handle northing a little differ-\nwest) of some principal\nently since UTM does not contain negative values. For southern hemisphere\nmeridian.\nlocations, count the number of meters south of the Equator to the point (as a\nnorthing a negative value), then add 10,000,000 m to the number. For example, a loca-\nmeasurement of so\ntion at the Opera House in Sydney, Australia (in the southern hemisphere),\nmany units north (or\nhas a northing of 6,252,268 m. This indicates it would be approximately\nsouth) of a baseline.\n3,747,732 meters south of the Equator (\u22123,747,732 + 10,000,000). A nother\nway of conceptualizing this is to assume that (for southern hemisphere loca-\ntions) rather than measuring north of the Equator, you\u2019re m easuring north\nof an imaginary line drawn 10,000,000 meters south of the Equator. This\nfalse northing a extra 10 million meters is referred to as a false northing since it\u2019s a value\nmeasurement made set up by the system\u2019s designers to avoid negative values.\nnorth (or south) of an The next step is to measure the point\u2019s easting. Unlike GCS, there is no\nimaginary line such\none Prime Meridian to make measurements from. Instead, each UTM Zone\nas used in measuring\nhas its own central meridian which runs through the zone\u2019s center. This cen-\nUTM northings in the\nsouthern hemisphere. tral meridian is given a value of 500,000 m, and measurements are made to\nthe east (adding meters) or west (subtracting meters) from this value. For\nexample, our specific spot at Mount Rushmore is located 123,733.22 m to the\neast of Zone 13\u2019s central meridian\u2014so the easting for this location at Mount\nRushmore is 623,733.22 m (500,000 + 123,733.22). A location in the town 41\nWhat Is SPCS?\n108\u00b0 102\u00b0\nFIGURE 2.7 A simplified\nrepresentation of\n111333NNN\ndetermining the UTM\nCoordinates (easting,\nnorthing, and zone)\nof a location at Mount\nRushmore.\nMt. Rushmore is\n123,733.22 m\neast of the\nzone\u2019s principal 623,733.22 m E\nmeridian 4,859,580.26 m N\nZone 13\nMt. Rushmore is\n4,859,580.26 m\nnorth of the\nequator\n555000000,,,000000000 mmm\nof Buffalo, Wyoming, is also in Zone 13, and has an easting value of 365,568\nm. This indicates this location is approximately 134,432 m to the west of Zone\n13\u2019s central meridian\u2014(500,000 \u2013 134,432). No zone is more than 1,000,000\nmeters wide, so no negative values will be assigned to eastings in a zone. This\nvalue for easting is referred to as a false easting since the easting calculation false easting a\nis based on a central meridian for each zone. measurement made\neast (or west) of an\nWhen UTM coordinates are written, the false easting, northing, and zone\nimaginary meridian set\nmust all be specified. If a false northing is used (that is, the coordinates are\nup for a particular zone.\nfound in a location south of the Equator), then a notation that the coordinates\nare from the southern hemisphere is included. For example, the UTM\nc oordinates of a location at Mount Rushmore would be written: 623733.22 E,\n4859580.26 N, Zone 13. (See Figure 2.7 for a simplified graphical example\nof finding the location at Mount Rushmore\u2019s coordinates with UTM.) Also, see\nHands-on Application 2.3: Converting from Latitude\/Longitude to UTM on page 42\nfor converting latitude and longitude coordinates to UTM.\nWhat Is SPCS?\nA second grid-based method for determining coordinates is the SPCS (State SPCS the State Plane\nPlane Coordinate System), a measurement system designed back in the Coordinate System\nused for determining\n1930s. Today, SPCS is used as a coordinate system for United States data, es-\ncoordinates of locations\npecially city or county data and measurements. SPCS is a projected coordinate\nwithin the United\nsystem that uses a translation method similar to UTM to set up a grid coordi-\nStates.\nnate system to cover the United States. SPCS data was originally measured\nusing the NAD27 datum, but there are now datasets using the NAD83 datum. 42\nChapter 2 Where in the Geospatial World Are You?\nHands-on Application 2.3\nConverting from Latitude\/Longitude to UTM\nGCS and UTM are completely different systems, equivalents are. A good source of coordinates can be\nbut there are utilities to make conversion from one found at http:\/\/lat-long.com. This Website allows\nsystem to another simple and easy. One of these is you to input the name of an object (for instance,\navailable at http:\/\/www.cellspark.com\/UTM.html. use the Empire State Building) and the state (New\nThis Web resource is used to convert from GCS to York) or county and it will return the lat\/long (in both\nUTM or vice versa. You can specify your datum (ref- DD and DMS) for that place. Copy those coordinates\nerence ellipsoid), and then enter the latitude and into the UTM converter (use the WGS84 ellipsoid) to\nlongitude coordinates of a location (in either DD determine the UTM coordinates of the Empire State\nor DMS) and the utility will calculate the location\u2019s Building. Use the two Websites in conjunction with\nUTM coordinates. It can also calculate the latitude each other to determine the UTM coordinates of\nand longitude values when given UTM coordinates. prominent locations around your area (public librar-\nThe first step is to obtain GCS coordinates ies, city hall, or your school).\nof p laces before figuring out what their UTM\nLike UTM, SPCS requires you to specify a zone, an easting, and a northing in\naddition to the state the measurements are being made in.\nSPCS divides the United States into a series of zones. Unlike UTM Zones,\nSPCS zone one of the which follow lines of longitude, SPCS zones are formed by following state\ndivisions of the United or county boundaries. A state is often divided into multiple zones\u2014for ex-\nStates set up by the\nample, Ohio has two zones: Ohio-North and Ohio-South. Nevada has three\nSPCS.\nzones: Nevada-West, Nevada-Central, and Nevada-East. Texas has five zones,\nCalifornia has six zones, while all of Montana is represented by a single zone\n(Figure 2.8). To determine the coordinates for a point in SPCS, you must first\nknow which state the point is in and then identify the zone in that state. For\nexample, to map data from Miami, the state would be Florida and the zone\nwould be Florida-East, while Cleveland would be in the state of Ohio and in\nthe Ohio-North zone.\nThe next step is to determine the northing and easting. Like UTM, both\nof these are measured north and east from a pre-determined baseline and\nmeridian, but in SPCS, measurements are made in feet (using the NAD27\ndatum) or in feet or meters (using the NAD83 datum). Each of the SPCS\nZones has its own baseline and its own principal meridian. Two projected ver-\nsions of SPCS are used\u2014the Transverse Mercator version, which is usually\nused for states that have a north-south orientation (such as Indiana), and the\nLambert Conformal Conic version, which is usually used for states that have\na more east-west orientation (such as Ohio). Depending on the version used,\nthe zones\u2019 baselines and meridians are positioned differently.\nBaselines are placed a distance under the zone\u2019s southern border and\nmeasured north from there (to ensure all positive values). In the NAD27 43\nWhat Is SPCS?\nWA-N\nWA-S ND-N MN-N ME-E\nOR-N ID-W MT\nND-S MN-C WI-N MI-N VT NH ME-W\nOR-S ID-C SD-N MN-S WI-C MI-C NY-CNY-E MA-M\nCC\nA\nCA\n-\nA2-1\n-3\nNV-W NV-CNV-E UT\nU-ID\nTN\n-- CE WY-W WY-WC WY-EC COWY-E\n-N\nSD N- ES\nIA IA-N -S\nIL-W WI-S\nIL-EIN\n-W\nIN\n-EMI-S\nOO HH -S-N\nWNY V- -W\nNP P VA A A- -N S -NMD NJ\nDCT ENY-LR II MA-I\nCA-4 UT-S CO-C KS-N M O M O KY-N WV-S VA-S\nCO-S KS-S -W -C KY-S\nMO-E\nCA-5 NC\nOK-N TN\nCA-6\nAZ-W\nAZ-E NM-C\nNM-E TX-N\nOK-S\nAR-N\nGA-W SC\nAZ-C AR-S\nNM-W TX-NC\nLA-N\nM S-WM S-EAL-WA L-E GA-E\nTX-C FL-N\nLA-S\nAK-9 TX-SC FL-W\nAK-8\nK-7\nAK-6 AK-5A K-4 A K-3A K-2 TX-S\nHI-4\nHI-3\nFL-E\nA\nHI-5 HI-2\nAK-1\nAK-10 HI-1 SPC Zones\nFIGURE 2.8 The SPCS\nzones for all states.\nversion, states using the Transverse Mercator format have their zone\u2019s ori-\ngin 500,000 feet to the west of the principal meridian, while states using\nthe Lambert Conformal Conic projection have their zone\u2019s origin 2,000,000\nfeet to the west of the meridian. Coordinates are determined by measuring\ndistance using these false origin points, creating a false northing and a false\neasting. Thus, as each of the zones uses its own coordinate measurements,\neach zone has a different false northing and false easting from all other\nzones. Note that in the NAD83 version of SPCS, different values may be used\nfor each zone\u2019s origin.\nSPCS coordinates are referenced by listing the false easting, false north-\ning, state, and zone. For example, the SPCS (NAD83 feet) coordinates of a\nlocation on the football field of Stambaugh Stadium in Youngstown, Ohio,\nare approximately: 2478424.96 E, 531216.31N, Ohio, N (North Zone; see\nFigure 2.9 on page 44 for a simplified graphical representation of SPCS\nmeasurements). Also, see Hands-on Application 2.4: Using the State Plane\nCoordinate System for a utility for converting latitude and longitude coordi-\nnates into SPCS values. 44\nChapter 2 Where in the Geospatial World Are You?\nFIGURE 2.9 A simplified\nrepresentation of SPCS\nmeasurements and\ncoordinates for a location\non the YSU football field\nin Youngstown, Ohio.\n(Source: Ohio Geographically\n2478424.96 E,\nReferenced Information\n531216.13N,\nProgram (OGRIP), Ohio\nOhio, N\nStatewide Imagery Program\n(OSIP), April 2006)\nHands-on Application 2.4\nUsing the State Plane Coordinate System\nLike the UTM conversion tool in Hands-on Appli- (see the example on the Website for more details)\ncation 2.3: Converting from Latitude\/Longitude to in order for it to work properly. Use Web resources\nUTM, there are Web-based utilities used to convert or other mapping data you may have available to\nGCS coordinates into their SPCS counterparts. A you to find lat\/long coordinates of some national\ngood one is set up by NOAA and available at http:\/\/ points of interest (such as the Transamerica Pyra-\nwww.ngs.noaa.gov\/cgi-bin\/spc_getpc.prl. Similar mid in California) and places more local to you and\nto the utility used in Hands-on Application 2.3, this calculate the SPCS coordinates for them. Be careful\nWeb resource enables you to enter the latitude and to keep in mind the datum with which you\u2019re work-\nlongitude of a location and it will calculate the SPCS ing for the lat\/long coordinates and also that SPCS\ncoordinates for that point. Note that the lat\/long coordinates are available only for places within the\nyou give it have to be formatted in a specific way United States. 45\nKey Terms\nChapter Wrapup\nThis chapter provided a look at what goes into setting up coordinates and ref-\nerences for data used with geospatial technology. Whether you\u2019re reading co-\nordinates from a GPS receiver or making measurements from a map, knowing\nhow that geospatial information is put together and referenced has an impact\non how you\u2019re able to make use of that data, especially if you\u2019re trying to fit\nseveral types of geospatial data together.\nIn the next chapter, we\u2019re going to switch gears and look at how you can\ntake your data (whether it\u2019s a remotely sensed image, some information cre-\nated using GIS, or a paper map of the local fairgrounds) and get it to match up\nwith all of the other geospatial data you have. Before that, this chapter\u2019s lab\n(see Geospatial Lab Application 2.1: Coordinates and Position Measurements on\npage 46) will have you start applying the coordinate and measurement con-\ncepts described in this chapter with some more work with Google Earth.\nImportant note: The references for this chapter are part of the online com-\npanion for this book and can be found at http:\/\/www.whfreeman.com\/\nshellito1e.\nKey Terms\ndatum (p. 30) degrees, minutes, and seconds\nellipsoid (p. 30) (DMS) (p. 32)\ngeoid (p. 30) decimal degrees (DD) (p. 33)\ngeodesy (p. 30) great circle distance (p. 33)\nNAD27 (p. 31) time zones (p. 33)\nNAD83 (p. 31) map projection (p. 36)\nWGS84 (p. 31) Universal Transverse Mercator\ndatum transformation (p. 31) (UTM) (p. 39)\ngeographic coordinate system UTM zone (p. 39)\n(GCS) (p. 31) easting (p. 40)\nlatitude (p. 31) northing (p. 40)\nEquator (p. 31) false northing (p. 40)\nlongitude (p. 32) false easting (p. 41)\nPrime Meridian (p. 32) SPCS (p. 41)\nInternational Date Line (p. 32) SPCS zone (p. 42) 2.1\nGeospatial Lab Application\nCoordinates and Position Measurements\nThis chapter\u2019s lab will continue using Google Earth (GE), but only to specifi-\ncally examine coordinate systems and the relationship between various sets of\ncoordinates and the objects they represent in the real world. In addition, you\u2019ll\nbe making some measurements using GE, as well as using some Web resources\nfor comparing measurements made using different coordinate systems.\nNote that this lab makes reference to things like \u201ccoordinates for the\nWhite House\u201d or \u201ccoordinates for Buckingham Palace\u201d\u2014these represent mea-\nsuring a set of coordinates at one specific location at these places and are used\nas simplifications of things for lab purposes.\nObjectives\nThe goals for you to take away from this lab are:\na Setting up a graticule of lines in GE.\na Locating places and objects strictly by their coordinates.\na Making measurements across long and short distances, and then compar-\ning measurements to surface distance calculations.\na Translating lat\/long coordinates into UTM.\nObtaining Software\nThe current version of Google Earth (6.0) is available for free download at\nhttp:\/\/earth.google.com.\nImportant note: Software and online resources sometimes change fast.\nThis lab was designed with the most recently available version of the software\nat the time of writing. However, if the software or Websites have significantly\nchanged between then and now, an updated version of this lab (using the new-\nest versions) is available online at http:\/\/www.whfreeman.com\/shellito1e.\nLab Data\nThere is no data to copy in this lab. All data comes as part of the GE data that gets\ninstalled with the software or is streamed across the Internet through using GE.\nLocalizing This Lab\nThis lab flies around the world to numerous locations, including Washing-\nton, D.C., and London. However, it can easily be changed to examine the\ncoordinates of nearby local areas. Rather than looking at the White House\nor Buckingham Palace, substitute the coordinates or locations for your local\ngovernment offices (like city halls or courthouses).\n46 47\nCoordinates and Position Measurements\n2.1 Examining Coordinates and Distance\nMeasurements in Google Earth\n1. Start GE. Once Earth settles into view, scroll your mouse around\nEarth. You\u2019ll see a set of latitude and longitude coordinates appear at\nthe bottom of the view representing the coordinates assigned to your\nmouse\u2019s location. By default, GE uses the GCS coordinate system and the\nWGS84 datum.\n2. To examine the full graticule of latitude and longitude lines, from\nthe View pull-down menu select Grid. Some key GCS lines will be\nhighlighted in yellow amidst the Web of lat\/long lines\u2014the Equator,\nPrime Meridian, the Antimeridian, the Tropic of Cancer, and the Tropic\nof Capricorn.\n3. We\u2019ll begin in Washington, D.C., so in the Fly To box, type Washington,\nD.C. GE will rotate and zoom to this area. You\u2019ll also see the spaces\nbetween the lat\/long lines grow smaller and new values appear as GE\nzooms in.\n4. Next, you\u2019ll go a specific location in Washington, D.C. In the Fly To box,\ntype the following coordinates: 38.897631, \u221277.036566. These are the\ndecimal degree lat\/long coordinates of the White House.\n5. Press the Placemark button on the toolbar (see Geospatial Lab\nApplication 1.1 for more information on placemarks). The yellow\npushpin will automatically be placed at the lat\/long coordinates you\nhad GE search for (so you don\u2019t have to manually place it anywhere). In\nthe New Placemark dialog box, type The White House for the name of\nthe placemark, and select\nanother symbol for the\nyellow pushpin if you want.\n6. The coordinates for the\nWhite House are in decimal\ndegrees, but other methods\nof displaying coordinates\nare available in GE. From\nthe Tools pull-down menu,\nselect Options.\n7. In the Show Lat\/Long\noptions, select the radio\nbutton next to Degrees,\nMinutes, Seconds, then\nclick Apply, and then click\nOK.\n8. From the Places box in GE,\nright-click on the White\nHouse placemark, and 48\nChapter 2 Where in the Geospatial World Are You?\nthen select Properties. The coordinates will be changed to Degrees,\nMinutes, and Seconds (DMS). Answer Question 2.1. Close the White\nHouse placemark dialog box.\nQuestion 2.1 What are the coordinates for the White House in Degrees,\nMinutes, and Seconds?\n9. Decimal degree coordinates for the Lincoln Memorial are: 38.889257,\n\u221277.050137. Use the Fly To box to zoom to these coordinates. Once GE\narrives there, put a placemark at that spot, name it Lincoln Memorial,\nand answer Question 2.2.\nQuestion 2.2 What are the coordinates for the Lincoln Memorial in\nDegrees, Minutes, and Seconds?\n10. Adjust the view so you can see both placemarks at the edges of the view.\nSelect the Ruler tool and compute the distance between the White\nHouse and the Lincoln Memorial (see Geospatial Lab Application 1.1 for\nthe use of the Ruler tool in measurements). Answer Question 2.3. Turn\noff the placemark for the Lincoln Memorial when you\u2019re done.\nQuestion 2.3 According to GE, what is the distance between the White\nHouse and the Lincoln Memorial?\n11. For such a short distance (as between the White House and the Lincoln\nMemorial), there should be little differences in measurements, so let\u2019s\nlook at some larger distances.\n12. Buckingham Palace (in London, England) is located at the following\ncoordinates: 51.500821, \u22120.143089. Use the Fly To box to zoom to\nthese coordinates. Once GE arrives there, put a placemark at that spot,\nname it Buckingham Palace, and then answer Question 2.4.\nQuestion 2.4 What are the coordinates for Buckingham Palace in Degrees,\nMinutes, and Seconds?\n13. Zoom out and rotate the view so that you can see the placemarks for\nthe White House and for Buckingham Palace in the same view. Use the\nRuler to compute the distance between these two locations. Answer\nQuestions 2.5 and 2.6. Close the Ruler dialog when you\u2019re done. Of\ncourse, this is a rough estimate of being able to place the start of the\nruler and the end point of the ruler directly on top of the placemarks,\ngiven the scale of the view.\nQuestion 2.5 According to your measurement in GE, what is the computed\ndistance (in miles) between the White House and Buckingham Palace?\nQuestion 2.6 Why is the line curved rather than a straight line? What kind\nof distance is being computed here?\n14. We\u2019ll now check your measurements using a Web utility. Open your Web\nbrowser and go to http:\/\/www.chemical-ecology.net\/java\/lat-long.\nhtm. This Website enables you to compute the surface (real-world) 49\nCoordinates and Position Measurements\ndistance between two sets of lat\/long coordinates (and also between two\ncities) in DMS format.\n15. Use your answer to Question 2.1 as the Degrees, Minutes, and Seconds\nfor the Latitude and Longitude of Point one (you are using North\nlatitude and West longitude).\n16. Use your answer to Question 2.4 as the Degrees, Minutes, and Seconds\nfor the Latitude and Longitude of Point two (you are again using North\nlatitude and West longitude).\n17. Click the Distance Between button. The surface distance between the\ntwo points will be computed in kilometers, miles, and nautical miles.\nAnswer Question 2.7. Your answers to Questions 2.5 and 2.7 should be\nrelatively similar given how they were both computed\u2014if they\u2019re really\nway off, re-do your Question 2.5 measurement and make sure you\u2019re\ninputting the correct numbers for Question 2.7.\nQuestion 2.7 According to this Website measurement, what is the\ncomputed surface distance (in miles) between the White House and\nBuckingham Palace?\n2.2 Using UTM Coordinates and Measurements\nin Google Earth\nUniversal Transverse Mercator (UTM) is a projected coordinate system and\nGE can track coordinates using this UTM system as well as lat\/long.\n1. From GE\u2019s Tools pull-down menu, select Options. In the Show Lat\/Long\noptions, select the radio button for Universal Transverse Mercator.\nClick Apply, and then click OK. 50\nChapter 2 Where in the Geospatial World Are You?\n2. Zoom out to see the whole world\u2014you\u2019ll also see that the graticule has\nchanged from a Web of latitude\/longitude lines to the UTM grid draped\nover Earth. You should see the UTM zones laid out across the world,\nnumbered from 1 through 60.\nQuestion 2.8 What UTM Zone is Buckingham Palace located in? What\nUTM Zone is the White House located in?\n3. Double-click on the White House placemark (in the Places box) and GE\nwill rotate and zoom back to the White House.\n4. Scroll the mouse around the White House area. You\u2019ll see the new\ncoordinates appear at the bottom of the view, but this time as the zone,\neasting, and northing measurements. Open the Properties of the White\nHouse placemark by right-clicking on it. The UTM easting, northing,\nand zone will appear.\nQuestion 2.9 What are the UTM coordinates of the White House?\n5. In the Places box, double-click on the Lincoln Memorial placemark and\nGE will zoom and rotate to it.\nQuestion 2.10 What are the UTM coordinates of the Lincoln Memorial?\n6. UTM coordinates are measured in meters, enabling an easier method of\ndetermining coordinates rather than degrees of latitude or longitude. If\nyou move the mouse east, your easting will increase, and moving it west\nwill decrease the easting. The same holds true for northing\u2014moving the\nmouse north will increase the value of northing, while moving it south\nwill decrease the northing value.\n7. Change the view so that you can see both the White House and the\nLincoln Memorial in the view. Using your answers for Questions 2.9 and\n2.10 (and perhaps the Ruler tool), answer Question 2.11.\nQuestion 2.11 How many meters away to the north and east is the White\nHouse from the Lincoln Memorial?\n8. UTM can also be used to determine the location of other objects.\nReturn to the White House placemark again. Determine what object\nis 912 meters south and 94.85 meters to the east of the White House.\nQuestion 2.12 What object is located approximately 912 meters south and\n94.85 meters east of the White House?\nQuestion 2.13 What are the full UTM coordinates of this object?\nClosing Time\nThis lab served as an introduction for using and referencing coordinates and mea-\nsurements using Google Earth. Chapter 3\u2019s lab will use a new software program,\nbut we\u2019ll return to Google Earth in Chapter 8 and continue using its functions in\nother chapters as well. You can now exit GE by selecting Exit from the File pull-\ndown menu. There\u2019s no need to save any data (or Temporary Places) in this lab. 63\nGetting Your Data to Match\nthe Map\nReprojecting, Georeferencing, Control Points,\nand Transformations\nSince there are all of these datums and map projections out there, it\u2019s important\nto know which one you\u2019re using when making measurements. Say, for instance,\nyou have two sets of geospatial data that you need to bring together on one\nmap of your town for analysis (that is, a street map and building f ootprints).\nBoth of these pieces of data are in a different projection and coordinate sys-\ntem, and both use a different datum\u2014the street map is in State Plane NAD27\nfeet and the buildings are in UTM NAD83 meters. When you put these things\ntogether, nothing is going to match up\u2014the streets and buildings are going to\nbe in different places in relation to one another since each one uses a different\nbasis for its coordinates.\nIt is important to note when you\u2019re dealing with geospatial data that it\ncan be constructed using a variety of different datums, coordinate systems,\nand projections. With all of the different variations involved with these items,\nit\u2019s best to try and work with all geospatial data on the same basis (that is, use\nthe same datum, coordinate system, and projection). For instance, if you have\nmultiple data layers in different map projections and you try to overlay them,\nthen those pieces of data won\u2019t properly align with each other (Figure 3.1 on\npage 52).\nHow Can You Align Different Geospatial\nDatasets Together?\nreproject changing\nBack in Chapter 2, we discussed the concept of having to transform data mea- a dataset from one\nsured from different datums to a common datum so that everything will align map projection (or\nmeasurement system)\ncorrectly. As such, it\u2019s best to reproject your data (or transform all of the\nto another.\ndatasets to match one, or transform them all to a common datum, coordinate\n51 52\nChapter 3 Getting Your Data to Match the Map\nFIGURE 3.1 An example\nLambert Conformal Conic\nof the same data Mercator\nmeasured with different\nprojections and how each\none doesn\u2019t match up with\nthe others.\nCenter of projection\nsystem, and projection) before moving forward. Reprojecting data will alter\nthe coordinates, map projections, and measurements from one data source\n(for instance, coordinates measured in SPCS) to another system (such as\nUTM). Many geospatial software packages will give you the capability to take\nyour initial dataset (such as your street map in State Plane NAD27) and repro-\nject it to create a new dataset with new coordinates (so now you would have\na new street map measured in UTM NAD83). This can also be done to change\ndata from one map projection into another. In some cases, the software will\nbe able to \u201cproject on the fly\u201d or calculate the necessary transformations with-\nout actually altering the base dataset. Whatever the case, you\u2019ll usually avoid\nThinking Critically with Geospatial Technology 3.1\nWhat Happens When Measurements Don\u2019t Match Up?\nThe scenario described in the text is not uncom- what if the sewer data and the street data that a\nmon, having multiple data sources all with different road construction work crew has for renovations of\nprojections, coordinate systems, and datums that subdivision streets doesn\u2019t align properly (due to\nall need to be used together in a project. Without differing datums or projections)? What potential ef-\neverything aligning properly, you can end up with fects can this have? When the misalignment of data\npoints being plotted in the wrong location with may cause one dataset to be off from its properly\nrespect to the base map they\u2019re being located on, matched place with a map by a large margin, what\nor street data not lining up with a satellite image could occur? Similarly, what types of effects could\nof the same area. What kind of effects could this happen with things like shoreline construction,\ntype of simple data mismatch have on real-world zoning maps, or species habitat mapping?\ncompanies or government agencies? For instance, 53\nWhat Is Georeferencing?\na lot of headaches later on by using a common basis for the various geospatial\ndatasets you use. This is especially critical when getting several types of data\nto accurately match up with one another.\nHowever, a problem\u2019s going to arise when one (or more) of your pieces\nof data doesn\u2019t have coordinates assigned to it, or you don\u2019t have any infor-\nmation as to what projection or datum was used to construct it. Think about\nthis\u2014you obtain an old map of your town, scan it into a computer, and want\nto add it to other layers (like a road network or parcel layer) in a GIS to see\nhow things have changed since that old map was made. You add the layers\ninto the GIS, but the scanned historic map won\u2019t match up with any of your\ndata. This is because that image doesn\u2019t contain any spatial r eference (such spatial reference the\nas a real-world coordinate system). Although it might show features like old use of a real-world\ncoordinate system for\nroads or the dimensions of a building, the only coordinate system the old\nidentifying locations.\nmap possesses is a grid that begins at a zero point at the lower left-hand\ncorner and extends to the dimensions of the scanned image. For instance,\ncoordinates would be r eferenced by whatever units you\u2019re measuring with\n(such as 200 millimeters over from the left-hand corner and 40 millimeters\nup from the left-hand corner).\nItems in the image can\u2019t be referenced by something like latitude and lon-\ngitude since those types of coordinates haven\u2019t been assigned to anything in\nthe image. Similarly, when you added this scanned image as a layer in a GIS,\nthe software wouldn\u2019t know what to do with it when trying to match it up with\nother data layers (which have referenced coordinate systems). Without know-\ning \u201cwhere\u201d your scanned image matches up with the rest of the data, you\u2019d\nbe unable to use this image as geospatial data (or reproject it to match your\nother data). The same concept holds for other types of data used with geospa-\ntial technology. That is, if they all automatically had the correct spatial refer-\nence when they were created, things would be a lot easier. So what you\u2019ll have\nto do is match the data to the map, or assign a real-world set of coordinates to\nyour data so it can synch up with other geospatial data. In other words, you\u2019re\ngoing to have to georeference your data.\nWhat Is Georeferencing?\nGeoreferencing is the process of aligning an unreferenced dataset with one georeferencing a\nthat has spatial reference information. Other terms used for similar types of process whereby\nspatial referencing\nprocesses include \u201cimage to map rectification,\u201d \u201cgeometric transformation,\u201d or\nis given to data\n\u201cregistration.\u201d For example, say you\u2019re hosting a meeting at your school and\nwithout it.\nwant to make a map for the invitation to direct people not only to the school,\nbut also to your specific building within the campus. You go to your school\u2019s\nWebsite and download a map that shows the campus boundaries and specific\nbuilding locations on the campus. So far so good, but you want to place that\ndownloaded map in the greater context of a map of the city in a software pro-\ngram to show where your campus building is in relation to the surrounding 54\nChapter 3 Getting Your Data to Match the Map\nHands-on Application 3.1\nDavid Rumsey Historical Map Collection\nAn ongoing project for the collection and preser- with current geospatial imagery. By looking at geo-\nvation of old historic maps on the part of David referenced maps in relation to current data, you\nRumsey can be seen at http:\/\/www.davidrumsey. can see how areas have changed over time. On\ncom. The maps on the Website are part of a col- the Website, select the link for Google Earth un-\nlection stretching from the seventeenth through der the Quicklinks section, then select the option\nthe twentieth century. While there are many ways to View Google Earth in your Web browser (you\u2019ll\nof viewing these maps, some of the historic maps have to install a Google plug-in to make this work\nhave been georeferenced to make them match up properly). Inside your Web browser, you\u2019ll be able\nroads and neighborhoods, so people can get to your meeting. The problem is\nthat while the data you have of the surrounding area (whether aerial photos,\nsatellite imagery, or GIS data layers) contains real-world spatial references,\nyour downloaded map is just a graphic or PDF without any spatial reference at\nFIGURE 3.2 An example\nall. Thus, the campus map won\u2019t match up with the spatially referenced data,\nof a David Rumsey\nhistorical map (of and your plan of a snappy geospatial-style invitation is ruined. However, if you\nAustralia) georeferenced could just georeference that campus map, everything would fit together. The\nand overlaid in Google\ngeoreferencing procedure requires a few steps of work to proceed (see Hands-\nEarth. (Source: \u00a92010\non Application 3.1: David Rumsey Historical Map Collection and Figure 3.2 for\nGoogle. Data SIO, NOAA, U.S.\nNavy, NGA, GEBCO. \u00a92011 an example of an online georeferenced map collection).\nCnes\/Spot Image. Image\nIBCAO.) 55\nHow Can Data Be Georeferenced?\nto interact with Google Earth just like in the last maps as another Google Earth layer (like the roads\ncouple of labs, but you\u2019ll also see special symbols or buildings), open the Gallery options and then\non the map representing the location of the geore- open the Rumsey Historical Maps option, and click\nferenced maps. Click on these symbols to see the on the Map Finder icons. You\u2019ll see a new icon rep-\nmaps appear\u2014properly georeferenced\u2014in relation resenting the maps appear in Google Earth that you\nto Google Earth\u2019s imagery. Check out some of the can use to view the georeferenced maps. There\u2019s\nmap icons for a variety of places around the globe also an \u201cAbout the Maps\u201d pop-up where you can\nand see how this historic information synchs up read more information about the map collection and\nwith modern boundaries and images. how these maps were scanned and georeferenced\nThe Rumsey Historical Map collection is also for use in Google Earth.\navailable as a layer in Google Earth itself. To use the\nHow Can Data Be Georeferenced?\nWhen data (such as an old scanned map or other unreferenced data) is geo-\nreferenced, each location in that data is aligned with real-world coordinates.\nSo to start the whole process, you\u2019re going to need some type of source with\nreal-world coordinates to align your unreferenced data against. The choice\nof a source to use for the matching is important since you have to be able to\nfind the same locations on both the unreferenced image and the source. This\nis crucial to georeferencing, since the process needs some common locations\nto \u201ctie\u201d the unreferenced data to the source data. In the georeferencing pro-\ncedure, these spots (where you can find the coordinates on the source data)\nare referred to as control points (sometimes called Ground Control Points or control points point\nGCPs). Field work, such as surveying or GPS measurements (see Chapter 4) can locations where\nprovide sources for these control points, but without going into the field, you the coordinates are\nknown\u2014these are\ncan also find common locations on the unreferenced image and the source to\nused in aligning the\nuse in the process.\nunreferenced image to\nThus, the source should be something you can find the same locations on the source.\nas the unreferenced data. For instance, a low-resolution satellite image isn\u2019t\ngoing to be a good match with the detailed map of your campus buildings.\nTrying to match individual locations like buildings to an image like this isn\u2019t\ngoing to work simply because the resolution is too low. A high-resolution im-\nage (where you can determine features of buildings) would be a more proper\nchoice for this specific task. Datasets such as road networks (see Chapter 8)\nor orthophotos (see Chapter 9) are also good sources to use in finding these\ncommon points. Also, your source data should be a similar projection to your\nunreferenced data. If the projections are very different, then some features\nwill be forced to match up well and others will end up distorted. Whatever the\nsource, the georeferencing process will align an unreferenced image against\nthe projection and coordinates of the source map (Figure 3.3 on page 56).\nWhere to select the control points is the key to the whole georeferenc-\ning procedure. Since control points are the objects that are going to connect 56\nChapter 3 Getting Your Data to Match the Map\nFIGURE 3.3 Comparing\nthe unreferenced\ndata (an older campus\nmap of Youngstown\nState University)\nto the reference\nsource (a referenced\nimage of campus).\n(Sources: Mahoning County\nEnterprise GIS, April 2004.) 57\nHow Can Data Be Georeferenced?\nthe unreferenced data to the source data, they have to be placed in the\nsame location in both sets of data, and real-world coordinates for these\npoints will be required. By finding these same locations on the source (that\nhas real-world coordinates) and on the unreferenced data (without them),\nwe can start to match the two datasets together. Thus, places where you\ncan clearly find the same spot in the unreferenced data and the source data\nwill be good choices for the placement of control points in that location.\nFor instance, the intersections of roads or the corners of buildings are well-\ndefined areas that can be found on both sets of data and would make good\ncontrol point locations. These are also locations that aren\u2019t likely to change\nover time, so even if the unreferenced data and the source aren\u2019t from the\nsame time period, they would still be valid as control point selections. Poor\nlocations for control point selection spots are those that can\u2019t be accurately\ndetermined between the two sets of data, either because they\u2019re poorly de-\nfined or in spots that would vary. Examples of poor control point selection\nchoices would be a spot on the top of a building, the center of a field, the\nshoreline of a beach, or a shrub on a landscaping display. See Figure 3.4\nfor some examples of good and poor control point location selections on\nthe campus map.\nAnother thing to keep in mind when selecting control point locations is\nto not bunch them all together in one area of the data and ignore the rest of\nthe map. Placing all of the control points at spots in one-quarter of the image\nis going to ensure that section of the map is well referenced while the rest of\nthe map may not fit very well. Make sure to try and spread out the control\npoints to representative locations throughout the data so that all areas fit well\nregarding their new referenced spots. You\u2019ll want to use several control points\nin the process\u2014some georeferencing processes may only need a few, while\nothers may require many more. FIGURE 3.4 Good (in\nred) and poor (in blue)\nThree control points is the minimum number required to try to fit the\nlocations for control\nunreferenced image to the source. With three control points, you\u2019ll be able point selection. (Sources:\nhave a rough fit of the image to the source. With this initial fit, some pro- Mahoning County Enterprise\ngrams (for instance, MapCruncher, the program used in this chapter\u2019s lab) GIS, April 2004. Esri\u00ae ArcGIS\nArcMap ArcInfo graphical user\ninterface Copyright\u00a9 Esri.) 58\nChapter 3 Getting Your Data to Match the Map\nHands-on Application 3.2\nOld Maps Online\nThere\u2019s a research project in the Czech Republic for georeferencing historic data (also available at\ndedicated to (in part) georeferencing old historic http:\/\/www.georeferencer.org). Click on the link to\nmaps. To check out their work, open your Web open the tool\u2014you\u2019ll see a scanned historic map on\nbrowser and go to http:\/\/help.oldmapsonline.org the left-hand side and a source map on the right.\nto read about their current efforts. On the Website is Zoom in on both maps to try and find some common\na link to their Georeferencing Tool, an online utility points, and double-click to create a control point.\nwill attempt to match up the next control point location with a spot you select\non the source. As you properly add some more control points, the fit between\nthe unreferenced data and the source should (hopefully) improve. Proper\nselection of control points is critical for georeferencing since the procedure\nneeds to have some sort of real-world coordinates to match to, and it\u2019s these\nplaces that will be used to calculate coordinates for the other locations in the\nimage. Often, you\u2019ll find that deleting a control point, adding another one,\nand tinkering with the placement of others is necessary to get the best fit of\nthe data (see Hands-on Application 3.2: Old Maps Online for an online example\nof placing control points in relation to historic maps). Once your data is prop-\nerly aligned using control points, you can then transform it to have the same\ncoordinate system information as the source.\nHow Is Data Transformed to a\nrotated when the\nGeoreferenced Format?\nunreferenced image\nis turned during the\ntransformation. Take another look at Figure 3.3, which shows both the unreferenced campus\nmap and the source image. In order to make the unreferenced map properly\nskewed when\nalign with the source image, there are a number of actions that have to be tak-\nthe unreferenced\nimage is distorted or en with the map. First, check out the football stadium (Stambaugh Stadium)\nslanted during the in the upper left part of the map and the upper portion of the image. In order\ntransformation. to make it match from the map to the image, it would have to be turned about\nscaled when the scale 45 degrees clockwise. Looking at the other features, the whole map would\nof the unreferenced have to be rotated some amount (since the image is oriented to the north\nimage is altered during and the map is oriented to the northeast). Also, the map might have to be\nthe transformation.\nstretched or skewed to match up with the image.\ntranslated when the When unreferenced data is transformed or \u201c rectified\u201d, it\u2019s warped to\nunreferenced image make it match up with the source. It can be rotated (turned, like changing the\nis shifted during the\norientation of the map), skewed (pulling the image, like at a slant), scaled\ntransformation.\n(altering the entire map to match the image), and\/or translated (altering 59\nHow Is Data Transformed to a Georeferenced Format?\ny y FIGURE 3.5 Four\nways that the\nunreferenced data can\nbe warped to align it\nwith the source.\nx x\nRotation Skew\ny y\nx x\nTranslation Scaling\nthe location and placement of the map). See Figure 3.5 for examples of\nthe effects of each of these transformations. This transformation of the data\nchanges it to align with the new coordinate system. There are a number\nof transformation methods that are used, but a common one is the affine affine transformation\ntransformation, which will properly rotate, skew, translate, and scale the a linear mathematical\nprocess by which data\ndata.\ncan be altered to align\nThe affine transformation calculates real-world X and Y coordinates for\nwith another data\neach unreferenced x and y coordinate. The formulae used for this are: source.\nX (cid:2) Ax (cid:3) By (cid:3) C\nand\nY (cid:2) Dx (cid:3) Ey (cid:3) F\nwhere A, B, C, D, E, and F are the values calculated internally by the procedure\nand applied to the mathematical formulae as coefficients (and also control\nthe properties like skewing and rotation). The affine transformation is a first-\norder transformation, and usually this type of transformation is what will be\nused in this procedure. The end result of a transformation is that the unrefer-\nenced data will now be spatially referenced.\nSo, once you select some control points, the software runs the transfor-\nmation and the data is georeferenced. Everything appears to be working just\nfine. However, there are two questions that you should be asking yourself,\nand they are: \u201cHow do I know it worked?\u201d and, more importantly, \u201cHow do\nI know the results are any good?\u201d After all, just because the unreferenced\ndata\u2019s been transformed, it doesn\u2019t necessarily mean that it\u2019s a good match\nwith the source. Since you know the coordinates of each place you\u2019re put-\nting a control point, you should be able to match up those locations and see 60\nChapter 3 Getting Your Data to Match the Map\nFIGURE 3.6 A sample of\ncontrol point coordinate\nlocations (in the\nunreferenced map and in\nthe referenced image),\nand the computed error\nterm for each control\npoint. (Source: Clark Labs,\nClark University, IDRISI Taiga)\nhow closely your selected control points match the source. Figure 3.6 shows\na report from a software program measuring error values for each control\npoint.\nRoot Mean Square A transformation procedure uses a value called Root Mean Square Error\nError (RMSE) an (RMSE) as an indicator of how well the now-referenced data matches with\nerror measure used the source. After the initial set of control points has been chosen and the un-\nin determining the\nreferenced data is aligned with the source, coordinates are computed for lo-\naccuracy of the overall\ncations of the selected control points. The known coordinate location of each\ntransformation of the\nunreferenced data. control point (in the source) is compared with the newly computed value for\nits corresponding location in the unreferenced data. If the control points were\nin the exact same location in both the unreferenced data and the source, then\nthe difference in their coordinates will be zero. However, chances are, when\nplacing coordinates\u2014even if you selected good locations like road intersec-\ntions\u2014the two points aren\u2019t going to exactly match up, though they might\nbe very close. The software will examine all error values for all control points\nand report back a value for the RMSE.\nThe lower the differences between where you put the control point on\nthe unreferenced data (the estimated location) and the source data (the true\nlocation), the better the match between them will be. Thus, the lower the\noverall RMSE is, the better the transformation between the two datasets will\nbe. Usually, the georeferencing program will allow you to also examine the\ndifference between each point, allowing you to see how closely your control\npoint selections matched each other. If the RMSE is too high, it\u2019s a good idea\nto check the difference in coordinates for each point\u2014if a few of them are off\ntoo much, they can be adjusted, changed, or deleted (and new points selected\ninstead) to try and get a better fit. If some parts of the data match well and\nothers don\u2019t, perhaps the control points need to be moved to other places to\nbetter balance out the alignment, or other control points need to be added, or\nothers may need to be removed.\nWhen georeferencing an image, new locations for the image\u2019s pixels will\nneed to be calculated and in some cases, new values for the pixels will need to\nbe generated as well. This process is referred to as resampling. The end result\nof a good georeferencing process will have your unreferenced data properly\naligned with the source data (Figure 3.7). Maps that were not referenced can\nnow match up with current maps and datasets (see Hands-on Application 3.3: 61\nHow Is Data Transformed to a Georeferenced Format?\nFIGURE 3.7 Detail\nfrom the end result\nof georeferencing the\ncampus map. (Source: Clark\nLabs, Clark University, IDRISI\nTaiga)\nMicrosoft MapCruncher Gallery and Hands-on Application 3.4: An Overview of\nthe Georeferencing Process in ArcGIS, both on page 62, for examples). Georef-\nerencing can also be performed on other data created without a spatial refer-\nence such as unreferenced drawings and plans. A related procedure can also\nbe used for remotely sensed images that need to be corrected.\nThinking Critically with Geospatial Technology 3.2\nWhat Happens When the Georeferencing Is Wrong?\nThink about this scenario\u2014surveying is being done incorrect coordinates. Then on-site GPS coordinates\nfor a new beachfront development and a base map and measurements are reprojected to match the\nof the area will be created (using GIS) from current new base map, and of course, they\u2019re now inaccu-\nhigh-resolution remotely sensed imagery. Since the rate since they use an incorrectly referenced base\nbase map (and all layers created from it, such as map. What\u2019s going to be the end result of all this\nproperty boundaries, utilities, and local land cover) for the land owners, developers, construction crews,\nwill be derived from it, this initial layer is critical. and other stakeholders involved in the project?\nHowever, something goes wrong in the georefer- What kind of long-term or widespread problems can\nencing process of the image used to create the base be caused by this type of \u201ccascading error\u201d when\nmap, and thus the initial base map has incorrect co- the initial referencing is incorrect or inaccurate?\nordinates\u2014and then each layer created from it has 62\nChapter 3 Getting Your Data to Match the Map\nHands-on Application 3.3\nMicrosoft MapCruncher Gallery\nIn this chapter\u2019s lab, you\u2019ll be using a free software and park maps). Examine the georeferenced maps\ntool called MapCruncher that enables you to geo- in the gallery, then locate and download some maps\nreference your own images using Microsoft\u2019s Bing of places in your local area (such as parks, festivals,\nMaps as the source. There are several examples of fairgrounds, tourist areas, or historical sites) in a for-\nMapCruncher\u2019s uses online. Open your Web browser mat to use with MapCruncher. Once you go through\nand go to http:\/\/research.microsoft.com\/en-us\/um\/ the steps in this chapter\u2019s lab, you\u2019ll have an inven-\nredmond\/projects\/mapcruncher\/Gallery to view tory of local maps on hand that you can apply your\nsome examples (including bike maps, campus maps, georeferencing skills to.\nHands-on Application 3.4\nAn Overview of the Georeferencing Process in ArcGIS\nWhile you\u2019ll use the free MapCruncher to align un- a video of how the steps are performed at http:\/\/\nreferenced data in the lab, georeferencing tools are webhelp.esri.com\/arcgisdesktop\/9.3\/index.cfm?\ncommon in other GIS software such as ArcGIS (which TopicName=Georeferencing_video_sample. Check\nwe\u2019ll discuss further in Chapter 5). In order to dem- out the video and see how GIS tools are used for\nonstrate georeferencing in ArcGIS, its company has georeferencing a dataset.\nChapter Wrapup\nProper georeferencing of data is a key element of working with geospatial\ntechnology and allows for all types of maps and imagery to work side by side\nwith other forms of geospatial data. As noted in Hands-on Application 3.3:\nMicrosoft Map Cruncher Gallery, this chapter\u2019s lab will have you apply all of\nthis chapter\u2019s concepts using the MapCruncher program\u2014you\u2019ll start with an\nunreferenced campus map, and by the end, it should be properly aligned with\na real-world dataset.\nIn the next chapter, we\u2019ll be looking at the Global Positioning System, an\nextremely useful (and freely available) system for determining your position\non Earth\u2019s surface and providing you with a set of coordinates, so you can find\nwhere you are.\nImportant note: The references for this chapter are part of the online com-\npanion for this book and can be found at http:\/\/www.whfreeman.com\/\nshellito1e. 63\nKey Terms\nKey Terms\nreproject (p. 51) scaled (p. 58)\nspatial reference (p. 53) translated (p. 58)\ngeoreferencing (p. 53) affine transformation (p. 59)\ncontrol points (p. 55) Root Mean Square Error (RMSE)\nrotated (p. 58) (p. 60)\nskewed (p. 58) 3.1\nGeospatial Lab Application\nGeoreferencing an Image\nThis chapter\u2019s lab will introduce you to some of the basics of working with\ngeoreferencing concepts by matching an unreferenced map image against a\nreferenced imagery source. You\u2019ll be using the free Microsoft MapCruncher\nutility for this exercise.\nImportant note: MapCruncher will take a map (in an image or PDF for-\nmat) and align it to its Virtual Earth (and transform it into a Mercator projec-\ntion). The end result of the process is a series of \u201ctiles\u201d used by Virtual Earth\nto look at your map matched up with their data. As such, while you\u2019ll be u sing\ngeoreferencing concepts from the chapter, the end result will be that your\nmap is turned into a \u201cmashup\u201d that could later be used on a Web page.\nObjectives\nThe goals for you to take away from this lab are:\na Familiarizing yourself with the MapCruncher operating environment.\na Analyzing the unreferenced image and the source for appropriate control\npoint locations.\na Performing a transformation on the unreferenced data.\na Evaluating the results of the procedure.\nObtaining Software\nThe current version of MapCruncher (3.2) is available for free download at\nhttp:\/\/www.microsoft.com\/maps\/product\/mapcruncher.aspx.\nImportant note: Software and online resources sometimes change fast.\nThis lab was designed with the most recently available version of the soft-\nware at the time of writing. However, if the software or Websites have signifi-\ncantly changed between then and now, an updated version of this lab (using\nthe newest versions) is available online at http:\/\/www.whfreeman.com\/\nshellito1e.\nLab Data\nCopy the Chapter 3 folder containing a PDF file (parkmap2010.pdf), which\nis a map\u2014with no spatial reference attached to it, it\u2019s only a graphic\u2014of the\nYoungstown State University (YSU) campus. This map of YSU can also be\ndownloaded from the campus map Web page at http:\/\/web.ysu.edu\/gen\/\nysu\/Directions_Maps__Tour_m107.html.\n64 65\nGeoreferencing an Image\nLocalizing This Lab\nMapCruncher is capable of georeferencing a graphic or PDF map of an area.\nIf you have a graphic or PDF of your school, it can be substituted for the YSU\ncampus map and you can match your own local data instead. The same steps\nof the lab will still apply; you\u2019ll just be finding appropriate local control points\nrather than the ones in Youngstown, Ohio.\nAlternatively, you could use a graphic or PDF of a map of a local park,\nhike or bike trail, fairgrounds, or local shopping district and georeference that\ninstead.\n3.1 Getting Started With MapCruncher\n1. Start MapCruncher. It will open with the default screen referred to as\n\u201cUntitled Mashup.\u201d Maximize the MapCruncher window to give yourself\nplenty of room to work with.\n2. The left side of the screen will contain the unreferenced image you want\nto match, while the right side of the screen contains the referenced\nimagery from Microsoft\u2019s Virtual Earth (what you\u2019ll be matching your\nimage to).\n3. As the text box notes, select the File pull\u2013down menu, then choose Add\nSource Map.\n4. Navigate to the Chapter 3 folder and select the parkmap2010.pdf file.\n5. In the upper right corner of the MapCruncher screen is the Virtual Earth\nPosition box.\nVirtual\nEarth\nPosition\n(Source: Used with permission from Microsoft) 66\nChapter 3 Getting Your Data to Match the Map\n6. Type the following coordinates and then click on Zoom In to move the\nview to YSU:\na Latitude: 41.10687400\na Longitude: \u201380.64670900\n7. The crosshairs in the image will center in northeast Ohio. Press the\nVirtual Earth Position\u2019s Zoom In button several times to zoom in to\nYSU\u2019s campus (Zoom Level 17 is probably a good starting point).\n8. Both of the views will now show the campus\u2014the Source Map (left-\nhand view) has the PDF map image (or graphic) while the Virtual Earth\n(right-hand view) is the aerial imagery from Virtual Earth.\n3.2 Selecting Correspondences\n1. The next step is to choose the same positions on both images to use as\ncontrol points (what MapCruncher refers to as \u201ccorrespondences\u201d).\nA good starting point is the intersection of Wick Avenue and Lincoln\nAvenue (a major road intersection that borders the southeastern portion\nof the campus).\n2. Click on Zoom In on the Source Map until you reach Zoom Level 7.\n3. The cursor will change to a hand\u2014use this to pan around the map until\nthe intersection of Wick and Lincoln Avenues is centered under the\nSource Map\u2019s crosshairs.\n4. In the Virtual Earth view, pan around the map until that same\nintersection is in the Virtual Earth\u2019s crosshairs.\n(Source: Used with permission from Microsoft)\nWick and Lincoln\nIntersection 67\nGeoreferencing an Image\n5. When you have both crosshairs accurately positioned, click Add in the\nCorrespondences box.\n(Source: Used with permission from Microsoft)\n6. The point will appear as ID 0. This is the first correspondence (what\nMapCruncher refers to as control points) being selected.\nQuestion 3.1 Why is this location a good one for a correspondence?\nQuestion 3.2 Just by visual examination of the two maps, what types of\ntransformative operations will have to be performed to the Source Map to\nmake it match the Virtual Earth?\n7. Use the combination of the Source Map and the Virtual Earth to select\nanother two correspondence points at clearly defined locations on both\nsources. Be sure to spread out the points geographically across the\nSource Map image, then click Lock. MapCruncher will perform an initial\nfit of the data, but pan around and you\u2019ll see that things don\u2019t match up\njust yet. Press Unlock and select a total of ten correspondences (you can\nuse the Lock option to see how things are fitting together as you go, then\nUnlock to adjust the points).\nQuestion 3.3 Which locations did you choose for the next nine\ncorrespondence points (and why)?\nQuestion 3.4 Why would you want to spread the correspondence point\nselection across the image instead of clustering the points together?\n8. Once you have about ten good locations chosen, click the Lock button in\nthe Correspondences box. 68\nChapter 3 Getting Your Data to Match the Map\n(Source: Used with permission from Microsoft)\n9. A value for error will be computed for each of the points and they will be\nsorted from highest error value to lowest error value (rather than by ID\nnumber).\nQuestion 3.5 What are the error values associated with your chosen\ncontrol points?\n10. Pan around the source image to the ID number with the highest error\nvalue. With the maps locked you\u2019ll see both the Source Map and the\nVirtual Earth move together.\nImportant note: To zoom directly to one of the correspondence points,\ndouble-click on its ID in the box. MapCruncher will now be centered on that\ncontrol point.\nIf any of your error values are very high, you can change them by\ndoing the following:\na. Zoom in to the selected point. You\u2019ll see a green dot indicating\nwhere MapCruncher thinks the point should be and a line showing\nthe distance from where you placed the correspondence point to\nwhere MapCruncher thinks it should be.\nb. Click Unlock.\nc. Move the crosshairs of the Source Map to the new spot.\nd. Click Update.\ne. Click Lock again to view the updated error terms.\n11. Pan around the Source Map, viewing how well points on the interior\nand on the fringes match up between the Source Map and the Virtual 69\nGeoreferencing an Image\nEarth representation. Add a few more correspondence points if you feel\nnecessary to adjust for areas that don\u2019t match up well.\n3.3 Matching the Source Map to the Virtual Earth\nWhen you have the Source Map set up the way you want it, it\u2019s time to com-\nplete the process.\n1. You may first want to save your work, by selecting the File pull\u2013down\nmenu and choosing Save Mashup. Give your work a file name and save\nit on your machine (it will save with the file extension \u201c.yum\u201d).\n2. Back in MapCruncher, click the Render button in the lower left\u2013hand\ncorner. The Render dialog box will open.\n(Source: Used with permission from Microsoft)\n3. Press the \u201cPick\u201d button (the one with the \u201c...\u201d symbol on it) next to the\nOutput Folder and then navigate to a folder on your machine to save the\naligned file to.\n4. Put checkmarks in the Copy source maps and crunchup data to\noutput folder and the Permit composition boxes.\n5. Click the Start button to begin the process.\nImportant note: The render process may take several minutes and could\nfill up about 200 MB of space on your machine.\n6. You\u2019ll see the new layer being \u201cbuilt\u201d in the Status window of the Render\ndialog box.\n7. After a couple minutes, the \u201cRender Complete\u201d box will appear. Click OK\nin this box.\n8. Click on the Preview rendered results text at the bottom of the Render\ndialog box. The Mashup Viewer will open to see the map lined up with\nVirtual Earth. 70\nChapter 3 Getting Your Data to Match the Map\n9. From the Mashup Viewer\u2019s Virtual Earth Background pull-down menu,\nselect the Hybrid option.\nQuestion 3.6 How does the map match up with the roads leading in and\nout of campus? Are there any locations where the roads don\u2019t match?\n10. Zoom in to the image\u2014you\u2019ll see that the color white in the original map\nis now transparent and thus features like the roads in the aerial image\ncan be seen through the map. Other colors can be made transparent as\nwell, to better match up the new map with aerial imagery.\n(Source: Used with permission from Microsoft)\n11. Close the Mashup Viewer and back in MapCruncher, select the\nTransparency tab.\n12. Put a checkmark in the Use document transparency box.\n13. Choose the Make selected colors transparent option.\n14. In the Source Map, pan the map so that the green color (for instance, the\ngreenery in the campus center) is under the crosshairs, and then click\nthe Add Color Under Crosshairs button. The green color will appear in\nthe box, while the green on the Source Map should change to pink.\n15. Click the Render button again.\n16. In the Render dialog box, save the results to a new folder. Click Start\nand the image will re-render, this time with the color green being made\ntransparent. Preview the rendered results again.\n17. In the new Mashup Viewer, the map will be rendered, but will be\ndifferent this time. 71\nGeoreferencing an Image\n18. From the Virtual Earth Background pull\u2013down menu, select the Aerial\nPhotos option.\nQuestion 3.7 How does the map match up with the aerial imagery around\ncampus? Are there any locations where the images don\u2019t really match (that\nyou can see with the greenery and roads made transparent)?\nClosing Time\nMapCruncher is a highly versatile tool that is used for making data (such as\nunreferenced images) match the map. Any type of map that is drawn to scale\n(such as a park trail map, a fairgrounds map, a theme park diagram, or an\nunreferenced image) can be aligned using MapCruncher. You can now exit\nMapCruncher by selecting Exit from the File pull\u2013down menu. This page was intentionally left blank 4\nFinding Your Location with\nthe Global Positioning System\nGPS Origins, Position Measurement, Errors, Accuracy,\nGNSS around the World, Applications, and Geocaching\nThink about this\u2014you\u2019re out hiking in a forest, a storm comes up and you\nneed to quickly take shelter well off the trail. When things are all clear, you\u2019ve\ngotten completely disoriented, can\u2019t find your location, and have gotten\ntruly lost. Or consider this\u2014you\u2019re driving in rural, unfamiliar territory late\nat night, you\u2019ve missed an important turn, and none of the names or route\nnumbers on the signs seem familiar at all. In both cases, you\u2019re out of cell\nphone range, and even though you have a printed map, you really have no\nidea where you are to start navigating yourself out of this mess.\nA few years ago, these types of situations could have been potentially\ndisastrous, but today there\u2019s a geospatial technology tool that\u2019s ready and avail-\nable to help get you out of these situations. Using a small, hand-held, relatively\ninexpensive device, you can find the coordinates of your position on Earth\u2019s\nsurface to within several feet of accuracy. Simply stand outside, turn the device\non, and shortly your position will be found, and perhaps even plotted on a map\nstored on the device. Start walking around and it\u2019ll keep reporting where you\nare, and even take into account some other factors like the elevation you\u2019re at,\nthe speed at which you\u2019re moving, and the compass direction in which you\u2019re\ngoing. Even though it all sounds like magic, it\u2019s far from it\u2014there\u2019s an exten-\nsive network of equipment across Earth and in space continuously working in\nconjunction with your hand-held receiver to locate your position. This, in a\nGPS the Global\nnutshell, is the operability of the Global Positioning System (GPS).\nPositioning System,\nGPS receivers have become so widespread that they seem to be used\na technology using\neverywhere. In-car navigation systems have become standard items for sale signals broadcast\non the shelves of an electronics store. GPS equipment is common at sport- from satellites for\ning goods stores, where it\u2019s used by campers, hikers, fishing enthusiasts, navigation and position\ndetermination on the\nand golfers. Runners can purchase a watch containing a GPS receiver that\nEarth.\nwill tell them their location, the distance they run, and their average speed.\n73 74\nChapter 4 Finding Your Location with the Global Positioning System\nSmartphones will often have GPS capability as a standard function. Use of the\nGPS is free for everyone and available worldwide, no matter what remote area\nyou find yourself lost in. So the first questions to ask are \u2014who built this type\nof geospatial technology and why?\nWho Made GPS?\nThe acronym \u201cGPS\u201d seems to be in the common vocabulary to describe any\nnumber of devices that use Global Positioning System technology for finding\na location, but it\u2019s really an inaccurately used term. The term \u201cGPS\u201d originated\nwith the initial designers and developers of the setup, the U.S. Department of\nNAVSTAR GPS the Defense, and is part of the official name of the system, called NAVSTAR GPS.\nUnited States Global Other countries besides the United States have developed or are c urrently en-\nPositioning System. gaged in developing systems like the NAVSTAR GPS, so when we\u2019re describ-\ning \u201cGPS,\u201d we\u2019re really discussing the United States system. Since the United\nStates isn\u2019t the only nation with this technology, the more accurate way of\ndescribing things would be to call GPS one type of Global Navigation Satellite\nGNSS the Global System (GNSS).\nNavigation Satellite GPS isn\u2019t the first satellite navigation system. During the 1960s, the U.S.\nSystem, an overall term\nNavy used a system called Transit, which determined the location of sea- going\nfor the technologies\nvessels with satellites. The drawback of Transit was that it didn\u2019t provide con-\nusing signals from\ntinuous location information\u2014you would have to wait a long time to get a\nsatellites for finding\nlocations on the Earth\u2019s fix on your position rather than always knowing where you were. The Naval\nsurface. Research Laboratory Timation program was another early satellite navigation\nprogram of the 1960s, which used accurate clock timings for ground position\ndetermination from orbit. The first GPS satellite was launched in 1978, and\nthe twenty-fourth GPS satellite was launched in 1993, completing an initial\nfull operating capacity of satellites of the system for use. Today, GPS is over-\nseen and maintained by the 50th Space Wing, a division of the U.S. Air Force\nheadquartered at Schriever Air Force Base in Colorado.\nWhat Does the Global Positioning System\nConsist of?\nFinding your position on the ground with GPS relies on three separate compo-\nspace segment one\nnents, all operating together. They are as follows: a space segment, a control\nof the three segments\nsegment, and a user segment.\nof GPS, consisting of\nthe satellites and the\nsignals being broadcast Space Segment\nfrom space.\nThere are numerous GPS satellites (also referred to as \u201cSVs\u201d or \u201cSpace\nconstellation the Vehicles\u201d) orbiting Earth in fixed paths, which make up the space segment.\nfull complement of GPS satellites make two orbits around Earth every day (their orbit time is\nsatellites comprising a\nactually about 11 hours and 58 minutes) at an altitude of 20,200 kilometers\nGNSS.\n(12,552 miles). The satellites are set in specific orbits, called a constellation, 75\nWhat Does the Global Positioning System Consist of?\nin which the satellites are specifically arranged for maximum coverage over\nFIGURE 4.1 The\nEarth (Figure 4.1). The way the constellation is designed allows for a person\nconstellation of GPS\nto be able to receive enough GPS signals to find their location wherever they satellites orbiting Earth.\nare on the planet. Twenty-four satellites is the minimum for a full constellation,\nand there are currently several additional operational GPS satellites in orbit to\nimprove the global coverage.\nThe job of GPS satellites is to broadcast a set of signals down to Earth from\norbit. These signals (we\u2019ll discuss this later) contain information about the 76\nChapter 4 Finding Your Location with the Global Positioning System\nposition of the satellite and the precise time at which the signal was transmitted\nfrom the satellite (GPS satellites measure time using extremely accurate atomic\nclocks onboard the satellites). The signals are sent on carrier frequencies: L1\n(broadcast at 1575.42 MHz) and L2 (broadcast at 1227.60 MHz). There are\nalso additional carrier frequencies planned for future GPS upgrades, including\nan L5 frequency to be used for safety-of-life functions. You need to have a direct\nline of sight to the satellites to receive these frequencies (which means that you\nhave to be in the open-air, not indoors).\nControl Segment\ncontrol segment one The control segment of GPS represents a series of worldwide ground stations\nof the three segments that track and monitor the signals being transmitted by the satellites. These\nof GPS, consisting of ground stations are spread out to enable continuous monitoring of the satel-\nthe control stations\nlites. The control stations collect the satellite data and transmit it to the mas-\nthat monitor the\nter control station at Schriever Air Force Base. In the control segment, cor-\nsignals from the GPS\nsatellites. rections and new data are uploaded to the satellites so that the satellites will\nbe broadcasting correct data. The ground stations also monitor the satellites\u2019\nposition and relay updated orbit information to the satellites (Figure 4.2).\nFIGURE 4.2 A control\nstation monitors the\nsignals from the satellites\nand sends correction data\nback to the satellite.\nGPS satellite\nData to satellite Data from satellite Data from satellite\nGround station GPS receiver 77\nHow Does GPS Find Your Position?\nUser Segment\nuser segment one of\nThe user segment represents a GPS unit somewhere on the Earth that is re- the three segments of\nceiving the signals from the satellite. The receiver can then use this informa- GPS, consisting of the\nGPS receivers on the\ntion to compute its position on the Earth. A key component of a GPS receiver\nground that pick up\nis how many channels the receiver has\u2014the number of channels reflects the\nthe signals from the\nnumber of satellites the receiver can obtain signals from at one time. Thus, a satellites.\n12-channel receiver can potentially pick up signals from up to 12 satellites.\nchannels the number\nAlso, the type of unit affects which of the frequencies it can receive\u2014it can\nof satellite signals a\neither be a single frequency receiver (which can pick up the L1 frequency) or GPS unit can receive.\na dual frequency model (which can read both L1 and L2). One thing to keep\nsingle frequency a\nin mind about the receiver is that no matter what size, capabilities, or number\nGPS receiver that can\nof channels it has, a receiver (like the name implies) can only receive satellite pick up only the L1\ndata, not transmit data back up to the satellites. As such, GPS is referred to frequency.\nas a one-way ranging system since the user on the ground cannot send any\ndual frequency a GPS\ninformation to a satellite. receiver that can pick\nup both the L1 and L2\nfrequency.\nHow Does GPS Find Your Position?\nalmanac data\nconcerning the status\nAt this point, the three segments of GPS have satellites broadcasting signals, of a GPS satellite,\nground stations monitoring and correcting those signals, and users receiving which is included in\nthose signals\u2014and all three of these segments work together to determine the information being\ntransmitted by the\nyour position on the Earth\u2019s surface. The signals being sent from the satellites\nsatellite.\nare the key to the whole process\u2014the signals contain information about the\nsatellite\u2019s status, the orbit, and location (referred to as the almanac) that\u2019s ephemeris data\nreferring to the GPS\nsending them, as well as more precise data about satellite location (this infor-\nsatellite\u2019s position in\nmation is referred to as the ephemeris).\norbit.\nThe information is sent in one of two digital pseudo-random codes\u2014the\nC\/A code the digital\nC\/A code (coarse acquisition code) and the P code (precise code). The C\/A\ncode broadcast on the\ncode is broadcast on the L1 carrier frequency and is the information that\nL1 frequency, which is\nc ivilian receivers can pick up. The P code is broadcast on the L1 and L2 carrier\naccessible by all GPS\nfrequencies and contains more precise information, but a military receiver is receivers.\nrequired to pick up this signal. The Y code is an encrypted version of the P code\nP code the digital\nand is used to prevent false P code information from being sent to a receiver by\ncode broadcast on the\nhostile forces. This \u201canti-spoofing\u201d technique is commonly used to make sure L1 and L2 frequencies,\nthat only the correct data is being received. Basically, the satellites are trans- which is accessible by\nmitting information in the codes about the location of the satellite, and they the military.\ncan also be used to determine the time when the signal was sent. By using this Y code an encrypted\ndata, the receiver can find its position relative to that one satellite. version of the P code.\nThe signals are transmitted from space using high-frequency radio\nwaves (the L1 and L2 carrier frequencies). These radio waves are forms of\nelectromagnetic energy (to be discussed in Chapter 10) and will thus be\nmoving at the speed of light. Your receiver can compute the time it took for\nthe signal to arrive from the satellite. If you know how long the transmission\ntime (t) was, and also that the radio waves are moving at the speed of light\n(c), then the distance between you and that one satellite can be calculated 78\nChapter 4 Finding Your Location with the Global Positioning System\nby multiplying t by c. This result will give you the pseudorange (or distance)\npseudorange the\nbetween your receiver and the satellite transmitting the signal.\ncalculated distance\nbetween a GPS satellite Unfortunately, this still doesn\u2019t tell us much\u2014you know where you are on\nand a GPS receiver. the ground in relation to the position of one satellite over 12,000 miles away,\nwhich gives very little information about where you are on the Earth\u2019s sur-\nface. It would be like waking up in an unknown location and a passerby tells\nyou that you\u2019re 500 miles away from Boston\u2014it gives you very little to go on\nin determining your real-world location.\ntrilateration finding The process of determining your position is referred to as trilateration,\na location in relation to which means using three points of reference to find where you are. Trilatera-\nthree other points of tion in two dimensions is commonly used when plotting a location on a map.\nreference.\nTake for example that you\u2019re on a trip to Michigan, you\u2019ve driven all night, and\narrived in an unknown location with no identifying information as to where\nyou are. The people you\u2019re travelling with are nowhere to be found, and the first\nperson you bump into tells you (somewhat unhelpfully) that you\u2019re 50 miles\naway from Ann Arbor, Michigan (see Figure 4.3). That puts you somewhere\non a circle sweeping outward 50 miles from Ann Arbor, giving you far too many\npotential places for you to be.\nThe second person you run into tells you (again, not being overly helpful)\nthat you\u2019re located 150 miles away from Traverse City, Michigan. This again\nputs your location somewhere on a circle 150 miles from Traverse City\u2014but\nwhen you combine this with the information that places you 50 miles away\nfrom Ann Arbor, you can limit your options down considerably. There are only\ntwo cities in Michigan that are 50 miles from Ann Arbor and 150 miles away\nfrom Traverse City\u2014you\u2019re either in Lansing or Flint (Figure 4.4).\nLuckily, the third person you see tells you that you\u2019re 60 miles from Kalam-\nazoo. Your first thought should be \u201cWho are all these geographically minded\npeople I keep running into?\u201d while your second thought is that you now know\nexactly where you are. Lansing is the only option that fits all three of the dis-\ntances from your reference points, so you can disregard Flint and be satisfied\nthat you\u2019ve found your location (see Figure 4.5 and Hands-on Application 4.1:\nTrilateration Concepts).\nFIGURE 4.3 Measuring\ndistance from one point.\nAnn Arbor\n50 miles 79\nHow Does GPS Find Your Position?\nFIGURE 4.4 Using\ndistances from two points\nin trilateration.\nTraverse City\n150 miles\nLansing Flint\nAAnnnn AArrbboorr\n50 miles\nFIGURE 4.5 The results\nof trilateration (with\ndistances from three\npoints) to find a location.\nTraverse City\n150 miles\nLansing Flint\nAAnnnn AArrbboorr\nKalamazoo 50 miles\n60 miles\nHands-on Application 4.1\nTrilateration Concepts\nTrilateration can be used to determine your location location (like in the previous Michigan example), then\non a map. Open your Web browser and go to http:\/\/ start Google Earth and zoom to your location. Use\nelectronics.howstuffworks.com\/gadgets\/travel\/ Google Earth and its measuring tools to set up a simi-\ngps1.htm to view the article by Marshall Brain and lar scenario\u2014find your location relative to three other\nTom Harris at How Stuff Works. Check out how three cities or locations and calculate the measurements\nmeasurements are being made to determine a single from each that are needed to trilaterate your position. 80\nChapter 4 Finding Your Location with the Global Positioning System\nThe same concept applies to finding your location using GPS, but rath-\ner than locating yourself relative to three other points on a map, your GPS\nreceiver is finding its position relative to three satellites. Also, since a position\non a three-dimensional (3D) Earth is being found with reference to positions\nsurrounding it, a spherical distance is calculated rather than a flat circular\ndistance. This process is referred to as trilateration in three dimensions (or\n3D trilateration 3D trilateration). The concept is similar, though finding the receiver\u2019s posi-\nfinding a location on tion relative to one satellite means it\u2019s located somewhere on one sphere (sim-\nthe Earth\u2019s surface in\nilar to only having the information about being 50 miles from Ann Arbor). By\nrelation to the positions\nthe receiver finding its location in relation to two satellites, it\u2019s finding a loca-\nof three satellites.\ntion on a common boundary between two spheres. Lastly, by finding the loca-\ntion relative to three satellites, there are only two places those three spheres\ncan intersect, and the way the geometry works out, one of them would be in\nouter space instead of on Earth\u2019s surface, so that one can be disregarded. That\nleaves one location where all three spheres intersect and thus one location on\nEarth\u2019s surface relative to all three satellites. That position would be the loca-\ntion of the GPS receiver (Figure 4.6).\nHowever, there\u2019s a problem with all of this. Remember that what\u2019s being\ncalculated is a pseudorange based on the speed of light (which is a constant)\nand the time it takes for the signal to transmit from space to Earth. If that time\nwas off slightly, a different value for distance would be calculated, and your\nr eceiver\u2019s position could be placed at a completely different location away from\nits real position. This becomes a bigger issue because the satellite has a super-\nprecise, but very expensive (somewhere in the range of tens of thousands of\ndollars) atomic clock, while the off-the-shelf GPS receiver has a less-precise\n(and inexpensive) quartz clock. Obviously, GPS receivers can\u2019t have atomic\nclocks in them or else they would be far too expensive to purchase. As such,\nthe receiver\u2019s clock isn\u2019t as accurate as the atomic clock, and time differences\nFIGURE 4.6 The results\nof 3D trilateration to find\na location using GPS.\nPosition at one of\ntwo possible points\n1\n2\n3 81\nWhy Isn\u2019t GPS Perfectly Accurate?\ncan cause inaccuracies in distances when it comes to finding a position. Fortu-\nnately, there\u2019s an easy solution to this mess, and that is, your receiver picks up\ninformation from a fourth satellite. By using a fourth measurement, the clock\nerrors can be corrected for and your position can be accurately determined.\nThe position you\u2019ll receive is measured using the WGS84 datum.\nWhy Isn\u2019t GPS Perfectly Accurate?\nIf GPS can be used to find a location on Earth\u2019s surface, the next question to\nask is \u201cjust how accurate is GPS?\u201d Through the use of pseudorange measure-\nments from four satellites, it would seem the system should only give the user\none accurately measured location. However, in average everyday use with an\nordinary off-the-shelf civilian receiver, accuracy would be about 15 meters (or\nless) in the horizontal direction. This doesn\u2019t mean that the location is always\noff by 15 meters\u2014it could be off by 5, 10, or 15 meters. This is due to a number\nof factors that can interfere with or delay the transmission or reception of the\nsignal\u2014and these delays in time can cause errors in position.\nOne of the biggest sources of error was actually a deliberate one worked\ninto the system to intentionally make GPS less accurate. The United States in-\ntroduced Selective Availability (SA) into GPS with the goal of intentionally Selective Availability\nthe intentional\nmaking the C\/A signal less accurate, presumably so that enemy forces couldn\u2019t\nalteration of the\nuse GPS as a tool against the United States military. Selective Availability intro-\ntiming and position\nduced two errors into the signals being transmitted by the satellites\u2014a delta er- information transmitted\nror (which contained incorrect clock timing information) and an epsilon error by a GPS satellite.\n(which contained incorrect satellite ephemeris information). The net effect of\nthis was to make the accuracy of the C\/A about 100 meters. As you can imagine,\nthis level of accuracy certainly limited GPS in the civilian sector\u2014after all, who\nwants to try to land an airplane with GPS when the accuracy of the runway\nlocation could be hundreds of feet off? In the year 2000, the U.S. Government\nturned Selective Availability off and has no announced plans to turn it back on\n(in fact, the U.S. Department of Defense has announced that new generations\nof GPS satellites will not have the capability for Selective Availability).\nWhile Selective Availability is an intentional error originating from the\nsatellites, other errors can naturally occur at the satellite level. Ephemeris\nerrors can occur when the satellite broadcasts incorrect orbit position infor-\nmation. These types of errors indicate that the position of the satellite is not\nwhere it is supposed to be and a correction of ephemeris data hasn\u2019t been\nupdated from a control station. Typically, these errors can introduce about\n2 meters (or so) of error.\nEven the arrangement of satellites in space (that is, how they are distributed)\nPDOP the Position\ncan have an effect on the position determination. The further the satellites are\nDilution of Precision\u2014it\nspread out from one another and the wider the angle between them, the more describes the amount\naccurate a measurement will be obtained by the receiver. The error introduced by of error due to the\na poor geometric arrangement of the satellites is referred to as the Position Dilu- geometric position of\nthe GPS satellites.\ntion of Precision or PDOP. Some receivers will calculate a value for PDOP for you 82\nChapter 4 Finding Your Location with the Global Positioning System\nFIGURE 4.7 The effects\nof the ionosphere and\ntroposphere on GPS\nsignals.\nSpace\nAltered signal\npropagation\nIonosphere\nTroposphere\nto indicate what relative error could be introduced because of the arrangement of\nthe satellites being used (basically, the lower the value the better).\nAlso keep in mind that the signals are being broadcast from space and\nhave to pass through Earth\u2019s atmosphere before reaching the receiver.\nThough some of these atmospheric effects can be modeled and attempted to\nbe accounted for, atmospheric problems can still introduce errors in accuracy\nby making changes to the speed of the signals (Figure 4.7). Atmospheric in-\nterference, especially in the ionosphere and troposphere, can cause delays in\nthe signal before it gets to the ground. The ionosphere can alter the propaga-\ntion speed of a signal, which can cause an inaccurate timing measurement,\nwhile the amount of water vapor in the troposphere can also interfere with\nthe signals passing through it and also cause delays. It\u2019s estimated that the\nproblems caused by the ionosphere can cause about 5 meters (or more) of\nposition inaccuracy, while the troposphere can add about another 0.5 meters\nof measurement error.\nThe atmosphere is not the only thing that can interfere with the signal\nreaching the receiver. The receiver needs a good view of the sky to get an\ninitial fix, so things like heavy tree canopy may interfere with the signal re-\nception. In addition, the signals may be reflected off objects (such as tall\nbuildings or bodies of water) before reaching the receiver rather than mak-\nmultipath effect an ing a straight path from the satellite (Figure 4.8). This multipath effect can\nerror caused by a delay introduce additional delays into the reception of the signals since the signal is\nin the signal due to\nreaching the receiver later than it should (and add further error to your posi-\nreflecting from surfaces\ntion determination).\nbefore reaching the\nreceiver. All of these various types of error measurements can stack onto one an-\nother to possibly contribute several meters worth of errors into your position\ndetermination. When ephemeris introduces a few meters of error, the iono-\nsphere and troposphere add a few more, and then multipathing and PDOP\nadd a few more, before you know it they all start to add up to some significant 83\nHow Can You Get Better Accuracy from GPS?\nFIGURE 4.8 The effect\nof multipathing on the\nGPS signal.\nSingle path from\nsatellite to receiver\n1\n2\nMultipath signal reaches receiver\nlater and causes errors\ndifferences in where your position gets plotted. So, there should be some\nmanner of solutions to improve the accuracy of GPS and get a better fix on\nyour position.\nHow Can You Get Better Accuracy from GPS?\nDGPS differential\nThere are a few methods of obtaining a more accurate position measurement\nGPS\u2014a method using\nusing GPS to try to overcome the error sources just described. The first of a ground-based\nthese is a process referred to as Differential GPS (or DGPS). This method correction in addition to\nuses a series of base stations at locations on the ground to provide a correc- the satellite signals in\nposition determination.\ntion for GPS position determination. The base station is positioned at a place 84\nChapter 4 Finding Your Location with the Global Positioning System\nFIGURE 4.9 The\noperation of DGPS.\nDGPS\nwhere the coordinates are known and it receives signals from the satellites.\nSince it knows what its coordinates are supposed to be, it can calculate a cor-\nrection to compensate for errors in where the position measurement from\nthe satellites says it should be. The base station can then broadcast this cor-\nrection out to receivers equipped to collect this differential correction. Thus,\nwhen you\u2019re using DGPS, your receiver is picking up the usual four satellite\nsignals plus an additional correction from a nearby location on the ground\n(see Figure 4.9 for an example of the operation of DGPS). As a result of this,\nthe position measurement will get reduced to less than 5 meters.\nToday, there are a number of DGPS locations worldwide. In the United\nNDGPS the National\nStates, the U.S. Coast Guard has set up several reference stations along the\nDifferential GPS\u2014it\nconsists of ground- coasts and waterways to aid ships in finding their location and navigation.\nbased DGPS beacons The U.S. Department of Transportation operates the NDGPS (Nationwide\naround the United Differential GPS) system of DGPS locations across the country, aimed at pro-\nStates.\nviding more accurate position information to drivers and travelers. NDGPS\nCORS the Continuously reduces position error down to one to two meters.\nOperating Reference A related program, CORS (Continuously Operating Reference Stations),\nStations\u2014a system\nis operated by the National Geodetic Survey, and consists of numerous base\noperated by the\nstations located across the United States and other places throughout the\nNational Geodetic\nSurvey to provide world. CORS collects GPS information and makes it available to users of GPS\na ground-based data (such as surveyors and engineers) to make more accurate position mea-\nmethod of obtaining surements. CORS data provides a powerful tool for location information for a\nmore accurate GPS\nvariety of applications (including being established in Iraq by the U.S. Army\npositioning.\nto aid in rebuilding there). 85\nHow Can You Get Better Accuracy from GPS?\nSatellite Based Augmentation Systems (SBAS) are additional resources\nSBAS Satellite\nto use in improving accuracy. SBAS works similar to DGPS in that a correc-\nBased Augmentation\ntion is calculated and picked up by your receiver in addition to the regular System\u2014a method\nfour satellites. However, in this case, the correction is being sent from an of using correction\nadditional new satellite, not from an Earth-bound station. An example of information sent from\nan additional satellite\na SBAS is called the Wide Area Augmentation System (WAAS), which was\nin the GPS position\ndeveloped by the Federal Aviation Administration (FAA) to obtain more\ndetermination.\naccurate position information for aircraft. WAAS has since spread to uses\nWAAS the Wide\nbeyond aircraft as a quick and easy method of improving overall GPS posi-\nArea Augmentation\ntion accuracy.\nSystem\u2014a satellite\nWAAS operates through a series of base stations spread throughout the based augmentation\nUnited States that collect and monitor the signals sent by the GPS satellites. system that covers\nThese base stations calculate position correction information (similar to the the United States and\nother portions of North\noperation of DGPS) and transmit this correction to a geostationary WAAS\nAmerica.\nsatellite. These WAAS satellites then broadcast this correction signal back\nto Earth, and if your receiver can pick up this signal, it can use this correc-\ntion information in its position calculation (Figure 4.10). WAAS reduces the\nposition error to 3 meters or less with its use. A drawback to WAAS is that\nit will only function within the United States (including Alaska and Hawaii) FIGURE 4.10 The setup\nand nearby portions of North America (although WAAS base stations have and operation of the\nWAAS system.\nGPS satellites\nWAAS satellite\nL1 L1\nWAAS ground WAAS WAAS\nstation master station reference station 86\nChapter 4 Finding Your Location with the Global Positioning System\nrecently been added to Mexico and Canada to greatly expand the range of\nWAAS benefits). Other systems exist beyond WAAS and DGPS but use similar\nconcepts (some sort of correction is calculated and transmitted to a receiver\nfor it to use in its position determination).\nWhat Other GNSS Are There Beyond GPS?\nThis chapter has been devoted to the operation, errors, and accuracy issues\nsurrounding GPS, but as mentioned early on, \u201cGPS\u201d refers to the US NAV-\nSTAR GPS. Other countries have developed (or are in the process of develop-\ning) their own versions of GPS or GPS augmentation systems, presumably as\nnot to be completely dependent on U.S. technology. There are other full con-\nstellations of GPS-style satellites being developed and placed into orbit, with\nnumerous other enhancement systems in operation or in planning around\nthe globe.\nGLONASS the GLONASS was the name of the USSR\u2019s counterpart to GPS and oper-\nformer USSR\u2019s (now ated in a similar fashion. The GLONASS equivalent to the C\/A code oper-\nRussia\u2019s) GNSS, ated at horizontal accuracies of about 100 m (or better) with an equivalent\ncurrently rebuilding\nof the P code for more accurate use. GLONASS consisted of a full constel-\nto a constellation of\nlation of satellites with an orbital setup similar to GPS. By 2001, however,\nsatellites.\nthere were only a few GLONASS satellites in operation, but Russia now has\na renewed program and more GLONASS satellites have been launched, to\npossibly have a full operational constellation in the near future. In 2007,\nRussia indicated that the civilian signal from GLONASS would be made freely\navailable for use.\nGalileo the European Galileo is the European Union\u2019s version of GPS, which, when com-\nUnion\u2019s GNSS, currently pleted, is projected to have a constellation of 30 satellites and operate in a\nin development. similar fashion to GPS. The first of the GIOVE (Galileo In Orbit Validation\nElement) satellites was launched in 2005, and the program has continued\ndevelopment from then. Galileo promises four different types of signals, in-\ncluding information available to civilians as well as restricted signals. Also\nCompass China\u2019s in development is China\u2019s version of GPS, Compass, which is projected to\nGNSS, currently in also have a full constellation of satellites. The first Compass satellite was\ndevelopment.\nlaunched in 2007.\nOther Satellite Based Augmentation Systems have been developed across\nthe world to act similar to WAAS but to function in other regions outside of\nEGNOS a Satellite North America. EGNOS (European Geostationary Navigation Overlay Sys-\nBased Augmentation tem) is sponsored by the European Union to provide improved accuracy of\nSystem that covers GPS throughout Europe. EGNOS functions the same way WAAS does\u2014a se-\nEurope.\nries of base stations throughout Europe monitor GPS satellite data, calculate\ncorrections, then transmit this correction to geostationary EGNOS satellites\nMSAS a Satellite Based over Europe, and these satellites broadcast this data to Earth. An EGNOS-\nAugmentation System enabled receiver is necessary to utilize this correction data, and the system\nthat covers Japan and is capable of accuracies of about 1.5 meters. Another SBAS operates out of\nnearby regions.\nJapan, called MSAS (Multifunctional Satellite Augmentation System), which 87\nWhat Are Some Applications of GPS?\ncovers Japan and portions of the Pacific Rim. MSAS operates in a similar way\nto WAAS and EGNOS, just in a different region of the world and consists of\ntwo satellites providing coverage.\nMany other GPS enhancement systems exist or are in development\nthroughout the world, including the following:\na Beidou\u2014an SBAS operating in China\na GAGAN\u2014an SBAS in development for India\na IRNSS\u2014an Indian satellite navigation system, consisting of seven satel-\nlites, currently in development\na QZSS\u2014an expanded SBAS based in Japan, currently in development\nWhat Are Some Applications of GPS?\nGPS is used in a wide variety of settings. Any application that requires\nlocation-based data to be collected firsthand can apply GPS to the problem.\nThe following are just a handful of ways GPS is being used in businesses, jobs,\nand the public sector.\nEmergency Response\nWhen you make a 911 call, the call center is able to fix your location and route\nemergency services (police, firefighters, or ambulances) to where you are.\nWhen you make a 911 call from a cell phone, the Enhanced 911 (E-911) sys-\ntem can determine your position\u2014either using the cell towers to determine\nwhere you are, or using a GPS receiver in your phone to find your location.\nFarming\nPrecision farming relies on getting accurate location data about factors such\nas watering levels, crops, or soils. Having the ability to map this data gives\nfarmers information about where crops are thriving and what areas need at-\ntention. GPS serves as a data-collection tool to quickly (and accurately) ob-\ntain location information for farmers (that can then be combined with other\naspects of geospatial technology, such as the mapping and analysis capabili-\nties of GIS or remotely sensed imagery).\nForensics\nHuman remains recovery has been greatly assisted through the use of GPS.\nRather than having to rely solely on surveying equipment, GPS can provide\nquick data and measurements, and can provide an additional tool in addition\nto other field-based measurements. The same concepts have been applied to\nidentifying the locations of artifacts and remains at archeological sites.\nPublic Utilities\nCity or county data for items that need to be measured manually (such as cul-\nverts, road signs, or manhole locations) can be done quickly and easily with 88\nChapter 4 Finding Your Location with the Global Positioning System\nGPS. When field-based data is necessary to create or update utility data, work-\ners equipped with GPS can collect this type of data quickly and accurately.\nTransportation\nGPS provides a means of continuously tracking travel, whether on the road,\nin the water, or in the air. Since GPS receivers can constantly receive signals\nfrom the satellites, your travel progress can be monitored to make sure that\nyou stay on track. When your GPS-determined position can be plotted in real-\ntime on a map, you can see just where you\u2019ve traveled to and measure your\nprogress. Delivery and shipping companies can use this type of data to keep\ntabs on their fleets of vehicles to see if they deviate from their routes.\nWildlife Management\nSimilarly to tracking a vehicle, wildlife and endangered species can be\ntagged with a GPS receiver which monitors their location and transmits it to\na source capable of tracking those movements. In this way, migration pat-\nterns of animals can be determined and patterns of wildlife movements can\nbe observed.\nBeyond its uses in the private and public sectors, GPS has become a tool\nto assist in personal recreation. If you\u2019re a runner, GPS can track your posi-\ntion while exercising, along with measuring your speed. If you\u2019re a hiker,\ngeocaching using GPS can keep you from getting lost when you\u2019re deep into unfamiliar terri-\nGPS to find locations of tory. If you\u2019re into fishing, GPS can help locate places supposed to be well\nhidden objects based stocked with fish. If you\u2019re a golfer, you can use GPS to measure your ball\u2019s\non previously obtaining\nlocation and the distance to the pin. In fact, there\u2019s a whole new form of\na set of coordinates.\nrecreation that\u2019s grown in popularity along with GPS, called geocaching.\nThinking Critically with Geospatial Technology 4.1\nWhat Happens if GPS Stops Working?\nWhat happens if the Global Positioning System would be impacted without having access to GPS?\nstops working\u2014or at least working at 100% capa- What would be the effect on navigation of aircraft or\nbility? If the satellites stop operating without be- other travel? What are the military implications of a\ning replaced, GPS would essentially stop function- world without GPS? How would all of these factors\ning. If the number of satellites would drop below affect your life?\nthe full operational capacity, then GPS would work By the same token, is society too reliant on GPS?\nsome of the time (or could have greatly reduced Since Selective Availability was turned off in 2000,\naccuracy). The scenario of an operational failure there\u2019s been an overwhelming amount of GPS us-\nof GPS is explored in this article from GPS World: age in many aspects of everyday life. Is the avail-\nhttp:\/\/www.gpsworld.com\/gnss-system\/news\/ ability of GPS taken for granted within society? If\ngps-risk-doomsday-2010-7092. the system could fail or be negated, what impacts\nIf something like this would come to pass, what would this have on people reliant on this technology\nwould be the effect of a lack of GPS? What industries always being available to them? 89\nChapter Wrapup\nHands-on Application 4.2\nThings to Do Before You Go Geocaching\nGeocaching is a widespread and popular recre- 2. Waymarking.com (http:\/\/www.waymarking.\national activity today. As long as you have a GPS com) \u2013 This Website provides a way to find\nreceiver and some coordinates to look for, you can the coordinates of interesting locations\nget started with geocaching. This chapter\u2019s lab will (\u201cwaymarks\u201d) that have been uploaded by\nhave you investigating some specific geocaching users. Type in your zip code to see what\u2019s\nresources on the Web, but before you start looking been labeled as a waymark (and collect its\nfor caches, there are some online utilities to explore coordinates) near you.\nfirst. Open your Web browser and go to http:\/\/www. 3. Whereigo.com (http:\/\/www.wherigo.\ngroundspeak.com. Groundspeak operates a main com) \u2013 A Website to download a player and\ngeocaching Website as well as providing some tools\na toolset for constructing games (referred\nto use. On the main page are links to several GPS-\nto as \u201ccartridges\u201d) and other applications for\nrelated sites:\nportable GPS devices.\n1. Geocaching.com (http:\/\/www.geocaching. 4. Cache In Trash Out (http:\/\/www.geocaching.\ncom) \u2013 Check around for locations of com\/cito) \u2013 An environmental initiative\ngeocaches near you (we\u2019ll use this during dedicated to cleaning up areas where\nthe lab). geocaches are hidden.\nIn geocaching, certain small objects (referred to as \u201cgeocaches\u201d) are hid-\nden in an area and the coordinates of the object are listed on the Web (a\nlog is maintained). From there, you can use a GPS receiver to track down\nthose coordinates (which usually involve hiking or walking to a location) and\nl ocate the cache. In essence, geocaching is a high-tech outdoor treasure hunt\n(see Hands-on Application 4.2: Things to Do Before You Go Geocaching). This\ntype of activity (using a GPS receiver to track down pre-recorded coordinates\nto find objects or locations) is also used in educational or classroom activities\nto teach students the relationship between a set of map coordinates and a\nreal-world location, as well as basic land navigation and GPS usage.\nChapter Wrapup\nThis chapter provided an introduction to how the Global Positioning System\noperates as well as how GPS can locate where a receiver is on the ground.\nA growing application of GPS is navigation systems used in your car, such\nas brands like G armin, Magellan, or Tom-Tom. These types of systems plot\nnot only your location on a map using GPS, but also show surrounding roads\nand nearby locations (such as restaurants or gas stations), locate an address,\nand then route you from your current location to another destination via\nthe shortest path. The only one of these features that\u2019s actually using GPS is\nfinding your current location\u2014the rest are all functions that are covered in\nChapter 8. Until then, starting in the next chapter, we\u2019ll start focusing on the 90\nChapter 4 Finding Your Location with the Global Positioning System\nactual computerized mapping and analysis that underlies those types of in-car\nsystems, all concepts of geographic information systems.\nThis chapter\u2019s lab will start putting some of these GPS concepts to work,\neven if you don\u2019t have a GPS receiver.\nImportant note: The references for this chapter are part of the online\ncompanion for this book and can be found at http:\/\/www.whfreeman.com\/\nshellito1e.\nKey Terms\nGPS (p. 73) trilateration (p. 78)\nNAVSTAR GPS (p. 74) 3D trilateration (p. 80)\nGNSS (p. 74) Selective Availability (p. 81)\nspace segment (p. 74) PDOP (p. 81)\nconstellation (p. 74) multipath effect (p. 82)\ncontrol segment (p. 76) DGPS (p. 83)\nuser segment (p. 77) NDGPS (p. 84)\nchannels (p. 77) CORS (p. 84)\nsingle frequency (p. 77) SBAS (p. 85)\ndual frequency (p. 77) WAAS (p. 85)\nalmanac (p. 77) GLONASS (p. 86)\nephemeris (p. 77) Galileo (p. 86)\nC\/A code (p. 77) Compass (p. 86)\nP code (p. 77) EGNOS (p. 86)\nY code (p. 77) MSAS (p. 86)\npseudorange (p. 78) geocaching (p. 88) 4.1\nGeospatial Lab Application\nGPS Applications\nThis chapter\u2019s lab will examine using the Global Positioning System for a va-\nriety of activities. The big caveat is that the lab can\u2019t assume that you have ac-\ncess to a GPS receiver unit and would be able to use it in your immediate area\n(and unfortunately, we can\u2019t issue you a receiver with this book). It would also\nbe useless to describe exercises involving you running around (for instance)\nBoston, Massachusetts, collecting GPS data, when you may not reside any-\nwhere close to Boston.\nThus, this lab will use the free Trimble Planning Software for examining\nGPS satellite positions and other factors used in planning for the use of GPS\nfor data collection. In addition, some sample Web resources are used to exam-\nine related GPS concepts for your local area.\nFor some good introductory information about Trimble Planning\nSoftware (whose info helped guide the development of this lab), also\ncheck out the article written by Leszek Pawlowicz titled \u201cDetermin-\ning Local GPS Satellite Geometry Effects On Position Accuracy.\u201d This\narticle is available online at http:\/\/freegeographytools.com\/2007\/\ndetermining-local-gps-satellite-geometry-effects-on-position-accuracy.\nImportant note: Though this lab is short, it can be significantly expanded\nif you have access to GPS receivers. In that case, some of the geocaching ex-\nercises described in Section 4.5 of this lab can be implemented. Alternatively,\nrather than just examining cache locations on the Web (as in Section 4.4), you\ncould use the GPS equipment to hunt for the caches themselves.\nObjectives\nThe goals for you to take away from this lab are:\na Examine satellite visibility charts to determine how many satellites are\navailable in a particular geographic location, as well as when during the\nday these satellites are available.\na Read a DOP chart to determine what times of day have higher and lower\nvalues for DOP for a particular geographic location.\na Explore some Web resources to see where publicly available geocaches or\npermanent marker sites are that you can track down with a GPS receiver.\nObtaining Software\nThe current version of Trimble Planning Software (2.9) is available for free\ndownload at http:\/\/www.trimble.com\/planningsoftware_ts.asp.\n91 92\nChapter 4 Finding Your Location with the Global Positioning System\nImportant note: Software and online resources sometimes change fast. This\nlab was designed with the most recently available version of the software at the\ntime of writing. However, if the software or Websites have significantly changed\nbetween then and now, an updated version of this lab (using the newest ver-\nsions) is available online at http:\/\/www.whfreeman.com\/shellito1e.\nLab Data\nThere is no data to copy in this lab. All data comes as part of the Trimble Plan-\nning Software that is installed with the program or can be downloaded as part\nof the lab.\nLocalizing This Lab\nNote: This lab uses one location and one date to keep information consistent.\nAlthough this lab looks at GPS sky conditions in Orlando, Florida, there are\nmany more geographic locations that can be selected using Trimble Plan-\nning Software. Its purpose is to be able to evaluate local conditions for GPS\np lanning\u2014so rather than selecting Orlando, find the closest city to your loca-\ntion and examine that data instead. You can also use today\u2019s date rather than\nthe one used in the lab.\nSimilarly, in Section 4.4 of the lab, you\u2019ll be examining Web resources for\ngeocaching locations in Orlando, Florida. Use your own zip code rather than\nthe one for Orlando and see what\u2019s near your home or school instead.\n4.1 Trimble Planning Software Setup\n1. Start the Trimble Planning Software program\u2014the default install\nfolder is called Trimble Office\u2014and then look inside the Utilities\nfolder where you will find an icon called Planning. This will launch the\nprogram.\n2. The first thing to do is download a copy of the most current almanac file.\nThis can be done from Trimble\u2019s Website at http:\/\/www.trimble.com\/\ngpsdataresources.shtml and selecting the option for GPS\/GLONASS\nalmanac in Trimble Planning file format. Download the file (it\u2019s\na small text file) by right-clicking on the hyperlink (GPS\/GLONASS\nalmanac in Trimble Planning file format) and click on Save Target\nAs . . . from the dialog box. Click on the Save button and the file\n\u201calmanac.alm\u201d will be downloaded to your computer.\n3. In the Planning program, select Load from the Almanac pull-down\nmenu. In the Almanac Files dialog, navigate to the location where you\nsaved the almanac.alm file, select it, and then click Open. A summary of\nthe satellite information contained in the file will appear on the screen\nand the almanac information will be loaded. 93\nGPS Applications\n4.2 Local Area GPS Information\nThe visibility of GPS satellites will change with the time of day and your posi-\ntion on the Earth. The Trimble Planning program will allow you to examine\ndata from stations around the Earth to determine GPS information.\n1. From the File pull-down menu, select Station. You could also select the\nStation icon from the toolbar:\n(Source: Courtesy of Trimble)\nThe Station Editor dialog box will open.\n(Source: Courtesy of Trimble)\n2. The Station Editor allows you to select any station across the world. For\nthis portion of the lab, we\u2019ll be examining GPS satellite visibility near\nOrlando, Florida.\n3. To select an area, click on the City . . . button on the right. From the pull-\ndown list, choose Orlando, FL.\nImportant note: An alternate method for selecting a location is to click\non the Map . . . button. A world map will appear that allows you to\nselect based on a geographic location\u2014scroll the mouse across the\nmap to see what options are available, and then double-click on the\ncity you want. 94\nChapter 4 Finding Your Location with the Global Positioning System\n(Source: Courtesy of Trimble)\n4. Make sure the Time Zone is set to Eastern Time (United States and\nCanada).\n5. You can also enter values for time and date. Change the date for this lab\nto 3\/10\/2011 (as this is the date used for measurements).\n6. When the settings are correct, click OK.\n7. The Trimble Planning Software will now report back satellite information\nin relation to Orlando.\n4.3 Satellite Visibility Information\n1. Trimble Planning Software allows you to examine satellite visibility (and\nother factors) from a local area at various times throughout the day. To\nchoose which group of satellites you\u2019re examining, put checkmarks in\nthe Satellite Systems boxes below the toolbar:\n(Source: Courtesy of Trimble)\n2. For now, only select the GPS box.\n3. First, we\u2019ll examine the number of GPS satellites potentially visible from\nOrlando at various points of the day. From the Graphs pull-down menu,\nselect Number of Satellites. You could also choose the Number of\nSatellites icon from the toolbar: 95\nGPS Applications\n(Source: Courtesy of Trimble)\n4. The number of satellites visibility graph will appear, showing the\nnumber of visible satellites on the y-axis and a 24-hour time frame on\nthe x-axis.\nQuestion 4.1 At what times of day are the maximum number of GPS\nsatellites visible? What is this maximum number?\nQuestion 4.2 At what times of day are the minimum number of GPS\nsatellites visible? What is this minimum number?\n5. To determine which satellites are visible at specific times, go to the\nGraphs pull-down menu, then choose Visible Satellites, then choose\nGeneral Visibility. You could also choose the Visibility icon from the\ntoolbar:\n(Source: Courtesy of Trimble)\n6. The Visibility graph will appear, showing the specific GPS satellite\nnumber on the y-axis and a 24-hour time frame on the x-axis.\nQuestion 4.3 Which satellites are visible from Orlando at noon?\nQuestion 4.4 Which satellites are visible from Orlando at 4:00 pm?\n7. To examine the Position Dilution of Precision (PDOP) at a particular\ntime of day, from the Graphs pull-down menu, select DOP, then select\nAll Together. You could also choose the DOPs icon from the toolbar:\n(Source: Courtesy of Trimble)\n8. The graph for Dilution of Precision will appear, showing the values for\nDOP on the y-axis and a 24-hour time frame on the x-axis.\nQuestion 4.5 At what time(s) of day will the maximum values for DOP be\nencountered? What is this maximum value?\nQuestion 4.6 At what time(s) of day will the minimum values for DOP be\nencountered? What is this minimum value?\n9. Take a look at all the graphs and answer Question 4.7. 96\nChapter 4 Finding Your Location with the Global Positioning System\nQuestion 4.7 Just based on satellite visibility and DOP calculations, what\nwould be some of the peak times to perform GPS field measurements in\nOrlando? Also, what would be some of the \u201cworst\u201d times?\n10. Close all the graphs, then uncheck the GPS box in Satellite Systems and\nplace a checkmark in the WAAS box instead.\n11. Examine the Visibility chart for the available WAAS satellites.\nQuestion 4.8 How many WAAS Satellites does Trimble Planning say are\navailable? At what times are they available? Why is this?\n12. Next, examine the Sky Plot (a chart showing the position of the satellites\nin orbit) for these WAAS satellites. From the Graphs pull-down menu,\nselect Sky Plot. You could also choose the Sky Plot icon from the toolbar:\n(Source: Courtesy of Trimble)\nQuestion 4.9 Based on the information from the visibility chart and the\nsky plot, what satellites are these? (Hint: The software says \u201cWAAS,\u201d but are\nthey all strictly functional WAAS satellites? You may have to look up some\nnames.)\n13. At this point, you can close all windows and exit the Trimble Planning\nSoftware.\n4.4 GPS and Geocaching Web Resources\nWith some preliminary planning information available from Trimble Plan-\nning, you can begin to head out with a GPS receiver. For some starting points\nfor GPS usage, try the following:\n1. Open a Web browser and go to http:\/\/www.geocaching.com. This\nWebsite is home to several hundred thousand geocaches at locations\nthroughout the world. The Website will list the coordinates of geocaches\n(small hidden items) so that you can use a GPS to track them down.\n2. On the main page of the Website, enter 32801 (one of Orlando\u2019s\nmany zip codes) in the box that asks for a zip code. Several potential\ngeocaches should appear on the next Web page.\n3. You can also search for benchmarks (permanent survey markers set\ninto the ground) by using the Website\u2019s Find A Benchmark option (at\nthe time of writing, it was listed at the bottom of the Webpage). On the\nbenchmark page, input 32801 for the zip code to search for.\nQuestion 4.10 How many benchmarks can be located within less than one\nmile of the Orlando 32801 zip code? What are the coordinates of the closest\none to the origin of the zip code? 97\nGPS Applications\n4.5 GPS and Geocaching Applications\nAs mentioned back in the introduction, there\u2019s a lot more that can be done\na pplication-wise if you have access to a GPS receiver. For starters, you could\nfind the positions of some nearby caches or benchmarks by visiting http:\/\/\nwww.groundspeak.com or http:\/\/www.geocaching.com and then track-\ning them down using the receiver and your land navigation skills. H owever,\nthere are a number of different ways that geocaching concepts can be\nadapted in different ways to be incorporated into a classroom setting for lab\napplications.\nThe first of these is based on material developed by Dr. Mandy Munro-\nStasiuk and published in her article \u201cIntroducing Teachers and University\nStudents to GPS Technology and Its Importance in Remote Sensing Through\nGPS Treasure Hunts\u201d (see the online chapter references for the full citation\nof the article). Like geocaching, the coordinates of locations have been de-\ntermined ahead of time by the instructor and given to the participants (such\nas university students or K\u201312 teachers attending a workshop) along with a\nGPS receiver. Having only sets of latitude\/longitude or UTM coordinates, the\nparticipants break into groups and set out to find the items (such as statues,\nmonuments, building entrances, or other objects around their campus or lo-\ncal area). The students are required to take a photo of the object, and all\nparticipants must be present in the photo (this usually results in some highly\ncreative picture taking). The last twist is that the \u201ctreasure hunt\u201d is a timed\ncompetition with the team that returns first (and having found all the correct\nitems) earning some sort of reward, such as extra credit in the class, or\u2014at\nthe very least\u2014bragging rights for the remainder of the class. In this way,\nparticipants learn how to use the GPS receivers, how to tie coordinates to\nreal-world locations, and land navigation skills (to further reinforce concepts\nsuch as how northings and eastings work in UTM).\nA similar version of this GPS activity is utilized during the OhioView\nS ATELLITES Teacher Institutes (see Chapter 15 for more information on this\nprogram). Again, participants break up into groups with GPS receivers, but\nthis time each group is given a set of small trinkets (such as small toys or plas-\ntic animal figures) and is instructed to hide each one and register the coordi-\nnates of the hiding place with their GPS receiver on a sheet of paper. When the\ngroups reconvene, the papers with the coordinates are switched with another\ngroup, and each group now has to find the others\u2019 hidden objects. In this way,\neach group sets up its own \u201cgeocaches\u201d and then gets challenged to find an-\nother group\u2019s caches. Like before, this is a timed lab with a reward awaiting\nthe team of participants that successfully finds all of the hidden items at the\ncoordinates they\u2019ve been given. This activity helps to reinforce not only GPS\nusage and land navigation, but also the ties between real-world locations and\nthe coordinates being recorded and read by a GPS receiver. Both of these ac-\ntivities have proven highly successful in linking these concepts to the partici-\npants involved. 98\nChapter 4 Finding Your Location with the Global Positioning System\nHowever you get involved with geocaching, a useful utility for managing\ngeocached data and waypoints from different software packages is the Geo-\ncaching Swiss Army Knife, available for free download at http:\/\/www.gsak.\nnet. This utility comes with a free trial and helps manage geocaching data.\nClosing Time\nThis lab illustrated some initial planning concepts and some directions in\nwhich to take GPS field work (and some caches and benchmarks that are out\nthere waiting for you to find) to help demonstrate some of the chapter\u2019s con-\ncepts. The two geocaching field exercises described above can also be adapted\nto classroom use to help expand the computer portion of this lab with some\nGPS field work as well.\nStarting with Chapter 5, you\u2019ll begin integrating some new concepts of\ngeographic information systems to your geospatial repertoire. Part 2\nGeographic Information Systems\n5\nWorking With Digital\nSpatial Data and GIS\nGeographic Information Systems, Modeling the Real World,\nVector Data and Raster Data, Attribute Data, Joining Tables,\nMetadata, Esri, and ArcGIS\nOnce you\u2019ve measured and collected your geospatial data, it\u2019s time to do\nsomething with it. The nature of spatial data lends itself to concepts like ana-\nlyzing where one location lies in relation to another or the capabilities of one\nlocation versus another. When a new library is to be built, it should be placed\nat the optimal location to serve the greatest number of people without dupli-\ncating the usage of other libraries in the area. When a company is deciding\nwhere to open a new retail store, they\u2019ll need information about the location,\nthe surrounding areas, and populations and demographics of these areas to\nmake a decision. When a family is planning its vacation to Florida, it\u2019ll want\nto know the best routes to travel and where the hotels are located in relation\nto the theme parks they\u2019re planning to visit. All of these concepts involve the\nnature of spatial data and being able to analyze or compare locations (and\nthe attributes of these locations). Geospatial technology is used to address\nthese ideas and solve these types of problems (and many, many more). When-\never examination, manipulation, or analysis of spatial data is involved (for\ninstance, if you need to tie non-spatial data to a location or if you want to use Geographic\nInformation System\nthis information to create a map of your analysis), Geographic Information\n(GIS) a computer-\nSystems (GIS) are essential to getting these tasks done.\nbased set of hardware\nGIS is a powerful tool for analyzing spatial data (the \u201cgeographic in- and software used\nformation\u201d of the title). This ability to explicitly utilize spatial data is what to capture, analyze,\nseparates GIS from other information systems or methods of analysis. For manipulate, and\nvisualize spatial\ninstance, a spreadsheet would allow you to tabulate housing values and the\ninformation.\namount of money paid for a house at a particular address. Even though you\u2019re\n99 100\nChapter 5 Working With Digital Spatial Data and GIS\nusing data that has something to do with a location, there\u2019s nothing in the\nspreadsheet that allows you to examine that data in a spatial context (such as\ntranslating the housing values into a map of where these houses are).\nThe ability to map the real-world locations of the houses, spatially exam-\nine trends in housing values, and search for patterns or clusters in the spatial\ndata is what makes GIS unique. Using the coordinate and measurement con-\ncepts discussed in the previous chapters allows us to create, map, and analyze\ndigital spatial data in a wide variety of geospatial technology applications (see\nFigure 5.1 and Hands-on Application 5.1: Using GIS Online for an example of\nGIS data used by a county government).\nGIS operates using some kind of computer-based environment\u2014whether\nit is software running on a desktop, laptop, mobile device, or served off the\nWeb, the \u201csystem\u201d being referred to is a computer system. The information\nbeing handled by GIS is spatial information\u2014something that has direct ties to\na location on the Earth\u2019s surface. GIS can also utilize non-spatial data and link\nit directly to a location. For instance, if you have a set of data points that rep-\nresent the locations of local car washes (that is, spatial data) and a separate\nspreadsheet of data of how many cars use that car wash (that is, non-spatial\ndata), you can link the usage data to the car wash location so that the location\ncontains that data. From there, you can do analysis of local population demo-\ngraphics or road conditions about how they relate to the usage statistics of the\ncar wash at that particular location. This wouldn\u2019t be possible without GIS\nbeing able to link non-spatial data to a location. Lastly, GIS can also perform\nmultiple types of operations related to spatial data\u2014it can be used to analyze,\ncapture, create, manipulate, store, or visualize all manner of spatial informa-\ntion, not just simply design maps (though it can do that too).\nFIGURE 5.1 The online\nGIS utility for Mahoning\nCounty, Ohio. (Source:\nCourtesy Mahoning County\nGIS Department.) 101\nHow Does GIS Represent Real-World Items?\nHands-on Application 5.1\nUsing GIS Online\nThere are numerous online GIS resources for viewing the box of the most recent available Ortho Photos\nand analyzing spatial data. Search on the Internet to (under the Imagery heading). Click on the Refresh\nsee if your county or local government has any on- button on the toolbar to see the results of adding\nline GIS resources or a GIS mapping Website set up. that layer (that is, high-resolution aerial photos of\nAn example of a local GIS is from Mahoning County, the park). Select some other layers to examine,\nOhio. Open your Web browser and go to http:\/\/gis. and then refresh the map (by pressing the Refresh\nmahoningcountyoh.gov and then select the op- button) to examine that data. Use the tools on the\ntion for Map Viewer. In the upper left-hand corner toolbar (the magnifying glass to zoom in or the hand\nof the screen will be the Property Search functions, to pan around the map) to get a closer look at the\nallowing you to search for a parcel of land by Parcel map and the surroundings of the park.\nNumber, Owner Name, or Address. Check your local state or county government\u2019s\nDo a search for parcel ID: 29-042-0-002.00-0 Websites to see if an online spatial data viewer (or\n(this is the parcel number for the Boardman Town- GIS) has been set up for your area. It may be part\nship Park). The map will zoom to the parcel of the of your county auditor\u2019s Website or a separate GIS\npark. In the lower left-hand corner of the screen are department for the state or county.\na series of available layers. Place a checkmark in\nGIS isn\u2019t a development that just popped up over the last couple of years.\nLarge-scale mapping efforts were done using computer-based cartography in\nthe 1950s and 1960s by a number of different government agencies and other\nlabs (including Harvard Laboratory). The term \u201cGIS\u201d first appeared in the same\ntime period of the early 1960s with the implementation of \u201cCGIS\u201d (the acro- CGIS the Canadian\nnym for the \u201cCanadian Geographic Information System\u201d), which was designed Geographic Information\nto provide large-scale mapping of land use in Canada. The development of CGIS System \u2014 a large land\ninventory system used\nis attributed to Roger Tomlinson, who has become known as \u201cthe father of GIS.\u201d\nin Canada and the first\nGIS developments have continued over the years through the efforts of gov-\ntime the name \u201cGIS\u201d\nernment agencies and the private sector, until the current time when GIS tech- was utilized for this\nnology and concepts are extremely widespread (see Hands-on Application 5.2: type of system.\nArcGIS Online, page 102, for another example of using GIS concepts online).\nHow Does GIS Represent Real-World Items?\nGIS has a multitude of applications, ranging from mapping natural gas pipe-\nlines to determining which sections of natural land are under the greatest\npressure to be developed into some sort of urban land use. No matter the ap-\nplication, the data (such as the locations and lengths of pipelines or the area\nand extent of land cover) needs to be somehow represented within the GIS. In\nthis sense, GIS provides a means to represent (or model) this data in a com-\nputer environment so that you can analyze or manipulate the data. A model\nindicates some sort of representation (or simplification, or generalization) of 102\nChapter 5 Working With Digital Spatial Data and GIS\nHands-on Application 5.2\nArcGIS Online\nThere are a lot of GIS applications available online developed and posted online. When opening a map\nthat are used for mapping and analysis of spatial application, you\u2019ll be placed into a GIS environment\ndata. For some interactive examples, open your where you can pan around the map, zoom in and\nWeb browser and go to http:\/\/www.arcgis.com\/ out, and use an \u201cidentify\u201d tool to get information\nhome (this is the Website for ArcGIS Online). We\u2019ll back about layers on the map. You can also select\ndiscuss a lot more about ArcGIS (and its manufac- the option for Make a Map, and a new Web page\nturer, Esri) later in this chapter, but for now, we\u2019ll will open where you can select an area of interest\nbe examining some of the online applications that (for instance, your state or county), then add data\nhave been developed with ArcGIS. From the main from ArcGIS Online or select a new basemap to use\npage, select the option for View the Gallery. Open (including street maps, topographic maps, or high-\nand examine some of the maps that people have resolution imagery).\nreal-world information. For instance, if you were tasked to make a map of all\nof the stop signs in your city, you\u2019d need to be able to represent the locations\nof all the signs on a map at the scale of the entire city. By laying out a map of\nthe city and placing a dot at each sign location, you would have a good repre-\nsentation of them. The same holds true if you had to come up with a map of\nall the roads in your town. You could attempt to draw the roads at their correct\nwidth, or more likely, just drawing a line along the road\u2019s path would suffice\nfor representing the road locations.\nWhenever you\u2019re deciding how to model real-world phenomena with a\nGIS, you first have to decide on the nature of the items you\u2019re trying to repre-\nsent. When trying to represent the real world in the GIS, there are two ways of\ndiscrete object view viewing the world. The first is called the discrete object view. Under this way\nthe conceptualization of thinking, the world is made up of a series of objects that have a fixed location,\nof the world that or a fixed starting and stopping point, or some sort of fixed boundary. A street\nall items can be\nstarts at one point and ends at another, or there is a definite property boundary\nrepresented with a\nbetween your property and your neighbor\u2019s. Around a university campus, there\nseries of objects.\nwill be numerous objects\u2014buildings, parking lots, stop signs, benches, roads,\ntrees, and sidewalks. By viewing the world in this way, real-world items can be\nmodeled in the GIS as a series of objects (such as a collection of lines to repre-\nsent roads or a series of points to represent fire hydrant locations).\nWhen adapting the discrete object view of the world to a data model, items\nin the real world are represented in the GIS by one of three objects (Figure 5.2):\npoints zero-dimensional\nvector objects. a Points: These are zero-dimensional objects, a simple set of coordinate\nlocations.\nlines one-dimensional\nvector objects. a Lines: These are one-dimensional objects, created from connecting starting\npolygons two- and ending points (and any points in between that give the line its shape).\ndimensional vector a Polygons: These are two-dimensional objects that form an area from a\nobjects.\nset of lines (or having an area defined by a line forming a boundary). 103\nHow Does GIS Represent Real-World Items?\nFIGURE 5.2 Basic points,\nlines, and polygons.\nPoints Lines Polygons\nThese three items are referred to as vector objects, as they make up\nvector objects points,\nthe basis of the GIS vector data model. Basically, the vector data model is a\nlines, and polygons\nmeans of representing and handling real-world spatial information as a series that are used to model\nof vector objects\u2014items are realized in the GIS as points, lines, or polygons. real-world phenomena\nFor instance, locations of fire hydrants, crime incidents, or trailheads can be using the vector data\nmodel.\nrepresented as points; roads, streams, or power conduits are represented as\nlines; and land parcels, building footprints, and county boundaries are repre- vector data model a\nsented as polygons. When dealing with a much larger geographic area (for in- conceptualization of\nrepresenting spatial\nstance, a map of the whole United States), cities may be represented as points,\ndata with a series of\ninterstates as lines, and state boundaries as polygons (Figure 5.3).\nvector objects (points,\nA lot of GIS data has already been created (such as road networks, parcel line, and polygons).\nboundaries, utilities, etc.), but in order to update or develop your own data,\ndigitizing the creation\nthere are a few steps that need to be done. Digitizing is a common way to cre-\nof vector objects\nate the points, lines, and polygons of vector data. With digitizing, you are (in through sketching or\ne ssence) \u201ctracing\u201d or \u201csketching\u201d over the top of areas on a map or other image tracing representations\nto model features in that map or image in the GIS. For instance, if you have an from a map or image\nsource.\naerial photograph of your house being shown on the screen of your GIS soft-\nware, you could use the mouse to sketch the outline of your house to create a\npolygon saved in the GIS. If you sketch outlines of four of your neighbors\u2019 houses\nFIGURE 5.3 Points\n(city locations), lines\nPolygons\n(interstates), and\npolygons (state borders)\nas represented in GIS.\n(Source: Esri\u00ae ArcGIS ArcView\ngraphical user interface\nCopyright \u00a9 Esri. All rights\nreserved.)\nLines\nPoints 104\nChapter 5 Working With Digital Spatial Data and GIS\nas well, you\u2019ll have several polygons stored in a single layer in the GIS. When\nyou examine the data, you\u2019ll find you have five polygon entries in that data layer\n(your house and four others). Similarly, you could sketch each house\u2019s driveway\nas a line object by tracing the mouse over the appropriate place in the image,\nwhich would result in a second data layer that\u2019s comprised of five lines. With\ndigitizing, each of your sketches can be translated into a vector object.\nDigitizing is commonly done through a process referred to as \u201cheads-up\ndigitizing\u201d or \u201con-screen digitizing,\u201d which is similar to the sketching just de-\nscribed. In \u201cheads-up\u201d digitizing, a map or other image (usually an aerial pho-\ntograph or satellite image) is displayed on the screen as a backdrop. Decide if\nyou\u2019ll be sketching points, lines, or polygons and create an empty data layer\ntopology how vector\nof the appropriate type (to hold the sketches you\u2019ll make). From there, you\nobjects connect to each\nother (in terms of their use the mouse to sketch over the objects, using as much detail as you see fit\nadjacency, connectivity, (Figure 5.4). Keep in mind that despite your best efforts, it may likely be im-\nand containment) possible or unfeasible to sketch every detail in the image. For instance, when\nindependently of the\ntracing a river, it may not be possible to capture each bend or curve by digitiz-\nobjects\u2019 coordinates.\ning straight lines.\nWhen objects are created in GIS, they can be set up with coordinates, but\nthe GIS still needs to have knowledge of how these objects connect to each\nFIGURE 5.4 An example of\nheads-up digitizing\u2014tracing other and what relation each object has to the others. For instance, when two\nthe boundary of the football lines are digitized that represent crossing streets, the GIS needs to have some\nfield on Youngstown State\nsort of information that an intersection should be placed where the two lines\nUniversity\u2019s (YSU\u2019s) campus\ncross. Similarly, if you have digitized two residential parcels, the GIS would\nto create a polygon object.\n(Source: Ohio Geographically require some information that the two parcels are adjacent to one another.\nReferenced Information Program This notion of the GIS being able to understand how objects connect to one\n(OGRIP), Ohio Statewide Imagery\nanother independent of their coordinates is referred to as topology. With\nProgram (OSIP), April 2006\/Esri\u00ae\nt opology, geometric characteristics aren\u2019t changed, no matter how the data-\nArcMap ArcInfo graphical user\ninterface Copyright \u00a9 Esri. All set may be altered (by such things as projecting or transforming the data).\nrights reserved.) 105\nHow Does GIS Represent Real-World Items?\nTopology establishes adjacency (that is, how one polygon relates to another\npolygon, in that they share a common boundary), connectivity (that is, how\nlines can intersect with one another), and containment (that is, how locations\nare situated inside of a polygon boundary).\nDigital Line Graphs (DLGs) are examples of vector data applications in DLG a Digital Line\nGIS. DLGs were created from United States Geologic Survey (USGS) topo- Graph\u2014the features\ngraphic maps (see Chapter 13 for more information about topographic (such as roads, rivers,\nor boundaries) digitized\nmaps), featuring vector datasets representing transportation features (such\nfrom USGS maps.\nas streets, highways, and railroads), hydrography features (such as rivers or\nstreams), or boundaries (such as state, county, city, or national forest bor-\nders). DLGs are created and freely distributed by the USGS (see Figure 5.5\nFIGURE 5.5 1: 24000\nDigital Line Graph\ndata (showing roads,\nrailroads, boundaries, and\nhydrologic features) of\nYoungstown, Ohio.\n(Source: USGS\/Esri\u00ae ArcGIS\nLayout graphical user\ninterface Copyright \u00a9 Esri.) 106\nChapter 5 Working With Digital Spatial Data and GIS\nHands-on Application 5.3\nUSGS Digital Line Graphs\nDigital Line Graphs are made freely available by the Large Scale boxes under the Digital Line Graphs\nUSGS. A good online utility for examining available option. Next, click Additional Criteria\u2014this is where\nDLGs is EarthExplorer, a Website designed to facili- you could further refine your search, if desired. Last,\ntate access to a wide range of geospatial data (not click Results to see what EarthExplorer turned up.\njust DLGs). To get started, open your Web browser From the available options in the results, you can\nand go to http:\/\/edcsns17.cr.usgs.gov\/NewEarth- see the footprint that the DLG would cover. If de-\nExplorer\/. Using EarthExplorer is a four-step pro- sired, DLGs can be downloaded to use in GIS (they\ncess. First, select the Search Criteria tab and type may need to be imported into a specific format to\nthe name of a location of an address or place name use, and the USGS requires a user to register and\n(such as Reno, Nevada). Next, click on Data Sets log in with them to download data). Check your local\nand place checkmarks in the DLG 1:100k and DLG area to see what types of DLGs are available.\nfor an example of several DLGs in GIS). The DLGs are created by digitizing\u2014\na USGS map is used as the source and the map features (like roads or rivers)\nare turned into digital format using GIS. See Hands-on Application 5.3: USGS\nDigital Line Graphs for more about how to obtain DLGs.\nThere\u2019s also a second way of viewing the world, in which not everything\nhas a fixed boundary or is an object. For instance, things such as temperature,\natmospheric pressure, and elevation vary across the Earth and are not best\nrepresented in GIS as a set of objects. Instead, they can be thought of as a\nsurface that\u2019s made up of a near-infinite set of points. This way of viewing the\ncontinuous field view world is called the continuous field view. This view implies that real-world\nthe conceptualization phenomena continuously vary, and rather than a set of objects, a surface filled\nof the world that all with values is used to represent things. For instance, elevation gradually var-\nitems vary across the\nies across a landscape from high elevations to low elevations filled with hills,\nEarth\u2019s surface as\nvalleys, and flatlands, and at every location, we can measure the height of the\nconstant fields, and\nvalues are available at land. Similarly, if you were standing at a ranger station in a park and prepar-\nall locations along the ing a rescue plan for stranded hikers, you would want to know the distance\nfield. from the station to every location in the park. Thus, you could have two layers\nin your GIS\u2014one showing the elevation at every place and another showing\nthe distance to the park at each location.\nHow Can You Represent the Real World\nas Continuous Fields?\nraster data model a\nconceptualization of\nrepresenting spatial When representing these kinds of continuous fields in GIS, the three vec-\ndata with a series of tor objects (and thus, the vector data model) are often not the best way of\nequally spaced and representing data. Instead, a different conceptualization called the raster\nsized grid cells.\ndata model is usually used. In the raster data model, data is represented 107\nHow Can You Represent the Real World as Continuous Fields?\nFIGURE 5.6 The grid\nY-axis Number of columns\ncells of the raster data\nmodel.\nNumber\nRow\nof rows\n+\nColumn\nX-axis\n+ Center of lower-left grid cell\nusing a set of evenly distributed square grid cells, with each square cell rep- grid cell a square unit,\nresenting the same area on the Earth\u2019s surface (see Figure 5.6 for the layout representing some\nof the grid cells). For instance, to represent an entire section of the land- real-world size, which\ncontains a single value.\nscape, each grid cell might represent 10 square feet, or 30 square meters, or\n1 square kilometer, depending on the grid cell size being used for the model.\nRegardless of the grid cell size, each cell has the same resolution (that is, the\nsame grid dataset cannot mix cells of 30 square feet together with cells of\n5 square feet). Also, each grid cell contains a single value representing the\ndata being modeled. For instance, in an elevation raster, each grid cell could\ncontain a value for the elevation at that area on the ground. Datasets such as\nland cover, soils, or elevation are all commonly represented using the raster\ndata model.\nThe National Land Cover Database (NLCD) is an example of a raster NLCD the National\ndata application in GIS. Developed by a consortium of United States agen- Land Cover Database\ncies (including the USGS and the Environmental Protection Agency), NLCD is a raster-based GIS\ndataset that maps the\nprovides 30-meter raster data covering the entire United States, wherein\nland cover types for the\neach grid cell is coded with a value that corresponds to a specific land cover\nentire United States at\ntype at that location. NLCD designations include categories such as \u201cOpen 30-meter resolution.\nWater,\u201d \u201cDeveloped, High Intensity,\u201d \u201cDeciduous Forest,\u201d \u201cPasture\/Hay,\u201d and\n\u201cWoody Wetlands.\u201d NLCD provides a means of broad-scale land cover clas-\nsification at a state, regional, or national scale (Figure 5.7, page 108). NLCD\nproducts are available for land cover circa 1992, 2001, and a new dataset\ncirca 2006 available at the time of this writing. A separate dataset is avail-\nable that details the change in land cover types between the 1992 and 2001\ndatasets, enabling researchers to not only measure land cover types, but also\nsee how the landscape is changing (such as where agricultural fields or for-\nests are converting over to urban land uses). NLCD data is distributed free of\ncharge via Web download in a format that can be easily read or converted by\nGIS software. See Hands-on Application 5.4: The National Land Cover Data-\nbase (NLCD) for how to access NLCD data (page 108). 108\nChapter 5 Working With Digital Spatial Data and GIS\nHands-on Application 5.4\nThe National Land Cover Database (NLCD)\nThe USGS has an online tool for viewing the 1992 can select different NLCD data to view. Zoom in to\nand 2001 NLCD (and some products derived from your local area and examine the NCLD\u2014what do the\nit), as well as letting you download sections of the various colors of the grid cells represent (in terms of\ndataset for use in GIS. The viewer is available at their land cover)? How are the 1992 and 2001 da-\nhttp:\/\/gisdata.usgs.net\/website\/MRLC\/viewer.htm. tasets different? If you want to download the data,\nThe default view should be the 2001 NLCD data the tools in the viewer will allow you to directly\n(although 2006 data is also available). From the download the raster datasets.\ndisplay options on the right side of the screen, you\nFIGURE 5.7 A section of the\n2001 NLCD showing land cover in\nColumbiana County, Ohio, modeled\nwith raster grid cells. (Source: USGS\/\nEsri\u00ae ArcGIS Layout graphical user\ninterface Copyright \u00a9 Esri.)\nHow Is Non-Spatial Data Handled by GIS?\nRegardless if you\u2019re mapping schools ranked by average SAT scores or the lo-\ncations of the United States\u2019 top tourist destinations, all of this numeric data\nneeds to be somehow represented in the GIS. For instance (going back to the\ndigitizing example), if you have two data layers (one containing polygons rep-\nresenting housing footprints and one containing lines representing the drive-\nways), each would contain multiple objects. The GIS will have information 109\nHow Is Non-Spatial Data Handled by GIS?\nabout the spatial properties of the objects in that layer, but what it does not\nhave is any other information about those objects (such as the tax-assessed\nvalue of the house, the house\u2019s address, the name of the owner, the number\nof people living in the house, the material the driveway\u2019s made of, or how old\nthe driveway is). All of these attributes represent other non-spatial informa- attributes the non-\ntion associated with each of the objects. When representing attribute data in spatial data that can\nbe associated with a\nGIS, these values can take one of four forms: nominal, ordinal, interval, or\nspatial location.\nratio data.\nNominal data are values that represent some sort of unique identifier. nominal data a type\nof data that is a unique\nYour Social Security number or telephone number would both be examples\nidentifier of some\nof nominal data\u2014both of these values are unique to you. Names or descrip-\nkind. If numerical, the\ntive information that are associated as a location would be nominal data, as\ndifferences between\nthey\u2019re unique values, the same as a value from a classification scheme. Also, numbers are not\nthe difference between numerical nominal values is not significant\u2014you significant.\ncan\u2019t add your Social Security number to a friend\u2019s number and come up with\nordinal data a type of\na relative\u2019s number\u2014the same way you can\u2019t subtract your phone number data that refers solely\nfrom one friend\u2019s number and come up with another friend\u2019s phone number. to a ranking of some\nOrdinal data is used to represent a ranking system of data. If you have kind.\ndata that is placed in a hierarchy where one item is first, another is second, and\nanother is third, that data is considered ordinal. For instance, if you had a map\nof the city and were going to identify the locations of the homes of the winners\nof a local car-racing event, the points on the map would be tagged as to the\nlocation of the first-place winner, the second-place winner, etc. Ordinal data\ndeals solely with the rankings themselves, not with the numbers associated\nwith these ranks. For instance, the only values mapped for the car-race win-\nners is their placement in the race, not their winning times or the cars\u2019 speed,\nor any other values. The only thing ordinal data represents is a clear value of\nwhat item is first, which is second, and so on. No measurements can be made\nof how much better the first-place winner\u2019s time was than the s econd-place\nwinner, only that one driver was first and one driver was second.\nInterval data is used when the difference between numbers is signifi- interval data a type of\ncant, but there is no fixed zero point. With interval data, the value of zero is numerical data in which\nthe difference between\njust another number used on the scale. For instance, temperature measured in\nnumbers is significant,\ndegrees Celsius would be considered interval data since values can fall below\nbut there is no fixed\nzero, and the value of zero only represents the freezing point of water, not the\nnon-arbitrary zero point\nbottom of the Celsius temperature scale. However, since there is no fixed zero associated with the\npoint, we can make differences between values (for instance, if it was 15 de- data.\ngrees yesterday and 30 degrees today, we can say it was 15 degrees warmer),\nratio data a type of\nbut dividing numbers wouldn\u2019t work (since a temperature of 30 degrees is not numerical data in which\ntwice as warm as a temperature of 15 degrees). the difference between\nRatio data are values with a fixed and non-arbitrary zero point. For in- numbers is significant,\nbut there is a fixed\nstance, a person\u2019s age or weight would be considered ratio data since a person\nnon-arbitrary zero point\ncannot be less than zero years in age or weigh less than zero pounds. With\nassociated with the\nratio data, the values can be meaningfully divided and subtracted. If we want data.\nto know how much time separated the car-racing winners, we could subtract\nthe winning driver\u2019s time from the second-place driver\u2019s time and get the 110\nChapter 5 Working With Digital Spatial Data and GIS\nnecessary data. Similarly, a textbook that costs $100 is twice as expensive\nattribute table a\nas a textbook that costs $50 (as we divide the two numbers). Ratio data is\nspreadsheet-style form\nwhere the rows consist used when comparing multiple sets of numbers and looking for distinctions\nof individual objects between them. For instance, when mapping the locations of car wash centers,\nand the columns the data associated with the location is the number of cars that use the ser-\nare the attributes\nvice. By comparing the values, we can see how much one car wash outsells the\nassociated with those\nothers, or calculate the profit generated by each location.\nobjects.\nEach layer in the GIS has an associated attribute table that stores addi-\nrecords the rows of an\ntional information about the features making up that layer. The attribute table\nattribute table.\nis like a spreadsheet where each object is stored as a record (a row) and the\nfields the columns of information associated with the records\u2014the attributes\u2014is stored as fields\nan attribute table.\n(columns). See Figure 5.8 for an example of an attribute table in conjunc-\ntion with its GIS objects. To use the housing example again, the houses attri-\nbute table would consists of five records, each representing a house polygon.\nNew fields could be created, having headings such as \u201cOwner\u201d and \u201cAppraised\nFIGURE 5.8 The\nattribute table associated Value\u201d so that this type of descriptive, non-spatial data could be added for\nwith a GIS data layer. each of the records. These new attributes can be one of the four data types (for\n(Source: Esri\u00ae ArcGIS ArcMap instance, \u201cOwner\u201d would be nominal data, while \u201cAppraised Value\u201d would be\nArcView graphical user\nratio data). In this way, non-spatial data is associated with a spatial location.\ninterface Copyright \u00a9 Esri.)\nFields\nRecords 111\nHow Is Non-Spatial Data Handled by GIS?\nA raster attribute table is handled differently than one for vector objects\nsince a raster dataset is composed of multiple grid cells (for instance, a 30-meter\nresolution grid of a single county could take millions of cells to model). Rather\nthan having separate records for each grid cell, raster data will often be set up\nin a table with each record featuring the value for a raster cell and an attri-\nbute showing the count of how many cells comprise the dataset. For instance, a\ncounty-land-use dataset may contain millions of cells, but only consist of seven\nvalues. Thus, the raster would have seven records (one for each value) and an-\nother field whose value would be a summation of how many cells have that\nvalue. New attributes would be defined by the raster values (for instance, the\ntype of land use that value represents).\nBesides creating new fields and populating them with attributes by hand,\na join is another way GIS allows non-spatial data to be connected to spatial join a method of\nlocations. This operation works by linking the information for records in one linking two (or more)\ntable to its corresponding records in another table. This is managed by both tables together.\ntables having a field in common (this field is also referred to as a key). For key the field that two\ninstance, Figure 5.9 shows two tables, the first from a shapefile\u2019s attribute tables have to have\ntable that has spatial information about a set of points representing the states in common with each\nother in order for the\nof the United States, while the second table has non-spatial Census informa-\ntables to be joined.\ntion about housing stock for those states. However, since both tables have a\ncommon field (that is, the state\u2019s abbreviation), a join can be performed on\nthe basis of that key.\nFIGURE 5.9 A joining\nof two attribute tables,\nbased on their common\nfield (the state\u2019s\nabbreviation attribute).\n(Source: Esri\u00ae ArcGIS\n(cid:2) ArcExplorer graphical user\ninterface Copyright \u00a9 Esri.)\n(cid:3) 112\nChapter 5 Working With Digital Spatial Data and GIS\nAfter the join, information for the records from the Census information ta-\nble is now associated with the corresponding records from the states\u2019 table. For\ninstance, you could now select the record for Alaska and have access to all of its\nhousing information since the tables have been related to each other. In this way,\njoining tables is a simple method for linking sets of data together and especially\nfor linking non-spatial spreadsheet data to spatial locations. With all of this other\ndata available (and linked to a spatial location), new maps can be made. For\ninstance, you could now create maps of the United States showing any of the\nattributes (total number of houses, number of vacation homes, or percentage of\nthe total housing that are vacation homes) linked to the spatial features.\nWhat Other Kind of Information\nDo You Need to Use GIS Data?\nOnce you have your spatial and attribute data within the GIS, there\u2019s one\nmore important piece of data you\u2019re going to need\u2014descriptive information\nabout your data. When creating data or obtaining it from another source, you\nshould have access to a separate file (usually a \u201creadme\u201d text file or an XML\ndocument) that fully describes the dataset. Useful things to know would be\ninformation about the coordinate system, projection, and datum of the data,\nwhen the data was created, what sources were used to create it, how accurate\nthe data is, and what each one of the attributes represents. All of this kind of\ninformation (and more) is going to be critical to understanding the data to\nproperly use it in GIS. For instance, if you have to reproject the data, you\u2019re\ngoing to have to know what projection it was originally in, or if you\u2019re going\nto be mapping certain attributes, then you\u2019ll have to know what each one of\nthose strangely named fields in the attribute table actually represents. In GIS,\nmetadata descriptive this \u201cdata about your data\u201d is referred to as metadata.\ninformation about The Federal Geographic Data Committee (FGDC) has developed some\ngeospatial data. standards for the content and format that metadata should have. These include:\na Identification of the dataset (a description of the data)\na Data quality (information about the accuracy of the data)\na Spatial data organization information (how the data is represented, such\nas vector or raster)\na Spatial reference information (the coordinate system, datum, and\nprojection used)\na Entity and attribute information (what each of the attributes means)\na Distribution information (how you can obtain this data)\na Metadata reference information (how current the metadata is)\na Citation information (if you want to cite this data, how it should be done)\na Time period information (what date does the data represent)\na Contact information (whom to get in touch with for further information) 113\nWhat Kinds of GIS Are Available?\nThinking Critically with Geospatial Technology 5.1\nWhat Happens When You Don\u2019t Have Metadata?\nSay you\u2019re trying to develop some GIS resources various attributes indicate, the date it was created,\nfor your local community, which includes needing or the datum, coordinate system, or projection the\ndata about the area\u2019s sewer system. You get the data is in. How useful is this dataset for your GIS\ndata in a format that your GIS software will read, work? When metadata for a GIS layer is unavailable\nbut it doesn\u2019t come with any metadata\u2014meaning (or i ncomplete), what sorts of issues can occur when\nyou have no information on how the sewer data trying to work with that data?\nwas created, the accuracy of the data, what the\nWhat Kinds of GIS Are Available?\nThere are many different types of GIS software packages available, every-\nthing from free lightweight \u201cdata viewers,\u201d to numerous Open Source pro-\ngrams, to expensive commercially available software packages. There are\nnumerous companies producing a wide variety of GIS software. According to\na 2007 GIS Salary Survey conducted by the Urban and Regional I nformation\nSystems Association (URISA), GIS products from the E nvironmental S ystems\nResearch Institute (Esri) were found to be among the most popular and Esri the Environmental\nwidespread in their usage. See Hands-on Application 5.5: Esri TV for more Systems Research\ninformation about Esri and their current GIS innovations and applications. Institute, a key\ndeveloper and leader of\nLocated in Redlands, California, Esri was founded by Jack and Laura\nGeographic Information\nD angermond in 1969. By 1982, Esri released the first of its \u201cArc\u201d products,\nSystems products.\nthe initial version of Arc\/Info, a key GIS software package. Though extremely\nArc\/Info an Esri GIS\npowerful in its analytical capabilities, Arc\/Info was also a product of the DOS\nproduct that required\ncommand-line era of computing. As such, there were literally hundreds of\ncommand-line entry for\ndifferent Arc\/Info commands, each with numerous variations in their usage, tools and capabilities.\noptional commands, and syntax. Today, new iterations of this version of Arc\/\nInfo are no longer being made, but the command-line software is available\nand referred to as \u201cWorkstation Arc\/Info.\u201d\nEsri would later release ArcView, a Windows-based tool for viewing ArcView an Esri GIS\nand examining spatial data. ArcView featured things common to a graphi- product developed with\ncal user interface, such as pull-down menus, icons, and windows, and as a Windows interface.\nHands-on Application 5.5\nEsri TV\nEsri is constantly coming up with new developments, out their online videos, open your Web browser and\nsoftware innovations, and GIS applications. As a go to http:\/\/www.youtube.com\/user\/E sritv. What\nway of distributing this type of news, Esri runs their kinds of applications or information about Esri soft-\nown channel on YouTube, called Esri TV. To check ware are being distributed in video form? 114\nChapter 5 Working With Digital Spatial Data and GIS\nsuch, was much simpler to use than the command-line interface. ArcView\nwent through several iterations to add a greater variety of functions and ex-\npansions to it to increase its analytical capabilities and was very commonly\nused by the late 1990s. The final version of ArcView was version 3.3, which\nis still available.\nArcGIS Esri\u2019s current Esri released its newest software package, ArcGIS (version 8, which built\nprimary GIS software off the numbering of the previous Arc\/Info versions) in stages between 1999\npackage. and 2001. ArcGIS was a blend of the analytical capabilities of Arc\/Info with a\nWindows graphical interface (similar to ArcView) in one package. As of this\nwriting, the most recent iteration of the software is ArcGIS Desktop 10. When\npurchased, ArcGIS Desktop is available in three different varieties (which re-\ncycle the previous names of some products): ArcView, ArcEditor, and ArcInfo.\nThese can best be thought of as different levels of the same software pack-\nage\u2014all of them are considered ArcGIS Desktop, but the ArcView version has\nArcMap the\nthe smallest number of functions, the ArcEditor version has the mid-range of\ncomponent of ArcGIS\nfunctions, and the ArcInfo version has the largest number of functions.\nused for viewing and\nanalyzing data. No matter which of the three versions of ArcGIS you\u2019re using, the main\ncomponent is referred to as ArcMap, which is used for viewing and analyzing\ndata (see Figure 5.10). Within ArcMap, GIS datasets can be added and treated\nFIGURE 5.10 The basic\nas different map layers. ArcGIS offers multiple sets of tools for analysis, data\nlayout of Esri\u2019s ArcMap.\nmanipulation, or making a map from your data (things you\u2019ll do in the labs for\n(Source: Esri\u00ae ArcGIS ArcMap\nArcView graphical user Chapters 6, 7, and 8). Previous versions of ArcGIS also featured ArcCatalog, a\ninterface Copyright \u00a9 Esri.) 115\nWhat Kinds of GIS Are Available?\nseparate utility for managing available GIS data. With ArcGIS 10, the Catalog\nCatalog the\nfunctions are now contained as a feature within ArcMap itself.\ncomponent of ArcGIS\nWhile it is the leading software product, ArcGIS is expensive, so Esri used for managing\nhas several other software options available for utilizing its tools. These in- data (which contains\nclude ArcExplorer Java Edition for Educators (AEJEE), a program that al- the functionality of the\nprevious ArcCatalog).\nlows a user to view GIS data, pan and zoom around data, do simple analysis\nlike queries and buffers, and create maps of data (which you\u2019ll use in the AEJEE the ArcExplorer\nlabs for this chapter, as well as the labs in Chapters 6, 7, and 8). AEJEE is Java Edition for\nEducators\u2014a free GIS\nan easy-to-use GIS software program that works like a simplified version\nprogram produced by\nof ArcMap. AEJEE and ArcGIS Desktop share a number of similarities in\nEsri that can display\ntheir layout, setup, menus and icons, and functionality. For instance, you\ndata, create maps, and\nadd data and work with it in AEJEE in very similar ways as you would in perform basic analysis.\nArcMap\u2014likewise, the Catalog functions of AEJEE are a simplified version\nArcGIS Explorer\nof ArcGIS\u2019s Catalog. Learning GIS basics with AEJEE should give you a good\na virtual Earth tool that\nbackground and (due to its similar setup and structure) reduce the learning can be downloaded for\ncurve if you use ArcGIS Desktop itself. See Figure 5.11 for the layout of AE- free from Esri.\nJEE (and note the similarities to ArcGIS\u2019 ArcMap design from Figure 5.10).\nThe labs for this section of the book are designed so that you can do them\nwith either ArcGIS 10 or AEJEE.\nFIGURE 5.11 The basic\nAnother free Esri product is ArcGIS Explorer, a free downloadable tool\nlayout of Esri\u2019s AEJEE\nthat Esri touts as being \u201cGIS For Everyone\u201d (see Figure 5.12, page 116),\nprogram. (Source: Esri\u00ae ArcGIS\nwhich should not be confused with AEJEE, despite the very similar names. ArcExplorer graphical user\ninterface Copyright \u00a9 Esri.) 116\nChapter 5 Working With Digital Spatial Data and GIS\nFIGURE 5.12 The ArcGIS Explorer has multiple functions for handling GIS data and other data\nHampton Roads area\nmade available for use via the Internet (you\u2019ll work with some of ArcGIS Ex-\nas seen in Esri\u2019s ArcGIS\nplorer\u2019s capabilities later in Chapter 15). See Hands-on Application 5.6: Esri\nExplorer software\nprogram. (Source: NASA\/ ArcNews, ArcUser, and ArcWatch for more information about Esri software\nUSGS\/NGA\/Esri\u00ae ArcGIS and applications.\nArcExplorer graphical user\nWith the proliferation of numerous Esri products on the market and in\ninterface Copyright \u00a9 Esri.)\nuse today, it\u2019s no surprise that a lot of GIS data that is available is in an Esri\ndata format. Raster data is commonly available in (or can be easily imported\ninto) Esri Grid format, which can be directly read by Esri software. Vector\ndata can usually be found in one of the following three different varieties:\ncoverage, shapefile, and geodatabase.\ncoverage a data Coverage is the original file format for Arc\/Info. The coverage itself con-\nlayer represented by sists of multiple files inside of a directory structure. The folder that a cover-\na group of files in a\nage is stored in is referred to as a workspace and a workspace may contain\nworkspace consisting\nmultiple coverages. A workspace will also contain a separate folder called the\nof a directory structure\nINFO directory, which contains extra files needed by each coverage in the\nfilled with files and also\nassociated files in an workspace. Because of this structure of holding files, Esri created a method\nINFO directory. to take all the necessary files that make up one coverage and export them to\na single file for portability. This exported file (which is called an \u201cinterchange\nfile\u201d that ends with the file extension .e00) can then be moved to a different\nlocation and reimported to rebuild the coverage structure on another folder\nor computer. 117\nWhat Kinds of GIS Are Available?\nHands-on Application 5.6\nEsri ArcNews, ArcUser, and ArcWatch\nEsri data and software products are used in numer- 3. ArcWatch: http:\/\/www.Esri.com\/news\/\nous applications across the globe. To keep informed arcwatch\/index.html\nregarding new Esri updates, software, and real-\nLook for some articles related to fields of inter-\nworld uses of GIS, Esri publishes a pair of print pub-\nest to you. For instance, see how Esri software is\nlications (and makes articles and content available\nused in environmental monitoring, national security,\nfree online) as well as an e-magazine. Check the fol-\nurban planning, or law enforcement\u2014and see what\nlowing sites for their publications:\nnew types of developments are currently underway\n1. ArcNews: http:\/\/www.Esri.com\/news\/ in the GIS field.\narcnews\/arcnews.html\n2. ArcUser: http:\/\/www.Esri.com\/news\/arcuser\/\nindex.html\nShapefile is the original file format for ArcView. A shapefile can hold only shapefile a series of\none type of vector object\u2014thus there are point shapefiles, line shapefiles, and files (with extensions\npolygon shapefiles. Despite the name, a shapefile actually consists of multiple such as .shp, .shx, and\n.dbf) that make up one\nfiles that have to be present together to properly represent the data. The files\nvector data layer.\nall start with the same prefix (that is, if the shapefile is called \u201croads,\u201d then\nall the files that it consists of will be called \u201croads\u201d) but will have different\nextensions, including .shp, .shx. .dbf, and others. When copying a shapefile,\nyou need to move all files with the same prefix to their new location for the\nshapefile to function properly.\nGeodatabase is a new file format established for ArcGIS. A geodatabase geodatabase a single\nconsists of a single file that contains all of the spatial information for a data- file that can contain\nset. Its object-oriented structure is set up such that multiple datasets can be multiple datasets, each\nas its own feature class.\nstored in a single geodatabase, with each one being its own feature class. For\nexample, a geodatabase called \u201cYellowstone\u201d could store the park boundaries\nas a polygon feature class, the roads running through the park as line feature\nclasses, and the hiking trails as another line feature class.\nEsri is not the only company making GIS software\u2014there are several\nother software packages and GIS companies providing a wide range of prod-\nucts used throughout the industry. For instance, there are many Open Source\nGIS products available for use and download via the Internet. An example\nis GRASS (Geographic Resources Analysis Support System), a long-running\nfree (and open source) GIS software that can handle vector, raster, and im-\nage data with many different types of analysis. Some notable commercial GIS\nvendors and their software products (and this list is not comprehensive) are:\na Geomedia (from Intergraph)\na IDRISI Taiga (from Clark Labs)\na Manifold System (from Manifold) 118\nChapter 5 Working With Digital Spatial Data and GIS\na MapInfo Professional (from Pitney Bowes Business Insight)\na Maptitude (from Caliper Corporation)\nChapter Wrapup\nNo matter the provider, the software, or the data representation format, GIS\nhas a wide variety of uses, ranging from public utility mapping to law enforce-\nment analysis, to fire tracking, to landscape planning. The next chapter will\nexamine a number of different types of GIS analysis and address how to take\nthis spatial data and start doing things with it.\nThe lab for this chapter provides an introduction to GIS and some basic\nhandling of spatial data. Two versions of the lab have been designed\u2014the first\none, Geospatial Lab Application 5.1: GIS Introduction: AEJEE Version, uses the\nfree AEJEE software package, and the second one, Geospatial Lab Application\n5.2: GIS Introduction: ArcGIS Version, uses ArcGIS 10. Both labs (5.1 and 5.2)\nuse the same data and concepts, but implement them differently, depending\non which software package you have available to you.\nImportant note: the references for this chapter are part of the online\ncompanion for this book and can be found at http:\/\/www.whfreeman.\ncom\/shellito1e.\nKey Terms\nGeographic Information System interval data (p. 109)\n(p. 99) ratio data (p. 109)\nCGIS (p. 101) attribute table (p. 110)\ndiscrete object view (p. 102) records (p. 110)\npoints (p. 102) fields (p. 110)\nlines (p. 102) join (p. 111)\npolygons (p. 102) key (p. 111)\nvector objects (p. 103) metadata (p. 112)\nvector data model (p. 103) Esri (p. 113)\ndigitizing (p. 103) Arc\/Info (p. 113)\ntopology (p. 104) ArcView (p. 113)\nDLG (p. 105) ArcGIS (p. 114)\ncontinuous field view (p. 106) ArcMap (p. 114)\nraster data model (p. 106) Catalog (p. 115)\ngrid cell (p. 107) AEJEE (p. 115)\nNLCD (p. 107) ArcGIS Explorer (p. 115)\nattributes (p. 109) coverage (p. 116)\nnominal data (p. 109) shapefile (p. 117)\nordinal data (p. 109) geodatabase (p. 117) 5.1\nGeospatial Lab Application\nGIS Introduction: AEJEE Version\nThis chapter\u2019s lab will introduce you to some of the basic features of GIS. You\nwill be using one of Esri\u2019s GIS programs to navigate a GIS environment and\nbegin working with spatial data. The labs in Chapters 6, 7, and 8 will utilize\nseveral more GIS features; the aim of this chapter\u2019s lab is to familiarize you\nwith the basic functions of the software. This lab uses the free ArcExplorer\nJava Edition for Educators (AEJEE).\nObjectives\nThe goals for you to take away from this lab are:\na Familiarize yourself with the AEJEE software environment, including ba-\nsic navigation and tool use with both the map and Catalog.\na Examine characteristics of spatial data, such as their coordinate system,\ndatum, and projection information.\na Familiarize yourself with adding data and manipulating data layer prop-\nerties, including changing the symbology and appearance of Esri data.\na Familiarize yourself with data attribute tables in AEJEE.\na Make measurements between objects in AEJEE.\nObtaining Software\nThe current version of AEJEE (2.3.2) is available for free download at http:\/\/\nedcommunity.esri.com\/software\/aejee.\nImportant note: Software and online resources sometimes change fast.\nThis lab was designed with the most recently available version of the soft-\nware at the time of writing. However, if the software or Websites have signifi-\ncantly changed between then and now, an updated version of this lab (using\nthe newest versions) is available online at http:\/\/www.whfreeman.com\/\nshellito1e.\nLab Data\nThere is no data to copy in this lab. All data comes as part of the AEJEE sample\ndata that gets installed with the software.\nLocalizing This Lab\nThe dataset used in this lab is Esri sample data for the entire United States.\nHowever, starting in Section 5.6, the lab focuses on Ohio and the locations\nof some of its cities. With the sample data covering the state boundaries and\ncity locations for the whole United States, it\u2019s easy enough to select your city\n119 120\nChapter 5 Working With Digital Spatial Data and GIS\n(or ones nearby) and perform the same measurements and analysis using\nthose cities more local to you than ones in northeast Ohio.\n5.1 An Introduction to ArcExplorer Java Edition for\nEducators (AEJEE)\n1. Start AEJEE (the default install folder is called AEJEE). AEJEE will open\nin its initial mode. The left-hand column (where the word Layers is) is\nreferred to as the Table of Contents (TOC)\u2014it\u2019s where you will have a\nlist of available data layers. The blank screen that takes up most of the\ninterface is the View, where data will be displayed.\nTable of\ncontents (TOC)\nView\n(Source: Esri\u00ae ArcGIS ArcExplorer graphical user interface Copyright \u00a9 Esri.)\nImportant note: Before you begin adding and viewing data, the first thing\nto do is examine the data you have available to you. When AEJEE installs, it\nalso gives you a sample set of data to work with. To examine this data (or any\nother GIS data you\u2019ll want to work with), you\u2019ll use AEJEE\u2019s Catalog\u2014a utility\ndesigned to allow you to organize and manage GIS data.\n2. From the Tools pull-down menu, select Catalog. The Catalog dialog box\nwill open in a new window.\n3. The Catalog can be used to manage your data as well as to view a\npreview of it. In the Catalog tree (the section going down the right-hand\nside of the screen), navigate to the C:\\drive, open the ESRI folder, and\nthen open the AEJEE folder. Next, open the Data folder and expand the\nusa folder. You\u2019ll see a list of all the shapefiles available for use in the\nsample data, as well as a couple of JPEG graphics. 121\nGIS Introduction: AEJEE Version\n(Source: Esri\u00ae ArcGIS ArcExplorer graphical user interface Copyright \u00a9 Esri.)\n4. In the Catalog tree, select the states.shp file. In the other section of\nthe Catalog, information about the dataset will be given. Click on the\nPreview tab to view what the dataset looks like.\n5. Do the same for the rest of the shapefiles, switching between the\nContents tab and the Preview tab.\nQuestion 5.1 How many polygons are in the lakes dataset (remember,\neach feature is a separate polygon)? How many lines are in the interstates\ndataset?\n5.2 Adding Data to AEJEE and Working with the TOC\n1. Back in the main window of AEJEE, click on the yellow and black \u201cplus\u201d\nbutton to start adding the data you\u2019ve previewed in the Catalog onto the\nmap.\n2. In the Content Chooser dialog box, navigate to the Data folder, open the\nusa folder, and then select the states.shp shapefile. Click OK. 122\nChapter 5 Working With Digital Spatial Data and GIS\n3. You\u2019ll now see the states.shp in the TOC, and its content is displayed in\nthe View.\n4. In the TOC, you\u2019ll see a checkmark in the box next to the states shapefile.\nWhen the checkmark is displayed, the layer will be shown in the View,\nand when the checkmark is removed (by clicking on it), the layer will\nnot be displayed.\n5. Now add two more layers, cities (a point layer), and intrstat (a line\nlayer). All three of your layers will now be in the TOC.\n6. You can manipulate the \u201cdrawing order\u201d of items in the TOC by grabbing\na layer with the mouse and moving it up or down in the TOC. Whatever\nlayer is at the bottom is drawn first, and the layers above it in the TOC\nare drawn on top of it. Thus, if you move the states layer to the top of\nthe TOC, the other layers will not be visible in the View since the states\u2019\npolygons are being drawn over the top of them.\n5.3 Symbology of Features in AEJEE\n1. You\u2019ll also notice that the symbology generated for each of the three\nobjects is simple\u2014points, lines, and polygons have been assigned a\nrandom color. You can significantly alter the appearance of the objects\nin the View to customize your maps.\n2. Right-click on the states layer and select Properties.\n3. In the Properties dialog box, you can alter the appearance of the states\nby changing their Style and Color, and then elements of how the\npolygons are outlined.\n(Source: Esri\u00ae ArcGIS ArcExplorer graphical user interface\nCopyright \u00a9 Esri.) 123\nGIS Introduction: AEJEE Version\n4. Change the states dataset to something more appealing to you, click\nApply to make the changes, and then click OK to close the dialog box.\n5. Do the same for the cities and interstates shapefiles. Note that you can\nalter the points for the cities into shapes like stars, triangles, or crosses,\nand change their size as well. Several different styles are available for\nthe lines of the interstates file as well.\n5.4 Obtaining Projection Information\n1. Chapter 2 discussed the importance of projections, coordinate systems,\nand datums. All of the information associated with these concepts can\nbe accessed using AEJEE.\n2. Right-click on the states shapefile and select Properties. Click on the\nProjection tab in the dialog box. Information about units, projections,\ncoordinate systems, and datums can all be accessed for each layer.\n3. Click OK to exit the states Properties dialog box.\n(Source: Esri\u00ae ArcGIS ArcExplorer graphical user interface Copyright \u00a9 Esri.)\nQuestion 5.2 What units of measurement are being used in the interstates\ndataset? What type of coordinate system (UTM, SPCS, etc.) is being used?\nQuestion 5.3 What datum and projection are being used for the cities\ndataset? 124\nChapter 5 Working With Digital Spatial Data and GIS\n4. AEJEE also allows you to change the projected appearance of the\ndata in the View from one projected system to another. From the\nmain AEJEE window, choose the Tools pull-down menu and select\nProjection. Numerous projection options are available to use. For this\nlab, select the projection options as follows:\na. Regional Projections\nb. Lambert Conformal Conic\nc. North America\n(Source: Esri\u00ae ArcGIS ArcExplorer graphical user interface Copyright \u00a9 Esri.)\nQuestion 5.4 What linear units and datum are used by the Lambert\nConformal Conic projection?\n5. Click Apply to make the changes, and then click OK to close the\ndialog box.\n6. Back in the View, you\u2019ll see that the appearance of the three layers has\ncompletely changed from the flat GCS projection to the curved LCC\nprojection.\n5.5 Navigating the View\n1. AEJEE provides a number of tools for navigating around the data layers\nin the View. Clicking on the globe icon will zoom the View to the extent\nof all the layers. It\u2019s good to use if you\u2019ve zoomed to far in or out in the\nView or need to restart.\nGlobe Zoom in Zoom out Pan 125\nGIS Introduction: AEJEE Version\n2. The magnifying glass icons allow you to zoom in (the plus icon) or out\n(the minus icon). You can zoom by clicking in the window or clicking\nand dragging a box around the area you want to zoom into.\n3. The hand icon is the Pan tool that allows you to \u201cgrab\u201d the View by\nclicking on it, and dragging the map around the screen for navigation.\n4. Use the Zoom and Pan tools to center the View on Ohio so that you\u2019re\nable to see the entirety of the state and its cities and roads.\n5.6 Interactively Obtaining Information\n1. Even with the View centered on Ohio, there are an awful lot of point\nsymbols there, representing the cities. We\u2019re going to want to identify\nand use only a couple of them. AEJEE has a searching tool that allows\nyou to locate specific objects\u2014on the toolbar, select the Find tool (the\nbinoculars icon) as follows:\n2. In the Find dialog box, select the Features tab.\n3. In the Value field type: Akron.\n(Source: Esri\u00ae ArcGIS ArcExplorer graphical user interface Copyright \u00a9 Esri.)\n4. Choose Cities for the Layers to Search.\n5. Click on the Find button. The returned results will be shown on the right\nof the dialog box.\n6. Drag the dialog box out of the way, so you can see both it and the View\nat the same time. 126\nChapter 5 Working With Digital Spatial Data and GIS\n7. Click on the Select button in the dialog box. You\u2019ll see that the point\nsymbol for the city of Akron has changed to a yellow color. This means it\nhas been \u201cselected\u201d by AEJEE\u2014any actions performed on the cities layer\nwill affect the selected items, not all of the cities.\nHowever, all that Find does is locate and select an object. To obtain infor-\nmation about it, you can use the Identify tool (the icon on the toolbar is the\nwhite \u201ci\u201d in a purple circle).\n8. Select the Identify tool, and then click on Akron (zooming in as\nnecessary). A new dialog box will open, listing all of the attribute\ninformation associated with Akron (it\u2019s actually giving you all of\nthe field attribute information that goes along with that particular\nrecord).\n9. Return to Find and look for the city called Youngstown, and then use the\nIdentify tool on the selected city.\nQuestion 5.5 According to AEJEE, what is the year 2000 population of\nYoungstown (carefully examine the attributes returned from Identify)?\n10. Another method of interactively selecting features is to use the Select\nFeatures tool. To select features from the cities layer, click on Cities in\nthe TOC. Next, on the toolbar, the icon is a cursor with a white and cyan\npolygon to its right.\n11. The Select Features tool allows you to select several objects at once by\ndefining the shape you want to use to select them with: rectangle, circle,\nline, or polygon. For now, choose rectangle, and select the four cities to\nthe immediate northwest and west of Youngstown.\n5.7 Examining Attribute Tables\n1. Each of the layers has an accompanying attribute table. To open the\nattribute table, click on the Attribute Table icon on the toolbar (it\u2019s the\none that looks like a three-column chart). You can also open a layer\u2019s\nattribute table by right-clicking on the layer in the TOC, and then\nselecting Attribute Table. 127\nGIS Introduction: AEJEE Version\nImportant note: At the bottom of the cities\u2019 attribute table, you\u2019ll see that\nfour records have been selected (the four cities near Youngstown). That\u2019s all\nwell and good, but you\u2019ll also see that there are 3557 records in the entire at-\ntribute table (and thus, there are 3557 point objects in the cities data layer).\nTo find those four records would mean a lot of digging through the data. How-\never, AEJEE has some helpful sorting features, including one that will sort the\ntable and move the selected records to the top.\n2. Scroll across the attribute table to find the field called ST (which has\nthe two-letter abbreviations for each state). It\u2019s between the CLASS and\nSTFIPS fields and may require stretching its boundaries out. Right-\nclick on the name of the ST field and from the new list of options,\nchoose Sort Selected Data To The Top. The selected records will be\nhighlighted in cyan and moved to the top records in the table.\nQuestion 5.6 Without using Identify, which four cities did you select?\nQuestion 5.7 How many total women (the attribute table lists this statistic\nas \u201cFemales\u201d) live in these four cities combined?\n3. Close the attribute table when you\u2019re done.\n4. To clear the selected features, select the eraser icon on the toolbar (the\nClear All Selection tool).\n5.8 Labeling Features\nRather than dealing with several points on a map and trying to remember\nwhich city is which, it would be easier to simply label each city so its name\nappears in the View. AEJEE gives you the ability to do this by creating a\nlabel for each record and allowing you to choose the field used to create\nthe label.\n1. Right-click on the cities layer in the TOC and select Properties.\n2. Click on the Labels tab.\n3. Under Label Features Using, choose NAME.\n4. For now, accept all the defaults and click Apply. Back in the View, you\u2019ll\nsee that labels of all the city names have been created and placed near\neach point.\n5. Change around any of the options\u2014font, color, text size, effects (plus\noptions like bold or italics), and even the rotation angle to set up the\nlabels however you feel is best for examining the map data. 128\nChapter 5 Working With Digital Spatial Data and GIS\n(Source: Esri\u00ae ArcGIS ArcExplorer graphical user interface Copyright \u00a9 Esri.)\n5.9 Measurements on a Map\nWith all the cities now labeled, it\u2019s easier to keep track of all of them. With\nthis in mind, your next task is to make a series of measurements between\npoints to see what the Euclidian (straight line) distance is between cities in\nnortheast Ohio.\n1. Zoom in tightly so that your View contains the cities of Youngstown and\nWarren and the other cities between them.\n2. Select the Measurement tool from the toolbar\u2014it\u2019s the one that\nresembles a ruler with a question mark positioned overtop.\n3. From the available options that appear when you select the\nMeasurement tool, choose Miles.\n4. You\u2019ll see that the cursor has turned into a crosshairs. Place the\ncrosshairs on the point representing Youngstown and left-click the\nmouse. Hold down the left mouse button and drag the crosshairs north\nand west to the point representing Girard. The distance measurement\nwill appear in a box in the upper left section of the screen. The value for\nsegment is the distance measured on the map, but the value for geodesic\nrefers to the \u201csurface\u201d distance measured on a sphere.\n5. Continue another line from Girard to Niles. 129\nGIS Introduction: AEJEE Version\nQuestion 5.8 What is the (geodesic) distance from Girard to Niles? What is\nthe total (geodesic) distance from Youngstown to Niles (via Girard)?\n6. You can clear all of the lines of measurement by again clicking on the\nMeasurement tool on the toolbar, and then selecting Clear Measure\nTotals.\nQuestion 5.9 What is the (geodesic) distance from Boardman to Warren,\nand then from Warren to Niles?\nQuestion 5.10 What is the (geodesic) distance from Austintown, Ohio,\nto New Castle, Pennsylvania, and then from New Castle to Hermitage,\nPennsylvania?\n5.10 Saving Your Work (and Working on It Later)\nWhen using AEJEE, you can save your work at any time and return to it. When\nwork is saved in AEJEE, an Extended ArcXML file is written to disk. Later, you\ncan re-open this file to pick up your work where you left off.\n1. Saving to an ArcXML file is done by clicking on the Save icon on the\ntoolbar (the floppy disk icon) or by choosing Save from the File pull-\ndown menu.\n2. Files can be re-opened by choosing the Open icon on the toolbar (the\nfolder icon) or by choosing Open from the File pull-down menu. If\nthe Catalog dialog box is still open, close it by selecting the red X in its\nwindow.\n3. Exit AEJEE by selecting Exit from the File pull-down menu.\nClosing Time\nThis lab was pretty basic, but served to introduce you to how AEJEE operates\nand how GIS data can be manipulated and examined. You\u2019ll be using either\nAEJEE or ArcGIS 10 in the next three labs, so the goal of this lab was to get the\nfundamentals of the software down. The lab in Chapter 6 takes this GIS data\nand starts to do spatial analysis with it, while the lab in Chapter 7 will have\nyou start making print-quality maps from the data. The lab in Chapter 8 will\ninvolve some further GIS analysis involving road networks. 5.2\nGeospatial Lab Application\nGIS Introduction: ArcGIS Version\nThis chapter\u2019s lab will introduce you to some of the basic features of GIS. You\nwill be using one of Esri\u2019s GIS programs to navigate a GIS environment and\nbegin working with spatial data. The labs in Chapters 6, 7, and 8 will utilize\nseveral more GIS features; the aim of this chapter\u2019s lab is to familiarize you\nwith the basic functions of the software. The previous Geospatial Lab Applica-\ntion 5.1: GIS Introduction: AEJEE Version asked you to use the free ArcExplorer\nJava Edition for Educators (AEJEE), however, this lab provides the same\nactivities but asks you to use ArcGIS 10.\nObjectives\nThe goals for you to take away from this lab are:\na Familiarize yourself with the ArcGIS software environment, including\nbasic navigation and tool use with both ArcMap and Catalog.\na Examine characteristics of spatial data, such as their coordinate system,\ndatum, and projection information.\na Familiarize yourself with adding data and manipulating data layer prop-\nerties, including changing the symbology and appearance of Esri data.\na Familiarize yourself with data attribute tables in ArcGIS.\na Make measurements between objects in ArcGIS.\nObtaining Software\nThe current version of ArcGIS (10) is not freely available for use. However,\ninstructors affiliated with schools that have a campus-wide software license\nmay request a 1-year student version of the software online at http:\/\/www.\nesri.com\/industries\/apps\/education\/offers\/promo\/index.cfm.\nImportant note: Software and online resources sometimes change fast.\nThis lab was designed with the most recently available version of the soft-\nware at the time of writing. However, if the software or Websites have signifi-\ncantly changed between then and now, an updated version of this lab (using\nthe newest versions) is available online at http:\/\/www.whfreeman.com\/\nshellito1e.\nLab Data\nThere is no data to copy in this lab. All data comes as part of the AEJEE sample\ndata that gets installed with the software.\nImportant note: In order to keep the data and results similar, both the\nAEJEE and ArcGIS 10 versions of the lab use the same set-up sample data that\n130 131\nGIS Introduction: ArcGIS Version\ncomes with AEJEE. Thus, if you\u2019re using ArcGIS 10 for the lab, please down-\nload and install AEJEE in order to use its sample data (see page 119).\nLocalizing This Lab\nThe dataset used in this lab is Esri sample data for the entire United States.\nHowever, starting in Section 5.6, the lab focuses on Ohio and the locations of\nsome Ohio cities. With the sample data covering the state boundaries and city\nlocations for the whole United States, it\u2019s easy enough to select your city (or\nones nearby) and perform the same measurements and analysis using those\ncities more local to you than ones in northeast Ohio.\n5.1 An Introduction to ArcMap\n1. Start ArcMap (the default install folder is called ArcGIS). ArcMap\nwill begin by asking you questions about \u201cmaps. \u201d In ArcMap, a\n\u201cmap document\u201d is the means of saving your work\u2014since you\u2019re just\nstarting, click on Cancel in the Getting Started dialog box. The left-\nhand column (where the word Layers is) is referred to as the Table of\nContents (TOC)\u2014it\u2019s where you will have a list of available data layers.\nThe blank screen that takes up most of the interface is the View, where\ndata will be displayed.\nTable of\ncontents (TOC)\nView\n(Source: Esri\u00ae ArcGIS ArcMap ArcInfo graphical user interface Copyright \u00a9 Esri.) 132\nChapter 5 Working With Digital Spatial Data and GIS\nBefore you begin adding and viewing data, the first thing to do is ex-\namine the data you have available to you. To examine data layers, you\ncan use the Catalog window in ArcMap\u2014a utility designed to allow you\nto organize and manage GIS data.\nImportant note: ArcGIS 10 also contains a separate application called\nA rcCatalog that can be used to perform the same data management tasks.\nHowever, ArcGIS 10 incorporates the same functionality into ArcMap\u2019s\nCatalog window, so you\u2019ll be using that in this Lab, rather than a separate\nprogram.\n2. By default, the Catalog window is \u201cpinned\u201d to the right-hand side of\nArcMap\u2019s screen as a tab. Clicking on the tab will expand the Catalog\nwindow. You can also open the Catalog window by clicking on its icon\non the main toolbar.\nCatalog window Catalog window\nicon tab\n(Source: Esri\u00ae ArcGIS ArcMap ArcInfo graphical user interface Copyright \u00a9 Esri.)\nImportant note: Various folders (representing network locations, hard\ndrives on a computer, or external USB drives) can be accessed by using the\nConnect to Folder button. For instance, the default install location for the\nEsri data used in this lab should be stored in a folder on the C:\\drive of your\ncomputer. 133\nGIS Introduction: ArcGIS Version\n3. Press the Connect to Folder button on the Catalog\u2019s toolbar.\n(Source: Esri\u00ae ArcGIS ArcMap ArcInfo graphical user interface Copyright \u00a9 Esri.)\n4. In the Connect to Folder dialog box, choose your computer\u2019s C:\\drive\n(or the drive or folder where the Esri data is stored) from the available\noptions and click OK.\n5. Back in the Catalog, whichever folder you chose (such as C:\\) will be\navailable by expanding the Folder Connections option.\nImportant note: This lab assumes the Esri sample data is stored on the\nC:\\ESRI\\AEJEE\\Data\\path. Navigate to that folder (or its equivalent on your\ncomputer) and open the usa folder. Several layers will be available, including\ncities, lakes, and states).\n6. The Catalog can be used to preview each of your available data layers\nas well as manage them. To see a preview of the layers you\u2019ll be using in\nthis lab, do the following:\na. Right-click on one of the layers in the Catalog (for instance, states).\nb. Select the option for Item Description.\nc. A new window will open (called \u201cItem Description - STATES\u201d). Click\non the Preview tab in this window to see what the dataset looks\nlike. Tools will be available in the window to zoom in and out (the\nmagnifying glasses), pan around the data (the hand), or return to\nthe full extent of the dataset (the globe). 134\nChapter 5 Working With Digital Spatial Data and GIS\n(Source: Esri\u00ae ArcGIS ArcMap ArcInfo graphical user interface Copyright \u00a9 Esri.)\nPreview each of the shapefiles (selecting a layer\u2019s Item Description\nwill place it into the new window). Note that once the Item Description\nwindow is open (and the Preview of the Geography option selected),\nyou can click once on the name of the file in the Catalog and it will\ndisplay a preview of it in the Item Description window.\nQuestion 5.1 What objects are each of the following datasets consisting of:\ncities, rivers, and counties?\n5.2 Adding Data to ArcMap and Working\nwith the TOC\n1. Back in the main window of ArcMap, click on the yellow and black\n\u201cplus\u201d button to start adding the data you\u2019ve previewed in the Catalog\nonto the map. 135\nGIS Introduction: ArcGIS Version\n2. In the Add Data dialog box, navigate to the ESRI folder and open the\nAEJEE folder, the Data folder, the usa folder, and select the states.shp\nshapefile. Click Add (or double-click on the name of the shapefile).\n3. You\u2019ll now see the states. shp in the TOC and its content is displayed in\nthe View.\n4. In the TOC, you\u2019ll see a checkmark in the box next to the states shapefile.\nWhen the checkmark is displayed, the layer will be shown in the View,\nand when the checkmark is removed (by clicking on it) the layer will not\nbe displayed.\n5. Now add two more layers, cities (a point layer), and intrstat (a line\nlayer). All three of your layers will now be in the TOC.\n6. You can manipulate the \u201cdrawing order\u201d of items in the TOC by grabbing\na layer with the mouse and moving it up or down in the TOC. Whatever\nlayer is at the bottom is drawn first, and the layers above it in the TOC\nare drawn on top of it. Thus, if you move the states layer to the top of\nthe TOC, the other layers will not be visible in the View since the states\u2019\npolygons are being drawn over top of them.\n5.3 Symbology of Features in ArcMap\n1. You\u2019ll also notice that the symbology generated for each of the three\nobjects is simple\u2014points, lines, and polygons assigned a random color.\nYou can significantly\nalter the appearance of\nthe objects in the View to\ncustomize your maps.\n2. Right-click on the states\nlayer and select Properties.\n3. Select the tab for\nSymbology.\n4. Press the large colored\nrectangle in the Symbol\nbox\u2014in the new Symbol\nSelector dialog box, you\ncan alter the appearance\nof the states by changing\ntheir color (choose one of\nthe defaults or select from\nthe Fill Color options), as\nwell as the outline color\nand thickness of the outline\nwidth itself.\n(Source: Esri\u00ae ArcGIS ArcMap ArcInfo graphical user interface Copyright \u00a9 Esri.) 136\nChapter 5 Working With Digital Spatial Data and GIS\n5. Change the states dataset to something more appealing to you, then\nclick OK. In the Layer Properties dialog box, click Apply to make\nthe changes, then OK to close the dialog. Do the same for the cities\nand roads shapefiles. Note that you can alter the points for the cities\ninto shapes like stars, triangles, or crosses, and change their size as\nwell. Several different styles are available for the lines of the roads\nfile as well.\n5.4 Obtaining Projection Information\n1. Chapter 2 discussed the importance of projections, coordinate systems,\nand datums, and all of the information associated with these concepts\ncan be accessed using ArcMap.\n2. Right-click on the states shapefile and select Properties. Click on the\nSource tab in the dialog box.\n3. Information about units, projections, coordinate systems, and datums\ncan all be accessed for each layer.\n4. Click OK to exit the states Layer Properties dialog box.\n(Source: Esri\u00ae ArcGIS ArcMap ArcInfo graphical user interface Copyright \u00a9 Esri.) 137\nGIS Introduction: ArcGIS Version\nQuestion 5.2 What units of measurement are being used in the interstates\ndataset? What type of coordinate system (UTM, SPCS, etc.) is being used?\nQuestion 5.3 What datum and projection are being used for the cities\ndataset?\nImportant note: ArcMap also allows you to change the projected appear-\nance of the data in the View from one projected system to another. In ArcMap,\nall of the layers that are part of the View are held within a Data Frame\u2014the\ndefault name for a Data Frame is \u201cLayers,\u201d which you can see at the top of\nthe TOC. By altering the properties of the Data Frame, you can change the\nappearance of how layers appear in the View. Note that this does not alter\nthe actual projection of the layers themselves, simply how you\u2019re seeing them\ndisplayed.\n(Source: Esri\u00ae ArcGIS ArcMap ArcInfo graphical user\ninterface Copyright \u00a9 Esri.)\n5. Right-click on the Data Frame in the TOC (the \u201cLayers\u201d icon) and select\nProperties.\n6. In the Data Frame Properties dialog box, select the Coordinate System\ntab.\n7. From the available options, select Predefined, then Projected\nCoordinate Systems, then Continental, and then North America.\nLastly, choose North America Lambert Conformal Conic. 138\nChapter 5 Working With Digital Spatial Data and GIS\n(Source: Esri\u00ae ArcGIS ArcMap ArcInfo graphical user interface Copyright \u00a9 Esri.)\nQuestion 5.4 What linear units and datum are used by the Lambert\nConformal Conic projection?\n8. Click Apply to make the changes, and then click OK to close the dialog\nbox.\n9. Back in the View, you\u2019ll see that the appearance of the three layers\nhas completely changed from the flat GCS projection to the curved\nLCC projection. You could select other projections for the Data Frame\nif you wish. Try out some other appearances\u2014when you\u2019re done,\nreturn to the Lambert Conformal Conic projection for the remainder of\nthe Lab. 139\nGIS Introduction: ArcGIS Version\n5.5 Navigating the View\n1. ArcMap provides a number of tools for navigating around the data\nlayers in the View. By default, these items are on a separate toolbar\ndocked under the pull-down menus. ArcMap has a variety of separate\ntoolbars to access\u2014the one with the main navigation tools is simply\ncalled \u201cTools. \u201d If it is not present, select the Customize pull-down\nmenu, select Toolbars, and then select Tools from the available\nchoices.\nImportant note: The magnifying glass icons allow you to zoom in (the plus\nicon) or out (the minus icon). You can zoom by clicking in the window or\nclicking and dragging a box around the area you want to zoom into.\n2. The hand icon is the Pan tool that allows you to \u201cgrab\u201d the View by\nclicking on it, and dragging the map around the screen for navigation.\n3. The globe icon will zoom the View to the extent of all the layers. It\u2019s\ngood to use if you\u2019ve zoomed to far in or out in the View or need to\nrestart.\n4. The four pointing arrows allow you to zoom in (arrows pointing inward)\nor zoom out (arrows pointing outward) from the center of the View.\n5. The blue arrows allow you to step back or forward from the last set of\nzooms you\u2019ve made.\n6. Use the Zoom and Pan tools to center the View on Ohio so that you\u2019re\nable to see the entirety of the state and its cities and roads.\n5.6 Interactively Obtaining Information\n1. Even with the View centered on Ohio, there are an awful lot of point\nsymbols there, representing the cities. We\u2019re going to want to identify\nand use only a couple of them. ArcMap has a searching tool that allows\nyou to locate specific objects\u2014on the toolbar, select the Find tool (the\nbinoculars icon): 140\nChapter 5 Working With Digital Spatial Data and GIS\n2. In the Find dialog box, select the Features tab.\nIn the Find field type: Akron.\n(Source: Esri\u00ae ArcGIS ArcMap ArcInfo graphical user interface Copyright \u00a9 Esri.)\n3. Choose Cities for the Layers to look In.\n4. Select the option for All fields for the Search.\n5. Click the Find button. The returned results will be shown in the bottom\nof the dialog box.\n6. Drag the dialog box out of the way so you can see both it and the View at\nthe same time.\n7. Double-click on the result for Akron in the bottom of the dialog box.\n8. You\u2019ll see that the point symbol for the city of Akron briefly highlights\nand changes color.\n9. Right-click on the same results in the Find dialog box and choose Select\nfrom the available options. You\u2019ll see that the point representing Akron\nhas changed color to a light blue. This means it has been \u201cselected\u201d\nby ArcMap\u2014any actions performed on the cities layer will affect the\nselected items, not all of the cities.\n10. However, all that Find does is locate and select an object. To obtain\ninformation about it, you can use the Identify tool (the icon on the\ntoolbar is the white \u201ci\u201d in a purple circle). 141\nGIS Introduction: ArcGIS Version\n11. Select the Identify tool, and then click on Akron (zooming in as\nnecessary). A new dialog box will open, listing all of the attribute\ninformation associated with Akron (it\u2019s actually giving you all of the\nfield attribute information that goes along with that particular record).\n12. In the Find dialog box, right-click on the results and choose Unselect to\nremove Akron from selection (this will prevent it from being used in the\nanalysis for the next sections).\n13. Return to Find and look for the city called Youngstown, and then use the\nIdentify tool on the selected city.\nQuestion 5.5 According to ArcMap, what is the year 2000 population of\nYoungstown (carefully examine the attributes returned from Identify)?\n14. Another method of interactively selecting features is to use the Select\nFeatures tool. To select features from the cities layer, click on cities in\nthe TOC. Next, on the toolbar, the icon is a cursor with a white and cyan\npolygon to its right.\n15. The Select Features tool allows you to select several objects at once\nby defining the shape you want to use to select them with: rectangle,\npolygon, lasso, circle, or line. For now, choose rectangle or polygon,\nand select the four cities to the immediate northwest and west of\nYoungstown (note that this will also select features from other layers,\nalthough we will just be working with cities in this lab).\n5.7 Examining Attribute Tables\n1. Each of the layers has an accompanying attribute table. To open a layer\u2019s\nattribute table, right-click on the layer in the TOC, then select Open\nAttribute Table. Open the attribute table for cities.\n2. At the bottom of the cities\u2019 attribute table you\u2019ll see that four records\nhave been selected (the four cities nearby Youngstown to its west and\nnorthwest). That\u2019s all well and good, but you\u2019ll also see that there\u2019s 3557\nrecords in the entire attribute table (and thus, there are 3557 point\nobjects in the cities data layer). To find those four records would mean 142\nChapter 5 Working With Digital Spatial Data and GIS\na lot of digging through the data. However, ArcMap has some helpful\nsorting features, including one that will sort the table and only show the\nselected records.\n3. At the bottom of the attribute table, select the icon for Show Selected\nRecords (it\u2019s the short stack icon in light blue).\n(Source: Esri\u00ae ArcGIS ArcMap ArcInfo graphical user interface Copyright \u00a9 Esri.)\nThe four selected records will be highlighted in cyan and be the only\nrecords shown in the table.\nQuestion 5.6 Without using Identify, which four cities did you select?\nQuestion 5.7 How many total women (the attribute table lists this statistic\nas \u201cFemales\u201d) live in these four cities combined?\n4. Close the attribute table. To clear the selected features, select the clear\nicon on the toolbar (the Clear Selected Features) tool.\n5.8 Labeling Features\n1. Rather than dealing with several points on a map and trying to remember\nwhich city is which, it would be easier to simply label each city so its\nname appears in the View. ArcMap gives you the ability to do this by\ncreating a label for each record and allowing you to choose the field used\nto create the label.\n2. Right-click on the cities layer in the TOC and select Label Features.\nDefault labels for the names of the cities will appear next to the points.\n3. To change the appearance of the labels, right-click on the cities layer\nand select Properties. In the Properties dialog box, choose the Labels\ntab. 143\nGIS Introduction: ArcGIS Version\n4. Change around any of the options\u2014font, color, text size, (plus options\nlike bold, italics, or underlining), and even the placement of the labels\nhowever you feel is best for examining the map data.\n(Source: Esri\u00ae ArcGIS ArcMap ArcInfo graphical user interface Copyright \u00a9 Esri.)\n5.9 Measurements on a Map\n1. With all the cities now labeled, it\u2019s easier to keep track of all of them.\nWith this in mind, your next task is to make a series of measurements\nbetween points to see what the Euclidian (straight line) distance is\nbetween cities in northeast Ohio.\n2. Zoom in tightly so that your View contains the cities of Youngstown and\nWarren and the other cities between them.\n3. Select the Measure tool from the toolbar\u2014it\u2019s the one that resembles a\nruler with two blue arrows over it. 144\nChapter 5 Working With Digital Spatial Data and GIS\n4. In the Measure dialog box, select the Choose Units pull-down menu,\nthen select Distance, and then select Miles (this will return all\nmeasurements to you in miles rather than the default of meters).\n5. Select the Measure Line tool from the toolbar. You\u2019ll see that the cursor\nhas turned into a crosshairs framed by a ruler.\n6. Place the crosshairs on the point representing Youngstown and left-click\nthe mouse (you\u2019ll also see a circle appear around Youngstown). Drag the\ncrosshairs north and west to the point representing Girard (once you\u2019ve\nreached the point representing Girard, a circle will appear around that\npoint) and left-click the mouse again. The distance measurement will\nappear in a box in the Measure dialog box. The value for Segment is the\nplanar distance measured on the map for one leg of the measure, while\nLength is the value for all segments being measured.\n7. Continue another line from Girard to Niles.\nQuestion 5.8 What is the (planar) distance from Girard to Niles? What is\nthe total (planar) distance from Youngstown to Niles (via Girard)?\n8. Double-clicking the mouse will stop the measurement tool and remove\nthe lines from the screen. To clear the results shown in the measurement\ndialog box, select the Clear and Reset Results button in the Measure\ndialog.\n9. There are a few more measurements to make to complete this chapter\u2019s\nquestions. 145\nGIS Introduction: ArcGIS Version\nQuestion 5.9 What is the (planar) distance from Boardman to Warren, and\nthen from Warren to Niles?\nQuestion 5.10 What is the (planar) distance from Austintown, Ohio,\nto New Castle, Pennsylvania, and then from New Castle to Hermitage,\nPennsylvania?\n5.10 Saving Your Work (and Working on It Later)\nWhen using ArcMap, you can save your work at any time and return to it.\nWhen work is saved in ArcMap, a map document file is written to disk. Later,\nyou can re-open this file to pick up your work where you left off.\n1. Saving to an ArcMap document file is done by clicking on the Save icon\non the toolbar (the floppy disk icon) or by choosing Save from the File\npull-down menu.\n2. Files can be re-opened by choosing the Open icon on the toolbar (the\nfolder icon) or by choosing Open from the File pull-down menu.\n3. Exit ArcMap by selecting Exit from the File pull-down menu. There\u2019s no\nneed to save any data in this lab.\nClosing Time\nThis lab was pretty basic but served to introduce you to how ArcGIS operates\nand how GIS data can be manipulated and examined. You\u2019ll be using either\nAEJEE or ArcGIS 10 in the next three chapter labs, so the goal of this lab was\nto get the fundamentals of the software down. The lab in Chapter 6 takes this\nGIS data and starts to do spatial analysis with it, while the lab in Chapter 7\nwill have you start making print-quality maps from the data. Chapter 8\u2019s lab\nwill involve some further GIS analysis involving road networks. This page was intentionally left blank 6\nUsing GIS for Spatial Analysis\nDatabase Query and Selection, Buffers, Overlay Operations,\nGeoprocessing Concepts, and Modeling with GIS\nThe previous chapter described the basic components of GIS and how it\no perated, so now it\u2019s time to start doing some things with that GIS data. Any-\ntime you\u2019ll be examining the spatial character of data, or how objects relate\nto one another across distances, you\u2019re performing spatial analysis. A very spatial analysis\nearly (pre-computer) example of this is Dr. John Snow\u2019s attempt to solve examining the\nthe mystery of several cholera-related deaths in London in 1854. Snow\u2019s characteristics or\nfeatures of spatial\nanalysis was able to identify an infected pump, which was the source of the\ndata, or how features\noutbreak. At some point, Snow mapped the locations of the cholera deaths\nspatially relate to each\nin relation to the water pumps (Figure 6.1). This type of spatial analysis other.\nFIGURE 6.1 A version of John\nSnow\u2019s map showing locations\nof cholera deaths and well\npumps.\n114477 148\nChapter 6 Using GIS for Spatial Analysis\n(relating locations of cholera deaths to the positions of water pumps) was\ninnovative for the mid-nineteenth century, but commonplace today\u2014with\nGIS, these types of spatial analysis problems can be easily addressed.\nOther common forms of spatial analysis questions are some of the\nfollowing:\na How many objects are within a certain distance of a particular location?\nAll manner of these types of questions can be answered, including mar-\nketing topics such as, \u2018\u2018How many fast food restaurants are within one\nmile of an interstate exit?\u201d; real estate topics regarding how many rental\nproperties are within two blocks of a house that is for sale; or planning\ntopics to determine the acreage of wetland threatened with removal by a\nnew proposed development plan.\na How do you choose the most suitable location based on a set of criteria?\nMany different types of questions related to this topic can be answered,\nsuch as those involving where to locate new commercial developments,\nnew real estate options, or the best places for animal habitats.\nThis chapter examines how GIS can be used with spatial data to answer\nquestions like these in the context of spatial analysis. Keep in mind the two\nways that GIS is used to model real-world data from the previous chapter\u2014\nvector and raster. Due to the nature of some of these properties, some types of\nspatial analysis are best handled with vector data and some with raster data.\nHow Can Data Be Retrieved\nFrom a GIS for Analysis?\nOne type of analysis would be finding which areas or locations meet particular\ncriteria\u2014for instance, being able to take a map of U.S. Congressional Districts\nand identify which districts had a Democratic representative and which had\na Republican representative. Alternatively, you might have a map of all resi-\ndential parcels and would want to see which houses have been sold in the last\nquery the conditions month. In both of these examples, you would perform a database query to\nused to retrieve data select only those records from the attribute table (see Chapter 5) that r eflect\nfrom a database.\nthe qualities of the objects you want to work with.\nIn other cases, before beginning the analysis, you may only want to utilize\na subset of your GIS data. For instance, rather than dealing with all Congres-\nsional Districts in the entire United States, you might want to examine only\nthose in Ohio. Or in the housing example, if you have a dataset of all residen-\ntial parcels in a county (which is likely thousands of parcels), you may only\nwant to do analysis related to one (or a handful) of the parcels, rather than\nexamining all of them. You\u2019d want to go into the dataset and select only those\nSQL the Structured parcels required for your study. In these cases, the query would again only\nQuery Language\u2014a select the records you want to deal with.\nformal setup for\nIn GIS, queries are composed in the Structured Query Language (SQL)\nbuilding queries.\nformat, like a mathematical function. SQL is a specific format that is used for 149\nHow Can Data Be Retrieved From a GIS for Analysis?\nFIGURE 6.2 A database\nquery for Ohio cities\nreturning multiple records\n(selected records and\ntheir corresponding cities\nare in cyan). (Source: Esri\u00ae\nArcGIS ArcMap ArcView\ngraphical user interface\nCopyright \u00a9 Esri.)\nquerying a layer or database to find what attributes meet certain conditions.\nFor example, a layer made up of points representing cities in Ohio also has a\nnumber of fields representing its attributes, such as city name, population,\nand average income. If you just wanted to do analysis with one of these records\n(the city called \u201cBoardman\u201d), then a query could be built to find all records\nwhere the field called CITY_NAME contained the characters of \u2018Boardman\u2019\nas: CITY_NAME = \u2018Boardman\u2019. This should return one record for you to use\nin your analysis. Another example would be to select all cities in Ohio with a\npopulation greater than or equal to 50,000 persons. The attribute table has a\nfield called POP2010 (representing the 2010 city population) that contains\nthis information. Thus, a query of POP2010 >= 50000 could be built, and all\nrecords whose population field contains a value of greater than or equal to the\nnumber 50,000 would be selected for your use (see Figure 6.2 for an example\nof a query in GIS).\nA query will use one of the following relational operators: relational operator\none of the six\na Equal (=): This is used when you want to find all values that match the\nconnectors (=, < >, <, >,\nquery. For instance, querying for CITY_NAME = \u2018Poland\u2019 will locate all >=, or =<) used to build\nrecords that exactly match the characters in the string that comprise the a query.\nword \u2018Poland\u2019. If the record was called \u2018Poland Township\u2019 it would not be\nlocated since the character string looks for an exact match.\na Not Equal (<>): This is used when you want to find all the records that\ndo not match a particular value. For instance, querying for CITY_NAME\n<> \u2018Poland\u2019 will return all the records that do not have a character string\nof the word \u2018Poland\u2019 (which would probably be all of the other cities with\nnames in the database). 150\nChapter 6 Using GIS for Spatial Analysis\na Greater Than (>) or Greater Than Or Equal To (>=): This is used for se-\ncompound query a\nlecting values that are more than (or more than or equal to) a particular\nquery that contains\nmore than one operator. value.\nBoolean operator one a Less Than (<) or Less Than Or Equal To (=<): This is used for selecting\nof the four connectors values below (or below and equal to) a particular value.\n(AND, OR, NOT, XOR)\nWhile a simple query only uses one operator and one field, a compound\nused in building a\nquery enables you to make selections using multiple criteria. There are differ-\ncompound query.\nent ways of linking multiple criteria together for creating one of these types of\nAND the Boolean\ncompound queries, each using a different option of querying (referred to as a\noperation that\nBoolean operator):\ncorresponds with an\nIntersection operation. a AND: For instance, you want to select cities in Ohio that have a popula-\nIntersection the tion of over 50,000 (a variable called POP2010 in the attribute table) and\noperation wherein the an average household income (a variable called AVERAGEHI) of more\nchosen features are than $30,000. A compound query can combine these two requests by say-\nthose that meet both\ning: POP2010>= 50000 AND AVERAGEHI > 30000. Because \u201cAND\u201d is\ncriteria in the query.\nbeing used as the operator, only records that match both the criteria get\nOR the Boolean returned by the query. If an attribute only meets one criterion, it is not\noperator that\nselected. An \u201cAND\u201d query is referred to as an Intersection since it returns\ncorresponds with\nwhat the two items have in common.\nUnion operation.\na OR: A different query could be built by saying: POP2010>= 50000 OR\nUnion the operation\nAVERAGEHI > 30000. This would return records of cities with a popula-\nwherein the chosen\nfeatures are all that tion of greater than or equal to 50,000, or cities with an average house-\nmeet the first criteria hold income of greater than $30,000, or cities that meet both. When an\nas well as all that meet OR operator is being used, records that meet one or both criteria are\nthe second criteria in\nselected. An \u201cOR\u201d query is referred to as a Union since it returns all the\nthe query.\nelements of both operations.\nNOT the Boolean\na NOT: A third query could be built around the concept that you want all of\noperator that\nthe data related to one criteria, but to exclude what relates to the second\ncorresponds with a\nNegation operation. criteria. For instance, if you want to select cities with a high population,\nbut not those with a higher average household income, you could build a\nNegation the\nquery like: POP2010>= 50000 NOT AVERAGEHI > 30000 to find those\noperation wherein the\nchosen features are particular records. A \u201cNOT\u201d query is referred to as a Negation since it\nthose that meet all of returns all the elements of one dataset, but not what they have in com-\nthe first criteria and mon with the other.\nnone of the second\ncriteria (including a XOR: A final type of query can be built using the idea that you want all\nwhere the two criteria of the data from each layer, except for what they have in common. For\noverlap) in the query. instance, if you wanted to find those cities with a high population or those\nXOR the Boolean that had high incomes, but not both types of cities, your query would be:\noperator that POP1990>= 50000 XOR AVERAGEHI > 30000. XOR works as the op-\ncorresponds with an posite of AND since it returns all data, except for the intersection of the\nExclusive Or operation.\ndatasets. An \u201cXOR\u201d query is referred to as an Exclusive Or, since it acts\nlike a Union but leaves out the Intersection data.\nCompound queries can use multiple operations to select multiple records\nby adhering to an order of operations, such as: (POP2010>=50000 AND 151\nHow Can You Perform Basic Spatial Analysis in GIS?\nHands-on Application 6.1\nWorking with Queries in GIS\nDurham, North Carolina, has an online GIS Spatial \u201cHousing Units\u201d). Third, select an operator (for in-\nData Explorer utility that allows the user to do a stance, choose the greater than \u201c>\u201d symbol). Press\nvariety of spatial analyses with the city\u2019s data, in- the \u201cadd to query\u201d button and you\u2019ll see the SQL\ncluding performing queries. To work with this tool, statement appear. To finish the query, type a value\nopen your Web browser and go to http:\/\/gisweb. (for instance, type 1000), and press the \u201cSend the\ndurhamnc.gov\/gomaps\/map\/index.cfm. In the Query\u201d button. All records meeting your query will\nupper left-hand corner are several tools (such as be retrieved (in this case, all Census block groups\nzooming and panning) to get oriented to the GIS with more than 1000 housing units in them). The\ndata. Included in the tools is a \u201chammer\u201d icon, which selected block groups will be highlighted on the\nallows you to build queries of the data. Press the map, while the records (and the attributes that go\n\u201chammer\u201d icon and a new set of options will appear along with them) will be displayed in a spreadsheet\non the right, allowing you to construct simple que- at the bottom of the screen. Investigate some of the\nries of the GIS. First, select an available layer (for in- other available data layers and the types of queries\nstance, select 2000 Census Block Groups). Second, that can be built with them.\nselect an attribute to query (for instance, choose\nAVERAGEHI > 30000) OR CITY_NAME = \u2018Boardman\u2019. This would select cit-\nExclusive Or the\nies that meet both the population and the income requirement and would se-\noperation wherein the\nlect cities named \u2018Boardman\u2019 as well. The result of a query will be a subset of chosen features are all\nthe records that are chosen, and then those records can be used for analysis, of those that meet the\nrather than the whole data layer. Selected records can also be exported to first criteria as well as\nall of those that meet\ntheir own dataset. For instance, the results of the previous query could be ex-\nthe second criteria,\ntracted to compose a new GIS layer, made up of only those records (and their\nexcept for the features\ncorresponding spatial objects) that met the query\u2019s criteria. Once you have that have both criteria\nyour selected records or new feature layers, you can start to work with them in in common in the query.\nGIS (see Hands-on Application 6.1: Working with Queries in GIS for an example\nof doing queries with online GIS utility).\nHow Can You Perform Basic\nSpatial Analysis in GIS?\nOne you have your selected data, it\u2019s time to start doing something with it.\nA simple type of analysis is the construction of a buffer around the elements buffer a polygon of\nof a data layer. A buffer refers to an area of proximity set up around one or spatial proximity built\naround a feature.\nmore objects. For instance, if you wanted to know how many rental properties\nwere within a half-mile of a selected house, you would create a half-mile buf-\nfer around the object representing the house and then determine how many\nrental properties were within the buffer. Buffers can be created for multiple\npoints, lines, or polygons (Figure 6.3 on page 152). Buffers are a simple tool\nto utilize, but they can provide useful information when creating areas of\nanalysis for various events. 152\nChapter 6 Using GIS for Spatial Analysis\nFIGURE 6.3 A fifty-\nmile buffer constructed\naround a major interstate\n(I-70). (Source: Esri\u00ae ArcGIS\nArcExplorer graphical user\ninterface Copyright \u00a9 Esri.)\ndissolve the ability GIS can also perform a dissolve operation, where boundaries between\nof the GIS to combine adjacent polygons (that have the same properties) are removed, merging the\npolygons with the polygons into a single, larger shape. Dissolve is useful when you don\u2019t need\nsame features together.\nto examine each polygon, only regions with similar properties. For example,\nyou have a land-use map of a county consisting of polygons representing each\nindividual parcel and how it\u2019s being used (commercial, residential, agricul-\ntural, etc.). Rather than having to examine each individual parcel, you could\ndissolve the boundaries between parcels, creating regions of \u201cresidential\u201d\nland use or \u201ccommercial\u201d land use for ease of analysis. Similarly, if you have\noverlapping buffers generated from features, dissolve can be used to combine\nbuffer zones together. See Figure 6.4 for an example of dissolve in action.\nBuffers and querying are basic types of spatial analysis, but they only\ninvolve a single layer of data (such as creating a buffer around roads or se-\nlecting all records that represent a highway). If you were going to do spatial\nanalysis along the lines of John Snow\u2019s hunt for a cholera source, you\u2019d have\nto be able to do more than just select a subset of the population who died\nor just selecting the locations of wells within the city borders\u2014you\u2019d have to\nknow something about how these two things interacted with each other over\na distance. For instance, you would want to know something about how many\ncholera deaths were found within a certain distance of each well. Thus, you\nwouldn\u2019t just want to create a buffer around one layer (the wells), but you\u2019d\nwant to know something about how many features from a different layer\n(the death locations) were in that buffer. Many types of GIS analysis involve\ncombining the features from multiple layers together to allow examination 153\nHow Can You Perform Basic Spatial Analysis in GIS?\nFIGURE 6.4 Before\nand after the dissolve\noperation\u2014Ohio county\nboundaries are dissolved\non the basis of their\nstate\u2019s FIPS code. (Source:\nEsri\u00ae ArcGIS ArcMap ArcView\ngraphical user interface\nCopyright \u00a9 Esri.)\ngeoprocessing the\nterm that describes\nwhen an action is\ntaken to a dataset that\nresults in a new dataset\nbeing created.\nof several characteristics at once. In GIS, when one layer has some sort of\nspatial query\naction performed to it and the result is a new layer, this process is referred to\nselecting records or\nas g eoprocessing. There are numerous types of geoprocessing methods and\nobjects from one layer\nthey are frequently performed in a series to solve a spatial analysis question. based upon their\nGIS gives you the ability to not just perform a regular SQL-style query, but spatial relationships\nto also perform a spatial query\u2014to select records or objects from one layer with other layers\n(rather than using\nbased upon their spatial relationships with other layers (rather than using at-\nattributes).\ntributes). With GIS, queries can be based on spatial concepts\u2014to determine 154\nChapter 6 Using GIS for Spatial Analysis\nFIGURE 6.5 Effect of\nselecting point features\n(cities) by using a buffer\n(fifty miles around\nI-70). (Source: Esri\u00ae ArcGIS\nArcExplorer graphical user\ninterface Copyright \u00a9 Esri.)\nthings like how many registered sex offenders reside within a certain dis-\ntance away from a school or which hospitals are within a particular distance\nof your home. The same kind of query would be done if a researcher wanted\nto determine which cholera deaths are within a buffer zone around a well.\nIn these cases, the spatial dimensions of the buffer are being used as the\nselection c riteria\u2014all objects within the buffer are what will be selected.\nSee Figure 6.5 for an example of using a buffer as a selection tool as well as\nHands-on Application 6.2: Working with Buffers in GIS.\nOther analyses can be done beyond selecting with a buffer or selecting by\nlocation. When two or more layers share some of the same spatial boundaries\nHands-on Application 6.2\nWorking with Buffers in GIS\nHonolulu has an online GIS application that looks To generate a buffer, select the Tools icon, and then\nat parcels and zoning for the area. On this Website, choose the option for Circle-Select Parcels. At this\nyou can select a location, generate a buffer around point, you can specify a buffer distance (the radius\nit, and all parcels touched by the buffer can be se- of the circle) and click on a place on the map. A buf-\nlected (and their parcel records displayed). To use fer will be generated around your placemark and the\nthis, open your Web browser and go to http:\/\/gis. parcels selected by the buffer will be highlighted on\nhicentral.com\/fastmaps\/parcelzoning\u2014then zoom the map and their records returned. Try examining\nin to a developed or residential area (if you\u2019re famil- some areas with various buffer sizes to see how\niar with the area, you can also search by address). buffers can be used as a selection tool. 155\nHow Can You Perform Basic Spatial Analysis in GIS?\nFIGURE 6.6 An example\nLayer 1 of overlaying two layers.\nLayer 2\nOverlay\n(but have different properties) and are combined together, this is referred to\nas an overlay operation in the GIS. For instance, if one layer contains informa- overlay the combining\ntion about the locations of property boundaries and a second layer contains of two or more layers in\ninformation about water resources, combining these two layers in an overlay the GIS.\ncan help determine which resources are located on whose property. An over-\nlay is a frequently used GIS technique for combining multiple layers together\nfor performing spatial analysis (Figure 6.6).\nintersect a type of\nThere are numerous ways that polygon layers can be combined together GIS overlay that retains\nthrough an overlay in GIS. Some overlay methods (Figure 6.7 on page 156) are: the features that are\ncommon to two layers.\na Intersect: In this operation, only the features that both layers have in\nidentity a type of GIS\ncommon are retained in a new layer. This type of operation is com-\noverlay that retains\nmonly used when you want to determine an area that meets two cri-\nall features from the\nteria\u2014for instance, a location needs to be found within a buffer and first layer along with\nalso on an agricultural land use\u2014you would intersect the buffer layer the features it has in\nand the agricultural layer together to find where, spatially, they have common with a second\nlayer.\nin common.\na Identity: In this operation, all of the features of an input layer are re- symmetrical\ndifference a type of\ntained, and all the features of a second layer that intersect with them are\nGIS overlay that retains\nalso retained. For example, you may want to examine all of the nearby\nall features from both\nfloodplain and also what portions of your property are on it. layers except for the\na Symmetrical difference: In this operation, all of the features of both lay- features that they have\nin common.\ners are retained\u2014except for the areas that they have in common. This 156\nChapter 6 Using GIS for Spatial Analysis\n+ =\nIntersect\n+ =\nIdentity\n+ = Symmetrical\ndiference\n+ =\nUnion\nFIGURE 6.7 The effects\nof various types of\ngeoprocessing overlay operation could show you, for example, all potential nesting areas along\noperations of two sample\nwith the local plan for development, except where they overlap (so those\npolygon layers.\nareas can be taken out of the analysis).\na Union (overlay): In this operation, all of the features from both layers\nunion (overlay) a are combined together into a new layer. This operation is often to merge\ntype of GIS overlay that features from two layers (for instance, in order to evaluate all possible op-\ncombines all features tions for a development site, you may want to overlay the parcel layer and\nfrom both layers.\nthe water resources layer together).\nMap Algebra\nAnother way of performing spatial overlay is by using raster data. With\ncombining datasets\ntogether using raster cells, each cell has a single value and two grid layers can be overlaid in\nsimple mathematical a variety of ways. The first of these is by using a simple mathematical operator\noperators. (sometimes referred to as Map Algebra), such as addition or multiplication.\nTo simplify things, we\u2019re only going to assign our grids values of 0 or 1, where\na value of 1 means that the desired criteria are met and a value of 0 is used 157\nHow Can You Perform Basic Spatial Analysis in GIS?\nwhen the desired criteria are not met. Thus, grids could be designed using\nthis system with values of 0 or 1 to reflect whether a grid cell meets or does\nnot meet particular criteria.\nLet\u2019s start with a simplified example\u2014say you\u2019re looking to find a plot of\nland to build a vacation home on, and your two key criteria are that it be close\nto more forested terrain for hiking and be close to a river or lake for water rec-\nreation. What you could do is get a grid of the local land cover and assign all\ngrid cells that have forests on the landscape a value of 1 and all non-forested\nareas a value of 0. The same holds true for a grid of water resources\u2014any cells\nnear a body of water get a value of 1 and all other cells get a 0. The \u201cforests\u201d\ngrid has values of 1 where there are forests and values of 0 for non-forested\nregions, while the \u201cwater\u201d grid has values of 1 where a body of water is pres-\nent and values of 0 for non-river areas (see Figure 6.8 for a simplified version\nof setting this up).\nNow you want to overlay your two grids and find what areas meet your\ntwo criteria to build your vacation home, but there are two ways of combining\nthese grids. Figure 6.9 on page 158 shows how the grids can be added and\nmultiplied together with varying results. By multiplying grids containing val-\nues of 0 and 1 together, only values of 0 or 1 can be generated in the resulting\noverlay grid. In this output, values of 1 indicate where (spatially) both criteria\n(forests and rivers) can be found, while values of 0 indicate where both can-\nnot be found. In the multiplication example, a value of 0 could have one of the\ncriteria, or none, but we\u2019re only concerned with areas that meet both criteria,\nor else we\u2019re not interested. Thus, the results either match what we\u2019re looking\nfor or they don\u2019t. Site Suitability the\nIn the addition example, values of 0, 1, or 2 are generated from the over- determination of the\n\u201cuseful\u201d or \u201cnon-useful\u201d\nlay. In this case, values of 0 contain no forests or rivers, values of 1 contain\nlocations based on a\none or the other, and values of 2 contain both. By using addition to overlay\nset of criteria.\nthe grids, a gradient of choices is presented\u2014the ideal location would be at\nlocations with a value of 2, but values of 1 represent possible second-choice\nalternatives, and values of 0 represent unacceptable locations.\nFIGURE 6.8 A\nThis type of overlay analysis is referred to as Site Suitability, as it seeks hypothetical landscape\nto determine which locations are the \u201cbest\u201d (or most \u201csuitable\u201d) for meeting consisting of forests,\nrivers, and non-forested or\ncertain criteria. Site Suitability analysis can be used for a variety of topics,\ndry areas.\n1 1\n0 1\n=\n0 1\n0 1 158\nChapter 6 Using GIS for Spatial Analysis\nFIGURE 6.9 Raster\noverlay examples for\nmultiplication and\naddition.\n1 1 0 1 0 1\n\u00d7 =\n0 1 0 1 0 1\n1 1 0 1 1 2\n+ =\n0 1 0 1 0 2\nincluding identifying ideal animal habitats, prime locations for urban devel-\nopments, or prime crop locations. When using the multiplication operation,\nyou will be left with sites classified as \u201csuitable\u201d (values of 1) or \u201cnon-suitable\u201d\n(values of 0). However, the addition operation leaves other options that may\nbe viable in case the most suitable sites are unavailable, in essence providing\na ranking of suitable sites. For instance, the best place to build the vacation\nhome is in the location with values of 2 since it meets both criteria, but the\ncells with a value of 1 provide less suitable locations (as they only meet one\nof the criteria).\nHow Can Multiple Types of Spatial Analysis\nOperations Be Performed in GIS?\nOften there are many criteria that play into the choice of site suitability or deter-\nmining what might be the \u201cbest\u201d or \u201cworst\u201d locations for something, and often\nit\u2019s not as cut and dried as \u201cthe location contains water\u201d (values of 1) or \u201cthe\nlocation does not contain water\u201d (values of 0). Think about the vacation home\nexample\u2014locations were classified as either \u201cclose to water\u201d or \u201cnot close to\nwater.\u201d When choosing a vacation home location, there could be locations right\non the water, somewhat removed from it but still close enough to recreation,\nand some that were much further away. Similarly, terrain is so changeable, it\u2019s\ndifficult to assign it a characteristic of \u201cmountainous\u201d or \u201cnot mountainous.\u201d\nInstead, a larger range of values could be assigned\u2014perhaps on a scale of\n1 through 5, with 5 being the best terrain for hiking or the closest to the water\nand 1 being furthest away from the water or the least desirable terrain.\nIn order to set something like this up, a distance calculation could be\nperformed. GIS can calculate the distance from one point to all other points\non a map or calculate the distance a location is from all other features. For\nexample, when examining a location for building a house in a rural locale, 159\nHow Can Multiple Types of Spatial Analysis Operations Be Performed in GIS?\nFIGURE 6.10 An\nexample of a raster\ndistance surface\ncalculation, where each\ngrid cell contains a value\nrepresenting the distance\nfrom a point (cities in\nOhio) and the same grid\nsliced into 10 ranges.\n(Source: Esri\u00ae ArcGIS ArcMap\nArcView graphical user\ninterface Copyright \u00a9 Esri.)\nyou may want to know the distance that spot is from a landfill or waste-water\ntreatment plant. Alternatively, you may want to know the distance a vaca-\ntion home is away from a lake or river. A way of conceptualizing distance (as\nis done in ArcGIS) is to think of it has a continuous surface (see Chapter 5),\nwhere each spot on that surface is has a value for the distance it is away from a\nfeature. ArcGIS makes these measurements by calculating a distance surface\nof raster grid cells, with each cell containing a value of the distance from the\nfeatures in a layer. Figure 6.10 illustrates this by mapping the location of a set\nof cities in Ohio and calculating the distance that each cell is away from a city.\nNext, the distance from the cities is sliced into ten equal ranges (that is, all of\nthe areas closest to a city are given a value of 1, the next closest are given a\nvalue of 2, and so forth).\nFor this example, we\u2019ll come up with a new set of grids classified along these\nlines and throw in a third option as well\u2014distance away from roads (using the\nassumption that a vacation home would be best placed away from busy roads\nand the accompanying automobile noise). In all of these, the highest values rep-\nresent the \u201cbest\u201d options (such as the furthest places away from busy roads),\nand the lowest values represent the \u201cworst\u201d options (such as right next to a busy\nroad). Figure 6.11 on page 160 shows the new grids and how they are over-\nlaid together through the addition operation to create a new output grid. Rather\nthan choices of 0, 1, and 2, the output grid contains a wide range of options,\nwhere the highest number reflects the most suitable site based on the criteria\nSuitability Index a\nand the lowest number reflects the least suitable sites. This new grid represents a system whereby\nvery simple Suitability Index, where a range of values are generated, indicating locations are \u201cranked\u201d\na ranking of the viability of locations (these would be ordinal values as discussed according to how well\nthey fit a set of criteria.\nback in Chapter 5). In the example in Figure 6.11, the values of 13 represent the 160\nChapter 6 Using GIS for Spatial Analysis\nFIGURE 6.11 A raster\n3 3 1 5 4 1 5 5 1\noverlay example for\ncombining three criteria\n5 1 1 3 2 1 5 4 1\ntogether to form a simple\nSuitability Index. 4 2 1 3 1 1 3 1 1\nDistance Distance Distance\nto forests to rivers to roads\n13 12 3\n13 7 3\n10 4 3\nSuitability\nindex\nmost suitable sites, the values of 12 and 10 are possible alternative venues, the\nvalue of 7 is a likely mediocre site, and the values of 4 and 3 are probably unac-\nceptable. The advantage of the Suitability Index is to offer a wider gradient of\nchoices for suitable sites instead of some of the previous options.\nGIS can process numerous large spatial data layers together at once.\nThinking about the vacation home example, there\u2019s many additional factors\nthat play into choosing the site of a second home\u2014access to recreational ame-\nnities, the price of the land, the availability of transportation networks, and\nhow far the site is away from large urban areas. All of these factors (and more)\ncome into play when identifying the optimal location for a vacation home.\nSimilarly, if you work in the retail industry and are assessing which place is\nthe best location for a new store, there are a lot of criteria that will influence\nyour decision\u2014how close the potential sites are to your existing store, how\nclose that area is to similar stores, the population of the area, the average\nhousehold income for each neighborhood, and the rental price of the prop-\nerty. GIS can handle all of these spatial factors by combining them together\n(see Figure 6.12 for an example of combining several data layers together).\nGIS Model a A GIS Model refers to a way of combining these spatial dimensions or\nrepresentation of characteristics together in an attempt to describe or explain a process or pre-\nthe factors used dict results. For instance, trying to predict where future urban developments\nfor explaining the\nwill occur based on variables such as a locations\u2019 proximity to existing urban\nprocesses that underlie\nareas, the price of the land parcels, or the availability of transportation cor-\nan event or for\npredicting results. ridors could be performed using GIS, with the end result being a map of the\nplaces most likely to convert to an urban land use. There are many different\ntypes of GIS Models with different features and complexities to them, and it\u2019s\nfar beyond the scope of this book to catalog and examine all of them.\nGIS Models are often used to examine various criteria together to deter-\nmine where, spatially, some phenomenon is occurring or could potentially oc-\ncur. These are often used in decision making processes or to answer questions\n(like the previous example of where to build a vacation home). An example 161\nHow Can Multiple Types of Spatial Analysis Operations Be Performed in GIS?\nFIGURE 6.12 An\nexample of combining\nmultiple GIS data layers\ntogether. (Source: Copyright\n\u00a9 2004 Esri. All rights\nreserved. Used by permission.)\nof this would be to try and determine which parcels of land surrounding Civil\nWar battlefields are under the greatest pressure to convert to an urban land\nuse. Battlefield land is often considered a historic monument to the men who\ndied during the Civil War, and while the core areas of several sites are under\nthe protection of the U. S. National Park Service, there are many other sites that\nhave been developed over or are in danger of having residential or commercial\nproperties built on them, forever losing the \u201challowed ground\u201d of history.\nGIS can be used to combine a number of factors together that influence\nwhere new developments could be occurring (such as proximity to big cities,\nexisting urban areas, or water bodies) similar to Site Suitability to try and find\nthe areas under the greatest development pressures. This type of information\ncould be used by preservationists when attempting to purchase lands to keep\nthem undeveloped.\nThe factors that contribute to a phenomenon can be constructed as layers\nwithin GIS, weighted (to emphasize which layers have a greater effect than\nothers) and combined together. The end result is a ranking of areas, to deter-\nMCE Multi-Criteria\nmine their suitability. An example of this process is referred to as Multi-Criteria Evaluation\u2014the use\nEvaluation (MCE), which takes layers that have been standardized and initial- of several factors\nly ranked against each other, then weights and combines them together. The (weighted and\ncombined) to determine\nMCE process produces a map of various \u201cpressures\u201d at each location (along the\nthe suitability of a site.\nlines of the suitability index map) to determine which places have the greatest 162\nChapter 6 Using GIS for Spatial Analysis\ninfluence placed on them (based on the combination of all the input layers).\nThis acts like a more complex form of the basic site suitability and can be used\nin decision making processes to help something like determining which pieces\nof battlefield land have the greatest development pressures on them to convert\nto an urban land use. Other problems, such as locating suitable animal habi-\ntats, areas of high flood potential, or places for retail sites can be approached\nusing similar types of techniques.\nA different example of a GIS Model is the Land Transformation Model\n(LTM), a tool used to combine various weighted raster layers together to de-\ntermine where land has changed to an urban land use, and then also to pre-\ndict urban land use change into the future. See Figure 6.13 for an example of\nthe output of the LTM\u2019s predictive capability for areas in Michigan out to the\nyear 2040. Also see Hands-on Application 6.3: The Land Transformation Model\nfor further examination of the LTM as a GIS Model.\nFIGURE 6.13 The use of\nthe Land Transformation\nModel in examining and\npredicting areas that\nwill change to an urban\nland use in the future.\n(Source: HEMA Laboratory\nPurdue University\/South\nEast Michigan Council of\nGovernments\/Michigan\nDepartment of Natural\nResources\/Esri\u00ae ArcGIS\nArcMap ArcView graphical\nuser interface Copyright \u00a9\nEsri.)\nHands-on Application 6.3\nThe Land Transformation Model\nThe Land Transformation Model is headquartered at The actual model itself (and sample datasets) can\nPurdue University. Open your Web browser and go be downloaded as well, if you\u2019re feeling adventur-\nto http:\/\/ltm.agriculture.purdue.edu\/default_ltm. ous, as well as a selection of graphics and papers\nhtm to visit the Website for LTM. Check it out for related to the model.\nfurther background on an example of a GIS Model. 163\nKey Terms\nThinking Critically with Geospatial Technology 6.1\nWhat Are Potential Societal or Policy Impacts of GIS Models?\nThe output of GIS Models is often used to aid in Also, consider, what if the results of the model\nd ecision-making processes and could have a signifi- are incorrect? If they predict one area is the optimal\ncant impact. For instance, trying to determine which location for a new shopping center\u2014and it turns out\nsites would make the most suitable locations for a not to be\u2014how could these results affect this area\nnew casino development would greatly affect the so- and future decision making? Keep in mind that you\ncial, economic, and land use conditions of an area. If could have a great model or design, but have inac-\nthe end result of a model is determining what areas of curate data for one (or more) of the layers. If that\u2019s\na residential area are under the greatest potential to the case, how can having this kind of poor-quality\nget flooded, this can have a strong impact on an area\u2019s data in a model affect the output (and what kinds of\nreal-estate market. What kinds of impacts can these impacts could the results of models using inaccurate\ntypes of models have on local, state, or federal policy? data have on policy decisions)?\nChapter Wrapup\nSpatial analysis is really at the core of working with geospatial data and GIS.\nWhile there are many more types of spatial analysis that can be performed\nwith GIS, this chapter simply introduces some basic concepts of how it can\nbe performed (and some applications of it). Next chapter, we\u2019re going to look\nat taking GIS data (for instance, the results of an analysis or a model) and\nmaking a map from the data. This chapter\u2019s labs will return you to working\nwith GIS software (either AEJEE or ArcGIS), to start in on more functions, like\nquerying and buffers.\nImportant note: The references for this chapter are part of the online com-\npanion for this book and can be found at http:\/\/www.whfreeman.com\/\nshellito1e.\nKey Terms\nspatial analysis (p. 147) buffer (p. 151)\nquery (p. 148) dissolve (p. 152)\nSQL (p.148) geoprocessing (p. 153)\nrelational operator (p. 149) spatial query (p. 153)\ncompound query (p. 150) overlay (p. 155)\nBoolean operator (p. 150) intersect (p. 155)\nAND (p. 150) identity (p. 155)\nIntersection (p. 150) symmetrical difference (p. 155)\nOR (p. 150) union (overlay) (p. 156)\nUnion (p. 150) Map Algebra (p. 156)\nNOT (p. 150) Site Suitability (p. 157)\nNegation (p. 150) Suitability Index (p. 159)\nXOR (p. 150) GIS Model (p. 160)\nExclusive Or (p. 151) MCE (p. 161) 6.1\nGeospatial Lab Application\nGIS Spatial Analysis: AEJEE Version\nThis chapter\u2019s lab builds on the basic concepts of GIS from Chapter 5 by apply-\ning several analytical concepts to the GIS data that you used in that chapter\u2019s\nlabs. This lab also expands on Chapter 5 by introducing several more GIS tools\nand features and how they are used.\nSimilar to the two Chapter 5 geospatial lab applications, two versions of\nthis lab are also provided. The first version (6.1 Geospatial Lab Application:\nGIS Spatial Analysis: AEJEE Version) uses the free ArcExplorer Java Edition\nfor Educators (AEJEE). The second version (6.2 Geospatial Lab Application:\nGIS Spatial Analysis: ArcGIS Version) provides the same activities for use with\nArcGIS 10. Although ArcGIS contains many more functions than AEJEE, these\ntwo programs share several useful spatial analysis tools.\nObjectives\nThe goals for you to take away from this lab are:\na Constructing database queries in AEJEE.\na Creating and using buffers around points, lines, and polygons.\na Using different selection criteria (from queries) to perform simple spatial\nanalysis.\nObtaining Software\nThe current version of AEJEE (2.3.2) is available for free download at http:\/\/\nedcommunity.esri.com\/software\/aejee.\nImportant note: Software and online resources sometimes change fast.\nThis lab was designed with the most recently available version of the soft-\nware at the time of writing. However, if the software or Websites have signifi-\ncantly changed between then and now, an updated version of this lab (using\nthe newest versions) is available online at http:\/\/www.whfreeman.com\/\nshellito1e.\nLab Data\nThere is no data to copy in this lab. All data comes as part of the AEJEE sample\ndata that gets installed with the software.\nLocalizing This Lab\nThe dataset used in this lab is Esri sample data for the entire United States.\nHowever, starting in Section 6.2, the lab focuses on North Dakota and South\n164 165\nGIS Spatial Analysis: AEJEE Version\nDakota and the locations of some cities and roads in the area, and then switch-\nes to the Great Lakes region in Section 6.6. With the sample data covering\nthe state boundaries and city locations for the whole United States, it\u2019s easy\nenough to select your city (or ones nearby), as well as major roads or lakes\nand perform the same measurements and analysis using those places more\nlocal to you than ones in these areas.\n6.1 Some Initial Pre-Analysis Steps\n1. Start AEJEE.\n2. From the USA folder, add the following shapefiles: states, cities, lakes,\nrivers, and intrstat.\n3. Change the color schemes and symbology of each layer to make them\ndistinctive from each other for when you\u2019ll be doing analysis. (See\n5.1 Geospatial Lab Application: GIS Introduction: AEJEE Version for\nhow to do this.)\n4. Position the layers in the Table of Contents (TOC) so that all layers are\nvisible (that is, the states layer is at the bottom of the TOC and the other\nlayers are arranged on top of it).\n5. When the layers are set how you\u2019d like them, zoom in to South Dakota,\nso the state (and the nearby data) fills up the screen.\n6.2 Database Queries\nA database query is a standard feature in AEJEE. We\u2019ll start off with a simple\nquery (using only one expression).\n1. To build a query in AEJEE, select the layer you want to query (in this\ncase, cities) by clicking on its name in the TOC. Then, select the Query\nBuilder icon from the toolbar:\nThe Query Builder dialog box will appear.\n2. To build a query, first select the Field to use. In this case, choose \u201cST\u201d\n(which contains the state name abbreviations).\n3. Next, select an operator to use. In this case, select the \u201c=\u201d operator.\n4. Lastly, select the appropriate values. In this case, choose \u201cSD\u201d (which\nstands for South Dakota). The query will appear in the dialog box\n(ST = \u2018SD\u2019). This will select all records whose ST field are equal to the\ncharacters \u2018SD\u2019. 166\nChapter 6 Using GIS for Spatial Analysis\n(Source: Esri\u00ae ArcGIS ArcExplorer graphical user interface Copyright \u00a9 Esri.)\n5. Click Execute to run the query. The returned records will be shown at\nthe bottom of the dialog box.\n6. This particular query returned nine records (listed as \u201cQuery Results\u201d at\nthe bottom of the dialog box).\n7. Take a look at the View again and you\u2019ll see that those nine cities in\nSouth Dakota have turned yellow (and have been selected).\n8. Next, we\u2019ll build a compound query to find all of the cities in South\nDakota with a Year 2000 population of greater than 50,000 persons.\nBring up the Query Builder again, this time creating a query looking for\nST = \u2018SD\u2019(it may still be there from the last time the query was created).\n9. Now click the button for the \u2018\u2018and\u2019\u2019 operator, then finish the second\nhalf of the compound query by selecting POP2000 as the Field (click\nNo if asked about displaying unique values), then select the operator,\nand lastly, type in the number 50000 in the query box. Your final query\nshould look something like this: 167\nGIS Spatial Analysis: AEJEE Version\n10. Click Execute to query the database.\nQuestion 6.1 How many cities in South Dakota have a population greater\nthan 50,000 persons? What are they?\n6.3 Creating and Using Buffers around Points\n1. AEJEE allows you to create buffers around objects for examination of\nspatial proximity, along with the ability to use the spatial dimensions of\nthe buffer as a means of selecting other objects.\n2. To create a buffer, select the layer whose selected features you wish to\nbuffer from the TOC (in this case, select the cities layer).\n3. Click on the buffer icon on the toolbar:\n4. The Buffer dialog box will appear:\n(Source: Esri\u00ae ArcGIS ArcExplorer graphical user interface Copyright \u00a9 Esri.)\n5. We\u2019ll start with a simple buffer around the cities.\n6. For Buffer Distance, type in 100.\n7. For Buffer Units, select Miles.\n8. Click Apply to create the buffer and then click OK to close the dialog\nbox.\n9. The circular 100-mile buffers will be created around the selected cities.\n10. The buffer can also be used as a selection tool (like the various shapes\nfor interactively selecting objects). Return to the Buffer dialog box and\nuse the same settings again (100 miles around the selected cities), but\nthis time, put a checkmark in the \u201cUse buffer to select features from\nthis layer\u201d box, and choose cities as the layer to select from. Click Apply 168\nChapter 6 Using GIS for Spatial Analysis\nand then OK. The buffers will be created again, but this time, the cities\nthat fall within the buffer will be selected as well.\n11. Open the attribute table and sort the Name field so that the selected\nrecords will be placed at the top (see 5.1 Geospatial Lab Application: GIS\nIntroduction: AEJEE Version on page 119 for how to do this).\nQuestion 6.2 How many cities are within a 100-mile radius of cities found\nin Question 6.1? (Be very careful to note what actually got selected in the\n\u201cselect with a buffer\u201d procedure when answering this question.)\n6.4 Creating and Using Buffers around Lines\n1. To create some new buffers, you\u2019ll first have to clear (unselect) the\ncurrently selected features. Click on the Clear All Selection icon on the\ntoolbar:\n2. All of the currently selected points will no longer be selected.\n3. For this section, you\u2019ll be creating buffers around line features and\nexamining the spatial relations of other objects to these lines.\n4. Select the intrstat layer in the TOC and bring up the Query Builder\nagain. Build a query to select all lines with a route number equal\nto I94.\n(Source: Esri\u00ae ArcGIS ArcExplorer graphical user interface Copyright \u00a9 Esri.)\n5. The field that corresponds with route number is ROUTE_NUM.\n6. Click Yes when AEJEE asks you to display all the values.\n7. Select \u2018\u2018=\u2019\u2019 for the operator, then from the values table, choose I94 (note\nthat AEJEE doesn\u2019t necessarily sort all values in numerical order).\n8. Click Execute when the query is built. Zoom out so that you can see the\nlength of I-94 (a major artery running east to west through the northern\nUnited States) on the screen. 169\nGIS Spatial Analysis: AEJEE Version\n9. Next, bring up the Buffer tool and create a 20-mile buffer around I-94,\nusing the buffer to select cities that fall within the buffer.\n(Source: Esri\u00ae ArcGIS ArcExplorer graphical user interface Copyright \u00a9 Esri.)\n10. When the buffer is set up correctly, click Apply, and then click OK to\nclose the dialog box.\n11. Select the cities layer from the TOC, then open the attribute table for\nthe cities and sort the results to the top.\nQuestion 6.3 How many cities are within 20 miles of I-94?\n12. Clear the selected features and redo the analysis, this time with a\n10-mile buffer.\nQuestion 6.4 How many cities are within 10 miles of I-94?\n13. Clear the selected features and redo the analysis one last time, now with\na 5-mile buffer.\nQuestion 6.5 How many cities are within 5 miles of I-94?\n14. Clear all of the currently selected features and buffers.\n6.5 Creating and Using Buffers around Polygons\n1. The next question you\u2019ll answer is how many cities are within states\nthat have a high population. Unfortunately, the states layer only has\ntotal population values, not any information about numbers of cities or\npopulated places. This type of question can be answered spatially using\nthe query and buffer tools you\u2019ve been using so far in this lab.\n2. Right-click on the states layer in the TOC and bring up the Query\nBuilder.\n3. Construct a query that will select all states with a year 2005 population\nof more than 2,000,000 persons. 170\nChapter 6 Using GIS for Spatial Analysis\n(Source: Esri\u00ae ArcGIS ArcExplorer graphical user interface Copyright \u00a9 Esri.)\n4. The field with a year 2005 population figure is POP2005.\n5. Click on Execute when the query is ready.\nQuestion 6.6 How many states had a year 2005 population of greater than\n2,000,000 persons?\n6. Bring up the Buffer tool to create a 1-foot buffer around the selected\nstates. Also, use the buffer to select cities that fall within the buffer\nzone.\n(Source: Esri\u00ae ArcGIS ArcExplorer graphical user interface Copyright \u00a9 Esri.)\n7. The reasoning here is that by creating a buffer of a minimum size, it will\nselect all cities within the polygons and only anything else that would\nbe within 1 foot of the buffer\u2019s edges (which should be no other cities\noutside of the selected states).\n8. Click Apply when the settings are correct and then click OK to close the\ndialog box.\n9. Answer Question 6.7 (this may involve examining and sorting the cities\u2019\nattribute table). 171\nGIS Spatial Analysis: AEJEE Version\nQuestion 6.7 How many cities are contained within the United States with\na population of more than 2,000,000 persons?\n10. Clear all of the currently selected features and buffers.\n6.6 More Spatial Analysis\n1. The next analysis that you\u2019ll perform involves analysis of the features\nin relation to the five Great Lakes, so zoom in on the View so that you\ncan see all five Great Lakes. The first thing you\u2019ll have to do is select the\nGreat Lakes to work with.\n2. Select the lakes shapefile in the TOC and bring up the Query\nBuilder. You\u2019ll see that the two available fields that are available\nfor query are the Name and Area of the lake records. You\u2019ll want\nto build a query that looks for the name of each of the five Great\nLakes as follows: (NAME = \u2018Lake Erie\u2019 or NAME = \u2018Lake Huron\u2019 or\nNAME = \u2018Lake Michigan\u2019 or NAME = \u2018Lake Ontario\u2019 or NAME = \u2018Lake\nSuperior\u2019).\n(Source: Esri\u00ae ArcGIS ArcExplorer graphical user interface Copyright \u00a9 Esri.)\n3. Answer Question 6.8, and then click Execute. All five Great Lakes will\nnow be selected. Close the Query Builder.\nQuestion 6.8 Why is the \u201cor\u201d operator used in this expression rather than\nthe \u201cand\u201d operator? What would the same query give you back is you used\n\u201cand\u201d rather than \u201cor\u201d in each instance?\n4. Next, select the buffer tool and find out how many rivers are within\n5 miles of the Great Lakes. Note that even if part of the river is within the\n5-mile buffer, the entire river system will be selected.\nQuestion 6.9 Which rivers does AEJEE consider within 5 miles of the\nGreat Lakes? Which river system are they part of? 172\nChapter 6 Using GIS for Spatial Analysis\n5. Lastly, create a new buffer to find out how many cities are within 5 miles\nof the Great Lakes.\nQuestion 6.10 How many cities are within 5 miles of the Great Lakes?\n6. Exit AEJEE by selecting Exit from the File pull-down menu. There\u2019s no\nneed to save any data in this lab.\nClosing Time\nNow that you have the basics of working with GIS data and performing some\ninitial spatial analysis down, the lab in Chapter 7 will describe how to take\nspatial data and create a print-quality map using AEJEE or ArcGIS 10. 6.2\nGeospatial Lab Application\nGIS Spatial Analysis: ArcGIS Version\nThis chapter\u2019s lab builds on the basic concepts of GIS from Chapter 5 by apply-\ning several analytical concepts to the GIS data you used in 5.2 Geospatial Lab\nApplication: GIS Introduction: ArcGIS Version. This lab also expands on Chapter\n5 by introducing several more GIS tools and features and how they are used.\nThe previous 6.1 Geospatial Lab Application, GIS Spatial Analysis: AEJEE\nVersion uses the free ArcExplorer Java Edition for Educators (AEJEE); how-\never, this lab provides the same activities for use with ArcGIS 10.\nAlthough ArcGIS contains many more functions than AEJEE, the two pro-\ngrams share several useful spatial analysis tools.\nObjectives\nThe goals for you to take away from this lab are:\na Constructing database queries in ArcGIS.\na Using different selection criteria (from queries and selecting by location)\nto perform simple spatial analysis.\nObtaining Software\nThe current version of ArcGIS (10) is not freely available for use. However,\ninstructors affiliated with schools that have a campus-wide software license\nmay request a 1-year student version of the software online at http:\/\/www.\nesri.com\/industries\/apps\/education\/offers\/promo\/index.cfm.\nImportant note: Software and online resources sometimes change fast.\nThis lab was designed with the most recently available version of the software\nat the time of writing. However, if the software or Websites have significantly\nchanged between then and now, an updated version of this lab (using the new-\nest versions) is available online at http:\/\/www.whfreeman.com\/shellito1e.\nLab Data\nThere is no data to copy in this lab. All data comes as part of the AEJEE sample\ndata that gets installed with the software.\nImportant note: In order to keep the data and results similar, both the\nAEJEE and ArcGIS 10 portions of the lab use the same sample data that comes\nwith AEJEE. Thus, if you\u2019re using ArcGIS 10 for the lab, please download and\ninstall AEJEE in order to use its sample data.\nLocalizing This Lab\nThe dataset used in this lab is Esri sample data for the entire United States.\nHowever, starting in Section 6.2, the lab focuses on North Dakota and South\n173 174\nChapter 6 Using GIS for Spatial Analysis\nDakota and the locations of some cities and roads in the area, and then switch-\nes to the Great Lakes region in Section 6.6. With the sample data covering\nthe state boundaries and city locations for the whole United States, it\u2019s easy\nenough to select your city (or ones nearby), as well as major roads or lakes\nand perform the same measurements and analysis using those places more\nlocal to you than ones in these areas.\n6.1 Some Initial Pre-Analysis Steps\n1. Start ArcMap.\n2. From the USA folder, add the following shapefiles: states, cities, lakes,\nrivers, and intrstat.\n3. Change the color schemes and symbology of each layer to make them\ndistinctive from each other for when you\u2019ll be doing analysis. (See\nLab 5.2 on page 130 for how to do this.)\n4. Position the layers in the Table of Contents (TOC) so that all layers are\nvisible (that is, the states layer is at the bottom of the TOC and the other\nlayers are arranged on top of it).\n5. When the layers are set how you\u2019d like them, zoom in to South Dakota,\nso the state (and the nearby data) fills up the screen.\n6.2 Database Queries\n1. A database query is a standard feature in ArcMap. We\u2019ll start off\nwith a simple query (using only one expression). To build a query\nin ArcMap, choose Select by Attributes from the Selection pull-down\nmenu.\n2. The Select by Attributes dialog box will appear.\n3. You\u2019ll be building a simple query to find which points (representing\ncities) are in South Dakota (SD).\n4. To build a query, first select the Layer to use. In this case, choose cities\n(since it\u2019s the layer that we will be querying).\n5. Next, select the Field to query. In this case, choose \u201cST\u201d by double-\nclicking it with the mouse.\n6. Next, select an operator to use. In this case, select the = operator,\nclicking on the symbol with the mouse.\n7. Lastly, select the appropriate values. Press the Get Unique Values\nbutton to get a list of all the possible values associated with the \u201cST\u201d\nField. In this case, choose \u2018SD\u2019.\n8. The query will appear in the dialog box (\u201cST\u201d = \u2018SD\u2019). This will select all\nrecords whose ST field is equal to the characters \u2018SD\u2019.\n9. Click OK to run the query. 175\nGIS Spatial Analysis: ArcGIS Version\n(Source: Esri\u00ae ArcGIS ArcMap ArcInfo graphical user interface\nCopyright \u00a9 Esri.)\n10. In the View, you\u2019ll see several points selected as a result of the query.\n11. Open the cities layer\u2019s attribute table and only show the selected records\n(see Lab 5.2 on page 130 for how to do this).\n12. This particular query returned nine records (listed as the nine records\nshown selected in the attribute table).\n13. Next, we\u2019ll build a compound query to find all cities in South Dakota\nwith a population of greater than 50,000 persons. Bring up the Select\nby Attributes dialog box again, this time creating a query looking for\n(\u201cST\u201d = \u2018SD\u2019) (it may still be there from the last time the query was\ncreated\u2014make sure to put in the parentheses).\n14. Now click the button for the \u2018\u2018And\u2019\u2019 operator, then finish the second\nhalf of the compound query by selecting the () symbol to place the new\nquery in parentheses.\n15. Then choose POP2000 as the Field (don\u2019t display its unique values),\nthen select the greater than (>) operator, and lastly, type the number 176\nChapter 6 Using GIS for Spatial Analysis\n50000 in the box. Your final query should look something like this:\n(Source: Esri\u00ae ArcGIS ArcMap ArcInfo graphical user interface\nCopyright \u00a9 Esri.)\n16. Click OK to query the database.\nQuestion 6.1 How many cities in South Dakota have a population of\ngreater than 50,000 persons? What are they?\n6.3 Selecting By Location with Points\n1. ArcMap allows you to select features based on their proximity to other\nfeatures. To do so, choose Select By Location from the Selection pull-\ndown menu. This will allow you to select features that are nearby your\nselected cities.\n2. In Select By Location, you will specify one of more layers as the Target\nlayer (the layer you will be selecting features from) and a second layer\nas the Source (the layer whose spatial dimensions you will be using to\nperform the selection). There are several different ways these two layers\ncan spatially intersect with each other\u2014however, in this case, you\u2019ll be\nfinding which of the features from the Target layer (the cities) are within\nthe boundaries of the Source layer (the selected cities). 177\nGIS Spatial Analysis: ArcGIS Version\n(Source: Esri\u00ae ArcGIS ArcMap ArcInfo graphical user\ninterface Copyright \u00a9 Esri.)\n3. For the Selection method, choose select features from.\n4. Choose cities for your Target layer.\n5. Choose cities as your Source layer. You will notice that the two\nfeatures you selected back in Question 6.1 remain selected\u2014thus,\nyou will be finding other cities based on their relation to only\nthese two.\n6. Choose Target layer(s) features are within a distance of the Source\nlayer feature as the Spatial selection method.\n7. Click Apply a search distance and use 100 miles.\n8. Leave the other options unused, and then click OK.\n9. Open the cities attribute table and examine the selected cities (see the\nlab in Chapter 5 for how to do this).\nQuestion 6.2 How many cities are within a 100-mile radius of the\ncities you selected in Question 6.1? (Be very careful to note what actually\ngot selected in the Select By Location procedure when answering this\nquestion.) 178\nChapter 6 Using GIS for Spatial Analysis\n6.4 Selecting By Location with Lines\n1. To create some new selections, you should first clear (unselect) the\ncurrently selected features. Click on the Clear Selected Features icon\non the toolbar:\nAll of the currently selected points will no longer be selected.\n2. For this section, you\u2019ll be using Select By Location with line features and\nexamining the spatial relations of other objects to these lines.\n3. Bring up the Select By Attributes dialog box again, and this time build\na simple query to select all lines in the roads layer with a route number\nequal to I-94 (a major cross-country east-west road that runs through\nthe northern United States).\n(Source: Esri\u00ae ArcGIS ArcMap ArcInfo graphical user interface\nCopyright \u00a9 Esri.) 179\nGIS Spatial Analysis: ArcGIS Version\n4. Use intrstat as the Layer to select from.\n5. Use Create a new selection as your selection method.\n6. Use \u201cROUTE_NUM\u201d as the Field to select from.\n7. Choose = as your operator.\n8. Press the Get Unique Values button and choose \u2018I94\u2019 as the value to use.\n9. Your query should read \u201cROUTE_NUM\u201d = \u2018I94\u2019\n10. Click OK when the query is built. Zoom out so that you can see the\nlength of I-94 (a major artery running east to west through the northern\nUnited States) on the screen.\n11. Next, use Select By Location to find all cities that lie within 20 miles\nof I-94.\n12. Open the cities attribute table and examine the selected records.\nQuestion 6.3 How many cities are within 20 miles of I-94?\n13. Clear the selected results and redo the analysis as follows:\na. U se Select By Location to find all cities that lie within 10 miles\nof I-94.\nb. Open the cities attribute table and examine the selected records.\nQuestion 6.4 How many cities are within 10 miles of I-94?\n14. Clear the selected results and redo the analysis one last time as\nfollows:\na. U se Select By Location to find all cities that lie within 5 miles\nof I-94.\nb. Open the cities attribute table and examine the selected records.\nQuestion 6.5 How many cities are within 5 miles of I-94?\n15. Clear all of the currently selected features.\n6.5 Selecting By Location with Polygons\n1. The next question you\u2019ll answer is how many cities are within states\nthat have a high population. Unfortunately, the states layer only has\ntotal population values, not any information about numbers of cities or\npopulated places. This type of question can be answered spatially using\nthe select by attribute and select by location tools you\u2019ve been using so\nfar in this lab. 180\nChapter 6 Using GIS for Spatial Analysis\n2. Bring up the Select By Attributes dialog box again.\n3. Construct a query that will select all states with a year 2005 population\n(the Field is called POP2005) of greater than 2,000,000 persons.\n(Source: Esri\u00ae ArcGIS ArcExplorer graphical user interface\nCopyright \u00a9 Esri.)\n4. Click OK when the query is ready.\nQuestion 6.6 How many states had a year 2005 population of greater than\n2,000,000 persons?\n5. Next, bring up the Select By Location tool to select cities that fall\nwithin one of these selected states (that have a population of more than\n2,000,000 persons). 181\nGIS Spatial Analysis: ArcGIS Version\n(Source: Esri\u00ae ArcGIS ArcMap ArcInfo graphical user interface\nCopyright \u00a9 Esri.)\n6. You\u2019ll want to select features from the cities layer (the Target) that are\ncompletely within the Source layer features (the States).Click Apply\nwhen the settings are correct and OK to close the dialog box.\n7. Answer Question 6.7 (this will involve examining the selected cities in\nthe attribute table).\nQuestion 6.7 How many cities are contained within the United States with\na population of more than 2,000,000 persons?\n8. Clear all of the currently selected features.\n6.6 More Spatial Analysis with the Selection Tools\n1. The next analysis that you\u2019ll perform involves analysis of the features\nin relation to the five Great Lakes, so zoom in on the View so that you\ncan see all five Great Lakes. The first thing you\u2019ll have to do is select the\nGreat Lakes to work with. 182\nChapter 6 Using GIS for Spatial Analysis\n2. Bring up the Select By Attributes dialog box. Choose lakes as the Layer\nto select from. You\u2019ll see that three fields are available to you \u2013 FID,\nAREA, and NAME. Select \u2018\u2018NAME\u2019\u2019 and press the Get Unique Values\nbutton.\n3. You\u2019ll want to build a query that looks for the name of each of the five\nGreat Lakes as follows: (NAME = \u2018Lake Erie\u2019 OR NAME = \u2018Lake Huron\u2019\nOR NAME = \u2018Lake Michigan\u2019 OR NAME = \u2018Lake Ontario\u2019 OR NAME =\n\u2018Lake Superior\u2019).\n(Source: Esri\u00ae ArcGIS ArcMap ArcInfo graphical user interface\nCopyright \u00a9 Esri.)\n4. Answer Question 6.8, and then click OK. All five Great Lakes will now be\nselected. Close the Select By Attributes dialog box.\nQuestion 6.8 Why is the \u201cOR\u201d operator used in this expression rather than\nthe \u201cAND\u201d operator? What would the same query give you back is you used\n\u201cAND\u201d rather than \u201cOR\u201d in each instance?\n5. Use the Select By Location tool to find out how many rivers are within\n5 miles of the Great Lakes. Note that even if part of the river is within the\n5-mile distance, the entire river system will be selected. 183\nGIS Spatial Analysis: ArcGIS Version\nQuestion 6.9 Which rivers are within 5 miles of the Great Lakes?\nWhich river system are they part of?\n6. Lastly, use Select By Location to find out how many cities are within\n5 miles of the Great Lakes.\nQuestion 6.10 How many cities are within 5 miles of the Great Lakes?\n7. Exit ArcMap by selecting Exit from the File pull-down menu. There\u2019s no\nneed to save any data in this lab.\nClosing Time\nNow that you have the basics of working with GIS data and performing some\ninitial spatial analysis down, the lab in Chapter 7 will describe how to take\nspatial data and create a print-quality map using AEJEE or ArcGIS 10. This page was intentionally left blank 67\nUsing GIS to Make a Map\nScale, Map Elements, Map Layouts, Type, Thematic Maps,\nData Classification Methods, Color Choices, and Digital Map\nDistribution Formats\nOne thing that makes spatial data unique is that it can be mapped. Once\nyou\u2019ve used GIS to create the boundaries of a Civil War battlefield and analyze\nits proximity to new housing and commercial developments, the next step is\nto make a map of the results of your work. However, making a good map with\nGIS involves more than taking the final product of the analysis, throwing on\na title and legend, clicking on \u201cprint,\u201d and then calling it a day. There are nu-\nmerous design elements and considerations that should be taken into account\nwhen laying out a map. In fact, there\u2019s a whole art and science of mapmaking\n(called cartography), involving things like color selection, the positioning of cartography the art\nitems on the map, or what kind of message the final map product is conveying and science of creating\nand designing maps.\nto the reader.\nA map is a representation of spatial data that is designed to convey in- map a representation\nformation to its reader. Making digital maps with GIS offers a wide variety of of geographic data.\noptions for creating an end-product, but there are a few considerations to take\ninto account before you start the mapping process. For example, say there\u2019s a\nnearby municipal wooded area that gets used by the local populace for walk-\ning trails, biking, picnicking, or walking the dogs. The only map of note is an\nold hand-drawn one you can pick up at a kiosk at the park entrance and the\nlocal government wants to update it. As you\u2019re the local geospatial technology\nexpert, they come to you to do the job. Before you break out your GIS tools,\nthere are some basic questions to ask that are going to define the map you\ndesign.\nFirst, what\u2019s the purpose of the map and who will be using it? If the park\nmap you\u2019ve been asked to design is intended for the park\u2019s visitors to use, then\nsome of the important features the map should contain are locations of park\nentrances, parking, comfort stations, and designated picnic areas. It should be\nat an appropriate scale, the trails should be accurately mapped, and trailhead\n118855 186\nChapter 7 Using GIS to Make a Map\nThinking Critically with Geospatial Technology 7.1\nWhy is Map Design Important?\nWhy is the design of a map\u2019s appearance so im- designed map can have and what sort of impact\nportant? After all, you could argue that a poorly it can make for some of the following concepts\u2014a\nput together map of New Zealand is still (at the map of a local county fair, a real estate map of a\nend of the day) a map of New Zealand. However, a new subdivision, a promotional map showing the\nwell-designed map is going to be more useful than location of a small business, or a map of a proposed\na poorly designed one, but why? Why are things new urban development readied for promotional\nlike the choice of colors, lettering, and placement purposes. Also, how can the design of a map influ-\nof items on the map critical to the usefulness ence how the map\u2019s information can be perceived\nof a map? Consider what kind of effect a poorly by its reader?\nlocations and points of interest throughout the park will likely be highlighted.\nIf the map is to be used for zoning purposes, things like exact boundaries,\nroad systems, and parcel information will likely be some of the most impor-\ntant factors.\nSecond, is the information on the map being easily conveyed to the map\nreader? For instance, if you\u2019re designing a trail map, would all trails be named\nor marked, or would that cause too much congestion and clutter on the map?\nAre the colors and symbols appropriate for the assumed novice user of the\nmap? For instance, the trail map would likely not need an inset map of the\nsurrounding area to put the park into a larger context, nor should the map\nreader be left guessing as to what symbols really mean on the map. Third, is\nthe map well designed and laid out properly? A good map should be well bal-\nanced, in regard to the placement of the various map elements, for best ease\nof use by the map reader. This chapter will examine several of these carto-\ngraphic design and data display functions, and by the time we reach the lab,\nyou\u2019ll be designing a professional-looking map of your own using GIS.\nHow Does the Scale of the Data Affect the\nMap (and Vice Versa)?\nA basic map item would be information about the scale of the map, and there\ngeographic scale\nare a couple different ways of thinking about scale. First is the geographic\nthe real-world size or\nextent of an area. scale of something\u2014things that take up a large area on the ground (or have\nlarge boundaries) would be considered large geographic scale. For instance,\nmap scale a metric\nstudying a global phenomenon is a much larger geographic scale than study-\nused to determine the\nrelationship between ing something at a city level, which would be a much smaller geographic\nmeasurements made scale. Something different is map scale, a value representing that x num-\non a map and their real- ber of units of measurement on the map equals y number of units in the real\nworld equivalents.\nworld. This relationship between the real world and the map can be expressed 187\nHow Does the Scale of the Data Affect the Map (and Vice Versa)?\nas a representative fraction (RF). An example of an RF would be a map scale\nRF representative\nof 1:24000\u2014a measure of one unit on the map would be equal to 24,000 units\nfraction\u2014a value\nin the real world. For instance, measuring one inch on the map would be the indicating how many\nsame as 24,000 inches in the real world or one foot on the map is equal to units of measurement\n24,000 feet in the real world (and so on). in the real world are\nequivalent to how\nMaps are considered large-scale maps or small-scale maps depending\nmany of the same units\non that representative fraction. Large-scale maps show a smaller geographic\nof measurement on a\narea and have a larger RF value. For instance, a 1:4000-scale map would be map.\nconsidered a large-scale map\u2014due to the larger scale, it would show a smaller\nlarge-scale map a\narea. The largest-scale map you could make would be 1:1\u2014where one inch\nmap with a higher\non the map was equal to one inch of measurement in the real world (that is, value for its\nthe map would be the same size as the ground you were actually mapping\u2014a representative fraction.\nmap of a classroom would be the same size as the classroom itself). Converse- Such maps will usually\nshow a small amount of\nly, a small-scale map would have a smaller RF value (such as 1:250,000) and\ngeographic area.\nshow a much larger geographic area.\nFor instance, on a very small-scale map (such as one that shows the entire small-scale map a\nmap with a lower value\nUnited States), cities would be represented by points, and likely only major\nfor its representative\ncities will be shown. On a slightly larger-scale map (one that shows all of the\nfraction. Such maps will\nstate of New York), more cities are likely to be shown as points, along with\nusually show a large\nother major features (additional roads can be shown as lines, for example). On amount of geographic\na larger-scale map (one that shows only Manhattan), the map scale allows for area.\nmore detail to be shown\u2014points will now show the locations of important fea-\ntures and many more roads will be shown with lines. On an even larger-scale\nmap (one that shows only a section of lower Manhattan) buildings may now\nbe shown as polygon shapes (to show the outline or footprint of the buildings)\ninstead of points, and additional smaller roads may also be shown with lines.\nThe choice of scale will influence how much information the map will be\nable to convey and what symbols and features can be used in creating the map\nin GIS. Figure 7.1 on page 188 shows a comparison between how a feature (in\nthis case, Salt Lake City International Airport) is represented on large-scale\nand small-scale maps. The actual sizes of the maps greatly vary, but you can\nsee that more detail and definition of features is available on the larger-scale\nmap than on the smaller-scale one (also see Hands-on Application 7.1: Powers\nof 10 \u2013 A Demonstration of Scale, page 188, for a cool example of visualizing\ndifferent scales).\nThe same holds true for mapping of data\u2014for instance, the smaller-scale\nmap of all of the state of New York could not possibly show point locations of\nall of the buildings in Manhattan. However, as the map scale grows larger, dif-\nferent types of information can be conveyed. For instance, in Figure 7.1, the\nlarge-scale map can convey much more detail concerning the dimensions of\nthe airport runways, while the smaller-scale map has to represent the airport\nas a set of simplified lines. If you were digitizing the lines of the airport run-\nways, you\u2019d end up with two very different datasets (one more detailed, one\nvery generalized).\nThis relationship between the scale of a map and data that can be de-\nrived from a map can be a critical issue when dealing with geospatial data. For 188\nChapter 7 Using GIS to Make a Map\nFIGURE 7.1 Salt Lake\nCity International\ninstance, say you\u2019re doing field mapping and GIS work of a university cam-\nAirport\u2014a comparison of\npus. At this small geographic scale, you\u2019re going to require detailed informa-\nits representation on a\n1:24,000 scale map (left) tion that fits your scale of analysis. If the hydrologic and transportation data\nand a 1:100,000 scale you\u2019re working with was derived from 1:250,000-scale maps, it\u2019s likely going\nmap (right).\nto be way too coarse to use. Data generated from smaller-scale maps is prob-\n(Source: Utah State\nably going to be incompatible with the small geographic scale you\u2019re working\nGeographic Information\nDatabase (SGID).)\nHands-on Application 7.1\nPowers of 10 \u2013 A Demonstration of Scale\nThough it\u2019s not a map, an excellent demonstration changes to a view of 1 million light years away, and\nof scale and how new items appear as the scale then 100,000 light years away (a factor of ten each\nchanges is available at http:\/\/micro.magnet.fsu. time). The scale continues changing until it reaches\nedu\/primer\/java\/scienceopticsu\/powersof10. Earth\u2014and then continues all the way down to sub-\nThis Website (which requires Java on a computer atomic particles. Let it play through, and then use\nfor it to properly run) shows a view of Earth start- the manual controls to step through the different\ning from 10 million light years away. Then the scale scales for a better view of the process. 189\nWhat Are Some Design Elements Included in Maps?\nat. For instance, digitizing features on a small-scale map (like a 1:250,000\nscale) are going to be much more generalized than data derived from larger-\nscale maps (like a 1:24,000 scale) or things such as aerial photos at a larger\nscale (for example, 1:6000). See Chapter 9 for more information on using\naerial photos for analysis.\nWhat Are Some Design Elements Included\nin Maps?\nThere are several elements that should show up on a good map. For instance,\nsomeone reading a map should be able to quickly figure out what scale the\nmap is. A map element might simply be text of the RF (such as 1:24,000,\n1:100,000, or whatever the map scale would be). A graphical representation\nof equivalent distances shown on a map would be a scale bar (Figure 7.2). scale bar a graphical\nThe scale bar provides a means of measuring the map scale itself, except using device used on a map\nto represent map scale.\na measurement of x distance on the scale bar is equal to y units of distance in\nthe real world. north arrow a\nA second map element is a north arrow, a graphical device used to ori- graphical device on a\nmap used to show the\nent the direction of the map. However the map is oriented, the north arrow is\norientation of the map.\nused to point toward the direction that is due north. A north arrow may some-\ntimes be drawn as a compass rose, which shows all cardinal directions on the legend a graphical\ndevice used on a map\nmap. North arrows can be as simple as an arrow with the letter \u201cn\u201d attached,\nas an explanation of\nor as complex as a work of art. See Figure 7.3 on page 190 for examples of\nwhat the various map\nnorth arrows used in map design.\nsymbols and color\nAnother item is the map\u2019s legend\u2014a guide to what the various colors and represent.\nsymbols on the map represent. A good legend should be a key to the symbology\nFIGURE 7.2 Examples\nof various scale bars.\n(Source: Esri\u00ae ArcGIS\nArcExplorer graphical user\ninterface Copyright \u00a9 Esri.) 190\nChapter 7 Using GIS to Make a Map\nFIGURE 7.3 Examples\nof various north arrows.\n(Source: Esri\u00ae ArcGIS ArcMap\ngraphical user interface\nCopyright \u00a9 Esri.)\nof the map (see Figure 7.4 for an example of a map legend). Since the legend\nis the part of the map where you can explain things to the map\u2019s reader, you\nmay want to call it something other than \u201clegend.\u201d Instead, consider calling\nthe legend something more useful like \u201cpark trail guide\u201d or \u201ccounty popula-\ntion change.\u201d\ntype the lettering The choice of type used on the map is also important, as it\u2019s used for\nused on a map. such things as the title, the date of the map\u2019s creation, the name of the map\u2019s\ncreator, the origin of the data sources used to make the map, and the letter-\nlabel text placed\non a map to identify ing attached to the map\u2019s features. GIS programs will usually allow you to\nfeatures. label map features (adding things like the names of rivers, roads, or cities to\nthe map) using an automatic placement tool or allowing you to interactively\nfonts various styles of\nlettering used on maps. select, move, and place map labels.\nWhen selecting the type to use for various map elements, a variety of\ndifferent fonts (or lettering styles) are available. GIS programs (like word-\nprocessing programs such as Microsoft Word) will often have a large num-\nber of fonts to select from, everything from common fonts (such as \u201cArial\u201d or\n\u201cTimes New Roman\u201d) to much flashier fonts (such as \u201cMistral\u201d or \u201cPapyrus\u201d).\nFIGURE 7.4 Examples See Figure 7.5 for some examples of different versions of type fonts avail-\nof various legend items.\nable in ArcExplorer Java Edition for Educators. Often, a map will contain only\n(Source: Esri\u00ae ArcGIS\ntwo different fonts, carefully selected to fit well with each other instead of\nArcExplorer graphical user\ninterface Copyright \u00a9 Esri.) 191\nWhat Are Some Design Elements Included in Maps?\nMap Source: Esri Data Map Source: Esri Data FIGURE 7.5 Examples of\nMap Source: Esri Data Map Source: Esri DATA various fonts available for\ntype in map layouts.\nMap Source: Esri Data Map Source: Esri Data\nMap Source: Esri Data Map Source: Esri DATA\nMap Source: Esri Data Map Source: Esri Data\nclashing. Using too many fonts or crafting a map\u2019s type out of several of the\nmore elaborate fonts is likely to make a map difficult to read and reduce its\nusefulness. See Hands-on Application 7.2: Typebrewer Online for a tool you can\nuse as an aid in selecting appropriate type for a map.\nIn GIS, a map is put together by assembling all of the elements together\nin a layout. A good way to think of a layout is a digital version of a blank piece layout the assemblage\nof paper that you will then arrange the various map elements together in a and placement of\ncartographic design to create a final map. Sometimes, the software (such as various map elements\nused in constructing a\nArcGIS) will include several map templates, which provide pre-created de-\nmap.\nsigns for your use. Using a template will take your GIS data and place things\nlike the title, legend, and north arrow at pre-determined locations and sizes map template a pre-\nmade arrangement of\non the layout. Templates are useful for creating a quick printable map layout,\nitems in a map layout.\nbut GIS will also allow you to design a map the way you want it in regard to\nsize, type, and placement of elements.\nWhen designing a layout, it\u2019s important to not simply slap a bunch of\nelements on the map and decide that the map\u2019s ready. Balancing the place-\nment of items is important, so as not to overload the map with too much\ninformation, but also to provide a useful, readable map. Good map balance\nprovides a means of filling in the \u2018\u2018empty\u2019\u2019 spaces on the map, so it doesn\u2019t\nhave all its information crammed into the top or bottom, leaving other parts\nof the map blank. Items should be of uniform size (instead of, say, making\nthe north arrow gigantic just to fill in some empty space) and placed in pro-\nportion to one another (see Figure 7.6 on page 192 for an example of trying\nto balance out the placement, size, and proportion of items in a map layout\nwith a template).\nHands-on Application 7.2\nTypebrewer Online\nTypebrewer is a very neat (and free) online utility on a map, then alter aspects of the type (such as\nthat allows you to examine different font styles and its size, density, and tracking) to see the effect of\ncombinations used on maps. With Typebrewer, you those changes on the map. The aim of Typebrewer\ncan interactively change font size, density, or ap- is to examine different forms of map type so you\npearance. Open your Web browser and go to http:\/\/ can apply those aspects to your own maps. Try look-\nwww.typebrewer.org to get started (you\u2019ll need to ing at several different formats of the type, then\nhave Adobe Flash Player installed on your computer decide on what the most appropriate one would be\nfor it to work properly). With Typebrewer, you can and why.\nselect from several different pre-set styles of type 192\nChapter 7 Using GIS to Make a Map\nFIGURE 7.6 Example of\nbalancing the placement\nof items on a map layout.\n(Source: Esri\u00ae ArcGIS ArcMap\ngraphical user interface\nCopyright \u00a9 Esri.)\nHow Is Data Displayed on a GIS Map?\nThere are several different types of maps that can be made using GIS tools.\nreference map a map The purpose of a reference map (like what you would see in an atlas or a road\nthat serves to show the map) is to give location information or highlight different features. A map of\nlocation of features, park trails, a Manhattan restaurant guide map, a map of the c asinos on the\nrather than thematic\nLas Vegas Strip, or a zoning map of your neighborhood are all examples of\ninformation.\nreference maps. Topographic maps (that also show landforms) are another\nexample of reference maps (and we\u2019ll deal with them in Chapter 13).\nthematic map a Another main type of map is the thematic map, which is geared toward\nmap that displays a conveying one particular theme to the reader. Thematic maps might be used\nparticular theme or to show things such as the increase in U.S. population for each state or U.S.\nfeature.\nPresidential election results by county (see Hands-on Application 7.3: Census\nBureau Thematic Maps for examples of different types of thematic maps on-\nline). In GIS, a layer\u2019s attributes are used for displaying information on the\nmap. For instance, to create a map of the 2008 U.S. Presidential election\nHands-on Application 7.3\nCensus Bureau Thematic Maps\nThe U.S. Census Bureau makes a number of thematic check them out. Use the tools on the Website to\nmaps of the United States available for viewing on- create quick thematic maps using pre-defined cen-\nline, using different census demographic informa- sus data (such as population, housing, or retail sales\ntion (such as states\u2019 population, income levels, hous- values). You can choose to create maps by divisions\ning costs, and retail sales levels). Open your Web including state, county, or region, as well as zoom in\nbrowser and go to http:\/\/factfinder.census.gov\/ and out of the maps and identify features.\njsp\/saff\/SAFFInfo.jsp?_pageId=thematicmaps to 193\nHow Is Data Displayed on a GIS Map?\nFIGURE 7.7 A thematic\nmap showing the\nresults of the 2008 U.S.\nPresidential election by\ncounty (the vote in blue\ncounties going for Barack\nObama and the vote in red\ncounties going for John\nMcCain). (Source: Library of\nCongress Geography and Map\nDivision.)\nresults by county, each polygon representing that county would have an at-\ntribute designating whether Barack Obama or John McCain had a higher\nnumber of votes for that county (colored red for counties won by McCain and\ngraduated symbols\nblue for counties won by Obama). This attribute would then be the data being\nthe use of different\ndisplayed on the map (Figure 7.7). Other thematic maps may use graduated sized symbology\nsymbols for display. In this case, points (or other symbols) are plotted of dif- to convey thematic\nfering sizes to represent the thematic factors. information on a map.\nOf course, many thematic maps don\u2019t rely on a simple two-choice attribute choropleth map\nsuch as the election map (in which each county will be marked Obama or Mc- a type of thematic\nCain\u2014or else contain no data). Attributes such as the percentage of colleges map in which data is\ndisplayed according to\nand universities (per state) that are using this textbook in a class (or some\none of several different\nother values) are mapped using a type of thematic map called a choropleth\nclassifications.\nmap. In order to best display this information on a map, the data will have\ndata classification\nto be classified or divided into a few categories. GIS gives several options for\nvarious methods used\ndata classification of multiple values\u2014each method classifies data differ-\nfor grouping together\nently and can thus result in some very different results being displayed on the\n(and displaying) values\nmaps. (See Figure 7.8 on page 194 for four different maps of the percentage on a choropleth map.\nof the total number of houses per state that are considered seasonal homes or\nNatural Breaks a data\nvacation homes by census, each created using a different data classification\nclassification method\ntechnique.) Note that each map has the data broken into the same number of that selects class break\nclasses (four) for comparison purposes. levels by searching\nThe first of these data classification methods is called Natural Breaks for spaces in the data\nvalues.\n(also referred to as the Jenks Optimization). Like the name implies, this 194\nChapter 7 Using GIS to Make a Map\n(a) (b)\n(c) (d)\nFIGURE 7.8 Four method takes all of the values being mapped and looks at how they\u2019re grouped\nchoropleth map examples together. The spaces in between the data values are used to make different\ncreated using the same classes (or ranges of data) that are displayed. This is shown in Figure 7.8a\u2014\ndata (the year 2000\nstates with the lowest percentages of seasonal homes values (such as Nebraska,\npercentage of the total\nnumber of houses that Oklahoma, and Texas) end up in one class, and states with the highest per-\nare considered seasonal centages of seasonal homes (such as Maine, Vermont, and New Hampshire)\nor vacation homes) end up together in another class.\nbut different data\nThe map in Figure 7.8b shows the results of using the Quantile method\nclassification methods as\nfollows: (a) Natural Breaks, of data classification. This method takes the total number of data values to be\n(b) Quantiles, (c) Equal mapped and splits them up into a number of classes. It tries to distribute val-\nInterval, and (d) Standard ues so that each range has a similar number of values in it. For instance, with\nDeviation. (Source: Esri\u00ae\n51 states being mapped (plus the District of Columbia as a 51st area), each of\nArcGIS ArcMap graphical user\nthe four ranges will have about 13 counties worth of data being shown in each\ninterface Copyright \u00a9 Esri.\nData: US Census Bureau.) class. Since the break points between the ranges are based on the total num-\nber of items being mapped (that is, how many states ending up in each range),\nQuantile a data\nrather than the actual data values being mapped, the Quantile method causes\nclassification method\nthat attempts to place a relatively even distribution of values on the map.\nan equal number of The third method (shown in the map in Figure 7.8c) uses Equal Inter-\ndata values in each vals for data classification. It works like it sounds\u2014it creates a number of\nclass.\nequally sized ranges and then splits the data values into these ranges. The 195\nHow Is Data Displayed on a GIS Map?\nsizes of each range are based on the total span of values to be mapped. For\nEqual Interval a\ninstance, in the seasonal home maps, the data is divided into four classes,\ndata classification\nand the range of values goes from the state with the lowest seasonal home method that selects\npercentage (0.6% of the total housing stock in Illinois) to the state with the class break levels by\nhighest seasonal home percentage (15.6% in Maine). Equal Interval takes taking the total span of\nvalues (from highest to\nthe complete span of data values (there is 15% separating the lowest and\nlowest) and dividing by\nhighest values) and divides it by the number of classes (in this case, four),\nthe number of desired\nand that value (in this case, 3.75%) is used to compute the breaking point classes.\nbetween classes. So the first class represents states that have a seasonal\nhome value of 3.75% more than the class\u2019 lowest end (for instance, the first\nclass would have values between 0.6% and 4.35%). Note that this method\nsimply classifies data based on the range of all values (including the highest\nand lowest) but does not take into account clusters of data or how the data\nis distributed. As such, only a few states end up in the upper class because\ntheir percentages of seasonal homes were greater than three-fourths of the\ntotal span of values.\nThe final method is presented in the map in Figure 7.8d, the Standard Standard Deviation\nDeviation method. A standard deviation is the average distance that a single a data classification\ndata value is away from the mean (the average) of all data values. The break- method that computes\nclass break values by\npoints for each range are based on these statistical values. For instance, the\nusing the mean of the\nGIS would calculate the average of all United States seasonal home values\ndata values and the\n(3.9%) and the standard deviation for them (3.1%). So when it comes to the average distance a\npercentage of the total housing stock that is seasonal, each state\u2019s percentage value is away from the\nis an average of 3.1% away from the average state\u2019s percentage. These values mean.\nfor the mean and standard deviation of the values are used to set up the break-\npoints. For instance, the breakpoint of the first range is of all states whose sea-\nsonal home values are less than half a standard deviation value lower than the\nmean\u2014those states with a seasonal home percentage of less than the mean\nminus 0.5 times the standard deviation (1.55%), or 2.34%. The fourth range\nconsists of those counties with a value greater than 1.5 times the standard\ndeviation away from the mean. The other ranges are similarly defined by the\nmean and standard deviation values of the housing data.\nLike Figure 7.8 shows, the same data can produce some very different\nlooking choropleth maps depending on which method is used to classify\nthe data, with differing messages from the maps. For instance, the map\nin Figure 7.8d (Standard Deviation) shows that most states have roughly\nan average (or below average) percentages of seasonal homes, while the\nother maps show various distinctions between which states are classified\nas a higher or lower percentage of homes that are seasonal. Thus, the same\ndata can result in different maps, depending on the classification method\nchosen. When selecting a method, having information about the nature of\nthe data itself (that is, if it is evenly distributed, skewed toward low or high\nnumbers, or all very similar values) will aid in ending up with the best kind\nof mapping.\nAlso keep in mind, while different data classification methods can affect\nthe outcome of the map, the type of data values being mapped can greatly 196\nChapter 7 Using GIS to Make a Map\naffect the outcome of the choropleth map. An issue involved with choropleth\nmapping is displaying the values of data that can be counted (for instance,\nthe total population values per state or the total number of housing sales\nper county) when the sizes of the areas being mapped are very different. If\nyou were mapping the number of vacation homes of each state, a very large\nstate like California is probably going to have a larger number of homes\n(12,214,549 total homes with 239,062 of them being seasonal, according to\nthe 2000 Census) than a smaller state like Rhode Island (439,837 homes t otal\nwith 13,002 of them seasonal). Thus, if you\u2019re making a choropleth map of\nthe number of vacation homes in each state, California will show many more\nvacation homes than Rhode Island, just because it has a lot more houses (due\nto a larger area and population).\nHowever, a better measure to map would be the percentage of the to-\ntal number of houses that are vacation homes\u2014in this way, the big differ-\nence in housing counts between California and Rhode Island isn\u2019t a factor\nin the map. Instead, you\u2019re mapping a phenomenon that can be compara-\nbly measured between the two states. When you map the percentages of\nthe total housing that are considered vacation homes, California\u2019s seasonal\nhomes only make up about 2% of the total houses, while Rhode Island has\nabout 3% seasonal homes. To make a choropleth map of count data, the\nnormalized altering data should first be normalized, or have all count values brought onto the\ncount data values same level. For instance, dividing the number of seasonal homes by the\nso that they are at total number of houses would be a way of normalizing the data. See Hands-\nthe same level of\non Application 7.4: Interactive Thematic Mapping Online for an online tool\nrepresenting the data\nthat is used for creating thematic maps.\n(such as using them as\na percentage).\nHands-on Application 7.4\nInteractive Thematic Mapping Online\nIt\u2019s now time to start making your own thematic new layer in Google Earth). After the Quantile map is\nmaps. Open your Web browser and go to http:\/\/ set up, try the same settings but with the Equal In-\nthematicmapping.org\/engine. This is the Website terval classification method to check out what kind of\nof the Thematic Mapping Engine, which allows you map is generated.\nto create numerous kinds of thematic maps of data Try some of the other indicators (such as CO2\nfrom a variety of topics. First, select an indicator (start emissions, GDP per capita, Infant mortality rate, or\nwith something simple like Population) and choose a Mobile phone subscribers) over a range of years.\nyear (like 2010). Choose Choropleth for the technique These thematic maps can be created using gradu-\nand pick a set of colors. For the Classification method, ated symbols, choropleth maps, or even 3D-style\nchoose Quantiles. Lastly, click on the Preview button prism maps (we\u2019ll use the Thematic Mapping Engine\nto see a thematic map of world population displayed again in Chapter 14 with these types of maps). You\nin an interactive Google Earth interface in your Web can specify which data classification method to use\nbrowser (alternatively, you can select the Download and finally display the mapped results using Google\noption to actually download the thematic map as a Earth or the online preview. 197\nWhat Kinds of Colors Are Best to Use with GIS Maps?\nWhat Kinds of Colors Are Best\nto Use with GIS Maps?\nThe choice of color for a map is also important\u2014first to create something\npleasing for the map reader, but also for graphic design and use in a digital\nformat. For instance, the choropleth maps in Figure 7.8 used a range of values\nof dark green for low percentages and bright blue for high percentages, but\nthere are a lot of ways those classifications could have been colored. There\nare different color schemes for use in GIS maps\u2014one of these is RGB, a setup RGB a color scheme\nused for design on computer screens. RGB utilizes the colors red, green, and based on using the\nblue as the primary colors, mixing them together as needed to create other three primary colors\nred, green, and blue.\nhues. Another is CMYK, used in graphic design, which uses the colors of cyan,\nmagenta, yellow, and black (the \u201cK\u201d of the title) for creating hues. Chapter 10 CMYK a color scheme\nwill deal further with generating different colors from an initial set of primary based on using the\ncolors cyan, magenta,\ncolors in the context of examining digital imagery.\nyellow, and black.\nWith a reference map, different colors are used to represent the various\nobjects and symbols. On a choropleth map (like those shown in Figure 7.8), a\nrange of shades or colors is needed to show the lowest classes of data through\nthe highest classes. In this case, a color ramp is used to show a set of colors color ramp a range of\nto represent the classes of values. A color ramp is a selected set of colors that colors that are applied\nwill show changes of a color scheme\u2014for instance, using a single hue of a to the thematic data on\na map.\ncolor in a color ramp might run the range of lightest to darkest blue or light\ngreen to very dark green. Other color ramps (like those shown in Figure 7.8)\nincorporate multiple hues ranging from green through blue. Several different\nexamples of color ramps are shown in Figure 7.9.\nPart of why color choice becomes important with a map is the wide range\nof visualization media that are available for viewing it. Colors of map items\nmay appear one way on a computer monitor, another way when they\u2019re pro-\njected onto a screen with an LCD projector, and also appear differently yet\nagain when printed by an inkjet or laserjet printer. Just because you like the\nlook of the shades of blue and green on the GIS map on your monitor, it doesn\u2019t\nFIGURE 7.9 Examples of\nSingle Hue Color Ramps various color ramps.\nMultiple Hue Color Ramps 198\nChapter 7 Using GIS to Make a Map\nHands-on Application 7.5\nColorBrewer Online\nTo further investigate the impact of color choices on maps can be distinguished from one another, or ex-\na map, check out ColorBrewer 2.0, which is available amine the best choices for printing, photocopies, and\nonline at http:\/\/colorbrewer2.org. ColorBrewer is a color-blind safe schemes. Try out several of the op-\ngreat tool for color choice and color ramp selection tions to determine (based on the pre-set choropleth\nfor a variety of different map data and formats. Color- maps) what the best color scheme would be. You can\nBrewer 2.0 sets up a pre-determined choropleth map use ColorBrewer in a similar way as Typebrewer\u2014to\nwhile allowing you to examine different color selec- determine the optimum setup of colors and then ap-\ntions, see how different color choices on choropleth ply those settings to your own maps.\nmean that the same colors will translate the same way to a printed page or\na photocopy of the printout. It might even have a \u201cwashed out\u201d appearance\nwhen the map is projected onto a big screen. Check out Hands-on Application\n7.5: ColorBrewer Online to investigate the effect that different color choices\nwill have on a final map.\nHow Can GIS Maps Be Exported\nand Distributed?\nOnce a map has been designed and formatted the way you want it, it\u2019s time to\nshare and distribute the results. Rather than just printing a copy of the map,\nthere are several digital formats that maps can be quickly and easily converted\nto for ease of distribution. A simple way is to export the map as a graphical\nraster file\u2014this saves a \u2018\u2018snapshot\u2019\u2019 of the map as a digital graphic that can be\nviewed like a picture, either as a file or an image placed on a Website. There\nare many different formats for map export, and two common ones are JPEG\nJPEG the Joint (Joint Photographic Experts Group) and TIFF (Tagged Image File Format).\nPhotographic Experts Images saved in JPEG format can experience some data loss due to the file\nGroup image, or graphic\ncompression involved with JPEGs. Consequently, JPEG images usually have\nfile format.\nsmaller file sizes for viewing or downloading. Images saved in TIFF format\nTIFF the Tagged have a much larger file size but are a good choice for clearer graphics.\nImage File Format used\nThe clarity of an image is a function of what DPI (Dots Per Inch) setting is\nfor graphics or images.\nused when the map is exported to a graphic. The lower the value of DPI (such\nDPI Dots Per Inch\u2014a as a value of 72), the less clarity the resultant image will have. Very low values\nmeasure of how coarse\nof DPI will result in the exported image being very blocky or pixilated. Maps\n(lower values) or\ne xported with higher values of DPI (such as 300) will be very crisp and clear.\nsharp (higher values)\nHowever, the higher the DPI value, the larger the file size will be, which be-\nan image or map\nresolution will be when comes important when distributing map data online, as the larger the file, the\nexported to a graphical more time will be needed to transfer or display the images. For professional-\nformat. print-quality maps (such as the maps and graphics in books), the TIFF file for-\nmat is used at a higher DPI value (such as 300) to create good resolution maps. 199\nKey Terms\nA GeoPDF is another option for exporting and distributing maps. A Geo-\nGeoPDF a format that\nPDF allows the user to export a GIS map in the commonly used PDF file for-\nallows for maps to\nmat, which can be opened by the free Adobe Reader software using a special be exported to a PDF\nfree plug-in. A GeoPDF differs from a regular PDF in that it allows the user to format, yet contain\ninteract with map layers and get information about the coordinates of loca- geographic information\nor multiple layers.\ntions shown in the PDF. For instance, a GeoPDF could contain multiple layers\nof data that the user could turn on and off (such as annotation for road names\nor a separate layer of the roads themselves) as in GIS (see Chapter 13 for more\nusages of GeoPDFs with other maps).\nChapter Wrapup\nOnce GIS data is created, compiled, or analyzed, the results are usually best\ncommunicated using a map, and the ability to create a map of data is a stan-\ndard feature in GIS software packages such as ArcGIS or ArcExplorer Java Edi-\ntion for Educators. However, for producing a useful, readable, well-b alanced\nmap, there are several choices that go into map design, the form of the layout,\nand the presentation of data, colors, and symbols.\nThis chapter presented an overview of several cartographic and map-\ndesign concepts, and this chapter\u2019s lab will have you take the Esri data you\u2019ve\nbeen analyzing in Chapters 5 and 6 and create a professional-quality map\nfrom it. Next chapter, we\u2019re going to examine some specific types of maps\u2014\nroad network maps\u2014and how GIS and geospatial technologies are used to\ndesign street maps that can be used for locating addresses and computing\nshortest paths (and directions) between destinations.\nImportant note: The references for this chapter are part of the online compan-\nion for this book and can be found at http:\/\/www.whfreeman.com\/shellito1e.\nKey Terms\ncartography (p. 185) thematic map (p. 192)\nmap (p. 185) graduated symbols (p. 193)\ngeographic scale (p. 186) choropleth map (p. 193)\nmap scale (p. 186) data classification (p. 193)\nRF (p. 187) Natural Breaks (p. 193)\nlarge-scale map (p. 187) Quantile (p. 194)\nsmall-scale map (p. 187) Equal Interval (p. 194)\nscale bar (p. 189) Standard Deviation (p. 195)\nnorth arrow (p. 189) normalized (p. 196)\nlegend (p. 189) RGB (p. 197)\ntype (p. 190) CMYK (p. 197)\nlabel (p. 190) color ramp (p. 197)\nfonts (p. 190) JPEG (p. 198)\nlayout (p. 191) TIFF (p. 198)\nmap template (p. 191) DPI (p. 198)\nreference map (p. 192) GeoPDF (p. 199) 7.1\nGeospatial Lab Application\nGIS Layouts: AEJEE Version\nThis lab will introduce you to the concepts of taking GIS data and creating a\nprint-quality map from it. This map should contain the following:\na The contiguous United States (48 states without Alaska and Hawaii) pop-\nulation per square mile, set up in an appropriate color scheme\na The data in a different projection than the default GCS one\na An appropriate legend (make sure your legend items have normal names\nand that the legend is not called \u201clegend\u201d)\na An appropriate title (make sure that your map title doesn\u2019t include the\nword \u201cmap\u201d in it)\na A north arrow\na A scale bar\na Text information: your name, the date, and the source of the data\na Appropriate borders, colors, and design layout (your map should be well\ndesigned instead of map elements thrown on at random)\nImportant note: At the end of this lab, there is a checklist of items to aid\nyou in making sure the map you make is complete and of the best quality\npossible.\nSimilar to the geospatial lab applications in Chapters 5 and 6, two ver-\nsions of this lab are provided. The first version (7.1 Geospatial Lab Application:\nGIS Layouts: AEJEE Version) uses the free ArcExplorer Java Edition for Educa-\ntors (AEJEE). The second version (7.2 Geospatial Lab Application: GIS Layouts:\nArcGIS Version) provides the same activities for use with ArcGIS 10.\nObjectives\nThe goals for you to take away from this lab are:\na Familiarize yourself with the layout functions of AEJEE.\na Arrange and print professional-quality maps from geographic data using\nthe various layout elements.\nObtaining Software\nThe current version of AEJEE (2.3.2) is available for free download at http:\/\/\nedcommunity.esri.com\/software\/aejee.\nImportant note: Software and online resources sometimes change\nfast. This lab was designed with the most recently available version of the\n200 201\nGIS Layouts: AEJEE Version\nsoftware at the time of writing. However, if the software or Websites have\nsignificantly changed between then and now, an updated version of this lab\n(using the newest versions) is available online at http:\/\/www.whfreeman.\ncom\/shellito1e.\nLab Data\nThere is no data to copy in this lab. All data comes as part of the AEJEE sample\ndata that gets installed with the software.\nLocalizing This Lab\nThe dataset used in this lab is Esri sample data for the entire United States,\nand you\u2019ll be creating a map layout of the United States. However, the layout\ntools can be used to make a map of whatever dataset you desire\u2014rather than\ncreating a layout map of the entire United States, focus on your home state\nand create a layout of that instead (see Section 7.3 for how to focus the layout\non one state instead of the whole United States). If you\u2019re only going to work\nwith one state, use the counties.shp file instead and make a map of county\npopulation by square mile instead.\n7.1 Initial Pre-Layout Tasks\n1. Start AEJEE (the default install folder is called AEJEE). After AEJEE\nopens, do the following (if needed, refer back to 5.1 Geospatial Lab\nApplication: GIS Introduction: AEJEE Version for specifics on how to do\nthese things):\n2. Add the states shapefile from the usa sample data folder.\n3. Pan and zoom the View so that the lower 48 states are shown filling up\nthe View.\n4. Change the projection in the View from the default GCS to something\nmore visually pleasing.\nImportant note: 5.1 Geospatial Lab Application: GIS Introduction: AEJEE\nVersion used the Lambert Conformal Conic projection, but there are many\nmore to choose from\u2014keep in mind that your final map will be of the United\nStates if selecting a regional or world projection.\n5. Leave the states symbology alone for now\u2014you\u2019ll change it in the next\nstep.\n7.2 Graduated Symbology\n1. AEJEE gives you the ability to change an object\u2019s symbology from a\nsingle symbol to multiple symbols or colors and allows for a variety of\ndifferent data classification methods. 202\nChapter 7 Using GIS to Make a Map\n2. Right-click on the states shapefile in the TOC and select Properties.\nClick on the Symbol tab. To display the states as graduated symbols, use\nthe following settings:\n(Source: Esri\u00ae ArcGIS ArcExplorer graphical user interface\nCopyright \u00a9 Esri.)\n3. Under \u201cDraw features using:\u201d select Graduated Symbols.\n4. The Field to use (in this exercise) is the POP05_SQMI (the states\u2019\npopulation per square mile from the year 2005).\n5. Use 5 for the number of Classes.\n6. Use Quantile for the Classified by option.\n7. For the color scheme, select an appropriate choice for the Start and End\nchoices. (The default should be yellow for Start and red for End, thus\ngiving you a range from yellow to orange to red for the five classes.)\n8. When you have things arranged how you want them, click Apply to\nmake the changes. Take a look at the map and make any color changes\nyou think are needed.\n9. Click OK to close the dialog box.\n10. Now, the symbology of the states has changed in the View and the values\nthat make up each one of the breaks can be seen in the TOC.\n7.3 Layouts in AEJEE\nTo begin laying out the print-quality version of the map, you\u2019ll need to begin\nworking in the Layout View. This mode of AEJEE works like a blank canvas,\nallowing you to construct a map using various elements. Note that whatever\nappears in the Map View will also appear in the Layout View. For instance, if 203\nGIS Layouts: AEJEE Version\nyou zoom the Map View to only show Ohio, and then switch to Layout View,\nthe layout will only show Ohio.\n1. To begin, select Layout View from the View pull-down menu.\n2. In the Layout View, the black border around the white page represents\nthe border of the printed page of an 8\u00bd (cid:2) 11 piece of paper, so be\ncareful when working near the edges of the page and keep all elements\nof the map within that border.\n3. The toolbar will switch to a new set of tools\u2014the navigation tools are as\nfollows:\n4. Starting at the left and moving right are the following:\na. The cursor that is used to select map elements\nb. The printer icon that is used when printing (see later in the\nexercise)\nc. The plus and minus magnifying glasses that are used to zoom in and\nout of the layout\nd. The hand icon that is used to pan around the map\n5. And also from the right and moving left are the following:\na. The 1:1 icon that is used to zoom the layout to 100%\nb. The white pages with the arrows that are used to (from right to left)\nzoom to see the entire page, zoom out a fixed amount, and zoom in\na fixed amount\n6. The other tools are used for the addition of map elements to the layout,\nand we\u2019ll examine them individually.\n7.4 Map Elements\n1. Again, think of the layout as a blank sheet of paper that you\u2019ll use to\nconstruct your map. There are numerous map elements that can be\nadded, including scale bars, north arrows, and a legend. Each element\nhas properties (such as creating borders, filling colors, or fixing size\nand position) that can be accessed by selecting the Map Elements\nProperties icon on the toolbar: 204\nChapter 7 Using GIS to Make a Map\n2. The default element that appears on the layout is the Data Frame that\ncontains all of the GIS data. All of the visible layers in the TOC under\nthe word \u201cLayers\u201d will appear in this frame. You can\u2019t manipulate\nindividual layers (for instance, you can\u2019t click and drag a state\nsomewhere else), but you can treat the entire Frame as if it\u2019s a map\nelement. Click and drag the Frame around the map or resize it as you\nsee fit (using the eight blue tabs at its corners and midpoints), or access\nits properties (like any regular element) to create borders around the\nmap or fix it in a particular position.\n3. Map elements can be resized and moved by selecting them with the\nmouse and resizing like you would the data frame.\n4. Map elements can be deleted by selecting them with the cursor and\npressing the delete key on the keyboard, or by selecting the Remove\nicon on the toolbar:\n7.5 Layer Properties and Layouts\nEach layer you use in AEJEE (such as the states) has a set of layer properties.\nAnything changed in the layer properties will be reflected in changes to the\nlayout. For instance, if you change the symbology of a layer in its properties,\nit will be changed in the layout elements such as the map legend. However, to\nchange the name of how the layer will appear in something like a map legend,\nthe shapefile must be renamed in the TOC.\n1. To do this, right-click on the name you want to change (the states layer)\nand select Rename.\n2. In the TOC itself, you can type a new name for the layer (like\n\u201cPopulation Per Square Mile\u201d). Click the enter key when you\u2019ve typed in\na new name.\n7.6 Inserting a Scale Bar\n1. To add a scale bar to the map, select the scale bar icon from the toolbar: 205\nGIS Layouts: AEJEE Version\n2. A number of options for scale bars will appear in a new Scale Bar\nSelector dialog box:\n(Source: Esri\u00ae ArcGIS ArcExplorer graphical user interface\nCopyright \u00a9 Esri.)\n3. Select an appropriate scale bar and click OK. Use the cursor to position\nor resize the scale bar on the layout.\n7.7 Inserting a North Arrow\n1. To add a north arrow to the map, select the north arrow icon from the\ntoolbar:\n2. A number of options for north arrows will appear in a new North Arrow\nSelector dialog box:\n(Source: Esri\u00ae ArcGIS ArcExplorer graphical user interface Copyright \u00a9 Esri.) 206\nChapter 7 Using GIS to Make a Map\n3. Select an appropriate north arrow and click OK. Use the cursor to\nposition or resize the north arrow on the layout.\n7.8 Inserting a Legend\n1. To add a legend to the map, select the legend icon from the toolbar:\n2. A default legend will be added to the map, consisting of all the layers in\nthe TOC with whatever names are assigned to them. Use the cursor to\nmove and resize the legend.\n3. To make changes to the default legend, right-click on the legend itself,\nthen select Properties from the new menu options.\n(Source: Esri\u00ae ArcGIS ArcExplorer graphical user interface Copyright \u00a9 Esri.)\n4. In the Legend Properties dialog box, under the Legend tab, you can\nchange the name of the legend, its font, justification, and appearance, as\nwell as spacing between items.\n5. Under the Items tab, you can select what layers from the TOC appear in\nthe legend.\n6. Under the Frame tab, you can change the legend\u2019s color, border, and fill. 207\nGIS Layouts: AEJEE Version\n7. Under the Size and Position tab, you can manually change the size and\nposition of the legend inside the layout.\n8. Click Apply to have your legend changes take effect and OK to close the\ndialog box.\n7.9 Adding Text to the Layout\n1. To add text to the map (such as a title or any other text you may want to\nadd), select the Add Text icon from the toolbar:\n2. Note that when text is added, a small box called \u201cright click this text\u201d is\nplaced near the center of the map.\n3. Do what AEJEE asks and right-click the text box and select Properties\nfrom the new menu items. The Text Properties dialog box will appear.\n(Source: Esri\u00ae ArcGIS ArcExplorer graphical user interface Copyright \u00a9 Esri.)\n4. Type the text you want to appear in the box. To alter the font, color, and\nother properties of the text, click the \u201cChange Properties . . .\u201d option. 208\nChapter 7 Using GIS to Make a Map\nMore options will appear\u2014note that you can alter each text box separately,\nto create larger fonts for titles, smaller fonts for type, and so on.\n(Source: Esri\u00ae ArcGIS ArcExplorer graphical user interface\nCopyright \u00a9 Esri.)\n7.10 Other Map Elements\n1. Though they\u2019re not used in this lab, two other map elements can be added:\na. An overview map\u2014this allows you to add a second map to the\nlayout to show a larger context of the area. Use this icon:\nb. An image\u2014this allows you to add graphics to your map, including\ndownloaded images, digital camera images, or other graphics. Use\nthis icon: 209\nGIS Layouts: AEJEE Version\n7.11 Printing the Layout\n1. When you have constructed the map the way you want, choose Print\nfrom the File pull-down menu (or select the Print icon from the toolbar):\n2. In the Print dialog box under the Page Setup tab, you can make\nadditional changes, such as moving the printed map from Landscape to\nPortrait or adding margins for printing.\n(Source: Esri\u00ae ArcGIS ArcExplorer graphical user interface Copyright \u00a9 Esri.)\n3. When the layout is ready the way you want it, click Print and AEJEE\nshould print to your computer\u2019s default printer.\n4. Exit AEJEE by selecting Exit from the File pull-down menu.\nClosing Time\nThe geospatial lab application in Chapter 8 will keep using AEJEE for analy-\nsis\u2014you\u2019ll be using a Web service to take a series of addresses and plot them\non a map, then using those results in AEJEE for some simple analysis. 210\nChapter 7 Using GIS to Make a Map\nFinal Layout Checklist\n____ The contiguous 48 states (without Alaska and Hawaii) United States\npopulation per square mile, classified, and displayed with an appro-\npriate color scheme\n____ States shown in a different (and appropriate) projection than the\ndefault\n____ Appropriate map legend, with all items listed with normal names\n(the legend should not be called \u201clegend\u201d and should be appropriate\nsize, font, number of columns, and so on)\n____ Scale bar\n____ Appropriate title (don\u2019t use the words \u201cmap\u201d or \u201ctitle\u201d in your title)\nand appropriate size and font\n____ A north arrow of appropriate size and position\n____ Type (your name, the date, the source of the data) in an appropriate\nsize and font\n____ Overall map design (appropriate borders, color schemes, balance of\nitems and placement, and so on) 7.2\nGeospatial Lab Application\nGIS Layouts: ArcGIS Version\nThis lab will introduce you to the concepts of taking GIS data and creating a\nprint-quality map from it. This map should contain the following:\na The contiguous United States (48 states without Alaska and Hawaii) pop-\nulation per square mile, set up in an appropriate color scheme\na The data in a different projection than the default GCS one\na An appropriate legend (make sure your legend items have normal names\nand that the legend is not called \u201clegend\u201d)\na An appropriate title (make sure that your map doesn\u2019t include the word\n\u201cmap\u201d in it)\na A north arrow\na A scale bar\na Text information: your name, the date, and the source of the data\na Appropriate borders, colors, and design layout (your map should be well\ndesigned instead of map elements thrown on at random)\nImportant note: At the end of this lab, there is a checklist of items to aid\nyou in making sure the map you make is complete and of the best quality\npossible.\nThe previous 7.1 Geospatial Lab Application, GIS Layouts: AEJEE Version\nuses the free ArcExplorer Java Edition for Educators (AEJEE); however, this\nlab provides the same activities for use with ArcGIS 10.\nObjectives\nThe goals for you to take away from this lab are:\na Familiarize yourself with the layout functions of ArcGIS.\na Arrange and print professional-quality maps from geographic data using\nthe various layout elements.\nObtaining Software\nThe current version of ArcGIS (10) is not freely available for use. However,\ninstructors affiliated with schools that have a campus-wide software license\nmay request a 1-year student version of the software online at http:\/\/www.\nesri.com\/industries\/apps\/education\/offers\/promo\/index.cfm.\nImportant note: Software and online resources sometimes change fast.\nThis lab was designed with the most recently available version of the software\n211 212\nChapter 7 Using GIS to Make a Map\nat the time of writing. However, if the software or Websites have significantly\nchanged between then and now, an updated version of this lab (using the\nnewest versions) is available online at http:\/\/www.whfreeman.com\/\nshellito1e.\nLab Data\nThere is no data to copy in this lab. All data comes as part of the AEJEE sample\ndata that gets installed with the software.\nImportant note: In order to keep the data and results similar, both the\nAEJEE and ArcGIS 10 portions of the lab use the same setup sample data that\ncomes with AEJEE. Thus, if you\u2019re using ArcGIS 10 for the lab, please down-\nload and install AEJEE in order to use its sample data.\nLocalizing This Lab\nThe dataset used in this lab is Esri sample data for the entire United States.\nYou\u2019ll be creating a map layout of the United States. However, the layout tools\ncan be used to make a map of whatever dataset you desire\u2014rather than creat-\ning a layout map of the entire United States, focus on your home state and\ncreate a layout of that instead (see Section 7.4 for how to focus the layout on\none state instead of the whole United States). If you\u2019re only going to work with\none state, use the counties.shp file instead and make a map of county popula-\ntion by square mile instead.\n7.1 Getting Data for Mapping\n1. Start ArcMap. After ArcMap opens, do the following (if needed, refer\nback to the 5.2 Geospatial Lab Application: GIS Introduction: ArcGIS\nVersion for specifics on how to do these things):\n2. Add the states shapefile from the USA sample data folder.\n3. Pan and zoom the View so that the lower 48 states are shown filling up\nthe View.\n4. Change the projection in the View from the default GCS to something\nmore visually pleasing.\nImportant note: 5.2 Geospatial Lab Application: GIS Introduction: ArcGIS\nVersion used the Lambert Conformal Conic projection, but there are many\nmore to choose from\u2014keep in mind that your final map will be of the United\nStates when selecting a regional or world projection.\n5. Leave the states symbology alone for now\u2014you\u2019ll change it in the next step.\n7.2 Setting Graduated Symbology\nArcMap gives you the ability to change an object\u2019s symbology from a single\nsymbol to multiple symbols or colors and allows for a variety of different data\nclassification methods. 213\nGIS Layouts: ArcGIS Version\n1. Right-click on the states shapefile in the TOC and select Properties.\nClick on the Symbology tab. To display the states as graduated symbols,\nuse the following settings:\n(Source: Esri\u00ae ArcGIS ArcMap ArcInfo graphical user interface Copyright \u00a9 Esri.)\n2. Under the Show: options, select Quantities, then select Graduated\ncolors.\n3. The Field to use (in this exercise) is the POP05_SQMI (the states\u2019\npopulation per square mile from the year 2005).\n4. For the color ramp, select an appropriate choice from the available\noptions.\n5. Use 5 for the number of Classes.\n6. To change the classification method, press the Classify button. From the\nnew options available in the pull-down menu, choose Quantile for the\nMethod.\n7. When you have things arranged how you want them, click Apply to\nmake the changes. Take a look at the map and make any color changes\nyou think are needed.\n8. Click OK to close the dialog box.\n9. Now, the symbology of the states has changed in the View, and the\nvalues that make up each one of the breaks can be seen in the TOC. 214\nChapter 7 Using GIS to Make a Map\n7.3 Page Setup and Layouts\n1. Before starting the layout process, first set up the properties of the map\npage, as they\u2019ll fit with the printer you\u2019ll be using to print the map (to\navoid any scaling or printing problems at the end).\n2. From the File pull-down menu, select Page and Print Setup.\n(Source: Esri\u00ae ArcGIS ArcMap ArcInfo graphical user interface Copyright \u00a9 Esri.)\n3. Under Name, select the printer you\u2019ll be using.\n4. For Orientation, choose how you want to compose the layout\u2014using\nPortrait (a vertical orientation for the map) or Landscape (a horizontal\norientation).\n5. Place check marks in the following boxes:\na. Use Printer Paper Settings\nb. Show Printer Margins on Layout\nc. Scale Map Elements proportionally to changes in Page Size\n6. Click OK when all settings are correct. 215\nGIS Layouts: ArcGIS Version\n7.4 Layouts in ArcMap\n1. To begin laying out the print-quality version of the map, you\u2019ll need\nto begin working in the Layout View. This mode of ArcMap works\nlike a blank canvas, allowing you to construct a map using various\nelements.\n2. Note that whatever appears in the Data View will also appear in the\nLayout View. For instance, if you zoom the Map View to only show\nOhio, and then switch to Layout View, the layout will only show\nOhio.\n3. To begin, select Layout View from the View pull-down menu.\na. Y ou can switch back to the Data View by selecting Data View from the\nView pull-down menu.\nb. You can also switch between Layout and Data Views by using the\nicons at the bottom left-hand corner of the View.\nData view\nLayout view\n4. In the Layout View, the black border around the white page represents\nthe border of the printed page of an 8\u00bd (cid:2) 11 piece of paper, so be\ncareful when working near the edges of the page and keep all elements\nof the map within that border.\n5. The toolbar will switch to a new set of tools\u2014the navigation tools are as\nfollows:\n6. Starting at the left and moving right are the following:\na. The plus and minus magnifying glasses that are used to zoom in and\nout of the layout\nb. The hand icon that is used to pan around the map\nc. The 1:1 icon that is used to zoom the layout to 100%\nd. The white pages with the four black arrows that are used for\nzooming in and out from the center of the page\ne. The white pages with the blue arrows that are used for returning to\nprevious scales 216\nChapter 7 Using GIS to Make a Map\n7.5 Map Elements\n1. Again, think of the layout as a blank sheet of paper that you\u2019ll use to\nconstruct your map. There are numerous map elements that can be\nadded, including scale bars, north arrows, and a legend. Each element\nhas properties (such as creating borders, filling colors, or fixing size\nand position) that can be accessed individually (using the black \u2018select\nelements\u2019 arrow on the Tools toolbar).\n2. The default element that appears on the layout is the Data Frame,\nwhich contains all of the GIS data. All of the visible layers in the\nTOC under the word \u201cLayers\u201d will appear in this frame. You can\u2019t\nmanipulate parts of individual layers (for instance, you can\u2019t click\nand drag a state somewhere else), but you can treat the entire\nFrame as if it\u2019s a map element. Click and drag the Frame around\nthe map or resize it as you see fit (using the eight blue tabs at its\ncorners and midpoints), or access its properties (like any regular\nelement) to create borders around the map or fix it in a particular\nposition.\n3. Map elements can be resized and moved by selecting them with the\nmouse and resizing like you would the data frame.\n4. Map elements can be deleted by selecting them with the cursor and\npressing the delete key on the keyboard.\n7.6 Layer Properties and Layouts\nEach layer you use in ArcMap (such as the states) has a set of layer proper-\nties. Anything changed in the layer properties will be reflected in changes\nto the layout. For instance, if you change the symbology of a layer in its\nproperties, it will be changed in the layout elements such as the map leg-\nend. However, to change the name of the layer and how it will appear in\nsomething like a map legend, the shapefile must be renamed in its Layer\nProperties.\n1. To do this, return to the Layer Properties and choose General. You can\ntype a new name for the layer (like \u201cPopulation Per Square Mile\u201d). Click\nOK when you\u2019ve typed in a new name.\n7.7 Inserting a Scale Bar\n1. To add a scale bar to the map, choose the Insert pull-down menu and\nselect Scale Bar. 217\nGIS Layouts: ArcGIS Version\n2. A number of options for scale bars will appear in a new Scale Bar\nSelector dialog box:\n(Source: Esri\u00ae ArcGIS ArcMap ArcInfo graphical user interface Copyright \u00a9 Esri.)\n3. Additional scale bars are available by clicking the More Styles button.\nYou can alter the appearance of the scale bar (adjusting the number of\ndivisions on the bar, for the display units shown on the scale bar, etc.) by\nselecting the Properties button.\n4. Select an appropriate scale bar and click OK. Use the cursor to position\nor resize the scale bar on the layout.\n7.8 Inserting a North Arrow\n1. To add a north arrow to the map, choose the Insert pull-down menu and\nselect North Arrow. 218\nChapter 7 Using GIS to Make a Map\n2. A number of options for north arrows will appear in a new North Arrow\nSelector dialog box:\n(Source: Esri\u00ae ArcGIS ArcMap ArcInfo graphical user interface Copyright \u00a9 Esri.)\n3. Additional north arrows are available by clicking the More Styles\nbutton. You can alter the appearance of the north arrow (adjusting the\ncolor, rotation angle, etc.) by selecting the Properties button.\n4. Select an appropriate north arrow and click OK. Use the cursor to\nposition or resize the north arrow on the layout.\n7.9 Inserting a Legend\n1. To add a legend on the map, choose the Insert pull-down menu and\nselect Legend.\n2. The Legend Wizard will open\u2014this is a set of menus to help you properly\nset up the legend for the map.\nImportant note: At any step, you can see what the legend will look like on\nthe map by clicking the Preview button in the wizard. By pressing Preview,\nthe legend will be added to the map, and you can see what it looks like at the\ncurrent stage of legend design. If you\u2019re satisfied, you can click Finish in the\nLegend Wizard to add the legend. If you want to keep going with the steps in\nthe Legend Wizard, press Preview again and the legend will disappear, so you\ncan advance to the next Wizard step.\n3. The first screen will allow you choose the layers that will appear in the\nlegend, along with the number of columns you want the legend to be. 219\nGIS Layouts: ArcGIS Version\nChoose the states layer for this map and select one column. Click Next\nto advance to the second menu.\n4. The second menu will allow you to give the legend a name\u2014give it\nsomething more appropriate besides \u201clegend.\u201d You can also adjust the\ncolor, size, font, and justification of the legend itself. Once you\u2019ve set\nthese values, click Next to advance to the third menu.\n5. The third menu will allow you to adjust the color, thickness, and\nappearance of the legend\u2019s border, background, and drop-shadow\neffects. Examine various options for the legend\u2019s appearance and, when\nready, click Next to advance to the fourth menu.\n6. The fourth menu will allow you to alter the symbol patch of the legend\nitems (in this case, the states layer). When ready, click Next to advance\nto the fifth menu.\n7. The fifth and final menu will allow you to adjust the appearance and\nspacing of items in the legend itself. When you\u2019re satisfied with how the\nlegend will look, click Finish. The legend will then be added to the layout.\n8. Once the legend is created, you can make changes to it by right-clicking\non the legend itself in the layout and selecting Properties. For instance,\nselecting the Items tab under Properties allows you to access a \u201cStyle\u201d\noption that you can use to further modify the appearance of the legend.\n(Source: Esri\u00ae ArcGIS ArcMap ArcInfo graphical user interface Copyright \u00a9 Esri.) 220\nChapter 7 Using GIS to Make a Map\n7.10 Adding Text and a Title to\nthe Layout\n1. To add text to the map (such as your name,\nthe date, or any other text you may want to\nadd), select Text from the Insert pull-down\nmenu.\n2. Note that when text is added, a small box\ncalled \u201cText\u201d is placed near the center of the\nmap. You\u2019ll have to move each text box to\nsomewhere else in the layout.\n3. Right-click the text box and select Properties\nfrom the new menu items. The Properties\ndialog box will appear.\n(Source: Esri\u00ae ArcGIS ArcExplorer graphical user interface\nCopyright \u00a9 Esri.)\n4. Under the Text tab, type the text you want to appear in the box. To alter the\nfont, color, and other properties of the text, click the \u201cChange Symbol . . .\u201d\nbutton. More options will appear\u2014note that you can alter each text box\nseparately, to create larger fonts for titles, smaller fonts for type, and so on.\n(Source: Esri\u00ae ArcGIS ArcMap ArcInfo graphical user interface Copyright \u00a9 Esri.) 221\nGIS Layouts: ArcGIS Version\n7.11 Other Map Elements\n1. Though they\u2019re not used in this lab, other map elements can be added\nfrom the Insert pull-down menu:\na. Dynamic Text allows you to insert information about the current\ndate, current time, the date the map was saved, and so on.\nb. Neatline allows you to draw a line around map elements.\nc. Scale Text allows you to insert verbal statements of scale rather than\nonly using a scale bar for representation.\nd. Picture allows you to add graphics to your map, including\ndownloaded images, digital camera images, or other graphics.\ne. Object allows you to insert things from other software applications,\nincluding video clips, Microsoft PowerPoint slides, or sound clips.\n7.12 Printing the Layout\n1. When you have constructed the map the way you want, choose Print\nPreview from the File pull-down menu. You\u2019ll see how your map will\nappear on the printed page. Pay careful attention to borders, lines, and\nplacement of objects when comparing the screen appearance to what\nthey will look like on paper. Make whatever adjustments are necessary\nto the final draft of the layout before printing. 222\nChapter 7 Using GIS to Make a Map\n2. When the map\u2019s ready, select Print from the File pull-down menu\u2014\nArcMap will use this dialog box to send the layout to the printer.\n3. Exit ArcMap by selecting Exit from the File pull-down menu. There\u2019s no\nneed to save any data in this lab.\nClosing Time\nThe geospatial lab application in Chapter 8 will keep using ArcGIS for\nanalysis\u2014you\u2019ll be using a Web service to take a series of addresses and plot\nthem on a map, then use those results in ArcGIS for some simple analysis.\nFinal Layout Checklist\n____ The contiguous 48 states (without Alaska and Hawaii) United States\npopulation per square mile, classified, and displayed with an appro-\npriate color scheme\n____ States shown in a different (and appropriate) projection than the\ndefault\n____ Appropriate map legend, with all items listed with normal names\n(the legend should not be called \u201clegend\u201d and should be appropriate\nsize, font, number of columns, and so on)\n____ Scale bar\n____ Appropriate title (don\u2019t use the words \u201cmap\u201d or \u201ctitle\u201d in your title)\nand appropriate size and font\n____ A north arrow of appropriate size and position\n____ Type (your name, the date, the source of the data) in an appropriate\nsize and font\n____ Overall map design (appropriate borders, color schemes, balance of\nitems and placement, and so on) 68\nGetting There Quicker with\nGeospatial Technology\nVehicle Navigation Systems, Road Maps in a Digital World,\nCreating a Street Network, Geocoding, Shortest Paths,\nand Street Networks Online\nVehicle navigation systems (like those made by companies such as Garmin, vehicle navigation\nMagellan, or Tom-Tom) are really revolutionary technologies. One small system a device\nused to plot the user\u2019s\ndevice mounted on the dashboard will find your precise location, plot it on a\nposition on a map,\nmap, determine where the nearest gas stations are, then compute the quickest\nusing GPS technology\nroute to get you there, all with turn-by-turn directions that announce names\nto obtain the location.\nof streets and the distance to the turn (Figure 8.1). The position determina-\ntion is straightforward\u2014the device has a GPS receiver in it that finds your\nlocation on Earth\u2019s surface using the methods discussed in Chapter 4. In fact,\nmost of these devices are simply referred to as a \u201cGPS,\u201d as in \u201cpunch our des-\ntination into the GPS\u201d or \u201cwhat does the GPS say the shortest route to get\nthere is?\u201d However, it\u2019s doing a disservice to these things to simply call them\nFIGURE 8.1 A Garmin\nGPS vehicle navigation\nsystem. (Source: Edward\nJ. Bock III\/Dreamstime.com)\n222233 224\nChapter 8 Getting There Quicker with Geospatial Technology\na \u201cGPS\u201d since it\u2019s obvious they do so much more than what a regular GPS\nreceiver does.\nThese devices rely on a GIS-style system at their core\u2014hence their abil-\nity to handle spatial data in the form of road-network maps, use those maps\nfor routing, determine the shortest path between two (or more) points, and\nmatch an address to a spatial location. This same type of data is used to route\nemergency vehicles to the site of a 911 emergency phone call or manage a\nfleet of delivery vehicles. The same sort of system is at the heart of online\nmapping applications such as MapQuest or Google Maps and in the location\nand mapping apps of a smartphone. This type of technology is changing fast\nand improving as time goes on.\nA March 2009 article in USA Today described an incident where a ve-\nhicle navigation device instructed the driver to turn off a road and follow\na snowmobile trail toward a destination, which ended with the car stuck in\nthe snow and the state police being called for emergency help. Similarly, a\nMarch 2009 article of the Daily Mail relates a story of a vehicle navigation\nsystem that d irected a driver along a walking footpath, which ended in a\nsheer cliff drop of 100 feet (luckily, the driver stopped in time). When these\nkinds of errors o ccur, it\u2019s usually not because the GPS receiver is finding the\nincorrect position from the satellite information. Rather, it is more likely\nthere are problems with the base network data itself. These types of systems\nare only as accurate as the base network data they have available to them.\nThis chapter delves into how these types of geospatial technology applica-\ntions function, how they\u2019re used, what makes them tick, and why they may\nsometimes lead you astray.\nThinking Critically with Geospatial Technology 8.1\nWhat Happens When the Maps Are Incorrect?\nHow many times has this happened to you when In addition, many vehicle navigation systems will\nu sing some sort of mapping service\u2014either the di- recompute a new route on the fly if you miss a turn\nrections take you to the wrong place, they indicate or start to take another route. However, if you have\nyou should turn where you can\u2019t, or they can\u2019t find been taken on an improper route, how useful will\nwhere you want to go (or the road you want to go the system be in getting you back to where\non)? The GPS location is probably correct, but the you want to go? A key purpose of mapping systems\nmaps being used for reference are likely either out- is using them to navigate through unfamiliar\ndated or have errors in them. How much does the t erritory\u2014but if they don\u2019t properly fulfill their func-\nusefulness of this aspect of geospatial technology tion, what do travelers rely on? Are there any sorts\nrely on the base data? If the base maps are incom- of liability issues that could result from inaccurate\nplete or not updated, then how useful is a mapping mapping systems?\nsystem to the user? 225\nHow Do You Model a Network for Geospatial Technology?\nHow Do You Model a Network\nfor Geospatial Technology?\nBack in Chapter 5, we discussed how real-world items are modeled or rep-\nresented using GIS. Any type of network is going to involve connections be- network a series of\ntween locations, whether streams, power lines, or roads. Thus (to use Esri junctions and edges\nterminology), in GIS a network in its most basic form is represented by a series connected together for\nmodeling concepts such\nof junctions (point locations or nodes) that are connected to each other by\nas streets.\na series of edges (lines or links). For instance, in a road network, junctions\nmight be the starting and ending points of a road or the road intersections, junction a term used\nfor the nodes (or places\nwhile the edge would be the line representing the road itself. When design-\nwhere edges come\ning a road network, keep in mind that there may be many types of edges and\ntogether) in a network.\njunctions to represent. For example, a city\u2019s road network would have edges\nedge a term used\nthat represent streets, highways, railroads, light-rail systems, subway lines, or\nfor the linkages of a\nwalking paths, while junctions may represent not only the starting and end-\nnetwork.\ning of streets, but also highway entrances and exits, freeway overpasses and\nconnectivity the\nunderpasses, subway stops, or rail terminals.\nlinkages between\nWhen dealing with all these different types of edges and junctions,\nedges and junctions of\nthe connectivity of the network in GIS is essential when modeling it. With a network.\nproper connectivity, all junctions and edges should properly connect to one\nanother, while things that should not connect, do not connect. For example,\nif a freeway crosses over a road via an overpass, the network connectivity\nshould not show that as a valid intersection allowing the street to turn onto\nthe freeway at that junction. If your vehicle navigation system leads you to\nthis point, then instructs you to \u201cturn right onto the highway,\u201d it\u2019s impos-\nsible for you to do so, but the device thinks you should be able to because\nof how the network data is set up. In the same way, a railroad line may in-\ntersect with a street, but the network should not have a connection showing\nthat the street could continue along the rail line. If this kind of connection\nwas built into the data, you could conceivably be routed to turn onto the\nrailroad line and continue on it toward your destination. It sounds silly to\nthink of driving your car on the railroad tracks, but due to incorrect network\ndata, this line would simply represent the next road to take to get to your\ndestination.\nThinking along these lines, other features of a road network must also\nbe included in the model. For instance, some streets may be one way, or\nsome junctions may not allow left-hand turns, or U-turns may not be per-\nmitted. These types of features need to be properly modeled for the system\nto be an accurate, realistic model of the road network. Although you may\nbe able to see the \u201cone way\u201d street sign when you\u2019re driving, if that feature\nhas not been properly set up in the network, the system would have no way\nof knowing not to try to route cars in both directions along the street. As\ndiscussed previously, an overpass or underpass should not show up as being\nconnected to the road network (to be a viable option for a turn\u2014if a device 226\nChapter 8 Getting There Quicker with Geospatial Technology\nor GIS instructs you to make a right-hand turn onto the freeway that you\u2019re\ncurrently driving under, then something\u2019s gone wrong with some aspect of\nthe technology).\nWhen a network is being modeled, each edge is considered to be a sep-\narate entity, not necessarily each individual street. A long city road may be\nline segment a single modeled in the GIS as several line segments, with each segment represent-\nedge of a network ing a section of the road. A major urban street may be made up of more than\nthat corresponds to\n100 line segments\u2014each segment being a different section of the road (with\none portion of a street\neach segment being delineated by roads that intersect it). For example, Fig-\n(for instance, the\nedge between two ure 8.2 shows a geospatial road network of Virginia Beach, Virginia. The road\njunctions). cutting through the center of the city (highlighted in blue) is Virginia Beach\nBoulevard, a major multi-lane city street, with numerous intersecting roads\nand street lights. Although we think of Virginia Beach Boulevard as one big,\nlong street, the system models it as 129 line segments, with each line segment\nrepresenting a portion of the road.\nBreaking a road up into individual line segments allows the modeling\nof different attributes for each segment. Attributes such as the name of the\nroad, the address ranges for each side of the street, the suffix of the road name\n(whether it\u2019s a Drive, Avenue, Boulevard, etc.), the type of road (such as resi-\ndential street, interstate, highway, etc.), and the speed limit are all examples\nof the types of values that can be assigned to individual segments. Thus, an\nattribute table of this layer in a GIS would consist of 129 records, each with\nstreet centerline a multiple attributes.\nfile containing line Several different types of geospatial road network files are available. A\nsegments representing\nstreet centerline file is a file modeling each city road as a line, containing the\nroads.\ndifferent types of roads. The U.S. Census Bureau also regularly issues this type\nFIGURE 8.2 Virginia\nBeach Boulevard shown\nhighlighted in a GIS\nnetwork. Although it is\nonly one road, it consists\nof 129 line segments.\n(Source: Esri\u00ae ArcGIS ArcMap\nArcInfo graphical user\ninterface. Copyright \u00a9 Esri.) 227\nHow Do You Model a Network for Geospatial Technology?\nHands-on Application 8.1\nTIGER Files Online\nTIGER files are made freely available from the to see what types of datasets are available for\nCensus Bureau via the Web. Open your Web brows- download.\ner and go to http:\/\/www.census.gov\/geo\/www\/ Esri also makes TIGER 2000 data freely\ntiger\u2014this is the part of the U.S. Census Bureau\u2019s available\u2014this data has already been separated into\nWebsite for downloading TIGER\/Line files. There\u2019s its various components (such as roads, railroads,\nalso full documentation of TIGER files available in etc.) for download so that each layer can be down-\nPDF format on the Website. Files for U.S. counties loaded as its own shapefile. Open your Web browser\ncan be downloaded in shapefile format (see Chapter and go to http:\/\/arcdata.esri.com\/data\/tiger2000\/\n5 for more info about shapefiles) to be used in GIS tiger_download.cfm. All data is downloaded in a\nproducts such as ArcGIS or ArcExplorer Java Edition zipped shapefile format (same as the U.S. Census\nfor Educators. You can also download several other Bureau\u2019s Website) for use in ArcGIS or AEJEE. Select\ntypes of TIGER files besides road-network data, in- your county to see what TIGER data is available for\ncluding census block information, h ydrography, land- you to use. This chapter\u2019s lab uses a TIGER dataset\nmarks, and American Indian reference data\u2014check downloaded from the Esri Website.\nof road network data in a format usable by geospatial technology software.\nThe data is provided in the TIGER\/Line files. TIGER stands for Topologically TIGER\/Line a file\nIntegrated Geographic Encoding Referencing and the files delineate different produced by the U.S.\nboundaries throughout the United States (such as congressional districts), in Census Bureau that\ncontains (among other\naddition to containing road-network data. (See Hands-on Application 8.1: TI-\nitems) the line segments\nGER Files Online for more information.)\nthat correspond with\nEach record in a TIGER\/Line file represents a segment of a road network, roads across the United\nand thus each segment (record) can have multiple attributes (fields) assigned States.\nto it. Figure 8.3 on page 228 again shows a TIGER\/Line file of Virginia Beach\nand a portion of the attribute table of those selected segments that make up\nVirginia Beach Boulevard. Note how many attributes there are (information\nthat gets encoded into each road segment, and the entire Virginia Beach TI-\nGER\/Line file is made up of over 19,000 segments). The TIGER\/Line file con-\ntains the following standard attributes (among others):\na FEDIRP: This is the prefix direction of the road (N. Smith St. or W.\nBroad Ave.).\na FENAME: This is the name of the road (N. Smith St. or W. Broad Ave.).\na FETYPE: This is the type of road (N. Smith St. or W. Broad Ave.).\na FEDIRS: This is the suffix direction of the road (Cherry Lane S. or Canal\nStreet E.).\na CFCC: This is the Census Feature Class Code, a standardized encoding\nused to separate different kinds of roads (such as residential street, high-\nways, or interstates). 228\nChapter 8 Getting There Quicker with Geospatial Technology\nFIGURE 8.3 The\nattributes of the selected\nrecords making up Virginia\nBeach Boulevard from\nthe Virginia Beach TIGER\/\nLine file in a GIS. (Source:\ndata: US Census Bureau.\nEsri\u00ae ArcGIS ArcMap ArcInfo\ngraphical user interface.\nCopyright \u00a9 Esri.)\na FRADDL: This is the start of the address range on the left side of the street\n(for instance, if the street addresses go from 101 to 199, this value would\nbe 101).\na TOADDL: This is the end of the address range on the left side of the street\n(for instance, if the street addresses go from 101 to 199, this value would\nbe 199).\na FRADDR: This is the start of the address range on the right side of the\nstreet (for instance, if the street addresses go from 100 to 198, this value\nwould be 100).\na TOADDR: This is the end of the address range on the left side of the street\n(for instance, if the street addresses go from 100 to 198, this value would\nbe 198).\na ZIPL: This is the zip code used for addresses on the left side of the\nstreet.\na ZIPR: This is the zip code used for addresses on the right side of the\nstreet.\nThese attributes define the characteristics of each road segment, and\nsimilar attributes would be found in road-network data, such as other street\ncenterline files. If these attributes are incorrect, then the base network map\nwill be incorrect. If the vehicle navigation system gives you incorrect street\nnames or calls a road \u201ceast\u201d when it\u2019s really \u201cwest,\u201d it\u2019s likely that there are\nincorrect values in the base network data\u2019s attributes.\nThe TIGER file attributes (or similar base road-network data created by\nothers) concerning specific address ranges, zip-code information, and de-\ntailed data for the names of roads can be used as a base map source for other 229\nHow Is Address Matching Performed?\napplications, such as pinpointing specific addresses on a road. It\u2019s this source\ndata that allows for a match of a typed street address to a map of the actual\nlocation.\naddress matching\nanother term for\ngeocoding.\nHow Is Address Matching Performed?\ngeocoding the process\nof using the text of an\nWhenever you use a program like MapQuest or Google Maps to find a map\naddress to plot a point at\nof a location, you\u2019re typing in something (like \u201c1600 Pennsylvania Avenue,\nthat location on a map.\nWashington, D.C.\u201d) and somehow the Website translates this string of char-\nreference database\nacters into a map of a spatial location (like the White House). The process of\nthe base network data\ntaking a bunch of numbers and letters and finding the corresponding loca-\nused as a source for\ntion that matches up with them is called address matching or geocoding geocoding.\n(Figure 8.4). Although the process seems instantaneous, there are several\nsteps involved in geocoding that are happening \u201cbehind the scenes\u201d when\nyou use an address-matching system (like those in GIS).\nFirst, you need to have some sort of reference database in place\u2014this\nis a road network that the addresses will be matched to. A TIGER\/Line file, FIGURE 8.4 A map\nanother type of street centerline file, or some other road-network data (like generated by querying\nMapQuest for \u201c1600\nthose created by commercial companies) is needed here. What\u2019s e ssential is\nPennsylvania Avenue,\nthat the line segments contain attributes such as those found in a TIGER\/Line\nWashington, D.C.\u201d (Source:\nfile\u2014for example, street direction, name of the street, address ranges on the Map(s) and data \u00a9 2011\nleft and right sides of the street, street suffix, and zip codes on the left and by MapQuest, Inc., Navteq,\nand Intermap as applicable.\nright sides of the street. This information will be used as the source to match\nMapQuest and the MapQuest\naddresses to as well as a source for the final plotted map.\nlogo are trademarks of\nMapQuest, Inc. Used with\npermission.) 230\nChapter 8 Getting There Quicker with Geospatial Technology\nTABLE 8.1 Addresses that have been parsed into their component parts and standardized.\nLocation Address Prefix Number Street name Street type Suffix\nWhite House 1600 Pennsylvania 1600 Pennsylvania AVE NW\nAvenue NW\nNational Gallery of Art 401 Constitution 401 Constitution AVE NW\nAvenue NW\nU.S. Capitol 1 1st Street NE 1 1st ST NE\nNational Archives and 800 North Capitol N 800 Capitol ST NW\nRecords Street NW\nWashington National 3101 Wisconsin 3101 Wisconsin AVE NW\nCathedral Avenue NW\nUnited States Holocaust 100 Raoul Wallen- 100 Raoul PL SW\nMemorial Museum berg Place SW Wallenberg\nNext, the address information is parsed into its component pieces, and\nparsing breaking\naddress standardization is performed to set up data in a consistent format.\nan address up into its\ncomponent parts. The geocoding process needs to standardize addresses to properly match a\nlocation using its appropriate attributes in the reference database. Table 8.1\naddress\nshows a number of locations in the Washington, D.C., area with their addres-\nstandardization\nsetting up the ses, as well as these addresses parsed and standardized. For instance, in the\ncomponents of an National Gallery of Art\u2019s address, the street name is \u201cConstitution.\u201d When the\naddress in a regular address matches, the system refers to line segments with a name attribute\nformat.\nof \u201cConstitution\u201d and those segments with a street-type attribute of \u201cAVE\u201d\n(rather than ST, BLVD, LN, or others) and a suffix direction attribute of NW\n(instead of some other direction). The street number, 401, is used to deter-\nmine which road segments match an address range (on the left or right side\nof the street, depending on whether the number is odd or even).\nAfter the address has been parsed and standardized, the matching takes\nplace. The geocoding system will find the line segments in the reference da-\ntabase that are the best match to the component pieces of the address and (in\nArcGIS) rank them. For instance, in trying to address match the National Gal-\nlery of Art, a line segment with attributes of Name (cid:2) \u201cConstitution,\u201d Type (cid:2)\n\u201cAVE,\u201d Suffix Direction (cid:2) \u201cNW,\u201d and an address range on the left side of \u201c401-\n451\u201d would likely be the best (or top-ranked) match. A point corresponding\nwith this line segment is placed at the approximate location along the line\nsegment to match the street number. For instance, our address of 401 would\nhave a point placed near the start of the segment, while an address of 425\nwould get placed close to the middle. The method used to plot a point at its\nlinear interpolation a\nmethod used in approximate distance along the segment is called linear interpolation.\ngeocoding to place an Keep in mind that the plotting is an approximation of where a specific\naddress location among point should be. For instance, if a road segment for \u201cSmith Street\u201d has an ad-\na range of addresses.\ndress range of 302 through 318, an address of 308 would be placed near the 231\nHow Is Address Matching Performed?\nFIGURE 8.5 Plotting an\nLine Segment:\naddress point on a line\nWick Avenue\nsegment in GIS. (Source:\nEsri\u00ae ArcGIS ArcMap ArcView\nAddress Range of Segment graphical user interface.\non Right: 301\u2013399 Copyright \u00a9 Esri.)\nAddress Range of Segment\non Left: 300\u2013398\nAddress:\n305 Wick Avenue\nmiddle. However, if the actual real-world location of house number 308 was\ncloser to the end of the street, then the placement of the plotted point would\nnot necessarily match up with the actual location of the house. In cases of\nstreets that contain only a handful of houses that correspond with the ad-\ndress range in the reference file, plotted locations may be estimated incor-\nrectly. See Figure 8.5 for an example of plotting a geocoded point on a road\nnetwork in GIS.\nIn GIS, or in a vehicle-navigation system or smartphone equipped with\ngeospatial technology, whenever you specify an address, the system will\nmatch the address and fix it as a destination point to be found. The process\nof geocoding multiple addresses at once is referred to as batch geocoding. batch geocoding\nFor instance, in batch geocoding, you could have a list of the addresses of all matching a group of\naddresses together\ncoffee shops in Seattle, and the GIS would match all addresses on the list,\nat once.\nrather than you having to input one at a time (for an example, see Hands-on\nApplication 8.2: Geocoding Using Online Resources on page 232). If no match\ncan be found, or if the ranking is so poor as to be under a certain threshold for\na match, sometimes a point will not be matched for that address (or it may be\nmatched incorrectly or placed at something like the center of a zip code). You\nwill sometimes be prompted to recheck the address or to try to interactively\nmatch the address by manually stepping through the process.\nFinally, when an address is plotted, the system may have the capabil-\nity to calculate the x and y coordinates of that point and return those val-\nues to the user. For example, the street address of the Empire State Build-\ning in New York City is \u201c350 5th Ave., New York, NY 10018.\u201d From address\nmatching using the free online gpsvisualizer.com utility (also used in this\nchapter\u2019s lab), the GIS coordinates that fit that address are computed to 232\nChapter 8 Getting There Quicker with Geospatial Technology\nHands-on Application 8.2\nGeocoding Using Online Resources\nMany online mapping resources (see Hands-on Ap- a batch of three or more addresses (such as home\nplication 8.4: Online Mapping and Routing Applica- addresses for a group of family members, a group\ntions and Shortest Paths for details on what they of friends, or several workplaces) and run the batch\nare and how they\u2019re used) will geocode a single geocode utility. The Website will give you examples\naddress (or a pair of addresses used for calculat- of how the addresses need to be formatted. From\ning directions between the two). You can use GIS there, you can view your results on the Google map\nsoftware to geocode multiple addresses, or you can that the Website generates.\nalso use a resource like the BatchGeo Website. Open This chapter\u2019s lab will utilize other functions of\nyour Web browser and go to http:\/\/www.batchgeo. the BatchGeo Website (and other resources)\u2014you\u2019ll\ncom\u2014this is a free online service that allows you first geocode a set of points, and then use GIS to\nto geocode an address (or multiples in batches), map them and conduct some analysis.\nand then view the plotted points on a map. Set up\nbe: latitude 40.74807 and longitude (cid:2)73.984959 (Figure 8.6). Thus, geo-\ncoded points have spatial reference attached to them for further use as a\ngeospatial dataset.\nGeocoding is a simple yet powerful process, but it\u2019s not infallible. There\nare several potential sources of error that can give an incorrect result and\nplot an address in the incorrect location. Since the addresses you\u2019re enter-\ning will be parsed to match up with the segments in the reference database,\nthe result can sometimes be an error in matching. For instance, the address\nof the White House in Table 8.1 is listed as \u201c1600 Pennsylvania Avenue NW.\u201d\nThe line segments that match up with this street should have the name listed\nwith \u201cAvenue\u201d (or \u201cAve.\u201d) as a suffix. If you input different things like \u201c1600\nPennsylvania Street\u201d or \u201c1600 Penn Avenue,\u201d the location could potentially\nFIGURE 8.6 The\ngeocoded result for the\nEmpire State Building in\nNew York City, with its\ncorresponding latitude\nand longitude calculation.\n(Source: GPSVisualizer.com\/\nMap data \u00a9 2011 Google,\nSanborn.) 233\nHow Are Shortest Paths Found?\nreceive a lower ranking or end up plotted elsewhere or otherwise not prop-\nerly matched. With more complete information (such as \u201c1600 Pennsylvania\nAvenue NW, Washington, D.C. 20003\u201d) the system should be better able to\nidentify a more accurate match.\nAlso, the geocoding system can only properly identify locations if the line\nsegments are in the reference database. If you\u2019re searching for an address\nestablished after the reference database network file was put together, the\nsystem will not be able to properly match the address. If the point is plot-\nted on the correct street but at an incorrect location on the street, it\u2019s likely a\nproblem with address range data in the reference database and how it reflects\nthe real world (for instance, a house at #50 Smith Street is not necessarily\nhalfway down a street segment that begins with 2 and ends with 100). The\ngeocoding will usually be only as accurate as the base data it is being matched\nto. If the reference database does not contain all line segments (for instance,\nsubdivisions, streets, or new freeway bypasses that haven\u2019t been mapped and\nadded to the reference database), or if its attributes contain inaccurate ad-\ndress ranges or incorrect or missing attributes, the geocoding process will\nlikely be unable to accurately match the addresses or may plot them in the\nwrong location.\nNew methods for geocoding addresses to get a more accurate match are\nbeing developed. Rather than using a line segment and interpolating the ad-\ndress location, point databases are being created in which a point represents\nthe center of a parcel of land (for instance, a house or a commercial property).\nWhen geocoding with this point data, address information can get matched\nto the point representing the parcel, and the address location can be found for\nthe road immediately next to the parcel.\nOnce locations are geocoded, the system (or GIS) can begin to examine\nthe routes between locations to determine the shortest path from one loca-\ntion to another. With a vehicle navigation system, you enter the address of\nthe destination you want to travel to, and the system will match that address.\nThe device\u2019s current position is determined using GPS and plotted on the\nnetwork map (and this will be the origin). With two points, the system will\nthen compute the shortest route between the origin and destination across\nthe network. The same holds true for an online system to find directions\u2014it\nhas a matched origin and destination and will compute what it considers the\nbest route for you to follow between the two points. With so many different\nways to get from the origin to the destination, the system now needs a way to\ndetermine the \u201cshortest path\u201d between these locations.\nHow Are Shortest Paths Found?\nWhen you leave your home to go to work, you likely have several different\nways you can go. Some of them are very direct and some of them are very\nroundabout, but you have plenty of options available. If you want to take the\n\u201cshortest\u201d path from home to work, you\u2019d likely focus on some of the more 234\nChapter 8 Getting There Quicker with Geospatial Technology\ndirect routes and eliminate some of the longer or more circuitous routes.\nshortest path the\nHowever, a shortest path can mean different things. For instance, driving\nroute that corresponds\nto the lowest through city streets may be the shortest physical driving distance (in terms\ncumulative transit cost of mileage), but if those streets have lots of stop lights and traffic congestion,\nbetween stops in a then this \u201cshortest path\u201d will likely take longer in terms of the time spent driv-\nnetwork.\ning, rather than traveling a longer distance on a highway that does not have\nthese impediments. In this case, if you wanted to minimize the time spent driv-\ning, the highway route would likely get you to work faster, but you\u2019d actually\nbe driving a longer distance. Major city streets may take a long time to traverse\nat rush hour, but you might sail through quickly if you\u2019re driving on them late\nat night.\nAll of these are things to consider when figuring out the shortest path (or\n\u201cbest route\u201d) to take when traveling from an origin to a destination. People\nuse their own decision-making criteria when determining the shortest path\nthey\u2019re going to take\u2014things like \u201calways use main roads\u201d or \u201ctry to use high-\nways whenever possible\u201d or \u201cdon\u2019t make a left turn.\u201d For this reason, vehicle\nnavigation systems often offer multiple options, such as \u201cshortest distance\u201d\nor \u201cshortest driving time\u201d or \u201cavoid highways\u201d to compute the best route be-\ntween points.\nWithin a vehicle-navigation system (or GIS), each line segment has a\ntransit cost a value transit cost (or impedance) assigned to it. The transit cost reflects how many\nthat represents how units (of things like distance or travel time) it takes to traverse that edge. For\nmany units (of time example, the transit cost may reflect the actual distance in miles from one\nor distance) are used\njunction to another along the edge. The transit cost could also be the equiva-\nin moving across a\nlent driving time it takes to traverse that particular segment. Whatever tran-\nnetwork edge.\nsit cost is used, that value will be utilized in the shortest path computation.\nOther impedance attributes can be modeled as well\u2014segments could have a\ndifferent transit cost under certain conditions (such as heavy traffic or con-\nstruction). These types of impedance factors can help in making a network\nmore realistic for use.\nalgorithm a set of The shortest path is then calculated using an algorithm, or a set of steps\nsteps used in a process used in determining the overall lowest transit cost to move along the network\n(for example, the steps from a starting point to a destination. There are various types of shortest path\nused in computing a\nalgorithms, including Dijkstra\u2019s Algorithm (see Hands-on A pplication 8.3:\nshortest path).\nSolve Your Network Problems with Dijkstra for more information on how to use\nDijkstra\u2019s Algorithm\nthis algorithm), which will compute the path of lowest cost to travel from a\nan algorithm used in\nstarting point to each destination in the network. For instance, say you have\ncalculating the shortest\nthree different destinations you plan to travel to (work, the pizza shop, and\npath between an\norigin node and other the grocery store). Dijkstra\u2019s Algorithm will evaluate the overall cost from\ndestination nodes in a your home to each destination and find the shortest path from your home to\nnetwork. work, from your home to the pizza shop, and from your home to the grocery\nstore.\nWhatever type of algorithm is used, the system will compute the shortest\npath, given the constraints of the network (such as transit cost or directional-\nity of things like one-way streets). The system can then generate directions\nfor you by translating the selected path into the various turns and names of 235\nHow Are Shortest Paths Found?\nHands-on Application 8.3\nSolve Your Network Problems with Dijkstra\nThe inner workings of Dijkstra\u2019s Algorithm are be- JamesStewart\/270\/9798s\/Laffra\/DijkstraApplet.\nyond the scope of this book, but there\u2019s an excellent html. On this Website, you can set up a series of\nfree online resource that allows you to construct nodes (junctions) and links (edges), assign weights\na sample network, then run the Dijkstra Algorithm (transit costs) and directions to them, and run\nto find the shortest path. The algorithm will walk the algorithm to find the shortest path between the\nthrough the shortest path step-by-step and de- origin and all destinations on the network. Use the\nscribe the actions it\u2019s taking (and how the short- interactive interface to construct a sample network\nest paths are created). Open your Web browser (or use the pre-made example) and use Dijkstra to\nand go to http:\/\/www.dgp.toronto.edu\/people\/ set up the shortest paths for you.\nFIGURE 8.7 The\n\u201cshortest path\u201d (the blue\nline) computed from\nMapQuest from the White\nHouse to Georgetown\nUniversity in Washington,\nD.C. (Source: Map(s) and\ndata \u00a9 2011 by MapQuest,\nInc., Navteq, and Intermap\nas applicable. MapQuest\nand the MapQuest logo are\ntrademarks of MapQuest, Inc.\nUsed with permission.)\nstreets that will be traversed on that path. See Figure 8.7 for an example of\nusing an online geospatial technology utility to compute the shortest path or\nbest route between two locations. Also check out Hands-on Application 8.4:\nOnline Mapping and Routing Applications and Shortest Paths on page 236 for\nsome other Web tools for online directions and routing.\nOf course, sometimes you have more than one destination to visit\u2014say\nyou have three places you want to stop at (shoe store, music store, and book\nstore) and you want to drive the overall shortest route to hit all three places.\nWhen using geospatial technology to compute a shortest route between sev-\neral of these stops, there are two types of scenarios to choose from: stops destinations to\nvisit on a network.\n1. Finding the shortest path when visiting stops in a pre-defined order: This\nmeans you have to stop at the shoe store first, the music store second,\nand the book store last. In this case, you\u2019d want to find the shortest path 236\nChapter 8 Getting There Quicker with Geospatial Technology\nHands-on Application 8.4\nOnline Mapping and Routing Applications and Shortest Paths\nThere are many online applications available for of them may be similar, some may be different, and\ncreating a map of an address and then generating some will likely give you more than one option for\na shortest path and directions from that address to the \u201dshortest path\u201d). In addition, the services will\nanother one. Examine the functionality of some of also allow you to alter the route interactively by\nthe following online mapping sites: moving and repositioning junctions of the path, so\nyou can tailor the route more to your liking. Set up\n1. Google Maps: http:\/\/maps.google.com\na shortest route, then position your cursor at junc-\n2. MapQuest: http:\/\/www.mapquest.com tions along the path\u2014you can move the nodes and\n3. Yahoo! Maps: http:\/\/maps.yahoo.com reset the routes. How do they change in terms of\ntime and distance by remaking them?\n4. Bing Maps: http:\/\/www.bing.com\/maps\nAlso, for each service, note the source of the\nTry inputting the same pair of locations (origin maps being generated (usually in small text at the\nand destination) into each Web service and compare bottom of the map). What is the copyrighted source\nthe shortest paths and routes they generate (some of the maps?\nfrom your house to the shoe store, then the shortest path from the shoe\nstore to the music store, and finally the shortest path from the music\nstore to the book store.\n2. Finding the shortest path when you can arrange the order of visiting the\nstops: For instance, if the book store and the shoe store are near each\nother, it makes sense to rearrange your travels, so you visit them one\nafter the other, and then drive to the music store.\nGeospatial technology applications can help determine some solutions\nto both of these scenarios. To visit stops in a pre-determined order, the sys-\ntem will find the shortest route (of all possible routes) from the origin to\nthe first stop, then choose the shortest route to travel from the first stop to\nthe second stop, and so on. An example of this in vehicle-navigation sys-\ntems is being able to set up a \u201cVia Point\u201d (another stop) between your ori-\ngin and final destination\u2014the device will then calculate the route from the\norigin (or current location) to the Via Point, then from the Via Point to the\nfinal destination.\nWith the ability to rearrange the order of stops, a system will evaluate\noptions by changing the order of visiting stops to produce the overall short-\nest route. See Figure 8.8 for an example of how the \u201cshortest path\u201d changes\nbetween visiting stops in order as opposed to being able to rearrange the order\nof visiting the stops. Additional constraints can be placed on the problem\u2014for\ninstance, you may have to return home (the starting point) after visiting all\nof the stops. When rearranging stops, the starting point then becomes the last\nstop and may affect how the reordering is done. Conversely, you may want 237\nHow Are Networks Used in Geospatial Technology?\nto have your starting point and ending point different from one another, and FIGURE 8.8 Two\nthese types of parameters would have to be placed on the problem. Keep in different shortest paths\ninvolving multiple stops\nmind, with rearranging the order of stops, the program will likely give a de-\n(selected public libraries\ncent solution to the problem; however, the only way to truly identify which\nin Youngstown, Ohio):\nconfiguration is the best would be for the program to work through every pos- First, stops are visiting\nsible arrangement\u2014something that would be impossible with a larger num- in numerical order, and in\nthe second image, stops\nber of stops.\nare rearranged to find the\nshortest route (without a\nfixed starting or ending\nHow Are Networks Used in Geospatial\npoint). (Source: Esri\u00ae ArcGIS\nArcMap ArcView graphical user\nTechnology?\ninterface. Copyright \u00a9 Esri.)\nAll of the things discussed in this chapter are used for a variety of applications\nwith geospatial technology. Through GIS, these types of network data can be\ncreated and utilized in various ways. The vehicle navigation systems integrate\nmany of these concepts together\u2014using GPS to pinpoint the device\u2019s location\non a map, then using network base data, geocoding, and shortest paths to\nnavigate through locales worldwide. Other options on these systems involve\nutilizing real-time data broadcast into the device to determine areas of con-\ngestion, high traffic, construction, or road accidents. This data can then be\nused in the shortest-path process to route you around these types of barriers\nor things that would slow down your travel. These same maps and technol-\nogy are being integrated into smartphones to put GPS locations, mapping,\ngeocoding, shortest-paths, and real-time routing information capabilities into\nthe palm of your hand (Figure 8.9). As noted, however, often these devices\nare only as accurate as the base maps they\u2019re utilizing, so companies provide\na means for consumers to obtain regular map updates or give users the option\nto make their own updates.\nThese same types of network base maps are used online for services FIGURE 8.9 A\nsmartphone running a\nlike MapQuest, Bing Maps, Yahoo! Maps, or Google Maps (see Hands-on Ap-\nGPS mapping application.\nplication 8.4: Online Mapping and Routing Applications and Shortest Paths),\n(Source: Pharos Science &\nwhich allow you to perform geocoding and obtain directions for a shortest Applications, Inc.) 238\nChapter 8 Getting There Quicker with Geospatial Technology\nFIGURE 8.10 The U.S.\nCapitol Building as seen\nfrom Google Maps Street\nView. (Source: \u00a9 2011\nGoogle)\npath between points. Street View allows a Google Maps (or Google Earth)\nStreet View a\nuser to examine 360 degrees of photography of a location along a street,\ncomponent of Google\nMaps and Google Earth as if your car was stopped at that location. With this function, you can ex-\nthat allows the viewer amine canned photography of an address or destination when planning a\nto see 360-degree stop (Figure 8.10). Cars equipped with special cameras that can capture\nimagery around an area\na 360-degree view travel along roads, taking imagery along the way (see\non a road.\nFigure 8.11 and Hands-on Application 8.5: Examining Google Street View).\nGoogle\u2019s even extending this Street View onto places inaccessible by cars\u2014\nby attaching the same type of camera equipment to bicycles and traversing\nhiking and biking trails.\nWith this level of data availability, and geospatial networks being inte-\ngrated into so many different applications, mapping (and providing accurate\nand up-to-date base maps) has become big business. Companies such as Tele\nAtlas and NAVTEQ produce these maps, and their products are often what\nyou\u2019re accessing on an online service or from a vehicle-navigation system.\nNext time you access one of these kinds of services, look for the copyright\nFIGURE 8.11 An\nexample of a Google\nStreet View car. (Source:\nBob Bobster\/Wikimedia) 239\nHow Are Networks Used in Geospatial Technology?\nHands-on Application 8.5\nExamining Google Street View\nGoogle Street View is a very cool application that on one of the available streets and the view will\ngives you a look at what the street network would shift to photography of that area (see Figure 8.10\nlook like if you were driving or riding past. Open for an example). Use the mouse to move the view\nyour Web browser and go to Google Maps at http:\/\/ about for 360-degree imagery of the area. You\u2019ll\nmaps.google.com, and then enter a particular ad- also see some white lines and arrows superimposed\ndress. For example, type in the address of the Rock on the road\u2014clicking on them will move you along\nand Roll Hall of Fame in Cleveland, Ohio (1100 Rock the street and you\u2019ll see the imagery change. Move\nand Roll Blvd., Cleveland, OH) and zoom in to the around until you can get a good street-level view of\nmap until you can see the distinctive building of the Rock and Roll Hall of Fame.\nthe Rock Hall and its surrounding streets. Above the Examine some local areas near you\u2014see if\nzoom slider on the left side of the map is an orange Google Street View is available for roads near\nicon that looks like a person (called the \u201cpegman\u201d). where you live, work, or attend school. If the im-\nGrab that with the mouse and place it on top of agery is available, use Google Street View to take\nthe streets in front of the Rock Hall (E. 9th Street). a \u201cvirtual tour\u201d of some of the main streets of your\nStreets that have been covered by Google Street town, or to examine street-level photographs of\nView will be highlighted in dark blue. Place the icon nearby places.\ndata somewhere in the map to see which company is producing the map data\nyou\u2019re using. NAVTEQ, for instance, sends teams out in high-tech cars to col-\nlect data on the location and attributes of new roads, housing developments,\nand other items for updates of maps (as well as photography equipment to\nproduce items like the Street View scenes).\nThinking Critically with Geospatial Technology 8.2\nWhat Kind of Issues Come with Google Street View?\nOpen your Web browser and go to Google Maps at View pose a security risk, not just for government\nhttp:\/\/maps.google.com, and then enter the ad- areas, but private security of homes or businesses?\ndress of the White House (1600 Pennsylvania Av- An article in the May 31, 2008, issue of the Star\nenue, Washington, D.C., 20006). When you try to Tribune (available online at http:\/\/www.startri-\nmove the Street View symbol to view the roads, bune.com\/lifestyle\/19416279.html) describes how\nyou\u2019ll see that the streets immediately surrounding the community of North Oaks, Minnesota, demand-\nthe White House are unavailable to view in Street ed that Google remove the Street View images\nView, presumably for security purposes. However, o btained of the privately owned roads in the area,\nmost of the surrounding streets can be viewed us- citing laws against trespassing. Is Street View en-\ning Street View, allowing you to look at the exte- croaching too far onto people\u2019s private lives or not\nriors of shops, residences, and other government (since you could just view or record the same things\nbuildings. Can the images being collected by Street by just driving down a street)? 240\nChapter 8 Getting There Quicker with Geospatial Technology\nChapter Wrapup\nNetworks, geocoding, and routing are all powerful tools for use in geospatial\ntechnology. With GIS, these concepts are used in a variety of applications and\nbusinesses today. Today 911 operators can geocode the address of a call and\nemergency services can determine the shortest route to a destination. Deliv-\nery services can use geocoding and routing applications to quickly determine\nlocations and reduce travel time to shortest paths. Also, the use and develop-\nment of these types of techniques in vehicle navigation systems represents a\nrapidly changing technology that keeps getting better.\nGeospatial Lab Applications 8.1 and 8.2 will use online geocoding appli-\ncations, as well as Google Earth and GIS software for investigating shortest\npaths and TIGER\/Line mapping uses. In the next chapter, we\u2019re going to start\nlooking at a whole different aspect of geospatial technology\u2014remote sens-\ning. All of these overhead images that you can see on applications like Google\nMaps or MapQuest have to come from somewhere to get incorporated into the\nprogram, and we\u2019ll start looking at the methods behind remote sensing in the\nnext chapter.\nImportant note: The references for this chapter are part of the online com-\npanion for this book and can be found at http:\/\/www.whfreeman.com\/\nshellito1e.\nKey Terms\nvehicle navigation system (p. 223) parsing (p. 230)\nnetwork (p. 225) address standardization (p. 230)\njunction (p. 225) linear interpolation (p. 230)\nedge (p. 225) batch geocoding (p. 231)\nconnectivity (p. 225) shortest path (p. 234)\nline segment (p. 226) transit cost (p. 234)\nstreet centerline (p. 226) algorithm (p. 234)\nTIGER\/Line (p. 227) Dijkstra\u2019s Algorithm (p. 234)\naddress matching (p. 229) stops (p. 235)\ngeocoding (p. 229) Street View (p. 238)\nreference database (p. 229) 8.1\nGeospatial Lab Application\nGeocoding and Shortest Path Analysis:\nAEJEE Version\nThis chapter\u2019s lab will introduce you to the concepts of calculating a shortest\npath between stops along a network as well as generating directions for the\npath using Google Earth and Google Maps. You\u2019ll also be performing geocod-\ning using a Web service, examining the geocoding results, then performing\nsome basic spatial analysis of the results and a TIGER file using Google Earth\nand GIS.\nSimilar to some of the geospatial lab applications in previous chapters,\ntwo versions of this lab are provided. The first version (Geospatial Lab Appli-\ncation 8.1: Geocoding and Shortest Path Analysis: AEJEE Version) uses the free\nArcExplorer Java Edition for Educators (AEJEE). The second version (Geospa-\ntial Lab Application 8.2: Geocoding and Shortest Path Analysis: ArcGIS Version)\nprovides the same activities for use with ArcGIS 10.\nObjectives\nThe goals for you to take away from this lab are:\na Familiarizing yourself with the shortest path and directions functions of\nGoogle Earth.\na Using Google Maps to alter the shortest path to account for route changes.\na Utilizing an online geocoding service to geocode a series of addresses,\nthen examining the results.\na Using the AEJEE software to plot the results of a geocoding operation.\na Examining some basic spatial analysis of locations and their relation to a\nroad network (TIGER file) in AEJEE.\nObtaining Software\na The current version of Google Earth (6.0) is available for free download\nat http:\/\/earth.google.com.\na The current version of AEJEE (2.3.2) is available for free download at\nhttp:\/\/edcommunity.esri.com\/software\/aejee.\nImportant note: Software and online resources sometimes change fast. This\nlab was designed with the most recently available version of the software at the\ntime of writing. However, if the software or Websites have significantly changed\nbetween then and now, an updated version of this lab (using the newest ver-\nsions) is available online at http:\/\/www.whfreeman.com\/shellito1e.\n241 242\nChapter 8 Getting There Quicker with Geospatial Technology\nLab Data\nCopy the folder \u2018Chapter8\u2019\u2014it contains:\na A Microsoft Excel spreadsheet featuring a series of addresses to be\ngeocoded\na A second Microsoft Excel spreadsheet containing latitude and longitude\ncoordinates for each address\na A .kmz file containing library locations in a format readable by Google\nEarth\na A TIGER 2000 line file in shapefile format\nLocalizing This Lab\nThe datasets in this lab focus on the locations of public libraries and the road\nnetwork in Virginia Beach, Virginia. However, this lab can be modified to ex-\namine your local area by doing the following:\na Use your local county library\u2019s Website (or the phone book) as a source\nof names and addresses of local libraries and use Microsoft Excel to\ncreate a file like the one in the \u2018Chapter8\u2019 folder (where each column\nhas a different attribute of data). If there are not enough local libraries\naround, use the addresses of something else, like coffee shops, pizza\nshops, or drugstores.\na A TIGER file (in shapefile format) of the roads of your county can\nbe downloaded free from Esri at: http:\/\/arcdata.esri.com\/data\/\ntiger2000\/tiger_download.cfm. Unzip the file and the shape-\nfile will be ready to go (this was the source of the Virginia Beach\nTIGER file used in this lab). Coordinates are in NAD83 decimal\ndegrees.\n8.1 Google Earth\u2019s Shortest-Path Functions\n1. Start Google Earth (GE) and \u201cFly To\u201d Virginia Beach, Virginia.\n2. Let\u2019s start with this scenario: You\u2019re on vacation in Virginia Beach,\nvisiting the Old Coast Guard Museum (which fronts on the oceanfront\nboardwalk). You have tickets to see a show at the Norfolk Scope, located\nin nearby downtown Norfolk. You want to take the shortest route to\nget from your beachfront location to the arena. Similar to what you\ndid in Geospatial Lab Application 1.1, select the Directions tab. Use the\nfollowing addresses for directions:\na. From: 2400 Atlantic Avenue, Virginia Beach, VA 23451 (this is the\nMuseum)\nb. To: 201 E. Brambleton Avenue, Norfolk, VA 23510 (this is the\nNorfolk Scope) 243\nGeocoding and Shortest Path Analysis: AEJEE Version\n3. Click the Begin Search button for Google Earth to compute the shortest\nroute between the two points.\n4. The shortest path will be highlighted in purple.\n5. Make sure the Roads layer is turned on in the Layers box.\nQuestion 8.1 Without transcribing directions, what is the general route\nthat Google Earth has created for the shortest route between the oceanfront\nmuseum and the arena?\n6. The bottom of the search box will give information about the length of\nthe route.\nQuestion 8.2 What is the distance (and estimated driving time) of the\nroute using this shortest path?\n8.2 Google Maps\u2019 Shortest-Path Functions\n1. Keeping in mind the shortest path that Google Earth calculated, open\nyour Web browser and navigate to the Google Maps Website at http:\/\/\nmaps.google.com.\n2. Type \u201cVirginia Beach, VA\u201d and click Search Maps.\n3. When the map of Virginia Beach appears, select Get Directions and\nuse the Old Coast Guard Museum address for option A and the Norfolk\nScope address for option B.\n4. Select \u201cBy Car\u201d for your method of travel (the car icon) and click on Get\nDirections.\n5. The shortest route between the two points will appear, highlighted in\npurple. It should be the same route that Google Earth calculated for you.\nIt may give an alternate route to take as well. 244\nChapter 8 Getting There Quicker with Geospatial Technology\n(Source: \u00a9 2010 Google, Imagery Copyright 2011 TerraMetrics, Inc. www.terrametrics.com)\n6. Zoom in closer to the original point A. Scroll the mouse over the purple\nshortest path line and you\u2019ll see a white circle appear along the path.\nThis lets you change the calculated route to account for any variety of\nfactors (such as travel preference, known congestion or construction\nareas, rerouting to avoid areas or known delays, and so on).\n7. Start by placing the mouse over the turn onto I-264 and drag the\ncircle up to state route 58 (also known as Laskin Road or Virginia\nBeach Boulevard\u2014the other major east-west road just above and\nparallel to I-264). The route will change by first having you drive\nnorth on Atlantic Avenue, turning west on 58, then merging back\nonto I-264 again a little later in the route. Even though it\u2019s a small\nchange, you\u2019ll see that the driving distance and estimated time have\nchanged. Answer Question 8.3. When you\u2019re done, return the circle\nback to its original starting point and the route will go back to how it\noriginally was.\nQuestion 8.3 What is the new distance (and estimated driving time) of the\nroute using this new path? What would account for this?\n8. Scroll across the map until you see where the route crossed I-64 (about\ntwo-thirds of the way between the two points). I-64 is the major highway\ninto the Hampton Roads area. Let\u2019s throw another change into the\nscenario\u2014say, for instance, that there is heavy construction on I-264\nwest of the I-64 intersection. At the intersection, move the circle off the 245\nGeocoding and Shortest Path Analysis: AEJEE Version\nroute and north onto I-64 far enough that the remainder of the route to\nNorfolk Scope diverts your shortest path onto I-64 headed north and not\nso that it returns you to I-264 west (you may have to move other circles\nas well if Google Maps diverts your path back onto I-264).\nQuestion 8.4 What is the new distance (and estimated driving time) of the\nroute using this new path?\nQuestion 8.5 Without transcribing directions, what is the general route\nthat Google Maps has created for the new route between the museum and\nthe arena (taking this detour into account)?\n9. Reset the route back to its original state. Now, change the route so you would\nhave to travel south on I-64 instead of north (and still avoiding I-264).\nQuestion 8.6 What is the new distance (and estimated driving time) of the\nroute using this new path?\nQuestion 8.7 Without transcribing directions, what is the general route\nthat Google Maps has created for the new route between the oceanfront\nresort and the arena (taking this detour into account)?\n10. All of these options can be used to model alternative routes or potential\nbarriers or user-defined choices during shortest-path calculations. Each\nof these routes is a \u201cbest route,\u201d just based on the parameters (such as\nsimulating barriers) given to the system.\n8.3 Geocoding and Google Earth\nBefore a shortest path could be calculated, the system first had to match the\naddresses of the starting and ending points to their proper location. The next\nsection of this lab will introduce a method of doing this address matching\n(geocoding) to create a new point layer for use.\n1. Open a new tab or window with your Web browser and go to the\nfollowing URL: http:\/\/batchgeo.com\/.\nImportant note: BatchGeo is a free online tool that will let you match mul-\ntiple (or single) addresses to their road-network locations and create a point\nlayer marking each of those address locations. In this portion of the lab, you\u2019ll\nstill be working with the Virginia Beach area, focusing on the locations of the\ncity\u2019s public libraries.\n2. Open the VBLibraries file in the \u2018Chapter8\u2019 folder (it\u2019s in Microsoft Excel\nformat). This lists the name and address of ten different public library\nvenues in Virginia Beach.\n3. Use the mouse to select all rows and columns in the Excel file that\ncontain data in them (including the column headers like \u201cName,\u201d\n\u201cAddress,\u201d and so on). Copy this data. 246\nChapter 8 Getting There Quicker with Geospatial Technology\n(Source: Batchgeo.com)\n4. On the BatchGeo Website, click the mouse in the Step 1 box to select all\nof the data listed in it and delete it. Then paste the data from the Excel\nfile in place of it. The Step 1 box will now show all of the data from the\nExcel file but separated by large spaces.\n5. Click on the Validate and Set Options button. This will ensure that the\ndata is in the proper format that batchgeo.com needs it to be. If an error\nmessage results, delete the information in the Step 1 box, then re-copy\nand paste the Excel data.\n6. In Step 2, BatchGeo needs the names of the fields identified for it to\nbegin the geocoding process (in other words, it wants to know which\ncolumn contains the Address data, which column contains the Zip Code\ndata, and so on).\n7. Make sure the following fields are selected:\na. Address \u2013 should be ADDRESS\nb. City \u2013 should be CITY 247\nGeocoding and Shortest Path Analysis: AEJEE Version\nc. State\/Province \u2013 should be STATE\nd. Zip\/Postal Code \u2013 should be ZIP\n8. All other fields can stay at their defaults.\n9. In Step 3, click on the Make Google Map button.\n10. You\u2019ll see a Google Map generated of Virginia Beach that contains\nsymbols indicating the libraries\u2019 locations. This indicates the geocoding\nprocess is complete.\n11. Pan and Zoom around the map to get a feel for where the libraries\nwere geocoded to. If you wanted, you could save this map for your\nown use. However, at the time if this writing, you cannot access\ncoordinate data for the points or download a Google Earth compatible\nfile. In order to get both of these things, you\u2019ll use a different Web\nresource.\n12. Open your Web browser and navigate to: http:\/\/www.gpsvisualizer.\ncom\/geocoding.html.\n13. On the Website, click the link for Geocode Multiple Addresses. A new\nWeb interface will appear.\n14. Return to Excel and re-copy all of the data, including the headers for\neach column.\n15. Back on the GPS Visualizer Website, paste all of the Excel data in the\nInput box.\n(Source: GPSVisualizer.com\/Map data \u00a9 2011 Google) 248\nChapter 8 Getting There Quicker with Geospatial Technology\n16. For the various settings, use the following:\na. For Type of data, choose tabular (columns & header row).\nb. For Source, choose Google.\nc. For Field separator in output, choose tab.\n17. When the settings are correct, press the Start geocoding button.\n18. You\u2019ll see a new map appear with markers showing the locations of the\nlibraries.\n19. In the format pull-down menu under the \u201cDraw a map\u201d button, select\nGoogle Earth. This will enable the GPS Visualizer Website to create a\nfile of the geocoded addresses that is compatible with Google Earth.\n20. After choosing that option, click on the Draw a map button (located\nunder the map).\n21. A new Web page will open and a blue and white \u201cKMZ\u201d icon will be\nshown with a link next to it (the link will be a string of numbers ending\nin the letters .kmz).\n22. Click on the link and if prompted choose to Open With Google Earth.\nClick OK in the dialog box.\n23. Also, do NOT close the browser window with the geocoded data. We\u2019ll\nreturn to it later in the lab.\n24. In the Places box, there should be a new heading for Temporary Places,\nand under that should be a new checkbox for a .kmz file called GPS\ndata. Turning this option on and off will toggle the display of the points\nrepresenting your geocoded libraries. Basically, the GPS Visualizer\nWebsite converted the geocoded locations into a file that could be read\nby Google Earth. Note that a copy of this GPS data .kmz file is included\nwith the \u2018Chapter8\u2019 data, just in case you\u2019re unable to access the Web\nresources used to generate it. 249\nGeocoding and Shortest Path Analysis: AEJEE Version\n25. Once the geocoded results are added to Google Earth, in the Google\nEarth view you\u2019ll see each of the libraries represented by a point.\n26. To make your layer easier to see, right-click on the .kmz file and select\nProperties. Under the Style, Color tab you can alter the color and size\nof both the icons and their labels.\n27. Rolling the mouse over the top of a point will display its name. Clicking\non a point will open a box with its name and address. Right-clicking on\na point will allow you to access options for Directions to be calculated to\nthat point or from that point.\n(Source: \u00a9 2010 Google, Data SIO, NOAA, U.S. Navy, NGA, GEBCO. \u00a9 2011 Europa Technologies)\n28. Select the Great Neck Area Library, right-click on it, and select the\ndirections from here option.\n29. In the Search box, you\u2019ll see the Directions From option switch to say\n\u201cGreat Neck Area Library\u201d with its latitude and longitude.\n30. Next, locate the Princess Anne Area Library, right-click on it, and select\nthe directions to here option.\n31. In the Search box, the To directions will switch to the new Princess\nAnne destination. A new shortest path from the Great Neck Area\nLibrary to the Princess Anne Area Library will be calculated and\ndisplayed in purple.\nQuestion 8.8 What is the distance (and estimated driving time) of the\nroute between the Great Neck Area Library and the Princess Anne Area\nLibrary? 250\nChapter 8 Getting There Quicker with Geospatial Technology\n32. Locate both the South Rosemont Youth Library and the Oceanfront Area\nLibrary.\nQuestion 8.9 What is the distance (and estimated driving time) of the\nroute between the South Rosemont Youth Library and the Oceanfront Area\nLibrary?\n8.4 Preparing Geocoded Results for GIS Analysis\nThe next step of the lab will investigate how to use the GIS road files (like a\nTIGER file) and these geocoded results in GIS.\nImportant note: ArcGIS 10 has full geocoding capabilities, allowing you\nto start with a table of addresses and create a point layer from them (without\nhaving to use a Website to perform the geocoding), but the free ArcExplorer\nJava Edition for Educators does not. However, you can use the results from\nthe gpsvisualizer.com Website with AEJEE by converting the address points\ninto latitude\/longitude coordinates and mapping those as points in AEJEE.\nThe gpsvisualizer.com Website will report back the latitude and longitude\ncoordinates for all geocoded points. If so, it would be easy enough to copy the\nfull contents of the \u201cResults as text\u201d box into an Excel spreadsheet. You would\nthen have the addresses and their coordinates together in a file. Also, the lati-\ntude and longitude coordinates for each point are stored in the .kmz file you\nloaded into Google Earth.\n1. Return to Google Earth, right-click on the GPS data .kmz file, and select\nCopy.\n2. Open a text editor utility like Notepad (in the Accessories folder in\nWindows), and Paste the .kmz file there.\nYou\u2019ll see the code that makes up the file, including the latitude and longi-\ntude coordinates for each geocoded point. You could make two new columns\nin the original Excel table\u2014one for latitude and one for longitude\u2014and copy\nand paste the coordinate information for each one into its proper place in the\ntable. It\u2019s a longer way of doing things, but in the end you\u2019ll have coordinates\nfor each geocoded address available.\nFor the purposes of this lab, a separate Excel file called VBCoords is avail-\nable within the \u2018Chapter8\u2019 folder that has all of this done for you.\nImportant note: The latitude and longitude computed by gpsvisualizer.\ncom are decimal degree values.\n8.5 Using TIGER Files and Geocoded Results in AEJEE\n1. Open the VBCoords.xls file in Excel. Use Excel\u2019s Save As option to save\nthe file not as a regular Excel file, but as a Comma Delimited (.csv)\nfile (in Excel 2007, this is available under the \u201cOther Formats\u201d options\nin Save As). Call this new file GISlibraries.csv and save it on your\ncomputer in the \u2018Chapter8\u2019 folder. 251\nGeocoding and Shortest Path Analysis: AEJEE Version\n2. GISlibraries.csv now contains all the library address information, plus a\nlatitude and longitude location value for each library. This information\nwill be used to plot their locations in AEJEE.\n3. Start AEJEE. From the \u2018Chapter8\u2019 folder, add the tgr51810lka.shp\nshapefile to the map (see Geospatial Lab Application 5.1 for the basics of\nstarting AEJEE and adding data to it). This shapefile is a TIGER 2000 file\nof the Virginia Beach road network.\n4. Open the TIGER file\u2019s attribute table. You\u2019ll see it contains 19,026\nrecords, each representing a link of the city\u2019s road network. You\u2019ll also\nsee the standard TIGER file information of address ranges, census\nfeature class codes, and so on assigned to each link. You can close the\nattribute table for now.\n5. Select the View pull-down menu and choose Add Event Theme.\n6. Choose the GISlibraries.csv file as the Table (AEJEE will use this as the\nsource of the latitude\/longitude values).\n7. Select LONG for the X Field.\n8. Select LAT for the Y Field.\n9. Select the \u2018Chapter8\u2019 folder as the Output Dir (where AEJEE will place\nits new shapefile of points).\n10. Choose an appropriate Style, Color, and Size for the appearance of the\nresulting points.\n11. Click OK when all settings are ready.\n12. A new point layer will be created (called GISlibraries), with each point\nrepresenting the location of a library in Virginia Beach. Open this new\nlayer\u2019s attribute table and you\u2019ll see that all of the data from the file with\nthe addresses has been importing into attribute table format. 252\nChapter 8 Getting There Quicker with Geospatial Technology\n8.6 Analysis of Geocoded Results in AEJEE\nIn this section, you will start performing some basic spatial analysis to exam-\nine the relationship between the library locations and the road network (see\nGeospatial Lab Application 6.1 for details on how to build queries and buffers\nusing AEJEE).\n1. First, use AEJEE to build a query selecting all road segments that have\ntheir name (FENAME) equal to \u2018Atlantic\u2019 (Atlantic Avenue is a main\nnorth-south road that runs parallel to the boardwalk along the Virginia\nBeach oceanfront). Even though there are more than 100 records, have\nAEJEE display all the unique names. Keep in mind that to build a query,\nyou will first have to click on the file (tgr51810lka) in AEJEE\u2019s Table of\nContents.\n2. If AEJEE asks, indicate that you want to see all records.\n3. You\u2019ll see the road segments that make up Atlantic Avenue appear in the\nyellow color of selected AEJEE objects.\n4. Next, build a 2-mile buffer around the selected features of the road. Use\nthe buffer to select libraries that are within the buffer.\nQuestion 8.10 How many libraries are within 2 miles of Atlantic Avenue?\nWhich libraries are these?\n5. Clear the selected features from the roads and libraries and then select\nall road segments with their name equal to \u2018Virginia Beach.\u2019 This will 253\nGeocoding and Shortest Path Analysis: AEJEE Version\nselect the segments that comprise Virginia Beach Boulevard, a major\neast-west corridor that runs through the center of Virginia Beach from\nthe oceanfront on the east straight through into Norfolk on the west.\n6. Construct a 2-mile buffer around the selected features of road and use\nthe buffer to select libraries that are within the buffer.\nQuestion 8.11 How many libraries are within two miles of Virginia Beach\nBoulevard? Which libraries are these?\n7. Clear the selected features and build a final query to select all road\nsegments with their name equal to \u2018Independence.\u2019 This will select all\nthe segments that comprise Independence Boulevard, a key north-south\nroad in the middle of Virginia Beach.\n8. Construct a 2-mile buffer around the selected features of road and use\nthe buffer to select libraries that are within the buffer.\nQuestion 8.12 How many libraries are within two miles of Independence\nBoulevard? Which libraries are these?\nClosing Time\nThis lab demonstrated several types of features associated with geospatial\nnetwork data, including calculating a shortest path, geocoding, and using\ngeocoded results in conjunction with a road network file in GIS. Chapter 9\nwill switch gears and introduce some new concepts dealing with remote sens-\ning of Earth (and the roads built on top of it).\na Exit Google Earth by selecting Exit from the File pull-down menu.\na Also, exit AEJEE by selecting Exit from the File pull-down menu.\na There\u2019s no need to save any data in this lab. 8.2\nGeospatial Lab Application\nGeocoding and Shortest Path Analysis:\nArcGIS Version\nThis chapter\u2019s lab will introduce you to the concepts of calculating a shortest\npath between stops along a network as well as generating directions for the path\nusing Google Earth and Google Maps. You\u2019ll also be performing geocoding us-\ning a Web service, examining the geocoding results, then performing some basic\nspatial analysis of the results and a TIGER file using Google Earth and ArcGIS.\nThe previous version of this lab (8.1 Geospatial Lab Application: Geocod-\ning and Shortest Path Analysis: AEJEE Version) uses the free ArcExplorer Java\nEdition for Educators (AEJEE). However, this lab provides the same activities\nfor use with ArcGIS 10.\nObjectives\nThe goals for you to take away from this lab are:\na Familiarizing yourself with the shortest path and directions functions of\nGoogle Earth.\na Using Google Maps to alter the shortest path to account for route changes.\na Utilizing an online geocoding service to geocode a series of addresses,\nthen examining the results.\na Using ArcGIS to plot the results of a geocoding operation.\na Examining some basic spatial analysis of locations and their relation to a\nroad network (TIGER file) in ArcGIS.\nObtaining Software\na The current version of Google Earth (6.0) is available for free download\nat http:\/\/earth.google.com.\na The current version of ArcGIS (10) is not freely available for use. However,\ninstructors affiliated with schools that have a campus-wide software license\nmay request a 1-year student version of the software online at http:\/\/\nwww.esri.com\/industries\/apps\/education\/offers\/promo\/index.cfm.\nImportant note: Software and online resources sometimes change fast.\nThis lab was designed with the most recently available version of the software\nat the time of writing. However, if the software or Websites have significantly\nchanged between then and now, an updated version of this lab (using the new-\nest versions) is available online at: http:\/\/www.whfreeman.com\/shellito1e.\n254 255\nGeocoding and Shortest Path Analysis: ArcGIS Version\nLab Data\nCopy the folder \u2018Chapter8\u2019\u2014it contains:\na A Microsoft Excel spreadsheet featuring a series of addresses to be\ngeocoded\na A second Microsoft Excel spreadsheet containing latitude and longitude\ncoordinates for each address\na A .kmz file containing library locations in a format readable by Google Earth.\na A TIGER 2000 line file in shapefile format\nLocalizing This Lab\nThe datasets in this lab focus on the locations of public libraries and the road\nnetwork in Virginia Beach, Virginia. However, this lab can be modified to ex-\namine your local area by doing the following:\na Use your local county library\u2019s Website (or the phone book) as a source\nof names and addresses of local libraries and use Microsoft Excel to\ncreate a file like the one in the \u2018Chapter8\u2019 folder (where each column\nhas a different attribute of data). If there are not enough local libraries\naround, use the addresses of something else, like coffee shops, pizza\nshops, or drugstores.\na A TIGER file (in shapefile format) of the roads of your county can be\ndownloaded for free from Esri at http:\/\/arcdata.esri.com\/data\/\ntiger2000\/tiger_download.cfm. Unzip the file and the shapefile will be\nready to go (this was the source of the Virginia Beach TIGER file used in\nthis lab). Coordinates are in NAD83 decimal degrees.\n8.1 Google Earth\u2019s Shortest-Path Functions\n1. Start Google Earth (GE) and \u201cFly To\u201d Virginia Beach, Virginia.\n2. Let\u2019s start with this scenario: You\u2019re on vacation in Virginia\nBeach, visiting the Old Coast Guard Museum (which fronts on the\noceanfront boardwalk). You have tickets to see a show at the Norfolk\nScope, located in nearby downtown Norfolk. You want to take the\nshortest route to get from your beachfront location to the arena.\nSimilar to what you did in Geospatial Lab Application 1.1, select the\nDirections tab. Use the following addresses for directions:\na. From: 2400 Atlantic Avenue, Virginia Beach, VA 23451 (this is the\nMuseum)\nb. To: 201 E. Brambleton Avenue, Norfolk, VA 23510 (this is the\nNorfolk Scope) 256\nChapter 8 Getting There Quicker with Geospatial Technology\n3. Click the Begin Search button for Google Earth to compute the shortest\nroute between the two points.\n4. The shortest path will be highlighted in purple.\n5. Make sure the Roads layer is turned on in the Layers box.\nQuestion 8.1 Without transcribing directions, what is the general route\nthat Google Earth has created for the shortest route between the oceanfront\nmuseum and the arena?\n6. The bottom of the search box will give information about the length of\nthe route.\nQuestion 8.2 What is the distance (and estimated driving time) of the\nroute using this shortest path?\n8.2 Google Maps\u2019 Shortest-Path Functions\n1. Keeping in mind the shortest path that Google Earth calculated, open\nyour Web browser and navigate to the Google Maps Website at http:\/\/\nmaps.google.com.\n2. Type \u201cVirginia Beach, VA\u201d and click Search Maps.\n3. When the map of Virginia Beach appears, select Get Directions\nand use the Old Coast Guard Museum address for option A and the\nNorfolk Scope address for option B.\n4. Select \u201cBy Car\u201d for your method of travel (the car icon) and click on\nGet Directions.\n5. The shortest route between the two points will appear, highlighted in\npurple. It should be the same route that GE calculated for you. It may\ngive an alternate route to take as well. 257\nGeocoding and Shortest Path Analysis: ArcGIS Version\n(Source: \u00a9 2010 Google, Imagery Copyright 2011 TerraMetrics, Inc. www.terrametrics.com)\n6. Zoom in closer to the original point A. Scroll the mouse over the purple\nshortest path line and you\u2019ll see a white circle appear along the path.\nThis lets you change the calculated route to account for any variety of\nfactors (such as travel preference, known congestion or construction\nareas, rerouting to avoid areas or known delays, and so on).\n7. Start by placing the mouse over the turn onto I-264 and drag the circle\nup to state route 58 (also known as Laskin Road or Virginia Beach\nBoulevard\u2014the other major east-west road just above and parallel to\nI-264). The route will change by first having you drive north on Atlantic\nAvenue, turning west on 58, then merging back onto I-264 again a little\nlater in the route. Even though it\u2019s a small change, you\u2019ll see that the\ndriving distance and estimated time have changed. Answer Question\n8.3. When you\u2019re done, return the circle back to its original starting\npoint and the route will go back to how it originally was.\nQuestion 8.3 What is the new distance (and estimated driving time) of the\nroute using this new path? What would account for this?\n8. Scroll across the map until you see where the route crossed I-64 (about\ntwo-thirds of the way between the two points). I-64 is the major highway\ninto the Hampton Roads area. Let\u2019s throw another change into the\nscenario\u2014say, for instance, that there is heavy construction on I-264\nwest of the I-64 intersection. At the intersection, move the circle off the\nroute and north onto I-64 far enough that the remainder of the route to\nNorfolk Scope diverts your shortest path headed north onto I-64 and not 258\nChapter 8 Getting There Quicker with Geospatial Technology\nso that it returns you to I-264 west (you may have to move other circles\nas well if Google Maps diverts your path back onto I-264).\nQuestion 8.4 What is the new distance (and estimated driving time) of the\nroute using this new path?\nQuestion 8.5 Without transcribing directions, what is the general route\nthat Google Maps has created for the new route between the museum and\nthe arena (taking this detour into account)?\n9. Reset the route back to its original state. Now, change the route so you\nwould have to travel south on I-64 instead of north (and still avoiding\nI-264).\nQuestion 8.6 What is the new distance (and estimated driving time) of the\nroute using this new path?\nQuestion 8.7 Without transcribing directions, what is the general route\nthat Google Maps has created for the new route between the oceanfront\nresort and the arena (taking this detour into account)?\n10. All of these options can be used to model alternative routes or potential\nbarriers or user-defined choices during shortest-path calculations. Each\nof these routes is a \u201cbest route,\u201d just based on the parameters (such as\nsimulating barriers) given to the system.\n8.3 Geocoding and Google Earth\nBefore any type of shortest path could be calculated, the system first had to\nmatch the addresses of the starting and ending points to their proper loca-\ntion. The next section of this lab will introduce a method of doing this address\nmatching (geocoding) to create a new point layer for use.\n1. Open a new tab or window with your Web browser and go to the URL\nhttp:\/\/batchgeo.com\/.\nImportant note: BatchGeo is a free online tool that will let you match mul-\ntiple (or single) addresses to their road-network locations and create a point\nlayer marking each of those address locations. In this portion of the lab, you\u2019ll\nstill be working with the Virginia Beach area, focusing on the locations of the\ncity\u2019s public libraries.\n2. Open the VBLibraries file in the \u2018Chapter8\u2019 folder (it\u2019s in Microsoft Excel\nformat). This lists the name and address of ten different public library\nvenues in Virginia Beach.\n3. Use the mouse to select all rows and columns in the Excel file that contain\ndata in them (including the column headers like \u201cName,\u201d \u201cAddress,\u201d and\nso on). Copy this data.\n4. On the BatchGeo Website, click the mouse in the Step 1 box to select all\nof the data listed in it and delete it. Then paste the data from the Excel 259\nGeocoding and Shortest Path Analysis: ArcGIS Version\nfile in place of it. The Step 1 box will now show all of the data from the\nExcel file but separated by large spaces.\n(Source: BatchGeo.com)\n5. Click on the Validate and Set Options button. This will ensure that the\ndata is properly in the format that batchgeo.com needs it to be in. If an\nerror message results, delete the information in the Step 1 box, then re-\ncopy and paste the Excel data.\n6. In Step 2, BatchGeo needs the names of the fields identified for it to\nbegin the geocoding process (in other words, it wants to know which\ncolumn contains the Address data, which column contains the Zip Code\ndata, and so on).\n7. Make sure the following fields are selected:\na. Address \u2013 should be ADDRESS\nb. City \u2013 should be CITY\nc. State\/Province \u2013 should be STATE\nd. Zip\/Postal Code \u2013 should be ZIP 260\nChapter 8 Getting There Quicker with Geospatial Technology\n8. All other fields can stay at their defaults.\n9. In Step 3, click on the Make Google Map button.\n10. You\u2019ll see a Google Map generated of Virginia Beach that contains\nsymbols indicating the libraries\u2019 locations. This indicates the geocoding\nprocess is complete.\n11. Pan and Zoom around the map to get a feel for where the libraries were\ngeocoded to. If you wanted, you could save this map for your own use.\nHowever, at the time if this writing, you cannot access any coordinate\ndata for the points or download a Google Earth compatible file. In order\nto get both of these things, you\u2019ll use a different Web resource.\n12. Open your Web browser and navigate to: http:\/\/www.gpsvisualizer.\ncom\/geocoding.html.\n13. On the Website, click the link for Geocode Multiple Addresses. A new\nWeb interface will appear.\n14. Return to Excel and re-copy all of the data, including the headers for\neach column.\n15. Back on the GPS Visualizer Website, paste all of the Excel data in the Input box.\n(Source: GPSVisualizer.com\/Map data \u00a9 2011 Google)\n16. For the various settings, use the following:\na. For Type of data, choose tabular (columns & header row).\nb. For Source, choose Google.\nc. For Field, separator in output choose tab. 261\nGeocoding and Shortest Path Analysis: ArcGIS Version\n17. When the settings are correct, press the Start geocoding button.\n18. You\u2019ll see a new map appear with markers showing the locations of the\nlibraries.\n19. In the format pull-down menu under the \u201cDraw a map\u201d button, select\nGoogle Earth. This will enable the GPS Visualizer Website to create a\nfile of the geocoded addresses that is compatible with Google Earth.\n20. After choosing that option, click on the Draw a map button (located\nunder the map).\n21. A new Web page will open and a blue and white \u201cKMZ\u201d icon will be\nshown with a link next to it (the link will be a string of numbers ending\nin the letters .kmz).\n22. Click on the link and if prompted choose to Open With Google Earth.\nClick OK in the dialog box.\n23. Also, do NOT close the browser window with the geocoded data. We\u2019ll\nreturn to it later in the lab.\n24. In the Places box, there should be a new heading for Temporary Places,\nand under that should be a new checkbox for a .kmz file called GPS\ndata. Turning this option on and off will toggle the display of the points\nrepresenting your geocoded libraries. Basically, the GPS Visualizer\nWebsite converted the geocoded locations into a file that could be read\nby Google Earth. Note that a copy of this GPS data .kmz file is included\nwith the \u2018Chapter8\u2019 data, just in case you\u2019re unable to access the Web\nresources used to generate it.\n25. Once the geocoded results are added to Google Earth, in the Google\nEarth view you\u2019ll see each of the libraries represented by a point. 262\nChapter 8 Getting There Quicker with Geospatial Technology\n26. To make your layer easier to see, right-click on the .kmz file and select\nProperties. Under the Style, Color tab you can alter the color and size\nof both the icons and their labels.\n27. Rolling the mouse over the top of a point will display its name. Clicking\non a point will open a box with its name and address. Right-clicking on\na point will allow you to access options for Directions to be calculated to\nthat point or from that point.\n(Source: \u00a92010 Google, Data SIO, NOAA, U.S. Navy, NGA, GEBCO. \u00a9 2011 Europa Technologies)\n28. Select the Great Neck Area Library, right-click on it, and select the\ndirections from here option.\n29. In the Search box, you\u2019ll see the Directions From option switch to say\n\u201cGreat Neck Area Library\u201d with its latitude and longitude.\n30. Next, locate the Princess Anne Area Library, right-click on it, and\nselect the directions to here option.\n31. In the Search box, the To directions will switch to the new Princess\nAnne destination. A new shortest path from the Great Neck Area\nLibrary to the Princess Anne Area Library will be calculated and\ndisplayed in purple.\nQuestion 8.8 What is the distance (and estimated driving time) of the route\nbetween the Great Neck Area Library and the Princess Anne Area Library?\n32. Locate both the South Rosemont Youth Library and the Oceanfront Area\nLibrary. 263\nGeocoding and Shortest Path Analysis: ArcGIS Version\nQuestion 8.9 What is the distance (and estimated driving time) of the route\nbetween the South Rosemont Youth Library and the Oceanfront Area Library?\n8.4 Preparing Geocoded Results for GIS Analysis\nThe next step of the lab will investigate how to use the GIS road files (like a\nTIGER file) and these geocoded results in GIS.\nImportant note: ArcGIS has full geocoding capabilities, allowing you to\nstart with a table of addresses and create a point layer from them (without\nhaving to use a Website to perform the geocoding). However, you can use the\nresults from the gpsvisualizer.com Website with ArcGIS by converting the ad-\ndress points into latitude\/longitude coordinates and mapping those as points\nin ArcGIS. The gpsvisualizer.com Website will report back the latitude and\nlongitude coordinates for all geocoded points. If so, it would be easy enough\nto copy the full contents of the \u201cResults as text\u201d box into an Excel spreadsheet.\nYou would then have the addresses and their coordinates together in a file.\nAlso, the latitude and longitude coordinates for each point are stored in the\n.kmz file you loaded into Google Earth.\n1. Return to Google Earth, right-click on the GPS data .kmz file, and select\nCopy.\n2. Open a text editor utility like Notepad (in the Accessories folder in\nWindows), and Paste the .kmz file there.\nYou\u2019ll see the code that makes up the file, including the latitude and longi-\ntude coordinates for each geocoded point. You could make two new columns\nin the original Excel table\u2014one for latitude and one for longitude\u2014and copy\nand paste the coordinate information for each one into its proper place in the\ntable. It\u2019s a longer way of doing things, but in the end you\u2019ll have coordinates\nfor each geocoded address available.\nFor the purposes of this lab, a separate Excel file called VBCoords is avail-\nable within the \u2018Chapter8\u2019 folder that has all of this done for you.\nImportant note: The latitude and longitude computed by gpsvisualizer.\ncom are decimal degree values.\n8.5 U sing TIGER Files and Geocoded Results\nin ArcGIS\n1. Start ArcMap. From the \u2018Chapter8\u2019 folder, add the tgr51810lka.shp\nshapefile to the map (see Geospatial Lab Application 5.2 for the basics of\nstarting ArcMap and adding data to it). This shapefile is a TIGER 2000\nfile of the Virginia Beach road network.\n2. Open the TIGER file\u2019s attribute table. You\u2019ll see it contains 19,026\nrecords, each representing a link of the city\u2019s road network. You\u2019ll also\nsee the standard TIGER file information of address ranges, census\nfeature class codes, and so on assigned to each link. You can close the\nattribute table for now. 264\nChapter 8 Getting There Quicker with Geospatial Technology\n3. Open ArcToolbox by selecting its icon from the Standard toolbar (it\u2019s\nthe small red box icon):\n4. ArcToolbox is a dockable window that contains a multitude of useful tools\nfor ArcGIS. From the Data Management tools, select Layers and Table\nViews, then select Make XY Event Layer.\n5. An Event Layer will be a temporary layer created from a set of coordinates.\nIn this case, you\u2019ll be plotting points based on the latitude and longitude\ncoordinates of the geocoded addresses. 265\nGeocoding and Shortest Path Analysis: ArcGIS Version\n6. Choose the VBCoords.xls file (the Sheet1$ part of it) as the XY Table\n(that ArcMap will use as the source of the latitude\/longitude values).\n7. Select LONG for the X Field.\n8. Select LAT for the Y Field.\n9. Leave the Z Field blank.\n10. Type GISlibraries for the Layer Name.\n11. From the Spatial Reference choices, select Geographic Coordinate\nSystems, then select World, then select WGS84.prj.\n12. Click OK when all settings are ready.\n13. A new point layer will be created\ncalled GISlibraries, with each\npoint representing the location\nof a library in Virginia Beach.\nOpen this new layer\u2019s attribute table\nand you\u2019ll see that all of the data from\nthe file with the addresses has been\nimported into ArcGIS attribute table\nformat.\n14. In order to use this layer for querying,\nit will first have to be converted over\nto a format like a shapefile for use.\nTo do this, first right-click on the\nGISlibraries file and select Data, then\nExport Data. A new Export Data dialog\nwill appear. 266\nChapter 8 Getting There Quicker with Geospatial Technology\n15. Use the same coordinate system as this layer\u2019s source data.\n16. In the Output feature class option, press the open button. By default,\nArcGIS will want to name the new shapefile Export_Output.shp, but you\nwill change that in the next step.\n17. In the Saving Data dialog, navigate to the \u2018Chapter8\u2019 folder and call the\nnew file you\u2019ll be creating Pointlibraries instead of Export_Output.\n18. In the Save as type: option, use the pull-down menu to select Shapefile.\n19. Click Save when everything\u2019s ready. This will return you to the\nExport Data dialog, except your Output feature class should now be\nPointlibraries.shp. If everything\u2019s okay, click OK to continue. ArcGIS will\nexport your event layer into a shapefile called Pointlibraries.\n20. Click Yes when prompted if you want to add the exported data as a new\nlayer. Pointlibraries will appear in the Table of Contents.\n21. Turn off the old GISlibraries event layer. You\u2019ll be working with the\nPointlibraries shapefile in the next part.\n8.6 Some Basic Analysis of Geocoded Results\nin ArcMap\nIn this part, you will start performing some basic spatial analysis to exam-\nine the relationship between the library locations and the road network (see\nGeospatial Lab Application 6.2 for details on how to build queries based on\nattributes and queries based on location using ArcMap).\n1. First, use the Select By Attributes tool to build a query selecting all road\nsegments that have their name (FENAME) equal to \u2018Atlantic\u2019 (Atlantic\nAvenue is a main north-south road that runs parallel to the boardwalk\nalong the Virginia Beach oceanfront). Press the Get Unique Values\nbutton to access the names of all the roads. You\u2019ll see the road segments\nthat make up Atlantic Avenue appear in the default cyan color of\nselected ArcMap features. 267\nGeocoding and Shortest Path Analysis: ArcGIS Version\n2. Next, use Select By Location to find all library points (the Pointlibraries)\nwithin 2 miles of the selected road segments of Atlantic Avenue. 268\nChapter 8 Getting There Quicker with Geospatial Technology\nQuestion 8.10 How many libraries are within 2 miles of Atlantic Avenue?\nWhich libraries are these?\n3. Clear the selected features from the roads and libraries and then select\nall road segments with their name equal to \u2018Virginia Beach.\u2019 This will\nselect the segments that comprise Virginia Beach Boulevard, a major\neast-west corridor that runs through the center of Virginia Beach from\nthe oceanfront on the east straight through into Norfolk on the west.\n4. Next, use Select By Location to find all library points within 2 miles of\nthe selected road segments of Virginia Beach Boulevard.\nQuestion 8.11 How many libraries are within two miles of Virginia Beach\nBoulevard? Which libraries are these?\n5. Clear all selected features and build a final query to select all road\nsegments with their name equal to \u2018Independence.\u2019 This will select all\nthe segments that comprise Independence Boulevard, a key north-south\nroad in the middle of Virginia Beach.\n6. Next, use Select By Location to find all library points within 2 miles of\nthe selected road segments of Independence Boulevard.\nQuestion 8.12 How many libraries are within two miles of Independence\nBoulevard? Which libraries are these?\nClosing Time\nThis lab demonstrated several types of features associated with geospatial\nnetwork data, including calculating a shortest path, geocoding, and using\ngeocoded results in conjunction with a road-network file in GIS. Chapter 9\nwill switch gears and introduce some new concepts dealing with remote sens-\ning of Earth (and the roads built on top of it).\na Exit Google Earth by selecting Exit from the File pull-down menu. Exit\nArcGIS as well.\na There\u2019s no need to save any data in this lab. Part 3\nRemote Sensing\n9\nRemotely Sensed Images\nfrom Above\nWhere Aerial Photography Came from, Color Infrared Photos,\nOrthophotos, Oblique Photos, Visual Image Interpretation,\nand Photogrammetric Measurements\nWhenever you take a picture of something with a digital camera, somehow\nthe image is captured and stored as a file on a memory card. Taking a pic-\nture at a birthday party or on vacation is actually a form of remote sensing\u2014\nacquiring data without being in direct contact with the subject. For instance,\nyou don\u2019t shove the camera right on top of the birthday cake to take a picture\nof it, you stand back a distance to do it. In essence, the camera is \u201csensing\u201d\n(taking the picture) while you\u2019re some distance (\u201cremotely\u201d) away from the\ntarget. In this same way, your eyes function as \u201cremote sensing\u201d devices\u2014when\nyou\u2019re looking at this book, you don\u2019t place your eyeball directly on the page.\nInstead, your eyes are \u201csensing\u201d the data and they\u2019re \u201cremotely\u201d located\nabout a foot away from the page. Both a camera and your eyes are perform-\ning \u201cremote sensing\u201d as they\u2019re acquiring information or data (in this case,\nvisual information) from a distance away without actually making contact\nwith the item.\nHowever, when it comes to remote sensing in geospatial technology, remote sensing the\nthings are a little more specific. In remote sensing, the data being acquired process of collecting\ninformation related to\nis information about the light energy being reflected off of a target. In this\nthe reflected or emitted\ncase, your eyes can still function like remote sensing devices, as what you\u2019re\nelectromagnetic energy\nactually seeing is light being reflected off objects around you processed by\nfrom a target by a\nyour eyes. A digital camera does the same thing by capturing the reflection device a considerable\nof light from whatever you\u2019re taking a picture of. In 1826, a French inventor distance away from\nnamed Joseph Niepce took the first photograph, capturing an image of the that target from an\naircraft or spacecraft.\ncourtyard of his home. Photography has come a long way since those days\n(when it took 8 hours to expose the film) to the point where digital cameras\n226699 270\nChapter 9 Remotely Sensed Images from Above\nare now commonplace and it\u2019s difficult to find a cell phone without a built-in\ncamera. In many ways, you can think of a camera as the original device for\nremote sensing\u2014it captures data from the reflection of visible light nearly\ninstantaneously from a distance away from the target.\nTaking pictures of the ground with such an instrument started not\naerial photography long after cameras became available, leading to the development of aerial\ntaking photographs of photography. Capturing images of the ground from the sky has been going\nobjects on the ground\non for over 150 years\u2014first using balloons and kites, then by utilizing air-\nfrom an airborne\ncraft and rockets, then using satellites to get images from space. When deal-\nplatform.\ning with remote sensing in geospatial technologies, we\u2019re concerned with\nacquiring information from a platform on some type of aircraft or spacecraft.\nCurrently, these types of imagery are obtained in both the public and private\nsector.\nWhether the use has been for military reconnaissance, surveillance, stud-\nies of the landscape, planning purposes, or even the good old \u201chey, I can see\nmy house,\u201d people have been acquiring images of Earth from above for a long\ntime. Today\u2019s aerial photographs are used for all manner of applications\u2014for\ninstance, aerial imagery serves as a base source for creation and updating of\ngeospatial data and maps (such as USGS topographic maps or road networks).\nSome of the extremely crisp high-resolution imagery you see on Google Earth\nis taken from aerial photography sources. In this chapter, we\u2019ll focus on how\naerial images are captured and analyzed, as well as getting used to looking at\nEarth from above.\nHow Did Aircraft Photography Develop?\nA French photographer named Gaspar Felix Tournachon (also known as\n\u201cNadar\u201d) took the first aerial photograph in 1858. Tournachon captured an\nimage of the landscape outside Paris while tethered above the ground in a\nballoon. This first aerial photograph no longer exists, but this was clearly\nthe beginning of something big. In 1860, aerial photographs were taken\nof Boston from a photographer in a balloon above the city (Figure 9.1).\nH owever, development times, problems with landing at the proper location,\nand interference from the balloon\u2019s gas with the photo plates all contrib-\nuted to balloon photography not being the best medium for capturing aerial\nphotographs.\nBy the end of the nineteenth century, photographic equipment was at-\ntached to kites (or a series of kites) and used to remotely photograph the\nlandscape while the photographer was safely on the ground. A famous ex-\nample of kite photography comes from 1906, when a photographer named\nGeorge Lawrence used a set of kites 1000 feet above San Francisco Bay to\ncapture an aerial image he termed \u201cSan Francisco in Ruins\u201d which showed\nthe aftermath of the 1906 earthquake (Figure 9.2). These early uses of\ntaking images from the air set the stage for the use of aircraft to take aerial\nphotographs. 271\nHow Did Aircraft Photography Develop?\nFIGURE 9.1 The\nearliest surviving aerial\nphotograph, taken of\nBoston in 1860. (Source:\nBoston Public Library)\nFlying with an aircraft started with the first flight of the Wright Broth-\ners at Kitty Hawk, North Carolina, in 1903. In 1908, a passenger aboard an\naircraft was able to capture aerial imagery for the first time over France. By\nthe time of World War I, aerial photography from an airplane was commonly\nused for mapmaking and planning of military tactics. Aerial photography of\nthe battlefields allowed for the patterns of trenches to be determined, troop\nmovements and encampments to be plotted, and the location of artillery or\nFIGURE 9.2 \u201cSan\nFrancisco in Ruins\u201d kite\naerial photo taken in\n1906. (Source: Library\nof Congress, Prints and\nPhotographs Division\n[LC-USZC4-3870 DLC]) 272\nChapter 9 Remotely Sensed Images from Above\nFIGURE 9.3 World War I\naerial photography\nshowing trench\nformations. (Source:\nNational Air and Space\nMuseum, National Air and\nSpace Museum Archives)\nsupplies to be found. Countless aerial photos were shot and developed over\nthe course of the war, and aerial photography greatly contributed to military\nintelligence during World War I. See Figure 9.3 for an example of trench\ndevelopment captured by an aerial camera.\nAerial photography continued to be extremely important for tactical plan-\nning and reconnaissance during World War II. Both the Allies and the Axis\nforces depended heavily on aerial photography for obtaining vital informa-\ntion about locations prior to military action at those places, and it became\na key reconnaissance technique during the war. For instance, a V-2 rocket\nbase in Peenemunde, Germany, was the subject of British aerial photography\nin June of 1943, which allowed for bombing strikes to destroy the base less\nthan two months later. Similarly, aerial reconnaissance was used in obtaining\nphotographs of dams prior to bombing missions sent to destroy them. A huge\namount of aerial photography was performed during the war, and a good\nsource of historical imagery is The Aerial Reconnaissance Archives (TARA),\ncurrently held at the Royal Commission on the Ancient and Historical Mon-\numents of Scotland (RCAHMS), which hosts a large archive of European\nWorld War II aerial photos (see Hands-on Application 9.1: World War II Aerial\nPhotography Online for a glimpse of these historic photos). 273\nHow Did Aircraft Photography Develop?\nHands-on Application 9.1\nWorld War II Aerial Photography Online\nThe National Collection of Aerial Photography is a from several countries. Select a location (such as\ngreat online resource for viewing aerial photogra- France or Germany), then select a region. Check out\nphy of Europe during World War II. Open your Web some of the countless historic photos that TARA has\nbrowser and go to http:\/\/aerial.rcahms.gov.uk and archived online.\nselect the option for TARA, which contains imagery\nPost\u2013World War II aerial photography remained critical for military appli-\ncations, especially for spy planes. The 1950s saw the development of the U-2\nspy plane, capable of flying at 70,000 feet and photographing the ground be-\nlow. The U-2 was used for taking aerial photos over the Soviet Union, but also UAV unmanned\naerial vehicle\u2014a\nenabled the acquisition of imagery during the Cuban Missile Crisis, which\nreconnaissance aircraft\nshowed the placement of missile launchers at key points in Cuba in O ctober\nthat is piloted from\n1962 (Figure 9.4). Spy plane technology continued to advance with the the ground via remote\ndevelopment of the Lockheed SR-71 Blackbird, which could fly at 80,000 feet control.\nor higher, while traveling at more than three times the speed of sound\n( Figure 9.5 on page 274), thus enabling it to go higher and faster than the\nU-2. The Blackbird was an essential aerial photography plane through the FIGURE 9.4 Aerial\nimagery of Cuba from\nCold War and b eyond, until it was retired in the late 1990s.\n1962, showing missile\nToday, digital aerial photography (among many other types of remotely\nfacilities during the Cuban\nsensed data) is obtained by unmanned aerial vehicles (UAVs). These planes Missile Crisis. (Source: U.S.\nAir Force Photo) 274\nChapter 9 Remotely Sensed Images from Above\nFIGURE 9.5 The SR-71\nBlackbird in action. (Source:\nUSAF\/Judson Brohmer)\nare piloted from the ground via remote control and are currently being used\nby the military for aerial reconnaissance in Iraq and Afghanistan. There are\nnumerous types of UAVs, ranging from smaller planes like the Predator to\nlarger ones such as the RQ-4 Global Hawk (Figure 9.6). Outside of military\nuses, aerial photography is extensively used by state and local governments\nor private companies for data collection (see Hands-on Application 9.2: The\nNational Aerial Photography Program for an example of an online government\naerial photo archive).\nFIGURE 9.6 An RQ-4\nGlobal Hawk Unmanned\nAerial Vehicle. (Source: U.S.\nAir Force photo\/Stacey Knott) 275\nWhat Are the Different Types of Aerial Photos?\nThinking Critically with Geospatial Technology 9.1\nHow Can UAVs Be Used for Security Purposes?\nUAVs are not limited to military operations. Aircraft else can UAVs be utilized for functions such as bor-\nthat can be piloted via remote control and equipped der security or law enforcement? How could police\nwith multiple types of aerial surveillance equip- or federal agents utilize UAV capabilities for gath-\nment have a variety of other uses. For example, ering intelligence about dangerous situations or\nan article in the January 2010 issue of Popular Sci- acquiring reconnaissance before entering an area?\nence ( available online at http:\/\/www.popsci.com\/ Also, if UAVs are being used to remotely monitor ci-\ntechnology\/article\/2010-01\/british-police-monitor- vilian conditions, what potential is there for abuse\ncivilians-uavs-2012) indicates that the British of these types of data-collection platforms? Is UAV\npolice intend to use UAVs for security purposes, surveillance in a domestic urban environment an\nespecially during the 2012 Olympics in London. How invasion of privacy or not?\nHands-on Application 9.2\nThe National Aerial Photography Program\nThe National Aerial Photography Program (NAPP) example, try New York City, NY). Next, select the Data\nwas a federal program sponsored by several agen- Sets tab and choose the datasets you want to search\ncies (including the U.S. Department of Agriculture (in this case, expand the Aerial Photography option\nand the U.S. Department of the Interior) to provide and select NAPP). In the third option, Additional Cri-\nregular aerial photography of the United States. teria, you could add other information to your search\nNAPP imagery consists of 1-meter-resolution pho- request to help narrow it down (if needed). Lastly,\ntography and is now available online via the USGS\u2019s click on Results to see what NAPP imagery is avail-\nEarthExplorer utility (also used back in Hands-on able for the place you\u2019ve searched for. A thumbnail\nApplication 5.3: USGS Digital Line Graphs). Open preview of the images will be available that you can\nyour Web browser and go to http:\/\/edcsns17. click on to expand so that you can view the image.\ncr.usgs.gov\/NewEarthExplorer\/ to begin (make The USGS requires users to be logged in with an ac-\nsure your computer can accept cookies for the Web- count before you can access download options for\nsite to load properly). the photos. Use EarthExplorer to view what NAPP\nUsing Earth Explorer is a four-step process. First, imagery is available for your local area or home\nenter your search criteria, such as a place name (for address, and then examine the results.\nWhat Are the Different Types\nof Aerial Photos?\nWhen you\u2019re flying in a plane and look out the window, you\u2019re looking down vertical photo an\non the landscape below. If you had a camera and took a picture straight down aerial photo in which\nthe camera is looking\nfrom under the plane, you\u2019d be capturing a vertical photo. Take a look back at\ndown at a landscape.\nFigure 9.4\u2014the Cuban landscape as seen by the U-2 is photographed like you 276\nChapter 9 Remotely Sensed Images from Above\nwere looking down from the plane. Many aerial photos are vertical (although\nnadir the location\nthere may be some minor tilting from the camera). The spot directly under the\nunder the camera in\naerial photography. camera is referred to as nadir.\nAerial photos can also be panchromatic or color. Panchromatic imagery\npanchromatic black-\nis only capturing the visible portion of light in its entirety. As a result, pan-\nand-white aerial\nimagery. chromatic aerial photos will be grayscale (that is, black and white\u2014like the\nCuban aerial photo). Color imagery is capturing the three main bands of vis-\nible light\u2014red, green, and blue\u2014and the colors are composited together in\nthe digital imagery (or film) as a true color composite (we\u2019ll discuss much\nmore about light and composites in Chapter 10).\nCIR photo color A distinctive type of color aerial photo is a color infrared (CIR) photo. A\ninfrared photo\u2014a CIR image is captured with a special type of film sensitive to infrared light.\nphoto where infrared Infrared energy is invisible to the naked eye, but special film processes can\nreflection is shown\ncapture it. In order for us to see infrared energy reflection, a special filter is\nin shades of red, red\napplied to the film and the end result is a CIR photo, wherein near-infrared\nreflection is shown\nin shades of green, (NIR) reflection is displayed in the color red, red reflection is displayed with\nand green reflection the color green, and green reflection is displayed in the color blue. Blue re-\nis shown in shades of flection is not shown in the image, as it is blocked by the filter and displayed\nblue.\nas black (Figure 9.7).\nBeing able to examine CIR imagery is very useful in environmental stud-\nies, for instance, to use the infrared reflection to examine the relative health\nof vegetation. For example, examine the CIR aerial photo of Burley, Idaho, in\nFigure 9.8. The bright red sections of the image are indicative of areas with a\nhigh amount of NIR reflection (since NIR reflection is being shown with the\ncolor red on the image), likely grass, trees, or healthy agricultural fields (all\nelements that would be reflecting large amounts of NIR light). The river ap-\npears black since water heavily absorbs all types of light, except the color blue\nFIGURE 9.7 How the Observed Actual\nactual reflection of colors wavelength\nblue, green, red, and\nNIR light appears in its\nBlack Blue\ncorresponding colors in a\nCIR image.\nBlue Green\nGreen Red\nNear\nRed\ninfrared 277\nWhat Are the Different Types of Aerial Photos?\nFIGURE 9.8 A CIR aerial\nimage of Burley, Idaho.\n(Source: NASA Airborne\nScience Program image\nacquired by NASA ER-2\naircraft 26 August 1983)\n(which is then blocked in the CIR photo and shows up as black). Developed\nareas appear as a white or cyan color, the same with barren lands that have\nbeen harvested or not yet planted. principal point the\nNo matter what type of aerial photo is being used, the big problem is that center point of an aerial\nthe photo cannot be used as a map. Even though it shows all the features you photo.\nmight need (like roads or footprints of buildings) you can\u2019t rely on the photo relief displacement\nto be a map for one reason\u2014the photo doesn\u2019t have the same scale at every the effect seen in aerial\nspot on the image. As a plane flies over the surface, some terrain features will imagery where tall\nitems appear to \u201cbend\u201d\nbe closer to the camera and some will be further away\u2014and thus, some areas\noutward from the\nwill have a larger or smaller scale than others. When looking down, the center\nphoto\u2019s center toward\nof the photo is referred to as its principal point. In aerial photos, you\u2019ll see\nthe edges.\ntall objects (such as terrain relief, towers, or buildings) having a tendency to\northophoto an aerial\n\u201clean\u201d away from this center point toward the edges of the photo\u2014this e ffect\nphoto with uniform\nis called relief displacement. See Figure 9.9 on page 278 for an example of\nscale.\nthis\u2014if you look at the photo\u2019s center, it\u2019s like looking straight down at that\northorectification a\narea. However, if you look outward from the principal point, you\u2019ll see that the\nprocess used on aerial\ntall buildings seem to be \u201cleaning\u201d away from the center of the photo (rather\nphotos to remove\nthan looking straight down on all the buildings in the image). the effects of relief\nRegular aerial photos do not have uniform scale, but a different type displacement and give\nof photos called orthophotos do. To create an orthophoto, a process called the image uniform\nscale.\northorectification is performed on a regular aerial photo to remove the 278\nChapter 9 Remotely Sensed Images from Above\nFIGURE 9.9 The effect\nof relief displacement in\nan aerial photo. (Source:\naerial photograph by VTN\nConsolidated, Inc. courtesy of\nJ. Van Eden)\nPrincipal\nPoint\neffects of terrain and relief displacement and enforce the same scale across\nthe photo. A regular aerial photo can\u2019t be used as a map since it doesn\u2019t have\nuniform scale everywhere, but an orthophoto can. Even with uniform scale,\nsome of the side appearances of objects can still be seen since, when the photo\nwas taken, some of the sides of the building may have been hidden due to re-\nlief displacement and not visible in the photo. This effect is removed in a true\ntrue orthophoto an orthophoto, which gives the appearance of looking directly straight down on\northophoto where all objects in the image. This is achieved by using several images of the area\nall objects look as if\nfor filling in the missing information, while keeping constant scale in the final\nthey\u2019re being seen from\nphoto.\ndirectly above.\nOrthophotos are used to create special products called Digital Ortho-\nDOQ a Digital\nphoto Quads, or DOQs. A DOQ is orthophotography that covers 3.75 minutes\nOrthophoto Quad.\nof latitude and 3.75 minutes of longitude of the ground, the same area as\nOrthophotos that cover\none-fourth of a 7.5 minute USGS topographic map (see Chapter 13 for more\nan area of 3.75 minutes\nof latitude by 3.75 about topographic maps). DOQs (sometimes referred to as DOQQs, or Digital\nminutes of longitude, Orthophoto Quarter Quads) have been georeferenced (see Chapter 3 for more\nor one-fourth of a 7.5 information on georeferencing) so that they can be matched up with other\nminute USGS quad.\ngeospatial datasets. Also note that the USGS produces certain other DOQ\nproducts that cover the area of the entire quad. See Hands-on Application 9.3:\nUsing MSR Maps for Viewing Aerial Images for an online application that is\nused for viewing aerial images. 279\nWhat Are the Different Types of Aerial Photos?\nHands-on Application 9.3\nUsing MSR Maps for Viewing Aerial Images\nMSR Maps (formerly TerraServer-USA) is an online have been pre-selected as \u201cFamous Places\u201d\u2014select\nutility sponsored by the USGS and Microsoft for that option and view aerial\/DOQ imagery of places\nviewing aerial imagery. Open your Web browser like Alcatraz, Three Mile Island, or Walt Disney World.\nand go to http:\/\/msrmaps.com to get started. MSR When you\u2019re done perusing aerial images of famous\nMaps allows you to search for a specific place, an ad- locales, search for your home address or other l ocal\ndress, or a set of latitude and longitude coordinates places and see what kinds of aerial imagery MSR\nand view available aerial imagery for that location. Maps has to offer.\nAlternatively, some landmarks around the world\nIn an oblique photo, the camera is tilted so that it\u2019s not positioned directly oblique photo an\nat nadir, but rather at an angle. Take a look at Figure 9.2 again for an example aerial photo taken at an\nangle.\nof this\u2014the image of San Francisco taken from the kites. Unlike a vertical pho-\nto, you can see the sides of some of the buildings, and you\u2019re really seeing the\ncity from a perspective view instead of an overhead one. Oblique images allow\nfor very different views of a target rather than from straight overhead and can\ncapture other types of information (such as the sides of buildings or moun-\ntains). Law enforcement agencies could make use of oblique photos of a build-\ning to obtain detailed information of all sides of a building (allowing a view of\nentrances, exits, windows, and so on) before sending police into a potentially\ndangerous situation. Companies such as Pictometry extensively collect high-\nresolution oblique aerial imagery for sale. You can view oblique images of sev-\neral locations online using Microsoft\u2019s free Bing Maps utility if its \u201cBird\u2019s eye\u201d\nimagery is available (see Figure 9.10 and Hands-on Application 9.4: Oblique\nImagery on Bing Maps for examples of oblique images created on Bing Maps).\nFIGURE 9.10 Oblique\nimagery of the Lincoln\nMemorial, Washington,\nD.C., as shown in Bing\nMaps\u2019 \u201cBirds\u2019s eye view\u201d\nfeature. (Source: Pictometry\nBird\u2019s Eye \uf6d9 2010 Pictometry\nInternational Corp \uf6d9 AND\n\uf6d9 2010 NAVTEQ \uf6d9 2011\nMicrosoft Corporation) 280\nChapter 9 Remotely Sensed Images from Above\nHands-on Application 9.4\nOblique Imagery on Bing Maps\nBing Maps is a great source of viewing online Reposition the images so you can see the\noblique imagery supplied via pictometry. Like boardwalk and oceanfront at an oblique angle\nGoogle Earth and Google Maps, Bing Maps allows and then move down the boardwalk to see the\nyou to view aerial or satellite images of areas but casinos, piers, and developments. In addition,\nalso gives you the option to view high-resolution Bing Maps provides oblique imagery from mul-\noblique images of areas (where oblique imagery is tiple angles\u2014click on one of the arrows pointing\navailable). To start, open your Web browser and go downward to the left and right of the compass\nto http:\/\/www.bing.com\/maps and type the name rose (around the letter \u201cN\u201d) to rotate the direc-\nof a location (for instance, Atlantic City, New Jersey). tion and load oblique imagery of the same loca-\nIn the Atlantic City map, zoom in until you can see tion but shot from a different angle. You can use\nthe ocean and boardwalk. From the Road pull-down this function to examine a spot from multiple\nmenu, select the option for \u201cBird\u2019s eye\u201d and be sure viewpoints. Once you get used to navigating and\nthere\u2019s a checkmark in the \u201cshow angled view\u201d box. examining \u201cBird\u2019s eye\u201d oblique images of the New\nThe view will switch to oblique photography of At- Jersey shore, search for your local area with Bing\nlantic City. You can zoom in and out and pan the im- Maps to see if Bird\u2019s eye imagery is available to\nagery (when you move past the edge of a photo, a view your home, school, or workplace.\nnew tile of oblique imagery should load).\nHow Can You Interpret Objects\nin an Aerial Image?\nUnderstanding what you\u2019re looking at when viewing Earth from the sky in-\nstead of the ground takes a completely new skill set. Objects that you\u2019re used\nto looking at head-on have a whole different appearance from above. For ex-\nample, Figure 9.11 shows two different views of the Stratosphere Hotel and\nCasino in Las Vegas, Nevada\u2014one a regular oblique photograph showing\ndetails of the tower and its buildings and the second photo showing an over-\nhead aerial view. Obviously, the complex looks completely different from\nthe ground than from directly above. If you look closely, you can see where\nareas in the two photos match up, such as the tower on the right-hand side\nof the image and the remainder of the hotel and casino on the left. How-\never, if you were only given the aerial image and asked which building (of\nall possible structures in the world) this was, it would be a whole different\nballgame.\nFrom the aerial image alone, you\u2019d have to search for clues to determine\nwhat was really being shown. For instance, based on the size of the complex\n(roughly a city block) you might guess that it was some sort of hotel, resort,\nentertainment attraction, or museum. By looking at its location amidst other\nlarge buildings and multiple-lane roads, you could guess that the building 281\nHow Can You Interpret Objects in an Aerial Image?\n(a) (b)\nwas in a big city. Looking closely, you can make out a different colored o bject\nFIGURE 9.11 The\n(the light blue) on the main complex and determine that it\u2019s a swimming Stratosphere Hotel and\npool, further narrowing down the building to some sort of hotel or resort. Casino in Las Vegas,\nNevada: (a) an overhead\nAlthough the height of the tower on the right-hand side of the image isn\u2019t\naerial view of the building\nreally discernible (since the image was taken looking down onto the tower\nand (b) an oblique image\nitself), the huge shadow being cast by the tower (stretching right to left) is showing the tower and\nvisible. Based on the length of the shadow (and the shape of the shadow the hotel layout. (Sources:\nbeing cast), you could determine that object was a tower (or a similar very (a) Bing Map \uf6d9 2011\nMicrosoft Corporation Image\nlarge, very tall object). Putting all of these clues together (city-block-sized\ncourtesy of the Nevada\nhotel\/resort in a big city with a massive tower in front of it), you would be State Mapping Advistory\nable to use some outside information (like examining some travel books or a Committee. Esri\u00ae ArcGIS\nshort Web search) to quickly determine that the image is of the Stratosphere ArcExplorer graphical user\ninterface Copyright \uf6d9 Esri.\nHotel in Las Vegas.\n(b) Tim Jarrett\/Wikipedia)\nWhen you try to interpret features in an aerial photo or other remotely\nsensed satellite image (for instance, objects in developed areas or physical\nfeatures on the landscape), you act like a detective searching for clues in\nthe image to figure out what you\u2019re really looking at. Like in the Strato-\nsphere Hotel example, clues like the size and shape of objects or the shad-\nows being cast aid in determining what you\u2019re really looking at. Visual\nimage interpretation is the activity of identifying features in an aerial\nvisual image\n(or other remotely sensed) image based on several distinct elements as interpretation the\nfollows: process of discerning\ninformation to identify\na Pattern: This is the physical arrangement of objects in an image. The pat- objects in an aerial (or\ntern of how objects are lined up (or disarrayed) will often aid in inter- other remotely sensed)\npreting an image. A large series of cars uniformly set up in a parking lot image.\naround a relatively small building would likely be a clue indicating a car pattern the\ndealership rather than another type of shopping area. Evenly spaced air- arrangement of\nplanes or jets may be a clue to identifying a military base, while a haphaz- objects in an image.\nAn element of image\nard arrangement of aircraft could indicate a group of statics on display at\ninterpretation.\nan aircraft or military museum. 282\nChapter 9 Remotely Sensed Images from Above\na Site and Association: Site represents the location characteristics of an\nsite and association\nitem, while association represents relating an object in an image to oth-\nthe information\nreferring the location er nearby features in the image. For example, a football field itself has\nof objects and their enough distinctive features to identify it, but the related phenomena you\nrelated attributes in could see in the image (the number of bleachers, the number and distri-\nan image. Used as\nbution of the seats, and perhaps the amount of nearby parking) would\nelements of image\nhelp in determining if you\u2019re looking at a field used by a high school team,\ninterpretation.\na 1-AA college team, or a professional NFL team.\nsize the physical\na Size: This is information about the length and width of objects in the im-\ndimensions (such as\nlength and width) of age. The relative size of objects in an image can offer good clues in visual\nobjects. An element of image interpretation. For instance, the average length of a car is about\nimage interpretation. 15 feet. If a car is present in an image, you could compare the length of a\ncar to the length of other objects to quickly gain information if a structure\nis the size of a house or a shopping center. Similarly, some sizes remain\nconstant throughout images. If objects such as a baseball diamond or a\nfootball field are present, elements in them (the 90 feet between bases or\nthe 100 yards between goal posts) can be compared to other objects in a\nphoto to gain relative size information.\nshadow the shadings a Shadow: This is the shading cast by light shining onto an object. Shadows\nin an image caused also help provide information about the height or depth of the objects that\nby a light source. An are casting the shadows. For instance, in the Stratosphere Hotel example,\nelement of image\nthe height of the tower itself was partially hidden due to the nature of\ninterpretation.\nbeing photographed from above, but the fact that the object was a very\ntall structure was evident from the shadow cast by the tower. Shadows\ncan also help in determining objects that would be near-indistinguishable\nfrom looking down on them, such as the shadows cast by things like rail-\nings or telephone poles.\nshape the form of a Shape: This is the form of objects in an image. The distinctive shapes of ob-\nobjects. An element of jects in an aerial photo can greatly aid in their interpretation. The diamond\nimage interpretation.\nshape of a baseball field will help to quickly identify it, or the circular shape\nof crops may indicate the presence of center-pivot irrigation in fields. A rac-\ning track has a distinctive oval shape, and even an abandoned horse race\ntrack may still show evidence of the oval shape on the landscape.\ntexture repeating a Texture: This refers to the differences of a certain tone throughout parts\ntones in an image. of the image. The texture of objects can be identified as coarse or smooth.\nAn element of image For instance, different types of greenery can be quickly distinguished\ninterpretation.\nbased on texture\u2014a forest of trees and a field of grass may have the same\ntone, but the trees appear very rough in an image, while grass will look\nvery smooth. The texture of a calm lake will look very different from the\nrocky beach surrounding it.\ntone the grayscale\na Tone: This is the grayscale (black to white) or intensity of a particular col-\nlevels (from black to\nor of objects in an image. The tone of an object can help discern important\nwhite) or ranges of\na color for objects information about items in a photo. Like in the Stratosphere example, the\npresent in an image. light-blue color of the swimming pool made identifying it easy. Similarly,\nAn element of image a wooden walkway extending into a sandy beach will have different tones\ninterpretation.\nthat help in distinguishing them from one another in a photo. 283\nHow Can You Interpret Objects in an Aerial Image?\nFIGURE 9.12 An\noverhead view of an\narea containing several\ndifferent objects. (Source:\nwww.satimagingcorp.\ncom, Quickbird Satellite\nSensor, February 2002 \uf6d9\n2007\u2014DigitalGlobe All rights\nreserved.)\nThese same elements can be used to identify items in satellite images as\nwell. Figure 9.12 is an overhead view of an area containing several objects\nthat all add up to represent one thing. We\u2019ll apply these elements to this image\nto determine what is being shown in the image.\na Pattern: The three main objects are arranged in a definite diagonal line.\na Texture: The area around the three objects is of very fine texture, com-\npared with some of the rougher textures nearby, especially in the east.\na Tone: The area around the three objects is much lighter than the area\nof rougher texture (and that area\u2019s probably a town or city of some sort,\ngiven the pattern of the objects in this range). Given the tone and texture\nof the ground around the three objects, it\u2019s likely not smooth water or\ngrass or concrete, so sand is a likely choice.\na Site and association: The three objects are located in a large sandy area,\nadjacent to a city, with what resembles roads leading to the objects.\na Size: Comparing the size of the objects to the size of the buildings indi-\ncates that the objects are very large\u2014even the smallest of the three is\nlarger than many buildings, while the largest is greater in size than blocks\nof the city.\na Shadow: The shadows cast by the objects indicate that not only are they\ntall, but they have a distinctive pointed shape at their top. 284\nChapter 9 Remotely Sensed Images from Above\na Shape: The objects are square at the base, but have pointed, triangular\ntops, making them pyramid shaped.\nBringing all of these elements of interpretation together, we can find\nthree pyramids of different sizes (although still very large), in a definite fixed\narrangement, in sand near a busy city. All of these clues combine to identify\nthe Great Pyramid of Giza in Cairo, Egypt. Aerial images often contain clues\nto try and determine the identity of the objects they contain, or to at least nar-\nrow down the options to a handful that could be determined through some\nbrief research through other collateral material (like investigating groupings\nof large desert pyramids in that particular arrangement).\nVisual image interpretation skills enable the viewer to discern more in-\nformation about the nature of objects within a remotely sensed image. The\nelements of interpretation can be used for closer investigation to identify (for\ninstance) the specific type of fighter jet that can be seen on a runway based\non the size and shape of its features, wingspan, engines, and armaments. A\nforestry expert could use visual image interpretation to determine what type\nof tree stand is being examined in a photo based on features associated with\nthe trees.\nHow Can You Make Measurements\nfrom an Aerial Photo?\nOnce identification of objects in an image has been positively made, these\nphotogrammetry the objects can be used for making various types of measurements. Photogram-\nprocess of making metry is the process of obtaining measurements from aerial photos. Photo-\nmeasurements using\ngrammetric techniques can be used for determining things like the height\naerial photos.\nand depth of objects in an aerial photo. For instance, the lengths of visible\nfeatures in a photo or heights of buildings seen in the photo can be calcu-\nlated. There are a lot of possible photogrammetric measurements, and the\nfollowing are two simple examples of how these types of measurements can\nbe made.\nphoto scale the Just like the map scale discussed in Chapter 7, every photo has a photo\nrepresentation used to scale, listed as a representative fraction. For example, in a 1:8000 scale\ndetermine how many aerial photo, one unit of measurement in the photo is equivalent to 8000 of\nunits of measurement\nthose units in the real world. Using the photo scale, you can determine the\nin the real world are\nreal-world size of features. For instance, say you measure a length of rail on\nequivalent to one unit\nof measurement on an an aerial photo to be a half-inch and the photo scale is 1:8000. Thus, 1 inch\naerial photo. in the photo is 8000 inches in the real world, and so a measurement of 0.5\ninches in the photo would be 4000 inches (0.5 times 8000), or 333.33 feet,\nin length.\nThe scale of a photo relies on the focal length of the camera\u2019s lens and\nthe altitude height of the plane when the image is taken. The problem comes\nwhen you\u2019re trying to make measurements from an orthophoto but you don\u2019t\nknow the photo scale. Without knowing how many real-world units equate to 285\nHow Can You Make Measurements from an Aerial Photo?\none aerial photo unit, measurements can\u2019t be accurately made. An unknown\nphoto scale (of a vertical photo taken over level terrain) can be determined by\nusing a secondary source that has a known scale, so long as an item is visible\nin both the photo and the source with the known scale (and that you would\nbe able to measure the item in both). A good secondary source would be a\ntopographic map (see Chapter 13) with a known scale since it would contain\nmany features that could also be clearly seen in an aerial photo (such as a\nroad where the beginning and ending are visible). Be cautioned when making\nthese types of measurements, as a regular aerial photo will not have the same\nscale everywhere in the photo, whereas an orthophoto does have uniform\nscale across the image.\nBy being able to make the same measurement on the map (where the\nscale is known) and on the aerial photo (where the scale is unknown), you can\ndetermine the photo scale. This works as follows\u2014your photo could be 1:6000\nor 1:12000 or 1: \u2018\u2018some number.\u201d Just like the representative fraction (RF)\ndiscussed in Chapter 7, the photo scale can be written as a f raction\u20141:24000\ncan be written as 1\/24000. So, assume your unknown photo scale is the\nRF\u2014this is equal to the distance measured on the photo, known as the photo\ndistance (PD), divided by the real-world distance measured on the ground,\nknown as the ground distance (GD), or:\nPD\nRF (cid:2)\nGD\nSay, for example, you have a 1:12000 scale map showing an oceanfront\nboardwalk and you also have an orthophoto of unknown scale showing the\nsame region. For the photo to be useful, you have to determine its scale. You\ncan find the same section of boardwalk in both the map and the photo. By\nmeasuring the boardwalk section on the map, you find it is 0.59 inches. How-\never, that\u2019s not the ground distance, or how long that section of the boardwalk\nis in the real world\u2014because of the map scale, every one inch measured on\nthe map translates into 12,000 inches in the real world. So, 0.59 inches on the\nmap is actually 7080 inches in the real world. This measure is the GD variable\nin the equation above (the ground distance):\nPD\n(cid:2)\nRF\n7080 in\nYou can measure the same section of boardwalk on the photo and find\nthat it\u2019s 1.77 inches. This measurement is the PD variable in the equation (the\nphoto distance):\n1.77 in\nRF (cid:2)\n7080 in\nDoing some quick division, you find that RF is equal to 1\/4000.\n1\nRF (cid:2)\n4000 286\nChapter 9 Remotely Sensed Images from Above\nSo the photo scale is 1:4000. One unit measured on the photo is equal to\n4000 units in the real world.\nA second type of measurement that you can make from the elements\nfound in an aerial photo is the ability to accurately calculate the height of an\nobject in the photo simply by examining its shadow in the photo. At first blush,\nit seems you\u2019d need to know all sorts of other information\u2014where the photo\nwas taken, what time of day it was taken, the date on which it was taken\u2014\nall variables related to the relative location of the Sun and how the shadows\nwould be cast. Chances are you wouldn\u2019t be able to easily get your hands on a\nlot of this type of information, so you\u2019re probably thinking that there must be\na better way to do this.\nYou\u2019d be right\u2014photogrammetric measurements give you a much sim-\npler way of determining the heights of objects in a photo from their shadows\nwithout needing all that other data. It relies on three things: (1) knowing the\nscale of the photo (which we just figured out); (2) being able to clearly see the\nfull shadow (from the top of the object) on level ground of all objects whose\nheights you want to measure; and (3) already knowing the height of one ob-\nject with a shadow you can measure. With these things, measuring heights is\na snap.\nLet\u2019s take that hypothetical 1:4000 photo from the last example and as-\nsume that it\u2019s got a number of large hotels casting shadows on the boardwalk.\nYou know the height of one building (115 feet) and can measure its shadow\nin the photo (from the base to the top) to be 0.10 inches. You can use this\ninformation to calculate the angle of the Sun, which is casting the shadows in\nthe photo as follows:\nh\ntan a (cid:2)\nL\nIn this equation, a is the angle of the Sun, h is the real-world height of the\nobject, and L is the real-world length of the shadow. From basic trigonometry,\nthe tangent of a right angle (tan a) is equal to its opposite value (h) divided by\nits adjacent value (L). See Figure 9.13 for a diagram of how this works. You al-\nready know the height, h (115 feet). The length, L, can be found by taking the\nheight of the shadow measured in the photo (0.10 inches) and multiplying by\nthe photo scale (1:4000). It turns out that 0.10 inches on the photo would be\n400 inches in the real world, or 33.33 feet. This means if you were to measure\nthe building\u2019s photo by actually going to the boardwalk, its shadow would be\n33.33 feet long. Plugging these numbers into the formula, we find that:\n115 ft\ntan a (cid:2)\n33.33 ft\nand that the tangent of angle a (or \u201ctan a\u201d) is equal to 3.45.\nNow, since an aerial photo represents a single snapshot in time, we can\nassume that the angle of the Sun is going to remain the same across the photo\nfor all of the buildings casting shadows. So, the measure for \u201ctan a\u201d will be the\nsame value when applied to all heights we\u2019re trying to determine. We can use 287\nChapter Wrapup\nFIGURE 9.13 The\nrelationship between the\nheight of an object (h),\nthe length of its shadow\n(L), and the angle of the\nSun\u2019s\nrays\nSun\u2019s rays.\nLength of shadow\nthis information to calculate the height of any object in the photo casting a\nshadow we can measure. Say you measure the shadow cast by a second build-\ning in the photo on the boardwalk and find it to be 0.14 inches in length. Us-\ning the 1:4000 photo scale, this shadow in the real-world would be 560 inches\nlong, or 46.67 feet in length. The only thing that\u2019s unknown now is the actual\nheight of the building. Using the equation:\nh\n3.45 (cid:2)\n46.67 ft\nand solving for h, we can find that the height of the new building is about\n161 feet. This type of measurement can be applied to other objects casting\nshadows, allowing one to quickly determine the heights of multiple objects in\na single photo. Again, be cautious on how you employ these techniques\u2014for\nfinding heights because of shadows, the object must be straight up and down\nwhile casting a full shadow on level ground.\nChapter Wrapup\nAerial photography has a wide variety of uses and is an integral part of geo-\nspatial technology. Aerial photos are used for interpretation, photogramme-\ntry, and additional data sources for other aspects of geospatial technology,\nsuch as GIS. Despite its myriad of uses and applications, aerial photography\nis just one part of remote sensing. The next chapter will delve into how the\nwhole remote sensing process actually works.\nThis chapter\u2019s lab will put you to work with several remotely sensed im-\nages and applying elements of visual image interpretation to items in the\nimages.\nImportant note: The references for this chapter are part of the online com-\npanion for this book and can be found at http:\/\/www.whfreeman.com\/\nshellito1e.\ntcejbo\nfo\nthgieH 288\nChapter 9 Remotely Sensed Images from Above\nKey Terms\nremote sensing (p. 269) DOQ (p. 278)\naerial photography (p. 270) oblique photo (p. 279)\nUAV (p. 273) visual image interpretation (p. 281)\nvertical photo (p. 275) pattern (p. 281)\nnadir (p. 276) site and association (p. 282)\npanchromatic (p. 276) size (p. 282)\nCIR photo (p. 276) shadow (p. 282)\nprincipal point (p. 277) shape (p. 282)\nrelief displacement (p. 277) texture (p. 282)\northophoto (p. 277) tone (p. 282)\northorectification (p. 277) photogrammetry (p. 284)\ntrue orthophoto (p. 278) photo scale (p. 284) 9.1\nGeospatial Lab Application\nVisual Imagery Interpretation\nThis chapter\u2019s lab will get you started about thinking about how objects\nappear from the sky rather than the ground. You\u2019ll be examining a series\nof images in terms of the elements of visual image interpretation from the\nchapter\u2014think of it like being a detective and searching the image for clues\nto figure out just what it is you\u2019re looking at. You should be able to figure\nout what, exactly, you\u2019re looking at, or to at least narrow down the objects\nto a short list before using other data to help in determining what you\u2019re\nlooking at.\nAlthough the items you\u2019ll be examining are all large and prominent\no bjects, buildings, or other features, the application of visual image interpre-\ntation elements for these simple (and hopefully, fun) examples will help get\nyou started with looking at the world from above.\nObjectives\nThe goals for you to take away from this exercise are:\na Thinking of how objects look from an aerial perspective.\na Applying the elements of visual image interpretation (as described in the\nchapter) in order to discern the identity of objects in the images.\nObtaining Software\nThere is no special software used in this lab, aside from whatever program\nyour computer uses to view graphics and images. You may find Google Earth\nuseful during the lab, however.\nLab Data\nCopy the folder \u2018Chapter9\u2019\u2014it contains a series of JPEG images (.jpg files)\nthat contain items you will be attempting to interpret. These are numbered,\nand tend to increase in difficulty of interpretation as the numbers go up.\nLocalizing This Lab\nThe images used in this lab were taken from a variety of locations around\nthe United States. You could use Google Earth, MSR Maps, or Bing Maps to\nlocate good overhead views of nearby areas (showing prominent developed\nor physical features) to build a dataset of local imagery to use for visual image\ninterpretation.\n289 290\nChapter 9 Remotely Sensed Images from Above\n9.1 Applying Elements of Visual Image Interpretation\nTake a look at each one of the images (there are questions related to each\nfurther down) and try to determine just what it is you\u2019re looking at. Although\neveryone\u2019s application of the elements may vary, there are some guidelines\nthat you may find useful in interpreting the images.\nSeveral images may contain multiple items, but all of them work together\nto help define one key solution.\n1. We\u2019ll start with a sample image (a digital copy of this sample image is\nalso in the \u2018Chapter 9\u2019 folder labeled \u201csample\u201d):\n(Source: Bing Map \uf6d9 2011 Microsoft Corporation. Esri\u00ae ArcGIS ArcExplorer graphical user interface Copyright\n\uf6d9 Esri.)\n2. First, look at the image as a whole for items to identify:\na. The central object is obviously some sort of structure, and a large\none at that, judging from the shadow being cast. The relative size of\nthe structure (when compared to the size of several of the cars seen\nthroughout the image) helps to likely identify it as a building and\nnot a monument of some kind.\nb. It\u2019s set on a body of water (as seen by the tone of the water being\ndifferent from the tone of the nearby concrete or greenery). Thus,\nthe site can be fixed.\nc. One of the most notable parts is the large triangular shape of\nthe main portion of the building as well as its texture being very\ndistinctive from the rest of the building (the three white sections).\nd. The shape of the entranceway is also very distinctive, being large\nand round with a couple of concentric circles.\n3. Second, take the interpretation of the initial items and start looking for\nspecifics or relationships between the items in the image:\na. There\u2019s something about the pattern of those concentric circles\nat the entrance plaza that\u2019s striking. Two outer rings and a third 291\nVisual Imagery Interpretation\ninner ring with some sort of a design placed in it. Judging from\nthe relative size of a person (seen by picking out shadows near the\nplaza or on the walkway along the water), the plaza is fairly large.\nThe pattern sort of resembles a giant vinyl record album (and the\nconcept solidifies further with the shape of the large curved area\nthat follows the plaza on its right that looks like the arm of an old\nvinyl record player).\nb. The texture of the triangular portions of the building looks like they\ncould be transparent, as if that whole portion of the structure was\nmade of glass.\nc. The association of the area shows no parking lot right next to\nthe building, indicating that you\u2019d have to park somewhere else\nand walk there. Likewise, there are no major roads nearby, again\nindicating this is some sort of destination you can\u2019t just drive up to\nand park in front of. Situated on an (apparently) large body of water\ngives a feel that this is in a large city somewhere.\n4. Third, put the clues together and come up with an idea of what the\nobject or scene could be:\na. The building\u2019s distinctive enough to be a monument, but too large to\nprobably be one. It\u2019s likely some sort of museum, casino, shopping\ncenter, or other attraction. However, the building\u2019s too small to be\na casino or resort and lacks the necessary parking to be a casino,\noffice, or shopping center.\nb. The very distinct \u201crecord\u201d motif of the front plaza seems to\nindicate that music (and more specifically older music, given the\nwhole \u201cvinyl record\u201d styling) plays a large part in whatever the\nbuilding is.\nc. Thus, from the clues in the image, this could be some sort of music\nmuseum, like the Rock and Roll Hall of Fame, located in Cleveland,\nOhio, right on the shore of Lake Erie.\n5. Fourth and final, use a source to verify your deduction, or to eliminate\nsome potential choices.\na. By using Google Earth to search for the Rock and Roll Hall of Fame,\nthe image would be confirmed. Doing an image search on Google\ncan also turn up some non-aerial pictures of the site to help verify\nyour conclusion as well.\nNot all images will be this extensive, and not all may make use of all\ne lements\u2014for instance, there may be one or two items in the image that will\nhelp you make a quick identification of it. However, there are enough clues\nin each image to figure out what you\u2019re looking at\u2014or to at least narrow the\nchoices down far enough that some research using other sources may be able\nto narrow it down. 292\nChapter 9 Remotely Sensed Images from Above\nFor instance, by putting all of the clues together, you could start looking\nup large, prominent music museums (with triangular glass structures) and\nthe Rock and Roll Hall of Fame would be determined in very short order.\n9.2 Visual Image Interpretation\n1. To examine an image, open the \u2018Chapter9\u2019 folder and double-click on\nthe appropriate image (.jpg) file. Whatever program you have on your\ncomputer for viewing images should open with the image displayed. You\nmay find it helpful to zoom in on parts of an image as well.\n2. For each image, answer the questions presented below, and then explain\nwhich of the elements of visual image interpretation led you to this\nconclusion (and how they did). Writing \u201cshadow and shape helped\nidentify traits of the building\u201d would be unacceptable. However, writing\nsomething that would answer the question \u201cWhat was so special about\nthe shadows in the scene or the shape of items that helped?\u201d would be\nmuch better.\n3. When identifying the items, be very specific (that is, not just \u201ca baseball\nstadium,\u201d but \u201cProgressive Field\u201d in Cleveland, Ohio).\nImportant note: All images used in this exercise are from areas within the\nUnited States.\n4. You may want to consult some outside sources for extra information,\nsuch as Websites, search engines, books, or maps. Lastly, when you\nfinally think you have the image properly identified, you may want to\nuse something like Google Earth or Bing Maps to obtain a view of the\nobject in question to help verify your answer.\nQuestion 9.1 Examine \u2018image1.\u2019 There are several items in this image,\nbut there\u2019s one that\u2019s the most prominent. What, specifically, is this image\nshowing? What elements of visual image interpretation lead you to draw this\nconclusion?\nQuestion 9.2 Examine \u2018image2.\u2019 What (specifically) is being displayed in\nthis image? What elements of visual image interpretation lead you to draw\nthis conclusion?\nQuestion 9.3 Examine \u2018image3.\u2019 What (specifically) is being displayed in\nthis image? What elements of visual image interpretation lead you to draw\nthis conclusion?\nQuestion 9.4 Examine \u2018image4.\u2019 There are several items in this image,\nbut there\u2019s one that\u2019s the most prominent. What, specifically, is this image\nshowing? What elements of visual image interpretation lead you to draw this\nconclusion?\nQuestion 9.5 Examine \u2018image5.\u2019 There are many similar objects in this\nimage, but you\u2019ll notice some differences related to some of them. What\n(specifically) is being displayed in this image, and what location is this an 293\nVisual Imagery Interpretation\nimage of? What elements of visual image interpretation lead you to draw\nthis conclusion?\nQuestion 9.6 Examine \u2018image6.\u2019 There are several items in this image\nbut they all add up to one specific thing. What, specifically, is this image\nshowing, and what location is this an image of? What elements of visual\nimage interpretation lead you to draw this conclusion?\nQuestion 9.7 Examine \u2018image7.\u2019 What (specifically) is being displayed in\nthis image? What elements of visual image interpretation lead you to draw\nthis conclusion?\nQuestion 9.8 Examine \u2018image8.\u2019 There are several items in this image, but\nthey all add up to one specific thing. What, specifically, is this image showing\n(and what is its geographic location)? What elements of visual image\ninterpretation lead you to draw this conclusion?\nQuestion 9.9 Examine \u2018image9.\u2019 What feature is prominent in the image?\nBe specific as to what the area is and what geographic location is being\nshown here. What elements of visual image interpretation lead you to draw\nthis conclusion? For instance, what are all those white objects throughout\nthe image (and how do they help identify the area)?\nQuestion 9.10 Examine \u2018image10.\u2019 What feature is prominent in the\nimage? Be specific as to what the object is and what geographic location\nbeing shown here. What elements of visual image interpretation lead you\nto draw this conclusion? For instance, several airplanes are prominently\ndisplayed\u2014how do they help identify the feature?\nClosing Time\nNow that you\u2019ve gotten your feet wet with some visual image interpretation\nand have hopefully gotten used to viewing Earth from above, Chapter 10 will\ndelve into satellite imagery and its exercise will have you doing (among other\nthings) some more interpretation tasks based on satellite capabilities.\nOnce you\u2019ve identified all of the images and explained your application of\nthe elements of image interpretation, you can close the \u2018Chapter9\u2019 folder and\nany other programs or resources you have open. This page was intentionally left blank 10\nHow Remote Sensing Works\nElectromagnetic Energy, the Remote Sensing Process, Spectral\nReflectance, NDVI, Digital Imagery, and Color Composites\nWhen you use Google Earth to get an overhead view of your house, Google\nhad to get those images from somewhere, but there are a number of steps that\noccur before that image of your house can be acquired and finally turned into\na picture you can view on the computer screen. In Chapter 9, we discussed\nhow what\u2019s really being captured via remote sensing is reflected light\u2014this remote sensing the\nis a form of electromagnetic energy and a sensor is measuring the amount of process of collecting\ninformation related to\nenergy that reflects off a target. In addition, in r emote sensing the device be-\nthe reflected or emitted\ning used to obtain this information about energy r eflectance will be on some\nelectromagnetic energy\ntype of airborne or spacecraft platform that will be a considerable distance\nfrom a target by a\naway from a target on the ground below. Some remote sensing devices mea- device a considerable\nsure the energy emitted from objects on the ground (for instance, as a means distance away from\nof measuring the heat or thermal properties of an item); however, this chap- the target onboard an\nairborne or spacecraft\nter focuses on measuring the reflection of energy by a sensor.\nplatform.\nIf remote sensing is measuring the reflectance of energy, there needs\nto be a source for that energy. Luckily, we have one nearby\u2014our solar sys-\ntem\u2019s Sun provides the source of the electromagnetic energy that is being\nsensed. The Sun is almost 93 million miles away from Earth, and sunlight\ntakes about 8.3 minutes to reach Earth from the Sun. This energy from the\nSun radiates through space at the speed of light (almost 300 million me-\nters per second) to reach Earth. This energy passes through and interacts\nwith Earth\u2019s atmosphere and then reaches objects on Earth\u2019s surface. The\nenergy then interacts with those objects, and some of the energy is reflected\nback into space. That reflected energy again passes through the atmosphere\na second time on its way back off Earth and finally makes it to a sensor\neither on an airborne or spaceborne platform, where the energy is recorded\n( Figure 10.1, page 296).\nThese measurements made at the sensor are being captured as remotely\nsensed data. Thus, the remotely sensed information you see on things like\nGoogle Earth is actually the reflection of energy off targets on the ground\n295 296\nChapter 10 How Remote Sensing Works\nRemote sensing satellite Sun\nAtmosphere\nRRReeefffllleeecccttteeeddd eeennneeerrrgggyyy IIInnnccciiidddeeennnttt eeennneeerrrgggyyy\nForest Water Grass Bare soil Paved road Developed area\nFIGURE 10.1 The\nremote sensing process:\nthat has been measured, processed, and turned into imagery to view. We\u2019ll\nEnergy radiates from the\nSun, interacts with the go through each of these stages in more detail to see how you can start with\natmosphere, interacts energy from the Sun and end up with a crisp satellite image of a target on the\nwith items on the ground,\nground (also see Hands-on Application 10.1: Viewing Remotely Sensed Imagery\nthen some energy reflects\nOnline for various examples of the finished product of remote sensing).\nupward to be measured by\nthe sensor.\nHands-on Application 10.1\nViewing Remotely Sensed Imagery Online\nAn excellent tool for viewing different types of re- available from the Map Source options\u2014you can se-\nmotely sensed data in your Web browser (without lect from Bing Maps or Yahoo! Maps imagery\u2014the\nusing a separate program like Google Earth) is Flash closer you zoom in, new imagery will load for dif-\nEarth. Like the name implies, make sure you have ferent scales. Examine some of the areas around\nFlash installed on your computer\u2014then open your Manhattan to see what types of remotely sensed\nWeb browser and go to http:\/\/www.flashearth.com images are available at each o ption. When you\u2019re\nto get started. When Flash Earth starts, you can pan done looking around New York, type your home lo-\naround Earth and zoom in on an area (or search by cation (or nearby local areas) into the Search box to\nlocation). To get started, look at the United States see what remotely sensed imagery you can view of\nand search for New York City. Zoom in toward Man- where you are now.\nhattan, and examine what types of imagery become 297\nWhat Is Remote Sensing Actually Sensing?\nNote that this process (and this chapter) describes passive remote\ns ensing\u2014where the sensor doesn\u2019t do anything but simply measures and\nr ecords reflected or emitted energy. A separate branch encompasses active\nremote sensing, wherein the sensor generates its own energy, casts it at a\ntarget, then measures the return of that form of energy. Radar is a good\nexample of active remote sensing\u2014a device on a plane throws radar waves\n(microwaves) at a target, those waves interact with the target, and the\nd evice measures the backscatter of the returning radar waves.\nWhat Is Remote Sensing Actually Sensing?\nThe information that is being sensed is the reflection of electromagnetic\n(EM) energy off a target. There are two ways of thinking of light energy,\nas either a particle or as a wave. For our purposes, we\u2019ll stick to the con-\ncept of light energy as a wave. When the Sun radiates this energy, it radi-\nates in the form of waves. Think of this like sitting on the beach and watch-\ning the waves come in from the ocean to crash on the shore. The distance\nbetween the crests of two waves is called the wavelength ((cid:2)), with some wavelength the\nwavelengths being very long and some being very short. If it\u2019s a calm day at distance between the\ncrests of two waves.\nthe ocean, you would see waves less frequently than you would if it was a\nrough day, thus there will be a longer distance between waves (and a longer\nwavelength). If it\u2019s a rough day at the ocean, you\u2019ll see waves a lot more\nfrequently and the distance between waves will be shorter (that is, a shorter\nwavelength).\nThus, there\u2019s a definite connection between wavelengths and the fre-\nquency of those wavelengths. With a longer wavelength, you\u2019ll see waves\nfar less frequently, and with a shorter wavelength, you\u2019ll see the waves a lot\nmore frequently (Figure 10.2). If you sat there on the beach and counted the\nwaves, you\u2019d count a higher number of waves when the wavelength between\nthem was small, and consequently you would count fewer waves when the\nwavelength between them was larger.\nFIGURE 10.2 The\n\u03bb\nrelationship between\nwavelength and\nfrequency: Long\nwavelengths equate to a\nlow frequency while short\nwavelengths equate to a\n\u03bb high frequency. 298\nChapter 10 How Remote Sensing Works\nWaves of energy work on a similar principle. Energy is moving at a con-\nstant speed, that is, the speed of light (c), which is about 300 million meters\nper second. If you multiply the frequency times the wavelength, it is always\nequal to the speed of light. When the value for the wavelength of light energy\nis high, the frequency is low, and vice versa. So if the value for the frequency is\nhigh, the value for the wavelength has to be lower to make the product of the\ntwo values always come out to be the same number.\nElectromagnetic energy at different wavelengths also has different\nelectromagnetic\nproperties. Light energy with very long wavelengths will have different\nspectrum the light\ncharacteristics than light that has very short wavelengths. The electro-\nenergy wavelengths\nand the properties magnetic spectrum is used to examine the properties of light energy in\nassociated with them. relation to the wavelengths of energy. The values of the wavelengths are\nusually measured in extremely small units of measurement called micro-\nmicrometer a unit of\nmeasurement equal meters ((cid:2)m), which is one-millionth of a meter (roughly the thickness\nto one-millionth of a of b acteria), or an even smaller measurement called nanometers (nm),\nmeter. Abbreviated (cid:2)m. which is one-billionth of a meter. Forms of electromagnetic energy with\nnanometer a unit of very short wavelengths are cosmic rays, gamma rays, and x-rays, while\nmeasurement equal to forms of e lectromagnetic energy with very long wavelengths are micro-\none-billionth of a meter. waves and radio waves (Figure 10.3).\nAbbreviated nm.\nSomething to remember is that the Sun is radiating all of these types\nof energy, but they\u2019re invisible to the human eye. For instance, when you\nput some cold pizza in the microwave oven to heat it, a light turns on in the\noven, but you don\u2019t actually see the microwaves bombarding the pizza. In the\nsame way, you can\u2019t see the x-rays that a doctor uses in an exam, or you can\u2019t\nsee the radio waves as they\u2019re picked up by your car\u2019s antenna. All of this\nenergy is out there, but you can\u2019t actually see it. This is because the composi-\nvisible light\ntion of your eyes (that allow you to see reflected energy) is only sensitive to\nspectrum the portion\nof the electromagnetic the reflection of light that has a wavelength between 0.4 and 0.7 microm-\nspectrum with eters. This portion of the electromagnetic spectrum is called the visible light\nwavelengths spectrum or \u201c visible spectrum\u201d (see Figure 10.3 for where the visible light\nbetween 0.4 and\nportion falls relative to other forms of electromagnetic energy). If your eyes\n0.7 micrometers.\ncould see the reflection of slightly shorter wavelengths of energy, you\u2019d be\nFIGURE 10.3 The\nelectromagnetic\nspectrum, showing the\n.4 .5 .6 .7 (\u00b5m)\nwavelengths of energy\nand the properties they\ncorrespond with. The\nvisible light portion of\nthe spectrum occurs at 10\u20135 10\u20132 3 102 104 106 (\u00b5m)\nwavelengths between 0.4\nGamma X-ray UV Radio and 0.7 micrometers. elbisiV RI\ndetcelfeR\nRI\nlamrehT\nevaworciM 299\nWhat Is Remote Sensing Actually Sensing?\nable to see ultraviolet (UV) light (like bees can) and if you could see slightly\nUV (ultraviolet)\nlonger wavelengths of energy, you\u2019d be able to see infrared light. However,\nthe portion of the\nthe human eye is only sensitive to electromagnetic energy in this very small electromagnetic\nrange of wavelengths. spectrum with\nThe visible spectrum can be broken down further to the colors that it wavelengths\nbetween 0.01 and\nrepresents. Shorter visible light wavelengths (between 0.4 and 0.5 micro-\n0.4 micrometers.\nmeters) represent the blue portion of the spectrum. Medium visible light\nband a narrow range\nwavelengths (between 0.5 and 0.6 micrometers) represent the green por-\nof wavelengths being\ntion of the spectrum. Longer visible light wavelengths (between 0.6 and\nmeasured by a remote\n0.7 micrometers) represent the red portion of the spectrum. For instance,\nsensing device.\nwhen you see a shirt with a bright red color, your eyes are actually sens-\nblue band the range\ning the reflection of light with a wavelength measurement somewhere be-\nof wavelengths\ntween 0.6 and 0.7 micrometers. between 0.4 and\nEach of these narrow portions of wavelengths of the electromagnetic 0.5 micrometers.\nspectrum that represents a different form of light is referred to as a band green band the\nof energy in remote sensing. For example, the shorter wavelengths of 0.4 range of wavelengths\nto 0.5 micrometers are referred to as the blue band of the electromagnetic between 0.5 and 0.6\nmicrometers.\nspectrum since the properties of those wavelengths of light correspond to the\ncharacteristics of blue energy. Similarly, the section of wavelengths between red band the range\nof wavelengths\n0.5 and 0.6 micrometers make up the green band of the electromagnetic\nbetween 0.6 and\nspectrum, and the range of wavelengths of 0.6 to 0.7 micrometers make up\n0.7 micrometers.\nthe red band.\nIR (infrared) the portion\nKeep in mind that while the human eye is restricted to seeing only\nof the electromagnetic\nthese narrow bands of the electromagnetic spectrum, remote sensing de-\nspectrum with\nvices have the capability to sense energy from other portions of the spec- wavelengths\ntrum. For instance, the infrared (IR) light portion of the electromagnetic between 0.7 and\nspectrum extends to wavelengths between 0.7 and 100 micrometers in 100 micrometers.\nlength. A sensor could be tuned to only measure the reflection of a nar- NIR (near infrared)\nrow infrared band of energy between 0.7 and 0.9 micrometers, and even the portion of the\nelectromagnetic\nthough this energy is not visible to the human eye, a remote sensing de-\nspectrum with\nvice can easily measure this reflection of energy. As we\u2019ll see, there\u2019s an\nwavelengths\nenormous amount of information that can be gained from examining the between 0.7 and\nreflection of these other forms of energy off objects. When doing remote 1.3 micrometers.\nsensing of infrared light, there are a few wavelength ranges that are com-\nMIR (middle infrared)\nmonly utilized: the portion of the\nelectromagnetic\na 0.7 to 1.3 micrometers\u2014Bands of this portion are referred to as \u201cnear\nspectrum with\ninfrared\u201d (NIR), which is commonly used by satellite remote sensing (see wavelengths\nChapter 11) and also in infrared photography (see Chapter 9). between 1.3 and\n3.0 micrometers.\na 1.3 to 3.0 micrometers\u2014Bands of this portion are referred to as \u201cmiddle\ninfrared\u201d (MIR), which is also utilized by different satellite sensors. For TIR (thermal infrared)\nthe portion of the\ninstance, measurements of bands of MIR energy are used in studies re-\nelectromagnetic\nlated to water content of plants.\nspectrum with\na 3.0 to 14.0 micrometers\u2014Bands of this portion are referred to as \u201cthermal wavelengths\nbetween 3.0 and\ninfrared\u201d (TIR), which is used for measuring heat sources or radiant heat\n14.0 micrometers.\nenergy. 300\nChapter 10 How Remote Sensing Works\nWhat Is the Role of the Atmosphere\nin Remote Sensing?\nEarth\u2019s atmosphere acts like a shield around the planet and the energy from\nthe Sun has to pass through this medium before it reaches Earth\u2019s surface.\nAs a result, a considerable portion of the Sun\u2019s electromagnetic energy nev-\ner actually makes it to the ground. Consequently, if it never makes it to the\nground, it\u2019s never going to be reflected back up to the remote sensing device\n(and is not part of the whole remote sensing process). Earth\u2019s atmosphere\ncontains a variety of gases (including carbon dioxide and ozone) that serve to\nabsorb numerous types of electromagnetic wavelengths. For i nstance, ozone\nabsorbs wavelengths of energy that correspond to ultraviolet light, the \u201cUV\nrays\u201d that can harm you. Thus, a considerable amount of ultraviolet radia-\ntion gets trapped in the atmosphere and doesn\u2019t pass through to Earth. Very\nshort wavelengths and some very long wavelengths are also absorbed by the\natmosphere.\nThose wavelengths of energy that pass through the atmosphere (rather\natmospheric windows than being absorbed) are referred to as atmospheric windows. Remote\nthose wavelengths sensing is done of the wavelengths that make up these \u201cwindows,\u201d as they\nof electromagnetic are the ones that transmit most of their energy through the atmosphere to\nenergy in which most\nreach Earth. An example of atmospheric windows would be the visible light\nof the energy passes\nwavelengths of the electromagnetic spectrum (wavelengths between 0.4 and\nthrough Earth\u2019s\natmosphere. 0.7 micrometers). Most of the energy at these wavelengths is not absorbed\nby the atmosphere and instead transmits to Earth to reflect off objects to be\nviewed by our eyes (as well as cameras and satellite sensors). Other windows\nused in remote sensing include portions of the infrared and thermal infrared\nFIGURE 10.4 The sections of the spectrum (see Figure 10.4 for a diagram of the atmospheric\nabsorption of various windows). Sensors are set up to measure the energy at the wavelengths of\nwavelengths by the windows. It wouldn\u2019t do any good whatsoever to have a sensor measuring\ndifferent gases in Earth\u2019s\nthe wavelengths that were completely absorbed by the atmosphere, as there\natmosphere, culminating\nin the atmospheric would be no later energy reflection to measure (such as the entire ultraviolet\nwindows used in remote portion of the spectrum).\nsensing.\nUV Visible Reflected IR Thermal (emitted) IR Microwave\n100 H 2O CO 2 O 3 H 2O\nH O\n2\nH O\n2 CO H O\nO O 2 2\n2 3\n0\n0.2\u00b5m 0.5 1.0 5 10 20 100\u00b5m 0.1cm 1.0cm 1.0m\nWavelength (not to scale)\nnoissimsnart\ncirehpsomtA\n% eulB\nneerG\ndeR 301\nWhat Happens to Energy When It Hits a Target on the Ground?\nWhile some light is absorbed by the atmosphere, other light is scattered\nby it. Scattering occurs due to particles in the atmosphere and is thus always\npresent, but also unpredictable. Particles will absorb energy and redirect it\nback out in random directions, causing scattering. There are three types of\nscattering that can occur: The first of these is Rayleigh scattering, which Rayleigh scattering\noccurs when the particles causing the scattering are significantly smaller scattering of light\ncaused by atmospheric\nthan the wavelengths being affected. Keeping in mind that the visible-light\nparticles smaller than\nwavelengths are smaller than one-millionth of a meter, Rayleigh scattering is\nthe wavelength being\ncaused by molecules in the atmosphere. In addition, shorter wavelengths are\nscattered.\nscattered to a much greater degree than longer wavelengths. This explains\nMie scattering\nwhy the sky is blue during the day: The shorter blue wavelengths are scat-\nscattering of light\ntered far more than the longer red wavelengths. However, at sunset or sun-\ncaused by atmospheric\nrise, sunlight has a longer way to travel to reach Earth, and thus with all of particles the same size\nthe blue wavelengths already scattered, we see the reds and oranges of longer as the wavelength\nwavelengths as the Sun goes out of view. being scattered.\nAnother type of scattering is Mie scattering, when the particles causing nonselective\nthe scattering are roughly the same diameter as the wavelengths they\u2019re scat- scattering scattering\ntering (this would include things like dust or smoke). Last is n onselective of light caused by\natmospheric particles\nscattering, where what\u2019s causing the scattering is larger than the wave-\nlarger than the\nlengths being scattered (such as water droplets or clouds) and scatters all\nwavelength being\nwavelengths equally. Nonselective scattering also explains why we see clouds scattered.\nas being white\u2014the visible colors of blue, green, and red are all scattered the\nsame, thus taking on the color of white (equal levels of red, green, and blue\nwill produce the color white).\nWhat Happens to Energy When It Hits\na Target on the Ground?\nAfter absorption and scattering by the atmosphere, the wavelengths that cor-\nrespond to the windows finally make it to Earth\u2019s surface and interact with\ntargets there. One of three things can happen to that energy (on a wavelength-\nby-wavelength basis)\u2014it can either be transmitted through the target, it can\nbe absorbed by the target, or it can be reflected off the target. Transmittance transmittance when\noccurs when a wavelength of energy simply passes through a surface to inter- light passes through a\nact with something else later. Think of light passing through a windshield of target.\na car\u2014most of the light will pass through the glass to affect things in the car\nrather than the windshield itself.\nAbsorption occurs when the energy is trapped and held by a surface absorption when light\nr ather than passing through or reflecting off it. Think of walking across a is trapped and held by\nblacktop parking lot during a hot summer day\u2014if you don\u2019t have something a target.\non your feet, they\u2019re going to hurt from the heat in the pavement. Since the\nblacktop has absorbed energy (and converted it into heat), it\u2019s storing that\nheat during the day. Absorption also explains why we see different colors.\nFor instance, if someone is wearing a bright green shirt, there\u2019s something in\nthe dyes of the shirt\u2019s material that is absorbing all other colors (that is, the 302\nChapter 10 How Remote Sensing Works\nportions of the visible light spectrum) aside from that shade of green, which\nis then being reflected to your eyes.\nDifferent surfaces across Earth have different properties that cause them\nto absorb energy wavelengths in various ways. For instance, we see a clear\nlake as a blue color\u2014it could be dark blue, lighter blue, maybe even some\ngreenish blue, but it\u2019s nonetheless a shade of blue. We see it this way because\nwater strongly absorbs all electromagnetic wavelengths aside from the range\nof 0.4 to 0.5 micrometers (with a little lesser absorption past the edges of that\nrange), which corresponds to the blue portion of the spectrum. Thus, other\nwavelengths of energy (such as the infrared portion) get absorbed and the\nblue portion gets reflected.\nWhat remote sensing devices are really measuring is the reflectance of\nenergy from a surface\u2014the energy that rebounds from a target to be collected\nby a sensor. Like absorption, different items on Earth\u2019s surface reflect e nergy\ndifferently. Thus, the total amount of energy that strikes a surface (the inci-\nincident energy the dent energy) can be computed by adding up the amounts of energy that was\ntotal amount of energy transmitted, absorbed, and reflected per wavelength by a particular surface\n(per wavelength) as follows:\nthat interacts with an\nobject. I (cid:3) R (cid:4) A (cid:4) T\nwhere I is the incident energy (the total amount of energy of a particular\nwavelength) that strikes a surface and is made up of the amount of R (reflec-\ntion), A (absorption), and T (transmittance) of that particular wavelength.\nSince remote sensing is focused on the reflection of energy per wavelength,\nlet\u2019s change that equation to focus on calculating what fraction of the total\nincident energy was reflected (and to turn this value into a percentage instead\nof a fractional amount, we\u2019ll multiply by 100):\n(cid:3) (cid:3) (R\/I) (cid:5) 100\nwhere (cid:3) is the portion of the total amount of energy composed by reflection\n(rather than absorption or transmittance) per wavelength. This final value is\nThinking Critically with Geospatial Technology 10.1\nHow Does Remote Sensing Affect Your Privacy?\nThe remote sensing processes described in this chap- without your explicit permission and likely without\nter are systems that measure energy reflection (and your knowledge. If you can see your car in your\nturn it into images) without interfering with what\u2019s driveway on Google Earth, then it just happened to\nhappening below them. For instance, when an aircraft be parked there when the airplane or satellite col-\nflies overhead to take pictures or a satellite crosses lected that image. Does this unobtrusive method of\nover your house (at more than 500 miles above it), data collection affect your privacy? Is the acquisi-\nthey collect their data and move on. The data col- tion of images via remote sensing invasive to your\nlection process doesn\u2019t physically affect you or private life? In what ways could this use of geospa-\nyour property in the slightest, and all of this is done tial technology intrude on someone\u2019s life? 303\nHow Can Spectral Reflectance Be Used in Remote Sensing?\nthe spectral reflectance\u2014the percentage of total energy per wavelength that\nspectral reflectance\nwas reflected off a target\u2014which makes its way toward the sensor and is what\nthe percentage of the\nis being utilized with remote sensing. total incident energy\nthat was reflected from\nthat surface.\nHow Can Spectral Reflectance Be Used\nin Remote Sensing?\nAll items on Earth\u2019s surface reflect energy wavelengths differently. For\ni nstance, green grass, bare soil, a parking lot, a sandy beach, and a large lake\nall reflect portions of the visible, near-infrared, and middle-infrared energy\ndifferently (these items emit thermal energy differently as well). In addition,\nsome things reflect energy differently depending on different conditions. As\nan example of examining these remote sensing concepts, let\u2019s look at some-\nthing simple\u2014measuring reflectance of energy from the leaves on a tree.\nDuring late spring and summer, a tree canopy will be fuller, as the leaves stay\non the trees and are a healthy green color. The human eye sees healthy leaves\nas green because the chlorophyll in the leaves is reflecting the green portion\nof the spectrum and absorbing the red and blue portions. What can\u2019t be seen\nby the human eye (but can be seen by remote sensing instruments) is that\nhealthy leaves also very strongly reflect near-infrared energy. Thus, measure-\nment of near-infrared energy is often used (in part) as an indicator about the\nrelative health of leaves and vegetation.\nHowever, leaves aren\u2019t always going to stay green. As autumn progresses,\nleaves go through a senescence process in which they lose their chlorophyll.\nAs this happens, the leaves reflect less green energy and begin to absorb more\nred and blue energy, causing the leaves to appear in other colors, such as\nyellow, orange, and red. As the leaves fall off the trees and turn brown, they\nhave a lot more red reflection, causing their brownish appearance (and there\nwill also be less reflectance of near-infrared energy from the tree).\nIf a sensor is able to measure all of these energy wavelengths simulta-\nneously for different objects, it would give a person examining the data the\ncapability to tell objects apart by examining their reflectance values in all of\nthese wavelengths. If you were to chart the spectral reflectance values against\nthe wavelengths being measured for each item, you would find that each of\nthese things would have a different line on the chart. This is referred to as an\nitem\u2019s spectral signature (also called a spectral reflectance curve), as each set spectral signature a\nof charted measurements will be unique to that item, just like your handwrit- unique identifier for\nten signature on a piece of paper is different from other people\u2019s signatures. a particular item,\ngenerated by charting\nRemote sensing analysts can use these spectral signatures to distinguish items\nthe percentage of\nin an image or use them to tell the difference between different types of plants\nreflected energy per\nor minerals (see Figure 10.5 on page 304 for an example of various spectral wavelength against\nsignatures compared to one another). a value for that\nAn application of remotely sensed imagery is its use in assessing the health wavelength.\nof green vegetation (such as fields, grass, and leaves on trees). When vegeta-\ntion is very healthy, it will have a strong reflection of near-infrared energy and 304\nChapter 10 How Remote Sensing Works\nFIGURE 10.5 Examples Grasslands\nR e d s a n d pit\nof spectral signatures\nP i n e w oods\ngenerated for different\nS i l t y w ater\nitems by charting the\npercentage of reflection\non the y-axis and the 60\nwavelengths being\nmeasured on the x-axis.\n20\n0\n0.4 0.6 0.8 1.0 1.2\nWavelength (\u00b5m)\nabsorption of red energy. As the vegetation becomes more stressed, less near-\ninfrared energy will be reflected and more red energy will be reflected rather\nthan being absorbed. An image can be processed to have a way of measur-\ning the healthiness of vegetated areas. The Normalized Difference Vegetation\nNDVI (Normalized Index (NDVI) is a means of doing so\u2014the process takes a remotely sensed\nDifference Vegetation image and creates a new NDVI image from it, containing values related to the\nIndex) a method of relative health of the vegetation being examined. As long as the sensor can\nmeasuring the health\nmeasure the red and near-infrared portions of the electromagnetic spectrum,\nof vegetation using\nNDVI can be calculated.\nnear-infrared and red\nenergy measurements. NDVI is computed using the measurements for the red and near-infrared\nbands. The formula for NDVI is:\n(NIR (cid:6) Red)\n(NIR (cid:4) Red)\nNDVI returns a value between (cid:6)1 and (cid:4)1, with the higher the value\n( closer to 1), the healthier the vegetation is at the area being measured. Low\nvalues i ndicate unhealthy vegetation or a lack of biomass. Very low or nega-\ntive v alues indicate non-vegetated areas (things like pavement, clouds, or ice).\nFigure 10.6 shows an example of NDVI being calculated for both a healthy\nvegetation source (one that reflects a high amount of near-infrared energy\nand less red energy), resulting in a high NDVI value (0.72) and an unhealthy\nsource (which reflects less near-infrared energy and more red energy), which\nresults in a lower NDVI value (0.14).\nNDVI can be calculated by a variety of remote sensing devices, so long\nas their sensors can measure the red and near-infrared bands (which sev-\neral of the specific sensors discussed in the next two chapters can do).\nNDVI gives a quick means of assessing vegetation health, which is used\nfor a variety of environmental applications, including crop monitoring or\nglobal vegetative conditions (see Figure 10.7 for an example and Hands-on\nApplication 10.2: Examining NDVI with NASA ICE on page 306 to use some\nremotely sensed imagery to examine the health of a rainforest).\necnatcelfer\ntnecreP 305\nHow Can Spectral Reflectance Be Used in Remote Sensing?\nFIGURE 10.6 An\nNear Near\nexample of NDVI\ninfrared Visible infrared Visible\ncalculation for healthy\n(left) and unhealthy\n5555500000%%%%% 888%%% 4444400000%%%%% 333000%%% (right) vegetation. (Source:\nAdapted from NASA\/Robert\nSimmon)\n(0.50 \u2014 0.08) (0.4 \u2014 0.30)\n= 0.72 = 0.14\n(0.50 + 0.08) (0.4 + 0.30)\nFIGURE 10.7 An NDVI\nimage of Africa, showing\nthe health (or presence)\nof vegetation on the\ncontinent. (Source: Adapted\nfrom NASA\/University of\nMaryland)\nSSSaaahhheeelll\nVegetation (NDVI)\n0 0.3 0.6 0.9 306\nChapter 10 How Remote Sensing Works\nHands-on Application 10.2\nExamining NDVI with NASA ICE\nNASA (the National Aeronautics and Space Admin- NDVI images using the near-infrared and red bands of\nistration) has an online tool called ICE (Image Com- a Landsat satellite image. To properly see the NDVI\nposite Explorer) that allows users to analyze differ- imagery, select Channel 4 (this is the near-infrared\nent applications of remotely sensed data, and one band) in the red Channel 1 and then select Channel 3\ncomponent features an interactive NDVI tool. To use (this is the red band) in the green Channel 2. Click\nICE\u2019s NDVI exercise, open your Web browser and go to Build and you can view the NDVI image of the area.\nhttp:\/\/earthobservatory.nasa.gov\/Experiments\/ Change the color table and zoom around the imag-\nICE\/panama\/panama_ex2.php. This section of ICE ery. How can NDVI be used to measure the health\nis part of a larger exercise examining the health of of the rainforests?\ntropical rainforests. The tools will allow you to build\nHow Do You Display a Digital Remotely\nSensed Image?\nOnce this digital data is collected, the next step is to transform it into some-\nthing that can be viewed on a computer screen, especially since many of these\nbands of energy are only visible to the sensor and not to the human eye. Sensors\ncan only \u201csee\u201d a certain area of the ground at a time and will record measure-\nments of that particular section. The size of this area on the ground is the sen-\nspatial resolution the sor\u2019s spatial resolution, representing the smallest amount of area the sensor\nsize of the area on can collect information about. For example, if a sensor\u2019s spatial resolution was\nthe ground being 30 meters, each measurement would consist of a 30 meter (cid:5) 30 meter section\nrepresented by one\nof the ground, and the sensor would measure thousands of these 30 meter (cid:5)\npixel\u2019s worth of energy\n30 meter sections at once. No matter how many sections are being measured,\nmeasurement.\nthe smallest area the sensor has information for is a block that is 30 meters by\n30 meters. Keep in mind, the spatial resolution of the image is going to con-\nstrain the kind of visual information you can obtain from it. For instance, in an\nimage with 4 meter resolution, you may be able to discern that an object is a\nlarge vehicle, but no details about it, whereas an image with 0.5 meter resolu-\ntion may allow you to determine specific features about the vehicle in question.\nWe\u2019ll discuss more about spatial resolution in Chapter 11, but for right\nnow, each one of these blocks of the ground being sensed represents the size\nof the area on the ground having its energy reflection being measured by\nthe s ensor. Due to the effects of the atmosphere, there\u2019s not a straight one-\nbrightness values the\nto-one relationship between the reflectance of energy and the radiance being\nenergy measured at a\nsingle pixel according m easured at the satellite\u2019s sensor. The energy measurements at the sensor are\nto a pre-determined converted to a series of pixels, with each pixel receiving a brightness value,\nscale. Also referred or BV (also referred to as a Digital Number, or DN), for the amount of radiance\nto as Digital Numbers\nbeing measured at the sensor for that section of the ground. This occurs for\n(DNs).\neach wavelength band being sensed. 307\nHow Do You Display a Digital Remotely Sensed Image?\nBrightness values are scaled to fit a range specified by the sensor.\nAgain, we\u2019ll discuss more about these various types of ranges in Chapter 11\n(with r egard to a sensor\u2019s radiometric resolution), but for now, this range of\n8-bit imagery a digital\nvalues represents the energy being recorded on a scale from a low number (in- image that carries a\ndicating very little radiance) to a high number (indicating a lot of radiance) range of brightness\nand measured as a number of bits. For instance, an 8-bit sensor will scale mea- values from 0 to 255.\nsurement values between the numbers of 0 (the lowest value) and 255 (the panchromatic\nhighest value). These values represent the energy being recorded in a range imagery black and\nfrom 0 to 255 (these values are related to the amount of energy being mea- white imagery formed\nby viewing the entire\nsured). For instance, all values in a band of an image could possibly only be in\nvisible portion of\na range such as 50 to 150, with nothing reaching the maximum value of 255.\nthe electromagnetic\nEach wavelength band that is being sensed is assigned a BV in this range spectrum.\nfor each pixel in the image. Brightness values (BVs) can be translated to a gray-\nmultispectral\nscale to be viewed on a computer screen. With the range of values associated\nimagery remotely\nwith 8-bit imagery, values of 0 represent the color black and values of 255 sensed imagery\nrepresent the color white. All other integer values between 0 and 255 are dis- comprised of the bands\nplayed in shades of gray, with lower numbers being darker shades and higher collected by a sensor\ncapable of sensing\nnumbers being lighter shades. In this way, any of the wavelength band mea-\nseveral bands of energy\nsurements (including the wavelengths normally invisible to the h uman eye)\nat once.\ncan be displayed in grayscale on a computer screen (see Figure 10.8). For in-\nhyperspectral\nstance, a near-infrared image would have the brightness value for each pixel\nimagery remotely\nrepresenting the near-infrared measurements at the sensor for that area.\nsensed imagery\nIf a sensor is measuring the visible portion of the spectrum and treating\ncomprised of the bands\nthe entire 0.4 to 0.7 micrometer range as if it was one band, the result will be collected by a sensor\nblack-and-white panchromatic imagery. However, remote sensing devices are capable of sensing\ncapable of sensing several wavelengths of the electromagnetic spectrum simul- hundreds of bands of\nenergy at once.\ntaneously. Some common remote sensing devices can sense several different\nbands at once\u2014sensing 7, 10, or even 36 bands at the same time is currently\ndone on a regular basis by United States government satellite s ensors. Imagery FIGURE 10.8 Pixels of\ncreated by sensing multiple bands together is referred to as multispectral im- a sample 8-bit image,\nwith their corresponding\nagery. Some technology can simultaneously sense over 200 bands of the elec-\nbrightness values and\ntromagnetic spectrum, producing hyperspectral imagery.\ngrayscale shading.\nBV Greyscale\n255 White\n170 85 17 136 119 221\n119 255 221 68 238 17\n238 170 221 0 85 255\n127 gray\n0 221 255 136 238 0\n85 170 119 255 238 136\n68 17 68 170 119 136\n0 black 308\nChapter 10 How Remote Sensing Works\nWhen multiple bands of imagery are available, they can be combined\nt ogether to view the image in color, rather than grayscale. Displaying im-\nagery in color requires three bands to be displayed simultaneously, each\nshown in a separate color other than gray. This is simplifying the technical\naspects a bit, but a display monitor (like a computer screen) is equipped\ncolor gun equipment with devices called color guns that can draw a pixel on the screen in vary-\nused to display a ing degrees of red, green, and blue. If an 8-bit image was drawn using the\ncolor pixel on a screen red color gun, the values of 0 to 255 would not be in a range of black, gray,\nthrough the use of the\nand white, but instead would be in a range of the darkest red, medium red,\ncolors red, green, and\nand very bright red. The same goes for the blue and green color guns\u2014each\nblue.\ngun would be able to display an image in its respective color on a range of\ncolor composite an\n0 to 255.\nimage formed by\nA color composite can be generated by displaying the brightness values\nplacing a band of\nimagery into each of of one band of imagery in the red gun, a second band of imagery in the green\nthe three color guns gun, and a third band of imagery in the blue gun. Each pixel would then be\n(red, green, and blue) drawn using brightness values as the intensity of the displayed color. For in-\nto view a color image\nstance, a pixel could be displayed using a value of 220 in the red gun, 128 in\nrather than a grayscale\nthe green gun, and 75 in the blue gun, and the resulting color shown on the\none.\nscreen would be a combination of these three color intensities. By using the\nthree primary colors of red, green, and blue, and combining them to varying\ndegrees using the range of values from 0 to 255, many other colors can be cre-\nated (using the 256 numbers in the 0\u2013255 range, with three colors, there are\n16,777,216 possible color combinations) to be displayed on the screen (see\nFigure 10.9 for examples of color formation).\nFIGURE 10.9 Colors that To create colors, different values can be assigned to the red, green, and\ncan be formed by adding blue guns. Table 10.1 shows some examples of ways to combine values to-\ndifferent colors together\ngether to form other colors (keeping in mind that there are millions of possible\n(left) and subtracting\ndifferent colors from each\nother (right).\nYellow Cyan\nGreen Red\nWhite\nGreen Blue\nCyan Magenta\nBlack\nYellow Red Magenta\nBlue 309\nHow Do You Display a Digital Remotely Sensed Image?\nTABLE 10.1 E xamples of colors generated through 8-bit combinations of\nred, green, and blue\nRed Value Green Value Blue Value Color Formed\n255 255 0 Yellow\n255 0 255 Magenta\n0 255 255 Cyan\n255 128 0 Orange\n0 64 128 Dark Blue\n255 190 190 Pink\n255 255 115 Bright Yellow\n115 76 0 Brown\n0 197 255 Sky Blue\n128 128 128 Gray\n255 255 255 White\n0 0 0 Black\ncombinations of these values). See also Hands-on Application 10.3: Color Tools\nOnline: Color Mixing for an online tool that can be used to create colors from\nvarious 0 to 255 values.\nWhen an image is examined, the color being displayed for each pixel\ncorresponds to particular values of red, green, and blue being shown on the\nscreen. For a remotely sensed image, those numbers are the brightness val-\nues of the band being displayed with that color gun. Any of the bands of an\nimage may be displayed with any of the guns, but as there are only three\ncolor guns, only three bands can be displayed as a color composite at any\ntime (Figure 10.10 on page 310).\nFor example, the near-infrared band values could be displayed using the\nred color gun, the green band values could be displayed using the green color\ngun, and the red band values could be displayed using the blue color gun. By\nHands-on Application 10.3\nColor Tools Online: Color Mixing\nThe Color Tools Website provides several great re- On the Website, type a value into the box in front\nsources related to color. The Color Mixer tool avail- of the \u201c\/255\u201d for each of the three colors and check\nable here: http:\/\/www.colortools.net\/color_mixer. out the result. Try the values listed in Table 10.1\nhtml allows you to input values (between 0\u2013255) and some of your own to see how different numbers\nfor red, green, and blue and see the resultant color. create different colors. 310\nChapter 10 How Remote Sensing Works\nFIGURE 10.10\nCombining the display of\nthree 8-bit imagery bands,\neach being displayed in\na different color gun, to\nform a color composite\nimage. (Source: Courtesy of\nNASA. Images acquired by\nLandsat 5.)\nusing this arrangement, a brown pixel can be seen on the screen, and after\ninquiry, it\u2019s discovered that the red gun (which has the near-infrared band)\nhas a value of 150, the green gun (which has the green band) has a value of\n80, and the blue gun (which has the red band) has a value of 20. These values\ncorrespond to the brightness values of their respective bands and are com-\nbined together to form that pixel color. If you change up the arrangement of\nbands and guns, you may have different colors displayed on the screen, but\nthe actual brightness values will not have changed. Table 10.2 shows how the\ndisplay color will change depending on which band is being displayed with\nwhich gun.\nNote that while the pixel being displayed on the screen changed color\ndepending on what band was being displayed with a different gun, none of\nthe brightness values of that pixel were altered. Different band combinations\ncan reveal different information about objects in an image due to the varying\nbrightness values of the bands for those items.\nSeveral different types of composites can be formed using the various\nbands and guns. Whenever a composite has the red band displayed in the red\nTABLE 10.2 D ifferent combinations of brightness values for various bands\ndisplayed with d ifferent color guns from different colors on\nthe screen\nRed Value Green Value Blue Value Color Formed\nNIR Band (150) Green Band (80) Red Band (20) Brown\nRed Band (20) Green Band (80) NIR Band (150) Deep Blue\nGreen Band (80) NIR Band (150) Red Band (20) Green\nRed Band (20) NIR Band (150) Green Band (80) Paler Green 311\nHow Do You Display a Digital Remotely Sensed Image?\n(a) (b)\nFIGURE 10.11 A\ncomparison of the\ngun, the green band displayed in the green gun, and the blue band displayed\nCleveland area as\nin the blue gun (the way natural colors would be shown), a true color com- viewed in (a) a true color\nposite is formed. A true color composite would look as if you would expect to composite and (b) a false\nsee a remotely sensed image as if you were looking down at the ground from color composite. (Source:\nNASA\/Purdue Research\nan airplane (that is, the trees would be green and the water would be blue).\nFoundation)\nWhenever the distribution of bands to guns deviates from this arrangement\n(some other combination is used besides the true color one), a false color true color composite\ncomposite is generated. For instance, all four of the combinations described an image arranged by\nplacing the red band\nin Table 10.2 would result in false color composites as none of them display\nin the red color gun,\nthe red band in the red gun, the green band in the green gun, and the blue\nthe green band in the\nband in the blue gun. See Figure 10.11 for a comparison of true color and\ngreen color gun, and\nfalse color composites. the blue band in the\nWith all of these different combinations possible, there is a standard blue color gun.\nfalse color composite that is often used in many remote sensing stud- false color composite\nies. In this standard false color arrangement, the near-infrared band is an image arranged by\ndisplayed in the red gun, the red band is displayed in the green gun, and not placing the red\nband in the red color\nthe green band is displayed in the blue gun. The false color composite im-\ngun, the green band\nage is similar to the CIR photos discussed back in Chapter 9. Some com-\nin the green color gun,\nmon effects of this arrangement are the displaying items that would have and the blue band in\na strong reflection of near-infrared energy (such as healthy plants, grass, the blue color gun.\nor vegetation) is bright shades of red, while water (which would have very standard false color\nlittle reflection of the near-infrared, red, and green energy) would be dis- composite an image\nplayed in black. With composites, multispectral imagery can be displayed arranged by placing the\nnear-infrared band in\nin color, regardless of what bands are being used and these composites\nthe red color gun, the\ncan be used for different types of image analysis (see Hands-on Application\nred band in the green\n10.4: Comparing True Color and False C olor Composites on page 312 for an color gun, and the\nonline method of comparing how objects on the ground are shown in true green band in the blue\ncolor and false color composites). color gun. 312\nChapter 10 How Remote Sensing Works\nHands-on Application 10.4\nComparing True Color and False Color Composites\nAn online resource for obtaining remotely sensed You can examine a true color composite and a false\nimagery is Ohiolink, the statewide online library sys- color composite (among other options) of the area.\ntem. Landsat satellite imagery (which we\u2019ll discuss Click on the true color composite option to view\nmore in Chapter 11) for Ohio can be downloaded the satellite imagery. You can choose a new win-\nfrom there (if you\u2019re in Ohio), and true color and false dow size and zoom in to the area to see how fea-\ncolor composite images of areas around Ohio can be tures such as urban areas, forests, fields, and water\nviewed as well. Open your Web browser and go to appear in true color. Once you\u2019ve zoomed in and\nhttp:\/\/landsat.ohiolink.edu\/GEO\/LS7 to get start- looked around, select the option for near-infrared in\ned. The graphic on the page shows Ohio broken up the right-hand corner\u2014this will switch to the same\ninto nine different sectors\u2014these correspond to the view, but in a standard false color composite. How\nLandsat reference system we\u2019ll discuss in Chapter 11. did all of the features you\u2019d been looking at change\nFor now, select the central block (Path 19\/Row 32), appearances in the false color composite and why?\nwhich contains imagery of Columbus, Ohio. A new list When you\u2019re done, check out some other portions\nwill appear of all the dates on which Landsat imagery of Ohio along Lake Erie (like Cleveland) to see how\nwas acquired for the region. Choose the most recent other features (such as the lake and lakefront areas)\ndate and other options for viewing will be available. appear in true color and false color composites.\nChapter Wrapup\nThe basics described in this chapter cover the process of remote sensing and\nhow a digital image can be formed from the measurement of electromagnetic\nenergy reflection. In the next chapter you\u2019ll be looking at how these concepts\ncan be applied to perform remote sensing from a satellite platform in space.\nR emotely sensed imagery is used for numerous applications\u2014Chapters 11\nand 12 will examine different types of remote sensing platforms and uses of\ntheir imagery, including everything from air pollution to wildfire monitoring\nto studying water quality.\nThe lab for this chapter uses a free program called MultiSpec to examine\na digital remotely sensed image as well as different band combinations and\ncolor composites.\nImportant note: The references for this chapter are part of the online compan-\nion for this book and can be found at http:\/\/www.whfreeman.com\/shellito1e. 313\nKey Terms\nKey Terms\nremote sensing (p. 295) transmittance (p. 301)\nwavelength (p. 297) absorption (p. 301)\nelectromagnetic spectrum (p. 298) incident energy (p. 302)\nmicrometer (p. 298) spectral reflectance (p. 303)\nnanometer (p. 298) spectral signature (p. 303)\nvisible light spectrum (p. 298) NDVI (p. 304)\nUV (ultraviolet)(p. 299) spatial resolution (p. 306)\nband (p. 299) brightness values (p. 306)\nblue band (p. 299) 8-bit imagery (p. 307)\ngreen band (p. 299) panchromatic imagery (p. 307)\nred band (p. 299) multispectral imagery (p. 307)\nIR (p. 299) hyperspectral imagery (p. 307)\nNIR (p. 299) color gun (p. 308)\nMIR (p. 299) color composite (p. 308)\nTIR (p. 299) true color composite (p. 311)\natmospheric windows (p. 300) false color composite (p. 311)\nRayleigh scattering (p. 301) standard false color\nMie scattering (p. 301) composite (p. 311)\nnonselective scattering (p. 301) 10.1\nGeospatial Lab Application\nRemotely Sensed Imagery and\nColor Composites\nThis chapter\u2019s lab will introduce you to some of the basics of working with\nmultispectral remotely sensed imagery through the use of the MultiSpec soft-\nware program.\nObjectives\nThe goals for you to take away from this exercise are:\na Familiarizing yourself with the basics of the MultiSpec software program.\na Loading various bands into the color guns and examining the results.\na Creating and examining different color composites.\na Comparing the brightness values of distinct water and environmental\nfeatures in a remotely sensed satellite image to create basic spectral\nprofiles.\nObtaining Software\nThe current version of MultiSpec is available for free download at http:\/\/\ncobweb.ecn.purdue.edu\/~biehl\/MultiSpec.\nImportant note: Software and online resources sometimes change fast.\nThis lab was designed with the most recently available version of the soft-\nware at the time of writing. However, if the software or Websites have signifi-\ncantly changed between then and now, an updated version of this lab (using\nthe newest versions) is available online at http:\/\/www.whfreeman.com\/\nshellito1e.\nLab Data\nCopy the folder \u2018Chapter10\u2019\u2014it contains a Landsat 5 TM satellite image file\ncalled \u2018cle.img\u2019 (which is a subset of a Landsat 5 TM scene from 9\/11\/2005 of\nnortheast Ohio). We will discuss more about Landsat imagery in Chapter 11,\nbut the TM imagery bands refer to the following portions of the electromag-\nnetic (EM) spectrum in micrometers ((cid:2)m):\na Band 1: Blue (0.45 to 0.52 (cid:2)m)\na Band 2: Green (0.52 to 0.60 (cid:2)m)\na Band 3: Red (0.63 to 0.69 (cid:2)m)\n314 315\nRemotely Sensed Imagery and Color Composites\na Band 4: Near infrared (0.76 to 0.90 (cid:2)m)\na Band 5: Middle infrared (1.55 to 1.75 (cid:2)m)\na Band 6: Thermal infrared (10.4 to 12.5 (cid:2)m)\na Band 7: Middle infrared (2.08 to 2.35 (cid:2)m)\nAlso, Landsat TM imagery is 30-meter spatial resolution (except for the thermal-\ninfrared band, which is 120 meters). Thus, each pixel you will be examining\nrepresents 30 meters square.\nLocalizing This Lab\nAlthough this lab focuses on a section of a Landsat scene from northeast Ohio,\nLandsat imagery is available for download via GLOVIS at http:\/\/glovis.usgs.\ngov. This will provide the raw data which will have to be imported into Multi-\nSpec and processed to use in the program.\nAlternatively, NASA has information for obtaining free Landsat data for\nuse in MultiSpec online at http:\/\/change.gsfc.nasa.gov\/create.html. The\npage provides step by step instructions for acquiring free data for your own\narea and getting it into MultiSpec format.\n10.1 MultiSpec and Band Combinations\n1. Start MultiSpec.\nM ultiSpec will start with an empty text box (that will give you updates\nand reports of processes in the program). You can minimize the text box\nfor now.\n2. To get started with a remotely sensed image, select Open from the File\npull-down menu. Alternatively, you can select the Open icon from the\ntoolbar:\n(Source: Purdue Research Foundation)\n3. Navigate to the \u2018Chapter10\u2019 folder and select \u2018cle.img\u2019 as the file to open\nand click Open.\n4. A new dialog box will appear that allows you to set the display\nspecification for the \u2018cle\u2019 image. 316\nChapter 10 How Remote Sensing Works\n(Source: Purdue Research Foundation)\n5. Under Channels, you will see the three color guns available to you (red,\ngreen, and blue). Each color gun can hold one band (see the Lab Data\nsection for a listing of which bands correspond to which parts of the\nelectromagnetic spectrum). The number listed next to each color gun\nrepresents the band being displayed with that gun.\n6. Display band 4 in the red color gun, band 3 in the green color gun, and\nband 2 in the blue color gun.\n7. Accept the other defaults for now and click OK.\n8. A new window will appear and the \u2018cle\u2019 image will begin loading. This\nmight take a minute or two to completely load.\n9. For best results, maximize both MultiSpec as well as the window containing\nthe \u2018cle\u2019 image. Use the bars at the edge of the window to move around the\nimage. You can zoom in or zoom out by using the zoom tools on the toolbar: 317\nRemotely Sensed Imagery and Color Composites\nT he large mountain icon will zoom in and the small mountain icon will\nzoom out.\n10. Zoom around the image, paying attention to some of the city, landscape,\nand water areas.\nQuestion 10.1 What wavelength bands were placed into which color guns?\nQuestion 10.2 Why are the colors in the image so strange compared\nto what we\u2019re normally used to seeing in other imagery (such as Google\nEarth or Google Maps)? For example, why is most of the landscape red?\nQuestion 10.3 In this color composite, what colors are the following\nfeatures in the image being displayed as: water, vegetated areas, and urban\nareas?\n11. Reopen the image, this time using band 3 in the red gun, band 2\nin the green gun, and band 1 in the blue gun (referred to as a 3-2-1\ncombination). Once the image reloads, pan and zoom around the image,\nexamining the same areas you just looked at.\nQuestion 10.4 What kind of composite did we create in this step? How\nare the bands being displayed in this color composite in relation to their\nguns?\nQuestion 10.5 Why can we not always use this kind of composite (from\nQuestion 10.4) when analyzing satellite imagery?\n12. Reopen the image yet again, this time using band 2 in the red gun,\nband 1 in the green gun, and band 3 in the blue gun (referred to as a\n2-1-3 combination). Once it reloads, pan and zoom around the image,\nexamining the same areas you just looked at.\nQuestion 10.6 Once again, what kind of composite was created in this\nstep?\nQuestion 10.7 How are vegetated areas being displayed in this color\ncomposite (compared to the arrangement in Question 10.4)?\n10.2 E xamining Color Composites and\nColor Formations\n1. Reopen the image one more time, returning the 4-3-2 combination\n(band 4 in the red gun, band 3 in the green gun, and band 2 in the blue\ngun). You can close the other images, as we\u2019ll be working with this one\nfor the rest of the lab. 318\nChapter 10 How Remote Sensing Works\n2. Zoom and move around the image to find and examine Burke Lakefront\nAirport as follows:\nBurke Lakefront Airport\n(Source: NASA\/Purdue Research Foundation)\n3. Zoom in to the airfield. From its shape and the pattern of the runways,\nyou should be able to clearly identify it in the Landsat image.\n(Source: NASA\/Purdue Research Foundation) 319\nRemotely Sensed Imagery and Color Composites\n4. Examine the airfield and its surroundings.\nQuestion 10.8 Why do the areas in between the runways appear red?\n5. Open a new image, this time with a 3-4-2 combination. Examine Burke\nLakefront Airport in this new image and compare it to the one you\u2019ve\nbeen working with.\nQuestion 10.9 Why do the areas in between the runways now appear\nbright green?\n6. Open another new image, this time with a 3-2-4 combination. Examine\nBurke Lakefront Airport in this new image and compare it to the others\nyou\u2019ve been working with.\nQuestion 10.10 Why do the areas in between the runways now appear\nblue?\n7. At this point, only keep the 4-3-2 image open and close the other two.\n10.3 E xamining Specific Brightness Values and\nSpectral Profiles\nRegardless of how the pixels are displayed in the image, each pixel in each\nband of the image has a specific brightness value set in the 0\u2013255 range. By\nexamining those pixel values for each band, you can chart a basic \u2018spectral\nprofile\u2019 of some features in the image.\n1. Zoom in to the area around Cleveland\u2019s waterfront and identify the\nurban areas (in the image these will mostly be the white or cyan\nregions).\n2. From the Window pull-down menu, select New Selection Graph. Another\n(empty) window (called Selection Graph) will open in MultiSpec.\n3. In the image, locate a pixel that\u2019s a good example of an urban or\ndeveloped area. The cursor will change to a cross shape, so give the pixel\none more click.\nImportant note: Zoom in so that you are only selecting one pixel with the\ncursor.\n4. A chart will appear in the Selection Graph window that will\ngraphically show the BVs for each band at that particular pixel\n(see the first page of this lab for the portions of the electromagnetic\nspectrum that match up with each band). The band numbers are on\nthe x-axis and the BVs are on the y-axis. The chart can be expanded\nor maximized as necessary to be better able to examine the values. 320\nChapter 10 How Remote Sensing Works\n(Source: Purdue Research Foundation)\n5. The Selection Graph window now shows the data that can be used to\ncompute a simplified version of a spectral profile for an example of the\nparticular urban land use pixel you selected from the image.\n6. For the next question, you\u2019ll be required to find a pixel that\u2019s a good\nexample of water and another pixel that\u2019s a good example of vegetation\nand translate the data from the chart to a \u2018spectral profile\u2019 for each\nexample. In drawing the \u2018profiles\u2019 from the information on the chart,\nkeep two things in mind:\na. First, the values at the bottom of the Selection Graph window\nrepresent the numbers of the bands being examined. On the\nanswer sheet, the actual wavelengths of the bands are plotted out,\nso make sure you properly match up each band with its respective\nwavelength (also note that Band 6 is not being charted).\nb. Second, the values on the x-axis of the Selection Graph window are\nBVs, not the percentage of reflection as seen in a spectral signature\ndiagram. There are a number of factors involved with transforming\nBVs into the actual percent reflectance values (A BV and the\npercent reflectance don\u2019t have an exact one-to-one ratio, as there\nare other factors that affect the measurement at the sensor, such as\natmospheric effects), but in this simplified example, you\u2019ll just chart\nthe BV for this \u2018spectral profile.\u2019\n7. Examine the image for a good example of a water pixel and a vegetation\npixel. 321\nRemotely Sensed Imagery and Color Composites\n8. Plot a \u2018spectral profile\u2019 diagram for the water and vegetation pixels\nyou chose along the following diagram (remember to plot values as\ncalculated from the BVs).\n255\n0\nBlue Green Red NIR MIR MIR\n0.45 to 0.52to 0.63to 0.76to 1.55 to 2.08 to\n0.52 \u00b5m 0.60 \u00b5m 0.69 \u00b5m 0.90 \u00b5m 1.75 \u00b5m 2.35 \u00b5m\n9. Examine your two \u2018profiles\u2019 and answer Questions 10.11 and 10.12.\nQuestion 10.11 What information can you gain from the spectral profile\nfor water about the ability for water to reflect and absorb energy (that is,\nwhat types of energy have the greatest amount of reflection and absorption\nby water)?\nQuestion 10.12 What information can you gain from the spectral profile\nfor vegetation about the ability for vegetation to reflect and absorb energy\n(that is, what types of energy have the greatest amount of reflection and\nabsorption by vegetation)?\n10. Exit MultiSpec by selecting Exit from the File pull-down menu. There\u2019s\nno need to save any work in this exercise.\nClosing Time\nNow that you have the basics of working with remotely sensed data and ex-\namining the differences in color composites, the next two chapters will build\non these skills. The Chapter 11 lab will return to using MultiSpec for more\nanalysis of Landsat satellite imagery.\neulav\nssenthgirB This page was intentionally left blank 11\nImages from Space\nSatellite Remote Sensing, Satellite Orbits, Sensor Resolutions,\nthe Landsat Program, and High-Resolution Satellite Sensors\nHow many times have you seen something like the following scenario in\na television show or movie, or read it in a novel? The heroes are racing\nagainst the clock to stop the villains from setting off whatever doomsday\nweapon they happen to have. In the tense mission-control center, the he-\nroes\u2019 computer expert taps a few keys on the keyboard, a spy satellite swings\ninto view, and suddenly the monitors fill up with crystal-clear images from\nspace of the villains\u2019 remote hideout. The imagery\u2019s crisp enough for the he-\nroes to run some facial recognition software and pick out the international\narms dealer selling the weapons of mass destruction (WMDs), as well as\nread the license plates of the nearby cars. Rest assured, the elite counter-\nintelligence strike team will show up on cue to save the world, probably\nwatched via live remotely sensed satellite video feed by their teammates at\nthe home base.\nBased on this fictional scenario, you might get the impression that remote\nsensing satellites are magical things that can do anything\u2014rotate into what-\never orbital position is required to track people, zoom in (from outer space)\nto make out the details of someone\u2019s license plate, read lips from space, or\nsend live video feeds to the good guys in the field. Certainly, books and films\nhelp perpetuate the idea that remote sensing (and anything using satellites) is\nsome sort of utopian technology that can do anything. The goal of this chapter\nis to examine how satellite remote sensing is really done, as well as its uses\nand limitations. NASA the National\nAeronautics and\nThe first man-made satellite to achieve orbit around Earth was Sput-\nSpace Administration,\nnik, launched in 1957 by the Union of Soviet Socialist Republics (USSR),\nestablished in 1958;\nand it was quickly followed by the United States the following year with the it is the United States\nlaunch of Explorer I. These initial satellite launches established that satel- government\u2019s space\nlites c arrying equipment (or, in the case of Sputnik II, animals) could be exploration and\naerospace development\nplaced into orbit and utilized. As a result, NASA (the National Aeronautics\nbranch.\nand Space Administration) was established in 1958 to head up the United\n323 324\nChapter 11 Images from Space\nStates space and aeronautics program. Satellites were used for remote sens-\nCorona a United\ning reconnaissance and surveillance beginning with the Corona program\nStates government\nsatellite remote in 1960.\nsensing program A Corona satellite had a camera onboard that took pictures of areas on the\nutilizing film-based ground, and then it would eject the film from the satellite to be collected by a\ncamera equipment,\nUnited States plane. The film was then developed and interpreted to p rovide\nwhich was in operation\nintelligence information about locations around the world (see Figure 11.1\nfrom 1960\u20131972.\nfor an example of Corona imagery). Corona and its counterpart\u2014the USSR\u2019s\nZenit satellite\u2014provided numerous images from orbit, with C orona itself pro-\nviding an e stimated 800,000 remotely sensed satellite images. C orona ceased\noperation in 1972 and its imagery was declassified after 1995, although satel-\nlite remote sensing has long since continued. A series of s atellites is now used\nby the United States government for acquiring digital imagery from orbit of\nareas around the world.\nBeyond government surveillance, satellites are used today for a wide va-\nriety of purposes, constantly collecting data and imagery of the world below.\nComparing satellite imaging to Chapter 9\u2019s aerial photography methods of re-\nmote sensing, several advantages to using a satellite instead of an aircraft are\napparent. Satellites are constantly orbiting Earth and taking images\u2014there\u2019s\nFIGURE 11.1 Corona\nimagery of Dolon Air Field\nin the former USSR (taken\nin 1966). (Source: National\nReconnaissance Office)\nTransports\nBombers 325\nHow Do Remote Sensing Satellites Collect Data?\nno need to wait for a plane to fly over an area. Satellites can also image a much\nlarger area than a single aerial photograph can. Also, remote sensing satel-\nlites provide global coverage and are not restricted to geographic boundaries\nor constraints the way aircraft are because satellites orbit hundreds of miles\nabove Earth\u2019s surface.\nHow Do Remote Sensing Satellites\nCollect Data?\nRemote sensing satellites are placed in a specific orbital path around Earth.\nOne type of orbit used in remote sensing is geostationary orbit, in which sat- geostationary orbit\nellites rotate at the same speed as Earth. Since a geostationary satellite takes an orbit in which an\nobject rotates around\n24 hours to make one orbit, it\u2019s always in the same place at the same time. For\nEarth at the same\ninstance, although it\u2019s not a remote sensing system, satellite television utilizes\nspeed as Earth.\na geostationary orbit\u2014the satellite handling the broadcasting is in orbit over\nthe same area continuously, which is why a satellite reception dish needs to\nalways be pointed at the same part of the sky. Likewise, the WAAS and EGNOS\nsatellites discussed in Chapter 4 are also in geostationary orbit, so they can\nprovide constant coverage to the same area on Earth. There are some remote\nsensing satellites that use geostationary orbit to always collect information\nabout the same area on Earth\u2019s surface (such as the GOES series of satellites\nused for obtaining weather imagery).\nMany remote sensing satellites operate in near-polar orbit, a north-to- near-polar orbit an\nsouth path wherein the satellite moves close to the north and south poles while orbital path that\ncarries an object around\nit makes several passes a day about Earth. For instance, the Landsat 7 satellite\nEarth, passing close to\n(a United States satellite we\u2019ll discuss in more detail in a few pages) makes\nthe north and south\nslightly more than 14 orbits per day (each orbit takes about 99 minutes)\u2014\npoles.\nsee Figure 11.2 for Landsat 7\u2019s near-polar orbit. Also, check out Hands-on\nApplication 11.1: Examining Satellite Orbits in Real Time (page 326) for two\nmethods of tracking satellite orbits in real time.\nGround track FIGURE 11.2 The near-polar orbit of\nLandsat 7.\nNear-polar\norbit 326\nChapter 11 Images from Space\nHands-on Application 11.1\nExamining Satellite Orbits in Real Time\nThink about all the jobs that satellites are Chapter 12 can be found by expanding the Weather\ns ervicing\u2014such as communication, satellite TV, and Earth Resources option, then selecting. Earth\nweather monitoring, military applications, and GPS\u2014 Resources). When you drag the name of a satellite\nthen add remote sensing to that list. When you start from the list to the Object List window on the right\nconsidering all these uses, you can see the need for side of the screen, the orbit track and current loca-\nmany, many satellites in orbit. tion of the satellite will be shown on the display\nJSatTrak is a very cool program that you can use window. Check out some satellites like Landsat 5,\nto view (in real time) the locations of numerous Landsat 7, Terra, and others. JSatTrak has many oth-\ns atellites (including remote sensing ones). JSatTrak er features to explore (such as the 3D Earth Window\ncan be downloaded from http:\/\/www.gano.name\/ for looking at satellite tracking).\nshawn\/JSatTrak\/. When JSatTrak opens, you can ac- NASA also provided an online satellite tracking\ncess a list of satellites via the Windows pull-down tool called J-Track (also J-Track 3D) that allowed users\nmenu and by choosing Satellite Browser. A list of to similarly track satellite positions in real time. Un-\navailable satellites will appear (many of the remote fortunately, at the time of this writing, J-Track is cur-\nsensing satellites described in this chapter and in rently unavailable, but hopefully will return one day.\nWhile a satellite in continuous orbit passes over Earth, it will be imaging\nthe ground underneath it. However, during an orbit, a satellite can image only\na certain size of the ground at one time\u2014it can\u2019t see everything at once. The\nswath width the swath width is a measurement of how much ground the satellite can image\nwidth of the ground during one pass. For instance, a Landsat satellite\u2019s sensor has a swath width\narea the satellite is\nof 185 kilometers, meaning that a 185-kilometer-wide area on the ground is\nimaging.\nimaged as Landsat flies overhead. As satellites orbit, Earth is rotating beneath\nthem, meaning that several days may separate orbital paths (and thus, swaths\nof the ground being imaged) over nearby geographic areas. For example, the\npath of a Landsat satellite may pass over the middle of the United States on Day\n1 of its orbit, pass over a swath of area hundreds of miles to the east on Day 2,\nand not return to the path next to the first one until Day 8 (Figure 11.3).\nAfter a certain amount of time the satellite will completely pass over\nall areas on Earth and start again, back on the first path. For instance, the\nL andsat series of satellites are set up in an orbital path such that they will pass\nover the same swath on Earth\u2019s surface every 16 days. So when Landsat\u2019s orbit\ncarries it to the swath where it\u2019s collecting imagery of your current location it\nwill be another 16 days before it\u2019s able to image your location again. A Sun-\nSun-synchronous\nsynchronous orbit occurs when a satellite\u2019s orbit takes it over the same area\norbit an orbital path\nset up so that the on Earth at the same local time. Because of this, images collected from the sat-\nsatellite crosses the ellite will have similar sun-lighting conditions and can be used for comparing\nsame areas at the same changes to an area over time. For instance, Landsat 7 has a Sun-synchronous\nlocal time.\norbit, allowing it to cross the Equator at a constant time every day. 327\nHow Do Remote Sensing Satellites Collect Data?\nFIGURE 11.3 An\nexample of the paths\ncovered for one section of\nthe United States during\nthe 16-day revisit cycle of\na Landsat satellite.\nGround track on Day 1\nDay 2\nDay 3\nDay 4\nDay 5\nDay 6\nDay 7\nDay 8\nDay 9\nDay10\nDay11\nDay12\nDay13\nDay14\nDay15\nDay16\nThe sensors onboard the satellites have different ways of actually scan-\nning the ground and collecting information in the swath being imaged. In\nalong-track scanning, a linear array is used to scan the ground along the along-track a\npath the satellite is moving in. As the satellite moves, the detectors collect scanning method\nthe information from the entire swath width underneath them. This type of using a linear array to\ncollect data directly on\ndetector is referred to as a \u201cpushbroom\u201d sensor. Think of pushing a broom in\nthe path the satellite\na straight line across a floor\u2014the broom itself is the sensor and each bristle\nmoves on.\nof the broom represents a part of the array sensing what is on the ground\nacross-track a\nunderneath it. A second method is an across-track scanner, where a rotating\nscanning method using\nmirror moves back and forth over the swath width of the ground collecting\na rotating mirror to\ninformation. This type of detector is referred to as a \u201cwhiskbroom\u201d sensor. collect data by moving\nThink of sweeping a floor with a broom, swinging the broom back and forth the device the width of\nacross a path on the floor\u2014the broom is the sensor and the bristles represent the satellite\u2019s swath.\nthe detectors collecting the data on the ground below them. Satellite systems\nusually use one of these types of sensors to obtain the energy reflectance mea-\nsurements from the ground (see Figure 11.4 on page 328 for examples of the\noperation of both sensor types). 328\nChapter 11 Images from Space\n(a) Across-track scanner (b) Along-track scanner\nFIGURE 11.4 The\noperation of across-track Detector array\nRotating\nand along-track scanners.\nscan mirror\nDetector\nScan Scan\ndirection direction\nAngular\nfield of\nview\nGround Ground\nresolution resolution\ncell cells\nAfter a satellite\u2019s sensors have collected data, the data needs to be off-\nEROS the Earth loaded from the satellite and sent to Earth. Because data collection is con-\nResources Observation\ntinuous, this information needs to get to people on the surface to process it so\nScience Center; located\nit can be utilized. The Earth Resources Observation Science (EROS) Center\noutside Sioux Falls,\nSouth Dakota, which located outside Sioux Falls, South Dakota, is one of the downlink stations in\nserves (among many the United States that receives data from numerous remote sensing satellites.\nother things) as a Satellite data can be directly sent to a receiver at a station or transmitted to\ndownlink station for\nother tracking and data relay satellites, which then send the data to Earth.\nsatellite imagery.\nOnce received on the ground, the data can be processed into imagery.\nThinking Critically with Geospatial Technology 11.1\nWhat Effect Does Satellite Remote Sensing Have on Political Borders?\nWhen imagery is acquired by satellite remote sens- these places are being ignored by the \u201ceye in the\ning for any part of the globe, the satellite sensors sky\u201d looking down from Earth\u2019s orbit. Satellites are\neffectively ignore political boundaries. After the not restricted by \u201cno-fly zones\u201d or other restrictions\n2010 earthquake in Haiti, imagery of the country involving a country\u2019s airspace and are capable of\nwas quickly acquired by satellites and made avail- acquiring imagery of nearly anywhere. Given these\nable for rescue and recovery efforts. When satel- types of imaging capabilities, what effect does ac-\nlite images of nuclear facilities in North Korea or quisition of images via satellites have on political\nIran are shown on the news, the satellite\u2019s sensors borders? There are many areas that can\u2019t be ap-\nwere able to image these places from orbit. Satel- proached from the ground without causing inter-\nlites are acquiring global imagery of events ranging national tensions, but these same places can easily\nfrom the 2010 Olympics in Vancouver to parades in be viewed from space. How are political boundaries\nBeijing, and in all cases the political boundaries of affected by these types of geospatial technologies? 329\nWhat Are the Capabilities of a Satellite Sensor?\nWhat Are the Capabilities\nof a Satellite Sensor?\nA satellite sensor has four characteristics that define its capabilities: spatial\nresolution, radiometric resolution, temporal resolution, and spectral reso-\nlution. In Chapter 10 we discussed the concept of resolution in the context\nof spatial resolution, the area on the ground represented by one pixel in spatial resolution\na satellite image. A sensor\u2019s spatial resolution will affect the amount of de- the ground size\ntail that can be determined from the imagery. For instance, a sensor with a represented by one\npixel of satellite\n1-meter spatial resolution has much finer resolution than a sensor with spa-\nimagery.\ntial resolution of 30 meters (see Figure 11.5 for examples of the same area\nviewed by different spatial resolutions). A satellite\u2019s sensor is fixed with one p an-sharpening fusing\na higher-resolution\nspatial resolution for the imagery it collects; for instance, a 30-meter resolu-\npanchromatic band\ntion sensor cannot be \u201cadjusted\u201d to collect 10-meter resolution images.\nwith lower-resolution\nHowever, if two bands (one with a higher resolution than the other) are used multispectral bands\nby the sensor to image the same area, the higher-resolution band can be used to to improve the clarity\nsharpen the resolution of the lower-resolution band. This technique is referred to and detail seen in the\nimage.\nas pan-sharpening because the higher-resolution band used is a p anchromatic\nband. As discussed in Chapter 10, in terms of imagery, a panchromatic sensor panchromatic\nwill be measuring only one large band of wavelengths at once (usually the entire sensor a sensor that\ncan measure one range\nvisible portion of the spectrum or the entire visible and part of the near-infrared\nof wavelengths.\nspectrum). For example, a satellite (like Landsat 7) that senses the blue, green,\nand red portions of the electromagnetic spectrum at 30-meter resolution could\nalso have a sensor equipped to view the entire visible portion of the spectrum\nand part of the near-infrared spectrum as a panchromatic band at 15-meter\nresolution. This panchromatic image can then be fused with the 30-meter reso-\nFIGURE 11.5 The same\nlution color imagery to create a higher-resolution color composite. Many satel-\narea on the landscape\nlites with high spatial-resolution capabilities sense in a panchromatic band that\nas viewed by three\nallows for pan-sharpening of the imagery in the other bands. different sensors, each\nAs mentioned in Chapter 10, a sensor scales the energy measurements having different spatial\ninto several different ranges (referred to as quantization levels) to assign resolutions. (Source: NASA\nMarshall Space Flight Center) 330\nChapter 11 Images from Space\nbrightness values to a pixel. The sensor\u2019s radiometric resolution refers to\nradiometric\nits ability to measure fine differences in energy. For instance, a sensor with\nresolution a sensor\u2019s\nability to determine 8-bit radiometric resolution (also mentioned in Chapter 10) places the energy\nfine differences in radiance values on a scale of 0 (lowest radiance) to 255 (highest radiance).\na band of energy The wider the range of measurements that can be made, the finer the sen-\nmeasurements.\nsor\u2019s radiometric resolution is. The radiometric resolution (and the resultant\nnumber of levels) is a measure of the number of the sensor\u2019s bits of precision.\nTo calculate the number of levels, take the number of bits and apply it as an\nexponent to the number 2. For instance, a 6-bit sensor would have 64 levels\n(26) ranging from a value of 0 to a value of 63. An 8-bit sensor would have 256\nlevels (28) ranging from 0 to 255. An 11-bit sensor would have 2048 levels\n(211) ranging from 0 to 2047.\nThe finer a sensor\u2019s radiometric resolution, the better it can discriminate\nbetween smaller differences in energy measurements. For example, a 2-bit\nsensor (22 or four values) would have to assign every pixel in an image a\nbrightness value of 0, 1, 2, or 3, which would result in most pixels having the\nsame value, and it would be difficult to distinguish between items or to create\neffective spectral signatures. However, an 11-bit sensor would assign a value\ntemporal resolution a anywhere from 0 to 2047, creating a much wider range of value levels and\nsensor\u2019s capability that allowing much finer distinctions to be made between items. See Figure 11.6\ndetermines how often for examples of differing effects of radiometric resolution on an image when\nit can view the same\nperforming remote sensing.\nlocation on the ground.\nThe sensor\u2019s temporal resolution refers to how often it can return to im-\noff-nadir viewing the age the same spot on Earth\u2019s surface. For instance, a sensor with 16-day tempo-\ncapability of a satellite\nral resolution will collect information about the swath containing your house\nto observe areas\nand not return to image it again until 16 days have passed. The finer a sensor\u2019s\nother than the ground\ntemporal resolution, the fewer days it will take between return times. A way to\ndirectly underneath it.\nimprove a sensor\u2019s temporal resolution is to use off-nadir viewing capability,\nin which the sensor is not fixed to sense what\u2019s directly underneath it (the nadir\npoint). A sensor that is capable of viewing places off-nadir can be pointed to im-\nFIGURE 11.6 The effect\nof different levels of age locations away from the current orbital path. Using off-nadir viewing can\nradiometric resolution on greatly increase a sensor\u2019s temporal resolution, as it can image a target several\nan image (simulated 2-bit, times during its orbits, even if the satellite isn\u2019t directly overhead. This capabil-\n4-bit, and actual 8-bit\nity is a great help in monitoring conditions that can change quickly or require\nimagery of one band).\n(Source: NASA) 331\nWhat Is a Landsat Satellite and What Does It Do?\nrapid responses (such as mitigating disasters resulting from hurricanes, torna-\ndoes, or widespread fires).\nThe sensor\u2019s last parameter is spectral resolution, which refers to the spectral resolution\nbands and their wavelengths measured by the sensor. For instance, a sensor the bands and\nwavelengths being\non the QuickBird satellite can measure four bands of energy simultaneously\nmeasured by a sensor.\n(wavelengths in the blue, green, red, and near-infrared portions of the spec-\ntrum), and the sensor onboard the Landsat 7 satellite can sense in seven differ-\nent wavelengths at the same time. A sensor with a higher spectral resolution\ncould examine several finer intervals of energy wavelengths (for example, the\nMODIS sensor onboard the Terra satellite\u2014see Chapter 12\u2014can sense in 36\ndifferent bands at once). Being able to sense numerous smaller wavelengths\nin the near-infrared portion of the spectrum gives much finer spectral resolu-\nLandsat a long-\ntion than just sensing one wider wavelength of near-infrared energy.\nrunning United States\nremote sensing project\nthat had its first\nWhat Is a Landsat Satellite satellite launched in\n1972 and continues\nand What Does It Do?\ntoday.\nMSS the Multi-Spectral\nThis chapter and Chapter 10 have made mention of a satellite system called\nScanner aboard Landsat\nLandsat but haven\u2019t really explained what it is, how it operates, or why it\u2019s\n1 through 5.\nso important. What would become the United States\u2019 Landsat Program was\nmultispectral\nconceived in the late 1960s as a resource for doing remote sensing of Earth\nsensor a sensor that\nfrom space. The program\u2019s first satellite, Landsat 1, was launched in 1972 (see\ncan measure multiple\nFigure 11.7 for a graphical timeline of Landsat launches). Landsat 1\u2019s sensor different wavelength\nwas the Landsat MSS (Multi-Spectral Scanner) and had a temporal resolu- bands simultaneously.\ntion of 18 days. The MSS would continue to be part of the Landsat missions\nthrough Landsat 5. Landsat 4 and 5\u2019s MSS was capable of sensing four bands\n(red, green, plus two bands in the near infrared) at 79-meter spatial resolution FIGURE 11.7 The\nand 6-bit radiometric resolution. Like the name implies, MSS is an example history of the Landsat\nprogram launches, along\nof a multispectral sensor, which can measure multiple wavelengths at once.\nwith upcoming dates.\nLANDSAT 1\n1972\nLANDSAT 2\n1975\nLANDSAT 3\n1978 LANDSAT 4\n1982\nLANDSAT 5\n1984 LDCM\n2012\nLANDSAT 7\n1999\n1970 1975 1980 1985 1990 1995 2000 2005 2010 2015 2020\nYear 332\nChapter 11 Images from Space\nLandsats 4 and 5 were also equipped with a new remote sensing device\nTM the Thematic called the TM (Thematic Mapper). The TM was also a multispectral sensor\nMapper sensor onboard that allowed for sensing in seven bands in the electromagnetic spectrum\nLandsat 4 and 5. simultaneously as follows:\na Band 1: Blue (0.45 to 0.52 micrometers)\na Band 2: Green (0.52 to 0.60 micrometers)\na Band 3: Red (0.63 to 0.69 micrometers)\na Band 4: Near infrared (0.76 to 0.90 micrometers)\na Band 5: Middle infrared (1.55 to 1.75 micrometers)\na Band 6: Thermal infrared (10.4 to 12.5 micrometers)\na Band 7: Middle infrared (2.08 to 2.35 micrometers)\nThese bands were chosen for TM because they were key to a variety of\nstudies (such as bands 3 and 4 being used for vegetation monitoring and NDVI\nas discussed in Chapter 10), while band 5 is useful when examining the water\ncontent of plants and band 7 is useful in separating various rock and mineral\ntypes. The thermal band 6 enabled the monitoring of heat energy in a variety\nof settings. The TM bands had improved radiometric resolution over MSS of\n8 bits while the spatial resolution of bands 1-5 and 7 was improved to 30 me-\nters (band 6 had a spatial resolution of 120 meters) and had a swath width\nof 185 kilometers. TM also had an improved temporal resolution of 16 days.\nLandsat 4 was launched in 1982 and ended its mission in 1993. Landsat 5\nLandsat 5 the fifth\nwas launched in 1984 and still continues sensing and transmitting v iable data\nLandsat mission,\nto Earth today. Landsat 5 is more than twenty-five years old, so it should raise\nlaunched in 1984,\nwhich carries both the some curiosity that it\u2019s still relied upon so heavily, given the importance of\nTM and MSS sensors. Landsat in remote sensing. Landsat 5 was not likely intended to be the corner-\nstone of the program\u2014however, Landsat 6 (launched in 1993) did not achieve\nLandsat 7 the seventh\nLandsat mission, orbit and was lost, continuing the mission of Landsat 5.\nlaunched in 1999, Landsat 7 (Figure 11.8) was launched in 1999, not carrying either the\nwhich carries the ETM+ MSS or the TM, but a new sensor called the ETM+ (Enhanced Thematic Map-\nsensor.\nper Plus). This new device senses the same 7 bands as TM at the same swath\nETM+ the Enhanced width and radiometric and spatial resolutions (although the thermal band 6\u2019s\nThematic Mapper sensor spatial resolution was improved to 60 meters), but also added a new eighth band,\nonboard Landsat 7.\na panchromatic one with a spatial resolution of 15 meters. With this improved\nLandsat scene a spatial resolution, the panchromatic band could be used to pan-sharpen the\nsingle image obtained quality of the other imagery. Landsat 7\u2019s temporal resolution was still 16 days\nby a Landsat satellite\nbut could acquire about 250 images per day (see Figure 11.9 for an example of\nsensor.\nLandsat 7 imagery and Hands-on Application 11.2: Streaming Landsat Imagery\nWorldwide Reference on page 334 for some examples of recent imagery collected by Landsat).\nSystem the global\nEach Landsat scene measures an area about 170 kilometers long by\nsystem of Paths and\n183 kilometers wide (for the TM and ETM+ sensors) and is considered a\nRows that is used to\nidentify what area separate image. At this size, it would take several images just to cover one\non Earth\u2019s surface state. Thus, if you want access to imagery of a particular area, you need to\nis present in which know which scene that ground location is in. Landsat uses a Worldwide Ref-\nLandsat scene.\nerence System that divides the entire globe into a series of Paths (columns) 333\nWhat Is a Landsat Satellite and What Does It Do?\nFIGURE 11.8 Landsat 7\nin action. (Source: Landsat\nimagery courtesy of NASA\nGoddard Space Flight Center\nand U.S. Geological Survey)\nFIGURE 11.9 A Landsat 7\nBeijing\nimage of Beijing, China,\nfrom 2000. (Source: Courtesy\nof NASA Images acquired by\nLandsat 7)\nForbidden City\nTiananmen Square 334\nChapter 11 Images from Space\nHands-on Application 11.2\nStreaming Landsat Imagery\nFor a dynamic look at streaming Landsat imagery, orbit of the Landsat satellite that acquired the im-\nthe EarthNow! Website (sponsored by the USGS and agery. The main EarthNow! window will show the\navailable online at http:\/\/earthnow.usgs.gov) is a imagery collected by Landsat (with attached labels\ngreat tool (use of EarthNow! requires you to have added by those who processed the data). Check out\nJava loaded on your computer). EarthNow! shows re- EarthNow! to see where recent Landsat imagery\ncent Landsat imagery (usually from today\u2019s date)\u2014a was collected and what the images of the ground\nsmall map on the screen will show the position and that are obtained by Landsat look like.\nand Rows. For instance, imagery of Columbus, Ohio, would be from the scene\nat Path 19, Row 32, while imagery of Toledo would be found in Path 20, Row\n31 (Figure 11.10). At an area of 170 kilometers long by 183 kilometers wide,\na scene will often be much larger than the area of interest\u2014for example, Path\n19, Row 32 contains much more than just Columbus, it encompasses the en-\ntire middle section of the state of Ohio, so to examine only Columbus, you\nwould have to work with a subset of the image that matches the city\u2019s bound-\naries. Also, sometimes an area of interest may straddle two Landsat scenes\u2014\nfor instance, Mahoning County in Ohio is entirely located in Path 18, but part\nof the country is in Row 31 and part of it is in Row 32. In this case, both im-\nages would need to be merged together and then have the boundaries of the\ncounty used to define the subsetted area.\nThere\u2019s been a growing trend to make Landsat imagery available to the\npublic at a reduced cost. Thanks in part to the activities of groups such as\nFIGURE 11.10 The total\nPath 20 Path 19 Path 18\nLandsat coverage for the\nstate of Ohio. It requires\nnine Landsat scenes to\ncover the entire state.\nRow 31\nRow 32\nRow 33 335\nWhat Is a Landsat Satellite and What Does It Do?\nFIGURE 11.11 The\nUSGS Global Visualization\nViewer utility used to\nexamine and download a\nLandsat ETM+ scene (note\nthe size of one scene in\nrelation to the entire state\nof Michigan). (Source: USGS)\nOhioView and AmericaView (see Chapter 15) a lot of Landsat imagery is freely\nGloVis the Global\navailable for distribution over the Internet. The USGS maintains an online\nVisualization Viewer\nutility called GloVis (Global Visualization Viewer) that is used to distribute\nset up by the USGS\nLandsat (and other satellite) imagery across the Internet (see Figure 11.11 for viewing and\nand Hands-on Application 11.3: Viewing Landsat Imagery with GloVis). In 2008, downloading satellite\nthe USGS announced that the entire Landsat 7 archive of imagery, plus the imagery.\nHands-on Application 11.3\nViewing Landsat Imagery with GloVis\nWith GloVis, Landsat imagery from multiple dates appear in a separate window, allowing you to select\n(and multiple sensors) can be viewed, examined (for imagery by a specific path and row, obtain informa-\nthe amount of cloud cover and the available imagery tion about the scene (such as the percentage of cloud\ndates), and downloaded for use by a GIS or image pro- cover), and available imagery from different sensors.\ncessing or viewing software package. To use GloVis, If the word \u201cdownloadable\u201d appears in red in the\nopen your Web browser and go to http:\/\/ glovis.usgs. upper-left-hand corner of the image, the raw band\ngov\u2014also make sure that you have Java installed on data for that Landsat scene can be obtained directly\nyour machine and your browser will allow for a popup via GloVis. Take a look at your local area ( finding it\nwindow from this Website. Either select a location via Path\/Row combination or latitude\/longitude)\nfrom the map or enter latitude\/longitude coordinates to see what Landsat imagery is available for those\nfor a place. The Landsat scenes for the area will coordinates. 336\nChapter 11 Images from Space\navailable data from Landsats 1 through 5, would be made freely available and\ndistributed via GloVis. This decision allows free worldwide access to a vast li-\nbrary of remote sensing data for everyone, and this availability of the data can\nonly help with furthering research and education in remote sensing.\nSLC the Scan Line In 2003, Landsat 7\u2019s SLC (Scan Line Corrector) failed and could not be\nCorrector in the ETM+ repaired. Due to the malfunctioning SLC, the ETM+ produces imagery con-\nsensor. Its failure in taining missing data. Without a working SLC, an ETM+ image contains only\n2003 causes Landsat 7\nabout 75% of the data that should be there. ETM+ data is still used by sub-\nETM+ imagery to not\nstituting pixels from other Landsat 7 scenes from the same area but a differ-\ncontain all data from a\nscene. ent date, or by using other methods to fill in the gaps for the missing pixel\nvalues. With these types of issues, Landsat 5 TM imagery is still being used by\nsome researchers for a source of remotely sensed Landsat data. However, with\nLandsat 5 being years beyond its mission life (and having had some mechani-\ncal issues in recent years) and Landsat 7 having issues with collecting imagery\ndue to the SLC failure, the question that has to be raised is: What is the future\nof the Landsat program, as there has been no Landsat launch since 1999?\nThe next planned Landsat launch is the Landsat Data Continuity Mission\nLDCM the Landsat (LDCM) scheduled for the end of 2012. The LDCM satellite\u2019s instrument will be\nData Continuity the next iteration of Landsat sensors (similar to ETM+), called the OLI (Opera-\nMission\u2014the future of\ntional Land Imager). Unlike ETM+, a thermal band will not be part of OLI, but\nthe Landsat program,\ntwo new bands, aimed at monitoring clouds and coastal zones, will be developed\nscheduled to be\ninstead. A separate instrument on the satellite, called TIRS (the Thermal Infra-\nlaunched in 2012.\nRed Sensor), will collect remotely sensed thermal data. LDCM is estimated to\nOLI the Operational\ncollect about 400 images per day. There\u2019s a strong need for continuing to obtain\nLand Imager, the\nLandsat imagery for monitoring larger-scale phenomena such as environmental\nmultispectral sensor\nthat will be onboard effects, land-use changes, and urban expansion. Long-term gaps in the dates of\nLDCM. imagery would have a huge negative impact on these types of studies.\nBeing able to monitor large-scale conditions on Earth on a regular basis\n(at 30-meter resolution) with a variety of multispectral bands (including near-\ninfrared, middle infrared, and thermal characteristics) will provide a rich data-\nset for numerous applications. With Landsat imagery stretching back for more\nthan 30 years providing a constant source of observations, long-term tracking\nof environmental phenomena (such as ongoing land-use or land-cover changes\nfor an area) can be performed. For instance, the growth of urban areas over\ntime can be observed every 16 days (cloud cover permitting, of course), en-\nabling planners to determine the spatial dimensions of new developments\nacross a region. Figure 11.12 shows an example of utilizing Landsat imagery\nover time to track the patterns of deforestation in Brazil through examination of\nclear-cut areas and new urban areas built up in the forest. The color composite\nimages show the rainforest in dark green shades, while the deep-cut patterns of\nforest clearing are clearly visible and the encroachment of development into the\nforest can be tracked and measured over time.\nBeyond examining land-cover changes over time, the multispectral capa-\nbilities of Landsat imagery can be used for everything from measuring crop\ndrought conditions to glacier features to the aftermath of natural disasters.\nBy examining the different band combinations provided by Landsat imagery, 337\nWhat Is a Landsat Satellite and What Does It Do?\nFIGURE 11.12 Multiple\ndates of Landsat imagery\nof Rondonia, Brazil,\nshowing the forested\nareas (in dark green) and\nthe growing patterns of\ndeforestation over time\n(in lighter shades). (Source:\nNASA\/Goddard Space Flight\nCenter Scientific Visualization\nStudio)\nscientists can find clues to understanding the spread of phytoplankton in\ncoastal waters, enabling environmentalists to try and head off potential al-\ngae blooms or red-tide conditions. Threats to animal habitats can be assessed\nthrough forest or wetland monitoring via Landsat imagery; Landsat images\nalso allow scientists to track the movements of invasive species that threaten\ntimber industries. Hands-on Application 11.4: Applications of Landsat Imagery\nlets you dig further into the myriad uses of Landsat remote sensing. EO-1 a satellite\nNASA also operates a remote sensing project with capabilities related to launched in 2000 and\nthe Landsat program. EO-1 (Earth Observing 1) was launched in 2000 as part set to orbit 1 minute\nafter Landsat 7.\nof NASA\u2019s New Millennium Program and placed in orbit so it was traveling\nHands-on Application 11.4\nApplications of Landsat Imagery\nLandsat imagery has been adopted for numerous Mapping, and Environmental Uses. Select one of the\nenvironmental monitoring, measuring, and track- categories of interest to you or related to your field\ning applications. To further investigate these uses, and explore some of the case studies and applica-\nopen your Web browser and go to http:\/\/landsat. tions presented in that column. How is Landsat im-\ngsfc.nasa.gov\/about\/appl_matrix.html. This Web- agery being applied to these fields, and how do the\nsite, maintained by NASA, highlights several dif- long-term consistent uses of multispectral imagery\nferent categories of Landsat imagery applications, lend themselves to these kinds of applications?\nincluding Agricultural, Coastal, Geologic, Hydrologic, 338\nChapter 11 Images from Space\nThinking Critically with Geospatial Technology 11.2\nWhat if There Was No Landsat Data Continuity Mission?\nThe Landsat Program has been ongoing since delayed or if disaster should strike and LDCM should\n1972, providing global coverage of imagery and a fail to achieve orbit (like Landsat 6), and Landsats 5\nmainstay of the United States remote sensing pro- and 7 would somehow be unavailable for imagery\ngram. However, it currently relies on the Landsat 5 acquisition, then what happens? Would the United\n(launched in 1984, long past its shelf life but still States have to purchase imagery from other sources\ndoing the job) and Landsat 7 (launched in 1999 and or countries to make up the data gap until another\nnow impaired due to the SLC error) satellites. With Landsat mission could be launched? Are there other\nthe next stage of the Landsat Program (LDCM) not sources that the United States government could\nplanned for launch until late 2012, the potential is use to make up for the lack of Landsat imagery?\nthere for a gap in the acquisition of Landsat imag- What impact would a lack of continuous Landsat im-\nery if Landsat 5 should fail and Landsat 7\u2019s issues agery have on the numerous applications and stud-\nshould become unsustainable. While this may sound ies previously described?\nlike something of a doomsday scenario, if LDCM is\nright behind Landsat 7 (within 1 minute). EO-1 features a multispectral sensor\nALI the multispectral called ALI (Advanced Land Imager) that is similar to ETM+ but can image 10\nsensor onboard EO-1. different bands in a 37-kilometer swath width at 30-meter resolution (along\nwith a 10-meter panchromatic band). ALI was also used to test and validate\nHyperion the\nhyperspectral sensor sensor technology planned for use in LDCM. EO-1 also carries H yperion, a\nonboard EO-1. sensor that detects 220 bands at 30-meter resolution in a smaller swath width\nof 7.7 kilometers. Hyperion is an example of a hyperspectral sensor, which\nhyperspectral\nsensor a sensor that can measure hundreds of wavelengths simultaneously.\ncan measure hundreds Although Landsat data has been widely used for numerous environmen-\nof different wavelength tal studies, 30-meter resolution isn\u2019t nearly enough to do very detailed, high-\nbands simultaneously.\nresolution studies. At 30-meter resolution, broad land-use change modeling\nand studies can be performed, but a much finer spatial resolution would be\nrequired to pick out individual details in an image. High-resolution sensors\nhave a different purpose than Landsat imagery\u2014with a very fine spatial reso-\nlution, crisp details can emerge from a scene. Spy satellites or military surveil-\nlance satellites would require this type of finer spatial resolution, but of course\nthe imagery they acquire remains classified. Much of the non-classified and\navailable high-resolution satellite imagery is obtained and distributed com-\nmercially rather than from government sources.\nWhat Satellites Have High-Resolution\nSensors?\nSPOT a satellite\nprogram operated\nby the French Space There are several remote sensing satellites with high-resolution capabilities.\nAgency and the Spot This section describes a few of the more prominent ones. The SPOT (Satellite\nImage Corporation.\nPour l\u2019Observation de la Terre) program is operated by a French company called 339\nWhat Satellites Have High-Resolution Sensors?\nSpot Image, which serves as the distributor of SPOT imagery. The SPOT series\nof satellites was developed by the French Space Agency, with the first (SPOT\n1) being launched in 1986. The most recent satellite in the series is SPOT 5,\nlaunched in 2002. SPOT 5 has a revisit cycle of 26 days but has off-nadir view-\ning capabilities that significantly improve some of its sensors\u2019 temporal resolu-\ntion. The highest spatial resolution of a SPOT 5 image is 2.5 meters, allowing a\nsignificant amount of detail to be determined from a single image.\nSPOT 5 carries three sensors:\na Vegetation-2: A sensor with a 1-kilometer spatial resolution, used for\nbroad-scale environmental monitoring.\na HRS (High Resolution Stereoscopic): A sensor with 10-meter resolution,\nused for the development of stereo imagery (see Chapter 15) or digital\nelevation models (see Chapter 13).\na HRG (High Resolution Geometric): A pair of sensors with multiple reso-\nlutions available\u201420-meter (an infrared band), 10-meter (multispec-\ntral sensor with green, red, near-infrared, and one other infrared band),\n5-meter (panchromatic), and a 2.5-meter panchromatic Supermode.\nSpot Image has announced SPOT 6 and SPOT 7 to be launched in the\nnear future with improved resolution capabilities (such as a 1.5-meter pan-\nPLEIADES a high-\nchromatic spatial resolution). A related program is the PLEIADES series of\nresolution series of\nsatellites, the first of which is projected to be launched in 2011. PLEIADES satellites.\nwill have very high-resolution panchromatic and multispectral capabilities.\nIKONOS a satellite\nLaunched in 1999, the GeoEye Company\u2019s (formerly SpaceImaging,\nlaunched in 1999 by\nInc.) IKONOS satellite imagery was one of the benchmarks of commercial SpaceImaging, Inc.\nhigh-resolution imagery. The IKONOS satellite has two sensors, a four-band (now called GeoEye),\n(blue, green, red, and near-infrared) multispectral sensor that has 4-meter which features\nmultispectral spatial\nspatial resolution and a panchromatic sensor with 1-meter spatial resolution.\nresolution of 4 meters\nBoth sensors have very high radiometric resolution (11 bits) and due to off-\nand panchromatic\nnadir viewing capabilities can achieve temporal resolution of about 3 days.\nresolution of 1 meter.\nPanchromatic or pan-sharpened 1-meter imagery provides very crisp and de-\nQuickBird a satellite\ntailed image resolution\u2014details of urban environments can be distinguished\nlaunched in 2001\nwith little difficulty, although objects smaller than 1 meter square cannot have\nby the DigitalGlobe\ndetails made out (see Figure 11.13 on page 340 for an example of IKONOS company, whose\nimagery for the kind of spatial resolution this sensor can provide). sensors have 2.4-meter\nIn 2001, another remote sensing company, DigitalGlobe, launched its multispectral resolution\nand 0.61-meter\nQuickBird satellite, whose sensors provided higher spatial resolution than\npanchromatic\nIKONOS. Like IKONOS, QuickBird featured two sensors\u2014a four-band (blue,\nresolution.\ngreen, red, and near-infrared) multispectral sensor with 2.4-meter spa-\nWorldView-1 a\ntial resolution and a panchromatic sensor with 0.61-meter spatial resolu-\nsatellite launched in\ntion. Both sensors feature very fine 11-bit radiometric resolution and with\n2007 by the Digital\noff-nadir viewing capability have a temporal resolution between 1 and 3.5 Globe company, whose\ndays. 61-centimeter imagery provides extremely high-resolution capabilities, panchromatic sensor\nallowing very crisp details to be observed in an image. In 2007, DigitalGlobe has 0.5-meter spatial\nresolution.\nlaunched WorldView-1, which featured an improved panchromatic sensor 340\nChapter 11 Images from Space\nFIGURE 11.13 An\nIKONOS image of the\ndevelopment of one of\nthe Palm Islands in Dubai,\nUnited Arab Emirates.\n(Source: Satellite Imaging\nCorporation Copyright \u00a9\n2007 GeoEye\/SIME. All Rights\nReserved.)\nWorldView-2 a\nsatellite launched\nin 2009 by the\nDigitalGlobe company,\nfeaturing an 8-band\nmultispectral sensor\nwith 1.84-meter\nresolution and a\npanchromatic sensor of\n0.46-meter resolution.\nwith 0.5-meter spatial resolution. DigitalGlobe also launched WorldView-2\nGeoEye-1 a satellite\nin October 2009 with 0.46-meter panchromatic and 1.84-meter 8-band mul-\nlaunched in 2008 by\ntispectral spatial resolution.\nthe GeoEye company,\nwhich features a The highest spatial resolution imagery currently commercially available\nspatial resolution of is from the sensors onboard the GeoEye-1 satellite, launched in 2008 by\n41 centimeters with its the GeoEye company. GeoEye-1 has two sensors, a four-band multispectral\npanchromatic sensor.\n(blue, green, red, and near infrared) with 1.65-meter spatial resolution and a\nHands-on Application 11.5\nViewing High-Resolution Satellite Imagery\nWhile the data acquired by satellites like IKONOS SPOT-5, WorldView-1, and WorldView-2. Examine\nand QuickBird is sold commercially, you can view some examples of each sensor, keeping in mind\nsamples of these satellite images at several places the specifications of each. You can zoom in on the\nonline. The Satellite Imaging Corporation\u2019s Website sample imagery from each satellite to really get a\nprovides a means for viewing high-resolution imag- sense of what can be seen via high-spatial resolu-\nery. Open your Web browser and go to http:\/\/www. tion satellites. Note that the images available on\nsatimagingcorp.com. Select the option for Gallery, the Website are just crisp graphics, not the actual\nand then select High-Resolution Sensors. Available satellite data itself.\nimagery includes GeoEye-1, IKONOS, QuickBird, 341\nKey Terms\npanchromatic sensor with 0.41-meter resolution. 41-centimeter spatial reso-\nlution represents the finest commercial resolution on the market today, cur-\nrently making GeoEye-1 the satellite with the sensors to beat when it comes\nto high-resolution imagery.\nThe GeoEye company has announced plans for the launch of the Geo-\nEye-2 satellite in 2012, which would provide 0.25-meter spatial resolution\nimagery. See Hands-on Application 11.5: Viewing High-Resolution Satellite Im-\nagery for examples of all of these high-resolution images online.\nChapter Wrapup\nSatellite remote sensing is a powerful tool for everything from covert surveil-\nlance to land-use and land-cover monitoring. The next chapter delves into a\nnew set of geospatial satellite tools\u2014the Earth Observing System, a series of\norbital environmental satellites that gather data about global processes on\nan ongoing basis. Geospatial Lab Application 15.1 returns to using MultiSpec\nand has you doing more with Landsat imagery and the multispectral capa-\nbilities of the TM sensor.\nImportant note: The references for this chapter are part of the online\ncompanion for this book and can be found at http:\/\/www.whfreeman.com\/\nshellito1e.\nKey Terms\nNASA (p. 323) Landsat 7 (p. 332)\nCorona (p. 324) ETM+ (p. 332)\ngeostationary orbit (p. 325) Landsat scene (p. 332)\nnear-polar orbit (p. 325) Worldwide Reference System\nswath width (p. 326) (p. 332)\nSun-synchronous orbit (p. 326) GloVis (p. 335)\nalong-track (p. 327) SLC (p. 336)\nacross-track (p. 327) LDCM (p. 336)\nEROS (p. 328) OLI (p. 336)\nspatial resolution (p. 329) EO-1 (p. 337)\npan-sharpening (p. 329) ALI (p. 338)\npanchromatic sensor (p. 329) Hyperion (p. 338)\nradiometric resolution (p. 330) hyperspectral sensor (p. 338)\ntemporal resolution (p. 330) SPOT (p. 338)\noff-nadir viewing (p. 330) PLEIADES (p. 339)\nspectral resolution (p. 331) IKONOS (p. 339)\nLandsat (p. 331) QuickBird (p. 339)\nMSS (p. 331) WorldView-1 (p. 339)\nmultispectral sensor (p. 331) WorldView-2 (p. 340)\nTM (p. 332) GeoEye-1 (p. 340)\nLandsat 5 (p. 332) 11.1\nGeospatial Lab Application\nLandsat Imagery\nThis chapter\u2019s lab application builds off the remote sensing basics of the pre-\nvious chapter and returns to using the MultiSpec program. In this exercise,\nyou\u2019ll be starting with a Landsat TM scene and creating a subset of it to work\nwith. During the lab, you\u2019ll examine the uses for several Landsat TM band\ncombinations in remote sensing analysis.\nObjectives\nThe goals for this exercise are:\na Further familiarizing yourself and work with satellite imagery in MultiSpec.\na Creating a subset image of a Landsat scene.\na Examining different Landsat bands in composites and comparing the\nresults.\na Examining various landscape features in multiple Landsat bands and\nc omparing them.\na Applying visual image interpretation techniques to Landsat imagery.\nObtaining Software\nThe current version of MultiSpec is available for free download at http:\/\/\ncobweb.ecn.purdue.edu\/~biehl\/MultiSpec.\nImportant note: Software and online resources sometimes change fast.\nThis lab was designed with the most recently available version of the software\nat the time of writing. However, if the software or Websites have significantly\nchanged between then and now, an updated version of this lab (using the new-\nest versions) is available online at http:\/\/www.whfreeman.com\/shellito1e.\nLab Data\nCopy the folder \u2018Chapter11\u2019\u2014it contains:\na A Landsat 5 TM satellite image (called \u2018neohio.img\u2019) from 9\/11\/2005 of\nnortheast Ohio, which was constructed from data supplied via OhioView.\na The Landsat TM image bands refer to the following portions of the elec-\ntromagnetic (EM) spectrum in micrometers ((cid:2)m):\n\u2022 Band 1: Blue (0.45 to 0.52 (cid:2)m)\n\u2022 Band 2: Green (0.52 to 0.60 (cid:2)m)\n342 343\nLandsat Imagery\n\u2022 Band 3: Red (0.63 to 0.69 (cid:2)m)\n\u2022 Band 4: Near infrared (0.76 to 0.90 (cid:2)m)\n\u2022 Band 5: Middle infrared (1.55 to 1.75 (cid:2)m)\n\u2022 Band 6: Thermal infrared (10.4 to 12.5 (cid:2)m)\n\u2022 Band 7: Middle infrared (2.08 to 2.35 (cid:2)m)\nAlso, keep in mind that Landsat TM imagery has a 30-meter spatial reso-\nlution (except for the thermal infrared band, which is 120 meters).\nLocalizing This Lab\nAlthough this lab focuses on a section of a Landsat scene from northeast Ohio,\nLandsat imagery is available for download via GloVis at http:\/\/glovis.usgs.\ngov. This site will provide the raw data that will have to be imported into Mul-\ntiSpec and processed to use in the program.\nAlternatively, NASA has information for obtaining free Landsat data for\nuse in MultiSpec online at http:\/\/change.gsfc.nasa.gov\/create.html. This\nWebsite provides instructions for acquiring free data for your own area and\ngetting it into MultiSpec format.\n11.1 Opening Landsat Scenes in MultiSpec\n1. Start MultiSpec.\n2. MultiSpec will start with an empty text box (which will give you updates\nand reports of processes in the program). You can minimize the text box\nfor now.\n3. To get started with the Landsat image, select Open from the File pull-\ndown menu. Alternatively, you can select the Open icon from the\ntoolbar:\n(Source: Purdue Research Foundation)\n4. Navigate to the \u2018Chapter11\u2019 folder and select \u2018neohio.img\u2019 as the file to\nopen and click Open.\n5. A new dialog box will appear to let you set the display specification for\nthe \u2018neohio\u2019 image. 344\nChapter 11 Images from Space\n(Source: Purdue Research Foundation)\n6. Under Channels, you will see the three color guns available to you (red,\ngreen, and blue). Each color gun can hold one band (see the Lab Data\nsection for information on which bands correspond to which parts of the\nelectromagnetic spectrum). The number listed next to each color gun\nrepresents the band being displayed with that gun.\n7. Display band 4 in the red color gun, band 3 in the green color gun, and\nband 2 in the blue color gun.\n8. Accept the other defaults for now and click OK.\n9. A new window will appear and the \u2018neohio\u2019 image will load in it.\n11.2 S ubsetting Images and Examining\nLandsat Imagery\nRight now, you\u2019re working with an entire Landsat TM scene, which is an area\nroughly 170 kilometers long by 183 kilometers wide (as shown in the graphic\nat the top of page 345\u2014you are using the scene encompassed by path 18, row\n31). For this lab, we want to focus only on the area surrounding downtown\nCleveland. Thus, you will have to create a subset\u2014in essence, \u201cclipping\u201d out\nthe area that you\u2019re interested in and creating a new image from that. 345\nLandsat Imagery\nPath 20 Path 19 Path 18\nRow 31\nPath 18\nRow 31\nRow 32\nRow 33\n1. Zoom to the part of the \u2018neohio\u2019 image that shows Cleveland (like in the\ngraphic below):\n(Source: NASA\/Purdue Research Foundation)\n2. In the image, you should be able to see many features that make up\ndowntown Cleveland\u2014the waterfront area, much urban development,\nmajor roads, and water features. 346\nChapter 11 Images from Space\n3. In order to create a new image that shows only Cleveland (a suggested\nregion is shown in the second graphic on page 345), select Reformat\nfrom the Processor pull-down menu, then select Change Image File\nFormat.\n4. You could draw a box around the area you want to subset using the\ncursor and the new image that\u2019s created will have the boundaries of the\nbox you\u2019ve drawn on the screen. However, for the sake of consistency in\nthis exercise, use the following values for the Area to Reformat:\na. Line:\n\u2022 Start 4305\n\u2022 End 4897\n\u2022 Interval 1\nb. Column:\n\u2022 Start 641\n\u2022 End 1313\n\u2022 Interval 1\nLeave the other defaults alone and click OK.\n(Source: Purdue Research Foundation)\n5. In the Save As dialog box that appears, save this new image in the\n\u2018Chapter11\u2019 folder (call it \u2018clevsub.img\u2019). Choose Multispectral for the\nSave as Type option (from the pull-down menu options next to Save as\nType). When ready, click Save.\n6. Back in MultiSpec, minimize the window containing the \u2018neohio\u2019 image.\n7. Open the \u2018clevsub\u2019 image you just created in a new window (in the\nOpen dialog box, you may have to change the Files of Type that it\u2019s\nasking about to \u201cAll Files\u201d to be able to select the \u2018clevsub\u2019 image option). 347\nLandsat Imagery\n8. Open the \u2018clevsub\u2019 image with a 4-3-2 combination (band 4 in the red\ngun, band 3 in the green gun, and band 2 in the blue gun).\n9. Use the other defaults for the Enhancement options (stretch is Linear\nand range is Clip 2% of tails).\n10. In the Set Histogram Specifications dialog box that opens, select\nthe Compute new histogram method, and use the default Area to\nHistogram settings.\n11. Click OK when ready. The new subset image shows that the Cleveland\narea is ready to use.\n12. Zoom into the downtown Cleveland area and especially areas along\nthe waterfront. Answer Questions 11.1 and 11.2. (You will want to also\nopen Google Earth and compare the Landsat image to its very crisp\nresolution imagery when answering these questions.)\nQuestion 11.1 What kinds of features on the Cleveland waterfront cannot\nbe distinguished at the 30-meter resolution you\u2019re examining?\nQuestion 11.2 Conversely, what specific features on the Cleveland\nwaterfront are apparent at the 30-meter resolution you\u2019re examining?\n11.3 E xamining Landsat Bands\nand Band Combinations\n1. Zoom in the waterfront area, looking at Cleveland Browns Stadium and\nits immediate surrounding area.\nThis area for\nnext section of\nthe exercise\n(Source: Purdue Research Foundation)\n2. Open another version of the \u2018clevsub\u2019 image using the 3-2-1 combination. 348\nChapter 11 Images from Space\n3. Arrange the two windows on the screen so that you can examine them\ntogether, expanding and zooming in as needed to be able to view the\nstadium in all three band combinations.\nQuestion 11.3 Which of the two band combinations best brought the\nstadium and its field to prominence?\nQuestion 11.4 Why did the band combination you chose best help in\nviewing the stadium and the field? (Hint: You may want to do some brief\nonline research into the nature of the new stadium and its field.)\n4. Return to the view of Cleveland\u2019s waterfront area. You\u2019ll now be\nexamining the water features (such as Lake Erie and the river). Paying\ncareful attention the water features, open four display windows and\nexpand and arrange them side by side to look at differences between\nthem. Create the following image composites (one for each window):\na. 3,2,1\nb. 7,5,4\nc. 4,2,1\nd. 1,1,1\nQuestion 11.5 Which band combination(s) best let you separate water\nbodies from land? Why?\n5. Return to the view of Cleveland\u2019s waterfront area. Focus on the urban\nfeatures and vegetated features (zoom and pan where necessary to get\na good look at urbanization). Open new windows with some new band\ncombinations, noting how things change with each one as follows:\na. 7,4,2\nb. 1,2,3\nc. 4,3,2\nQuestion 11.6 Which band combination(s) best let you separate urban\nareas from other forms of land cover (that is, vegetation, trees, etc.)? Why?\n6. Open a new display window and examine the full view of the\nentire \u2018clevsub\u2019 image. Open this window with the following band\ncombination:\na. 6,6,6\n7. Zoom in and move around the Cleveland waterfront area we\u2019ve been\nexamining.\nQuestion 11.7 Why is the image so \u201cblurry\u201d or \u201cfuzzy\u201d in this combination\n(compared to all the other band combinations you\u2019ve looked at in this\nexercise)? 349\nLandsat Imagery\nClosing Time\nThis exercise wraps up working with Landsat information as well as working\nwith imagery in MultiSpec. Chapter 12 focuses on a whole different set of\nremote-sensing satellites (part of the Earth Observing System) and its exer-\ncise uses a new software program called NASA World Wind to examine this\nimagery.\nExit MultiSpec by selecting Exit from the File pull-down menu. There\u2019s\nno need to save any work in this exercise. This page was intentionally left blank 12\nStudying the Environment\nfrom Space\nNASA\u2019s Earth Observing System Program, Terra, Aqua, Aura,\nand Viewing and Examining EOS Imagery\n\u201cClimate change\u201d and \u201cglobal warming\u201d have become hot-button issues in to-\nday\u2019s world, affecting everything from a country\u2019s energy policies to its econ-\nomy to its citizens\u2019 choices of homes or vehicles. The study and monitoring of\nthe types of environmental processes that affect the land, oceans, and atmo-\nsphere occur on a global scale, and a continuous flow of constant information\nis needed. For instance, monitoring changes in global sea surface temperature\nis going to require information captured for the world\u2019s oceans on an ongoing\nbasis. In order to acquire the large-scale information of the entire planet in\na timely fashion, remote sensing technology can be utilized. After all, satel-\nlites are continuously orbiting the planet and collecting data\u2014they provide\nthe ideal mechanism for monitoring Earth\u2019s environmental conditions and\nchanges. There are a lot of other remote sensing satellites up in orbit doing\ntheir jobs besides Landsat and the high-resolution satellites discussed in Chap-\nter 11. In fact, there\u2019s an entire set of science observatories orbiting hundreds\nof miles over your head dedicated to continuously monitoring things like sea\nsurface temperature, glaciers, trace gases in the atmosphere, humidity levels,\nand Earth\u2019s heat radiation.\nThe Earth Observing System (EOS) is a series of remote sensing satellites EOS NASA\u2019s Earth\noperated and maintained by NASA. Each satellite\u2019s capabilities are dedicated Observing System\nto an aspect of environmental monitoring or data collection related to Earth\u2019s mission program.\nsurface, oceans, atmosphere, or biosphere. These EOS satellites often have\ncoarse spatial resolution (for instance, some instruments have a spatial reso-\nlution of each pixel representing 1000 meters) but consequently have a much\nlarger swath width (such as the ability to \u201csee\u201d up to 2300 kilometers in one\nswath). Their instruments are also set up to monitor very specific wavelengths\ntied to the purpose of that particular sensor. Their revisit times are usually\nrelatively short (like imaging the whole Earth in a couple of days) to provide\n351 352\nChapter 12 Studying the Environment from Space\nFIGURE 12.1 The health\nof land vegetation and the\ntemperature of the sea\nsurface measured through\nEOS remote sensing\ninstruments. (Source: MODIS\nInstrument Team, NASA\nGoddard Space Flight Center.)\nconstant data streams of observation data about whatever environmental sys-\ntem they\u2019re examining.\nThere are a lot of different uses and products of EOS data. This chapter\ncan\u2019t hope to cover all of them, just demonstrate some sample applications.\nSee Figure 12.1 for an example of the kind of data and imagery that can be\ncollected by EOS\u2014the image was collected by a sensor onboard an EOS sat-\nellite over eight days in April, 2000. It shows the health of vegetation (the\n\u201cgreening\u201d of springtime) of North America and the sea surface temperature\nof its surrounding waters. The overall mission of EOS is to examine global en-\nvironmental phenomena to advance knowledge and understanding of Earth\u2019s\nsystems as a whole. The EOS missions were begun during the 1990s\u2014there\nare several ongoing EOS missions, with more planned for the future, with\nthree key satellites being Terra, Aqua, and Aura.\nWhat Is Terra and What Can It Do?\nTerra the flagship\nsatellite of the EOS\nprogram. Terra serves as the \u201cflagship\u201d satellite of the EOS program. It was launched in\n1999 as a collaboration between the United States (NASA\u2019s Goddard Space-\nMorning\nflight Center, NASA\u2019s Jet Propulsion Lab, and the Langley Research Center)\nConstellation a set\nof satellites (including and agencies in Canada and Japan, and continues its remote sensing mission\nTerra, Landsat 7, SAC-C, today. Terra\u2019s orbit is Sun synchronous and it flies in formation as one of four\nand EO-1) that pass the satellites called the Morning Constellation. This series of satellites gets this\nEquator in the morning\nname because Terra crosses the Equator at 10:30 a.m. each day (it was originally\nduring their orbits.\nnamed EOS-AM-1 because of this morning crossing). The other satellites in 353\nWhat Is Terra and What Can It Do?\nFIGURE 12.2 A global\ncomposite of CERES\nimagery from June 2010,\nshowing the radiated\nenergy from Earth (blue\nareas are colder, red areas\nare warmer). (Source:\nKevin Ward, NASA Earth\nObservations)\nthis morning constellation are Landsat 7 (which passes at 10:01 a.m.), EO-1\n(which passes at 10:02 a.m.), and another satellite called SAC-C.\n\u201cTerra\u201d means \u201cEarth\u201d in Latin, which is an appropriate name for the sat-\nellite, as its instruments provide numerous measures of (among many other\nfunctions) the processes involved with Earth\u2019s land and climate. There are five\ninstruments onboard Terra, each with its own purpose and environmental\nmonitoring capabilities. The data and imagery collected by Terra\u2019s instru-\nments are processed into a number of specific products that are then distrib-\nuted for use.\nThe first of Terra\u2019s instruments is CERES (Clouds and Earth\u2019s Radiant CERES the Clouds and\nEnergy System), a system designed to provide regular energy measurements Earth\u2019s Radiant Energy\n(such as heat energy) from Earth\u2019s surface to the top of the atmosphere. Like System instruments\nonboard Terra and\nits name indicates, CERES is used to study clouds and see what effect they\nAqua.\nhave on Earth\u2019s energy. Using CERES to study clouds provides critical infor-\nmation about Earth\u2019s climate and temperature. As NASA notes, clouds play a\ndefining role in the heat budget of Earth, but different type of clouds affect\nEarth in different ways\u2014for instance, low clouds reflect more sunlight (and\nthus, help cool the planet) while high clouds reflect less but effectively keep\nmore heat on Earth. By combining CERES data with data from other EOS in-\nstruments, scientists are able to examine cloud properties and information\nabout Earth\u2019s heat and radiation (see Figure 12.2 for an example of using\nCERES for examining Earth\u2019s radiated energy).\nMISR (Multi-angle Imaging SpectroRadiometer) is the second of Terra\u2019s MISR the Multi-\ninstruments. It has nine sensors that examine Earth, but each sensor is set up angle Imaging\nat a different angle. One of these sensors is at nadir, four of them are set at SpectroRadiometer\ninstrument onboard\nforward-looking angles, and the remaining four are set at the same angles, but\nTerra.\nlooking backwards. MISR is a multispectral sensor, meaning that objects on\nthe ground get viewed nine times (from different angles) in multiple bands,\nwhich means that Terra can effectively cover the entire planet in about nine\ndays. By viewing things at multiple angles, stereo imagery (see Chapter 15)\ncan be constructed, allowing scientists to measure things like cloud heights. 354\nChapter 12 Studying the Environment from Space\nFIGURE 12.3 The\noperation of Terra\u2019s MISR\ninstrument, showing the\nAppalachian Mountains,\nthe Chesapeake Bay, and\nDelaware Bay as captured\nfrom looking straight\ndown (on the right) and\nfrom a variety of other\nangles (the images on the\nleft). (Source: NASA\/GSFC\/\nJPL, MISR Science Team)\nLikewise, the heights of plumes of smoke given off by wildfires or volcanoes\ncan be measured using MISR\u2019s stereo capabilities. In addition, by showing the\nsame area nine times in rapid succession, the images can provide a series of\nvery quick temporal snapshots of the same phenomenon (Figure 12.3).\nMISR data aids in environmental and climate studies by giving scientists\nthe ability to better examine how sunlight is scattered in different directions,\nparticularly by aerosols and atmospheric particles. For instance, a NASA ar-\nticle by Rosemary Sullivan details how MISR can study air pollution in the\nform of tiny particulates (caused by things like automobile or factory emis-\nsions, or from natural occurrences such as smoke or dust) that can lead to\nincreased health risks. Detailed information on concentrations of aerosols\nand atmospheric particles can help scientists understand the level of exposure\npeople are having to these types of pollutants. MISR is extremely sensitive to\nmeasuring these types of aerosols, and its data can help scientists tell the dif-\nference between what\u2019s on the ground and what\u2019s in the atmosphere. By using\nMISR (in conjunction with other instruments) to study aerosols, scientists can\nhelp determine areas of high aerosol concentrations and provide data to aid in\ncleaning up air pollution.\nMOPITT the\nAnother Terra instrument used for measuring airborne pollutants (but\nMeasurements\nof Pollution in in a different way) is MOPITT (Measurements of Pollution in the Tropo-\nthe Troposphere sphere), designed for monitoring pollution levels in the lower atmosphere.\ninstrument onboard MOPITT\u2019s sensors collect information to measure the amount of air pollu-\nTerra.\ntion concentrations across the planet, creating an entire global image about 355\nWhat Is Terra and What Can It Do?\nFIGURE 12.4 MOPITT\nimagery showing growing\ncarbon monoxide\nconcentration levels in\nSouth America. (Source:\nImages courtesy David\nEdwards and John Gille,\nMOPITT Science Team, NCAR)\nevery three days. Specifically, MOPITT monitors carbon monoxide concen-\ntrations in Earth\u2019s atmosphere. Carbon monoxide, a global air pollutant, is\ncaused by a number of factors, whether naturally occurring (such as from\nvolcanoes or wildfires) or by human causes (such as vehicle emissions or\nindustrial output). Being able to track long-term carbon monoxide sources\ncan aid in determining areas generating large amounts of pollutants and\nthen monitoring how those pollutants are being circulated through the\natmosphere. Figure 12.4 shows an example of this\u2014two MOPITT images\nof South America. One from March 2000 shows relatively low amounts of\ncarbon monoxide. A second image from September 2000 shows the dimen-\nsions of a large mass of carbon monoxide, caused by biomass burning in the\nAmazon and the movement of carbon monoxide across the ocean to South\nAmerica from fires in Africa.\nProbably the most widely used instrument onboard Terra is MODIS (the MODIS the Moderate\nModerate Resolution Imaging SpectroRadiometer), a sensor that produces Resolution Imaging\nover 40 separate data products related to environmental monitoring. Being SpectroRadiometer\ninstrument onboard\nable to measure 36 spectral bands (including portions of the visible, infrared,\nTerra and Aqua.\nand thermal) makes MODIS an extremely versatile remote sensing instru-\nment, with multiple applications of its imagery. For instance, Figure 12.1 on\npage 352 was made from MODIS imagery and showed vegetation health and\nwater temperatures. MODIS products are utilized for examining numerous\ntypes of environmental features, including (among many others) snow cover,\nsea surface temperature, sea ice coverage, ocean color, and volcanoes. In ad-\ndition, MODIS\u2019s thermal capabilities can track the heat emitted by wildfires,\nallowing for mapping of active fires across the globe (see Figure 12.5 on page\n356 for an example of MODIS imagery of fires and their aftermath recorded\nin Wilsons Promontory National Park in Australia). 356\nChapter 12 Studying the Environment from Space\nFIGURE 12.5 MODIS\nimagery examining the\ndimensions of a fire and\nresulting smoke plume\nat Wilsons Promontory\nNational Park. The other\ntwo MODIS images show\nthe park before and after\nthe fire. (Source: NASA\/GSFC,\nMODIS Rapid Response)\nBurned area\nMarch 31, 2005 April 9, 2005\nMODIS imagery is very coarse spatial resolution (most images have\n1 -kilometer resolution). However, the swath width of MODIS is 2330 kilo-\nmeters, making it possible to examine broad-scale phenomena of large geo-\ngraphic regions. MODIS views most of the globe in one day with complete\nglobal coverage being completed on a second day. With this quick revisit\ntime, MODIS allows scientists to look at changes in sea surface temperature\nor snow cover over time, global leaf-area index studies, and active large-scale\nfires, all on a regular basis. For instance, the red and near-infrared bands\nsensed by MODIS can be used to regularly compute global measures of NDVI\n(see Chapter 10) allowing monitoring of the health of the planet\u2019s trees and\nv egetation. Another application of MODIS is the ability to study the health\nof phytoplankton in the oceans. As NASA notes, when phytoplankton is un-\nhealthy or under heavy stress, it takes the sunlight it\u2019s absorbed and re-emits it\nas fluorescence, which can then be observed by MODIS. In this way, scientists\ncan track potential harmful algal (phytoplankton) blooms around the world\nvia MODIS. See Figure 12.6 for an example of MODIS imagery of Egypt and\nthe Nile River\u2014new lakes in the desert are visible adjoining the Nile. Also,\ncheck out Hands-on Application 12.1: MODIS Rapid-Fire Online for examples\nof current MODIS imagery online. 357\nWhat Is Terra and What Can It Do?\nFIGURE 12.6 MODIS\nimagery showing new\nlakes along the Nile River.\n(Source: Image by Robert\nSimmon, Reto St\u00f6ckli, and\nBrian Montgomery, NASA\nGSFC)\nThe last Terra instrument is ASTER (Advanced Spaceborne Thermal Emis- ASTER the Advanced\nsion and Reflection Radiometer). ASTER has relatively high spatial resolution Spaceborne Thermal\nand is often referred to as a sort of \u201czoom lens\u201d for Terra\u2019s other instruments. Emission and Reflection\nRadiometer instrument\nWith its thermal and infrared measurements, ASTER is also used to obtain\nonboard Terra.\ninformation about land surface temperature. In addition, through A STER\u2019s\nthermal capabilities, natural hazards such as wildfires can be monitored, or\nphenomena such as volcanoes can be observed and data obtained on their\nhealth and lava flows (see Figure 12.7 on page 358 for an example of ASTER\nHands-on Application 12.1\nMODIS Rapid-Fire Online\nRecent and archived MODIS data is available online most recent MODIS acquisition data available. Use\nat NASA\u2019s MODIS Rapid Response System. Open the imagery from today\u2019s date (or if today\u2019s date\nyour Web browser and go to http:\/\/rapidfire.sci. isn\u2019t available yet, use the most recent day). What\ngsfc.nasa.gov\/gallery. Several MODIS images (not kinds of features are currently being monitored or\nthe raw data, but processed images) can be viewed. examined by MODIS?\nCheck out some of the images from recent dates In this chapter\u2019s Geospatial Lab Application,\nand see what kind of phenomena NASA is tracking. you\u2019ll examine more MODIS imagery using some\nSelect the Real-Time link on the Web page for the other tools available from NASA. 358\nChapter 12 Studying the Environment from Space\nFIGURE 12.7 A\ncomposite image of the\nBezymianny volcano and\nits lava flow as seen by\nASTER\u2019s sensors. (Source:\nNASA\/GSFC\/MITI\/ERSDAC\/\nJAROS, and U.S.\/Japan ASTER\nScience Team, University of\nPittsburgh)\nHands-on Application 12.2\nASTER Applications\nFor a look at a variety of other applications of the options nearest to where you are now), or choose\nASTER, open your Web browser and go to http:\/\/ from the categories on the left-hand menu. Check\nasterweb.jpl.nasa.gov\/gallerymap.asp\u2014this is the out how ASTER is used for archeology, geology, and\nASTER Web Image Gallery, a collection of imagery hydrology, or for studying phenomena like volcanoes,\ndetailing how ASTER is used in a wide variety of glaciers, or natural hazards, or for its use in cities or\nreal-world situations. You can select a geographic land-use studies. What are some of the specific uses\nlocation from the map (for instance, select some of of ASTER\u2019s capabilities for these types of studies?\nimagery of the lava flow from a volcano). See Hands-on Application 12.2:\nASTER Applications for further examples of ASTER imagery.\nWhat Is Aqua and What Does It Do?\nLaunched in 2002 as a joint mission between NASA and agencies in Bra-\nAqua a key EOS zil and Japan, Aqua is another key EOS satellite. In Latin, \u201cAqua\u201d means\nsatellite whose mission \u201cwater,\u201d indicating the main purpose of the satellite\u2014to examine multiple\nis to monitor Earth\u2019s facets of Earth\u2019s water cycle. Analysis of water in all its forms\u2014solid, liq-\nwater cycle.\nuid, and gaseous\u2014is the key element of all six instruments onboard Aqua. 359\nWhat Is Aqua and What Does It Do?\nTerra and Aqua are designed to work in concert with one another\u2014their\nSun- synchronous orbits are set up similarly so that while Terra is on a de-\nscending path, Aqua is ascending (and vice versa). Because of this setup,\nTerra crosses the Equator in the morning while Aqua crosses in the after-\nnoon (Aqua was originally called EOS-PM to complement Terra\u2019s original\nEOS-AM-1 name). This connection is further strengthened as both satellites\ncarry a MODIS and a CERES instrument, in essence doubling the data col-\nlection performed by these two tools.\nBeyond duplicate MODIS and CERES instruments, Aqua carries four oth-\ners that are unique to its mission of examining Earth\u2019s water cycle: AMSU-A,\nHSB, AIRS, and AMSR-E. The AMSU-A (Advanced Microwave Sounding Unit) AMSU-A the Advanced\ninstrument is used for creating profiles of the temperature in the atmosphere. Microwave Sounding\nAMSU-A uses 15 microwave bands and is referred to as a \u201csounder\u201d because Unit instrument\nonboard Aqua.\nits instruments are examining a three-dimensional atmosphere, similar to the\nway \u201csoundings\u201d were used by ships to determine water depths. AMSU-A\u2019s HSB the Humidity\ndata provides estimates of not only temperature data but also precipitation Sounder for Brazil\ninstrument onboard\nand atmospheric water vapor. Similarly, the HSB (Humidity Sounder for Bra-\nAqua.\nzil) instrument\u2019s four microwave bands are used for measuring atmospheric\nAIRS the Advanced\nwater vapor levels (in other words, humidity) in the atmosphere (but HSB\nInfrared Sounder\nunfortunately stopped operating in 2003). Aqua\u2019s fifth instrument is AIRS\ninstrument onboard\n(Advanced Infrared Sounder), whose uses include measuring temperature\nAqua (used in\nand humidity levels of the atmosphere as well as information about clouds. conjunction with Aqua\u2019s\nAMSU-A and HSB data is used in close conjunction with AIRS to comprise HSB and AMSU-A\na sounding system for Aqua. Data products derived from this suite of instru- instruments).\nments include three-dimensional maps of atmospheric temperature, cloud\ntypes, ozone profiles, carbon dioxide levels, and sea surface temperature. This\nkind of information can be used for improving weather forecasting as well as\nstudying Earth\u2019s atmosphere and climate system.\nFigure 12.8 on page 360 shows an example of these three instruments\n(sometimes referred to as the \u201cAIRS Suite\u201d) working together for one purpose\u2014\nnamely, tracking and monitoring a tropical cyclone. The four images show (1)\nthe visible portion of the spectrum, sensed by AIRS, showing the extent and di-\nmension of tropical cyclone Ramasun in 2002, (2) a temperature profile of the\ncyclone sensed by AIRS, (3) imagery of the surface below the clouds as sensed\nby AMSU-A, which can see through the cloud cover of the hurricane, and (4)\nthe level of precipitation produced by the hurricane as sensed by HSB. All of the\nAIRS Suite instruments provide different \u201csnapshots\u201d of the same feature, allow-\ning a wealth of data to be collected about different kinds of weather phenomena.\nAMSR-E (the Advanced Microwave Scanning Radiometer for EOS) is the AMSR-E the Advanced\nlast of the Aqua instruments. It uses 12 microwave bands to cover most of the Microwave Scanning\nplanet in one day and completes a global dataset in the second day. AMSR-E Radiometer for EOS\ninstrument onboard\nmonitors a variety of environmental factors that affect global climate condi-\nAqua.\ntions, including sea ice levels, water vapor, wind speed, and amounts of global\nrainfall. For example, by assessing the amount of rain across the planet on a\nnear-daily basis, AMSR-E can provide measures of how much precipitation\nstorms can produce as they move across land or oceans. 360\nChapter 12 Studying the Environment from Space\nFIGURE 12.8 Imagery of\ntropical cyclone Ramasun\nin 2002 as obtained by\nthe AIRS, AMSU-A, and\nHSB instruments onboard\nAqua. (Source: NASA\/JPL)\n(1) (2)\n(3) (4)\nAMSR-E is also used to measure sea surface temperature (from a millime-\nter of water on the surface of the ocean) across the globe. As NASA notes, reg-\nular sea surface temperatures can be used as an aid in hurricane forecasting\nFIGURE 12.9 An AMSR-E or tracking tropical storms. A hurricane requires warm water in the oceans to\nglobal composite of sea power up and sustain itself (the water has to be 82oF or greater for a hurricane\nsurface temperature from\nto form). With regular temperature measurements, conditions for potential\nJune 2002\u2013September\n2003. (Source: NASA\/ hurricanes can be quickly assessed. See Figure 12.9 for an e xample of A MSR-E\nGoddard Space Flight Center sea surface temperature imagery\u2014those places with ocean temperatures of\nScientific Visualization Studio) 361\nWhat Is Aura and What Does It Do?\nHands-on Application 12.3\nAqua Products and the Visible Earth\nThe Visible Earth is a content-rich online resource 3. CERES Applications: http:\/\/visibleearth.nasa.\nset up by NASA for examining applications of the gov\/view_set.php?sensorID=62\nEOS Missions. Graphics and animations are also 4. HSB Applications: http:\/\/visibleearth.nasa.\nsearchable by geographic region or by event. For gov\/view_set.php?sensorID=231\ninstance, if you wanted to view graphics related to\n5. MODIS Applications: http:\/\/visibleearth.nasa.\nwater quality, a variety of imagery is available from\ngov\/view_set.php?sensorID=64\nnumerous sensors (and not just limited to the three\nAfter examining how the Aqua instruments are\nEOS satellites described in this chapter). Animations\nused, be sure to check out a few specific applica-\nof data showing changes over time or short video\ntions (including some of those previously discussed)\nclips of simulations of satellite orbit or sensors are\nto see how Aqua instruments are applied to the\navailable as well.\nfollowing topics:\nTo examine some of the Aqua applications, first\nopen your Web browser and go to http:\/\/visibleearth. 1. How clouds affect Earth\u2019s radiation: http:\/\/\nnasa.gov, then the option for Satellites, and then the visibleearth.nasa.gov\/view_rec.php?id=103\noption for Aqua. All of the Visible Earth categories for\n2. How sea surface temperature affects\nAqua will be displayed. Select a few categories. What\nhurricane locations: http:\/\/visibleearth.nasa.\ntypes of uses are Aqua\u2019s instruments monitoring?\ngov\/view_rec.php?id=13853\nSpecific subcategories for Aqua\u2019s instruments\n3. How the AIRS Suite is used for studying\nare also available to examine:\ntropical storms: http:\/\/visibleearth.nasa.gov\/\n1. AIRS Applications: http:\/\/visibleearth.nasa. view_rec.php?id=3439\ngov\/view_set.php?sensorID=63\n4. Using AMSR-E to measure extent of sea ice:\n2. AMSR-E Applications: http:\/\/visibleearth. http:\/\/visibleearth.nasa.gov\/view_rec.\nnasa.gov\/view_set.php?sensorID=61 php?id=3621\n82oF or more are shown in yellow and orange. By knowing where the conditions\nare right, scientists can forecast the potential for hurricane development. See\nHands-on Application 12.3: Aqua Products and the Visible Earth for more about\nAMSR-E and the other Aqua instruments.\nWhat Is Aura and What Does It Do?\nThe Aura EOS satellite was designed as a collaboration between NASA and\nAura An important\nagencies in Finland, the Netherlands, and the United Kingdom. In Latin, \u201cAura\u201d\nEOS satellite\nmeans \u201cbreeze,\u201d which helps describe the satellite\u2019s mission\u2014examination of dedicated to\nelements in the air, especially the chemistry of Earth\u2019s atmosphere. Aura orbits monitoring Earth\u2019s\nin formation with Aqua and other EOS satellites to form what is referred to as atmospheric chemistry.\nthe A-Train of satellites. As an aside, the A-Train was to be joined by another A-Train Another term\nsatellite, OCO, launched in 2009. Unfortunately, however, OCO did not achieve for the Afternoon\norbit, leaving the A-Train one satellite short of its projected configuration. Constellation. 362\nChapter 12 Studying the Environment from Space\nThis organization of satellites is called the A-Train because it serves as the\nAfternoon Afternoon Constellation of satellites (to complement the Morning Constel-\nConstellation a set lation that Terra flies in) and also because two of the key satellites (Aqua and\nof satellites (including Aura) begin with the letter \u201cA.\u201d Currently, Aqua flies lead in the constellation\nAqua and Aura) that\n(although the successful 2009 launch of OCO would have put it in the lead posi-\npass the Equator in the\ntion, flying in front of Aqua). CloudSat follows closely after, then CALIPSO about\nafternoon during their\norbits. 15 seconds after that, and lastly Aura bringing up the rear (Figure 12.10). A sat-\nellite called PARASOL flew between CALIPSO and Aura, although it was recent-\nHIRDLS the High\nly removed from the formation. Other satellites are currently planned to become\nResolution Dynamics\nLimb Sounder part of the A-Train after their launch. The combined data from the A-Train of\ninstrument onboard satellites gives scientists a rich dataset for analysis of climate change questions.\nAura. Aura carries four instruments onboard, each utilized in some type of atmo-\nMLS the Microwave spheric observation (such as ozone concentrations or air quality) as it relates\nLimb Sounder to global climate change. The first, HIRDLS (High Resolution Dynamics Limb\ninstrument onboard Sounder) measures phenomena such as temperature, water vapor, ozone, and\nAura.\nother trace gases to examine qualities such as the transportation of air from one\nTES the Tropospheric section of the atmosphere to another. As NASA notes, HIRDLS data is also used\nEmission Spectrometer to examine pollution to see what is naturally occurring (from ozone) and what is\ninstrument onboard\nhuman generated. Similarly, Aura\u2019s MLS (Microwave Limb Sounder) instrument\nAura.\nsenses microwave emissions in five bands to examine carbon monoxide and\nOMI the Ozone ozone in the atmosphere. MLS data can be used as an aid in measuring ozone\nMonitoring Instrument\ndestruction in the atmosphere. Aura\u2019s third instrument, TES (Tropospheric\nonboard Aura.\nEmission Spectrometer), measures things related to pollution, including ozone\nand carbon monoxide. Since TES is capable of sensing from the land surface up\ninto the atmosphere, its data can be used to assess air-quality levels in urban\nareas by measuring the levels of pollutants and ozone in cities.\nFIGURE 12.10 The Aura\u2019s final instrument is OMI (Ozone Monitoring Instrument) which (as\nconfiguration of EOS the name implies) is dedicated to keeping an eye on changes in ozone across\nsatellites that make up the globe. OMI is a hyperspectral sensor that views sections of the visible-light\nthe A-Train.\nCloudSat\nCALIPSO Aqua\nPARASOL\nOCO\nAura 363\nWhat Is Aura and What Does It Do?\nFIGURE 12.11 OMI\nimagery showing a hole\nin the ozone layer above\nAntarctica. (Source: NASA\/\nGoddard Space Flight Center\nScientific Visualization Studio)\nspectrum as well as the ultraviolet portion of the electromagnetic spectrum.\nBack in Chapter 10, we discussed how the atmosphere absorbs a lot of electro-\nmagnetic radiation and that ozone is the primary greenhouse gas that absorbs\nharmful ultraviolet light. OMI data aids scientists in measuring the amount of\nultraviolet radiation penetrating to Earth by examining clouds and ozone lev-\nels. Figure 12.11 shows an example of imagery from OMI, showing the thin-\nning (or hole) in the ozone layer above Antarctica. See Hands-on Application\n12.4: Aura Products and the Visible Earth for more examples of Aura\u2019s instru-\nments\u2019 applications.\nHands-on Application 12.4\nAura Products and the Visible Earth\nTo examine some of applications of Aura\u2019s instru- (including some of those previously discussed) to\nments, the Visible Earth is a great reference. Like see how Aura is applied to the following topics:\nin Hands-on Application 12.3: Aqua Products and\n1. Monitoring holes in the ozone layer: http:\/\/\nthe Visible Earth, open your Web browser and go to\nvisibleearth.nasa.gov\/view_rec.php?id=14958\nhttp:\/\/visibleearth.nasa.gov\u2014then select the op-\ntion for Satellites, and then the option for Aura. All 2. The effects of biomass burning: http:\/\/\nof the Visible Earth categories for Aura will be dis- visibleearth.nasa.gov\/view_rec.php?id=15006\nplayed. What specific effects are being monitored or\n3. Monitoring aerosol and smoke effects from\nexamined using Aura\u2019s instrumentation?\nwildfires: http:\/\/visibleearth.nasa.gov\/view_\nAfter examining how the Aura instruments are\nrec.php?id=20523\nused, be sure to check out a few specific applications 364\nChapter 12 Studying the Environment from Space\nThinking Critically with Geospatial Technology 12.1\nHow Can EOS Data Be Used in Studying and Monitoring Climate Change?\nClimate change studies focus on Earth\u2019s climate\u2014for of instruments and their uses and data products\nexample, assessing global temperature changes you\u2019ve examined in this chapter and their applica-\nand the kinds of environmental conditions that in- tions, how are EOS satellites\u2019 data used for studying\nfluence alterations in Earth\u2019s processes. The Earth climate-change conditions? What are some specific\nObserving System satellites provide constant sets measurements of climate-change-related condi-\nof data related to all manner of environmental and tions that EOS data can be used for? How can this\nclimate conditions on our planet. Given the variety data be used in climate change studies?\nHow Can EOS Applications Be Easily\nViewed and Analyzed?\nWith all of this EOS data and imagery collected on a near-daily basis, you\nwould expect there to be a mechanism to get this information out into the\nhands of the public for viewing or analysis. Much of the EOS data is compiled\ninto a series of data products that are delivered rather than raw imagery or\nvalues. NASA has a number of Web-based platforms in place to distribute\nthese products (as well as EOS imagery), so they can be viewed or analyzed.\nVisible Earth a One of these, Visible Earth (used in Hands-on Applications 12.3 and 12.4 on\nWebsite operated by pages 361 and 363), is a NASA- operated Website from which you can down-\nNASA to distribute EOS load pictures and animations related to EOS missions (online at: http:\/\/\nimages and animations\nvisibleearth.nasa.gov\/). However, the pictures are just that\u2014graphics of\nof EOS satellites or\nthe processed data that show images from EOS instruments. The graphics\ndatasets.\ncome with detailed descriptions of the scientific event being shown (such\nas a hole in the ozone layer as measured from OMI or global sea surface\ntemperature as measured by AMSR-E). Visible Earth also features popular\ndetailed composite images such as NASA\u2019s Blue Marble (showing the entire\nEarth from space) or the global nighttime city lights.\nA second source for accessing and viewing EOS imagery and data is\nBlue Marble a the NASA Earth Observatory Website (http:\/\/earthobservatory.nasa.\ncomposite MODIS\ngov\/), a sort of online \u201cmagazine\u201d devoted to global environmental issues\nimage of the entire\nand how EOS is used with them. The Earth Observatory has articles d ating\nEarth from space.\nback to 1998, dealing with topics such as global warming, tropical defor-\nNASA Earth\nestation, and climate and Earth\u2019s energy budget. The Website also features\nObservatory a\nan extensive \u201cimage of the day\u201d archive (Figure 12.12), with each feature\nWebsite operated by\nshowcasing a different environmental occurrence from around the globe.\nNASA that details\nhow EOS is utilized For example, the January 29, 2009, image of the day used MODIS imag-\nwith numerous global ery to examine forest and grassland fires in Australia, while the January\nenvironmental issues 27, 2009, image looks at sulfur dioxide emissions in Bulgaria as captured\nand concerns.\nby OMI. Earth Observatory is an excellent source for seeing (and in some 365\nHow Can EOS Applications Be Easily Viewed and Analyzed?\nTahiti\nPPPaaaccciiififificcc OOOccceeeaaannn\n100 km\nFIGURE 12.12 Cyclone\ncases interacting with) remotely sensed data and imagery in action and ap- Oli as viewed by Aqua\u2019s\nMODIS instrument\u2014one\nplied to all manner of real-world situations (see Hands-on Application 12.5:\nof the striking Images\nUsing the Earth Observatory to Interactively Work With EOS Imagery on page\nof the Day available via\n366 for more about the interactive components of the Website). Earth Observatory. (Source:\nVisible Earth and Earth Observatory are extensive resources for NASA Image by Jeff Schmaltz,\napplication of imagery, but to access the results of some data products them- MODIS Rapid Response Team,\nGoddard Space Flight Center)\nselves, NASA has set up another Website, NASA Earth Observations (NEO), at\nhttp:\/\/neo.sci.gsfc.nasa. NASA NEO enables users to download processed\ndata from EOS satellites to examine applications centered on five main top-\nNASA NEO the NASA\nics: oceans, atmosphere, energy, land, and life. NEO allows you to download\nEarth Observations\ni magery from multiple dates, to view it interactively online, or to download Website, which allows\nthe data in a format compatible with Google Earth. When you view the data users to view or\nin Google Earth format, the EOS imagery \u201cwraps\u201d around the virtual globe download processed\nEOS imagery in a\nin Google Earth, allowing you to interact with the imagery the same as you\nvariety of formats,\nwould with any other Google Earth usage. Working with the data in this in-\nincluding a version\nteractive format treats the EOS imagery as an overlay on top of Google Earth,\ncompatible for viewing\nallowing you to see how EOS imagery fits with other data layers. In Geospatial in Google Earth.\nLab Application 12.1, you will be making use of NASA NEO and some of its\nNASA World Wind a\ndatasets in Google Earth.\nvirtual globe program\nA comprehensive EOS imagery tool is NASA World Wind, a virtual globe from NASA, used for\nprogram designed for analysis and manipulation of several types of remote examining various\nsensing imagery (and is the software used in Geospatial Lab Application 12.1). types of remotely\nsensed imagery.\nNASA World Wind is available for free download from the Web and includes 366\nChapter 12 Studying the Environment from Space\nHands-on Application 12.5\nUsing the Earth Observatory to Interactively Work with EOS Imagery\nThe Earth Observatory also features the ICE (Image to conditions in the Channel Islands. The Website\nComposite Explorer) tool that allows users to ana- gives the background on the Channel Islands and\nlyze applications of remotely sensed data (and was how MODIS is used for monitoring phytoplankton\nalso used back in Hands-on Application 10.2: Exam- conditions. Within the page is a link to launch ICE,\nining NDVI with NASA ICE). To get started with using which will allow you to observe imagery related to\nEOS data in one of the ICE scenarios, open your Web sea surface temperature, chlorophyll content, and\nbrowser and go to http:\/\/earthobservatory.nasa. fluorescence. How does MODIS measure these fac-\ngov\/Experiments\/ICE\/Channel_Islands. This will tors and how can EOS imagery and data be used to\nallow you to examine MODIS products as they relate monitor phytoplankton content in the oceans?\nnumerous ways of utilizing EOS data. It contains several layers that can be\nexamined, including Landsat true color and false color imagery, Landsat-\nderived land-cover data and high-resolution orthophotos. These data layers\ncan be used alongside Rapid-Fire MODIS imagery taken from numerous sites\naround the globe of phenomena like fires, storms, and volcanoes. NASA World\nWind also features the Scientific Visualization Studio (SVS), a tool designed\nto create animations of EOS imagery (such as several days of MODIS imagery\nof Hurricane Katrina or MOPITT imagery of a year\u2019s worth of carbon monox-\nide pollutants) for observation and analysis. With plenty more features than\nthese added in, NASA World Wind is a very versatile tool that combines the\nvirtual globe environment with EOS imagery and environmental applications.\nThere area lot of other ongoing EOS ongoing beyond Terra, Aqua, and\nAura. For instance, the Jason-1 mission studies properties of the oceans,\nwhile the SORCE mission studies energy from the Sun. See Hands-on\nHands-on Application 12.6\nNASA Eyes on the Earth\nNASA has set up a Web application that allows you satellite mission you wish to view (which includes\nto view the real-time positions of Terra, Aqua, Aura, Terra, Aqua, and Aura, along with other missions such\nand the other EOS satellite missions, as well as view- as Acrimsat, Calipso, and Cloudsat), then select either\ning 3D data maps of the products and imagery they the view of the globe or the view from the satellite\ncapture. Open your Web browser and go to http:\/\/ itself. Under the \u201cShow Data Map\u201d options, you can\nclimate.nasa.gov\/Eyes\/eyes.html (you may have select from many different types of data products,\nto download and install a special plugin for all of the including several of the applications discussed in this\nfunctions to work correctly). You can select the EOS chapter. 367\nChapter Wrapup\nHands-on Application 12.7\nExamining Other Environmental Remote Sensing Applications\nNOAA operates two types of remote sensing satel- High Resolution Radiometer) sensor that views\nlites. The GOES program is a series of geostationary in six bands (early versions viewed only in five).\nsatellites tasked with monitoring weather condi- NOAA\u2019s remote sensing program is a critical part\ntions and are constantly observing weather of the of global data collection as it relates to climate.\nsame location. NOAA\u2019s POES satellites are polar You can examine NOAA satellite images online at\norbiting and are part of a series of satellites. The the NOAA CoastWatch Website: http:\/\/www.nodc.\nNOAA satellites carry the AVHRR (Advanced Very noaa.gov\/dsdt\/cw.\nA pplication 12.6: NASA Eyes on the Earth for a very cool NASA Website filled\nwith interactive information and data about all of the EOS satellites. Also,\nkeep in mind that NASA doesn\u2019t have a monopoly on broad-scale environ-\nmental remote sensing satellites. For example, NOAA (the National Oceanic\nand Atmospheric Administration) operates and maintains a series of satel-\nlites dedicated to numerous environmental monitoring functions, including\nweather observations. See Hands-on Application 12.7: Examining Other En-\nvironmental Remote Sensing Applications for some examples of NOAA satel-\nlite imagery as well.\nChapter Wrapup\nThe satellites of the Earth Observing System provide a constant source\nof remotely sensed data that can be utilized in numerous studies related\nto the climate and environment of our planet. Whether used in monitor-\ning w ildfires, ozone concentrations, potential algae blooms, or tropical\ncyclones, the applications described in this chapter just scratch the surface\nof the uses of the EOS and how these geospatial tools affect our lives. This\nchapter\u2019s lab will have you start working with EOS imagery using NASA\nWorld Wind and NEO to get a better feel for some of the applicability of\nthe data.\nIn the next chapter, we\u2019ll dig into a new aspect of geospatial technology,\nthat of modeling and analyzing landscapes and terrain surfaces.\nImportant note: The references for this chapter are part of the online\ncompanion for this book and can be found at http:\/\/www.whfreeman.com\/\nshellito1e. 368\nChapter 12 Studying the Environment from Space\nKey Terms\nEOS (p. 351) Aura (p. 361)\nTerra (p. 352) A-Train (p. 361)\nMorning Constellation (p. 352) Afternoon Constellation (p. 362)\nCERES (p. 353) HIRDLS (p. 362)\nMISR (p. 353) MLS (p. 362)\nMOPITT (p. 354) TES (p. 362)\nMODIS (p. 355) OMI (p. 362)\nASTER (p. 357) Visible Earth (p. 364)\nAqua (p. 358) Blue Marble (p. 364)\nAMSU-A (p. 359) NASA Earth Observatory (p. 364)\nHSB (p. 359) NASA NEO (p. 365)\nAIRS (p. 359) NASA World Wind (p. 365)\nAMSR-E (p. 359) 12.1\nGeospatial Lab Application\nEarth Observing System Imagery\nThis chapter\u2019s lab will introduce you to some of the basics of examining\nEOS (Earth Observing System) imagery from the Terra and Aqua satel-\nlites. You will be examining data from MOPITT as well as many types of\nMODIS imagery using online resources along with NASA World Wind and\nGoogle Earth.\nNASA World Wind is a free program (like Google Earth) created by NASA\nto incorporate many types of satellite imagery (such as Landsat, Terra, and\nAqua) into a virtual globe interface.\nObjectives\nThe goals of this lab are:\na Examining the usage and function of MODIS imagery.\na Utilizing NASA World Wind as a tool for examining remotely sensed\nimagery.\na Using Terra and Aqua imagery for environmental analysis.\na Examining EOS imagery in Google Earth.\nObtaining Software\na The current version of NASA World Wind (1.4) is available for free down-\nload at http:\/\/worldwind.arc.nasa.gov\/download.html.\na The current version of Google Earth (6.0) is available for free download\nat http:\/\/earth.google.com.\nImportant note: Software and online resources sometimes change fast.\nThis lab was designed with the most recently available version of the soft-\nware at the time of writing. However, if the software or Websites have signifi-\ncantly changed between then and now, an updated version of this lab (using\nthe newest versions) is available online at http:\/\/www.whfreeman.com\/\nshellito 1e.\nLab Data\nThere is no data to copy in this lab. All data comes as part of the NASA soft-\nware or will be downloaded from sources on the Internet.\n369 370\nChapter 12 Studying the Environment from Space\nLocalizing This Lab\nThis lab uses EOS data from a variety of locations across the globe, but there\nare several ways to examine some EOS imagery at a more local level:\na In Section 12.2, use a wider variety of dates than those presented to\naccess a greater number of MODIS-monitored events. Look for some\nthat are relatively close to your location or whose MODIS imagery would\noverlap onto your area.\na In Section 12.3, examine one year\u2019s worth of MOPITT data as it affects\nyour local area, rather than the globe, and keep track of the carbon mon-\noxide (CO) levels for your region.\na In Section 12.4, examine the local effects of some of the Terra and Aqua\nimagery products on your region, rather than evaluating them on a\nglobal scale. Open and examine images from multiple dates in Google\nEarth for your area, rather than one month\u2019s worth of data.\n12.1 Using NASA World Wind\n1. Start NASA World Wind.\n2. World Wind will open with the default setting of NASA\u2019s \u201cBig Blue\nMarble\u201d imagery of Earth. Use the cursor to rotate the globe around to\nNorth America and the mouse wheel to zoom in and out of the globe.\n3. In World Wind, several options are available for display on the\nglobe, shown at the top of the button bar. If a black triangle appears\nover the option, that layer is turned \u201con\u201d and being displayed on the\nglobe; if there is no black triangle, that layer is turned off and is not\nshown.\n12.2 MODIS Rapid Fire in World Wind\n1. Click on the option for Rapid Fire MODIS to open a new dialog box. 371\nEarth Observing System Imagery\n2. The Rapid Fire Modis dialog box will appear.\n3. Select all six of the phenomena that are being monitored using MODIS.\n4. Select 1 km for the resolution to examine.\n5. Use Entire Group for the Display Mode.\n6. Change the date ranges to those shown here (January 1, 2006, to\nDecember 1, 2010).\n7. A number of graphics corresponding to one of the six phenomena being\nmonitored by MODIS imagery will appear on the globe.\n8. Zoom in to California, where there is a cluster of the \u201cfire\u201d symbols.\nDouble-click on the fire icon and a MODIS image will appear overlaid\nonto the globe. Examine several of the images related to California fires.\nQuestion 12.1 How are the fires being shown in this MODIS imagery?\nQuestion 12.2 How is the extent of the fires being tracked via MODIS?\n(Hint: What else is visible in the MODIS scene aside from the fires\nthemselves?)\n9. Rotate the view around to Western Europe, particularly off the west\ncoast of Portugal. Click on the green \u201cother\u201d icon whose label indicates\na \u201cphytoplankton bloom.\u201d A new MODIS scene will appear. Also,\nexamine some of the other \u201cphytoplankton\u201d images off the coasts of\nother European countries, such as France or Denmark.\nQuestion 12.3 According to the caption, this is a MODIS image of\nphytoplankton in the ocean. Why would this be a phenomenon that is\nimportant enough to be tracked by MODIS, and what does the imagery show\nan observer about this phenomenon? 372\nChapter 12 Studying the Environment from Space\n10. Return to North America and examine some of the imagery associated\nwith the \u201cStorm\u201d icons\u2014there are some good examples off the coast\nof Florida (related to Tropical Storm Ernesto) and off the coast of New\nEngland (for Tropical Storm Beryl).\nQuestion 12.4 How are the scope and capabilities of the MODIS\ninstrument used for monitoring weather or storm formations such as these\nTropical Storms?\n11. Close the MODIS Rapid Fire dialog box.\n12.3 T he Scientific Visualization Studio in\nWorld Wind\nThe Scientific Visualization Studio is a tool in World Wind for using imagery\nderived from some of the instruments aboard Aqua or Terra to examine Earth\nphenomena over time through animations.\n1. Click on the icon on the toolbar representing the Scientific Visualization\nStudio (SVS).\n2. When started, SVS will perform a short download of current datasets to\nuse. Give it a couple of seconds to do so. 373\nEarth Observing System Imagery\n3. SVS will allow you to access various EOS datasets and examine them in a\ntime series. The controls in the SVS dialog box act as DVR-style controls\nto run the time-series animation forward, backward, stop, or play.\n4. Select the Atmosphere options, then choose the animation for AQUA\nMODIS imagery of Hurricane Katrina.\n5. Click on the Play button. It will take a couple minutes to download all of\nthe animation global data.\n6. The animation will play, showing several days of imagery from Aqua of\nHurricane Katrina building, then making landfall in the Mississippi Gulf\nCoast\/New Orleans region.\nQuestion 12.5 Given what you\u2019ve seen of MODIS imagery and the\ninformation in the SVS, should MODIS be used as the sole instrument to\nmonitor a massive storm formation like Hurricane Katrina? Why or why not?\n7. Stop playing the animation, and then from the Atmosphere options,\nselect Global Atmospheric Carbon Monoxide in 2000. Play the\nanimation (it should replace the Hurricane Katrina imagery and\ncover the entire globe). Make sure to read the SVS description that\naccompanies the animation as well. This will show higher levels of\ncarbon monoxide concentration from the MOPITT instrument onboard\nTerra.\n8. Answer Questions 12.6 and 12.7. (You will have to rotate the globe\nwhile the animation is playing to answer the questions\u2014also keep in\nmind the times and dates that are running in the upper-right-hand side\nof the screen. You may want to pause or rewind to examine certain dates\nin the past.)\nQuestion 12.6 At what dates are the highest concentrations of carbon\nmonoxide being monitored by MOPITT in Africa and South America?\nQuestion 12.7 Beyond the carbon monoxide levels in Africa and South\nAmerica (which were likely generated by wildfires), what other geographic\nareas can you see high concentrations of carbon monoxide developing and\nspreading from?\n9. At this point, you can close World Wind by selecting Exit from the File\npull-down menu.\n12.4 U sing the NASA Earth Observations (NEO)\nWeb Resources\n1. Open your Web browser and go to http:\/\/neo.sci.gsfc.nasa.gov. This\nis the Website for NEO (NASA Earth Observations), an online source of\ndownloadable Earth observation satellite imagery. In this portion of the\nlab, you\u2019ll be using EOS imagery in conjunction with Google Earth. 374\nChapter 12 Studying the Environment from Space\n(Source: NASA)\n2. Click on the Atmosphere tab under the main graphic.\n3. Select the option for Carbon Monoxide (MOPITT).\n4. This is an image of global carbon monoxide concentration for one month\ncollected using the MOPITT instrument aboard Terra (click on the\nAnalyze this image option for more detailed information).\n5. Rather than viewing a flat satellite image, NEO gives you the option of\nexamining the imagery draped across a virtual globe.\nImportant note: We will be using Google Earth for this portion of the lab,\nas the imagery can be downloaded in a \u201c.kmz\u201d file (the file format for Google\nEarth). NASA World Wind has a \u201ckmz\u201d converter tool that will also let you\nexamine the imagery in World Wind if you don\u2019t have access to Google Earth.\n6. Click on the Open in Google Earth option associated with the October 1,\n2010\u2013October 31, 2010, imagery. The \u201ckmz\u201d file of this dataset will\ndownload and Google Earth will automatically open to display it draped\naround the globe. (If prompted by your Web browser, select Open using\nGoogle Earth.) 375\nEarth Observing System Imagery\n7. Rotate Google Earth to examine the MOPITT imagery draped over the\nglobe.\nQuestion 12.8 Where (geographically) were the highest concentrations of\ncarbon monoxide in October 2010?\n8. Back on the NEO Website, select the Energy tab, then select Land\/\nSurface temperature (Day) MODIS.\n9. Choose the image for September 1, 2010\u2013October 1, 2010.\n10. Choose to Open this composite image in Google Earth.\n11. In Google Earth, turn off the MOPITT image (it will be in your\nTemporary Places listings).\nMOPITT Imagery\nMODIS Land Temp\n12. Rotate Google Earth to examine the new MODIS image.\nQuestion 12.9 Where (geographically) were the lowest daytime land\ntemperatures in September 2010?\n13. Back on the NEO Website, select the Life tab, and then select Vegetation\nIndex NDVI Terra MODIS. Choose the image for July 1, 2010, to\nAugust 1, 2010.\n14. NDVI is a metric used for measuring vegetation health for a pixel. As\ndescribed in Chapter 10, the higher the value of NDVI, the healthier the\nvegetation at that location.\n15. Choose to Open this composite image in Google Earth.\n16. In Google Earth, turn off your other two Terra images.\n17. Rotate Google Earth and zoom in to examine the new MODIS composite\nimage.\nQuestion 12.10 Where (geographically) are the areas with the least\namount of healthy vegetation on the planet in July 2010?\n12.5 E OS Imagery from the National Snow and\nIce Data Center\n1. NASA NEO is not the only source of EOS data for viewing in a virtual\nglobe format. One example is the National Snow and Ice Data Center\n(NSIDC). Open your Web browser and go to their Website at http:\/\/\nnsidc.org\/data\/virtual_globes. 376\nChapter 12 Studying the Environment from Space\n2. Go to the Featured Data section of the Website. Look for the section\ncalled Snow.\n(Source: NASA)\n3. Read about what the imagery is showing, and then open the \u201cKML\u201d file\nfor 2009 snow cover. The \u201cKML\u201d file will open in Google Earth like the\nones from NEO.\n4. In Google Earth, turn off your other three NEO images.\n5. In Google Earth\u2019s Temporary Places, there will be a new expandable\nitem for NSIDC. By expanding this item and looking through the\noptions, you\u2019ll see another expandable item called Snow Cover that\nshows snow data for each month of 2009 is available as an option that\ncan be clicked on and off. To examine just one month of data, turn off\nall of the other months. You can also use the slider bar that appears at\nthe top of the Google Earth view to adjust the snow data in Google Earth\nfrom month to month.\n6. Rotate Google Earth and zoom in to examine the new MODIS image.\nQuestion 12.11 Where (geographically) are the greatest concentrations of\nsnow cover in the Southern Hemisphere in June 2009?\nQuestion 12.12 Where (geographically) are the greatest concentrations of\nsnow cover in the Northern Hemisphere in December 2009?\n7. At this point, you can exit Google Earth and your Web browser.\nClosing Time\nThis exercise showed off a number of different ways of viewing and visual-\nly analyzing data available from EOS satellites. The next chapter\u2019s going to\nchange gears and start looking at the ground itself that those satellites are\nviewing.\nIf World Wind is still open, exit it by selecting Exit from the File pull-\ndown menu. Also, if Google Earth is still open, exit it by selecting Exit from\nthe File pull-down menu. Part 4\nGeospatial Applications\n13\nDigital Landscaping\nDigital Topographic Maps, Contours, Digital Terrain Modeling,\nDigital Elevation Models (DEMs), NED, SRTM, LIDAR, and 3D\nViews of Landscapes and Terrain\nThere\u2019s one big element missing in geospatial technology that we haven\u2019t yet\ndealt with: the terrain and surface of Earth. All of the maps, imagery, and co-\nordinates (while taking into account numerous critical features of the planet)\nhave been dealing with two-dimensional (2D) images of the surface or of de-\nveloped or natural features on the land. We haven\u2019t yet d escribed how the ac-\ntual landscape can be modeled with geospatial technology. This chapter delves\ninto how terrain features can be described, modeled, and analyzed using sev-\neral of the geospatial tools (like GIS and r emote sensing) that have been previ-\nously described. Whether you are modeling topographic features for construc-\ntion or recreational opportunities, or interested in seeing what the view from\nabove would be, the landforms on Earth\u2019s surface are a necessary component\nof geospatial analysis (see Figure 13.1 on page 378 for an example of visual-\nizing Mount Everest with Google Earth).\nFirst and foremost, when we\u2019re examining landforms on Earth\u2019s surface,\neach location will have to have an elevation assigned to it. In terms of coordi-\nnates, each x\/y pair will now have a z-value accompanying it that indicates z-value the elevation\nthat location\u2019s elevation. These elevations have to be measured relative to assigned to an x\/y\ncoordinate.\nsomething\u2014when a point has an elevation of 900 feet, this indicates that it is\n900 feet above something. The \u201csomething\u201d represents a baseline, or v ertical vertical datum\ndatum, that is the zero point for elevation measurements. Some maps of land- a baseline used as\na starting point in\nforms indicate the vertical datum is taken as mean sea level (r epresented by\nmeasuring elevation\nthe National Vertical Datum of 1929). Thus, when this vertical datum is used\nvalues (either above or\nand an elevation value on a map indicates 1000 feet, that number can be read\nbelow this value).\nas 1000 feet above mean sea level. Other geospatial data in North America\nutilizes the North American Vertical Datum of 1988 (NAVD88). Coastal ter-\nrain models may also use mean high water.\n337777 378\nChapter 13 Digital Landscaping\nFIGURE 13.1 An\nexample of Mount Everest\nmodeled as a digital\nterrain landscape and\nshown in Google Earth.\n(Source: \u00a9 2009 Google,\nImage \u00a9 2009 DigitalGlobe,\nImage Copyright 2011\nTerraMetrics, Inc. www.\nterrametrics.com, Image \u00a9\n2009 Geo Eye)\nHow Can Terrain Be Represented?\nA common (and widely available) method of representing terrain features is\ntopographic map a the topographic map. As its name implies, a topographic map is designed\nmap created by to show the topography of the land and the features on it. Topographic maps\nthe USGS to show\n(commonly called topo maps, for short) are published by the USGS (United\nlandscape and terrain,\nStates Geological Survey) and are available at a variety of map scales. A com-\nas well as location of\nfeatures on the land. mon topographic map is a 1:24000 topoquad, also referred to as a 7.5 minute\ntopo map (as it displays an area covering 7.5 minutes of latitude by 7.5 min-\nDRG a Digital Raster\nutes of longitude in size). Topo maps are also available in smaller map scales,\nGraphic\u2014a scanned\nversion of a USGS such as 1:100000 or 1:2000000. Topo maps are available in a digital format\ntopographic map. as a DRG (Digital Raster Graphic). DRGs are scanned versions of topo maps\nthat have been georeferenced (see Chapter 3), so they\u2019ll match up with other\nGeoTIFF a graphical\nfile format that can geospatial data sources.\nalso carry spatial Usually, DRGs are available in GeoTIFF file format (see Chapter 7 for\nreferencing. more information about TIFFs), which allows for high-resolution images\ncollar the white while also having a spatial reference. Thus, when a DRG is used as a layer in\ninformation-filled geospatial technology, it contains information to allow it to match up with\nborder around a other data layers. A topo map (and thus, a DRG) also contains a lot of data\ntopographic map.\n(including the scale, the dates of map creation and revision, and similar in-\nformation) in the white border that surrounds the map. This border is re-\nferred to as the map\u2019s collar. A topo map will either come with all of this\ninformation intact, or in some cases, the collar will be removed and you will\nhave only the map itself. See Figure 13.2 for an example of a DRG complete\nwith collar. 379\nHow Can Terrain Be Represented?\nFIGURE 13.2 A portion\nof a DRG and its map\ncollar. (Source: USGS)\nTopo maps (and thus, DRGs) model the landscape through the use of\ncontour lines\u2014imaginary lines drawn on the map that represent areas of contour an imaginary\nconstant elevation (Figure 13.3). The elevations on the surface can be rep- line drawn on a map\nresented by numerous contour lines drawn on the map. However, because to connect points of\ncommon elevation.\nelevation is really a continuously variable phenomenon, it\u2019s often difficult to\ndraw a line representing every change in elevation (such as from eight feet of\nelevation to ten feet of elevation) without overloading the map to the point of\nuselessness because it\u2019s filled to the breaking point with contour lines. Thus,\ncontour lines are drawn a certain elevation distance apart (such as drawing a\nnew contour line every 50 feet).\nFIGURE 13.3 Contour\nlines as shown on a DRG\n(at a 20 foot contour\ninterval). (Source: USGS) 380\nChapter 13 Digital Landscaping\nHands-on Application 13.1\nDigital Raster Graphics Online\nIf you need to look up features on a topographic Use the Libre Map project to search for a promi-\nmap, there\u2019s no need to wade through a map library nent feature in your hometown or near your house or\nfilled with topoquads of all sizes and scales. USGS school. Then download the DRG in the GeoTIFF file\ntopo maps are made available online as DRGs. The format to your hard drive (and download the TFW\nLibre Map Project is a great online resource for file too, to make sure the georeferencing information\n1:24000 DRGs. Open your Web browser and go comes with the map) and view it using a utility like\nto http:\/\/libremap.org, then select the Data tab. ArcGIS or AEJEE. Look over the DRG to find the fea-\nFrom there, select your state. A new menu will ap- ture you\u2019re looking for. While you\u2019re at it, examine the\npear and let you search for specific features (for contour lines near the selected feature and see how\ninstance, selecting Florida for the state will allow they model the nearby elevation and terrain. Lastly, if\nyou to search for Walt Disney World as a feature, the DRG comes with a collar, look for the information\nand then download the DRG that contains Disney that tells you what the vertical datum is, when the\nWorld). map was created, and when it was last photo-revised.\nThis elevation (or vertical) difference between contour lines is called the\ncontour interval the contour interval and is set up according to the constraints of the map and the\nvertical difference area being measured. In general, a wider contour interval is selected when\nbetween contour lines\nmapping more mountainous terrain (since there are numerous higher eleva-\ndrawn on a map.\ntions) and a narrower contour interval is used when mapping flatter terrain\n(since there are fewer changes in variation and more details can be mapped).\nSmall-scale maps (see Chapter 7) tend to use a wider contour interval because\nthey cover a larger geographic area and are presenting more generalized ter-\nrain information, whereas large-scale maps generally utilize a narrow contour\ninterval for the opposite reason (they show a smaller geographic area and\nthus can present more detailed terrain information). DRGs (and their contour\nlines) are frequently used as data sources within geospatial technology (such\nas overlaying them with other layers in GIS). See Hands-on Application 13.1:\nDigital Raster Graphics Online for a method of obtaining DRGs online.\nWhile DRGs remain available (and many software programs give you\nthe capability of examining \u201cseamless\u201d digital topographic maps), the \u201cnext\nUS Topo a digital generation\u201d of digital topographic maps is the US Topo series. The US Topo\ntopographic map series maps are delivered in 7.5 minute quad sections, but in GeoPDF file format\ncreated by the USGS to (see Chapter 7), which allows multiple layers of data to be stored on the\nallow multiple layers\nsame map. These layers can then be turned on and off. US Topo maps in-\nof data to be used on\nclude contours, recent transportation features (roads, railroads, airports),\na map in GeoPDF file\nformat. transportation names, hydrography, boundaries, and orthophoto images, all\nas separately accessible layers (see Figure 13.4 for an example of a US Topo\nmap) as well as the full collar of information (which can have its informa-\ntion be displayed or removed as a separate map layer). The USGS makes US\nTopo maps available for free download (see Hands-on Application 13.2: US\nTopo Maps as GeoPDFs for accessing and utilizing US Topo maps). Although 381\nHow Can Terrain Be Represented?\nFIGURE 13.4 A US Topo\nGeoPDF of Coffeyville,\nKansas showing the\navailable layers for the\nmap. (Source: USGS)\nHands-on Application 13.2\nUS Topo Maps as GeoPDFs\nTo see what US Topo maps are available for At this point, you can type the name of a place\ndownload in GeoPDF file format, open your Web to search for available US Topo maps, and then\nbrowser and go to http:\/\/nationalmap.gov\/ choose to download the maps as a free GeoPDF.\nustopo\/index.html\u2014the main Website for US For instance, search for Kansas City, then from the\nTopo maps. Click on the link for Coverage to see available options, download the 7.5 (cid:2) 7.5 US Topo\nthe current status of which states have been map for Kansas City. A zip file will be downloaded\nprocessed into US Topos. Follow the link on the to your computer containing the GeoPDF. Open the\nWebsite to the current download site (at the time PDF (you may be prompted to install a plug-in to\nof this writing, it was the USGS Store) to access access all of the GeoPDF features) and examine the\nthe Map Locator. various layers available within the US Topo map. 382\nChapter 13 Digital Landscaping\nThinking Critically with Geospatial Technology 13.1\nIf Everything\u2019s Digital, Do We Still Need Printed Topo Maps?\nA similar question was raised back in Chapter 2: If all while in the field) need to carry printed topo maps\nof the latest USGS topographic maps are available with them? Is there a need for geographers to have\ndigitally and in easily accessible formats, is there printed versions of several quads when seamless\nstill a need to have printed copies (at a variety of digital copies (which can be examined next to one\nmap scales) on hand? For instance, do surveyors, another) can be easily (and freely) obtained? Are\ngeologists, botanists, or archeologists (or any other there situations where a printed topo map is a nec-\nprofessional who requires topographic information essary item given the readily available digital data?\ntopographic maps in all of their incarnations can represent terrain with con-\ntours, there are more detailed methods of digital terrain modeling available\nthrough geospatial technology.\nHow Can Geospatial Technology\nRepresent Terrain?\nDTM a representation In geospatial technology, terrain and landscape features can be represented\nof a terrain surface by more than just contour lines on a map. A Digital Terrain Model (DTM)\ncalculated by is the name given to a model of the landscape that is used in conjunction\nmeasuring elevation\nwith GIS or remotely sensed imagery. The function of a DTM is to accurately\nvalues at a series of\nrepresent the features of the landscape and be useful for analysis of the ter-\nlocations.\nrain itself. The key to a DTM is to properly represent a z-value for x and y\ntwo-and-a-half-\nlocations. With a z-value, the model can be shown in a perspective view to\ndimensional (2.5D)\ndemonstrate the appearance of the terrain, but this doesn\u2019t necessary make\nmodel a model of the\nterrain that allows for it a three-dimensional model. In fact, a DTM is usually best described as a\na single z-value to be two-and-a-half dimensional model. In a two-dimensional (2D) model, all\nassigned to each x\/y c oordinates are measured with x and y values without a number for z at\ncoordinate location.\nthese coordinates. In a two-and-a-half-dimensional (2.5D) model, a sin-\nthree-dimensional gle z value can be assigned to each x\/y coordinate as a measure of elevation\n(3D) model a model of at that location. In a full three-dimensional (3D) model, multiple z-val-\nthe terrain that allows\nues can be assigned to each x\/y coordinate. Most DTMs have one elevation\nfor multiple z-values to\nvalue measured for the terrain height at each location, making them 2.5D\nbe assigned to each x\/y\ncoordinate location. models (Figure 13.5).\nAn example of a type of DTM that\u2019s used in terrain modeling is a TIN (Tri-\nTIN Triangulated\nangulated Irregular Network), in which selected elevation points (those that\nIrregular Network. A\nterrain model that the system deems the \u201cmost important\u201d) of the terrain are used in construct-\nallows for non-equally ing the model. Points are joined together to form non-overlapping triangles,\nspaced elevation points representing the terrain surfaces (see Figure 13.6). While TINs are often\nto be used in the\nused in terrain modeling in GIS, there\u2019s another very common type of digital\ncreation of the surface.\nterrain model called the DEM. 383\nWhat Is a DEM?\nz\nz z\nx x z x\n3\n2D z zz 22 z 3 2.5D z 2 3D\n1 z\n1\n(a) (b) (c)\ny y y\nFIGURE 13.5 A comparison of (a) 2D, (b) 2.5D, and (c) 3D models of terrain.\nFIGURE 13.6 A TIN\nrepresentation of the\nterrain around Gate City,\nVirginia. (Source: Esri\u00ae\nArcGIS ArcScene graphical\nuser interface Copyright \u00a9\nEsri. All rights reserved.)\nWhat Is a DEM?\nA DEM is a Digital Elevation Model, a specific type of model of the terrain and DEM Digital Elevation\nlandscape. A DEM is a terrain model produced by the USGS and others. A DEM Model\u2014a representation\nof the terrain surface,\nis based on regularly spaced point data of elevations but can be c onverted to a\ncreated by measuring\nraster grid representation (see Chapter 5) for use with other geospatial data,\na set of equally spaced\nsuch as satellite imagery or GIS layers. When used in grid format, the eleva-\nelevation values.\ntion values are represented by the grid cell value, while the grid resolution\nrepresents the size of the cell being measured. Thus, DEM resolution is mea-\nsured in a manner similar to the way a remotely sensed image\u2019s resolution SRTM the Shuttle\nwould be measured (see Chapter 10). If a DEM has 30-meter resolution, then Radar Topography\nMission, flown in the\neach of the DEM\u2019s raster grid cells is set up at 30 meters in size. USGS DEMs\nyear 2000, which\nhave been created using Digital Line Graph (DLG) information. Other meth-\nmapped Earth\u2019s\nods of DEM creation involve the use of remotely sensed data or stereo imagery\nsurface from orbit\n(see Chapter 15) and photogrammetry to derive elevation values. for the purpose of\nAnother source of terrain data is the Shuttle Radar Topography Mission constructing digital\n(SRTM) a part of a mission of the Space Shuttle Endeavor in the year 2000. elevation models of\nthe planet.\nFor 11 days, Endeavor used a special radar system to map Earth\u2019s terrain 384\nChapter 13 Digital Landscaping\nHands-on Application 13.3\nSRTM Imagery Online\nNASA maintains an online gallery of SRTM imagery States (Alaska, Utah, and Washington State), Central\nfrom the 2000 mission, as well as other products America (C osta Rica), and Africa (Congo). Note that\nrelated to SRTM. Open your Web browser and go they\u2019re only e xample graphics of SRTM results, not a\nto http:\/\/www2.jpl.nasa.gov\/srtm. Select the op- way to download the actual SRTM data.\ntion for Gallery of Images to view image results There\u2019s also a Multimedia option on the main\nof SRTM results from around the globe. Check out page, which will allow you to view a series of video\nseveral of the examples of SRTM imagery that are files (in Real Player or Quicktime) of SRTM in action.\nset up in perspective view\u2014among others, check These should help give you a feel for how terrain\nout the image galleries of North America\u2014United can be mapped from space.\nand topographic features from orbit, resulting in a highly accurate digital\ne levation model. At the mission\u2019s close, roughly 80% of Earth was examined\nand modeled as 90-meter DEMs (and 30-meter DEM data is available for the\nUnited States). See Hands-on Application 13.3: SRTM Imagery Online for more\nabout STRM data.\nLIDAR Light Detection An additional remote sensing method of terrain mapping is called LIDAR\nand Ranging. A process (Light Detection and Ranging). Rather than firing a microwave pulse at a\nin which a series of\nground target like a RADAR system would, LIDAR uses a laser beam to mea-\nlaser beams fired at the\nsure the terrain. In LIDAR, a plane flies over the ground equipped with a\nground from an aircraft\ns pecial system that fires a series of laser beams (between 2000 and 5000 puls-\nis used for creation of\nhighly accurate DEMs. es per second) at the ground. The laser beams are reflected from the ground\nback to the plane, and based on the distance from the plane to targets on the\nground, the elevation of the landscape (as well as objects on the surface of\nthe terrain) can be determined. GPS (see Chapter 4) is used in part to deter-\nmine where the beams were striking the ground. After the data is collected\nand p rocessed, the end result of the LIDAR mission is a highly accurate set\nof x\/y locations with a z-value. The elevations of the DEM products derived\nfrom LIDAR have a vertical accuracy of 15 centimeters (see Figure 13.7 for an\nexample of a LIDAR-derived DEM).\nThe USGS has numerous types of DEMs available, depending on the size\nNED the National of the area being covered and the DEM resolution. The USGS distributes this\nElevation Dataset,\nelevation data for free download via the NED (National Elevation Dataset).\nwhich provides digital\nThe purpose of the NED was to have a digital elevation product that covers\nelevation coverage\nthe entire United States at the best resolution possible and to eliminate any\nof the entire United\nStates. gaps in the data. The NED is designed to be \u201cseamless\u201d in that its data use the\nsame datum, projection, and elevation unit, without having separate tiles of\nSeamless Server an\ndata that the user needs to match up. With the NED, users can select which\nonline utility used\nby the USGS for the sections of the national elevation dataset they need and download those\ndownload of different areas.\ndatasets, including NED data is made freely available online via the USGS and the Nation-\nelevation data.\nal Map Seamless Server, a very versatile online utility for downloading 385\nWhat Is a DEM?\nFIGURE 13.7 Mount St.\nHelens as viewed from\nLIDAR data. (Source: USGS)\ngeospatial data (see Chapter 15 for more about the National Map itself). With\nthe National Map Seamless Server, you can specify the geographic region you\nrequire data for and select from multiple datasets that cover that region to\ndownload. NED data is available at 1 arc second (about 30-meter), 1\/3 arc\nsecond (about 10-meter), and 1\/9 arc second (about 3-meter) resolutions,\ndepending on the region. In addition, digital topo maps and high-resolution\northoimagery are among the many other datasets available for download (see\nHands-on Application 13.4: The National Map Seamless Server on page 386 for\nmore information on using the Seamless Server). viewshed a data layer\nWith terrain data, many different types of terrain analysis can be per- that determines what\nformed. DEMs are used in geospatial technology for creating viewsheds\u2014 an observer can see\nand cannot see from a\nmaps that show what can be seen or not seen from a particular vantage point.\nparticular location due\nA viewshed is used for determining how far a person\u2019s visibility is (that is,\nto terrain.\nwhat they can see) from a location before his or her view is blocked by the\nslope a measurement\nterrain. DEMs are also used for a variety of hydrologic applications, such as\nof the rate of elevation\ncalculating the accumulation of water in an area or extraction of stream chan-\nchange at a location,\nnels or watersheds. found by dividing the\nDEMs can be used to derive a new dataset of slope information\u2014r ather vertical height (the\nthan elevation values, slope represents the change of elevations (and the rise) by the horizontal\nlength (the run).\nrate of change) at a location by calculating the rise (vertical distance) over\nthe run (horizontal distance). When a slope surface is created from a DEM, slope aspect a\ninformation can be derived not just about heights, but about the steepness determination of the\ndirection that a slope is\nor flatness of areas. Similarly, a surface of slope aspect can be computed,\nfacing.\nwhich will show (for each location) the direction that the slope is facing. 386\nChapter 13 Digital Landscaping\nHands-on Application 13.4\nThe National Map Seamless Server\nWith the National Map Seamless Server, you can de- Next, select the option for View and Down-\nfine the geographic area that you want data for, and load United States Data and use the online tools\ndatasets aren\u2019t limited by things like state bound- to zoom in an area near you (like your township or\naries, county boundaries, or USGS topoquad size. your school or workplace\u2019s city). Check which kinds\nWhen you\u2019re downloading NED data (for instance), of data are available to download. Some other data\nyou\u2019re selecting the NED information from a \u201cseam- types (such as the orthoimagery) can be unzipped\nless\u201d dataset without breaks in it. Open your Web and viewed using a utility like ArcGIS, AEJEE, or\nbrowser and go to http:\/\/seamless.usgs.gov and MultiSpec.\nselect the option for the Seamless Viewer. A new Also, from the main Seamless Web page, NED\nwindow will open for the interactive viewer\u2014at this data can be downloaded in a pre-packaged for-\npoint, examine the options listed under Display. mat, enabling you to access NED data for states or\nWhat kinds of data are available beyond just the counties.\nthree types of NED data?\nNow, you not only have information about where the steepest slopes are lo-\ncated, but whether they are north-facing, east-facing, and so on. Figure 13.8\nshows a comparison of a DEM with the slope and slope aspect maps derived\nfrom it.\nFIGURE 13.8 A USGS\nDEM of a portion of the\nLaurelville\/Hocking region\nof Ohio and the slope and\nslope aspect grids derived\nfrom it. (Source: USGS. Esri\u00ae\nArcGIS ArcScene graphical\nuser interface Copyright \u00a9\nEsri. All rights reserved.) 387\nHow Can You Make Terrain Look More Realistic?\nHow Can You Make Terrain Look\nMore Realistic?\nDigital terrain models aren\u2019t just limited to flat images (like a DRG) on a\npseudo-3D a term\ncomputer screen or a paper map\u2014remember, constructs such as DEMs are\noften used to describe\n2.5D models, and since they have a z-value, that z-value can be visualized\nthe perspective view of\nin a 3D view. It\u2019s more accurate to say these types of visualizations are re- a terrain model since it\nally pseudo-3D views since they\u2019re really 2.5D, not really a full 3D model. is often a 2.5D model,\nSetting up a pseudo-3D view of a digital terrain model involves examining not a full 3D model.\nit at a perspective view (or an oblique view). Points or raster cells are perspective\nelevated to the height of their z-value and the resultant model is shown view viewing a\nfrom an angular view. In this way, mountain peaks can be seen jutting up digital terrain model\nat an oblique angle\nfrom the surface in the same way a meteor crater looks like a depression\nin which it takes on a\nin the ground.\n\u201cthree-dimensional\u201d\nIn many geospatial technology applications, the model then becomes in-\nappearance.\nteractive, allowing the user to move or \u201cfly\u201d over the terrain, skimming over\nthe surface and banking past mountains. Beyond viewing the terrain in per-\nFIGURE 13.9 A pseudo-\nspective view, there are numerous ways to make the terrain more realistic-\n3D view of a digital\nlooking, such as artificially altering its appearance or draping imagery over\nterrain model enhanced\nthe surface (see Figure 13.9 for an example of a terrain surface shown in per- with a digital topographic\nspective view with a digital topographic map overlaid to show off the contour map displayed over the\nlines in relation to the pseudo-3D model). terrain. (Source: National\nGeographic Society. Esri\u00ae\nRemember that a DEM (or other digital terrain model) is just a map\nArcGIS Explorer graphical user\nor grid of elevation surfaces. Like any other map, it can be displayed with interface Copyright \u00a9 Esri. All\nrights reserved.) 388\nChapter 13 Digital Landscaping\nvarious color ramps to show the differences in elevations, but there are sever-\nhillshade a shaded\nal ways to make the terrain model look more realistic (and start to resemble\nrelief map of the terrain\ncreated by modeling that image of Mount Everest from the beginning of this chapter). The first of\nthe position of the Sun these techniques is called a hillshade, which models how the terrain would\nin the sky relative to look under different lighting conditions (based on the location of the Sun in\nthe landscape.\nthe sky). When constructing a hillshade, you can determine where the light\nSun altitude the value source (that is, the Sun) is located with respect to the terrain, thus simulat-\nbetween 0 and 90 ing the effects of shadows being cast on the landscape by the various terrain\nused in constructing a\nfeatures.\nhillshade to model the\nThis is done by setting two parameters for the Sun\u2014the first of these is\nSun\u2019s elevation above\nthe terrain. the Sun altitude, a value between 0 and 90, representing the angle of the Sun\nin the sky between 0 degrees (directly at the surface) and 90 degrees (directly\nSun azimuth the value\noverhead). The second parameter is the Sun azimuth, a value between 0 and\nbetween 0 and 360\nused in constructing a 360, representing the location of the Sun in relation to where the Sun\u2019s rays\nhillshade to model the are coming from. The values represent a circle around the landscape, with 0\nSun\u2019s position in the sky being due north, 90 being due east, 180 being due south, and 270 being due\nto show the direction of\nwest (the values are measured clockwise from due north). By setting values\nthe Sun\u2019s rays striking\nfor Sun altitude and Sun azimuth, the terrain can take on various appear-\nthe surface.\nances to simulate how the landscape looks under different conditions during\ndraping a process\nthe day. A hillshade using a Sun altitude of 45 degrees and a Sun azimuth of\nin which an image is\n315 degrees is shown in Figure 13.10.\ngiven z-values to match\nHillshading provides a good shaded map of what the terrain would\nthe heights in a digital\nterrain model. look like under various lighting conditions, but there are plenty of features\non the landscape (likes roads and land cover) that aren\u2019t shown with a hill-\nshade. In order to see these types of features, a process called draping is\nFIGURE 13.10 A DEM of\nused to essentially show the terrain model with a remotely sensed image\nColumbiana County, Ohio,\nand a hillshade of the DEM (or another dataset) on top of it. Figure 13.11 shows an example of a Land-\nmade using a Sun elevation sat TM image draped over a DEM. Draping is achieved by first aligning the\nof 45\u00b0 and a Sun azimuth\nimage with its corresponding places on the terrain model, then assigning\nof 315\u00b0. (Source: USGS. Esri\u00ae\nthe z-values from those locations on the terrain (in Esri terminology, these\nArcGIS graphical user interface\nCopyright \u00a9 Esri. All rights\nreserved.) 389\nHow Can You Make Terrain Look More Realistic?\nare referred to as base heights) to those locations on the image. In essence, FIGURE 13.11 A DEM\nof the Laurelville\/Hocking\nlocations on the image are assigned a z-value that corresponds with the\nregion of Ohio and a\nterrain model.\nLandsat TM image draped\nDraping is a common technique to show remotely sensed imagery on ter- over the DEM. (Source:\nrain features (similar to how Mount Everest looks in Figure 13.1 on page 378). USGS. Esri\u00ae ArcGIS graphical\nuser interface Copyright \u00a9\nVirtual globes (like Google Earth and NASA World Wind) can show pseudo-\nEsri. All rights reserved; USGS,\n3D landscapes by draping imagery over the terrain models that make up their\nNASA. Esri\u00ae ArcGIS graphical\nlandscapes. By creating a new draped image in perspective view, one can get user interface Copyright \u00a9 Esri.\nnew visual information about the appearance of the landscape that\u2019s not di- All rights reserved.)\nrectly obtainable through examination of contours, DRGs, or non-perspective\nDEMs (see Hands-on Application 13.5: Terrain and Imagery Examples in Google base heights the\nz-values of a digital\nEarth for more information). For example, draping a DRG over a DEM and\nterrain model that can\nlooking at it in perspective can visually demonstrate how contour lines match\nthen be applied to an\nup with the elevations that they represent. image in a draping\nEven with hillshading or draping to improve the terrain\u2019s appearance, procedure.\nthere\u2019s no getting around the fact that some sections of terrain have relatively\nlow variability. In these cases, differences in landscape elevations or slopes\nvertical\nmay be difficult to see when viewing or interacting with a digital terrain mod-\nexaggeration a\nel. The solution is to artificially enhance the differences between elevations process whereby the\nso that the landscape can be better visualized. Vertical exaggeration is the z-values are artificially\nprocess of artificially altering the terrain model for visualization purposes so enhanced for terrain\nvisualization purposes.\nthat the vertical scale of the digital terrain model is larger than the horizontal\nHands-on Application 13.5\nTerrain and Imagery Examples in Google Earth\nStart up Google Earth and \u201cFly To\u201d Glacier National you\u2019re examining here is imagery that has been\nPark in Montana. Make sure the \u201cTerrain\u201d option is draped over a digital terrain model representing the\nturned on (by default it should be on when Google landscape of this section of the country. Fly around\nEarth begins) and use the zoom slider and the other the Glacier area (see Figure 13.12 for an example),\nGoogle Earth navigation tools to change the view getting a feel for navigation over draped imagery.\nso that you\u2019re examining the mountains and terrain You\u2019ll be doing more of this (among many other\nof Glacier in a perspective view. Remember what things) in Geospatial Lab Application 13.1. 390\nChapter 13 Digital Landscaping\nFIGURE 13.12\nExamining imagery on a\ndigital terrain model of\nGlacier National Park in\nGoogle Earth. (Source: \u00a9\n2009 Google, Image \u00a9 2009\nDigital Globe)\nscale. For instance, if the model\u2019s vertical exaggeration were \u201c5x,\u201d then the\nvertical scale (the z-values) would be five times that of the horizontal scale.\nThese types of artificial changes can really enhance certain vertical features\n(such as making valleys appear deeper or peaks appear higher) for visual pur-\nposes. The downside to vertical exaggeration is that it alters the scale of the\ndata and should be used only for visualizing the data. For a comparison of dif-\nferent vertical exaggerations applied to a DEM, see Figure 13.13.\nFIGURE 13.13 Different\nlevels of vertical\nexaggeration for a DEM\n(values of none, 2, 5, and\n10). (Source: USGS. Esri\u00ae\nArcGIS ArcScene graphical user\ninterface Copyright \u00a9 Esri. All\nrights reserved.) 391\nKey Terms\nChapter Wrapup\nThis chapter explored methods of modeling and visualizing the landforms\non Earth\u2019s surface. In the next chapter, we\u2019ll do a lot more with the 3D as-\npect of visualization. After all, there\u2019s more than just the terrain that can be\nviewed in 3D\u2014there are plenty of structures, buildings, and natural growth\nthat can be added to a pseudo-3D terrain model to improve the realism of the\nscene. Chapter 14 picks up where this one leaves off to continue pursuing new\navenues of 3D visualization.\nThis chapter\u2019s lab will allow you to implement all of the chapter topics\nusing not only Google Earth but also a new software package (called MICRO-\nDEM) that allows you to work directly with a DEM and its derivations.\nImportant note: The references for this chapter are part of the online com-\npanion for this book and can be found at http:\/\/www.whfreeman.com\/\nshellito1e.\nKey Terms\nz-value (p. 377) SRTM (p. 383)\nvertical datum (p. 377) LIDAR (p. 384)\ntopographic map (p. 378) NED (p. 384)\nDRG (p. 378) Seamless Server (p. 384)\nGeoTIFF (p. 378) viewshed (p. 385)\ncollar (p. 378) slope (p. 385)\ncontour (p. 379) slope aspect (p. 385)\ncontour interval (p. 380) pseudo-3D (p. 387)\nUS Topo (p. 380) perspective view (p. 387)\nDTM (p. 382) hillshade (p. 388)\ntwo-and-a-half-dimensional (2.5D) Sun altitude (p. 388)\nmodel (p. 382) Sun azimuth (p. 388)\nthree-dimensional (3D) model draping (p. 388)\n(p. 382) base heights (p. 389)\nTIN (p. 382) vertical exaggeration (p. 389)\nDEM (p. 383) 13.1\nGeospatial Lab Application\nDigital Terrain Analysis\nThis chapter\u2019s lab will introduce you to some of the basics of digital terrain\nmodeling\u2014working with DEMs, contours, and draped imagery over the ter-\nrain model. You\u2019ll be using Google Earth and the MICRODEM software pro-\ngram (developed by Professor Peter Guth of the Oceanography Department of\nthe U.S. Naval Academy) for this lab.\nObjectives\nThe goals of this lab are:\na Examining pseudo-3D terrain and navigating across it in Google Earth.\na Filming an animation of flying over 3D terrain in Google Earth.\na Examining the effects of different levels of vertical exaggeration on the terrain.\na Familiarizing yourself with the MICRODEM operating environment.\na Draping an image over the terrain in Google Earth.\na Deriving hillshades, contours, and slope measurements from a DEM for\nt errain analysis.\na Examining a DEM in both 2D and perspective view.\na Using a DEM to create a flight path for a video in MICRODEM.\nObtaining Software\na The current version of Google Earth (6.0) is available for free download\nat http:\/\/earth.google.com.\na The current version of MICRODEM (12) is available for free download\nat http:\/\/www.usna.edu\/Users\/oceano\/pguth\/website\/microdem\/\nmicrodem.htm.\nImportant note: Software and online resources sometimes change fast.\nThis lab was designed with the most recently available version of the software\nat the time of writing. However, if the software or Websites have significantly\nchanged between then and now, an updated version of this lab (using the new-\nest versions) is available online at http:\/\/www.whfreeman.com\/shellito1e.\nLab Data\nAll data used in this lab is either part of the software or comes included as\nsample data as part of the software installation.\nLocalizing This Lab\nThe Google Earth sections of this lab can be performed using areas nearby\nyour location, as this lab features underlying terrain that covers the globe.\n392 393\nDigital Terrain Analysis\nThe MICRODEM section of the lab can be performed with a DEM of your\nlocal area. See Hands-on Application 13.4: The National Map Seamless Server\nfor more information on downloading elevation datasets.\n13.1 E xamining Landscapes and Terrain\nwith Google Earth\n1. Start Google Earth and for a good example of some variable terrain, \u201cFly\nTo\u201d the Grand Canyon. For more information about the Grand Canyon,\ncheck out this Website: http:\/\/www.nps.gov\/grca\/index.htm.\nBy default, Google Earth\u2019s \u201cTerrain\u201d options will be turned on for you.\n2. Also, select the Tools pull-down menu and choose Options. In the\ndialog box that appears, click on the Navigation tab, and make sure\nthe box that says \u201cAutomatically tilt when zooming\u201d has its radio button\nfilled in. This will ensure that Google Earth will zoom in to areas while\ntilting to a perspective view.\n3. Use the Zoom Slider to tilt the view all the way down that it will go, so\nthe view looks like you\u2019re standing somewhere in the Grand Canyon (see\nGeospatial Lab Application 1.1 for more info on using this tool). Basically,\npush the \u201cplus\u201d button on the Zoom Slider as far down as it\u2019ll go and\nyour view will zoom in and tilt down to \u201ceye level.\u201d\n4. Use your mouse wheel to vertically move the view backwards (do\nthis instead of using the Zoom Slider\u2014otherwise you\u2019ll keep tilting\nbackwards as well) so you can see some areas of the Grand Canyon,\nsome of the terrain relief, and the horizon. See the accompanying\ngraphic of Google Earth (below) for an example of examining the Grand\nCanyon area from this type of perspective view.\n(Source: \u00a9 2010 Google, Image \u00a9 DigitalGlobe, Image USDA Farm Service Agency, Image NMRGIS) 394\nChapter 13 Digital Landscaping\n5. There are two references to heights or elevation values in the bottom\nportion of the view:\na The heading marked \u201celev\u201d shows the height of the terrain model (that\nis, the height above the vertical datum) where the cursor is placed.\na The heading marked \u201cEye alt\u201d shows Google Earth\u2019s measurement for\nhow high above the terrain (or the vertical datum) your vantage point is.\nManeuver your view so your \u201cEye alt\u201d is at a good level above the terrain\nso that you can see over the mountains and into the canyons\u2014this will\nbe a good height to start flying over the terrain.\n6. Use the mouse and the Move tools to practice flying around the Grand\nCanyon. You can fly over the terrain, dip in and out of valleys, and skim\nover the mountaintops. When you\u2019ve got a good feel for flying over 3D\nterrain in Google Earth, then move onto the next step.\n13.2 Recording Animations in Google Earth\n1. On Google Earth\u2019s toolbar, select the Record a Tour button.\nThis option will have Google Earth record your flight and save it as a\nvideo. A new set of controls will appear at the bottom of the view:\n2. To start recording the video, press the red record button.\nImportant note: If you have a microphone hooked up to your computer\nyou can get really creative and narrate your flight\u2014your narration or sounds\nwill be saved along with your video.\n3. Keep flying around for a short tour (30 to 60 seconds) of the Grand Canyon.\n4. When you\u2019re done, press the red record button again to end the\nrecording of the tour.\n5. A new set of controls will appear at the bottom of the screen and Google\nEarth will begin playing your video.\n6. Use the rewind and fast-forward buttons to skip around in the video,\nand the play\/pause button to start or stop. The button with the two\narrows will repeat the tour or put it on a loop to keep playing. 395\nDigital Terrain Analysis\n7. You can save the tour by pressing the disk icon (the Save Tour) button.\nGive it a name in the dialog box that opens. The saved tour will be added\nto your Places box (just like all other Google Earth layers).\n8. Put together a good tour (perhaps about a minute long) of the Grand\nCanyon that shows off many of the area\u2019s terrain features and save the\ntour as a Google Earth layer.\nQuestion 13.1 What areas in the Grand Canyon did you select for your\ntour, and what terrain features did you highlight during the tour?\n13.3 V ertical Exaggeration and Measuring Elevation\nHeight Values in Google Earth\nGoogle Earth also allows you to alter the vertical exaggeration of the terrain layer.\nAs discussed on page 389, vertical exaggeration is used for visualization purposes.\n1. To look at different levels of vertical exaggeration, select Options from\nthe Tools pull-down menu. Select the 3D View tab.\n2. You can alter the quality of the terrain (from lowest to highest) for\nvisualization purposes. In the box marked Elevation Exaggeration, you\ncan type a value (between 0.5 and 3) for vertical exaggeration of Google\nEarth\u2019s terrain. Type a value of 1, then click Apply and OK, and then\nre-examine the Grand Canyon.\nQuestion 13.2 Try the following values for vertical exaggeration: 0.5, 1, 2,\nand 3. How did each value affect the visualization of the terrain? Which was\nthe most useful for a visual representation of the Grand Canyon and why?\n(Source: \u00a9 2011 Google) 396\nChapter 13 Digital Landscaping\n3. Reset the Elevation Exaggeration to a value of 1 when you\u2019re done.\nFrom here, we\u2019ll examine the elevation values of the terrain surface\nitself. Although Google Earth will always show the imagery over the\nterrain model, elevation information of each location is available.\n4. In the Layers box, under the More option, turn on the \u201cParks and\nRecreation\u201d layer. Several new symbols will appear around the Grand\nCanyon, designating specific locales within the park. Wherever you\nmove the cursor on the screen, a new value for elevation is computed in\nthe \u201celev\u201d option at the bottom of the view. By zooming into one of the\nnew marker spots and placing the cursor on its symbol on the screen,\nyou can determine the elevation value for that location.\nQuestion 13.3 At what elevations are the heights of all the Visitors Centers\n(labeled with a question mark in a circle as their placemark) within the park?\n5. At this point, we\u2019ll be moving onto a new software package called\nMICRODEM, which allows you to work directly with the terrain models and\ndata themselves, not just use the fully processed versions in Google Earth.\nImportant note: Don\u2019t close Google Earth just yet. You\u2019ll use it later for\nanother way to take a look at the data you\u2019ll use next with MICRODEM.\n6. Use Google Earth to \u201cFly To\u201d Hanging Rock Canyon, California.\n13.4 Getting Started with MICRODEM\n1. Start MICRODEM (the default install is an icon called MICRODEM). It\nwill open with the default screen full of buttons on the toolbar.\n2. To examine a DEM, select the File pull-down menu and choose Open DEM.\n3. When MICRODEM installed, it added a new folder called \u201cmapdata\u201d to\nyour computer (the default location is a C:\\mapdata folder). Navigate\nto this folder, select the folder called DEMs, and then select the\n\u201cHangRockCanyon_DEM_2.tar.gz\u201d file. 397\nDigital Terrain Analysis\n4. Click Open to start using this DEM in MICRODEM. (The DEM will open\nin a new viewer.)\n5. Before moving on, use the DEM and the view in Google Earth to orient\nyourself to where, spatially, Hanging Rock Canyon is and what section\nof the landscape is being displayed by the DEM.\n6. When you have yourself oriented, right-click anywhere on the DEM and\na new menu will appear. From the available options, select Load, then\nselect Google Earth overlay.\n7. MICRODEM will then apply a draped image of the DEM over its\ncorresponding area inside of Google Earth (and add a new object called\nMICRODEM to Google Earth\u2019s Temporary Places folder). This will show\nyou where the DEM\u2019s parameters match up with the Google Earth\nimagery.\n13.5 Hillshades\n1. To change the appearance of the DEM, right-click anywhere on the\nimage and a new menu will appear. From the available options, select\nReflectance Options.\n2. In the Reflectance Map Options dialog box that opens, select Grays for\nthe Colors option.\n3. You\u2019ll also see options for Sun azimuth (default of 335 degrees) and\nSun elevation (45 degrees above the surface). The circle in the upper\nright of the dialog box shows the position of the Sun relative to the DEM\nwith a red dot. This use of the values for azimuth and elevation creates\nthe hillshade effect for viewing the DEM. To create other visualizations\nof the landscape, you can use other values for Sun azimuth and Sun\nelevation.\n4. Change the Sun azimuth to 90 and the Sun elevation to 10, and then\nclick OK. 398\nChapter 13 Digital Landscaping\nQuestion 13.4 What time of day and Sun conditions do these values for\nSun azimuth and Sun elevation simulate? Why?\n5. Return to the Reflectance Map Options. Test out the effect of other\nhillshade visualizations based on some other Sun azimuth and Sun\nelevation values. Answer Question 13.5. When you\u2019re done, return to\nthe norm of Sun azimuth = 335 and Sun elevation of 45.\nQuestion 13.5 What values for Sun azimuth and Sun elevation would\nsimulate a \u201csunset\u201d viewing appearance of the DEM? Why?\n13.6 Contour Lines and Contour Interval on a DEM\n1. Contour lines can be generated from the DEM and then displayed as a\nmap overlay (among other layers) to examine how the contours match\nup with the terrain represented by the DEM. To see the overlay options,\nselect the Manage Overlays icon from the view\u2019s toolbar.\n2. In the Map Overlays dialog box, placing a checkmark in one of the\noptions will turn on that particular overlay. Place a checkmark in the\nContours box and a new button labeled Contours will be added. Press\nthis button to examine the Contour Intervals.\n3. In the Contour Map Options dialog box, you can specify the Contour\nInterval of the contours to be drawn on the DEM. 399\nDigital Terrain Analysis\n4. Change the Contour Colors option to Single.\n5. Use the default option of 50 meters for the Contour Interval and then\nclick OK. The contours will be drawn over the DEM.\n6. Contours drawn with a 50-meter interval will be placed on the DEM\nas an overlay. Bring up the Map Overlays dialog box again, press the\nContours button, and switch the contour interval to 100 meters. The\n50-meter contours will go away and be replaced by new contours drawn\nat a 100-meter interval.\n7. Try out several different contour intervals as follows: 5 meters, 25 meters,\n250 meters, and 1 kilometer (in addition to the 50- and 100-meter\noptions). See how the use of those intervals changes how contours are\ncreated and drawn.\nQuestion 13.6 From what you\u2019ve seen of the Hanging Rock Canyon,\nCalifornia region (both in Google Earth via the overlay and from the\nHillshade), which of the values for contour intervals (5 meters, 25 meters,\n50 meters, 100 meters, 250 meters, and 1 kilometer) best represents the\nregion? That is, if you were drawing a contour map of Hanging Rock Canyon,\nwhich would you choose and why?\nQuestion 13.7 Following up on Question 13.6, which of the values for\ncontour intervals (5 meters, 25 meters, 50 meters, 100 meters, 250 meters,\nand 1 kilometer) worst represents the region? That is, if you were drawing a\ncontour map of Hanging Rock Canyon, which options would you not use and\nwhy? 400\nChapter 13 Digital Landscaping\n13.7 Slope and a DEM\n1. Because a DEM is showing elevations, factors such as the slope (for\neach location) can be derived. Right-click on the DEM, select Display\nparameter, and choose Slope.\n2. In the Slope Map Options dialog box, select the option for Trafficability\nCategories.\n3. Click OK.\n4. MICRODEM will calculate a measurement for slope (in degrees) for each\nlocation on the DEM.\n5. Examine the new slope map.\nQuestion 13.8 What areas in Hanging Rock Canyon have the steepest\nslopes and what physical features are causing these steep slopes? (Refer to\nsome specific areas.)\n6. Re-open the DEM again to place it in a new view.\n13.8 Three-Dimensional (3D) Visualization of a DEM\n1. The last thing we\u2019re going to do in this lab is to examine the DEM\nin a pseudo-3D view and do some more flying. In a program like\nGoogle Earth, the terrain data has already been processed for you\nto work with\u2014in these steps, we\u2019re actually going to set up the\npseudo-3D view.\n2. From the main MICRODEM toolbar, click on the Oblique Plot button. 401\nDigital Terrain Analysis\n3. The Oblique Plot will enable you to take a section of the DEM and\nexamine it in oblique perspective view. After clicking on Oblique\nPlot, double-click on a section of the DEM and then drag the mouse\nto another section. The red box that you are drawing by clicking\nand dragging represents the area of the DEM that will be shown in\nperspective view. Double-click again when you\u2019ve set up the red box for\nthe area you want.\nImportant note: Be sure not to select too large an area. MICRODEM may\nnot be able to handle a section that\u2019s too big.\n4. A new dialog box (called Oblique selection) will occur asking what\nyou want to display in the perspective view. Choose the options for\nReflectance and Show overlays on drapes.\n5. Click OK in the Oblique selection box to begin the process.\n6. The Oblique View of the DEM section will appear. A new toolbar is part\nof the Oblique View\u2014the controls will allow you to (in order):\na Print the image\na Save the image (as a graphic)\na Edit the image (in a photo manager program)\na Copy the image to the clipboard (to use in programs like PowerPoint\nor Word)\na Redraw the image with different parameters\na Rotate the image counterclockwise (to see different sides of it)\na Rotate the image clockwise\na Increase the vertical exaggeration\na Decrease the vertical exaggeration 402\nChapter 13 Digital Landscaping\n7. Close the Oblique plot when you\u2019re done investigating it. This tool\nwill allow you to generate a perspective image of a part of the DEM,\nbut in order to interact with the DEM in a pseudo-3D environment,\nMICRODEM has other tools.\n8. In the main MICRODEM toolbar, select the Flythrough icon.\n9. In the DEM, double-click on a starting point for flying. Drag the mouse\nin the direction you want to fly, and then double-click the mouse again.\nA red line will be drawn between the two points. If you want to add a\nsecond leg to the flight, drag the line another direction and double-click\nagain. This red line indicates the flight path\u2014once you have the flight\npath set up, right-click the mouse and choose End selection.\n10. A new dialog box will appear. Select the Viewport tab. These options\nwill allow you to change factors such as how high you will be flying over\nEarth (the Observer above ground option) or the Observer elevation.\nFor your first flight, just accept the defaults and click OK. 403\nDigital Terrain Analysis\n11. Two new windows will appear, one showing the flight path as red and\nblack lines and another window that shows a video clip of flying over the\nDEM.\n12. MICRODEM will prompt you to \u201cContinue w\/these parameters?\u201d Click\nYes to watch the video clip.\n13. A short clip will play and then a new dialog box will open prompting you\nto open a movie. Select the file called \u201cFLYT.MOV\u201d and click Open.\n14. A set of controls will appear at the top of the screen as part of the\nPETMAR Trilobites Movie Player to allow you to play, pause, rewind, set\nup a loop, or alter the delay between the images shown that make up the\nanimated video.\n15. You can stop playing and exit the video at any time and return to change\nflying options or select a new path to fly over. Play around with some of\nthe flying options, then answer Question 13.9.\nQuestion 13.9 What chosen flight path and elevation parameters enabled\nthe best-appearing flying video for you and why?\nClosing Time\nMICRODEM is a very powerful program with a lot of functionality and options\nfor working with DEMs and digital terrain analysis, and it has plenty more\nfeatures to investigate for your own future use. When you\u2019re finished, close\nboth MICRODEM and Google Earth. This page was intentionally left blank 614\nSee the World in 3D\n3D Geovisualization, 3D Modeling and Design, Prism Maps,\nGoogle SketchUp, and Google Earth in 3D\nUp until the previous chapter, all topics in this book dealt with geospatial\ntechnology in a two-dimensional format, whether as geospatial data, measure-\nments, maps, or imagery. Chapter 13 began to go beyond two dimensions and\nstart on a third, presenting terrain in a perspective and pseudo-3D view. This\nchapter looks entirely at presenting geospatial information in three dimen-\nsions, as well as designing and visualizing 3D data and concepts.\nThere\u2019s no doubt that 3D visualization of data is impressive\u2014as technol-\nogy has improved and computers have become faster and more powerful, 3D\nrendering has become more commonplace. Video games and simulators are\nextremely impressive to watch, as are computer-animated movies. While geo-\nspatial technology hasn\u2019t reached the level of the newest CGI film (yet), many\n3D modeling and visualization techniques are available for creating perspec-\ntive views, 3D maps, and realistic-looking 3D objects. For instance, Bing Maps\nand Google Earth both support viewing and interacting with 3D geospatial\ndata. See Figure 14.1 on page 406 for an example of realistic-looking 3D geo-\nspatial visualization in Google Earth.\nBefore proceeding, keep in mind the discussion from Chapter 13 concern-\ning 2.5-dimensional (2.5D) data versus 3-dimensional (3D) data. If only one\nheight value can be assigned to an x\/y location, then the data is considered\n2.5D, and if multiple z-values can be assigned to an x\/y location, then the\ndata is fully 3D (3D data also has volume). For ease of reading and usage, this\nchapter uses the term \u201c3D\u201d throughout to refer to all geospatial phenomena\nthat incorporate a third dimension into their design or visualization, although\ntechnically, some examples will be 2.5D or pseudo-3D.\n440055 406\nChapter 14 See the World in 3D\nFIGURE 14.1 Lower What Is 3D Modeling?\nManhattan (New York\nCity) rendered in 3D.\n(Source: \u00a9 2009 Google, Gray 3D modeling (in geospatial terms) refers to the design of spatial data in a\nBuildings \u00a9 Sanborn, Image three-dimensional (or pseudo-3D) form. Designing a 3D version of the build-\n\u00a9 2009 DigitalGlobe, Image\nings in a city\u2019s downtown or a 3D version of the Eiffel Tower would be ex-\n\u00a9 2009 Sanborn)\namples of 3D modeling of geospatial data. Like all of our spatial data, the 3D\nmodels should be georeferenced to take advantage of real-world coordinate\n3D modeling\nsystems and measurements. Say for instance you\u2019re designing a 3D model of\ndesigning and\nyour house\u2014you\u2019d want to start with a georeferenced base to accurately mea-\nvisualizing data\nsure the dimensions of your house\u2019s footprint. An orthophoto, high-resolution\nthat contains a third\ndimension (a z-axis) satellite image, or architectural diagram with spatial reference would all be\nin addition to x- and good starting points for a georeferenced base to begin modeling from. By\ny-values. digitizing the house\u2019s footprint, you now have a polygon with two dimensions\nz-value the numerical (x and y) to begin with, but to work with a 3D-style object, it has to have\nvalue representing the a third dimension, or a z-dimension. This z-value will be the height of the\nheight of an object. object above the surface.\nextrusion the In order to create a 3D version of the polygon, it will have to be extruded\nextending of a flat to reach the height specified by the z-value. Extrusion is the process of giving\nobject to have a height to an object. If your house is 15 feet high, the footprint polygon can be\nz-value.\nextruded to a height of 15 feet. Extrusion will change the objects\u2014 extruding\nblock a flat polygon a polygon will turn it into a block. In this case, the building footprint will\nobject that has been change to a block object 15 feet high (see Figure 14.2 for the comparison\nextruded.\nbetween the two-dimensional polygon footprint and the extruded block). 407\nWhat Is 3D Modeling?\nFIGURE 14.2 A flat\npolygon versus an\nextruded block. (Source:\n\u00a9 2011 Google SketchUp)\nAny object can be extruded to a particular height by assigning a z-value\nto extrude to. Keep in mind that these items extrude from the ground level up\nand some objects can\u2019t be designed this way. For example, think about what\nyou would do if you were designing a 3D model of a 10-foot-high pedestrian\nwalkway positioned 15 feet over a busy road. If you digitize the bridge\u2019s foot-\nprint and extrude it to 10 feet, it would look like a wall in the middle of the\nroad rather than a bridge over the road. To get around this, you have to apply\nan offset, or a z-value, where the initial object is placed before extrusion. In offset a value applied\nthis case, the digitized polygon representing the bridge would be offset to a to objects to move\nheight of 15 feet (placing it well over the road), then extruded to a height of them off the ground\nlevel.\n10 feet (to capture the dimensions of the walkway). Figure 14.3 illustrates\nFIGURE 14.3 An offset\npolygon that has been\nextruded versus a single\nextruded block. (Source:\n\u00a9 2011 Google SketchUp) 408\nChapter 14 See the World in 3D\nFIGURE 14.4 Multiple\nextruded blocks all making\nup one building (a very\nearly version of Weller\nHouse on Youngstown\nState University\u2019s\ncampus). (Source: \u00a9 2011\nGoogle SketchUp)\nthe difference between a regular extruded block and an extruded block that\nhas been offset from the surface.\nA building or object being modeled will likely consist of multiple blocks\n(or other shapes), each representing a different portion of the object. Multiple\nlevels of a house or differently sized sections of a building can each be repre-\nsented with their own block. Just like a GIS dataset can consist of multiple\npolygons, a 3D representation can consist of multiple extruded shapes and\nblocks (see Figure 14.4). Each of these blocks can then be treated as its own\nobject for further modeling.\nAnother consideration in 3D modeling is that the objects being const-\nructed are not just placed in their correct georeferenced location, but that\nthey also conform correctly to the terrain and real-world elevations. For in-\nstance, if your house is in a location 900 feet above sea level, you don\u2019t want\nto start the base of your house at a ground level of zero feet. When you merge\nthe 3D model of your house with other geospatial data, your house would\nbe shown 900 feet underground rather than at its proper location on Earth\u2019s\nsurface. Terrain modeling was discussed back in Chapter 13, but the key ele-\nment to remember with 3D modeling is the concept of (to use Esri terminol-\nbase heights the ogy) base heights. These values represent the elevation of the terrain above\nelevation values a vertical datum.\nassigned to the terrain\nWhen designing 3D objects to be placed on the terrain (like a model of\nupon which the objects\nyour house), you want to make sure that the terrain\u2019s base heights are applied\nwill be placed.\nto the objects you\u2019re designing. In this way, the software will understand that\nthe base of your house begins at an elevation of 900 feet, and extruding 15 feet\nhigh makes your house\u2019s roof top out at a measurement of 915 feet above\nsea level rather than 15 feet above ground level (zero feet). Many geospatial\nsoftware packages enable the user to combine 3D modeling techniques with 409\nWhat Is 3D Modeling?\nterrain modeling in order to design more realistic 3D visualizations (some\nspecific programs that do this are described later in this chapter).\nOnce the footprints and elevations are correctly set in place, it\u2019s time to\nstart making those gray blocks look like what you\u2019re trying to model. A block\nconsists of several faces, with each face representing one side of the object.\nFor example, an extruded rectangular block would have six faces\u2014the top, face one of the sides\nbottom, and four sides. Each face can be \u201cpainted\u201d to give it a more realistic of a block object.\nappearance. This \u201cpainting\u201d can take a variety of forms\u2014simply changing the\ncolor of the face (for instance, changing the appearance of a side of the house\nto a yellow color) can add to its appearance.\nMore realistic appearances can be achieved by applying a texture to a\nface. A texture is a graphic designed to simulate the appearance of materials textures graphics\n(like brick, wood, stone, glass, etc.) on a face. With textures, you don\u2019t have to applied to 3D objects to\ncreate a more realistic\ndraw individual bricks on the side of a building; just apply a red-brick texture\nappearance.\nand the face\u2019s appearance is changed. Similarly, windows can be designed\nby applying a translucent texture to a window object drawn onto a face, or\ndifferent roofing materials can be applied to the faces making up the top of\nthe building (see Figure 14.5 for an example of a 3D building with various\ntextures applied to the block faces).\nSince a texture is just a graphic, it\u2019s possible to apply other graphics to\na face. For instance, you could take a digital camera picture of your home\u2019s\nparticular brick and apply that to a face. You can even take a picture of the\nentire side of a house or building, resize it, and \u201cpaste\u201d that image onto the\nentire face for an even better appearance. However, it\u2019s not just buildings and\nobjects that can be visualized using 3D modeling techniques. 3D maps can\nalso quickly communicate spatial information to the viewer.\nFIGURE 14.5 A version\nof Youngstown State\nUniversity\u2019s Weller House\nwith other features added,\nas well as further details,\ncolors, and textures\napplied to the block faces\n(compare to the earlier\nversion in Figure 14.4).\n(Source: \u00a9 2011 Google\nSketchUp) 410\nChapter 14 See the World in 3D\nThinking Critically with Geospatial Technology 14.1\nWhat\u2019s the Advantage of Using 3D Design?\n3D visualization has become increasingly com- to the user or viewer of the data? Beyond the\nmon for viewing and using geospatial data. As \u201ccool\u201d factor, what makes 3D visualization so use-\nprocessors have gotten faster and graphics ful for conveying data to the user or viewer as\nand animation have improved, rendering large opposed to a simple 2D map? What advantages\namounts of geospatial data in realistic 3D form does 3D visualization carry over regular 2D for\nis easier than ever. Why is visualization of geo- presentation of data or communicating spatial\nspatial data so critical to conveying a message information?\nHow Are 3D Maps Made?\nA 3D-style map can be made by applying the previously described 3D model-\ning concepts. However, this time you\u2019re not working with extruding polygons\nrepresenting building footprints, but instead you\u2019re working with the shapes\nand objects on a map. A regular choropleth or thematic map (see Chapter 7)\ncan be transformed into a 3D-style map by extruding the polygons displayed\nprism map a thematic on the map. The resulting extruded map is referred to as a prism map, while\nmap that has the map the extruded shapes are called raised prisms. In a prism map, the shapes on\nshapes extruded to\nthe map are extruded to a relative height that reflects the data value assigned\na value based on the\nto them. For instance, if a thematic map of the United States showed popula-\nvalues shown on the\ntion by state, then each state\u2019s shape would be extruded to a \u201cheight\u201d of the\nmap.\nnumber of units equal to that state\u2019s population. Thus, the polygon shape of\nprisms the extruded\nCalifornia would be a raised prism extruded to a value of 36.7 million, while\nshapes on a prism map.\nneighboring Nevada\u2019s shape would be extruded to a value of 2.6 million.\nFigure 14.6 shows an example of a prism map dealing with world popu-\nlation. Each country polygon has a value of the population statistics for that\nFIGURE 14.6 An\nexample of a prism\nmap showing the 2010\npopulation of each\ncountry in the world.\n(Source: thematicmapping.\norg. \u00a9 2010 Google, Data SIO,\nNOAA, U.S. Navy, NGA, GEBCO,\nImage IBCAO, \u00a9 2010 CNES\nSpot Image) 411\nHow Can 3D Modeling and Visualization Be Used with Geospatial Technology?\nHands-on Application 14.1\nCreating Prism Maps Online\nThe Thematic Mapping Engine (also used back see the results in your browser or Download\nin Chapter 7) allows you to create prism maps to open the prism map in Google Earth. Try sev-\nand d isplay them using Google Earth. Open your eral of the classification combinations to see\nWeb browser and go to http:\/\/thematicmapping. how they alter the raised prisms in the data.\norg\/ engine. Select an indicator that you want to Lastly, download a prism map of a dataset into\nmap (such as population, life expectancy, or mo- Google Earth along with a regular 2D thematic\nbile phone subscribers) and a year, and then se- map and examine them to see the difference in\nlect Prism for the technique. Press Preview to visual communication that a prism map offers.\ncountry. Those countries with the highest raised prisms have the largest\nnumber of persons (in this example, the map shows China and India with\nhuge populations relative to neighboring countries). The polygon shape of\neach country is extruded relative to the number of units being measured. See\nHands-on Application 14.1: Creating Prism Maps Online for an online tool to\ncreate prism maps with.\nHow Can 3D Modeling and Visualization Be\nUsed with Geospatial Technology?\nThere are numerous 3D design and modeling programs on the market today,\nand many commercial geospatial programs offer some sort of capability of\nvisualizing or working with 3D data (and there\u2019s no way this chapter can get\ninto all of them). For instance, Esri\u2019s ArcGIS program contains the 3D Ana-\nlyst extension, which enables numerous 3D features related to terrain and ArcGlobe a 3D\nmodeling, but also two other interfaces for working with 3D data\u2014ArcGlobe visualization tool used\nto display data in a\nand ArcScene. As its name implies, ArcGlobe allows for 3D visualization on a\nglobal environment,\n\u201cglobal\u201d level. If you\u2019re examining 3D surfaces and objects in Los Angeles, you\nwhich is part of Esri\u2019s\ncan fly north across the globe to view other 3D spatial data in San Francisco, ArcGIS program.\nthen continue north to look at the landscape in Seattle. ArcGlobe functions\nArcScene the 3D\nsimilarly to a virtual globe program and works with all types of Esri file for-\nvisualization and\nmats such as 3D shapefiles. ArcScene contains a full set of 3D-visualization design component of\ntools for ArcGIS and allows for the same type of 3D modeling, except on a Esri\u2019s ArcGIS program.\n\u201clocal\u201d scale. If you were modeling one section of a 3D Los Angeles, that\u2019s the\nextent of the data you\u2019d be working with.\nGoogle Earth also supports 3D visualization. Back in Geospatial Lab Ap-\nplication 1.1: Introduction to Geospatial Concepts and Google Earth, you exam-\nined a 3D representation of places in South Dakota. Google Earth also con-\ntains other 3D objects, such as representations of 3D buildings and structures\nacross the world (such as buildings in New York City, Boston, and other cities 412\nChapter 14 See the World in 3D\nFIGURE 14.7 A version\nof Youngstown State\nUniversity\u2019s football\nstadium complex,\ndisplayed in Google\nSketchUp. (Source: \u00a9 2011\nGoogle SketchUp)\naround the world). It\u2019s easy to create 3D objects, structures, buildings, land-\nGoogle SketchUp a forms, and more through the use of the Google SketchUp (GSU) software\n3D modeling and program and then view them in Google Earth. The regular version of GSU is\ndesign software made freely available for download by Google, while a Pro version with ad-\nprogram distributed\nditional features is available for purchase (you\u2019ll be downloading and using\nonline by Google, it\nthe free version of GSU in this chapter\u2019s lab). GSU is a very versatile (and\ncontains linkages with\nGoogle Earth. intuitive to use) program that allows for a full range of 3D design capabilities.\nStructures created with GSU can be as realistic, detailed, or as basic as you\nwant them to be. See Figure 14.7 for an example of a football stadium in GSU.\nShapes can be molded, extruded, bent, and moved to build any type of\nstructure you can envision. Realistic textures can be applied to faces for a bet-\nter appearance, and digital photos can also be resized and applied to a face\nfor a more photo-real appearance. GSU also features a process called Photo\nMatch, in which a digital photo of a building can be used as the starting point\nand you can design the polygon dimensions of the building straight from the\nphoto. From there, the photo can be directly applied to the faces for a realistic-\nlooking 3D design.\ncomponents pre- GSU also enables the use of components (or pre-made 3D objects) for\ndesigned (or user- added realism and detail in designing 3D models. Components allow you to\ndesigned) objects in quickly place items such as trees, cars, furniture, landscaping, and similar fea-\nGoogle SketchUp.\ntures into 3D models (see Figure 14.8 for examples of components used in\nGSU). Like other 3D objects, components are constructed using shapes and\ntextures but are stored for re-use throughout a model. For instance, if you\u2019re\ndesigning a 3D version of a car dealership, you can quickly add a couple dozen\ndifferent car components and customize them rather than designing each 3D\ncar model individually. Components are also useful for elements you\u2019ll use\nmultiple times in a model. For example, if you\u2019re designing a 3D version of\na school building, you could design a window once, store it as a component, 413\nHow Can 3D Modeling and Visualization Be Used with Geospatial Technology?\nFIGURE 14.8 Several\nsample components\ndisplayed in Google\nSketchUp. (Source:\n\u00a9 2011 Google SketchUp)\nand then easily re-use the same model of a window multiple times around the\nschool.\nGeospatial Lab Application 14.1 will introduce you to several aspects of us-\ning GSU for 3D design, where you\u2019ll begin with a building\u2019s footprint, use tools\nto create a shape, design elements of the building (such as the entranceway),\nas well as design components to use. A file created with GSU (like the one\nyou\u2019ll make in the lab) uses the \u201c*.skp\u201d file extension. The SKP file consists of SKP file the file format\ninformation about the polygons that make up the structure, the textures ap- and extension used by\nplied, and the geometry of how these objects fit together. However, the SKP an object created in\nGoogle SketchUp.\nformat is native to GSU. You (or whoever you share your data with) has to\nbe running GSU to open and examine an SKP file. In addition, you can open\nonly one SKP file at a time, which causes some difficulties if you\u2019ve designed\nmultiple buildings of a city block (each in its own SKP file) and want to see\nhow they look next to one another.\nOne way around these issues is to convert the SKP files into another for-\nmat called a KMZ, which can be used in Google Earth. Each building becomes KMZ a file that is the\nits own KMZ file, and many KMZ files can be opened at once in Google Earth. compressed version of\na KML file.\nKMZ is a compressed version of another file format called KML, short for Key-\nhole Markup Language. KML and KMZ are file formats that can be interpreted KML Keyhole Markup\nusing Google Earth. By converting an SKP file into a KMZ file, that GSU model Language\u2014the file\nformat used for Google\ncan be viewed interactively in Google Earth. Figure 14.9 on page 414 shows an\nEarth data.\nexample of this\u2014Youngstown State University\u2019s stadium complex (containing\nboth sides of the stadium, stadium lights, the scoreboard, and the goal posts)\nwas converted from an SKP file into a KMZ file. The KMZ file can be opened\nin Google Earth, which overlays the 3D model of the stadium over the Google\nEarth imagery, allowing the user to rotate around the stadium, fly across the\nfield, zoom into a bleacher seat, or any of the other usual uses of Google Earth.\nKMZ has become a standard for creating objects to view in Google Earth\u2014other 414\nChapter 14 See the World in 3D\nFIGURE 14.9\nYoungstown State\nUniversity\u2019s Stambaugh\nStadium complex,\nconverted from an SKP file\nin Google SketchUp and\ndisplayed as a \u201c*.kmz\u201d file\n(KMZ file) in Google Earth.\n(Source: \u00a9 2009 Google.\nImage NOAA. Image State of\nOhio\/OSIP. \u00a9 2011 Google\nSketchUp)\nfile formats can be converted to KMZ (such as satellite imagery), which can\nthen be placed or draped across Google Earth (recall how you examined EOS\nimagery in Google Earth in Geospatial Lab Application 12.1).\nThe big question that should be raised at this point is: How does Google\nEarth know where to properly place that KMZ file? The Youngstown State\nUniversity stadium must have had some sort of spatial reference attached to it so\nthat it ends up placed over the imagery of the stadium\u2019s location and not at some\nrandom point on the globe. The best way of going about this is to know the ob-\nject\u2019s spatial location before any modeling begins\u2014otherwise, you\u2019ll be t rying\nto properly georeference the 3D model\u2019s location after the fact. GSU works hand\nin hand with Google Earth to obtain this data. GSU has the capability to take a\n\u201csnapshot\u201d of imagery (like the area you can see with Google Earth), then i mport\nthat snapshot (complete with a model of the terrain being shown as well as the\nspatial reference information for the area in the view) into GSU. Thus, in GSU,\nyou\u2019ll have a color overhead image with spatial reference information to use as a\nstarting point for your modeling efforts (see Figure 14.10 for e xamples). When\nyou\u2019ve completed your work in GSU, you can convert your SKP file (*.skp) into\na KMZ file (*.kmz), which will then contain the information to place the model\ninto its proper spatial location in Google Earth. This is how you\u2019ll begin 3D mod-\neling in Geospatial Lab Application 14.1\u2014first by o btaining a \u201csnapshot\u201d of the\noverhead view of the building, and then second by creating the building\u2019s foot-\n3D Warehouse\nprint from this starting point.\nGoogle\u2019s online With the popularity of GSU and Google Earth, it\u2019s become easy for peo-\nrepository of 3D ple to create spatially referenced data in GSU and then place it into Google\nobjects created using Earth. To help facilitate the use of the programs (and to foster a geospatial\nGoogle SketchUp.\n3D modeling community), Google has created an online 3D Warehouse as 415\nHow Can 3D Modeling and Visualization Be Used with Geospatial Technology?\nFIGURE 14.10 A section\nof Theodore Roosevelt\nNational Park in North\nDakota in both Google\nEarth and an image\nversion including terrain in\nGoogle SketchUp. (Sources:\nUSDA Farm Service Agency,\n\u00a9 2010 Google; USDA Farm\nService Agency, \u00a9 2011\nGoogle SketchUp)\na central repository of 3D models from users around the world. In the 3D\nWarehouse, you can find new components, objects, and a wide variety of 3D\nbuildings, ranging from state capital buildings, to national monuments, to\nsports arenas from around the world. GSU makes it easy to interact with the\n3D W arehouse\u2014the software allows you to upload your model to the ware-\nhouse and also to download items from the warehouse into GSU or directly\nto Google Earth. The 3D Warehouse is a great source of models from users of\nall levels of ability and contains guidelines on how to model for inclusion into\nGoogle Earth (see Hands-on Application 14.2: Digging into Google\u2019s 3D Ware-\nhouse on page 416 for more information about the 3D Warehouse). 416\nChapter 14 See the World in 3D\nHands-on Application 14.2\nDigging into Google\u2019s 3D Warehouse\nThe Google 3D Warehouse is full of all sorts of models that have been uploaded to the Warehouse.\n3D models from around the world. To start dig- Take a look at some of them, select their particu-\nging through the 3D Warehouse, open your Web lar Warehouse page, and then download the model\nbrowser and go to http:\/\/sketchup.google. (you\u2019ll see the option for it on its page) into Google\ncom\/3dwarehouse. Take a look at several of the cat- Earth to interact with it.\negories\u2014including the \u201cPopular Models,\u201d \u201cF eatured As long as you\u2019re looking around the Ware-\nCollections,\u201d and \u201c3D Building Collections.\u201d Each one house, also check out the option for Google\ncontains several subcategories (such as Cathedrals Building Maker, a utility that will help you design\nand Churches of the World or Sculptures and Monu- buildings based on photos and upload them into\nments of the World), which contain numerous 3D the Warehouse.\nWith these types of 3D modeling and design techniques, all manner of 3D\nfeatures can be created and utilized with other geospatial data. For instance,\nit\u2019s one thing to look at a flat map of a university campus, but it\u2019s another\nthing entirely to have it laid out in a realistic 3D environment that you can\ninteract with. Prospective students are likely to get a better sense of place by\nviewing a 3D representation of their dorms and where they are in relation to\nthe surrounding urban areas of the campus. The same idea holds true for de-\nsigning any area in interactive 3D (see Hands-on Application 14.3: 3D Hawaii\nfor an example of this).\nFuture planning efforts can be better implemented with the ability to\nsee the visual impact of numerous types of building plans in relation to their\nnearby areas (with respect to height, texture, and landscaping). With 3D\ngeospatial data, the locations of items and structures can be seen in relation\nto one another or to the surrounding terrain. For example, in Google Earth,\nHands-on Application 14.3\n3D Hawaii\nFor an interactive example of 3D with Google Earth, you can select an island (Oahu, Maui, Hawaii I sland,\nopen your Web browser and go to http:\/\/3dhawaii. K auai, Molokai, or Lanai) and view many of their\ncom. Note that you\u2019ll first need to load the Google building features and landscaping in interactive 3D.\nEarth plug-in (see also Hands-on Application 1.5: The main window will let you pan and zoom around\nThe Google Earth Plug-In) in order to work with this the islands like you would in Google Earth. Check\nWebsite. (You can download and install the plug- out the variety of features available in interactive\nin from http:\/\/www.google.com\/earth\/explore\/ 3D\u2014select the option for 3D tour to see animated\nproducts\/plugin.html.) From the available options, tours around various areas. 417\nHow Can 3D Modeling and Visualization Be Used with Geospatial Technology?\nFIGURE 14.11 The Eiffel\nTower (and surrounding\nareas in Paris) as shown\nin 3D using Google\nEarth. (Source: \u00a9 2010\nGoogle, Image \u00a9 Institut\ng\u00e9ographique national, Image\n\u00a9 Aerodata International\nSurveys, Gray Buildings\n\u00a9 2010 CyberCity)\nFIGURE 14.12 The U.S.\nCapitol (and surrounding\nareas in Washington, D.C.)\nas shown in 3D using\nGoogle Earth. (Source:\n\u00a9 2010 Google, Gray\nBuildings \u00a9 District of\nColumbia (DC GIS) & CyberCity,\nGray Buildings \u00a9 Sanborn)\nturning on the \u201c3D Buildings\u201d layer activates a wide range of 3D content tied\nto spatial locations that you can interact with (see Figures 14.11 and 14.12\nfor examples of rendered 3D features in a geospatial technology environment\nand Hands-on Application 14.4: 3D Buildings in Google Earth on page 418 for\nhow to interact with this data in Google Earth). There\u2019s also Google\u2019s Building\nMaker utility, which allows a user to examine photos of buildings and ma-\nnipulate shapes to match the photos in order to create a 3D version of the\nbuilding for Google Earth. 418\nChapter 14 See the World in 3D\nHands-on Application 14.4\n3D Buildings in Google Earth\nFigures 14.11 and 14.12 are both images of 3D view down so you\u2019re looking at Boston from a per-\nbuildings within Google Earth, but they\u2019re far from spective view and you\u2019ll see various buildings and\nthe only places that have been built and placed in structures start to appear in 3D. Fly around the city\nGoogle Earth. For this application, start up Google and see how Boston was put together in 3D.\nEarth and zoom to Boston, Massachusetts, looking When you\u2019re done, try looking at the 3D design\nat the area around Boston Common and the down- of some other urban areas and their 3D buildings,\ntown. From the available options in the Layers Box such as New York City, Washington, D.C., or Las\n(if necessary, see Chapter 1 for more information Vegas, Nevada. Check some other cities as well to\nabout Google Earth\u2019s layout), select the option for see if their buildings have been designed in 3D for\n3D Buildings, and place a checkmark in it. Tilt your Google Earth.\nChapter Wrapup\nThe goal of this chapter was to provide an introduction to some of the ba-\nsic concepts of 3D visualization and how they can be incorporated into geo-\nspatial technology. Geospatial Lab Application 14.1 will get your hands dirty\nwith working with 3D design and visualization using Google SketchUp and\nGoogle Earth. However, 3D modeling goes well beyond Google SketchUp\u2014\ndesigning, programming, and working with 3D modeling and visualization\nis a whole industry of its own\u2014video games, movies, television and Web\nanimation, and simulators regularly serve up extremely impressive and ever-\nmore-realistic 3D designs. There\u2019s no way to cover all the possible applica-\ntions, software, companies, and changes going on in this rapidly developing\nfield in just one short chapter.\n3D visualization is very cool and just one of the quickly growing aspects\nof geospatial technology, but there are many new developments (and some\nother cool data visualizations) yet to come as technology advances. Chap-\nter 15 delves into some of these new and upcoming frontiers in geospatial\ntechnology.\nImportant note: The references for this chapter are part of the online com-\npanion for this book and can be found at http:\/\/www.whfreeman.com\/\nshellito1e. 419\nKey Terms\nKey T erms\n3D modeling (p. 406) prisms (p. 410)\nz-value (p. 406) ArcGlobe (p. 411)\nextrusion (p. 406) ArcScene (p. 411)\nblock (p. 406) Google SketchUp (p. 412)\noffset (p. 407) components (p. 412)\nbase heights (p. 408) SKP file (p. 413)\nface (p. 409) KMZ (p. 413)\ntextures (p. 409) KML (p. 413)\nprism map (p. 410) 3D Warehouse (p. 414) 14.1\nGeospatial Lab Application\n3D Modeling and Visualization\nThis chapter\u2019s lab will introduce you to the concepts of modeling and design\nusing three-dimensional objects. Starting from an aerial image of a build-\ning with spatial reference, you will design the building in 3D using Google\nSketchUp, including its height, textures, and other effects, and place it into its\nproper location in Google Earth. While there are many methods that can be\nused for designing a building (including Google\u2019s Photo Match and Building\nMaker tools), this lab shows you how to use a variety of Google SketchUp tools\nthrough the context of creating a structure.\nImportant note: Even though this lab uses a fixed example, it can be per-\nsonalized by selecting another structure you\u2019re more familiar with\u2014such as\nyour home, workplace, or school\u2014and using the same techniques described\nin this lab to design that structure instead.\nLike Geospatial Lab Application 7.1 there are no questions to answer\u2014the\nfinal product you\u2019ll create at the end is a 3D representation of a building in\nGoogle SketchUp as well as a version of it placed into Google Earth. The an-\nswer sheet for this chapter is a checklist of items to help you keep track of your\nprogress in the lab.\nObjectives\nThe goals for this lab are:\na Obtaining imagery to use in a design environment.\na Familiarizing yourself with the use of the Google SketchUp software\npackage, including its operating environment and several of its tools.\na Applying textures to objects.\na Utilizing components in 3D design.\na Transforming your 3D model into the KMZ file format to be placed into\nGoogle Earth.\nObtaining Software\na The current version of Google SketchUp (8) is available for free download\nat http:\/\/sketchup.google.com\/intl\/en\/product\/gsu.html.\na The current version of Google Earth (6.0) is available for free download\nat http:\/\/earth.google.com.\nImportant note: Software and online resources sometimes change fast.\nThis lab was designed with the most recently available version of the software\nat the time of writing. However, if the software or Websites have significantly\n420 421\n3D Modeling and Visualization\nchanged between then and now, an updated version of this lab (using the new-\nest versions) is available online at http:\/\/www.whfreeman.com\/shellito1e.\nLab Data\nThere is no data to be copied for this lab. You will, however, use the \u201cBird\u2019s\neye\u201d imagery from Bing Maps (see Hands-on Application 9.4: Oblique Imagery\non Bing Maps on page 280), so start that up in a Web browser.\nLocalizing This Lab\nIn this lab you will design a 3D model of a building on Youngstown State Uni-\nversity\u2019s (YSU\u2019s) campus, and then view it in Google Earth. In Section 14.2,\nyou will take a \u201csnapshot\u201d of the building to start with. Rather than examin-\ning a campus building at YSU, you can start at this point with a base image of\nany local building\u2014such as the school building you\u2019re in, a nearby landmark\nor government building, or your house. You can use the same steps in this lab\nand the GSU tools to design any building or structure you so choose in place\nof the one used in this lab\u2014the same lab techniques of using Google SketchUp\nwill still apply, you\u2019ll just have to adapt them to your own 3D design needs.\n14.1 Starting Google SketchUp\n1. Start Google SketchUp (GSU). GSU will open in its initial mode. Select\nthe option for Choose Templates, and then from the available options\nselect Google Earth Modeling\u2014Feet and Inches.\n2. There are several different templates available, depending on what type\nof 3D design you want to do.\n3. Next, select the Start using SketchUp button.\n4. GSU will open with a blank working environment, with three intersecting\nlines at the center. These represent the axes you will use for 3D design\u2014\nred and green are the horizontal (x and y) axes, while blue is the vertical\n(z) axis.\n5. You\u2019ll see the GSU toolbar at the top of the screen. You\u2019ll use many of\nthese tools during the lab\u2014the tools that you\u2019ll use often in this lab\n(from left to right on the toolbar) function as follows:\nSelect Line Eraser Tape Paint Push \/ Move \/ Orbit Pan Zoom\nMeasure Bucket Pull Copy 422\nChapter 14 See the World in 3D\na The black cursor is the Select tool that lets you choose or select\nobjects.\na The pencil is the Line tool that lets you draw lines or construction lines.\na The pink eraser is the Eraser tool that allows you to remove lines and\nobjects.\na The extended yellow tape measure is the Tape Measure tool that allows\nyou to measure the length or width of objects.\na The bucket spilling paint is the Paint Bucket tool that allows you to\nchange the color or texture of objects.\na The box with the red arrow coming up from it is the Push\/Pull tool that\nallows you to alter the shape of polygons.\na The four red arrows represent the Move\/Copy tool that can be used to\ndrag objects around the screen and also to create copies of objects.\na The swirling blue arrows represent the Orbit tool that allows you to\nchange your view and maneuver around the scene.\na The hand is the Pan tool that allows you to move around the view.\na The magnifying glass is the Zoom tool that allows you to zoom in and\nout of the scene.\na There are three other specialized tools (Add Location, Toggle Terrain,\nand Preview Model in Google Earth) that you\u2019ll make use of, but the\nlab will point these out to you when you need to use them.\n14.2 Obtaining Google Imagery\nGSU directly interfaces with Google Earth (GE), in that you can use Google\nimagery as a starting point for your design work in GSU and transfer your 3D\nstructures into GE.\n1. To begin, start Google Earth.\nImportant note: This lab will have you design a simplified representation\nof Lincoln Building (formerly known as Williamson Hall, the original College\nof Business building) on YSU\u2019s campus. All of the measurements of things\nlike heights, window lengths, window measurements, and so on are either\njust approximations or values generated for this lab and used for this simpli-\nfied model. To use real values for Lincoln (or your own buildings) you would\nhave to make measurements of the structure or consult blueprints for actual\nheights and sizes.\n2. Lincoln Building is located on YSU\u2019s campus at the following latitude\/\nlongitude decimal degrees coordinates:\na Latitude: 41.103978\na Longitude: (cid:2)80.647777 423\n3D Modeling and Visualization\n3. Zoom GE in to this area and you\u2019ll see the top of the building (see the\nfollowing graphic). Fill the GE view with the entirety of the building and\nreturn to GSU. This should give you a good look at the overhead view of\nthe Lincoln Building for reference in the next step. You\u2019ll use GE later, so\nminimize it for now.\n(Source: \u00a9 2011 Google)\n4. Back in GSU, select the Add Location icon from the toolbar:\n5. Add Location allows you to take a \u201csnapshot\u201d of Google imagery and\nimport it (with information on the coordinates of the view as well as\nterrain information) into GSU for you to use as a digitizing source.\n6. In the Add Location box that appears, type the coordinates of Lincoln\nBuilding and click Search. Use the zoom and pan tools (which are the\nsame as the Google Earth tools) to expand the view so that you can see\nall of Lincoln Building in the window (like you just did in Google Earth).\n7. When you\u2019ve got the view set up how you want it, click the Select\nRegion button. 424\nChapter 14 See the World in 3D\n(Source: \u00a9 2011 Google SketchUp)\n8. A new image will appear\u2014a bounding box with blue markers at the\ncorners. Drag the corners of the box so that it stretches all across Lincoln\nBuilding (like in the following graphic). This area will be the \u201csnapshot\u201d\nthat GSU takes of the imagery and that you\u2019ll use in the modeling\nprocess, so be sure to capture the whole area.\n(Source: \u00a9 2011 Google SketchUp) 425\n3D Modeling and Visualization\n9. When everything is ready, click Grab.\n10. After the importing process is done, the image (in color) should appear\nin GSU centered on the axes.\n(Source: \u00a9 2011 Google SketchUp)\n11. In GSU, you can turn the terrain on and off within the snapshot, using\nthe Toggle Terrain icon on the toolbar. For now, toggle the terrain off\n(so the image is perfectly flat).\n14.3 Digitizing the Building Footprint\n1. You can digitize the dimensions of Lincoln Building\u2019s footprint directly\nfrom the imported aerial view. The easiest way to do this is to position\nthe view so it\u2019s looking directly down on the image. From the Camera\npull-down menu, select Standard Views, then select Top.\n2. The view will switch to looking at the image directly from above. To\nzoom in or out so you can see the entire building, select the Zoom icon\nfrom the toolbar (or if your mouse has a scroll wheel, you can use that\nto zoom as well). To zoom using the icon, select it, then roll the mouse\nforward or back to zoom in and out.\n3. When you can see the entire base of the building, it\u2019s time to start\ndigitizing. Click on the pencil icon (the Line tool) on the toolbar to\nstart drawing. 426\nChapter 14 See the World in 3D\n4. Your cursor will now switch to a pencil. Start at the lower left-hand\ncorner of the building and click the pencil onto the building\u2019s corner. A\npurple point will appear. Now, drag the pencil over to the lower right-\nhand corner and hover it there for a second. The words \u201con face in\ngroup\u201d will appear. This is GSU\u2019s way of telling you just what surface\nyou\u2019re drawing on (in this case, the face is the surface of the image).\n5. Click the pencil on the lower right-hand corner, then at the upper\nright-hand corner, then at the upper left. The guide may change to read\n\u201cPerpendicular to the edge,\u201d indicating you\u2019ve drawing perpendicular\nlines. Finally, drag the pencil back to where you started from. When you\nreach the beginning point, the cursor should turn green and the word\n\u201cEndpoint\u201d will appear. This indicates you\u2019ve closed the polygon of the\nbuilding\u2019s footprint in on itself. Click the pencil at the endpoint and the\npolygon will appear, covering the entire building.\nImportant note: If you have only lines and not a complete polygon, return\nto the lower corner and remake the footprint polygon.\n(Source: \u00a9 2011 Google SketchUp)\n14.4 Extruding the Building Footprint\n1. With the building\u2019s footprint successfully digitized, the next step is to\ngive it some height.\n2. From the Camera pull-down menu, select Standard Views and then\nselect Iso. The view will switch to a perspective view.\n3. From the toolbar, select the Push\/Pull tool. 427\n3D Modeling and Visualization\n4. Your cursor will change again. Hover it over the footprint polygon and\nyou\u2019ll see it slightly change texture. Click on the footprint and hold\ndown the mouse button, then use the mouse to extrude the polygon by\npushing forward.\n5. In the lower-right-hand corner of GSU, you\u2019ll see the real-world height\nyou\u2019re extruding the building to. Lincoln Building is approximately\n72 feet high. During the extrusion operation, you can type 72\u2032 and hit\nthe Enter key on the keyboard while holding down the mouse button,\nand the polygon being push\/pulled will automatically extrude to that\nheight.\n(Source: \u00a9 2011 Google SketchUp)\n6. You can double-check the measurement by selecting the Measure Tape\nicon from the toolbar.\n7. Click the measuring tape at the base of the Lincoln Building block and\ndrag it to the top. The measurement (in feet and inches) will appear. You\ncan use the push\/pull tool to readjust the building height if necessary.\n14.5 Maneuvering in GSU\n1. An easy way to maneuver and examine the 3D objects being created is to\nuse the Orbit and Pan tools available on the toolbar.\n2. Use the Pan tool (the hand) to drag the current view in any direction.\n3. Use the Orbit tool (the blue curved arrows) to rotate the view around an\nobject. 428\nChapter 14 See the World in 3D\n4. Practice with the two tools for a couple minutes to get the feel for\nmaneuvering in the GSU environment.\n5. Maneuver around so that you can view the front of the building (the\npart that opens out onto the street).\n14.6 Toggle Terrain\n1. For this simplified example, we\u2019ve been assuming the terrain\nunder Lincoln Building is completely flat. When the GE image was\nimported, it also included data about the terrain built into the image.\nYou would probably want to take terrain into account early in the\nprocess if you were modeling a structure or object built into a\nhill or on a large slope so that measurements and features properly\nmatch up.\n2. To examine the \u201creal\u201d terrain about Lincoln Building, toggle the terrain\non (again using the Toggle Terrain icon on the toolbar).\n3. Orbit around the building and you\u2019ll see that some of the back corners\nappear to be slightly \u201cfloating\u201d over the terrain. To rectify this, orbit\nunderneath the building and the image and use the Push\/Pull tool to\nslightly \u201cpull\u201d the building down below the image. This will ensure that\nthe building extends completely into the ground and that none of it will\nfloat over the terrain.\n14.7 Adding Features to the Building\n1. Use Bing Maps\u2019 \u201cBird\u2019s eye\u201d feature to get a closer look at Lincoln\nBuilding. Viewing from the north, you may be able to see a recessed\nentranceway leading into the front of Lincoln Building.\n2. Set up the entranceway approximately 27 feet (27\u2032) across and beginning\napproximately 34 feet (34\u2032) from each side of the front of the building.\nThe opening itself is about 9 feet (9\u2032) tall. This is the size of a block you\nwant to draw on the front of the building.\n3. In order to find these dimensions, you can use the Tape Measure to\ncreate some guide lines (or \u201cconstruction lines\u201d) on the face of the\nbuilding to use.\n4. Select the Tape Measure icon and start from the lower left corner\n(as you face the building). Measure up approximately 9 feet, and then\ndouble-click the mouse.\nImportant note: A more precise way to use the Tape Measure is to type\n9\u2032 (for 9 feet) while dragging the tape in the direction you want to measure.\nA small mark will appear at the 9\u2032 length, and you can use that as a starting\npoint for more measurements. 429\n3D Modeling and Visualization\n5. Next, drag the mouse over the other side of the building and double-\nclick. A dashed line will appear on the face of the building at 9 feet in\nheight. This is a construction line\u2014it\u2019s invisible in the modeling process\nbut used for drawing measurements.\n6. Starting along one side of the building, measure across 34 feet, double-\nclick the mouse, and then measure down to the base of the building. Do\nthe same on the other side. You should have construction lines set up\nthat block out the entranceway on the face of the building.\n(Source: \u00a9 2011 Google SketchUp)\n7. If you find you\u2019re creating too many construction lines, you can always\nselect them with the cursor icon and delete them by using the Eraser\nicon on the toolbar (in fact, anything you create can be removed by\nusing the eraser).\n8. Now, use the pencil to draw the entranceway on the face of the building,\ntracing over the construction lines.\n9. Finally, select the push\/pull tool and use it to recess the entranceway by\napproximately 6 feet. Use the mouse to push the block inward, watching\nthe distance bar in the lower right corner.\n10. By examining the building with Bing Maps\u2019 Bird\u2019s eye imagery, you\u2019ll\nsee that Lincoln Building has several windows stretched around the top\nof the building\u2014two sets of four windows on the front and back, and\n18 windows on either side. Drawing all 52 of these identical windows\nindividually would be a huge chore, so to reduce the time and effort, 430\nChapter 14 See the World in 3D\nyou\u2019ll be designing the window only once and reusing it throughout the\nmodel.\n11. One way of doing this is to set up multiple construction lines across the\nfront of the building. Orbit around to see the top front of the building.\nSet up construction lines as follows (these are simplifications of the real\nmeasurements for purposes of this lab):\na. The first window begins 6 feet from the edge of the building.\nb. The first window begins 3 feet from the top of the building.\nc. The window is 3 feet wide and 21 feet tall.\nd. There are five divisions on the window, and their measurements are\n(from the top):\na 2 feet\na 5 feet\na 7 feet\na 5 feet\na 2 feet\n(Source: \u00a9 2011 Google SketchUp)\n12. Next, draw the outline of the window and the five divisions using the\npencil tool in conjunction with the construction lines.\n13. With the outlines drawn, it\u2019s time to make it actually look like a window\nby adding some textures.\n14. Select the paint bucket icon from the toolbar. 431\n3D Modeling and Visualization\n15. A new window of Materials will open. These are the various textures\nthat can be \u201cpainted\u201d on a face. For the purposes of this simplified\nmodel, select Asphalt and Concrete from the Materials window pull-\ndown menu, then select the option for Concrete Block 8 (cid:2) 8 Gray.\nBack in the model of the window, click the paint bucket on the top and\nbottom sections of the window. They should change appearance to a\n\u201cconcrete block\u201d finish.\n16. Next, select Translucent from the Materials window pull-down menu\nand choose the option for Translucent Glass Blue. Click on the second\nand fourth blocks of the window with this new material.\n17. Lastly, choose Colors from the Materials window pull-down menu,\nselect Color 007 (it\u2019s a black color), and then click on the final (third)\nsection of the window. All five sections of the window should now be\ncolored in. You can now close the Materials box.\n(Source: \u00a9 2011 Google SketchUp)\n14.8 Working with Components\nAs noted before, you\u2019ve made one window, but the model requires a total of\n52 of them across the four sides of the building. If you\u2019re going to re-use some-\nthing many times, it\u2019s easier to create a Component out of the object.\n1. Select the cursor icon and drag a box over all sections of the window.\nThey\u2019ll become highlighted in blue, signifying they\u2019re now selected. Be\nvery careful not to select any of the building faces themselves\u2014you\nwant only the sections of the window. (To de-select items, hold down\nthe Shift key and click on the items with the mouse.)\n2. Right-click on the selected window and choose Make Component. 432\nChapter 14 See the World in 3D\n(Source: \u00a9 2011 Google SketchUp)\n3. In the Create Component dialog box, type in the name \u201cLincwind\u201d for\nthe new component.\n4. Select Any for \u201cGlue to\u201d from the drop-down box.\n5. Place a checkmark in the \u201cCut opening\u201d option.\n6. Place a checkmark in the \u201cReplace selection with component\u201d box.\n7. Click Create when you\u2019re done.\n8. The selected section of the window will now be a GSU Component and it\nwill be stored for later use in the model.\n14.9 Making Multiple Copies\nNow that the window is a component, it\u2019ll be easier to work with the same\nitem. In this section of the building, there are four windows, each approxi-\nmately 2.5 feet apart and 3 feet wide. What you\u2019re going to do is make mul-\ntiple copies of the component window, then have GSU automatically space\nthem out for you.\n1. Create a construction line 13.5 feet away (the space of two window\nlengths and the distances between them) from the window\u2019s edge,\nstretching from the top to the bottom of the building. This is where the\ncopy of the window will go.\n2. Next, use the cursor to select the window component. It will be outlined\nin blue when it\u2019s selected.\n3. Next, choose the Move\/Copy tool from the toolbar.\n4. The cursor will change to the new icon.\nImportant note: Hold down the Ctrl key on the keyboard\u2014this tells GSU\nyou want to copy the window instead of moving it somewhere. If you don\u2019t 433\n3D Modeling and Visualization\nhold down the Ctrl key, GSU will try to move the window somewhere else\ninstead of making a copy.\n5. Use the mouse to select and drag the copy of the window over to the\nright of the new 13.5-foot distant construction line. Make sure that the\ninference dialog box that appears indicates you\u2019re copying it \u201cOn the\nFace\u201d (that is, the copy will be moved to the face of the building instead\nof into space somewhere).\n6. Once the copy of the window is positioned properly, type \/3 on the\nkeyboard. This will tell GSU to make a total of three copies, evenly\nspaced between the original and the location of the new copy.\nYou will now have four windows on the front left-hand side of the\nbuilding.\n(Source: \u00a9 2011 Google SketchUp)\n7. Now it\u2019s time to put windows on the other side of the front. Create a\nconstruction line 6 feet from the right-hand side of the building.\n8. To add the component of the window, select Components from the\nWindow pull-down menu.\n9. In the Components dialog box, select the \u201chouse\u201d icon (under the Select\ntab) to utilize only those components that exist in the current model\n(the only component should be your \u201cLincwind\u201d window).\n10. Click on the component itself in the window and drag it onto the face\nof the building in the model, positioning it properly in conjunction with\nthe construction line you just drew.\n11. Now, repeat the previous steps (draw a construction line 13.5 feet away\nand copy the windows, creating a total of three copies properly spaced\nout from one another). 434\nChapter 14 See the World in 3D\n12. You should now have two sets of four windows each on the front of the\nbuilding.\n13. Rotate to the back of the building. You\u2019ll want to create the same layout\n(two sets of four windows each) using the same measurements on the\nback face of the building.\n14. Next, rotate to one side of the building. Each side of the building has\n18 windows on it, so you\u2019ll use the same technique\u2014position the\ncomponent, create a copy at the farthest point away, then have GSU\nautomatically fill in the evenly-spaced copies.\n15. Starting at the left-hand side of the building, create a vertical construction\nline 6 feet from the edge and a horizontal construction line 3 feet from the\ntop. Create another vertical construction line 93.5 feet from the building\u2019s\nedge\u2014this is where the copy will be placed.\n16. Place the window component 6 feet from the building\u2019s left-hand edge\nand 3 feet from the top of the building.\n17. Drag a copy to the right of the 93.5-foot construction line.\n18. Then type \/17 on the keyboard.\n19. A total of 18 windows should be created on the side of the building.\n(Source: \u00a9 2011 Google SketchUp)\n20. Repeat this process on the other side of the building to create an\nadditional 18 windows.\n21. At this point, all 52 windows should be created on all four sides of the\nbuilding. 435\n3D Modeling and Visualization\n14.10 Working with Textures\nNow, it\u2019s time to give the building a textured appearance to make it look like\nthe orange brick in the photos instead of the white block.\n1. Select the paint bucket icon again, and from the Materials pull-down\nmenu, select Brick and Cladding. Next, choose the option for Brick\nRough Tan. Click on each of the sides of the building and each will be\n\u201cpainted\u201d with the new texture.\n2. For the roof and the entranceway, select Colors from the Materials\npull-down menu and then select Color 006. \u201cPaint\u201d the roof and the\nentranceway faces with this color.\n14.11 Working with 3D Text\nThe words \u201cLINCOLN BUILDING\u201d are placed on the front of the building, near\nthe lower right-hand side. 3D text can be easily added to a model to create\nnames, signs, or other uses of block lettering.\n1. From the Tools pull-down menu, select 3D Text.\n2. In the text box, type LINCOLN BUILDING (all caps and also hit the enter\nkey between the two words so they appear on two different lines).\n3. Choose Arial for the Font.\n4. Type 8\u2033 (for 8 inches) for the Height of the letters.\n5. Type 3\u2033 (for 3 inches) for the Extruded letters.\n6. Make sure the Filled and Extruded checkboxes are marked.\n7. When ready, click Place.\n8. The 3D Text will appear as a selected object\u2014use the cursor to place it\non the face of the building near the right-hand side of the entrance.\n14.12 Adding Other Features\nThe purpose of this lab is to get you familiar with using GSU\u2019s features and creating\nan approximation of a building, not modeling it down to its last detail (although\nGSU is certainly powerful enough to do just that). However, there are other fea-\ntures on the building you may want to add to continue utilizing your GSU skills.\n1. Just from looking at the oblique imagery, you can see some other 3D\nfeatures the building has:\na The black section of the building stretching up from the entranceway\nthat also contains windows\na The black pillars in the front of the building that stretch up to the roof.\na The units on the roof of the building itself\na The side doorway entrance on the west side of the building 436\nChapter 14 See the World in 3D\n2. Using the techniques described in this lab, you can create\nrepresentations of these other features using a mixture of extruding\nitems (like drawing a polygon on the roof and extruding it) or textures\n(drawing in the blocks for the front windows above the entrance and\nfilling them in).\n3. When you have the model completed how you want it, there is only one\nmore step to do.\n14.13 Placing the Model into Google Earth\n1. The final step is to place the finished product into GE as a 3D model. To do\nthis, select the Preview Model in Google Earth icon from the toolbar:\n2. GSU will export the model to a \u201c*.kmz\u201d file and place it into its proper\nlocation in GE. If GE is not already open, this option will automatically\nstart it, then zoom to its proper spot.\n3. In GE, use the zoom and rotate tools to examine your 3D model.\n(Source: \u00a9 2011 Google SketchUp)\n14.14 Saving Your Work (and Working On it Later)\n1. You can save your work at any time in GSU by selecting Save from the\nFile pull-down menu.\n2. You can reopen your GSU model to work on it later by choosing Open\nfrom the File pull-down menu. 437\n3D Modeling and Visualization\nClosing Time\nThis lab was involved, but served to show off many of the 3D modeling fea-\ntures available within GSU and how to interface GSU together with GE. Exit\nGSU by selecting Exit from the File pull-down menu. Make sure you saved the\nfinal version of your GSU model and the \u201c*.kmz\u201d version of it as well.\nFinal Google SketchUp Modeling Checklist\n____ Obtain overhead view of Lincoln Building and place it into GSU\n____ Digitize building footprint\n____ Extrude building to proper height\n____ Set building properly on the terrain\n____ Create entranceway opening on front of building\n____ Create components for windows\n____ Apply windows to all sides of building\n____ Apply appropriate colors and textures to all faces\n____ Create 3D text on front of building\n____ Add other features (front columns, windows, roof unit)\n____ Place 3D model into appropriate place in GE This page was intentionally left blank 615\nWhat\u2019s Next for Geospatial\nTechnology?\nNew Frontiers in Geospatial Technology, Geospatial\nTechnologies in K\u201312 Education, College and University\nEducational Programs, How to Easily Get Data,\na Geospatial World Online, and Other Developments\nIt\u2019s an understatement to say that geospatial technology is a rapidly chang-\ning and advancing field. New Websites, imagery, tools, gadgets, and develop-\nments are coming at an incredible rate. GPS, digital maps, and satellite im-\nages are part of everyday life. When your phone can provide you with the\ncoordinates of your location, a map of what\u2019s nearby, a high-resolution re-\nmotely sensed image of your surroundings, directions to where you want to\ngo, and then display it all with a smartphone app, you know that geospatial\ntechnologies are part of your life and not going away. Or, as the Geospatial\nRevolution Project from Penn State University put it: \u201cThe location of any-\nthing is becoming everything.\u201d As noted back in Chapter 1, geospatial tech-\nnology\u2019s being used in a wide variety of fields\u2014as long as there\u2019s some sort of\nlocation involved, geospatial technologies are somehow involved. Even with\nall the material covered in the previous 14 chapters of this book, there are a\nlot of new frontiers to explore in this ever-changing field.\nGeospatial technology is increasingly used in our daily lives, whether in\nour work, travel, or recreation. In fact, observance days have been established\nto prominently showcase these types of technologies. The National Geo-\ngraphic Society kicks off Geography Awareness Week each November, and\nsince 1998, GIS Day has been set up as the Wednesday of the week. GIS Day GIS Day the\nis sponsored by a number of prominent agencies and groups (including the Wednesday of\nGeography Awareness\nNational Geographic Society, Esri, the USGS, and the Library of Congress,\nWeek (observed in\namong others). It is aimed at promoting GIS for students, educators, and pro-\nNovember) dedicated\nfessionals in the field. Each year on GIS Day, there are several GIS-related\nto GIS.\nevents at schools, universities, and workplaces across the world. Similarly,\n443399 440\nChapter 15 What\u2019s Next for Geospatial Technology?\n2006 saw the launch of Earth Observation Day to showcase remote sensing\nEarth Observation\nresearch and education across the globe.\nDay a day dedicated\nto remote sensing One of the neatest recent advances is the opportunity for everyday people\nresearch and education. (without geospatial training) to get involved in this field. With geospatial tools\nbecoming increasingly utilized by everyone, people now have the chance to\ncreate their own geospatial content. Whether it\u2019s creating new maps, updating\nexisting maps, tying digital pictures to locations in Google Earth, or adding to\ngeocaching databases, people are using geospatial technologies to add to or en-\nhance geospatial data and knowledge. Dr. Michael Goodchild coined the term\nVGI Volunteered Volunteered Geographic Information (VGI) to give a name to these actions.\nGeographic Information, A wiki is a Web utility that allows users to freely create, manage, update,\na term used to describe and change the content of a Website. Possibly the best known one is Wikipe-\nuser-generated\ndia, an online encyclopedia that anyone can edit. By extending this wiki con-\ngeospatial content and\ncept to geospatial data, individuals can contribute their own maps, images,\ndata.\nand updates to geospatial content available on the Internet, thereby generat-\nwiki a database\ning VGI. Websites such as Wikimapia or OpenStreetMap (see Figure 15.1 and\navailable so that\nHands-on Application 15.1: User-Generated Geospatial Content Online for more\nanyone can edit it.\ninformation) are dedicated to the VGI principle of users generating or contrib-\nuting to the geospatial content they contain.\nFIGURE 15.1 Detroit,\nMichigan, and Windsor,\nCanada, as shown in\nOpenStreetMap. (Source:\nhttp:\/\/www.openstreetmap.\norg\/)\nHands-on Application 15.1\nUser-Generated Geospatial Content Online\nVolunteered Geographic Information (VGI) repre- What types of data and information are available\nsents user-generated geospatial content online. via them, and how can you add your own geospatial\nUse your Web browser to investigate both of these content? Search for your local area on both of them\nsites as follows: to see what\u2019s available and what you could poten-\ntially add or update.\n1. Wikimapia: http:\/\/wikimapia.org\n2. OpenStreetMap: http:\/\/www.openstreetmap.org 441\nWho Is Involved with Geospatial Technology?\nThis chapter explores these types of new frontiers and advances in the\ngeospatial world and who\u2019s involved with these types of activities to sponsor\nand support them. How can you get involved with creating your own maps on-\nline or obtaining the newest (and coolest) geospatial devices, tools, and data\nto use? What kind of educational opportunities are there in higher education,\nand how is geospatial technology being used at the K\u201312 education levels?\nWe\u2019ll look into all of these topics and some examples of what else is out there\nin geospatial technology (and, again, there\u2019s no way we\u2019re going to be able to\ncover everything or everybody out there\u2014the following sections provide a\nlook at several examples).\nWho Is Involved with Geospatial Technology?\nThere are numerous \u201cpower players\u201d in the geospatial world\u2014we\u2019ve already\ndiscussed the role of companies such as Microsoft, Google, Esri, GeoEye, and\nDigitalGlobe, as well as government agencies including NASA, NOAA, and\nOGC the Open\nthe USGS in providing geospatial data and infrastructure. Other federal gov-\nGeospatial Consortium,\nernment agencies including the EPA (Environmental Protection Agency) and\na group involved\nNGA (the National Geospatial-Intelligence Agency) play important roles in with developing new\napplications of geospatial technology. Many state and local agencies utilize standards of geospatial\nGIS, GPS, or remotely sensed data through city planning, county auditors\u2019 of- data interoperability.\nfices, or county engineers\u2019 offices. Another notable organization in the geo- AAG the Association of\nspatial field is the Open Geospatial Consortium (OGC), a group involved with American Geographers,\ndeveloping new standards of geospatial data interoperability. the national\norganization for the\nThere are also several prominent professional organizations in the geo-\nfield of geography.\nspatial field. For example, the Association of American Geographers (AAG)\nis the national organization for geographers of all fields, whether physical, ASPRS the\nAmerican Society\nregional, cultural, or geospatial. Boasting several thousand members, AAG is\nfor Photogrammetry\na key organization for geographers, hosting a large annual national meet-\nand Remote Sensing,\ning, multiple regional branches, and publishing key geographic journals. An- a professional\nother major professional group is the American Society for Photogrammetry organization in the\nand Remote Sensing (ASPRS), a strong national organization dedicated to geospatial and remote\nsensing field.\nr emote sensing and geospatial research. ASPRS publishes a prominent aca-\ndemic journal in the field and hosts national and regional meetings well at- NCGE the National\ntended by geospatial professionals. Also, the National Council for Geographic Council for Geographic\nEducation, a\nEducation (NCGE) is a nationally recognized organization of educators dedi-\nprofessional\ncated to promoting geographic education through conferences, research, and\norganization dedicated\njournal publications.\nto geographic\nThere are many ongoing research and educational initiatives dedicated education.\nto furthering geospatial technology and its applications, as well as improving\nOhioView a research\nand fostering access to geospatial data and tools. For example, OhioView is\nand education\nan initiative created in 1996 dedicated to remote sensing research and edu- consortium consisting\ncational efforts in Ohio. Part of OhioView\u2019s efforts led to the free distribu- of the state universities\ntion of Landsat imagery (where previously a single Landsat scene used to cost in Ohio along with\nrelated partners.\n$4000). Today, OhioLINK, the online library system for the state, can be used 442\nChapter 15 What\u2019s Next for Geospatial Technology?\nto download Landsat scenes of Ohio via NASA Glenn in Cleveland (and also\nvia OhioView\u2019s Website). OhioView serves as a consortium for remote sensing\nand geospatial research and is composed of members from each of the state\nuniversities in Ohio, along with other partners such as the Ohio Aerospace\nInstitute. With all of these statewide affiliations, OhioView serves as a source\nof many initiatives within the state, including educational efforts.\nThis type of statewide consortium may have originated with OhioView\nAmericaView but it has expanded to a national program called AmericaView. Adminis-\na United States tered in part by the USGS (and set up as a 501(c)(3)), AmericaView aims to\nnational organization spread remote sensing research and educational efforts across all the states\ndedicated to remote\nof the United States (see Figure 15.2). Many states have developed a Stat-\nsensing research and\neView program similar to OhioView (such as VirginiaView, WisconsinView,\neducation.\nTexasView, and so on), with the eventual goal of having a complete set of 50\nStateView the term\nStateView programs. Each StateView is composed of state universities (and\nused to describe\nrelated partners from that state) and offers various geospatial services or data\neach of the programs\naffiliated with for that state. For example, WisconsinView provides fresh MODIS imagery\nAmericaView in each each day of Wisconsin and the other AmericaView member states, along with\nstate. other freely available data products (such as high-resolution aerial photog-\nraphy and Landsat imagery). Similarly, TexasView provides access to free\nremotely sensed images along with links to GIS data of Texas. See Hands-on\nA pplication 15.2: AmericaView and the StateView Programs to further investi-\ngate the resources available through the various StateView programs.\nFIGURE 15.2 The 2010\nmembership composition\nof AmericaView. (Source:\nCourtesy AmericaView) 443\nHow Is Geospatial Technology Used in K\u201312 Educational Efforts?\nHands-on Application 15.2\nAmericaView and the StateView Programs\nTo see if your state is part of the AmericaView contact info and check out the specific S tateView\u2019s\nconsortium (and to also see the current version of Website (that is, MichiganView\u2019s Website or Virgin-\nthe program\u2019s membership), open your Web brow- iaView\u2019s Website) to see who that state\u2019s members\nser to the interactive map located at http:\/\/www. are and what type of data and services they pro-\namericav iew.org\/membership-map. What is the vide. If your state\u2019s not (yet) a full member, examine\nstatus of your home state as a StateView program? some of your nearby states and see what they offer\nIf it is a full member, click on the state to bring up the as a StateView program.\nHow Is Geospatial Technology Used in K\u201312\nEducational Efforts?\nGroups such as NCGE, OhioView, and AmericaView are dedicated to promot-\ning education in this ever-growing field of geospatial technology, but especially\nin K\u201312 classrooms. With geospatial technologies becoming increasingly\nutilized in today\u2019s society, introducing concepts early in the grammar-school\nclassroom and using them for illustrating topics such as geography, spatial\nthinking, or earth science is becoming increasingly common. These efforts are\naided by the prevalence of free software programs (such as those used in this\nbook) and plenty of freely available geospatial data. For instance, remotely\nsensed images from the EOS can help in explaining environmental or climatic\nconditions such as changing global temperatures, while Landsat imagery can\nbe used to evaluate the health of vegetation or loss of forested lands (and the\nsubsequent effects). NASA and Google have released teacher kits and supple-\nmental lesson plans to accompany their products, and many other examples\nof geospatial technology being integrated into K\u201312 curricula can be found on\nthe Internet (see Hands-on Application 15.3: Educational Resources and Lesson\nPlans on page 444 for several examples).\nBeyond using Google Earth (and other software) for interactive class-\nroom investigations, many other programs exist that are aimed at promoting\ngeospatial technology at the K\u201312 level. The National Council for Geographic\nEducation (NCGE) is at the forefront of geographic education, supporting\nteachers and furthering geographic concepts at the K\u201312 level. Another or-\nganization dedicated to promoting geographic education is the Geographic Geographic Alliance\nAlliance Network, a national organization of geography educators at the Network a National\nK\u201312 and higher-education levels. Geographic Alliances are set up in each Geographic-sponsored\norganization set up in\nstate as a way of hosting workshops, professional development, meetings,\neach state to promote\nevents, involvement with Geography Bees, lesson plans, and more at the\ngeographic education.\nstatewide level. 444\nChapter 15 What\u2019s Next for Geospatial Technology?\nHands-on Application 15.3\nEducational Resources and Lesson Plans\nThe widespread nature of geospatial technologies 4. Juicy Geography - Lesson plans and ideas\nand the availability and access to geospatial soft- related to geospatial technology: http:\/\/www.\nware have allowed educators to integrate GIS, GPS, juicygeography.co.uk\nand remote sensing into lesson plans in multiple 5. NASA\u2019s Teacher\u2019s Kit - For use with Landsat\nways. Examine some of the following examples to 7 imagery: http:\/\/landsat.gsfc.nasa.gov\/\nsee how geospatial technology is being incorporat- education\/teacherkit\ned into K\u201312 curricula. What kinds of activities are\n6. Sciencespot - GPS lesson plans: http:\/\/\nteachers using in the classroom that involve geo-\nsciencespot.net\/Pages\/classgpslsn.html\nspatial technology?\n7. NASA Teacher\u2019s Guide - Using the Image\n1. Educaching - GPS lesson plans: http:\/\/www. Composite Explorer in the classroom:\neducaching.com http:\/\/earthobservatory.nasa.gov\/\n2. GIS 2 GPS - GIS and GPS lesson plans: http:\/\/ Experiments\/ICE\/ice_teacher_guide.php\ngis2gps.com\nWhen you\u2019ve examined these examples, fire up\n3. Google for Educators - Using Google Earth, a search engine and locate some other examples of\nMaps, and SketchUp in the classroom: http:\/\/ how geospatial technologies are being applied in\nwww.google.com\/educators\/geo.html the classroom.\nAmong OhioView\u2019s initiatives is SATELLITES (Students and Teachers\nSATELLITES Students\nExploring Local Landscapes to Interpret the Earth from Space), a program\nand Teachers Exploring\nLocal Landscapes to designed to instruct K\u201312 teachers about geospatial technologies and aid\nInterpret the Earth from them in incorporating it into their classrooms. SATELLITES hosts week-long\nSpace\u2014an OhioView \u201cTeachers\u2019 Institutes\u201d during the summer (in locations in Ohio). These in-\ninitiative involving K\u201312\nstitutes are free for teachers to attend (and teachers can earn free graduate\nteachers and students\ncredits, when available, through successful participation). During the week,\nwith geospatial\ntechnologies. teachers receive introductory instruction in several of this book\u2019s topics (in-\ncluding GIS, GPS, remote sensing, landscapes, and remotely sensed environ-\nment or climate data) and hands-on work with the technology. Teachers also\nreceive free equipment to take back to their classrooms, including a GPS re-\nceiver and an infrared thermometer. The goal is for the teachers to develop an\ninquiry-based research project incorporating geospatial technologies and an\nenvironmental theme (such as previous SATELLITES institutes that involved\nthe International Polar Year).\nAn example of a SATELLITES success story is the \u201cSatellite Girls\u201d (a group\nof four middle school students from Akron, Ohio). Their teacher attended the\nSATELLITES program in the summer of 2007 and involved a group of his stu-\ndents with data collection and analysis through GLOBE. The girls competed\nwith their project at the state and national levels and were selected to be one\nof five teams of students to represent the United States at the international\nGLOBE Learning Expedition in Cape Town, South Africa, in 2008. 445\nWhat Types of Educational Opportunities Are Available with Geospatial Technology?\nFIGURE 15.3 The\nWebsite of the GLOBE\nProgram. (Source: The GLOBE\nProgram\/The University of\nTexas at Tyler)\nThe inquiry-based projects the K\u201312 students become involved with\nare part of GLOBE (Global Learning and Observations to Benefit the En- GLOBE Global Learning\nvironment), a program sponsored by NASA and NOAA that incorporates and Observations\nover 20,000 schools from 114 countries across the world dedicated to study to Benefit the\nEnvironment\u2013an\nof Earth\u2019s environment (see Figure 15.3). Using specific data-collection\neducation program\nprotocols, students involved with GLOBE collect data related to land cov-\naimed at incorporating\ner, temperature, water, soils, or the atmosphere. This data is shared with user-generated data\nother s tudents around the world via the GLOBE Website, creating rich and observations from\nextensive datasets for use (for instance, via NASA World Wind). Teachers at- around the world.\ntending the SATELLITES Institutes learn GLOBE protocols and engage their\nstudents through field data collection coupled with geospatial technology\napplications.\nWhat Types of Educational Opportunities Are\nAvailable with Geospatial Technology?\nBeyond K\u201312 involvement, geospatial technology is widely integrated into the\ncurricula of higher education programs around the world. Geospatial tech-\nnology course offerings are commonly found in college and university geog-\nraphy departments, but also within civil engineering, natural resources, envi-\nronmental sciences, or geology programs. Bachelor\u2019s degrees may offer some 446\nChapter 15 What\u2019s Next for Geospatial Technology?\nsort of concentration in geospatial technology, or at least the option to take\nseveral courses as part of the major. A minor in GIS or geospatial technolo-\ngies is also a common option in geography departments. Graduate programs\n(both master\u2019s and doctorate degrees) in geography often have an option to\nfocus on aspects of geospatial technology as well.\nHowever, with the growth of the geospatial field, it\u2019s now possible to get\na higher-education degree in geospatial technology (or GIS, or geographic\ninformation science, depending on the name of the program). Bachelor\u2019s,\nmaster\u2019s, and even doctorate programs have become available at several uni-\nversities. Some of these are interdisciplinary programs, sometimes involving\nelements of various social sciences or computer science and information sys-\ntems topics such as programming, databases, or Web design, coupled with\nclasses drawn from other related fields. For instance, Youngstown State\nUniversity offers a bachelor\u2019s degree in \u201cSpatial Information Systems\u201d that\ncombines elements from the geospatial side of the geography discipline with\ncomputer-related coursework in programming and databases, classes in CAD\nand professional writing, and an option for the student and advisors to design\na set of courses for application (such as environmental science, biology, or\narcheology).\nCertificate programs are also a widespread option for geospatial educa-\ntion. These programs vary from school to school, but typically involve taking\na structured set of geospatial classes (perhaps four to seven classes total).\nThese programs tend to be focused squarely on completing the require-\nments for a set of courses, and are often available at both the undergraduate\nand graduate levels. For example, the Geospatial Certificate at Youngstown\nState University involves students taking four required courses\u2014an intro-\nductory mapping or geospatial class, an introductory GIS class, an intro-\nductory remote sensing course, and either an advanced GIS or advanced\nremote sensing class. Students take an additional two courses from a pool\nof options, including field methods, GPS and GIS, a geospatial-related in-\nternship, object-oriented programming, and database design. Certificates\nand degree programs are becoming increasingly available at the commu-\nnity or technical college level as well (see Hands-on Application 15.4: De-\ngree Programs and Certification for Geospatial Technology for some further\ninvestigation).\nWith geospatial technology entering so many different fields, students\nfrom outside the geography field may seek out a couple of courses or certifi-\ncation to gain a more formal background in geospatial technology to apply to\nwhatever field they\u2019re in (such as business, real estate, archeology, geology,\nor human ecology). Online courses are being offered by a variety of schools\nto allow students to complete a geospatial certificate or a degree remotely.\nCompanies like Esri and organizations like ASPRS offer their own certifica-\ntion programs, and \u201cvirtual universities\u201d offer students at one institution the\noption to take a geospatial course from a professor at another school. Even\nmore specialized courses are being developed\u2014for example, today you can 447\nWhat Other Kinds of Data and Software Are Available Online?\nHands-on Application 15.4\nDegree Programs and Certification for Geospatial Technology\nAn online utility is available for examining the Schools are mapped by categories\u2014whether they\navailable geospatial programs at community col- offer a degree in an aspect of geospatial technology,\nleges or technical colleges throughout the Unit- a certificate program, or have classes available. Check\ned States\u2014open your Web browser and go to your local area for what nearby schools (select the\nhttp:\/\/216.69.2.35\/flexviewer\/index.html. This option for CC Info on GIS Offerings in the cube menu\nmap (part of the GeoTech Center Website at http:\/\/ icon to get the names of the schools) are involved\nwww.geotechcenter.org) shows community and with teaching geospatial courses and what kinds of\ntechnical colleges involved with geospatial education. degree programs they offer.\nreceive a bachelor\u2019s degree from the University of North Dakota in piloting\nunmanned aerial vehicles (UAVs).\nWhat Other Kinds of Data and Software Are\nAvailable Online?\nThe previous chapters have described numerous datasets and how to obtain\nthem (either for free or for minimal cost), often through the applications in\nthe chapters. To build a database of available geospatial data, you could get\nyour hands on:\na 3D models for Google Earth \u2013 see Chapter 14\na Digital Line Graphs (DLGs) \u2013 see Chapter 5\na Digital Raster Graphics (DRGs) \u2013 see Chapter 13\na Geocaches or other GPS destination locations \u2013 see Chapter 4\na Landsat imagery (current or historic) \u2013 see Chapter 11\na MODIS images \u2013 see Chapter 12\na NAPP aerial photos or orthophotos\u2013 see Chapter 9\na National Elevation Data (NED) and DEMs \u2013 see Chapter 13\na National Land Cover Database (NLCD) \u2013 see Chapter 5\na TIGER\/Line files \u2013 see Chapter 8\na US Topo maps \u2013 see Chapter 13\nBeyond these options, there are plenty of other online resources available\nfor obtaining geospatial data, and usually for free. State, county, or municipal\nWeb services will often make their GIS data or remotely sensed imagery avail-\nable free of charge (as you saw in Chapter 5). Other Web resources used in 448\nChapter 15 What\u2019s Next for Geospatial Technology?\nprevious chapters allow you to specify a geographic area and then download\nmultiple datasets that cover that one area. For instance, if you\u2019re studying East\nLiverpool, Ohio, Web resources like the Seamless Server (from Chapter 13) or\nEarthExplorer (from Chapters 5 and 9) allow you to select many types of data,\nincluding remotely sensed imagery, land-cover data, elevation data, and road\nnetworks of East Liverpool.\nThe Seamless Server is only one component of a much larger geospatial\nNational Map an data distribution program called the National Map. Users of the National\nonline base map Map can access multiple types of datasets, including elevation, hydrography,\nof downloadable transportation, land-cover data, boundary files, and more, all in formats read-\ngeospatial data\nable by GIS software. The National Map is part of larger National Geospatial\nmaintained and\nProgram and serves as a source of geospatial data for the United States (see\noperated by the USGS\nand part of the National Figure 15.4 and Hands-on Application 15.5: The National Map Viewer for more\nGeospatial Program. information about the National Map). You can think of the National Map as a\n\u201cone-stop-shopping\u201d venue for free geospatial data\u2014numerous types of data-\nGOS the Geospatial\nOne-Stop, an online sets are available for a geographic area that you define, a very useful option\nfederal government that prevents you from having to download data from multiple sources, then\nportal used for clip or alter the data to make it fit the region you\u2019re examining.\nobtaining geospatial\nThe United States government has created other online resources, such\ndata.\nas the Geospatial One Stop (GOS) for obtaining geospatial data. GOS pro-\nvides users free access to numerous datasets via its portal\u2014orthophotos, par-\ncel information, cadastral data, DEMs, transportation, hydrology datasets,\nand more are available. There are many data repositories available to easily\nget the geospatial data you need quickly into your hands. The various State-\nView programs make Landsat imagery (and sometimes other remotely sensed\ndata) for their state available. For instance, OhioView servers permit the free\nFIGURE 15.4 The online download of Ohio Landsat data from their multi-year archive. Other online\nNational Map Viewer. services, such as the GIS Data Depot, contain numerous datasets (including\n(Source: U.S. Geological\nDRGs, DEMs, DOQQs, and hydrologic data).\nSurvey) 449\nHow Can You Use Geospatial Technology Online?\nHands-on Application 15.5\nThe National Map Viewer\nYou can access the National Map online at http:\/\/ map changes, press the Download Data button in\nnationalmap.gov\/index.html\u2014then select the op- the upper-right corner of the map. New download\ntion for National Map Viewers. From this new page, options will appear that allow you to select the\nchoose the option to Open the Viewer. A new Web extent of the area you want to download data for\npage will open with the Viewer itself, the tool used (such as counties, congressional districts, or the ex-\nin accessing data via the National Map. The National tent of the view seen on the screen). Next, the data\nMap can be used to view available data\u2014from the layers available to download will be shown (includ-\nOverlay options on the left side of map, you can se- ing US Topo, structures, transportation, boundaries,\nlect what layers to display. High-resolution imagery hydrography, land cover, elevation, and orthoimag-\ncan be accessed as an additional layer from the Im- ery) as well as the formats these layers are available\nagery option on the right side of the map. in. Select the layers you want, then choose Add to\nThe layers you\u2019re viewing in the National Map can Cart and follow the remaining steps to download\nalso be downloaded for use in GIS or other software your data (you will have to register with the USGS\nprograms. First, search for an area of interest (for to receive your data\u2014they may send you an e-mail\ninstance, Reno, Nevada), then after the view of the with a link you can use to retrieve your data).\nSometimes the geospatial data comes available in a format called SDTS SDTS the Spatial Data\n(the Spatial Data Transfer Standard), a system established by the United Transfer Standard, a\nStates government. The intent with SDTS was to place data in a \u201cneutral\u201d file \u201cneutral\u201d file format\nfor geospatial data,\nformat rather than a software-specific one. Agencies like the USGS, NOAA,\nallowing it to be\nand the Census Bureau utilize SDTS as the format for delivering their geospa-\nimported into various\ntial data to you. Software programs will often have a converter that enables geospatial software\nthe import of data in SDTS format to a form directly readable by the software programs.\n(for instance, ArcGIS has a utility to import an SDTS DLG into a coverage for\nyou to use).\nFor some other examples of freely available geospatial software online,\ncheck out Hands-on Application 15.6: Some Other Available Geospatial Technol-\nogy Programs and Tools Online on page 450.\nHow Can You Use Geospatial Technology\nOnline?\nOnline resources are the quickest and easiest way to access geospatial data.\nWhether you want to acquire archived satellite imagery or get the newest\nupdates for Google Earth, you can do everything via the Internet. The same\nholds true with distributing your own data and information\u2014setting up a\nWeb interface where your data can be used interactively is the way to go.\nPosting parcel databases and DEMs for download will be of great aid to a\nperson with some geospatial knowledge, but other individuals may not know 450\nChapter 15 What\u2019s Next for Geospatial Technology?\nHands-on Application 15.6\nSome Other Available Geospatial Technology Programs and Tools Online\nIn the previous chapters, you\u2019ve used a variety of for Google Earth, DEM tools, or GPS utilities, and see\nfree software programs, like MultiSpec for examin- what kinds of other software programs are available\ning satellite imagery, MapCruncher for georeferenc- and how you can utilize them.\ning a scanned map, and Trimble Planning for looking In addition, this book has made extensive use of\nat GPS satellite conditions. However, there are a lot Google Earth and other virtual globe programs (like\nof other free geospatial tools available out there NASA World Wind in Chapter 12 and ArcGIS Explorer\nbeyond just the software and programs used in this in Geospatial Lab Application 15.1). Virtual globes\nbook. An excellent resource for locating and down- are very powerful utilities for examining features\nloading free geospatial software is the Free Geogra- around the planet, and are also very cool and fun\nphy Tools Website\u2014open your Web browser and go to use. There are other virtual-globe-style programs\nto http:\/\/freegeographytools.com. From the Top- available\u2014fire up a search engine and try to locate\nics list on the right-hand side, choose the type of some other examples of virtual globes or related\ntool you\u2019re interested in, whether it\u2019s new additions geospatial programs available online.\nwhat GIS is, or what a shapefile is, or how to georeference imagery, and may\nbe baffled as to how to properly use the data. However, when all of the data is\nassembled into an online map, allowing a user to find the address of a house\nfor sale, access data about it, determine the land value of the property, and\nsee whether or not the house is on the floodplain, the online utility becomes\na lot more useful. In addition, other users may not have access to the specific\nGIS program you\u2019re running, so if all they need is a Web browser to interact\nwith your data, you\u2019ve removed another obstacle in working with the geospa-\ntial data. You\u2019ve already used some examples of these types of online GIS tools\nback in the Hands-on Applications in Chapters 5 and 6.\nThere are many programs available for taking geospatial data and setting it\nup online in an easy-to-use interactive format, or to simply create an interactive\nArcGIS Server an Esri map and incorporate it into a Website. An example of this is ArcGIS Server, Esri\nutility for distribution software designed to facilitate the distribution of interactive maps and geospa-\nof geospatial data in tial data across the Internet for viewing and analysis (see Figure 15.5 for an\nan interactive mapping\nexample of an online mapping and analysis utility created using ArcGIS Server\nformat on the Internet.\nfor the city of Philadelphia). Setting up an interactive tool online for managing\nand analyzing geospatial data is becoming increasingly common for a variety\nof utilities. Check out Hands-on Application 15.7: ArcGIS Server Live User Sites\nfor a way to access and examine several examples of ArcGIS Server in action for\nworking with geospatial data online (rather than being required to have ArcGIS\nrunning on your own computer).\nTools like these are part of a larger initiative that Esri refers to as \u201cMap-\nping for Everyone,\u201d which allows users to not just add to existing databases\nor use special software to distribute geospatial data, but to create their own\nmaps with their own data (or data already processed for them) and publish\nthe maps on their own Website. Like the user-generated content of VGI, this 451\nHow Can You Use Geospatial Technology Online?\nFIGURE 15.5 Online\nGIS for zoning in the city\nof Philadelphia, created\nusing ArcGIS Server.\n(Source: http:\/\/citymaps10.\nphila.gov, Esri\u00ae ArcGIS Server\ngraphical user interface\nCopyright \u00a9 Esri. All rights\nreserved.)\nAPI Application\noption puts more mapping choices and geospatial data into the hands of end\nProgramming Interface\nusers, allowing you to build, design, and customize your own mapping appli-\n\u2014the functions and\ncations, and then embed them onto a Website or e-mail a link to others. commands used when\nThe basis for \u201cMapping for Everyone\u201d is an available ArcGIS Javascript making requests of\n(or Flex or Silverlight) API (Application Programming Interface) that you software.\ncan use to develop custom applications. Esri makes several samples (such as Map Services pre-\nbase maps or layers, referred to as Map Services) available for free as part made base maps and\nof their online resources so that creating a simple Web-mapping application layers that can be\nused in creating Web-\nis as easy as copying their base code and customizing it. Other companies,\nmapping applications.\nsuch as Google, Microsoft, and Yahoo!, provide APIs that allow you to embed\nHands-on Application 15.7\nArcGIS Server Live User Sites\nIn previous chapters, you\u2019ve explored several on- of the available maps. Click on a map and a brief\nline mapping applications, whether to perform description will appear, along with a link to \u201cView\nqueries, build buffers, check color choices, find a Live Site\u201d (that is, interact with that particular map\nshortest path, or georeference a map. There are utility itself). Examine several of the Live Sites.\nmore\u2014a lot more\u2014online mapping sites out there. All of these mapping sites linked to this Website\nFor multiple examples, check out some of the Arc- are examples of using ArcGIS Server to construct\nGIS Server Live User Sites\u2014open your Web browser Web mapping utilities. What are some of the\nand go to http:\/\/www.esri.com\/software\/arcgis\/ functions being mapped and how are they being\narcgisserv er\/live-user-sites.html and examine some implemented? 452\nChapter 15 What\u2019s Next for Geospatial Technology?\nHands-on Application 15.8\nArcGIS Web Mapping\nTo get started building your own Web map using are available for you to utilize in your own maps?\nsome available resources, and to see what sort By selecting an option, you can choose to get more\nof data is available to start with, open your Web information about what the data on that map rep-\nbrowser and go to http:\/\/www.esri.com\/mapping- resents. What types of Web applications could you\nfor-everyone\/index.html and then select one of design using this available data?\nthe options for making a map. What map layers\ntheir mapping products\u2019 (such as Google Earth, Google Maps, Bing Maps, or\nYahoo! Maps) functions on a Website or design applications. By combining\nmashup the several map layers together, a user can create a new map mashup, which uses\ncombination of two or different datasets together in one map. Check out Hands-on Application 15.8:\nmore map layers into ArcGIS Web Mapping for some examples of available map layers to get started\none new application.\ndesigning your own Web-mapping applications.\ncloud a computer Geospatial technology is increasingly becoming part of a field referred to\nstructure wherein as cloud computing, wherein resources (such as data storage or other geospa-\ndata, resources, or\ntial applications, such as GIS) are being utilized at another location but served\napplications are kept\nto a user across the Internet. Several of the previous examples describe uses\nat another location and\nmade available to the of cloud computing, where you don\u2019t need the analysis tools downloaded and\nuser over the Internet. installed on your computer, but are rather accessing them via a Web browser.\nFor instance, geospatial data can be obtained through the cloud structure in\nthe form of aerial or satellite imagery, digital topographic maps, or digital\nstreet maps that can be used as a base map with your own applications\u2014\nArcGIS Online a but this data is being served to you over the Internet, rather than you having\ncollection of available to go out, download it, and set it up on your own computer. For example,\nWeb resources made Esri makes a number of the base maps just described available through its\navailable by Esri.\nArcGIS Online resources\u2014when using ArcGIS or ArcGIS Explorer, you can\nThinking Critically with Geospatial Technology 15.1\nWho Owns Geospatial Data?\nThink about this\u2014you start with a basemap sample if you make changes or updates to a resource like\n(for instance, a world topographic map), obtain data Wikimapia, can you claim ownership of a shared on-\nabout earthquake locations and other natural haz- line resource? With more and more geospatial data\nards, create a mashup, and post it onto your Web- and customization functions becoming available via\nsite. Who owns this geospatial resource or the geo- cloud resources (such as the high-resolution Bing\nspatial dataset you\u2019ve created? In essence, you\u2019ve Maps imagery through ArcGIS Online), who really\ntaken other freely available data and created your owns the end products created from them (or does\nown unique geospatial resource\u2014but can you really anybody really own them)?\nlay claim to this as something you own? Similarly, 453\nWhat Are Some Other Cool Developments in Geospatial Technology?\nstream these base layers (such as topographic maps, Bing Maps imagery, or FIGURE 15.6 The types\nof base maps available\nstreet maps) onto your computer through the Internet and use them in your\nvia the Internet through\nGIS analyses (see Figure 15.6 for an example of the basemap data available ArcGIS Explorer Online.\nthrough ArcGIS Online). (Source: USGS, FAO, NPS, EPA,\nDeLorme, TANA. Esri\u00ae ArcGIS\nArcExplorer graphical user\nWhat Are Some Other Cool Developments interface Copyright \u00a9 Esri)\nin Geospatial Technology?\nWith computers becoming faster and having the ability to handle geospatial\ndata more quickly and easily, new methods of visualizing data have been de-\nveloped. It\u2019s one thing to view a 2D choropleth map (like in Chapter 7), an-\nother thing to create a pseudo-3D representation of the mapped data (like\nin Chapter 14), and yet another thing altogether to be able to interact with\nthe data in an immersive environment. Examining a realistic geospatial land-\nscape and flying across it (like in Chapter 13) is very neat, but what if you\ncould simulate the full 3D effect of diving through canyons or banking over\nridgelines? Immersive \u201cvirtual reality\u201d technology allows the user to actually\nbecome part of the data, moving with the landscape, being placed inside a\nstructure, or being on the surface of the map. This sense of immersion or in-\nteractivity with geospatial data adds immensely to the understanding of the\nnature of spatial data, both for researchers and students.\nAnother example of enhanced geospatial visualization is NOAA\u2019s Science\nSOS Science on\non a Sphere (SOS) device. SOS features a large six-foot sphere that has a Sphere, a NOAA\na nimated images projected onto its surface, turning the sphere into a global initiative used in\nmodel of Earth. With SOS, viewers can see geospatial data such as ocean cur- projecting images of\nEarth onto a large\nrents, sea-surface temperature, land-use information, global fires, and data prod-\nsphere.\nucts derived from satellites in the Earth Observing System (see Chapter 12) on a 454\nChapter 15 What\u2019s Next for Geospatial Technology?\nFIGURE 15.7 Science\non a Sphere in action in simulated planet surface (see Figure 15.7). Think of the global EOS datasets\nAtlanta, Georgia. (Source:\nused in Geospatial Lab Application 12.1 or some of the Google Earth imagery\nNOAA)\nbut projected onto an actual spherical surface several feet tall. Museums and\nscience centers around the world are using SOS technology for geospatial\neducation related to Earth and its climate and environment.\nAnother way of making geospatial data appear more realistic (and adding\na sense of immersion) is to view the data in stereo. Stereo imagery shows two\nimages (for instance, two images of the same landscape), rendered apart from\neach other. When you look at something, both of your eyes are seeing the\nsame thing, just from different points of view, allowing you to see depth and\ndistance (and view the real world in three dimensions). Stereo images use a\nsimilar principle, as they are two representations of the same thing, just taken\nfrom two different positions of the camera or sensor. Stereo imagery has been\nused for a long time in remote sensing for measuring heights and depths of\nareas. Overlapping aerial photography or images acquired from sensors (such\nas those onboard SPOT 5 or the ASTER or MISR sensors on Terra) can be used\nfor creating stereo data.\nanaglyph a new image A simple way of viewing imagery or data in stereo (which several geo-\ncreated by slightly\nspatial software programs have the capability for) is to create a red and blue\noffsetting the features\nanaglyph of the data\u2014in which the two images are slightly separated, but\nin an existing image\nslightly from each other one is highlighted in red and the other in blue. By wearing a pair of 3D glass-\nand highlighting them es (the ones that have colored gels for lens, with one lens being red and the\nin different colors. other being blue), the anaglyph images appear to be raised or elevated off the 455\nWhat Are Some Other Cool Developments in Geospatial Technology?\nFIGURE 15.8 Use of the\nanaglyph viewing mode\nof a perspective view of\nthe terrain in southern\nCalifornia (from NASA\nWorld Wind). (Source: NASA)\nscreen in a \u201c3D effect\u201d (see Figure 15.8). This type of stereo effect (where one\neye views the red and the other eye views the blue) creates a type of stereo- stereoscopic 3D an\nscopic 3D that gives the illusion of depth in parts of the image. Some items effect that simulates\nmay appear in the foreground and others in the background. Red and blue the illusion of depth or\nimmersion in an image\nanaglyphs are certainly nothing new, but today, when stereoscopic 3D mov-\nby utilizing stereo\nies have become all the rage (and the top-grossing film of all time being one\nimaging techniques.\nof them), there are more high-tech ways of using these stereo concepts with\ngeospatial technology.\nIf you go to a 3D movie today, you\u2019re not going to be wearing red and blue\nlenses, but rather a pair of polarized glasses instead. These same types of 3D\nglasses can be used with the GeoWall, a computer tool used for examining GeoWall a powerful\nimagery in an immersive three-dimensional environment with stereoscopic computer tool used for\n3D viewing techniques. First developed by the members of the GeoWall Con- displaying data and\nimagery in stereoscopic\nsortium in 2001, it has spread to more than 400 systems developed for use in\n3D.\nschools and colleges (along with places like the EROS Data Center and NASA\u2019s\nJet Propulsion Laboratory). The GeoWall can be used for viewing geospatial\ndata in a stereoscopic 3D format.\nThe GeoWall consists of several components that can be purchased sepa-\nrately to keep the cost of the whole setup affordable (about $10,000 or so).\nFirst, two high-power Digital Light Processing (DLP) projectors are used to\nproject the two images needed for stereo viewing. These projectors get hooked\ninto a powerful computer with a dual-output video card (so the computer can\nsend its output into both of the projectors). The images get projected through\nspecial filters in front of the projectors\u2019 lenses, which polarize the light. The\nlight is projected onto a special screen made of material that will preserve\nthe polarized light (rather than scattering it). Lastly, the user wears a pair of\npolarized 3D glasses to view the imagery. See Figure 15.9 on page 456 for the\ncomponents of the GeoWall projectors and their filters. 456\nChapter 15 What\u2019s Next for Geospatial Technology?\nFIGURE 15.9 The\nbasic setup of the\nGeoWall\u2014the twin DLP\nprojectors connected to\nthe computer and the\npolarizing filters in front\nof the lenses. (Source:\nCourtesy Bradley A. Shellito)\nWearing the 3D glasses, the viewer perceives the images in a 3D effect as\nif the items and images are appearing off the screen\u2014similar to watching a\nstereoscopic 3D movie. Viewing and interacting with imagery like this gives\na sense of depth or immersion, as some items will be in the foreground of the\nimage and some will be in the background. Figure 15.10 shows an image of\na GeoWall in use from the perspective of an outsider not wearing the polar-\nized glasses (without the glasses, the images would appear blurry and slightly\naskew from one another). GeoWall viewers position themselves in front of the\nscreen and are able to see the images projected in stereoscopic 3D.\nFIGURE 15.10 Imagery\nprojected onto the screen\nby the GeoWall system\n(as viewed without the\nglasses). (Source: Courtesy\nBradley A. Shellito) 457\nWhat Are Some Other Cool Developments in Geospatial Technology?\nHands-on Application 15.9\nSome Blogs about Geospatial Technologies\nLet\u2019s face it, geospatial technology is a rapidly 4. Google Earth Blog: http:\/\/www.gearthblog.\nchanging field, with new innovations and advance- com\nments coming all the time, and you\u2019ll probably first 5. Google Lat Long Blog: http:\/\/google-latlong.\nhear about these things online. Blogs are a great blogspot.com\nway to keep informed on up-to-date geospatial\n6. Mapperz - The Mapping News Blog: http:\/\/\ndevelopments. Check out the following examples\nmapperz.blogspot.com\nof blogs to see what people are buzzing about con-\n7. Official Google SketchUp Blog: http:\/\/\ncerning current or upcoming developments in the\nsketchupdate.blogspot.com\ngeospatial universe:\n8. Ogle Earth: http:\/\/ogleearth.com\n1. Digital Geography: http:\/\/www.\n9. Planet Geospatial: http:\/\/www.planetgs.com\ndigitalgeography.co.uk\n2. Digital Urban: http:\/\/digitalurban.blogspot. Once you\u2019ve examined these examples, fire up a\ncom search engine and try to locate some other blogs or\nforums or pages on a social networking site concern-\n3. Esri blogs: Esri maintains many different\ning new developments in geospatial technology.\nblogs about different software programs,\ndevelopment tools, and applications: http:\/\/\nwww.esri.com\/news\/blogs\/index.html\nUsing the GeoWall for visualizing geospatial data carries a lot of ben-\nefits. For instance, the GeoWall enables the user to see maps draped over a\nsurface model or terrain data displayed in stereoscopic 3D. Certainly, being\nable to dive inside a meteor crater or view volcanic features, coastlines, or\nriver valleys in a stereoscopic 3D environment allows the viewer to get a\nbetter sense of the terrain features. Landscapes, geologic processes, draped\nsatellite imagery, draped topographic maps, stereoscopic photos, and tours\nof virtual environments are all types of data that can be viewed using a\nGeoWall setup.\nThinking Critically with Geospatial Technology 15.2\nWhat Do You See as the Next Frontiers for Geospatial Technology?\nRight now, you can run Google Earth on your geospatial field going? As access to information\nphone, download satellite imagery for free, view becomes faster and easier, as computing power\nlandscapes and cities in immersive 3D, create advances, and as our lives become more and more\nyour own geospatial maps and post them on a tied together in a global web, what\u2019s going to be\nWebsite, and earn a doctorate in the geospatial the next stages for geospatial technology? What\nfield. If that\u2019s what you can do today, what\u2019s go- do you see the geospatial world looking like five\ning to happen tomorrow? Where do you see the years from now? 458\nChapter 15 What\u2019s Next for Geospatial Technology?\nChapter Wrapup\nSo this is it, the end of the last chapter. For 15 chapters now, we\u2019ve looked\nat multiple aspects of geospatial technologies, both from the theoretical side\nand also from working hands-on with several different aspects of the tech-\nnologies themselves. The geospatial world has changed a lot in the last few\nyears (for example, Google Earth only debuted back in 2005), and it\u2019s going\nto keep changing (see Hands-on Application 15.9: Some Blogs about Geospatial\nTechnologies on page 457 for some online blog sources related to geospatial\ntechnologies).\nGIS, GPS, remote sensing, and all of their applications are only going to\ncontinue becoming more important in our lives, and even though many people\nmay still not be familiar with the term \u201cgeospatial technology,\u201d by this point\nyou\u2019ve probably got a pretty good idea of what it is and what it can do. There\u2019s\nstill one last Geospatial Lab Application. In Geospatial Lab Application 15.1\nyou\u2019ll be using Esri\u2019s ArcGIS Explorer program to examine a variety of geospa-\ntial applications as a \u201csummary\u201d of what we\u2019ve been looking at for 15 chapters.\nImportant note: The references for this chapter are part of the online com-\npanion for this book and can be found at http:\/\/www.whfreeman.com\/\nshellito1e.\nKey Terms\nGIS Day (p. 439) GLOBE (p. 445)\nEarth Observation Day (p. 440) National Map (p. 448)\nVGI (p. 440) GOS (p. 448)\nwiki (p. 440) SDTS (p. 449)\nOGC (p. 441) ArcGIS Server (p. 450)\nAAG (p. 441) API (p. 451)\nASPRS (p. 441) Map Services (p. 451)\nNCGE (p. 441) mashup (p. 452)\nOhioView (p. 441) cloud (p. 452)\nAmericaView (p. 442) ArcGIS Online (p. 452)\nStateView (p. 442) SOS (p. 453)\nGeographic Alliance Network anaglyph (p. 454)\n(p. 443) stereoscopic 3D (p. 455)\nSATELLITES (p. 444) GeoWall (p. 455) "}