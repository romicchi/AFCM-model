{"text":"1. Being a systems innovator\nEditor: David A. Bray (Emory University, USA)\nContributors:Benn Konsynski and Joycelyn Streator (Emory University, USA)\nReviewer: John Beachboard (Idaho State University, USA)\nLearning objectives\n\u2022 define what broadly constitutes a \u201csystem\u201d and an \u201cinnovation\u201d\n\u2022 describe examples of innovation\n\u2022 describe how one might strive to be a systems innovator\n\u2022 describe the benefits of innovation to society at-large\nIntroduction\nLet us welcome you the modern age, so full of promise both in terms of human and technological progress! This\nis the first chapter of several in this textbook. Later chapters will discuss what information systems are, how\ninformation systems are integrated into the workplace, the role of knowledge workers alongside information\nsystems, and how information systems link to the success of organizations. In this opening chapter, we address the\nrole of innovation and being a systems innovator. Our goal is to motivate you to want to be a systems innovator, not\njust a maintainer of existing systems or a support of systems that have already been built.\nWe want you to innovate! Without systems innovators, it is quite possible that our modern age would not be so\nfull of promise and potential. In fact, without systems innovators, humanity might never have reached modernity at\nall.\nSeveral historians say we humans are \u201cmodern\u201d when we do not automatically reject new or foreign elements in\nsociety. For human society, modernity begins when communities began to explore, tolerate, and accept the new and\ndiverse. Thus, modernity includes a receptiveness of human societies to new ideas. Living in the modern age allows\nus to expect that modern enterprises and markets will tolerate and potentially reward to new ideas and new\npractice. In a modern age, those individuals who design insightful innovations (i.e. innovators) can be highly\npraised if their innovations are well timed, well designed, and well implemented.\nAs systems innovators, we welcome the modern age and strive to be open to new and beneficial ideas of change.\nHuman societies value and evaluate new ideas by expected impact and effect. Modern markets and firms represent\nparticular types of human organizations. Markets and firms can incorporate innovations by changing either their\ndesign or practices.\nBeing a systems innovator\nLet us briefly consider the meaning of the essential words in the title: \u201csystems\u201d and \u201cinnovator\u201d (defining\n\u201cbeing\u201d is something we will leave to the philosophers).\n9 Systems are the object of particular designs. Broadly speaking, systems involve the organization of things,\nlogical and physical. Systems include data, processes, policies, protocols, skill sets, hardware, software,\nresponsibilities, and other components that define the capabilities of an organization. Systems include human and\nnon-human aspects. The components, or parts, of a specific system can be either real or abstract. Components\ncomprise an aggregate \u201cwhole\u201d where each component of a system interacts with at least one other component of\nthe system. Cumulatively, all the components of a system serve a common system objective. Systems may contain\nsubsystems, which are systems unto themselves that include a smaller set of interactions among components for a\nmore narrowly defined objective. Systems may also connect with other systems. The following diagram (Exhibit 1)\nillustrates an example system.\nExhibit 1: A sample system\nLater on in this textbook, you will learn about information systems and how they support organizations. We will\nleave defining the specifics of information systems to those chapters\u2014though will will say that information systems,\nas with all systems, consist of multiple components. All the components of a system serve a common system\nobjective in support of the information system. Frequently this objective supports the purpose of the organization\u2014\nthe organization itself serving as a system of humans and tasks as well.\nLater chapters will discuss the different components a of information systems and how these components\nshould interact. Other chapters will discuss the consequences of components not interacting. For now, we want to\nmotivate you to be interested in systems\u2014and innovation.\nInnovation is the process of \u201cmaking improvements by introducing something new\u201d to a system. To be\nnoteworthy, an innovation must be substantially different, not an insignificant change or adjustment. It is worth\nnoting that innovation is more a verb than a noun in our context. Innovation is similar to the word evolution, which\nderives from the Latin root for staying \u201cin motion.\u201d Systems innovations often include an expectation of forward\n10 motion and improvement. To be worthwhile, innovations must be worth the cost of replacement, substitution, or\nupgrades of the existing order.\nThe term innovation may refer to both radical and incremental changes to products, processes, or services. The\noften unspoken goal of innovation is to solve a problem. Innovation is an important topic in the study of economics,\nbusiness, technology, sociology, and engineering. Since innovations are a major driver of the economy, the factors\nthat lead to innovation are also critical to government policy-makers. In an organizational context, innovations link\nto performance and growth through improvements in efficiency, productivity, quality, competitive positioning,\nmarket share, etc. All organizations can innovate, including for example hospitals, universities, and local\ngovernments.\nRather than construct a narrow definition of innovation, it is useful to think of innovation as including, but not\nlimited by, a few key dimensions. Successful innovations include these dimensions.\nThe first dimension is that of innovation form. Innovations manifest in many ways, but generally are either\ntangible or intangible. Tangible innovations result in new goods, services, or systems that you can physically touch.\nExamples include the introduction of new products or a style of architecture. Intangible innovations include the\ncreation of new services, processes, modes of operating, or thinking. Intangible innovations might introduce greater\nefficiency into an existing process or create an entirely new way of doing something. For example, an innovation\ncould reduce the time required to manufacture a car. This intangible innovation might translate into greater profits\nfor a car manufacturer.\nThe second dimension is that of innovation degree. Innovation degree compares a particular innovation to that\nof the status quo. In 1980, a researcher named John Hage introduced the concept of \u201cradical\u201d versus \u201cincremental\u201d\ninnovation\u2014representing two different types of innovation. An incremental innovation introduces an idea, process,\nor technological device that provides a slight improvement or causes minor change in a normal routine. Sometimes\nthe impact of incremental innovation may require only minor adjustments in the behavior, processes, or equipment\nassociated with a system. A manufacturing facility upgrading to a new version of software that provides additional\nfeatures to enhance existing operations is an example of an incremental innovation.\nConversely, radical innovations introduce an idea, process, or technological device that dramatically alters a\ncurrent system. For example, if a manufacturing firm acquired a new technology that allowed the firm to completely\nredefine and streamline its production processes, then this new technology represents a radical innovation. Often\nradical innovations involve not only new technologies and processes, but also necessitate the creation of entirely\nnew patterns of behaviors.\nSystems innovators are individuals who design and implement innovations. To design refers to the process of\ndeveloping a structural plan for an object. Systems innovators are individuals who transform the practice of\norganizations, markets, or society by making significant forward moving improvements.\nWhat qualities are required of a good system innovator? Systems innovators seek to designs that improve on the\nold to take advantage of new technologies, new techniques and new practice and processes. We would suggest that\nsystems innovators not only recognize that social and economic structures are all human-made, but also recognize\nthat human structures are always open to changes, enhancements, and redesign.\n11 It is important to recognize that systems operate within systems. Identifying the connections and layers of these\nsystems will make you a successful systems innovator. Often identifying new connections or new layers that no one\nelse has identified yet can provide new opportunities for innovation.\nThis book seeks to discuss with you the capabilities, approaches, and skills required of the systems innovator in\nthe 21st century. How does one prepare for the assessment, evaluation, design, and implementation of the\nimprovements to systems, particularly those that incorporate information technologies, particularly those systems\nthat incorporate information technologies?\nSystems innovators are designers\nSociologists note that humans are unique in their invention and adoption of tools. Among these human-made\ntools are the systems and procedures that govern, direct, and enable modern societies to function. These tools also\ninclude the systems that enable the actions of commerce and exchange. Systems enable patterns of work and\nreward and the conduct of participants in enterprise. For our modern age, systems have never been more relevant\nas the speed of society and the enhancement of information access and opportunity for social interaction increase.\nAlmost all aspects of modern commerce, modern society, and modern life are connected the designs of humanity.\nMuch of what defines the pace and practices of our modern age are systems and technology-enabled.\nWhat factors are required for innovation? Good design and good management. We now discuss design and\nmanagement.\nFirst, designers matter. To be a designer implies the task of creating something, or of being creative in a\nparticular area of expertise. Part of being a systems innovator includes being a designer. It is worth considering that\nthe fields of \u201csystems design\u201d and \u201corganization design\u201d are similar as both incorporate creatable, changeable, and\nlinkable elements.\nDesigners seek the requirements and expectations, identify the objectives and measurements of success, give\nstructure to the elements, and form to the components of systems. Success or failure hinge on the ability of a\ndesigner to attain the proper requirements and expectations of a system. For example, a systems innovator plans to\ndesign a new cell phone network for 500,000 subscribers. Unfortunately, the innovator fails to include the\nrequirement of future growth of the cell phone network to 2,000,000 individuals in five years. When the network is\nbuilt, per the design of the innovator, new cell phone subscribers must be turned-away from accessing the network\nbecause of the omitted designer requirement. Since the designer failed to include the proper requirements, this\nomission diminishes the success of the system.\nSecond, managers matter. In addition to developing a structural plan for a system, designers must manage the\nprocess of systems development, to include overseeing systems implementation, adoption, and continuing\noperation. Design also sometimes involves the augmentation and extension of an existing system. Part of being a\nsystems innovator includes the enhancement of an existing or legacy system with a new idea, method, or\ntechnological device. Extending the life of a useful system, or upgrading capabilities to better align with the\nenterprise objective, may be the best service of the systems innovator. Often, it is easier to enhance an existing\nsystem, than it is to decode, decipher, or replace such a system.\nSocial systems are tools designed by humanity. These systems reflect the bias and the values of the designers, or\nthose that task the designers with requirements and expectations. Thus, designers, who create rules, influence\n12 systems greatly. Essential elements of the process and product of system development include the unique style and\npreferences of a designer.\nDesigners leave their mark, their trail, and their values reflected in the tools they produce. Style and preferences\nalso guide systems implementation. It is also important to note that systems are networks of interacting elements.\nThus, the aggregate \u201cwhole\u201d of a large system may be more capable, stronger, or beneficial than the sum of its\nindividual components\u2014or it might be less so. Systems amplify the strengths and the weakness of their design.\nIdeally, well-designed systems amplify the benefits of their individual components.\nInnovations are new answers to problems\nWhat are possible areas demanding innovation? What are different strategies for innovation? These are\nchallenging questions with no easy answer. The concept of innovation has been widely studied, yet it remains a\ndifficult topic to define. Merriam-Webster\u2019s online dictionary describes innovation as \u201cthe introduction of\nsomething new\u201d or \u201ca new idea, method, or device\u201d.\nWhile this definition provides a good starting point for our discussion of innovation, there are still a number of\ndimensions to consider for a more thorough understanding of the concept. Careful observation of our surroundings\nreveals a multitude of innovations. Everything from electricity to running water, or from personal computers to cell\nphones, represents some form on innovation from past systems.\nInnovations are not limited to tangible products. Innovations also occur when processes are dramatically\nimproved. For example, through advances in cell phones, very little human effort is required to communicate a\nmessage across great distances quickly. More than 100 years ago, the similar transactions would have required\nsignificant manual work and time for a message to be sent by postal mail.\nMany things can trigger innovation. An individual or team of individuals may seek to address an existing\nproblem, respond to a new situation, or explore new ability.\nWhile innovations typically add value, innovations may also have a negative or destructive effect as new\ndevelopments clear away or change old organizational forms and practices. Organizations that do not innovate\neffectively may die or be destroyed by those organizations that do. Systems innovators are critical to our modern\nage. Innovators must insure that their envisioned innovations are appropriate to the environment of today and\ntomorrow.\nInnovations are also reactions to change\nWhile innovation can occur as individuals and groups wrestle with new problems, innovation can also be\nreactionary and occur as a response to unplanned changes. The ancient philosopher Heraclatus once said: \u201cThere is\nnothing permanent except change.\u201d\nThe statement is certainly true today in our high-tech world. Advances in computing power, communication\ntechnologies, and networking of computers around the world has quickened the pace at which dramatic change can\noccur across large and diverse groups of electronically connected people. Innovation often arises as a way of coping\nwith, attempting to control, or benefit from changes.\nChanges in the use of information technology often provide the impetus for innovation. There might be\ninstances where local conditions encourage a particular innovation. For example, if past historical conditions\nprevented installation of wired telephone networks because they were too expensive, but now cell phone networks\n13 are both more affordable and available; the innovation of cell phone networks might open up new capabilities for\nareas that previously did not have such technology. As cell phone networks networks become more prevalent, the\nways individuals communicate, compute, and exchange information will change and local companies may seek to\nintroduce cell phones with new features that adapt to these changing communications patterns.\nExciting times for systems innovators\nWe live in exciting times for systems innovators. Advances in electronic communications, airline transportation,\nand international shipping, increasingly connect the lives of multiple individuals throughout the world. Such\nconnective advances are part of a greater trend known as globalization. For the modern age, globalization includes\nthe opening of commercial markets, increased free trade among nations, and increased education for a larger\nnumber of people. With globalization, what you do may influence events on the other side of the world.\nWith globalization, environments for organizations, both businesses and world governments, are becoming\nmore complex. The reasons for this increased environmental complexity include \u201cthe four V's\u201d, specifically:\n\u2022 increased Volume (from local to global context in terms of transactions)\n\u2022 increased Velocity (faster transactions between people)\n\u2022 increased Volatility (organizations change and reorganize faster)\n\u2022 increased concerns regarding Veracity (the truth is harder to distinguish)\nFor systems innovators, it is important to recognize this perspective of increased complexity. This perspective is\nimportant both because it presents opportunities to innovate\u2014by addressing the complexities and challenges\nmentioned above\u2014as well as the risks associated with not innovating. Failure to innovate in an increasing complex\nand interconnected world may mean that your organization, be it a business or government, might become\nirrelevant and outdated quickly.\nIncreased complexity also makes the job of a systems innovator a bit trickier: an innovative solution needs to\naccount for increasing complex environment. What may seem to be a straightforward solution may have\nunintended effects on a system or other systems connected to a system.\nThis leads to a second important perspective: systems operate within systems. Specifically, our world is a system\nof multilayered, interconnected systems. Homes connect to gas, water, and electrical systems that link to other\nhomes. Traveling exposes us to systems of highways, public transportation, trains, planes, and ocean-going ships.\nA business is an organization comprised of multiple workers, interdependent in the tasks they perform. Within\nthe organization, there may be a system of monitoring the funds received into and paid out by the business\u2014and\naccounting system. This accounting system would include humans (managers and accountants), accounting data,\nprocesses for managing accounting data, rules for recording accounting data, as well as technology components.\nWithin this accounting system may be another system: an information system running a computer program\ndedicated to tracking electronically the accounts of the organization. A systems innovator looking to improve the\norganization may focus on the system of the overarching organization itself, the accounting system, or the\ninformation system dedicated to tracking electronically the accounts of the organization.\n14 General insights into human (and information) systems\nFor systems innovators, it is important to note that all human systems are artificial. By \u201cartificial\u201d, we mean that\nhuman systems would not exist naturally in the world without humans. No natural rules govern the systems\nhumans create\u2014whether the systems are governments, businesses, educational institutions, or information\nsystems. This is not to say that the systems humans create do not have rules; rather, they often do! For systems\ninnovators, you can influence and change the rules. Part of innovating is identifying when the rules of a system, be\nit an organization or information system, could be modified to provide a better benefit.\nSo what are rules? Rules are defined ways of interacting with elements in a system, often proscribing an action.\nOne rule might be \u201cdo not steal\u201d. This rule means that individuals should not take an element that does not belong\nto them. Another rule might be \u201cif an electronic message is received from Company ABC, route it to our Accounts\nPayable\u201d. With rules, it is important to note that they link elements with actions. Rules can form the policy of a\nsystem. By system policy, we mean rules that link actions to elements in a system.\nInformation systems include data and processes. Data can be logical values (true vs false), numbers, words, or\nstrung-together sentences. Actions, known as processes, are required to actively exchange, transform, and move\ndata. For a computer to \u201ccompute\u201d, processes actively manipulate data. Components of an information system\ndetail the rules for what processes can do to data, under what circumstances. A systems innovator seeking to\nimprove an information system might look to modify the data an information system contain or collect. Equally, a\nsystems innovator might improve an information system by modifying what processes manipulate data\u2014or an\ninnovator might modify the policies of a system to reuse existing processes in new ways on data.\nRecognizing that all human systems are artificial leads to another equally important perspective for our modern\nage: organizations are becoming like markets. By markets, we mean places where no one person is commanding\neveryone else. With marketplaces, you are free to wander to different vendors, try their wares, and are under no\nobligation to purchase their goods or services. No one is commanding you to buy from Company ABC vs. XYZ\u2014you\nget to decide.\nFor organizations, this means that traditional \u201cmanagement\u201d of individuals by command and control is\nincreasingly becoming difficult in our complex, global world. Reasons for this reduced ability to command are\npartially dependent on globalization. Businesses may be partnered with other businesses where they do not have\nthe ability to directly tell these other companies what to do. The same may be true for world governments. There\nalso may be instances where organizations are competing with one another, perhaps to sell similar goods or services\nto you as a consumer\u2014or perhaps to discover a new idea or innovation.\nCumulatively, these factors mean that organizations will be less able to command individuals or other\norganizations what they would like them to do, and instead have to rely on other mechanisms. These other\nmechanisms include using diplomacy to influence individuals or organizations, being smarter or stronger than\nother competing organizations, or giving rewards to elicit desired behaviors from individuals.\nGeneral implications for a systems innovator\nSo what does this mean for you as a future systems innovator? It means you should be mindful of the\nincreasingly complex environment of our global world. You should seek out the connections and layers among\nsystems. If you spot new connections or uncover a new layer, you may also identify radical innovations. Sometimes\n15 the most important part of being an innovator is having the wisdom to know when to form partnerships and with\nwhom to make friends.\nIt also means you should seek to identify what are the rules of a human system. You should be open to asking\n\u201cwhy\u201d a certain rule is in place and allow yourself to consider what would happen if that rule was changed. What\nwould improve the current rules? Are there rules that no longer help as they previously did? You should also\nrecognize that the ability for organizations to command others is decreasing. As an innovator, this trend is helpful,\nsince it increases the chances for you to \u201cmarket\u201d and spread innovative ideas. This also means you need to consider\nhow to influence and encourage others to adopt your innovations.\nSpecifically, as a systems innovator, you will need to \u201cmarket\u201d your innovation. Simply because you have\nthought of an innovation does not mean it will succeed. If you are not skillful at influencing others to consider and\nadopt your innovation, your innovation may not succeed. Further, you may need to be smarter or stronger than\nother innovators\u2014and you may need to consider what rewards would encourage individuals to adopt your\ninnovation. Sometimes an innovation itself can encourage individuals to adopt it, but this often is not immediate.\nBy their nature, humans are not prone to change if they are relatively happy. Even if your innovation provides new\nbenefits, you may need to consider wants to encourage individuals to shift from their old \u201cways\u201d to your innovation.\nFinally, it means you should recognize that innovation is necessary to deal with change. Change is constant in\nour world, so innovation also needs to be constant. Yes, innovation can be risky as sometimes an idea might not be\nincomplete, not right for the current environment, or not aligned with the needs of an organization. However, there\nis a greater, more certain risk that any system will become outdated without innovations. As a systems innovator,\nyou should search, dream, and reach for the future.\nHow can I innovate?\nIt would be great if it were possible to describe systems innovation as a simple formula. However, this is not the\ncase. Just as modern societies are open to differing views and ideas, there are many ways routes to innovation.\nSometimes, an existing issue or obstacle with a system prevents the achievement of a certain goal. Individuals\nmay brainstorm solutions to this problem and a novel idea will emerge that provides a good fit for removing or\nminimizing the obstacle. In other cases, an individual who is unfamiliar with the obstacle may bring an entirely new\nperspective that leads to an innovative solution. For systems innovators, continuous exposure to new ideas on\ndifferent topics can bring fresh perspectives to familiar issues, thereby triggering new ideas and insights.\nInnovation also necessitates a careful balancing-act between risks versus rewards. Many new ideas promise a\ntremendous payoff and recognition. However, with increasing rewards often comes increasing risk. For example,\nintroducing an entirely new information system to a company\u2019s operations department may hold the promise of\nmaking inventory management more efficient, producing faster product availability, and increased sales. At the\nsame time, the initial implementation of a new information system probably will cause disruption within an\norganization, perhaps in the form of requiring new processes or employee training. When undertaking an ambitious\neffort, it is essential that a systems innovator be aware of the potential downsides and risk factors that will\nundermine success if not adequately addressed. Complex systems often have unexpected consequences, some of\nwhich are likely to be undesirable. Failed innovations are not only time consuming but can be costly and a source of\nembarrassment for a would-be innovator.\n16 While it may seem wise to take the safe route and focus on smaller, seemingly less risky projects, this may mean\naddressing small problems or introducing ideas that have a minimal impact on a system\u2019s performance. For\nexample, rather than addressing inventory management problems directly, simply upgrading the computers that\nrun the inventory management without actually changing the software that manages the processes, might have a\nminimal impact on the core problems. In addition, systems projects can often grow in scope as the project\nprogresses. What started as a small effort might uncover additional requirements or system dependencies,\nprompting a project that started out as a low risk to grow into a longer, larger, more risky endeavor. Systems\ninnovators must balance the reward a potential innovation might provide with the risk that implementation or\nadoption of such an innovation may go awry.\nIn addition, systems innovators should appreciate the importance of appropriate timing. Sometimes innovations\ncan be \u201cahead of its time\u201d or \u201ctoo late.\u201d When designing innovations, it is important to consider environmental\nfactors. An innovation must fit the needs of an organization, market, or society. An innovation introduced out of\nphase can undermine a system and other innovation efforts. Remember our earlier example of a systems innovator\nplanning to design a new cell phone network for 500,000 subscribers. The systems innovator failed to take into\naccount the requirement of future growth of the cell phone network to 2,000,000 individuals in five years. A skilled\nsystems innovator would have planned for both the present and future of his or her designed system.\nFor our modern age, systems innovators can design and create innovation in ways previously unavailable.\nInnovators must insure that their envisioned innovations are appropriate to the environment of today and\ntomorrow. Through technology, there are new ways for individuals to combine ideas for entirely new outcomes.\nThis \u201cre-mix\u201d age allows recombination of systems elements to produce results greater than the sum of the parts.\nWhat do innovations achieve?\nUltimately, any systems innovator is important in what their innovations achieve for organizations and\nindividuals. Thus, it is appropriate to conclude discussion of \u201cBeing a Systems Innovator\u201d with reflections on what\nultimately are the fruits of innovation, and what makes being a systems innovator such an important and essential\nrole for the fast-moving world of the 21st century. For a successful systems innovator, keeping a long-term view on\nthe outcomes achieved from any future innovation is vital.\nFirst, innovations marry insights and existing knowledge to produce new knowledge. Without new knowledge,\nyour organization, be it a business or government, might become irrelevant and outdated quickly. By creating new\nknowledge, innovations are the only sustainable advantage. The present \u201cways\u201d of systems, with time inevitably\nbecome old \u201cways\u201d and outdated. For our modern age, that some individual or organization will eventually identify\nan innovative \u201cway\u201d better than the old \u201cways\u201d is almost certain. Changes happen, and without innovation,\norganizations might become irrelevant quickly. New knowledge also allows your organization to gain positive\nbenefits from previously unforeseen approaches or opportunities. These new approaches can help your\norganization grow or profit. Our world\u2019s future is made by innovations and new knowledge gained from these\nachievements.\nImagine individuals at the dawn of the 1900s. If you could go back in time and tell them about the modern\nworld, what would be the \u201cnew\u201d knowledge you would share with them? What innovations would be the most\nimportant to you? Would you discuss modern jets that travel the global daily? Or would you explain how we have\nsent rockets into outer space and astronauts to the moon? Or would you tell them about the Internet and personal\n17 computers? Or would you talk about our use of antibiotics and modern medicines to treat diseases? What other\ninnovations do you think are most noteworthy?\nNow think about those individuals in the year 1900. Would they even believe some of the innovations you told\nthem? How would they react if you tried to tell them about the ability to share electronic messages with people\naround the world in less than a second? How would you even begin to describe the ability to search for information,\nmusic, or videos on the Internet\u2014recognizing that they did not even have television yet in the year 1900?\nAll of these innovations (and many, many more) occurred in less than 100 years, and our world is moving\nforward ever faster, and with ever more complexity, in our innovations and discoveries. With these innovations\ncomes new knowledge, knowledge we now take for granted in our daily lives. This new knowledge improves our\nability to work more productively, live longer and fuller lives, communicate across large distances, and perform\ntasks in hours that previously took weeks or months to complete.\nInnovation also achieves shared knowledge. For innovations to succeed, they often must share (either within\nyour organization or with the world) insights that one or two people previously may have observed or discovered. If\nyou are a systems innovator and you realize a better way for your company to interact with its customers, you will\nneed to share your idea with others to encourage its adoption. Equally, if you discover an improved way for\nindividuals to manage their email messages, you may incorporate this innovation into a software product that you\nthen make available to others (to buy or for free). The knowledge produced by innovators needs to be \u201cshared\u201d for\ntheir innovations to be truly realized and recognized.\nInnovations achieve new products and profits\nSecond, innovations translate new knowledge into new products and profits, particularly for business (but also\nfor organizations where performing efficiently is important). Even for governments, innovations can allow\ngovernments to save money or do more with the same amount of funds. The radio, the television, the personal\ncomputer, the cell phone\u2014all inventions we take for granted today, were innovations that had to be dreamed of,\nexperimented with, tested, and refined before they could be products and produce profits for businesses.\nInnovations take time and courage to see an idea through to reality. For example, websites like Amazon.com or\neBay.com were once innovative start-up companies with untested ideas. Their different innovative visions were\nbelieved by some, uncertain by several, and publicly dismissed as not possible by several (at the time).\nSystems innovations can produce increased profits for an organization either by producing new products or by\nproducing new ways of doing old activities. Should you accept the challenge of being a systems innovator, you need\nto be in love with not just the new and exciting, but also with understanding the current context and history upon\nyour area of focus. Past and present events provide a context to find innovations.\nIt is the mid 1990s and you are a systems innovator. As a systems innovator, you know that historically most\npeople have to go to a bookstore to buy a book. They have either to call or visit the bookstore to see if it has a\nparticular book, and physical bookstores can only carry a limited number of books. For rare or unique books,\nchances are your local bookstore will not have the product. Equipped with this knowledge of past and present\nevents, you might think about launching a company where people can visit a central website, search through\nmillions of books, and order the book online and have it delivered to their home. Such an innovation became\nAmazon.com, and produced millions of dollars for its founding innovators.\n18 Again, it is the mid 1990s, you are a systems innovator, and you see a trend where hard drives increasingly are\ngetting physically smaller with more storage space. You also notice a new audio compression technology that allows\nentire songs to be compressed into small files (called MP3s). Equipped with this knowledge of past and present\nevents, you might think about building a device that would allow individuals to store MP3s on a portable hard drive\nwith a nice, friendly interface for people to search and find the songs they want to play on this portable device. Such\nan innovation was Apple\u2019s iPod\u2014which included not only a hardware device, but also an information system (a\nwebsite, called iTunes.com) for people to find, purchase, and download the songs they would like to play on their\niPods. This innovation also achieved both a new product and large profits for Apple and its Chief Executive Officer,\nSteve Jobs.\nInnovations increase effectiveness\nThird, related to the earlier two points, innovations increase the effectiveness of individuals and organizations.\nBy effectiveness, we mean how well actions of an individual or organization lead to a desired outcome. If an\nindividual has to do a lot of work to produce only a small amount of a desired outcome, the effectiveness of that\nindividual\u2019s actions is low. Conversely, if an individual has to do minimal work to produce a large amount of a\ndesired outcome, the effectiveness of that individual\u2019s actions is high.\nInnovations can make existing ways of doing activities more effective and thus either more profitable or\nenriching for the participants. Sometimes the art of being a systems innovator is not necessarily about discovering\nsomething completely new, but instead is about \u201crefining\u201d some processes exist and making these processes better\nand more effective. The Internet is full of examples where existing ideas were translated into the digital world and\nmade more effective. Email allows individuals to send electronic messages to each other and receive them in much\nfaster time than it would take to deliver a hand-written message. Computers allow individuals to compose and edit\ndocuments electronically using a word processing program in ways that are much more effective than retyping the\ndocument numerous times and changing revisions manually.\nIndividual improvements in effectiveness can also translate into organizational effectiveness. If a team of people\ndiscovers an innovative way of rearranging how they work together, this innovation may translate into faster results\nor better outcomes for the team. For information systems, innovators are often striving to make not only the system\nwork better and more effectively\u2014but also the organizations of people who interact with the technology also work\nbetter and more effectively.\nNo human system is completely effective and all of our systems have the potential to be improved. As a systems\ninnovator, your mission is to seek ways of increasing individual and organizational effectiveness. You want to\ndiscover innovations that require the minimal amount of work to produce the largest amount of a desired outcome.\nChallenge the unknown, not feasible, or impossible.\nSummary\nAs we have discussed, systems are the object of particular designs. The components, or parts, of a specific\nsystem can be either real or abstract. Components comprise an aggregate \u201cwhole\u201d where each component of a\nsystem interacts with at least one other component of the system. To innovate is to make \u201cimprovements by\nintroducing something new\u201d. A noteworthy innovation must be substantially different, not an insignificant change\nor adjustment. Innovations can be tangible or intangible, radical or incremental.\n19 Systems innovators are individuals who design and implement innovations. To design refers to the process of\ndeveloping a structural plan for an object. Designers seek the requirements and expectations, identify the objectives\nand measurements of success, give structure to the elements, and form to the components of systems. Success or\nfailure hinges on the ability of systems innovators, as designers, to attain the proper requirements and expectations\nof a system.\nAs a systems innovator, you should be mindful of the increasingly complex environment of our global world. You\nshould seek out the connections and layers among systems. If you spot new connections or uncover a new layer, you\nmay also identify radical innovations. Sometimes the most important part of being an innovator is having the\nwisdom to know when to form partnerships and with whom to make friends. You should also remember that all\nhuman systems are artificial. By \u201cartificial\u201d, we mean that human systems would not exist naturally in the world\nwithout humans. Part of innovating is identifying when the rules of a system, be it an organization or information\nsystem, could be modified. You should seek to identify what are the rules of a human system. As a systems\ninnovator, you should be open to asking \u201cwhy\u201d a certain rule is in place and allow yourself to consider what would\nhappen if that rule was changed.\nAs a final important point, systems innovators achieve \u201cmagic\u201d. By \u201cmagic\u201d, we mean that innovations designed\nby systems innovators allow abilities or feats that were previously not possible or realistically feasible. If\ninnovations allow the impossible to be technologically possible, innovations allow \u201cmagic\u201d. New, innovative\ntechnologies often allow such innovation, thereby helping humanity to reshape the natural world.\nHumans have a long history of using new technologies to overcome the physical limitations of human beings.\nFor example, the use of a plow and the irrigation of crops allowed humans to productively farm land and grow\ncrops. By growing crops, humanity began to build settlements (which themselves began to use new technologies\nlike levels, bricks, hammers, and nails). These technologies helped human civilization to grow. With civilization,\nhumanity began to focus on things beyond immediate, short-term survival\u2014to include education. Education is only\npossible because we have technologies that allow other human individuals to grow enough food for individuals\nbeyond themselves. We can go to school because others will work while we are studying and provide the necessary\nresources for our society to function, including running water, electricity, healthcare systems, construction of\nbuildings, transportation systems, and more.\nIn a sense, all the systems that we discussed at the start of this chapter are a result of innovations and human\ntechnologies that have allowed us to reshape our world. Civilization is possible by employing innovative\ntechnologies and systems that allow humanity to think beyond short-term survival and pursue education, research,\nglobal commerce, foreign relations, and even fun recreational activities like books, movies, and television.\nInnovations are \u201cmagic\u201d\u2014they reshape the natural world. Humans use tools to accomplish tasks that were either\nnot feasible or impossible. Innovative tools also increase the effectiveness of systems and individuals. Historically,\nhuman use of tools has allowed us to extend our physical abilities. Now, with information systems, there is the\nunique opportunity for human beings to extend not only their physical abilities, but also our cognitive abilities. Not\nonly can we work better or faster, but also we might be able to think better or faster as an individual or\norganization. All of this will be possible through future innovations.\nInnovative information systems in the last 40 years have already dramatically changed our world, to include\nfaster, global transactions between people and the ability to collaborate and electronically share commerce,\n20 government, or entertainment-related activities with millions of people. Innovative information systems of the\nfuture will achieve what we would label \u201cmagic\u201d today. As a systems innovator, the fruits of your successful\ninnovations will not only produce new knowledge, new products, profits, and increased organizational effectiveness\n\u2014your innovations will also achieve that which previously was impossible or infeasible.\nOur closing advice: search for beneficial, new ideas. Through your efforts, bold innovations will\nproduce the world of tomorrow.\nExercises\n1. You are able to go back in time to visit members of your local neighborhood in the 1900's. What would be the\n\u201cnew\u201d knowledge you would share with them? What innovations would be the most important to you and\nwhy?\n2. You are able to go forward in time to your local neighborhood in the year 2075. What do you imagine, as a\ntime-traveler from the present, would be some of the future innovations that you would observe? How would\nthey change human societies? What innovations do would be the most important to you and why?\n3. You have been hired as a systems innovator to design a new cell phone network for 500,000 subscribers. You\nare wise enough to include the requirement of future growth of the cell phone network to individual\nadditional subscribers. What other requirements might be worth considering when you design the system?\nWhat requirements might influence the success (or failure) of the designed system?\n4. If you could work on designing any innovation, what would it be and why? Would you create something new\nor extend an existing system? What requirements and other concerns would you need to consider in\ndesigning your innovation? What benefits do you think would occur if you could achieve your innovation as\nyou imagine it?\nChapter editor\nDavid A. Bray is currently a PhD candidate at the Goizueta Business School, Emory University. His research\nfocuses on \"bottom-up\" (i.e., grassroots) socio-technological approaches for fostering inter-individual knowledge\nexchanges. Before academia, he served as the IT Chief for the Bioterrorism Preparedness and Response Program at\nthe Centers for Disease Control where he led the technology response to 9\/11, anthrax, WNV, SARS, and other\nmajor outbreaks.\nReferences\nArgote, L., Ingram, P., Levine, J. M., & Moreland, R. L. (2000). Knowledge Transfer in Organizations:\nLearning from the Experience of Others. Organizational Behavior and Human Decision Processes, 82, 1-8.\nClippinger, J. H. (1999). The Biology of Business: Decoding the Natural Laws of Enterprise (1st ed). San\nFrancisco: Jossey-Bass Publishers.\nCummings, J. N. (2004). Work Groups, Structural Diversity, and Knowledge Sharing in a Global\nOrganization. Management Science, 50, 352-364.\nDaft, R. L., & Weick, K. E. (1984). Toward a model of organizations as interpretive systems. Academy of\nManagement Review, 9, 284-295.\n21 Davis, F. D. (1989). Perceived Usefulness, Perceived Ease of Use, and User Acceptance of Information\nTechnology. MIS Quarterly, 13, 319-340.\nDawes, R. M., Orbell, J. M., Simmons, R. T., & Kragt, A. (1986). Organizing Groups for Collective Action. The\nAmerican Political Science Review, 80, 1171-1185.\nGalbraith, J. R. (1982). Designing the innovating organization. Organizational Dynamics.\nHeckscher, C. C., & Donnellon, A. (Eds.). (1994). The Post-Bureaucratic Organization: New Perspectives on\nOrganizational Change. Thousand Oaks, Calif: Sage Publications.\nKling, R. (1991). Cooperation, coordination and control in computer-supported work. Communications of the\nACM, 34, 83-88.\nKling, R. (2003). Reconceptualizing Users as Social Actors IN Information Systems Research. MIS Quarterly,\n27, 197-235.\nKuhn, T. S. (1962). The Structure of Scientific Revolutions. Chicago: University of Chicago Press.\nMarch, J. G. (1991). Exploration and Exploitation in Organizational Learning. Organization Science, 2, 71-87.\nMarkus, M. (2001). Toward a Theory of Knowledge Reuse: Types of Knowledge Reuse Situations and Factors\nin Reuse Success. Journal of Management Information Systems, 18, 57-93.\nNonaka, I. (1994). A Dynamic Theory of Organizational Knowledge Creation. Organization Science, 5, 14-37.\nSimon, H. A. (1969). The Sciences of the Artificial. [Cambridge: M.I.T. Press.\nWade-Benzoni, K. A., Tenbrunsel, A. E., & Bazerman, M. H. (1996). Egocentric Interpretations of Fairness in\nAsymmetric, Environmental Social Dilemmas: Explaining Harvesting Behavior and the Role of\nCommunication. Organizational Behavior and Human Decision Processes, 67, 111-126.\n22 2. 2. Achieving Efficiency\nand Effectiveness through\nSystems\nEditor: Gabrielle Piccoli and Iris Liu (Cornell University, USA)\nReviewer:\nLearning objectives\n\u2022 understand the differences between information systems and information technology\n\u2022 be able to identify the four components of information systems\n\u2022 understand the relationships between the four components of information systems\n\u2022 understand the reasons for having an information system\n\u2022 be able to assess the value of information systems from the financial as well as managerial points of view\nIntroduction\nAn information system is designed to collect, process, store and distribute information. Although information\nsystems need not be computerized, Information Technology (IT) plays an increasingly important role in\norganizations due to the fast pace of technological innovation. Today most information systems beyond the smallest\nare IT-based because modern IT enables efficient operations as well as effective management in organizations of all\nsizes. This chapter will first define the critical components of modern information systems, and then discuss how\norganizations achieve higher efficiency, better effectiveness, and improved coordination through their use.\nWhat is an information system?\nTo understand what an information system is we need to first clearly differentiate it from information\ntechnology\u2014with which it is often confused. Let\u2019s look at example. A manufacturing company with 1,200 employees\nused to pay the employees by checks. At the end of every month, the human resources staff would look at how much\neach employee should be paid and cut 1,200 checks; one for each employee. To collect their pay, the employees\nwould have to go to the human resources office with their employee identification cards. Every employee would\nshow his\/her identification card to the human resources staff so the staff could ensure the checks were given to the\nright employees. Note that, while there is a system managing the payroll, the process of issuing paycheck requires\nno information technology and is completely manual.\nOne day an ill-advised employee visits the human resources office with a fake identification card and obtains the\ncheck which does not belong to him. As a result of the stolen check, the company suffers a loss\u2014having to\ncompensate the employee whose check was stolen.\n23 Reacting to this event, the director of human resources considers installing a system that automates the end-of-\nmonth payment process. The information of employees\u2019 bank accounts will be stored in the system and their pay\nwill be directly deposited into their bank accounts at the end of every month. The objectives of this new information\nsystem is to improve efficiency\u2014saving the human resources staff\u2019s time in manually preparing 1,200 checks and\nverifying employees identification cards 1,200 times a month\u2014and to improve effectiveness of the organization by\nreducing the possibility of lost checks.\nAfter a few months, the human resources director finds out that the human resources staff are still preparing the\nchecks manually for employees every month, and that the employees are still coming to collect their checks in\nperson. In other words, the new technology is not being used. When the director investigates what went wrong with\nthe system, the staff tell him that there is nothing wrong with the system. When an employee\u2019s account number is\ninput, the system will notify the bank to deposit the salary into that account on every pay day.\nHowever, upon further investigation, the director discovers several possible causes for the system\u2019s failure. First,\nthe employees are reluctant to provide their bank account information for various reasons, such as not feeling\ncomfortable with releasing their personal information. Also, when the wrong account number is entered and the\nmoney is deposited into the wrong account, nobody in the human resources department is in charge of contacting\nthe bank to rectify the mistakes. This discredits the new system, and employees who encounter this problem no\nlonger want their salaries directly deposited.\nIT is not information system\nAs can be seen from the above example, information technology and information system are two related but\nseparate concepts. In our example the IT component seems to be working quite well, yet the organization is not\nreaping the benefits of the time saved by human resources staff and employees. In other words, the system fails to\nachieve its objectives due to the failure of other components.\nLet\u2019s look at another example. When the most famous banker in the Ching Dynasty, Mr. Hu Syue-Yan,\nestablished his first bank, Fu-Kang, in the mid-1800s, we can be absolutely sure that there were no computer\nsystems in the bank! At that time, the services a retail bank provided were very similar to those offered today: a\ncustomer could deposit money in the bank and earn interests, borrow money, or remit money orders. All these\nactivities had to be recorded to reflect a customer\u2019s current balance in the bank. That is, an information system\nneeded to be in place in order to keep track of how much the customer deposited, how much the customer\nwithdrew, how much the customer borrowed, and how much the customer transferred into other accounts.\nHow did Mr. Hu Syue-Yan\u2019s employees do so? Relevant information was collected, processed, stored and\ndistributed using pen and paper. Thus, although a computerized technology was unavailable at the time, the bank\u2019s\ninformation system still achieved its goals\u2014enabling the business to serve its customers. Again, here is evidence\nthat information technology is not information system. Even though IT is often at the core of modern information\nsystems, information technology and information system are two different concepts. But what is the difference?\nWhat are the components of an information system?\nThe four components of an information system\nAn information system is defined as a socio-technical system comprised of two sub-systems: a technical sub-\nsystem and a social sub-system. The technical sub-system encompasses the technology and process components,\nwhile the social sub-system encompasses the people and structure components. The critical insight from the\n24 Exhibit 2: The socio-technical system\nexamples introduced earlier is that for an information system to perform and achieve its objectives, all four\ncomponents have to be present and working together. We now define and describe the four components of a\nmodern information system (see Exhibit 2).\nInformation technology\nAs discussed earlier, an information system needs not to use computers. However, modern organizations\nincreasingly rely on information technology as the core of their information systems. We define information\ntechnology to include hardware, software and telecommunication equipment that is used to capture, process, store\nand distribute information.\nHardware is the physical equipment\u2014such as a personal computer, a laptop, a portable computing device, and\neven a modern cell phone\u2014used to process information. Software is the set of coded instructions (programs) that\ndirect the hardware to perform the required tasks. A typical example is Google Docs\u2014a word processing program\ndesigned to instruct a computer to create text documents. Telecommunication systems are the networking\nequipment enabling users and devices to communicate. An example of a telecommunication system is a telephone\nnetwork, which allows two callers to interact by voice over a distance.\nThese three elements\u2014hardware, software and telecommunication systems\u2014comprise the IT component of an\ninformation system. For example, the technology components of the automated payroll system mentioned in the\nfirst example include:\n\u2022 hardware\u2014computers and printers\n\u2022 software\u2014the accounting software application designed to keep track of the salaries and the staff\nscheduling system designed to keep track of hours worked and how much each employees should be paid\n25 \u2022 telecommunication systems\u2014local and inter-organizational channels of communication and routing\nequipment designed to connect the company to the bank for automatic money transfers.\nProcess\nA process is the set of steps employed to carry out a specific business or organizational activity. In other words, a\nprocess maps the set of actions that an individual, a group or an organization must enact in order to complete an\nactivity. Consider the job of a grocery store manager and the process he engages in when restocking an inventory of\ngoods for sale. The store manager must:\n\u2022 check the inventory of goods for sale and identify the needed items\n\u2022 call individual suppliers for quotations and possible delivery dates\n\u2022 compare prices and delivery dates quoted among several suppliers for the same goods\n\u2022 select one or more suppliers for each of the needed items based on the terms of the agreement (e.g.\navailability, quality, delivery)\n\u2022 call these suppliers and place the orders\n\u2022 receive the goods upon delivery, checking the accuracy and quality of the shipped items; pay the suppliers\nNote that there are multiple viable processes that an organization can design to complete the same activity. In\nthe case of the grocery store, the timing and form of payment can differ dramatically, from cash on delivery to direct\ntransfer of the payment to the supplier\u2019s bank account within three months of the purchase. The critical insight here\nis that the design of the process must fit with the other components of the information system and be adjusted\nwhen changes occur. For example, imagine the grocery store manager purchasing a new software program that\nenables her to get quotations from all of the suppliers in the nearby regions and place orders online. Clearly the\npreceding process would need to change dramatically, and the store manager would need to be trained in the use of\nthe new software program\u2014in other words, changes would also affect the people component.\nPeople\nThe people component of an information system encompasses all those individuals who are directly involved\nwith the system. These people include the managers who define the goals of the system, and the users. In the\nopening example concerning the automated payroll system, the people component of the system includes the\nhuman resources director who wants to enhance an efficient and effective payroll process, the human resources\nstaff who maintain the correct employee account information, and the employees whose salaries will be deposited\ndirectly into their account. An analysis of the opening example clearly shows that problems with the people\ncomponent were partly to blame.\nThe critical insight here is that the individuals involved in the information system come to it with a set of skills,\nattitudes, interests, biases and personal traits that need to be taken into account when the organization designs the\ninformation system. Very often, an information system fails because the users do not have enough skills, or have a\nnegative attitude toward the system. Therefore, there should be enough training and time for users to get used to\nthe new system.\nFor example, when implementing the automated payroll system, training on how to enter employees\u2019 account\ninformation, how to correct wrong entries, and how to deposit the salaries into each account should be provided to\n26 the human resources staff. The benefits of the system should be communicated to both the human resources staff\nand the employees in order to build up positive attitudes towards the new system.\nStructure\nThe structure (or organizational structure) component of information systems refers to the relationship among\nthe individuals in the people component. Thus, it encompasses hierarchical and reporting structures, and reward\nsystems. The structure component plays a critical role in an information system, simply because systems often fail\nwhen they are resisted by their intended users. This can happen because individuals feel threatened by the new\nwork system, or because of inherent human resistance to change. When designing a new information system the\norganization needs to be cognizant of the current and future reward system in order to create incentives to secure\nits success.\nRelationships between the four components At this point it should be clear how information systems, while\nenabled by IT, are not synonymous with IT. Each of the four components discussed above can undermine the\nsuccess of an information system\u2014the best software application will yield little result if users reject it and fail to\nadopt it. More subtly, the four components of information systems must work together for the systems to perform.\nThus, when the organization decides to bring in a new technology to support its operation, the design team must\nadjust the existing processes or develop new ones. The people involved must be trained to make sure that they can\ncarry out the processes. If the skills of these individuals are such that they can\u2019t perform the required tasks or be\ntrained to do so, a different set of individuals need to be brought in to work with the system. Finally, the design\nteam must evaluate whether the organizational structure needs to be modified as well. New positions may need to\nbe created for additional responsibilities, and old jobs may need to be eliminated. The transition from the old way\nof doing things to the new system needs to be managed, ensuring that appropriate incentives and a reward\nstructure is put in place. Following is an example that illustrates the interdependence of the four components of\ninformation systems.\nMrs Field\u2019s Cookies (Ostofsky and Cash, 1988), one of the world\u2019s largest snack-food stand franchisors, which\ncurrently owns stores in the United States, Canada, Hong Kong, Japan, the United Kingdom and Australia, was\nstarted by a young mother with no business experience. Debbi Fields started baking when she was a teenager. Her\ncookies were so popular that she decided to open her first store in Palo Alto, California in 1977. When the business\nstarted expending, Randy Fields, Debbi\u2019s husband, believed that it was more important to keep the size of the staff\nsmall in order to enable the decisions making process to be faster and more accurate. He saw information systems\nas a way to avoid expanding staff while growing the business.\nThe system introduced at Mrs Field\u2019s that was used by the store manager on a daily basis was the day planner\nsystem. Every morning, the store manager entered information, such as day of the week and weather condition,\ninto the system. Then, the system computed the projected sales and recommended when the cookies should be\nbaked. The store sales were then periodically entered into the system during the day to adjust the projections and\nrecommendations. Every day, the sales results were sent to the corporate database for review so the headquarters\ncould respond quickly if any store was not performing well.\nThe objectives of building information systems: Having defined what information systems are, we now look at\nthe reasons why modern organizations introduce them.\n27 Efficiency\nEfficiency is often referred to as \u201cdoing things right\u201d In this chapter, we define efficiency in general terms as the\nratio of output to input. In other words, a firm is more efficient when it produces more with the same amount of\nresources, produces the same amount of output with a lesser investment of resource, or\u2014even better\u2014produces\nmore output with less input. The firm achieves efficiency improvements by reducing resource waste while\nmaximizing productivity.\nMore output with the same input\nTo illustrate how organizations can be more efficient by introducing an information system, we provide an\nexample of a hospital using an information system to manage patient information. Without a system to manage\npatients\u2019 personal and historical information, a doctor would need to ask a patient the same questions about\nallergies, family history and the like each time they visit the hospital\u2014even if the patient has been to the same\nhospital and visited the same doctor a number of times before. As a result, the doctor\u2019s time is wasted in asking\nredundant questions each time the patient visits.\nThe most immediate solution to this information management problem is to create and maintain a folder for the\npatient containing their medical history, which is then used by any doctor treating the same patient in the future.\nThe doctor retrieves the patient\u2019s historical information from the folder and saves time asking the same questions\nagain.\nWith this simple information system, a doctor can serve more patients (more output) within the same amount of\ntime (same input). An even higher degree of efficiency can be achieved by using a computerized information\nsystem. That is, doctors enter the patients\u2019 clinical results into a computerized database instead of writing on a\npiece of paper and filing the paper in a folder. When a patient returns to the hospital in the future, a doctor can\nobtain the patient\u2019s information at the click of a mouse. As a result, doctors can serve even more patients, since they\ndo not need to search through all the patients\u2019 folders in order to find the specific information needed.\nSame output with less input\nOne of the main objectives of any organization is to attempt to control costs and reduce the investment\nnecessary to produce its output\u2014in other words, most organizations are constantly trying to become more efficient\nby way of cost reductions. Information systems can help in this regard when they help lower costs, for example\nthrough a reduction in excess inventory, or by eliminating mistakes in operations.\nConsider a grocery store as an example. If the store is able to better communicate with its suppliers, thus placing\nmore recurrent orders for smaller quantities, it can minimize the costs of holding inventory (less input), yet be able\nto maintain the same level of service to its customers (same output). The store manager can also install a system\nthat maintains inventory information. The data entered into the system are the items that are sold in the store and\nthe quantity of these items. Every time an item is sold or ordered, the manager adjusts the quantity of the item in\nthe inventory system. Without this system, the manager has to periodically go around the shop and the storage\nroom to check if any items need to be restocked. After this system is installed, the manager can just look at the\nrecord to identify which items are almost sold out and need to be restocked. This also reduces the input (manager\u2019s\ntime) to achieve the same output (restock all items).\n28 Effectiveness\nEffectiveness is often referred to as \u201cdoing the right thing\u201d. In this chapter, we define effectiveness as the ability\nof an organization to achieve its stated goals and objectives. Typically, a more effective firm is one that makes better\ndecisions and is able to carry them out successfully.\nResponding better to the needs of different customers: An organization can create or refine its products and\nservices based on data collected from customers as well as information accumulated from its operations. In other\nwords, information systems help organizations to understand their customers better, and provide products and\nservices customers desire. Doing so even helps organizations to provide personalized service if the organization\ncollects customer data at the individual level.\nWe can once again use the grocery store as an example. The grocery store can accumulate information about\nwhat customers have been purchasing in the past and analyze this information to find out what items tend to sell\nwell and at what time. The manager can also ask customers what kind of products and services they would like to\npurchase in the future, thereby attempting to anticipate their needs. With this information in hand the grocery\nstore management can order products that will attract customers and stop ordering unpopular products.\nInformation systems can help organizations to improve product or service quality, or maintain the consistency\nof quality. To be able to improve product or service quality or to ensure consistency, organizations need information\nfrom the past as a source of error correction and as a reference point of improvement or consistency.\nWith the information system, which keeps track of the inventory in a grocery store, the manager can identify\nwhich items are popular and which are not. As a result, the manager can reduce the quantity ordered or stop\nordering these slow-selling products.\nIn the same manner, a manufacturing company can collect information from quality control tests so as to\nanalyze the most recurrent problems during the manufacturing process. The company can then find a solution to\nreduce these recurring problems. For example, a furniture manufacturer may find that the majority of the chairs\nproduced do not pass quality control tests. The manager then reviews the results of these quality control tests and\nfinds out that most of them fail because they are unstable. The manager can then look at the machine which\nproduces the chairs and change the specification to rectify the problem. As a result, the quality of the chairs\nimproves.\nThe company can also collect customer feedback on its products and services, and make improvements based on\nthat feedback. For example, a telephone company collects customer feedback on their phone calls and then adds\nservices such as call waiting according to customer suggestions. As a result, the telephone company can deliver\nproducts and services that fit their customers\u2019 needs.\nResponding better to the needs of different employees: An opportunity to improve effectiveness that is often\noverlooked involves better catering to the needs of the firm\u2019s employees. This can be achieved by providing useful\ninformation to employees or faster access to information that helps them to perform their job. An information\nsystem can respond to the needs of employees by collecting data from various sources, processing the data in order\nto make it useful, and finally distributing it according to the needs of employees.\nAnother often overlooked opportunity to use information systems to fulfill the needs of employees is through\nempowerment. Empowerment represents the notion that the organization\u2019s employees can be trusted to take on\n29 more responsibility and make more independent decisions when they are given the information necessary to do so.\nConsider, for example, the employees of a large grocery store who typically receive and stock goods to be\ninventoried. If they have access to the appropriate information, such as original order forms and the invoices, they\ncould be given responsibility to check, accept and even pay for the goods.\nBetter communication and coordination: Coordination is rooted in the ability to share information so that\ndifferent individuals, different departments within an organization, or different organizations are brought together\nto pursue a common goal. Information systems support communication and coordination by better managing the\ndistribution of information.\nCommunication consists in the exchange of information between two points, with the goal of having the\nrecipients understand the sender\u2019s message. Communication is essential to every organization, as communication\namong employees ensures that they work together to carry out internal activities; communication between an\norganization and its suppliers ensures the suppliers provide correct materials for the organization to generate\nproducts and services to sell; and communication between an organization and its customers ensures that\ncustomers understand the products and services they are buying, receive confirmation when transactions occur,\nand are able to resolve problems that may occur encounter purchasing\u2014the after-sale service.\nInformation systems can enhance communication by providing for more, and at times superior, channels. For\nexample, the invention of electronic mail (email) has reduced the use of memos and written correspondence within\nan organization. As a consequence, the speed at which communication takes place improves. Multimedia\ncommunication elements, including images, sound and video files that employ a combination of presentation\nformats (text, graphics, animation, audio, and video) have also improved the richness of communication. These\nmultimedia elements can be attached to an email and the email can be sent to suppliers or clients to better present\nor describe the parts wanted or the products and services provided.\nFor example, a salesperson from a hotel can attach a video clip with an advertising email to better illustrate the\nquality of its guest room. Such attachments can not be done by handwritten correspondence. Thereby, the quality of\ncommunication is improved. The invention of email has also reduced the use of the telephone. Now employees can\nread messages at their convenience without being interrupted by telephone calls while working.\nInformation systems not only improve point-to-point communication, but also within networks, which involves\nmore than two parties. A computer network is a group of hardware (nodes in the network) with links to each other\nso that information can travel among them. A network helps organizations to collect information from and\ndistribute information to different parties (such as suppliers, customers, and partners) in order to receive a more\ncomplete set of information of business activities, which then enhances coordination within the organization. For\nexample, the operation department in an manufacturing company can collect information from the sales and\nmarketing department to find out how many products need to be produced, information from the purchasing\ndepartment to find out the costs of the parts, information from the executives to find out special about product\nchanges, and information from quality control to find out how to improve product design and minimize defects.\nRegulatory compliance\nAt times an organization will introduce information systems that may not improve the organization\u2019s efficiency,\neffectiveness, or enable it to communicate and coordinate better. This happens when regulations and laws require\nthe organization to perform certain tasks\u2014for example, recurrent maintenance on the machinery they use\u2014or\n30 produce some information\u2014for example, tax reports. Regulatory compliance typically requires the organization to\nbe able to create, manage, store or produce information\u2014for example, maintenance logs or financial reports to\ncompute taxes. In these situations the firm will introduce an information system.\nMeasuring the impact of an information system: The measurement of efficiency and effectiveness gives\nmanagers guidelines to assess the value of information systems. Without these measures, managers may be misled\nand make wrong decisions when investing in new technology and designing information systems. On the one hand,\nif the value of the system is underestimated, managers may cut back the allocated resources, which will result in\nforegoing the benefits of the new system. If the value of the system is overestimated, managers are wasting\nresources which could be used in other projects with higher returns. In this section, we introduce several\nestablished methods to measure efficiency and effectiveness improvements (or lack thereof) deriving from the use\nof information system.\nFinancial measures\nA number of financial metrics have been proposed and used over the years to evaluate the costs and benefits\nassociated with information systems implementation. In this chapter, we will discuss two of them: Return on\nInvestment and IS Budgeting.\nReturn on Investment\nReturn on investment (ROI) is the ratio of monetary benefits gained from an investment to the amount of\nmoney invested.\nestimatedbenefit\u2212initialinvestment\nROI=\ninitialinvestment\nROI looks at how the introduction of the information system enables the usage of resources to contribute to the\norganization. The organization can benefit from the introduction of the new system in various ways. First, a new\nsystem can reduce the costs of current operation by increasing efficiency. For example, a business can implement a\nnew system which stores transactional information automatically, therefore saves the labor costs of data entry.\nTherefore, the estimated benefit in the above equation will be the differences in labor costs. Second, a company may\nmodify the current system to take advantage of newly developed technology. For example, a bank which offers\nonline banking can reduce the cost of mailing monthly statements to clients. Therefore, the estimated benefit will\nbe the differences between the cost of mailing the statements before and after the installation of the system. Finally,\na new information system may also support growing business transactions. For example, a retail store may switch\nto an Internet ordering system from a call center to be able to serve more customers. Doing so enables the business\nto respond to more customers at the same time by letting customers browse products and services online and enter\nordering information by themselves. The estimated benefit is the extra revenue generated from online ordering.\nIn all three examples, the initial investment is the cost of bringing in the technology, setting up the new business\nprocesses, training the employees, and organizing the reporting and reward structures. In other word, it is the cost\nof designing, building and implementing the appropriate information system (as defined above). With this\ninformation, we can compute the ROI. An information system with a positive ROI indicates that this system can\nenhance efficiency and\/or improve effectiveness of the organization.\n31 The advantage of using ROI is that we can explicitly quantify the costs and benefits associated with the\nintroduction of an information system. Therefore, we can use such metric to compare different systems and see\nwhich systems can help your organization to be more efficient and\/or more effective.\nThe disadvantage of using ROI is that it may be difficult to justify the causal link between the investment in\ninformation systems and the gained benefits. For example, the extra revenue generated from online ordering may\nnot be due solely to the introduction of the new system. It may be because your product is in the growing phase and\nrapidly increasing in popularity, how can you be sure that you would not have generated the increased revenues\neven without the new online ordering system? As a result, the benefits of the system may be overestimated. On the\nother hand, some customers may browse your products online but still order through the call center; therefore, you\nunder-estimate the benefits of the system. As you can see, it is difficult to distinguish which part of the revenue is\nstrictly due to the introduction of the new system and this will lead to an inaccurate ROI.\nIS budgeting\nThe IS budget provides a reference point of efficiency at the firm level instead of the system level. An\norganization with relative less IS budget when comparing with similar organizations is considered to be more\nefficient since it achieves the same level of services (output) with less resource (input). The advantage of using IS\nbudget as a reference is that the information needed can be obtained relatively easily from financial documentation.\nIS budgets, as measure of efficiency, have some significant limitations however. For one, assuming that two\norganizations are very similar is an over-simplification of reality. Moreover, a firm\u2019s IS budget will depend on the\nsystems it currently has, as well as the one it is currently developing.\nManagerial performance measures\nEffectiveness measures relate to how well a firm is able to meet its business objectives once it is enabled by the\nnew information system, and therefore measures whether the system has improved the organization\u2019s effectiveness.\nInformation usage\nOnce an information system is implemented, the behaviors of acquiring and using information may be directly\ninfluenced. For example, a restaurant manager would not be able to make good staffing decisions without\ninformation about forecasted business. An information system which collects data of past business patterns and\nforecasts future business can provide the restaurant manager with sufficient information to make competent\nstaffing decisions. Therefore, we can measure effectiveness by assessing information usage. Information usage can\nbe evaluated by\n\u2022 the extent to which the system is used\n\u2022 the correlation between the system inputs and the business objectives\nThe system usage can be measured by the amount of queries needed to make managerial decisions. The\ncorrelation between the system inputs and the business objectives should be assessed to ensure the inputs serve\ntheir purpose. For example, an information system would not be effective if it was designed to forecast future\nbusiness, but only allowed the input of supplier information.\nCustomer and employee satisfaction\nInformation systems should also be able to help organizations better respond to the needs of different customers\nand employees. Therefore, we can also assess the impact of information systems by measuring the extent to which\n32 the system improves customer satisfaction, and the extent to which the system fits the needs of employees and\nowners. These measures can be obtained by self-reported surveys.\nSummary\nAn information system, designed to collect, process, store and distribute information, is comprised of four\ncritical components: technology, process, structure, and people. Technology and process represent the technical\nsub-system of an information system, while structure and people represent the social sub-system.\nThe technology component includes hardware, software and telecommunication equipment. A process is a set of\nactions that are designed to carry out a specific business or organizational activity. The people component include\nall of the individuals who are directly involved with the information system. Finally, the structure component refers\nto the relationship among the individuals in the people component.\nThese four components of information systems are interdependent. That is, changes in one component may\naffect the other components. The major reasons of organizations introducing a new information system are to\nenhance efficiency (doing things right), and\/or to improve effectiveness (doing the right thing). Efficiency can be\nenhanced by reducing inputs while producing same or more outputs, or producing more outputs while using the\nsame level of inputs. Effectiveness can be improved by better responding to the different needs of stakeholders. The\nimpact an information system brought to an organization can be assessed from the financial point of view as well as\nfrom the managerial performance point of view.\nCase\nRoyal Hotel\u2019s Espresso! Rapid Response Solution\nThe Royal Hotel in New York City, NY was a luxury all-suite hotel primarily serving an executive client\u00e8le\nvisiting Manhattan on business. These guests were busy and demanding as they used their suite not only as a place\nto sleep but also as a temporary office. The general manager stressed the importance of the high quality of service\ndue to the high percentage of repeat guests. \u201cOur guests are extremely discerning, it is completely unacceptable to\nhave a light bulb out in the bathroom when the guest checks in, particularly if she is a returning guest\u201d, the general\nmanager said.\nTo ensure the extremely high quality of service, the general manager decided to purchase and install M-Tech\u2019s\nEspresso! Rapid Response Solution. With this new technology, the housekeepers could report deficiencies directly\nto the computer, instead of verbally communicated to the maintenance department after they ended of their shift.\nThe housekeepers just needed to dial a special code from the phone in the guest room and an automated attendant\nwould walk them through the reporting process step by step in the language of their choice. Espresso! Then\nautomatically generated, prioritized and dispatches a work order to a printer, fax, or alphanumeric pager.\nTherefore, the new system should be able to reduce the response time as the housekeepers did not have to wait until\nthe end of the shift to tell the maintenance department, and sometimes they even forgot to tell the maintenance\ndepartment. Also, Espresso! had a reporting function so that the management team could obtain information about\nmost frequently occurring or recurring issues, top reporting and completing performers and so on. With this kind of\ninformation, the maintenance department could identify recurrent problems and stop them before they even\noccurred.\nUpon installation, a week of on site training was also offered. The installation and the training session seemed to\nrun smoothly. Employees appeared eager to learn about the new system. However, soon after roll-out the general\n33 manager discovered that the employees had reverted to the old manual system and had rapidly lost interest in\nEspresso!\nCase questions\n\u27a2 What are the elements comprising the four components (technology, process, people and structure)\nof new reporting system?\n\u27a2 Why do you think the new reporting system failed? In other words, which of the four components of\nthe information systems failed to support the goal of the system?\nCase\nLands\u2019 End\u2019s Custom Tailored Apparel Program\nIn October 2001, Lands\u2019 End, a direct merchant of traditionally styled clothing who offers products through\ncatalogs and the Internet, announced its new IT-driven strategic initiatives, a custom tailored apparel program. By\nNovember 2002, 40 per cent of Lands\u2019 End\u2019s web shoppers were buying custom-tailored chinos and jeans, while 20\nper cent of these shoppers were new customers.\nThe concept of this initiative is mass-customization, a process that uses the same production resources to\nmanufacture a variety of similar, yet individually unique products. Experts have found that consumers were willing\nto pay more for custom apparel and footwear. Other than increasing sales, the custom tailored apparel program\nbrought Lands\u2019 End other benefits, including enhancing customer loyalty and lowering the operating costs spent in\ncreating, printing and mailing catalogs. However, withholding catalogs from Internet buyers does not generate\nonline sales. Therefore, sending catalogs at the optimum frequency and pages to keep them apprised of new\nproducts is necessary.\nLands\u2019 End\u2019s proprietary products, strong distribution infrastructure and established brand made the company\nready for this electronic commerce initiative. Also, Lands\u2019 End did not set up a separate Internet division; hence,\navoided internal competition. To manufacture these individually unique garments, Lends\u2019 End partnered with\nArchetype Solutions, Inc (ASI). After customers entered sizing information on Lands\u2019 End website, the orders were\nsent to ASI and software produced electronic patterns and order files for each order, which were then sent via email\nto production facilities in Latin America or Asia. Manufacturers produced, inspected and packed the garments. The\ngarments were shipped to a third-party shipping center in the US and then shipped to consumers. During the\nproduction process, the garments were scanned and the status was updated at each stage of the process. The status\nreport for all orders was sent nightly to Lands\u2019 End. ASI contracted with retailers (i.e. Lands\u2019 End) and\nmanufacturers. Retailers pay ASI a license fee, which include an annual fixed component based on number of\ncategories and a per unit fee. Therefore, both retailers and ASI had the incentive to sell a lot of units. The\nmanufacturers were also required to license manufacturing and tracking software from ASI. Therefore, the\nmanufacturers need to be able to be adept and flexible, and able to learn new technologies fairly rapidly.\nCase questions\n\u27a2 Why did Lands\u2019 End introduce this new information system? What are the benefits this new system\nbrought to Lands\u2019 End?\n\u27a2 How can the executives of Lands\u2019 End assess the financial and managerial performance impact of\nthis new IT-dependent strategic initiative?\n34 References\nCash, J. I., & Ostrofsky, K. (1989). Mrs Field's Cookies. Harvard Business School Case Study.\nIves, B., & Piccoli, G. (2003). Custom made apparel and individualized service at Lands\u2019 End.\nCommunications of the Association for Information Systems, 11, 79-93.\n35 3. Achieving efficiency and\neffectiveness through\nsystems design\nEditor: Per Flaatten (Retired Accenture Partner)\nIntroduction\nIn the previous chapter, you learned that an efficient and effective information system (IS) is composed of\npeople, processes, structure and technology. However, the process by which you can create an IS was not covered.\nThis chapter describes the efficient and effective development of the technology part of an IS; other chapters to\nfollow will describe the activities required for the people, process and structure aspects of the IS.\nThe approach we follow is to first define in general terms the sequence of activities required to go from the\ndecision to create a new IS to its implementation and subsequent maintenance. We then describe the most\nimportant issues or difficulties that you may encounter during the process, based on the experience developers have\nencountered on projects in the past. The rest of the chapter\u2014the bulk of it, in fact\u2014is devoted to describing for each\nactivity possible approaches to resolving the issues and avoiding the difficulties. These approaches are not the only\nones that are possible; those that are mentioned here have been selected because they have been successful in the\npast, are documented in the literature (so you can learn more about them by consulting various reference works),\nand enjoy widespread acceptance in real-life IS departments.\nUnless otherwise indicated, we assume that the IS being developed is a web-based system that processes\nbusiness transactions of some kind, and that the project is of medium size\u2014say 5 to 25 people on the team. This\nmeans that we do not consider the development of websites that are purely informational, nor for personal\nproductivity, nor that result in a software product for sale to individuals or organizations.\nDevelopment process: from idea to detailed instructions\nWhat the development process essentially does is to transform the expression of an idea for an IS\u2014a problem to\nbe solved, an opportunity to be taken advantage of\u2014into a set of detailed, unambiguous instructions to a computer\nto implement that idea. The biggest problem is that computers are excessively stupid and will only do what they\nhave been told to do. For example, suppose you create a billing program for an electric utility and specify that bills\nmust be paid within a certain time or the customer\u2019s electricity will be cut off. Suppose further that a customer\nreceives a bill stating that he owes USD 0.00 (he might have a previous credit). Contrary to a manual system where\nall the work is done by humans, a computerized system may well treat this bill as any other bill and insist on\npayment; it may even send a signal to the customer relations department to cut off power for non-payment. To\navoid this, explicit instructions must be included in the billing program to avoid dunning for amounts of less than a\ncertain limit.\n36 Exhibit 3.:Waterfall model\nSystems developers must therefore pay attention to an excruciating amount of detail\u2014not only when business\ngoes on as normal, but anticipating all the exceptions that may arise. The exceptions may in fact amount to several\ntimes the work required for normal cases. The system then becomes, through sheer accumulation of details, more\nand more complex. This complexity is in itself a source of difficulty\u2014it becomes hard to \u201csee the forest for all the\ntrees,\u201d to keep your eye on the big picture of the business benefits to be achieved, while at the same time making\nsure that every technical and business detail is right and finding out what went wrong whenever something does go\nwrong\u2014as it invariably will.\nFrom the earliest days of computer technology, the method for developing information systems has addressed\nthe need to proceed from the general to the ever more detailed. The first well-known effort at formalizing the\nprocess came in 1970, in an enormously influential paper by W. W. Royce describing the waterfall model of the\nsystems development life cycle.1 Every author on systems development bases his or her work on some variation of\nthis model, and we, too, have our favorite, depicted in Exhibit 5.\nThe work products or \u201cdeliverables\u201d to be created during systems development start with the business case, the\nformal description of the rationale for developing a system in the first place, the results to be achieved and a cost-\nbenefit analysis detailing planned development costs (often treated as an investment) as well as operational costs\nand savings. The business case is often considered part of project management rather than the development\nprocess per se; we include it here because it is the developers\u2019 best tool for not losing sight of the essential\u2014the end\nresult they are trying to achieve. As development work proceeds, developers make choices: one of the main factors\nin deciding which alternative to pick is the impact of the choice on the business case. For example, a choice that\nincreases benefits may also increase costs: is the trade-off worth it? As a result, the business case must be\nmaintained all along the life of the project as decisions are made.\nThe next deliverable in the waterfall approach is the information system\u2019s requirements. Requirements come in\ntwo flavors: functional requirements\u2014what the system should do: processing, data and media content\u2014and quality\nrequirements\u2014how well the system should do it: performance, usability, reliability, availability, modifiability and\nsecurity.\n1 Royce, Winston W. \u201cManaging the Development of Large Software Systems,\u201d Proceedings of IEEE WESCON. August 1970.\n37 The business objectives documented in the business case and the requirements, especially the quality\nrequirements, dictate the architecture or overall shape of the information system being developed. The architecture\ndescribes the major components of the system and how they interact. It also documents design decisions that apply\nto the entire system, so as to standardize the solutions to similar design problems occurring at different places.\nArchitectures or elements of architecture can be common to many applications, thus saving the project team time\nand money and reducing the amount of risk inherent in innovating. For example, many sales applications on the\nweb use the concept of a \"shopping cart\", a temporary storage area accumulating product codes and quantities\nordered until the customer decides that she has all she wants, at which point the application gathers all the\nproducts, computes the price, and arranges for payment and delivery. Shopping cart routines are available\ncommercially from several sources.\n\u2022 The design of the information system is at the heart of the development process. The design is in two parts:\n\u2022 A description of how the system will appear to its users. This is called the external design or functional\ndesign.\nA description of how the system will be operate internally, largely hidden from users. This is called the internal\ndesign (or technical design), and it lays the foundation for programmers to create the code which the hardware will\nexecute.\nThe functional design specifies the interaction between the users and the system\u2014what actions the user can take\nand how the system will react; it describes the inputs and outputs (screens, reports, messages exchanged with other\nsystems); it establishes the different databases and their structure; and it shows at least a storyboard of the media\ncontent (text, graphics, photos, audio\/video clips, etc.).\nThe technical design inventories the programs and modules to be developed, and how processing flows from one\nto the other. It also takes the architecture one step further in the implementation of some of the quality attributes,\nsuch as data integrity, fallback operation in case the system is unavailable, and recovery and restart from serious\nincidents. Finally, here is where any routines to create the initial data and media content required on day one of the\nnew system\u2019s operation.\nCode is the technical name for the programming statements and database specifications that are written in a\nlanguage that can be understood by the technology. Creating code is a highly self-contained activity; the details\ndepend on the environment and the subject will not be treated further in this chapter.\nThroughout these steps, the initial idea for the system has been transformed into a set of computer instructions.\nEach step is performed by human beings (even if assisted by technology in the form of development tools) and is\ntherefore subject to error. It is therefore necessary to conduct tests to make sure that the system will work as\nintended. These tests are organized in reverse order from the development activities: first, the code is tested,\nmodule by module or program by program, in what is called unit tests. Next come string tests, where several\nmodules or programs that normally would be executed together are tested. Then follow integration tests, covering\nall of the software and system tests, covering both the software and the people using the system. When the system\ntest has been completely successful, the information system is ready to for use. (Some organizations add an\nadditional test, called acceptance test, to signify a formal hand-over of the system and the responsibility for it from\ndevelopers to management. This is especially used when the software is developed by a third party.)\n38 To put the importance of testing into perspective, note that it statistically consumes just about 50 per cent of the\nresources of a typical IS department.\nWith the system test complete and the initial data and media content loaded, the information system is put into\nproduction, and the development team is done. Right? Wrong! In fact, the most expensive part of the development\nprocess, called maintenance, is yet to come.\nNo sooner has a new system started to operate than it requires changes. First, users quickly discover bugs\u2014\nerrors in how the system operates\u2014and needed improvements. Second, the environment changes: competitors take\nnew initiatives; new laws and regulations are passed; new and improved technology becomes available. Third, the\nvery fact that a new system solves an old problem introduces new problems, that you didn\u2019t know about\nbeforehand. To illustrate this, take the standard sign at railway level crossings in France: Un train peut en cacher un\nautre (\u201cOne train may hide another one\u201d.). This caution alludes to the fact that you don\u2019t know what lies beyond\nyour current problem until you have solved it\u2014but then the next problem may take you completely unawares.\nIn theory, every change you make must go through a mini-life cycle of its own: business case, requirements,\narchitecture, design, code and test. In reality, only the most critical fixes are done individually and immediately.\nOther changes are stacked up and implemented in periodic releases, typically a release of minor fixes every month\nand a major release every three to six months. Most often, the maintenance phase is performed by a subset (often\naround 25 per cent) of the initial development team. But since the maintenance phase is likely to last for years\u2014\ncertainly more than ten and not infrequently 20 years, the total cost of maintenance over the life of a system can\neclipse the cost of initial development, as shown in Exhibit 6.\nExhibit 4.: Total costs of a system\n(The increasing maintenance cost towards the end of the system life is due to the fact that the more a system has\nbeen modified, the harder it is to understand and therefore to make additional modifications to.\nIssues\nIn this section, we will describe the difficulties that designers have historically had (and to some extent continue\nto have) in performing their tasks. These difficulties will help explain the widely accepted approaches, and some of\nthe more innovative ones, that are the subject of the bulk of the chapter.\n39 Cost\nThe first and most apparent issue with systems development is one of cost. From the earliest days, systems\ndevelopment has been seen as a high-cost investment with uncertain returns. It has always been difficult to isolate\nthe impact of a new business information system on the bottom line\u2014too many other factors change at the same\ntime.\nThere are two components to total cost: unit cost and volume. Unit cost can be addressed by productivity\nincreases. Volume can only be reduced by doing less unnecessary work.\nSystem developer productivity was the earliest point of emphasis, as evidenced by counting lines of code as a\nmeasurement of output. (Lines of code is still a useful measure, but not the most critical one.) Both better computer\nlanguages and better development tools were developed, to a point where productivity is no longer the central issue\nof systems development. It is generally assumed that a development team is well trained and has an adequate set of\ntools.\nReducing the amount of unnecessary work is a more recent trend. Unnecessary work arises from two main\nsources: \u201cgold plating\u201d and rework.\nGold plating refers to the tendency of users to demand extras\u2014features that they would like to have but that do\nnot add value to the system. What is worse, developers have tended to accept these demands, mostly because each\none seems small and easy to implement. The truth is that every time you add a feature, you add to the complexity of\nthe system and beyond a certain point the cost grows exponentially.\nRework becomes necessary when you make an error and have to correct it. If you catch the error and correct it\nright away, no great damage is done. But if the error is left in and you don\u2019t discover it until later, other work will\nhave been done that depends on the erroneous decision: this work then has to be scrapped and redone. Barry\nBoehm has estimated that a requirements or architecture error caught in system testing can cost 1000 times more\nto fix than if it had been caught right away2. Another way of estimating the cost of rework is to view that testing\ntakes up an average of 50 per cent of the total initial development cost on most projects, and most of that time is\nspent, not in finding errors, but correcting them. Add the extra cost of errors caught during production, and the cost\nof rework is certainly over one-third and may approach one-half of total development and maintenance costs.\nAnd this is for systems that actually get off the ground. A notorious study in the 1970s concluded that 29 per\ncent of systems projects failed before implementation and had to be scrapped (although the sample was small\u2014less\nthan 200 projects). These failures wind up with a total rework cost of 100 per cent!3 More recently, Bob Glass has\nauthored an instructive series of books on large systems project failures4.\nSpeed\nIn more recent years, concerns with the speed of the development process have overshadowed the search for\nincreased productivity. If you follow the waterfall process literally, a medium-to-large system would take anywhere\nfrom 18 months to three years to develop. During this time, you are spending money without any true guarantee of\nsuccess (see the statistics on number of failed projects above), with none of the benefits of the new system accruing.\n2 Boehm, Barry W. Software Engineering Economics. Prentice-Hall, 1981.\n3 GAO report FGMSD-80-4, November 1979\n4 Glass, Robert L. Software Runaways and Computing Calamities. Prentice-Hall, 1998 and 1999.\n40 It is a little bit like building a railroad from Chicago to Detroit, laying one rail only, and then laying the second rail.\nIf instead you lay both rails at once, you can start running reduced service from Chicago to Gary, then to South\nBend, and so one, starting to make some money a lot earlier.\nAnother factor that increases the need for speed is that the requirements of the business changes more quickly\nthan in the past, as the result of external pressure\u2014mainly from competitors but also from regulatory agencies,\nwhich mandate new business processes and practices. Eighteen months after your idea for a new system, that idea\nmay already be obsolete. And if you try to keep up with changes during the development process, you are creating a\nmoving target, which is much more difficult to reach.\nComplexity\nOne of the main characteristics of information systems is that they are large, made up as they are of hundreds or\nthousands of individual components. In an invoicing subsystem, you might have a module to look up prices, a\nmodule to extend price by quantity, a module to add up the total of the invoice, a module to look up weight, a\nmodule to add up weights and compute freight costs, a description of the layout of the invoice, a module for\nbreaking down a multi-page invoice, a module for printing... Each module is quite simple, but still needs to be\ntracked, so that when you assemble the final system, nothing is forgotten and all the parts work together.\nCompounding this is the fact that each module is so simple that when somebody requests a change or a refinement,\nyou are tempted to respond, \u201cSure, that\u2019s easy to do\u201d.\nAnd even though the components may be simple, they interact with each other, sometimes in unanticipated\nways. Let us illustrate with an example\u2014not taken from the world of IS, but relevant nonetheless. A large company\ninstalled a modern internal telephone system with many features. One of the features was \u201ccall back when\navailable.\u201d If you got a busy signal, you could press a key and hang up; as soon as the person you were calling\nfinished his call, the system would redial his number and connect you. Another feature was \u201cautomatic extended\nbackup\u201d. This feature would switch all the calls that you could not or would not take to your secretary, including the\ncase where your line was busy. If your secretary did not respond, the call would be sent to the floor receptionist, and\nso on, all the way to the switchboard, which was always manned. (This was in the era before voicemail.) The\nproblem was of course that the backup feature canceled out the call-back feature\u2014since you could never actually get\na busy tone.\nThe effect of interaction between components in a business information system are often in the area of and\nquality requirements described earlier, such as performance, usability, reliability, availability, modifiability and\nsecurity. None of these requirements are implemented in any one component. Rather, they are what are called\nemergent properties in complexity theory. For example, an important aspect of usability is consistency. If one part\nof the system prompts you for a billing address and then a shipping address, other parts of the system which need\nboth should prompt for them in the same sequence. If you use a red asterisk to mark mandatory fields to be filled in\non one screen, then you shouldn\u2019t use a green asterisk or a red # sign on another screen. Neither choice is wrong\u2014it\nis making different choices for the same function that reduces usability.\nFinally, a critical aspect of complexity is the difficulty of reliably predicting system behavior. This means that\nyou cannot be content with designing and coding the software and then start using it directly. You first have to test\nit to see whether it actually does behave as predicted (and specified). This test must be extremely thorough, because\nerrors may be caused by a combination of conditions that occur only once in a while.\n41 Unpredictability also applies to making changes to the system. This means that once you have made a change (as\nwill inevitably happen), not only must you test that the change works, but you must also test that all those things\nthat you didn\u2019t want to change continue to work as before. This is called regression testing; how to do it at\nreasonable cost will be discussed later.\nTechnology and innovation\nOne of the main drivers of information systems development is to take advantage of technological innovation to\nchange the way business is done. As an example, take how the emergence of the Internet changed business\npractices in the late 1990s allowed new businesses to flourish (Amazon.com, Google, and eBay spring immediately\nto mind) and, to a lesser extent, existing businesses to benefit.\nHowever, for every success, there were many failures. Every innovative venture carries risk, and while many dot-\ncom failures were due to a lack of solid business planning, others failed because they could not master the new\ntechnology or tried to use it inappropriately. (Bob Glass\u2019s books refered to previously is filled with horror stories\nillustrating these dangers.) The problem is that if the technology is new, there are no successful examples to follow\n\u2014and by the time these examples show the way, it may be too late, since others, the successful adventurers, may\nhave occupied the space you would like to carve out for yourself. The difficulty, then, is to know how close to the\nleading edge you want to be: not too close, or you might be bloodied; and not too far behind, or you\u2019ll be left in the\ndust.\nA related problem is that of change saturation. A mantra dear to business authors is \u201creinventing the\norganization\u201d. This may be good advice, but an organization cannot keep reinventing itself every day. Your most\nimportant stakeholders\u2014customers, employees, even shareholders\u2014may get disoriented and no longer know what\nto expect, and the organization itself may lose its sense of purpose.\nAlignment on objectives\nAny system development project is undertaken for a reason, usually to solve some operational difficulty (high\ncosts, long processing delays, frequent errors) or to take advantage of a new opportunity (new technology, novel use\nof existing technology). However, many stakeholders have an interest in the outcome. Workers may resist\ninnovation, regulators may fear social consequences, management may be divided between believers and skeptics,\ndevelopment team members may be competing for promotions or raises etc. If the objectives are not clearly\nunderstood and supported, the new system is not likely to succeed\u2014not the least because the various stakeholders\nhave different perceptions of what constitutes success.\nAdoption\nOnce a new system has been created, the next challenge is to make people\u2014employees, customers\u2014use it. In the\npast, back-office systems such as billing, accounting and payroll were easy to implement. The users were clerks who\ncould be put through a few hours or days of training and told to use the system; they had no choice. Today\u2019s system\nusers may be less pliable and may refuse to go along or protest in such a way that you have to change the system, or\neven abandon it. As an example, Internet customers may \u201cvote with their feet,, i.e. go to another website that\nprovides the same service or goods at a better price or more easily.\nAnother example of how things can go wrong was recently provided by a large hospital organization that had\ncreated at great expense a system for physicians and surgeons. It was based on portable devices that the physician\nwould carry around and on expensive stationary equipment at the patients\u2019 bedsides and in nursing stations. Three\n42 months after the launch, it became apparent that practically all the physicians refused to use the system, and it had\nto be uninstalled, at the cost of tens of millions of dollars.\nThe issue of user adoption will be covered in more detail in Chapter 5, \u201cSystem Implementation\u201d.\nUseful life\nThe final issue we will consider is how to plan for a system\u2019s useful life. This is important for two reasons. First,\nas with any investment, this information is used to determine whether a new system is worthwhile or not. If you\nhave a system that is expected to cost USD 5 million and bring in USD 1 million per annum, you know that the\nsystem must have a useful life of at least five years.\nSecond, planning the useful life of a system gives you at least a chance to decide how and when to withdraw or\nreplace the system. Most development projects do not address the issue of decommissioning at all. As a result,\nsystems live for much longer than anyone would have imagined. This is how so many systems were in danger of\ncrashing on the first day of the year 2000\u2014none of the developers had imagined that their systems would last so\nlong. A perhaps more extreme example is that of the United States Department of Defense, which is reputed to have\nhad more than 2,200 overlapping financial systems at one time5. Efforts to reduce this number have not been very\nsuccessful, proving that it is much harder to kill a system than to create one.\nOverall development strategy\nBefore we go into the various techniques and tools that are specific to each of the steps in the systems\ndevelopment life cycle, let us first look at the overall approach. The issues that you must address when you choose\none approach over another have already been outlined above: development projects take too long to pay back; by\nthe time they are implemented, the needs have changed, and the time elapsed before errors and shortcomings are\nexperienced in operation make corrections costly. On the other hand, the system may require a critical mass of\nfunctionality before it can be truly useful, and a rapid implementation of a part of the system may do no good.\nFinally, If the system you are building is truly innovative, especially in its use of technology, the risks of failure are\nhigh and special precautions must be taken.\nIterative development\nIterative development is the standard approach today. It is characterized by the following:\n\u2022 a series of short (3-6 month) development cycles, allowing for quick feedback from experience gained with\nthe working system\n\u2022 Each cycle delivers some significant, useful functionality.\n\u2022 The early cycles focus on \u201clow-hanging fruit\u201d\u2014functionality which is cheap to develop and has a high\npayback. Early successes give credibility to the project and enables you to do more difficult things later.\n\u2022 The experience with the functionality in one cycle allows you to adjust the system in the next cycle.\n\u2022 The project management style is called \u201ctime-boxing\u201d: each iteration has a deadline for implementation\nwhich cannot be exceeded. If some planned functionality takes too long to develop, you postpone it to the\nnext cycle instead of delaying the cycle itself.\n5 San Francisco Chronicle. May 18, 2003\n43 \u2022 Maintenance follows initial development in a smooth transition (see the section on Maintenance later in\nthis chapter). In fact it is difficult to tell when development ends and maintenance starts.\nExhibit 5.: Iterative development\nThe first version of iterative development was called Rapid Application Development, created by James Martin\nin the 1980s in response to the emergence of CASE (Computer-Aided Software Engineering) tools6, and the\napproach has remained tool-intensive. Iterative Development also goes under the name Incremental Development,\nemphasizing that functionality is added gradually.\nAlternative approaches: \u201cBig Bang\u201d\nOccasionally, the circumstances of a given project may dictate that you use a \u201cbig bang\u201d approach, where all the\nfunctionality of the planned system has to be delivered at the same time. The London Stock Exchange underwent\ntwo major transformations, one in 1986, when computerized and phone technology displaced face-to-face trading;\nand another one thirteen years later, when the phone technology was phased out, at least for the top 100 stocks.\nThese two \u201cbig bangs\u201d were successful, but both carried a built-in transition period where old and new systems\ncoexisted.\nIn other cases, you may be building a brand new facility and want to use new technology, such as the\ncomputerized baggage handling at Denver International Airport in 1995. This system resulted in an unmitigated\ncatastrophe and had to be scrapped at least temporarily.\nThe last temptation to adopt a \u201cbig bang\u201d approach may be when you want to create an integrated enterprise-\nwide system, managing all the transactions and management data of an entire enterprise. Hardly anyone attempts\nthis anymore after the consistently unsuccessful attempts in the early days of database management systems\n(around 1970) and Information Engineering (1985-1900).\nIn summary, rather than cede to the temptation of delivering everything at once, do your utmost to find a\ngradual approach, where you can learn as you go and change direction as dictated by reality.\n6 Martin, James. Rapid Application Development. Macmillan, 1991.\n44 Alternative approaches: Prototyping\nExhibit 6.: The spiral approach\nPrototyping is a variation on iterative development, used mostly for very innovative projects, those where the\nrisk of failure is the greatest. Mostly, it is used with new technologies that requires new architectures. The basic\nprinciples of this approach were documented in an article by Barry Boehm, and illustrated by the following\nschematic which gave the apt name \u201cSpiral Methodology\u201d to the approach7.\nThe principle behind prototyping is similar to iterative development, with the following two exceptions:\n\u2022 Instead of starting with low-hanging fruit, start with the highest risk area. This enables you both to avoid\nspending large amounts on unfeasible systems and to learn the most from your errors, by making them\nearly.\n\u2022 Be prepared to throw away version 1 and even possibly version 2 of your system. The purpose of the\nprototype isn\u2019t to provide business functionality so much as it is to learn what not to do.\nRequirements\nOnce you have decided on the approach to the development project, the next step is to find out in detail what the\nsystem should do\u2014and, assuming you have opted for iterative development, what should take the highest priority.\nThen you need to formulate these requirements so that they can be used to drive the project, in particular when the\nrequirements change in midstream.\nRequirements elicitation\nWhen you are eliciting system requirements, focus on what functions the system should perform, what data is\nrequired to support those functions, and how important the quality requirements\u2014performance, reliability,\nusability and flexibility\u2014are likely to be.\nIf the system you are planning is primarily intended to solve operational problems (high cost, high error rates,\nunresponsiveness, customer complaints...), you should spend some time getting familiar with the current\n7 Boehm, Barry W. \u201cA Spiral Model of Software Development and Enhancement.\u201d IEEE Computer, 21, No 5\n(May 1988): 61.\n45 operation, focusing on those people who actually experience the problem (as opposed to those to whom the\nproblem or its symptoms are reported). This approach was systematic in the early days of information processing\nand was called \u201cAnalyzing the Current System.\u201d (One of the additional advantages of analyzing the current system\nwas that it allowed the project team to learn about the business while doing something productive. Project teams\ntoday usually have to meet the expectation that they already know the basics of the business.)\nThere is less emphasis on the current system today, especially if the motivation for the system you want to build\nis to take advantage of technology to exploit a new opportunity. In that case, you want to focus on finding people\nwho have a vision of what could be done, preferably in a group setting, where creative ideas are likely to flow.\nInterviewing is by far the most common technique used in reviewing a present system. It relies heavily on such\nactive listening skills as open-ended questions, appropriate words and phrases, acceptance cues, and restatement at\nboth the direct and emotional level. Used properly, silence can also be effective.8\nThe most widespread group technique is probably Joint Application Design. JAD was developed in the early\n1980s to overcome some of the difficulties caused by the more classic approaches to requirements analysis.9\nThe technique itself consists of gathering highly competent users (about ten) for a two-four day workshop to\ndiscuss, conceptualize, and analyze. IS personnel are present, mainly to listen, to record what is being said, and to\ndocument the users\u2019 contributions. The workshop is led by an independent person to whom neither the users nor\nthe IS personnel report. The role of the leader is twofold: to elicit decisions by consensus and compromise rather\nthan by fiat or majority vote and to keep the meeting on track so that the system to be built is the one that is\ndiscussed.\nA particularly effective way of documenting what is being said is to create a prototype of the application on the\nfly. When you hear a user express an idea, it is particularly effective to be able to say, \u201cIs this what you mean\u201d? as\nyou illustrate a screen layout, mouse clicks or navigation paths on a computer (projected on a large screen for\neverybody to see). The group can then react, critique, and build on what is shown.\nThere are a couple of traps you need to avoid with group techniques. The first is called \u201cgroupthink\u201d and consists\nin the group losing its critical sense and going along with absurd or unworkable ides\u2014partly because it is difficult\nfor a group member to appear to be the only one who is against some idea. The other danger is related: a group may\nbecome overenthusiastic and push for \u201cgold-plating,\u201d features and functions that look nice but serve no useful\nfunction\u2014or not useful enough to be worth the cost. Don Gause and Gerald Weinberg have written an excellent and\nthought-provoking book on the subject.10\nFinally, if the system you are planning is going to be used by outsiders (consumers, customers, suppliers, the\ngeneral public) you need to devise a way to make their voices heard, for example by surveys or focus groups.\n8 See for example: Gildersleeve, Thomas R. Successful Data Processing Systems Analysis. Ed 2. Prentice-Hall,\n1985.\n9 Joint Application Design. GUIDE Publication GPP-147. GUIDE International, 1986\n10 Gause, Donald C., and Gerald M. Weinberg. Exploring Requirements: Quality Before Design. Dorset House,\n1989.\n46 Requirements prioritization\nIt is impossible to satisfy all the requirements that a group of users and other stakeholders may come up with,\nespecially if you are doing iterative development. The decision to include one set of requirements and not another is\ndifficult. First, different stakeholders are interested in different results. Second, two requirements may be mutually\ncontradictory; for example, higher performance may require more powerful hardware at greater cost. Third, some\nrequirements may need to be satisfied before others can be implemented. The process needs to be seen as both fair\nand responsive to stakeholders\u2019 wishes.\nIn some cases, a straightforward economic analysis such as return on investment may be the right approach.\nThis usually works best for requirements in the aggregate and less well for each detailed requirements. The\ndifficulty is tying each individual requirement to a specific benefit and a specific cost. It is also a tedious job when\nthe number of requirements is large. Thus, a prioritization scheme based on pure economics is best suited for the\nbig-bang approach to systems development.\nA similar approach, but based on risk (highest risk first) is appropriate for the prototyping approach to very\ninnovative projects.\nAn alternative approach is Quality Function Deployment. This approach (introduced by Toyota in the 1970s to\nimprove their process for designing automobiles)11 sets up a series of matrices describing how well each\nrequirement contributes to fulfill each business objective, then how each design feature (developed at a later stage)\nimplements each requirement, and so on, thus illustrating qualitatively where the most benefit will be had. The\nunique part of this approach is that a group of stakeholder is asked to rank features pairwise\u2014\u201cI prefer B to A, C to\nB, C to E....\u201d The end result is a series of ranked requirements where all the participants feel that their voice has\nbeen heard, thus helping to build consensus.\nFunctional requirements formulation\nThe requirements need to be documented unambiguously, to minimize later differences in interpretation. There\nis no standard format: text, schematics, and even prototypes can be used.\nThe most useful guideline is to document the requirements in plain language as testable statements. For\nfunctional requirements, this is pretty straightforward, for example: \u201cAfter the customer has tried unsuccessfully to\nenter a Personal Identification Number (PIN) three times, the system will terminate the transaction, send an alert\nto the Security Department and retain the customer\u2019s card.\u201d This statement can now be cross-referenced to the\ndesign (in what module is this function implemented?) and to the test data (where are the instructions to the tester\nto try three wrong PINs and how is the result of this test documented?).\nPlain language is useful for rigor and completeness; however, for any but the smallest systems, plain text\nbecomes cumbersome and confusing. It is hard to communicate to users what the system will do and how it will\naccomplish its overall objective, and asking them to take a position on whether everything is covered is unfair. In\nparticular, you are likely to hear, later on, comments of the type, \u201cI assumed that this or that function was\ncovered...\u201d or, \u201cIt goes without saying that...\u201d If these comments come as responses to processing errors once the\nsystem has been implemented, it is too late.\nTo address this problem, information systems professionals have developed a range of graphical representation\ntools, the three most widely used of which are decomposition diagrams (also known as Warnier charts after their\n11 Hauser, John R., Don Clausing. \u201cThe House of Quality.\u201d Harvard Business Review, May 1988.\n47 advocate, a French analyst named Jean-Dominique Warnier), data flow diagrams (DFDs), and entity-relationship\ndiagrams (ERDs).\nWarnier charts, an example of which is shown in Exhibit 7, show how a function can be decomposed into\nsubfunctions. In turn, each subfunction can be further subdivided, all the way down to an elementary level. (The\nsame hierarchical decomposition technique can be used in many other contexts, for example in outlining an essay.)\nThe technique has the advantage of being easy to grasp and easy to use. However, it is not suitable for all purposes.\nIt essentially can depict only a static structure, which must, in addition, be composed of mutually exclusive and\ncompletely exhaustive elements\u2014MECE in analyst jargon. In other words, the decomposition must not leave\nanything out and no element can be used in more than one place.\nExhibit 7.: A Warnier chart\n48 Exhibit 8.: A data flow diagram (DFD)\nDFDs depict a flow of activities whereas the Warnier chart technique depicts static structures. An example is\nshown in Exhibit 8. Data flow diagrams are more powerful than Warnier diagrams but are a little more difficult to\ngrasp and to draw correctly. They are best adapted to repetitive administrative work: processing a pile of customer\norders that have come in through the mail, picking the orders from inventory, delivering them, mailing invoices and\ncollecting payments.12 (This was how most information systems were organized before the days of computers and in\nthe early period of information technology; it is still used by utilities such as phone and electric companies for\nbilling and payments; it goes under the name of batch processing.) As a result, DFDs are no longer as widely used as\nbefore. A variation that remains in widespread use is a diagram that shows the flow of activity down the page which\nis divided in columns, one for each organizational entity or system that intervenes in the processing of a single\nfunction. (This type of chart has been called \u201cswimlane diagrams.\u201d)\nERDs have a different purpose from Warnier charts and DFDs. They depict data structures: entities that\ncorrespond to things about which the system maintains data, such as customers, products, and orders; and\nrelationships between them, such as the fact that each order belongs to one customer, but each customer may place\nseveral orders, and that each order consists of several products. In addition to identifying the entities and\nrelationships, the model identifies the main attributes, or pieces of data, to be stored for each entity. An example is\n12 Gane, C., and T. Sarson. Structured Systems Aanalysis: Tools and Techniques. IST, Inc., 1977\n49 shown in . The ERD is an invaluable tool not only to point out dependencies and relationships among pieces of\ndata, but also for the actual design of relational databases.13\nExhibit 9.: ERD diagram\nOther techniques are available for more specialized purposes, and you can also, if you have a specific need,\ndevelop your own. Bear in mind that you may then have to invest in training other people to use your homegrown\ntechnique.\nGraphical techniques are indispensable tools for communicating and explaining requirements. But that is all\nthey are. If a given technique falls short of this objective, do not use it. Above all, do not get caught up in the\nformalism and the stringent rules that some authors prescribe if these rules interfere with the main purpose. (But\ndo remember that you are communicating not only with users and management, but also with your colleagues, who\nmay have very specific expectations about the techniques you have selected.)\nQuality requirements formulation\nThe formulation of quality requirements (performance, reliability, usability, flexibility...) should also be\ntestable. This implies most often that you set a measurable, numeric goal, for example: \u201cThe system must be\navailable to end users at least 98 per cent of the time in any given week\u201d. This cannot necessarily be tested directly\nbefore the system goes live. However, your design must include a way to keep track of whether the performance\nobjective is being met.\nThe Software Engineering Institute has developed an entire design methodology driven by quality requirements\nthat explains this process in detail (reference required).\nArchitecture\nAn architecture of any kind of complex man-made system, such as a cathedral or an office park, describes the\noverall structure of the system: its major components and how those components interrelate.\n13 Chen, Peter. The Entity-Relationship Approach to Logical Data Base Design. QED Information Sciences,\nInc., 1977.\n50 The architecture of a business information system can be defined as the infrastructure and interfaces that enable\nsystem components (such as hardware, software and network communications) to work together to accomplish the\nsystem\u2019s purpose. The architecture is defined for the system as a whole, rather than for each of its components. In\nfact, the infrastructure and interface standards are usually shared among multiple systems. For example, all the\nweb-based systems in an organization, whether Internet or intranet, might share the same architecture.\nGenerally, the system architecture is imposed on a project team by the organization\u2019s Information Systems (IS)\ndepartment. Depending on the needs of the business system, the project team may request changes to the\narchitecture, which are then typically implemented by a separate architecture team led by the organization\u2019s\nsystems architect described in Chapter 3.\nSometimes, a new architecture is needed. This typically happens when an organization decides to use technology\nin a novel way to take advantage of a business opportunity. In such cases, a new architecture may be developed at\nthe same time, or slightly ahead of, the business information system. A brand-new architecture can carry\ntremendous risks and should only be undertaken after careful consideration by the systems architect.\nImportance of architecture\nCould you build an information system without an architecture?\nYes, it is possible to build small, isolated information systems without a formal architecture, just as it is possible\nto build a log cabin without one. But as soon as you want to build a building which is larger or has multiple\ncomponents\u2014an energy efficient home, an apartment complex, an office high-rise\u2014you need an architecture to\nshow where the electric wiring, the plumbing, heating and air conditioning, the stairs and the elevators should go\nand how they should work together.\nThe main role of a system architecture is to help manage the complexity and size of modern business\ninformation systems. The architecture embodies important design decisions that have already been made. This is a\nconstraint on the team, which is not free to make decisions that run counter to the architecture, but it also means\nthat there is some kind of support within the organization\u2014including knowledgeable people, development\nguidelines, reusable code, and implementation experience with the architecture\u2014making the team\u2019s job easier.\nThe fact that many design decisions have already been taken has the following advantages:\n\u2022 Risk reduction: first, the use of proven components and interfaces reduces the number of unforeseen\nproblems arising during system development and operation; second, technological innovation is easier to\ncontrol, because the architecture pinpoints the main risk areas, isolating them from what remains routine.\n\u2022 Integration: well-architected systems are easier to integrate with each other and with legacy systems,\nbecause the architecture defines and describes in some detail the points at which two systems are allowed\nto interact.\n\u2022 Productivity: an architecture is an investment\u2014the stronger the architecture, the higher the productivity of\nthe development team after an initial learning curve. Developers can in a certain sense \u201creuse\u201d the technical\ndecisions made by the system architect and less time is spent analyzing and testing alternative solutions.\n\u2022 Reliability: the use of proven components and approaches that have already been successful in the specific\nenvironment in which you are working reduces the total number of errors and makes it easier to find those\nerrors that do occur.\n51 \u2022 Maintainability: standardized design and construction makes it easier for maintenance teams to correct,\nenhance and extend them to a changing business and technical environment.\n\u2022 Predictability: an established architecture has well-known technical characteristics, making it relatively\neasy to analyze and predict performance, reliability, usability and other quality measures.\nSelecting, extending or creating an architecture\nOn projects where both the business functionality and the technology are well understood, the project team will\nbe best served by adopting an existing architecture, with minor modifications where needed. If the system is small\nenough, the architecture may not need to be specified in much detail. For example, a system may be simply defined\nas being web-based: this implies a host of technical decisions, as will be illustrated in later chapters.\nOf course, if some element is absent from the architecture needed for a project, the team may be free to add it.\nThis increases the risk and the cost, since no support will be available from the system architect and her team.\nWhere a new development project is driven by business innovation, it is best to adopt an existing, robust\narchitecture. This reduces the development risks by not adding technical risks to the business risks.\nOn projects driven by technology\u2014when someone has had a novel idea for applying technology in a way that has\nnot been done before\u2014you may well need a new architecture altogether. This architecture must at least be designed\nbefore the requirements process goes too far. In most cases, it will need to be developed and tested on a pilot\nproject, to verify that the technology is workable and will help solve the business problem.\nFinally, note that most of the systems \u201cdevelopment\u201d activity of existing IS departments is in fact maintenance.\nMaintenance requests very rarely have an impact on the system architecture.\nIn summary, in most cases, the system architecture will be given at the outset of a development project, or at\nleast in its very early stages\u2014certainly before requirements gathering is complete and before design has gone very\nfar.\nDesign\nTo design a system is to decide how the requirements formulated as described in the previous section and\napproved by all the stakeholders are going to be implemented. The design process has two main focal points: the\nfuture users of the system (external, functional, or user design) and the IS professionals who will code and test it\n(internal or technical design).\nExternal design\nThe first question of a design is, how will the users interact with the system?\nPhysical interaction\nPhysically, the interaction takes place via some device. In the generic case assumed in this chapter, this will be a\npersonal computer hooked up to a network (often the Internet, but sometimes to the organization\u2019s internal\nnetwork). The devices that are available are then a keyboard with a mouse, a screen with a given resolution, and a\nprinter, most often based on inkjet technology. When the user population is the general public, you must be\nprepared to accommodate a variety of technical solutions: some users may have a lower screen resolution, some\nmay have slow printers or no printers at all. The operating system and the browser\u2014the software that enables the\nPC to communicate with the Internet\u2014may also vary from user to user. You must therefore be careful to use the\n\u201clowest common denominator\u201d, the minimum capability likely to be available to all users.\n52 Alternative means of interaction are available. Historically, users communicated with computers via forms that\nwere filled in and transcribed to paper tape or cards in the form of patterns of holes. Early on, machine-readable\ndata formats were developed such as optical character recognition (OCR) and magnetic ink character recognition\n(MICR). These had severe technical constraints but are still widely used in specialized applications such as check\nprocessing and invoice payments (where the account number and invoice amount are printed in OCR font on a slip\nthat the customer detaches from the invoice and sends in by mail with the payment). The retail industry developed\nan industry-wide standard Universal Product Code (UPC) in the 1970s, embodied in bar codes on grocery packages,\nin books and on most other products. Similar in principle (but technologically quite different) are the magnetic\nstripes on the back of credit cards (often extended to membership cards that enable checking in and out as well as\ndispensing membership services). These forms of interaction require some specialized equipment, ranging from a\nhand-held scanning wand to an automated teller machine (ATM).\nMore recent developments include using touch-tone telephones for data entry. Automated response systems\ntake phone callers through a series of menus to direct their calls either to a person with whom they can interact or\nto voice messages (either generic information useful to any customer or customer-specific messages, such as a\ncredit card balance, generated on the fly). It is also possible for the caller to be asked to enter a series of digits\u2014a\ncustomer number, a password or Personal Identification Number\u2014or even alphanumeric text. Internet applications\nare emerging that are specifically designed to interact with mobile phones that use video and audio as well.\nAnother recent development is the use of Radio Frequency Identification (RFID), miniature transponders\nlocated in products, library books, electronic toll collection devices in cars and trucks, and cattle, to mention but a\nfew uses.\nFor some very large systems, the development of a new input\/output device may even be justified, as with the\nhand-held, wireless package tracking devices used by Federal Express.\nIn all cases, you must address two issues. First, how can the users get access to the interaction device? Are they\nlikely to have a PC or a cell phone? Are they likely to adapt to using them as interfaces? (Cell phone texting may not\nbe optimal for a system catering to retirees, nor may PCs with landlines connected to the Internet be the most\nappropriate solution in remote, non-electrified regions.) Alternatively, if the users are the organization\u2019s own\nworkforce, what is the cost of equipping them? Second, whenever a user interacts directly with an information\nsystem, there is always room for doubts about the identity of that user, so you must decide what level of user\nauthentication is necessary and how you can achieve it. There exists technology that is solely devoted to this\nauthentication, such as fingerprint and retina scanner; other, frequently used solutions rely on password or PINs.\nInteraction flow\nThe interaction flow follows directly from the functional requirements, and in particular the process\ndescriptions or schematics (DFDs or \u201cswimlane\u201d diagrams) described in section 5 above. To revert to our default\nexample of a web-based application using a PC with a mouse, a keyboard, a VDU and a printer, you need to develop\ndetailed layouts of screens and printouts (you can use presentation software such as Powerpoint to mock up these)\nand descriptions of what the system does in response to each possible user action\u2014entering text, clicking on radio\nbuttons, check boxes and navigation arrows, or pressing the Tab or Enter keys, for example. For each item that the\nuser can act on, also called a control, you describe all the possible user actions and for each of those, the system\u2019s\n53 processing and possible responses. (This is called by some a Control-Action-Response, or CAR, diagram, an\nexample of which is shown in Exhibit 4.)\nExhibit 10.: CAR diagram\nFor each process, you have a choice between two basic interaction styles, system-driven and user-driven.\nSystem-driven systems take usually either a question-and-answer approach or a menu-and-forms approach. Take\nas an example an income tax filing application. One approach is to have the system ask a series of questions in a\nsequence that is determined by the designer, prompting successively for the taxpayer\u2019s name, ID number, marital\nstatus, income from salary, taxes withheld, and so on. This is a very appropriate method for an individual taxpayer,\nwho does this only once a year and whose knowledge of the intricacies of what you have to report and on which\nform is limited.\nFor a tax preparer with multiple clients, a menu-and-forms approach may be better. The user can choose which\nform to fill in from a menu; in response, the system displays a screen image of the form and the user can fill it in,\nusing the Tab key to move back and forth between fields and character, arrow, backspace and delete keys to fill in\neach field on the form.\nA third form of system-driven interaction, often neglected because the interaction itself is minimal, is batch\nprocessing. The best example comes from periodic applications such as utility billing or bank statement printing. In\nthese examples, the processing is initiated by a timer (every night at 1:30, or every Monday, etc...). An entire file or\ndatabase is processed without human intervention and the result is distributed by electronic (email) or physical\n(\u201csnail\u201d mail) when the processing is complete. A useful trick for weekly or monthly applications that print out large\nvolumes of data is to organize the process by slicing the input file into segments\u2014as many segments as there are\ndays in the period. For instance, for a monthly application, you could print out all the customers whose name begin\nwith A one night, B the second night, and so on. This is called cycle billing and is in general use.\nOther types of applications may be best served by a user-driven approach. One example is Internet navigation,\nwhere the user can look up information (for instance in an on-line product catalog) by navigating freely from web\npage to web page, following links or invoking embedded search functions. As he or she finds products that are of\n54 interest, they are put in a shopping cart; at the end, when the user wants to confirm the order, the application takes\non a system-driven character again, to enter delivery, invoicing and payment data.\nAnother example of a user-driven application is the spreadsheet. The user is given a variety of tools and\nfunctions: putting them together step by step enables him or her to complete the task at hand, whether it is to\ncompute loan amortization calendars or organize the music collection of a church choir. Spreadsheet software is in\nfact so flexible that it is questionable whether to call it an application at all; it is more like a tool. In fact, many\norganizations create applications from spreadsheet software by distributing pre-formatted templates: budgeting\nand expense tracking are often treated this way. In this case, you could argue that the spreadsheet software is like a\nprogramming language and the template is the true application.\nA final consideration is that of user identification and authentication. Most networked applications deal with\nresources that must not be tampered with\u2014whether it is selling products or giving access to confidential\ninformation. In these cases, any user should be identified\u2014who is it?\u2014and authenticated\u2014is the user really who he\nor she says? The usual procedure is to assign the user an identifier the first time he or she uses the application. This\nidentifier is then stored by the system and permits subsequent activities. In addition, associated with the identifier\nis a password that only the legitimate user is to know. When logging in, the user supplies both the identifier and\npassword. The combination of the two, if correct, authenticates the user who is then authorized to use the\napplication.\nThe choice of style and flow thus depends on the application, but also on the user population. An inappropriate\ninteraction style can cause a system to fail, especially if the users of the system are outside your control (e.g. the\ngeneral population) and have a choice whether or not to use the system. This is a little less critical if the system is\naimed at the employees of your organization, but a poorly designed interface can only have detrimental\nconsequences: loss of productivity and lack of cooperation to cite only two.\nThe following are some guidelines for good interface design. The list is not exhaustive, but it contains some hints\nthat you may not otherwise have though of; user interface design is a vast subject and you should reference the\nample literature before starting.14\n\u2022 Do not consider yourself a surrogate user, thinking, \u201cThis is how I would do it if I were to use the system\u201d.\nYou are not. You have a different educational and professional background. Get help from actual users to\nevaluate your design.\n\u2022 Be consistent. Use the same controls and actions to accomplish similar functions. If you use feedback cues\nsuch as icons, color, fonts, sounds, etc..., make sure they are associated systematically with the same actions\nthroughout. For instance, if you want to use a garbage can icon to discard unneeded data, then make sure\nyou always use the same garbage can icon, in the same place on the screen, and with the same \u201cOops\u201d\nfunction.\n\u2022 Provide \u201cOops\u201d functions at all points where it seems useful. Users are often afraid to use system functions,\nespecially those they do not use very often, because they are afraid that the consequence of an error may be\nto break the system. Consistently having \u201cOops\u201d or Undo functions available makes the system much more\nusable.\n14 See for example: Shneiderman, Ben, and Catherine Plaisant. Designing the User Interface: Strategies for Effective Human-\nComputer Interaction (4th Edition). Addison-Wesley, 2004.\n55 \u2022 Pay attention to users who have some physical impairment, such as color blindness, vision or hearing\nimpairment, only one arm...Do not rely exclusively on color or sound to convey information.\n\u2022 Make it very clear when a transaction, a data base update, or other irrevocable action has been performed,\nat what is called a commitment point.\n\u2022 Do not overcrowd screens and reports, but don\u2019t commit the opposite error either: you do not want to\ndevote a whole screen to a single item of data.\n\u2022 Avoid distracting graphics and decorative elements, especially for high-usage systems. \u201cPizzazz\u201d may be fun\nthe first time, but users quickly get tired of it.\n\u2022 Pay special attention to error processing. Make sure the user understands what is wrong and which steps to\ntake to correct it. Anticipate that the user will try to perform unexpected actions and design the interface to\nreact accordingly.\n\u2022 Let the user choose the identifier and the password with as few constraints as you can. This is especially\nvalid for external customers and occasional users. You can be more demanding with frequent users,\nespecially within your own organization\u2014assigning an ID rather than letting the user choose, forcing a mix\nof alphabetic and numeric characters for the password, making the user change the password periodically.\n(To force this discipline on occasional users\u2014who may not use the system for months at a time\u2014is\ncounterproductive, because he or she is likely to compromise password security by writing it down.)\nData\nIn the typical case, data design consists of deciding how to organize the permanent data maintained by the\napplication into a relational data base, made up of a series of tables\u2014usually one for each entity identified in the\nentity-relationship model during the requirements analysis. Each line (record, tuple) of a table describes one\nparticular occurrence of the entity type. For instance, in a customer table, there will be one record per customer;\nand each of these records will contain attributes or data items describing that customer: name, address, telephone\nnumber...\nEach attribute is described with respect to its functional definition, its value domain, its physical representation\non various media, and any restrictions on the operations that can be performed. This documentation decreases the\nnumber of programming errors and data base integrity problems during implementation and operation of the\nsystem.\nFor each entity, a key is chosen to identify it uniquely. A good key has no meaning outside of identifying the\nentity uniquely. In addition, a good key never changes its value for any given entity over the entire life of that entity.\nFor each relationship, an implementation is chosen. In most cases, relationships are implemented via foreign\nkeys: an attribute of the entity is the key of another entity, serving as a pointer. For example, an order entity\ncontains the customer number of the customer who placed the order. Then, when the order is processed, the\napplication can access the customer data (for instance, to ascertain the delivery address).\nOne of the most important criteria for a good data design is that the data items (attributes) are assigned to the\nright table (entity). The objective is to avoid storing data redundantly. For example, in the order process outlined\nabove, you want to store the customer\u2019s delivery address in the customer table rather than in each of the customer\u2019s\norders in the order table. This isn\u2019t so much to save space as to insure consistency. If the same piece of data is\n56 stored in several places, there is a risk that the values at some point will come to differ. Then, you are faced with the\ndilemma of choosing one, but which one?\nThe process of assigning attributes to the correct tables is called normalization. There is a set of precisely\ndefined steps and criteria that you can follow to make this process almost foolproof. (Need reference here). The acid\ntest is to run down all the functions of the system and ask yourself whether any of them would require updating the\nsame piece of data twice.\nEven though the occasion of all this work is the implementation of a single system, one of the main purposes of\ndata base design is to ensure coherence of definitions and consistency of data values across all the systems of an\norganization. Thus, much of this work is ideally performed by a data administration section, which covers the\nactivities of the entire IS department, not only the current project. If there is no data administration section, then\none of the analysts on the project plays the same role, covering only the project.\nMedia content\nThe last aspect of external design that we\u2019ll cover is media content design. This refers to the design of\napplications whose main purpose is to convey information to the application users, whether it be products for sale,\npress releases, academic articles or similar content.\nMedia content can be in several different forms: text, still images, animations, video or audio, or any\ncombination of the three. Designing the content and the presentation of the information is the affair of\ncommunications specialists. Your responsibility as a system designer is primarily to provide for the storage and\nretrieval of each component as needed. You must also decide how the media content will be maintained (say, by\nchanging the photographs when a product is modified) and how to ensure that all the navigation links available to\nthe user work correctly. It is easy to radically modify the content of a page; it may not be so easy to locate all the\nother pages that point to the one you\u2019ve modified, to see whether those links still make sense. It is even worse when\nyou suppress a page without suppressing the links to it, leaving the user high and dry in the middle of navigating\nthe web.\nInternal design\nThe internal, or technical, design of an information system is the description of how the system will be\nimplemented. Most of the work done here is intimately dependent on the architecture adopted, but we can identify\na few general principles that apply across the board. This will be the simplest and the most useful if we look at the\nend result of the design rather than the process to get there,\nThe internal design identifies and describes all of the components of the system: hardware, software,\ncommunications protocols. For each component, it describes which functions are fulfilled by, or allocated to, that\ncomponent. The decomposition\/allocation should meet the following criteria:\n\u2022 Completeness. Each functional requirement (see section 5 above) is allocated to a component.\n\u2022 High cohesion: Each component performs functions that belong logically together. For example, extending\nand totaling the lines of an invoice and the computation of sales tax arguably belong together; checking a\nuser\u2019s password does not.\n\u2022 Low coupling: Each component interfaces as simply as possible with other modules. For instance, an on-\nline ordering application might have a module to process credit card payments. The invoicing program\n57 should just communicate the invoice amount to the credit card payment module, which, after processing\nthe credit card data entered by the user should simply indicate to the calling module whether the payment\nwas accepted or not.\nOne particular strategy of allocation is worth bringing out: object orientation. Whereas traditional systems\nallocate functions to modules based on the processing flow, object orientation uses the data structure as the\ncriterion. Each object (corresponding to an entity in the entity-relationship diagram or a table in a relational\ndatabase) is packaged with its code The functions that such an object can perform are typically to modify the value\nof an attribute (say, the customer address) or to send the value of an attribute back to the requesting program. The\napplication still needs to implement a flow of processing (for example, log in, followed by browsing of the catalog,\ndepositing items in a virtual shopping cart, checking out and paying). The object-oriented paradigm can be made\nsystematic by considering each such process as an object in itself, even though it is not an entity in the entity-\nrelationship diagram.\nObject orientation has the advantage of making it easy to meet all three of the criteria enumerated above. The\nstructure of the system is simpler and more reliable than traditional systems design. The disadvantages are that\nperformance may be a problem and that the approach is less familiar, especially for IS professionals of a certain\nage.\nCode\nCompleting the design process results in a series of detailed specifications. The translation of these\nspecifications into a form that can be understood by the technological components of the system\u2014PCs servers,\nmainframes, communications gear... This is called coding. (It used to be called programming, but it entails in fact\nmore than writing programs\u2014for example, database specifications).\nHow this is done is specific to each environment and will therefore not be further described here. Each\nprogramming language has its own peculiarities, and each IS department has its own habits; the best organized\ndepartments have in fact established coding standards to be followed by all team members. This has the advantage\nof making work more interchangeable: if everybody uses the same style, work on one component can be taken over\nby anyone. Maintenance is also greatly facilitated by standards.\nTest\nWhy we test\nWe test a software program, an application, an entire business information system to detect errors\u2014things that\ndon\u2019t work the way they are supposed to. If the system doesn\u2019t work properly, we find out why and fix it (or\nabandon it, if we judge it too expensive to fix).\nWe test other things, too. New airplanes are tested before they are declared airworthy. Before buying a new car,\nyou probably want to test-drive it. In the United States, the Food and Drug Administration mandates extensive\ntests, lasting several years, of any new medication before it can be prescribed or sold over the counter. Sometimes,\nthe tests are insufficient, as with the case of the X-ray equipment that would, under certain circumstances, irradiate\npatients with 1,000 times the normal dose; as a result, patients died. In other cases, it isn\u2019t possible to test a product\nunder realistic conditions: the Lunar Excursion Module could not be tested on the moon before the real moonshot.\n58 Software\u2014beyond a few hundred instructions\u2014becomes so complex that its behavior cannot be accurately\npredicted just by inspecting it. Errors creep in through inattention, misunderstanding, or other human weaknesses.\nJust as no one would want to fly an untested plane, no one would want to use untested software.\nTesting is a part\u2014a critical one\u2014of a more general process called software defect removal.15 (Other techniques\ninclude inspection, prototyping, and automated syntax checking.) Even used in conjunction, these defect removal\ntechniques cannot guarantee defect-free software. The most thorough of tests cannot address all the possible values\nof input data against all the possible states of the database. A defect may remain latent for several years until some\nhighly unlikely combination of circumstances reveals it. Worse, the test itself may be defective: after all, the test,\ntoo, is designed and executed by fallible human beings.\nIn traditional systems development methodology, testing is seen as a discrete activity, which occurs once\ndevelopment is substantially complete. This view causes problems, because the cost of correcting errors becomes\nexponentially larger the more time passes during which the error remains undetected. An error that is discovered\nby its author as it is committed can be corrected immediately, at no cost. A requirements error discovered during\noperation may invalidate the entire system\u2014and carry business interruption costs in addition. Experts estimate\nthat correction costs can be a hundred times higher when the error is discovered after the system has been\ndelivered. The reason for these high costs are that the longer the time between committing an error and discovering\nit, the more work will have been done and therefore have to be corrected or scrapped and recreated.\nThe V model of verification, validation, testing\nBefore explaining how testing is done, let us place it in a wider context. At about the time that the IS community\ndiscovered the high cost of error correction, the United States Department of Defense devised a set of processes to\ncontrol the work of contractors on large, complex weapons systems procurement projects. Because of the increasing\nsoftware content of modern weapons systems, these processes were naturally adapted to the world of software\ndevelopment. They also put testing in proper perspective as one means among many to ensure software quality.\nBecause of the shape of the graphical representation of these processes, the name V model was coined. An\nexample of a V-model that applies to business information systems development is depicted in Exhibit 9.\n15 Dunn, Robert. Software Defect Removal. McGraw-Hill, 1984.\n59 Exhibit 11.: A V model\nThe left-hand side of a V depicts activities of decomposition: going from a general objective to more and more\ndetailed descriptions of simpler and simpler artifacts (such as program code in a business system). The right-hand\nside depicts integration: the assemblage into larger and larger components of the individual pieces of code created\nat the bottom angle. As each assembly or subassembly is completed, it is tested, not only to ensure that it was put\ntogether correctly, but that it was also designed correctly (at the corresponding level on the left-hand side of the V).\nThe final characteristic to be noted is that each phase corresponds to a hand-off from one team to another (or\nfrom a contractor to a subcontractor, in the original Department of Defense version of the V model). In the case of\nsystems development, what is handed off is a set of deliverables: a requirements or design document, a piece of\ncode, or a test model. The hand-off is only allowed to take place after an inspection (verification and validation)of\nthe deliverables proves that they meet a set of predefined exit criteria. If the inspection is not satisfactory, the\ncorresponding deliverables are scrapped and reworked.\nLet us define verification and validation. Verification checks that a deliverable is correctly derived from the\ninputs of the corresponding activity and is internally consistent. In addition, it checks that both the output and the\nprocess conform to standards. Verification is most commonly accomplished through an inspection. Inspections\ninvolve a number of reviewers, each with specific responsibilities for verifying aspects of the deliverable, such as\nfunctional completeness, adherence to standards, and correct use of the technology infrastructure.\nValidation checks that the deliverables satisfy the requirements specified at the beginning, and that the\nbusiness case continues to be met; in other words, validation ensures that the work product is within scope,\ncontributes to the intended benefits, and does not have undesired side effects. Validation is most commonly\n60 accomplished through inspections, simulation, and prototyping. An effective technique of validation is the use of\ntraceability matrices, where each component of the system is cross-referenced to the requirements that it fulfills.\nThe traceability matrix allows you to pinpoint any requirement that has not been fulfilled and also those\nrequirement that need several components to be implemented (which is likely to cause problems in maintenance.)\nDetailed descriptions of various do\u2019s and don\u2019ts of inspections are available from most systems analysis and\ndesign textbooks, as well as from the original work by Ed Yourdon. (reference here)\nTesting, the third component of the V model, applies to the right-hand side of the V. The most elementary test\nof software is the unit test, performed by a programmer on a single unit of code such as a module or a program (a\nmethod or a class if you are using object-oriented programming). Later on, multiple units of software are combined\nand tested together in what is called an integration test. When the entire application has been integrated, you\nconduct a system test to make sure that the application works in the business setting of people and business\nprocesses using the software itself.\nThe purpose of a unit test is to check that the program has been coded in accordance with its design, i.e., that it\nmeets its specifications (quality definition 1\u2014see Chapter 1). It is not the purpose of the unit test to check that the\ndesign is correct, only that the code implements it. When the unit test has been completed, the program is ready for\nverification and validation against the programming exit criteria (one of which is the successful completion of a unit\ntest). Once it passes the exit criteria, the program is handed off to the next phase of integration.\nThe purpose of integration testing, the next level, is to check that a set of programs work together as intended by\nthe application design. Again, we are testing an artifact (an application, a subsystem, a string of programs) against\nits specification; the purpose is not to test that the system meets its requirements. Nor is the purpose to repeat the\nunit tests: the programs that enter into the integration test have been successfully unit-tested and therefore are\ndeemed to be a correct implementation of the program design. As for unit tests, the software is handed off to the\nnext level of testing once the integration test is complete and the exit criteria for integration testing are met.\nThe purpose of system testing, the next level, is to check that the entire business information system fulfills the\nrequirements resulting from the analysis phase. When system testing is complete and the exit criteria for system\ntesting are met, the system is handed off to IS operations and to the system users.\nHow to test\nPrepare the test\nLet us start with unit testing. To unit-test a program, you: first create test cases. A test case is the description of a\nunique combination of inputs and data base states. The secret to good, efficient testing is to plan the test cases so\nthat each is unique and together they are exhaustive. In practice, you develop unit test cases from the program\nspecification to make sure that all the processing rules and conditions are reflected in at least one test case. You\ncreate a separate test case for each state of the data base that could have an impact on how the program works. Pay\nspecial attention to boundary conditions, those that are abnormal or at the edge of normality. For instance, when\nprocessing a multi-line order, pay special attention to the first and the last lines of the order. Include orders that\nhave one line, or none at all, or a very large number. Consider also abnormal cases, such as that of trying to create\nan order for a customer who is not recorded on the data base. Create different orders for different types of\ncustomers. Consider what happens for the first order of the day, when the order file is empty, or after a heavy day,\nwhen the order file becomes very large. Create order lines with very small and very large amounts. Are negative\n61 amounts accepted? It is as important for a program to raise the proper exceptions for erroneous inputs as it is to\ncheck that valid data is processed correctly.\nOnce you believe the test cases are complete, cross-reference them against the program specification to ensure\nthat each specification is addressed.\nOnce the test cases have been created, prepare the test scaffolding, the environment in which the test will be run.\nIf the program is a user interface program, most of the test data needs to be laid out in a script for the user to\nfollow. If not, input or transaction files must be loaded. This can be done with a test data load utility or it may\nrequire a specially written program. Next, the test data base (including web site content, if applicable) must be\ncreated. The best way to do this is to have the project team maintain a master database of shared test data from\nwhich each programmer can extract relevant tables, rows and files , modifying them where necessary for the\npurpose of the test. (You cannot use the database directly, since your program is likely to update it, thus creating\nunexpected conditions for everybody else.) If the project team doesn\u2019t have a common test data base, create your\nown, as you did for input and transaction data.\nNext, consider what stubs and drivers you need to test your program. In many cases, a program needs to\ncommunicate with other programs, some of which may not have been created yet. A stub is a small program that\ncan be called by the program being tested and can simulate the services rendered by the called program. A driver\nworks the other way around: if your program is designed to be called by a program that doesn\u2019t exist yet, and you\nmust write a simple program that activates yours.\nAnother useful tool is one that does screen capture. This tool records keystrokes and screen display changes; it\nenables you to repeat exactly the same test and to compare its output before and after you make modifications.\nBefore running the tests, you should prepare the expected results for each test case in each test cycle. Most\ndevelopers do not do this. They believe that it is sufficient to run the test and review the output to see if it is correct.\nThis is insufficient, for two reasons. First, people have a propensity to see positive results. Subconsciously, the brain\nwill often rationalize an error and tend to think that the result is actually correct when it isn\u2019t. Second, preparing\nexpected results is an excellent way to review the completeness and relevance of the test data.\nIdeally, you would load the expected results on to a file in the same format as that produced by the test. Then,\nthe comparison between expected and actual results can be done electronically\u2014a much more reliable approach\nthan trusting human faculties. A keystroke\/screen capture facility comes in handy here, especially for programs\nthat are part of the user interface. But even if you don\u2019t have such a tool, comparing a predetermined expected\nresult against an actual one is a lot more reliable than just viewing the actual result to decide whether it is correct.\nPreparing expected results is time-consuming and not much fun. Ultimately, however, by avoiding false positive\nresults and by making it easier to repeat the same tests after correction, you save time and increase quality.\nHowever, since the cost is incurred earlier than the benefits materialize, project management has a tendency, when\nput under time pressure, to shortcut this indispensable investment.\nExecute the test and record the results\nWith all the preparation that has gone on in advance of the unit test, executing it and recording results is as\nsimple: just do it.\n62 Well, not quite. If you do not have a scripting tool and you are testing at the user interface, you must observe an\niron discipline of recording what you do and how the system responds, by taking notes and printing screens\u2014or\neven photographing screens using digital photography or Polaroids. This is easier said than done.\nFind and correct errors\nWhen you have executed a test, review the output, either by examining the results of a program to compare the\nexpected v. the actual results, or by scanning the results yourself. A discrepancy between the two may have one of\nthe three following causes:\n\u2022 The discrepancy is only apparent: the results are just in a slightly different format (such as a zero being\nunsigned in one and signed in the other). Correct the expected results so that the discrepancy does not\nrepeat, wasting time every time the test is rerun.\n\u2022 The discrepancy may stem form an error in the test data. This occurs quite frequently. The expected result\nitself may be wrong; there may be a clerical error in the test data; or the test data may not correspond to the\nintent of the test case. In the first two instances, the error is easily corrected. In the third case, it may be a\nlittle more complex, because the problem is a sign that the program specification may have been\nmisunderstood.\n\u2022 The discrepancy is due to an error in the program. In that case, correct the program and rerun the test. You\nmust also make sure that the correction has not disturbed a part of the program that has already been\ntested, if necessary by rerunning all previously run test cycles. This process is called regression testing. In\npractice, you do not run a complete regression test every time you find a small error in your program\u2014you\ncan postpone it until the end. However, before you can hand off the program to the next phase, you need to\nrerun all the test cycles once after all your corrections have been completed.\nExecuting the other tests\nThe V-model introduced describes the flow of development and integration, specifying that for each\ndevelopment phase, there is a corresponding test. The V-model is only a schematic representation, however; in real\nlife, more tests are required than those shown, and some things are difficult to test for\u2014in particular the quality\nrequirements, performance, usability, reliability and maintainability, and may require extensive work.\nHow much testing is enough?\nA central question is, when are you done testing? We have already said that it is impossible to guarantee that a\nsystem is free of defects, no matter how long you test it. So when do you stop?\nThis is essentially an economic question. It can be formulated in two ways:\n\u2022 The benefits of delivering the system, including its residual errors, outweighs the risks, or\n\u2022 The cost of discovering additional defects is greater than the savings gained from finding and removing\nthem.\nEither case boils down to a judgment call. This is most apparent in the first formulation, which refers explicitly\nto risk. In the second view, we need to add the word \u201cprobable\u201d in front of both the cost and the savings, to\nemphasize that it is hard to predict how much effort you will need to find the next bug and indeed how grave that\ndefect is going to be.\n63 The decision is illustrated by the defect removal curve in Exhibit 10.\nExhibit 12.: Defect removal\ncurve\nPrototyping\nAccording to the V-model, each test is assumed to be executed on the actual code after it has been developed. In\nmany cases, preliminary tests can be devised that do not require a fully developed system. Rather, they work on a\n\u201csynthetic\u201d application, one which has the same technical characteristics as the application under development, but\nwhich doesn\u2019t really do anything of functional value. In the V-model, such tests are identified as prototypes; setting\nthem up and running them is called prototyping. The benefit of prototyping over testing on a full system is that you\ncan do it earlier; any correction you need to make to the specification you are prototyping will cost much less to\nmake than if you wait until actual testing is possible.\nIf you create a prototype to test out some specification, it doesn\u2019t mean that you can omit the corresponding test\nlater on. In fact, the test is still needed, because the implementation of the specification may be incorrect. But you\nshould be able to run the test, and especially post corrections, in much less time than if you don\u2019t prototype.\nA corollary of the view of testing that we have just described is that any deliverable created by an activity on the\nleft side of the V-model is liable to be tested. It should therefore be couched in testable\u2014concrete, operational\u2014\nterms. A requirement that states, \u201cThe system must be extremely reliable,\u201d is not useful. Rather, use descriptions\nsuch as, \u201cThe system must be available to users during 99 per cent of its scheduled uptime\u201d and \u201cThe system will\nproduce no more than one Type 0 System Change Request per week..\u201d The deliverable is not complete unless it also\nis accompanied by a test model, containing at least a description of the testing approach and ideally a set of test\ncases.\nAnother corollary is that test data have to be designed. You cannot use live data\u2014data extracted at random from\nan existing application, at least not exclusively. (An exception may be made for volume testing.)\nMaintain\nProblems with the waterfall life cycle\nWhen the waterfall life cycle was first published, development project planners used it as a generic systems\ndevelopment plan, translating each activity as a separate bar on a GANTT chart which couldn\u2019t be started until the\nprevious activity was completed. This approach caused a number of difficulties and has largely been abandoned.\nThe waterfall life cycle is now used almost exclusively as a training framework, because it distinguishes between the\n64 different kinds of activities that take place during systems design\u2014although it does not represent an actual\nchronological sequence of events.\nThe main reasons for the inadequacy of the waterfall concept are related to the need to manage changes to the\nsystem. We have already mentioned that as soon as a system has been launched, requests for changes start\naccumulating. In fact, these requests start almost as soon as the first systems requirements have been agreed to,\nwhether it is because the business and technological environments keep changing or because verification, validation\nand testing (including prototyping) uncover defects that need to be corrected.\nSpecifically, the problems that arise are the following:\n5. Projects take too long. By placing each activity on the critical path rather than doing some of them\nsimultaneously, the elapsed time of a project cannot be shortened beyond a minimum of a year or more in\nthe typical case. During the long development phase, costs are incurred; benefits do not start accruing for\nseveral years. This interferes with the realization of the economic benefits supposed to be brought by the\nsystem.\n6. Requirements have to be finalized\u2014\u201cfrozen\u201d is the term most used\u2014too early in the project. During a year or\nmore of development, the world does not stand still, and what may have been useful at the start of a project\nmay be irrelevant a year or two later. This became especially true when information technology started to be\nused as a competitive weapon in the early 1980s.\n7. Users frequently do not get a full understanding of the system until it is almost done. Accepting a\nrequirement or a design on paper is not at all the same as seeing and using a real system, and users who see a\nsystem in operation for the first time may quickly discover major problems. As a result, either the system has\nto be delayed by major corrections or it will be released on time, but with known defects.\n8. Once the developers have handed the system over to operations, they quickly find, contrary to their\n(somewhat unrealistic) hopes and expectations, that they can\u2019t just forget about it. As people start using the\nsystem, they find undesirable system behaviors such as bugs, poor performance, etc. They also develop ideas\nfor additional functions to add to the system, whether these ideas for modifications come from their own\nexperience in using the new technology or are due to changes in the environment\u2014imposed by the\ncompetition, regulators, and society at large. In other words, maintenance hits.\n9. In fact, maintenance is already needed during development. As programmers test, they find bugs that have to\nbe fixed. Some bugs are just errors they have made themselves, but sometimes errors are due to poor design\nor to a misunderstanding of the real requirements. Errors may in fact be found at any point during the\ndevelopment process. To correct these errors, you may have to go back one or several phases in the life cycle\nto scrap some work previously done and rework it. (Royce knew this: he provided the back arrows on his\ndiagram to address this need. But project planners did not truly understand the problem. As a result, the\nmaintenance activity\u2014or bug fixing\u2014that goes on in a waterfall project is usually conflated with the testing\nactivity.)\n10. On innovative projects, another difficulty arises. The team members may not know how to perform their\ndesignated activities in the best way, because they have never done it before. When this happens, designers\n65 experiment by quickly implementing and testing a possible solution, thus anticipating the later stages of the\nwaterfall life cycle.\nDevelopment vs. maintenance\nAll of the points enumerated above have been extensively analyzed and solutions have been proposed. Most of\nthe solutions result in a shorter initial period of systems development\u2014say, three to six months\u2014after which\nperiodic new releases of the system provides additional functionality every three months or so. While many authors\nhad started promoting Rapid Application Development (RAD) in the early 1990s, this did not become generally\naccepted until the race to create web-based applications towards the end of the decade. However, by the time the e-\nbubble burst in March, 2001, RAD and incremental releases had become the norm for all but the most ambitious\nsystems development projects. Even though the pace of creation of web applications has leveled off, we have not\nseen a return to large, big-bang projects.\nA consequence of this trend is that maintenance has now become the normal mode of application creation. In\nthe past, you might have a large team working for three years on the initial development of an application, followed\nby a team reduced to one-half or one-third the initial team, working for ten to twenty years to do maintenance. This\nwould imply a ratio of initial development to maintenance of 1 to 2 or 1 to 3. With the new model, the initial\ndevelopment would take place with a smallish team over three months and maintenance would still last for ten or\ntwenty years with a team of the same size or slightly smaller, giving us a ration of development to maintenance of 1\nto 50 or 1 to 100 in extreme cases.\nThis tremendous shift has not attracted the interest of researchers and authors. IS departments have realized it,\nhowever, and the best-run organizations articulate their work around maintenance as the default activity, initial\ndevelopment being almost an afterthought.\nIn the new model, you do not have a unidirectional flow from requirements (supposed complete) through design\nto construction. Rather, you have system change requests flowing from any part of the process (wherever an issue\nhas arisen) to any other part of the process (wherever the request has an impact).\nMost IS departments use the following classification for their system change requests:\nType 0: Emergency fix. The system has a critical error and is not operational; no bypass is available.\nType 1: Error correction. The system does not work according to specifications. A temporary bypass is\navailable.\nType 2: Enhancement. The system does not work satisfactorily: there are reliability, usability or performance\nissues\nType 3: Extension. The system needs new or modified functionality to respond to external changes or to\nincrease its business value.\nType 4: No system change required. The request is one that can be satisfied by more information or by routine\naction (such as a password reset). This category is included here only for the sake of completeness.\nIn the standard case, Type 0 requests are addressed immediately. Type 1 and simple Type 2 requests are\nbundled together in what is often called a \u201cdot\u201d release, typically monthly (or in some cases weekly). More complex\n66 Type 2 and all Type 3 requests are bundled in major releases, typically every three months (or in some cases every\nmonth).\nThe resulting life cycle might look somewhat like .\nExhibit 13.:A systems life cycle\nTesting during maintenance\nOne of the major problems with maintenance is that modifying a system, whether by correcting a component or\nextending its functionality, there is a high risk of disturbing, unintentionally but unavoidably, functions that do not\nneed to be (and in fact should not be) changed. This accentuates the need for regression testing: testing that not\nonly does the modification work, but everything that was not modified continues to work as before. Any but the\nmost trivial change may require you to rerun an entire system test.\nDuring maintenance, you will create new programs or even subsystems (to address Type 3 SCRs\u2014functional\nextensions) and you will modify existing programs (typically to address other types of SCRs, but sometimes also\nType 3s). Unit testing new programs during maintenance is no different from doing it during development. Testing\nprogram modifications, however, can be a little less rigorous. In general, you will test only that the modification\nworks, but perform no regression testing (at the unit level) to check that the unmodified parts still work as\nspecified. This is possible because the risk of errors is relatively low and because the correction-induced errors you\nmay have caused will most likely be caught by the system-test level regression test, which is not optional, whether\nyour maintenance release contains major new functionality or not.\nFor emergency fixes (Type 0), the same rule holds. Regression testing of emergency fixes just is not cost-\neffective. However, in the next scheduled release, the type 0 fix must be backed out and resubmitted as a type 1 fix.\nThis way, it will be re-implemented, unit tested at the program level and regression tested at the release level, just\nlike other SCRs.\nCorrespondingly, the regression test at the system test level becomes more important. This has several\nconsequences:\n\u2022 The regression test is time-consuming and costly. However, it does not cost much more to test many\nmodifications than just a few. This is a powerful argument for organizing maintenance in releases, as\ndescribed in the previous paragraph.\n67 \u2022 The system test from the first release becomes the regression test for the following releases. This is, in fact,\npart of the \u201cproduct\u201d as delivered to operations; no system is complete without it.\n\u2022 When planning a new release, one of the first deliverables is the revised regression test model. For releases\nthat have a lot of new functionality, you may also want to schedule other tests, depending on how you judge\nthe risk of an unsatisfactory result. These tests are likely to be fairly infrequent and also fairly release-\nspecific, and the maintenance of a regression test for these purposes is usually not economical.\n68 4. Business process\nmodeling and process\nmanagement\nEditor: Franz Lehner (University of Passau)\nLearning objectives\n\u2022 understand why process management is important\n\u2022 be able to explain what a process is and the different types of processes\n\u2022 state the difference between process orientation and other organizational principles\n\u2022 understand the necessity of process modeling\n\u2022 be able to draw simple ePK-diagrams\n\u2022 be able to discuss the use software for process modeling\n\u2022 be able to name and briefly summarize some methods used for the analysis of processes\nUnderstanding the key steps of process management\nThe development of process management\nTo improve and maintain organizational efficiency, there must be a constant willingness for innovation and\nreorganization. Information and communication technology has become an indispensable aid and medium for\nefficiency gains. Organizational science and information systems are close partners in this pursuit of efficiency. A\nfocus on process thinking is a feature of the modern organization. As a result, techniques such as Business Process\nReengineering (BPR), Business Engineering (BE), Business Modeling (BM) have emerged to support the methods\nand goals of process management. In this chapter, the fundamental principles and perspectives are conveyed and\nthe state of technology and its practical application are introduced.\nSince the beginning of the 1990s, enterprises have put greater emphasis on analysis for optimizing business\nprocesses. A clear trend is observed is the shift in attention from a functional orientated organization to an\nalignment around business processes. In functional orientated organizations, the traditional functional\ndepartments such as procurement, production, logistics, finance, IS, marketing and so forth are dominant. Now,\nthere is a new way of thinking: \u201cThe endeavour for optimum and profit generating satisfaction of the clients wishes\nshould (\u2026) come from a process orientated organization structure, in which the position and department formation\nwould be conceived considering specific requirements in the course of the process for performance in the\norganization\u201d (Striening 1988, 28). The goals are more precisely explained in the following section on the Concepts\nof Process Management, namely the optimization of the combined work of all functional areas independent of the\n69 organizational structure and therefore there it is more common to find an overlapping of functions, areas, and\ndepartments.\nBusiness processes are value added activities that produce a strategically valuable output for an organization.\nBusiness Process Management is the optimization, automation, as well as specific regulation and improvement of a\nbusiness process. The tasks of Business Process Management, can be divided into Process Identification, Process\nModeling, Process Analysis and Process Management. Process modeling documents the identification of processes\nin a standardized form, which is usually supported by software. These process models form the basis for process\nanalysis and the adaptation or redesign of a process. Process management should ensure systematic planning,\nsteering, and supervision of the execution of a process. At the same time, the process results will be controlled by\nthe previously established measurement system that used to monitor performance and will be the basis for future\nprocess adjustments.\nThe general goal of process management is to increase client (customer) satisfaction as well as to improve the\nefficiency of business processes. Consequently, the firm should gain a productivity increase. Both a client and value\nadded orientation is necessary. A client can be the final consumer of a product or a person within an organization.\nProcess management can be considered from three organizational perspectives: strategic, operational, and\nadministrative:\n\u2022 Strategic management is concerned with creating the general framework in which the business process are\nexecuted.\n\u2022 Operational management creates the workflow for formalizing and automating business processes and\napplying information systems, as appropriate, to support process execution.\n\u2022 Administrative management, typically the role of the IS unit, is responsible for developing the\norganizational and information systems infrastructure to support business processes.\nWhat is a process and what are the different types of processes?\nThere are many definitions of a process Lehner et al. (2007), and we will discuss a few of these (see Exhibit 14). The\nselected examples illustrate that each perspective on a process each emphasizes a particular trait or characteristics\nof a process. As a result of these different views, there is no single definition.\nExhibit 14.: Definitions of a process\nDefinition Source\nRepeated tasks that arise in the performance of an assignment in different sectors Fischer (1993, 312)\nof an enterprise. They are the recurring results of for an individual task with:\n\u2022 measurable input\n\u2022 measurable added value\n\u2022 measurable output\nA succession of tasks, activities, and performances for the creation of products or Striening (1988, 57)\nservices, which are directly connected with one another and in their sum determines\nthe business management, technical production, technical administration, and\nfinancial success of the enterprise\n70 A manual, partly automated, or fully automatic business activity, which is Oberweis (1997)\nperformed following definite rules and leads to a particular goal. A business process\ncreates, in this way, a valuable result for the client.\nThe content, timing, and natural sequencing of an object necessary to complete a Vossen and Becker\nbusiness management function (1996, 19)\nMost of the definitions have in common that there is measurable information and measurable results, a definite\nbeginning (this means that it has a starting occurrence or initiation) and an end, as well as a demand for value or a\ncontribution for the creation of value. The main features of a process are depicted in Exhibit 15. Process\nmanagement supervises all aspects of a process from the initial event until its completion. It goes beyond\ndepartmental or functional barriers, and sometimes beyond organizational boundaries, to cover the entire process.\nProcess management is an integrated approach to managing processes.\nExhibit 15: The structure of a process (Tonchia and Tramontano)\nExhibit 16.: Business process example\n71 The classification of processes is another key aspect of process modeling. One can broadly distinguish between\nmaterial processes (e.g. procuring, producing, storing and financing) and formal processes (e.g. planning,\ncontrolling and decision-making). Exhibit 20 lists a few examples of processes often found in enterprises.\nA further distinction is whether a process is a main, service or support, and management and leadership\nprocess. Main processes are those that directly create value for an external client. They can be product related (e.g.\nproduction, R&D) or client related processes (e.g. order completion, distribution, acquisition). Main processes are\nsometimes also called core processes, because they are central to the strategic goals of the business. They are the\nmeans by which the business creates value. Service processes deliver value to internal clients and support other\nprocesses. (e.g. personal recruiting, maintenance, quality, and security). Management and leadership processes act\nupon main processes. Planning, accounting, and budgeting fit into this category.\nProcesses can be further subdivided into strong or weak structured processes. Strong structured processes are\nfrequently repeated, structured data, and well documented. They are the day-to-day transactions of an\norganization. They are well suited to conversion to electronic workflow and document management systems. Weak\nstructured processes are, on the contrary, characterized by low predictability and infrequent repetition and\nunstructured data. One uses groupware and communication systems to support weak structured processes.\nProcess orientation as prerequisite for process management\nAll over the world, one can observe an economic and societal restructuring. A dynamic business environment\nand the pressure for firms to increase their competency is forcing enterprise to develop new abilities. Typically,\nfirms adapt to a new environment by following a learning-process to gain efficiency and flexibility. To increase their\ncompetitiveness, many enterprises reorganize their systems and structures. They apply one of the many \u201csalvation\nplans\u201d, such as \u201cBusiness Process Reengineering\u201d, \u201cBusiness Process Design\u201d, \u201cBusiness Process Optimizing\u201d, Work\nFlow Management\u201d, \u201cBusiness Modeling\u201d or \u201cBusiness Process Oriented Organizational Set-up\u201d, just to name of\nfew of them. The choice of methods is good, however careful examination of each of these methods reveals that they\nall are based on the same central foundation. The focus is on identifying business processes and modeling them.\nThey have a common process orientation.\nProcess orientation is booming in information systems, and has also become an important basic component in\nmany organizations. Indeed, process thinking has a long tradition in business administration. The innovation in\nprocess orientation is in the expanded context and in the use of computer software. Business processes can be\nanalyzed in their entirety throughout the organization, and this analysis is shaped and steered with the support of\nprocess modeling software. Process models document the business processes and support their systematic analysis.\nAs a result of this analysis, some processes can be implemented electronically using specialized software, variously\nknow as Work Processing Systems, Workflow Management Systems, and Business Engineering Tools.\nProcess orientation, as it is understood today, is strongly connected with strategic thinking and organizational\ndevelopment (OD). Porter's value chain, probably the most well known of the general process models, is a highly\nuseful tool for strategic thinking.\nProcess orientation gained considerable additional attention with the rise of the Business Process Reengineering\n(BPR). The work on BPR of a few American authors (e.g. Hammer and Champy 1993, Davenport and Short 1990,\nDavenport 1993) started a worldwide trend, moved the discussion of processes in to the center of much\n72 organizational thinking about efficiency. Organization sought to use BPR to achieve reduce the complexity of\ninternal operations and more efficiently achieve organizational goals.\nTo achieve the promise of BPR of simplifying organizational processes, it is of crucial to have an exact and\nthorough modeling of the targeted business processes. Process modeling is a methodology that explicitly or\nimplicitly helps an enterprise to:\n\u2022 understand the nature of its various processes (business processes, service processes etc.)\n\u2022 recognize the resources necessary for the execution of each process\n\u2022 rearrange the system of processes and resources (i.e. process orientation)\n\u2022 to permanently improve processes\nIn general, an improvement in process can be reached through simplifying and standardizing the elements of a\nprocess and their relationship to each other. Through an automation of a chain of activity, for example, through\nnew technology and information systems, the efficiency of the processes can often be increased. Also it is often\npossible to achieve efficiency gains by restructuring single parts of an enterprise. For example, by changing the\norder of the activities of an internal process or the sequencing of the procedures of a process.\nThe traditional organization is orientated around functions, hierarchies, competencies, departments, capacities\nand so forth. This leads in times of market change and competition to a critical disadvantage due to inflexibility,\nslow adaptation, and a loss of customer focus. Process orientation is a solution to these problems because it pays\nattention to products, value chains, and process connections. The goal is to ensure the processes are performed\nefficiently and effectively irrespective of organizational boundaries.\nExhibit 17: Process oriented enterprise\nProcess orientation is a philosophy of business, it is more than a methodology, and it implies no accepted\nactions of a single or connected system. However, methods and concepts have been developed, founded on process\norientations, that have been widely adopted. As well as BPR, other names for this philosophy of business include\nLean Management, Total Quality Management (TQM), and Process Cost Calculation. In the classical hierarchical\norganization, process management performs operates in parallel and complements the functional structure. A\nfundamental alteration of the organization's structure is not necessary. Finally, it is important to point out that\nprocess orientation should co-exist with other management approaches and management science applications.\n73 What is business process modeling and what is it good for?\nA purely verbal description of the sequencing of tasks is not suitable for describing a process because of the level\nof detail and interaction that must be captured. Graphical methods are clearer for showing the ordering of\nactivities, identifying those that occur in parallel and detailing a task's resource requirements. Graphs are easier to\nread and convey the overall nature of a process.\nProcess modeling is a method for enabling an enterprise to document its processes and to recognize the\nresources required by each process. and to depict or to document them, that is to model them, as activities, events\nand resources. There is no generally accepted standard for process modeling and it is strongly influenced by the\ncapabilities of the selected tool. In additionprocess modeling, most of the time, takes place as part of a larger project\n(e.g. business process reengineering or introducing a workflow system). The higher goals or the larger project\nusually strongly influences the approach to process modeling.\nIn practice, organizational problems often trigger process modeling (e.g. a decrease in turnover, a loss of market\nshare, a decrease of the quality of work). It often takes place as a reaction to a critical concern. Because process\nmodeling is time consuming, Ideally, it should be take undertaken without undue pressure for rapid change.\nSeeking to use process modeling as a quick fix to a critical problem is unlikely to lead to the best outcome because it\nencourages short cuts in modeling and analysis that are detrimental to a high quality solution.\nThere are two major approaches of process modeling. The first kind assumes the existing processes must be\nunderstood before taking action (e.g. Bevilaqua and Thornhill 1992). The second approach starts with the results\nthat this process should accomplish. It argues that analyzing existing processes will not produce radical change.\nHammer and Champy's BPR represents the second school. In reality, a synthesis of these two extremes would likely\nbe most useful.\nWe now examine Nagl's (1993) four steps for the general order of procedures in process modeling:\n\u2022 understand existing processes, their resource requirements, strengths and weakness, and identify any risk\nfactors\n\u2022 define planned processes and describe the current functional processes\n\u2022 determine of the planned use of resources (future state) considering the critical success factors\n\u2022 identify the stages of implementation (actual or current state), including describing the system of resources,\nuse of process as well as the measures taken in different functional areas\nThe resulting process model should, among other things, do the following:\n\u2022 identify and define processes\n\u2022 support process analysis and make needs for improvement visible\n\u2022 document the change in the order of a process as well as recognizing the effects of such changes\nAs already stated, business process modeling is concerned with the portrayal and description of all relevant\naspects of a business process in an abstract model with the help of a formal or semi-formal language of description.\nAt the same time, it actively supports the development of the new processes and reorganization of the business. Its\n74 goal is to improve organizational efficiency and competitiveness. Process modeling supports these goals in diverse\nways, including:\n\u2022 Documentation: Business process modeling provides a simplified, but at the same time exact, description of\nthe enterprise. All the elements and sections are described as well as their relationships and connections\nwith one another. It provides the means to analyze emerging problems and to analyze their effects within\nthe process or other related processes.\n\u2022 Organizational analysis and reorganization: The sequencing of each process is analyzed in the course of the\nprocess modeling making possible to identify needless or awkward parts of the organization. As a result,\nparts of the process can be changed and organizational roles modified. As well, redundant tasks can be\neliminated.\n\u2022 Planning the use of resources: Because a process model provides an overall view of the enterprise and the\nexact knowledge of the way that the process functions, it is possible to determine the exact resource\nrequirements of individual organizational units. Available resources can then be better planned and\nallocated. In addition, possible bottlenecks can be highlighted, so that the enterprise can react\nappropriately to relieve them.\n\u2022 Process management, monitoring, and directing the course of a process: A process model delivers a\nbusiness' leadership with an exact picture of the different trends and courses in the business. Input and\noutput, distribution of resources, and the relationship between and in the individual elements of the\nenterprise are represented. Thus, more precise control of the course of business is possible. The\norganization should be more able to react in a timely manner to unexpected events.\n\u2022 System and software development: A business process model gives management and analysts the\nopportunity to simulate proposed new processes before implementation. As a result, problems, possible\nimprovements, and unanticipated outcomes are more likely to be recognized before implementation. It is\ncheaper and less disruptive to simulate change before making a final decision on the form of a new process.\n\u2022 Creating a foundation for the establishment of a workflow management system: After successful business\nmodeling, installing a workflow management is a possibility. A workflow system completely defines,\nmanages and executes workflow processes through the execution of software whose order of execution is\ndriven by a software representation of the workflow's process logic. Such systems are frequently used for\ndocument management.\nThe optimization of an organization is only possible if processes are modeled accurately, only then, does exact\nknowledge of possible improvements or problems become available. Many different elements can play a part in a\nprocess, and the more aspects that are recorded, the more exact will be the process model. Each organization needs\nto determine the appropriate detail for its process model to meet organizational goals.\nAccording to the requirements of the respective organization, the following information needs may be relevant:\n\u2022 Activities and structure of the process: This point is essential for modeling, it determines the purpose and\nstructure of the process. It describes what happens and how it is to happen.\n75 \u2022 Resources: This aspect has to do with the available internal as well as the inflowing external resources of a\nprocess. It defines what material is necessary for the correct sequencing of the process and the source of\nthese resources. Data that are essential for the progress of the process are defined.\n\u2022 Costs: An organization has a limited budget, and thus it is essential to have an exact list of the different\ntasks. Process Modeling can record the cost clearly for each individual process, recognizing where a\nredistribution or a change of budget is necessary or possible.\n\u2022 Time: As with the cost, time also plays an important role in the schedule of a business. Through including\ntime in process modeling, processes that last too long or create bottlenecks can be recognized and\nadditional resources can be deployed to solve these timing issues.\n\u2022 Exceptional incidents: The normal process is often the practice, though unusual events (e.g. lack of\nresources, short notice of deadlines changes) disturb process completion. Such disturbances need to be\nconsidered when modeling to ensure the process is fulfilled.\n\u2022 Priority and role in the organizational structure: Each process has a definite status in the enterprise.\nDecision-making can be streamlined by the identification of organizational priorities. For example, if there\nis a shortage of resources, the decision as to where scarce materials should be delivered first is automatics\nbecause it is defined with the businesses processes.\n\u2022 Communication structures: This item describes the internal communications structure of a process as well\nas its corresponding relationship to other units of the enterprise. For example, it describes what messages\nare exchanged between processes.\n\u2022 Quality requirements: Quality requirements are included in process modeling to ensure customers' needs\nare met at the right standard. As well as defined quality requirements for output, quality standards for the\nprocess and specific dependencies are also recorded.\n\u2022 Security requirements: With some processes, security factors have to be considered. For example, it might\nbe important to prevent unauthorized access to internal data or minimize the risk of an accident in a\nmanual process.\nThe preceding list is not necessarily complete. It shows that processes are very complex and that the information\nneeds for process modeling are dependent on the purpose of the model.\nProcess modeling can serve multiple needs. The most important among them being:\n\u2022 communication with co-workers and partners\n\u2022 establishing a basis for understanding a process and analyzing it\n\u2022 planning of work processes and exceptional situations\n\u2022 implementation of a Workflow systems\n\u2022 training people to use organizational processes\n\u2022 input for software development\n\u2022 a foundation of expertise for information management\n76 For the systematic description of processes, several different notation formats are available. They vary from\ninformal descriptive techniques (e.g. verbal or text based descriptions, flowcharts), to semi-formal techniques (e.g.\nePK-diagrams, BPMN-diagrams, and UML-diagrams), to formal methodologies (e.g. Petri nets). Formal methods\nare based upon a theoretical foundation, usually based on diagram theory. It is helpful to use a common standard to\nmake communication with others easier.\nBPMN (Business Process Modeling Notation) <www.bpmn.org\/> is an open standard for modeling,\nimplementing, and performing business processes. It is similar in its notation to the activity diagram of UML\n(Unified Modeling Language), which is widely used in the software industry. An activity diagram can be\ncomplemented with an application or sequence diagram.\nePK (event controlled chain of process) diagrams are also widely used, and appear in ARIS platform and SAP\nsoftware. Business processes are portrayed as chains of activities that can be connected through events.\nProcess description languages are typically strongly structured, often tabular or graphically oriented. They often\nuse diagrams or structured language to capture detail. As a result, they are more suited for describing processes\nthan everyday language.\nModeling with ePK\nA major advantage of ePK-diagrams is that they are easy to read and are intuitively understood. An important\ndisadvantage is in the limited ability for automatic analysis. A sample ePK diagram is shown in Exhibit 18. As a core\nelement of ARIS, ePK diagrams will be treated when we cover tools for process modeling.\nExhibit 18: Example elements of an ePK diagram\nWe can describe the business processes within an organization in terms of events and function. Examples of\nevents include an incoming order and sending out of invoice. The total of all possible events in an organization and\nits environment is defined as the organization's event scope. Functions include or describe a business management\nchain of activity (e.g. examination of creditworthiness). A function is a time consuming occurrence, which is\ntriggered through a starting event and is completed by an ending event. Sometimes a function is also called a chain\nof activity, occupation, or operation. Functions can also be broken down or aggregated. An informational object\ndescribes the data that are processed by a function (e.g. a customer's record). Connectors describe the different\nforms of process branching (see Exhibit 19). In Exhibit 17, we see an example of an ePK diagram.\n77 Exhibit 19: Connectors\nExhibit 20.: an ePK diagram\nFunctions and events create an event controlled chain of process, with arrows showing the flow of the process.\nAn incoming event initiates a function and a function when completed, in turn, initiates another event. Connectors\ncan be used to combine parts of a chain. Complex chains can be broken down into layers, with high level layers\nshowing the overall flow and lower layers recording the fine details. There are some common conventions for\nprocess modelling, including:\n\u2022 Events (e.g. parts missing) initiate functions (procurement).\n78 \u2022 Parallel of functions should be modeled with logical operators.\n\u2022 A process model should flow from the top to the bottom.\n\u2022 Clear and understandable terms should be used for functions and events.\n\u2022 An organizational unit responsible for the execution of each function should be identified.\nSoftware for process modeling and process support\nThe planning, realization, supervision, and control of processes can supported by software. The spectrum of\nsoftware support includes simple modeling tools to Workflow Systems, Document Management Systems, EAI-tools\n(Enterprise Application Integration), Business Rules Engine (which can automatically perform certain defined\nprocesses). The supply and demand for process technology is still growing strongly. We discuss a few tools that are\nmainly used for process modeling and process analysis.\nProcess modeling software is often similar to CASE-tools (Computer Aided Software Engineering), which were\noriginally created for software design and software development. CASE tools provide some support for process\nmodeling.\nWorkflow management describes another group of tools, which are well-suited for chain of activity modeling.\nThe tools are frequently used to support group work and are sometimes called groupware. The tools were designed\nto support office automation, office communication, and office organizational systems, and their development\nstarted in the 1980s.\nTools support the systematic definition, storage and analysis of data collected during process analysis and\nprocess modelling. In large projects, they are especially useful in coping with handling volumes of data. They\npromote process understanding and process thinking. However, they don't automatically indicate how to improve a\nprocess, that is something for humans to undertake.\nA process modeling tool needs to support different views of a process (e.g. understanding a process across\nfunctions or departments).Tools should also be able to show processes at different levels of detail and integration.\nThe goal is to have an Enterprise or Information Model. An enterprise model supports definition of:\n\u2022 functions\n\u2022 data objects, documents, and data flows\n\u2022 processes\n\u2022 organizational structures (organizational units and organizational set-up, employment structures).\n\u2022 resources and other process features (e.g. cost, resource consumption, length of a run, frequency, volume\netc.),\n\u2022 process responsibility (e.g. process owner)\n\u2022 process results\n\u2022 process triggers (events)\nAn enterprise model creates the possibility of a range of views of the organization to suit different purposes (e.g.\nstrategic planning or process improvement). Most process modeling tools offer the breadth of functionality to\n79 support development of an enterprise model. The centre of a process modeling tool is a description language, which\nmust fulfill certain requirements for it to be useful. The most important features include:\n\u2022 Power of expression: A language must be capable of representing all relevant aspects of a process. It must\ncapture all properties and relationships.\n\u2022 Formalization and degree of precision: A description language must be flexible to adapt use to a particular\nproject's goals. If the language is not flexible enough, it loses power of expression and might be unsuitable\nfor some modeling tasks.\n\u2022 Visualization: A modeling language should support multiple organizational views, so that all aspects of the\nvarious processes are represented. A graphical representation is very helpful, as one loses the general idea\nvery easily with a plain text description of a system. The ability to change the level of detail in a process\nmodel is also useful. One should be able to fade out irrelevant facts for a closer inspection of the fine detail\nof a process or summarize different processes to gain a high level perspective of a process.\n\u2022 Development support: A process modeling tool, ideally, should also support software development as\nsoftware is often written to automate processes.\n\u2022 Analysis and validation: In most cases process modeling enables analysis of internal processes and to\nrepresent different feasible process structures and sequences. The modeling language must support precise\nrepresentation of existing and proposed processes and validation for the testing of redesigned processes.\n\u2022 Performance simulation: An analysis of organizational processes often identifies possible improvements in\nthe structure and working of processes. To ensure that the potential changes result in process\nimprovement, it is necessary to test them before implementation. Process simulation is a way of testing a\nchange before making a costly commitment to an organizational process change. Implementation without\ntesting or simulation can have costly consequences if the new process is flawed.\nThe usefulness of a modeling tool is also determined by other factors beyond the capability to meet the\npreceding requirements. A tool must also be easy to use and support the interaction of the team working on the\nproject.\nProcess models must be constantly updated to reflect organizational change if they are to remain of value. Only\nthe employees that carry out a process, will know if the process has changed or its requirements modified. Ideally,\nmodel maintenance is undertaken by employees and not of the modeling specialists. Consequently, process\nmodeling tools should be readily used by employees. Of course, they might need some appropriate training, but this\nshould not be extensive because they are likely to be infrequent users of the tool.\nProcess analysis and the benchmarking of processes\nGenerally, a process improvement can be reached by simplifying and standardizing the process and its\nrelationships. Automation of an activity chain (e.g. new technology and an information system) can lead to rapid\nperformance increases. Also, restructuring parts of a process (e.g. a change in the internal sequencing of a process)\ncan increase the reduce costs and the time for process execution. Some potential changes include:\n\u2022 Automation ( e.g. cessation of manual activity)\n\u2022 Information level (e.g. better reporting on the stages of process execution)\n80 \u2022 Process sequence (e.g. changes in the order of a process' steps and elimination of unnecessary steps)\n\u2022 Control (e.g. improved monitoring of a process at key steps)\n\u2022 Decision support (e.g. improving the information supplied to decision makers typically results in higher\ndecision quality)\n\u2022 Regional co-operation (e.g. better coordination among different locations)\n\u2022 Coordination (e.g. better coordination between individual tasks and also between two or more processes)\n\u2022 Organizational learning (e.g. collecting and transmitting strategically important information to key\nmanagers)\nTime, cost, and quality play an important role in process improvement. Thus, the individual process objects and\nactivities must be examined to record their duration, quality, and content. The capacity, the consumption, the\nresults, and responsibility of individual processes must also be determined. A variety of data might be collected,\nincluding production time, punctuality in meeting deadlines, work capacity, processing time, waiting time, and\ntransport time, postprocessing time, and error rates. Careful and detailed process modeling lays the foundation\nanalysis and redesign. There are a number of process assessment approaches including:\n\u2022 net value analysis\n\u2022 cost calculation\n\u2022 benchmarking\n\u2022 profit value analysis\n\u2022 controlling\n\u2022 portfolio analysis\n\u2022 strengths, weaknesses, opportunities and threats (SWOT) analysis\n\u2022 process simulation\n\u2022 total quality management\n\u2022 six sigma\nBenchmarking requires comparison of current process time, cost, and quality with earlier outputs of the\norganization or with a similar organizations in the same sector and of comparable size. It should methodically\nevaluate processes and products with their equivalent in the comparison organization. Benchmarking should\nidentify target time, cost, and quality metrics for process redesign based on the best practices of the benchmarked\norganizations. The central question is: \u201cWhy and how are others performing better?\u201d\nOften benchmarking projects are carried out with the assistance of a consulting service, which collects and\nevaluates the necessary data of a group of similar organizations. Participants gain access to their performance data\nand a comparative analysis of their performance relative to the group average and to the anonymous best or worst\nresults of the group.\n81 Profit value analysis is used to compare alternative processes with respect to established goals.The analysis is\norganized into six steps:\n11. Identify the criteria for assessment (e.g. short production time, high quality, low cost).\n12. Determine the weight of each goal (e.g. 25 per cent, 25 per cent, 50 per cent).\n13. Assess the alternatives and give each a score for each criterion.\n14. Calculate the profit value by applying the weights to each score and summing these.\n15. Complete a sensitivity analysis of the profit value to determine how sensitive the findings are to key\nassumptions.\n16. Rank the alternatives.\nProfit value analysis takes into account the importance of different goals. However, the evaluation of each\nalternative is often subjective because objective measures for each criterion are often lacking. In addition, there is a\ncertain amount of subjectivity in selecting weights.\nThe purpose of cost calculation is to investigate process costs beyond an organization's boundaries so that the\nfull cost of a product to customers can be determined. Process cost calculation started in the 1980s. Because of a\nchanging business environment, it was necessary to depart from the traditional method. Overhead costs were\nincreasing because of several factors, including manufacturing flexibility, shortened product life cycles, a wider\nrange of products, and variations within products. The goal is to make the cost structure transparent as this often\nleads to considerable process rationalization and savings. Cost calculation can highlight overheads that are too high\nand common processes that increase the cost of many other processes.\nProcess cost calculation is not an alternative to traditional cost accounting calculations. They are\ncomplementary. Process cost calculation tries to compensate for some of the shortcomings of the traditional cost\ncalculation system in regards to the basic causes of organizational overheads. Increasing cost transparency should\nlead to more efficient resource consumption, report accurately the use of capacity, and improve the product cost\ncalculations. The goal is to improve the quality of decision making related to new product introductions and pricing\npolicies.\n82 Exhibit 21.: Boundary plan cost calculation compared to process oriented cost\ncalculation\nProcess cost calculation should document the cost drivers for each process. These cost drivers can be related to\nquality, time and efficiency goals that have an impact on organizational costs. Reducing cost drivers can decrease\ncosts and improvement competitiveness. It might not be economical to undertake a complete cost analysis of all\ntasks, so process cost analysis should concentrate on those areas that are the main financial stress of the business.\nIt should also focus on those areas for which no analysis of the basic cause of overheads has been completed.\n83 Exhibit 22: Process cost calculation (based on Horv\u00e1th)\nProcess cost calculation makes no judgment about the value of a process and only considers the cost side.\nTherefore, it should be combined with qualitative procedures. For example, a particular process might have a\nconsiderable impact on an organization's reputation and this impact would need to be considered when reviewing\nthe overall value of the process related to its costs.\nA roadmap to process management\nProcess management should be an enduring element of an enterprise. Organizations need to continually\nevaluate about both their larger goals and the fine details of the steps that they take to achieve those goals. Process\nmanagement provides support across this broad spectrum, as illustrated by Exhibit 10. Process management has\nbecome a basic requirement for nearly all organizations because it provides a methodical and systematic set of\nprocedures for continuous improvement. It charts the roadmap to the future for most organizations.\n84 Exhibit 23: Steps in process management\nIn order for process management to be successful, there must be a person responsible for this task within the\norganization. Typically, the appointee is called a process manager. In addition, there can be process owners for key\norganizational processes. Process management occurs in parallel with the traditional functional structure of an\norganization. Thus, there is no need for a fundamental structural change, but there is a need to introduce new roles\nand recognize that management of a process often must span functional boundaries because processes span these\nboundaries.\nProcess management must support an organization's strategic success factors. Successful process management\ncan create a competitive advantage and provide administrative efficiencies. It can also increase an enterprises\u2019\ninnovativeness when applied to areas such as research and development.\nIn general, the success of process management is dependent on the skill of the process analysts, who must be\nsupported by appropriate tools for modelling and analysis. Successful automation of processes is reliant upon a\nnumber of systems, such as a workflow system, a process management system, or the quality of the information\nsystem in which the processes are embedded.\nA five stage model can be applied for introducing business process management (Kleinsorge, 1994). The stages\nare completed sequentially.\n1. Preliminary study: Those responsible for making the change are identified. A process manager is nominated\nfor each process. The extent of the project should also be established at this stage as it might not be sensible\nto model all processes in an entire enterprise in one project.\n2. Process modeling and documentation: All the elements (clients\/customer, customer request, output,\nsupplier, request to the suppliers, input, work capacity, etc.) of each process are recorded in a process model.\nThis work is directed by the appropriate process manager. As well, the span and limits of these processes are\ndocumented.\n85 3. Measuring achievement: Only a measurable process is controllable, so appropriate metrics must be\ndetermined for each process. These metrics should be highly objective and easily measured. Once the metrics\nhave been defined, they should be measured for each process.\n4. Active steering: Active steering (or controlling of a process) means that a process is managed to ensure high\ncustomer satisfaction, deviation from the predetermined goals is minimized, and problems are identified\nquickly and corrective action taken immediately. A sound measurement system will help in recognizing\nproblems promptly and provide data for their rapid resolution. Active steering will increase customer\nsatisfaction and efficiency and lower costs.\n5. Continuous process improvement: Technological change and a dynamic business environment create the\nneed for continuous process improvement. Organizations should strive to continually raise the quality of\nprocesses by reducing errors and required resources for execution. Errors in products and processes should\nnot be tolerated and processes should be monitored constantly to identify opportunities for further\nimprovement and lowering of costs.\nProcess management started with the Japanese concept of Lean Management, and now we have a wealth of\napproaches to process management. Organizations are increasingly combing process management and information\nsystems to increase their efficiency and effectiveness. They realize that they need to redesign their processes first\nbefore automating them if they are to gain the greatest benefit from information systems.\nFirms adopting process orientation can transform in many ways, including positive changes in:\n\u2022 consumer orientation\n\u2022 competence orientation\n\u2022 concentration on creating added value\n\u2022 effectiveness and efficiency\n\u2022 delegation of responsibility and empowerment\n\u2022 support for organizational learning\nBusiness process management has become a core competency for many organizations and is thus closely related\nto strategic management. In addition, there is a strong linkage between information systems and process\nmanagement. On the one hand, information is needed for the performance of processes, and on the other hand,\nnew information is created by processes. Once again, we see evidence of the critical role of information systems in\norganizational success. Indeed, in today's information intensive economy, process management and information\nsystems are critical partners in organizational success.\nExercises\nChapter editor\nFranz Lehner.\nReferences\nBEVILAQUA, J.; THORNHILL, D.: Process Modelling. In: American Programmer 5 (1992), p. 2\u20139\n86 COENENBERG, A.G.; FISCHER, T.M.: Proze\u00dfkostenrechnung \u2013 Strategische Neuorientierung in der\nKostenrechnung. In:DBW 51 (1991), Nr. 1, p. 21\u201338\nDAVENPORT, T.H.: Process Innovation \u2013 Reengineering Work through Information Technology. Boston:\nHarvard Business School Press, 1993\nDAVENPORT, T.H.; SHORT, J.E.: The New Industrial Engineering \u2013 Information Technology and Business\nProcess Reengineering. In: Sloan Management Review 31 (1990), No. 4, p. 1\u201327\nFISCHER, T.M.: Sicherung unternehmerischer Wettbewerbsvorteile durch Proze\u00df- und Schnittstellen-\nManagement. In: Zeitschrift f\u00fcr F\u00fchrung und Organisation 5 (1993), p. 312\u2013318\nGAITANIDES, M.: Prozessorganisation \u2013 Entwicklung, An\u00e4tze und Programme prozessorientierter\nOrganisationsgestaltung. M\u00fcnchen: Verlag Vahlen, 1983\nHAMMER, M. ; CHAMPY, J.: Business Reengineering \u2013 Die Radikalkur f\u00fcr das Unternehmen.\nFrankfurt\/Main, New York: Campus Verlag, 1994\nHammer, M., Champy, J.: Reengineering the Corporation - A Manifesto for Business Revolution. New York\n1993\nHAMMER, M.: ReengineeringWork \u2013 Don\u2019t automate, obliterate. In: Harvard Business Review 4 (1990), p.\n104\u2013111\nHORV\u00c1TH, P. ; REICHMANN, T.: Vahlens Gro\u00dfes Controlling Lexikon. 2nd Ed.. M\u00fcnchen: Verlag Vahlen,\n2003\nHORV\u00c1TH, P.: Controlling. 9. Auflage. M\u00a8 unchen: Verlag Vahlen, 2003\nKleinsorge, P.: Gesch\u00e4ftsprozesse. In: Masing, W. (Hrsg.): Handbuch Qualit\u00e4ts-Management, 3. Auflage, Carl\nHanser Verlag, M\u00fcnchen, Wien 1994, p. 49-64\nLehner, F., Scholz, M., Wildner, St.: Wirtschaftsinformatik. Hanser Verlag, M\u00fcnchen 2007\nNAGL, G.C.: Erfolgspotential Unternehmensproze\u00df \u2013 Modellierung von Unternehmensprozessen mit\nComputer Aided System Engineering. In: Zeitschrift f \u00a8ur Organisation 3 (1993), p. 172\u2013176\nOBERWEIS, A.: Gesch\u00e4ftsproze\u00dfmodellierung. EMISA-Fachgruppentreffen 1997 \u2013 Workflow-Management-\nSysteme im Spannungsfeld einer Organisation, Oktober 1997\n87 REMUS, U.: Prozessorientiertes Wissensmanagement. Konzepte und Modellierung, Universit\u00e4t Regensburg,\nPhD Thesis, Regensburg, 2002\nSTRIENING, H.-D.: Proze\u00dfmanagement. Frankfurt\/Main: Verlag Peter Lang, 1988\nTonchia, S., Tramontano, T.: Process Management for the Extended Enterprise \u2013 Organizational and ICT\nNetworks, Springer, Heidelberg 2004\nVossen, G., Becker, J.: Gesch\u00e4ftsproze\u00dfmodellierung und Workflow-Management. Modelle, Methoden,\nWerkzeuge. Bonn et al. 1996\n88 5. Information systems\nmethodologies\nEditor: Salam Abdallah\nLearning objectives\n\u2022 define in general terms what is a methodology\n\u2022 describe the role of methodologies in information systems\n\u2022 describe the main features of methodologies\n\u2022 describe some of the challenges when adopting a methodology\n\u2022 break down methodologies into its fundamental components\n\u2022 assess methodologies using an evaluation framework\nIntroduction\nInformation systems (IS) are affecting most management functions and have become essential to firms'\ncompetitive advantage and survival in the \u201cnew global digital economy\u201d. Organizations are under pressure and they\nare responding by continually updating and aligning their IS strategies with their business strategic goals resulting\nin new information systems. Organizations need to also adapt to changes and align themselves with newly\nintroduced information systems.\nThis alignment usually triggers a number of \"transformational projects\" across the organization that affect a\nlarge number of stakeholders; this may include new IS strategic planning, restructuring the organization and its\nbusiness processes, security planning and building, and managing variety of applications (see Exhibit 30).\nTransformations and developments are usually carried out with guidance of methodologies to ensure business\nvalue and to provide the means to bring people, processes, and technology together. It is known that the field of IS\ncontains a jungle of methodologies. It is estimated that there are hundreds of methodologies for different purposes\nand scopes and this reflects the fact that no single methodology is the best. Which will be the most appropriate\nmethodology for a given project depends on the nature of the problem and the organization fitness.\nMethodologies share general common philosophies and approaches and can play a central role in the working\nlife of IS mangers. It is important for managers to have a conceptualized understanding of the philosophies behind\nmethodologies and to be aware of the challenges behind adopting methodologies. Managers should be able to\nevaluate and select an appropriate methodology to ensure the project will be completed on time, on budget and\naccording to specifications. This chapter provides a foundation for understanding and selecting methodologies.\n89 Exhibit 24. Business pressures triggering various types of projects\nWhy IS projects fail\nInformation systems related projects frequently fail; it has been reported that between 50 per cent-80 per cent\nof project fail. For example, projects in Business Process Re-engineering (BPR) have been failing at the rate of 70\nper cent (Grant, 2002).\nDifferent projects regardless of their nature have experienced poor quality outcomes, missed deadlines, over\nbudget and cancellations. In a survey (Whittaker, 1999) the nature of the failure was the result of:\n\u2022 the project budget was overrun by 30 per cent or more; and\/or\n\u2022 the project schedule was overrun by 30 per cent or more; and\/or\n\u2022 the project was canceled or deferred due to its inability to demonstrate or deliver the planned benefits\nExhibit 25. Serious budget and schedule overrun by\nExhibit 26.: A) Projects overrunning budget by 50 per cent or more;\norganization size (Whittaker, 1999)\n(B) Projects overrunning budget by 50 per cent or more (Whittaker,\n1999)\n\u2022\n90 One of the common stated reasons for failures is: organizations are not adopting a sound methodology or\nlack planning. Other common reasons of failure have been also stated, such as the business case of the\nproject was weak in several areas, missing several components and a lack of management involvement and\nsupport (Whittaker, 1999).\nMethodologies are necessary; they can provide organizations with useful ways to efficiently and effectively\nfacilitate transformation to fully benefit from IS and to reduce the risks of project failures. Verner et al. (1999)\nsurveyed twenty experienced software project managements, who agreed that the right choice of the methodology\nhas an effect on the success of the project.\nMethodologies defined\nThe terms \u201cmethodology\u201d and \u201cmethod\u201d have been used interchangeably in the IS field. Usually the term\n\u201cmethodology\u201d means a holistic approach to the problem-solving process and the word \u201cmethod\u201d is a subset of a\nmethodology. Holistic means the methodologies include project management issues, education and training and\nsupport.\nA methodology can be defined as \u201can organized collection of concepts, beliefs, values, and normative principles\nsupported by material resources\u201d (Lyytinen, 1987). Methodologies in general are viewed as problem-solving\nprocesses, to solve mainly ill-structured problems where the solution and the outcome are not easily predictable i.e.\nhaving high uncertainty. Methodologies provide users with the means to reflect on reality, determine the problems\nand solve them. They aim at reducing complexities, providing means of involving stakeholders and collecting\nrequirements and capturing a holistic picture, allowing for control, and standardizing practices to reduce risks of\nproject failure.\nAckoff (1981, p.354) argues that \u201cto solve a problem is to select a course of action that is believed to yield the\nbest possible outcome\u201d. Methodologies contain a set of practices to suit special circumstances and usually follow a\nlife cycle aiming at an effective solution process and ensuring quality outcomes. Methodologies are usually based on\ndifferent philosophies as a way to approach the problem and reaching a solution.\nSolving problems may follow these three generic life cycle stages, see Exhibit 24.\n\u2022 Plan: This stage attempts to capture a holistic picture of the problem by involving all the relevant variables\nand stakeholders and laying down practical strategies and plan to solve the problem.\n\u2022 Develop: Develop the solution according to the plans.\n\u2022 Manage: Implement the developed solution and monitor and assess the results.\nThese three generic stages are the basis of most methodologies. Methodologies would consist of detailed\ndescriptive steps to guide the user on how to proceed with these stages of solving problems and what work products\nor deliverables will be required as an output form each stage. Methodology also refers to a \u201cwell-tried method, often\ndeveloped by experts and publicized through training courses and books\u201d and these are called \u201cformalized\u201d\nmethodologies, as opposed to \u201chomegrown\u201d. Homegrown are methodologies developed inside an organization\nbased on personal experience of managers or developers.\n91 Exhibit 27.: Generic stages for solving problems\nMethodologies are mostly developed and used in systems development. We find that development of other types\nof methodologies relies on knowledge and practices gained from systems development methodologies. For example\nbusiness process re-engineering or information security relies on the established knowledge gained from the\ninformation systems development environment. Baskerville (1993) and Dhillon and Backhouse (2001) have all\nargued that information security methodologies are similar to information systems development methodologies\nsince they consist of phases and procedural steps. Because of this, methodologies usually follow the same three\ngeneric stages but differ in their details and the expected work products.\nSome of the common types of methodology used in the IS discipline are:\n\u2022 Strategic Information Systems Planning (SISPM): This type of methodology seeks to integrate and to align\nan organization\u2019s strategic objectives with its existing information systems plan or business needs. SISP\nmethodologies aim at assisting organizations in identifying opportunities by capitalizing on technologies for\ncompetitive advantage.\n\u2022 Business Process Re-engineering (BPRM): Used to maximize corporate profitability by redesigning and\ntransforming the organization\u2019s business processes.\n\u2022 System Development Methodologies (SDMs): Used in designing different types of information systems\nsuch as transaction processing systems, enterprise information systems, decision support systems, etc.\n\u2022 Information Security Methodologies (ISM): Assist organizations to establish a security plan to address\nvulnerability associated with unauthorized misuse of information.\nTo sum up, methodologies provide users with ways of thinking, doing and learning. Methodologies\nprovide users with ways to approach the problem and controlling the process of solution development. They\nprovide an appreciation of standards and they enforce a more disciplined and consistent approach to planning,\ndeveloping and managing. Their deliverables may be checked for quality. A study has shown that the use of a\nstructured software and maintenance methodology can contribute to the quality and business value (Nelson and\nGhods, 2002). Methodologies are also seen as a way of learning by acquiring knowledge from previous projects and\npreserving it in a set format to guide organizations on their next project.\n92 Waterfall model methodology\nThe oldest and most known methodology used to coordinate the efforts of information systems development is\nknown as the \u201cWaterfall\u201d. The model is based on a generic life cycle stages to guide developers from an initial\nfeasibility study through maintenance of the completed systems. Each stage of the model becomes the input for the\nfollowing stage, see Exhibit 28. These stages are described as follows:\nExhibit 28.:Waterfall model methodology\nFeasibility study: studies the existing systems to provide a clear understanding of the problem and to\ndetermine project goals\nSystems analysis: analyzes end-user information and determines any deficiencies of the existing systems.\nData collected in this stage will be used as part of the new system\nDesign: reatures and systems functions are described in details, including screen layouts, business rules,\nprocess diagrams, programming instructions and other various documentations\nImplementation: designs are translated into code using programming languages\nTesting: system is put into a testing environment to check for errors and bugs\nMaintenance: rth ce system is put into production in a real business environment;hanges, corrections and\nadditions start to crop and the system begins to evolve.\nEvolution of Methodologies\nMethodologies are considered useful and influence practice, therefore we find practitioners and academics\ncontinue to develop methodologies to improve projects success rates. Knowledge and lessons learned from\nprevious practices are embedded into these methodologies; therefore, methodologies are evolutionary and their\ndevelopment is an endless effort. Methodologies have been in use since the 1960s and they have evolved in their\napproaches.\nTribal Era (1960-1970): During this period developers did not use a formalized methodology, the emphasis in\nthis era was on understanding technologies and determining ways to solve problems. Little attention was given to\nthe exact needs of users resulting in poor solutions that did not meet objectives. The effort was individualistic\n93 resulting in poor control and management of projects. Excessive failures called for a systematic development\napproach.\nAuthoritarian Era (1970s to early 1980s): In this era, developers believed that adhering strictly to the details of\nmethodologies ensured successful project and would meet management and users requirements. This era also\nproduced undesired outcomes, because the emphasize was on adhering sacredly to methodologies and neglecting\nthe fact that businesses are always in transit and have changing requirements reacting to business pressures.\nMethodologies in this era were seen as lacking comprehensive and flexibility.\nDemocratic Era (1980s to current): Unsatisfied with the restrictive approach of methodologies, new\nmethodologies emerged with more maturity, that are more holistic based on philosophies, clear phases, procedures,\nrules, techniques, tools, documentation, management and training, including success factors such as the need for\nuser involvement. Methodologies in this era produced better business modeling techniques and can be described as\nbeing incremental, participative (sociotechnical), systemic (holistic), and business oriented i.e. alignment of IS\nwith business strategic goals.\nIn this era the concept of method engineering has also emerged. Method engineering is based on the philosophy\nthat users should be able to mix and match between the different methodologies extracting specific methods,\ntechniques and tools depending on the current problem to be solved, rather than adhering to a single methodology.\nAlso, recently agile or light methodologies have emerged, and they are based on the belief that projects should be\nsmall with minimal features. These methodologies are characterized as being adaptive to the environment, having\nsmall teams, and feedbacks are very essential, teams are self-organizing, informal in approach to development,\nflexible, participative and social collaborative.\nMethodologies have been going through a phase of transformation\u2014moving from a mechanistic to socio-cultural\nparadigm or from the hard to the soft approach. See Exhibit 29 for a comparison between the hard and soft\nmethodologies.\nExhibit 29.: Comparisons between hard and software methodologies\nHard methodology Soft methodology\nUsed to solve well-defined problems\u2014simple problem Used to solve ill-structured problems\u2014world problems\nand solution is known are complex, ambiguous require novel solution\nFocus on technical perspectives, in terms of solving the Social constructivism, humanistic, people views are\nproblem and controlling the project. They are rational important and joint solution constructed\nand scientific based approach.\nFocus on the final outcome\u2014reach a solution with The focus is on the process to encourage knowledge\nshortest route building form multiple views and creative thinking\nMethodologies basic structure\nMethodologies usually have the same basic structure and share common terminologies. The basic structure of\nmost methodologies will have the following three components:\n\u2022 Principles are the guiding rules, standards and beliefs of the methodology. The collection of these\nprinciples makes up the philosophy and the aims of the methodology.\n94 \u2022 Processes express the principle in the form of stages, steps, and tasks. The processes expound the\nprinciples. They contain advice and recommendations on how to proceed and complete deliverables using\ndifferent techniques.\n\u2022 Services may include different formats, they may include detailed documents to elucidate the process and\nprinciple expressed by practical examples, case studies, illustrations, guiding templates, documentations,\netc. They contribute to the understanding and learning of the user. Services may also include various tools\nto assist the user in completing processes or provide advice and assistance.\nExhibit 30.: Methodologies are\nbasically collections of three elements\nSelecting a methodology\nMaybe one day you will be an IS manager and who will be involved in transformational projects, such as systems\ndevelopment, security planning, strategic planning, or business process engineering, and will be confronted with a\ndifficult and critical task of selecting a methodology appropriate for the project. You should be able to carefully\ncompare and select an effective methodology. Prior to selecting a methodology, you need to consider the following\nstatements:\n\u2022 When selecting a methodology choose one among proven methodologies. Choose methodologies that have\nbeen used successfully in other organizations.\n\u2022 The benefits of using the methodology should be clear in your mind, in order to justify your reasons for\nadopting a methodology.\n\u2022 What method and criteria will you use to choose an appropriate methodology? Appropriate in terms of\nusefulness and easy of use?\nA useful generic framework that can be used to assist in the selection process is the NIMSAD, which stands for\nNormative Information Model-based Systems Analysis and Design (Jayaratna, 1994). NIMSAD is a tool that can\nassist users to evaluate and get better conceptual understanding of a problem to be solved and its environment. The\nframework also assists users to understand the complexity of the technical and social aspects of development. The\nframework is based on three important perspectives that the evaluator should consider in order to raise important\nquestions and to answer them. These three perspectives are related to the \u201dproblem situation\u201d, \u201dmethodology users\u201d\nand the \u201cmethodology\u201d itself. The main purpose of the framework is to evaluate these three perspectives.\n95 \u2022 The problem situation (the methodology context): To get an understanding of the problem context,\nthe problem solver should first develop an understanding of the organization. The organization may be\nunderstood by examining its main components, such as its people, processes, technology, information,\nmaterial flow and structure. To get a clear picture of the problem, problem solvers need to gain deep\nunderstanding of these components and their interactions with each other. Jayaratna argues: \u201cThe richer\nthe knowledge the intended problem solvers can obtain about the organization, the better the position they\nmay be in for understanding the \u2018real\u2019 problems of the organization. It may also help them to make better\njudgments about the relevance of information to those in the \u2018problem situation\u2019 and ready to raise\nquestions.\u201d Some of the questions that the methodology user may ask:\nWho are the clients? How strong is their commitment? Does methodology help in identifying clients and\ntheir concerns? What\u2019s the situation like? (simple or ill-structured) For which situation is methodology\nsuitable? What does the situation demand, (identify problems, design solutions for already identified\nproblems, implement an already designed solution)?\n\u2022 The intended problem solver (the methodology user): An individual or group of people are\nresponsible for the problem formulation and course of action to arrive at a solution. The problem solver\ncould be a consultant, systems analyst etc. and could be from inside or outside the organization. The focus\nis on the role, rather than the person. The choice of the solution is usually reached in agreement between\nmethodology users and stakeholder.\nThe solution to the problem is greatly shaped by the mental constructs of the methodology users shaped by\ntheir personal characteristics, such as:\n\u2022 perceptual process\n\u2022 values\/ethics\n\u2022 motives and prejudices\n\u2022 experiences\n\u2022 reasoning ability\n\u2022 knowledge and skills\n\u2022 structuring processes\n\u2022 roles\n\u2022 models and frameworks\nSome of the questions users may ask: What level of abstract and technical thinking does methodology\ndemand from the user? Do philosophical views advocated by methodology match user\u2019s view? What\nknowledge sets and skills does methodology require from the user? Are mental constructs of the user\nconsidered?\n\u2022 3The Problem-Solving Process: This perspective examines the details of the process of solving the\nproblem. The process has three primary phases, problem formulation, solution design and design\nimplementation. Some of the questions that may be asked at each phase:\n96 Problem formulation Does methodology offer assistance for boundary construction? What is role of\nclient? Does methodology discuss particular methods of investigation? What techniques does\nmethodology offer for expressing situation characteristics? What environmental (context) information is\ncaptured? What tools and techniques are available? What problems or problem types are of concern to\nthe methodology? How does it help in deriving problem statements? Does it offer help in formulating\nnotional systems?\nDesign phase: Does it help in formulating design solutions? What aspects cannot be captured by\nmethodology? How experienced is user to be expected in the solution domain? Who decides on which\nsolution to take?\nImplementation Phase: What steps does methodology offer for developing the project? What does it offer\nin terms of tools and techniques? How does it help in handling major changes?\nNIMSAD can provide a useful way to understand critically the problem being solved, users involved and the\nprocess of solution and the interactions between them. Once the methodology has been adopted it should be\ncontinually evaluated at the various stages of its use i.e. before, during and after. We should be prepared to adjust\nthe methodology to fit better the problem situation or to abandon the methodology if no business value is being\nadded.\nQuality selection criteria of methodologies\nQuality Attributes are the benchmarks that assess the methodology in terms of its content and operation. The\nquality attributes provide the means for understanding the fitness and suitability of a methodology. One needs to\nprepare a collection of attributes that are representative of the methodology requirements. For example, the\nattribute of being flexible (Can I change it?), comprehensive (Does it have everything that I need?), and practical (Is\nit useful and useable?).\nMethodologies misuse\nIdeally, methodologies should be adopted to become part of the working culture of practitioners as a way of\ncompleting projects successfully. Examination of practice on use of methodologies shows that methodologies have\nbeen adopted, or rejected, or in some cases has been misused. Have a look at the different ways how users have\nbeen misusing systems development methodologies.\n1. Users are using methodologies as a cover up and not following the principles and the guidelines of the\nmethodology, for several reasons:\n\u2022 to impress clients and win contracts or as a political protection for civil servants should projects go\nwrong\n\u2022 for image building to demonstrate that things are under control\n\u2022 to please their managers\n\u2022 to feel secure that they are doing that right thing, which is seen as \u201dsocial defense\u201d\n2. Follow a methodology that they learned previously e.g. from their previous managers, and continue\nusing the methodology blindly without questioning its merits.\n97 3. Users are misinterpreting the methodology\u2019s principles to suit their personal needs and desires and this\ncauses deviation from the intended principles, which may eventually lead the user to abandoning the\nmethodology. Studies show that 80-90 per cent of methodologies are being modified.\n4. Some of the practitioners (60 per cent) do not fully understand methodology\u2019s principles and they end\nup with partial use.\nSummary\nMethodologies are extensively used by IS professionals to compete various tasks that involve planning, building\nand managing plans or systems. Methodologies can assist in reducing the risk of project failure, since they allow\nusers to capture the real picture of the problem, develop strategies to solve the problem and to take actions; in other\nwords, they can provide users with a way of thinking and doing and ensuring quality. Therefore, methodologies can\nbe influential and careful evaluation and selection must precede the adoption of methodologies. Methodologies\nshould be adopted to add business value and not to be used as cover up. Key Terms\nAd-hoc methodologies Methodolgies structure\nAgile Methodolgies Methods\nBPRM Problem situation\nFormalized Methodolgies Problem-solving processing\nGeneric cyclic stages SDM\nHard methodologies SISPM\nHomegrown methodogies Soft methodology\nISM The intended problem solver\nMethod engineering Transformational projects\nMethodolgies Waterfall\nExercises\n1. Search and reflect (individually or in groups) on the various methodology definitions and formulate your own\ndefinition of what a methodology is. Discuss and compare your definition in class.\n2. Methodologies use many common terminologies that were not covered in this chapter. Define the following\ncommon terms: Paradigm, Techniques, Task, Tools, Iterations, Prototyping, Design Specifications,\nNotations, and Modeling.\n3. Find a formalized methodology that is well documented on the web, briefly examine its details, and prepare a\nbrief report on the methodology in terms of its general basic structure i.e. principles, processes, and services.\nAlso identify the main stages of the methodology.\n98 4. Find a formalized methodology that is well documented on the web, briefly examine its details, and prepare a\none page report on the methodology using the dimensions provided by NIMSAD. Students are advised to\nselect different methodologies to include, but are not restricted to: ISDM, SISP, ISM, BPRM.\nDiscussions questions\n\u27a2 The more detailed the methodology the better it is. Do you agree?\n\u27a2 Do you think one day will come where all practitioners will follow a methodology strictly and according to\nits principles?\n\u27a2 How can we ensure that methodologies are not being misused in an organization?\n\u27a2 Group project: Break class into groups and debate the issue of adopting or not adopting methodologies\nfor a specific type of transformational project. The debate should explore issues from a technical and\nbusiness perspective.\nChapter editor\nSalam Abdallah\nReferences\nAckoff, R. L. 1981, 'On the Use of Models in Corporate Planning', Strategic Management Journal, vol. 2, no. 4,\npp. 353-359.\nAng, J., Shaw, N. & Pavri, F. 1995, 'Identifying strategic management information systems planning\nparameters using case studies', International Journal of Information Management, vol. 15, no. 6, pp. 463-\n474.\nAvison, D. E. & Fitzgerald, G. 2003, Information Systems Development: Methodologies, Techniques and\nTools, McGraw-Hill, Maidenhead.\nBadenhorst, K. P. & Eloff, H. P. 1990, 'Computer security methodology: Risk analysis and project definition',\nComputer & Security, no. 9, pp. 339-346.\nBaskerville, R. 1993, 'Information systems security design methods: Implications for information systems\ndevelopment', ACM Computing Surveys, vol. 25, no. 4, pp. 375-414.\nBrinkkemper, S. 1996, 'Method engineering: Engineering of information systems development methods and\ntools', Information and Software Technology, vol. 38, pp. 275-280.\nde Koning, W. F. 1995, 'A methodology for the design of security plans', Computer & Security, no. 14, pp.\n633-643.\nDhillon, G. & Bakchouse, J. 2001, 'Current directions in IS security research: Towards socio-organizational\nperspectives', Information Systems Journal, no. 11, pp. 127-153.\nEarl, M. 1994, 'How new is business process redesign', European Management Journal, vol. 12, no. 1, pp. 20-\n30.\nFitzgerald, B. 1998, 'An empirical investigation into the adoption of systems development methodologies',\nInformation & Management, vol. 34, pp. 317-328.\n99 Grant, D. 2002, 'A wider view of business process reengineering', Communications of the ACM, vol. 45, no. 2,\npp. 85-90.\nJayaratna, N. 1994, Understanding and Evaluating Methodologies, McGraw Hill Book Company Europe.\nKettinger, W., Teng, T. C. J. & Guha, S. 1997, 'Business process change: A study of methodologies,\ntechniques, and tools', MIS Quarterly, vol. 21, no. 1, pp. 55-80.\nLederer, A. & Sethi, V. 1996, 'Key prescriptions for strategic information systems planning', Journal of\nManagement Information Systems, vol. 13, no. 1, pp. 35-62.\nLyytinen, K. 1987, 'Taxonomic perspective of information systems development: Theoretical constructs and\nrecommendations', in Boland & Hirschheim, (eds.), Critical issues in Information Systems Research, pp.\n3-41.\nMingers, J. & Brocklesby, J. 1997, 'Multimethodology: Towards a framework for mixing methodologies',\nOmega International Journal Management Science, vol. 25, no. 5, pp. 489-509.\nMumford, E. 1998, 'Problems, knowledge, solutions: Solving complex problems', in Proceedings of the\ninternational conference on Information systems, Helsinki, Finland, pp. 447 - 457.\nOlle, T. W., Hagelstein, J., Macdonald, G. I., Rolland, C., Sol, G. H. & Van Assche, J. M. F. 1991, Information\nSystems Methodologies: A Framework for Understanding, 2nd edn, Addison-Wesley Publishing\nCompany, Wokingham, England ; Reading, Mass.\nRoberts, T. J., Gibson, M., Fields, K. & Rainer, K. J. 1998, 'Factors that Impact Implementing a System\nDevelopment Methodology', IEEE Transactions on Software Engineering, vol. 24, no. 8, pp. 640-649.\nValiris, G. & Glukas, M. 1999, 'Critical review of existing BPR methodologies: The Need for a holistic\napproach', Business Process Management Journal, vol. 5, no. 1.\nWastell, D. G. 1996, 'The fetish of technique: Methodology as a social defence', Information Systems Journal,\nvol. 6, pp. 25-40.\nWastell, G. D., White, P. & Kawalek, P. 1994, 'A methodology for business process redesign: Experiences and\nissues', Journal of Strategic Information Systems, vol. 3, no. 1, pp. 23-40.\nWhittaker, B. 1999, 'What went wrong? Unsuccessful information technology projects', Information &\nManagement & Computer Security, vol. 7, no. 1, pp. 23-29.\n100 6. Implementing systems\nImplementing systems\nEditor: Kevin C. Desouza (University of Washington, USA)\nContributors: Leslie L. Mittendorf, Lokesh Ramani, Prem Kumar, Yared Ayele, Matt Sorg, Stephanie Morton,\nTing-Yen Wang, Kelly Ann Smith, Benji Schwartz-Gilbert, Jaret Faulkner, and Kendal Sager (students at the\nInformation School, University of Washington, USA)\nReviewer: Ron Vyhmeister (Adventist International Institute of Advanced Studies, Philippines)\nLearning objectives\n\u2022 understand why social problems must be considered along with the technical problems when implementing\nsystems; understand the dangers of ignoring social issues when implementing systems\n\u2022 discern different ways of adjusting to social issues when implementing systems\n\u2022 be able to understand how ethics plays a role when implementing systems, especially the difficultly in\ndefining this issue\n\u2022 understand the significance and social\/technical problems inherent in storing and disseminating information\n\u2022 attain a general appreciation for the problems that emerge when conducting change management\n\u2022 realize the roles people and company cultures play when implementing systems\n\u2022 appreciate a modern socio-technical problem (Knowledge Harvesting), the challenges it presents, and some\nof the solutions that have been recommended\nIntroduction\nThe most important advantage that any organization can have over another is in the organization\u2019s knowledge\nworkers. Workers that can create inventive processes and solutions to problems can improve the organization\u2019s\neveryday functions as well as its planning for future goals and objectives. The exploitation of this advantage can\nmean a reduction in the expenditure of both time and capital on the organization\u2019s projects, thus improving\nmargins in both respects. However, this goal requires that the knowledge worker, whose main job is to assemble\nideas rather than sellable products, be given the resources and time to solve new problems, rather than having to\nreinvent solutions to old, and often previously solved, problems. Within the Information Technology sector is a\ngrowing area of development focused on creating programs to reduce the amount of time knowledge workers must\nspend creating, recreating, learning, and relearning the processes and solutions to past problems. This area is\nknown as Knowledge Harvesting, or more technically, Information Capital Management.\nKnowledge Harvesting can be most simply defined as a set of processes or systems designed to capture and\npreserve the knowledge of an organization. In this case a process is considered a codified set of rules or guidelines\ndesigned to elicit certain key pieces of information, from the entity it is used on, and organize this information into\na uniform and consistent package. This allows a knowledge worker familiar with the structure of the packages to\n101 extract the useful information they may need without having to search for it. Secondly, a system is considered the\nphysical technologies and techniques used to facilitate or support the Knowledge Harvesting processes. Some\nexamples of technology systems include servers or computer terminals while system techniques could be exit\ninterviews or electronic answer and question databases. Though these two pieces of Knowledge Harvesting exist\nand can be used by themselves, they are most effective when combined together. In this way they make up the\nfundamental building blocks for the harvesting of knowledge from the employees of an organization.\nIn order to analyze this subject to the fullest possible extent we have divided it into five major areas of\ndiscussion: the demand for Knowledge Harvesting within organizations, the affect Knowledge Harvesting has on\nknowledge workers, the effects from and impact on organizational culture, current Knowledge Harvesting practices\nwith specific focus on a major case study, and finally, recommendations we have developed for the improvement of\nthe Knowledge Harvesting functions in each of these areas. This structure allows us to progress through an\norganization from the bottom up and thus gives clear insight into the importance that Knowledge Harvesting is\nplaying now and will play in the future.\nDemand for Knowledge Harvesting\nWith the development of new technologies, business is moving at an increasingly faster rate. In order to stay at\nthe leading edge of the market, organizations have to keep up or face being left behind by the competition.\nKnowledge Harvesting is one way that organizations can keep their employees well-educated and train new\nemployees quickly. It is a great setback when organizations lose a veteran employee, and with the baby boom\ngeneration nearing retirement, \u201corganizations will face a major exodus of institutional knowledge as their most\nexperienced employees leave the work force\u201d (\"Intelligence at risk: capturing employee knowledge before\nretirement\u201d 14). Without any way of capturing the knowledge from these employees, years of valuable experience\nwill be lost. In fact, in a recent survey of over five hundred fulltime workers, more than twenty-six per cent of\nrespondents noted that their organization would let them retire without any form of exit interview or transfer of\nknowledge. Fifty per cent of the five hundred surveyed said that their organization had absolutely no formal way to\ncapture knowledge prior to losing an employee (Knowledge Harvesting - KM Toolkit: inventory of tools and\ntechniques). The fact that these employees are allowed to leave without passing on any form of their experience\nmeans that new employees will have to continuously rediscover and solve past mistakes, creating a major loss of\nefficiency.\nThrough the implementation of Knowledge Harvesting systems, many hindrances created by the loss of\nexperienced employees can be avoided. One of the most notable benefits of Knowledge Harvesting is that vital\nknowledge is not lost when experts leave the company (Knowledge Harvesting - KM Toolkit: inventory of tools and\ntechniques). If this experience is properly harvested and stored, the knowledge of a few key individuals can then be\nmade available to others who need it without creating a crippling dependence on the expert. This Knowledge\nHarvesting technique can also aid in training new employees. Since past experience is taken into account, the\nlearning curve will be shortened due to the fact that the individual will no longer be reliant on their own problem\nsolving skills; they can use past experience to solve problems more quickly.\nFormalizing the Knowledge Harvesting Process\nMany aspects of an organization can contribute to successful Knowledge Harvesting. In an idealistic setting, the\norganization has a fully developed knowledge management process and employees frequently contribute and\nextract knowledge from the organization. However, the actual process of harvesting knowledge is an intricate mix of\n102 methods, behaviors, and motivations. Although these factors complicate knowledge harvesting, understanding\nthem provides insight into the process of knowledge harvesting. As discussed earlier, an understanding of\nknowledge harvesting can allow organizations to use it to provide value to the business and eliminate redundancy.\nUnderstanding where knowledge harvesting should occur is one crucial portion of formalizing the knowledge\nharvesting process in an organization. Knowledge is exchanged in two broad settings, formal and informal\n(Coakes). Formal settings include situations like exit interviews and official meetings for project updates. Informal\nsettings are much more varied, ranging from chatting at water coolers to visiting a co-worker\u2019s office. Arguably,\nmost of the knowledge sharing in an organization takes place at informal locations, because these interactions are\nmore frequent and comfortable due to the informal nature. However, harvesting knowledge from informal\ninteractions is challenging because the interaction is unstructured. Also notable is the role that geographic\nproximity plays in Knowledge Harvesting. While co-workers on a co-located team rapidly acquire knowledge from\nteam members, the same process occurring across teams or office buildings is much slower (Brown).\nEmployee Contribution and Resistance\nWith the understanding of where knowledge harvesting occurs, the rewards or incentives for an employee to\ncontribute knowledge can be explored. As a concrete definition, a reward is a condition that can motivate an\nemployee to contribute knowledge with the expectation of receiving something in return (Coakes). Coakes explains\nresearch divided rewards into two categories, intrinsic and extrinsic. The extrinsic category of rewards includes\nincreased salary or additional vacation time. As an example of an extrinsic reward, employees who contribute\nknowledge to a company system could be rewarded during performance reviews. The intrinsic category of rewards\nincludes job satisfaction, decision making power, and improved career prospects. One particularly interesting\nexample of an intrinsic reward can be seen in the case of the Xerox technician who received a standing ovation at a\ncompany conference because the technician had contributed many useful articles to Xerox\u2019s knowledge\nmanagement system (Brown). Knowing these rewards helps information systems builders understand what\nmotivates employees to contribute to knowledge harvesting activities. Thus, understanding and utilizing rewards\nsystems represents a substantial part of successful knowledge harvesting.\nHowever, in spite of rewards, many employees consider reasons to resist information harvesting. Most popular\namong these reasons is self-preservation. A number of studies have found that in organizations implementing\nknowledge databases, organization members have been willing to use the system to search for information, but\nhave been reluctant to submit their own information to the system (Cress 371). The sharing of information presents\na social dilemma for members of an organization; information is often perceived as power, and sharing of\ninformation without a guarantee that others will do the same could mean the relinquishing of power and\ndegradation of one\u2019s position within the group (372). Another employee concern is the possibility that others will\ntake credit for knowledge that an employee contributes. This concern is particularly strong in situations where a\nreward system can be abused to benefit those who steal ideas. Additionally, employees may feel that harvested\nknowledge will be used for negative purposes. As Harrison explains, on any team of computer programmers, there\nwill always be a top performer and a bottom performer (6). With knowledge harvested about the performance of\neach team member, the intention of management becomes a concern for the employees. With positive intention,\nmanagement could provide training and career development materials for the worst performers in the team.\nHowever, with negative intention, management could choose to fire the weakest performers. Finally, some\nindustries and organizations have competitive cultures that encourage individualism and discourage cooperation.\n103 For example, stock brokers within a large financial firm might act as individualists in order to retain their clientele\nand to keep secret their investing strategies. With these reasons to resist knowledge harvesting, the systems and\nprocesses of knowledge harvesting face additional challenges to successful implementation.\nIssues of Organizational Culture\nOrganizational culture can be defined as the \u201ccharacter or personality of the organization\u201d (Ribiere 32). It\nencompasses the norms and values of the organization, as well as the accepted behaviors and actions of\norganization members. The culture of an organization has a significant impact on the sharing of knowledge among\nthe organization\u2019s members, a key influencing factor in a successful Knowledge Harvesting process. It is therefore\nessential for the organization to develop a culture in which knowledge sharing is encouraged.\nWhile developing objective measures of organizational culture is a matter of debate, Davenport and Prusak have\nidentified four main aspects of culture whose presence increases information sharing. The first, altruism, is a\ngeneral feeling of goodwill in which members of an organization share their knowledge for the greater good of the\norganization, without expecting anything in return (Ribiere 51). Reciprocity, the second characteristic, is the belief\nthat if one member of an organization contributes useful knowledge, others will also contribute knowledge that will\nhelp that member (52). The third factor, repute, is the belief by members of an organization that contributing\nknowledge will improve their reputation within the organization, which can lead to tangible benefits such as job\nsecurity and bonuses. Trust is the final characteristic, considered the most important of the four. Without trust,\nDavenport and Prusak find that all knowledge management efforts will fail (54). Trust can be fostered internal job\ncharacteristics such as benefits as work environment, as well as external characteristics such as job satisfaction (57).\nIn his research into the factors affecting success of knowledge management initiatives, Ribiere defined success\nas growth of project resources, growth of knowledge shared within a team, valuable input from more than just one\nor two team members, and some evidence of financial return on investment (78). Among the organizations that had\nsuccessful knowledge management initiatives, Ribiere identified high levels of trust and solidarity among\norganization members as characteristics that promote success (128). Cress et. al, found that members of an\norganization often overestimated the cost of contributing to a knowledge database, but when they understood the\nvalue of their contributions in relation to the actual costs, they were more likely to contribute their knowledge\n(Cress 375). They did discover, however, that in some extreme situations, members would not contribute\ninformation simply to hurt other members of the organization, even when it decreased their personal benefits\n(374). This shows the importance of altruism, reciprocity, repute, and trust as cultural factors within the\norganization; if these characteristics are present in the culture, it is unlikely that such extreme behavior would\noccur.\nCurrent Practices in Knowledge Harvesting\nClearly, there is a demand for knowledge harvesting. However, in order to successfully harvest knowledge, an\norganization must go much further than just making technological changes. On the contrary, successful knowledge\nharvesting is largely based upon organizational adjustments; technological changes simply support these\norganizational activities (Laudon and Laudon, 446). The good news is that most successful knowledge harvesting\nplans use very similar organizational activities, allowing us to extract a general organizational plan to successfully\nharvest knowledge (Eisenhart, 52; Chua, 252). Knowledge Harvesting Inc.\u2019s process for knowledge harvesting does\na good job of capturing the processes used by most successful knowledge harvesting implementations. According to\nKnowledge Harvesting Inc., harvesting requires the following organizational activities: focus\u2014deciding upon the\n104 knowledge that needs to be sought out; find and elicit\u2014identifying the experts and interviewing them; and organize\n\u2014categorizing, expanding, and pruning the results in an appropriate manner (Eisenhart, 52).\nTo illustrate an implementation of this harvesting process, we will examine how the Army\u2019s Center for Army\nLessons Learned (CALL) harvests knowledge. Even though CALL does not consciously use Knowledge Harvesting\nInc.\u2019s process for harvesting, we will notice how CALL still distinctly adheres to the same organizational procedures\noutlined by Knowledge Harvesting Inc.\nCase Study\nThe first step in the process of knowledge harvesting, focus, involves determining what knowledge to harvest.\nMany firms determine what knowledge to harvest by having the harvesters meet with management. Note that the\nharvesters can either be employees from inside the company or external consultants. Through the meeting, the\nharvesters can determine the target audience and choose to harvest knowledge based on the target audience\u2019s needs\n(Eisenhart, 50). CALL takes this process a step further, by not only deciding upon the content to harvest based on\nthe target audience, but also upon the significance of the data to be harvested\u2014\u201cpotential for generating\u2026future\nstrategic value\u201d (Chua, 254). This means that valuations of knowledge will have to be determined; CALL works with\nsenior Army officers to identify specific learning objectives that the harvested knowledge needs to fulfill. This latter\nmethod resolves, from the very start, the potential for information overload due to harvesting too much knowledge\nand overwhelming target audiences with superfluous data (Chua, 257). However, in addition to avoiding\ninformation overload, excessive focus can also cause CALL to \u201cmiss\u201d information; high-potential information is lost\ndue to its lack of fitting the established criteria (fitting a team\u2019s needs and fulfilling a learning objective).\nDepending on the chosen method, once the knowledge to be harvested has been determined, the next step is to\n\u201cfind and elicit\u201d the knowledge. Find and elicit refers to discovering the experts in a knowledge area and then\ninterviewing or extracting the desired knowledge from them. Common strategies for the find and elicit step employ\na first-person or second-person approach. Cerabyte Inc. produces software, called Infinos System, which supports a\nfirst-person style. The system asks experts a series of leading questions while he or she works through a process in\norder to obtain best practices and procedures knowledge. \u201cThe questions are designed to help employees clearly\narticulate best practices by encouraging them to think, clarify, and record their actions\u201d (Duffy, 60). Georgia-\nPacific, a forest products company, uses a second-person format of extracting knowledge. The company first\ndirectly interviews both identified experts and the target audience to determine gaps in knowledge. The harvester\nthen interviews the experts again to fill in these gaps (Eisenhart, 50). CALL\u2019s method to find and elicit knowledge\nutilizes a unique third-person approach of gathering knowledge, which places less emphasis on the \u2018expert\u2019 and\nmore emphasis on outside observers. To gather information from identified experts\u2014soldiers in the field\u2014CALL\nfirst assembles a data collection team of about eight to fifty \u201csubject matter experts\u2026from various units across the\nArmy\u201d (Chua, 254). The team members are selected specifically for their removal from the event being studied, in\norder to promote objectivity. The team members are also selected such that many different fields of study are\nrepresented, which \u201cenables deep knowledge to be collected for each event\u201d and enhances \u201cthe reliability and\nvalidity\u201d of gathered knowledge (Chua, 255). Additionally, the use of outsiders from different specialties allows for\n\u201cfresh ideas\u201d to be added to analysis (Chua, 258). The CALL data collectors are then dispatched alongside troops to\nobserve, capture, and document in the midst of the event (Chua, 257). The collectors document with a variety of\nmedia in order to capture the reality of the situation as fully as possible, usually employing video and photographs,\nas well as diagrams and written descriptions. This detail not only allows end-users to gain from the knowledge by\n105 \u2018reliving\u2019 the experience (Chua, 255), but also to retain the context of the event (Chua, 257). Occasionally, the\ncollectors will also interview a wide range of personnel to gather feedback and interpretations in order to get more\nperspectives on the event. Note that a peripheral benefit of this method is that it is not affected by an organization\u2019s\nculture of sharing. CALL\u2019s method is not centered on the individual expert and his or her desire to share knowledge;\nrather, CALL focuses on third-party observers to make unbiased observations from multiple viewpoints. Despite its\nbenefits, CALL\u2019s avoidance of the expert is also the third-person approach\u2019s biggest shortcoming. Many companies\nhave found that their knowledge harvesting projects are reliant upon the fact that their harvested knowledge has\ncome directly from experts. Employees are more receptive of harvested knowledge that come first- or second-hand\nfrom experts. They claim that information is more \u201cmemorable\u201d and \u201ceducational\u201d if the sources are fellow\nemployee-experts (Keelan).\nOrganize, the third step in the Knowledge Harvesting process, refers to categorizing, fixing, adding, or pruning\nthe gathered information. In this stage, the harvester asks: \u201cIf I was in this situation, would I get to the same place\n[the experts] did [using this gathered knowledge]?\u201d If the answer is no, then the harvester needs to better organize\nor edit the gathered knowledge (Eisenhart, 52). CALL utilizes a very extensive human-centered approach to the\norganize step. First, the data collection team communicates within itself to organize individual insights. These\ninsights are then sent to CALL headquarters to be analyzed by another group, the analysts, who act a conduit,\n\u201cseek[ing] input and insights from other Army experts\u201d. The analysts are ultimately responsible for further\norganizing and editing in an attempt to construct new knowledge from the disparate pieces of knowledge sent in\nfrom the data collectors. After indexing this new knowledge, it is transmitted electronically for review by other\nprofessionals in the Army. Finally, after review by Army professionals\u2014upon which the knowledge is codified and\ndivided into \u2018lessons\u2019\u2014the data collectors return to CALL headquarters and review all of the compiled lessons.\nThrough a process called the \u2018murder board\u2019, the data collectors decide which lessons are thrown out and which are\nimportant enough to be packaged for distribution (Chua, 255-256). Despite this exhaustive and complete approach\nto organizing, the amount of work that must be done\u2014in addition to the lack of use of technology\u2014to reach this\npoint of completeness dramatically increases the delay in getting harvested information to the target audience. This\ncould pose a major problem if the harvested knowledge is time-sensitive in the environment (who wants obsolete\ninformation?) or within the target audience (what team wants knowledge that it needed last week?).\nImplementation Differences\nAmidst the presence of general organizational processes for knowledge harvesting, however, there is a lot less\nsimilarity in how firms have implemented each process. Some have used technological approaches, some have used\nhuman-based approaches, some have used interviewing, and some have used observations to support knowledge\nharvesting processes. Despite such variety in implementation practices, this has helped firms to distinguish\nthemselves and their knowledge harvesting agendas from competitors.\nWe have already discussed how many firms implement focus by determining target audiences and their\nknowledge needs. We have also seen how CALL also focuses on the \u2018strategic value\u2019 of harvested knowledge. Yet\nanother method to determine what knowledge to harvest is to focus only upon knowledge has been successful in the\npast. Currently, firms have implemented this solution with a technology-based approach; Intellectual Capital\nManagement (ICM) software is employed to focus solely on extracting and recording employee expertise during\nsuccessful business process creation or change. This method tries to capitalize on \u201cpractices that have proven\nefficient and effective\u201d (Duffy, 59).\n106 Find and elicit has even more varied implementation than focus. We have already observed some of these\ndifferent implementations in the first-, second-, and CALL\u2019s third-person approaches. However, whereas these\napproaches defined humans as the experts from whom to gather knowledge, some implementations choose non-\nhuman experts to \u2018interview.\u2019 Hewlett-Packard\u2019s implements find and elicit by using software to examine the\ncompany\u2019s own technical notes, frequently asked questions, help files, call log extracts, and user submissions (Delic,\n75). Roche Labs, a healthcare company, gathers desired knowledge by \u2018interviewing\u2019 global news sources, specialty\npublishers, health care Web sites, government sources, and the firm\u2019s proprietary internal information systems\n(Laudon and Laudon, 424).\nLastly, the organize process has also seen a number of different implementation. We have seen how CALL uses a\nhuman-centered approach. Another example of this is KPMG, an international auditing and accounting firm. To\norganize gathered knowledge, KPMG employs an extensive staff of \u201canalysts and librarians\u201d to categorize elicited\ninformation and to assess its quality (Laudon and Laudon, 424). Another method of organizing is to use a software-\ncentered approach, which can be exampled by Cerabyte\u2019s Infinos System. After eliciting best practices knowledge,\nthe system itself will try to ascertain bottlenecks, risks, and tradeoffs. The system will then interact with users and\nexperts to solve these issues (Duffy, 60).\nBridge Building\nTo illustrate the importance of Knowledge Harvesting in training new employees and gaining knowledge for\norganizations, we constructed an experiment to compare and contrast two different systems. One system had an\noutline of the knowledge that should be entered into system; the other contained no guidelines whatsoever and\nallowed entry of any free-form comments. A group of students was divided into two groups of equal size and given\nthe same instructions: Build a model bridge from Lego blocks that spans a six-inch ravine and is at least two inches\nwide. The two groups developed different designs based on their interpretations of these instructions. Neither\ngroup had any idea that they would need to explain how to build their bridge via written instructions. After the\ngroups were done building, Group A was given a detailed form asking for certain sketches of their model, what\nproblems they had run into, and any other comments they would like to add. Group B was merely told to write\ninstructions on how to build the bridge. The two sets of instructions represent the aforementioned Knowledge\nHarvesting systems.\nWhen the groups traded instructions and were asked to reproduce each other\u2019s models, both groups struggled to\nreconstruct the bridges, but for very different reasons. The instructions that Group A wrote prompted them to draw\nthe different layers of their bridge, which they did, but not completely to scale. The form did not specify whether\nlayer one was the top or bottom layer. When Group B received these instructions, they had a general idea of what\nthe bridge should look like, but due to the lack of knowledge about the organization of the layers and the size of the\npieces, were unable to reconstruct the bridge exactly. The instructions that were written by Group B were very\nvague and contained a great deal of information that Group A did not find pertinent to the building of the bridge.\n107 Exhibit 31.: Bridge building exercise\nGroup A Original Reconstruction of A by B Group B Original Reconstruction of B by A\nThis situation is not far from what businesses must cope with today. Effective Knowledge Harvesting helps\ncreate value for the organization and makes it easier to train new employees, because the usefulness of certain\ninformation is already determined. Group A, by answering prompts similar to that which would be given in an exit\ninterview, was able to establish exactly what information should be passed the next group in order to construct the\nbridge. However, Group B could not establish what information was essential, and when the instructions were\npassed on, Group A had a hard time determining what information would help them remake the bridge and what\nwas extraneous.\nGroup B also had a hard time constructing their bridge despite the fact that they had most of the essential\ninformation. The problem in this case lies more so in the presentation of the information. Group B had all of the\ninformation that they needed, but they could not determine where to place each of the layers nor could they\ndetermine the scale of the drawing in the directions. If the experiment could be repeated, the follow up form would\nbe further revised so as to better represent the way in which the bridge could be constructed.\nAn organization implementing Knowledge Harvesting techniques, such as exit interviews, informal transition\nprocesses, or periodic meetings with employees, has an advantage when it comes to retaining knowledge that is\nimportant to the organization. As with the bridge building experiment, when an exit interview is conducted, the\ncompany gains possession of information that previously would have left the company along with the employee.\nRather than valuable experience being lost when the employee leaves or is required to pass a project on to another\nteam, the company maintains control of that knowledge. The training process, as illustrated in the bridge building\nexample, can also be shortened with effective Knowledge Harvesting. The employee being trained no longer has to\nsolve every problem already solved by previous employees. Through a brief description of the problem and solution,\nthe new employees can now overcome obstacles without wasting time rediscovering the solution, and organizations\ncan save time and money. The organization would also have to go through a revision process to make sure that the\ninformation is being presented to the employees in a way such that it is helpful.\nRecommendations\nBased on our analysis of the demand for and factors affecting Knowledge Harvesting, we created a list of general\nrecommendations that organizations should heed if they expect to remain competitive in the knowledge market of\nthe near future. By following these recommendations, the knowledge created by their employees can be successfully\nharvested and then, in turn, used to increase the productivity and uniqueness of the organization\u2019s projects and\nprocesses. These recommendations can be summarized as: a broad implementation of standardized Knowledge\nHarvesting techniques in organizations, the creation of a cooperative work environment, the creation of a definitive\n108 rewards system, and the disassociation between the idea of sharing knowledge and that of being replaced within the\norganization.\nThe broad adoption of a standardized Knowledge Harvesting system is, as we have demonstrated, becoming an\never pressing demand on organizations today. The baby boomer generation, which makes up a large part of the\ncurrent workforce of knowledge workers, is steadily moving towards retirement. For many organizations this will\nmean a loss of much of their experience and knowledge, unless they put into place a dedicated system to harvest\nthese from their departing employees. However, the system must be standardized across the entire organization, as\ndifferences in technique or thoroughness can create problems with organizing and sharing the collected pieces at a\nlater date.\nSecond, the creation of a working environment centered on sharing or cooperating with fellow employees is\nessential to the Knowledge Harvesting process. If employees feel that they are part of a larger team working\ntowards a common goal, it will become easier for them to share their own knowledge in the hopes that it will further\nthe team as a whole. In contrast, a work environment that fosters individualism and the hoarding of knowledge will\nmake individual workers reluctant to share their own knowledge with others, as it may give others an advantage\nover the sharer. One possible way organizations might accommodate for this is by structuring employees into teams\ncentered on certain projects, and then rewarding those teams or individuals who contribute the most in the form of\nshared knowledge or experience. This same idea could then be extended to the individual level by rewarding those\nemployees who contribute knowledge to a knowledge base and then have their contributions used by their co-\nworkers. These developments could have the effect of both providing an incentive to share and allowing the more\nexperienced employees to pass on knowledge to those with less experience through direct interaction.\nDirectly related to the creation of a strong group work environment is the creation of a definitive rewards system\nthat helps to facilitate knowledge sharing. This reward program would have to track both the amount of work any\ngiven employee accomplishes and how much knowledge they contribute to other employees of the organization.\nOne idea for this could be a Wikipedia-like database where information is collectively edited and shared, but that\nwould keep track of those who contributed the information for tracking purposes. Though in many cases the reward\nsystem developed would have to accommodate for the organizational culture in which the information is being\nshared, as each organization might handle group collaboration differently (e.g. one organization might hold group\nsessions where ideas are traded and discussed, while another might rely on an intranet-based discussion board).\nThe final, and possibly one of the most essential, recommendations for the implementation of Knowledge\nHarvesting processes is the disassociation of sharing knowledge with the fear of being replaced within the\norganization. In many cases, employees are hesitant to share their knowledge as they believe that the company will\neventually decide that the employees are no longer useful, and can thus be replaced with less experienced and lower\npaid workers who can use the experienced employees\u2019 harvested knowledge. In order to dispel this myth,\norganizations will have to make an effort to show their employees that sharing knowledge is beneficial to them,\ntheir fellow employees, and the organization as a whole, while also demonstrating that it is the employees who do\nnot share with others that are replaced. This sort of organizational culture will create a sense of the necessity of\nsharing while also giving employees a greater sense of pride and security in their jobs.\nAlthough these recommendations are far from exhaustive in respect to actions which organizations can take to\nensure the preservation of their current employees\u2019 knowledge, they are the essential pieces needed to create an\n109 effective Knowledge Harvesting system. The production of usable artifacts of knowledge, and especially the most\neffective and extensive methods for doing so, can be expected to become a major issue within organizations, both\nlarge and small, in the coming years. Those organizations which take these initial steps towards creating a\nKnowledge Harvesting system will be the same organizations that can expect to be at the forefront of their\nindustries and the public sphere of the future.\nEditor\nKevin Desouza\nReferences\nBrown, John Seely and Paul Duguid. \"Balancing Act: How to Capture Knowledge Without Killing It.\"\nHarvard Business Review May-June 2002: 73-80.\nChua, AYK, W Lam and S Majid. \"Review - Knowledge reuse in action: the case of CALL.\" Journal of\nInformation Science 32.3 (2006): 251-260.\nCoakes, Elayne, ed. Knowledge Management: current issues and challenges. Hershey, PA: IRM Press, 2003.\nCress, Ulrike, Joachim Kimmerle and Friedrich W. Hesse. \"Information exchange with shared databases as a\nsocial dilemma: the effect of metaknowledge, bonus systems, and costs.\" Communication Research 33.5\n(2006): 370-391.\nDelic, Kernal A and Lahaix, Dominique. \"Knowledge harvesting, articulation, and delivery.\" Hewlett-Packard\nJournal 1998: 74-81.\nDuffy, Jan. \"Managing Intellectual Capital.\" Information Management Journal 35.2 (2001): 59-63.\nEisenhart, Mary. \"Gathering Knowledge While It\u2019s Ripe.\" Knowledge Management (2001): 48-53.\nHarrison, Warren. \"Whose Information Is It Anyway?\" IEEE Software 20.4 (2003): 5-7.\n\"Intelligence at risk: capturing employee knowledge before retirement.\" Industrial Engineer August 2005: 14.\nKeelan, Tim. \"Keane Inc. Utilizes StoryQuest, iPods and Podcasting to Support Sales Training and\nOrganizational Change.\" Business Wire 3 February 2006.\n\"Knowledge Harvesting - KM Toolkit: inventory of tools and techniques.\" 31 May 2005. National Electronic\nLibrary for Health. Crown. 26 November 2006\n<http:\/\/www.nelh.nhs.uk\/knowledge_management\/km2\/harvesting_toolkit.asp>.\n110 Laudon, Kenneth C. and Jane P. Laudon. Management Information Systems: Managing the Digital Firm.\nNew Jersey: Pearson Education, 2006.\nRibiere, Vincent Michel. Assessing knowledge management initiative successes as a function of\norganizational culture. Dissertation. The George Washington University. District of Columbia, 2001.\n111 7. How hardware and\nsoftware contribute to\nefficiency and effectiveness\nEditor: Larry Press ( California State University-Dominguez Hills, USA)\nReviewer: Geoff Dick (University of New South,Australia)\nLearning objectives\n\u2022 describe and quantify the improvement in electronic technology used in information processing systems\n\u2022 describe and quantify the improvement in storage technology used in information processing systems\n\u2022 describe and quantify the improvement in communication technology used in information processing\nsystems\n\u2022 describe and differentiate between the major software development platforms: batch processing, time-\nsharing, personal computers, local area networks and the Internet.\n\u2022 describe and give examples of the following modern, Internet-based software categories: open source,\napplications with user-supplied content, composite applications, software as a service, mobile, portable and\nlocation-aware applications, long tail applications, and collaboration support applications.\n\u2022 describe impediments to innovation and the adoption of new applications: limited communication and data\ncenter capacity, inequity in access to information technology, organizations with vested interests in the status\nquo, and concerns over privacy and security.\nInformation technology is improving at an accelerating rate. This opens the way for innovative applications,\nwhich make organizations and individuals more efficient and effective. This chapter outlines hardware progress,\nwhich has led to new forms of software and software development. Software evolution has brought us to the current\nera of Internet-based software. After describing some of the characteristics of Internet-based software, we ask\nwhether the progress we have enjoyed will continue and conclude with a discussion of some of the non-technical\nissues, which tend to impede that progress. The sections read as follows:\n\u2022 Hardware progress\n\u2022 Software progress\n\u2022 Internet based software\n\u2022 Will the progress continue?\n\u2022 Bumps in the information technology road\n112 Hardware progress\nTechnology is improving rapidly. It seems that a new cell phone or computer is outdated the day after you buy it.\nThis is nothing new. Consider manned flight for example. The Wright Brothers first flight in 1903 lasted only 12\nseconds and covered 37 meters.16 Once we understand the science underlying an invention, engineers make rapid\nimprovements in the technology. Within 66 years of that historical first flight, Apollo 11 landed on the moon.\nWould you guess that that information technology progress is slowing down, holding steady, or accelerating? It\nturns out that it is accelerating\u2014the improvements this year were greater than those of last year, and those of next\nyear will be still greater. We often use the term exponential to describe such improvement. Informally, it means that\nsomething is improving very rapidly. More precisely, it means that the improvement is taking place at a constant\nrate, like compound interest. In this section, we consider three technologies underlying IT\u2014electronics, storage,\nand communication. Each of these technologies is improving exponentially.\nSidebar: Exponential growth\nTake, for example, a startup company with USD 1,000 sales during the first year. If sales double (100 per cent\ngrowth rate) every year, the sales curve over the first 12 years will be:\nTwelve years sales growth, linear scale\n$2,500,000\n$2,000,000\n$1,500,000\n$1,000,000\n$500,000\n$0\n0 2 4 6 8 10 12 14\nExhibit 32: Exponential growth graphed with a\nlinear scale\nSales are growing exponentially, but the curve is almost flat at the beginning and then shoots nearly straight up.\nGraphing the same data using a logarithmic scale on the Y axis gives us a better picture of the constant growth rate:\nTwelve years sales growth, logarithmic scale\n$10,000,000\n$1,000,000\n$100,000\n$10,000\n$1,000\n$100\n$10\n$1\n0 2 4 6 8 10 12 14\nExhibit 33: Exponential growth graphed with a\nlogarithmic scale\nNote that since the growth rate is constant (100 per cent per year in this example), the graph is a straight line.\nYou can experiment with differing growth rates using the attached spreadsheet.\n16 http:\/\/www.nasm.si.edu\/galleries\/gal100\/wright1903.html.\n113 Of course, nothing, not even technology, improves exponentially forever. At some point, exponential\nimprovement hits limits, and slows down. Consider the following graph of world records in the 100 meter dash:17\n100 dash world record for men (seconds)\n10.7\n10.6\n10.5\n10.4\n10.3\n10.2\n10.1\n10\n9.9\n9.8\n9.7\n1900 1920 1940 1960 1980 2000 2020\nExhibit 34: Linear improvement\nThere is steady improvement, but it is at a roughly linear rate. The record improves by a constant amount, not at\na constant rate.\nProgress in electronic technology\nTransistors are a key component in all electronic devices\u2014cell phones, computers, portable music players, wrist\nwatches, etc. A team of physicists at the Bell Telephone research laboratory in the United States invented the first\ntransistor, shown below.\nExhibit 35: The first\ntransistor\nThis prototype was about the size of a 25 cent coin, and, like the Wright brothers\u2019 first plane, it had no practical\nvalue, but was a proof of concept. Engineers soon improved upon the design. In 1954, Texas Instruments began\nmanufacturing transistors. They were about the size of a pencil eraser, and several would be wired together to make\na circuit for a device like a transistor radio. In the late 1950s, engineers began making integrated circuits (ICs or\nchips) which combined several transistors and the connections between them on a small piece of silicon. Today, a\nsingle IC can contain millions of transistors and the cost per transistor is nearly zero.\nConsider the central processing unit (CPU) chip that executes the instructions in personal computers and other\ndevices. Most personal computers use CPU chips manufactured by Intel Corporation, which offered the first\ncommercial microprocessor (a complete CPU on a chip) in 1971. That microprocessor, the Intel 4004, contained\n2,300 transistors. As shown here, Intel CPU transistor density has grown exponentially since that time:18\n17 http:\/\/en.wikipedia.org\/wiki\/World_Record_progression_100_m_men.\n18 http:\/\/www.intel.com\/pressroom\/kits\/quickreffam.htm.\n114 Transistors on Intel CPU chips (millions)\n1000\n100\n10\n1\n0.1\n0.01\n0.001\n1971 2005\nExhibit 36: Improvement in electronic technology\nWith the packaging of multiple CPU cores on a single chip, transistor counts are now well over one billion.\nIntel co-founder, Gordon Moore, predicted this exponential growth in 1965. He formulated Moore\u2019s Law,\npredicting that the number of transistors per chip that yields the minimum cost per transistor would increase\nexponentially. He showed that transistor counts had pretty much doubled every year up to the time of his article\nand predicted that improvement rate would remain nearly constant for at least 10 years.19\nWe have used Intel CPU chips to illustrate exponential improvement in electronic technology, but we should\nkeep in mind that all information technology uses electronic components. Every computer input, output or storage\ndevice is controlled and interfaced electronically, and computer memory is made of ICs. Communication systems,\nhome appliances, autos, and home entertainment systems all incorporate electronic devices.\nProgress in storage technology\nStorage technology is also improving exponentially. Before the invention of computers, automated information\nprocessing systems used punched cards for storage. The popular IBM card could store up to 80 characters, punched\none per column. The position of the rectangular holes determined which character was stored in a column. We see\nthe code for the 10 digits, 26 letters and 12 special characters below.20 21\nExhibit 37: Punch card code for alphabetic and\nnumeric symbols.\n19 http:\/\/download.intel.com\/research\/silicon\/moorespaper.pdf.\n20 http:\/\/www.columbia.edu\/acis\/history\/026-card.jpg\n21 Http:\/\/www.cs.uiowa.edu\/~jones\/cards\/history.html.\n115 Punch card storage was not very dense by today\u2019s standards. The cards measured 3 \u00bc by 7 (3\/8) inches,22 and a\ndeck of 1,000 was about a foot long. Assuming that all 80 columns are fully utilized, that works out to about 48,000\ncharacters per cubic foot, which sounds good until we compare it to PC thumb drives which currently hold up to 8\nbillion characters.\nEvery type of data\u2014character, audio, video, etc.\u2014is stored using codes of ones and zeros called bits (short for\nbinary digits). 23 Every storage technology distinguishes a one from a zero differently. Punched cards and tape used\nthe presence or absence of a hole at a particular spot. Magnetic storage differentiates between ones and zeros by\nmagnetizing or not magnetizing small areas of the media. Optical media uses tiny bumps and smooth spots, and\nelectronic storage opens or closes minute transistor \u201cgates\u201d to make ones and zeros.\nWe make progress both by inventing new technologies and by improving existing technologies. Take, for\nexample, the magnetic disk. The first commercially available magnetic disk drive was on IBM's 305 RAMAC\n(Random Access Method of Accounting and Control) computer, shown below.\nExhibit 38: RAMAC, the first\nmagnetic disk storage device\nIBM shipped the RAMAC on September 13, 1956. The disk could store 5 million characters (7 bits each) using\nboth sides of 50 two-foot-diameter disks. Monthly rental started at USD 2,875 (USD 3,200 if you wanted a printer)\nor you could buy a RAMAC for USD 167,850 or USD 189,950 with a printer. (In 1956, a cola or candy bar cost five\ncents and a nice house in Los Angeles USD 20,000).\nContrast that to a modern disk drive for consumer electronic devices like portable music and video players. The\ncapacity of the disk drive shown here is about 2,700 times that of a RAMAC drive, and its data access speed and\ntransfer rate are far faster, yet it measures only 40x30x5 millimeters, weighs 14 grams, and uses a small battery for\npower. The disk itself is approximately the size of a US quarter dollar.\n22 This was the size of the US paper currency at the time Herman Hollerith invented the machines that used them. His\ncompany eventually became IBM.\n23 For example, the American Standard Code for Information Interchange (ASCII) code of the letter Q is\n01010001.\n11.\n116 Exhibit 39: A modern disk storage\ndevice, manufactured by Seagate\nTechnology\nProgress in communication technology\nPeople have communicated at a distance using fires, smoke, lanterns, flags, semaphores, etc. since ancient times,\nbut the telegraph was the first electronic communication technology. Several inventors developed electronic\ntelegraphs, but Samuel Morse\u2019s hardware and code (using dots and dashes) caught on and became a commercial\nsuccess. Computer-based data communication experiments began just after World War II, and they led to systems\nlike MIT\u2019s Project Whirlwind, which gathered and displayed telemetry data, and SAGE, an early warning system\ndesigned to detect Soviet bombers. The ARPANet, a general purpose network, followed SAGE. In the late 1980s, the\nUS National Science Foundation created the NSFNet, an experimental network linking the ARPANet and several\nothers\u2014it was an internetwork. The NSFNetwork was the start of today\u2019s Internet.24\nImprovement in the Internet illustrates communication technology progress. There are several important\nmetrics for the quality of a communication link, but speed is basic.25 Speed is typically measured in bits per second\n\u2014the number of ones and zeros that can be sent from one network node to another in a second. Initially the link\nspeed between NSFNet nodes was 64 kilobits per second, but it was soon increased to 1.5 megabits per second then\nto 45 megabits per second.26\n24 Press, Larry, Seeding Networks: the Federal Role, Communications of the ACM, pp 11-18, Vol. 39., No. 10,\nOctober, 1996, http:\/\/bpastudio.csudh.edu\/fac\/lpress\/articles\/govt.htm. A one page history of the early\nInternet and the ideas behind it http:\/\/bpastudio.csudh.edu\/fac\/lpress\/471\/hout\/netHistory\/.\n25 Packet loss and data transfer rates are also common measures of communication quality. Researchers monitoring Internet\nperformance have observed that between the spring of 1997 and fall of 2006 packet loss rates between North America and\nother regions of the world have typically fallen between 25 and 45 per cent per year. Packet loss rates to developed nations\nare under 1 per cent, but Africa has the highest loss rates and is falling further behind the other regions. They found\ncomparable improvement when monitoring data transfer rates. (xx reference to SLAC site).\n26 The letters k and m are abbreviations for kilo and mega respectively. One kilobit is 1,024 bits and a megabit is\n1,048,576 bits. For most practical purposes, we can think of them as 1 thousand and 1 million bits.\n117 Exhibit 40: The NSFNet backbone connected\nconnected 13 NSF supercomputer centers and regional\nnetworks\nSidebar: Commonly used prefixes\nMemory and storage capacity are measured in bits\u2014the number of ones and zeros that can be stored. Data\ntransmission rates are measured in bits per a unit of time, typically bits per second. Since capacities and speeds are\nvery high, we typically use shorthand prefixes. So, instead of saying a disk drive has a capacity of 100 billion bits, we\nsay it has a capacity of 100 gigabits.\nThe following table shows some other prefixes:\nPrefix 1,024n English Approximate number\nterm\nkilo 1 Thousand 1,000\nmega 2 Million 1,000,000\ngiga 3 Billion 1,000,000,000\ntera 4 Trillion 1,000,000,000,000\npeta 5 quadrillion 1,000,000,000,000,000\nexa 6 Quintillion 1,000,000,000,000,000,0000\nzetta 7 Sextillion 1,000,000,000,000,000,0000,000\nyotta 8 Octillion 1,000,000,000,000,000,0000,000,000\nNote that the numbers in the fourth column are approximate. For example, strictly speaking, a megabit is not\none million (1,000,000) bits it is 1,024 x 1,024 (1,048,576) bits. Still, they are close enough for most purposes, so\nwe often speak of, say, a gigabit as one billion bits.\nCapacities and rates may also be stated in bytes rather than bits. There are 8 bits in a byte, so dividing by 8 will\nconvert bits to bytes and multiplying by 8 will convert bytes to bits.\nThe NSFnet was the first nationwide Internet backbone, but today there are hundreds of national and\ninternational backbone networks. High speed links now commonly transmit 10 gigabits per second, and a single\nfiber can carry multiple data streams, each using a different light frequency (color).27 Of course, progress continues.\n27 A gigabit is 1,024 megabits.\n118 For example, Siemens researchers have reliably transmitted data at 107 gigabits per second over a one hundred\nmile link and much faster speeds are achieved in the lab.28\nThere has been similar improvement in local area network (LAN) technology. Ethernet is the most common\nLAN technology. When introduced in 1980 Ethernet links required short, thick cables and ran at only 10 megabits\nper second. Today, we use flexible wires, Ethernet speed is 10 gigabits per second today, and standards groups are\nworking on 40 and 100 gigabits per second.29\nIndividuals and organizations also use wireless communication. The WiFi30 standard in conjunction with the\navailability of license-free radio frequency bands led to the rapid proliferation of wireless local area networks in\nhomes and offices. When away from the home or office, we often connect to the Internet at WiFi hotspots, public\nlocations with Internet-connected WiFi radios. We also connect distant locations with wide-area wireless links\nwhen installing cable is impractical. And we can use satellite links to reach remote locations, but they are expensive\nand introduce a delay of about .24 seconds because of the distance the signal must travel.31\nCellular telephone networks are also growing rapidly. There were 1,263 million cellular users in 2000, and that\nhad increased to 2,168 five years later.32 (The number of wired telephone lines fell from 979 million to 740 million\nduring the same period). Cellular communication has improved with time, and, in developed nations, we are\ndeploying third and even fourth generation technology, which is fast enough for many Internet applications.\n28 http:\/\/www.siemens.com\/index.jsp?sdc_p=fmls5uo1426061ni1079175pcz3&sdc_bcpath=1327899.s_5,&sdc_sid=334871161\n05&.\n29 Gittlin, Sandra, Ethernet: How high can it go?, Network World, 11\/22\/06,\nhttp:\/\/www.networkworld.com\/research\/2006\/112706-ethernet-10g.html.\n30 WiFi is a trade name belonging to a trade association, the WiFi Alliance. It is an association of manufacturers who make\nequipment based on technical standard 802.11 if an engineering society, the Institute of Electrical and Electronic Engineers.\nThe WiFi alliance was formed to resolve standardization differences, publicize and market the technology, and test and\ncertify equipment as being IEEE 802.11 standard compliant.\n31 Radio signals propagate at the speed of light (300 kilometers per second) and the round trip distance to a communication\nsatellite is around 70,000 kilometers.\n32 International Telecommunication Union database, http:\/\/www.itu.int\/ITU-D\/ict\/statistics\/.\n119 Software progress\nThe hardware progress we have enjoyed would be meaningless if it did not lead to new forms of software and\napplications. The first computers worked primarily on numeric data, but early computer scientists understood their\npotential for working on many types of application and data. By 1960, researchers were experimenting non-\nnumeric data like text, images, audio and video; however, these lab prototypes were far too expensive for\ncommercial use. Technological improvement steadily extended the range of affordable data types. The following\ntable shows the decades in which the processing of selected types of data became economically feasible:\nExhibit 41.: Commercial viability of data types\nDecade Data Type\n1950s Numeric\n1960s Alphanumeric\n1970s Text\n1980s Images, speech\n1990s Music, low-quality video\n2000s High-quality video\nBut none of this would have been possible without software, and we have seen evolution in the software we use,\nand, underlying that, the platforms we use to develop and distribute it.33 Let us consider the evolution of software\ndevelopment and distribution platforms from batch processing to time sharing, personal computers, local area\nnetworks, and wide area networks.\nInternet resource:\nListen to TCP\/IP co-inventor Vint Cerf\u2019s Stanford University presentation on the Internet and its future.\nIn this historic video, Cerf\u2019s collaborator Bob Kahn and other Internet pioneers describe the architecture and\napplications of their then brand new research network.\nBatch processing\nThe first commercial computers, in the 1950s, were extremely slow and expensive by today\u2019s standards, so it was\nimportant to keep them busy doing productive work at all times. In those days, programmers punched their\nprograms into decks of cards like the one shown above, and passed them to operators who either fed the cards\ndirectly into the computer or copied them onto magnetic tape for input to the computer. To keep the computer\nbusy, the operators made a job queue\u2014placing the decks for several programs in the card reader or onto a tape. A\nmaster program called the operating system monitored the progress of the application program that was running\non the computer. As soon as one application program ended, the operating system loaded and executed the next\none.\nBatch processing kept the computers busy at all times, but wasted a lot of human time. If a programmer made a\nsmall error in a program and submitted the job, it was typically several hours before he or she got the resultant\nerror message back. Computer operators also had to be paid. Finally, professional keypunch operators did data\nentry using machines with typewriter-like keyboards that punched holes in the cards. This tradeoff of human for\ncomputer time reflected the fact that computers were extremely expensive.\n33 The software platform evolution outlined in this section was based on prototypes which had been developed in research labs\nmany years earlier. We are giving the chronology of their commercial deployment, not their invention. A short survey of that\ninvention is given in Press, L., Before the Altair \u2014 The History of Personal Computing, Communications of the ACM,\nSeptember 1993, Vol. 36, no 9, pp 27-33.\n120 Time sharing\nBy the early 1960s, technology had progressed to the point, where computers could work on several programs at\na time, and time-shared operating systems emerged as a viable platform for programming and running\napplications. Several terminals (keyboard\/printers) were connected to a single computer running a time-sharing\noperating system. Programmers entering instructions or data entry operators used the terminals. They received\nimmediate feedback from the computer, increasing their productivity.\nLet\u2019s say there were 10 programmers working at their own terminals. The operating system would spend a small\n\u201cslice\u201d of time\u2014say a twentieth of a second\u2014on one job, then move to the next one. If a programmer was thinking\nor doing something else when his or her time slice came up, the operating system skipped that person. Since the\ntime slices were short, programmers had the illusion that the computer was working only on their own job and they\ngot immediate feedback in testing their programs. The computer \u201cwasted\u201d time switching from one job to the next,\nbut it paid off in saving programmer time.\nExhibit 42: An early timesharing\nterminal\nTime-sharing terminals were also used for data entry, so we began to see applications in which users, for\nexample, airline reservation clerks entered their own data. Professional keypunch operators began to disappear.\nPersonal computers\nTime-sharing continued to improve resulting in a proliferation of ever smaller and cheaper \u201cmini-computers\u201d.\nThey might be the size of a refrigerator rather than filling a room, but users still shared them. As hardware\nimproved, we eventually reached the point where it was economical to give computers to individuals. The MITS\nAltair, introduced in 1975, was the first low-cost personal computer powerful enough to improve productivity. By\nthe late 1970s, programmers, professional users and data entry workers were using personal computers. They were\nmuch less powerful than today\u2019s PC, but they began to displace time-sharing systems.\nExhibit 43: MITS Altair\n121 Programmers could write and test programs on their own machines\u2014they could, for the first time, own their\nown tools and be independent of employers. They could also work at home, and hobbyist programming began to\nspread. Secretaries and typists began using personal computers for their work, using desktop computers with built-\nin programs for creating documents\u2014the original \u201cword processors\u201d. Knowledge workers\u2014managers, engineers,\nand others\u2014began using personal computers to increase their productivity. Software companies quickly offered\nprograms for word processing, maintaining small databases and doing spreadsheet calculations on personal\ncomputers.\nThe first personal computers used the same types of terminals as time-shared computers. They consisted of a\nkeyboard and a printer or screen, which could display characters, but not pictures or diagrams. As such, the user\ninterface of early personal computer operating systems was similar to that of time-sharing systems. The computer\ndisplayed a prompt indicating that it was ready for a command, which the user typed. For example, the user could\nerase a file called myfile.txt by typing the command:\n> delete myfile.txt\nAs personal computer hardware improved, it became feasible to move from character displays to displays which\ncould turn the dots making up the characters on and off individually. This made it possible to display and work with\nimages as well as characters. Graphic displays, when coupled with a pointing device like a mouse, ushered in\napplications and operating systems with graphical user interfaces (GUIs). One could now delete a file by dragging\nicon or small picture representing it to a trash can on the screen. The Apple Macintosh, introduced in 1984, was the\nfirst low-cost personal computer with a GUI operating system.\nExhibit 44: The\noriginal Apple\nMacintosh\nUsers liked GUIs because they did not have to memorize commands to use a program, and operating systems\nlike the Macintosh OS and Windows have come to dominate personal computer desktops. However, character-\noriented user interfaces are still popular among technicians and system administrators who operate and maintain\nour networks and back-end computers. They find typing commands faster than using a GUI once they have the\ncommands memorized. It is also possible to write small programs containing several commands to automate\ncommon multi-command tasks like creating user accounts.\nPersonal computers with GUI operating systems like the Macintosh OS and Windows quickly made their way\ninto homes and organizations. Their low cost encouraged the founding of many companies to develop software for\nthe personal computer platform. Some of these, for example, Microsoft, Lotus, Adobe, and Electronic Arts, are\nmajor companies today.\n122 Local area networks\nAt first, personal computers were stand-alone productivity tools. To share data with a colleague, one had to store\nit on a floppy disk or some other removable media and hand deliver it. Time sharing systems had enabled users to\ncommunicate with each other and conveniently share data, but early personal computers did not.\nThe solution to this problem was hardware and communication software for connecting the computers in an\noffice or a building, forming a local area network or LAN. There were several competing LAN technologies at first,\nbut they were proprietary. Because it was an open standard, Ethernet eventually became dominant.\nOnce connected to a LAN, users could share common databases and documents as well as hardware like\nprinters. Each user had his or her own computer, and common computers were added to the network. These\ncomputers were called servers, since they were programmed to offer a service like printing or database\nmanagement to the user\u2019s computers, the clients.\nThe LAN became the next important software development platform. Programmers developed client-server\napplications in which they separated the user interface program, which ran on the user\u2019s personal computer, from\nthe application logic and databases, which ran on servers. Programmers also had to develop software for hardware\nservices like sharing files, printers, and modems. As we see below, client-server applications are also prominent on\nthe Internet. When you use the Web, you are running a client program like Internet Explorer or Firefox on your\ncomputer (the client) and retrieving information from Web servers.\nSidebar: Internet client options\nThe client-server model has moved from the LAN to the Internet. Several client options are found on the\nInternet. The most common is the Web browser. Early browser-based applications retrieved Web pages with\nhypertext markup language (HTML ) tags added to control the formatting of the text and images.34 Any computer,\nregardless of its operating system, could use these applications, and very little skill was required to add HTML tags.\nHowever, the pages were largely static text and images. The next generation of Web clients could execute simple\nprograms included in Web pages, making dynamic behavior like cascading menus and buttons that changed\nappearance when clicked possible. These programs are generally written in the Javascript programming language.\nAJAX, asynchronous Javascript and XML, uses Javascript to download content without displaying it while the\nuser looks at a page. Consider for example Google maps. While a user is looking at a portion of a map, adjacent\nportions are downloaded and cached. When the user scrolls the display, the adjacent portions are displayed\nimmediately.\nAn applet is a program that is automatically downloaded and executed when the user links to an HTML page\nthat contains it. Applets are useful when a fairly complex client program is needed for an application. They are most\ncommonly written in the Java programming language.\nA Web browser can be augmented using a plug-in like the Macromedia Flash player, which enables your Web\nbrowser to show animations and movies. Once you install this plug-in, your browser will use it to play Flash movies\nfrom any Web site.\n34 For example, the tag <b> starts boldface text and the tag <\/b> ends it.\n123 For complex tasks rich user interfaces and specialized server software and databases, a Web browser may not\nsuffice. In those cases, the user must install a custom client program. That program, not a Web browser, would be\nused to interact with the server. These applications can be fast and complex, but the user must be able to install\nthem. Examples of websites that require custom clients include Apple's iTunes client, Google Earth and Microsoft's\nVirtual Earth 3D.\nWide area networks\u2014the Internet\nAround the time organizations were rolling out their early LANs, the Internet was beginning to spread within\nthe research and academic communities. The Internet was not the first wide area network (WAN). Earlier research\nnetworks had preceded it, and IBM and Digital Equipment Corporation both marketed widely used WAN\ntechnology to large organizations. There had also been several commercial WANs, which resembled time-sharing\nsystems with remote users dialing in using personal computers. The Internet was different in two key ways. First, it\nused public domain communication software developed by researchers while the IBM and DEC networks were\nproprietary, requiring one to use their hardware and communication software. Second, the Internet was a network\nof networks, an internet (small i). An organization could connect its LAN to the Internet regardless of networking\nhardware and software they used internally.\nThe Internet also differed from the telephone, cable TV and cellular phone networks. Those networks were\ndesigned to provide a specific service\u2014telephony or broadcast video, while the Internet was designed to provide\nlow-cost communication between computers connected at the edge of the network. In the Internet model, the users\nwho connect to the network invent and provide applications and services, not the network operator. The Internet\nwas designed from the start to be an end-to-end, \u201cdumb\u201d network that could connect complex devices (computers)\nat its edges.35\nThis end-to-end architecture was critical to the rapid expansion of the Internet. Any user could invent and\nimplement a new application. For example, when Tim Berners-Lee invented the World Wide Web protocol, he\nwrote Web client and server programs, and allowed others to copy them. Anyone could use his software to create\nwebsites and to retrieve information from other\u2019s websites. Contrast this to, say, the telephone network where only\nthe telephone company can decide to offer a new service like caller ID. On the Internet, anyone could be an inventor\nor an entrepreneur\u2014innovation and investment took place at the edge of the network.\nProgrammers realized they could use the Internet protocols within their organizations as well as on the Internet.\nThey began developing applications for their intranets\u2014accessible only to authorized employees\u2014and for extranets,\nwhich only selected stakeholders like suppliers and customers could access.\nInternet based software\nThe rapid spread of the Internet is well known, and its size and power have made it today\u2019s dominant software\ndevelopment platform. We have gone beyond client-server applications, and new forms of software are\ntransforming our individual work, organizations, and society. The Internet has facilitated and given rise to open\nsource software, user contributed content, composite applications, software as a service, mobile, portable and\nlocation-aware applications, long tail applications, and applications in support of collaboration.\n35 For more on the end-to-end design of the Internet see: Saltzer, J. H., Reed, D.P. and Clark, D.D., End-to-end\nArguments in System Design, Second International Conference on Distributed Computing Systems (April 1981)\npages 509-512. Published with minor changes in ACM Transactions in Computer Systems 2, 4, November 1984,\npages 277-288 and Isenberg, David S. The Dawn of the Stupid Network, Networker 2.1, February\/March 1998, pp.\n24-31.\n124 Open source software\nProgrammers typically write programs in a programming language. This is called the source program. The\nsource program is compiled (translated) into a machine-language object program that the user\u2019s computer can\nexecute. Commercial software companies generally distribute only the object program along with a license to use it\non a single computer. The source program is a trade secret.\nWith open source software, source and object code are publicly available at no cost, and programmers making\nmodifications or improvements often feed them back to the community.\nWhile sharing of open source software dates back to the batch processing era, the practice has thrived on the\nInternet platform.36 The culture of the academic and research community which developed the Internet was\nconducive to sharing. The Internet also eliminated the cost of software distribution and enabled programmers to\nform a community around an open source project and communicate with each other.\nWhile the Internet is now open to commercial activity (it was not at first), open source software is still widely\nused. Users may not realize it, but many websites, for example Google or Yahoo, run on open source software. The\nLinux operating system is perhaps the most significant open source program. Linux has undergone continuous\nimprovement since its launch, and is now found on everything from large mainframe computers to cell phones.\nIn 1991, while he was a student in Finland, Linus Torvalds posted the first version of Linux on the Internet. The\nemail announcing its availability ends with the line: \u201cAny suggestions are welcome, but I won't promise I'll\nimplement them :-).\u201d In keeping with the last sentence of his announcement, Torvalds has retained editorial control\nof Linux. This is typical of open source projects. One person is often in control, with a relatively small group of\nactive developers contributing significant upgrades and extensions. A greater number find and correct errors. Some\npeople do this as part of their work, others as a hobby.\nPerhaps the greatest contribution of the open source community is not its software, but the culture and\norganizational openness it engenders. Open source projects and management techniques have spurred the need for\napplications that support collaboration and have facilitated the creation of distributed organizations.\nInternet resource:\nPerspectives on Free and Open Source Software, edited by Joseph Feller, Brian Fitzgerald, Scott A. Hissam\nand Karim R. Lakhani, is an excellent anthology on open source software. There are 24 chapters from a well-\nknown group of authors on a wide variety of topics. You can download the book from\nhttp:\/\/mitpress.mit.edu\/catalog\/item\/default.asp?ttype=2&tid=11216&mode=toc or purchase a printed copy.\nMany open source projects are coordinated through online repositories like Sourceforge. Sourceforge hosts\nover 140,000 open source projects. Project pages allow for downloads of source and object programs,\ndocumentation, bug reports, feature requests, and many facilities for enhancing communication among the\nprogrammers.\nUser supplied content\nWith the moves form batch processing to time sharing to personal computers, users did more and more of their\nown data entry. The Internet extends this capability significantly. Users typically complete transactions without\ninvolving vendors. We pay income tax, renew automobile licenses, purchase goods for our homes and businesses,\n36 SHARE, a users group for IBM computers was formed to share software in 1955. Software was distributed in card decks or\non 9-inch tape reels.\n125 and contract for business and consumer services online. This is often more convenient and economical for the user,\nand nearly eliminates the transaction cost. Organizations are also able to aggregate and mine transaction data,\nlooking for patterns and preferences. This is common in high-volume retail applications like the Netflix movie\nrental service or online book sales. The retailer recommends movies or books based upon prior purchases of the\nuser and others.\nGoing beyond transaction processing, the Internet enables applications that derive value from user supplied\ndata. Take, for example, the case of Amazon.com, the first online bookstore. When Barnes and Noble, a large\nbookstore chain, established an Internet site, many predicted they would quickly displace the upstart Amazon since\nthey were selling essentially the same books. Amazon prevailed because they encouraged users to write reviews of\nthe books they purchased. Both companies sold the same books, but the customer reviews differentiated Amazon.\nMany of today\u2019s Internet applications center on user supplied content. websites allow users to publish their own\nmusic, videos, and photographs, and others to find and retrieve them. Users supply descriptions of everything for\nsale in online marketplaces like Ebay and Craigslist. Millions of users contribute to blogs (web logs) on the Internet,\nand many organizations use them to communicate internally and with outside stakeholders like customers,\nsuppliers and shareholders. The book you are now reading is a wiki, created by inviting users to draft and\nsubsequently revise and improve chapters. The online encyclopedia Wikipedia is perhaps the best known example\nof an application based solely on user content. Organizations often use wikis for applications like internal\nknowledge sharing, project planning and documentation. Audio and video podcasts, recorded material that is\ndelivered automatically to subscribers, are also common on the Internet and within organizations.\nWe might ask what motivates people to contribute explicit content without pay. Why take the time to improve a\nWikipedia article or submit a bug fix to an open source program? Why contribute to the book you are now reading?\nMotivation includes:\n\u2022 creating or writing something, perhaps a program or reference source, for one\u2019s own use\n\u2022 having the pleasure of knowing others with shared interest may find it useful\n\u2022 having fun creating it\n\u2022 enhancing one\u2019s reputation\nAs Yochai Benlker says: \u201cWe act for material gain, but also for psychological well-being and gratification, and for\nsocial connectedness.\u201d37 These motivations in conjunction with the fact that \u201cthe declining price of computation,\ncommunication, and storage have, as a practical matter, placed the material means of information and cultural\nproduction in the hands of a significant fraction of the world's population-on\u201d lead to significant, non-market\ninformation production.\nHow many millions of hours did Internet users contribute yesterday? What is the economic value of those hours\n\u2014 how does the Gross Contributed Product compare to the Gross National Product? What will it be in ten years?\nMight this non-market economy one day rival the market economy in importance? (Stay-at-home mothers and\ngrandmothers might ask the same question). If so, what are the implications for organizations or management or\nthe entertainment industry?\n37 Benkler, Yochai, The Wealth of Networks, www.benkler.org.\n126 Composite applications\nEarly websites were pure client-server applications\u2014a user retrieved information from a single server. Soon,\napplications began to emerge that, without the user knowing it, combined information from multiple servers. For\nexample, when one purchased a book from Amazon, it was shipped via United Parcel Service (UPS), but one could\ntrack its shipment from the Amazon website. Behind the scenes, the Amazon server queried the UPS server to get\nthe current shipping information then relayed it back to the user in an \u201cAmazon\u201d web page. This required\nagreement and coordination between programmers at Amazon and UPS. The UPS programmers had to give the\nAmazon programmers technical details on how to access information from the UPS server\u2014they had to define the\nUPS application program interface or API.\nBut, Amazon was not the only UPS customer who would like to provide shipment tracking information, so UPS\nand others began publishing their APIs so others could access their services. Today, companies commonly publish\nAPIs and encourage others to create composite applications, often called mashups, incorporating data or\ncomputation from their servers.38\nExhibit 32 illustrates a mashup. The application at Housingmaps.com is used to rent and sell apartments and\nhouses. In this case, we see listings in the USD 1500-2000 range in the western portion of Los Angeles. The data\nwere retrieved from Craigslist.com, an online application listing items for rent and sale, and displayed on a map\nfrom Google.com. It is noteworthy that the map on the left side is not a static picture. It offers the full function of\nGoogle\u2019s mapping service\u2014scrolling, zooming, panning and alternating street maps (shown here) and satellite\nphotos. The links on the right hand side retrieve detailed descriptions of the rental offerings from Craigslist.com.\nExhibit 45: A Google Map-Craigslist mashup\nWhile Google and Craigslist currently offer access to their applications on the open Internet at no cost, others\ncharge for such services. For example, Amazon operates an extensive online store, and they offer access to the\ntechnology that powers that store to others. Organizations as varied as Target Stores, the National Basketball\nAssociation, and Bohemica.com's bookstore on Czech culture all use Amazon.com Web services.\nAmazon, Google and Craigslist have extensive datacenters operating around the clock. They have made very\nlarge investments in programming and building expertise. The cost of expanding and maintaining Google\u2019s global\nmap database is very high, and Craigslist has a vast store of user contributed data. But, since their APIs are simple,\nit is relatively easy for a programmer to incorporate these valuable assets into a new application. Applications with\nopen APIs are becoming part of our information infrastructure, part of today\u2019s software development and delivery\nplatform.\n38 One directory, http:\/\/www.programmableweb.com, listed 2,300 such mashups in January 2007.\n127 Sidebar: Using a Web service to add audio to an application\nI maintain a blog as a supplement to a course I teach. The blog is at http:\/\/cis471.blogspot.com\/, and, if you visit\nit, you will see a link at the bottom of each post that reads listen to this article. In the right-hand column, you will\nalso notice a link reading Audio RSS Feed.\nIf you follow the first link, you will hear a slightly stilted sounding woman reading the text of the article. The\nprogram that does this text to speech conversion is quite complex and requires a fast computer, but that complexity\nis hidden. A server at Talkr.com does the conversion and the link to it required adding only one line of HTML code\nto the blog template:\n<a href='http:\/\/www.talkr.com\/app\/fetch.app?feed_id=25222&perma_\nlink=<$BlogItemPermalinkURL$>'>Listen to this article <\/a>.\nAdding the Audio RSS feed converts the blog to a podcast. A user who subscribes to the RSS feed automatically\nreceives audio recordings of articles when they are posted. Again, adding this complex feature required only a single\nline of HTML code in the blog template:\n<a href=\"http:\/\/www.talkr.com\/app\/cast_pods.app?feed_id=25222\"> Audio\nRSS Feed<\/a>.\nAdding these audio features brought the blog into compliance with university regulations on accessibility by\nblind people. Doing so took only a few minutes because of the simple API used by the service at Talkr.com.\nSoftware as a service\nSince the earliest days of computing, users have had the choice of owning and operating their own computers or\nrunning their applications at a remote service bureau. Whether one used the service by submitting a deck of cards\nor by typing information at a time-sharing terminal, the service bureau owned and operated the equipment.39\nAs computer costs fell and millions of computing professionals were trained, in-house computing grew much\nmore rapidly than the service bureau business. Today, nearly every organization operates its own IT department\nand either purchases software or develops it themselves; however, a trend back to software as a service may be\nemerging. As network speed and reliability increase, the case for hosted applications improves.\nAdvantages to running software as a service include:\n\u2022 The application can be run from anywhere, including by mobile workers.\n\u2022 There are savings in in-house IT infrastructure and personnel.\n\u2022 The service provider has specialized expertise and skilled employees.\n\u2022 The software vendor gets rapid feedback from users and can make changes.\n\u2022 The software can be continuously upgraded and debugged.\n\u2022 Upgrades do not have to be installed on client machines.\n\u2022 Initial investment by the user is minimized.\nOn the other hand, there may be problems with software as a service:\n39 Running jobs on remote computers was one of the key goals of the funding for ARPANet, the network\nthat preceded the Internet. ARPA wanted the researchers they funded to be able to run jobs on each\nother's computers. (See this historic paper).\n128 \u2022 Intruders may be able to access the application.\n\u2022 The service might not be as flexible and well-tailored as software written in house.\n\u2022 The service vendors may go out of business.\n\u2022 The service vendor may alter the service, pricing, etc.\n\u2022 Data may be insecure at the vendor's site, particularly if it is out of the country.\n\u2022 Exporting your data may be difficult or impossible, locking you into the service.\nIt is clear that the decision as to whether to use a software service or to run your own software internally\ninvolves both business and technical considerations. Even if a service does everything you want, and it is cost\neffective, it will be a bad choice if the company servers are frequently down or the company goes out of business.\nBusiness contracts and service level agreements are as important as technical specifications.\nInternet resources:\nSalesForce.com was an early, successful vendor of software as a service. Company founder Marc Benioff\nexpects hosted software to largely replace user-owned and operated software as networks spread and improve.\nBenioff feels hosted software levels the playing field between large and small organizations by enabling them all\nto use the same cheap, reliable programs. He also encourages outside programmers to use Salesforce.com tools\nto develop their own hosted applications. He describes his vision in this conference presentation.\nThe University of Arizona has turned to Google for electronic mail, and plans to use other Google services.\nASU IT director Adrian Sannier explains why he made that choice in this blog entry. Sannier feels that consumer\napplications, not military or business, are now driving innovation.\nMobile, portable and location-aware applications\nThe Internet platform combined with wireless technology enables a new class of mobile, portable and location-\nbased applications. In many parts of the world, it is now possible to connect to the Internet while outside the home\nor office. The connection may be fully mobile, for example, while walking or moving in a car, or portable, for\nexample, using a laptop computer in an airport.\nToday, we make these connections using the cellular telephone network or a WiFi access point.40 The cellular\nnetworks are large, generally national enterprises, while WiFi networks are decentralized. They may be as small as a\nsingle access point in a caf\u00e9 or hotel lobby or as large as a municipal network covering a city. WiMAX, a third\nwireless technology for Internet access, is partially standardized and may become important in the future.\nBoth WiFi and cellular networks are growing rapidly. Third generation cellular networks, which are fast enough\nfor many Internet applications, are being deployed in many cities and developed nations. WiFi networks are being\ndeployed by large and small entrepreneurs and, increasingly, by cities.41 Unless heavily congested, Wifi access is\ngenerally faster than third generation cellular access. Some devices can connect to either cellular or WiFi networks,\nautomatically choosing WiFi if it is available.42\n40 WiFi technology was initially developed for LANs inside home and offices, but it has been, somewhat unexpectedly, used in\nwide area networks as well.\n41 See http:\/\/www.muniwireless.com\/ for coverage of municipal networks and their applications.\n42 See, for example, http:\/\/www.theonlyphoneyouneed.com].\n129 Laptop computers are generally used for portable access, but they are too large and consume too much power\nfor mobile applications. There are competing designs or form factors for mobile access. Some devices are ultra-\nmobile PCs like the Microsoft Oragami shown below.43 Others are fairly standard looking cell phones.\nExhibit 46:\nMicrosoft's Oragami\nBetween these extremes, we see devices like the Apple iPhone, shown below. These smartphones combine\ntelephony, Internet access, music and video play, and contact and address books in one device. The marketplace\nwill determine the most popular form factor.\nExhibit 47: Apple\niPhone\nIn addition to communication functions like email and instant messaging, mobile workers are able to access the\nWeb and corporate databases. For example, a sales person might book an order or check the production or delivery\nstatus of an open order while at a customer\u2019s location or real estate appraiser or agent might send a photograph or\nvideo from a listed property to a central database. An October 2006 survey found that 64 per cent of mobile\nworkers use their smartphones to access enterprise applications and business data.44\nMobile connectivity also enables location-aware applications. One may determine their exact location in several\nways. If connected to the Internet, various databases and services are available. For example, Mapbuilder converts\nstreet addresses to geocodes (latitude and longitude) and displays the location on a Google map. The global\npositioning system (GPS) may also be used. A GPS receiver uses the signal transmission time to orbiting satellites\n43 http:\/\/www.microsoft.com\/windows\/products\/winfamily\/umpc\/default.mspx.\n44 Malykhina, Elena, Leave the laptop at home, Information Week, October 30 2006, page 47-9,\nhttp:\/\/www.nxtbook.com\/nxtbooks\/cmp\/infoweek103006\/index.php?startpage=51.\n130 to determine location. Where satellite signals are blocked, say by tall buildings, other techniques like estimating the\ndistance to cell towers or TV broadcast antenna may be used.45\nEmergency service (wireless 911) is a key location-aware application. It led the United States Federal\nCommunication Commission to mandate location-awareness for cell phones sold in the US. While not yet fully\nimplemented, the increased demand has driven cost down dramatically.46 While emergency service is driving the\nmarket, we can anticipate applications like directing ads to people in specific locations, location-specific search, taxi\nand delivery fleet scheduling, and automatic geocoding of photos when they are taken.\nThe long tail\nIn a conventional store, stocking a new item for sale is costly. Inventory must be acquired, shelf space allocated\nand the merchandise displayed. The cost of adding an item for sale at an online store is essentially zero. Low\nstocking cost leads to the concept of the \u201clong tail\u201d.\nThe long tail was documented in a 2003 study of Amazon.com. Eric Brynjolfsson and his colleagues estimated\nthe proportion of Amazon book sales from obscure titles.47 As we see in Exhibit 17, 47.9 per cent of Amazon's sales\nwere of titles ranked greater than 40,000th in sales. This leads a large online store like Amazon to carry\napproximately 3 million books compared to 40-100,000 for a large brick and mortar store. The result is increased\nprofit for the vendor, increased choice for the customer, and increased incentive for authors with niche products.\nExhibit 48.: The long tail of Amazon book sales\nSales rank Per cent of sales\n>40,000 47.9%\n>100,000 39.2%\n>250,000 29.3%\nThe long tail is not limited to books. It is even more important in the case of information goods, where it is not\nnecessary to deliver a physical object like a book. Consider, for example, the online music market. Both Apple\niTunes and eMusic, which specializes in music from independent labels, offer over 2 million songs in their online\ncatalogs.48 Google advertising is another illustration of the long tail. A large per cent of their ad revenue comes\nfrom advertising by millions of small, often local companies paying just a few cents per click (AdWords) or millions\nof websites, which display Google ads (AdSense). Vendors of software as a service also target the long tail\u2014small\norganizations wishing to use packages previously affordable only by larger organizations.\nInternet resources\nChris Anderson has extended and popularized the notion of the long tail, see:\n\u2022 Anderson's initial long tail article\n\u2022 Anderson\u2019s long tail talk\n45 Receiving GPS signals in doors has been difficult in the past, but the latest GPS chips can detect signals with power of only\n0.0000000000000000001 watts, enabling them to be used indoors.\n(http:\/\/www.infoworld.com\/article\/07\/01\/31\/HNubloxgps_1.html).\n46 The cost is under USD 1 in some circumstances. See\nhttp:\/\/www.eetimes.com\/news\/semi\/showArticle.jhtml?articleID=196900828.\n47 Brynjolfsson, Erik, Hu, Yu Jeffrey and Smith, Michael D., \"Consumer Surplus in the Digital Economy: Estimating the Value\nof Increased Product Variety at Online Booksellers\" (June 2003). MIT Sloan Working Paper No. 4305-03 Available at SSRN:\nhttp:\/\/ssrn.com\/abstract=400940 or DOI: 10.2139\/ssrn.400940.\n48 http:\/\/www.emusic.com\/about\/pr\/pr20061213.html.\n131 \u2022 The Long Tail blog\n\u2022 Anderson's The Long Tail book\n\u2022 Interview on The Long Tail and the way he wrote his book\n\u2022 Review of the book\nCollaboration support applications\nThe visionaries, who imagined, funded and worked on the research that lead to today\u2019s platforms and\napplications, understood that networked computers would allow for collaborative communities of common interest.\nFor example, in 1968, J. C. R. Licklider, who was instrumental in conceiving of and funding time-sharing, personal\ncomputers and network research wrote:\nWhat will on-line interactive communities be like? In most fields they will consist of geographically separated\nmembers, sometimes grouped in small clusters and sometimes working individually. They will be communities not\nof common location, but of common interest.49 (Emphasis in the original).\nThe early time sharing systems allowed people to send messages and share files, but there was little if any\nsoftware for explicit support of collaboration. Perhaps the first major application of collaboration support software\nwas in decision support rooms where users sat at computers connected to a local area network and shared a\ncommon, projected screen. These group decision support systems (GDSS) are used primarily for group decision\nmaking and consensus building. They are equipped with software for brainstorming, outlining, voting, etc.\nA GDSS room supported collaboration among people who were working at the same time (synchronously) and\nthe same location. With the advent of the Internet as a platform, software was developed to support people working\nsynchronously at different places, for example, software for chat, video conferencing, instant messaging, voice\ntelephony, sharing the same screen views and editing documents. Today, distant colleagues can open a chat\nwindow, wiki document, telephone session, and shared workspace and work together for hours.\nVirtual worlds like Second Life are also used for synchronous collaboration.50 The following shows a panel\ndiscussion in a virtual classroom51 belonging to the Harvard University Berkman Center.52 Each panelist and\nattendee designs an avatar, which they control and move around in the virtual space. We see the panelists\u2019 avatars\nseated at the front of the room and those of the attendees are seated around them. There are also shared spaces like\nthe \u201cflipchart\u201d to the right of the panel. Organizations are experimenting with meetings, press conferences,\nconcerts, and other synchronous events in Second Life.\n49 J.C.R. Licklider and Robert W. Taylor, The Computer as a Communication Device, Science and Technology,\nApril 1968, http:\/\/scpd.stanford.edu\/sonnet\/player.aspx?GUID=6F704E8F-352F-42E0-BC58-3F9014EA1F81.\n50 http:\/\/www.secondlife.com.\n51 http:\/\/www.flickr.com\/photos\/pathfinderlinden\/174071377\/.\n52 http:\/\/www.vedrashko.com\/advertising\/2006\/06\/panel-on-marketing-to-virtual-avatars.html.\n132 Exhibit 49: A virtual classroom in Second Life\nWe also have software that supports collaboration among people working at different times (asynchronously)\nand at different places. Examples here include email, voice mail, listservers, online questionnaires, threaded\ndiscussions or forums, blogs, and wikis. There are a also a number of network-based suites of standard productivity\ntools\u2014software for word processing, spreadsheets, database and presentation. These have all of the advantages and\ndisadvantages of other software as a service, but the documents they create are available online for sharing and co-\nauthoring. In some cases, these suites may replace conventional desktop productivity tools; in others, they will\ncomplement them.\nCollaboration support software has facilitated a trend toward increasing collaboration and distributed\norganizations, for example:\n\u2022 business extranets allowing employees from multiple companies to work together (outsourcing)\n\u2022 business intranets allowing employees from different locations within a firm to work together\n\u2022 ad hoc organizations set up for a particular event or emergency like the Tour de France or a hurricane\n\u2022 ad hoc organizations (perhaps within a company) set up to do a short-term particular project\n\u2022 open source software communities\n\u2022 cooperative industry organizations like the WiFi Alliance\n\u2022 loosely-coupled interest group like those who have purchased a particular product or are thinking of doing\nso\nIt has also reduced the cost of employees telecommuting\u2014working from home full or part time. Network-based\nsoftware also facilitates the outsourcing of programming, graphic design, accounting, x-ray interpretation, and\nother information intensive work. Telecommuting and outsourcing require skilled management, but can result in\nsignificant savings for organizations and benefit to employees and society if done well.\nInternet resources:\nFor a website devoted to GDSS, see http:\/\/www.dssresources.com\/.\nA brief discussion of the applications of group decision support is found at\nhttp:\/\/www.groupsystems.com\/resources\/custom\/PDFs\/Gartner-web_conferencing_amplifies_d_138101.pdf.\nFor a brief history of GDSS, see http:\/\/dssresources.com\/history\/dsshistory.html.\n133 Will the progress continue?\nThe exponential improvement we have seen in hardware has enabled the invention of new software and\napplications. If this improvement and innovation continues, it will have profound effects on individuals,\norganizations and society. But, will the progress continue?\nAt some point, physical limits cause exponential hardware improvement to level off. Today, semiconductor\ncompanies are mass producing chips with a feature size of 65 nanometers (billionths of a meter), and that is\nexpected to drop to 22 nanometers by 2011.53 Halving feature size every two years after that would imply transistors\nthe size of a single atom around 2033. Clearly, the current technology will reach physical limits before that time.\nDoes that mean that exponential improvement in electronic technology will end? Not necessarily, since we may\nshift to a different technology.\nThe logic circuits of the earliest programmable computers were constructed using electromechanical relays.\nThose gave way to vacuum tubes, which gave way to discrete transistors then integrated circuits. As we approach\nthe limits of current electronic technology, we may see a shift to something else, perhaps three-dimensional\nfabrication or growing of organic logic devices. We have seen similar progression in storage and communication\ntechnology. During the computer era, we have used punched cards and paper tape, magnetic tape, drums and disks,\noptical disks and electronic storage. Communication technologies have also shifted from the time of the direct-\ncurrent telegraph to today\u2019s multi-wavelength optical links.\nAs we reach the end of the exponential growth period of a given technology, entrepreneurs and inventors have\nincreased incentive to find replacements. But, what if we hit a double dead end? What if we reach the limits of IT\ntechnology and fail to find a replacement?\nThat is a possibility, but many argue that it is unlikely. Consider, for example, inventor and futurist Ray\nKurzweil54 who predicts continued exponential improvement in information and other technologies far into the\nfuture.55 He notes that new information technologies like writing, mathematical notation, printing, computers, and\ncomputer networks accelerate learning and the accumulation of knowledge.\nThe global distribution of research also gives us reason to be optimistic. Much of the largesse we are reaping\ntoday is rooted in research. Modern personal computers, computer networks, and applications were conceived of\nand prototyped decades ago. For example, Ivan Sutherland, shown here, used a room sized \u201cpersonal computer\u201d to\ndevelop very early image processing software. There have been many hardware and software refinements in the\nensuing years, but a modern personal computer running a graphic design program is an obvious descendent of\nSutherland\u2019s Sketchpad program.\n53 http:\/\/www.networkworld.com\/news\/2006\/121306-amd-seeks-efficiency-in-making.html?page=1.\n54 http:\/\/www.kurzweilai.net\/.\n55 Kurzweil, Ray, Law of Accelerating Returns, Lifeboat Foundation special report, 2001.\nhttp:\/\/lifeboat.com\/ex\/law.of.accelerating.returns.\n134 Exhibit 50: Ivan\nSutherland operating his\nSketchpad graphic design\nprogram in 1963\nMuch of the research underlying today\u2019s hardware and software took place in the United States. Today,\nimportant research is taking place in many nations. As technology reaches developing nations with unique\nproblems and ways of seeing the world, we will see even more innovation\u2014they will think outside our boxes.\nInternet resource:\nListen to Ray Kurzweil\u2019s presentation of his view of exponential improvement in technology and knowledge in a\ntalk at the Technology Entertainment and Design Conference.\nBumps in the information technology road\nWhile we cannot know the future with certainty, it seems safe to say that information technology will continue\nimproving at an accelerating rate for some time. But, even if that is the case, there are important barriers to its\nadoption and application. Is spite of technical progress, we are facing insufficient communication and data center\ncapacity to handle the next wave of video traffic. If we add the capacity, we will also need large amounts of electrical\npower. Equitable access to information technology will also require policy innovation. Organizations with vested\ninterests in the status quo and intellectual property assets may try to slow the adoption of technology. Privacy and\nsecurity concerns are also impediments.\nCapacity to handle video traffic\nVideo traffic is growing rapidly. Movies, television shows, news and documentaries, and amateur video are\nappearing on the Internet, and organizations are using video conferencing and podcasts to communicate and\ndisseminate information internally and externally. One can imagine that rather than sponsoring network television\nprograms, organizations will begin producing their own programs and distributing them over the Internet. But,\nvideo files are large. A single episode of an hour long TV drama recorded in the small screen iPod format is over\n200 megabytes.56\nVideo traffic will require innovation and large investments even in developed nations. Telephone and cable\ncompanies favor separating and charging for video traffic, in order to justify capital investment. Others have\nproposed technical solutions. One possibility is peer-to-peer networking, which automatically uses excess capacity\non user\u2019s computers to store and distribute files, appears to have great potential57. However, wide spread peer-to-\npeer networking would also require capital investment in \u201clast mile\u201d connections between Internet service\nproviders and users\u2019 homes and businesses.\n56 Cringley, Robert, Peering into the future, The Pulpit, March 2 2006,\nhttp:\/\/www.pbs.org\/cringely\/pulpit\/2006\/pulpit_20060302_000886.html.\n57 Norton, William B., Video Internet: The Next Wave of Massive Disruption to the U.S. Peering Ecosystem, September 29,\n2006, http:\/\/www.pbs.org\/cringely\/pulpit\/media\/InternetVideo0.91.pdf.\n135 Data center capacity and electric power\nAs technology changes, the costs and benefits of centralization of resources change. In the batch processing and\ntime sharing eras, computer operations were centralized within organizations. The personal computer enabled\nsignificant decentralization. Organization continued operating large mainframe computers, but the personal\ncomputers on our desks and in our homes outpaced them. In the early days of the Internet, organizations and even\nusers to operated their own servers. While many still do, economies of scale are leading to re-centralization, and\nservers are increasingly located outside the organization. Every major city has large buildings loaded with\ncommunication and computing equipment. The One Wilshire building,58 shown below, looks like any downtown\nLos Angeles office building from the outside, but inside it is packed with racks of servers and communication\nequipment.\nExhibit 51: One Wilshire, a Los\nAngeles data center\nData center capacity is in great demand. Keeping up with video and other Internet activity will require rapid\nexpansion and improved density. The economies of scale are such that datacenter ownership will become highly\nconcentrated, which may lead to non-competitive markets, exposing users to non-competitive prices and service.\nData centers also require great amounts of electric power for equipment operation and air conditioning. We see\nlarge operators like Google and Microsoft building data centers near sources of low-cost power like The Dalles,\nOregon.59\nInequity\nThe hardware and software advances we have spoken of are only widely available in developed nations, and even\nwithin those nations, there is substantial inequality. Information technology requires money and education, which\nare scarce in many parts of the world. The following map, showing values of the International Telecommunication\nUnion Digital Opportunity Index for each nation, illustrates this \u201cdigital divide\u201d.60\n58 For a photo tour of One Wilshire, see http:\/\/www.clui.org\/clui_4_1\/pro_pro\/exhibits\/onewilshire.html#.\n59 Gilder, George, The Information Factories, Wired, October 2006,\nhttp:\/\/www.wired.com\/wired\/archive\/14.10\/cloudware.html.\n60 World Information Society Report 2006, International Telecommunication Union,\nhttp:\/\/www.itu.int\/osg\/spu\/publications\/worldinformationsociety\/2006\/report.html.\n136 Exhibit 52: The digital divide\nSince telecommunication infrastructure is economically and socially important, nearly every nation has a\ntelecommunication policy. Some nations, like China, Singapore, Korea, Japan, and Chile enact policies and make\ninvestments designed to provide ample capacity, others take a more laissez-faire approach, counting on the efficacy\nof markets by encouraging privatization, independent regulation and competition. 61 There is growing evidence that\nthis laissez-faire policy has reached its limits and is failing to generate the investment needed to keep up with\nforthcoming software and applications.62 If that is the case, we need policy innovation as well as technical progress.\nVested interests can impede progress\nEvery nation enacts laws and policies regarding communication and information technology. The vested\ninterests of telephone, cable TV, and media companies and government agencies influence this policy. Political\nconsiderations are often more important than technological considerations.\nFor example, repressive governments face a \u201cdictator\u2019s dilemma\u201d in that free and open communication is\nimportant for the economy, but can undermine a regime, and, to some extent, all governments curb open\ncommunication. Many nations take steps to filter information coming in and out and restrict access to computers\nand the Internet. This may have to do with maintaining political control or with enforcing cultural norms, for\nexample, against pornography. The Open Net Initiative monitors government Internet filtering, as shown in the\nfollowing map.\nExhibit 53: Government Internet filtering levels\nIncumbent telephone companies and telecommunication ministries may fear a loss of revenue. For example,\ntelephone companies and ministries have traditionally received significant income for completing international\ncalls, but calls between two Internet-connected computers are free, and international calls routed from the Internet\n61 Consider, for example, Chile\u2019s Digital Agenda, http:\/\/www.agendadigital.cl\/aws00\/servlet\/aawscolver?2,inicio,,1,0.\n62 See, for example, Press, Larry and Dumans, Marie-Elise, The Digital Divide Report: ICT Diffusion Index 2005, United\nNations Conference on Trade and Development (UNCTAD), 82 pages, Geneva, July, 2006. (summary, full document).\n137 to a standard telephone cost only a few cents a minute. Those who stand to lose from such improvements often\nlobby against new competition.\nFor example, in 1996, the United States passed a Telecommunication Act designed to increase competition.\nWilliam Kennard, chairman of the United States Federal Communication from 1967-2001 was charged with\nimplementing the act. Near the end of his term, he expressed his frustration that \u201call too often companies work to\nchange the regulations, instead of working to change the market,\u201d and spoke of \u201cregulatory capitalism\u201d in which\n\u201ccompanies invest in lawyers, lobbyists and politicians, instead of plant, people and customer service\u201d (Kennard,\n2000). He went on to remark that regulation is \u201ctoo often used as a shield, to protect the status quo from new\ncompetition - often in the form of smaller, hungrier competitors \u2014 and too infrequently as a sword \u2014 to cut a\npathway for new competitors to compete by creating new networks and services.\u201d63\nIncumbent wireless communication companies\u2014radio and television stations and cellular operators\u2014lobby to\nkeep their licenses to broadcast at specified frequencies in a geographic area. These spectrum licenses are valuable\nassets for their owners, but the rapid growth of WiFi would not have occurred if it did not use license-free radio\nfrequencies. The current licensing regime is based on old technology. Modern radios are able to switch\ntransmission modes to avoid interference, but to take full advantage of those advances, we will have to overcome\nvested interests and open more license-free spectrum.\nInternet resource:\nBill Moyers\u2019 Public Broadcasting System documentary The Net at Risk illustrates the conservative impact of\norganizations with vested interest in the United States.\nIntellectual property restrictions\nConsiderations of intellectual property may impede information technology application. Publishers fear illegal\ndistribution of copyrighted material on the Internet. They use techniques like encryption to add digital rights\nmanagement (DRM) to their material. So, for example, most songs purchased at Apple\u2019s iTunes store may only be\nplayed on Apple devices and movies on DVDs cannot be copied into a standard video files on disk drives. Illicit\nprogrammers attempt to circumvent DRM techniques, and indeed, copyrighted material is widely available on the\nInternet. The threat of piracy tends to keep audio, video and print material locked away.\nOpen media advocates say DRM harms honest people who wish to make legitimate backup copies of material or\nto play it on more than one device\u2014for example watching a movie on either a computer or a TV screen. They argue\nthat lobbyists for powerful media companies, not consideration of the public interest, often determine copyright\nlaw. The United States exemplifies this. In 1790, Congress established a copyright term of 14 years. The term has\nbeen frequently extended (11 times in the last 40 years), and is now the life of the creator plus 70 years. Media\ncompanies contend that without strong copyright, they have no incentive to produce content.\nThere has also been copyright innovation. Richard Stallman and other early open source programmers\ndiscovered that their public domain programs were incorporated in proprietary products. In response, Stallman\ninvented the Gnu General Public License (GPL).64 Under the GPL, an author retains copyright, and others are\nlicensed to use the work as long as the derivative work also has a GPL license. Creative Commons (CC) licensing\n63 Kennard, W. (2000), \"Internet Telephony: America Is Waiting\", Voice Over Net Conference, September 12,\n2000, http:\/\/www.fcc.gov\/Speeches\/Kennard\/2000\/spwek019.html.\n64 http:\/\/www.fsf.org\/.\n138 offers other alternatives.65 For example, one CC variation allows others to copy, display or perform, but not modify,\na work as long as they credit the author and attach the same license to any derivative work.\nThere has also been marketplace innovation. For example, we saw that http:\/\/www.emusic.com has a library of\nover two million songs, all of which are sold without DRM. Apple\u2019s Steve Jobs reports that only 3 per cent of the\nsongs on a typical high capacity iPod music player use DRM, the rest are unprotected,66 and suggests doing away\nwith DRM for music. (Copies may still be watermarked, making each slightly different so they can be traced to their\nsource). Many print publishers, for example MIT Press Journals and the University of Michigan Press Digital\nCulture imprint, charge for printed copies, but offer downloaded versions without DRM.\nInternet resource\nPodcasting pioneer Doug Kaye spoke about the value of free information in a keynote presentation at the\nPortable Media Expo. Kaye feels you maximize the value of content by making it free and having no impediments\n(like registration or charging) to accessing it. You can listen to this 3m 19s excerpt or the entire presentation.\nListen to Stanford Law Professor Larry Lessig\u2019s presentation on the history of US Copyright and Creative\nCommons. You can also read the transcript.\nSecurity and privacy\nAs organizations and individuals interconnect and become dependent upon information systems, security and\nprivacy threats arise. We are all too familiar with criminal schemes to exploit these interconnected systems. Thieves\nhave stolen files with credit card and other information on countless individuals. Millions of computers are\ninnocently running \u201czombie\u201d software, which can be used to mass mail spam (unsolicited email) or flood a target\nserver with fake information requests in denial of service attacks. Criminals impersonate banks or other\norganizations in phishing schemes designed to trick users into logging onto their websites and divulging sensitive\ninformation. Governments also wage \u201cinformation warfare\u201d in trying to access and compromise other government\nand military systems.\nEfforts to protect the privacy of honest individuals may be in conflict with efforts of governments and law\nenforcement agencies trying to gather intelligence on terrorists or criminals. For example, Skype Internet telephony\nsoftware encrypts conversations. This protects the privacy of honest citizens discussing sensitive family or business\nmatters, but it also protects criminals. Government agencies now have the technical capability to track purchases,\nInternet searches, website visits, etc. of individuals or large groups of people.\nThese concerns impede the acceptance of information technology and its applications. They also raise the cost\nand reduce the efficiency of those that are implemented.\nSummary\nWe have traced the exponential improvement in electronic, storage and communication technology during the\nlast half century. That change has enabled qualitative shifts in the dominant software development and delivery\nplatform, beginning with batch processing, and extended by time sharing, local area networks, personal computers\nand now computer networks.\n65 http:\/\/creativecommons.org\/.\n66 Jobs, Steve, Thoughts on Music, February 6, 2007, http:\/\/www.apple.com\/hotnews\/thoughtsonmusic\/.\n139 Internet based software is now becoming dominant. Moving to the network platform facilitates open source\nsoftware, applications based on user-supplied content, composite applications, software as a service, mobile,\nportable and location aware applications, long-tail applications, and applications in support of collaboration.\nIt seems likely that this progress will continue as existing technologies are refined and new ones invented. Still,\nwe can see barriers, which will slow the adoption of new applications. Video applications will require enormous\nexpansion of our current communication and data center capacity and vast amounts of electric power. Inequity\u2014a\ndigital divide\u2014within and among nations will limit the access to the benefits of our progress. Conservative vested\ninterests work to preserve the status quo and we also face barriers in intellectual property restrictions, security and\nprivacy.\nExercises\n1. Exhibit 33 illustrates exponential growth in electronic technology, as measured by transistors on a chip.\nResearch improvements in storage technology measured in cost per bit, and plot a similar graph.\n2. Does your nation have a technology policy? Briefly describe it, providing links to relevant documents on\nthe Web.\n3. Does the government, private enterprise or a combination of the two operate telephone service? Cellular\ntelephony service? Internet service? Is there competition in these three areas or is there only a monopoly\nor oligopoly?\n4. How does Internet service in your nation compare with others? For example, what is the cost of home\nDSL service compared to other nations? How does the speed compare? What per cent of average monthly\nhousehold income is an Internet account in your nation? Is high speed Internet service available in every\npart of your nation?\n5. How long would it take to download a 200 megabyte copy of a one-hour television show using a 1 megabit\nper second DSL link? How long would it take to transmit it across a 10 gigabit per second backbone link?\nEstimate the number of copies of a popular TV show that might be distributed to homes and the time that\nwould take using DSL.\n6. Telephone and cable TV companies differentiate between telephony, video broadcast and Internet,\ncharging separately for each service. Water companies could also decide to enter the service business,\ndifferentiating and charging separately for drinking water, garden water, etc. That sounds kind of goofy\ndoesn\u2019t it? Design a spoof water bill that makes fun of the idea. What would your water billl look like if\nwater service were delivered by your telephone company?\n7. Create a blog using blogger.com or another online service and post something of interest to this course\neach week.\n8. Select a Wikipedia article on a topic with which you are familiar and interested, and make a substantive\nimprovement to it.\n9. Find an online forum on a topic with which you are familiar and start a new thread.\n10. Post a review of a book or other item that you already own on Amazon.com.\n140 11. Netflix, an online video rental service, offers 70,000 DVD titles. Estimate the total number of bits in\n70.000 DVDs. Assuming the average movie is 4 gigabytes, how many high-capacity PC disk drives would\nbe required to hold all of their titles? Estimate the number of years it will be before the entire Netflix\nlibrary will fit on a single disk drive.\n12. Contact the IT department at your school or an outside organization and find out if they are still running\nany batch processing applications.\n13. A GPS receiver can detect a signal with only 0.0000000000000000001 watts of power. How does that\ncompare with the power consumed by a light bulb in your home? How does it compare to the power of the\nsignal from a WiFi radio?\n14. It is interesting that, at this time, we are witnessing a fundamental shift in CPU chip architecture. As the\ntransistor density continues to increase, designers are now placing multiple CPUs on single chips. This\nshift to parallel processing is promising, but raises yet unsolved programming and inter-processor\ncommunication problems.\nChapter editor\nLarry Press, Professor of Information Systems at California State University-Dominguez Hills, has been studying\nthe global diffusion of the Internet, with an emphasis on policy and technology in developing nations, for over a\ndecade. Dr. Press and his colleagues at the Mosaic Group developed an early framework to characterize the state of\nthe Internet in a nation, and they, and others, have used this framework in many national case studies and surveys.\nThis work has been supported by organizations including Rand, The International Telecommunication Union,\nSAIC, UNDP, UNCTAD, and the US State Department as well as governments in developing nations. Dr. Press was\nalso an organizer and instructor in the World Bank\/Internet Society workshops, which trained over 2,500\nnetworking leaders from nearly every developing nation.\nDr. Press has worked in both industry and academia, consulted to a variety of industrial and multilateral\norganizations, and published numerous articles and reports. He has published two books, edited two book series,\nand been an editor or contributing editor for several magazines, trade publications and academic journals and\nperiodicals. He has also received awards for excellence in teaching. His MBA and PhD in information processing\nare from University of California, Los Angeles.\n141 8. Utilizing data for\nefficiency and effectiveness\nEditor: Ron L. Thompson (Wake Forest University, USA)\nReviewer: Geoffrey N. Dick (Australian School of Business, Australia)\nLearning objectives:\n\u2022 describe competing views of how decisions are made within organizations\n\u2022 describe ways in which data may be used to improve decision making, including:\n\u2022 controlling business processes\n\u2022 automating decision making\n\u2022 supporting complex decisions\n\u2022 augmenting knowledge\nIntroduction67\nIn a previous chapter, you learned how data are typically collected, stored and updated within information\nsystems. In this chapter, we take more of an applications perspective. That is, we examine how individuals and\norganizations use information systems (and more specifically, computer software applications) to make use of the\ndata that have been collected and stored.\nTo put this topic into context, we start by examining organizational decision making. Since decision making is at\nthe heart of organizational operations, it is important to understand the link between the use of information\nsystems and efficient and effective decision making.\nOrganizational decision making\nIn any organization, decisions must be made every day. Let\u2019s consider a grocery store, as an example. Someone\nneeds to decide how many employees need to be on duty, and what tasks they need to perform (who should be\nworking the check-out lines and self-checkout lines, who should be re-stocking shelves, who should be preparing\nfresh baked goods, etc.). Decisions need to be made about inventory control issues (e.g. what items need to be re-\nordered, and how many of each), pricing issues (e.g. what items to place on sale, how much to reduce their prices),\nand so on. In addition to the normal, daily decisions, there are many others that occur less frequently (e.g. whether\nor not to hire another employee, and if so, whom), and some that occur quite infrequently (e.g. whether or not to\nopen another store in another location). While decision-making environments vary substantially, they typically\nhave one thing in common. More specifically, ready access to good data, information and (in more complex\nenvironments) knowledge may lead to better (more efficient and effective) decision making.\n67 Some of the material in this chapter has been adapted from Thompson and Cats-Baril (2003).\n142 There are different views of how decision making does (and should) occur within organizations. We start with a\nquick overview of the rational perspective, and then briefly discuss alternative views.\nRational view\nThe rational view of decision-making describes an ideal situation that some people argue is difficult to achieve.\nIn an ideal world, organizational members charged with making a decision will ask all the correct questions, gather\nall the pertinent information, discuss the situation with all interested parties, and weigh all relevant factors\ncarefully before reaching their decision. This ideal world rarely exists, however. In reality, most decision makers are\nfaced with time pressures, political pressures, inaccurate or insufficient information, and so on. As a result, many\ndecisions that are made might appear irrational when viewed from the outside. Still, it is useful to employ the\nrational decision making model as a starting point.\nHerbert Simon (a Nobel Prize winner) proposed a model of rational decision making near the middle of the 20th\ncentury (Simon, 1960). His model includes four stages: (1) intelligence (is there a problem or opportunity?), (2)\ndesign (generate alternative solutions), (3) choice (which alternative is best?), and (4) implementation (of the\nselected alternative). The basic model of how rational decision-making should proceed is:\n\u2022 Intelligence phase\u2014collect data (and information) from internal and external sources, to determine if a\nproblem or opportunity exists. To the extent possible, ensure that the data are accurate, timely, complete,\nand unambiguous.\n\u2022 Design phase\u2014generate possible alternative solutions. Ensure that as wide a selection of alternatives as\npossible are considered.\n\u2022 Choice phase\u2014select the best alternative solution. Identify relevant criteria for evaluation, as well as\nappropriate weighting for each criterion, and use these to objectively weigh each alternative.\n\u2022 Implementation\u2014perform whatever steps are necessary to put the selected alternative into action.\nAs an example, think of getting dressed in the morning\u2014you gather intelligence, such as the weather forecast (or\nby looking out the window). You consider issues such as what you are doing that day, and who you are meeting. You\nthink about alternative clothing options, while considering constraints such as what clothes you have that are clean.\nYou might try on various combinations or outfits, and then make a decision.\nRationality assumes that the decision maker processes all information objectively, without any biases. In\naddition, this view of rational decision making implies that decision makers have a clear and constant purpose, and\nare consistent in their decisions and actions.\nWhile many people strive for rational decision making, and intellectually acknowledge that rational decision\nmaking is a preferred goal, the realities are often far from this ideal. For example, many decision makers do not\ntake the time to collect all relevant data, nor do they use well-known idea-generation techniques to help them\ngenerate a wide selection of alternatives.\nIn the above example (getting dressed in the morning), you may not go through all of the steps; instead, you\nmight just pull on the first clean clothes that you find, and occasionally will find yourself inappropriately dressed\n(clothing that is too warm, or too informal for a meeting that you forgot about, etc.).\n143 Alternative views\nBehavioral theorists argue that the rational view of decision making is too simplistic. Although decision makers\nmay go through the four phases of the rational model, they do not normally do so in any straightforward manner.\nMany decision makers, especially those with managerial responsibilities, have work that is fragmented, brief and\nvaried. They face constant interruptions (telephone, email, impromptu meetings), and can only spend a short\namount of time considering most decision situations. They are often under considerable stress, which can further\ndegrade their ability to follow a rational approach to making all of the decisions that are needed.\nAs a result, decision makers often take shortcuts to reduce the burden of making decisions. They typically rely\non a network of contacts (both inside and outside of their organization) to help them collect data and to come up\nwith alternative solutions to a particular problem or opportunity. The term \u201csatisficing\u201d has been used to describe a\ncommon approach, which is to settle for the first alternative that seems \u201cgood enough\u201d, rather than expending\nadditional time and effort to collect all available data and to consider all possible options.\nIn addition, human beings have many biases that can influence their decision making. We may be biased by our\nupbringing and educational background, by the opinions of persons whom we look up to, by our experiences in\nsimilar situations in the past, and so on. We also have personality traits (such as dogmatism, or a lack of creativity,\nor low willingness to accept risk). These biases and personality traits often keep decision makers from considering a\nfull range of alternatives.\nAlso, it has been noted that decisions are often reached based on \u201cpolitical\u201d motivations, rather than as a result\nof rational thought and consideration. For example, a purchasing agent might decide to obtain goods from a\nsupplier whose products and services are inferior (lower quality products, higher price, etc.) than those offered by a\ncompetitor, because he knows that his boss is a good friend of an important individual who works for the supplier.\nWhile rational decision making implies putting organizational goals above departmental or individual ones, we\nknow that this does not always happen.\nIn addition to situational factors (too many decisions requiring attention, inadequate data available, not enough\ntime to consider options fully, and so on), decision makers are human beings with limitations. We can only keep so\nmuch information available in our short-term memory (which makes comparing options more difficult), we are\npoor at seeing trends in data, and we are slow (and often inaccurate) in accessing information stored in our long-\nterm memory.\nThe reason for considering these issues, is to acknowledge they exist and then design information systems that\ncan help us overcome individual and situational limitations, to try and help us move closer to having the ability to\nuse a rational decision making process. For example, designing systems that provide summarized data (with access\nto more detailed data on demand) makes it easier for decision makers to retrieve the information they need, which\nincreases the probability that they will do so (rather than taking shortcuts). Similarly, we can design systems that\nhelp decision makers to see trends in data, to compare multiple options simultaneously, and to provide more\ntransparency in decision making (which reduces the probability of decisions being made for political reasons).\nAs an example, several large cities exist in dangerous areas\u2014near nuclear power stations or in areas that prone\nto hurricanes, tornadoes, or floods. City administrators have plans to evacuate in cases of emergency\u2014but think of\nthe complications. The time of the emergency (which will affect where people are and what transportation is\navailable), the amount of warning, the public transportation capacity (roads, railroads, airports), the severity of the\n144 emergency, the cost of the disruption\u2014all these need to be considered and possibly traded-off against one another.\nIdeally, planners will have developed comprehensive plans in advance that will have considered these issues and\ndeveloped plans to minimize loss of life and economic disruptions. One way to assist these planners is to provide\nthem with robust information systems that help forecast the impact of different possible scenarios, and also to help\nthem weigh trade-offs among competing criteria.\nDecision environments (degree of structure)\nObviously, not all situations that require decisions are the same. While some decisions will result in actions that\nhave a substantial impact on the organization and its future, others are much less important and play a relatively\nminor role.\nOne criterion that may be used to differentiate among decision situations is the degree of structure that is\ninvolved. Many situations are highly structured, with well-defined inputs and outputs. For example, it is relatively\neasy to determine how much to pay someone if we have the appropriate input data (e.g. how many hours worked\nand their hourly pay rate), and any relevant decision rules (e.g. if the hours worked for one week are greater than\n40, then overtime pay needs to be calculated), and so on. In this type of situation, it is relatively easy to develop\ninformation systems which can be used to support (or even automate) the decision.\nIn contrast, some decision situations are very complex and unstructured, where no specific decision rules can be\nreadily identified. As an example, assume that you have been assigned the following task: \u201cCreate the design for a\nnew vehicle that has at least a four-foot long truck bed, is a convertible (with a retractable hard-top roof), gets at\nleast 50 miles per gallon of gasoline, has a high safety rating, and is esthetically pleasing to a relatively wide\naudience.\u201d There is no \u201coptimal\u201d solution to this task; finalizing a design will involve many compromises and trade-\noffs, and will require considerable knowledge and expertise.\nWith this brief introduction, we move to a more detailed discussion of the role of information systems in\ndecision making.\nDecision making: systems view\nA previous chapter introduced the idea of viewing an organization as a system that acquires inputs, processes\nthem, and generates outputs. The organization interacts with its environment, in that it acquires inputs from the\nenvironment (e.g. purchasing parts from suppliers), and creates outputs that it hopes will be accepted by the\nenvironment (e.g. through sales of products to customers). The organization also receives feedback from the\nenvironment, in the form of customer compliments or complaints, etc. This way of perceiving an organization is\ntypically referred to as the systems view, in that the organization is essentially viewed as a system operating within\nan environment.\nAs discussed in a previous chapter, it is also possible to break the organization into a series of smaller sub-\nsystems, or business processes. For example, we might view the purchasing function as a system that accepts inputs\n(e.g. materials requests from the production process), processes them (e.g. reviewing pricing and delivery details\nfor a variety of suppliers), and generates outputs (e.g. purchase orders forwarded to specific suppliers). The\npurchasing process also receives feedback (details concerning orders received by the inventory control function,\netc.).\nFor many organizations, these business processes (organizational subsystems) are supported by information\nsystems. Before describing how this occurs, we need to define a few terms.\n145 Data, information and knowledge\nIn a previous chapter, definitions and examples were provided to help differentiate between the terms data,\ninformation, and knowledge. As was discussed, the term data is generally used in reference to representation of raw\nfacts. This might include mathematical symbols or text that is used to identify, describe or represent something,\nsuch as a temperature or a person. We should also note that this definition of data is considered by some people to\nbe rather narrow; the term is sometimes also used to include images, audio (sound), and video. For the purposes of\nour discussion in this chapter, we will focus on the more narrow definition of data.\nAgain referring to a previous chapter, information is data that is combined with meaning. A temperature reading\nof 10o will have a different meaning if it is combined with the term Fahrenheit or with the term Celsius. Additional\nmeaning could be added if more context for the temperature reading is added, such as whether the reading was for\na liquid (e.g. water) or a gas (e.g. air), or knowledge that the \u201cnormal\u201d temperature for this time of year is 20 o.\nAs such, the term information is generally used to imply data combined with sufficient context to provide\nmeaning for a human being.\nKnowledge can be thought of as information that is combined with experience, context, interpretation and\nreflection. We will expand on this definition as we discuss specific examples later in this chapter.\nInformation System as an Input-Process-Output Model\nOne way of viewing possible relationships between data, information and knowledge is to consider an\ninformation system from the perspective of an IPO (input-process-output) model. On the input side we have data,\nas discussed previously. These data are then massaged or manipulated in some way (e.g. sorting, summarizing,\nfiltering, formatting) to obtain information. Note that the transformation of data into information may be\ncompleted by a person (e.g. using a calculator) or by a computer program, although for our purposes we are\ntypically more interested in situations where computer programs are employed.\nA simple example could be the \u201cwhat if\u201d type of analysis that an electronic spreadsheet package offers. We can\nuse our current understanding of a situation to develop a model of how sales will go up or down by a certain factor\nbased on the amount we spend on advertising and other factors such as price.\nThe resulting information is used by a human to reach decisions (how many people to hire, how many products\nto produce, how much to spend on advertising). The outcomes of these decisions are observable results, such as\nsales volume during a certain time period, or the number and size of back-orders, etc. If these objective outcomes\n(results) are monitored and examined, then knowledge may be gained (e.g. how to avoid inventory shortages, or\nhow to balance inventory carrying costs against costs associated with product shortages).\nThe role of feedback\nIn addition, the observable results can be used to provide feedback into the system. This feedback can be used to\nhelp improve knowledge. The improved knowledge can then be used in two ways. First, it can be used to determine\nwhat (if any) changes are needed in the way data are transformed into information. For example, it might be\ndecided that summarizing sales data by product category and time period is not adequate, and that a breakdown by\ngeographic region is also needed. If such a determination is made, the result could be a change to a computer\nprogram to provide sales reports in a new format, showing information that was previously hidden within the raw\ndata.\n146 The second way that knowledge can influence the information system is that it can be used by decision makers\nto help them interpret information, influencing future decisions and actions. An example could be a review of\noutstanding debts in the light of prevailing or expected changes in interest rates. In this way, the quality of the\ndecisions reached should hopefully improve over time, leading to more effective actions.\nOrganizations do not operate within a vacuum; they interact continuously with their environment. As such,\norganizations need to constantly adjust to changes in their environment. Similarly, information systems should not\nbe viewed as being static. As new knowledge is obtained, information systems are modified, updated and expanded\nto address challenges or to take advantage of opportunities.\nUsing data to improve decision making\nMost important management activities can be viewed from a decision making perspective. Within organizations\n(especially larger ones), numerous decisions are being made on a continuous basis. For example, decisions about\nhow to design an assembly line in a production facility, and how to structure work tasks for employees working on\nthe line, will have direct and substantial impacts on the efficiency and effectiveness of the use of resources\n(employees, production materials). Not surprisingly, many organizations have expended considerable resources to\nacquire or develop information systems that are designed to help improve the efficiency and effectiveness of\norganizational decision making.\nA wide variety of terms have been used to describe information systems that are designed to support the\ndecision making of organizational members. These include decision support systems (DSS), group decision support\nsystems (GDSS), executive information systems (EIS), knowledge management systems (KMS), and business\nintelligence (BI). Additionally, the term expert system (ES) is often used to describe systems that attempt to\naugment human knowledge by providing access to reasoning used by experts in reaching their decisions.\nOn occasion, the term managerial support system or management support system (MSS) is used as an umbrella\nterm to encompass these diverse (yet related) types of information systems (Benbasat and Nault, 1990; Clark et al.,\n2007). While each type of system has some unique aspect (e.g. DSS are designed to support one individual, while\nGDSS are used by groups; EIS are geared toward the unique monitoring and control needs of individuals that are\nhigher in the organization; and so on), they also share some common elements. At their core, all are designed to\nimprove decision making within organizations.\nRather than examining each of these related types of system in detail, we will focus on the functions that\norganizational members need to perform and decisions they need to make, and then show how information systems\nmay be used to support them. In doing so, we\u2019ll also see how different types of management support systems can\ncome into play.\nControlling\nOne important function that needs to be performed within organizations is that of control, and managers are\nfrequently charged with controlling certain organizational processes (or functions). Data, and information, are\ngenerally essential components for aiding control.\nAs an example, consider the task of managing an assembly line in a production facility. For the purposes of\nillustration, we\u2019ll use the example of a production facility that assembles office chairs. The facility obtains parts\nfrom suppliers, assembles them into chairs, and releases the chairs to customers. The customers are distributors\nwho then sell the chairs to the ultimate buyers (mostly larger companies that buy office chairs in bulk).\n147 Note that for the moment, it isn\u2019t important to distinguish who has the responsibility for ensuring that the\nassembly line uses resources efficiently in creating chairs that are of sufficiently high quality; it could be a shift\nmanager, or it could be the employees working on the line. Either way, certain data need to be captured, and certain\ninformation created, in order to control the operations of the assembly line.\nTo be more specific, assume that a decision has been reached to keep track of each part (chair back, right chair\narm, etc.) that is used as input; this is accomplished by ensuring that each part has a UPC (Universal Product Code)\nbar-code affixed to it when it is received from a supplier, and each part\u2019s bar-code is scanned by a bar-code reader\nbefore being used in a chair\u2019s assembly. When a part is scanned, the information contained on the bar-code is\ncopied and stored in a production database. In addition, as each part is added to the chair moving through the\nassembly line, a record is kept (in the database) of the time at which the part was scanned. When the chair has been\ncompletely assembled, it is placed inside a plastic bag, and either a UPC bar-code or an RFID (radio-frequency\nidentification) tag is attached (depending on the needs of the customer). The bar-code is then scanned (or the RFID\ntag is read by an RFID reader), which records the time at which the chair was completely assembled and ready for\nstorage (or shipment). This record is also added to the production database.\nOne way of using data to control this process is to constantly monitor the length of time it takes from when the\nUPC bar-code for the first part is scanned to when the bar-code (or RFID tag) for the assembled chair is scanned.\nBy recording this information over a period of time, it is possible to obtain a distribution of observations (e.g. the\nmean [average] length of time taken to assemble a chair is 15 minutes, and the standard deviation is 1.5 minutes).\nUsing this information, it would be possible to write a computer program to monitor the times taken as each chair\nis produced, and notify someone if the time taken is excessively long or unusually short. Note that a person (or\ngroup of people) would determine the rule for identifying exceptions (based on past experience), and the software\nwould be programmed to enforce the rule.\nUsing this approach, a shift manager could be alerted, for example, when a chair takes longer than normal to be\nassembled. The shift manager could then investigate possible reasons for the delay (e.g. a temporary delay occurred\nwhen there were several defective left chair arms in a pallet; the immediate supply of left chair arms was depleted\nfaster than the right chair arms, and a fork-lift truck had to be sent to the parts storage area to retrieve another\npallet of left chair arms). As a result of this delay, the shift manager might institute or revise a policy to reduce the\npossibility of a similar delay occurring in the future.\nNote that the scenario described above is only one of many possibilities of how this business process might be\ndesigned and controlled, and hence how an information system could be designed to support it. For example, an\nalternative would be to have the employees on the assembly line responsible for controlling the assembly process.\nInstead of notifying someone of the time taken after a chair has been completely assembled, it would be possible to\ncompare the time from the start of assembling a chair until each part is scanned, and therefore it would be possible\nto know much sooner if a problem is occurring. As a general rule, the business process should be designed first, and\nthen the information system should be designed to best support the process.\nAutomating decisions\nWhenever possible, organizations strive to automate certain types of decisions. Automating decisions can be\nmuch more efficient, since you are essentially allowing a computer program to make a decision that was previously\n148 made by a human being. In addition, automating decisions can lead to more consistent and objective decisions\nbeing reached.\nEarlier in this chapter, we discussed the issue of the degree of structure for decision situations. Basically, the\nmore highly structured the decision situation, the easier it is to automate it. If it is possible to derive an algorithm\nthat can be used to reach an effective decision, and the data that are needed as input to the algorithm can be\nobtained at a reasonable cost, then it typically makes sense to automate the decision.\nFor example, the chair assembly company discussed previously might find it possible to automate the decision\nas to when to request the transportation of a pallet of parts from the parts storage area to the assembly line. This\ncould be done by scanning not only the parts that are used in the chair assembly, but also any defective parts (which\ncould be tagged as defective and noted as such in the database through a touch-screen monitor, keyboard or mouse\nat a workstation on the assembly line). By monitoring all parts that are removed from the temporary storage on the\nassembly line, the information system could determine when a pre-determined re-order level has been reached, and\nissue a request to the next available fork-lift operator to deliver the needed parts to the assembly line.\nDavenport and Harris (2005) offered a framework for categorizing applications that are being used for\nautomating decisions. Most of the systems that they describe include some type of expert system, often combined\nwith aspects of DSS, GDSS, and\/or EIS. The categories they provided include:\nSolution configuration\u2014these systems are employed to help users (either internal staff or customers) work\nthrough complex sets of options and features to select a final product or service that most closely meets their needs.\nExamples might include configuring a large or medium-sized computer system or selecting among a wide variety of\ncellular telephone service plans. The underlying computer programs would involve a type of expert system,\nincluding a set of decision rules that have been obtained from experts from the decision context.\nYield optimization\u2014describes systems which use variable-pricing models to help improve the financial\nperformance of a company, or to try and modify the behavior of people in some way. One example would be an\nairline, where 10 different people on the same flight might pay 10 different amounts for their tickets, depending on\nwhen they purchased the ticket, how frequently they fly with that airline, how full the flight is when they book their\nticket, and so on.\nRouting or segmentation decisions\u2014these systems perform a type of triage for handling incoming requests\nfor information or services. Examples include systems that balance the loads on Internet Web servers by routing\nrequests to different servers, or systems for insurance companies that handle routine insurance claim requests and\nonly route exceptional (unusual) requests to human claims processors.\nCorporate or regulatory compliance\u2014these systems ensure that an organization is complying with all\ninternal or external policies in a consistent manner. For example, mortgage companies that want to sell mortgages\non the secondary market have to ensure that they comply with all of the rules of that market when they are\npreparing the original mortgage. Similarly, insurance companies have to comply with federal and state regulations\nwhen writing insurance policies.\nFraud detection\u2014these systems provide a mechanism for monitoring transactions and noting possible\nfraudulent ones. The approach used might be very simple, such as checking for a credit card\u2019s security code (in\naddition to the credit card number, to prove the person physically has the card in their possession). Other\n149 approaches can be quite sophisticated, such as checking for purchases that seem to be out-of-character for the\ncredit card holder (based on past purchasing history). By automatically identifying potentially fraudulent\ntransactions, and then having a human operator contact the card holder to verify the transaction, the credit card\ncompany can reduce fraud losses and increase their customers\u2019 satisfaction.\nDynamic forecasting\u2014organizations all along a supply chain can decrease their costs of operations by\nreducing the amount of product (raw materials, work-in-process, finished goods) that they hold in inventory.\nDynamic forecasting systems (that use historical sales data etc.) help manufacturing companies align their\ncustomers\u2019 forecasts with their own internal plans. This in turn helps them to reduce their inventory carrying costs\nand make more efficient use of their production resources (facilities and people).\nOperational control\u2014these systems monitor some aspect of the physical environment (such as wind speed or\nrainfall amount) or some type of physical infrastructure (such as an electrical power grid or a communications\nnetwork). If an unusual event occurs (such as a sudden surge in electrical power at one point in the electrical grid),\nthe system automatically performs some type of action (such as shutting down some nodes and re-directing power\nover others).\nMany management decision situations are not highly structured, however, and hence cannot (or should not) be\ncompletely automated. Next we describe systems that are designed to support decision making, rather than\nautomate it.\nSupporting complex decisions\nIn an unstructured decision context, there may be numerous factors or variables that need to be considered.\nOften, an attempt to find the \u201cbest\u201d decision with respect to one factor will lead to a poor solution with respect to\nanother. Even when the situation is very complex, however, it is often possible to use information systems to help\nsupport the decision-making context.\nAn entire branch of management theory and practice, termed management science, has evolved to try and bring\nmore structure to unstructured decision situations. Management science is based on the application of\nmathematical models, and draws fairly heavily on the use of statistical analysis techniques. Examples include the\nuse of regression analysis (to assess possible empirical relationships), simulation (to identify potential solutions by\nvarying certain assumptions), and optimization models (to generate a \u201cbest\u201d solution when resource constraints\nexist).\nWhen a mathematical model fits well with reality, it may be possible to create an information system to help\nautomate the decision. When the fit is less than perfect, we need to augment the use of the model with the judgment\nof a human decision maker. The term decision support system is sometimes used to describe information systems\nthat are designed to help address unstructured decision situations. Many decision support systems use\nmanagement science techniques to provide decision makers with alternative options.\nDecision support systems do not necessarily need to be large, complex information systems. For example, a sales\nmanager for the chair assembly company might use spreadsheet software to develop a forecasting model that could\nbe used to predict demand levels for a product (line of chairs). After building the model to include criteria believed\nto impact demand (price, success rates for promotional [marketing] campaigns, etc.) the sales manager could use it\nto help forecast demand and then decide what demand levels to forward to the production group.\n150 Consider a more complex example. In the early 1990s, American Airlines was faced with the daunting task of\nscheduling about 11,000 pilots and 21,000 flight attendants on close to 700 airplanes on flights to over 200 cities.\nIn addition, they had certain constraints, such as the maximum time pilots and flight attendants can be in the air\nduring a specific time period. This problem could generate between 10 and 12 million possible solutions (U.S. News\n& World Report, 1993).\nThe scheduling challenge facing American Airlines (and every other major airline) is very complex. When you\nconsider overtime costs, labor contracts, federal labor mandates, fuel costs, demand for routes, and so on, it is\nobvious that there is no perfect solution. If a solution is derived that is \u201cbest\u201d for one dimension (e.g. reducing\novertime pay), another dimension (such as holiday preferences) will likely be compromised. To address the\nsituation, American Airlines spent over two years working on a scheduling system that used management science\ntechniques. The result was an information system that saves the company between USD 40 and USD 50 million per\nyear, by reducing wasted flight crew time.\nThe scheduling information system uses data such as flight crew availability (e.g. federal mandates concerning\nnecessary \u201cdown time\u201d between flights, etc.), flight crew capabilities (e.g. pilots licensed to operate certain types of\nairplanes), flight crew preferences (e.g. base airport, requested vacation days, etc.), airplane characteristics (seating\ncapacity, range, etc.), and route characteristics (e.g. distance, historical demand, etc.) as input. It then uses the\noptimization rules and logic embedded in the software to generate possible schedules that satisfy as many of the\nconstraints as possible. The proposed schedules could be viewed as information, which is then used by decision\nmakers (those responsible for scheduling airplanes and flight crews) to produce final schedules.\nDecisions concerning the scheduling of airplanes and flight crews to flight routes has observable outcomes (e.g.\nthe number of times a flight is delayed because a flight crew was delayed, the number of flights crew members\ncomplete over a given time period, and so on). These outcomes can be measured and examined, leading to greater\ninsights (knowledge) which can then be used to fine-tune the rules and logic in the scheduling information system.\nKnowledge management\nWhen experienced people retire or leave an organization, typically their knowledge leaves with them. In\naddition, many larger organizations (e.g. major information technology consulting firms) have many people who\nhave similar responsibilities (e.g. IT consulting) that could benefit from each others\u2019 experiences, but because of the\nnumbers involved (and geographical separation) personal communications among the employees is not practical. A\ntype of information system that is designed to help address these situations is often referred to as a knowledge\nmanagement system (KMS).\nKnowledge management systems can take many different forms, but the basic objectives are to (a) try and\nfacilitate communications among knowledge workers within an organization and (b) try to make the expertise of a\nfew available to many. Consider an international consulting firm, for example. The company will employ thousands\n(or tens of thousands) of consultants across numerous countries. It is quite possible (in fact, quite likely) that one\nconsulting team in, say, Spain is trying to solve a problem for a client that is very similar to a similar situation that a\ndifferent consulting team in Singapore already solved. Rather than reinventing a solution, it would be much more\nefficient (and effective) if the team in Spain could use the knowledge gained by the team in Singapore.\nOne way of addressing this situation is to have case histories for all client engagements posted to a case\nrepository, which employees from all over the world can access (using the Internet) and search (using a search\n151 engine). If the case documentation is of good quality (accurate, timely, complete, etc.), then the consultants will be\nable to share and benefit from each others\u2019 experiences and the knowledge gained. Unfortunately, however, it is\noften difficult to get employees to contribute in a meaningful way to the knowledge base (since they are probably\nmore concerned about moving forward on their next client engagement, rather than documenting their experiences\nwith the last one). In order for such systems to have any chance of working successfully, management may need to\nconsidered changes to reward systems and even to the organizational culture.\nA different approach to knowledge management focuses more on identifying (and storing) details about the\nexpertise of employees, and then allowing other employees to locate and contact these internal experts. This\napproach also has weaknesses, however, since the \u201cexperts\u201d may spend so much time responding to requests and\neducating other employees that they have little time for completing their own work. As a result, employees may\nhesitate to volunteer information about their expertise.\nBusiness intelligence\nThe term business intelligence (BI) is generally used to describe a type of information system which is designed\nto help decision-makers spot trends and relationships within large volumes of data. Typically, business intelligence\nsoftware is used in conjunction with large databases or data warehouses. While the specific capabilities of BI\nsystems vary, most can be used for specialized reporting (e.g. summarizing data along multiple dimensions\nsimultaneously), ad hoc querying, and trend analysis.\nAs an example, consider a large grocery store chain. Similar to most competitors, the store keeps track of all\npurchases, down to the item category level. By that, we mean that the store keeps track of all the items that are\npurchased together (e.g. a package of diapers, a bag of potatoes, etc.) on a single receipt. The detailed data is\ncaptured and stored in a large database (and possibly copied into a data warehouse). Once that data is available,\ndata analysts use business intelligence software to try and identify products that seem to be purchased together\n(which can help in product placement decisions), evaluate the success of marketing promotions, and so on.\nWhen you consider the extent of the data that is captured at a check out, (goods purchased, prices,\ncombinations, what is not purchased, date and time, how payment was made, and that much of that data can be\ncombined with other data such as information available about the purchaser on loyalty cards, advertising\ncampaigns, weather, competitors activities etc.) you begin to see the extent of the possibilities.\nAs with knowledge management systems, the value of business intelligence systems can be hindered in several\nways. The quality of the data that is captured and stored is one concern. In addition, the database (or data\nwarehouse) might be missing important data (for example, the sales of ice cream are probably correlated with the\ntemperature; without temperature information, it might be difficult to identify why sales of ice cream increase or\ndecrease). A third challenge can be that while data analysts may know how to use the BI software, they may not\nknow too much about the context for the organizations operations. In contrast, a manager may know the\norganization, but not know how to use the BI software. As a result, it is not uncommon to have a team (a manager\npaired with a data analyst) to try and get the most information (and\/or knowledge) out of a business intelligence\nsystem.\n152 Conclusion\nEven if people wish to use a rational approach to decision making, significant obstacles face those who attempt\nit. Decision makers within organizations are frequently faced with stressful environments of tight deadlines,\ninterruptions, multiple issues requiring attention, and inadequate information.\nInformation systems may be used in many ways to improve decision making. Systems which capture raw data\nand transform it into information can help control business processes. Decisions which are very structured may be\nautomated. For more unstructured or complex decision environments, decision support systems can help bring\nmore structure and provide an initial assessment of alternatives. Expert systems can help capture the expertise of\none or more humans, and make that expertise more widely available. Knowledge management systems can be used\nto both retain organizational knowledge when people leave, and to facilitate knowledge sharing among\norganizational members. Business intelligence systems are used to identify trends and relationships within large\ndatabases or data warehouses.\nIn short, information systems can be used in many ways to improve both the efficiency and effectiveness of\norganizational operations and decision making.\nExercises\n1. Identify someone you know that works for a relatively large organization (e.g. a bank, insurance company,\nmanufacturer, electric utility, etc.). Ask that person to identify an information system (along the lines of the\nones mentioned in this chapter) that is used by the organization to automate or support decision making. By\ninterviewing your contact person (and possibly others within the organization, if appropriate), try to address\nthe following issues:\na. Describe the data that is used as input to the information system. How is the data obtained? How is it\ninput into the system? How good is the data (e.g. using characteristics such as accuracy, timeliness,\ncompleteness)? How expensive is it to obtain the data?\nb. How were the decision rules or algorithms that are used by the system developed? By interviewing\nexperts? By reviewing historical data?\nc. What outputs (e.g. recommendations, decisions, actions) are generated by the system? Does the system\nmake decisions, provide recommendations, or both?\nd. Does the company keep track of the outcomes of decisions made (or recommended) by the system? If so,\nhow?\ne. Does the system get updated or modified (e.g. by updating the decision rules)? If so, how?\nf. In what ways does the system improve the efficiency and\/or effectiveness of the organization? Does the\norganization attempt to measure the impact of the system? If so, how?\n2. Think of an organization that you are familiar with (e.g. a university, a bank, an online retailer). Identify:\na. Two decisions that must be made on a regular basis which are highly structured, and hence probably\ncould be automated. Are these decisions currently automated, or performed by humans? If they are not\nautomated, how challenging would it be to do so? Would it make sense to create a system (or systems) to\n153 automate them? Consider issues such as how many people are currently used to make the decisions, how\nmuch their salaries are, how long it takes them to make decisions, etc.\nb. Two decisions that are unstructured, and hence probably should not be automated. Even if they are not\nautomated, are there ways that an information system could provide decision-makers with information\nthat could help them make the decisions? What information would help? Is it currently available to the\ndecision maker?\nReferences\nAnderson-Lehman, R., Watson, H.J., Wixom, B.H., and J. A. Hoffer (2004). Continental Airlines Flies High\nwith Real-Time Business Intelligence. MIS Quarterly Executive. 3(4), 163-176.\nBenbasat, I. and B.R. Nault (1990). An Evaluation of Empirical Research in Managerial Support Systems.\nDecision Support Systems. 6(3), 203-226.\nClark, T.D., Jones, M.C. and C.P. Armstrong (2007). The Dynamic Structure of Management Support\nSystems: Theory Development, Research Focus, and Direction. MIS Quarterly. 31(3), 579-615.\nDavenport, T.H. and J.G. Harris (2005). Automated Decision Making Comes of Age. Sloan Management\nReview, 46(4), Summer, 83-89.\nFahey, L. and L. Prusak (1998). The Eleven Deadliest Sins of Knowledge Management. California\nManagement Review. 40(3), Spring, 265-276.\nKling, R. and P.V. Marks (2008). Motivating Knowledge Sharing Through a Knowledge Management System.\nOmega. 36(1), 131-146.\nLamont, J. (2005). Predictive Analytics: An Asset to Retail Banking Worldwide. KM World.\nNovember\/December. (Available online at\nLeidner, D.E. And J. J. Elam (1995.) The Impact of Executive Information Systems on Organizational Design,\nIntelligence, and Decision-Making. Organization Science. 6(6), 645-664.\nNegesh, S. (2004). Business Intelligence. Communications of the Association for Information Systems. (13),\n177-195.\nNevo, D. and Y. Chan (2007). A Delphi Study of Knowledge Management Systems: Scope and Requirements.\nInformation & Management. 44(6), 583-597.\nSimon, H.A. (1960). The New Science of Management Decisions. New York: Harper and Row.\nThompson, R.L. and W.L. Cats-Baril. (2003). Information Technology and Management, 2nd Edition. Burr\nRidge, Illinois: McGraw-Hill\/Irwin, Chapter 8.\nU.S. News and World Report. (1993). December 6, pp. 48-49.\nChapter editor\nRonald L. (Ron) Thompson is an associate professor of Management in the Babcock Graduate School of\nManagement at Wake Forest University in Winston-Salem, North Carolina, USA. His has published in a variety of\njournals, such as MIS Quarterly, Information Systems Research, Journal of Management Information Systems,\nOmega, and Information & Management. Ron has served as an associate editor for MIS Quarterly, and has\n154 presented at national and international conferences. He holds a Ph.D. from the Ivey School at the University of\nWestern Ontario in London, Ontario, Canada. He has also served as a faculty member at the University of Calgary\n(Calgary, Alberta, Canada) and the University of Vermont (Burlington, Vermont, USA). His current research\ninterests include communication in Information Systems (IS) projects and the use of advanced data analysis\ntechniques in IS research.\n155 9. Managing data for\nefficiency\nEditor: William McIver, Jr., National Research Council Canada\nDedication: Roger \u201cBuzz\u201d King\nLearning objectives\n\u2022 understand the concepts of data, information, and knowledge\n\u2022 understand the role that data management plays in achieving efficiency in an organization\n\u2022 understand the issues involved in representing reality through data modeling\nIntroduction and Data Modeling\nIntroduction\nThe management of data is ultimately about representing, capturing and processing information about some\naspect of reality. Organizations, communities or individuals must determine\u2014perhaps with the help of a data\nmanagement professional\u2014which aspects of reality contain the knowledge they need to perform their tasks. Such\nknowledge is represented at a fundamental by data. An organization might be interested in managing data about\nany number of domains, including, but not limited to:\n\u2022 employee information\n\u2022 customer information\n\u2022 documents\n\u2022 part inventories\n\u2022 product orders\n\u2022 service orders\n\u2022 geographic or spatial information\n\u2022 environmental conditions\n\u2022 information systems logs\nThe key elements of effective data management for any organization are: data models, an accurate and flexible\nrepresentation of the concepts to be managed within the organization; information systems, a technical\nimplementation and arrangement of data, software and hardware that provides for efficient processing of the data\nspecified in the data model; and social processes, an appropriate organization of humans which allows the\ninformation system to be used in a safe and effective manner. This chapter will explore each of these areas.\n156 Purpose of this chapter\nThe purpose of this chapter is to provide a set of tools for understanding data management from conceptual,\ntechnical, and social perspectives. The intention is that this chapter will serve people involved in various\nmanagement roles in an organization, including:\n\u2022 general clients: those who depend on a data management system to carry out daily tasks, but who are not\ninvolved directly in the technical aspects of the system's design or operation\n\u2022 managers: those who manage people and operations through the use of a data management system and\nmay or may not be be directly involved in the system's technical aspects of its design or operation\n\u2022 technical clients: those who are involved directly in the operation and maintenance of a data\nmanagement system\n\u2022 systems analysts: those who are directly involved in the planning, design, implementation, and\nmaintenance of data management systems.\nPeople occupying the roles listed above will most likely have different level of expertise in data management, but\nideally everyone occupying those roles should share some common understanding of the issues involved in\nmanaging data within their organization. For example, a person whose role in an organization is management must\nbe able to communicate her data management needs with systems analysts and technical users. A manager must\nalso be able to reason about the implications of technical issues communicated to them by systems analysts on their\nmanagement tasks.\nExample questions for managers:\nWill the elimination of a certain database remove essential information needed to manage the organization?\nAre expenses from new hardware recommended by technical users justified in terms of new efficiencies that\nwould be realized?\nOn the other side, technical users and systems designers must be able to translate the organizational and social\nissues communicated to them by users and managers into appropriate features or modifications to the information\nsystems they design and operate.\nTopics covered\nThis chapter is organized into two parts. Chapter 1 provides a conceptual perspective on data management\nissues. It focuses, in particular, on the use of data to represent reality, otherwise known as data modeling. Chapter\n2, which is preparation, extends the discussion in Chapter 1 to examine the ways in which systems are constructed\nfor managing data. This includes the use of database management systems and the role of networking in managing\nremote and distributed collections of data.\nWhat is not covered in this chapter\nThe topics covered in this book span or are related to a number of different computer science and information\nscience domains, most especially database systems. This chapter covers several topics that are traditionally taught\nwithin database systems courses or some systems design courses, namely data modeling and database systems\narchitectures. There are many database systems concepts that are beyond the scope of this chapter, however. These\ninclude: theoretical approaches to data modeling, including normalization theory; a comprehensive treatment of\n157 indexing in database management systems, and programming language aspects of data management. These topics\nmay be included in future versions of this chapter. Feedback is welcome from instructors and students alike as to\ntheir needs.\nHow to use this chapter\nKey terms introduced in the chapter are presented first in bold typeface. This chapter presents conceptual and\ntheoretical discussions interleaved with one or more related examples. This is designed to show the reader how the\nconcepts and theories being presented at the moment are applied in real data management contexts. Readers are\nencouraged to develop the examples further based on their own interests.\nThe examples presented in this chapter are described in the section Data management domain examples below.\nExamples are referenced with the abbreviation \u201cEX\u201d. followed by the two or three letter code representing the\ndomain example and a number representing its order within that domain example. Thus, the second example\ninvolving the Weather monitoring domain would be referenced as \u201cEX. WM-2.\u201d Some of the domain examples\npresented in this chapter are not be used until the forthcoming Chapter 2 of this chapter, but the reader is\nencouraged to draw parallels for them while considering the examples in Chapter 1.\nThe role of data management technologies in achieving organizational efficiencies\nOne of the most important benefits that information systems provide to organizations is the ability to manage\ndata at rates and quantities that far exceed human abilities. As computerization of organizations has evolved so too\nhas the ability of organizations to manage ever more complex and numerous tasks. Consider the following realities:\n\u2022 postal and courier services: The Universal Postal Union (UPU) estimates that in 2006 the number of\n\u201cdomestic letter-post\u201d items delivered in Europe and the Commonwealth of Independent States was\n17,591,462,361 [UPU 2007]. The courier service FEDEX reported that between 2005 and 2008 it handled\nan average of 3,476,000 packages per day, including an average of 496,000 packages per day for\ninternational delivery. To facilitate these deliveries it coordinates a worldwide system of over 600 aircraft,\n40,000 motorized vehicles, 1,400 stations, and 140,000 employees respectively [FEDEX 2007a ; FEDEX\n2007b].\n\u2022 air travel: The US Federal Aviation Administration (FAA) reported that in 2006 the number of passengers\nwho boarded airplanes in the US was 716,818,000 [FAA 2006].\n\u2022 health care: As part of its mandate the World Health Organization (WHO) collects and reports on\nstatistical information on over 150 health indicators for 193 countries [WHO 2007].\n\u2022 telephone services: The International Telecommunication Union (ITU) estimated that by August 2007\nthere would be 3 billion mobile telephone subscribers in the world. African countries accounted for an\nestimated 198 million mobile telephone subscribers in 2006 alone [ITU 2007].\n\u2022 banking services: It is now common for a bank to manage several million individual accounts for\ncustomers and provide access to their services via hundreds of branch offices and automated teller\nmachines.\n\u2022 library services: The US Library of Congress began with 3,000 items. Its collection now comprises over\n134 million items with 10,000 new additions daily [LoC 2007].\n158 Organizations in most of these sectors operated prior to the development of computerized data management\nsystems. Posted letters were once sorted, routed, and delivered to letter carriers by hand. Airlines once processed\nreservations and handled baggage without computers. Telephone calls were once set up by hand without access to\nan electronic number database and after that calls were established mechanically based on detecting the numbers\nthat a caller dialed. Libraries and banks also managed their customer services without the benefit of\ncomputerization. Humans' abilities to manage these endeavors has limits, however. These operations could not\nhave scaled to the sizes they are now without automation.\nTo be more specific, none of these situations would be possible without powerful data management\ntechnologies. The services delivered in each type of organization described above necessitates the storage,\nprocessing, and tracking of millions of data items. Further complicating this is the fact that most data items have\ncritical relationships with other data items which must also be maintained reliably. Data about an airline passenger,\nfor example, represents not only their identity, but the flights on which they are booked, payment information for\ntheir tickets, and information about the baggage they checked before boarding their flight. By implication an airline\npassenger's reservation also represents changes to the current inventories of available seats on each the aircraft\nthey will be flying. Make one error in any one of these areas of data management and a passenger may find\nthemselves in the wrong city without their luggage; an airline may accidentally book multiple passengers for the\nsame seat or conversely fail to realize that they still have seats available to sell. Proper data management allows\norganizations to perform well in the face of complexity and volume.\nAs the reader realizes by now the phrase \u201cdata management\u201d means far more than performing numeric\ncalculations which are the stereotypic duties of computers. The basic responsibilities that data management\ntechnologies have are more far reaching. They include:\n\u2022 data models: mechanisms that allow clients to specify what data are to be managed including the logical\nrelationships amongst them and constraints which must hold\n\u2022 storage management: providing mechanisms for storing data in a logically coherent and space efficient\nmanner\n\u2022 access methods: providing mechanisms for locating desired data amongst a very large collection of data\nand retrieving them efficiently\n\u2022 query processing and data manipulation: providing mechanisms that allow clients\u2014people or\nsoftware\u2014to create, examine, change, and delete data in a convenient manner\n\u2022 security: providing mechanisms for making data secure from unwanted access\n\u2022 transaction processing: providing mechanisms which allow multiple clients to simultaneously examine\nand change data without destroying its logical coherence\n\u2022 application program interface: providing a mechanism by which other software systems can make use\nof the data management system\nCorrectly designed and operated data management technologies allow systems such as those described above to\nrun reliably and efficiently. Data management technologies are used most often in conjunction with other\ntechnologies. Well-designed data management technologies combined with other technologies such as barcodes\n159 and optical scanners allow courier services and airlines to track packages and luggage and ensure that they are\ndelivered to their proper destinations. Effective data management technologies combined with switching devices\u2014\nthemselves computers\u2014allow mobile telephone service providers to route high volumes of calls to the proper\nphones and to associate text messages and voicemail with the correct recipients. Libraries are now better able to\nkeep track of their massive holdings and to maintain an accurate accounting of which borrowers have which items.\nLikewise banks are better able to keep track of their customers' accounts and the transactions that take place\nagainst them.\nRepresenting reality through data management68\nData management is ultimately concerned with representing some aspect of reality that must be recorded,\nanalyzed, or communicated. The complete representation of reality itself\u2014our world, the universe\u2014is an intractable\nproblem as we will see shortly, but even representing some small aspect of it can be a daunting a task.\nThe information management professional must remember that many of these issues are often not obvious to\npeople outside of the discipline.\nThe aspect of reality that is chosen to be represented depends upon two main determinants. The first\ndeterminant is the nature of the domain for which the data are to be managed. Common generic data management\ndomains are:\n\u2022 objects: These can be physical or conceptual entities such as automobiles, train or airplane reservations,\nor health records. Data management in this domain involves recording and tracking objects.\n\u2022 events: These can be any type of occurrence for which a record is desired, such as a business transaction or\na bank deposit. Data management in this domain involves recording in a highly reliable fashion any facts\nabout events necessary for reconstructing them at a later time.\n\u2022 organizations: These can be individual businesses, communities, governmental agencies, or departments\nwithin larger entities. Organizational data management in this domain involves recording and tracking:\nobjects within the organization, including people; events and processes within the organization; and\nrelationships between processes and entities.\n\u2022 physical phenomena: These can be any observable occurrences such as: geological conditions, weather\nconditions, or astronomical processes. Data management in this domain involves recording any\nmeasurements necessary for deducing, at a later time, an understanding of a phenomenon.\n\u2022 multiple domains: This can be the management of data across several domains given above or multiple\ninstances of the same domain. A common example is the need to manage data about multiple\norganizations, each having their own data management scheme, in a coordinated fashion. Data\nmanagement in this context may involve the development of a global data model that encompassed the data\nmodels of the individual domains or it may involve the creation of mechanisms to map between the data\nmodels in the constituent domains.\nOne requirement that is common to most data management domains is the need to support queries. A query in\nthis context is a question that is posed to and answered by a data management system using the data it is managing.\n68This section is inspired by the original edition of William Kent\u2019s cult classic book: Data and Reality [Kent\n1978].\n160 The second determinant of what aspects of reality are to be represented is the set of tasks to be performed with\nthe data that are to be managed. Suppose we are designing a data management scheme for a grocery store. For\nexample, suppose that one of the tasks the store needs to perform is tracking its inventory so that it can know when\ncertain items must be replenished. The aspects of reality that must be represented then include not only product\nnumbers and their quantities for the inventory, but information about which of those products have been\npurchased by customers.\nThe concepts discussed in this section apply to many forms of data management. Most can be applied using\npencil and paper. That is, they do not necessarily require computers and database management software.\nTechnology will be discussed later. Concepts surrounding the nature of data will be discussed in the remainder of\nthis chapter.\nData management domain examples\nWe need to be able to represent any aspects of reality that humans find important to remember. What books are\nin their library and who has which books checked out? Which people have reserved a seats on our buses, trains, or\nairplanes? What motor vehicles are registered within our province or country? What was the weather like where we\nlive on a given date? These are but a few common examples of realities which are represented by data.\nWe will identify several canonical examples here that will be used throughout the following sections of this\nchapter to explain data management concepts and techniques. Each example represents some aspect of some\nreality about which it is common to manage data. Each of these examples brings unique challenges to data\nmanagement. Each domain example description is given here along with the abbreviation that will be used to refer\nto it in the various example in this chapter.\n\u2022 Weather monitoring (WM): This example involves the recording of temperature across a geographic\nregion.\n\u2022 Motor vehicle registration (MVR): This example involves recording license information about all\nvehicles in a geographic region.\n\u2022 Library management (LM): This example involves the tracking of library books through the processes\nof borrowing and return.\n\u2022 Train reservations (TR): This involves recording seat reservations for train trips.\n\u2022 Manufacturing (MAN): This involves the tracking of parts that are used to fabricate a product.\nThere are many other are many other examples that could be included here. The reader should be able to\nhow these examples can be extended into other data management domains.\nWhat are data?\nA datum is either a mathematical quantity, a set of symbols, or some combination of the two that is used to\nrepresent a fact. Datum is the singular of data. Facts that are represented by data may be natural objects or\nphenomena, human-derived concepts, or some combination of the two. Several fundamental issues are involved in\nusing a datum in data management:\nRelationships: What relationships exist between a datum and other data?\n\u2022 Form: In what form must a datum exist?\n161 \u2022 Meaning: What does a datum actually mean?\nFirst, to be useful, a datum is usually managed in relation to some other data. For example, it is usually not\nuseful to record and manage someone\u2019s name only. We are more likely interested in the relationships between that\nperson\u2014as represented by their name\u2014and other data, such as their address, their telephone number, or anything\nelse that is necessary for a particular set of data management tasks.\nSecond, the form in which a datum is presented carries a lot of meaning. In fact, it is often the case that most\ntypes of data must be presented in certain formats in order that they are understood. Standard conventions are\nusually developed for the visual layout of a given data type. Consider the conventions that apply to the presentation\nof telephone numbers, addresses, dates and times. One difficulty in managing data is that many conventions vary\nby region and country. For example, 2.5 hours past noon is represented variously as \u201c2:30 p.m.\u201d, \u201c14:30\u201d, or \u201c14 h\n30\u201d.\nThird, what a datum might mean is the topic of next section.\nEX. WM-1:\nCurrent measurements determine that the quantity -10 in the Celsius scale represents the temperature\noutside the author\u2019s window as he writes this. To make sure that this quantity is understood to be a\ntemperature value and not something else, we may wish to add a symbol indicating the unit of measure to be\ndegrees. The \u2018\u00b0\u2019 symbol has become customary for this. Thus, we might have -10\u00b0. Further, in a world with\ndifferent temperature scales, it is important to specify on which scale our quantity is to be interpreted. We\nwould customarily add \u201cC\u201d, to indicate the Celsius scale. Thus, we have \u201c-10\u00b0 C\u201d.\nThe datum -10\u00b0 C does not make sense to us unless we are given a context such as the time and location of\nthe reading. Thus, we will likely want to keep temperature in relation to a number of other data quantities or\nsymbols. One representation that we might record is the following grouping of data <2006-12-04 07:31,\nFredericton, -10\u00b0 C>, where date and time, city name, and the temperature are all represented\ntogether. In data management, this type of grouping is called a tuple.\nThe characters \u201c\u00b0 C\u201d when presented together with a numeric value help us understand that the datum -10 is\na temperature reading. Likewise, the other characters present in the tuple help us realize the domains to\nwhich each other datum belongs: date\/time, city, and geographic coordinates.\nQuestion\n\u27a2 Why do we need to represent the city in the form of its name when we have its precise geographic\ncoordinates?\nEX. MVR-1 :\nGovernments have a need to identify motor vehicles on their roads. This is normally done by assigning to\neach vehicle a unique identifier, a unique set of symbols. A motor vehicle identifier is affixed to vehicles on\nlicense plates\u2014its presentation format\u2014and stored within some data management systems\u2014its storage\nformat\u2014along with other information, such as the owner, make, and model of the vehicle.\nA common format is %c%c%c %d%d%d, where %c represents a single alphabetical character and %d\nrepresents a single digit. So an example license plate code would be:\n162 ABC 123\nA clear pattern of characters separated by spaces is present in the presentation format of our license plate\nschema.\nQuestions\n\u27a2 Should the presentation format of a datum be the same as its storage format in an information system?\n\u27a2 What value does the space have in the presentation format?\nInformation and meaning\nThe concept of data is usually distinguished from the concepts of information and knowledge. If a datum is a\nfact about the real world, the meaning that we derive from it is information.\nEX. WM-2:\nSome data can have objective meanings. One objective meaning of -10\u00b0 C is that it is below the freezing point\nof water.\nData are often given subjective meanings. A subjective meaning of -10\u00b0 C for the author is usually the\nfollowing: \u201cIt\u2019s cold!\u201d Given that it is subjective, this temperature reading could be interpreted differently by\nother people. It may be considered \u201cwarm\u201d by people who live in colder climes.\nMeaning may be conveyed in the data format conventions developed for a given data type.\nEX. MVR-2:\nAuthorities such as the police and tax officials can derive meaning from our motor vehicle identifier\ndepending on how the codes are assigned. Suppose the fields of characters and numbers are each given a\nrole. They could each be designated to represent something specific beyond providing a unique identifier for\na vehicle. The first field, %c%c%c, could be used to represent cities or areas of the province. The second field,\n%d%d%d, could be used to provide a unique number for a vehicle within an area. Suppose all vehicles within\nthe town of Fredericton, New Brunswick are assigned values in the first field of their licenses starting with\nthe characters \u2018FR\u2019. Examples would be:\nFRA 102\nFRA 037\nFRZ 321\nThus, people who know these roles would be able to derive some knowledge about the vehicle\u2019s place of\nregistration by looking at the first field.\nSometimes it is necessary to distinguish one meaning from another. In other words, we may wish to represent\nsuch distinctions in the data themselves. This data modeling problem helps give us a partial understanding of what\nknowledge is.\nEX. WM-3:\nIf the author thinks that -10\u00b0 C is cold, what about -9\u00b0 C or -11\u00b0 C? We can measure the numeric differences\nbetween these quantities. In a data management system, however, it may be necessary to define what \u201ccold\u201d\n163 means in a given context. In this case, we might define the following rule in our data management system to\nestablish the meaning of cold.\nIF temperature <= 0 \u00b0 C THEN cold = true;\nKnowledge\nKnowledge constitutes an additional level of meaning that we derive from information through some process.\nSometimes this process is observational. What happens when it gets cold? Sometimes it is computational where\ndata are processed to arrive at the higher level of meaning that represents knowledge. Data when interpreted within\na certain context yields information having meaning. Information when applied within a certain context yields\nknowledge. A more important component in the concept of knowledge is that it also represents how we apply the\ninformation contained within it.\nEX. WM-4:\nMeanings of data have impacts on how an organization runs. This is an extension of information to\nknowledge. People who manage aircraft, for example, will be concerned with the impact of cold weather on\nits fleet. Observational knowledge of the cold told engineers long ago that ice can build up on an airplane\nreducing aerodynamic lift and potentially impeding the performance of its control surfaces. In extreme cases,\nthis can have disastrous consequences. Here we see that how we manage data can sometimes have life-\ncritical implications.\nThus, we are compelled to transform this type of knowledge into a computational context as the following set\nof rules extended from our last example.\nIF temperature <= 0 \u00b0 C THEN cold = true;\nIF cold == true THEN notify ground crew to de-ice aircraft.\nBy this point in the text it is clear that representing reality\u2014facts, information, and knowledge\u2014can be tricky.\nThis is the topic of the next section.\nData versus reality\n\u201cDie Welt is durch die Tatsachen bestimmt und dadurch, dass es alle Tatsachen sind.\u201d [The world is\ndetermined by the facts, and by these begin all the facts.] 69 (Ludwig Wittgenstein, Logische-Philosophische\nAbhandlung)\nWe stated earlier that representing reality is an intractable problem. There are too many data types and\nrelationships that must be identified and modeled, and some method for continuously collecting all of the data\nabout reality would have to be implemented. Further, reality changes and so any representation of it would also\nhave to be changed. An even trickier philosophical issue is the recursive reality that the data management process\nitself would also be a part of reality and, thus, it too would have to be recorded and managed.\nThus, one can only hope to represent some relatively simple aspect of reality. This can still be a challenge as we\nhave begun to see. More specifically, this challenge can be decomposed and classified into a number of different\nproblems, including, but not limited to, the following:deciding what objects or concepts to represent\n\u2022 identifying objects or concepts inside the reality\n69(\u00a7 1.1)\n164 \u2022 deriving new data from existing data\n\u2022 representing multiple entities of the same type\n\u2022 representing entities that have complex structures\n\u2022 representing relationships between entities\n\u2022 restricting or controlling data values\nWe examine each of these problems in the remainder of this section.\nWhat must be represented?\nThe fundamental problems of data modeling include: deciding what entities must be represented within a\nchosen aspect of reality, what characteristics of those entities must be represented, and how best to balance the\nsolutions to both of the above problems.\nFor any given aspect of reality, there are many real world objects or abstract concepts that we could choose to\nrepresent. This is part of the activity known as data modeling. The decision of what to represent is constrained, in\nprinciple, by the limited time and resources that an organization has to put into data modeling. This includes a\nlimited capacity to design a representation scheme, called a schema; and a limited capacity to collect data to\npopulate the data collection defined by that schema.\nPracticality\u2014beyond principle\u2014dictates that the design of data management systems should be as simple as\npossible. As complexity in a schema increases the computation necessary to process it and maintenance\nrequirements necessary to correct and modify it increase significantly [Banker 1993]. Thus, it is necessary to decide\non a constrained version of reality that we wish to represent.\nIn data modeling, we will call the basic unit of representation an entity. Sometimes they are called objects\u2014\neven if they represent non-physical entities. Any entity in the world can be described in terms of one or more\ncharacteristics. In data modeling, we call these attributes.\nEX. WM-5:\nWe last chose to represent temperature readings using the following schema:\ntemperature_readings(date-time, city-name, temperature)\nThe entities in this case are the individual temperature readings. Each entity is described in terms of the\nattributes: date-time, city-name, and temperature.\nIs it realistic to say that each city has only one temperature at any one time? The concept known as a \u201ccity\u201d\nestablishes both real geographic and conceptual boundaries around which we wish to record the real,\nphysical concept of temperature.\nWe know very well that there may very well be different temperature readings at any one time at different\nlocations within a given city. So which one do we record?\nFredericton is a relatively small town of 131.23 km\u00b2 by Canadian standards, thus one temperature reading\nmay suffice. The average resident of Fredericton could easily comprehend the idea of a single temperature\nreading in relation to the entire city since it covers a small area. That one reading would suffice in informing\na resident about whether to wear warm clothes or not.\n165 On the other hand, there are certain data management domains where greater precision in the data is\nnecessary. The operations staff at the airport, for example, will not be satisfied with a reading taken from the\ncity centre. They must have a precise reading at their location since temperature can greatly impact the\noperations of the aircraft and the working conditions for the ground crew.\nThis last issue in the Weather monitoring example\u2014the one of precision\u2014is what is often referred to in data\nmodeling as granularity.\nGranularity\nGranularity in the context of data modeling refers to the level of specificity or detail at which something is\nrepresented. Almost all data modeling problems offer the opportunity to represent some aspect of reality at ever\ngreater levels of detail. Representations that contain a lot of detail are said to be fine grained with respect to\ngranularity. Conversely, coarse grained representations contain less detail. Coarse grained representations are\nalso often referred to as high level. The data modeler must ask the question: \u201cHow fine grained must the model\nbe?\u201d\nEX. WM-6:\nThere are several dimensions along which we can increase the level of detail of this particular representation.\nWeather monitoring includes the dimensions of time and space. We are keeping temperature readings by\ntime and geographic location. We have chosen thus far to represent the geographic location at a level of\ngranularity of whole cities.\nAs was discussed above, some users of weather data may call for a finer-grained representation. To satisfy the\nrequirements of organizations that manage airports and similar agencies, for example, we would have to\nadjust our current weather monitoring data model to be finer-grained with respect to geographic area. That\nis, our monitoring should include more locations within a given geographic area and perhaps mandated\nlocations, such as airports, irrespective of other considerations.\nThe other dimension of granularity impacting Weather monitoring that was discussed above is time. In this\ncontext, we can adjust the granularity by increasing or decreasing the frequency with which we take and\nrecord measurements. Again, different users may have different requirements of the same type data.\nIndividuals may be satisfied with an hourly or even daily temperature reading if they need only a sense of\nhow hot or cold it is. Other types of users of weather data, such as airports, will require more frequent\nreadings.\nTemperatures can shift significantly in most locations. This is another aspect of representing reality that can\nhave a critical impact on people as was discussed in our earlier discussion about deicing of airplanes. Thus, to\nmeet the needs of airports, we must be sure to collect temperature data with high enough frequency that they\nare able to see temperatures rise or fall past critical points when they need to see it.\nThe level of granularity at which we choose to represent something brings with it simultaneous advantages and\ndisadvantages. There are trade-offs between cost and accuracy where granularity is concerned. With a fine-grained\nrepresentation, we can answer more questions with our data; however, the cost of storing a collection of data will\nrise as its granularity increases. Any detail we add to a data model means that more data must be collected to\n166 populate it. Obviously, any increase in the frequency that data are collected into a given data model will also result\nin higher storage costs. Data management professionals must calculate these costs when designing a system.\nEX. WM-7:\nThe Global Climate Observing System (GCOS) Network consisted of 1,016 stations around the world as of\nJanuary 2007. Each station collects data anywhere from every five minutes to twice daily.\nSuppose every GCOS station collects data every five minutes according to the current temperature_readings\nschema. Suppose further that date-time, city-name, and temperature requires 64, 40, and 4 bytes of data\nstorage respectively. We would then require 31,104 bytes\/day for each GCOS station or 31,601, 664 bytes\/day\nfor all GCOS stations in the network. The annual storage requirements would, thus, be 11,534,607,360 bytes.\n(The reality is that the real GCOS stations collect far more data with each reading.)\nQuestion\n\u27a2 Can you derive these answers?\nIndependent of the level of granularity, one key aspect of representing some part of reality is the need to record\ninformation about things\u2014objects, concepts, locations, etc.\u2014in such a way that they can be accurately identified\nlater.\nIdentity\nIn creating a data model, it is almost always the case that multiple entities must be represented and, further,\nthat multiple types of entities must be represented. If entities within a collection of data lack identity it will be\ndifficult, if not impossible, for users to find data they need. It is also important to be able to distinguish between\nentities of different types.\nGiving each entity an identity\u2014or identifier\u2014gives users of a data management system a way of specifying which\nentities are of interest to them when they ask questions of or perform tasks with a collection of data.\nEX. WM-8\nIn the previous discussion of this the Weather monitoring example, we decided that it is necessary to\nincrease the granularity with respect to geographical locations of the temperature readings that are collected.\nThat is, for a given date-time value it should be possible to record the collection of multiple locations within a\nsingle geographic entity such as a city. For example, for the city of Fredericton, we wish to take readings not\nonly at the city centre, but at the airport as well, and perhaps other key locations.\nSince multiple readings in this context represent different locations within a geographic entity\u2014in this case a\ncity\u2014we must decide how to distinguish the readings from one another. Our current schema is:\ntemperature_readings(date-time, city-name, temperature). Suppose that on 4 December, 2006 at 07:31 in\nthe morning, the temperature at Fredericton city center is 5\u00b0 C while at Fredericton airport it is 0\u00b0 C. If we\nuse the current schema to store readings at Fredericton\u2019s city centre and at the airport at the same time (i.e.\nDate-time), we might naturally decide to record the readings as follows:\n<2006-12-04 07:31, Fredericton, 5\u00b0 C>\n<2006-12-04 07:31, Fredericton, 0\u00b0 C>\n167 This problem is that this approach does not allow us to distinguish between Fredericton city centre and\nFredericton airport. We could change the schema or data values we use to distinguish between each reading.\nArguably the value for attribute city-name in the example above should be \u201cFredericton\u201d, not \u201cFredericton\nairport\u201d or \u201cFredericton city centre\u201d, but this would not help us to uniquely identify each location within\nFredericton.\nOne approach might be to use geographic coordinates as discriminators. Thus, we could change our schema\nto: temperature_readings(date-time, city-name, latitude, longitude, temperature). Representing\nFredericton city centre (66\u00b0 32\u2019 W) and Fredericton airport (66\u00b0 10\u2019 W), we would then record the readings\nsuch as:\n<2006-12-04 07:31, Fredericton, 45\u00b0 52\u2019 N, 66\u00b0 32\u2019 W, 5\u00b0 C>\n<2006-12-04 07:31, Fredericton, 45\u00b0 52\u2019 N, 66\u00b0 10\u2019 W, 0\u00b0 C>\nFor the same date-time value, we now have two readings, one for the city centre and the other for the airport,\nwhich is to the west of the city centre.\nWould this suffice? How would the average person know which reading is for the airport and which one is for\nthe city centre? Not many people memorize geographic coordinates in the form of latitude and longitude.\nThus, we must find another way to discriminate between locations.\nWe probably should retain the coordinates, however, because in aviation and other application domains they\ncould be important. One approach is to change the role of the second field in our scheme. We could make its\nrole more general in geographic terms by calling it location-name. Values in this field will now be understood\nto indicate the names of any type of geographic entity\u2014at least geographic entities that are deemed\nimportant in this application domain.\nThus, we could make use of names to such as \u201cFredericton City Centre\u201d and \u201cFredericton Airport\u201d in storing\ntemperature readings. This would give us the following:\n<2006-12-04 07:31, Fredericton City Centre,\n45\u00b0 52\u2019 N, 66\u00b0 32\u2019 W, 5\u00b0 C>\n<2006-12-04 07:31, Fredericton Airport,\n45\u00b0 52\u2019 N, 66\u00b0 10\u2019 W, 0\u00b0 C>\nWould this suffice? Technically, it might work. The geographic coordinates are likely to be associated with\nonly one geographic name. If \u201cFredericton City Centre\u201d and \u201cFredericton City Hall\u201d effectively share the same\ngeographic coordinates (45\u00b0 52\u2019 N, 66\u00b0 32\u2019 W), but we want to allow users to be able refer to either one in\nlooking for temperature readings and assuming we record data using both location names, they could still\ndistinguish between the two readings by the location name. A more common problem, however, is for the\nsame name to refer to multiple entities, particularly cities and people.\nWhat if we measure data for the city of Albany and its airport? There are at least two cities named Albany in\nNorth America: Albany, New York (42.6525, -73.75667) and Albany, Georgia (31.57833, -84.15583). Thus, we\nmight be faced with:\n<2006-12-04 07:31, Albany City Centre,\n42.6525, -73.75667, 14\u00b0 C>\n168 <2006-12-04 07:31, Albany Airport,\n42.74806, -73.80278, 14\u00b0 C>\n<2006-12-04 07:31, Albany City Centre,\n31.57833, -84.15583, 37\u00b0 C>\n<2006-12-04 07:31, Albany Airport,\n31.53528, -84.19444, 37\u00b0 C>\nWe could address this problem in several ways. Perhaps we could add more attributes to specify locations,\nsuch as country, state or province, or county. The problem is that different location name would require\ndifferent combinations of such location names.\nAssigning identities to entities is often necessary for data modeling, but it is often not sufficient. To be useful, an\nidentity must also be unique.\nUniqueness\nThe assignment of unique identifiers to entities is often the ideal way of distinguishing one entity from another.\nThere are several ways to create a unique identifier. One way to create unique identifiers is to use a combination of\nexisting attributes whose data values, taken together, are certain to be unique within a given data management\ncontext.\nEX. WM-9:\nThe approach that we have used to uniquely identify locations for temperature readings has been to combine\nlocation names with geographic coordinates. While technically sufficient, we saw that it was less than ideal in\nterms of usability. Users would be able to distinguish between identical location names (e.g. Albany City\nCentre) using the coordinates, which would have to be different; however, coordinates would not be the most\nintuitive type of data for must users.\nEX. TR-1:\nSuppose we are designing a data management system to handle train reservations. One data modeling task\nwould be to represent the trains. What we mean by \u201ctrains\u201d in this context will determine what combination\nof attributes might be a suitable unique identifier.\nSuppose we take \u201ctrain\u201d to mean a route between an origin and a destination. In this case, we could achieve\nuniqueness by combining attribute values for origin and destination. This assumes that our train network\ndoes not have duplicate city names. A schema suitable for uniquely identifying train routes might be the\nfollowing: trains (origin, destination). Note: the use of bold within a schema indicates the attributes that\nmake up a unique identifier or key. The following is an example data collection that follows this schema:\n<Montr\u00e9al, Halifax>\n<Halifax, Montr\u00e9al>\n<Montr\u00e9al, Ottawa>\n<Ottawa, Montr\u00e9al>\nNote, that this schema has the advantage of yielding a unique identity for reciprocal routes. For example,\n<Montr\u00e9al, Halifax> \u2260 <Halifax, Montr\u00e9al> and <Montr\u00e9al, Ottawa> \u2260 <Ottawa, Montr\u00e9al>.\n169 What if our train service offers multiple trips on the same route? For example, there are at least two\n<Montr\u00e9al, Ottawa> trips each day, in both directions. In this case, origin and destination would not\nguarantee uniqueness. The condition that prevents uniqueness\u2014multiple trips\u2014points to one possible\nsolution, which is to use those data values that distinguish multiple trips on the same route. In this case it is\ntime. Thus, we might modify our schema to the following: trains (origin, destination, time_of_day).\nWe may find situations where it is important to distinguish between the same train route on different days of\nthe week or specific dates. If a passenger orders two round trips between Montr\u00e9al and Ottawa on different\ndays, we might record those reservations as follows in our database:\n<\u201dJim Smith\u201d, Montr\u00e9al, Ottawa, 08:00, 2007-09-01>\n<\u201dJim Smith\u201d, Ottawa, Montr\u00e9al, 20:00, 2007-09-01>\n<\u201dJim Smith\u201d, Montr\u00e9al, Ottawa, 08:00, 2007-09-07>\n<\u201dJim Smith\u201d, Ottawa, Montr\u00e9al, 20:00, 2007-09-07>\nAs we can infer from the data values that origin, destination, time_of_day, and date must be combined to\nuniquely identify a route; we might add the passenger_name to attempt overall uniqueness of each record.\nWhy does that not work?\nIt is easy to see that the combining of attributes can easily become tedious. We quickly went from juggling\ntwo attributes to four attributes. If we add the passenger_name attribute, we are still not guaranteed\nuniqueness in certain cases. What are they?\nThus, we need another approach to creating unique identifiers.\nThe use of multiple attributes to guarantee uniqueness can become quite messy. Often the most reliable\napproach to creating unique identifiers is to use a single, special attribute to which values are assigned that are\nguaranteed to be unique. Such an attribute is usually called a key or object identifier. One approach is simply to\nassign a unique number as the identifier to each entity.\nEX. WM-10:\nInstead of using a combination of location names and geographic coordinates to distinguish between the\nlocations where a temperature reading was recorded, we could devise a set of unique identifiers for each\nstation. Thus, if we have location names that are the same, like Albany, New York and Albany, Georgia, we\ncan create a unique code for each. This is actually the approach taken by weather monitoring networks\naround the world. For example, Environment Canada assigns a unique five digit code, called an \u201cindex\nnumber\u201d, to each of the monitoring stations in its network. As an example, the stations listing \u201cFredericton\u201d\nin its name are recorded as follows [EnvironmentCanada 2007]:\nIndex Number Station Name Lat Lon Other attributes ...\n... ... ... ... ...\n71668 FREDERICTON CDA CS, NB 45 55N 66 36W ...\n... ... ... ... ...\n71700 FREDERICTON A, NB 45 52N 66 32W ...\n170 Index Number Station Name Lat Lon Other attributes ...\n... ... ... ... ...\nNote that there are two stations in Fredericton with different geographic coordinates, each having its own\nIndex Number attribute value. The other parts of the name have some meaning to those in the Weather\nOffice. Index Number 71700 represents the Fredericton Airport (cf. A website such as\nhttp:\/\/maps.google.com and enter the coordinates).\nManually working out unique identifiers for each weather monitoring station works since there are a finite\nand slowly growing number of them. In the Weather monitoring example, however, we can imagine other\nentities that will have to be modeled for which this method of assigning unique identities is not feasible.\nThere is a rapidly growing collection of individual temperature readings from each site in the network. It\nwould not be possible for humans to manually assign unique identifiers to each temperature reading record\nthat is stored. We would need to have our data management system generate unique identifiers somehow.\nEX. TR-2:\nIt is common in transportation networks to assign a unique number to each route. People in a given location\noften know the routes by these numbers. \u201cIf you want to go to Ottawa, take the 35, 37, or 39,\u201d a Montr\u00e9al\nresident might say. Recall, that the issue we had with uniquely identifying train routes had not only to do\nwith unique <origin, destination> pairings, but times of day. Thus, VIA Rail assigns a unique number to each\n<origin, destination, time_of_day> combination. Thus, the Montr\u00e9al to Ottawa route has unique numbers\nfor each time of day. An example of the routes listed previously in EX. TR-1 with their unique \u201cTrain\nnumbers\u201d is as follows:\nTrain Route name Origin Destination Departure Time\nNumber\n35 Montr\u00e9al-Ottawa Montr\u00e9al Ottawa 15:05\n37 Montr\u00e9al-Ottawa Montr\u00e9al Ottawa 16:45\n39 Montr\u00e9al-Ottawa Montr\u00e9al Ottawa 18:00\n14 The Ocean Montr\u00e9al Halifax 18:30\n15 The Ocean Halifax Montr\u00e9al 12:35\n34 Ottawa-Montr\u00e9al Ottawa Montr\u00e9al 15:10\n36 Ottawa-Montr\u00e9al Ottawa Montr\u00e9al 16:25\n38 Ottawa-Montr\u00e9al Ottawa Montr\u00e9al 17:55\nAs we discussed above in Weather monitoring example EX. WM-10, the hand assignment of unique train\nroute codes is feasible because there are a fixed number of routes and new routes are added infrequently. The\ntrain reservations, on the other hand, will not be amenable to this approach. The reasons are the same as\ngiven for the temperature readings. Need to have our data management system generate unique identifiers\nsomehow.\n171 Question\n\u27a2 Some trains do not travel each day of the week. How would you model that?\nIt has been seen in EX. WM-10 and EX. TR-2 that sometimes uniqueness can be worked out manually. Often\ntimes, it is necessary to automate the process of assigning unique identifiers. There are a number of ways this can\nbe done. Software developers can create services that give each requester a unique identifier. Thus, each time a new\nentity is created, a unique identifier is requested from the service. A few basic ideas about how this can be done are\ngiven in the following examples. In some cases it is not necessary to write new software to generate unique\nidentifiers since most database management systems provide services for doing this.\nEX. WM-11:\nAs was discussed in example EX. WM-10, we must somehow generate unique identifiers for each\ntemperature reading. One approach is to simply ask the database management system used to store our\ntemperature readings to generate a new unique identifier each time we create a new temperature reading\nrecord. That is, each time we store a new record in our collection, the database management system software\nwould be instructed to insert a unique piece of information in that new record. A common method for a\ndatabase management system to do this is to simply keep a counter. Each time a new record is created, the\ncounter is incremented by 1 and the new value is given. Using this method, we might have something like the\nfollowing for our temperature readings:\nReading Number Station Index Number Date Time Temperature\n1 71668 2007-08-14 08:00 AM ADT -10\n2 71627 2007-08-14 07:00 AM EDT -15\n3 71700 2007-08-14 08:00 AM ADT -11\n... ... ... ...\n107 71668 2007-08-14 08:05 AM ADT -11\n108 71700 2007-08-14 08:05 AM ADT -12\nIn the example above, we see that the reading_number attribute is simply an integer whose values are\nobtained from the database management system. It provides each record with a unique identity. This\napproach works as long as there is a central authority for generating the unique identifiers. In this case, that\nauthority is the database management system. In examples WM-10 and TR-2 the central authorities where\nEnvironment Canada and VIA Rail respectively.\nWhat happens, however, if each region or each station in our weather monitoring network has its own\ndatabase management system. In this scenario, the records would be stored in regional collections, each\nmanagement by its own system, and then consolidated periodically into a national collection so that the data\ncan be used together. In this scenario, how do we guarantee that the database management system\nresponsible for Fredericton\u2019s two weather stations generates identifiers that are unique from those of some\nother region, let\u2019s say Montr\u00e9al? If each database management system simply increments an integer, then we\n172 have no practical way of insuring that each system generates identifiers that are different from the other\nsystems.\nOne solution would to combine the integer values assigned by each system with some unique location or\nregion identifier. Suppose we assign the Atlantic region in which Fredericton exists the code \u201cATL\u201d and the\nEastern region in which Montr\u00e9al exists the code \u201cEAST\u201d. We might then unique reading numbers such as\nthe following:\nReading Code Station Index Number Date Time Temperature\nATL.1 71668 2007-08-14 08:00 AM ADT -10\nEAS.1 71627 2007-08-14 07:00 AM EDT -15\nATL.2 71700 2007-08-14 08:00 AM ADT -11\n... ... ... ...\nATL.107 71668 2007-08-14 08:05 AM ADT -11\nATL.108 71700 2007-08-14 08:05 AM ADT -12\nA critical problem with simple schemes for selecting unique identifiers is that they are often not secure. This is of\nspecial concern if such values are to be used by humans or if they can somehow be intercepted by other information\nsystems.\nAt least two types of security are of concern here: (1) security from imitation identifiers and (2) security from\nerrors introduced into the identifier itself. Ideally, one should not be able to guess a valid value for a unique\nidentifier. Someone might attempt this in order to co-opt records for malicious reasons.\nThe introduction of errors is related in a way to the risk in (1). That is, if it is easy to guess a valid value for an\nidentifier, then it is probably also very easy to introduce an erroneous value into a system. Common data recording\nerrors for humans include the transposition of data (e.g. 5,401 vs 4,501) and the addition or removal of data. Thus,\nan ideal scheme for generating unique identifiers or other types of codes is one which is said to be error-\ncorrecting. For example, if given an identifier a human should not be able to introduce an error into the code that\nproduces another valid identifier. The corollary to this is that the introduction of such an error should be detectable.\nAnother way to view this is that the code should be self-validating. That is, it should be possible to determine the\nvalidity of a code value by examining the value itself.\nEX. TR-3:\nWe could employ the same approaches that were just discussed in EX. WM-11 to identify the reservation\nrecords discussed in EX. TR-1. Other ideas might be explored, however. One problem with simply selecting\nunique identifiers from a sequence is that they carry no useful information other than a guarantee from the\ndatabase management system from which we get them that they are unique. In the last iteration of the design\nof our reading-number attribute in EX. WM-11 above, we can now discern regions from identifier.\nLikewise, in creating a suitable identifier for train reservations, it would be useful if, beyond being unique,\none could glean critical information from them upon examination. For example, an official from the train\nnetwork might be able to offer additional help to a passenger when presented with such an identifier. We\n173 must also be wary of: (1) the possibility of creating imitation reservation numbers or (2) the ease with which\nerrors can be introduced into an identifier\u2019s data value.\nIn case (1), suppose people know that the train network issues reservation numbers that are integers. It\nwould then be very easy for people to create fake reservation numbers. With a fake reservation number\u2014and\nlacking any other security measures\u2014someone might then be able to co-opt someone else\u2019s reservation for\ntheir own or to discover information about the holder of the reservation. Ideally, we should use a unique\nidentifier scheme for which it is not be possible for someone to guess a valid value.\nCase (2) can be seen as related to case (1). It should not be possible for someone to introduce an error into a\nreservation number that produces another valid reservation number. Suppose Jim Smith makes a\nreservation over the telephone and is told by the operator that his reservation number is \u201c1010\u201d while he\nmistakenly writes down \u201c1001.\u201d Suppose that \u201c1001\u201d is someone else\u2019s reservation number. Jim Smith now\nhas created an identifier that can potentially violate someone\u2019s privacy. Train officials cannot easily tell that it\nis erroneous and, thereby, help Jim Smith to get his correct reservation number. This is a simple example, of\ncourse. In reality, other data, such as identifying information, are often used to cross check such things as\nreservation numbers.\nEX. LM-1:\nThe International Standard Book Number (ISBN) is one scheme that has been developed that addresses the\nissues of error detection and the introduction of meaning into the structure of data values, as discussed above\nin example EX. TR-3. Each identifier is composed of unique numbers for group (i.e. language group or\ncountry), publisher, title, and check digit (cf. http:\/\/www.isbn.org ). An example of the 10 digit version,\ncalled ISBN-10 is: 1-40207-067-5.\n1 := English-speaking country group\n40207 := Publisher number\n067 := Title (within the publisher\u2019s collection)\n5 := Check digit\nIf we have the lists of codes for the different language groups, publishers, and each publisher\u2019s titles, the\nvalue of a ISBN itself will lead us to all of that information. The check digit is a value calculated using the\nvalues of the group, publisher, and title fields. Thus, if the check digit does not match the value calculated\nusing the other digits, then we know there is an error in that particular ISBN number. The check digit is\ncalculated using the following formula, where d is the ith digit from the left:\ni\ncheck digit = d +2d +3d +4d +5d +6d +7d +8d +9d mod 11\n1 2 3 4 5 6 17 8 9\nThus, the first 9 digits of the ISBN 1-40207-067 would be used as follows to calculate the check digit:\ncheck digit = 1+2*4+3*0+4*2+5*0+6*7+7*0+8*6+9*7 mod 11\n= 170 mod 11\n= 5\nNote: The ISBN-10 standard specifies that if the remainder is 10, then the character \u201cX\u201d is to be used as the\ncheck digit so as to keep the number to 10 digits.\n174 A data management system can then easily validate any value presented as ISBN-10 by the following rule:\nIF (d +2d +3d +4d +5d +6d +7d +8d +9d mod 11) == check digit THEN\n1 2 3 4 5 6 17 8 9\nISBN is correct\nELSE\nISBN is invalid.\nEND IF\nFollowing our discussion in example EX. WM-11, we can see that this particular approach to generating\nidentifiers counts on a central authority to define the group and publisher codes, but that once a publisher\nhas been assigned its own publisher number, it can assign its own titles without interfering with any other\npublisher. Of course, it must notify some central authority of the new titles it creates.\nAssigning values\nAnother level of reality is represented by the values we choose to assign to the attributes of the entities in a\nschema. Typically, data management software, particularly database management systems, can support the storage\nof various basic data types. These include types that represent the well-known numerical and logical domains\ninteger, real, and boolean. Obviously, we need to be able to represent words in some way. For this, such systems\nallow the storage of character or string data. Strings are sequences of characters. Most software systems used for\ndata management also permit the representation of other more specialized types such as dates, times; or arbitrarily\ncomplex combinations of other data types. The data modeler must decide on the types of values to be stored that\nbest suit their use in a data management system. For example, if it will be necessary to perform arithmetic\ncalculations using an integer value, the data values should be integers and not string versions of integers (i.e. 10 vs\n\u201c10\u201d).\nEX. WM-12:\nThe reader should note that we used two data types across past examples to express geographic coordinates.\nOne was the traditional degrees, minutes (and seconds) form, where North\/South relationships and\nWest\/East relationships relative to the equator and prime meridian are represented with characters \u201cN\u201d, \u201cS\u201d,\n\u201cW\u201d, and \u201cE\u201d respectively. Values in this form look like the following:\nlatitude = 45\u00b0 52\u2019 N, longitude = 66\u00b0 32\u2019 W\nAnother form that is more common these days is a degree decimal representation of coordinates. The above\ncoordinates in degree decimal form are:\nlatitude = +45.866667, longitude = +66.533333\nCoordinates in this form make it easier to perform mathematical calculations with the values, such as\ndetermining the distance between locations. North and South are represented by positive and negative\nlatitude values, respectively. West and East are represented by positive and negative longitude values,\nrespectively.\nSuppose, however, that we need to cross-reference coordinates in our data collection with existing\ndocuments that employ the traditional form. In this, case it may be advantageous to store our data values in\nthe same format.\nOne alternative is to derive values in the form we need them from one chosen data type.\n175 Another data modeling problem, beyond the question of data types, is to determine how the values are to be\ninterpreted. Another way of viewing this is as the type of units that a value represents. This is particularly the case\nwhen physical phenomena are being represented for which various scientific units apply. This data modeling issue\nrelates to the earlier discussion of the meaning of data.\nEX. WM-13:\nIf we examine a data value for an entity\u2019s attribute in a data management system, the value alone will not tell\nus what it means. Suppose we decide to represent temperatures in our temperature_readings schema as\nintegers. If we then examine the value of the temperature attribute in some record, we will see only an\ninteger, say -10. What would -10 mean though? We as humans know from the name of the attribute that it\nrepresents the physical quantity known as temperature.\nThe problem is that we also know that -10 could represent a temperature on one of several scales. The\nquestion is then: what type of units\u2014beyond integers\u2014does this value represent? Some geographic regions\nstill use the Fahrenheit scale. Some use Celsius. Some scientific domains assume the use of the Kelvin scale.\nThus, it is necessary in our schema to somehow indicate what type of units this attribute is to represent.\nThis is a complex issue in data modeling. Addressing it ultimately entails the use of what is called meta-\ndata. Meta-data are data about data. This will be discussed later. For now, let\u2019s focus on the representation\nof the data values themselves.\nWe could decide to have two attributes for the Fahrenheit and Celsius temperature scales and record the\nequivalent values in each attribute. This may not be the best approach, however. We will need twice the\nstorage space now to record temperature values. We will also have to spend twice as much time recording\nvalues.\nAll decent modeling or design choices have trade-offs. That is, there are advantages and disadvantages to\neach. Some modeling choices are just bad!\nWhat would be useful here is to be able to store the temperature in one chosen scale and to be able to ask to\nderive it in another scale.\nDerived data values\nData modeling options have varying degrees of disadvantages in terms of the amount of space and time they\nrequire.\nIn deciding what must be represented by data, it is also possible to decide what need not be represented\nexplicitly in the data. What need not be represented explicitly depends on the nature of the data. For example, in\ncases where multiple unit systems exists, it may make sense to store values in only one of the possible systems and\nthen provide conversions to other systems. Data values that are represented this way are known as derived data\nvalues. Using a derived data mechanism, we need not stored an actual value in a data collection. Values can be\ncalculated on demand instead, provided that the necessary input data values exist in the collection. It is important\nto note that derived data need not be limited to numeric data values.\nEX. WM-14:\n176 Suppose we must support users with temperature readings in both metric and imperial units. Using a derived\ndata mechanism, we might modify the latest version of our temperature_reading schema in example EX.\nWM-11 to arrive at the following:\ntemperature_readings(\nreading_code,\nstation_index_number,\ndata,\ntime,\ncelsius_temperature,\nfahrenheit_temperature(celsius_temperature))\nBy this syntax, we mean that the attribute fahrenheit_temperature is actually a function and that it derives\nits value using the value of the celsius_temperature attribute. Using some type of programming language, we\nwould specify to the data management system the following algebraic equation in defining the\nfahrenheit_temperature attribute:\nfahrenheit_temperature(celsius_temperature) :=\n1.8 x celsius_temperature + 32\nQuestion\n\u27a2 What are the trade-offs in choosing a derived data value approach in this case?\nAnother case where derived values are beneficial is in the calculation of aggregate data values. Aggregate\ndata values are derived using a collection of data values. The most common examples are statistical measures such\nas the average, mean, or median values of a collection of numbers. Other aggregate measures such as the sum,\nmaximum value, or minimum values of a set of numbers are common as well. Database management systems\ntypically provide built-in services or functions that calculate such measures over a collection of data specified by\nthe user.\nEX. WM-15:\nSuppose we must support users with the average, maximum, and minimum temperature readings at each\nstation for the last 24 hours. One method would be the following:\nGiven the schema:\nstations (station_index_number, location_name)temperature_readings(\nreading_code,\nstation_index_number,\ndate,\ntime,\ntemperature)\nstation_statistics (\nunique_id,\ndate,\ntime,\n177 station_index_number,\navg_temperature,\nmax_temperature,\nmin_temperature)\nCollect current temperature readings from all stations.\ng. Store each reading collected during Step 1 as an entity in the temperature_readings data collection.\nh. For each station_index_number in the stations data collection, calculate the average, maximum,\nand minimum temperature values recorded in the temperature_readings collection over the last 24\nhours; and store each calculation in the station_statistics data collection.\ni. Wait for a specified time interval then got o Step 1.\nStep 3 in our method above will cost us space, but it will also cost us in computational time. The latter will\ncome about with the need to read values from the stations and temperature_readings collections in order to\ncalculate the values to be stored in the station_statistics collection.\nUsing a derived data value approach, we can save space and potentially time by calculating the aggregate\nstatistics only when requested by a user.\nQuestions\n\u27a2 What approach would you choose if there are likely to be many requests for statistics?\n\u27a2 Can you think of a way to optimize the procedure above in terms of computational time?\nThe costs in space expenditures are not zero for derived values. In a computer system at least, some amount of\nspace is required to store the methods that are required to derive a value. These are traditionally called stored\nprocedures in database management systems. For example, our temperature conversion formulas would be\nstored in the form of a small computer program. Many database management systems allow such procedures to be\nstored with the data. In other cases, developers decide to implement the procedures within external computer\nprograms.\nAs was discussed at the start of this section and but can be seen more clearly by now, deciding what should be\nrepresented in data that we collect should depend on finding the best ways in which it will be understood and used.\nRepresenting composite entities\nCertain types of entities are better understood\u2014and, thus, modeled\u2014as being comprised of not just a set of\nattributes, but of other whole entities. These are called composite entities or composite objects. The entities\nwhich comprise another entity are referred to here as constituent entities. Sometimes they are referred to as\ncomponents or child objects of an encompassing parent entity or parent object. It is important to note that\nmany types of entities cannot exist in their own right without the existence of their constituent entities.\nEX. TR-4:\nOne trip on one train route in our example can be modeled as a number of composite entities. The equipment\nthat is to be used is itself is a composite. There may be several engines and any number of cars. A train\ncompany would certainly use a data management system to keep track of each of these pieces of equipment\nindividually. For a given trip, the company would also need to know which of these individual pieces of\n178 equipment are used together. Which engine did we send to Ottawa today with train 39? Thus, we might look\nat a composite model of the equipment used for a trip as follows:\ntrain_equipment(trip_id=2007-08-17 18:00, ... )\n\u2937engines\n\u2937{engine(id=E105, model=General Electric P27,... ),\nengine(id=E2711, mode=Budd Diesel #2, ... )}\n\u2937passenger_cars\n\u2937{passenger_car(id=P143,model=Bombardier 101,\ncapacity=100,...),\npassenger_car(id=P2008,model=Bombardier 101,\ncapacity=100,...),\n... ...\npassenger_car(id=P2009,model=Bombardier 101,\ncapacity=90,...) }\n\u2937dining_cars\n\u2937{dining_car(id=D013, model=Bombardier D101,...)}\n\u2937caboose(id=C007, model=Bombardier C101,...)\nThus, the train_equipment entity can be seen as having many components: collections of engines,\npassenger_cars, a dining car, and a caboose. Each of the constituent entities exist by themselves in their own\nright, but in order to understand the concept of train equipment for a particular trip, they are aggregated\ntogether as a single composite entity.\nQuestion\n\u27a2 Can you create models of passenger reservations looking at them from a composite context.\nRelationships\nAnother aspect of reality that must often be captured are the relationships that exist between entities in the real\nworld. Relationships can be viewed in a variety of ways. From one perspective, relationships can be used to define\nthe logical structure of a set of entities. Another important perspective is that which defines what various entities\nmean to each other.\nThe preceding section discussed the concept of a composite entity, an entity that contains other entities. A\ncomposite entity is one that defines structure. It represents the structure of containment between parent entities\nand their constituent entities. These are often called has-a relationships.\nEX. TR-5:\n179 The train-equipment schema discussed in example EX. TR-4 defines has-a relationships. (There are other\ntypes of relationships in that schema that we will discuss below.) Using the has-a relationship, we can make\nthe following assertions about the train-equipment schema:\ntrain_equipment has-a set of engines\ntrain_equipment has-a set of passenger_cars\ntrain_equipment has-a set of dining_cars\ntrain_equipment has-a caboose\nThe concept of data collections that have been referred to extensively in this section imply the set-theoretic\nconcept of membership. A data collection can be viewed as a set, though most often not in strict set-theoretic\nterms since duplicate elements may sometimes exist. Nonetheless, an entity of a given type is often defined in terms\nof set of zero or more entities of that type.\nEX. TR-6:\nThe train-equipment schema discussed in example EX. TR-4 is a composite entity that is partly composed of\nseveral types of sets of entities. Note the use of braces (i.e. \u201c{\u201c and \u201c}\u201d ) in that example. The train-equipment\nschema contains a set of engines, a set of passenger_cars, and a set of dining_cars. From the\ntrain_equipment entities perspective, it has several has-a relationships with respect to these sets as we\ndiscussed in example EX. TR-5. Relationships also can be viewed from the perspective of the elements of\nthose sets. Using the element-of relationship, we can make the following assertion about the train-\nequipment schema:\npassenger_car having id=P143 is an element-of passenger_cars\nAnother perspective on relationships describes the cardinality of the associations between entities. There are\nthree basic types of cardinalities: one-to-one, one-to-many, and many-to-many. These can be visualized in the\nfollowing ways:\nCardinalities\none-to-one\nWe depict a 1:1 relationship between type A and type B.\nEntity type A Entity type B\n180 one-to-many\nWe depict a 1:m relationship between type A and type B.\nEntity type A Entity type B\nmany-to-many\nWe depict a m:m relationship between type A and type B.\nEntity type A Entity type B\nMeta-data\nReiterating, meta-data are data about data. One key theme in this discussion on data modeling has been the\nproblem of understanding the meaning of the values of data in a collection. The meaning of a datum can be\nunderstood in a set-theoretic sense. The number \u201c10\u201d is easily understood in terms of its membership in the domain\nInteger, for example. It is not possible looking at a value in isolation what its purpose is or what it represents.\nHuman understanding might make use of the attribute names, but as has been shown, even those are limited in\n181 explaining the contents of the attribute. For example, the attribute name \u201ctemperature\u201d does not convey what scale\nits values represent. The purpose of meta-data is to fill these gaps in meaning.\nData can be explained at different levels. Thus, there are different levels of meta-data. At the most fundamental\nlevel are data about the physical storage properties of data. This is known as the physical schema. Meta-data at\nthis level can include information about file formats in which data are stored, properties of the devices used for\nstorage, and the bit-level and byte-level representations of data values.\nIn discussions above, data have been explained at a logical level. This is a level that is free of considerations\nabout the physical details of data formats and their storage. This is known as the logical schema. Meta-data at\nthis level describes the data model: entities, attributes of entities, data types of attributes, value constraints on\nattributes, and relationships between entities.\nHigher levels of explanation can be given concerning data. As discussed above, meta-data are necessary for\ndescribing the meaning of data beyond what is usually described by a logical schema. Descriptions at these levels\ncomprise what are sometimes called semantic data models (cf. [Hammer 1978]). Meta-data at this level may:\nprovide human descriptions of attributes and define the value system within which data should be interpreted (e.g.\nCelsius vs Fahrenheit).\nLong-standing research has been done to facilitate levels of understanding of data above the logical schema.\nThat is, to facilitate understanding of data that approaches that of artificial intelligence.\nConstraints\nIn a data model, the values that attributes can take are constrained at a basic level by the data type of the\nattribute. For example, if the attribute \u201cnumber\u201d is defined as an integer, it may only hold numeric integer values,\nsuch as the number 10; it would not be allowed to hold string values, even the string \u201cten\u201d.\nIt is often the case that a data model must enforce constraints on data values even if they are technically legal\nfrom the standpoint of the domain of an attribute. For example, even if an attribute is defined as an integer, it may\nbe necessary to restrict which integers it will be allowed to hold. Ideally, a data management system allows\nconstraints to be defined within data models that enforce such restrictions.\nA common situation where constraints on data values are necessary is in representing entities that have a fixed\nset of values. For example, an application that asks users to input a postal address can and should enforce the\ncorrectness of certain values that are a part of an address. Postal codes, provinces or states, and even city names in\na given country exist within fixed sets. Thus, even if the postal code and province attributes are of a string data\ntype, the particular string values they are allowed to hold can be restricted to the values contained in the official\nlists of postal codes and province names, respectively.\nThe principle of using constraints to enforce attribute data values can be applied to any type of data, even those\nof mixed types. Database management systems typically provide mechanisms that allow designers to specify\nconstraints at the same time they specify a schema.\nAnother category of constraints within a data model involve the cardinality of relationships between\nentities. Thus, they are called cardinality constraints. These are used to restrict the types of cardinalities that\nexist between entities, as discussed in the section on relationships. Corresponding to the discussion in that section,\nthe cardinality of relationships between two types of entities can be constrained at a basic level to: one-to-one, one-\n182 to-many, or many-to-many. A one-to-one constraint between entity type A and entity type B in a schema says that\na given entity of type A may be associated with only one other entity of type B. One-to-many and many-to-many are\nused similarly.\nA cardinality constraint may go further, however. It is sometimes necessary to specify a minimum cardinality in\na given relationship. For example, given a one-to-one relationship between entity type A and entity type B, the\nfollowing specific types of cardinality constraints might also be added:\nc. 1..1 := one entity type A must be associated with one entity of type B\nd. 0..1 := one entity type A may be associated with zero or on entity of type B\nGiven a one-to-many relationship between entity type A and entity type B, the following specific types of\ncardinality constraints might also be added:\n1..m := one entity type A must be associated with one or more entities of type B\na. 0..m := one entity type A may be associated with zero or more entities of type B\nb. 0..x := one entity type A may be associated with zero or up to x entities of type B\nc. 1..x := one entity type A must be associated with at least one or up to x entities of type B\nGiven a many-to-many relationship between entity type A and entity type B, the following specific types of\ncardinality constraints might also be added:\n0..m := each entity may be associated with 0 or more entities of the other type\na. n..m := each entity must be associated with 1 or more entities of the other type\nAnother perspective on cardinality constraints is as dependencies. A dependency defines a relationship that\nis required between entities.\nEX. TR-7:\nIn our train_equipment schema, some cardinality constraints are required. For example, that\ntrain_equipment must include at least one engine in its set of engines. This may seem obvious to us, but in\nterms of data management, we must try to architect our systems to ensure that erroneous data are not\nentered. The use of cardinality constraints helps in this cause. In defining the train_equipment schema then,\nwe can specify a minimum number of engines.\nIn reality in this particular application domain\u2014the management of trains\u2014we may need to derive the\nminimum cardinality of the engines set based on other factors.\nQuestion\n\u27a2 Without necessarily being an expert on trains, can you make educated guesses about the variables that\nshould be considered in determining the cardinality of the engines set?\nEditor\nDr McIver is a Senior Research Officer in the National Research Council of Canada Institute for Information\nTechnology (NRC-IIT), where he is a member of the People-Centered Technologies Group. His research is\nmultidisciplinary, covering computer science and community informatics. At NRC-IIT he combines his computer\n183 science and social informatics research interests to develop information technologies for enhancing community life.\nThis includes democratic participation, exchange of life-critical information, and culture. Dr McIver is a graduate of\nMorehouse College, Georgia Institute of Technology, and the University of Colorado at Boulder with BA, MS, and\nPh.D. degrees, respectively, in computer science. He may be reached at Bill.McIver@nrc.gc.ca. See also the NRC-\nIIT Web site at <http:\/\/iit-iti.nrc.gc.ca> .\nReferences\n[Banker1993] Banker, R. D.; Datar, S. M.; Kemerer, C. F. & Zweig, D. Software complexity and maintenance\ncosts. Commun. ACM, ACM Press, 1993, 36, 81-94.\n[EnvironmentCanada2007] Environment Canada, WMO Volume A Report\u2014Canada, REGION IV\u2014NORTH\nAND CENTRAL AMERICA, Generated 2007-08-13 9:00 pm,\nURL: http:\/\/www.climate.weatheroffice.ec.gc.ca\/prods_servs\/wmo_volumea_e.html\n[FAA2006] Federal Aviation Administration, Passenger Boarding and All-Cargo Data, 2006.\nURL:http:\/\/www.faa.gov\/airports_airtraffic\/airports\/planning_capacity\/passenger_allcargo_stats\/pass\nenger\/media\/cy06_primary_np_comm.pdf\n[FEDEX2007a] FedEx, FedEx Statistical Book, 2007. URL:\nhttp:\/\/www.fedex.com\/us\/investorrelations\/downloads\/sec\/corp\/current_stat_book.xls\n[FEDEX2007b] FedEx, FedEx Express Facts, 2007. URL:\nhttp:\/\/www.fedex.com\/us\/about\/today\/companies\/express\/facts.html\n[Hammer1978] Hammer, M. & McLeod, D. The semantic data model: a modelling mechanism for data base\napplications SIGMOD \u201878: Proceedings of the 1978 ACM SIGMOD international conference on\nmanagement of data, ACM Press, 1978, 26-36.\n[ITU2007] International Telecommunication Union, ICT Statistics, 2007. URL:http:\/\/www.itu.int\/ITU-\nD\/ict\/statistics\/ict\/index.html\n[Kent1978] Kent, W. Data and Reality, Amsterdam; New York: North Holland, 1978.\n[LoC2007] Library of Congress, Fascinating Facts, 2007. URL: http:\/\/www.loc.gov\/about\/facts.html\n[UPU2007] Universal Postal Union, Postal Statistics: Query Results, 2007 URL:\nhttp:\/\/www.upu.int\/pls\/ap\/ssp_report.main?p_language=AN&p_choice=AGGREG\n[WHO2007] World Health Organization, WHO Statistical Information System: Core Health Indicators,\n2007.\nURL: http:\/\/www.who.int\/whosis\/database\/core\/core_select.cfm\n[Wittgenstein1922] Wittgenstein, L. Tractatus Logico-Philosophicus, C.K. Ogden, Trans., Routledge and\nKegan Paul, 1922.\n184 10. Opportunities in the\nnetwork age\nEditor: Leyland Pitt (Simon Fraser University, Canada)\nLearning Objectives\n\u2022 define a business opportunity, especially in a business to customer setting\n\u2022 understand what is meant by the \u201cnew five forces\u201d and how these forces generate business opportunities\n\u2022 explain Moore\u2019s Law and its effects\n\u2022 explain Metcalfe\u2019s Law, and how it operates in network settings\n\u2022 understand \u201cCoasian\u201d, or transaction cost economics, and how innovations in communication technologies\naffect the costs of firms and markets\n\u2022 understand and explain the \u201cflock-of-birds\u201d and \u201cfish-tank\u201d phenomena\nIntroduction\nUS General Douglas MacArthur once said: \u201cThere is no security on this earth, there is only opportunity.\u201d An\nopportunity is simply an appropriate or favorable time or occasion. Alternatively it may be a situation or condition\nthat is favorable for attainment of a goal. It might just be a good position to be in, a good chance, or a prospect.\nOpportunities abound in the network age, but many organizations are blinded to them because they seek security in\nenvironments that make them comfortable. They like dealing with familiar customers, and satisfying familiar needs\nwith familiar offerings, and competing against easily recognizable rivals who do similar things that they do. Yet\nsignificant growth seldom comes from safe comfortable business environments\u2014it usually comes from change.\nBusiness opportunities arise most often when customers don\u2019t have a choice.\nAccording the \u201ctheory of competitive rationality\u201d, ideas developed by marketing professor Peter Dickson (1992),\nin most markets and industries there is a situation of over-supply, or simply, there is more capacity to produce the\ngoods or services than there is a demand for them. There is more capacity to produce cars than there is a demand\nfor them (witness the large number of cars on dealer lots), more capacity to produce life insurance than people who\nwant it, and more capacity to produce clothes than there is a demand for them (as the stacked racks and shelves in\nclothing stores will attest to). Over-supply creates customer choice\u2014without over-supply, customers have no\nalternatives. And how do customers behave when they are faced with choice? They exercise it. In doing so, they\nacquire information, they make all kinds of trade-offs, and they make decisions. In doing so, they learn, become\nsmarter, and more sophisticated with regard to the particular product or service.\nHow do firms respond to sophisticated, smarter customers? They innovate\u2014they try to offer customers\nsomething they haven\u2019t seen before, something new, that will get their attention and hopefully their spending. But\nthe problem with innovations is that competitors copy them, and that imitation leads to\u2026more oversupply. And so\nthe cycle of competitive rationality keeps turning.\n185 By now, some readers will have thought of current exceptions to the oversupply rule. There is no over-supply for\nexample, of organs for transplantation. There is no over-supply of great works of art\u2014Rembrandt, Vermeer and\nVan Gogh are deceased. There are many other situations where there is no over-supply, and of course this means\nthat customers have no choice. Biotech firms are working hard to find answers to the organ donation problem, and\nmuseums strive to find solutions to the fact that lots of people want to see, appreciate and perhaps even own a great\nand famous picture. The point is simple: When customers don\u2019t have a choice, smart entrepreneurs see\nopportunities.\nThe network age\u2014the age of the Internet, cellular phones, digital media, satellites and GPS systems\u2014is spinning\noff opportunities faster than almost any age in history. In this chapter we consider some fundamental forces that\nmay serve as guideposts to entrepreneurs in identifying, at best, opportunities that might be identified, or at least,\nrecognize potential threats to existing firms in business to customer markets.\nTraditional strategy and killer applications\nA leading influence on strategic thinking throughout the 1980s and 1990s has been Harvard Business School\nstrategy professor, Michael Porter. Practitioners, academics and consultants alike have used his well-known \u201cfive\nforces\u201d model to evaluate industry attractiveness as strategic positioning. Porter\u2019s \u201c5 Forces Model\u201d2,3 is very\norderly and structured\u2014in fact, very applicable to the 1980s, which was a far less fragmented era than what the late\n1990s, and especially the new millennium, have proven to be. It explains neatly why some industries are more\nattractive than others in a way that at least gave managers confidence in their judgment, even if it didn\u2019t make them\nfeel better about being in a dead-loss market. Similarly, at least the forces were all about business and management\nissues\u2014customers and suppliers, barriers to entry and substitute products, and of course, good old firm-to-firm\ncompetition itself. Small wonder then that practitioners, consultants and academics loved the approach, for it gave\nreasonable predictability in a reasonably predictable world.\nThe traditional view of strategy in organizations has been that it is possible to understand fully the environment\nin which the organization functions, and therefore to plan for the firm's future accordingly. This view might be\nacceptable when the environment changes slowly, and the future is reasonably predictable. It might even be\ngratifying when trends are linear. However, in recent years some observers have noted that the environment is\nchanging so swiftly that it is not possible to strategically plan for the future (see for example, Downes and Mui4). As\nwe saw in the first chapter, trends nowadays are usually paradoxical and contradictory rather than linear. The new\nenvironments that emerge, especially as the result of phenomenal changes in technology, have profound effects on\nsociety, and not just on the firms that operate within it.\nIf one were to study the occurrence of inventions in history, one phenomenon is particularly prominent\u2014the\nrate at which technological change occurs over time. During the Middle Ages, for example, significant innovations\nappeared at a very slow rate-sometimes as infrequently as 200 or 300 years apart. During the time of the\nRenaissance, new technologies begin to emerge slightly more rapidly\u2014for example the invention of movable type by\nGutenberg. In the era of the Industrial Revolution, inventions begin to surface far more frequently, now at the rate\nof one every 5 or 10 years. Entering the 21st century, we begin to see innovations break the surface once every two\nyears, or indeed every year. The kinds of innovations that we are talking about are not simple improvements\u2014\nrather, we are referring to what have become known as \"killer applications\".\n186 A killer application or \"killer app\"4 is not merely an innovation that improves the way something is done. It is\nnot even something that merely changes a market or an industry\u2014indeed, a killer application is one that changes\nthe way society itself, works and functions. The automobile was a \u201ckiller app\u201d because it didn\u2019t just simply replace\nhorse-drawn carriages, or alter the way people traveled\u2014it transformed the way we live, shop, work and spend our\nleisure time. It also changed the appearance of the physical environment in most countries. In the past 10 or 15\nyears we have seen killer applications arise at the rate of more than one a year, and this frequency is increasing in\nan exponential fashion at the moment due to \u201cspreading\u201d technologies such as the Internet. So strategy that\nattempts to plan five years ahead is befuddled by the fact that society and the way the world works may indeed\nchange at the rate of one or two killer applications a year. The more traditional strategic planning models such as\nthose of Michael Porter are less effective at dealing with the kind of strategic planning problems that killer\napplications and rapid technological changes cause.\nThe five new forces\nWe need to develop a perspective on the new forces that impact on strategy and the way organizations deal with\nthe future. One possibility is that, in the spirit of Porter\u2019s 5 forces, we consider five new forces that will affect the\nway the business and management environment works. We also illustrate these forces and their effects, and how\nthey spun off opportunities in business to customer situations for alert entrepreneurs as well as threats to the\nincumbents, using two cases: First, the music industry worldwide and second, the online betting exchange,\nBetfair.com. (These two industries are briefly described in the vignettes illustrated in Exhibit 54 and Exhibit 55).\nThe effect of these forces on individuals and organizations is illustrated and summarized in Exhibit 54. In no\nparticular order these forces are:\nExhibit 54: The new five forces\nMoore\u2019s Law\nIn 1965, an engineer at Fairchild Semiconductor named Gordon Moore noted that the number of transistors on\na computer chip doubled roughly every 18 to 24 months. A corollary to \"Moore's Law\", as that observation came to\nbe known, is that the speed of microprocessors, at a constant cost, also doubles every 18 to 24 months. Moore's Law\nhas held up for more than 40 years. It worked in 1969 when Moore's start-up, Intel Corp., put its first processor\n187 chip\u2014the 4-bit, 104-KHz 4004\u2014into a Japanese calculator. It worked at the return of the century for Intel's 32-bit,\n450-MHz Pentium II processor, which had 7.5 million transistors and was 233,000 times faster than the 2,300-\ntransistor 4004. And it works today, when Intel\u2019s Rio Rancho factory is expected to begin producing 45-nanometer\nchips\u2014meaning they will have features as tiny as 45-billionths of a meter\u2014in the second half of 2008. The\ntransistors on such chips are so small that more than 30 million can fit onto the head of a pin. Intel says it will have\na 1-billion-transistor powerhouse performing at 100,000 MIPS in 2011.\nFor users ranging from vast organizations to children playing computer games, it's been a fast, fun and mostly\nfree ride. But can it last? Although observers have been saying for decades that exponential gains in chip\nperformance would slow in a few years, experts today generally agree that Moore's Law will continue to govern the\nindustry for another 10 years, at least. Moore\u2019s Law is illustrated graphically in Exhibit 55, which shows the\nincreases.\nExhibit 55: Moore's Law\nThe implications of Moore\u2019s Law are that computing power becomes ever faster, ever cheaper. This not only\nmeans that just about everyone can therefore have affordable access to powerful computing, but that the power of\ncomputers can be built into devices other than computers themselves. Moore\u2019s Law also drives convergence by\nplacing computer chips into objects that previously had nothing to do with them\u2014today there is more processing\npower in the average cellular telephone or digital television set than NASA had access to when Neil Armstrong\nlanded on the moon in 1969. Already, computers are used in products as diverse as vehicles, surgical equipment\nand elevators, enabling these machines to operate more efficiently, predictably and more safely. We are beginning\nto see computer chips in disposable products such as packaging, as the costs continue to decline. Hitachi Ltd., a\nJapanese electronics maker, recently showed off radio frequency identification, or RFID, chips that are just 0.05\nmillimeters by 0.05 millimeters and look like bits of powder. They're thin enough to be embedded in a piece of\npaper, yet are able to store considerable amounts of information which can then be read and processed. Computers\nhave become ubiquitous\u2014they are everywhere, but we do not consciously think of them or notice them.\nThe primary question that Moore\u2019s Law should prompt in strategic planners is this: what will our industry or\nmarket be like when computers or chips are literally everywhere\u2014in every product we make or part of every service\n188 we deliver? Some managers may think this is silly, simply because it is difficult for them to imagine a computer or\nchip in their product or service. Yet there are countless products or services being delivered today that have\ncomputers as an integral part that the same reasoning would have applied to just twenty years ago: Hotel doors\nwith chips in that facilitate card access and record entry and exit; microwave ovens; and digital television. In the\nrecent past we have witnessed the demise of the VCR, as home owners turn to hard drives to record many hours of\ntelevision entertainment. An 80 gigabyte hard drive recorder costs less than USD 300. In the lifetime of most\nreaders of this book, there was a time when the combined computer storage of most countries did not reach 80\ngigabytes.\nMetcalfe\u2019s Law\nHow useful is a piece of technology? The answer depends entirely on how many other users of the technology\nthere are and on how easily they can be interconnected. For example, the first organization with a facsimile\nmachine had no one to fax to, and no one to receive faxes from! One telephone is useless; a few telephones have\nlimited value. Many millions of telephones create a vast network. These effects are known as Metcalfe's law. Robert\nMetcalfe, founder of Novell, 3COM Corporation and the designer of the robust Ethernet protocol for computer\nnetworks observed that new technologies are valuable only if many people use them. Roughly, the usefulness, or\nutility of the network equals the square of the number of users, the function known as Metcalfe's Law. This is\nillustrated in the simple line graph in Exhibit 56.\nExhibit 56.: Metcalfe's Law\nThe more people who use software, a network, a particular standard, a game, or a book, or indeed a language\nsuch as English, the more valuable it becomes and the more new users it will attract. This in turn increases both its\nutility and the speed of its adoption by still more users. The Internet is perhaps the best illustration of Metcalfe's\nlaw. While it began in the 1960s, it is only in the past dozen years that it has gained momentum\u2014as more users\njoined the medium it became more useful to even more users, thus accelerating its growth. Now its potential to\nspread new ideas, products and services is awesome. Other good examples of Metcalfe\u2019s Law in recent years have\nbeen cellular telephones and the ubiquitous Palm digital assistant. In the case of the latter for example, most early\nadopters recommended the product to friends and colleagues, and the Palm succeeded not because of a large\n189 advertising budget but because of word-of-mouth. The early adopters insisted that others buy the product not just\nbecause it was good, but because it increased the size of their network, which made their own Palms more useful.\nOne of the key factors in this success was the Palm\u2019s ability to \u201cbeam\u201d to other Palms via a built-in infrared device.\nPalm users were proud not to carry paper business cards, and preferred to beam their details to others.\nNetworks are important because they create short cuts. Anyone who is part of the network can contact anyone\nelse who is part of it, and bypass more traditional channels and structures. This is important for planners who\nshould consider what the effects of technology will be that enables their customers to talk to each other, suppliers to\ntalk to each other, and customers to talk directly with suppliers, wherever in the world they may be. As networks\ngrow, their utility increases, so Metcalfe\u2019s Law tells us\u2014this is true for those who are part of the network, and for\nthose who then choose to join it.\nCoasian economics\nNobel Prize winner in economics, Ronald Coase made a discovery about market behavior that he published in a\n1937 article entitled \"The Nature of the Firm\"5. Coase introduced the notion of \u201ctransaction costs\u201d\u2014a set of\ninefficiencies in the market that add or should be added to the price of a good or service in order to measure the\nperformance of the market relative to the non-market behavior in firms. They include the costs of searching,\ncontracting, and enforcing. Transaction cost economics gives us a way of explaining which activities a firm will\nchoose to perform within its own hierarchy, and which it will rely on the market to perform for it. One important\napplication of transaction costs economics has been as a useful way to explain the outsourcing decisions that many\nfirms face\u2014for example, whether the firm should do its own cleaning, catering or security, or pay someone else to\ndo this.\nThe effect of communication technology on the size of firms in the past has been to make them larger.\nCommunication technologies permits transaction costs to be lowered to the extent that firms are capable of\nsubsuming many activities within themselves, and thus are able to operate as larger entities even across continents.\nThis has permitted multi-nationals such as General Motors, Sony and Unilever to operate as global enterprises,\nessentially managed from a head office in Detroit, Tokyo or London, or wherever. Communication technology such\nas telephones, facsimile machines and telex machines enabled these operators to communicate as easily between\nDetroit and Sydney as between Detroit and Chicago. Smaller firms found this more difficult and more expensive.\nSo, large firms brought more activities within the firm (or the \u201chierarchy\u201d in transaction cost terms), for it was\ncheaper to do this than to rely on the market.\nWhat strategic planners will overlook at their peril in the age of the Internet is that these same communication\ncapabilities are now in the hands of individuals, who can pass messages round the world at as low a cost as the\nbiggest players\u2014essentially, for free. Free voice over Internet protocol services such as Skype allow individuals to\ntalk for free, regardless of location or distance. They can also hold multi-user conferences, including live video, for\nfree, and simultaneously transmit documents and images. The effect of the new communication technologies,\naccelerated by Moore\u2019s Law and Metcalfe\u2019s Law will be to reduce the costs of the hierarchy. But more especially,\nthey will reduce the costs of the market itself. As the market becomes more efficient the size of firms might be\nconsiderably reduced. More pertinently, as the costs of communication in the market approach zero, so does the\nsize of a firm, which can now rely on the market for most of the activities and functions that need to be performed.\nA very thorny strategic issue indeed!\n190 A simple illustration of the effects of transaction cost reductions in an industry is shown in Exhibit 57. When\nbanking is done across the counter with a teller, the average cost per transaction exceeds a dollar; when the same\ntransaction is conducted online the costs reduce to nominal cents. Yet most banks charge their customers more for\nonline banking! One wonders how long they will continue to do this, and how long customers will tolerate it?\nAlready, alternative services are beginning to emerge that may prove more appealing to many customers.\nExhibit 57: Reducing costs of banking\ntransactions\nThere are many strategic questions that Coasian economics prompts in the age of the Internet. However, what\nshould undoubtedly top the agendas of many strategic planners in this regard is the issue of what functions the\nInternet will permit them to outsource. Allied to this is the matter of responding to competitors who do not carry\nthe burden of infrastructure normally borne by traditional firms, having relied on technology to effectively\noutsource infrastructure and functions to the market.\nThe flock-of-birds phenomenon\nA feature of many of the new communication technologies has been the fact that in most cases they do not\n\u201cbelong\u201d to any one institution, nor does any particular authority control them. The Internet is a case in point.\nSome have referred to this as the \u201cflock-of-birds phenomenon\u201d6. When one observes a flock of birds flying in\nformation, or fish schooling in a distinct pattern, one is tempted to speculate whether there is or could be a \"bird in\ncharge\" or an \"alpha fish\". Naturalists will explain that flocking is a natural phenomenon and that there are indeed\nno \u201chead\u201d fishes or birds in charge. The network becomes the processor.\nHumans have been conditioned to seek a controlling body or authority for nearly all of the phenomena that we\nexperience in life, for that is indeed how most modern societies have been organized. The response to the questions:\n\u201cWho\u2019s in charge?\u201d, \u201cWho controls this?\u201d, or \u201cWho owns it?\u201d is generally, \u201csome large firm\u201d, \u201cthe government\u201d, a\ngovernment department or ruling institution. In the case of many of the phenomena of the observed on the\nInternet, there is indeed no one in charge. They are like giant flocks of birds or schools of fish. The response to\nquestions such as: \u201cWho owns them?\u201d or \u201cWho\u2019s in charge?\u201d is either \u201cWe all do\u201d, or \u201cNo one does\u201d. These are great\nmechanisms for democracy, but their effects can also be anarchic, and society may have to develop new ways to deal\nwith these liberating effects.\nThe effect of the flock-of-birds phenomenon is that access is equalized, unlike what occurs in traditional media.\nIn a very real sense, no one has a \u201cbetter right of access\u201d, and no one, even the largest corporation, can shout louder.\nThe smallest player, the individual, has a right and opportunity to be seen and heard. Furthermore, many laws\ndesigned to regulate a physical world do not work as effectively when no one owns or controls the medium.\n191 The fish-tank phenomenon\nMoore's Law and Metcalfe's Law combine to give individuals inexpensive and easy access to new media such as\nthe Internet. This means that any one can set up a website and theoretically at least, be seen by the world. As a\nresult many have noticed the so-called \u201cfish-tank phenomenon\u201d6. The fish-tank phenomenon is named after the\nfact that in the early days of websites, people used to put a video camera on top of their tropical fish tank (or coffee\npercolator, for that matter), so that when you logged on to their site that is what you saw. This added to the clutter\nand junk on display\u2014today there are hundreds of thousand of silly, futile, \"junk\" sites that only do something silly\u2014\nlet the viewer build a cow, tickle Elvis Presley\u2019s tummy, cure their addiction to lip balm, or whatever. The question\nthat this prompts is, wouldn't it be better if, rather than relying on individuals for their input on the Net, we\ndepended instead on the considerable resources of large institutions and corporations?\nThe answer to this question really lies in another: what is more profound? And the answer to this second\nquestion is that, actually, it is the creative inputs of millions of individuals, all over the world who now have the\nability to show us what they can do. In other words, the creative outputs of millions of individuals will beat the\ndoings of large institutions in a great majority of cases. So, while we may see lots of junk, such as fish tanks, coffee\npercolators, and devices to tickle a long dead rock artist, every now and then some individual (probably a\nseventeen- year-old in his or her bedroom) is going to produce something so revolutionary that it will change our\nworld. For strategists this means that many firms may find themselves threatened by small start-ups that were\npreviously unable to get access to the market. No longer will it be good enough to merely observe one\u2019s close and\nknown competitors, in the future these competitors could be anyone and anywhere. They may be difficult to see\nbefore it is too late\u2014they usually fly under the radar.\nHow the new five forces work in industries and markets\nThe music industry (see Exhibit 58) and Betfair (see Exhibit 59) represent classic cases of the five technology-\nrelated forces, which when working in concert, cause radical change in industries and markets. They also illustrate\nhow astute entrepreneurs have utilized the five forces to uncover immensely valuable opportunities in business to\ncustomer systems. Observers of industries diverse as music, online betting, and telecommunications (e.g. Skype)\nwill immediately recognize the generalizable parallels between these innovations and Betfair, and the applicability\nof the five forces. An examination and comparison of the music industry and Betfair enable an intriguing\nexamination and extension of these forces.\nExhibit 58.: The global recorded music industry\nFor many years\u2014indeed since Edison\u2019s invention of the phonograph in 1877\u2014the music recoding industry did\nnot change that much. While technological changes did come to the market for recorded music over the years, in\nthe form of improved recording techniques, hi-fidelity stereo, and the advent of the compact disk (CD), essentially\nthe industry remained stable, with its structure largely unaffected by technological developments. Recording\ncompanies found and recorded talent and marketed it, and the products of the industry\u2014essentially disks and\ncassette tapes\u2014were distributed through record stores. Artists were remunerated in the form of royalties, retailers\nin the form of margins, and the record companies kept the rest.\nThe fundamental distribution issue of assortment was (and in many ways still is) perhaps the most significant\ndilemma in the market for recorded music. The structure of the industry and the way the product was produced\nheld an inherent problem for both the retailer and the consumer. The retailer\u2019s predicament is that of inventory\u2014\n192 the need to hold very large stocks of records, in order to provide a selection to customers, and to be able to make\navailable to the customer the one that they will choose when they want it. This means that a lot of capital is tied up\nin stock, much of which moves slowly and often needs to be discounted in order to meet working capital\nrequirements. The consumer is also in a quandary: Will the particular retailer stock the one album that they are\nlooking for? And, will they be able to find it among the thousands of other items? Even once found, the consumer\u2019s\nproblems are not over\u2014there may be 12 songs on the album, and they may only really want three or four. But they\nare forced to purchase the entire album with all 12 songs.\nExhibit 59.: The market for sports betting\nPlayers wishing to have a wager on a football game, or a flutter on a horse race, or to back up their opinions on\na political election or the outcome of the Oscars, have until recently had few alternatives. In a majority of\nEuropean countries, Canada, and most states in the USA their only option would have been to place a bet on a\npari-mutuel or totalisator system, while in countries such as the United Kingdom and Australia, and in US states\nsuch as Nevada, they would also have access to licensed bookmakers. Both of these systems place the player at a\nsignificant disadvantage, not least of which is the fact that the \u201crake-off\u201d or house percentage is considerable. This\nmeans that winners get paid well below the \u201ctrue\u201d odds against their choice. Furthermore, neither of the two\nsystems allows a player to pick a \u201closer\u201d\u2014the player can only stake on a winning outcome. In simple terms a player\ncannot back team to lose\u2014they can only back the other team to win or on a draw. A specific disadvantage of pari-\nmutuel systems is that subsequent weight of money for a player\u2019s choice will reduce the payoff, so that there is no\nopportunity to exercise any skill in the timing of a bet. Both systems profit not from the losers (as most inexpert\ngamblers believe) but from winners, by paying them out at less than true odds. In the case of pari-mutuels the\nhouse percentage is around 20 per cent, and even the most generous bookmakers make books that have an edge of\naround 14 per cent in their favor.\nBetfair (www.betfair.com) which commenced trading in 2002 is the world\u2019s largest betting exchange. As a\nbusiness, Betfair has no interest in the outcome of any event it makes available to gamblers. It simply provides a\nmarket for opinions and for trades to take place. It requires players to make and\/or take fixed odds and all income\nis derived from a small percentage commission (ranging between two and five per cent, depending on a player\u2019s\nturnover) on a player\u2019s net winnings on an event. In general terms, the greater an event\u2019s turnover, the more\nrevenue and profit Betfair generates, although Betfair\u2019s income is strictly a function of the total net winnings on an\nevent, not turnover.\nHow Moore\u2019s law affects music and gambling\nIn the music industry, many consumers can now afford a relatively powerful computer, and use it as a device for\nrecording and playing recorded sound. The cost of storage media has declined exponentially\u2014multi-gigabyte hard\ndrives that would have exceeded the storage capacity of entire countries just a few years ago are now so inexpensive\nthat individuals can use them not just to store complete music collections, but to carry around in their pockets. The\nsuccess of Apple\u2019s iPod is testimony to this. The hugely popular devices\u2014some as large as 60 gigabytes in capacity\u2014\nhave become fashion icons. No one thinks of them as \u201ccomputers\u201d with \u201chard drives\u201d.\n193 It would be fair to say that without the effects of Moore\u2019s Law, Betfair would not exist. On the one hand, just ten\nyears ago, the computing power required to process, manage and store the millions of rapid transactions Betfair\nhandles each day (Betfair processes more transactions each day than Visa) was simply unavailable. On the other,\nBetfair relies on the fact that its customers also have access to considerable computing power, in the form of\naffordable and easy to use laptop and desktop computers, and that these have access to the Internet. A generation\nago, only pari-mutuels and the very largest bookmaking firms had access to computers, and the ordinary player had\nnone.\nMetcalfe\u2019s Law\u2014networks in music and wagering\nMusic consumers find that purchasing or obtaining music online suits them far better than acquiring hard copy\nmusic products through traditional retail outlets. Napster, the first major advance in music downloads provided a\nfree music sharing service that enabled consumers to choose and download only those songs they wanted. Its main\nproblem was that it relied on one central exchange to facilitate the exchange of music, and at its peak its network\nconsisted of more than 20 million users. The fact that it had a centralized server (or in network terms, one main\nhub) meant that it made an easy target for the organized music industry in its legal battle to close Napster down.\nApple\u2019s iTunes service sells music (legally) under a similar system, and charges around USD 1 per song, grossing\nover USD 1 million daily.\nHowever, users no longer need to rely on centralized distribution or even central servers for their music\u2014the\nInternet is a huge network of distributed power, which connects anyone to anyone else in a very short time.\nNowadays services such as Morpheus permit users to share millions of files (including movies and pictures as well\nas music) daily (although the legality of these exchanges is questionable). The first of these distributed network\nsystems was Kazaa, the brainchild of Niklas Zennstr\u00f6m and Janus Friis. After numerous legal battles with the\nrecording industry, these entrepreneurs used their peer-to-peer technology to create Skype, the Internet telephony\nnetwork. Nowadays Skype exceeds an average of 9 million daily users worldwide. The network effects are immense\n\u2014users encourage friends and family to download Skype so that they can all engage in free communication. Skype\nwas acquired by eBay in 2006 in a multi-billion dollar deal.\nMetcalfe\u2019s Law is a very real factor explaining Betfair\u2019s success. The fact that more than fifty thousand players\nuse the site to place and lay bets each day means that the likelihood of any individual player finding a match for\nwhat they want (assuming that what they are asking for is reasonable) is very high. This of course results in highly\nefficient and very liquid markets, which are to the advantage of all players.\nNetworks are important because they create short cuts. Anyone who is part of the network is by definition in\ncontact with anyone else who is part of it, and can therefore bypass more traditional channels and structures (such\nas conventional bookmakers and totalisators in this case). As networks grow, their utility increases, so Metcalfe\u2019s\nLaw tells us\u2014this is true for those who are part of the network, and for those who then choose to join it. Neither\ntraditional bookmakers nor totalisators enjoy the same network advantages as Betfair: bookmakers only have\ncontact with players in their geographic vicinity or those with telephone access. Totalisators are on tracks only in\nsome countries, where they will be affected by liquidity problems on quiet days, and even when they are not, they\nare restricted by local geography. Betfair faces none of these restrictions, and can enable the world to wager.70\n70 While this is true in principle, in reality there are some exceptions. For example, wagering online with sites outside of its\njurisdiction has been outlawed by the US government. While many other countries decry this as just another example of\nAmerican government bullying and in blatant disregard of all of the initiatives on free global trade, the US government has\nbeen ruthless in pursuing this policy. It has arrested executives of foreign online wagering operations on entry into the USA;\n194 Coasian economics: transaction costs in online music and wagering\nHierarchies such as large music companies and their distribution networks are no longer able to achieve the\nlowest transaction costs in the recording industry. The low costs of communication and distribution have made the\nmarket more efficient, and millions of individuals benefit as a result. After the invention of the tape recorder,\ncopying music became technically feasible, but of course the network effects were very limited: the song had to be\nplayed while it was being copied, and distribution was limited to a few local friends who had tape players.\nComputers and the Internet nowadays mean that a song(s) can be copied very quickly, and distributed, theoretically\n(if not always legally) at least, to a multitude of other users in a few seconds.\nWhen contrasted with traditional bookmaking firms, Betfair is a fine example of the potential of low cost,\nnetworked computing to make markets more efficient by lowering transaction costs. Traditional bookmakers need\nto be well informed (studying horse racing and sporting events carefully) in order to make odds, and need to\nmonitor market changes constantly in order to avoid being taken advantage of or of being over-exposed. Betfair\nsimply provides the platform for many individuals (theoretically, \u201cfirms of one\u201d) to do this for themselves. These\nindividuals do not have to rent space or equipment, advertise, or employ staff. With a click of a mouse they can\nessentially either do what Ladbrokes or William Hill does\u2014or, if the fancy takes them, play their fancies.\nThe flock-of-birds phenomenon: lawlessness in music and gambling\nThe exchange and distribution of recorded music has become very difficult to legislate and control. While it\neventually became possible for the Recording Industry Association of America (RIAA), after extensive legal and\npolitical efforts to take action against and eventually close Napster, it is unlikely that it can have the same results\nagainst millions of individuals all over the world. When it has attempted to pursue these, it has come out in\npublicity looking simply like a blundering bully. Also, just because it may succeed in one country (e.g. the USA)\ndoes not mean it will succeed in another with a different legal system. Technologies like Gnutella, Kazaa and\nMorpheus made this even more unlikely and complex. Commenting on the RIAA\u2019s attempts to block Napster,\nCharles Nesson (cited in 7), professor at the Berkman Center for Internet and Society, Harvard Law School had this\nto say of Gnutella: \u201cThere is a generation of young people out there who have already learnt that music is something\nyou get on the Net, rather than buy. The only way for the music industry to stop Gnutella is to turn off the Internet\u201d.\nHe added: \u201cAnd, as no one owns it or controls it, that is impossible.\u201d\nThe case of Betfair has seen some interesting reactions from incumbents and also governments at various levels.\nTraditional competitors have attempted to cry, \u201cno fair\u201d, as they struggle to contend with a player whose methods\nthey do not fully understand. Sports bodies are anxious to understand the possible impact of a firm such as Betfair\non the potential for financial abuses in their domains. Governments, while supposedly welcoming competition as a\nforce in consumer interest, will immediately seek new ways to tax new entrants such as Betfair, for it seems like, yet\nis not like, traditional firms such as bookmakers and totalisators.\nIn the United Kingdom, Betfair has been the focus of lawsuits by the major British bookmaking firms such as\nCoral, William Hill, and Ladbrokes. They have charged that by allowing individuals to lay bets (rather than merely\ntake them), they are acting as bookmakers, and are not licensed to do so. Graham Sharpe, spokesman for William\nHill argues: \"If you have a bet on an exchange, you don't know who it's with; if [the person] is offering extravagant\nodds, you don't know why.\"\nthese include Canadians and UK citizens.\n195 The British courts have thrown out all the cases and rejected the bookmakers\u2019 arguments, and Betfair has been\nallowed to proceed with its business. While bookmakers might not like the idea of the site, it is evident that many of\nthem are using it, either to buy back bets at advantageous prices, or to lay bets for which they might otherwise not\nhave been able to find takers. The substantiation for this is the significant amounts of money that are available to be\ntraded on many events.\nIn Australia, not only have bookmakers objected to the site and attempted to close it down through legal action,\nit has also been the focus of an aggressive advertising campaign by TABCORP (essentially a consortium of\nTotalisator operators), a listed company that is the world\u2019s fourth largest gambling and entertainment business. Its\nadvertising has attempted to cast Betfair in a negative light by claiming that betting on exchanges encourages\ndishonesty in sporting events and racing, a similar argument used in the United Kingdom by bookmakers. Betfair\nhas disabled certain features on its Australian site to comply with Australian gambling legislation (for example,\nAustralian players are not able to bet \u201cin the running\u201d\u2014that is, after an event has started or a race is running).\nGambling is more a matter of state than federal legislation in Australia. While the legislation is not quite clear\nwhether it is legal or not for Australians to bet on betting exchanges or online casinos outside of Australia, there is\nno legislation prohibiting players in one state betting on operations in another.\nThe real effect of the flock-of-birds phenomenon is that access is equalized by mechanisms such as Betfair when\nthey operate online, unlike what occurs in traditional operations such as bookmakers and totalisators. In a very real\nsense, no one has a \u201cbetter right of access\u201d, and no one, even the largest corporation, can shout louder. The smallest\nplayer, the individual, has a right and opportunity to be seen and heard. Furthermore, many laws designed to\nregulate a physical world do not work as effectively when no one owns or controls the medium. This has been\napparent in the lack of success in the traditional incumbents\u2019 attempts to fight Betfair in courts of law.\nIt has also been demonstrated by Betfair being a better, not less effective, mechanism for the detection of\ndishonest dealings in sport. If a particular player has inside information on an event, there is nothing stopping\nthem from exploiting this advantage by placing large cash bets either with bookmakers or totalisators, and reaping\nthe benefits of this insider trading anonymously. Betfair is arguably in a better position to deal with this type of\nproblem. For example, in July 2004, Betfair was dragged into the spotlight when it reported suspicious betting\npatterns on its exchange to the Jockey Club in the United Kingdom just before the Lingfield race in which leading\njockey, Kieren Fallon, riding favorite Ballinger Ridge, lost to Rye after seemingly easing down before the finishing\nline. News of the World alleged Fallon had told an undercover journalist that Rye would win. No proof was found\nthat the race was fixed, but a Betfair spokesperson was quoted as saying: \"We are putting a searchlight on the sport\nand helping it clean up its act. There is a clear paper trail on our site that doesn't exist in high-street [betting] shops.\nWe are entirely transparent. We have no vested interest in the outcome of a horse race.\"\nAs observed, there is also the issue of how governments will attempt to tax firms such as Betfair. Ideally, from\nthe firm\u2019s point of view, tax would best be levied on net profits (which in simple terms would amount to net\ncommissions on winnings less other direct costs). However, governments might not see it in that way, and might\nattempt to tax market makers such as this based on other measures, such as turnover. Such a measure could be very\ndetrimental to the firm and its thousands of players, because turnover is not an accurate measure of financial\nperformance.\n196 The fish-tank phenomenon: the power of creative individuals in music and wagering\nWhile there are a lot of junk as well as stupid schemes placed on the Internet, there are also an incredible\nnumber of good ideas developed by individuals (often a teenager in their bedroom) that can now finally see the light\nof day through this medium. In the latter half of 1998, the ground shifted in the music business when MP3 arrived\non the Web. MP3 is short for Moving Picture Experts Group Audio Layer III or \u201cMPEG3\u201d, and is a compression\nformat that shrinks audio files with only a small sacrifice in sound quality. MP3 files can be compressed at different\nrates, but the more they are scrunched, the worse the sound quality. A standard MP3 compression is at a 10:1 ratio,\nand yields a file that is about 4 MB for a three-minute track. MP3 started life in the mid-1980s, at the Fraunhofer\nInstitut in Erlangen, Germany, which began work on a high quality, low bit-rate audio coding with the help of\nDieter Seitzer, a professor at the University of Erlangen. In 1989, the Fraunhofer Institut was granted a patent for\nMP3 in Germany and a few years later it was submitted to the International Standards Organization (ISO).\nIn 1997, a developer at Advanced Multimedia Products created the AMP MP3 Playback Engine (essentially a\npiece of software that plays MP3 recordings), which is regarded as the first serious MP3 player. Shortly after the\nAMP engine was made available on the Internet, two university students, Justin Frankel and Dmitry Boldyrev, took\nthe AMP engine, added a Windows interface and dubbed it \"Winamp\". In 1998, when Winamp was offered up as a\nfree music player, the MP3 craze began: music enthusiasts all over the world started MP3 hubs, offering\ncopyrighted music for free. Before long, other programmers also began to create a whole toolset for MP3\nenthusiasts. Search engines made it even easier to find the specific MP3 files people wanted, and portable\nWalkman-size players like the Rio let them take MP3 tracks on the road after first downloading them on to a\ncomputer hard drive and then transferring them across.\nWhen Napster became available on the Internet in 1999, it allowed anyone with a connection to find and\ndownload just about any type of popular music they wanted, in minutes. By connecting users to other users' hard\ndrives, Napster created a virtual community of music enthusiasts that has grown at an astonishing pace. Developed\nby a twenty-year-old student named Sean Fanning, Napster boasted some twenty million members throughout the\nworld by the end of 2000.\nThe common thread through all of the above online music history is the absence of any major, for-profit\nrecording company in any of the technological developments. Indeed, the only role played by any of the incumbents\nwas that of stifling, or attempting to stifle, progress. The major innovations came from academic institutions,\nstudents, and penniless enthusiasts whose only real resources were talent, persistence, creativity\u2026..and an Internet\nconnection. Sean Fanning was a young, not very wealthy student, and not in the research department of a major\nrecording company. A firm\u2019s next serious competitor might not be a multinational conglomerate, but an individual\noperating from home. This individual now has a unique mechanism for bringing good ideas to market.\nSimilarly, Andrew Black invented Betfair and changed sport wagering forever. He never worked for a\ngovernment totalisator or pari-mutuel agency, nor was he availed of the significant resources of one of the major\ntraditional bookmaking firms. He was simply a very talented individual, frustrated that trying to pick a winner was\ndifficult enough without having the odds truly stacked against the player. There had to be a better way\u2014and that\nway was through the Internet. Neither Sean Fanning nor Andrew Black had previous experience of the industries\nthat they changed forever, and neither had worked for any of the established incumbents previously. The\nincumbents\u2019 only action was to try to smother the innovations that they saw as threats, rather than opportunities at\n197 best, or at worst, the writing on the wall. Recent evidence is that their attempts at suppression have failed\nmiserably.\nSummary\nIn the future, firms may still need to consider Porter\u2019s 5 forces\u2014but they will be a very different set of forces, if\nthey are to uncover the opportunities that networks present in business to customer markets. There will be the\ntechnological effects of Moore\u2019s and Metcalfe\u2019s laws, hyper-accelerating change and spreading it like a deadly virus.\nThere will be the contradictory effects of transaction cost economics, not only making firms smaller, but virtual too.\nThere will be the societal effects of the flock-of-birds phenomenon bringing undreamed of democracy along with\nthe threat of anarchy. And the fish-tank phenomenon brings access to all.\nMichael Porter2 argues that his five forces determine the attractiveness of an industry, which in turn influences\nstrongly the profitability of firms within that industry. The five new forces of the information age are more ethereal\nand impact on firms and industries in ways that are far less predictable or structured. To use the new five forces\nastutely the decision maker must depend on them not so much as guidelines and prescriptions, but as prods from\nbehind to keep challenging oneself, one\u2019s firm and one\u2019s market. When this is done effectively, it is likely that many\nopportunities will raise their heads.\nThe technological forces of Moore\u2019s Law and Metcalfe\u2019s Law accelerate change not only within a firm, but also\nwithin industries and markets, and this acceleration tends to be exponential. The decision maker must consider\nwhat will happen when computer chips are not just in computers, but also in every device and product, and what\nwill happen when these computers, like all computers, in their turn become part of an exponentially growing\nnetwork.\nTransaction cost economics, and technology\u2019s effects on the efficiency of firms and markets means that the\nmanager must constantly reflect on what will happen to the shape and size of the firm. The decision maker must\ncontinually evaluate what activities the technology will allow to be performed in the market, and what functions\nmay indeed be brought back within the firm itself. In channels of distribution, managers will have to observe the\nconstant tussle between disintermediation and reintermediation. In the case of the former, many traditional\nintermediaries will disappear from channels as their roles are either usurped by technologies, or performed more\nefficiently by other channel members. Already the Internet is having a profound effect on institutions such as travel\nagents and financial brokers, and the long-term impact of online music on conventional record stores is obviously\nof great concern to those institutions. In example after example of reintermediation, we are seeing new\nintermediaries enter channels using technology to improve the channel\u2019s efficiency while taking a share of the\nmargins available in the channel for themselves. Online consolidators such as Priceline.com in the travel industry,\nand Autobytel.com in the channel for new and used cars are prime examples of this.\nThe social forces of the flock-of-birds phenomenon and the fish-tank phenomenon will require managers to\nwork in a new environment where control and governance are not as structured and clear as they have been\nthroughout most of our lives. Managing in a world where significant issues are not really within the control of a\ngovernment or a government department, or under the remit of a large organization will be a new and often scary\nexperience for most executives. Not knowing where competition may come from, because it may not be upfront and\nvisible will also require a constant revisiting of strategy. When competition comes head on, or at least from the side\n198 or from behind, it can be seen, and dealt with, even if slowly. When competition has the potential to come from a\ncomputer in the bedroom of a seventeen-year-old in another country, life becomes less predictable.\nMany managers may take cold comfort from an identification of the five new forces, and what they will do to the\nbusiness environment. They are not neat and structured, like Porter\u2019s 5 Forces, nor do they seem to suggest much in\nterms of strategic direction, as do popular analysis tools such as the Boston Consulting Group grid. Much of the\nrecent writing on strategy emphasizes the effects of these forces however, and suggests that conventional\napproaches to strategy will at least be insufficient, if not ineffective, for coping with corporate survival. A number of\nthese authors (cf. 4; 9; 10; 11) offer perspectives that are worth considering. While, as would be expected, there is no\nabsolute concurrence on their advice for strategy in the future, these authors do tend to agree on certain\nfundamentals.\nIn closing, it is worth summarizing some of these basics. First, change is too rapid for anyone anywhere to feel\ncomfortable\u2014success has an anesthetizing effect that becomes its own enemy. Second, it may be a good idea to\ncontinually seek ways of destroying one\u2019s own firm\u2019s value chain and putting oneself out of business\u2014if one does\nnot, someone else will in any case. Third, resources are increasingly less about tangible assets and more about\nknowledge and the ability to constantly innovate. Fourth, firms should constantly find and exploit ways to give the\ncustomer as much of an opportunity to do as much of the work as possible. Technology offers great opportunities in\nthis regard. Strangely (and as we pointed out in the first chapter), customers do not want more service, they want\nless. They want control, and the power to solve their own problems, and victory will go to those players who find\nways for them to do this well12. Finally, strategy is no longer long-term, as the half-life of ideas diminishes. The\nfive-year plan or the long-term strategy is no longer viable, and the value of the annual strategic planning session is\nto be questioned. Strategy becomes incremental, rather than planned. It is revisited and revised not annually or\neven bi-annually, but monthly, probably weekly, and possibly, daily. It\u2019s not that strategy should be thrown out with\nthe bath water. Rather, perhaps, managers should consider that the strategy needed for the 21st century might\nindeed be a new baby, born of five new forces in an age of convergence. And in an age of opportunity.\nFor many years\u2014indeed since Edison\u2019s invention of the phonograph in 1877\u2014the music recoding industry did\nnot change that much. While technological changes did come to the market for recorded music over the years, in\nthe form of improved recording techniques, hi-fidelity stereo, and the advent of the compact disk (CD), essentially\nthe industry remained stable, with its structure largely unaffected by technological developments. Recording\ncompanies found and recorded talent and marketed it, and the products of the industry\u2014essentially disks and\ncassette tapes\u2014were distributed through record stores. Artists were remunerated in the form of royalties, retailers\nin the form of margins, and the record companies kept the rest.\nThe fundamental distribution issue of assortment was (and in many ways still is) perhaps the most significant\ndilemma in the market for recorded music. The structure of the industry and the way the product was produced\nheld an inherent problem for both the retailer and the consumer. The retailer\u2019s predicament is that of inventory\u2014\nthe need to hold very large stocks of records, in order to provide a selection to customers, and to be able to make\navailable to the customer the one that they will choose when they want it. This means that a lot of capital is tied up\nin stock, much of which moves slowly and often needs to be discounted in order to meet working capital\nrequirements. The consumer is also in a quandary: will the particular retailer stock the one album that they are\nlooking for? And, will they be able to find it among the thousands of other items? Even once found, the consumer\u2019s\n199 problems are not over\u2014there may be 12 songs on the album, and they may only really want three or four. But they\nare forced to purchase the entire album, with all 12 songs.\nPlayers wishing to have a wager on a football game, or a flutter on a horse race, or to back up their opinions on a\npolitical election or the outcome of the Oscars, have until recently had few alternatives. In a majority of European\ncountries, Canada, and most states in the USA their only option would have been to place a bet on a pari-mutuel or\ntotalizator system, while in countries such as the United Kingdom,Australia and in US states such as Nevada, they\nwould also have access to licensed bookmakers. Both of these systems place the player at a significant disadvantage,\nnot least of which is the fact that the \u201crake-off\u201d or house percentage is considerable. This means that winners get\npaid well below the \u201ctrue\u201d odds against their choice. Furthermore, neither of the two systems allows a player to pick\na \u201closer\u201d\u2014the player can only stake on a winning outcome. In simple terms a player can\u2019t back team to lose\u2014they\ncan only back the other team to win or on a draw. A specific disadvantage of pari-mutuel systems is that subsequent\nweight of money for a player\u2019s choice will reduce the payoff, so that there is no opportunity to exercise any skill in\nthe timing of a bet. Both systems profit not from the losers (as most inexpert gamblers believe) but from winners,\nby paying them out at less than true odds. In the case of pari-mutuels the house percentage is around 20 per cent,\nand even the most generous bookmakers make books that have an edge of around 14 per cent in their favor.\nBetfair (www.betfair.com) which commenced trading in 2002 is the world\u2019s largest betting exchange. As a\nbusiness, Betfair has no interest in the outcome of any event it makes available to gamblers. It simply provides a\nmarket for opinions and for trades to take place. It requires players to make and\/or take fixed odds and all income\nis derived from a small percentage commission (ranging between two and five per cent, depending on a player\u2019s\nturnover) on a player\u2019s net winnings on an event. In general terms, the greater an event\u2019s turnover, the more\nrevenue and profit Betfair generates, although Betfair\u2019s income is strictly a function of the total net winnings on an\nevent, not turnover.\nExercises\n1. Explain in your own words what is meant by a \u201ckiller app\u201d, and find some examples not discussed in the\ntext.\n2. Identify five more examples of the effects of Moore\u2019s Law in products or services not already discussed in\nthe text.\n3. What is mean by \u201ctransaction costs\u201d\u2014explain these by using practical examples from an existing\norganization.\n4. It has been said by many observers that Google is the most important new firm for the new millennium. Is\nGoogle an example of the new five forces at work? Explain how the forces apply to Google.\n5. Do a \u201cnew five forces\u201d analysis for the organization you work in or one that you are very familiar with. List\nways in which the forces apply to this organization, and how they represent either opportunities or\nthreats.\nEditor\nLeyland Pitt\n200 References\nDickson, P. R. (1992) Toward a General Theory of Competitive Rationality, Journal of Marketing, 56, 1, 69\u2014\n84\nPorter, M. E. (1998) Competitive Advantage: Creating and Sustaining Superior Performance, New York, NY:\nFree Press\nPorter, M.E. (1980) Competitive Strategy: Techniques for Analyzing Industries and Competitors. New York,\nNY: Free Press.\nDownes, L., and Mui, C. (1998) Unleashing the Killer App, Boston MA: Harvard Business School Press\nCoase, R.H. (1937) The Nature of the Firm, Economica, 4, 386\u2013405\n\u201cThe Accidental Superhighway\u201d, (1995) The Economist, July 1, special supplement\nFor a full description see Pitt, L.F. (2001) \u201cTotal E-clipse: Five New Forces for Strategy in the Digital Age\u201d,\nJournal of General Management, 26, 4 (Summer), 1- 15\nFor a full description see Davies, M., Pitt, L.F., Shapiro, D., and Watson, R.T. (2005) Betfair.Com: Five\nTechnology Forces Revolutionize Worldwide Wagering, European Management Journal, 23, 5 (October),\n533-541\nKelly, K. (1998) New Rules for the New Economy: 10 Radical Strategies for a Connected World, London, UK:\nFourth Estate\nShapiro, C. and Varian, H. R. (1998) Information Rules: A Strategic Guide to the Network Economy, Boston\nMA: Harvard Business School Press\nSchwartz, E. (Lauren Marino, ed.) (1999) Digital Darwinism : Seven Breakthrough Business Strategies for\nSurviving in the Cutthroat Web Economy, New York, NY: Broadway Books\nBerthon, P.R., Pitt, L.F., Katsikeas, C., and Berthon, J-P. (1999) \u201cVirtual Services Go International:\nInternational Services in the Marketspace\u201d, Journal of International Marketing, 7, 3, 84-105\n201 11. Opportunities in\nbusiness to business systems\nEditors: Maha Shakir (Zayed University, United Arab Emirates) and Guy Higgins (New York Institute of\nTechnology-UAE Campus, United Arab Emirates)\nLearning objectives\n\u2022 describe what a B2B system is and why such systems are important in the modern economy\n\u2022 define the concept of integration in terms of information systems and discuss why it is important to the\nmodern business organization\n\u2022 describe what an ERP (or enterprise) system is and why such systems are important in the modern economy\n\u2022 define the concept of sSupply cChain in terms of external integration and discuss why it is important to the\nmodern business organization\n\u2022 describe several technologies (EDI, E- Marketplaces, Web Services) in current use in B2B systems\n\u2022 discuss both the opportunities arising from the adoption of B2B systems and the challenges to their\nimplementation\nIntroduction\nInformation systems provide many opportunities for businesses to improve the efficiency and effectiveness of\ntheir operations through the integration of business systems. While the majority of information systems in the past\nwere focused on applications within the boundaries of the enterprise, the focus is gradually shifting outwards.\nBusiness-to-business (B2B) systems are a part of those systems that are applied to relationships outside of the\nboundaries of the enterprise.\nOngoing innovations in Web technologies are making the integration of business systems across companies (i.e.\noutside the boundaries of an individual company but forming its supply chain) technically possible and financially\nfeasible. Managing the B2B aspects of these supply chain relationships is creating substantial opportunities, both in\nthe streamlining of operations and in the development of new and innovative business delivery mechanisms. These\nopportunities are the focus of this chapter.\nThis chapter explores the importance of system integration (both internally and across the supply chain),\nprovides a brief history of B2B systems, introduces the types of information technologies enabling these systems,\nand identifies the challenges to their adoption. New business models that are enabled utilizing these technologies\nare also discussed.\nWhat is integration and why is it important?\nAlthough this chapter is about B2B systems, throughout the chapter we will be discussing integration. But what\nis integration and why is it so important? Integration simply means causing or allowing things to work together\n202 cooperatively. Any business organization consists of a set of diverse people and systems, but the underlying premise\nof that organization is that all of its diverse elements will work smoothly together in order to meet its organizational\ngoals in an efficient and effective manner.\nIf you consider a very small company, say a company consisting of only one person, then all of the functions of\nthat company are integrated because that one person performs all of the needed functions of the company. Refer to\nThirkettle and Murch [1] for an insight into the challenges pertaining to managing information systems in a\noneperson company. There a many examples of one person companies that are very successful because that one\nperson is good at doing whatever it is that the company does. Unfortunately, few people can do everything needed\nby a company well. Even if they could, the quantity of output of a small company is necessarily limited. One of the\ntenets of the modern organization is that, as the organization grows in size, its members tend to specialize in\nspecific functions. While this specialization allows each of those members to do their specific jobs better (as they\nare more focused on their individual tasks), it also decomposes what was originally a set of integrated tasks into a\nchain of separate, but related, functions. Thus, while functional specialization (i.e. decomposition) allows the\nvarious components of an organization to act more effectively, it increases the possibility of friction between those\ncomponents. This friction would tend to reduce the efficiency of the organization. Making this chain of separate,\nbut related, functions work cooperatively (or to reduce their friction) is the function of the executive management of\nthe company. Finding ways to enable the people in an organization to cooperatively work together is a part of the\nstudy of Management. Finding ways to cause the information systems of an organization to cooperatively work\ntogether is a part of the study of Management Information Systems, or the topic of this book.\nUnfortunately, the information systems of most organizations are not a set of programs that work together\ncooperatively; in other words, these systems are NOT integrated. Instead, in most organizations they are a\ncollection of disparate systems which were individually developed over time to meet specific demands within the\norganization. This happened for many reasons. Certainly it is easier to develop an information system intended to\nprovide for a limited number of demands rather than to provide for all of the needs of a company. Additionally, in\nthe early days of IT the available technology was simply not robust enough to attempt what we now call integrated\nsystems. Today, we call this collection of disparate systems, which are found in most companies, legacy systems.\nThese systems are our legacy from those systems developers that went before us in the organization. Typically,\nthese legacy systems do the jobs for which they were designed extremely well; after all, they have been used, tested,\nand improved over the years. Legacy systems were typically designed to support only one business function and are\noften called functional systems as they were not intended to be integrated with other systems within the\norganization. Any needed integration would be provided by people taking the output of one system and adding it as\ninput to another system. For example, the accounting department usually has one or even a collection of legacy\nsystems which support accounting operations. However, the same information would not necessarily automatically\nflow through this set of systems that support the accounting operations.\nThe problem with legacy systems is not with the jobs that they do, it is with their inability to cooperate with each\nother\u2014they are not integrated. The lack of integrated systems requires that people work harder to provide the\nintegration. They must manually enter one system\u2019s output as an input to another. This is the friction that we\ndiscussed earlier and it creates inefficiencies within the overall set of business systems of the organization. It also\nmeans that the reports that these systems generate only provide a snapshot of the results for the specific function(s)\nthat the system was designed to support. This makes it extremely difficult to have up-to-date information about the\n203 overall status of the organization. Having this up-to-date information, or in practical terms integrating the\norganization's information systems, has been the holy grail of executive managers and systems professionals for\nmany years.\nERP systems: the means of internal integration\nIntegrating key business functions so that information can flow freely between them has become the job of a\ntype of software application known as an enterprise resource planning (ERP) system or an enterprise system. Since\nthe mid- to late-1990s, many organizations have been replacing their disparate legacy systems with these\norganization-wide ERP systems. Although originally focused on the internal functions of an organization, ERP\nsystems are typically considered the most feasible solution for integrating business information systems within the\norganization. Therefore, a basic understanding of ERP systems is helpful in understanding the similar integration\nissues that organizations face nowadays in their pursuit of external business relationships.\nAn ERP is a packaged software application which includes a collection of software modules that are designed\nusing best-practice business processes [2]. Each module may replace one, or even many, legacy systems in the\ncompany. The module meets the same demands met by the legacy systems that they replace, but they are also\ndesigned to integrate with the other modules with the ERP. An ERP is primarily responsible for managing the\ntransactional processing operations of the business in its various areas (e.g. accounting, finance, human resources,\nmarketing, manufacturing, logistics, sales, procurements, etc.). Each of these functional areas is usually supported\nby a software module (or modules) that is designed to integrate with the other modules; these packaged modules\nthen need to be configured, and sometimes customized to meet the specific demands of the company implementing\nthe ERP. ERP systems are available from vendors such as SAP, Oracle, and Microsoft.\nOrganizations have different drivers for implementing an ERP [3]. These drivers mostly fall into two main\ncategories. The first is concerned with solving those existing business problems caused by inadequate IT\ninfrastructure and disparate information systems (i.e. integrating existing operations). However, the second driver\nis related to improving future business operations. This could include support for future business flexibility and\ngrowth, reducing operational costs, supporting customer responsiveness, improving data visibility, and making\nbetter business decisions.\nWhen implemented, an ERP often replaces the many existing legacy systems within the organization. Due to the\nintegrated nature of ERP, its modules overcome many of the drawbacks of legacy systems and enable online\nintegration not only within the same function but within and across the other functions of the business. As a result,\nERP systems are considered good candidates for forming a technology platform to support the integration of other\nintra- and inter-organizational applications such as supply chain management (SCM), customer relationship\nmanagement (CRM), and e-commerce (refer to Exhibit 60).\n204 Exhibit 60: ERP as a platform for business applications (Adapted from Broadbent\nand Weill [4])\nIn the early-2000s, the high-end of the ERP market became saturated because most large organizations had\nalready implemented an ERP. In response to the increased competition in the ERP applications market, ERP\napplication vendors started including other applications as part of their ERP offerings. In order to achieve this, ES\nvendors built the new functionalities in-house, and\/or acquired, or made partnerships with, specialized enterprise\napplication vendors. ERP systems are gradually evolving to become inter-organizational and Internet-enabled. New\nmodules are added to the product portfolio, such as supply chain management (SCM), customer relationship\nmanagement (CRM), data warehousing, and business intelligence.\nThe supply chain: the focus of external integration\nEvery business is made up of a complex set of relationships between the business, as a producer of goods or\nservices, and its suppliers and customers. This set of relationships is called a supply chain (refer to Exhibit 61).\nExhibit 61: Supply Chain\nIt is from within its supply chain that a business must first be able to procure its raw materials and other\nnecessary means of production from its suppliers. This requires that the business be able to communicate what it\nneeds to those suppliers. Suppliers who have the materials that meet those needs must then offer them to the\nbusiness. Then, following its evaluation of the available offers, the business must actually purchase the materials\nfrom its preferred supplier. This communication of needs, offers, and purchases results in much paperwork\u2014\nrequests for quotation (RFQs), quotations, purchase orders (POs), etc. The purchased materials must then be\ndelivered to the business' place of production. Hence, more paperwork is generated by the actual delivery of the\n205 purchased material\u2014bills of lading, receipt documentation, invoices, and finally payment. All of this paperwork is\nboth time consuming and expensive to all of the organizations involved.\nThe other end of a business' supply chain shows its relationships with its customers to whom it sells its products.\nOnce the goods of the business have been produced, yet another set of paperwork is generated with sales orders\nfrom the customer, sales receipts to the customer, shipping documents and invoices to the customer, and finally,\npayment from the customer. There is additional expense in generating all of this paperwork, but paperwork is just\nanother one of the costs of the production of the goods or services of the business.\nSuccessful businesses generate wealth by identifying opportunities to produce desired goods and\/or services at\nprices that will make it possible to sell those goods and\/or services to enough customers to produce a profit. One\nway that businesses can increase their profits is to reduce the costs of production of their goods and\/or services.\nThus, businesses must constantly monitor and, when possible, improve their internal processes in order to identify\nopportunities to reduce their costs of production. This is the application of microeconomic, or transaction cost\ntheory, analysis to the operations of the business. The application of transaction cost theory to reduce production\ncosts was the impetus for first automation and, more recently, the implementation of ERP systems. But a business\nmust also monitor and improve its external relationships, in both directions, within its supply chain. B2B systems\ninclude those efforts to monitor and improve the firm\u00eds external relationships and to integrate the processes\ninvolved in supporting these relationships.\nHistory of B2B systems\nThroughout most of our history, cities have been the generators of opportunity and wealth. In her seminal work\nof 1984, Cities and the Wealth of Nations, Jane Jacobs argued that \u201ceconomic life depends on city economies;\u201d\nbecause, wherever economic life is developing, the very process itself creates cities'[5]. Thus, economic activity\nrequires the bringing together of the producers of goods and\/or services, their suppliers, and their customers.\nHistorically these groups are brought together in physical proximity\u2014this is called agglomeration. This\nagglomerative process resulted in the development of cities throughout the world. This was true because with the\nlimited communications and transportation systems of earlier times, businesses generally needed to be physically\nnear to either their suppliers or their customers, and preferably near to both. But the last quarter of the twentieth\ncentury saw the decline of the industrial output of many of the world\u2019s great cities and the transfer of this\nproduction to other parts of the world. This was possible as improvements in transportation and communication\nsystems began to counter-balance the need to for businesses to be co-located in cities. However, the trend towards\ngreater separation between suppliers, producers, and customers (also called globalization) has increased the\nvolume of paperwork, and its attendant expense, needed to produce and sell goods and services. If you are\nimporting raw materials from one country, producing your goods in another country, and selling your product\naround the world, you can no longer simply walk across the street to talk to your supplier about what you need or to\ndeliver your product to your customer (refer to Exhibit 62).\n206 Exhibit 62: Global supply chain\nTraditionally, the method of exchanging data between members of a supply chain has been through the use of\npaper-based systems. As discussed, paper-based systems are inefficient because they are both slow and expensive to\nmaintain. B2B systems were first used to replace these inefficient paper-based systems with the electronically\nexchanged data. This was much faster, and because it involved less manual processing of that data, it was also much\nmore cost-effective. Seen from an economic perspective, these early B2B systems improved a company's processes\nfor ordering supplies and paying for the ordered supplies\u2014this is a simple application of Transaction cost theory,\nwhere the efficiency of the individual transactions are improved in order to reduce the overall cost of production.\nBut, in addition to efficiency improvements, modern B2B systems also provide increased opportunities for finding\neven better suppliers and for reaching out to more customers. Thus, modern B2B systems are also improving the\neffectiveness of a firm's supply chain.\nThe earliest B2B systems used proprietary systems which, while an improvement over the paper-based systems\nthat they replaced, were still relatively expensive to operate. The advent of the Internet has led to new and improved\ntechnologies for B2B commerce that are less expensive and easier to implement. Thus, the current rapid growth in\nInternet-based B2B commerce is based on the twin facts that electronic exchange of data is more efficient than the\ntraditional paper-based exchange and that Internet-based technologies are less expensive and simpler to\nimplement. The arrival of Web services appears to promise an even closer integration of the corporate systems that\nrun modern business organizations. This may call for new models of cooperation between members of a supply\nchain.\nWhat is a B2B system?\nAs a part of the trend towards a more global economy, the final decades of the twentieth century saw the birth of\nelectronic commerce (e-commerce). E-commerce, the buying and selling of goods and services over electronic\nchannels, has become an exploding phenomenon. E-commerce can be divided into three major categories:\n1. B2C\u2014Business to Consumer: those aspects of e-commerce that involve Internet sales by businesses to\nconsumers.\n2. B2B\u2014Business to Business: those aspects of e-commerce that involve the exchange of goods or services\nbetween companies over the Internet.\n207 3. C2C\u2014Consumer to Consumer: those aspects of e-commerce that involve Internet sales by consumers to\nconsumers.\nThe B2B category involves the exchange of goods or services between companies with the receiving company\nintending to use the received good or service in the furtherance of its own production processes (not the final\nconsumption of that good or service). Still, one must remember that all businesses are both the suppliers and\ncustomers of other businesses. B2B is thus involved in ALL aspects of the supply chain. Therefore, B2B systems are\na part of the efforts of a company to monitor and improve its external relationships with the other businesses in its\nsupply chain.\nBut e-commerce is not only about buying and selling in the global marketplace, it is about being aware of global\nopportunities and reducing the production and sales cost of the business. You may be more familiar with the B2C\ntype of business, as you may have personally purchased something over the Internet. However, by far the greatest\nvolume of e-commerce is found in B2B transactions. A recent study by the US Department of Commerce found that\nsales via e-commerce had a reported growth of 24.6 per cent for the year 2005 versus 2004 [6]. Another recent\nreport from Forrester Research projects that the European Union's online B2B transactions will surge from the\n2001 figure of 77 billion to 2.2 trillion in 2006\u2014increasing from less than 1 per cent of total business trade to 22 per\ncent [7].\nThis is an information systems book; hence the B2B systems discussed in this chapter involve the combination\nof information technologies and the business processes that support the exchange of goods, services, information,\nor money between organizations. However, we must acknowledge those B2B systems that have existed long before\ncomputers or the Internet were invented. Such systems include those that were not originally technology-based,\nsuch as the postal system, the banking system, the accounting system, the legal system, the taxation system, etc. In\ntheir own times, these systems were considered exemplars of innovative means to facilitate the exchange of both\ninformation and goods between business entities. Nowadays, all of these systems utilize technology to a great\nextent. Other technology based-systems that fulfilled similar roles in the recent past include the telegraph,\ntelephone, and television systems. Taking this perspective, we can say that B2B systems are not new but have\nevolved from the older manual or paper-based systems of the past. The main difference however is that information\nand communication technologies (ICT) are now key components of B2B systems. As a result, ICT have both\naccelerated the business processes they are supporting and reduced their costs.\nB2B technologies\nIn this section we discuss three types of technologies that play an important role in B2B systems. Two of these,\nEDI and e-marketplaces, support inter-organizational integration. The third, Web services, is a new flexible and\nplatform-independent method for integrating information systems. Similar to ERP systems, Web services can be\nused for both intra- and inter-organizational integration.\nEDI\nThe history of electronic B2B systems starts with the electronic data interchange (EDI) systems which were first\ndeveloped in the mid 1960s. Prior to the advent of EDI, when one company generated paperwork to be sent to\nanother company, the receiving company would be required to retype that paperwork into the format that they\nwanted for their computer-based systems (refer to Exhibit 63). This would occur even if both the producing and the\n208 receiving companies were using computer-based systems, because the means of transmitting of data between them\nwas still that piece of paper that had been used for thousands of years for this purpose.\nExhibit 63: Traditional paper based systems\nAn EDI system facilitates a computer-to-computer exchange of data according to predefined standards. Thus,\nboth the company generating the data and the company receiving the data must agree in advance as to the exact\nelectronic format that the transmitted data will take. Only then, an electronic file can be exchanged between the two\ncompanies instead of paper. This type of data exchange greatly reduces the human labor, and therefore the cost, of\nexchanging data between companies. The typical use for EDI is in the procurement of goods and services. Most\nlikely the exchange is between a big company and its suppliers (refer to Exhibit 64).\nThe advantage of EDI systems is twofold, efficiency and effectiveness. The first is associated with automation. As\ndiscussed elsewhere in this book, efficiency is a result of doing more with less. Electronic communications between\nthe buyer and suppliers result in the elimination of paper-based documents and all the manual processes involved\nin handling, verifying, entering, sending, receiving, and recording these transactions, plus the reduction in time for\ncompleting many of the procurement processes involved. Additionally, accurate and timely information helps EDI\npartners improve the effectiveness of their collaborative business relationships, particularly when business\nprocesses are redesigned.\n209 Exhibit 64: Traditional paper based systems\nThe main obstacle for implementing EDI is the setup cost required for both EDI technology and the changes\nrequired in business processes to utilize this technology effectively. These setup costs can be substantial. For this\nreason, it is more affordable for big businesses that have both the financial resources and IT expertise to facilitate\nits adoption. Nevertheless, there is little benefit to any business in using EDI if the majority of its small-and-\nmedium supply chain partners are reluctant to invest in EDI. Consequently, the big business that is still committed\nto EDI may have to use a combination of strategies to encourage its adoption among suppliers[8, 9]. These include\n(a) providing financial subsidies, (b) providing technological assistance, and (c) enforcing mandatory use as a\ncondition for doing business.\nEven now when the Internet has become a universal platform that facilitates anytime-anywhere communication,\nthe majority of e-commerce transactions are still conducted using EDI. Considering that big businesses are typically\nthe initiators and key users of this technology, the significant number of transactions they generate may be\nresponsible for such a result. This is expected to change in the future when small businesses become comfortable\nand more technologically capable with using e-commerce tools, particularly with the introduction of Internet-based\ntechnologies such as Web services and service oriented architecture (SOA).\nE-marketplaces\nThe rise of the Internet in the late 1990s provided businesses with an easier means of providing for the\nelectronic exchange of information pertaining to the procurement of goods or services between companies within\ntheir supply chains. By then, many organizations had already made substantial investments in ERP systems, and\nfound that they are not using these systems to their full potential simply because ERP systems did not support\nexternal integration. By then, a substantial amount of business operations involved interacting with members of the\nsupply chain outside of one\u00eds own organization, particularly with suppliers. After establishing integration of their\ninternal systems, organizations started exploring alternatives for inter-organizational integrations, and hence e-\nmarketplaces came into existence. An e-marketplace is an Internet-based digital marketplace that provides a\ncentralized point of access for many different buyers and sellers.\nMany businesses already had an experience with digital marketplaces in the past through their EDI systems. The\ndifference however is in accessibility. Internet-based technologies have made it possible for anyone, anywhere, and\nat anytime to communicate. In contrast, the older EDI technologies were proprietary, difficult to implement, and\n210 not accessible to those outside of the closed network. The lowered adoption cost and reduced system complexity\nprovided by these Internet-based technologies are quite attractive, particularly to small-and-medium size\nbusinesses. Since the value of these inter-organizational networks increases when more participants are added,\nmany businesses started capitalizing on the idea of these linkages through the creation of e-marketplaces.\nE-marketplaces can be classified according to ownership (private, shared, or public), industry (vertical vs\nhorizontal), trading mechanism (catalogue, auction, or exchange) or service type (transactional vs collaborative)\n[10]. Transaction services support procurement activities where: (a) product specifications are explicit, (b)\npurchases are high-volume, (c) demand can be easily consolidated, and (d) common quality standards can easily be\ndefined [11]. Many e-marketplaces, especially these that use catalogues, fall into these categories. Collaborative\nservices support procurement activities on the other side of the continuum; that is where (a) product specifications\nare tacit, (b) purchases are low-volume, (c) demand is fragmented, and (d) there are sensitive quality requirements.\nAccording to ownership, e-marketplaces can be classified as:\n\u2022 Private\u2014 To establish own marketplace. This option is similar to the EDI model and is dually the most\nexpensive and the highest risk. An example is SeaPort (http:\/\/www.seaport.navy.mil), which supports the\ncontracting for professional support services for the US navy. Most often this type of marketplace is also\nclassified as both vertical industry and relationship-oriented.\n\u2022 Shared\u2014To establish a trading consortium often with organizations in the same line of business (i.e.\nvertical industry). This latter option involves collaboration with others. Surprisingly, because it involves\ncreating a network of organizations with similar interests, many members are existing competitors. An\nexample of the trading consortium marketplace is Covisint (http:\/\/www.covisint.com) which is an\nautomotive marketplace that was established in 2000. Covisint has since diversified into the healthcare\nindustry. A shared marketplace serving organizations in different industries is called a horizontal\nmarketplace. Often the marketplace supports the spot buying and selling of indirect goods or service (e.g.\noffice supplies) and is transaction-oriented. An example of which is Tejari (http:\/\/www.tejari.com).\n\u2022 Public (managed by an intermediary)\u2014To join an independent marketplace. This is the low risk, least\ncapital intensive option, particularly in the short term. The main difficulty however is to balance the needs\nof the diverse population of trading partners. An example of this is Ariba (http:\/\/www.ariba.com), which\noffers procurement solutions to different vertical industries.\nMany of the advantages of EDI systems also apply to e-marketplaces. However, one key differentiator is that\nthere is more than one way for an e-marketplace to be put together. The options discussed above provide several\navenues for companies considering using Internet technologies for inter-organizational integration. As with the\nearlier EDI technologies, having a substantial number of participants is frequently the key to the sustainability of an\ne-marketplace. So, big businesses are still often the initiators for such B2B participation as they can be more easily\nsuccessful in securing the necessary numbers of participants. As a result, e-marketplace setups still seem to favor\nvalue creation for the big over small- and medium-size businesses. For this reason, value creation is likely to be\nbiased to the powerful side of the relationship (i.e. the big buyer or the big supplier). One new development in Web\ntechnologies, Web services, is empowering small- and medium-size businesses to take the lead in initiating B2B\nrelationships and hence creating more value for the networks they form. Refer to Ray and Ray [12] for an insight\ninto the value of using Web services to conduct B2B ecommerce by small-and-medium size businesses.\n211 Web services\nThe explosion of the popularity of the Internet in recent years has greatly increased the demand for better means\nof electronic connection in the whole world of computers. As you now know, B2B is about using electronic means to\nconnect businesses in order that they become more efficient and effective in the pursuit of their individual\ncorporate goals. Moreover, with almost forty years of experience in electronic connections, users of B2B systems\nhave become increasingly sophisticated in their expectations of these systems. These factors combine to form an\nimperative to connect people, information, and processes, and this imperative is changing the way that software in\nall areas is being developed.\nSuccessful business systems increasingly require interoperability across platforms and a range of flexible\nservices that can easily evolve over time. XML appears to be becoming the universal standard for representing and\ntransmitting structured data that is independent of programming language, software platform, and hardware. The\nterm Web services refers to new methods for integrating programs through standardized XML technologies with\nopen, standardized interfaces that mask applications program interfaces (APIs).\nWeb services are based on four emerging Internet standards:\n\u2022 XML schema\n\u2022 Simple Object Access Protocol (Soap)\n\u2022 Web Services Definition Language (WSDL)\n\u2022 Universal Description, Discovery and Integration (UDDI)\nEach of these emergent standards forms an important foundation piece for the provision of Web services. The\nXML schema provides the common syntax for representing data. The Simple Object Access Protocol (SOAP)\nprovides the semantics for data exchange. The Web Services Description Language (WSDL) provides a mechanism\nto describe the capabilities of a Web service. Universal Description, Discovery and Integration (UDDI) is an XML-\nbased registry that allows businesses worldwide to list themselves on the Internet. Registering allows them to\nstreamline their online transactions by enabling other businesses to find them on the Web. But more importantly,\nusing Web services should make the various company systems interoperable for e-commerce. UDDI is often\ncompared to a telephone book's \u201cyellow pages\u201d; it allows businesses to list themselves by name, product, location,\nor the Web services they offer.\nWeb services enable computer systems on any platform to communicate over corporate intranets, extranets, and\nacross the Internet. Therefore they have the potential to become the next major step in the ongoing evolution of the\nInternet. As such, Web services have the potential to redefine the world of B2B technologies. With the deployment\nof a platform using Web services, companies can have more opportunities to identify and communicate with\nexisting and potential members of their supply chain, either suppliers or customers. Unlike earlier e-commerce\ntechnologies, the use of Web services allows for complete platform independence. Platform independence helps to\nreduce the integration problems between different corporate systems that have been the bane of B2B and other\nsystems developers for the past forty years.\nChallenges for B2B adoption\nThere are many challenges to the adoption of B2B systems regardless of the particular technology to be\nimplemented; these can be categorized into [11, 13, 14]:\n212 (a) economic feasibility challenges, (b) business environment challenges, (c) buyer challenges, and (d) supplier\nchallenges.\nEconomic feasibility\nThe issues involving the economic feasibility of B2B adoption can be categorized as: (a) higher setup costs, (b)\nunsustainable growth, (c) unconvincing pricing, and (d) not enough profit.\nHigh setup costs\nOften the setup of a B2B system is an expensive endeavor. Many of the earlier B2B systems (e.g. the EDI\ntechnologies) were proprietary systems, meaning that they were custom-built, and thus there were higher upfront\ninvestment costs for each participating member in the EDI system. Telecommunications costs were also significant,\nparticularly because many of these systems used dedicated communication lines. Total setup costs included\ntechnology systems setup as well as the cost of change in internal business processes and the education and training\nrequired to put these systems into operations. The cost barrier has been reduced significantly with the arrival of the\nInternet, however, cost remains an issue for many organizations. New models for B2B system delivery promise to\nbe significantly cheaper as many Internet-enabled B2B systems are not technology dependant. This means that any\nbusiness can implement the B2B solution irrespective of the type of IT system the business and its trading partners\nare using.\nUnsustainable growth\nWhile many B2B systems were profitable when they started, many failed to maintain growth for reasons such as:\n\u2022 One or more members decided that the B2B technology was not suitable for themselves either because of\ncost or fit to business concerns.\n\u2022 The B2B technology did not deliver what it promised.\n\u2022 It was difficult to foster a collaborative spirit for doing business in a highly competitive market.\nUnconvincing pricing\nPricing models for earlier B2B systems did not have a direct relationship with the value these systems delivered.\nMany B2B delivery modes were new so pricing was experimental. Many small- and medium- size business\nparticipants felt disadvantaged having to pay a substantial upfront setup cost and\/or a high annual subscription\ncost. More recently, transaction-based pricing has become more popular as members pay according to volume of\ntransactions they generate. However, this may only be possible when there is substantial volume to accommodate\nB2B system profitability.\nNot enough profit\nThis is of particular significance to the B2B solution provider since not generating enough profit means going\nout of business. The cause for this can be any\/or a combination of the challenges mentioned here.\nBusiness environment\nThe issues involving the business environment for B2B adoption can be categorized as: (a) ineffective public\ninfrastructure, and (b) restrictive regulations.\nIneffective public infrastructure\nIn many countries, particularly in the developing world, the public IT infrastructure is not well established.\nProblems included breakdown in service due to disruptions in telecommunications and electricity blackouts. One\n213 means of resolving such problems can be in the ability of the B2B solution provider to identify these problems early\non and to provide an alternative means to accomplish the task on hold. For example, an extension of time is granted\nto all bidders if a breakdown in service is identified at the time of bids closing. This extension might be made using\nother telecommunications means such as text messaging in order to reduce bidder anxiety.\nRestrictive regulations from domestic governments\nIn many countries, the rules governing business communications and contracts using electronic means are yet\nto develop. This is a major obstacle that hinders B2B interaction because participants would have no protection if\nanything went wrong. Workarounds for this issue could be to get the B2B service endorsed by the government or to\nenroll a major governmental organization as one of the key participants.\nBuyer Issues\nThe buyer issues involved in a B2B adoption can be categorized as: (a) organizational inertia, (b) buyer's fear\nabout the capability of the B2B solution provider, and (c) buyer's fear about the integrity of the B2B solution\nprovider.\nOrganizational inertia\nEmployees within the buyer organization may resist the move to B2B system for reasons such as: limited\nunderstanding of what the systems can so, inability to use the technology involved, or fear of exposing dishonest\npractice (particularly in procurement). This issue can be reduced through informing the buyer's community,\nproviding education\/training, and instilling rigorous ethical guidelines.\nBuyer's fear about the capability of the B2B solution provider\nMany of B2B solutions are new. As a result, many have undergone incomplete testing, or they have never been\ntested in a real-work environment. This issue is especially important when the buyer is required to invest in the\nB2B market setup. One way to resolve this issue can be to tie financial commitment to outcomes or deliverables.\nThat is, the buyer pays for actual use and according to the services delivered.\nBuyer's fear about the integrity of the B2B solution provider\nMany of the interactions handled through B2B systems include sensitive data that is used to determine the\naward of contacts. The B2B solution provider needs to demonstrate compliance with various data management and\nsecurity standards so that the system can be considered both trustworthy and reliable.\nSupplier issues\nThe supplier issues involved in a B2B adoption can be categorized as:\n\u2022 supplier's fears of competitive bidding\n\u2022 few benefits to suppliers\nSupplier's fears of competitive bidding\nThis is one of the key issues affecting supplier\u2019s adoption. One of the key benefits for using an electronic B2B\nsystem is to get the best price for the buyer. This is sometimes achieved to the disadvantage of the supplier (i.e. the\nsupplier with the lowest bid wins). The effect of this issue can be softened when the buyer only invites qualified\nsuppliers and includes criteria other than the price as part of bid evaluation.\n214 Few benefits to suppliers\nMany suppliers are reluctant to commit to using B2B systems when these systems are complex, expensive and\npromise little or no benefit. It is then vital for the buyer to consider criteria as ease-of-use, cost, and benefit to\nvendors as key when evaluating electronic B2B systems.\nThis section touched upon some of the common challenges to the adoption of electronic B2B systems. It also\nprovided a few suggestions on how to deal with these challenges. The next section offers a few thoughts on the\nopportunities for the development of new business models that are enabled by the adoption of B2B systems. Some\nof these thoughts were developed with an understanding of the challenges discussed here.\nOpportunities: New business models enabled by B2B systems\nThe basic business model, the supply chain, is not modified by the integration of B2B systems into a company\u2019s\nexternal operations. However, the actual implementation of that supply chain model is significantly changing. By\nproviding increased opportunities for finding better suppliers and for reaching out to additional customers, B2B\nsystems (along with modern transportation systems) are reducing the former need of businesses to be located in\nphysical proximity to either its suppliers and\/or its customers (i.e. agglomeration). This is increasing the trend\ntowards globalization while providing for the more efficient and effective operation of businesses around the world.\nWhile a simple statement, this is a complex change in the way that businesses can operate. It has social and political\nimplications far beyond the profit and loss calculations that are typically used to evaluate changes in business\noperations. Will new business models develop as a result of these changes? Stay tuned more to come.\nFuture of B2B systems\nMany B2B systems were setup creating new, and sometimes, unconventional business models. These systems\nmodified the conventional way businesses interacted. Some of these interactions (e.g. information sharing, online\nbidding, and auctions) affected the competitive position of the business. Hence, many participants agreed to come\nonboard, but their participation was minimal. With few active participants, these business networks did not\nproduce the value they promised.\nThe trend towards more integrated systems, both intra- and inter-organization, is expected to not only continue,\nbut to increase in pace. This trend towards integration, particularly inter-organizational integration, is the central\nconcept of B2B systems. As more businesses accept the use of B2B systems, the users of these systems will become\neven more sophisticated in their expectations and the designers of these systems will provide better technologies to\nmeet these increased expectations. Will this result in new ways of operating within the supply chain of the\nbusiness? Certainly! Will this result in a new business model? Possibly! However, the supply chain model, although\nextremely simple, is very capable of representing any modification in the ways that business partners interact.\n215 Editor\nMaha Shakir\nReferences\nThirkettle, F. and R. Murch, Business of art. 2006, Ivey. Retrieved January, 2007, from\nhttp:\/\/cases.ivey.uwo.ca\/Cases\/Pages\/home.aspx?Mode=showproduct&prod=9B06E008|\nhttp:\/\/cases.ivey.uwo.ca\/Cases\/Pages\/home.aspx?Mode=showproduct&prod=9B06E008\n(http:\/\/cases.ivey.uwo.ca\/Cases\/Pages\/home.aspx?Mode=showproduct&prod=9B06E008)\nDavenport, T.H., Putting the Enterprise Into the Enterprise System. Harvard Business Review, 1998. 76(4):\np. 121-131.\nRoss, J.W., Surprising Facts about Implementing ERP, in IT Professional. 1999. p. 65-68.\nBroadbent, M. and P. Weill, Management by Maxim: How Business and IT Managers Can Create IT\nInfrastructures. Sloan Management Review, 1997: p. 77-92.\nJacobs, J., Cities and the Wealth of Nations: Principles of Economic Life. 1985, New York: Vintage Books\nUSA.\ncomScore E-Commerce Sales Data Accurately Predict U.S. Department of Commerce Estimates Weeks Ahead\nof Their Publication. 2006, PR Newswire. Retrieved January, 2007, from\nhttp:\/\/www.prnewswire.com\/cgi-bin\/stories.pl?ACCT=109&STORY=\/www\/story\/02-23-\n2006\/0004288403&EDATE=|http:\/\/www.prnewswire.com\/cgi-\nbin\/stories.pl?ACCT=109&STORY=\/www\/story\/02-23-2006\/0004288403&EDATE=\n(http:\/\/www.prnewswire.com\/cgi-bin\/stories.pl?ACCT=109&STORY=\/www\/story\/02-23-\n2006\/0004288403&EDATE=)\nGreenspan, R., EU B2B Expected to Explode. 2002. Retrieved January, 2007, from\nhttp:\/\/www.clickz.com\/showPage.html?page=1453831|\nhttp:\/\/www.clickz.com\/showPage.html?page=1453831\n(http:\/\/www.clickz.com\/showPage.html?page=1453831)\nIacovou, C.L., I. Benbasat, and A.S. Dexter, Electronic data interchange and small organizations: Adoption\nand impact of technology. MIS Quarterly, 1995. 19(4): p. 465-485.\nWang, E.T.G. and A. Seidmann, Electronic data interchange: competitive externalities and strategic\nimplementation policies. Management Science, 1995. 41(3): p. 401-418.\n216 Christiaanse, E. and M.L. Markus. Participation in Collaboration Electronic Marketplaces. in Hawaii\nInternational Conference on System Sciences (HICSS). 2003.\nHsiao, R.-L. and T.S.H. Teo, Delivering on the Promise of E-Procurement. MIS Quarterly Executive, 2005.\n4(3): p. 343-360.\nRay, A.W. and J.J. Ray, Strategic benefits to SMEs from third party web services: An action research analysis.\nJournal of Strategic Information Systems, 2006. 15(4): p. 273-291.\nDay, G.S., A.J. Fein, and G. Ruppersberger, Shakeouts in Digital Markets: Lessons from B2B\nexchanges.California Management Review, 2003. 45(2): p. 131-150.\nWise, R. and D. Morrison, Beyond the Exchange: The Future of B2B. Harvard Business Review, 2000. 78(6):\np. 86-96.\nGlossary\nBusiness-to-business (B2B): B2B refers to the transactions (purchases of equipment, supplies, services,\netc.) that occur between businesses as opposed to between businesses and\nconsumers.\nE-commerce: E-commerce is the buying and selling of goods and services over the\nInternet, especially over the World Wide Web.\nE-marketplace: The E-marketplace is the electronic community of buyers and suppliers that\nintegrates buyers\u2019 procurement systems with suppliers\u2019 fulfillment systems\nthereby creating a single standard process for transacting business\nelectronically.\nEnterprise resource planning (ERP): ERP systems (also called enterprise systems) are information systems that\nare intended to integrate all of an organization\u2019s data and processes into a\nsingle uniform information system.\nEnterprise system: Enterprise systems (also called ERP systems) are information systems that\nare intended to integrate all of an organization\u2019s data and processes into a\nsingle uniform information system.\nGlobalization: The concept of Globalization is the acknowledgment of the increasing\ninterconnectedness of people and places that were formerly seen as\nunconnected because of the physical distances between them. Their new\ninterconnectedness is a result of the recent advances in transportation,\ncommunication, and information technologies.\nIntegration: Organizations integrate the functions of many different departments in\norder to produce the output of the organization; however, in the past the\ninformation systems that served these different departments were unable to\nshare data between systems\u2014\u2014they were unintegrated. In IT the term\nIntegration refers to the trend of designing information systems that are\nable to share data across functional boundaries. See \u201cERP\u201d and \u201cEnterprise\nsystems\u201d.\nLegacy systems: A Legacy System is simply an older information system that has not been\n217 Business-to-business (B2B): B2B refers to the transactions (purchases of equipment, supplies, services,\netc.) that occur between businesses as opposed to between businesses and\nconsumers.\nreplaced by a newer system. Legacy systems are frequently unintegrated and\nthe trend towards integration is seeing many legacy systems replaced by\nmore integrated or even enterprise systems; however, most legacy systems\nactually perform the functions for which they were designed quite well, it is\nsimply that they were not designed to integrate with other systems.\nSupply chain: Supply Chain is a concept that sees the various organizations, people,\nresources, and information involved in bringing products to their ultimate\nconsumers as a continuous stream (or chain) where resources and\ninformation flow through to that final purchase. Supply chain sees all of the\ninvolved organizations as connected in a network whose purpose is to\ndeliver the final product.\nTransaction cost theory: Transaction Cost Theory is one of the earliest attempts to theoretically\ndefine the business organization in relation to the marketplace. Because an\norganization cannot control its interactions with the marketplace (leaving\ncontinual uncertainty as to costs), it attempts to minimize these interactions\nby bringing many otherwise market transactions inside the functions of the\norganization. Bringing these \u201ctransactions\u201d inside the firm rationalizes the\ncosts of the firm.\n218 12. Opportunities in peer-\nto-peer systems\nEditors: Janis L. Gogan and James L. Linderman (Bentley College, USA)\nIntroduction and overview\nMany highly effective managers have proven to be rather ineffective in predicting the diffusion and impacts of\nnew information technologies. For example, IBM founder Tom Watson, Sr. initially foresaw a very small market for\nmainframe computers, and Digital Equipment Corporation founder Ken Olsen famously remarked that personal\ncomputers were not likely to catch on, except with electronics hobbyists. More recently, one IT application that\ncaught many managers by surprise in the early years of the twenty-first century is peer-to-peer (or P2P) file\nsharing. Although the basic elements of P2P were invented back in the 1960\u2019s, a confluence of enabling technologies\nand applications caused P2P file sharing and other P2P Internet applications (including some in which \u201cP2P\u201d\nstands for \u201cperson-to-person\u201d rather than \u201cpeer-to-peer\u201d) grew rapidly in the late 1990\u2019s. In some domains P2P has\nnot had much impact, but in others \u2013especially file sharing in the music industry and person-to-person auctions in\nelectronic commerce\u2014P2P has had a tremendous impact and continues to generate great turmoil (Stainaker,\n2008).\nThis chapter is organized as follows:\nIn section 2 we place P2P in its historical context by describing how the evolution of computing, networking, and\nsoftware technologies led to P2P applications. By understanding the evolution of information technologies, you will\nbe better prepared to capitalize on as yet unforeseen capabilities and applications when they become available\nduring the course of your career.\nIn section 3 we explain how P2P file sharing works today. This section helps you understand how P2P differs\nfrom other approaches to making data and information products available to individual users. We then discuss the\nearly impacts of some specific P2P file-sharing applications on individuals, business organizations, and institutions\nsuch as governmental and educational organizations. We explore how P2P file-sharing has brought both intended\nand unintended benefits to individuals and organizations, while also ushering in significant technical, legal, and\nethical challenges to individuals and organizations.\nIn section 4 we examine other P2P Internet applications \u2013 such as person-to-person sales on eBay and person-\nto-person video creation and sharing on YouTube-- and compare them with their traditional counterparts for\ncommunication, commerce, and collaboration, in terms of advantages and disadvantages.\nLastly, in section 5 we discuss P2P in light of the Law of the Double-Edged Sword\u2014why every P2P application\nbrings both opportunities and challenges. Neither P2P nor any other existing or emerging information technology is\nimmune from this law. Effective managers and entrepreneurs recognize this, plan accordingly, and are ready to\nchange those plans as unforeseen opportunities or challenges arise.\n219 P2P in its historical context\nIn order to understand the recent impact of P2P applications, it is helpful to understand the evolution of\ncomputing and networking devices and data management and file sharing applications over the last several\ndecades. This section briefly reviews these topics.\nThe earliest computers (1940\u2019s and 1950\u2019s) were just that\u2014devices that performed numerical computations.\nComputations were strictly numeric; many years would have to pass before digitized data included audio or visual\ninformation. Forget sounds and colors, music and graphics, never mind pictures and video clips. Even\nalphanumeric characters were unusual and\u2014if present\u2014in a generic device-specific mono-spaced font. Many early\ncomputers had no keyboards, printing devices or alphanumeric display monitors, instead relying on binary input\ntoggles and binary output light arrays. These computers were designed to process numbers, provide small amounts\nof numeric results, and little else. By contrast, in today\u2019s world the numeric processing is confined largely to\nresearch, science and engineering applications, whereas the average computer user deals with almost every form of\ndata other than numbers. When computer manufacturers boast billions of arithmetic computations per second,\nmany users wonder why bother, since arithmetic is the last thing on their minds; they want music, graphics and\nvideo\u2014preferably in combination. Yet, today the binary number system remains the means of representing all data,\nincluding numbers, words, music, graphics and video. A great deal of computational power is essential to quickly\ninterpret, record and re-construct such data. Some computer tasks that we take for granted, such as re-sizing and\nre-locating monitor windows, demand prodigious amounts of numerical computations to accomplish. The take-\naway lesson here is that computers still compute, even when our inputs, files and outputs are non-numeric.\nFurthermore, the non-numeric applications that are in demand today were all but impossible just a few decades\nago.\nComputers were introduced into large business organizations in the 1950\u2019s, and the first applications automated\nsimple accounting tasks. At first, the transaction data associated with an application were linked directly to the\napplication that created it. Most businesses created a data processing organization to build and run these\napplications. Timesharing techniques made it possible for individual users to view data on computer screens\n(\u201cdumb terminals\u201d).\nBy the time the Internet (actually, ARPANET, the Internet\u2019s predecessor) came along in the sixties, data\nrepresentation had advanced beyond numbers to include alphanumeric text files. The storage representation for\nfiles had also come a long way from the hole-punched cards and paper tapes of early computers to magnetic tapes\nand disk drives. The original rationale for ARPANET was to enable long-distance transmission of files. ARPANET\u2019s\ndevelopers would have been astounded to think that one day such information interchange would (1) be available\nfor mass usage by non-technical citizenry and (2) be bi-directional and highly interactive in real time. Keep in mind\nthat digital file sharing involving the average individual was all but impossible (let alone affordable) only a few\ndecades ago.\nIn the sixties, the key Internet protocols \u2013 TCP and IP\u2014were developed to support communication among the\nUS armed forces, other governmental agencies, and a small number of defense contractors. TCP\/IP was designed so\nthat if an enemy were to destroy one or more network nodes, communication would still be possible (as a network-\nof-networks, the Internet does not have a central point of failure). A group of computer science professionals, the\nso-called Network Working Group, developed various networking tools. As this group worked, they recorded their\ndeliberations via memos called Requests for Comment (RFC). RFC 1, written by Stephen Crocker on April 7, 1969\n220 discussed approaches for transferring files from one computer to another over a network (see\nhttp:\/\/tools.ietf.org\/html\/rfc1).\nIn 1965 Intel co-founder Gordon Moore foresaw a long-term trend of dramatic improvements in the integrated\ncircuit (the foundation technology for computing devices). As predicted by \u201cMoore\u2019s Law,\u201d in the sixties and\nseventies there were dramatic price\/performance improvements in computer hardware. While business computers\nin the sixties were huge mainframes managed by a data processing (DP) organization and used for routine\ntransaction processing, by the seventies many companies used less expensive mini-computers for less routine\napplications in design, engineering, manufacturing, and marketing. Some of these applications processed a greater\nvariety of data types, including text, graphs, and some images. Many organizations invested in database\nmanagement systems, which enable data to be shared among many software applications. Management\nInformation Systems (MIS) departments were formed to oversee data administration and create various reports\nbased on the information stored in these databases. Despite receiving very fat stacks of paper reports, some\nmanagers complained that the particular information they needed was not quite reflected in them! Subsequently,\nsoftware was created to allow individual users to make queries of these databases and to generate their own reports.\nIn the seventies, File Transfer Protocol (FTP) was developed to make it easier for people to share files of various\nsizes containing various data types. An FTP server \u201clistens\u201d for requests coming from other computers. When such\na request is detected, the server then establishes a connection with the requesting computer and subsequently\nreceives a file and instructions on where the file is to be delivered. Then, in a process that is similar to how\nElectronic Data Interchange (EDI) works, the server forwards the file to the Internet address specified by the\nsender.\nThe microcomputer was also invented in the seventies, but it really burst on the business scene in the early\neighties (the IBM PC, introduced in August 1981, dominated the early days of the end-user computing revolution).\nThe first \u201ckiller application\u201d was an electronic spreadsheet (VisiCalc, which later lost ground to a product called\nLotus 1-2-3, which in term was supplanted by Microsoft Excel). To share spreadsheet or word processing files, a\nuser might save a file to a floppy disk and walk over to another user (via \u201csneakernet\u201d) who would insert it into their\ndisk drive. It did not take long for managers to realize that this was not an efficient way to work within groups or\nacross business units, so local area networks (LANs) were developed to share files on businesses campuses.\nCompanies used wide-area networks (WANs, which were leased or owned) to send some well-structured\ntransaction data to their business partners, replacing paper documents such as purchase orders and invoices. Much\neffort went into standardizing business messages so they could be communicated using EDI protocols such as X.12\nand EDIFACT.\nBy the eighties, the Internet had broadened to include many armed services personnel, as well as defense\ncontractors, researchers and computer science professionals. Another key development in the late eighties was\nclient\/server computing. With the advent of PCs, companies retired many \u201cdumb\u201d terminals. Also, managers noted\nthat while it made sense to give many PC users their own word processing and perhaps spreadsheet software, it was\ntoo expensive to provide individual copies of other software that would only be used occasionally. Instead, users\nwould gain access to that software via the company network. This client\/server computing idea evolved further, so\nthat some business applications might entail having some code and\/or data residing on a user\u2019s desktop \u201cclient\u201d\nmachine and other code and\/or data residing on a server machine. A \u201cfat\u201d client would have quite a lot of software\n221 and\/or data but occasionally rely on a server, while a \u201cthin\u201d client would rely heavily on software and data provided\nby one or more servers.\nFTP was a useful mechanism for sharing files in the seventies and eighties (see RFCs 114, 454 and 765).\nHowever, many users\u2019 connections were over slow telephone lines, which limited both the size of files that could be\nsent and the speed of transmission. And, the U.S. National Science Foundation, which maintained several key\nelements of the Internet, decreed that use was restricted to defense- and research-related activities. So, researchers,\narmed services personnel and defense contractors were able to FTP files to one another, but most other PC users\ndid not have access to this tool.\nAlthough the average Internet user may regard this history as quaint and irrelevant, many real advantages and\ndisadvantages of the Internet in general, and P2P applications in particular, relate directly to (1) the ability to\ndigitize all forms of data using binary numbers, and (2) the nature of high-speed, long-distance computer-mediated\ninterchange of information. As just one example of what is both an opportunity and a risk of the ability to digitize\ninformation, this means data can rapidly be captured, stored, copied, and distributed\u2014nice for many applications,\nbut not so nice if the capture\/copying is unauthorized and the distribution illegal or unethical and nearly\nundetectable. For an example of what is both an opportunity and a risk of long-distance computer-mediated\ncommunications, consider the long-distance aspect\u2014nice for reaching a remote audience, but not so nice if fraud or\ndeception is involved and the damage is done in a \u201chit and run\u201d high-speed manner with little legal recourse due to\nconflicts in geographical jurisdiction. As will be developed throughout this chapter, any Internet application can be\nused or abused.\nIn the nineties, several elements combined to turn the Internet into a global platform for collaboration. Prices\ndropped to levels that enabled very rapid growth of sales of PCs to businesses, educational institutions, and home\nusers. The Internet expanded and the National Science Foundation dropped its restrictive Acceptable Use Policy,\nopening the door to private and commercial use. E-mail (which was previously used primarily by scientists or by\nvery large organizations) now became commonplace. Furthermore, the key technologies of the World Wide Web \u2014\n(hypertext markup language) html, uniform resource locators (URLs), and web browsers, made it easy for users to\ngain access to data in a huge variety of forms (numbers, words, images, music, video) stored in files all over the\nworld. File transfer protocol (FTP) remained a useful mechanism for transferring large files (however, FTP\ntransfers require the use of an available FTP server, which is a limiting factor).\nDigital music or video files do not play very well over slow or unreliable Internet connections. Also, such files are\nmore useful offline, especially for users who want access to the music or videos when they are not sitting at a\ndesktop computer. So, PC users sought easy ways to locate and download specific types of files (especially music),\nwhich they could store both on their computer and on other devices (such as iPods or cell phones). This became a\nmajor impetus for the development of some P2P file sharing services.\nA second motivator for P2P applications was the observation that personal computers typically sit idle for long\nperiods of time (such as when you are asleep) and thus are not used to their capacity. Since PCs were being\nconnected to the Internet in rapidly increasing numbers, some people reasoned that there ought to be a way to\ncapitalize on all that collective excess capacity. These two needs led to several new varieties of peer-to-peer file\nsharing.\n222 Peer-to-peer file sharing\nPreviously we noted the rise of client\/server computing in organizations, including the use of \u201cfat\u201d clients\ncontaining many applications and some data, and \u201cthin\u201d clients which do not store data and contain so little\napplication software that they are similar to a dumb terminal (yet much nicer to work with, since a \u201cthin\u201d client\nwould normally have a web browser and might support both audio and video). Some industry leaders, such as\nOracle CEO Larry Ellison, predicted in the nineties that the PC would become obsolete because networks would\nhave such robust bandwidth and speed that a user could work online with applications in such a transparent way\nthat they would be barely aware that the data and applications were not stored on their \u201cthin\u201d client device. Others\ntook a different view. They reasoned that since many users make regular use of a few key applications (such as word\nprocessing), there would continue to be a need (whether real or perceived) for those applications to reside directly\non the user\u2019s PC (so, it would be a \u201cfat\u201d client). Since, thanks to Moore\u2019s Law, those devices were getting ever more\npowerful yet less expensive, most users\u2019 PCs would have a lot of excess computing capacity. Instead of relying on a\nclient\/server architecture to serve up applications via the Internet, why not find ways to tap into the excess storage\nand processing capacity of the many PCs already connected to the net?\nWhen FTP is used to share files, an FTP server must (at least temporarily) store the file before it can be\nforwarded to its destination. This is not necessary in a peer-to-peer network, in which resources (data and\napplications) are distributed throughout \u201cpeer\u201d computers, instead of being concentrated in server computers.\nOnce you have obtained a particular file from another user, your machine can provide it to other users on the\nnetwork. Any peer computer can at different times act as either a client or a server. And, some services are designed\nto ensure that every participant is both a \u201ctaker\u201d and a \u201cgiver\u201d; users who prevent access to files residing on their\ncomputers are eventually barred from further use of the network (a strategy known as \u201cgive to get\u201d).\nEarly P2P applications worked as follows: Users install a P2P software application on their machines. When a\nuser wants a particular file, he issues a request for that file, and agent software then searches the Internet for\nmachines containing the P2P software. On each such machine found, the agent then searches for the requested file.\nThis process worked pretty well at first, since computers operate at the speed of light and the random path of\nsearches meant that different user PCs were tapped at different times. However, it was inherently inefficient, since\nit meant that if multiple users sought the same file, each user\u2019s agent would go through the same process of hunting\nup machines and then checking the contents of each machine it encountered until it found the desired file. So, very\nsoon new P2P application software was developed that would create one ormore indexes, which were stored on one\nor more computers. Most P2P software today involves creating at least one index, such as a directory of machines\ncontaining the P2P software. This index can be kept on every participant\u2019s machine or, as with the Gnutella P2P\nservice, the index can be kept on a small number of machines that act as directory servers. Upon receiving a user\nrequest, the agent first examines the directory on one of these machines, and then uses that information to find\nthose machines.\nSome P2P services also create an index of music files stored on specific users\u2019 machines. Since the\ndirectory\/index itself is not a large file (in comparison with the music file itself), most experts do not consider this\napproach to be a violation of the basic P2P principle. Music and video files that are to be shared are distributed\nacross users rather than stored in a few centralized servers. Music files are large; when they are stored centrally a\nserver can easily become paralyzed by multiple simultaneous user requests. In contrast, when these files are\ndistributed across the Internet, no one user\u2019s machine is heavily burdened. Also, to reduce burden on individual\n223 users, some services have different users\u2019 machines provide portions of large files rather than the entire multi-\nmegabyte file. Thanks to meta-data, which describes where each block fits, these portions are then re-assembled at\nthe recipient\u2019s machine.\nIn 1999, while a student at Northeastern University in Boston, Sean Fanning created the Napster P2P music file-\nsharing service. Napster caught on so quickly that soon record labels and some artists (such as Madonna and\nmembers of the Metallica band) complained that it was being used to illegally share copyrighted songs. By\nDecember 1999 the Recording Industry Association of America (RIAA) had already filed a lawsuit against the\nfledgling company, leading to its shutdown in 2001 (subsequently another company bought the Napster name and\nlogo and it offers a fee-based file sharing service). In the meantime, other Napster-like services, including Grokster,\nKazaa, BitTorrent, eDonkey and others were coming online, so P2P file sharing continued to be easy and\nubiquitous. Many people have debated whether these services are unethical. U.S. courts have ruled that if\ncopyrighted material is exchanged over such a service, it can be legally shut down.\nAlthough P2P file sharing eliminates the bottlenecks that arise when many users seek files stored on few servers,\nit can nevertheless increase network congestion when users are concentrated geographically, such as on college\ncampuses (Dzubeck, 2005). Both colleges and Internet service providers have complained about this problem. To\nreduce their internal network traffic, as well as out of respect for intellectual property rights and to avoid lawsuits,\nsome U.S. colleges have banned their students from using their college e-mail accounts and other college-provided\nnetwork services for file sharing. However, critics have noted that some P2P file sharing is perfectly legal and that\nan outright ban unfairly penalizes those users who act in an ethical and responsible fashion (see for example Navin,\n2006).\nAfter Napster was shut down, another P2P player that emerged was Kazaa, a music file-sharing service founded\nby Janus Friis and Niklas Zennstrom, who subsequently sold parts of the company to Sharman Networks. They\nthen formed an Internet telephony business, Skype, which also was based on P2P. eBay saw enough potential in\nFriis and Zennstrom\u2019s P2P technology that they bought the company for $2.6 billion in 2005. In 2006, Friis and\nZennstrom announced that they were working on a venture dubbed the Venice Project, in which P2P technology\nwill provide the foundation for a licensed video and film-sharing service (Green, 2006; Rosenbush, 2006). Clearly,\nthese two visionaries believe that the ability of P2P to spread the processing load across participating computers\nrepresents a viable business opportunity in several domains.\nWith the resolution of some of the lawsuits, Napster re-emerged as a legitimate fee-based file-sharing service,\nalthough many reports indicate that they are struggling to reach profitability. BitTorrent, another popular file\nsharing service, was also re-launched and now offers ways for paying customers to legitimately pay for and obtain\nmusic, film, and video files.\nThe P2P controversy continues. Business Week reported in February 2007 that \u201cillegal files still account for an\nestimated 90% of the music download market.\u201d (Holahan, 2007). However, some music labels have stated that they\nno longer plan to attempt to block illegal downloading, and some companies, such as Skyrider and Internet\nMediaWorks, offer services that attach advertisements to shared files (Myer, 2007).\nP2P networks are reportedly targets for distributed denial-of-service attacks, worms, and other malware that\ncan affect users\u2019 computers and their ability to share files. \u201cBad guys\u201d recognize the enormous popularity of file\nsharing and launch opportunistic attacks aimed to affect large numbers of users. Other bad guys use \u201cbotnets\u201d to\n224 rapidly reach large numbers of users\u2019 machines, leading to the emergence of an underground market for advertising\nbounties. Bots are programmed to look like P2P users\u2019 agents, which in turn can fool advertising network services\ninto believing that large numbers of real users have viewed various advertisements (companies usually pay\nadvertising networks based on the number of \u201cimpressions,\u201d or user hits; Hines, 2007).\nMeanwhile, all the publicity about Napster got many IT managers thinking about the possibilities for using peer-\nto-peer technologies to improve corporate IT productivity. One observer (Kharif, 2001) summed up the perceived\npotential:\n\u201cFor big companies, the promise of P2P is threefold. By using idle PCs, P2P connections could tap into cheap,\nunderutilized processing power and bandwidth using a technique called distributed processing. By linking users\ndirectly, P2P could create easier collaboration, allowing the rapid formation of work groups that sidestep traditional\nbarriers such as firewalls and restricted intranets. Finally, P2P could make information more accessible throughout\nan enterprise by opening up the desktops of individual employees \u2013 allowing staffers to search each other\u2019s virtual\ndesk drawers more freely.\u201d\nFor the distributed processing benefit, companies looked to examples such as the SETI project (Search for\nExtraTerrestrial Intelligence), in which millions of volunteers agreed to download software that enabled their\nmachines to analyze telescope data when they would otherwise be sitting idle. SETI has since inspired many other\nexamples of this type of volunteer distributed computing (see http:\/\/boinc.berkeley.edu\/volunteer.php ), such as\nfor doing gene sequencing, protein analysis, analyzing clinical trials data, and stress-testing of eCommerce sites.\nWhile these distributed computing examples are a P2P application, they are not really a P2P file-sharing\napplication, since (with SETI, for example) a central server sends out the telescope data, and peers do not directly\ncommunicate with one another.\nAn example of an initiative aimed at using P2P technologies to both enable collaboration and to make\ninformation available throughout an enterprise is a product offered by Groove Networks, a company founded in\n1997 by Ray Ozzie, a renowned computer scientist who was the architect of Lotus Notes. Groove users can easily\nshare in the preparation of various documents, and files stored on any Groove user\u2019s machine can be available to all\nother Groove users. This is an interesting example, because it represents a departure from how companies have\nbeen approaching enterprise software in recent years. ERP software relies on a centralized (or virtually centralized)\ndatabase of business and financial transactions, and many early knowledge management systems took the same\napproach by creating centralized (or virtually centralized) knowledge repositories. Groove, in contrast, does not\nattempt to centralize data and documents. In 2005 Microsoft announced that it was acquiring Groove;\nsubsequently Ray Ozzie became the chief software architect for Microsoft. Microsoft\u2019s acquisition of Groove, and\nthe appointment of Ozzie to his influential post are two indicators that this software giant sees potential in P2P file\nsharing and in distributed computing in general.\nAlthough in the late nineties there was much excitement about P2P\u2019s potential for both consumer and enterprise\napplications, a dramatic and widespread U.S. technology downturn in spring 2000 put such ideas on hold for\nseveral years, and many managers adopted a conservative wait-and-see attitude to spending plans for all new\ntechnologies. After robust growth from 2005 to 2007, a stock market meltdown in January 2008 led analysts to\npredict another U.S. technology downturn. As to P2P in particular, many managers also preferred to wait until\nsome of the lawsuits were resolved.\n225 There continues to be strong interest in understanding how P2P impacts network traffic, and how this approach\ncan be harnessed to utilize excess capacity on user\u2019s PCs. MIT\u2019s Technology Review magazine reported in 2007 that\n\u201cTV shows, YouTube clips, animations, and other video applications already account for more than 60 percent of\nInternet traffic\u201d and this number was predicted to rise to as high as 98 percent by 2010 (Roush, 2007). Both this\npiece and another study reported in 2007 that P2P connections can help alleviate network congestion when files are\nserved up from nodes distributed all over the world, instead of concentrated in a few servers.\nIn an interesting twist, researchers used P2P technology to gather more than six million data points per day for\ntwo years, yielding a map of the Internet\u2019s structure. They found that although a huge volume of Internet traffic\nworldwide today passes through about 80 key nodes, many other nodes are well connected with other peer\ncomputers and could thus bypass these key nodes if necessary. Researchers are working to develop new networking\ntools to distribute network loads more effectively (Carni, et al., 2007; Graham-Rowe, 2007). For example, a system\ncalled Chunkyspread developed by Cornell University professor Paul Francis, reduces the need to broadcast\nmetadata about files, by assigning \u201cslices\u201d of files to each participating user. As explained in Technology Review: \u201cA\nslice consists of the nth bit of every block \u2013 for example, the fifth bit in every block of 20 bits. Alice\u2019s PC might\nobtain a commitment from Bob\u2019s PC to send bit five from every block it possesses, from Carol\u2019s PC to send bit six,\nand so forth. Once these commitments are made, no more metadata need change hands, saving bandwidth.\u201d\n(Roush, 2007).\nIn recent years there has been an increase in reports of government organizations\u2014such as the U.S. Department\nof Homeland Security \u2013investing in P2P applications. And, another acclaimed computer scientist \u2013 Alan Kay, who\ninvented several key personal computer innovations (including the graphical user interface) during his years at\nXerox PARC\u2014is touting the expected benefits of a new software developer kit for collaborative applications,\nCroquet, which will be based on P2P principles (see http:\/\/croquetconsortium.org).\nThere have also been reports of P2P problems. For example, in June 2007 pharmaceutical giant Pfizer stated\nthat the spouse of an employee had inadvertently leaked personal information on more than 17,000 employees\nstored on a company computer. The individual had included the directory in which the file was stored as an\nallowable source for a music-sharing application (Leyden, 2007).\nOther P2P Internet applications: communication, commerce, and collaboration\nIn our discussion of P2P file sharing applications in Section 3, we noted that \u201cP2P\u201d stands for \u201cpeer-to-peer,\u201d\nreferring to an architecture of computer \u201cpeers\u201d (as opposed to \u201cclients\u201d or \u201cservers\u201d). Of course, in Section 3 many\nof the examples also involved person-to-person file sharing (after all, college students are sharing music files with\none another; they just happen to be using their computers to do so, in a peer-to-peer computer network\narchitecture). In this section, we broaden our discussion to include person-to-person Internet applications that do\nnot necessarily involve a peer-to-peer network from a computational perspective. In other words, while some of the\napplications which we discuss next could be designed using peer-to-peer tools and principles, others are based on\nclient\/server tools and principles.\nSo, in this section P2P stands for \u201cperson-to-person.\u201d We classify person-to-person applications according to\nwhat we call participation: the number of people who normally participate in the application as suppliers (senders)\nor consumers (receivers) of information:\n\u2022 One-to-One participation implies communication between exactly two persons\n226 \u2022 One-to-Many participation implies communication between one person and many others\n\u2022 Many-to-Many participation implies communication between many persons\nA secondary categorization is that of symmetry: whether participants have equivalent\/interchangeable roles in\nthe application (for example, VoIP telephone communications, in which participants are both senders and receivers\nof information), or reciprocal\/unique roles (for example, e-mail, in which one participant is a sender only and the\nother is a receiver only).\nIn the following subsections we discuss a few applications, and compare them against traditional means of\nperforming similar communication or collaboration tasks (for example, how Internet telephony bypasses\nconventional telephones). We discuss advantages and disadvantages of these applications, and precautions\nappropriate to their use.\nOne-to-one P2P applications\nOne-to-One participation implies communication between exactly two persons, who normally know one\nanother well enough to initiate communication by invoking each other\u2019s address (be it a postal address, telephone\nnumber, Web URL or e-mail address). Participants\u2019 prior knowledge of one another affects their interpersonal\ntrust, which in turn presumably affects both their willingness to use the communication tool as well as the content\nof their communication. Two familiar pre-Internet examples of one-to-one communications are land line telephone\ncalls and written mail. One-to-one Internet examples are e-mail (reciprocal), VoIP (equivalent) and Instant\nMessaging (both reciprocal and equivalent).\nTwo traditional One-to-One examples differ in terms of symmetry:\nWritten mail is reciprocal in that one party is a sender and the other is a receiver.\n\u2022 Land line or cellular telephone calls and text messages are equivalent in symmetry, in that participants act\nas both senders and receivers of information.\n\u2022 Each of these traditional examples can be compared with similar one-to-one Internet applications, as well\nas one-to-one applications delivered via other network technologies:\n\u2022 Like traditional written mail, communication using e-mail is reciprocal in that one party is a sender and\nthe other is a receiver.\n\u2022 Like traditional telephone calls, communication using Internet telephony (VoIP, using services such as\nSkype) is equivalent in symmetry, in that participants act as both senders and receivers.\n\u2022 Like traditional telephone calls, instant messaging is reciprocal in that at a given moment, one party is\nsender and the other is a receiver. However, it is also equivalent in that these role-changes occur on a\nmoment-by-moment basis, giving an experience that is more like a telephone call in its immediacy.\n\u2022 One-to-One P2P Internet applications (such as e-mail, VoIP, and IM) offer both advantages and\ndisadvantages over their traditional predecessors (mail and phone).\n\u2022 Some users view e-mail as \u201cfree\u201d because someone else (such as an employer, school or advertiser) pays for\nthe service. Most U.S. schools provide complimentary e-mail accounts to registered students, for example.\nMany (not yet all) e-mail users also feel it is far easier to compose and send an e-mail than to hunt down\n227 pen, stationery, envelope, and postage stamp, use legible handwriting to write the letter, and post it. E-mail\nhas additional advantages: near-instant delivery, ability to append photos, documents, and other digital\nattachments, ease of reply, inexpensive storage, and the option to send the same message to multiple\nrecipients at widely dispersed locations.71 Some users nevertheless do feel that traditional written mail has\nits advantages. For example, because the sender takes time to carefully think through and phrase a paper\nletter, some feel that better communication results. Others feel that the time investment (in good language\nand personal if not good penmanship) itself is meaningful to the recipient and that personal touches\n(enclosed mementos, applied fragrances, etc.) transform written letters into cherished keepsakes, which is\nrarely the case for e-mails.\n\u2022 VoIP is often (although not always) considerably less expensive than long-distance telephone calls (this\ndepends on the price of the Internet access and the volume and average length of the calls in comparison\nwith your phone company\u2019s pricing plans). On the other hand, many VoIP users mention dropped\nconnections and poor clarity as irritating problems as compared with traditional land lines (on those\naspects, VoIP is more comparable to cell phones, at least in the U.S. where users continue to complain\nabout dropped or unclear calls).\n\u2022 Like e-mail, IM is viewed as \u201cfree\u201d if Internet access is provided by a third party for free. Also, a given user\nmay be view it as \u201cfree\u201d if the user accepts their Internet access bill as a fixed expense and sees IM as an\nunexpected or \u201cbonus\u201d application.\n\u2022 Traditional and Internet One-to-One applications can also be compared in terms of security risks.\n\u2022 Traditional mail is insecure in that it can be intercepted before it reaches the sender or after the letter has\nbeen discarded. The U.S. Postal Service is generally reliable, but there are occasional reports of mail that is\nstolen before it reaches its destination (theft of elderly citizens\u2019 Social Security checks, for example, is so\ncommon that the U.S. Social Security Administration now urges retirees to arrange for direct electronic\ndeposits into their bank accounts). There are many reports of identity theft based on \u201cdumpster diving\u201d\n(thieves rummaging through discarded garbage to unearth credit card account numbers and such), which\nhas given rise to robust sales of inexpensive document shredders for home use. Also, because a sender can\neasily disguise the letter\u2019s origin, there are risks of mail fraud, hate mail, and unsolicited \u201cjunk\u201d mail.\n\u2022 E-mail is vulnerable to risks that mirror traditional mail risks. A sender needs to know the address of the\nintended recipient, but if so inclined the sender can disguise his or her e-mail address by routing the\nmessage through an Internet \u201canonymizer\u201d service, which gives rise to threats of fraud, spam, and hate\nmail. So threats to e-mail privacy include unauthorized interception of the e-mail before it reaches the\nrecipient or unauthorized access after the recipient has read it. Also, e-mail has given rise to other privacy\nissues. For example, many users do not understand that when they hit the \u201cdelete\u201d key, the e-mail has not\nactually been destroyed. Also, many employees do not realize that (at least in the U.S.) the employer is\nlegally allowed to look at their messages and can use them as to find evidence of offensive language,\nharassment, revelation of company secrets, and other offenses which can lead to the employee\u2019s dismissal.\n71 We consider e-mail to be a 1-1 application (rather than 1-M) despite the option of multiple recipients, which is merely a\nconvenience equivalent of sending multiple 1-1 communications.\n228 \u2022 A traditional phone conversation is subject to the threat of external parties listening in via wiretapping or\noverhearing a telephone conversation (or at least one half of the conversation) when it takes place in a\npublic location. Unauthorized recording is also a problem. For example, a participant or an eavesdropper\n(including law enforcement officials, presumably with authorization) could secretly record the conversation\nwithout one or both parties\u2019 permission, giving rise to subsequent issues related to access to the\ninformation that was exchanged and\/or actions subsequently taken as a result of the conversation.\n\u2022 Unauthorized interception or recording of VoIP conversations gives rise to similar issues as in the\ntraditional telephone application.\n\u2022 Instant Messaging is similar to VoIP except that the communication is text based rather than voice based.\nThus, One-to-One P2P applications give rise to similar risks as their pre-Internet predecessors, and they also\ngive rise to new risks. Personal communications are almost always intended (or assumed) to be private, so the most\nobvious threat is compromised privacy through interception before, during, or after communication events.\nWhereas there are usually governmental prohibitions and sanctions for tapping telephone conversations or\nviolating post office regulations, Internet communications are both less secure and less regulated. Because it is\ninexpensive to copy, transmit, and store information in digital form, it is also easy for unauthorized users and\nthieves to gain access to it.\nNote also that both traditional and Internet one-to-one applications continue to evolve, including taking on the\ncharacteristics of one-to-many applications. For example, as telephone networks \u201cgo digital,\u201d new threats have\narisen. Identity-blocking technologies and agent software can help voice \u201cspammers\u201d flood phone lines with\nincoming calls. This flooding tactic was reportedly used as a \u201cdirty trick\u201d in the United States by some computer-\nsavvy people who sent false messages about a candidate to voters just before a presidential election to dissuade\nvoters from supporting that candidate.\nOne-to-many P2P applications\nOne-to-Many participation implies communication or commerce between one person and many others, who\nmay or may not know either the one person or the many others.\nA traditional example is a community bulletin board, in which an individual may post an announcement\n(perhaps for an upcoming event, or a lost and found notice), an offer of services (tutoring, babysitting, gardening,\netc.) or a For Sale sign. Reciprocal communication is involved, since one individual posts the information in hopes\nthat many individuals will respond to it. One problem with bulletin boards occurs when individuals post offensive\ninformation; the remedy is that a sponsor (or concerned citizen) simply removes the posting). Another problem is\nthat individuals may fail to remove a notice when it is no longer useful (because the event has already occurred or\nthe lost item found and returned, for example). Also, the authenticity of a posting, including the reliability of the\nindividual who posted it, may be beyond the control of the organization providing the bulletin board.\nAnother traditional one-to-many example is a yard sale (which in different regions of the U.S. is called a garage\nsale, tag sale, or other name). An individual (personally or acting as an agent for members of his\/her household)\nplaces items (furniture, tableware, clothing, books, toys, etc.) on display in their yard, and invites passersby to\npurchase them. The relationship is reciprocal because one seller supplies items in the hope that many individuals\nwill make purchases. The prices charged might be pre-determined, negotiable, or subject to an auction. Items that\nare not acquired by the end of the sale can be put aside for another sale or disposed of in some other way.\n229 These traditional one-to-many examples can be compared with similar P2P Internet applications:\nLike a community bulletin board, Internet bulletin boards require an individual to initiate a posting, and let\nusers decide for themselves if and how to respond. Organizational sponsors usually monitor postings for offensive\ncontent, and the principle of caveat emptor \u201clet the buyer (or responder) beware\u201d usually applies. Aside from these\nsimilarities, there are significant differences between old fashioned physical bulletin boards and the Internet\nvariety. For one, Internet bulletin boards usually have no theoretical space limitations, although sponsors may\nestablish prioritization rules for message deletion or visual placement. Also, except when restricted to an\norganizational intranet or extranet, Internet bulletin boards are potentially accessible to all Internet users which\ncan complicate the identification and authentication of both posters and responders. Even intranet and extranet\nbulletin boards are far more readily accessible to authorized users, because users\u2019 physical location is irrelevant in\ncyberspace. Lastly, even when a sponsor monitors postings for content and attempts to control the identification\nand authentication of those who post notices, significant damage can be done quickly if monitoring does not take\nplace in real time.\n\u2022 Like a yard sale, on Internet-based person-to-person marketplaces such as eBay or Craig\u2019s List individuals\nput objects up for sale and negotiate with potential buyers. Site sponsors monitor activity to some extent,\nbut the principle of \u201clet the buyer beware\u201d still applies. Ironically, the increased anonymity on the Internet\n(and long distance nature of the transactions) introduces the reciprocal principle of \u201clet the seller beware\u201d;\nthat is, the buyer may be using someone else\u2019s identity, acting as a shill bidder to bring up the prices, or\notherwise acting fraudulently. Such actions occur far less frequently during traditional yard sales, especially\nwhen sellers require payment in cash. Also, in the traditional yard sale transactions are normally\nconsummated on the spot; the buyer pays for and receives the items concurrently. Also yard sales are\nusually subject to geographical jurisdiction, whereby a dishonest seller can be traced to a location,\nidentified, and prosecuted under the laws of the jurisdiction; Internet sales transcend geographical\njurisdiction, severely complicating tracing or identification, let alone prosecution.\nThe One-to-Many P2P Internet applications discussed here (Internet bulletin boards and P2P marketplaces)\nhave clear advantages over their traditional counterparts. Internet bulletin boards are dramatically more accessible\nto both suppliers and consumers of information\u2014so long as all parties have Internet access. Constraints of time and\ndistance are all but eliminated. Internet P2P marketplaces are accessible to a far wider audience of potential buyers.\nNot only does this increase the likelihood of a sale, but it also can facilitate buyer competition (auctions) and\ntherefore higher profitability. Still, the \u201cold fashioned\u201d yard sale does have a few advantages. The seller can deal on\na more secure cash basis, and buyers get the goods immediately without having to wait or pay for subsequent\ndelivery. Also the potential buyer gets to see, touch, and potentially try on or test items, and there is some (albeit\nlimited) recourse if fraud is involved.\nMany-to-many P2P applications\nMany-to-Many participation implies communication between or collaboration among many persons and\nmany others, who may or may not know one another well. A few \u201ctraditional\u201d examples are:\nA meeting involving participants who share a common interest (such as belonging to the same political party).\nFor example, in the U.S., the Democrat and Republican political parties have conventions in which representatives\nfrom the 50 states meet to decide on their parties\u2019 platforms and to nominate presidential candidates. Such\n230 meetings normally take place in one location at the same time. There is normally a schedule associated with the\nmeeting, and a record of what people say, decisions taken, etc. might be captured in the form of minutes prepared\nby an observer. Participation is equivalent, with most participants acting as both suppliers and consumers of\ninformation. Facility and regulatory constraints (such as fire or other safety rules) normally place restrictions on\nthe number of participants.\n\u2022 In another many-to-many example, individuals with a common interest come together to produce a\ndocument, such when a fund-raising group produces a cookbook of members\u2019 favorite recipes. Such a\npublication involves reciprocal communication, in that many participants contribute and many\nparticipants consume the information. The consumers are rarely identified or tracked.\n\u2022 A poll is also a many-to-many example, in that many individuals\u2019 views on an issue (a political issue, for\nexample, or student evaluations of a teacher) are captured and then shared with many other issues. Here\nagain the communication is reciprocal in that many participants supply and many participants consume the\ninformation. A consumer survey of products is quite similar.\nIn all of the above traditional many-to-many examples above, there is a third role besides \u201cprovider\u201d and\n\u201cconsumer\u201d of information. That role is that of sponsor, organizer or mediator. Some entity (an individual or a\nrelatively small group) organizes the logistics of a meeting. A potentially different entity edits and potentially a\ndifferent entity publishes a group publication. Yet another entity may conduct a poll and\/or analyze the polling\ndata.\nMany-to-many Internet P2P applications have rather similar characteristics as their traditional counterparts, as\nwell as offering new capabilities:\n\u2022 The use of the Internet as a P2P platform for political conventions and other meetings of large or small\ngroups of people allows participation by home-bound or disabled individuals and those who are unable or\nunwilling to travel to a meeting location. It also provides the ability to capture a complete record of an\nentire meeting.\n\u2022 The use of the Internet as a P2P platform for the production of group documents such as cookbooks offers\nadvantages of wider and timelier access. Both the number of contributors and the number of information\nconsumers is likely to be significantly larger. Also, since the publication is not static, if a consumer uncovers\na mistake in a recipe, a corrected version can be easily posted. This is why wikis (such as Wikipedia) are\nsuch powerful and popular P2P applications. A wiki allows the rapid evolution of useful information, in a\nsignificantly compressed time frame, for larger groups of both contributors and consumers.\n\u2022 Similarly, the Internet offers a powerful platform for polling or rating systems that capture citizens,\u2019 affinity\ngroup members\u2019 or consumers\u2019 views on issues or products. On the Internet, the reduced costs of capturing\npolling and rating data, analyzing it and reporting on it (even in real time), increases the number of views\nthat can be captured and the ability to track the evolution of opinions over narrower slices of time.\nMany-to-Many P2P Internet applications have advantages over their traditional counterparts. Internet-hosted\nmeetings eliminate distance constraints, enable time shifting (that is, the meeting can be held in synchronous or\nasynchronous mode), and facilitate record keeping (including capturing a record of the entire meeting). There is\nalso the possibility of admitting many more participants, potentially in different categories such as disabled or\n231 otherwise home-bound citizens, anonymous attendees, and non-contributing observers. Some feel that a\ndisadvantage is the reduction in the personal touch of face-to-face communication (such as the ability to slap a\nparticipant on the back or shake his or her hand). Security is a great challenge in order to avoid breaches of privacy,\nunauthorized participation, and unauthorized recording or subsequent use of the meeting record.\nThe use of Internet-based P2P collaboration systems such as wikis for shared authorship offer the potential\nadvantages of capturing ideas and information from many more contributors while reaching a potentially wider\naudience of consumers. However, some people feel that such systems face steeper challenges compared with their\ntraditional counterparts in ensuring the competence and reliability of the contributors and the information that\nthey provide.\nP2P and the Law of the Double-Edged Sword\nOur discussion of P2P applications illustrates an important phenomenon that applies across the full spectrum of\nIT applications. Every IT application brings challenges and risks along with benefits. We call this the Law of the\nDouble-Edged Sword. Note that peer-to-peer file sharing, by enabling every PC on a network to act as both client\nand server, brought the benefit of avoiding bottlenecks (which occur when excess demand for music or video files\nstrains servers located centrally or on only a few nodes), and introduced an easy, non-commercial approach to file\nsharing. However, it also introduced ways for users to violate intellectual property rights by essentially stealing\nmusic and video files \u2013 an ethical breach. While P2P is a great way to share open-source music or video files, as well\nas to increase the portability and usability of music files that are legitimately owned, it is also a way to avoid paying\nfor music and videos, depriving musicians, studios, and others of their rightful share of profits. Reports of hacker\nattacks and worms that propagate across these P2P networks again illustrate the Law of the Double-Edged Sword.\nIt seems that every day, somebody announces a new IT application. In the eighties, the personal computer was a\nnew phenomenon which brought such new capabilities as word processing, spreadsheets, and end-user databases,\nas well as new risks, such as flawed decision-making based on poorly designed spreadsheet models, inconsistencies\nin data that were not centrally managed, and viruses that spread from one PC to another. In the nineties many\ncompanies invested in enterprise resource planning (ERP) software in hopes of speeding up business transactions\nand improving the transparency of business processes. Along with these benefits came new challenges, including\nless flexible business processes (since now the process had to conform to the software), issues related to heavy\nreliance on the ERP vendors, and other challenges that helped confirm the Law of the Double-Edged Sword. Some\nnew IT application categories in recent years include wikis, blogs, mashups, and widgets. Each of these has already\ngiven rise to stories of success, failure, and challenge. Next year other applications \u2013 including new P2P\napplications for one-to-one, one-to-many, and many-to-many file sharing, communication, commerce, and\ncollaboration\u2014will inevitably be added to the list.\nEvery category of IT applications brings both opportunities and challenges for individual users and for\ncorporations, consistent with the Law of the Double-Edged Sword. Thus, employees, managers and citizens need to\nrealistically assess each new application that comes along. Sometimes people focus only on the good side, and fail to\nadequately prepare for the inevitable challenges. Other people focus only on the risks, and fail to harness the\npotential of the new technology. Either approach falls short of effectively managing IT. Only by simultaneously\nconsidering both sides of the Law of the Double-Edged Sword can managers expect their organizations to realize\nthe awesome potential of new technologies. With that in mind, we offer a set of P2P precautions below. Some\n232 precautions concerning P2P Internet applications \u2013 such as caveat emptor\u2014mirror those for their traditional\ncounterparts, but in addition, other precautionary measures are also needed. For example:\n\u2022 Internet service providers should offer clear and user-friendly options for secure composition,\ntransmission, and reception of e-mail messages.\n\u2022 Providers of VoIP, e-mail, and IM systems (which support the reception, storage, backup, retrieval, and\neventual \u201cdeletion\u201d of messages) must ensure secure handling of messages and communication events.\n\u2022 End users of P2P applications should learn about the limits of these applications to protect them against\nsuch dangers as identify theft, fraud, communications falling into unauthorized hands, and illegal,\nimproper, or dangerous attachments (e.g. viruses).\n\u2022 It is important for end users to recognize that the Internet\u2019s global reach amplifies the number of adverse\nevents that can occur, and that the \u201cbad guys\u2019\u201d ability to conceal their true identity creates an especially\nproblematic risk.\n\u2022 End users as well as system providers and other intermediaries need to recognize that even when \u201cbad\nguys\u201d are caught and sanctioned (such as by blacklisting or blocking access) severe damage may already\nhave been done. Furthermore, since laws vary around the world there may be little if any legal recourse.\nAnd, since it is relatively easy to adopt a new online disguise, individuals who fabricate or misrepresent\ninformation often return to attack again.\n\u2022 End users should also learn about the limits of P2P applications to protect them against problems resulting\nfrom gullible reliance on misinformation. For instance, many college professors do not allow their students\nto use Wikipedia as a reference source, since there is only weak assurance that the information posted there\nis correct.\n\u2022 End users should also learn about and adhere to ethical principles in the use of these applications,\nincluding not sending or forwarding offensive or nuisance messages, not violating intellectual property\nrights, and (if called for) the reporting such violations to appropriate authorities.\n\u2022 Employers and other organization which regulate who is allowed to use their communication systems and\nunder what conditions should clearly communicate their policies and procedures and commit to\nappropriate education and enforcement.\n\u2022 Organizations and individuals who host bulletin board or auction Internet sites should adopt rigorous\npolicies and procedures\u2014including sanctions such as blacklisting\u2014and clearly communicate them to all\nparticipants. The policies and procedures should be strictly enforced.\nIn April 2005, on the fortieth anniversary of his influential article in Electronics Magazine, Gordon Moore\npredicted that the ability to pack more and more transistors onto a chip (Moore\u2019s Law) will continue to hold true for\nat least a few more decades (Niccolai, 2005). Networking technologies (including computing devices like routers as\nwell as software tools and protocols that help networks work well) have also improved dramatically in the last few\ndecades. In contrast, tools and methods for developing software applications improved at a far slower rate in the\neighties and nineties, but the pace has accelerated recently and, with the advent of web services, is expected to\ncontinue to gain. Taken together, improvements in foundation technologies and methodologies for computing,\n233 networking and software development will give rise to many new IT applications \u2013 including P2P applications\u2014in\nthe coming years, just as these foundation technologies and methodologies gave rise to peer-to-peer file sharing and\na host of person-to-person applications for communication, collaboration, and commerce. Smart managers and\neffective companies will harness these new applications in the service of key business processes and competitive\ninitiatives, while ineffective companies and managers will fail to recognize or capitalize on their potential.\nExercises\n1. Given the IT trends discussed in this and other chapters, what scenarios do you foresee for P2P in the coming\ndecade?\n2. Does your school have a P2P file sharing policy? Should it? If so, what should it say? If not, why not?\n3. The concept of intellectual property involves the right of an individual or organization to profit in some way\nfor the effort invested. Even where profit is not an issue, the concept disallows unauthorized resale and\/or\ntampering with the material. Given the ease with which information can be copied, modified and\nredistributed over the Internet, should traditional copyright protections still apply, and if so, what\njurisdictional recourse is available once national boundaries are crossed?\n4. Suppose you were the chief information officer for your school. If a worm propagated through your school\u2019s\nsystems as a result of illegal P2P file sharing, what actions (if any) would you take against the students who\nwere participating in the problematic file sharing services?\n5. What are some examples of wikis that would be particularly useful in your job or to support your personal\ninterests? Who would be the best contributors to their development? Do the advantages of having access to\nreliable information in wikis outweigh the dangers of contamination with unreliable information?\n6. What are other examples of one-to-one, one-to-many and many-to-many person-to-person systems? What\nare the benefits and challenges that each application brings?\nReferences\nCarmi, Shai; Havlin, Shlomo; Kirkpatrick, Scott; Shavitt, Yuvall; Shir, Eran. A model of Internet topology\nusing k-shell decomposition. Proceedings of the National Academy of Sciences of the United States of\nAmerica 104 (27): 11150-11154, July 3, 2007.\nDzubeck, Frank. Get ready for corporate P2P apps. NetworkWorld, April 11, 2005. Online at\nhttp:\/\/www.networkworld.com\/columnists\/2005041105dzubeck.html.\nGraham-Rowe, Duncan. Mapping the Internet. MIT Technology Review, June 19, 2007.\nGreen, Heather with Lacy, Sarah and Rosenbush, Steve. What comes after YouTube. BusinessWeek, October\n30, 2006.\nHolahan, Catherine. Advertising to the file-sharing crowd. BusinessWeek, February 26, 2007.\nhttp:\/\/www.businessweek.com\/print\/technology\/content\/feb2007\/tc20070226_793620.htm\nKharif, Olga. Waiting for the killer apps. BusinessWeek, August 1, 2001.\nLeyden, John. Pfizer worker data leaked via P2P. The Register, June 14, 2007.\nwww.theregister.co.uk2007\/06\/14\/pfizer_p2p_data_leak\/print.html\n234 Myer, Michael. Free music downloads, lawsuit not included. Business 2.0 Magazine, April 23, 2007.\nNavin, Ashwin. The P2P mistake at Ohio University. C\/net news.com, May 7, 2006.\nhttp:\/\/news.com.com\/2102-1027_3-6181676.html\nNiccolai, James. Gordon Moore looks back \u2013 and forward. PC World April 18, 2006. Online at\nwww.pcworld.com\/article\/id,120429\/\nPlanner, Eric. Music industry reaches deal with file sharing site. International Herald Tribune, July 27, 2006\nRosenbush, Steve. Kazaa, Skype, and now \u201cThe Venice Project.\u201d BusinessWeek, July 24, 2006.\nRoush, Wade. Peering into video\u2019s future. MIT Technology Review, March-April, 2007.\nStainaker, Stan. Here comes the P2P economy. In: Harvard Business Review Breakthrough Ideas for 2008,\nHarvard Business School no. R0802a, February 1, 2008.\n235 13. Opportunities for new\norganizational forms\nEditor: Antony Bryant (Leeds Metropolitan University, United Kingdom)\nReviewer: Geoffrey Dick (University of New South Wales, Australia)\nLearning objectives\n\u2022 In this chapter you will be introduced to the following concepts:-\n\u2022 Formal & Informal Organizations\n\u2022 Horizontal & Vertical Division of Labour\n\u2022 Management & Specialization\n\u2022 Organizations and the impact of Information & Communications Technology [ICT]\n\u2022 Globalization & the Global Economy\n\u2022 Outsourcing and Off-shoring and associated business models\n\u2022 The Open Source model\n\u2022 Linux, Wikipedia and the Wiki model\n\u2022 You will also be introduced to the ideas of:-\n\u2022 Peter Drucker\n\u2022 Henri Fayol\n\u2022 Chester Barnard\n\u2022 Henry Mintzberg\n\u2022 Eric Raymond\nIntroduction\nThis chapter discusses the concept of organizations against the context of Information Systems [IS], and then\ndevelops this to indicate the ways in which Information & Communications Technology [ICT] makes possible new\nforms and conceptions of organizations in the 21st century.\nAt certain points in this chapter you will be asked one or two questions, and also to complete one or two simple\ntasks. Please give some thought to answering these before continuing to read the material. Also please note down\nyour answers so that you can develop them as further questions are asked, and you are requested to complete some\ntasks.\n236 What is an organization?\nYou probably have some idea what an organization is, and can think of some examples; but can you define an\norganization? Here are some tasks to help you.\n1. list some of the organizations you have encountered in any way over the last few years (5 or 6 will be\nadequate)\n2. try to explain some of the features that they have in common\n3. then try to produce a definition of an organization, using the examples and features from your list.\nThe sort of answers you might have given\n\u2022 a company you have worked for or currently work for\n\u2022 a company someone in your family works for\n\u2022 a local sports team\n\u2022 a government department\n\u2022 local government\n\u2022 a religious organization\n\u2022 radio or TV station\n\u2022 a group to which you belong that has some common interest \u2013 sports activity, political interest, hobby, etc.\n\u2022 You probably did not find it hard to list some examples. People know what organizations are, and if asked\nto give examples most people could readily produce a list of five, six or more. Your list probably includes\norganizations such as where you were educated, where you are currently and\/or were previously employed,\ntogether with certain government and financial examples. You might also mention organizations connected\nto some sport or recreation that you follow as a spectator, or that you participate in more actively; and some\nof these may be less formally recognized than others.\nThe list might also include large commercial companies (local, national, multinational), government\ndepartments, churches or other religious institutions, universities, colleges, labour units, football teams, social\nclubs, orchestras, and charities.\nBut you may have had more difficulty in dealing with the second task.\nYou may well have hesitated when you tried to list what these all have in common. Some of them are what might\nbe termed \u2018formal\u2019 organizations with legal charters, definite structures, recognizable characteristics and locations.\nOthers are far less formal, with little or no existence other than through the activities of members.\nThis difficulty has long been an issue in the study of organizations, and many theorists have argued that it is\neasier to describe specific organizations than it is to offer a single definition. Peter Drucker, one of the key\nmanagement thinkers of the postwar period, has argued that we now live in a \u2018society of organizations\u2019. But in\nseveral of his key works he avoids the issue of defining the term \u2018organization\u2019.\nChester Barnard, an early writer on management, did offer a definition of a formal organization in his work on\n\u2018Functions of the Executive\u2019 (1938). He defined a formal organization as a \u2018system of consciously coordinated\n237 activities of two or more persons\u2019. This is a very wide-reaching definition and would include two people planning to\nvisit somewhere together, as well as the United Nations or the World Health Organization.\nWe can expand on this slightly to offer a minimal definition of an organization as something that requires at\nleast two people, who acknowledge each other as members, and have at least one shared objective or common\npurpose, deliberately working together to attain that objective.\nA quick search on the Internet produces the following variations.\n\u2022 A group of people who work together wordnet.princeton.edu\/perl\/webwn\n(http:\/\/wordnet.princeton.edu\/perl\/webwn)\n\u2022 Basically, an organization is a group of people intentionally organized to accomplish an overall, common\ngoal or set of goals. Business organizations can range in size from two people to tens of thousands.\nwww.managementhelp.org\/org_thry\/org_defn.htm\n(http:\/\/www.managementhelp.org\/org%20thry\/org%20defn.htm)\n\u2022 An organization is a formal group of people with one or more shared goals. This topic is a broad one.\nen.wikipedia.org\/wiki\/Organization (http:\/\/www.managementhelp.org\/org_thry\/org_defn.htm)\nIn order for an organization to exist over any significant period of time \u2013 from a few months to many decades \u2013\nit will also need some resources drawn from its environment. Small organizations may need no more than the time\nand effort of its members. Larger ones will need a range of resources, particularly financial and material ones. One\nway to visualize this is to think of an organization as a system \u2013 which at the very simplest level can be depicted as a\nsystem, with a boundary, an input and an output (and feedback).\nadd diagram here\nThe inputs can include time, skills, effort, raw materials, finance, components.\nThe outputs can include products, services, increased skills, profits, achievement of the primary objective.\nLet\u2019s think of one of the simplest possibilities; my friend and I decide to go on a walking tour for our vacation.\nWe meet a few times to consider possible places to go, and we eventually agree on a location and a date on which we\nwill start on our tour. We make all the arrangements for travel, accommodation and so on. We start our walk and\ncomplete our tour. For the time from which we first decided to go on our tour, to the time when we completed it, we\ncould be considered to be an organization: But a very small and an informal one. If we no longer saw each other\nafterwards \u2013 perhaps we had some major disagreement during the vacation \u2013 then our particular organization\nwould simply cease to exist.\nNow consider if, after we had completed our tour, we decided that we could book similar tours for our friends so\nthat they would not have to do all the arranging themselves \u2013 and they would pay us a small commission for doing\nthis. After a few months we find that we are spending all our time on these activities, not just for our friends but\nalso for others who have heard about our service. In fact we have to employ three other people, and rent a small\noffice, and we can only do this after we have been to a bank to arrange credit to make the down payments and pay\nthe first few months\u2019 wages of our new colleagues. After a further period we start to book other sorts of tour \u2013\ncycling, climbing, sports holidays and many other types. By this time we have become not only a larger\norganization, but a more complex and formal one.\n238 In fact some other important changes have happened with our organization. We are now using far more\nresources, including\n\u2022 other people;\n\u2022 financial resources \u2013 such as the bank loan;\n\u2022 office space \u2013 including overheads such as lighting, energy for heating or air-conditioning, etc;\n\u2022 phones, computers and printers.\nWe also have more established links with our environment. For instance we may negotiate favourable terms\nwith hotels or travel agencies for our bookings. We may set up an agreement with an insurance company to cover\nour bookings and our employees. We will want to ensure that we have relevant sources of information so that we\ncan make and confirm our bookings, contact potential customers and suppliers, and so on.\nWe have also become more formal and have a structure of some sort\n\u2022 our employees should have a legal contract of employment detailing their conditions and responsibilities\namongst other things;\n\u2022 we may have decided that one of the three is the office manager, and so is responsible for managing the\nother two, as well as reporting to the two founders \u2013 this would be a form of vertical division of labour;\n\u2022 we may have divided responsibilities between the employees and ourselves, so that some of us deal with the\nwalking tours, and the others deal with cycling, climbing and so on \u2013 this is a form of horizontal division of\nlabour.\nOur organization will now have to become a legal entity, which in most countries will mean that it has to be\nregistered and be accountable in some sense. It is also operating as part of an environment which will include\ncustomers, suppliers and competitors. This means the organization is dependent on its environment; the economy\nand market, competitors, government and legal factors, and other external factors and forces. A very large\norganization will be able to influence its environment to an extent, while smaller organizations will have to adapt to\nenvironmental factors if they are to survive. Microsoft and Sony can exert enormous pressure on their environment:\nour small travel company cannot.\nManagement and the Division of Labour\nNote that a distinction has been made between horizontal and vertical division of labour. The horizontal division\nof labour refers to the ways in which a large project or task can be split into several smaller tasks. The classic\nexample of this is to be found in Adam Smith\u2019s \u2018An Inquiry into the Wealth of Nations\u2019, published in 1776. He noted\nthat if a group of workers wanted to produce metal pins, then they would be far more efficient \u2013 producing far more\npins \u2013 if each person undertook a specialized sub-task than if each person attempted to complete the entire process\nfrom beginning to end. A more modern example would be a highly automated factory producing computers or high-\ndefinition TVs, with each unit moving slowly along an assembly line with individual workers completing short and\nhighly specific tasks.\nThe vertical division of labour, on the other hand, implies a hierarchy of command as opposed to merely a\ndifferentiation and specialization of tasks. So within the factory context mentioned earlier there may be a\nsupervisor who has to oversee the activities of other workers \u2013 as the term \u2018oversee\u2019 implies, the supervisor\u2019s\n239 position is considered to be above that of the other workers. Similarly a production manager may be put in charge\nof several supervisors. The traditional organization chart is a model of this hierarchy, illustrating the vertical\ndivision of labour.\nIn many cases it can be argued that this vertical division also implies specialization of tasks \u2013 the supervisor or\nmanager needs to be skilled in dealing with people, monitoring activities, anticipating problems and difficulties,\nand generally ensuring that things run smoothly and efficiently. This is the point at which \u2018management\u2019 as a\nspecific skill becomes apparent. In a small scale organization management might be carried out by one or more of\none\u2019s colleagues, who essentially are performing the same tasks as everyone else, but in addition are overseeing the\nwork of others. So in the case of our fictitious travel company, the office manager works alongside the other two\nemployees, carrying out similar tasks plus some of the management tasks mentioned earlier.\nOnce an organization grows to any appreciable size, management becomes a specialized task in itself. People\nwho manage large departments in government, or who run large private companies will spend their entire time\n\u2018managing\u2019. On a farm or in a factory the manager \u2013 or managers \u2013 will spend most of their time \u2018managing\u2019 rather\nthan working on the farm itself or on the factory-floor. In a large school the head teacher will often do no teaching\nat all, since all the available time will be taken up with \u2018managing\u2019 the school itself. Now this raises the question \u2013\n\u2018What does a manager actually do?\u2019\nWhat Do Managers Do?\nImagine that you are successful in a job application for the post of manager in our small travel organization \u2013\nnow grown to 15 employees. What sorts of responsibilities and activities would be expected of you? How do you\nexpect you would spend your average day at work?\nNow repeat the exercise; but this time assume you are to take up the post of manager in a large chain of travel\ncompanies, with about 50 shops. Each shop employs 10 staff, both full-time and part-time. You work in the Head\nOffice which employs 80 administrators and support staff. The company is itself part of a larger group with shops\nand offices in many different countries.\nWhat aspects are common to the two situations? What aspects are different? Try to note down some ideas before\nreading the next section.\nThe classical view of management is derived from the work of early theorists such as Henri Fayol (1841-1925).\nFayol defined the five functions of management as:\n\u2022 planning\n\u2022 organizing\n\u2022 co-ordinating\n\u2022 deciding\n\u2022 controlling\nFayol\u2019s experience as head of a coal mine in France led him to identify these functions or activities, and they are\nrelated to the ideas discussed earlier particularly those concerned with the two forms of the division of labour.\n240 Using the list you prepared earlier, can you match your ideas against the five functions given by Fayol? It may\nnot always be easy to select just one function. Did you find that some of the tasks you specified do not fit with any of\nFayol\u2019s five functions?\nIt is important to understand that although Fayol\u2019s ideas might seem fairly obvious and conventional now, they\nwere not really taken up until late in the 20th century. The concept of management as something distinctive did not\nreally become widespread and important until after World War II. Peter Drucker, generally regarded as the most\ninfluential management guru of the last 35 years, argues that it was really only after World War II that what we now\nconsider to be the essential aspects of management came into being. After World War II, management became a\nkey focus for \u2018big business\u2019 and the private sector. In particular the practice of management was encouraged in a\nsystematic manner by the head of General Motors, Alfred P Sloan Jr. (1875-1966). Sloan developed a systematic\napproach to management of large corporations. Many common ideas such as developing business objectives,\nformulating business strategies and strategic planning were started by Sloan. At this time the first multi-national\norganizations appeared, including the Unilever Companies that merged Dutch and English organizations. So it can\nbe argued that the core concepts underlying modern management were first formulated in the late 1940s; but it\nshould also be noted that some key features such as leadership, influence and power are far older, dating back at\nleast to the 16th century and the publication of Niccolo Machiavelli\u2019s book The Prince.\nDrucker extends Fayol\u2019s ideas by proposing three \u2018dimensions of management\u2019, each of which requires a\nparticular task; each is \u2018equally important but essentially different\u2019:\n1. \u2018to think through and define the specific purpose and mission of the institution, whether business enterprise,\nhospital or university\u2019;\n2. \u2018to make work productive and the worker achieving\u2019;\n3. \u2018to manage social impacts and social responsibilities\u2019 (p36).\nMany might question whether modern management actually encourages the third set of tasks. Some would\nargue that it certainly does not; some that it should not; and others that it should, but does not.\nA more important point is that models such as those put forward by Fayol, Drucker and many other theorists\noften rely on highly idealized views of the daily routines of organizational reality. This was noted in particular by\nHenry Mintzberg who demonstrated the discrepancy between what many managers said they did, and what they\nactually do. He argued that the view of managers as rational decision-makers and planners was at best only\npartially true. More importantly he identified a series of roles that managers undertake in the course of their\nactivities. These were grouped under the headings \u2013 interpersonal, informational, decisional. Each one subdivided\nas follows:-\n\u2022 Interpersonal\n\u2022 figurehead\n\u2022 leader\n\u2022 liaison\n\u2022 Informational\n\u2022 monitor\n241 \u2022 disseminator\n\u2022 spokesperson\n\u2022 Decisional\n\u2022 entrepreneur\n\u2022 disturbance handler\n\u2022 resource allocator\n\u2022 negotiator\nThe details of Mintzberg\u2019s ideas, and the general issues of management are beyond the scope of this chapter; but\nit should be noted that the concepts of organization, division of labour, and management are closely related to one\nanother, although their actual realization in practice will depend on many factors, including economic, cultural and\nsocial ones; but more particularly for the purposes of this chapter the impact of technology.\nDevelopment of information and communication technology\nInformation and communications technology (ICT) changes the extent to which organizations have to be located\nin a particular place, with all \u2013 or almost all \u2013 the important functions occurring face-to-face in a specific location.\nEarly factories had to be built close to sources of power and if possible key raw materials and workers. Inside the\nfactories the workers themselves were closely watched and monitored. As technologies have developed many of\nthese issues have become less important. When people talk about the \u2018global economy\u2019 they often have in mind the\nways in which processes of production and manufacture can now extend across continents and time-zones in ways\nthat were not possible in the early days of industrialization\nAs the impact of technological development is realized what was previously assumed to be the only way of doing\nsomething is seen as just one possibility amongst many. At the same time the core aspects become more prominent.\nAn example of this, and one relevant to this chapter, is \u2018office automation\u2019. This became a popular idea in the 1980s\nas computer technology became more widely available and affordable \u2013 particularly with the appearance of the\npersonal computer [PC]. This information technology [IT] readily lent itself to many of the tasks associated with the\noffice \u2013 typing letters could be done more efficiently and effectively with word-processing software; filing could be\ndone using database software if the material was in electronic form; calculations and estimating could be\naccomplished with spreadsheets; some document handling could be accomplished using fax machines. The\ntechnology was seen as a solution to many problems faced by organizations \u2013 small and large \u2013 such as delays in\nsending out letters and invoices, losing important documents, staff shortages, and so on. This led some people to\nconclude that the office in its previous form would disappear. Why maintain a special space and group of staff,\nwhen all the key functions could be carried out by technology? This proved to be as false and unfounded as the idea\nof \u2018the paperless office\u2019. The outcome of office automation was a better understanding of the role and nature of \u2018the\noffice\u2019 in organizational life. It was not simply somewhere that letters got typed, and papers got filed; but the site of\nmany other activities, many of which were essential to the smooth running of the organization. The office was a\nspace where people met colleagues and engaged in informal discussions, where rumours and gossip were\nexchanged, and so on. The office was not simply a place but the location for a whole range of processes and\ninteractions. The introduction of IT and other technology changed people\u2019s ideas about what actually went on at\npeople\u2019s place of work, in many cases altering or challenging long-held the assumptions.\n242 The impact of IT since the 1980s on administrative and secretarial type activities has led to a dramatic\nrestructuring in many organizations. Some of the core functions remain, but with an altered emphasis as a result of\nincorporation of new technology and new ideas about the role and purpose of these functions and activities.\nA similar pattern has come about in many other aspects of contemporary organizations. An increasing range of\norganizational activities are now bound up with ICT. This has meant that many aspects regarded as essential to the\nsmooth operation of an organization have become topics for discussion and re-evaluation. In some cases these\nreconsiderations apply to specific types of organization; but in many cases they have a far wider and more general\nscope.\nThe general issues develop from the potential for technologies, particularly but not only ICT, to allow a far wider\nrange of options for an organization to exist and function. An early example was the way in which factories reduced\nthe levels of their inventories \u2013 i.e. the raw materials or components needed to produce their finished product. The\nearliest pioneers of this were in Japan, where manufacturers were encouraged to aim for \u2018zero inventory\u2019. In other\nwords goods delivered to the manufacturing site were not booked in to stock areas, waiting to be used at a later date\n\u2013 and so consuming space, effort and money. Instead the delivered items were immediately sent to production and\nmanufacture on a \u2018just in time\u2019 [JIT] basis. For JIT to work in practice, there needs to be a fairly accurate model of\nmaterial requirements, together with reliable and responsive communications links between the manufacturer,\nsuppliers and transport (what is now often referred to as \u2018logistics\u2019). Without these just-in-time is in danger of\nbecoming just-too-late. But if such facilities can be assured then the potential cost savings and efficiency gains can\nbecome a reality.\nIn the 1970s when these ideas were in their infancy the relevant technology was fairly low-level compared to\nwhat is now potentially available \u2013 although not universally widespread. The Internet, particularly email, real-time\ncommunications, mobile phones, on-line tracking, developments in e-commerce and the like have all had an\nimpact. Initially this impact was centred on the supply and production part of the value chain, but more recently it\nhas also affected the consumer. This has been described by Sviokla and Rayport as the move from \u2018market-place to\nmarket-space\u2019.\nSviokla and Rayport argue that value for consumers is created by three components which are usually found\ntogether, but which with the development of the internet and e-commerce have become distinguishable. Moreover\norganizations can position themselves to focus on one or two, rather than all three. The three components are;\ncontent \u2013 what is offered; context \u2013 the form in which it is offered; infrastructure \u2013 how it is delivered or\ndistributed.\nThey offer as an example a newspaper. Until recently the first two value components were tightly bound\ntogether. The organization that produced the paper was also responsible for printing multiple copies and delivering\nthem somewhere from where they could be sold to the readers. In some cases the final part of the logistics \u2013 the\ndelivery \u2013 involved it arriving at the consumer\u2019s house in a more-or-less readable condition. With the advent of the\nInternet, email, RSS feeds and a whole host of other alerting and delivery possibilities all three components have\nbecome far more flexible. A consumer can still purchase a newspaper in the traditional manner, but there are also\nother options including paying for an on-line service by an internet service provider, receiving an email with the\ndocument attached ready for printing, headlines and extracts sent to one\u2019s mobile phone and so on. Here again is\n243 an example of the way in which ICT and related technologies dismantle existing structures and open up new\npossibilities.\nAn interim summary\nSo as we move towards the latter part of the first decade of the 21st century we are all bound up with a\nmovement that has, amongst other effects, resulted in a dismantling of the ways in which organizations need to\noperate. They no longer need to be situated in a single location, their routine operations can be tightly linked to\nother organizations, and aspects of their routine existence such as division of labour and management are open to\nseveral possible alternative forms.\nIn manufacture this can be most widely understood as an increasing number of manufacturing processes are\ndismantled and spread across the globe in what is one of the primary and most visible forms of \u2018globalization\u2019. The\nglobal economy is a complex concept, but one of its key characteristics is the way in which a finished product\navailable for sale in a shop in, for example, the USA or Western Europe has passed through a series of stages of\nmanufacture and packing that may have taken in factories and warehouses in Asia, Africa, and Eastern Europe.\nAt an abstract level these sorts of developments have been widely discussed and investigated under such\nheadings as \u2018the virtual organization\u2019 and \u2018computer supported cooperative work\u2019 [CSCW] \u2013 both terms originating\nin the 1980s. In the commercial domain virtual organizations and CSCW have taken on the form of \u2018out-sourcing\u2019 or\n\u2018off-shoring\u2019 both or goods and services. Some people have welcomed these developments on the grounds that they\noffer employment and development opportunities (both individually and more generally) to areas that previously\nhave been deprived and under-developed. Others criticize such moves as simply perpetuating dependency and\nunder-development, since these strategies are all-too-often driven by the aim of cutting costs and so involve child\nlabour, very low wages, and poor and dangerous working conditions. A further criticism from developed countries\nis that such practices move jobs away from developed economies, and so deprive employment opportunities\nto lower-skilled people in those countries.\nVirtual organizations and CSCW have also had an impact in the non-commercial sectors \u2013 affecting NGOs, civil\nsociety organizations [CSOs], community organizing and many other forms of collective activity. These issues will\nbe discussed further in the later sections of this chapter. For the moment it needs to be understood that all the\ndevelopments discussed so far lead to the necessity to rethink the \u2018value chain\u2019, the role of management,\norganizational forms, the division of labour, the nature of competition and cooperation.\nNew Models of Organization\nIn response to the developments discussed above people have started to question traditional models of the\norganization, management and associated concepts. Two concepts in particular are worthy of further analysis - the\nvirtual organization, and the open source model: Each in its own way demonstrating opportunities for new\norganizational forms.\nVirtual Organizations \u2013 Outsourcing and Off-shoring\nA virtual organization is one that exists very much along the lines of the earlier example of two friends getting\ntogether with a very specific and limited objective; usually a single project. To an extent virtual organizations have\nalways existed, but only with the advent of the internet have they really flourished. Virtual organizations are virtual\nin that they do not exist as organizations in the formal sense, but they do exist in the sense of having an on-line\nexistence. In many cases commercial virtual organizations \u2013 or virtual enterprises \u2013 were the most visible examples\n244 of this trend in the 1990s. In these cases several already existing commercial organizations would collaborate on a\nspecific project, often an exploratory one or one in which some new idea or product required development and\ntesting. These early forms of virtual organization were largely established as loosely linked alliances; with little or\nno formal structure or hierarchy. In terms of division of labour, there was certainly a horizontal form, but little or\nno vertical form. The emphasis was on trust and collaboration, rather than command and control.\nIn the period since the 1990s many organizations have emerged that are part-virtual, part-actual; making use of\nmodels centred on a strategy of \u2018out-sourcing\u2019 or \u2018off-shoring\u2019. These organizations grow from existing, traditional\nforms, taking advantage of the significant developments in ICT that allow cheap, fast, and reliable forms of\ncommunication and monitoring. A company that previously manufactured something from start to finish in one\nlocation may now be dismantled so that some of the tasks are completed elsewhere \u2013 in a different state or even in a\ndifferent continent. This may be done for a variety of reasons; perhaps the initial raw materials or components are\nmore readily available, and cheaper, elsewhere: Or a particular task or process is done more efficiently and\neffectively by a specialist company. In many cases out-sourcing or off-shoring is seen as a way in which commercial\norganizations can reduce their labour costs, locating some or all of their core labour-intensive processes where the\nworkforce is less expensive to employ and retain. Thus some aspects of the organization's activities become virtual,\nwith links, collaborations and associations often being temporary or on an 'as needed' basis. For instance Wal-Mart,\nNike and Dell have grown using a strategy of being 'highly decentralized' or 'highly distributed'; they exist physically\nand over time in the sense that they have headquarters, offices, depots and so on, but in other regards many of their\noperations are 'virtual'.\nAt one extreme outsourcing simply becomes a form of extension of a traditional organization. All the key aspects\nremain, but the core processes are undertaken in a slightly modified form. So the chain of activities and processes\nleading to the finished product are dispersed across the globe. Nike was one of the first companies to use this as a\ncentral part of their organizational operation, locating almost all of the production of their sports and leisure wear\nto countries outside the US. The actual number of people directly employed by Nike was always very small given the\nsize and turnover of the company; with most of these employees being based in the United States.\nDell adopted a customer-directed business model, essentially reducing its inventory costs by assembling\ncomputers on demand, and only accepting components for delivery and payment when actually needed for\nassembly. This allowed the company to take payment for its finished products before they were actually assembled,\nso avoiding the costs of paying for the components in advance and then waiting for the orders. They were able to do\nthis because they quickly saw the ways in which the internet could be used to re-engineer many of the core\norganizational and management processes. Thus they were able to achieve co-ordination of the entire process from\nmanufacture to assembly to delivery. They could tie-in suppliers so that supplies were only delivered as needed.\nMoreover, since Dell only sold directly to customers, they were able to use this relationship to promote follow-on\nsales to their customer base.\nThe ways in which an organization such as Dell or Nike operates can be outlined using the concepts introduced\nearlier. In terms of the horizontal division of labour, the range of activities and processes involved occur along the\nsame lines but can now be located where raw materials, labour costs, economies of scale, specialized skills or other\nfactors can be optimized. The internet offers the rapid communication infrastructure for co-ordinating and\ncontrolling these activities. Companies that have pioneered and taken advantage of these potentials include Toyota\nand Wal-Mart. In recent times Wal-Mart and Nike, amongst others, have been heavily criticized for the ways in\n245 which they have implemented their particular forms of outsourcing. These criticisms include using child labour and\nother forms of non-unionized cheap labour particularly in third world countries, flouting environmental legislation,\nand selling at below-cost prices in order to drive other, smaller companies out of business. These criticisms and\nrebuttals from Nike, Wal-Mart and others are all well documented on various websites \u2013 Wikipedia is a good place\nto start if you wish to look into these issues in further detail.\nIn this chapter these arguments can be put to one side. Whatever the rights and wrongs of the ways in which\nthese commercial companies have taken advantage of technological advances, they clearly demonstrate the\npotential for ICT to facilitate new forms of organization. This can be seen by a fairly simple reconsideration of\nFayol\u2019s five functions of management and how they have been affected by ICT;\n\u2022 Planning; this can now be undertaken on a more-or-less continuous basis. Increasingly organizations are\nfocusing on far shorter-term planning for at least two reasons\n\u2022 the global context is changing so quickly that long-term planning will be at best only of limited value\n\u2022 communicating planning decisions can be done quickly and effectively, and so can any new plans or\nchanges in direction. It should be noted, however, that this does not necessarily mean that planning\nis any more accurate or effective than it was previously \u2013 the concluding section of this chapter\nconsiders this in more detail.\n\u2022 Organizing; the ways in which tasks, people and groups need to be structured can now be established and\nthen changed quickly and easily. As will be explained briefly below, organizations can now function as\nflexible networks, altering their patterns of communication and interaction to take best account of changing\ncircumstances, priorities and opportunities.\n\u2022 Co-ordinating; The model of Just-in-Time production can now be extended to services as well as\nmanufacturing activities, since ICT allows fast and efficient communication. The models used by Toyota,\nWal-Mart and many other companies rely on co-ordination of a whole range of core and support activities\ndispersed across the globe. In recent years these models have themselves come in for intense criticism, and\nsome of the pioneering organizations such as Wal-Mart and Dell have sought to make significant changes in\nthe ways in which they organize and co-ordinate their core processes \u2013 see the concluding section.\n\u2022 Deciding; decision-making is often thought to rely on information to the extent that more information is\nlikely to produce better \u2013 i.e. more accurate \u2013 decisions. This has led to a demand for real-time updating of\nmany aspects of an organization\u2019s activities. This does not always lead to better decisions.\n\u2022 Controlling; Based on the points about the other four functions, the ability to control widely dispersed but\npotentially well co-ordinated activities should be well established. ICT certainly allows this, although other\n\u2013 non-technical \u2013 issues can over-ride this.\nOpen source and related models\nWal-Mart and Nike can be seen as examples of firms that have established themselves using novel forms of\norganization and co-ordination, but still centred on what has been termed a command-and-control model. This\nmodel is most often associated with military organizations and also with governmental ones, where there is a\ncentralized structure and all communications and decision-making goes through this centre. One of the weaknesses\nof such an approach is that if anything happens to the central command, or to its ability to communicate with the\n246 periphery, then the entire organization can be prevented from operating. In the 1960s the US Department of\nDefense identified precisely this weakness in its reliance on a centralized structure, and so encouraged the\ndevelopment of what became known as ARPANET \u2013 a network of inter-related computer-based communication\nsites. The technology at the heart of ARPANET became the basis for what we now know as the internet \u2013 a network\nof networks. It is this model and also this technology that has prompted and promoted new forms of organization\nthat have moved away from \u2013 or beyond \u2013 the command-and-control structure.\nThe most visible form of this model is probably Wikipedia and the various other forms of Wiki now available.\nBut one of the key building blocks was the open source software movement. We now all rely on software in a whole\nhost of various guises. Software is produced by people who often identify themselves as software engineers, and in\nthe earliest days (1950s) of software development large-scale software was often seen in terms of a major\nconstruction project requiring a command-and-control approach to its project management. But since at least the\n1970s there have been software engineers who have sought a different approach. This was outlined by F.P. Brooks\nin 1986 when he argued that instead of thinking about software as something to be built, it might be more useful to\nthink in terms of growing or cultivating software. Large complex software systems could then be seen as things that\ngrew, incrementally in stages, rather than as one-off large-scale constructions. By arguing in this fashion Brooks\nwas countering the view of software development as a large-scale engineering project \u2013 almost inevitably managed\nin a command-and-control manner; instead seeing it as far more untidy and disorganized.\nThis idea of cultivating software was linked with the idea of developers contributing their efforts for the common\ngood rather than on the basis of some commercial contract or agreement. The idea might not have developed much\nbeyond the confines of this very specialized group without the contribution of Linus Torvalds. Torvalds at the time\n(1991) was a graduate student who had developed his own operating system, Linux, as part of his studies. He\nallowed anyone who wished to do so to contribute to this system, revising and enhancing it. From this basis the\nLinux operating system has flourished into the open source model of development and collaboration. If Torvalds\nwas the initial driving force behind the model, Eric Raymond must be regarded as its chief advocate, supplying the\nopen source manifesto in his paper contrasting the cathedral with the bazaar.\nFor Raymond the image of a cathedral is a model that conjures up a vast, complex structure developed by people\nwith near-magical skills and powers; Raymond even calls them wizards. These people work in \u2018splendid isolation\u2019\ndeveloping a product that needs to be completed and fully guaranteed or secured prior to its \u2018release\u2019. Raymond\ncontrasts this to \u2018a great babbling bazaar of differing agendas and approaches \u2026 out of which a coherent and stable\nsystem could seemingly emerge only by a succession of miracles\u2019. Raymond specifically centres his writings on\nsoftware development, more specifically on software debugging \u2013 the process of locating and fixing problems in\nsoftware-based systems: A process that is truly endless in all commercial systems. The cathedral model relies on a\nsmall group of proficient developers working in splendid isolation, only releasing their software to users after\nextensive and thorough testing \u2013 all of which takes time and effort. In stark contrast stands the bazaar-like model,\nwhereby disparate groups and individuals with differing agendas and approaches somehow produce a coherent and\nstable outcome.\nThe Linux philosophy is encapsulated in Linus Torvalds\u2019 philosophy as stated by Raymond \u2013 \u2018release early and\nrelease often; delegate everything you can, be open to the point of promiscuity\u2019. The result ought to be chaotic and\nanarchic, a hotchpotch of different versions of software, proliferating to the consternation of developers and the\ndespair of users and customers. Yet precisely the opposite has occurred. Linux has survived, thrived and continues\n247 to flourish. Moreover the model has been used and adopted with regard to other activities, although Raymond and\nmany others associated with the open source software community are very hesitant about such claims and\ndevelopments.\nFor the purposes of this chapter the bazaar model indicates that it might be possible to undertake even fairly\ncomplex development projects without having to resort to command-and-control policies; instead relying largely on\nvoluntary and collaborative involvement. To an extent in such cases there is a level of control and management, but\nit is carried out in a more distributed and semi-autonomous fashion. People take on tasks and responsibilities\nbecause they feel motivated to contribute and exchange their views, their ideas and their efforts. The development\nof open source software \u2013 specifically Linux \u2013 has come about on a model based on several principles that at first\nsight ought not to prove effective or successful; but they have, and they do. The question at this point is can such\nprinciples of operation be applied to organizations in 21st century?\nOne of the key issues, and one that comes through very clearly in Raymond\u2019s description of his own participation\nin the open source community, is that involvement must at least owe some of its initial impetus to the motivation\nand enthusiasm of the participant. Peter Drucker is credited with the statement that \u2018in the knowledge society we\nare all volunteers\u2019. This might appear strange to most of the working population who are certainly not engaged in\ntheir particular employment voluntarily. In fact Drucker\u2019s saying is more insightful in its complete form: \u2018Everyone\nin the knowledge economy is a volunteer, but we have only trained our managers to manage conscripts\u2019. For our\npresent purposes the key points are developed by Drucker himself in an article for Forbes Magazine in the late\n1990s.\nWhat motivates workers -- especially knowledge workers -- is what motivates volunteers. Volunteers, we know,\nhave to get more satisfaction from their work than paid employees precisely because they do not get a pay check.\nThey need, above all, challenge. They need to know the organization's mission and to believe in it. They need\ncontinuous training. They need to see results. Implicit in this is that employees have to be managed as associates,\npartners -- and not in name only. The definition of a partnership is that all partners are equal. It is also the\ndefinition of a partnership that partners cannot be ordered about. They have to be persuaded. (Drucker, 1998)\nThis is clearly at the opposite end of the spectrum from commercial organizations such as Dell, Nike and Wal-\nMart. Perhaps these companies would like to think that concepts of partnership, challenge and the like ought to\nmotivate workers; but it is all too obvious that this has little relevance to the vast majority of people\u2019s experience.\nOn the other hand Raymond\u2019s characterization of the open source model demonstrates many of the features to\nwhich Drucker refers. The participants are motivated by the challenge, and the capacity to act as partners. A similar\nmotivational mix can be demonstrated by many organizations operating in the voluntary and civil society sectors \u2013\nincluding many NGOs.\nIt should be noted that this model is really only possible given the existence of the internet. Without such\nresilient, extensive, and virtually effortless communication the open source community simply would not have been\nable to develop as far as it did. The arrival of Linux coincided with the point at which the Internet, as a key\ncomponent of everyday life for a significant and rapidly growing proportion of people, was taking up its central role\nas the communications technology par excellence. The bazaar model, with its \u2018great babbling ... of differing agendas\nand approaches\u2019, demands constantly available and extensive means of communication and co-ordination.\nRaymond recognizes this in asserting that: \u2018Provided the development coordinator has a communications medium\n248 at least as good as the Internet, and knows how to lead without coercion, many heads are inevitably better than\none.\u2019 The importance of the internet is not that it allows individuals to talk to each other on an individual basis, but\nthat it affords a forum for exchange and co-operation; with both asynchronous and synchronous interactions.\nIt should also be noted that this grouping might include co-developers, suppliers, users and advisers. In fact\nanyone who feels that might have some thing to contribute to the specific task or project. In this case the\norganization is virtual not merely in the sense of being on-line and taking advantage of ICT, but in the sense of not\nreally existing. Outsourcing and other forms of virtual organization still have an existence as organizations, but at\nthis end of the scale a bazaar-like organization may well have only a fleeting existence for as long as the project or\ntask is relevant.\nA further look at Fayol\u2019s five functions indicates how the open source model is distinctive \u2013 at least in its most\nextreme form.\n\u2022 Planning; any planning is restricted to the extremely short-term.\n\u2022 Organizing; this is most definitely accomplished in an informal and bottom-up fashion \u2013 similar in some\nregards to the very small, informal organization mentioned at the start of this chapter.\n\u2022 Co-ordinating; again this is done on the smallest of scales, based on direct interaction with other members.\n\u2022 Deciding; there is no single, central decision-making; individuals make decisions and these then contribute\nto the overall development of the loose alliance,\n\u2022 Controlling; there is no single point of control.\n\u2022 Three of Raymond\u2019s maxims, taken together, summarize the position. \u2018The next best thing to having good\nideas is recognizing good ideas from your users. Sometimes the latter is better.\u2019 [#11] \u2018Often, the most\nstriking and innovative solutions come from realizing that your concept of the problem was wrong.\u2019 [#12]\nAnd even more insightfully he states that \u2013 \u2018Given a large enough beta-tester and co-developer base, almost\nevery problem will be characterized quickly and the fix obvious to someone.\u2019 [#8]\nTaken together these offer an outline of a form of organization that is best seen as a loose alliance of interested\nand motivated people, acting together in ways that may achieve both expected and unexpected purposes and\nresults.\nAn example of this model, with which you may be familiar, is the Wiki, particularly in the form of Wikipedia.\nThe term Wiki seems to have several meanings and derivations. The word itself means \u2018quick\u2019 or \u2018fast\u2019 in Hawaiian,\nand the slogan WikiWiki is apparently used by the shuttle bus company at Honolulu International Airport. It is also\nclaimed that Wiki is an acronym for \u2018What I Know Is\u2019. Hence the term has come to denote collaborative efforts\nwhere people come together to pool their knowledge and expertise with a minimum of fuss and formality. In many\nregards the Wiki principles are more easily understood from stating what the Wiki movement is not, rather than\nwhat the Wiki movement actually is. Hence the following headings from the Wikipedia entry on Wikipedia itself;\n\u2022 What Wikipedia is not\n\u2022 Wikipedia is not a paper encyclopedia\n\u2022 Wikipedia is not a dictionary\n249 \u2022 Wikipedia is not a publisher of original thought\n\u2022 Wikipedia is not a soapbox\n\u2022 Wikipedia is not a mirror or a repository of links, images, or media files\n\u2022 Wikipedia is not a free host, blog, or webspace provider\n\u2022 Wikipedia is not an indiscriminate collection of information\n\u2022 Wikipedia is not a crystal ball\n\u2022 Wikipedia is not censored for the protection of minors\n\u2022 What the Wikipedia community is not\n\u2022 Wikipedia is not a battleground\n\u2022 Wikipedia is not an experiment in anarchy\n\u2022 Wikipedia is not a democracy\n\u2022 Wikipedia is not a bureaucracy\nIn fact the Wikipedia organizational model relies on a sufficient number of people feeling motivated and\nenthused to contribute and participate; exactly the same prerequisites that Drucker identifies for volunteers or\nassociates, and that Raymond describes for the Linux participants. Moreover the Wiki philosophy is best seen as\ncultivation as opposed to construction. The Wiki model and the open source model share a view of organization that\nis chaotic and disordered, but also more extensive, more accessible, more visible, and more speedily updated and\ncorrected than standard command-and-control centralized organizations.\nMany of you will be familiar with Wikipedia, but there are numerous other forms of 'on-line bazaar', with people\ncontributing voluntarily - you can probably name several, and may well already be signed up to sites such as\nMySpace and Facebook.\nCritique and Conclusions\nThe range of possible opportunities for new organizational forms includes both those devised around the\nbusiness models of Dell, Nike, and Wal-Mart amongst others, and the open source and Wiki approaches. The\ncriticisms of Wal-Mart\u2019s approach have already been hinted at, and in recent years the ability of these approaches to\ncontinue to deliver benefits, returns and profits in a consistent and reliable manner has been brought into question.\nOnly recently Dell has gone through a dip in performance and a crisis in its organizational structure and business\nmodel. Similarly Wal-Mart has begun to change its model of operations and focus less on cost-savings and more on\ncustomers.\nOn the other hand the bazaar model is also open to criticism and re-evaluation. Raymond himself makes a key\npoint about the bazaar model: \u2018one cannot code from the ground up in bazaar style\u2019. In other words, for the bazaar-\nmodel to work something, perhaps more cathedral-like, must already be in existence. This can be illustrated with\nregard to the development of Wikipedia and Wikis in general. They could only develop once the internet was a\nreliable and accessible reality. Moreover in the years since they first emerged the Wiki model has undergone various\ndevelopments which take it away from total reliance and commitment to a bazaar of multiple babbling agendas.\nThere is now a form of hierarchy within the Wikipedia community, effectively a horizontal and vertical division of\n250 labour, although it is far less formal and more fluid. Similarly developing features may be found within the open\nsource community.\nSo it is important to note that these new \u2018opportunities\u2019 require careful thought and reflection. This is not to\nimply that the command-and-control model is immune from precisely the same criticisms. On the contrary,\ndevelopments such as outsourcing and open source ventures demonstrate that there are good grounds to challenge\nthe general arguments that justify the need for command-and-control management. Raymond goes even further in\nstressing that the overheads required for these forms of management cannot be justified; they do not even deliver\nwhat they are meant to do.\nIn the discussion of office automation it was noted that technological advances often result in undermining what\nwas previously assumed to be the only way of doing something; opening out other possibilities or opportunities: So\ntoo with the development of ICT in general and the internet in particular with regard to organizations and\nmanagement. The context now is that there needs to be recognition that the functions identified by Fayol, and the\nroles described by Mintzberg and other management and organizational theorists, all need to be re-evaluated in\nthis new context. Some indication of this has already been given with regard to Fayol\u2019s functions.\nThe opportunities for new organizational forms made possible by ICT provide a basis for innovation and also for\nre-evaluation of traditional and accepted ideas about organization and management. Raymond, despite arguing in\nsome places against the application of the open source, bazaar-like model to contexts other than software\ndevelopment, clearly sees the open source experience as offering a glimpse of a new way of organizing. He ends his\nclassic paper with a quote from the 19th century Russian anarchist Pyotr Alexeyvich Kropotkin:\nHaving been brought up in a serf-owner\u2019s family, I entered active life, like all young men of my time, with a\ngreat deal of confidence in the necessity of commanding, ordering, scolding, punishing and the like. But\nwhen, at an early stage, I had to manage serious enterprises and to deal with [free] men, and when each\nmistake would lead at once to heavy consequences, I began to appreciate the difference between acting on\nthe principle of command and discipline and acting on the principle of common understanding. The former\nworks admirably in a military parade, but it is worth nothing where real life is concerned, and the aim can\nbe achieved only through the severe effort of many converging wills.\nWhat ICT offers are various ways in which command-and-control models might be realized on a global scale, but\nalso a host of possible alternatives and opportunities.\nSummary\nThis chapter has sought to outline some central issues for an understanding of the ways in which organizations\nhave developed, and the impact that developments in ICT in particular have had; causing people to investigate and\nquestion existing ideas and assumptions, challenging these with innovative ideas. The net result is that to some\nextent the nature of an organization is now more clearly understood, while in other respects the range of\npossibilities for organizational development and continuity has grown.\nChapter editor\nAntony Bryant\nReferences\n251 14. Information systems\nsecurity\nEditor: Gurpreet Dhillon (Virginia Commonwealth University, VA, USA)\nLearning objectives\n\u2022\nBackground\nThe security of information systems has always held a relevant position in CIO\u2019s agendas. However, in the past\nyears, information security related issues have been brought to the public fore as a consequence of media attention\nto such incidents as the collapse of Barings Bank, Enron and WorldCom, and the security lapses at ChoicePoint,\nBank of America, T-Mobile, and LexisNexis. The consolidation of IS security as an important topic in today\u2019s\nbusiness world results from the interaction of several technological and social factors.\nFirst has been the increased dependence of individuals, organizations, and societies on information and\ncommunication technologies. As individuals, we rely on these technologies to execute a wide spectrum of tasks,\nfrom communicating with other people, improving our job performance, accessing various sources of information,\nbooking flights, or buying a book. For organizations, information and communication technologies are not only a\nmajor component of basic operational systems and an enabler of productivity improvements, but also a means for\ngaining competitive advantage, developing new businesses, and promoting new management practices. As a\nsociety, it is enough to consider the role played by these technologies in the working of critical infrastructures, from\ntransportation to energy supply and financial services, as well as in the provision of public services to citizens and\ncompanies.\nSecond, resulting from the exploitation of information and communication technologies\u2019 capabilities in the\nbusiness arena, the whole business model for many organizations has been transformed. In the past, companies\ncould rely on staying in a particular geographical area to conduct their activity. However, developments such as\nglobal scale interconnectivity, distributed processing, explosive growth of the Internet, open architectures,\nliberalization of telecommunication markets, and e-commerce diffusion have dramatically changed the business\nlandscape. Today, employees experience increasing mobility, which results in the need to access information by\ndiverse means, in different situations, and from distinct places, many of these outside their own organization. As an\nimplication of this increasing location independence, companies are finding themselves to be strategically\ndisadvantaged if they are confined to a particular place.\nAdvances in information technologies and the changing boundaries of the firm have stressed the importance of\ninformation. It is information that helps companies realize their objectives, keeping them in touch with their\nenvironment, serving as an instrument of communication, helping managers to take adequate decisions and\nproviding support for the exchange of employee knowledge (Chokron and Reix 1987).\n252 In the past, information to a large extent was confined to a particular location and it was relatively easy to\npreserve its confidentiality, i.e. restricting access to those authorized. Because information was usually processed in\na central location, it was also possible, to a reasonable level of certainty, to preserve its integrity, i.e. ensuring that\nits content and form were not subject to unauthorized modification, as well as maintaining the availability of\ninformation and related resources, i.e. preventing their unauthorized withholding.\nMaintaining confidentiality, integrity, and availability were the three main goal of managing security. Today,\nconsidering the transformed nature of organizations and the expanded scope of information processing, managing\ninformation security is not just restricted to preserving confidentiality, integrity, and availability. The emphasis\nshould move to establishing responsibility, integrity of people, trustworthiness, and ethicality (Dhillon and\nBackhouse 2000)\nYet, the protection of IS assets is not an easy task. There is often a lack of a unique and well-defined purpose to\nprotecting these assets. Besides the preservation of confidentiality, integrity, and availability, among other\nimmediate goals that an organization may pursue in securing its information system may be the maintenance of\nprivacy of the data related to their employees, customers, and partners; the minimization of the effects resulting\nfrom the dependence on non trustable or non reliable systems and entities, and the resilience to technological\nmalfunctions (Neumann 1994b).\nThe achievement of many of these goals raises significant difficulties for organizations because they may be\nconflicting. An organization must be closed to intrusions, fraud, and other security breaches, and at the same time\nit needs to remain open in order to share information with its partners and customers (Erwin 2002). As well firms\nface \u2013the continuous development of new forms of attacks, the discovery of new vulnerabilities in technologies and\nbusiness processes, and the increased need for organizational flexibility.\nIn order to manage IS security, organizations have to encompass a broad set of factors, ranging from the\ntechnical ones, to the consideration of the business environment, organizational culture, expectations and\nobligations of different roles, meanings of different actions, and related patterns of behavior. This means that IS\nsecurity can be understood in terms of \u201cminimizing risks arising because of inconsistent and incoherent behavior\nwith respect to the information handling activities of organizations\u201d (Dhillon 1997, p. 1). These inconsistencies and\nincoherencies in behavior may lead to the occurrence of adverse events. Besides losses from natural causes, such as\nfires and floods, the majority of adverse events can be traced back to deliberate or non-deliberate inappropriate\nbehavior of individuals, whether in the form of human error, systems analysis and design faults, violations of\nsafeguards by trusted personnel, system intruders or malware, such as viruses, worms and Trojan horses (OTA\n1994).\nIn order to prevent, detect, and react to the occurrence of these events, organizations may apply a set of\nmeasures usually know as security controls. Because information handling in an organization can be undertaken at\nthree levels \u2013 technical, formal, informal (Liebenau and Backhouse 1990) \u2013 information systems security can be\nachieved only by coordinating and maintaining the integrity of operations within and between those three levels\n(Dhillon 2007). This implies that an organization should adopt a holistic posture in managing IS security, namely\nby implementing a set of security controls that as a whole support the integrity of the organization\u2019s IS.\nAt the technical level, an organization may adopt security controls such as anti-virus software, firewalls,\nintrusion detection systems, access control devices, and cryptographic controls.\n253 To be effective, the deployment of technical controls requires adequate organizational support. Consequently,\nformal controls need to be put in place. These controls take the form of rule-based formal structures that assist on\ndetermining how specific responsibilities are allocated and define the consequences of misinterpretation of data\nand misapplication of rules. Security policies, structures of responsibility and contingency plans are examples of\nformal controls.\nThe previous two levels need to conform to the normative schemes prevalent in the organization. At the informal\nlevel, measures such as awareness programs, adoption of good management practices, and development of a\nsecurity culture that fosters the protection of information assets are illustrative of security controls.\nIn recent years organizations have fallen short of developing adequate security controls to deal with information\nsecurity problems. Various studies have reported significant losses in explicitly reported security breaches (Garg\n2003; Gordon et al. 2006) and as a consequence of computer crimes because of violation of safeguards by internal\nemployees of organizations (Dhillon 1999a). Not only are organizations suffering from a \u2018policy vacuum\u2019 to deal\nwith information security problems, as well authorities have been experiencing a certain inability to establish an\nadequate basis to deal with such cyber crimes.\nConsider the case of Randal Schwartz, a well known programmer and author of programming books, in which it\nwas difficult to establish whether illicit use of computers by Schwartz amounted to a computer crime (Dhillon and\nPhukan 2000). In 1995, Schwartz was brought to trial for illegally bypassing computer security in order to gain\naccess to a password file while working as a consultant for Intel. According to Schwartz, he was only trying to show\nthat Intel employees were selecting weak passwords that could be easily guessed by crackers who then could\ncompromise information security. Schwartz was convicted on three felony counts, but in 2007 his arrest and\nconviction records were sealed through an expungement action.\nAdvances in information technologies have introduced another kind of problem for organizations which many\nclassify as \u2018input crimes\u2019 (Dhillon 1999b). In one case, a former employee of a wholesaler was convicted under the\nUK Computer Misuse Act when he obtained for himself a 70 percent discount when the regular staff of the\nwholesaler was otherwise engaged.\nGiven the increased dependence of companies on information systems, one would assume that most firms would\nhave well established contingency and disaster recovery plans. Unfortunately research seems to suggest otherwise\n(Adam and Haslam 2001). Many managers tend to think that contingency and disaster recovery planning is an\nirrelevant issue and hence prefer to concentrate on projects that generate direct revenues.\nIt emerges from the prior discussion that IS security management is a complex task that poses a number of\nchallenges for maintaining the integrity of information handling activities in an organization. The challenges\nIn a climate where incidents of computer crime, information security problems, and IS enabled frauds have been\non the increase, any attempt to deal with the problem demands an adequate understanding of the four challenges\nthat organizations must confront, namely\n\u2022 Establishing good management practices in a geographically dispersed environment and yet being able to\ncontrol organizational operations.\n\u2022 Establishing security policies and procedures that adequately reflect the organizational context and new\nbusiness processes.\n254 \u2022 Establishing correct structures of responsibility, given the complex structuring of organizations and\ninformation processing activities.\n\u2022 Establishing appropriate contingency plans.\nSeveral authors such as Dhillon (1997), Dhillon et al. (2004) and Siponen (2001) have noted the There is a major\nproblem in managing information security, especially with respect to regulating the behavior of internal employees\n(Dhillon 1997; Dhillon et al. 2004; and Siponen 2001). Internal employees frequently subvert existing controls to\ngain an undue advantage because either an opportunity exists or they are disgruntled (Audit Commission 1994;\nBackhouse and Dhillon 1995). This problem gets compounded even further when an organization is geographically\ndispersed, and it becomes difficult to institute the necessary formal controls. This was evidenced in the case of Nick\nLeeson, who brought about the downfall of Barings Bank in Singapore. Barings collapsed because by its reliance on\ninformation technology for IS security, Leeson was able to successfully conceal the positions and losses from the\nBarings management, internal and external auditors, regulatory bodies in Singapore, and the Bank of England.\nLeeson\u2019s case is illustrative of breaches of control, trust, confidence, and deviations from conventional accounting\nmethods or expectations.\nThe management of Barings had confessed in internal memos that clearly its systems and controls were\ndistinctly weak. However, there was nothing new in this confession, and it has long been known that lapses in\napplying internal and external controls are perhaps the primary reason for breaches in information security (Audit\nCommission 1990; 1994). Failure of management to curtail Leeson's sole responsibilities, which empowered him to\ncreate an environment conducive to crime, lack of independent monitoring and control of risk, communication\nbreakdown between managers, and the believe that IS can overcome basic communication problems in organizations\nwere other reasons that created an opportunity for Leeson to deceive many.\nThere is also the challenge of establishing appropriate security policies and procedures that adequately reflect\nthe organizational context and new business processes. This challenge is present at two levels. First, at an internal\norganizational level, businesses are finding increasingly difficult to develop and implement appropriate security\npolicies. Second, at a broad contextual level, it is becoming less effective to rely on traditional legal policies to\nregulate behavior.\nAt an internal organizational level, there is a problem with respect to establishing security policies. This problem\nstems directly form a general lack of awareness within organizations that such a need exists. Based on a\nlongitudinal study of information security problems within the health services sector and the local government\ncouncils, Dhillon (1997) advances two reasons that explain this state of affairs. One of the reasons is the lack of\ncommitment from top management in the security policy formulation process. The other reason is that security\npolicies are conceived in a formal-rational manner. Indeed, the assessment of security problems is characterized as\n\u2018acontextual\u2019 and the organizational responses to address the security issues are at the best superficial.\nAt a broad contextual level, although a number of regulations have been enacted in recent years, sometimes\ntheir nature and scope seems to be at odds with the reality. Clearly there are a number of situations where it is\nimportant to institute punitive social controls in order to curtail criminal activities and in some cases to recover\nstolen money or goods. There are perhaps a number of other computer crimes where severe punitive control may\nnot be the best option. In many cases monetary gain is not the prime motive, but the intellectual challenge of\n255 tearing apart computer systems. In such cases it would perhaps be counter-productive to institute severe punitive\ncontrols.\nAnother challenge in managing information system security is the establishment of correct structures of\nauthority and responsibility. The inability to understand the nature and scope of such structures within\norganizations or to specify new ones aligned with organizational routines and goals are a source of information\nsecurity problems. One example of this kind of problems comes from Daiwa Bank. When this Japanese bank fell\nshort of understanding the patterns of behavior expected of businesses operating out of the USA and allowed\nJapanese normative structures to dominate, it resulted in a bond trader, Toshihide Iguchi, accruing losses to the\ntune of $1.1 billion. At the same time, it also allowed Iguchi to engage in at least 30,000 illicit trades. The drama\nended in Iguchi being prosecuted and Daiwa\u2019s charter to conduct business in the USA being suspended.\nSituations such as the one illustrated by Daiwa pose a challenging issue of managing access to information\nprocessing facilities. It is insufficient to merely stating \u2018read only\u2019 or \u2018write only\u2019 accesses according to an\norganization\u2019s hierarchical structure, especially in light of the transformation in organizational forms. Modern\nenterprises are in a constant state of \u2018schizoid incoherence,\u2019 and there are very short periods of stability in\norganizational forms (Dhillon and Orton 2000). This is especially true for businesses structured in a \u2018networked\u2019 or\n\u2018virtual\u2019 manner. As a consequence of the evolving nature of organizational forms, the applicability of formal\nmethods for instituting access control is open to debate.\nThe last challenge concerns dealing with contingency plans, namely information technology disaster recovery\nplans and policies. These plans have a central place in today\u2019s technological dependent business world. However,\ntheir success is not only a function of the ability of an organization to recover its technical infrastructure capability,\nbut also of its capacity to replicate and apply business process knowledge and to reshape communications circuits\nbetween key organizational members. In other words, organizations need sound competencies in business\ncontinuity management.\nOften, disasters occur because of staff complacency . An illustrative case is the disabling of Northwest Airlines\u2019\nbackup system. The investigation of this incident showed that a sub-contractor laying new lines in Eagan,\nMinnesota bored through a cluster of cables cutting 244 fiber optic and copper telecommunications lines. Airline\npassengers nationwide were stranded since those communications lines linked the Northwest\u2019s Minneapolis-St.\nPaul hub to the rest of the nation. Situations similar to the Northwest Airlines incident are usually prevented by the\nuse of redundant lines, but apparently that airline\u2019s redundant communication lines ran alongside those used for\nbacking up its system (Lehman 2000).\nIn a 1996 survey on business continuity practices conducted by IBM, 293 of the 300 surveyed companies had\nsuffered security incidents in the previous year (IBM 1996). The estimation of loss of system capability due to these\nincidents was calculated as 500,000 man-hours. This study suggested that 89 percent of the responding\norganizations believed their computer systems to be critical. Nearly 25 percent of the companies stored 60 percent\nof their data in PCs and 76 percent were not aware of the cost of back up. A study of Irish experiences in disaster\nrecovery planning presents a similar scenario (Adam and Haslam\u2019s 2001). Even after highly publicized terrorist\nattacks, recurrent distributed denial-of-service attacks, and of the forecasts about climate change, a quarter of U.K.\ncompanies do not store backup data off-site, two-fifths have no recovery plan in place and of those that do, less than\nhalf of the plans had been tested within the last year (ISBS 2006). Overcoming this challenge gets even more\n256 complex when one observes that today\u2019s professionals, knowledge workers, can leave an organization anytime,\ntaking with them their, and the firm\u2019s, means of production (Drucker 2001).\nThe Principles\nHaving sketched the background for information systems security and advanced the main security challenges\nconfronting organizations, how should organizations proceed in the complex task of protecting their information\nassets?\nThe solution to the pressing problems of managing information security lies in shifting emphasis from\ntechnology to organizational and social process. Although this orientation has been defended by many, in practice\nthe design of over-formalized, acontextual, ahistorical and reactive security solutions still dominates. Many\nsolutions don\u2019t fit. because there is inadequate consideration of information security issues.\nAlthough there is no magic bullet to solve IS security challenges, this section presents a set of fundamental\nprinciples necessary for managing current information security issues. This management framework is composed\nof six principles, which are classified into three classes, namely:\n\u2022 Managing the informal aspects of IS security\n\u2022 Managing the formal aspects of IS security\n\u2022 Managing the technical aspects of IS security\nFollowing a brief description of each class, each principle is elaborated and suggestions regarding its\napplicability advanced.\nPrinciples for Managing the Informal Aspects\nIn the final analysis, the security of an information system is dependent on the people that form that system.\nPeople design, implement, apply, and execute security measures (Schultz et al. 2001). In the same vein, people\naccess, use, manage, and maintain the IS resources of an organization (Henry 2004). As a consequence, the security\nculture shared by organizational members plays a critical role in ensuring IS security. Central to developing and\nfostering a security culture is the need to understand context. Research has shown the importance of the broader\nsocial and organizational issues that influence the management of information security.\nAn analysis of prescription fraud in the British National Health Services (Pouloudi 2001), suggests that by\ncarefully interpreting issues and concerns of the various stakeholders, it is possible to understand the interaction\nbetween technical and social aspects of an IS implementation, thus facilitating fraud prevention. Similarly, an\nevaluation of the unethical computer use practices by Joseph Jett at Kidder Peabody & Co (Dhillon and Backhouse\n1996), shows that it is important to create a culture of trust, responsibility, and accountability. It is evident that\norganizations need to develop a focus on the pragmatic aspects in managing IS security.\nPrinciple 1: Education, training and awareness, although important, are not sufficient for managing\ninformation security. A focus on developing a security culture goes a long way in developing and sustaining a\nsecure environment.\nEducation, training and awareness have long been suggested as important measures for improving the IS\nsecurity level of an organization. However, unless or until an effort to inculcate a security culture exists, the desired\norganizational integrity will not be achieved. Clearly, issues such as lack of human centered security controls\n257 (Hitchings 1994), mismatch between the needs and goals of the organization, poor quality of management and\ninadequate management communication (Dhillon 1997), can be considered as precursors of an unethical\nenvironment, thus endangering the health of an organization and making its information systems vulnerable to\nabuse or misuse.\nAlthough managers are aware of the potential problems related with a disaster, they tend to be rather\ncomplacent in taking any proactive steps (Adam and Haslam (2001). Such an attitude can be explained considering\nthe relative degree of importance placed on revenue generation. Hence, while automating business processes and\npursuing optimal solutions, backup and recovery issues are often overlooked. Failing to recognize that\norganizational processes such as communications, decision making, change, and power are culturally ingrained is\nan attitude that can lead to problems in IS security .\nTo minimize the potential adverse events arising because of inability to appreciate human and social factors,\nnormative or informal controls should be established (Dhillon 1999a). These security measures should instill and\nsustain a security culture and contribute to the protection of information assets.\nBesides personal factors, work situations and opportunities available leverage the engagement in computer\ncrimes (Backhouse and Dhillon 1995). Monitoring employee behavior is an essential step to maintain the integrity\nof an IS. Although such monitoring may be formal and rule based, informal monitoring, comprising the\ninterpretation of behavioral changes and the identification of personal and group conflicts, can play an important\nrole in establishing appropriate checks and balances. In the end, what an organization should seek is the\nestablishment of an ethical environment among collaborators.\nPrinciple 2: Responsibility, integrity, trust, and ethicality are the cornerstones for maintaining a secure\nenvironment.\nIn the beginning of this chapter, it was argued that the traditional three tenets for managing information\nsecurity \u2013 confidentiality, integrity and availability \u2013 were too restrictive to develop secure environments in current\norganizations. Although this set of fundamentals was enough when organizations were structured hierarchically, its\napplication falls short in networked organizations. This situation becomes clear as we consider the following facts.\nConfidentiality mostly concerns restricting data access to those who are authorized. However, information and\ncommunications technologies developments are pulling in the opposite direction, aiming at making data accessible\nto the many, not the few. This trend gets stressed if we consider the new configurations organizations are adopting,\ncharacterized by less authoritarian structures, more informality, fewer rules, and increased empowerment.\nConventionally, integrity regards the maintenance of the values of the data stored and communicated. Equally\nimportant, however, is the way those values are interpreted. A secure organization not only needs to ensure that\ndata do not suffer unauthorized modification, but also to guarantee that data get interpreted according to the\nprevailing norms of the organization, something that has been termed \u201cthe maintenance of interpretation integrity\u201d\n(Dhillon and Backhouse 2000, p. 127). Although availability may be less controversial than the previous two tenets,\nthe reality is that system failure is an organizational security issue.\nIn face of this new organizational paradigm, characterized by loosely coupled organic networks, cooperation\ninstead of autonomy and control, intense sharing of information and high level of interpersonal and inter-\norganizational connectivity, a new set of fundamentals is required. In response to this quest, Dhillon and\nBackhouse (2000) suggest the RITE (responsibility, integrity, trust, and ethicality) principles. These principles\n258 were inspired by an earlier period when extensive reliance on technology for close supervision and control of\ndispersed activities was virtually non-existent. The RITE principles are:\nResponsibility\nIn a boundary diffused organization, members need to know their respective roles and responsibilities. This\nknowledge should not be seen as static, and it should enable organizational members to deal with new\ndevelopments that require ad hoc responsibilities not anticipated in the company\u2019s organizational chart or formal\nprocedures.\nIntegrity\nIntegrity is an important precondition for creating a secure environment, and personnel integrity is fundamental\nto ensuring the sustainability of that environment. Personnel integrity should be a requirement of membership in\nan organization. Prospective employees references should be properly checked, and once they are inside the\norganization processes should maintain and strengthen their personal integrity. As previously observed, the\nmajority of security breaches come from existing employees. Individuals might be the target of pressures, and they\nmight be subject to different kinds of problems, such as marital, financial, and medical.\nTrust\nModern organizations are shifting their emphasis from external control and supervision to self-control and\nresponsibility. In a physically dispersed organization, close supervision of employees is less viable, so trust must act\nas the glue between organizational nodes. Organizations need to set up mutual systems of trust, where members are\ntrusted to act according to company norms and accepted patterns of behavior.\nEthicality\nA formalized set of rules suit foreseen and predictable circumstances. However, new and dynamic situations\ncreate difficulties. In many situations there simply are no established rules for action. The way forward is to ensure\nmembers will act according to a set of working norms embedded in ethical standards. The difficulty that\norganizations nowadays experience is where do new and existing members get the ethics needed to shape informal\nnorms and behavior. In recent years, the lowering of ethical standards in business has led to an increase in the\nnumber of frauds. Without a strong ethical foundation and in the absence of a supportive environment,\norganizations will confront serious IS security issues.\nPrinciples for Managing the Formal Aspects\nOrganizational theorists have suggested that the formalization of organizational tasks through division of labor\nand coordination of efforts is an answer to the increased complexity within organizations (Mintzberg 1983). With\nthe advent of computerization, technology has been used to automate many of the formal activities. This process\ninvolves deciding which aspects should be automated and which should be left alone (Liebenau and Backhouse\n1990). Therefore, it is relevant to understand the nature and scope of formal rule based systems and the\ninterrelationships between those systems and the design of information security in an organization. The following\ntwo principles should be considered when instituting IS security formal controls.\nPrinciple 3: Establishing a boundary between what can be formalized and what should be norm based is the\nbasis for establishing appropriate control measures.\nThe establishment of formalized rules is one step that could assist in managing IS security. An example of such\nformalized rules are the security policies that assist in clarifying bureaucratic functions in order to minimize\n259 ambiguities and conflicting interpretations within organizations. The definition of security controls at the formal\nlevel of an organization should however take into consideration that the possibility of over-formalization.\nManagement\u2019s inability to balance the rule and norm based aspects of work are a source of security problems. In\norder to prevent the misinterpretation of data and the misapplication of rules, formal rules and procedures need to\nbe in place, and applied with other existing controls and their contexts. If formal rules are primarily designed as\nisolated and disconnected solutions for specific problems, they will have dysfunctional effects.\nAlthough security policies are perceived as essential for expressing rules of conduct, the success of their\napplication is a function of their integration with the organization\u2019s strategic vision. If, as in the past, enterprises\nkeep formulating security policies based on checklists, following a rationale of identifying specific security\nresponses to specific conditions, they will not be able to draw a line between formal rule based systems and\npragmatic responses.\nTo design a well-balanced set of controls in a highly integral business environment, information security\nmanagement needs to be on top management\u2019s agenda. Only then it will be possible to shift attention to the\ncreation of a security vision and strategy where appropriate consideration is given to the threats and vulnerabilities\nof the business process architecture and of the technological infrastructure. When this state is reached, security\nconsiderations will acquire a strategic nature and will demand attention in order to serve as a business enabler,\nnamely by maintaining the consistency and coherence of organizational operations. In this framework, security\npolicies will tend to assume the role of functional strategies.\nPrinciple 4: Rules for managing information security have little relevance unless they are contextualized.\nAn implication from the previous principle is that exclusive reliance on either the rules or norms will not provide\nenough protection for IS assets. If rules for managing information security are applied without due appreciation of\ncontext, the outcomes may be detrimental to the security of the company. Only by conducting a thorough security\nevaluation will the organization be able to design an integrated set of technical, formal, and informal controls. This\nevaluation will review the current security controls, taking into consideration the context in which each of the\nprojected controls will be implemented and ponder on how different controls should be integrated.\nThe context dependence of security controls application may be appreciated by considering two formal controls,\nnamely security policies and structures of responsibility and authority. The formulation of a security policy should\nresult from the application of sound business judgment to the value ascribed to the data and the risks associated\nwith its acquisition, storage, recovery, management, manipulation, communication, and interpretation. Because\neach organization is different, the content and form of a security policy is case specific, and it is difficult to draw any\ngeneralization. This suggests that a situational centered approach should be preferred when managing IS security\ncontrols.\nThe second illustrative control: structures of responsibility and authority (Backhouse and Dhillon 1996). The\nadoption of appropriate structures is an important step in establishing good management practices and to assist in\nthe prevention of computer crime and of communication breakdowns within organizations. The concept of\nstructures of responsibility and authority provides an effective means to identify the responsible agents in the\nformal and informal organizational environments and to determine what behaviors those agents perform. By\nfacilitating the understanding of the ranges of conduct open to responsible agents, the influences they are subjected\n260 to, the manner in which they make sense of the occurrence of events and the communications in which they\nparticipate, structures of responsibility and authority create a means to manage the formal aspects of IS security.\nIn order to benefit from the application of such framework, an organization needs to go beyond the sole concern\nof specifying an appropriate organizational structure, since this attitude usually results in a skewed emphasis\ntowards formal specification. The most important step to solve the problems when establishing structures of\nresponsibility and authority is the capacity to understand the underlying patterns of behavior of organizational\nmembers. The goal of developing and designing secure environments will only be successful if the context that\nshapes those attributes is taken into account.\nPrinciples for Managing the Technical Aspects\nFrom the previous discussion, it should be apparent that the security of the technical infrastructure is a function\nof the effectiveness of formal and informal organizational arrangements. Exclusive reliance on technical controls\nwill not be enough to create a secure environment. Traditionally organizations have been conceived as purposeful\nsystems, where security has not been considered part of the \u2018useful system\u2019 designed for the purposeful activities\n(Longley 1991). Actually, IS security management has always been considered as an activity that aims to warranty\nthat the useful activities of an organization will continue to be performed and harmful incidents avoided. However,\nIS security management should be perceived as a key enabler in the smooth running of the business processes of an\norganization (Dhillon 1997), by the development of security visions, strategies, and cultures.\nOf course from a holistic point of view, besides focusing on formalized rule structures and establishing an adequate\nunderstanding of behavioral practices, an organization also needs to develop and implement appropriate technical\ncontrols. These are vital measures, especially concerning who accesses the technical systems and what they are allowed\nto do once admitted. Two fundamental principles should be considered for adequately managing the technical\naspects of information systems security. These follow.\nPrinciple 5: In managing the security of technical systems a rationally planned grandiose strategy will fall\nshort of achieving the purpose.\nMany organizations focus on formulating security strategies, policies and procedures, and then hope their\nimplementation will make them more secure. Although strategies, policies and procedures are important\ncomponents of the organizational security effort, an exclusive emphasis on this top-down stepwise effort may be\ncounterproductive. There are two main reasons for this argument.\nFirst, there is the possibility that an exercise of rationally planned strategy may not necessarily consider the\ncontext where that strategy is being formulated and will be implemented. Several studies have provided supported\nfor the multi-faceted nature of formulating strategy, where intended strategies interact with emergent strategies,\nand where formulation and implementation co-exist and do not always follow each other in the expected order\n(Mintzberg 1994).\nSecond, the fast changing pace of information technologies and the dynamic nature of businesses raises\nconsiderable obstacles to the formulation of grandiose strategies and waiting for them to play out. In the past,\nwhere a hierarchy was the dominant organizational structure and stability was the norm, it made sense to formulate\nstrategies and policies and then proceed with their implementation, allowing the time for organizations to adapt to\nthem. Back then, a rationally planned approach for information security formulation and implementation could\nhave sufficed. Nowadays, with the emergence of new technologies, constant innovation and transformed structure\n261 and business processes, context is a determining factor for maintaining organizational integrity of networked and\nvirtual enterprises.\nPrinciple 6: Formal models for maintaining the confidentiality, integrity and availability (CIA) of information\ncannot be applied to commercial organizations on a grand scale. Micro-management for achieving CIA is the\nway forward.\nConfidentiality, integrity, and availability are key attributes of information systems security. From a technical\nperspective, security can only be achieved if these three aspects have been clearly understood. One key ingredient in\nthe design of technical controls is to apply formal models of security. Examples of these models are the Bell La\nPadula and Denning\u2019s Models for confidentiality of access control; and Rushby\u2019s Separation Model and Biba\u2019s\nModel for integrity. Any formal model is an abstraction of reality and its adequacy and preciseness are crucial for\ndetermining model\u2019s usefulness.\nTo a large extent, the previously mentioned models have proved valid and complete. However, their validity\nexists not because of their mathematical correction, but because the reality they are mapping is well defined,\nnamely the military organization. To a large extent, the military environment is characterized by a culture of trust\namong its members and a system of clear roles, lines of authority and responsibilities. As far as the organization\nworks according to the stated security policy, the models successfully adhere to reality. However, the transferability\nof these formal models to a different reality, particularly the commercial one, calls in question the maintenance of\ntheir completeness and validity.\nThe first shortcoming is that organizational reality is not the same for all enterprises. Therefore, the stated\nsecurity policy for one organization, might be radically different from that of the other because of environmental\ndifferences. Second, a model conceived for information security within a military organization may not necessarily\nbe valid and applicable for a commercial enterprise. Consequently, any attempt to use models based on the military\u2019\nsituation may prove inadequate in a commercial setting, together with the possibility of such application generating\na false sense of security. The way forward for achieving confidentiality, integrity, and availability is to create newer\nmodels for particular aspects of the business for which information security needs to be designed. This requires the\ndevelopment of micro-strategies for unit or functional levels.\nConclusion\nThis chapter has sketched the background for IS security, presented the major challenges that organizations\nneed to address when establishing a secure environment and presented six principles for managing IS security in\nmodern enterprises. The discussion has essentially focused on three core concepts: the technical, formal and\ninformal aspects of IS security.\nIS security has always remained an elusive goal and it is rather difficult to deal with security. As a concluding\nremark, no one approach is adequate in managing the security of an organization and clearly a more holistic\napproach is needed. Continued and new research and practitioners efforts are needed to help addressing issues and\nconcerns in the complex and engaging field of IS security.\nChapter editor\nGurpreet Dhillon\n262 References\nAdam, F., & Haslam, J. A. (2001). A Study of the Irish Experience with Disaster Recovery Planning: High\nLevels of Awareness May Not Suffice. In G. Dhillon (Ed.), Information Security Management: Global\nChallenges in the New Millennium . Hershey, PA: Idea Group Publishing.\nAudit Commission. (1990). Survey of Computer Fraud & Abuse: The Audit Commission for Local Authorities\nand the National Health Service in England and Wales.\nAudit Commission. (1994). Opportunity Makes a Thief. Analysis of Computer Abuse: The Audit Commission\nfor Local Authorities and the National Health Service in England and Wales.\nBackhouse, J., & Dhillon, G. (1995). Managing Computer Crime: A Research Outlook. Computers & Security,\n14(7), 645\u2013651.\nBackhouse, J., & Dhillon, G. (1996). Structures of Responsibility and Security of Information Systems.\nEuropean Journal of Information Systems, 5(1), 2\u20139.\nBaskerville, R. (1992). The Developmental Duality of Information Systems Security. Journal of Management\nSystems, 4 (1), 1\u201312.\nChokron, M., & Reix, R. (1987). Planification des Syst\u00e8mes d\u2019Information et Strat\u00e9gie de l\u2019Enterprise. R\u00e9vue\nFran\u00e7aise de Gestion, Janvier\/Fevrier, 12\u201317.\nDhillon, G. (1997). Managing Information System Security. London: Macmillan.\nDhillon, G. (1999a). Computer Crime: Interpreting Violation of Safeguards by Trusted Personnel. In M.\nKhosrowpour (Ed.), Managing Information Technology Resources in Organizations in the New\nMillennium. Hershey: Idea Group Publishing.\nDhillon, G. (1999b). Managing and Controlling Computer Misuse. Information Management & Computer\nSecurity, 7(5), 171\u2013175.\nDhillon, G. (2007). Principles of Information Systems Security: Text and Cases. Hoboken, NJ: John Wiley &\nSons.\nDhillon, G., & Backhouse, J. (1996). Risks in the Use of Information Technology Within Organizations.\nInternational Journal of Information Management, 16(1), 65\u201374.\nDhillon, G., & Backhouse, J. (2000). Information System Security Management in the New Millennium.\nCommunications of the ACM, 43(7), 125\u2013128.\nDhillon, G., & Orton, J. D. (2000). Schizoid Incoherence and Strategic Management of New Organizational\nForms. Paper presented at the International Academy of Business Disciplines, March 30-April 2, Las\nVegas.\nDhillon, G., & Phukan, S. (2000). Analyzing Myth and Reality of Computer Crimes. Paper presented at the\nBITWorld Conference, Mexico City, Mexico.\nDhillon, G., Silva, L. & Backhouse, J. (2004). Computer Crime at CEFORMA: A Case Study, International\nJournal of Information Management, 24(6), 551-561.\nDrucker, P.F. (2001). Management Challenges for the 21st Century. New York: Harper Collins.\n263 Erwin, D. G. (2002). Understanding Risk (or the Bombastic Prose and Soapbox Oratory of a 25-Year Veteran\nof the Computer Security Wars). Information Systems Security, 10(6), 14\u201317.\nGarg, A. (2003). The Cost of Information Security Breaches. The SGV Review, 33\u201340.\nGordon, L. A., Loeb, M. P., Lucyshyn W. and Richardson, R. (2006). CSI\/FBI Eleventh Annual Computer\nCrime and Security Survey. Computer Security Institute.\nHenry, K. (2004). The Human Side of Information Security. In H. F. Tipton & M. Krause (Eds.), Information\nSecurity Management Handbook (Fifth ed.). Boca Raton: Auerbach.\nHitchings, J. (1994). The Need for a New Approach to Information Security. Paper presented at the 10th\nInternational Conference on Information Security (IFIP Sec '94), 23-27 May, Curacao, NA.\nIBM (1996). A Risk too Far?, April, IBM, London.\nISBS (2006). DTI Information Security Breaches Survey 2006 \u2013 Technical Report, Department of Trade and\nIndustry, UK.\nLehman, D. (2000). Cable cuts ground Northwest flights. Computer World.\nLiebenau, J., & Backhouse, J. (1990). Understanding Information. London: Macmillan.\nLongley, D. (1991). Formal Methods of Secure Systems. In W. Caelli, D. Longley, & M. Shain (Eds.),\nInformation Security Handbook. New York: Stockton Press.\nMintzberg, H. (1983). Structures in Fives: Designing Effective Organizations. Englewood Cliffs, NJ: Prentice-\nHall.\nMintzberg, H. (1994). The Rise and Fall of Strategic Planning. New York: The Free Press.\nNeumann, P. G. (1994b). Inside Risks \u2014 Technology, Laws, and Society. Communications of the ACM, 37(3),\n138.\nOTA, (1994). Information Security and Privacy in Network Environments. Office of Technology Assessment.\nPouloudi, A. (2001). Addressing Prescription Fraud in the British National Health Service: Technological and\nSocial Considerations. In G. Dhillon (Ed.), Information Security Management: Global Challenges in the\nNew Millennium. Hershey, PA: Idea Group Publishing.\nSchultz, E. E., Proctor, R. W., Lien, M.-C., & Salvendy, G. (2001). Usability and Security: An Appraisal of\nUsability Issues in Information Security Methods. Computers & Security, 20(7), 620\u2013634.\nSiponen, M. (2001). An Analysis of the Recent IS Security Development Approaches: Descriptive and\nPrescriptive Implications. In G. Dhillon (Ed.), Information Security Management: Global Challenges in\nthe Next Millennium. Hershey, PA: Idea Group Publishing.\n264 15. Avoiding information\nsystems failures\nEditor: John Beachboard (Idaho State University, Pocatello, USA)\nContributors: Nelson Massad and Charmaine Barreto (Florida Atlantic University, USA), Alma Cole,\nSteven Hernandez, Mike Mellor and Moses Okeyo (Idaho State University, USA)\nReviewer: Geoffrey Dick (University of New South Wales, Australia)\nLearning objectives\n\u2022 Be able to explain why business managers need to understand the consequences of information system (IS)\nfailure and actively participate in planning to avoid such failures.\n\u2022 Be able to identify the components comprising an information technology (IT) and explain how IT\ninfrastructure is necessary to support the delivery of IS-enabled business services.\n\u2022 To recognize that IS system failures are not limited to the loss or destruction of information systems, but\nincludes breaches of information confidentiality, integrity and availability.\n\u2022 To be briefly identify and explain how managerial, operational and technical controls are used to minimize\nthe probability of system failure and to minimize adverse consequences resulting from those failures which\ndo inevitably occur.\nIntroduction\nAvoiding information system (IS) failures; isn\u2019t that the job of the information technology (IT) staff? Why should\nI care? What is an IS failure anyway? This chapter is intended to help you understand what IS failures are, what\ntends to cause IS failures, and how to minimize the probability of experiencing IS failures. While the concept of IS\nfailures is presented in more detail later in this chapter, we need simple definition of the concept of IS failure to\nunderstand the motivation for including this chapter in a textbook for business students. An IS failure occurs\nwhen an information system, that combination of computer hardware, software, data, and processes designed\nto support some type of organizational activity, fails to meet the organization's requirements. We will see that\ndefining IS failures is a bit more complicated than defining failure for other organizational assets such as a motor\nvehicle failure. That is because an organization not only requires a system to be available to meet its requirements,\nbut in many cases also requires the information system to ensure that the information stored is accurate and cannot\nbe accessed by unauthorized individuals. Before getting into the details of this subject, we want you to understand\nwhy we, the authors of this text, think this subject is important for all organizational managers, not just IT\nprofessionals.\nA considerable portion of this text is devoted to helping you understand how information technology can be\nused to benefit your business or organization and consequently this chapter does not explicitly discuss information\nsystem success. In this chapter we will look at the dark side of information systems (IS) and discuss the\n265 implications of system failure and what organizations can do to reduce although not eliminate the probability of\ntheir information systems failing. Upon the completion of this chapter, we want you to appreciate that while an\norganization's professional IT staff may be responsible for implementing the majority of technical details associated\nwith avoiding information system failures, organizational management must be involved in:\n\u25cf Understanding the benefits derived from information systems and assessing the consequences to the\norganization of IS failure,\n\u25cf Prioritizing IS investments required to improve system reliability and security, and\n\u25cf Ensuring that policies are in place across the organization so that all organizational members recognize\ntheir individual and organizational responsibilities for maintaining the availability, integrity and security of\nthose information systems on which the organization depends.\nManagement must recognize the potential downside of relying on information systems. The greater the benefits\nderived from information systems, the greater the potential for loss resulting from system failure. While the IT staff\nshould play a critical role in helping organizational management to understand potential weaknesses in its\ninformation systems, the organizational management must be prepared to assess operational, financial and even\nlegal implications of system failure. That is, what are the consequences to an organization if particular a particular\nIS service (e.g., email, production scheduling, point of sale, transportation scheduling) fails? Furthermore, do the\nconsequences vary if multiple services fail individually or in combination?\nIS failures can have financial, legal and moral consequences. Consider, if there were no adverse consequences\nresulting from an IS failure, then one must wonder why the information system exists. Perhaps the most critical\ntakeaway from this chapter is that organizational managers -- clearly top management, but operational and\nstaff management as well -- must be involved in assessing the consequences of system failure and ensuring that\ntheir organizations have properly invested in reducing the probability of experiencing serious IS failures. That is,\nafter understanding the consequences of IS failure, organizational management must actively participate in the\ndevelopment of appropriate policies, procedures, training and technical safeguards to reduce the probability of\nsustaining unacceptable IS failures.\nManaging the delivery of IS services: The role of IT infrastructure\nBefore launching into a discussion of system failures and management techniques intended to avoid them, we\nwant to introduce some terms intended to give us a common set of definitions and a common understanding of how\nIT supports the delivery of IS services. In truth, organizational managers are generally more interested in the actual\ndelivery of IS services than the performance of their information systems. However, the reliable delivery IS services\nis dependent upon the reliability of supporting information systems. Perhaps it is a problem within all disciplines,\nbut the IT field seems particularly prone to assigning varying definitions to common terms. Unsurprisingly, the lack\nof universally accepted definitions can result in miscommunication. Given this somewhat chaotic state of affairs, we\ndo not claim that the definitions and usage employed in this chapter are authoritative. However, they do reflect the\nunderstandings of important IS researchers and practitioners. We provide this cautionary note because we\nrecognize the terms can be differently understood and that that assuming a common understanding of particular\nterms often leads to miscommunication.\n266 Key Concepts: Distinguishing between IT infrastructure and applications\nIn this section, we rely heavily on the work of Weill and Broadbent (1998) in their study of IT infrastructure and\nits relationship to business performance. While their research primarily focuses on the competitive advantages\nderived from effective IT infrastructure investment, their conceptualization of IT infrastructure, as distinguished\nfrom business applications, can be usefully applied within a variety of organizations. Just as with virtually any\nfinancial investment, they view IT investments as consisting of numerous individual investment decisions \"... each\nwith different objectives -- each with different risk-return profiles to be balanced to meet the goals of the firm\" (p.\n24). In evaluating potential IT investments, they find it useful to distinguish between IT infrastructure and IT\napplications. Business applications represent the software developed specifically to automate or inform\nbusiness activities. For example, for airlines, the ticket sales and seat reservation system is considered a critical\nbusiness application of IT. As you have seen in the many of the preceding chapters, there is a tremendous variety of\napplications that an organization can adopt to achieve its objectives. Throughout the chapter, when we refer to IS\nservices, we are generally speaking of the services provided by business applications. However, what is relevant to\nour discussion here is that since business applications tend to directly support the needs of the organization, it is\ntypically not too difficult to obtain management involvement in making investment decisions relevant to purchase\nor development of IT applications.\nIn contrast, IT infrastructure represents a foundation or platform which is needed to support the business\napplications. Weill and Broadbent conceptualize IT infrastructure as including:\n\u25cf IT Components: the computers (desktops, servers, specialized storage devices), system software, and\nnetworking hardware, software and communications links,\n\u25cf Human IT Infrastructure: the human resources required to configure, operate and maintain the IT\ncomponents and applications,\n\u25cf Shared IT Services and Applications: an array of shared IT services oriented more toward enabling the\norganization to function more effectively, but not directly related to the support of specific business\nprocesses, e.g., email, SPAM filtering, even widely adopted Enterprise Resource Planning (ERP) Systems.\nExhibit 15.2: The structure of IT infrastructure. Source: Figure 4-1 P. Weill & M. Broadbent, Leveraging the New\nInfrastructure: How Market Leaders Capitalize on Information Technology, Harvard Business School Press, June\n1998.\n267 We have taken the space to distinguish between IT applications and IT infrastructure, because organizational\nmanagement has too often failed to appreciate the critical relationship that exists between IT infrastructure\ninvestment and the performance, reliability and security of its business applications. Organizational management is\nprimarily concerned with the failure of the business applications on which they have grown to depend. It is\nimportant to recognize the extent to which these business applications depend upon a secure and reliable IT\ninfrastructure.\nInformation Technology Infrastructure Library (ITIL): An IT management framework\nFor managers to effectively address the issue of information systems failures, they should have a general\nunderstanding of what constitutes effective IT management practices and processes. ITIL provides a\ncomprehensive framework of IT \"best management\" practices developed by the Office of Government Commerce\n(OGC) of the United Kingdom. The ITIL framework intentionally emphasizes the critical role of people and\nprocesses relative to technology in delivering delivery of high-quality IT services. While a thorough introduction to\nthe ITIL framework lies well beyond the scope of this chapter (OGC presents the framework in multiple book-\nlength publications), the framework provides several principles particularly relevant to our discussion of avoiding\nsystem failures. The ITIL framework is built around the core process of IT service delivery and management. ITIL\npromotes business driven identification of Service Level Requirements (SLRs) to be incorporated into Service\nLevel Agreements (SLAs). SLRs are a set of operational requirements for individual IT services. Typically SLRs\ninclude a specification of service availability (the time of day that the service should be accessible and the\npercentage of downtime acceptable during those times the service is expected to be available), and service response\ntimes (how fast the automated service is provided once the action has been initiated by the system user). The\n268 operational requirements represent a clear articulation of the organization's IT service needs to the IT service\nactivity.\nConsider the example of an information system supporting a bank's tellers. If the teller's terminal fails, that\nteller can no longer perform his or her primary functions of accepting and disbursing money. If the system to\nwhich the teller terminal is connected fails, then all of the tellers are unable to perform this activity. The banks\ncustomers cannon deposit or withdraw their money. The number and duration of system failures can be used to\ncalculate the overall system availability. Sometimes an information system does not fail completely. Perhaps one\ncomponent fails and the system runs slowly. Or perhaps, so many users are accessing the system and it becomes\noverloaded and slows down. A bank customer may be willing to wait 1 minute or two minutes for a withdrawal to\nbe completed, but what if it took 15 minutes. The lines would probably grow very long the the bank's customers\nwould be dissatisfied. No information system is 100% reliable. Accordingly, the bank owners or managers must\ndecide what level of reliability and performance they need to maintain the quality of service expected by the bank's\ncustomers. Note also, that different information systems may have different performance standards. In the bank\nexample, the failure of one teller's terminal would not be as serious as the failure of the server system to which all of\nthe teller terminals are connected. You should be able to explain why this is so.\nITIL calls for the specification of service requirements into SLAs. The SLA essentially represents a contract\nbetween the organization and its IT service provider specifying the type and quality of IT services to be provided. As\na contractual-like document, SLAs should specify user responsibilities as well as those of the IT activity. SLAs\nshould specify service availability, reliability, performance and security in terms and measures that organizational\nusers are capable of understanding. For example:\n\u25cf Service availability of 99.5% during business hours and 95% availability on nights and weekends --\nexcluding scheduled outages for required maintenance and upgrades,\n\u25cf Transaction response times, database queries to complete in less than 5 seconds during peak usage\nhours and in less than 2 seconds during non-peak hours,\n\u25cf Telephone hold times for service desk support are not to exceed 2 minutes\n\u25cf Replies to emailed service request should be within two business hours from time submitted.\nThe development of SLRs and negotiation of SLAs provides a basis for reaching an understanding among system\nowners, system users and service providers. For our purposes, the process provides a business driven approach to\nestablishing realistic criteria for judging success or failure in the delivery of IT services. The ITIL documentation\ngoes into great depth describing the numerous IT management processes that can contribute to fulfilling the service\ndelivery obligations specified in an organization's SLRs and SLAs. Later in the chapter, we will be discussing ITIL\nmost closely associated with avoiding and recovering from systems failure.\nDefining information system failure: Confidentiality, integrity and availability\nInformation systems are somewhat unique with respect to the specification of failure conditions relative to other\norganizational assets. Typically, an asset failure can be described in terms of availability. That is, if the organization\nrelies on a truck for transporting goods or a drill press for manufacturing products, failure occurs when the asset is\nbroken or stolen. The asset is simply not available to support its intended use. Information systems, however, are a\nbit trickier in that they may well be present and appear to be running, when they are in fact in a failure mode.\n269 Unlike tangible assets, information does not necessarily disappear when it has been stolen. If an organization\nholds confidential information, perhaps a list of potential clients or information describing a new manufacturing\nprocess, the information may be downloaded by an unauthorized individual but remain available to the\norganization. Exposure of information to unauthorized personnel constitutes a breach of confidentiality irrespective\nof whether the information is actually lost during the breach. That is, the breach of information confidentiality,\nthe exposure of information to unauthorized persons, may constitute an IS failure.\nAnother type of system failure occurs when the integrity of the information can no longer be trusted. That is,\nrather than an unauthorized exposure of information, there are unauthorized changes to the information. A bank\nmay be perfectly willing to allow its customers to log on and check their account balances but it certainly does not\nwant to permit customers to adjust their account balances without ensuring that funds have actually been\ndeposited. A business website containing documentation about how to configure or repair its products might suffer\nserious financial harm if an intruder were able to modify those instructions leading customers to mis-configure or\neven ruin the product they have purchased.\nFinally, denial of access to the information or information service represents another type of information failure.\nAccess denial is referred to as a breach of availability and constitutes another type of system failure. Failure of a\npayroll system resulting in a delay of depositing pay to employee accounts can result in serious hardship. But there\ncan be even more serious consequences of system failures. If a doctor is prevented from accessing the results of\ndiagnostic tests, a patient may suffer or might even die. A commercial website might lose important sales if it were\nto fail for an extended time.\nWe see then that defining failure for information systems can be more complicated than one might at first\nexpect. IS failures may be reflected in loss of information confidentiality, loss of information integrity or an inability\nto access the information or automated service, i.e., a loss of system availability. Organizational management must\nwork with its IT professionals to understand the types of failures that can occur and to assess adverse consequences\nshould failures do occur. While a variety of techniques to minimize the probability of experiencing system failures\nare discussed in following sections of this chapter, all organizations must recognize that some failures will\ninevitably occur and should establish recovery procedures to minimize adverse consequences when they do.\nPotential causes of systems failure\nNow that we have described a variety of ways in which information systems can fail and recognize the potential\nconsequences these various failures can hold for the organization, we want to get a better understanding of why or\nhow failures occur. It is only through understanding potential causes of systems failure that we are able to take\nappropriate action to avoid them.\nThere are a wide variety of potential threats to an organization's information systems. Threats are any person,\nobject or circumstance that has the potential for causing an IS failure. that Exhaustive threat lists are difficult, if\nnot impossible, to create and security professionals often use threat categories in organizing their analysis of\nthreats. Each category could broken into additional categories, as we have done with the category of human threats,\ndepending on the level of detail desired. A representative set of categories follows:\n\u2022 Human: Human threats are perhaps the most complicated in that the category includes such a wide variety\nof behaviors. To illustrate how the degree of detail may vary, some relevant subcategories include:\n270 \u2022 Accidental behavior by organizational members\n\u2022 Accidental behavior by technical support personnel\n\u2022 Accidental behavior by organizational clients and other individuals that have authorized access to the\ninformation or information service\n\u2022 Malicious behavior by organizational insider\n\u2022 Malicious behavior by organizational outsider (malicious behaviors can be further broken out to include:\ntheft, sabotage, extortion).\n\u2022 Natural: Flood, fire, tornado, ice storm, earthquake, flu pandemic\n\u2022 Environmental: Utility failure, chemical spill, gas line explosion.\n\u2022 Technical: Hardware or software failure (whether maliciously intended or through normal ware and tear),\nperimeter defense failures (faulty closed circuit TV, key-code access system, fire alarm)\n\u2022 Operational: A faulty process that unintentionally compromises information confidentiality, integrity or\navailability. For example, an operational procedure that allows application programmers to upgrade software\nprograms without testing or notifying system operators may result in prolonged outages.\nUpon reviewing the many potential causes of system failure, it becomes apparent that the use of information\ntechnology to support critical needs, while extremely beneficial, can be fraught with peril. Certainly, we do not\nmean to discourage the use of information technology in this chapter, but we intend to emphasize that careful\ninformation system planning, implementation and operation are required to minimize the probability of system\nfailure as well as minimize the adverse consequences resulting from those failures which will inevitably occur.\nExhibit 15.2: A rats nest of cables in an anonymous business's server room. This is not exactly accidental and\nnot exactly malicious. However, it does exemplify how human actions can potentially contribute to system failures.\nPermission for use of this photo granted by the photographer, Cormac Phelan.\n271 Mitigating risk: Reducing the probability of system failure\nRisk Mitigation refers to the actions designed to counter identified threats. These actions are also referred to\nas controls and as with information system threats, there are numerous frameworks for categorizing the various\ncontrols intended to avoid system failure or compromise. A framework that we have found to be both\ncomprehensive and comprehensible divides mitigation controls into three broad categories:\n1. Management controls: managerial processes which identify organizational requirements for system\nconfidentiality, integrity and availability and establish the various management controls intended to ensure\nthat those requirements are satisfied.\n2. Operational controls: include day-to-day processes more directly associated with the actual delivery\nof the information services.\n3. Technical controls: technical capabilities incorporated into the IT infrastructure specifically to\nsupport increased confidentiality, integrity and availability of information services.\nThe remaining sections of this chapter present a general overview of managerial, operational controls and a\nsubset of technical controls (technology investments associated primarily with improving system availability in the\nface of non-malicious threats.\nMitigating risks with management controls\nManagement controls include management activities related establishment of information system requirements\nand control processes intended to ensure that those requirements are met. Critical information assurance\nmanagement controls include:\n1. Creation of policies, procedures, standards and training requirements directly relating to the\nimprovement of information system confidentiality, integrity and availability.\n2. Performance of risk analyses to evaluate risk potential of new information systems and re-evaluate risks\nassociated with existing business applications and IT infrastructure.\n3. Management of information system change\nThe following sections provide a general overview of each of these three important information assurance\nmanagement controls.\nInformation Assurance Policies, Procedures, Standards and Education\nThe overall objective of an information assurance program is to protect the confidentiality, integrity and\navailability of organizational information and IT-enabled services. Fundamental to the establishment of an effective\ninformation assurance program is the organization's establishment of appropriate information assurance policies,\nprocedures and standards.\nPolicies are high-level statements communicating an organization's goals, objectives, and the general means\nfor their accomplishment. The creation of information assurance policies may be driven by the need to comply with\nlaws and regulations or simply reflect executive management's analysis of the organization's information assurance\nrequirements. There can actually be a hierarchy of policies with each lower layer providing increasing degrees of\nspecificity, but still recognizable as policies by their focus on \"know what\" content rather than \"know how.\" Because\npolicies tend to be formulated in general terms, organizations will generally develop procedures and standards that\n272 more specifically elaborate what needs to be done. Policies might be used to identify information assets meriting\nspecial safeguards (e.g., client lists, product designs, market analysis), delineating information related roles and\nresponsibilities (e.g., establishing a Chief Security Officer position) specifying the establishment and performance\nof information assurance related tasks or processes (e.g., organizational policy might dictate the establishment and\nconduct risk assessment and change management processes described below).\nStandards can be thought of as a specific class of policies and represent mandatory rules (e.g., ensure desk is\ncleared of working papers before leaving worksite for the day), technical choices (e.g., all desktop systems\nconnecting to the organizational network will have a particular anti-virus program loaded), or some combination of\nthe two (e.g., the signature file for the anti-virus software is to be updated on a daily basis). The delineation of\nstandards and policies can be fuzzy. A policy might dictate that servers containing confidential information reside\nbehind a network firewall. A standard might specify the type of firewall to be used and even specify the\nconfiguration of the firewall. But all of that information might reside in a single policy document. Finally, an\norganization might specify procedures that spell out the specific activities or steps required to conform with\ndesignated policies and procedures. The procedures constitute the instructions for performance of policy- or\nstandard-related tasks. The formal definitions matter less than the way the terms are actually employed with any\ngiven organization. The important point to understand is that the formulation of policies, procedures and standards\nconstitute important elements of an organization's information assurance program and an organization's ability to\navoid system failures.\nThere are extensive guidelines governing the development of effective policies, procedures and standards, and\nthe reader is encouraged to consult such guidance if he or she becomes directly involved in the process of writing\npolicies and procedures. However, we think it useful to briefly describe criteria for judging the effectiveness of\ninformation assurance policies. Good policies should:\n\u2022 Good policies have the support of upper management. One can hardly imagine a factor more likely to\nundermine policy compliance within an organization than the realization that upper levels of management do\nnot care about the policy, are unwilling to provide resources required to implement the policies or have no\nintention of conforming to the policies in their own behavior.\n\u2022 Good policies are clear, concise and well written. Every attempt must be made to reduce ambiguity by\nselecting appropriate language, identifying a clear scope to which the policy applies and ensuring the policies\nare consistent with other organizational policies and practices. Organizational members cannot comply with\npolicies if they cannot understand them and ambiguity may encourage the development of undesirable policy\ninterpretations.\n\u2022 Good policies will clearly delineate responsibilities and identify the resources required to support their\nimplementation. If one commonly hears the phrases, \"it's not my job\" or \"I don't have the resources\" with\nrespect to policy compliance, problems with compliance likely exist.\n\u2022 Good policies are living documents. It seems that the only constant in today's world is change. Policies can\nquickly become outdated. Out-of-date policies lead to two problems. First, the policies gradually become\ninadequate as organizational requirements change over time and as well as due to changes in the types of risks\npresent in the organization's environment. Second, as policies become increasingly inaccurate and irrelevant to\nthe organization's needs, there is a natural tendency for the policies to be ignored.\n273 \u2022 Good policies specify enforcement provisions and a process for handling policy exceptions. If there are no\nadverse consequences associated with policy non-compliance, then compliance will likely suffer. As it is\ndifficult if not impossible to anticipate every contingency in the formulation of policies, long term compliance\nwill be enhanced by specifically including provisions for requesting policy exceptions.\nFinally, it is difficult to overestimate the importance of education and training in establishing effective policy\ncompliance. The effectiveness of policies, procedures and standards are seriously undermined if organizational\nusers are able to claim ignorance of their existence. This is particularly true with respect to compliance with specific\nstandards and procedures. Education and training requirements will vary depending on the job responsibilities.\nEmployees who deal with confidential information may require guidance concerning legitimate use of the\ninformation. IT professionals may require specialized training in order properly configure and employ technology\nused to increase reliability and security of information services. In short, the establishment of a comprehensive\ninformation assurance training program constitutes a critical a critical management risk mitigation control.\nRisk assessment: Evaluating new systems and old\nIn an effort to speed delivery and reduce costs associated with the delivery of information services, many\norganizations short-change the planning and design phases of their information system projects. However, the\nconsequences of adopting such a strategy often result in the delivery of services that do not adequately meet\norganizational requirements and may well end up increasing lifecycle system costs. The organization certainly\nleaves itself open to future problems if requirements for information confidentiality, integrity and availability are\nspecified for the original system design.\nIT professionals widely recognize that it is much more effective to design security and reliability directly into\ntheir systems from the outset than to try and add such capabilities after-the-fact. Consequently, the conduct of a\nrisk assessment is essential in the planning of any major new information system or upgrade of existing\ncapabilities.\nA risk assessment essentially consists of:\n\u2022 Clearly identifying organizational information assets, the data and information systems on which the\norganization depends\n\u2022 Understanding vulnerabilities, the susceptibility of the asset to breakdown or malicious attack, associated\nwith identified assets\n\u2022 Identifying threats, object, person or incident capable of exploiting identified vulnerabilities.\nAn analysis system risks, that is, the probability of threats being realized, is performed to determine the\nprobabilities of loss. Based on expected losses, the organization is better able to determine which countermeasures\nor controls are appropriate to its needs.\nDuring the planning stage, organizations need to estimate the consequences of service failure, including how the\nconsequences vary as a function of the duration of service failure, and the various threats capable of exploiting\nidentified vulnerabilities. The participation of organizational management is critical to this process because they\nshould best able to evaluate the consequences of system failure and determine the level of investment warranted to\nminimize adverse consequences.\n274 IT and and security specialists can be expected to also play an important role by helping organizational\nmanagers to understand vulnerabilities, threats, and even probabilities associated with various threats.\nAs entire books have been dedicated to the subject, we do not attempt to provide a thorough treatment of risk\nassessments here. However, we do think it useful to include a brief discussion of a few representative issues that are\nusefully considered during the planning phase of an information system.\nInformation system planning necessarily focuses on IT solutions to meet identified requirements and\nminimizing system non-availability. IT solutions might include the purchase of redundant servers, tape backup\nsystems, network firewalls and the like. These technology investments may represent warranted investments and\nwe do not discount such recommendations. However, in considering overall systems availability and security, the\nphysical location of the IT and information assets and the environmental systems on which they depend should also\nbe carefully considered. For example, it is not uncommon to place computer centers in the basements of multi-story\nbuildings, even if those buildings are located in known floodplains. Computers do not tolerate water well, and since\nwater tends to seek the lowest levels within a building, a basement computer facility represent a risk that might be\nbeen easily avoided.\nPower, air conditioning, external communications links all represent potential points of failure for computer\nsystems. The likelihood of such events must be considered in the selection of information services on which an\norganization is to depend. In many areas, commercial power and communications are unreliable. Accordingly,\nmanagers must consider the probability and length of service outages and include additional investments, e.g., for\nuninterruptible power supplies capable of conditioning the power and generating backup power if commercial\nservices are disrupted. When planning for the provision of IT-enabled services, organizational managers must\nrealistically appraise the constraints and limitations imposed by the organization's environment.\nIn short, effective IT planning should incorporate a rigorous assessment of threats and the inclusion of\nappropriate safeguards and countermeasures within the overall design of proposed information systems.\nManaging change and system configurations\nA widely cited Gartner research report concludes that \"80 percent of mission-critical application service\ndowntime is directly caused by people or processes failures. The other 20 percent is caused by technology or\nenvironmental failure or a disaster\"\n(http:\/\/www.gartner.com\/5_about\/press_releases\/2002_03\/pr20020325a.jsp). Often these failures result from\nthe modification of software, loading a software patches to fix a security flaw or add some new functionality or the\nmis-configuration of critical servers or network devices. For example, important information services may be cutoff\nby mis-configuring security or communications devices such as network firewalls or routers.\nIT management best practices such as those provided in ITIL emphasize the importance of change management.\nWhile often derided by IT practitioners as consisting of unnecessarily bureaucratic procedures that actually impede\nthe practitioner's ability to quickly respond to customer requests, change management processes is intended to\nensure that system changes are properly authorized, prioritized and tested, and that all interested parties are\ninformed regarding proposed changes. Element of an effective change management process include:\n\u2022 Selection of the appropriate and qualified staff to participate on the change management team.\n\u2022 Establishment of formal change request and tracking system.\n275 \u2022 Regular scheduling of change management team meetings.\n\u2022 A formal means of ensuring that approved changes, including their implementation schedules) are\ncommunicated relevant stakeholders.\n\u2022 A formal means, such as regularly scheduled system audits, to ensure that change management practices\nare being followed.\nWhen system outages cost $20,000 a minute (see mini-case insert) the need to invest in a disciplined change\nmanagement system becomes much clearer. Organization must assess the consequences of particular system\nfailures to determine the level of investment in change management that warranted for the particular system. While\nrecognizing that highly formalized procedures can pose an unacceptable burden on small- and medium-sized\norganizations, these organizations are still likely to benefit from managing change.\nConsequences of an Untested Software Upgrade\nA firm conducting much of its business over the Internet suffered a IS service failure during its peak\nsales season just before Christmas. The failure, the firm's web servers began to lockup so tightly that\nthey could not even reboot themselves. This particular firm conducts almost 80% of its annual\nbusiness in the weeks preceding Christmas. Losses were estimated at approximately $20,000 a\nminute. The cause of the failure was a supposedly \u201cminor\u201d software upgrade that a programmer\nmade just prior to departing on holiday. It took the firm over 24 hours to discover the changes that\nhad been made and even more time to back out the change to restore service. The programmer,\nonce contacted, insisted that it was \u201cinconceivable\u201d that his change would have caused this outage\n(Behr, Kim and Spafford, 2004).\nBefore leaving the topic of change management, it is also useful to introduce the closely related topic of\nconfiguration management. While the term is sometimes used interchangeably with change management,\nconfiguration management refers specifically to implementation of a database that records critical elements of the\nIT infrastructure and applications necessary for the provision of IT services. The database should not simply be\nviewed as an IT inventory. Properly conceived and implemented, a configuration management database (CMDB)\nwill include information documenting movement, maintenance, and problems experienced with various\nconfiguration items. Configuration items, those elements under configuration management and recorded in the\nCMDB, can include policies and procedures, human resources in addition to the hardware and software one would\ntypically expect to find in an asset inventory database. Configuration management provides a necessary foundation\nfor an effective change management process and as we shall see below contributes to the effectiveness of multiple\nservice, infrastructure and security management processes.\nMitigating risks with operational controls\nEven the authors sometimes wonder about the true distinction between management and operational controls.\nThe easiest way to think about it is that management control functions are performed by managers and operational\ncontrols are performed by operators. However, if you look at real organizations, the distinctions between operations\nstaff and management may not be all that clear. Nonetheless, we will use the categories because that is they do\n276 reflect the terminologies that one commonly finds in both the trade and academic literature. Three operational\ncontrols commonly associated with maintaining system availability are:\n\u2022 System monitoring and incident response\n\u2022 Performing system backups\n\u2022 Planning for disaster recovery\nThe careful reader will have noticed that these processes do not really help avoid system failures. Good catch!\nWe hope that you will have noted that it is impossible to totally avoid system failures. Despite an organization's best\nefforts, sometimes things just go wrong. These processes are primarily intended to minimize the adverse\nconsequences that can result if and when things do go wrong.\nSystem monitoring and incident management\nOrganizations of almost any size will have some type of help desk function. When a user has a problem, they call\nfor help and sooner or later they usually get it. Larger organizations may include another activity called an\noperations control center. In some organizations the help desk and operations control center are part of the same\nactivity, i.e., the help desk may also serve as an operations center as well as providing user assistance. In other\norganizations the two functions are managed separately. The important point here is that the functions are being\naccomplished, not how they are organized.\nWe are concerned primarily with what are normally considered to be operations center functions. Those two\nfunctions are system monitoring and incident response. Quite logically, we cannot expect someone to respond to an\nincident before he or she has discovered that it has occurred. Detecting incidents is the function of system\nmonitoring. Incidents can be defined as any event that impacts the confidentiality, integrity or availability of an\nIT-enabled information service. As we have discussed above, there are a lot of potential threats to an information\nsystem. When a threat occurs, it is an incident. An operator may back up old data over new data, a lightning strike\nmay lead to a power surge that destroys a critical piece of equipment, a hacker may break into the system and steal\nor modify data. The list of possible incidents is limitless.\nThe challenge to maintain systems availability is to be able to detect incidents and respond to them quickly and\neffectively. While we are not trying to turn the readers of this text into ops center personnel, organizational\nmanagers should have knowledge of the system monitoring and incident response functions if their organizations\nsignificantly depend upon IT-enabled services. That is, organizational managers should ensure that the system\nmonitoring and incident response functions are aligned to the operational needs of the organization.\nFirst, the organization needs to decide whether it is willing to invest in proactive system monitoring as opposed\nto reactive system monitoring. Reactive monitoring simply means that the activity will be attempting to detect\nincidents as soon as possible after they occur and then respond to them. At its most basic level, there is virtually no\nmonitoring going on at all. When someone calls to complain, the incident is recorded and the response is initiated.\nBut there are also specific computer applications that can be used to monitor systems so that operations center\npersonnel may receive an alarm that a component has failed before any user has detected that failure. These are\ntypically referred to as systems or network management applications. These applications will monitor designated\nservices or system components. When the component stops or fails to perform correctly, the application will initiate\n277 some type of alarm or other notification. Some systems management applications have been designed to send email\nor telephone personnel to advise them that an alarm has occurred.\nProactive monitoring is watching for indications that a failure may occur. For example, if a hard drive, a\ncomputer component on which information is stored, becomes full, an important application might fail. With\nproactive monitoring, the systems management application monitors hard disk usage and sends an alarm when it\nreaches 80% of capacity. The IT support activity can either clear off some data or install a larger hard disk before a\nfailure actually occurs. That is proactive system monitoring.\nThe key to both reactive and proactive system monitoring is to understand the system baseline. Operations\ncenter personnel want to have a very accurate understanding of how the system is configured and behaves when\neverything is working properly. Monitoring then becomes largely a function of detecting deviations from the system\nbaseline. For example, imagine that an organization typically used very little Internet connectivity during the\nevening hours. The network administrator notices that for the last three nights that there has been a lot of network\nutilization starting at 2 am. It might be legitimate traffic; perhaps the organization had decided to back up its data\nto an offsite location during the early morning hours so that it would not interfere with normal system use.\nHowever, it could mean that an intruder had compromised the organizational system and was copying confidential\ndata or perhaps using the organization's computers to launch SPAM out onto the Internet. Under reactive\nmonitoring, the organization may not respond. After all, nothing appears broken and no one has complained. A\nproactive monitoring system will detect the incidents, the early morning network use, and investigates the cause of\nthat traffic so that an appropriate response can be taken.\nThis example leads us to the second operations center function, incident management. Incident\nmanagement refers to the actions that an organization takes in response to detected incidents. Upon incident\ndetection, the first action is to minimize or contain damage resulting from the incident. The second response is to\nrestore the service. Depending upon the organization and the type of incident, a range of other responses may be\nappropriate. There may be a need to provide notifications to key personnel. If there are multiple incidents occurring\nat the same time, a prioritization scheme may be required to determine which incidents are likely to cause the\ngreatest damage to the organization. Effective organizations have a method of documenting incidents, such as a\ntrouble ticketing system, and will perform after-action-analysis of incidents to determine if there are recurring\npatterns of incidents occurring.\nGiven that there are a near infinite number of possible incidents, there is also a near infinite number of possible\nresponses. If a circuit to the Internet fails, the operations center will typically look to see if there is a problem with\nthe organization's equipment. If the organization cannot isolate the problem to its equipment, then the appropriate\nresponse is to notify the telephone company or Internet service provider.\nAs indicated in the introduction to this section, there are a variety of ways that this function may be organized.\nMany organizations have adopted a three-tiered response model. The three-tiered response model reflects the\nobservation that some operational personnel are quite knowledgeable while others are less so. The more\nknowledgeable, highly skilled personnel are paid more while less knowledgeable personnel are paid less.\nOrganizations have learned that it is economically beneficial to have the low skilled personnel working on the easer\nincidents while the high-skilled personnel work on the more difficult incidents.\n278 The tiered response model supports this objective. Less skilled individuals serve as the first responders. They\nrecord the incident information and resolve as many incidents as they are capable of resolving. Tougher incidents\nget passed to a second tier of fairly skilled personnel for resolution. Hopefully most of the incidents can be resolved\nin the first two tiers. However, sometimes things are really complicated and you need to bring in the really\nexperienced professionals to resolve them. Some organizations will have these highly skilled individuals on their\nstaffs. But other might rely on outside personnel to provide this third level of support. These individuals tend to be\nvery expensive and an organization does not want them to spend their time working on easier problems that less-\nskilled, lower-paid staff or capable of handling.\nBefore leaving the topic of system monitoring and incident response, there is one other subject that merits\ndiscussion. We previously mentioned the ITIL framework as providing best management practices for managing IT\noperations. The authors of the ITIL have found it useful to distinguish between incident management and problem\nmanagement. The incident response process just described falls under the category of incident management.\nProblem management is a bit trickier. If incidents are events that result in system failures, what are problems?\nUnder ITIL, problems are the causes that underlie incidents. To explain, let us go back and reconsider the\nlightening strike that fried some of our equipment. The incident was the equipment failure. The equipment could\nno longer fulfill its intended function because of surge of electricity melted critical circuitry.\nThe problem, then, might be defined as the fact that lightening is a recurring problem in that particular\ngeographic area. The organization may stock spare supplies of equipment and recover the service fairly quickly.\nHowever, services will still be disrupted and replacing the equipment could prove expensive depending upon how\noften lightening can be expected to strike. Or the problem might even be more broadly defined as an unstable\npower supply. Lightening may be one cause of power fluctuations, but there might be a variety of reasons that\npower might fluctuate. Problem management attempts to broadly identify the underlying causes of the\nincidents that are occurring, e.g. unstable power, and determine how that problem can best be managed. While\nreplacing fried equipment can resolve the incident, other measures are required if the organization wants to avoid\nservice failures resulting from unstable power. The examination of outages over a period of time to see how many\nresult from unstable power is an example of using a trend analysis. When properly conducted, the combination of\ndocumenting incidents, conducting trend analyses, and resolving identified problems can greatly increase the\navailability of system services, reduce the number of incidents that must be fielded and may allow for reductions in\nthe number of IT support staff.\nOrganizational managers might start to think that this discussion is getting technical and that the IT personnel\nshould be taking care of these problems. Certainly, an organization should staff its operations center with\ntechnically competent IT staff. However, as was discussed above, organization management needs to determine\nwhat level of risk it is able to tolerate and ultimately determine the capabilities of system monitoring and incident\nresponse functions.\nSystem backups\nA recurring theme in this textbook has been the idea that value resides in the information and technology-\nenabled services rather than in the information technology itself. On a very fundamental level, this means that if\nyour information technology (e.g., an application or web server) is damaged or destroyed, the organization can\npurchase a replacement. However, if the data on that server is destroyed, it may be much more expensive to replace\n279 than the technology. Furthermore, the data may irreplaceable. Depending on the value of the data, the data's loss\nmay result in business failure or a prolonged disruption of services provided by non-profit organizations.\nThe point is that information stored on an organization's automated system can be very valuable and merits\norganizational investment to ensure that it is safeguarded. Consequently, it is important that organizational\nmanagers understand system backup technology to the extent that they can confirm that their organizational data is\nsafe guarded. Making a system backup entails saving a copy of data stored on the system's hard drives to a\ndifferent data storage facility. A system backup can be made to hard drives on a different system or made to some\nother type of storage media such as digital tape or CD-ROMs. There are different types of systems backups that can\nbe made.\nNairobi Fire and the Loss of Irreplaceable Documents\nMarch 2, 2004. \u201cThe blaze started at 0200 local time (2300 GMT on Monday),\nsending sheets of orange flames into the night sky.... Council fire engines were\nunable to cope on their own and several ran out of water as no working hydrants\ncould be found near the city hall.... Among the thousands of documents destroyed\nwere maps showing the routes of new road bypasses (BBC, 2004 at\nhttp:\/\/news.bbc.co.uk\/2\/hi\/africa\/3524855.stm). Computers as well as paper\nmay be lost in fires; both can constitute invaluable information resources of the\nenterprise. While insurance claims may help to restore the building, important\ndocuments, including contractual records and city planning drawings, were lost\nforever in the Nairobi City Council fire. These types of losses can be prevented by\nhaving having tested data recovery capabilities in place.\nWhile the technical details of designing and maintaining a robust data storage infrastructure can be daunting,\nthe fundamental principles for implementing storage backups are relatively simple. First among these is developing\nan understanding that there are different types of system backups. It is not sufficient to learn from your IT support\nstaff that backups are being performed; managers need to ensure that the required types of backups are being\nperformed. Different guides will approach the issue of backup types in varying levels of details.\nThere are different types of data or information stored on information systems. We discuss just two categories\nwhile acknowledging that backup planners will often use a more detailed set of categories. The distinction that we\nwish to emphasize is between the software programs (we include the operating system and business application\nsoftware in this category) and the actual data or information that is manipulated by the software. The rationale for\nthis distinction is that one generally expects the operating system and programs to remain fairly stable, i.e., not\nchange too much. The data, on the other hand, likely changes, or grows, daily. An organization does not want to\nrepeatedly backup data that are not changing. This would pose an unnecessary expense. Plus, an operating system\nor application is lost, it still should be possible to rebuild the system by reloading the required operating system and\napplications. However, if operational data is lost, it is lost for good, if no backup exists.\n280 We do not mean to imply that organizations are not concerned with backing up their software programs. They\nare. If the software programs and data have been backed up, an organization will be able to more quickly restore\nservice than if it had to rebuild the system by reloading and reconfiguring the entire operating system and\napplications programs. Consequently, organizations do need to backup their entire systems, but not necessarily as\noften as they need to backup their data.\nAccordingly, there are two major types of system backups (actually there are more; if interested see Wikipedia\narticle on backups). There are full system backups, where all the software and data residing on the system are\nbacked up. Then there are incremental system backups, where only the data that has changed since the last full\nbackup or last incremental backup is saved. The advantage of this approach is to reduce the amount of time\nrequired to accomplish the backup. This is a particularly important issue on systems which must be taken out of\noperational status while performing the backup. Restoration is accomplished by restoring the last full backup and\nthen applying the required incremental backups.\nAnother backup technique which has proven useful is the use of multiple copies of backups. Maintaining\nmultiple backup copies provides several benefits. First, if data becomes corrupted and is accidentally backed up, it\nis possible to overwrite the good data. By maintaining multiple copies of backups, it may be possible to detect the\ndata corruption before all versions have been overwritten, allowing recovery from an uncorrupted backup. Second,\nbackup media occasionally fails. If there is only one backup, the restoration fails and the organization is out of luck.\nThe probability of three or more sets of media failing simultaneously is extremely small. Consequently, even if some\ndata is lost due to the need to restore from an older backup, the organization should still be able to recover the\nmajority of its data. Finally, it is common to keep the most recent backup in general proximity to the devices on\nwhich it normally resides. The availability of the backup minimizes restoration time in the event of common\nfailures. However, some failures are catastrophic and destroying not only the IT equipment, but the entire facility in\nwhich the equipment resides, e.g., flood or fire. In such cases the backup data is destroyed with the equipment and\nrecovery becomes impossible, even when the equipment and facilities are replaced. Consequently, best practices\ndictates that at least one version of the backups be maintained off site to preclude catastrophic loss of all of the\norganization's data.\nExhibit 15.3.: Smoke damaged interior of computer. The system does not need to be completely destroyed by\nfire in order for data to be lost. In this case, however, a specialty firm was able to restore the system. Permission for\nuse of this photo granted by the firm, Newlifeserviceco.\n281 There are a variety of technologies that can be used to support the system backup function. They vary in terms of\nprice, capacity [the amount of data that can be backed up], performance [the speed at which data can be backed\nup], reliability [the probability that the backup technology or media will fail] and overall functionality [special\nfeatures included make the backup process easier to perform]. For our purposes, it is only necessary to recognize\nthat the price of the backup escalates with speed and capacity. Thus, organizational management needs to\nunderstand the value of its information and information services and the consequences of system and data service\ninterruption or loss so that it can make appropriate investment decisions in the purchase of backup system\nhardware and software.\nThere is one last critical principle that management truly needs to understand in it evaluation of its system\nbackup process. That is, the organization does not truly have a backup plan if it is not willing to invest the time and\nresources to test its data restoration capabilities. More than a few organizations having made the investment in\nbackup technology and have experienced the unpleasant surprise learning that their backup process was faulty. The\ntakeaway is to insist that recovery processes be practiced regularly and particularly when significant system\nchanges are implemented. A backup system that has not been tested should not be considered a backup system.\nPlanning for disaster recovery and business continuity\nSome incidents are bigger than others and constitute catastrophic failures or disasters. Quite often these are\nassociated with natural disasters, such has flood, earthquake or hurricane, but catastrophic failures may result from\nany number of causes. A major industrial accident might require vacating an organization's premises; a flu\nepidemic might incapacitate a significant percentage of an organization's staff. And of course, there is the chance of\nintentional sabotage. The issues of disaster recovery and business continuity range far beyond the domain of IT\nplanning. However, to the extent an organization depends upon IT support to support core business processes, IT\nconsiderations must be fully integrated into an organization's overall disaster recovery and business continuity\nplanning.\n282 A disaster recovery plan is intended to provide detailed guidance concerning the actions to be taken in the\nevent of that a disaster occurs. Disaster recovery plans may be written to address a wide variety of crises. Here,\ndisaster recovery planning is discussed with reference to the restoration services disrupted by severely damaged IT\nfacilities and services. In contrast, business continuity plans are more broadly concerned with ensuring that\nessential organizational functions can continue to be performed in the event of any circumstance that massively\ndisrupts the normal operations of an organization. The two functions are closely related. To the extent an\norganization depends upon IT services to support core functions, then an effective business continuity plan\nnecessarily ensures provision are made for restoration of essential IT services.\nKey elements of both sets of plans require clearly established priorities, delegation of responsibilities (including\ncontingency delegations should primary assignees prove unable to accomplish assigned tasks, and pre-staging of\nminimum essential infrastructure and assets to continue operations. For example, an organization may have\ndesignated a relocation point from which to continue its operations. However, if required phone lines are not\navailable, business operations will remain disrupted until such time as new lines can be ordered and installed.\nNo organization can be expected to fully reconstitute itself in the face of a catastrophic loss of infrastructure or\npersonnel. The key to business continuity planning is to identify the minimum essential functions that must be\navailable if the organization is to survive (from a business perspective) or meet mandatory obligations (from a non-\nprofit or government agency perspective).\nFrom an IT perspective, disaster recovery and business continuity usually require designation of some type of\nback up facilities. As one might guess, the costs of alternative facilities can vary considerably. Maintaining\ndedicated facilities that replicate existing operational infrastructure is quite expensive. Accordingly, organizations\nemploy a variety of techniques to manage the costs of disaster recovery and business continuity. The most\nexpensive capability is the maintenance of hot backup site. In a hot backup site, essential systems have been\nduplicated at the alternative facility and are fully configured to pickup operations should the primary site fail. Given\nadvances in communications technology, some companies continuously replicate their data to the alternative site\nthus minimizing the potential for data loss and the time it takes to restore service.\nA warm backup site has many of the capabilities of a hot site, but its systems are not fully configured. While\nservers, workstations and communications facilities are in place, the organization will typically need to load its\napplications and data to make the facility operational. This can take from hours to days depending on the number,\nsize and complexity of services to be supported. While still expensive, the maintenance costs of a warm site can be\nconsiderably less than those of a hot site. Some organizations maintain a cold backup site, essentially an empty\nfacility in which an organization can reconstitute its system. There is essentially no hardware pre-installed;\norganizations will have to either relocate or purchase required equipment then install and configure it before\noperations can be resumed. Obviously, a lower cost alternative, but one that will likely result in a prolonged outage\nshould the organization's primary facility becomes unavailable.\nTo reduce the cost of maintaining a hot site or a warm site, organizations may also contract with a service to\nprovide contingency services. For a recurring fee, the service bureau agrees to maintain required equipment and\nfacilities for the contracting organization to move into should the need arise. The service company can offer\nfavorable prices by offering its facilities to multiple organizations or businesses - under the assumption that not all\nof its clients will require the use of its facilities at the same time.\n283 We close this section with the same admonition provided at the close of the section on system backups. Disaster\nrecovery and business continuity plans are not all that beneficial if the organization never tries to exercise them.\nWhile expensive, realistic test scenarios must be run to ensure that important details have not been overlooked. In\ntruth, it is unlikely that an organization can adequately anticipate and plan for all possible contingencies. Yet those\nthat do invest in contingency planning and in testing their plans are far more likely to survive the consequences of\ncatastrophic disasters.\nMitigating risks with technical controls\nThe final set of controls associated with the avoidance of systems failures is related to technology. That is,\norganizations often make additional investments in their IT infrastructure with the explicit goal of avoiding or at\nleast minimizing the consequences of information system failure.\nThere are numerous technical controls associated with information security. In this section, we introduce a\nrange of infrastructure investments intended to improve the overall reliability of infrastructure and consequently,\nthe reliability of organizational IT-enabled information services. These include:\n\u2022 Redundant critical components (equipment, communications, etc.)\n\u2022 Power conditioning and backup power\n\u2022 System backup capabilities\n\u2022 Network and system monitoring tools\nOrganization requiring high levels of availability will often find themselves needing to buy redundant hardware.\nIn simplest terms, redundancy occurs when you buy to pieces of hardware to perform a function when one device is\ncapable of performing the function adequately. Normally this is considered a bad thing to do. However, if the\npossibility exists of that component failing and disrupting a critical information service, then purchasing and\ninstalling the second device may be quite reasonable. The basic philosophy of operational redundancy is best\nillustrated with some simple probability calculations.\nFor example, assume an important device, say a hard disk used for storing data, has I device has a 1% probability\nof failing and disrupting service in any given year. The organization must determine if it is willing to accept the\npossibility of data loss (the data added since the last time the disk was backed up) and service disruption. If the 1%\nprobability seems too high, the organization can purchase a second drive which we also assume has a 1% annual\nprobability of failing. Now, what is the probability of the organization experiencing the data loss and service failure?\nAssuming drive failures are independent events, the probability of both drives failing simultaneously is: .01% or 1 in\na thousand. This represents a considerable improvement over the 1 in 100 chance of experiencing a single drive\nfailure. In complex environments, the probability calculations can become quite complex but the general logic\nremains the same. Investing in redundancy decreases the probability of systems failure.\nManagement must consider all potential causes of failure in prioritizing its investments in technical controls. In\nareas where power stability is a problem, power conditioning and the provisioning of on-site generators to provide\nbackup power are critically important. Computer equipment is not just susceptible to power outages. Voltage\nfluctuations, drops as well as surges, can damage expensive equipment. Many organizations find it prudent to\npurchase uninterruptible power supplies (UPSs), essentially large batteries with regulators, to condition the power\nto avoid such damage. Where prolonged power outages occur, or where even short disruptions of service cannot be\n284 tolerated, organizations will find it necessary to provide backup generators to ensure that at least the most critical\nsystems remain operating. As with all technical investments, the cost of systems varies greatly.\nWe have already discussed the importance of backing up system data and will not elaborate much beyond that\nhere. With respect to technical investments, organizational managers must recognize that purchasing some type of\nbackup system is probably essential. However, there is a wide range of backup systems. As indicated above,\nmanagers will want to assess the consequences of data loss and service disruption in determining how\nsophisticated, really meaning how expensive, a backup system is required.\nLikewise with systems and network management applications, we have addressed the usage of these tools in the\ndiscussion of systems monitoring ans incident response. A wide range of tools exist and the technology in this area\nis continuously improving. Early versions of these tools were largely limited the reporting of component failures.\nLater generations of system management tools started to provide more information about the health and status of\nthe IS components they were designed to monitor. The latest generation of tools has begun to more directly\nmonitor the health and performance of actual information services rather than monitoring the technical\ncomponents which comprise the services. By monitoring services rather than technical components, these tools\nmonitor what organizations are really interested in, service quality. The most advanced systems and network\napplications are even capable of conducting rudimentary forms of causal analysis such that when service outages or\nperformance degradations are detected, the applications are able to combine service and component analysis to\ndetermine the specific causes of the observed problem.\nIt is unrealistic to expect that organizational managers to be involved in making the technical choices required in\nthe acquisition of technical controls. However, management does need to understand that such technical choices\nare required if the organization's IT infrastructure is going to support the quality of service delivery that the\norganization requires. Better managers will be able to engage their IT staff in a conversation in which the\nrelationship between recommended technical controls and service quality can be explained.\nSummary\nIt seems odd to write a chapter summary when all of the sections in this chapter constitute summaries of\nconcepts and issues commonly presented in much greater depth. However, that is the nature of a textbook and it is\nappropriate that we reemphasize those details that are most salient to future managers concerned with the\navoidance of information systems failures.\nAs indicated in the introduction, perhaps the most important point to takeaway from this chapter is that\norganizational management should be involved assessing the consequences of systems failures and deciding an\nappropriate level of investment to minimize the potential failures. As with many management prescriptions, the\nadvice is far easier to understand than to execute.\nThe chapter began with an introduction of some important IT terms: IT business applications and IT\ninfrastructure. These terms provide a useful way of thinking about how information systems are constituted within\nan organization and a language for communicating with IT professionals. The IT Infrastructure Library (ITIL) was\nbriefly described to introduce the concepts of service level requirements in service level agreements. Service level\nagreements, when properly negotiated provide a solid foundation for planning and implementing appropriate IT\nsolutions that are effectively aligned with the organization's needs. Thus the negotiation of a level agreement\nprovides a means for understanding and documenting the organization's tolerance for system failure.\n285 The chapter then explores the nature of information system failure. Information systems are unique among\norganizational assets in the variety of possible failure modes. Failure may occur where the confidentiality of\ninformation stored within a system is breached, irrespective of whether the system is physically damaged or\ninformation destroyed. Additionally, organizations must be concerned with the integrity or accuracy of its\ninformation as well as ensuring that the information or IT enabled services remain available to meet its operational\nneeds.\nThe chapter introduces risk management as a disciplined means for organizations to identify their information\nsystem assets, the vulnerabilities associated with those assets, and potential threats (the means that exploit\nvulnerabilities). Various qualitative and quantitative methods can be used to analyze these data in order to\ndetermine the selection and implementation of countermeasures appropriate for organizational needs. A more\ndetailed discussion of risk and the risk assessment process is provided elsewhere in the book. While organization\nmanagement may rely on IT and security professionals for information regarding relevant vulnerabilities, threats\nand countermeasures, the identification, valuation of assets and the determination of consequences in the event of\nsystem failure, most appropriately lies on their shoulders.\nAfter presenting these foundational concepts, the bulk of the briefly introduces organizational activities\nassociated with mitigating risks. Risk mitigation efforts are broadly categorized as following within two categories:\nmanagement controls and operational controls. Management controls include:\n\u2022 Establishing appropriate information assurance policies, procedures, standards and education,\n\u2022 Incorporating risk management concerns into the organization's information system planning and design\nprocesses, and\n\u2022 Establishing of formal change and configuration management processes.\nOperational controls include:\n\u2022 Establishing information system monitoring and incident response capabilities,\n\u2022 Performing system backups, and\n\u2022 Planning for disaster recovery and business continuity.\nSelected technical controls, representing additional IT infrastructure investments, were briefly described as well.\nIn closing, we note that managers, at least within larger organizations, will rely on IT and security professionals\nto assist in the analysis of risk and the design and implementation of countermeasures. However, non-IT managers,\nparticularly those working at executive levels, need a basic understanding of the concepts presented in this chapter\nto ensure that their organizations are properly protected. Depending on the organization's specific circumstances,\nthere may be strong financial, legal and moral obligations to avoid information systems failures.\nChapter editor\nJohn C. Beachboard joined the Computer Information Systems faculty at Idaho State University in 2001. He\ncompleted the Ph. D. in Information Transfer and the M.S. in Information Resources Management at the School of\nInformation Studies, Syracuse University. He holds an M.S. in Business Administration from Boston University\nand a B.S. in Public Administration from the University of Arizona. Dr. Beachboard has taught graduate courses in\nresearch methods, project management, and IT use in business, and undergraduate courses in IT management,\n286 systems architectures, information assurance and networking. He has held staff and management positions\ndeveloping, implementing and operating information and telecommunications systems for the Department of\nDefense. He is keenly interested in the development and effective implementation of information technology\nservice management practices within private and public sectors and information technology adoption within the\nhealth-care industry.\nReferences\nAlberts, C., & Dorofee, A. (2002). Managing information security risks: The OCTAVE(sm)\napproach. Boston, MA: Addison-Wesley.\nBehr, K., Kim, G. & Spafford, G. (2004). The visible ops handbook: Starting ITIL in 4\npractical steps . Eugene, OR: Information Technology Process Institute.\nMcCumber, J. (2005). Assessing and managing security risk in IT systems: A structured\nmethodology. Boca Raton, FL: Auerbach Publications.\nMicrosoft (2006). Security risk management guide.\nhttp:\/\/www.microsoft.com\/technet\/security\/guidance\/complianceandpolicies\/secrisk\n\/)\nNational Institute of Standards and Technology. (2002). Risk management guide for\ninformation technology systems. Special Publication 800-30. Washington, DC: U.S.\nGPO. (http:\/\/csrc.nist.gov\/publications\/nistpubs\/800-34\/sp800-30.pdf)\nNational Institute of Standards and Technology. (2002). Contingency planning guide for\ninformation technology systems. Special Publication 800-34. Washington, DC: U.S.\nGPO. (http:\/\/csrc.nist.gov\/publications\/nistpubs\/800-34\/sp800-34.pdf)\nOffice of Government and Commerce. (2007). Introduction to the ITIL service lifecycle\n(ITIL Version 3). United Kingdom: The Stationary Office.\nWeill, P., & Vitale, M. (2002). What IT infrastructure capabilities are needed to implement\nE-business models? MIS Quarterly Executive, 1(1), 17-34.\nWeill, P., & Broadbent, M. (1998). Leveraging the new infrastructure: How market\nleaders capitalize on information technology. Boston, MA: Harvard Business School\nPress.\n287 http:\/\/en.wikipedia.org\/wiki\/ITIL\nhttp:\/\/en.wikipedia.org\/wiki\/Risk_management\nhttp:\/\/en.wikipedia.org\/wiki\/Backup\nhttp:\/\/en.wikipedia.org\/wiki\/Disaster_recovery\nGlossary\nAvailability Denial of access to the information or information service; an IS failure mode.\nBusiness application The software developed to automate or inform specific business activities or processes.\nBusiness continuity plan Provides detailed guidance for reestablishing critical organizational functions or\nprocesses the event of any circumstance that massively disrupts the normal operations of an organization.\nChange management Processes intended to ensure that system changes are properly authorized, prioritized\nand tested, and that all interested parties are informed regarding proposed changes.\nCold [backup] site A backup facility that contains required floorspace, electrical power and environmental\ncontrols required to support necessary information systems, but the hardware, software and communications to\nestablish those systems are not installed.\nConfidentiality The exposure of information to unauthorized persons; an IS failure mode.\nDisaster recovery plan Provides detailed guidance concerning the actions to be taken to restore operations of\ninformation system facilities in the event of catastrophic failures, e.g., flood, fire or prolonged power failure.\nFull system backups All the software and data residing on the system is backed up.\nHot [backup] site Fully operational site where essential systems, hardware and systems software, have been\nduplicated; data and application software data may need to be installed on these backup systems before operations\ncan be resumed.\nIncidents Any event that impacts the confidentiality, integrity or availability of an IT-enabled information\nservice.\nIncident management The processes that an organization has in place to respond to detected incidents.\nIncremental system backups Only data that has changed since the last full backup or last incremental\nbackup is backed up.\nInformation system failure When an information system fails to meet the organization's requirements.\nInformation technology infrastructure A foundation or platform , consisting of hardware, software,\ncommunications and the human resources, which is needed to support business applications.\nInformation Technology Infrastructure Library A comprehensive framework of IT \"best management\"\npractices developed by the Office of Government Commerce (OGC)of the United Kingdom.\nIntegrity Accidental or unauthorized modification information; an IS failure mode.\n288 Management controls: managerial processes which identify organizational requirements for system\nconfidentiality, integrity and availability and establish the various management controls intended to ensure that\nthose requirements are satisfied.\nOperational controls: include day-to-day processes more directly associated with the actual delivery of the\ninformation services.\nPolicies High-level statements communicating an organization's goals, objectives, and the general means for\ntheir accomplishment.\nProblem management The processes to broadly identify and resolve the underlying causes of incidents that\nare occurring.\nProcedures Instructions for performing policy- or standard-related tasks.\nRisk assessment An analysis of organizational assets in terms of vulnerabilities and threats capable of\nexploiting those vulnerabilities performed as a critical initial step in determining which risk mitigation techniques\nthat an organization should adopt.\nRisk Mitigation Actions designed to counter identified IS threats or causes of IS failure.\nService Level Agreements Essentially, a negotiated contract between the organization and its IT service\nprovider specifying the type and quality of IT services to be provided; the formality of this agreement may be\ndetermined to the extent within an in-house or external organization provides the IT services.\nService Level Requirements A set of operational specifications to assess the quality of individual IT services,\ntypically stated in terms of availability, performance and security requirements.\nStandards A specific class of policies representing rules, technical choices or a combination of the two.\nSystem backup Saving a copy of data stored on a system's hard drives to a different data storage facility.\nSystem monitoring Process for detecting incidents or events indicating system failure or an increased\npotential for information failure; system monitoring can be performed reactively, monitoring for actual system\nfailures, or proactively, monitoring for indications that a system failure is likely to occur.\nTechnical controls: technical capabilities incorporated into the IT infrastructure specifically to support\nincreased confidentiality, integrity and availability of information services.\nThreats Any person, object or circumstance that has the potential for causing an IS failure.\nWarm [backup] site Information systems, e.g., servers, workstations and basic communications facilities,\nexist but are not necessarily configured for operation or have required data on-site to restore services; additional\nhardware and communications capabilities may need to be installed.\n289 16. Green IS: Building\nSustainable Business Practices\nEditor: Richard T. Watson (University of Georgia, USA)\nContributors: Marie-Claude Boudreau, Adela Chen, Mark Huber (University of Georgia, USA)\nReviewer: Geoff Dick (University of New South Wales, Australia)\nLearning objectives\n\u2022 Understand the need for sustainability\n\u2022 Know the difference between green Information Systems (IS) and green Information Technology (IT)\n\u2022 Use the u-factors to analyze how IS can support a green physical system\n\u2022 Apply a framework to identify opportunities for green IS and green IT\n\u2022 Understand the need to align corporate and IS green strategies\n\u2022 Understand three different approaches to ecological thinking\nSustainability\nA global UN survey to determine the issues dominating the future identified sustainable economic development\nas the preeminent issue. The report notes, \u2018Never before has world opinion been so united on a single goal as it is on\nachieving sustainable development\u2019. The current trend in our consumption of the earth\u2019s resources is unsustainable\nand is creating major environment problems. Climate change, resource depletion, loss of biodiversity, and air\npollution have a major impact on many citizens and the earth, and we need to change our current behavior. Our\npresent use of the earth\u2019s finite resources cannot be maintained. We need to move to sustainable development,\nwhich \u2019meets the needs of the present without compromising the ability of future generations to meet their own\nneeds\u2019 (Brundtland, 1987, p. 8).\nThe environmental burden is a function of population, wealth, and technology and controlling the first two\nfactors is extremely challenging. The larger the population, the more impact it has upon the earth. In addition, the\nvast majority of people aspire to affluent lifestyles, and wealthier people consume far more resources than less\naffluent people. Technology is both a cause of the environmental burden and also a potential solution.\nTechnology such as coal-fired power stations provides the electricity we need to support an affluent lifestyle, but\nat the same time it creates carbon emissions that contribute to global warming. Alternatively, renewable energy\ntechnologies based on wind and solar, for example, are possible solutions for sustainability, though each has\nnegative consequences as well (e.g., the energy and materials required to construct wind turbines or solar panels).\nIn the IT space, the disposal of equipment is a major environmental problem because of the toxic products in\ncomputers and displays. However, IS has been the major contributor to productivity growth in many countries over\n290 the last half century. We will need IT to run the information systems that will support sustainable business\npractices.\nTechnology is an important means by which we can address our global problem. Leveraging technologies to\nproduce goods and services that are environmentally friendlier is a momentous endeavor, and may in fact\nconstitute \u2018one of the biggest opportunities in the history of commerce\u2019 (Hart, 1997).\nMany business leaders are linking sustainability to their corporate strategy. They recognize that they have key\nresponsibility to participate in solving this critical global problem and that their customers expect them to provide\ngreen products and services. Sustainability requires sustainable business practices because of the dominant role of\ncorporations in the global economy, and IS will be a major element in the transition to a sustainable economy (Esty\n& Winston, 2006)\nThe need for green IS and green IT\nThe IT industry, often at the forefront of managerial practice, is an active player in supporting sustainable\neconomic development. CIOs have identified Green IT as one of the most important strategic technologies for\n2008. We carefully distinguish between green IS and green IT. There is a key difference.\n\u2022 An information technology (IT) transmits, processes, or stores information.\n\u2022 An information system (IS) is an integrated and cooperating set of software using information technologies\nto support individual, group, organizational, or societal goals.\nGreen IT is mainly focused on energy efficiency and equipment utilization. It addresses issues such as\n\u2022 Designing energy efficient chips and disk drives\n\u2022 Replacing personal computers with energy efficient thin clients\n\u2022 Use of virtualization software to run multiple operating systems on one server\n\u2022 Reducing the energy consumption of data centers\n\u2022 Using renewable energy sources to power data centers\n\u2022 Reducing electronic waste from obsolete computing equipment\n\u2022 Promoting telecommuting and remote computer administration to reduce transportation emissions\nGreen IS, in contrast, refers to the design and implementation of information systems that contribute to\nsustainable business processes. Green IS, for example, helps an organization to\n\u2022 Reduce transportation costs with a fleet management system and dynamic routing of vehicles to avoid\ntraffic congestion and minimize energy consumption\n\u2022 Support team work and meetings when employees are distributed throughout the world, and thus reduce\nthe impact of air travel. IS can move remote working beyond telecommuting to include systems that\nsupport collaboration, group document management, cooperative knowledge management, and so forth.\n\u2022 Track environmental information (such as toxicity, energy used, water used, etc.) about the creation of\nproducts, their components, and the fulfillment of services\n\u2022 Monitor a firm's operational emissions and waste products to manage them more effectively\n291 \u2022 Provides information to consumers so they can make green choices more conveniently and effectively.\nGreen IS has a greater potential than green IT because it tackles a much larger problem. It can make entire\nsystems more sustainable compared to reducing the energy required to operate information technologies.\nGreen IS, and sustainable development, should not been seen as a cost of doing business. Rather, they are\nopportunities for organizations to improve productivity, reduce costs, and enhance profitability. Poor\nenvironmental practices result in many forms of waste. Unused resources, energy inefficiency, noise, heat, and\nemissions are all waste products that subtract from economic efficiency. Less waste means a more efficient\nenterprise. Firms that actively pursue green IS to create sustainable business practices are doing the right thing for\ntheir community, customers, investors, and future generations.\nManagers seeking to create sustainable organizations and green IS should find frameworks very useful for\nthinking about problems, brainstorming solutions, and planning implementation of innovations. Hence, it is\nimportant to provide some frameworks for assisting in the development of green IS. We start by recognizing the\nfour fundamental drives of information systems.\nThe information drives\nWe are addicted to information. People in affluent societies surround themselves with information appliances,\nsuch as cell phones, music players, and navigational systems. In the developing economies, nearly everyone can see\nthe value of a cell phone and aspires to own one. Humans\u2019 inner need for information leads them to seek\ninformation systems that provide ubiquity (e.g., cell phones), uniqueness (e.g., navigation systems), unison (e.g.,\nsynchronized calendars), and universal services (e.g., high functionality smart phones) (Junglas & Watson, 2006).\nSatisfying these four information drives is a key ingredient in creating a successful IS, and we also believe critical to\ndesigning sustainable business practices. Only recently have we had the IT to fulfill these intrinsic human\ninformation drives, mainly because of the advent of network technologies, such as the Internet, WiFi, GPS, and\nmobile phone systems.\nTable 1: The information drives and their physical counterparts\nInformational Physical\nU-construct The drive to \u2026 The drive to \u2026\nUbiquity have access to information unconstrained by have ready availability of a desired resource\ntime and space\nUniqueness know precisely the characteristics and location have the capability to tailor precisely the use\nof a person or entity of a physical resource to one\u2019s unique needs\nUnison have information consistency have procedural consistency\nUniversality overcome the friction of information systems\u2019 overcome the friction of physical differences\nincompatibilities\nIf we are to change industry and society in the direction of greater ecological sustainability, we need to\nunderstand how to satisfy the four information drives. First, let\u2019s clearly define each of the drives from both a\nphysical and informational perspective.\nUbiquity\nUbiquity, in an informational sense, is \u2019access to information unconstrained by time and space\u2019. This means, for\ninstance, that I want to be able to use my cell phone to call anyone no matter where I am in the world, or that I\n292 expect to be able find a WiFi connection in a hotel room or coffee shop so I can access the Internet. The worldwide\npopularity of cell phones is clear evidence of the strength of this drive.\nIn a physical sense, ubiquity is the ready availability of a desired resource. While we might expect that\ninformation and communication service should be accessible nearly everywhere, our expectations of physical\nresources are tempered by experience and reality. If we are to build a sustainable society, there needs to be a certain\ndensity of critical physical resources for them to be generally useful (e.g., the frequency of buses and placement of\nbus stops will affect patronage).\nAn appropriate IS can enhance physical ubiquity by supplying customers with information about the physical\nsystem. For example, people using a public transit system would find it very convenient to know the location of the\nnearest bus stop, their distance from it, the arrival time of next bus, and whether seats are available. Ubiquitous\ninformation access could be used to increase the utilization of many physical assets and thus contribute to\nsustainability.\nUniqueness\nUniqueness, from an information point of view, means \u2019knowing precisely the characteristics and location of a\nperson or entity\u2019. We can use a GPS72 to find out where we are. Companies are using RFID73 tags for identifying\nproducts (so they can look up a database to find out their characteristics) and scanners to track their movement (so\nthey know where they are). Some people embed RFID tags in their car so they can find it if it is stolen.\nPhysically, uniqueness is the capability to tailor precisely the use of a physical resource to one\u2019s unique needs.\nPeople often prefer using a personal car to taking public transportation because a car can get you can from A to B\nexactly the way you want to go. To provide higher levels of physical uniqueness for public systems, we need to\nsupport them with information systems so that consumers can more readily match available resources to needs\n(e.g., how to use a public transit system to get from the airport to a hotel).\nSome luxury cars now offer the capacity to remember a particular person's setting for the driver's seat, external\nmirrors, favorite radio, station and so forth. Each time the person hops into the driver seat, she can select her\nunique identifier (e.g., driver number), and her predefined preferences are automatically set. This example\nillustrates how an IS, the car's preference memory system, supports tailoring physical resources to a person's\nunique needs.\nUnison\nIn an information sense, unison is \u2019information consistency\u2019. People want a single source of accurate data.\nCorporations talk of the \u2019single view of the customer\u2019, which means an integrated database that contains a single\nentry for each customer. Too often, organizations have built functional systems that serve the needs of different\nsectors of the organization (e.g., marketing or production), and consequently, they can have duplicate, not\nnecessarily identical, information about customers, suppliers, and so forth in different databases. At a personal\nlevel, we prefer to have a single set of browser bookmarks that are available on whichever computer we use to\naccess the Web.74\n72 Global Positioning System.\n73 Radio frequency identification.\n74 There are plug-ins for Firefox that support bookmarks unison.\n293 Physically, unison is procedural consistency. This refers to a procedure for accessing or using a physical\nresource that has little variation across access points. A city transit system, for instance, might have the same\nprocess for buying tickets for bus, train, and water transport. Thus, consumers have to learn only one convenient\nprocedure. Because personal time is a scarce resource for many people, procedural consistency is desirable and\nreassuring.\nInformation systems can made procedures simple and familiar. They can provide easy to use interfaces that\nhide procedural complexity and integrate information across physical systems. For example, an Australian arriving\nat Paris\u2019 Charles de Gaulle airport can use a familiar ATM-like kiosk with commands in English that accepts credit\ncards to purchase a ticket for travel within Paris that covers use of three transit systems. English and the use of a\ncredit card with an ATM are all familiar procedures and highly consistent across many such encounters.\nUniversality\nUniversality, on the information side, is the drive to \u2019overcome the friction of information systems\u2019\nincompatibilities\u2019. The universality drive surfaces in standards (e.g., XML75), currency unions (e.g., the euro), and\nmulti-functional smart phones (e.g., one that includes a phone, GPS, camera, PDA, media player). An outstanding\nexample of universality in action is the metric system of measurement.76 The vast majority of countries follow this\nstandard system. Imagine the difficulty of trading if there were multiple measurement systems, which there were\nbefore standardization.\nPhysically, universality is sought to overcome the friction of physical differences. Travelers often take along\nuniversal adapters because of regional differences in electrical outlet connections. The chargers for different brands\nof laptop computers are usually incompatible because of different types of connectors. Fortunately, we have\nstandards, such as USB,77 that facilitate the transfer of data between computers.\nInformation systems can help the transition between physical systems. For example, some power supplies for\nlaptop computers can sense the characteristics of the power supply (i.e., voltage and frequency) and transform the\ninput to that required by the computer. The sensor is a simple information system. Physical payment systems are a\nmajor form of inefficiency (e.g., national currencies), and a smart card based information system can lubricate\npayment by storing multiple currencies and dynamically converting between them when payments are made.\nIf a system is to serve its customers, then it should satisfy the four u-drives from both a physical and\ninformational sense. Given the flexibility of information, it is much easier to provide high levels of the informational\nu-drives and, in so doing, increase the utility of the physical drives. An example illustrates the symbiotic78\nrelationship that can be established between physical and informational systems to reduce carbon emissions..\nV\u00e9lib\nV\u00e9lib (a short form of V\u00e9lo Libert\u00e9, i.e., Bicycle Freedom) is the world\u2019s largest public self-service bicycle rental\nsystem.79 The mayor of Paris launched the project in July 2007 to reduce the number of cars on the French capital\u2019s\n75 XML is a language for data exchange that makes it simpler for the exchange of data by creating industry\nstandard descriptions of items such as a credit card statement, an airline reservation, and medical record.\n76 Also known as the International System of Units (SI) (\u2019Syst\u00e8me International d\u2019Unit\u00e9s\u2019 in French, hence \u2019SI\u2019).\n77 Universal serial bus (note the use of universal)\n78 Symbiosis describes the interaction between two different organisms living in close physical association, and it\nthis case we are applying this concept to two systems, the physical and informational, that can be designed to\ninteract with each other.\n79 http:\/\/www.velib.paris.fr\/\n294 roads. The intention is to enable Paris\u2019 citizen to use bicycles, instead of cars, for short trips (typically less than 30\nminutes) within Paris. V\u00e9lib started with 10,648 bicycles and 750 stations and by early 2008 there were 20,000\nbikes and 1,450 stations. The project is a great success and recorded 2 million trips on the first 40 days. Its\noperation is simple; subscribers go to a station, identify themselves, and rent a bicycle. The first 30 minutes of\nrental are free, with an incremental cost thereafter. Subscriptions are available for one day (\u20ac1), for one week (\u20ac5),\nor for a year (\u20ac29), with a security deposit of \u20ac150.\nV\u00e9lib\u2019s information system is state-of-the-art for bicycle-sharing programs. Each station has a computer\nterminal (\u2018borne\u2019), from which an individual can purchase a subscription, recharge an account (for one-year\nsubscriptions), determine the number of available bicycles at nearby stations (useful if the current location is\nempty), or see the state of that person\u2019s account. Stations consist of a series \u2018bornettes\u2019, with each bornette\nconsisting of a bicycle stand, locking mechanism, and a swiper to read the subscriber\u2019s information. All bikes are\nuniquely identified with an RFID tag.\nWe now examine how the informational and physical, respectively u-factors [shouldn't we stick to u-drives?]\nreinforce each other to contribute to V\u00e9lib\u2019s success. Informational ubiquity is high because members can\ndetermine bicycles availability for any given station from any device that can connect to the Internet (e.g., a cell\nphone) or from another station. On the physical side, ubiquity is also high because bicycle stations are 300 meters\napart, more than four times the density of M\u00e9tro stations, which is very impressive because Paris\u2019s subway stations\nare the most closely spaced of any such mass transit system. Bicycles and customers are uniquely identified.\nConsequently, the beginning and ending stations of every ride can be tracked, and this information can be used to\ndecide the placement and size of stations and when to move bicycles between them. Renters can tailor their ride to\ntheir personal needs quite accurately (at most 300 meters from the desired destination). A single integrated\ndatabase is used to keep complete records of stations, bikes, customers and so forth. Furthermore, there is a\nstandard process for rental (i.e., the same type of rental and payment procedures for all stations). Because the\ninformational and physical aspects are both highly consistent , the system is high on the two types of unison.\nInformational universality is high, because payment and subscription systems work across all bicycle stations.\nV\u00e9lib terminals accept all traditional forms of payments (bank card, credit card, and cash), including the Moneo\ncard, an electronic purse system for cashless small purchases. The Navigo smartcard, which works with the entire\nParisian public transportation system, also gives access to V\u00e9lib for one-year subscribers. In the physical domain,\nuniversality is high because of the uniformity of bicycles, which reduces human learning and gives economies of\nscale in manufacturing and maintenance.\nSystems design\nThe V\u00e9lib case illustrates the importance of co-designing physical and informational systems. In this case, the IS\nincreases the convenience of V\u00e9lib. Customers can quickly find the nearest free bike, pay for a rental rapidly and\nsimply, and be on their way. V\u00e9lib\u2019s managers can monitor the demand at each station and increase or decrease the\nnumber of bikes to maximize utilization. Information adds value to physical systems and in so doing increasing\ntheir potential patronage. High quality public systems, from both in a physical and informational sense are required\nto create a sustainable society.\nA frameworks of sustainability options\nThere are three types of sustainability goals (Hart, 1997). The first goal is to prevent pollution by minimizing the\nlevel of emissions, effluents, and wastes. The second and higher level goal is product stewardship, where one\n295 focuses on both reducing pollution and also minimizing the adverse environmental effects associated with the full\nlife cycle of a product. This is also known as the \u2019cradle-to-cradle\u2019 approach, where the end state of a product is\ninvolved in the beginning of another. The third and ultimate goal is the use of clean technology that creates no\nharmful emissions or waste.\nThe three goals can apply at three different levels: individual, organizational, and societal. The combination\nframework (Table 2) can be used to identify opportunities to deploy IS or IT to improve sustainability. We now\ndiscuss each of the cells.\nTable 2: Green IS and IT opportunities\nIndividual Organizational Societal\nPollution Prevention flexible printing thin client electronic exchange of\ncapabilities information\nvirtualization\nautomated energy congestion systems\ntelecommuting\nconservation system\nProduct Stewardship recycling reuse components governmental policies\nrecycle computers societal norms\nClean Technology paperless interaction video conferencing open source\ncollaboration tools Smart homes and\nappliances\ne-commerce vs. traditional\ncommerce\nIndividual pollution prevention\nThere are many actions that you can take to reduce the IT impact on pollution. For example, you can turn off\nyou computer when you will not use it for some hours. You might practice shutting it down when you go to bed. You\ncould print on both sides of a sheet of paper (i.e., duplex) or turn on the energy conservation preferences for your\noperating system (see Exhibit 1) so that your computer will go to sleep after a certain period of inactivity. Flexible\nprinting capabilities exist for most operating systems; yet, they are rarely activated. It is estimated that applying\nenergy settings, such as \u2018sleep when inactive\u2019, can reduce greenhouse gas emissions at a rate equal to taking more\nthan 8,000 passenger cars off the road for an entire year, or conserving 16 million liters of gasoline.80\nIndividual product stewardship\nIn additions to using energy more efficiently, you can play a significant role in product stewardship, such as in\nrecycling used electronic products. For example, in our city there is a group81 that takes unwanted computers,\nrefurbishes them, installs Linux and OpenOffice, and gives them to charitable organizations. Organizations are\nrelying on you to support cradle-to-cradle manufacturing. When you decide to dispose of an electronic product,\ncheck its manufacturer\u2019s web site for recycling options and procedures.\n80http:\/\/www.techworld.com\/green-it\/features\/index.cfm?featureid=3496\n81 http:\/\/freeitathens.com\/\n296 Information systems can facilitate product stewardship by providing information and creating networks to\nsupport recycling. Started in 2003, the Freecycle Network,82 promotes waste reduction by providing individuals and\nnon-profits an electronic forum to recycle unwanted items. As they say, \u2018one person\u2019s trash can be another\u2019s\ntreasure\u2019. The Freecycle concept has since spread to over 75 countries and includes millions of members. Freecycle\nclaims to keep over 275 metric tons of goods per day out of landfills.\nIndividual adoption of cleaner technology\nMany of us find it difficult change established habits. Substitution is the simplest change. For example\nsubstituting check writing by paying bills online is a relatively easy change with a positive impact on the\nenvironment. It is faster and more convenient, and adds up: If every US home received and paid its bills online,\nannual greenhouse gas emissions would drop by 1.9 million metric tons, and waste would be reduced by nearly 1.45\nmillion metric tons per a year.83 UNESCO reports that of the average 1,510 sheets of paper produced per person in\nthe world per year, at least half of this sheets goes through printers and copiers to produce office documents. A\nsingle tree produces about 80,500 sheets of paper.84 Electronic media can be more environmentally friendly than\npaper. Acquiring news, music, movies, and books in electronic format is now possible because of the technological\ninfrastructure and information systems in place. E-books (such as Sony\u2019s eBook reader) can reduce paper\nconsumption. While the earlier e-readers are quite expensive, expect the cost to decline with time and volume.\nMany cleaner technologies rely on an IS. The iPod (for music and movies) is backed by a sophisticated IS called\niTunes. Amazon\u2019s Kindle (for books, magazine, and newspapers) is supported by a similar IS. The Toyota Prius, the\nworld\u2019s most popular hybrid car, contains multiple computer chips to run its many information systems. It needs\nan IS to decide when the run the gas and electric engines, when to charge the battery, and what information to\ndisplay to the driver.\nOrganizational pollution prevention\nOrganizations can redesign their IT infrastructure to make it more energy efficient. A thin client, a lean PC that\nrelies on a central server for disk storage and applications processing, uses less energy than a regular PC. Verizon,\nfor example, reduced energy consumption by 30 percent by replacing personal computers in its call center with thin\nclients.85 Germany\u2019s Fraunhofer Institute reports that, when comparing thin clients to personal computers, energy\nconsumption is at least twice as low, even when factoring in the additional energy and cooling power required by\nthe server associated with the thin clients. In addition to the reduction of emissions, e-waste is also reduced by\nswitching to thin clients. A thin client contains significantly fewer components and has a longer life expectancy than\na regular PC.\nVirtualization, software running on a virtual foundation rather than the physical hardware, has become a\npopular energy saver. Server virtualization (the most common form of virtualization) makes the physical resource\n(i.e., the server) function as multiple logical resources (e.g., running multiple operating systems). Virtualization\nmeans doing more work with fewer resources, which in turn frees up data center space and lowers energy bills.\nVirtualization has existed in the computer industry for decades, but it is now getting a lot of attention because of its\n82http:\/\/www.freecycle.org\/\n83http:\/\/www.time.com\/time\/specials\/2007\/environment\/article\/0,28804,1602354_1603074_1603109,00.html\n84http:\/\/www2.sims.berkeley.edu\/research\/projects\/how-much-info-2003\/print.htm\n85http:\/\/www.businessweek.com\/technology\/content\/may2007\/tc20070514_656985.htm?campaign_id=rss_tec\nh\n297 capacity to reduce energy consumption by increasing the utilization of excess or idle capacity possessed by existing\nhardware.\nOrganizational product stewardship\nSustainability requires that we develop extensive recycling systems and we change our behavior to think of\nrecycling as the first step when we dispose of items we no long want. In many cases, electronic goods are not\nrecycled because organizations have not created the procedures and information systems to facilitate recycling.\nMost organizations have complex information systems for manufacturing, distribution, and sales to get their\nproducts into the homes of consumer, but few go the complete cycle. That is, they don\u2019t consider that they are\nresponsible for taking back products that their customers no long want. A few forward looking companies, however,\nhave created such full cycle systems. Dell, for example, allows its customers to recycle their old printers (if they buy\na new one) by simply providing the new printer\u2019s service tag and scheduling a pick-up; customers accomplish both\nactivities online via Dells\u2019s web site. Dell has created a simple and convenient process, assisted with an IS to track\nthe movement of the products to be recycled. It can also gain by recycling some of the returned products, or parts of\nthem, where this is possible.\nOrganizationally cleaner technology\nHumans prefer a face-to-face meeting over a telephone conversation on many occasions because of the richness\nof the interaction. Face-to-face meetings, however, can consume considerable energy when the attendees are\nscattered across the globe. Video conferencing is a good alternative, particularly with today\u2019s high quality systems.\nVideo conferencing can transcend distance to replicate face-to-face communication. In the era of globalization and\nglobal climate change, organizations need to substitute cleaner technologies, such as video conferencing and\nelectronic collaboration tools, to bridge the distance when a meeting\u2019s participants or a work team are scattered\nacross different cities, countries, and continents. Electronic distributed meetings support communication without\nthe carbon footprint of travel.\nOrganizations can also used cleaner technology (e.g., solar or hydro power) to run their data centers. Data center\nenergy consumption is one of the most important green technological concerns because power and cooling account\nfor up to 40 percent of a data center\u2019s costs.86 Google, Yahoo, and Microsoft have located some of their data centers\nin the Pacific Northwest of the U.S., close to cheap hydro-electric power. Some organizations are looking at solar\npower for their data centers.\nSocietal pollution prevention\nCountries and economic regions can reduce pollution by encouraging a shift to technologies that produce less\nemissions. In the case of IS, the energy cost of exchanging data can be significantly reduced by moving from the\npostal system to electronic networks. Electronic Data Interchange (EDI), for example, supports the majority of\nelectronic commerce transactions. Depending on which standard is in use (ANSI ASC X12 in North America and\nEdifact elsewhere), structured information can be interchanged between and within organizations, governments,\nand other groups. In a similar way, XML supports the electronic exchange of information through an open\nstandard. Both of these technologies can reduce the use and manipulation of physical administrative documents\n(e.g., invoices, sales orders, etc.), and thus minimize pollution. They also, as is the case with most efforts to increase\nsustainability, greatly reduce an organization\u2019s administrative costs.\n86 http:\/\/www.cioinsight.com\/c\/a\/Trends\/The-Greening-of-the-CIO\/1\/\n298 Traffic congestion is a major issue for most large cities. It wastes energy and increases pollution. Cities, such as\nSingapore and London, now levy fees for the use of particular city roads, with the help of information system, to\nreduce congestion. The US Department of Transportation employs the intelligent transportation system (ITS) to\noptimize public transportation by reducing congestion, improving road safety, and enhancing productivity. ITS is\nbuilt upon a broad range of wireless and wire line communications-based information and electronics technologies.\nThe US federal government is fostering widespread deployment of the system by integrating it into the\ntransportation system\u2019s infrastructure. Current applications of ITS system include computer aided dispatching of\nvehicles, automatic vehicle location for public buses, and electronic toll booths that do not require driver to stop,\nand electronic freeway surveillance.\nSocietal product stewardship\nGovernments can play an active role in encouraging, and where necessary forcing, organizations to become\nbetter product stewards. Legislation is being used to make recycling of electronic products mandatory. The\nCalifornia state government, for instance, introduced an Electronic Waste Recycling Fee in 2004 on all new\nmonitors and televisions sold. California\u2019s Electronic Waste Recycling Act mandates that retailers collect a set\nrecycling fee and pass it on to the Board of Equalization. British Columbia in Canada has a similar policy. In 2003,\nthe European Union enacted the Waste Electrical and Electronic Equipment Directive (WEEE Directive), which has\nbecome European Law, setting collection, recycling, and recovery targets for all types of electrical goods.\nSocietal cleaner technology\nAn information society that consumes (e.g., downloading movies via the Internet rather than renting from a\nlocal store) and exchanges information electronically (e.g., emailing rather than posting a letter) is cleaner than a\nsociety in which information exchange is based on paper and the postal system. An information society can also\norganize for the production and distribution of electronic goods to be cleaner. The open source model is a very good\nexample of a cleaner form of production and distribution. Software is developed without requiring the physical\npresence of workers in the same physical space, that is, an office building and its significant infrastructure and the\nenvironmental costs of daily commutes. Moreover, once developed, open source software can freely flow across\nborders at electronic speed, without the need for wasteful packaging and retail store shelf space. The footprint\nassociated with both production and distribution can be much lower for information products.\nBeyond information products, the information age needs to find many other ways in which it can deploy IS to\nminimize society\u2019s ecological footprint. We need a generation of innovation to create a sustainable society, and\nmuch of this innovation will involve IT and IS is a variety of ways.\nOrganizational perspectives\nOrganizations are the major force for innovation in most societies, and corporations in particular are major\nchange agents. As a result, we further examine some frameworks for promoting thinking about organizational\nsustainability.\nStrategic alignment\nNearly all major enterprises establish a corporate strategy that guides their major actions and set directions for\nthe future. To achieve societal sustainability, we need the great bulk of major corporations to incorporate\nsustainability as part of their corporate strategy. As most of the major firms are global, we turn to a global strategic\nframework as the foundation for discussing how enterprises can approach integration of sustainability into their\n299 corporate strategy. Corporations who move faster and more effectively than those in their industry to create\nsustainable business practices should gain a competitive advantage. Eliminating waste increases profitability, and\norganizations need to learn how to operate in a world in which emission constraints are a part of doing business.\nStrategic issues can be addressed by from the perspective of aggregation, adaptation, and arbitrage, the AAA\ntriangle (Ghemawat, 2007).\nAggregation\nOrganizations strategically seek economies of scale by aggregating development and production processes.\nThe intention is to reduce costs by combing activities into optimal units for efficiency. From a sustainability angle,\norganizations also want to aggregate activities to reduce emissions and waste.\nWal-Mart's green supply chain\nThrough emphasizing a green supply chain, Wal-Mart plans to create less waste. It has taken\nstrategic action to minimizing packaging. The idea is to reduce the size of products to save energy,\nshipping costs, and shelf space. It wants vendors to think \u2018small and mighty\u2019 by aggregating goods\nin the minimal space. It has, for instance, convinced vendors to replace bulky plastic jugs with\ncondensed, slimmed-down containers for liquid laundry detergents. Toilet paper manufacturers\nhave compacted their products so that a greater quantity can fit in a given volume. Through such\ninitiatives, Wal-Mart aligns one of its key goals, low cost leadership in retailing, with sustainability\nbecause its movement in the direction of sustainable business practices reduce emissions, wastes,\nand costs.\nIS can be used to measure and monitor the costs, emissions, and waste of each phase of a supply chain and\npackaging alternatives. It is also a tool for coordinating and aggregating the many activities in a supply chain to\nminimize overall emissions. From an IT perspective, aggregation describes actions such as locating servers in a\nsingle data center to reduce energy costs. Virtualization can be thought of as aggregating several software systems\nonto one server to increase utilization and lower costs.\nAdaptation\nAdaptation defines an organization\u2019s efforts to maximize its local relevance by being responsive to local\nstakeholders\u2019 needs and desires. Again, this can be done by exploiting in the power of IT and IS strategically. From\nan environmental perspective, this means adopting specific environmental initiatives that reduce emissions and\nwastes in the communities in which the organization operates.\nEverything and Everybody Connected to the Network\nSun Microsystems Inc. is a global provider of network computing infrastructure solutions. Its\ncorporate vision of \u2018Everything and Everybody Connected to the Network\u2019 is reflected in its desire to\nlet \u2018everyone take part in opportunities and contribute to solutions regardless of their geographic\nlocation or economic situation\u2019 (company web-site). In this spirit, Sun created the \u2018Open Work\u2019\ninitiative, which consists of a solution suite of products, policies, and support tools that enable Sun\nemployees to work effectively wherever their work takes them, may this be at the office, at home, on\nthe road, or in drop-in centers. The company has about 43 percent of its workforce participating in\n300 this program, utilizing its 115 flexible office locations worldwide. Through its initiative, Sun thus\nfosters the use of cleaner technology through a program that is in alignment with its strategic\norientation.\nSun Microsystem's \u201cOpen Work\u201d initiative, expands the definition of local and enhances its\nemployees' abilities to work locally while competing globally. At the same time, reducing the need\nfor employees to travel to work locations and reducing Sun's employees' overall carbon footprint.\nArbitrage\nArbitrage, the third global strategy, is the exploitation of differences between different markets. Considering the\netymology of the word arbitrage, its French origin defines it as \u2018rendering judgment\u2019. The underlying idea of this\nstrategy is to achieve absolute economies through judging (and selecting) the very best alternatives. In the context\nof an environmental IT and IS initiative, this can viewed as achieving the most environmentally friendly product by\nselecting the least polluting vendors.\nCradle-to-cradle design at Herman Miller\nHerman Miller, the international designer, manufacturer, and distributor of furnishings and\ninterior products, follows an arbitrage strategy. Given that sustainability is one of its core values,\nHerman Miller has taken many green initiatives to distinguish itself from its competitors. One of\nthese is the development of a cradle-to-cradle design into its products, such that all constituent\ncomponents in a given product can be put back into service. This initiative led to the creation of an\nIS (the Design for Environment system, DfE), which allows Herman Miller to assess the extent to\nwhich a final product meets the goal of the cradle-to-cradle ideal, that is, made from 100 percent\nbiological or technical nutrients. With DfE, Herman Miller can thus assess the components it\nacquires from its suppliers, and if a component does not meet its cradle-to-cradle metric, Herman\nMiller either helps the supplier to make needed formulation changes in its component, or, if the\nsupplier is unwilling or incapable of making such changes, seeks an alternative supplier. In other\nwords, Herman Miller is changing its supply chain to include only the suppliers that can contribute\nto the achievement of its sustainability goal. Thus, Herman Miller\u2019s goal of product stewardship is\nconsistent with its values and is addressed by an arbitrage strategy.\nIn summary, the most successful sustainability initiatives will be those carried on by organizations aligning their\ngreen IS initiatives with their overall strategies, in ways that will achieve their business goals while simultaneously\nreinforcing their environmental goals, i.e., the reduction of pollution, product stewardship, or cleaner technology.\nThree approaches to ecological thinking\nOrganizations strive to sustain their existence, and the notion of corporate sustainability incorporates ecological\nthinking and three different approaches to it: eco-efficiency, eco-equity, and eco-effectiveness. We now discuss each\nof these idea\nEco-efficiency\nEco-efficiency combines traditional efficiency goals with ecological considerations, and is defined as, \u2019the\ndelivery of competitively-priced goods and services that satisfy human needs and bring quality of life, while\n301 progressively reducing ecological impacts and resource intensity throughout the life-cycle to a level at least in line\nwith the earth\u2019s carrying capacity\u2019 (DeSimone & Popoff, 2000, p. 47) Quite simply, it means consuming non-\nrenewable materials more productively. Under eco-efficiency, financial goals still remain foremost in management\u2019s\nmind, but it should be mindful of the need to pursue sustainable practices where they do not interfere with financial\nconsiderations.\nAll waste products are a cost that a company has to bear, unless it can externalize them and make the\ncommunity pay. For example, a carpet manufacture that has chemicals left over after production is legally required\nto dispose of these in a in a manner that does not damage the environment. This can be a costly process, and the\ncarpet manufacturer would be more profitable if he did not have to dispose of these chemicals. The ecological\napproach is to switch to chemicals that don\u2019t harm the environment and thus avoid high disposal costs, or even\nbetter, the firm finds a way that requires no chemicals and thus avoid the costs of buying the chemicals.\nUnfortunately, in too many cases industry passes on the costs of its eco-inefficiency to the community. It\n\u2018externalizes\u2019 its costs. A company that pollutes a stream with its waste products forces society to deal with the costs\nof environmental degradation. If the company were forced to bear the full cost of its polluting activities, it would\nhave a strong incentive to be eco-efficient.\nEco-equity\nEco-equity aims for the fair distribution of natural resources between current and future generations. One group\nin society should not consume so much that it denies other members of its generation their fair share of that\nresource. Similarly, one generation should not over consume a resource to the point that it is unavailable or\ndegraded for a future generation.\nThere is limited knowledge of the Earth's total stock of critical resources such as oil and water. Before we can\nstart to implement eco-equity as a societal goal, we first need to know what resources we have and how rapidly they\nare being consumed. We need a data base for the full range of global resources with details of available stocks and\ndepletion rates. Then, we need to develop methods for determining equitable distributions between and across\ngenerations. We cannot achieve eco-equity if we have insufficient data to determine what is equitable.\nEco-effectiveness\nEco-effectiveness means that we end current practices that result in ecological degradation. We need to mimic\nnature and create ongoing healthy systems where the waste products of one process become inputs to other\nprocesses. For example, a tree\u2019s dead leaves become food for insects and nutrients for the soil. Natural systems have\nhad millions of years to evolve. Initially, waste products might have remained unused for eons until a species\nlearned, or evolved, that could use the waste as its food. An eco-effective conversion cycle creates minimal waste.\nIn the industrial world, we can use information systems to accelerate the matching between output and input.\nWe need to create information markets where the producers of the waste from one process can find a buyer, who\nwill use the waste as input to another process. Eco-effectiveness requires a highly efficient information exchange so\nthat the waste market continually clears. Attaining eco-effectiveness means that toxic dumps are no longer required\nand that landfills are a historical oddity that reminds future generations of their profligate forebears.\nSummary\nSustainable development \u2018meets the needs of the present without compromising the ability of future generations\nto meet their own needs\u2019. Information systems, as the major force driving productivity growth in the last half\n302 century, should have a critical role in creating sustainable business systems. Green IT is mainly focused on energy\nefficiency and equipment utilization. Green IS refers to the design and implementation of information systems that\ncontribute to sustainable business processes. There are several frameworks for identifying Green IS opportunities.\nFirst, the information drives (ubiquity, uniqueness, unison, and universality). Second, sustainability options\n(pollution prevention, product stewardship, and clean technology) by action levels (individual, organization, and\nsocietal). Third, strategic alignment (aggregation, adaptation, and arbitrage) of IS with the enterprise. Fourth,\necological thinking (eco-efficiency, eco-equity, and eco-effectiveness).\nExercises\n1. What could your university do to increase its sustainability? How might students help?\n2. What personal actions could you take to reduce energy consumption? What behaviors are you likely to\nchange on an ongoing basis?\n3. Thinking of a business process with which you are familiar, such as a stock ordering system, using the U\ninformation drives, outline how IS might improve sustainability in that process?\n4. Using the U drives model, evaluate the public transport system in your city or town. How well does it meet\nthe four information drives? How might information systems be improved to increase utilization of the\ntransport system.\nReferences\nBrundtland, G. H. (1987). Our Common Future: Report of the World Commission on Environment and\nDevelopment. Oxford: Oxford University Press.\nDeSimone, L. D., & Popoff, F. (2000). Eco-Efficiency: The Business Link to Sustainable Development. MIT\nPress.\nEsty, D. C., & Winston, A. S. (2006). Green to Gold: How Smart Companies Use Environmental Strategy to\nInnovate, Create Value, and Build Competitive Advantage (1st ed., p. 384). Yale University Press.\nGhemawat, P. (2007). Managing differences: the central challenge of global strategy. Harvard Business\nReview, 85(3), 58-68, 140.\nHart, S. L. (1997). Beyond greening: Strategies for a sustainable world. Harvard Business Review, 75(1), 66-\n76.\nJunglas, I. A., & Watson, R. T. (2006). The U-Constructs: Four Information Drives. Communications of AIS,\n17, 569-92.\n303 17. Moving forward as a\nsystems innovator\nEditor: Paul Bauer (University of Denver, Denver, USA)\nLearning objectives\n\u2022\nThis chapter is about creating the software-based products and services of the future (and what\nsignificant ones won't be)?\nMoving Forward as a Systems Innovator\nIn the little over a half century since the first commercial computer was introduced, there has been phenomenal\nimprovement in the performance to price ratio of these devices, not to mention the significant reduction in size. In\nthe summer before graduate school, I worked for a company which made me responsible for running a critical\nproduction computer which had 4 kilo-words of core memory, a 30 kilobyte drum, 1-inch magnetic tape drives, a\npaper tape reader, a crude assembler, no compiler, and required 1000 square feet of floor space. The machine was\nreported to have cost over $1 million. The cell phone in your pocket today dwarfs that early computer in processing\npower by several orders of magnitude. A friend sent me an email just last week saying that he had found a 500\ngigabyte hard drive on sale for $100. His comment was that less than fifteen years ago when he was working on a\nproject to provide movies on demand over a cable network, he priced out terabyte hard drives at $10 million each.\nAgain we see five orders of magnitude improvement in performance to price in less than 15 years. A very relevant\nquestion for a systems innovator is what key improvements are to come? And, of course, the answer to that\nquestion depends on what systems innovators do in the years to come. To explore this topic, we will look at the\npromise of information technology, the challenges we face in realizing the promise, and some of the resources\navailable to help guide the systems innovator.\nThe Promise of Information Technology\nThe Industrial Revolution significantly improved mankind's living standard by replacing muscle power with\nmechanical devices driven by chemical, electrical, and other forms of energy. The promise of information systems,\nalready realized to a great extent, is to produce a similar transformation not only in our mental capabilities but\nmore importantly in our ability to communicate. Communications is at the heart of commerce-we tend to not do\nbusiness with people we don't know or cannot trust. And as the pendulum of commerce has swung from one-on-\none interaction with artisans and craftsmen to mass production of essential and even luxury goods, and now back\nagain toward mass customization-mass producing goods which meet the unique requirements of each customer-\ninformation about markets, market segments, and market segments of one has become vitally important.\nInformation technology not only gives us the opportunity to capture the required data, but to use them effectively in\ndealing with these diverse market populations. How many individual customer preferences can you hold in your\nhead? Writing them down on paper significantly increases this number but also increases the work to organize\n304 them to find or share a specific one when needed. Information systems allow us to both increase the number\nindefinitely and retrieve quickly a specific one as needed.\nComputers are actually very dumb devices capable of dealing with only ones and zeroes in extremely logical\nways. Business people, indeed most people, don't normally think that way. So initially a lot of energy went into\ntrying to \"think like a computer\" to get them to produce results of value, at the expense of focusing on user\nrequirements. This led to a schism between the techies and the tycoons; IT folks were seen as more interested in the\ntechnology than in the business objectives. As computers have gotten easier to program, we have made progress in\nclosing that gap. Although in many companies today, there is still a process which creates a strategic plan for the\ncompany or division, then appliqu\u00e9s onto it an \"IS strategy or plan.\" The opportunity here is to recognize that\ninformation technology, while becoming in one sense a commodity like electricity or water, in another sense will\nnever become a commodity because it enables one to generate, gather and use information in unique ways. So\ninstead of thinking of how IS can support the company strategy, leading edge companies are building their\nstrategies around what information can be obtained and how they can use that information for competitive\nadvantage. Their business models are built on capturing, creating, and effectively using information. And the\ncapabilities of information systems, both extant and envisioned, are an integral part of that business model and\ncompetitive strategy.\nIn a CAIS working paper, Vasant Dhar [footnotes shown as such] identifies three invariant concepts upon which\nthinking about future business models and industry structure can be based:\n1. rendering of things as information; for example, a bank balance rather than physical money as an indication\nof wealth.\n2. exponential growth of hardware power, bandwidth, storage and the accompanying miniaturization of IT-\nbased devices; and\n3. sustained increase in programmability through modular software.\nThe consequences of these invariants are of substantive and lasting importance:\n4. digitization facilitates the separation of information from artifacts which alters the fundamental economics\nof a number of industries, such as music, film and publishing.\n5. IT infrastructures are becoming more powerful and more accessible; high speed digital connections now\nreach a large percentage of businesses and residences.\n6. the importance and variety of \"spaces of interactions\" in society that are mediated by IT are growing; and\nfinally\n7. more data about these spaces of interaction are made available as is the ability to process these data\nintelligently.\n\"These consequences suggest a future for business that is inextricably intertwined with information technology.\"\nFor the systems innovator of the future, this is a good starting point.\nThe Promise of Business\nBusiness likewise is undergoing fundamental shifts with a new emphasis on sustainability and a triple bottom\nline. The concept of sustainability often carries overtones of \"environmentally friendly\" or \"green\", but actually\n305 deals more broadly with the ability of a company to meet its goals today as well as position itself to meet them in the\nfuture. Couple environmental concerns with economic and social concerns, and you have the triple bottom line on\nwhich many companies today are reporting. Information technology again not only can be a catalyst and vehicle of\neffective execution, but it raises many relevant concerns of its own in this area. As John Thakara points out in \"In\nthe Bubble\"[footnotes shown as such], \"it takes 1.7 kilograms of materials to make a microchip with 32 megabytes\nof random-access memory-a total of 630 times the mass of the final product. The 'fab' of a basic memory chip, and\nrunning it for the typical life span of a computer, eats up eight hundred times the chip's weight in fossil fuel.\nThousands of potentially toxic chemicals are used in the manufacturing process\"\u00a6 The amount of waste matter\ngenerated in the manufacture of a single laptop computer is close to four thousand times its weight on your lap.\nFifteen to nineteen tons of energy and materials are consumed in the fabrication of one desktop computer. To\ncompound matters: As well as being resource-greedy to make, information technology devices also have notoriously\nshort lives. The average compact disc is used precisely once in its life, and every gram of material that goes into the\nproduction and consumption of a computer ends up rather quickly as either an emission or solid waste. In theory,\nelectronic products have technical services lives on the magnitude of thirty years, but thanks to ever-shorter\ninnovation cycles, many devices are disposed of after a few years or months.\" So while information technology\nmakes more plausible achieving the promise of sustainability in business, it also adds significantly to the challenges\nfor the systems innovator.\nThe Challenges of Information Technology\nRapidly changing technology, alluded to above, is a key challenge for the systems innovator. Most people today\nare familiar with Moore's Law: early in the life cycle of the large scale integrated circuit, Gordon Moore predicted\nthat the density of components on a chip would double every 18 months. For over thirty years, the semiconductor\nindustry has made good on that prediction. Think about that for a minute: if you double something every 18\nmonths, in 30 years you would have doubled the amount 20 times, or produced 220 ~ 106 (210 = 1024 ~ 103), or a\nmillion-fold increase. So if at the start there were 1000 components on an integrated circuit (which is the order of\nmagnitude achieved on the first memory chips in 1971), then today a chip should have a 1000 x 106 or a billion\ncomponents. Intel announced in 2004 an SRAM chip with over a half billion transistors using 65 nanometer line\nwidths.\nThe kind of challenge this presents can be seen in operating system programming. Early operating systems were\nvery tightly coded because memory was scarce and cycle times relatively long. With today's billion transistor\nprocessors operating at gigahertz cycle times, that is no longer the case; Microsoft's Windows XP operating system\nreportedly has over 35 million lines of source code. And that, contend some, is inherently a problem since one\nsoftware bug per 1000 lines of code is thought to be the currently achievable quality level. Further, such operating\nsystems can take tens of seconds to boot up while they run though files for literally thousands of drivers for which\nno device is attached because it was easier to program that way. Even though it appears we will soon reach the\nphysical limit of Moore's Law, the rapid pace of technology change will probably not slow as new techniques are\nintroduced to compensate.\nRapidly changing technology leads directly to another challenge for the systems innovator: growing\nexpectations. From Andrew Tanenbaum's web site (http:\/\/www.cs.vu.nl\/~ast\/reliable-os\/): \"TVs don't have reset\nbuttons. Stereos don't have reset buttons. Cars don't have reset buttons. They are full of software but don't need\nthem. Computers need reset buttons because their software crashes a lot. I know that computer software is different\n306 from car software, but users just want them both to work and don't want lectures why they should expect cars to\nwork and computers not to work.\" Tanenbaum further observes that TVs, stereos, and cars don't take 30 to 50\nseconds to boot up. They start immediately when turned on. Users aren't interested in the gee whiz factors of the\ntechnology. They just want it to do useful things for them. And given the increases in computer power, they expect\nmore and better useful things.\nA third challenge for the systems innovator is managing the \"soft side\" of technology innovation. In \"The\nInmates Are Running the Asylum\" Alan Cooper [footnotes shown as such] tells us that programming is such a\ndifficult and absorbing task, that the creation of software is so all-consuming that programmers must immerse\nthemselves in an alien thought process which supersedes the demands of the user. The goals of the programmer\nand the goals of the user are different, and the latter usually loses out. And that is tragic because when we let our\nproducts frustrate, cost, confuse, irritate and maim us, we are not taking advantage of the real promise of software\nbased products: \"to be the most human, powerful, and pleasurable creations ever imagined.\"\nSome Resources for Systems Innovators\nFortunately significant progress has been made over the last fifty years, and there are known resources the\nsystems innovator can draw on.\nThe whole process of innovation has been studied extensively (see, for example, Christensen, The Innovator's\nDilemma, The Innovator's Solution, and Seeing What's Next; or Geoffrey Moore's Crossing the Chasm, Inside the\nTornado, and Dealing with Darwin.) New product development and project management, two specific pieces of the\ninnovation process, have also been widely studied and documented. The Project Management Institute\n(www.pmi.org (http:\/\/www.pmi.org)) has certification levels, training materials to help achieve these certifications,\nand periodic meetings of local chapters to hone and maintain skill sets. Similarly the Product Management and\nDevelopment Association (www.pdma.org (http:\/\/www.pdma.org)) is a loose federation of professionals involved\nin the new product development and delivery process who share experiences and insights. Both organizations have\nextensive libraries which capture many of the successes and failures of the past, so that we can learn from others'\nmistakes rather than repeat them all ourselves.\nScenario planning is another indispensable tool for the systems innovator. Originally pioneered by Royal Dutch\nShell before the energy crisis of the early 1970s, this technique is used by businesses today to plan their strategies.\nSince the most important technology related events in business are often disruptive events, linear projections of\npast performance are of little value in \"seeing the future\". With scenario planning, one envisions several futures-a\ndesirable one, a troubled one and one somewhere in between. Assumptions leading to each of these possible futures\nare then captured and tracked, along with contingency plans related to each assumption. This puts the company in\na position to quickly take advantage of opportunities and sidestep pitfalls. Since complex IT systems often require\nlong lead times, combining scenario planning with modular, agile development techniques helps systems\ninnovators be more responsive to business needs.\nAn increased emphasis on innovation in business has led to greater focus on design. Software developers have\nbeen doing design, either explicitly implicitly or explicitly, for over fifty years. What's new is the recognition, not\nonly on the part of IT personnel, but managers in general that design does not apply uniquely to software, but to the\nwhole business process in general. Rather than design software to meet current needs, companies on the leading\nedge are redesigning processes to take advantage of the opportunities software systems provide. And this design is a\n307 joint activity of both business and technical managers, a process of give and take of equals with a common purpose-\nto create a more customer responsive business. Lego provides a recent example. When Lego sold its first robot kit,\nsomeone reverse engineered and published on the web the software which drove the microprocessor brain of the\nrobot. The company's initial reaction was to sue for patent infringement. They then reconsidered the free publicity\nthey got from this individual's efforts, and invited more customers to participate in the design of a new kit, the\nSanta Fe diesel engine. Two hundred volunteers contributed freely of their time, talent, and ideas. Lego planned a\nproduction run of 10,000 kits, and had all of them sold via word of mouth advertising of the two hundred\nparticipants before they even finished the production run. This kind of design cooperation in the \"interaction space\"\nprovided by web technologies contributes immeasurable business value to the company and product satisfaction to\nthe consumer.\nAnd last but not least, professional organizations provide support for systems innovators. The Association for\nComputing Machinery (www.acm.org (http:\/\/www.acm.org)), which is actually an association of professionals, not\nof machines, provides resources that advance computing as a science and profession. The Association for\nInformation Systems (http:\/\/plone.aisnet.org\/) \"is a professional organization whose purpose is to serve as the\npremier global organization for academics specializing in Information Systems.\" It does this through Special\nInterest Groups (SIGs), conferences and publications. The IEEE Computer Society's\n(http:\/\/www.computer.org\/portal\/site\/ieeecs\/index.jsp) \"vision is to be the leading provider of technical\ninformation, community services, and personalized services to the world's computing professionals.\" Participation\nin these professional societies can provide the system innovator a network of like-minded individuals to help him or\nher learn, grow, and succeed.\nChapter editor\nPaul Bauer\n308 "}